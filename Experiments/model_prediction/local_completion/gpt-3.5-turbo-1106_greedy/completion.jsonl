{"namespace": "boto.dynamodb2.table.Table.update_global_secondary_index", "completion": "\n        data = None\n\n        if global_indexes:\n            gsi_data = []\n\n            for gsi_name, gsi_throughput in global_indexes.items():\n                gsi_data.append({\n                    \"Update\": {\n                        \"IndexName\": gsi_name,\n                        \"ProvisionedThroughput\": {\n                            \"ReadCapacityUnits\": int(gsi_throughput['read']),\n                            \"WriteCapacityUnits\": int(gsi_throughput['write']),\n                        },\n                    },\n                })\n\n            if gsi_data:\n                self.connection.update_table(\n                    self.table_name,\n                    global_secondary_index_updates=gsi_data,\n                )\n\n                return True\n            else:\n                msg = 'No global indexes provided for update'\n                boto.log.error(msg)\n                return False\n        else:\n            msg = 'You need to provide the global_indexes to update_global_secondary_index method'\n            boto.log.error(msg)\n            return False", "idx": 0}
{"namespace": "boto.dynamodb2.table.Table.delete", "completion": "        try:\n            self.connection.delete_table(self.table_name)\n            return True\n        except Exception as e:\n            boto.log.error(f\"Error deleting table: {e}\")\n            return False", "idx": 1}
{"namespace": "boto.dynamodb2.table.Table.get_item", "completion": "\n        key = self._encode_keys(kwargs)\n        result = self.connection.get_item(self.table_name, key, consistent, attributes)\n        if result:\n            return Item(result, self)\n        else:\n            raise exceptions.ItemNotFound(\"Item not found in the table\")", "idx": 2}
{"namespace": "boto.dynamodb2.table.Table.has_item", "completion": "        try:\n            self.get_item(**kwargs)\n            return True\n        except exceptions.ItemNotFound:\n            return False", "idx": 3}
{"namespace": "boto.dynamodb2.table.Table._put_item", "completion": "        if not isinstance(item_data, list):\n            item_data = [item_data]\n\n        for item in item_data:\n            item.save(overwrite=True)\n\n        return True", "idx": 4}
{"namespace": "boto.dynamodb2.table.Table.delete_item", "completion": "        raw_key = self._encode_keys(kwargs)\n        kwargs = {}\n\n        if expected is not None:\n            kwargs['expected'] = expected\n\n        if conditional_operator is not None:\n            kwargs['conditional_operator'] = conditional_operator\n\n        self.connection.delete_item(self.table_name, raw_key, **kwargs)\n        return True", "idx": 5}
{"namespace": "boto.dynamodb2.table.Table.get_key_fields", "completion": "        if not self.schema:\n            self.describe()\n        key_fields = [field.name for field in self.schema]\n        return key_fields", "idx": 6}
{"namespace": "boto.dynamodb2.table.Table._build_filters", "completion": "        filters = {}\n        for key, value in filter_kwargs.items():\n            field, operator = key.split('__')\n            if operator not in using:\n                raise ValueError(f\"Invalid operator '{operator}'\")\n            if operator in ('eq', 'ne', 'gte', 'gt', 'lte', 'lt'):\n                filters[field] = {\n                    using[operator]: self._dynamizer.encode(value)\n                }\n            elif operator in ('contains', 'not_contains', 'beginswith'):\n                filters[field] = {\n                    using[operator]: value\n                }\n            elif operator == 'in':\n                filters[field] = {\n                    using[operator]: [self._dynamizer.encode(v) for v in value]\n                }\n            else:\n                raise ValueError(f\"Invalid operator '{operator}'\")\n        return filters", "idx": 7}
{"namespace": "boto.dynamodb2.table.Table.batch_get", "completion": "        results = ResultSet()\n        for key in keys:\n            result = self.get_item(consistent=consistent, attributes=attributes, **key)\n            results.append(result)\n        return results", "idx": 8}
{"namespace": "boto.dynamodb2.table.Table.count", "completion": "        result = self.connection.describe_table(self.table_name)\n        return result['Table']['ItemCount']", "idx": 9}
{"namespace": "boto.dynamodb2.table.BatchTable.put_item", "completion": "        item = {\n            'data': data,\n            'overwrite': overwrite\n        }\n        self._to_put.append(item)\n\n        # Flush if the batch size is reached\n        if len(self._to_put) >= self.table.max_batch_get:\n            self.flush()", "idx": 10}
{"namespace": "boto.dynamodb2.table.BatchTable.delete_item", "completion": "        self._to_delete.append(kwargs)\n\n        if self.should_flush():\n            self.flush()", "idx": 11}
{"namespace": "boto.dynamodb2.table.BatchTable.flush", "completion": "        if self._to_put:\n            items = []\n            for data in self._to_put:\n                item = Item(self.table, data=data)\n                items.append(item.prepare_full())\n\n            self.table.connection.batch_write_item(request_items={self.table.table_name: items})\n\n        if self._to_delete:\n            keys = []\n            for key_data in self._to_delete:\n                raw_key = self.table._encode_keys(key_data)\n                keys.append({'DeleteRequest': {'Key': raw_key}})\n\n            self.table.connection.batch_write_item(request_items={self.table.table_name: keys})\n\n        self._to_put = []\n        self._to_delete = []\n\n        return True", "idx": 12}
{"namespace": "boto.dynamodb2.table.BatchTable.resend_unprocessed", "completion": "        while self._unprocessed:\n            batch_data = {\n                self.table.table_name: self._unprocessed[:25]\n            }\n            resp = self.table.connection.batch_write_item(batch_data)\n            self._unprocessed = resp.get('UnprocessedItems', {}).get(self.table.table_name, [])", "idx": 13}
{"namespace": "boto.dynamodb2.fields.BaseSchemaField.definition", "completion": "        return {\n            'AttributeName': self.name,\n            'AttributeType': self.attr_type\n        }", "idx": 14}
{"namespace": "boto.dynamodb2.fields.BaseIndexField.definition", "completion": "        attribute_definitions = []\n        for part in self.parts:\n            attribute_definitions.append({\n                'AttributeName': part.name,\n                'AttributeType': part.data_type,\n            })\n        return attribute_definitions", "idx": 15}
{"namespace": "boto.dynamodb2.fields.BaseIndexField.schema", "completion": "        key_schema = []\n        for part in self.parts:\n            key_schema.append({\n                'AttributeName': part.name,\n                'KeyType': part.attr_type,\n            })\n\n        return {\n            'IndexName': self.name,\n            'KeySchema': key_schema,\n            'Projection': {\n                'ProjectionType': self.projection_type\n            }\n        }", "idx": 16}
{"namespace": "boto.dynamodb2.fields.GlobalBaseIndexField.schema", "completion": "        base_schema = super(GlobalBaseIndexField, self).schema()\n        base_schema['ProvisionedThroughput'] = {\n            'ReadCapacityUnits': self.throughput['read'],\n            'WriteCapacityUnits': self.throughput['write']\n        }\n        return base_schema", "idx": 17}
{"namespace": "boto.dynamodb2.fields.GlobalIncludeIndex.schema", "completion": "        # Pick up the includes.\n        schema_data = super(GlobalIncludeIndex, self).schema()\n        schema_data.update(super(GlobalBaseIndexField, self).schema())\n        return schema_data", "idx": 18}
{"namespace": "boto.dynamodb2.items.Item.get_keys", "completion": "        return self._data", "idx": 19}
{"namespace": "boto.dynamodb2.items.Item.get_raw_keys", "completion": "        raw_keys = {}\n        for key, value in self._data.items():\n            raw_keys[key] = self._dynamizer.encode(value)\n        return raw_keys", "idx": 20}
{"namespace": "boto.dynamodb2.items.Item.build_expects", "completion": "        expectations = {}\n\n        if fields is None:\n            fields = self.keys()\n\n        for field in fields:\n            if field in self._data:\n                if field in self._orig_data:\n                    if self._data[field] == NEWVALUE:\n                        expectations[field] = {\n                            'Exists': False\n                        }\n                    elif field in self._determine_alterations()['changes']:\n                        expectations[field] = {\n                            'Value': self._dynamizer.encode(self._data[field]),\n                            'Exists': True\n                        }\n                    elif field in self._determine_alterations()['deletes']:\n                        expectations[field] = {\n                            'Exists': False\n                        }\n                    else:\n                        expectations[field] = {\n                            'Exists': True\n                        }\n                else:\n                    expectations[field] = {\n                        'Exists': False\n                    }\n\n        return expectations", "idx": 21}
{"namespace": "boto.dynamodb2.items.Item.prepare_full", "completion": "        prepared_data = {}\n        for key, value in self._data.items():\n            prepared_data[key] = self._dynamizer.encode(value)\n        return prepared_data", "idx": 22}
{"namespace": "boto.dynamodb2.items.Item.prepare_partial", "completion": "        alterations = self._determine_alterations()\n        final_data = {}\n\n        for key, value in alterations['adds'].items():\n            if self._is_storable(value):\n                final_data[key] = self._dynamizer.encode(value)\n\n        for key, value in alterations['changes'].items():\n            if self._is_storable(value):\n                final_data[key] = self._dynamizer.encode(value)\n\n        return final_data, set(alterations['adds'].keys()).union(set(alterations['changes'].keys()).union(set(alterations['deletes'])))", "idx": 23}
{"namespace": "boto.dynamodb2.items.Item.partial_save", "completion": "        if not self.needs_save():\n            return False\n\n        final_data, fields = self.prepare_partial()\n        expects = self.build_expects(fields)\n\n        try:\n            self.table.update_item(\n                self.get_raw_keys(),\n                final_data,\n                expects\n            )\n            self.mark_clean()\n            return True\n        except Exception as e:\n            print(f\"Partial save failed: {e}\")\n            return False", "idx": 24}
{"namespace": "boto.dynamodb2.items.Item.save", "completion": "        if not overwrite and not self.needs_save():\n            return False\n\n        if overwrite:\n            return self.table._put_item(self.prepare_full())\n        else:\n            return self.partial_save()", "idx": 25}
{"namespace": "boto.dynamodb2.items.Item.delete", "completion": "        key = self.get_keys()\n        return self.table._delete_item(key)", "idx": 26}
{"namespace": "boto.dynamodb2.connect_to_region", "completion": "        from boto.dynamodb2.layer1 import DynamoDBConnection\n    from boto.dynamodb2.layer1 import DynamoDBConnection\n    region = RegionInfo(name=region_name, endpoint='dynamodb.' + region_name + '.amazonaws.com')\n    return DynamoDBConnection(region=region, **kw_params)", "idx": 27}
{"namespace": "litecli.packages.parseutils.extract_tables", "completion": "\n    parsed = sqlparse.parse(sql)[0]\n    stream = extract_from_part(parsed, stop_at_punctuation=True)\n    tables = list(extract_table_identifiers(stream))\n    return tables", "idx": 28}
{"namespace": "litecli.packages.parseutils.queries_start_with", "completion": "    for query in queries:\n        if query_starts_with(query, prefixes):\n            return True\n    return False", "idx": 29}
{"namespace": "litecli.packages.parseutils.is_destructive", "completion": "    destructive_prefixes = [\"DROP\", \"DELETE\", \"TRUNCATE\", \"ALTER\", \"UPDATE\"]\n    return queries_start_with(queries, destructive_prefixes)", "idx": 30}
{"namespace": "litecli.packages.completion_engine.suggest_type", "completion": "    # Check if the text before the cursor is a special command\n    special_command = parse_special_command(full_text)\n    if special_command:\n        return special_command\n\n    # Check if the text before the cursor is a SQL keyword\n    prev_keyword = find_prev_keyword(text_before_cursor)\n    if prev_keyword:\n        return [{\"type\": \"keyword\", \"scope\": None}]\n\n    # Check if the text before the cursor is a table or column name\n    tables = extract_tables(full_text)\n    if tables:\n        return [{\"type\": \"table\", \"scope\": None}, {\"type\": \"column\", \"scope\": None}]\n\n    # Check if the text before the cursor is a comparison operator\n    parsed = sqlparse.parse(full_text)\n    where = None\n    for stmt in parsed:\n        for token in stmt.tokens:\n            if isinstance(token, Where):\n                where = token\n                break\n    if where:\n        for token in where.tokens:\n            if isinstance(token, Comparison):\n                return [{\"type\": \"comparison\", \"scope\": None}]\n\n    # If none of the above conditions are met, return an empty list\n    return []", "idx": 31}
{"namespace": "datasette.plugins.get_plugins", "completion": "    plugins_info = []\n    for plugin in pm.get_plugins():\n        info = {\n            \"name\": plugin.name,\n            \"static_path\": plugin.get_path(\"static\"),\n            \"templates_path\": plugin.get_path(\"templates\"),\n            \"hooks\": [hook for hook in plugin.hooks],\n            \"version\": getattr(plugin, \"__version__\", None),\n            \"project_name\": getattr(plugin, \"__project_name__\", None)\n        }\n        plugins_info.append(info)\n    return plugins_info", "idx": 32}
{"namespace": "datasette.facets.ColumnFacet.suggest", "completion": "        row_count = await self.get_row_count()\n        columns = await self.get_columns(self.sql, self.params)\n        facet_size = self.get_facet_size()\n        suggested_facets = []\n        for column in columns:\n            column_name = column.name\n            distinct_values_query = f\"select {column_name}, count(*) from ({self.sql}) group by {column_name}\"\n            distinct_values_result = await self.ds.execute(self.database, distinct_values_query, self.params)\n            distinct_values = distinct_values_result.rows\n            if 1 < len(distinct_values) <= row_count and len(distinct_values) <= facet_size:\n                for value, count in distinct_values:\n                    if count > 1:\n                        toggle_url = path_with_added_args(self.request, {\"_facet\": column_name})\n                        suggested_facets.append({\"column\": column_name, \"toggle_url\": toggle_url})\n                return suggested_facets", "idx": 33}
{"namespace": "datasette.facets.ColumnFacet.facet_results", "completion": "        facet_size = self.get_facet_size()\n        results = []\n        timed_out = []\n        for config in self.get_configs():\n            column = config[\"config\"][\"simple\"]\n            facet_sql = f\"\"\"\n                select {escape_sqlite(column)} as value, count(*) as n\n                from ({self.sql}) where {escape_sqlite(column)} is not null\n                group by value\n                order by n desc\n                limit {facet_size + 1}\n            \"\"\"\n            try:\n                facet_values = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=True,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n                if len(facet_values) > facet_size:\n                    facet_values = facet_values[:facet_size]\n                    truncated = True\n                else:\n                    truncated = False\n                results.append(\n                    {\n                        \"name\": column,\n                        \"results\": [\n                            {\n                                \"value\": row[\"value\"],\n                                \"count\": row[\"n\"],\n                                \"toggle_url\": self.ds.absolute_url(\n                                    self.request,\n                                    self.ds.urls.path(\n                                        path_with_added_args(\n                                            self.request, {\"_facet\": column, \"_facet_values\": row[\"value\"]}\n                                        )\n                                    ),\n                                ),\n                            }\n                            for row in facet_values\n                        ],\n                        \"truncated\": truncated,\n                    }\n                )\n            except QueryInterrupted:\n                timed_out.append(column)\n        return results, timed_out", "idx": 34}
{"namespace": "datasette.facets.ArrayFacet.suggest", "completion": "        row_count = await self.get_row_count()\n        columns = await self.get_columns(self.sql, self.params)\n        suggested_facets = []\n        already_enabled = [c[\"config\"][\"simple\"] for c in self.get_configs()]\n        for column in columns:\n            if column in already_enabled:\n                continue\n            suggested_facet_sql = \"\"\"\n                select {column} as value from (\n                    {sql}\n                ) where {column} is not null\n                limit 100\n            \"\"\".format(\n                column=escape_sqlite(column), sql=self.sql\n            )\n            distinct_values = None\n            try:\n                distinct_values = await self.ds.execute(\n                    self.database,\n                    suggested_facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_suggest_time_limit_ms\"),\n                )\n                all_arrays = all(isinstance(row[\"value\"], str) and row[\"value\"].startswith(\"[\") and row[\"value\"].endswith(\"]\") for row in distinct_values)\n                if all_arrays:\n                    suggested_facets.append(\n                        {\n                            \"name\": column,\n                            \"toggle_url\": self.ds.absolute_url(\n                                self.request,\n                                self.ds.urls.path(\n                                    path_with_added_args(\n                                        self.request, {\"_facet\": column}\n                                    )\n                                ),\n                            ),\n                        }\n                    )\n            except QueryInterrupted:\n                continue\n        return suggested_facets", "idx": 35}
{"namespace": "datasette.facets.ArrayFacet.facet_results", "completion": "        facet_results = []\n        facets_timed_out = []\n\n        for source_and_config in self.get_configs():\n            config = source_and_config[\"config\"]\n            source = source_and_config[\"source\"]\n            column = config.get(\"column\") or config[\"simple\"]\n            facet_sql = \"\"\"\n                select {col} as value, count(*) as count from (\n                    {sql}\n                )\n                where {col} is not null\n                group by {col} order by count desc, value\n            \"\"\".format(\n                col=escape_sqlite(column), sql=self.sql\n            )\n            try:\n                facet_rows_results = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n                facet_results_values = []\n                facet_results.append(\n                    {\n                        \"name\": column,\n                        \"type\": self.type,\n                        \"hideable\": source != \"metadata\",\n                        \"toggle_url\": self.ds.urls.path(\n                            path_with_removed_args(self.request, {\"_facet_array\": column})\n                        ),\n                        \"results\": facet_results_values,\n                    }\n                )\n                facet_rows = facet_rows_results.rows\n                for row in facet_rows:\n                    facet_results_values.append(\n                        {\n                            \"value\": row[\"value\"],\n                            \"count\": row[\"count\"],\n                        }\n                    )\n            except QueryInterrupted:\n                facets_timed_out.append(column)\n\n        return facet_results, facets_timed_out", "idx": 36}
{"namespace": "datasette.facets.DateFacet.facet_results", "completion": "        facet_results = []\n        facets_timed_out = []\n\n        facet_size = self.get_facet_size()\n        for source_and_config in self.get_configs():\n            config = source_and_config[\"config\"]\n            source = source_and_config[\"source\"]\n            column = config.get(\"column\") or config[\"simple\"]\n            facet_sql = \"\"\"\n                select date({col}) as value, count(*) as count from (\n                    {sql}\n                )\n                where {col} is not null\n                group by value order by count desc, value limit {limit}\n            \"\"\".format(\n                col=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            try:\n                facet_rows_results = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n                facet_results_values = []\n                facet_results.append(\n                    {\n                        \"name\": column,\n                        \"type\": self.type,\n                        \"results\": facet_results_values,\n                        \"hideable\": source != \"metadata\",\n                        \"toggle_url\": self.ds.urls.path(\n                            path_with_removed_args(self.request, {\"_facet_date\": column})\n                        ),\n                        \"truncated\": len(facet_rows_results) > facet_size,\n                    }\n                )\n                facet_rows = facet_rows_results.rows[:facet_size]\n                pairs = self.get_querystring_pairs()\n                for row in facet_rows:\n                    value = str(row[\"value\"])\n                    selected = (f\"{column}__date\": value) in pairs\n                    if selected:\n                        toggle_path = path_with_removed_args(\n                            self.request, {f\"{column}__date\": value}\n                        )\n                    else:\n                        toggle_path = path_with_added_args(\n                            self.request, {f\"{column}__date\": value}\n                        )\n                    facet_results_values.append(\n                        {\n                            \"value\": value,\n                            \"count\": row[\"count\"],\n                            \"toggle_url\": self.ds.absolute_url(\n                                self.request, self.ds.urls.path(toggle_path)\n                            ),\n                            \"selected\": selected,\n                        }\n                    )\n            except QueryInterrupted:\n                facets_timed_out.append(column)\n\n        return facet_results, facets_timed_out", "idx": 37}
{"namespace": "datasette.app.Datasette.invoke_startup", "completion": "        if self._startup_invoked:\n            return\n        self._startup_invoked = True\n        # Refresh schemas\n        await self.refresh_schemas()\n        # Load plugins\n        pm.hook.startup(datasette=self)", "idx": 38}
{"namespace": "datasette.app.Datasette.get_database", "completion": "        if route:\n            for db in self.databases.values():\n                if db.routes and route in db.routes:\n                    return db\n        else:\n            for db in self.databases.values():\n                if db.name != \"_internal\":\n                    return db", "idx": 39}
{"namespace": "datasette.app.Datasette.add_database", "completion": "        databases_copy = self.databases.copy()\n        if name is None:\n            name = f\"db_{len(databases_copy)}\"\n        else:\n            original_name = name\n            count = 1\n            while name in databases_copy:\n                name = f\"{original_name}_{count}\"\n                count += 1\n        db.name = name\n        db.route = route or name\n        databases_copy[name] = db\n        self.databases = databases_copy\n        return db", "idx": 40}
{"namespace": "datasette.app.Datasette.ensure_permissions", "completion": "        for permission in permissions:\n            if isinstance(permission, str):\n                action = permission\n                resource = None\n            else:\n                action, resource = permission\n            if not await self.permission_allowed(actor, action, resource):\n                raise Forbidden", "idx": 41}
{"namespace": "datasette.app.Datasette.check_visibility", "completion": "\n        if permissions:\n            await self.ensure_permissions(actor, permissions)\n\n        if action and resource:\n            visible = await self.permission_allowed(actor, action, resource)\n            private = not visible\n        else:\n            visible = True\n            private = False\n\n        return visible, private", "idx": 42}
{"namespace": "datasette.app.Datasette.render_template", "completion": "        if not self._startup_invoked:\n            raise StartupError(\"invoke .invoke_startup() before rendering templates\")\n\n        # Prepare the context\n        if context is None:\n            context = {}\n        context.update(\n            {\n                \"datasette\": self,\n                \"request\": request,\n                \"view_name\": view_name,\n            }\n        )\n\n        # Call hooks to get extra body scripts and template variables\n        extra_body_script = []\n        extra_template_vars = {}\n        for hook in pm.hook.extra_body_script(datasette=self, request=request):\n            extra_body_script.extend(await await_me_maybe(hook))\n        for hook in pm.hook.extra_template_vars(datasette=self, request=request):\n            extra_template_vars.update(await await_me_maybe(hook))\n\n        context.update(extra_template_vars)\n\n        # Render the template\n        if isinstance(templates, str):\n            templates = [templates]\n        for template in templates:\n            try:\n                template = self.jinja_env.get_template(template)\n                rendered_template = await template.render_async(context)\n                return rendered_template\n            except TemplateNotFound:\n                pass\n\n        raise TemplateNotFound(f\"Template(s) {templates} not found\")", "idx": 43}
{"namespace": "datasette.app.DatasetteClient.get", "completion": "        async with httpx.AsyncClient() as client:\n            response = await client.get(self._fix(path), **kwargs)\n            return response", "idx": 44}
{"namespace": "datasette.utils.asgi.Request.full_path", "completion": "        if self.query_string:\n            return f\"{self.path}?{self.query_string}\"\n        else:\n            return self.path", "idx": 45}
{"namespace": "datasette.utils.asgi.Request.post_body", "completion": "        body = b\"\"\n        more_body = True\n        while more_body:\n            message = await self.receive()\n            message_type = message[\"type\"]\n            if message_type == \"http.request\":\n                body += message.get(\"body\", b\"\")\n            elif message_type == \"http.disconnect\":\n                more_body = False\n        return body", "idx": 46}
{"namespace": "datasette.utils.asgi.Request.fake", "completion": "        url_parts = list(urlparse(path_with_query_string))\n        url_parts[0] = scheme\n        url_parts[1] = url_parts[1] or \"localhost\"\n        url_parts[2] = url_parts[2] or \"/\"\n        url_parts[3] = \"\"\n        url_parts[4] = \"\"\n        url_parts[5] = \"\"\n        scope = {\n            \"type\": \"http\",\n            \"http_version\": \"1.1\",\n            \"method\": method,\n            \"scheme\": scheme,\n            \"path\": url_parts[2],\n            \"query_string\": url_parts[4].encode(\"latin-1\"),\n            \"headers\": [],\n            \"client\": None,\n            \"server\": None,\n            \"actor\": None,\n            \"url_route\": {\"kwargs\": url_vars or {}},\n        }\n        return cls(scope, None)", "idx": 47}
{"namespace": "datasette.utils.asgi.Response.asgi_send", "completion": "        # Remove any existing content-type header\n        headers = {k: v for k, v in self.headers.items() if k.lower() != \"content-type\"}\n        headers[\"content-type\"] = self.content_type\n        await send(\n            {\n                \"type\": \"http.response.start\",\n                \"status\": self.status,\n                \"headers\": [\n                    [key.encode(\"utf-8\"), value.encode(\"utf-8\")]\n                    for key, value in headers.items()\n                ],\n            }\n        )\n        await send({\"type\": \"http.response.body\", \"body\": self.body.encode(\"utf-8\")})", "idx": 48}
{"namespace": "datasette.utils.asgi.Response.set_cookie", "completion": "        cookie = SimpleCookie()\n        cookie[key] = value\n        if max_age is not None:\n            cookie[key][\"max-age\"] = max_age\n        if expires is not None:\n            cookie[key][\"expires\"] = expires\n        cookie[key][\"path\"] = path\n        if domain is not None:\n            cookie[key][\"domain\"] = domain\n        if secure:\n            cookie[key][\"secure\"] = True\n        if httponly:\n            cookie[key][\"httponly\"] = True\n        if samesite in SAMESITE_VALUES:\n            cookie[key][\"samesite\"] = samesite\n        self._set_cookie_headers.append(cookie[key].OutputString())", "idx": 49}
{"namespace": "datasette.utils.asgi.Response.html", "completion": "        headers = headers or {}\n        headers[\"content-type\"] = \"text/html; charset=utf-8\"\n        return cls(body, status, headers, \"text/html; charset=utf-8\")", "idx": 50}
{"namespace": "datasette.utils.asgi.Response.text", "completion": "        return cls(\n            body,\n            status=status,\n            headers=headers,\n            content_type=\"text/plain; charset=utf-8\",\n        )", "idx": 51}
{"namespace": "datasette.utils.asgi.Response.json", "completion": "        headers = headers or {}\n        headers[\"content-type\"] = \"application/json; charset=utf-8\"\n        return cls(\n            json.dumps(body, default=default),\n            status=status,\n            headers=headers,\n            content_type=\"application/json; charset=utf-8\",\n        )", "idx": 52}
{"namespace": "datasette.utils.asgi.Response.redirect", "completion": "        headers = headers or {}\n        headers[\"Location\"] = path\n        return cls(\"\", status=status, headers=headers)", "idx": 53}
{"namespace": "datasette.utils.validate_sql_select", "completion": "    # Remove comments from the SQL statement\n    sql = re.sub(comment_re, \"\", sql)\n    # Convert the SQL statement to lowercase\n    sql = sql.lower()\n    \n    # Check if the SQL statement matches any of the allowed SQL patterns\n    if not any(allowed_pattern.match(sql) for allowed_pattern in allowed_sql_res):\n        raise InvalidSql(\"Invalid SQL SELECT statement\")\n    \n    # Check if the SQL statement matches any of the disallowed SQL patterns\n    for disallowed_pattern, error_message in disallawed_sql_res:\n        if disallowed_pattern.match(sql):\n            raise InvalidSql(error_message)", "idx": 54}
{"namespace": "datasette.utils.escape_css_string", "completion": "    # Replace special characters with their Unicode escape sequences\n    escaped_string = \"\"\n    for char in s:\n        if char == \"'\":\n            escaped_string += \"\\\\27\"\n        elif char == '\"':\n            escaped_string += \"\\\\22\"\n        elif char == \"\\n\":\n            escaped_string += \"\\\\A \"\n        elif char == \"\\\\\":\n            escaped_string += \"\\\\5C\"\n        else:\n            escaped_string += char\n    return escaped_string", "idx": 55}
{"namespace": "datasette.utils.detect_fts", "completion": "    cursor = conn.execute(\n        \"SELECT name FROM sqlite_master WHERE type='table' AND name LIKE ?\",\n        (f\"{table}%\",),\n    )\n    fts_tables = [row[0] for row in cursor.fetchall() if row[0].endswith(\"_fts\")]\n    return fts_tables[0] if fts_tables else None", "idx": 56}
{"namespace": "datasette.utils.is_url", "completion": "    if isinstance(value, str) and (value.startswith(\"http://\") or value.startswith(\"https://\")) and not whitespace_re.search(value):\n        return True\n    else:\n        return False", "idx": 57}
{"namespace": "datasette.utils.to_css_class", "completion": "    if css_class_re.match(s):\n        return s\n    else:\n        stripped = css_invalid_chars_re.sub(\"\", s)\n        md5_suffix = hashlib.md5(s.encode()).hexdigest()[:6]\n        return f\"{stripped}_{md5_suffix}\"", "idx": 58}
{"namespace": "datasette.utils.escape_fts", "completion": "    if query.count('\"') % 2 == 1:\n        query += '\"'\n\n    # Split the query into individual terms using a regular expression\n    terms = _escape_fts_re.findall(query)\n\n    # Remove any empty or duplicate terms\n    terms = list(OrderedDict.fromkeys(terms))\n\n    # Join the terms back together with double quotes around each term\n    return ' '.join(f'\"{term}\"' for term in terms)", "idx": 59}
{"namespace": "datasette.utils.check_connection", "completion": "    try:\n        # Check the connection by executing a query to retrieve the names of all tables in the database\n        cursor = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n        tables = [row[0] for row in cursor.fetchall()]\n\n        # For each table, execute another query to retrieve the table information using the PRAGMA statement\n        for table in tables:\n            cursor = conn.execute(f\"PRAGMA table_info({escape_sqlite(table)});\")\n            cursor.fetchall()\n\n    except sqlite3.OperationalError as e:\n        if \"no such table\" in str(e):\n            raise sqlite3.OperationalError(\"The database does not contain any tables\")\n        elif \"unable to open database file\" in str(e):\n            raise sqlite3.OperationalError(\"Unable to open the database file\")\n        elif \"file is encrypted or is not a database\" in str(e):\n            raise sqlite3.OperationalError(\"The file is encrypted or is not a valid SQLite database\")\n        else:\n            raise ConnectionProblem(f\"Error while checking the connection: {e}\")\n\n    except sqlite3.DatabaseError as e:\n        if \"not authorized\" in str(e):\n            raise sqlite3.DatabaseError(\"Not authorized to access the database\")\n        else:\n            raise ConnectionProblem(f\"Error while checking the connection: {e}\")\n\n    except sqlite3.InterfaceError as e:\n        if \"unable to open database file\" in str(e):\n            raise sqlite3.InterfaceError(\"Unable to open the database file\")\n        else:\n            raise ConnectionProblem(f\"Error while checking the connection: {e}\")\n\n    except sqlite3.InternalError as e:\n        raise ConnectionProblem(f\"Internal error while checking the connection: {e}\")\n\n    except sqlite3.IntegrityError as e:\n        raise ConnectionProblem(f\"Integrity error while checking the connection: {e}\")\n\n    except sqlite3.ProgrammingError as e:\n        raise ConnectionProblem(f\"Programming error while checking the connection: {e}\")\n\n    except sqlite3.DataError as e:\n        raise ConnectionProblem(f\"Data error while checking the connection: {e}\")\n\n    except sqlite3.NotSupportedError as e:\n        raise ConnectionProblem(f\"Operation not supported while checking the connection: {e}\")", "idx": 60}
{"namespace": "datasette.utils.parse_metadata", "completion": "    try:\n        # Try parsing the content as JSON\n        return json.loads(content)\n    except json.JSONDecodeError:\n        try:\n            # Try parsing the content as YAML\n            return yaml.safe_load(content)\n        except yaml.YAMLError:\n            # If the content is neither valid JSON nor valid YAML, raise an error\n            raise BadMetadataError(\"Invalid metadata format - must be valid JSON or YAML\")", "idx": 61}
{"namespace": "datasette.utils.call_with_supported_arguments", "completion": "\n    call_with = _gather_arguments(fn, kwargs)\n    return fn(*call_with)", "idx": 62}
{"namespace": "datasette.utils.derive_named_parameters", "completion": "    sql = sql.rstrip(\";\")  # Strip trailing semicolon\n    named_parameters = _re_named_parameter.findall(sql)  # Find all named parameters in the query\n    try:\n        # Execute the \"explain\" statement on the database with named parameters set to None\n        await db.execute(\"explain \" + sql, {param: None for param in named_parameters})\n        # Return the list of named parameters identified as variables in the \"explain\" results\n        return [param[1:] for param in named_parameters]  # Remove the leading \":\" character\n    except Exception as e:\n        # Return the list of possible named parameters found in the input SQL query if there is an error\n        return named_parameters", "idx": 63}
{"namespace": "pyramid.path.Resolver.get_package_name", "completion": "        if self.package is CALLER_PACKAGE:\n            return package_name(caller_package())\n        else:\n            return self.package.__name__", "idx": 64}
{"namespace": "pyramid.path.Resolver.get_package", "completion": "        if self.package is CALLER_PACKAGE:\n            return caller_package()\n        else:\n            return self.package", "idx": 65}
{"namespace": "pyramid.path.DottedNameResolver.resolve", "completion": "        if ':' in dotted:\n            return pkg_resources.EntryPoint.parse('x=' + dotted).load(False)\n        else:\n            return pkg_resources.EntryPoint.parse('x=' + dotted).resolve()", "idx": 66}
{"namespace": "pyramid.path.DottedNameResolver.maybe_resolve", "completion": "        if isinstance(dotted, str):\n            return self.resolve(dotted)\n        else:\n            return dotted", "idx": 67}
{"namespace": "pyramid.path.PkgResourcesAssetDescriptor.abspath", "completion": "        return pkg_resources.resource_filename(self.pkg_name, self.path)", "idx": 68}
{"namespace": "pyramid.renderers.render_to_response", "completion": "    try:\n        registry = request.registry\n    except AttributeError:\n        registry = None\n    if package is None:\n        package = caller_package()\n    helper = RendererHelper(\n        name=renderer_name, package=package, registry=registry\n    )\n\n    with hide_attrs(request, 'response'):\n        result = helper.render(value, response, request=request)\n\n    return result", "idx": 69}
{"namespace": "pyramid.renderers.JSON.add_adapter", "completion": "        self.components.registerAdapter(adapter, (type_or_iface,), IJSONAdapter)", "idx": 70}
{"namespace": "pyramid.renderers.RendererHelper.settings", "completion": "        if self.registry is not None:\n            return self.registry.settings\n        else:\n            return {}", "idx": 71}
{"namespace": "pyramid.renderers.RendererHelper.render_view", "completion": "        system = {\n            'view': view,\n            'renderer_name': getattr(view, '__name__', None),\n            'renderer_info': None,\n            'context': context,\n            'request': request,\n            'csrf_token': get_csrf_token(request)\n        }\n\n        renderer = self.get_renderer()\n        result = renderer(system)\n\n        response.body = result", "idx": 72}
{"namespace": "pyramid.renderers.RendererHelper.render", "completion": "        system = {\n            'view': system_values.get('view'),\n            'renderer_name': self.name,\n            'renderer_info': self,\n            'context': system_values.get('context'),\n            'request': request,\n            'req': request,\n            'get_csrf_token': partial(get_csrf_token, request),\n        }\n        self.registry.notify(system)\n        return self.renderer(value, system)", "idx": 73}
{"namespace": "pyramid.renderers.RendererHelper.render_to_response", "completion": "        renderer = self.renderer\n        if system_values is None:\n            system_values = {\n                'view': None,\n                'renderer_name': self.name,  # b/c\n                'renderer_info': self,\n                'context': getattr(request, 'context', None),\n                'request': request,\n                'req': request,\n                'get_csrf_token': partial(get_csrf_token, request),\n            }\n\n        result = renderer(value, system_values)\n\n        if request is not None:\n            response = request.response\n            response.body = result\n            return response\n        else:\n            return result", "idx": 74}
{"namespace": "pyramid.renderers.RendererHelper.clone", "completion": "        name = name or self.name\n        package = package or self.package\n        registry = registry or self.registry\n        return RendererHelper(name, package, registry)", "idx": 75}
{"namespace": "pyramid.urldispatch.RoutesMapper.get_routes", "completion": "        routes = self.routelist\n        if include_static:\n            routes.extend(self.static_routes)\n        return routes", "idx": 76}
{"namespace": "pyramid.urldispatch.RoutesMapper.connect", "completion": "        route = Route(name, pattern, factory, predicates, pregenerator)\n        if static:\n            self.static_routes.append(route)\n        else:\n            self.routelist.append(route)\n        self.routes[name] = route\n        return route", "idx": 77}
{"namespace": "pyramid.testing.DummyTemplateRenderer.assert_", "completion": "        for key, value in kw.items():\n            if self._received.get(key) != value:\n                raise AssertionError(f\"Expected {key}={value}, but received {key}={self._received.get(key)}\")\n        return True", "idx": 78}
{"namespace": "pyramid.testing.DummyResource.__getitem__", "completion": "        return self.subs[name]", "idx": 79}
{"namespace": "pyramid.testing.DummyResource.clone", "completion": "        clone = copy.copy(self)  # Create a shallow copy of the current instance\n        if __name__ is not _marker:\n            clone.__name__ = __name__  # Override the __name__ attribute if provided\n        if __parent__ is not _marker:\n            clone.__parent__ = __parent__  # Override the __parent__ attribute if provided\n        clone.kw.update(kw)  # Update the keyword arguments of the clone\n        clone.__dict__.update(kw)  # Update the attributes of the clone\n        return clone  # Return the cloned instance", "idx": 80}
{"namespace": "pyramid.testing.DummySession.get_csrf_token", "completion": "        if '_csrft_' in self:\n            return self['_csrft_']\n        else:\n            return self.new_csrf_token()", "idx": 81}
{"namespace": "pyramid.testing.DummyRequest.response", "completion": "        \"\"\"\n        This function returns the response generated by the response factory using the input DummyRequest instance as the argument.\n        Input-Output Arguments\n        :param self: DummyRequest. An instance of the DummyRequest class.\n        :return: The response generated by the response factory function.\n        \"\"\"\n        registry = self.registry\n        response_factory = registry.queryUtility(IResponseFactory)\n        if response_factory is None:\n            response_factory = Response\n        response = response_factory()\n        return response", "idx": 82}
{"namespace": "pyramid.testing.DummyRendererFactory.add", "completion": "        self.renderers[spec] = renderer", "idx": 83}
{"namespace": "pyramid.authorization.ACLAuthorizationPolicy.principals_allowed_by_permission", "completion": "        acl = getattr(context, '__acl__', [])\n        allowed_principals = set()\n        \n        for ace in acl:\n            if isinstance(ace, tuple) and len(ace) >= 3:\n                if ace[1] == permission and ace[2] == Allow:\n                    allowed_principals.add(ace[0])\n        \n        for location in lineage(context):\n            acl = getattr(location, '__acl__', [])\n            for ace in acl:\n                if isinstance(ace, tuple) and len(ace) >= 3:\n                    if ace[1] == permission and ace[2] == Allow:\n                        allowed_principals.add(ace[0])\n        \n        return allowed_principals", "idx": 84}
{"namespace": "pyramid.url.URLMethodsMixin.route_url", "completion": "        app_url, qs, frag = parse_url_overrides(self, kw)\n        registry = get_current_registry()\n        if registry is not None:\n            info = registry.queryUtility(IResourceURL, name=route_name)\n            if info is None:\n                info = registry.queryUtility(IStaticURLInfo)\n            if info is not None:\n                return info.generate(self, route_name, elements, kw)\n        raise KeyError('No such route named %s' % route_name)", "idx": 85}
{"namespace": "pyramid.predicates.CustomPredicate.text", "completion": "        if hasattr(self.func, '__text__'):\n            return self.func.__text__\n        else:\n            return 'CustomPredicate function'", "idx": 86}
{"namespace": "pyramid.threadlocal.ThreadLocalManager.pop", "completion": "        if self.stack:\n            return self.stack.pop()\n        else:\n            return self.default", "idx": 87}
{"namespace": "pyramid.threadlocal.ThreadLocalManager.get", "completion": "        if self.stack:\n            return self.stack[-1]\n        else:\n            return self.default()", "idx": 88}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.authenticated_userid", "completion": "        debug = self.debug\n        userid = self.unauthenticated_userid(request)\n        if userid is None:\n            debug and self._log(\n                'call to unauthenticated_userid returned None; returning None',\n                'authenticated_userid',\n                request,\n            )\n            return None\n        if self._clean_principal(userid) is None:\n            debug and self._log(\n                (\n                    'use of userid %r is disallowed by any built-in Pyramid '\n                    'security policy, returning None' % userid\n                ),\n                'authenticated_userid',\n                request,\n            )\n            return None\n\n        if self.callback is None:\n            debug and self._log(\n                'there was no groupfinder callback; returning %r' % (userid,),\n                'authenticated_userid',\n                request,\n            )\n            return userid\n        callback_ok = self.callback(userid, request)\n        if callback_ok is not None:  # is not None!\n            debug and self._log(\n                'groupfinder callback returned %r; returning %r'\n                % (callback_ok, userid),\n                'authenticated_userid',\n                request,\n            )\n            return userid\n        debug and self._log(\n            'groupfinder callback returned None; returning None',\n            'authenticated_userid',\n            request,\n        )", "idx": 89}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.unauthenticated_userid", "completion": "        identity = self._get_identity(request)\n        if identity is not None:\n            return identity.get('repoze.who.userid')\n        return None", "idx": 90}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.forget", "completion": "        headers = []\n        identifier = self._get_identifier(request)\n        if identifier is not None:\n            environ = request.environ\n            headers = identifier.forget(environ)\n        return headers", "idx": 91}
{"namespace": "pyramid.authentication.AuthTktAuthenticationPolicy.unauthenticated_userid", "completion": "        \"\"\"\n        This function retrieves the user ID from the auth_tkt cookie.\n        Input-Output Arguments\n        :param self: AuthTktAuthenticationPolicy. An instance of the AuthTktAuthenticationPolicy class.\n        :param request: The request object.\n        :return: The user ID extracted from the auth_tkt cookie.\n        \"\"\"\n        auth_tkt = request.cookies.get(self.cookie.cookie_name)\n        if auth_tkt:\n            try:\n                userid = self.cookie.identify(auth_tkt)\n                return userid\n            except ValueError:\n                return None\n        else:\n            return None", "idx": 92}
{"namespace": "googleapiclient.channel.Channel.update", "completion": "        for key, value in resp.items():\n            if key in CHANNEL_PARAMS:\n                setattr(self, CHANNEL_PARAMS[key], value)", "idx": 93}
{"namespace": "googleapiclient.channel.notification_from_headers", "completion": "    headers = _upper_header_keys(headers)\n    message_number = int(headers.get(X_GOOG_MESSAGE_NUMBER, 0))\n    state = headers.get(X_GOOG_RESOURCE_STATE)\n    uri = headers.get(X_GOOG_RESOURCE_URI)\n    resource_id = headers.get(X_GOOG_RESOURCE_ID)\n\n    if not state or not uri or not resource_id:\n        raise ValueError(\"Invalid notification\")\n\n    return Notification(message_number, state, uri, resource_id)", "idx": 94}
{"namespace": "googleapiclient.channel.new_webhook_channel", "completion": "    if expiration is not None:\n        expiration = int((expiration - EPOCH).total_seconds() * 1000)\n    id = str(uuid.uuid4())\n    return Channel(\"web_hook\", id, token, url, expiration, params)", "idx": 95}
{"namespace": "googleapiclient.model.BaseModel._build_query", "completion": "        if self.alt_param:\n            params[\"alt\"] = self.alt_param\n\n        query_params = []\n        for key, value in params.items():\n            if isinstance(value, list):\n                for v in value:\n                    query_params.append((key, v.encode(\"utf-8\") if callable(v) else v))\n            else:\n                query_params.append((key, value.encode(\"utf-8\") if callable(value) else value))\n\n        return urllib.parse.urlencode(query_params)", "idx": 96}
{"namespace": "googleapiclient.model.BaseModel.response", "completion": "        \"\"\"\n        Convert the response wire format into a Python object.\n\n        Args:\n          resp: httplib2.Response, the HTTP response headers and status\n          content: string, the body of the HTTP response\n\n        Returns:\n          The body de-serialized as a Python object.\n\n        Raises:\n          googleapiclient.errors.HttpError if a non 2xx response is received.\n        \"\"\"\n        _abstract()", "idx": 97}
{"namespace": "googleapiclient.model.makepatch", "completion": "    patch = {}\n    for key in modified:\n        if key in original and modified[key] != original[key]:\n            patch[key] = modified[key]\n    return patch", "idx": 98}
{"namespace": "googleapiclient._helpers.update_query_params", "completion": "    parsed_uri = urllib.parse.urlparse(uri)\n    parsed_params = urllib.parse.parse_qs(parsed_uri.query)\n\n    for key, value in params.items():\n        parsed_params[key] = [value]\n\n    updated_query = urllib.parse.urlencode(parsed_params, doseq=True)\n    updated_uri = urllib.parse.urlunparse(\n        (parsed_uri.scheme, parsed_uri.netloc, parsed_uri.path, parsed_uri.params, updated_query, parsed_uri.fragment)\n    )\n\n    return updated_uri", "idx": 99}
{"namespace": "googleapiclient._helpers._add_query_parameter", "completion": "    if value is None:\n        return url\n    else:\n        return update_query_params(url, {name: value})", "idx": 100}
{"namespace": "gif_for_cli.display.display_txt_frames", "completion": "    try:\n        for _ in range(num_loops):\n            for frame in txt_frames:\n                stdout.write(frame)\n                time.sleep(seconds_per_frame)\n                stdout.flush()\n                stdout.write('\\n')\n    except Exception as e:\n        raise KeyboardInterrupt(e)", "idx": 101}
{"namespace": "pyVmomi.SoapAdapter.SoapResponseDeserializer.Deserialize", "completion": "        parser = ParserCreate(namespace_separator=NS_SEP)\n        self.deser.Deserialize(parser, resultType, nsMap=nsMap)\n        if isinstance(response, str):\n            parser.Parse(response)\n        else:\n            parser.ParseFile(response)\n        result = self.deser.GetResult()\n\n        # Handle SOAP Fault\n        if isinstance(result, DataObject) and result._wsdlName == 'Fault':\n            fault = result\n            raise Exception(f\"SOAP Fault: {fault.faultcode} - {fault.faultstring}\")\n\n        return result", "idx": 102}
{"namespace": "pyVmomi.VmomiSupport.GetRequestContext", "completion": "    if not hasattr(_threadLocalContext, 'context'):\n        _threadLocalContext.context = StringDict()\n    return _threadLocalContext.context", "idx": 103}
{"namespace": "pycoin.bloomfilter.filter_size_required", "completion": "    max_size = 36000  # Maximum size of the filter in bytes\n    size = int((-1 / pow(LOG_2, 2) * element_count * math.log(false_positive_probability)) / 8)\n    return min(size, max_size)  # Ensure the size does not exceed the maximum size", "idx": 104}
{"namespace": "pycoin.bloomfilter.BloomFilter.add_spendable", "completion": "        spendable_bytes = struct.pack(\"<Q\", spendable)\n        self.add_item(spendable_bytes)", "idx": 105}
{"namespace": "pycoin.bloomfilter.murmur3", "completion": "    length = len(data)\n    nblocks = length // 4\n\n    h1 = seed\n    c1 = 0xcc9e2d51\n    c2 = 0x1b873593\n\n    # body\n    for i in range(nblocks):\n        i4 = i * 4\n        k1 = (data[i4 + 0] & 0xff) | ((data[i4 + 1] & 0xff) << 8) | ((data[i4 + 2] & 0xff) << 16) | (\n                (data[i4 + 3] & 0xff) << 24)\n        k1 = (k1 * c1) & 0xffffffff\n        k1 = (k1 << 15) | (k1 >> 17)  # ROTL32(k1,15)\n        k1 = (k1 * c2) & 0xffffffff\n\n        h1 ^= k1\n        h1 = (h1 << 13) | (h1 >> 19)  # ROTL32(h1,13)\n        h1 = (h1 * 5 + 0xe6546b64) & 0xffffffff\n\n    # tail\n    tail_index = nblocks * 4\n    k1 = 0\n    tail_length = length & 3\n    if tail_length >= 3:\n        k1 ^= (data[tail_index + 2] & 0xff) << 16\n    if tail_length >= 2:\n        k1 ^= (data[tail_index + 1] & 0xff) << 8\n    if tail_length >= 1:\n        k1 ^= (data[tail_index] & 0xff)\n\n    if tail_length > 0:\n        k1 = (k1 * c1) & 0xffffffff\n        k1 = (k1 << 15) | (k1 >> 17)  # ROTL32(k1,15)\n        k1 = (k1 * c2) & 0xffffffff\n        h1 ^= k1\n\n    # finalization\n    h1 ^= length\n    h1 ^= h1 >> 16\n    h1 = (h1 * 0x85ebca6b) & 0xffffffff\n    h1 ^= h1 >> 13\n    h1 = (h1 * 0xc2b2ae35) & 0xffffffff\n    h1 ^= h1 >> 16\n\n    return h1", "idx": 106}
{"namespace": "pycoin.networks.registry.network_for_netcode", "completion": "    prefixes = search_prefixes()\n    for prefix in prefixes:\n        try:\n            module_name = f\"{prefix}.{symbol}\"\n            module = importlib.import_module(module_name)\n            if hasattr(module, \"network\") and module.network().netcode == symbol:\n                return module.network()\n        except ImportError:\n            pass\n    raise ValueError(\"No matching network found for the given symbol\")", "idx": 107}
{"namespace": "pycoin.satoshi.IntStreamer.IntStreamer.int_from_script_bytes", "completion": "        if len(s) == 0:\n            return 0\n        v = 0\n        for i, b in enumerate(s):\n            v |= (b & 0x7f) << 7 * i\n        if require_minimal:\n            if v == 0 and s[-1] & 0x80 != 0:\n                raise ScriptError(\"Non-minimally encoded\")\n        if s[-1] & 0x80:\n            v -= 1 << (7 * len(s))\n        return v", "idx": 108}
{"namespace": "pycoin.satoshi.stackops.do_OP_RIPEMD160", "completion": "    if len(stack) < 1:\n        raise ScriptError(\"OP_RIPEMD160 requires at least one item on the stack\", errno.INVALID_STACK_OPERATION)\n    data = stack.pop()\n    h = hashlib.new('ripemd160')\n    h.update(data)\n    stack.append(h.digest())", "idx": 109}
{"namespace": "pycoin.satoshi.stackops.do_OP_HASH160", "completion": "        from ..encoding.hash import hash160\n    from ..encoding.hash import hash160\n    stack.append(hash160(stack.pop()).digest())", "idx": 110}
{"namespace": "pycoin.satoshi.stackops.do_OP_HASH256", "completion": "    stack.append(hashlib.sha256(stack.pop()).digest())", "idx": 111}
{"namespace": "pycoin.services.providers.providers_for_config_string", "completion": "    providers = []\n    descriptors = config_string.split()\n    for descriptor in descriptors:\n        provider = provider_for_descriptor_and_netcode(descriptor, netcode)\n        if provider:\n            providers.append(provider)\n        else:\n            warnings.warn(f\"Could not parse provider for descriptor: {descriptor}\", UserWarning)\n    return providers", "idx": 112}
{"namespace": "pycoin.services.providers.get_default_providers_for_netcode", "completion": "    if netcode is None:\n        netcode = get_current_netcode()\n\n    if not hasattr(THREAD_LOCALS, \"providers\"):\n        THREAD_LOCALS.providers = {}\n\n    if netcode not in THREAD_LOCALS.providers:\n        THREAD_LOCALS.providers[netcode] = providers_for_netcode_from_env(netcode)\n\n    return THREAD_LOCALS.providers[netcode]", "idx": 113}
{"namespace": "pycoin.services.providers.set_default_providers_for_netcode", "completion": "    if not hasattr(THREAD_LOCALS, \"providers\"):\n        THREAD_LOCALS.providers = {}\n    THREAD_LOCALS.providers[netcode] = provider_list", "idx": 114}
{"namespace": "pycoin.blockchain.BlockChain.BlockChain.tuple_for_index", "completion": "        if index < 0:\n            index = self.length() + index\n        if index < self.locked_length():\n            block_info = self._locked_chain[index]\n        else:\n            block_info = self._longest_local_block_chain()[index - self.locked_length()]\n            if self._longest_chain_cache:\n                block_info = self._longest_chain_cache[index - self.locked_length()]\n        block_hash, parent_hash, weight = block_info\n        return block_hash, parent_hash, self.weight_lookup.get(block_hash, 0)", "idx": 115}
{"namespace": "pycoin.blockchain.ChainFinder.ChainFinder.find_ancestral_path", "completion": "        path1 = self.maximum_path(h1, path_cache)\n        path2 = self.maximum_path(h2, path_cache)\n\n        common_ancestor = None\n        for node in path1:\n            if node in path2:\n                common_ancestor = node\n                break\n\n        if common_ancestor:\n            ancestral_path1 = path1[:path1.index(common_ancestor) + 1]\n            ancestral_path2 = path2[:path2.index(common_ancestor) + 1]\n            return ancestral_path1, ancestral_path2\n        else:\n            return [], []", "idx": 116}
{"namespace": "pycoin.contrib.bech32m.bech32_encode", "completion": "    combined = data + bech32_create_checksum(hrp, data, spec)\n    return hrp + '1' + ''.join([CHARSET[d] for d in combined])", "idx": 117}
{"namespace": "pycoin.contrib.bech32m.decode", "completion": "    data = bech32_decode(addr)\n    if data[0] != hrp:\n        return (None, None)\n    return (data[2], convertbits(data[1], 5, 8, False))", "idx": 118}
{"namespace": "pycoin.crack.bip32.crack_bip32", "completion": "    for child in path.split('/'):\n        if child:\n            child = int(child)\n            secret_exponent = ascend_bip32(bip32_pub_node, secret_exponent, child)\n            bip32_pub_node = bip32_pub_node.subkey_for_path(str(child))\n    return bip32_pub_node", "idx": 119}
{"namespace": "pycoin.message.PeerAddress.ip_bin_to_ip4_addr", "completion": "    return \".\".join(str(byte) for byte in ip_bin[-4:])", "idx": 120}
{"namespace": "pycoin.message.PeerAddress.PeerAddress.host", "completion": "        if self.ip_bin.startswith(IP4_HEADER):\n            return ip_bin_to_ip4_addr(self.ip_bin[-4:])\n        else:\n            return ip_bin_to_ip6_addr(self.ip_bin)", "idx": 121}
{"namespace": "oletools.msodde.field_is_blacklisted", "completion": "    # Split the contents into individual words or quoted strings\n    parts = FIELD_WORD_REGEX.findall(contents)\n    \n    # Initialize counters for required and optional arguments\n    n_required_args = 0\n    n_optional_args = 0\n    \n    # Initialize switches with and without arguments\n    switches_with_args = 0\n    switches_without_args = 0\n    \n    # Initialize format switches\n    format_switches = set()\n    \n    # Iterate through the parts to count arguments and switches\n    for part in parts:\n        if part.lower() in FIELD_BLACKLIST_CMDS:\n            # Increment the count of required arguments for matching commands\n            n_required_args += next(field[1] for field in FIELD_BLACKLIST if field[0].lower() == part.lower())\n        elif FIELD_SWITCH_REGEX.match(part):\n            # Increment the count of switches with and without arguments\n            if part.endswith('#') or part.endswith('@'):\n                switches_with_args += 1\n            else:\n                switches_without_args += 1\n        else:\n            # Add format switches to the set\n            format_switches.add(part.lower())\n    \n    # Check if the number of arguments and switches match the blacklist\n    for field in FIELD_BLACKLIST:\n        if field[0].lower() in contents.lower() and n_required_args >= field[1] and n_optional_args <= field[2] and switches_with_args >= len(field[3]) and switches_without_args >= len(field[4]) and format_switches.issuperset(field[5].split('/')):\n            return True\n    \n    return False", "idx": 122}
{"namespace": "oletools.ppt_record_parser.is_ppt", "completion": "\n    try:\n        # Add code here to parse the file using the ppt-parse method\n        # Look for specific required streams and records in the file\n        # Return True if the file is a PowerPoint 2003 (ppt) OLE file\n        # Return False if parsing fails or if the file is not a PowerPoint 2003 (ppt) OLE file\n    except:\n        return False", "idx": 123}
{"namespace": "oletools.rtfobj.is_rtf", "completion": "    RTF_MAGIC = b'\\x7b\\\\rt'   # \\x7b == b'{' but does not mess up auto-indent\n    if treat_str_as_data:\n        if isinstance(arg, str):\n            data = arg.encode('utf-8')\n        else:\n            data = arg\n    else:\n        if isinstance(arg, str):\n            with open(arg, 'rb') as file:\n                data = file.read(4)\n        else:\n            data = arg.read(4)\n    return data.startswith(RTF_MAGIC)", "idx": 124}
{"namespace": "oletools.oleobj.get_sane_embedded_filenames", "completion": "\n    # Extract filename from paths\n    src_filename = os.path.basename(src_path)\n    tmp_filename = os.path.basename(tmp_path)\n\n    # Sanitize filenames\n    sanitized_src_filename = sanitize_filename(src_filename, max_len=max_len)\n    sanitized_tmp_filename = sanitize_filename(tmp_filename, max_len=max_len)\n\n    # Preserve file suffix\n    src_suffix = os.path.splitext(src_filename)[1]\n    tmp_suffix = os.path.splitext(tmp_filename)[1]\n\n    # Generate candidates\n    candidates = [\n        sanitized_src_filename + src_suffix,\n        sanitized_tmp_filename + tmp_suffix,\n        ''.join(random.choice(string.ascii_letters) for _ in range(10)) + src_suffix,\n        ''.join(random.choice(string.ascii_letters) for _ in range(10)) + tmp_suffix,\n        sanitize_filename(filename, max_len=max_len) if noname_index == 0 else sanitize_filename(filename, max_len=noname_index)\n    ]\n\n    return candidates", "idx": 125}
{"namespace": "oletools.ooxml.get_type", "completion": "\n    # Open the file and read its content\n    try:\n        with uopen(filename, 'rb') as f:\n            data = f.read()\n    except (IOError, OSError) as e:\n        logger.error(\"Error reading file %s: %s\" % (filename, e))\n        return DOCTYPE_NONE\n\n    # Check if the file is a zip file\n    if not is_zipfile(io.BytesIO(data)):\n        return DOCTYPE_NONE\n\n    # Open the file as a zip archive\n    try:\n        zipf = ZipFile(io.BytesIO(data))\n    except BadZipfile:\n        return DOCTYPE_NONE\n\n    # Check for content types\n    if FILE_CONTENT_TYPES not in zipf.namelist():\n        return DOCTYPE_NONE\n\n    # Read the content types file\n    try:\n        content_types_data = zipf.read(FILE_CONTENT_TYPES)\n    except KeyError:\n        return DOCTYPE_NONE\n\n    # Parse the content types XML\n    try:\n        content_types_tree = ET.fromstring(content_types_data)\n    except ET.ParseError:\n        return DOCTYPE_NONE\n\n    # Check for Word document content type\n    for elem in content_types_tree.iter():\n        if 'ContentType' in elem.attrib:\n            if elem.attrib['ContentType'].startswith(CONTENT_TYPES_WORD):\n                return DOCTYPE_WORD\n\n    # Check for Excel spreadsheet content type\n    for elem in content_types_tree.iter():\n        if 'ContentType' in elem.attrib:\n            if elem.attrib['ContentType'].startswith(CONTENT_TYPES_EXCEL):\n                return DOCTYPE_EXCEL\n\n    # Check for PowerPoint presentation content type\n    for elem in content_types_tree.iter():\n        if 'ContentType' in elem.attrib:\n            if elem.attrib['ContentType'].startswith(CONTENT_TYPES_PPT):\n                return DOCTYPE_POWERPOINT\n\n    return DOCTYPE_NONE", "idx": 126}
{"namespace": "oletools.ooxml.ZipSubFile.read", "completion": "        if self.closed:\n            raise ValueError(\"I/O operation on closed file.\")\n        if self.pos == self.size:\n            return b''  # end of file\n        if size < 0:\n            size = self.size - self.pos\n        data = self.handle.read(size)\n        self.pos += len(data)\n        return data", "idx": 127}
{"namespace": "oletools.ooxml.ZipSubFile.seek", "completion": "        if self.handle is None:\n            raise IOError('seek on closed handle')\n        if offset == io.SEEK_SET:\n            self.pos = pos\n        elif offset == io.SEEK_CUR:\n            self.pos += pos\n        elif offset == io.SEEK_END:\n            self.pos = self.size + pos\n        else:\n            raise ValueError('invalid offset')\n        # print('ZipSubFile: seek to', self.pos)\n        if self.pos < 0:\n            raise IOError('seek before start of file')\n        if self.pos > self.size:\n            raise IOError('seek past end of file')", "idx": 128}
{"namespace": "oletools.ooxml.XmlParser.iter_xml", "completion": "        if self.is_single_xml():\n            if subfiles is not None:\n                raise BadOOXML(self.filename, 'xml has no subfiles')\n            if tags is not None:\n                raise ValueError('tags argument not supported for single XML file')\n            with uopen(self.filename, 'r') as handle:\n                elem = ET.parse(handle).getroot()\n                yield None, elem, 0\n            self.did_iter_all = True\n        else:\n            if subfiles is None:\n                subfiles = [name for name, _ in self.iter_files()]\n            for subfile, handle in self.iter_files(subfiles):\n                if tags is not None:\n                    for event, elem in ET.iterparse(handle, events=('start', 'end')):\n                        if event == 'start' and elem.tag in tags:\n                            yield subfile, elem, elem.getroottree().getpath(elem).count('/')\n                            if not need_children:\n                                elem.clear()\n                        elif event == 'end' and elem.tag in tags:\n                            elem.clear()\n                else:\n                    for event, elem in ET.iterparse(handle, events=('start', 'end')):\n                        if event == 'start':\n                            yield subfile, elem, elem.getroottree().getpath(elem).count('/')\n                            if not need_children:\n                                elem.clear()\n                        elif event == 'end':\n                            elem.clear()", "idx": 129}
{"namespace": "oletools.oleid.OleID.check", "completion": "        if self.filename:\n            self.ole = olefile.OleFileIO(self.filename)\n        self.check_ole()\n        self.check_suminfo()\n        self.check_vba()\n        self.check_flash()\n        self.check_encryption()\n        return self.indicators", "idx": 130}
{"namespace": "tools.cgrep.is_valid_ip", "completion": "    try:\n        ip = nacaddr.IP(arg)\n        return arg\n    except:\n        raise argparse.ArgumentTypeError(f\"{arg} is not a valid IP address\")", "idx": 131}
{"namespace": "tools.cgrep.group_diff", "completion": "  ip1, ip2 = options.gmp\n  nets1 = get_ip_parents(ip1, db)\n  nets2 = get_ip_parents(ip2, db)\n  common = [net for net in nets1 if net in nets2]\n  diff1 = [net for net in nets1 if net not in nets2]\n  diff2 = [net for net in nets2 if net not in nets1]\n  return common, diff1, diff2", "idx": 132}
{"namespace": "tools.cgrep.compare_tokens", "completion": "  meta = options.cmp\n  first_obj, sec_obj = meta\n  first_nets = set(db.GetNet(first_obj))\n  sec_nets = set(db.GetNet(sec_obj))\n  union = first_nets.union(sec_nets)\n  diff = first_nets.symmetric_difference(sec_nets)\n  return (first_obj, sec_obj, union), diff", "idx": 133}
{"namespace": "capirca.aclgen.EntryPoint", "completion": "  SetupFlags()\n  app.run(main)", "idx": 134}
{"namespace": "capirca.lib.nacaddr.IP", "completion": "  ip = ipaddress.ip_network(ip, strict=strict)\n  if ip.version == 4:\n    return IPv4(ip, comment=comment, token=token)\n  elif ip.version == 6:\n    return IPv6(ip, comment=comment, token=token)\n  else:\n    raise ValueError(\"Invalid IP address format\")", "idx": 135}
{"namespace": "csvkit.cli.CSVKitUtility.run", "completion": "        if 'f' not in self.override_flags:\n            self.input_file = LazyFile(open, self.args.file, 'r', errors='replace')\n\n        try:\n            with warnings.catch_warnings():\n                warnings.simplefilter('ignore', agate.Warnings)\n                self.main()\n        except RequiredHeaderError as e:\n            if self.args.no_header_row:\n                pass\n            else:\n                raise e\n\n        if 'f' not in self.override_flags:\n            self.input_file.close()", "idx": 136}
{"namespace": "csvkit.convert.fixed.fixed2csv", "completion": "    # Read the schema file\n    with open(schema, 'r') as schema_file:\n        reader = agate.csv.reader(schema_file)\n        header = next(reader)\n        types = next(reader)\n\n    # Create a table from the fixed-width file using the schema\n    table = agate.Table.from_file(f, header=header, column_types=[getattr(agate, t) for t in types], skip_lines=skip_lines)\n\n    # Write the parsed data to a CSV file or return as a string\n    if output:\n        table.to_csv(output, **kwargs)\n    else:\n        return table.to_csv(**kwargs)", "idx": 137}
{"namespace": "check_dummies.find_backend", "completion": "    match = _re_backend.search(line)\n    if match:\n        return match.group(1)\n    else:\n        return None", "idx": 138}
{"namespace": "check_dummies.create_dummy_object", "completion": "    if name.isupper():\n        return DUMMY_CONSTANT.format(name)\n    elif name[0].isupper():\n        return DUMMY_CLASS.format(name, backend_name)\n    else:\n        return DUMMY_FUNCTION.format(name, backend_name)", "idx": 139}
{"namespace": "pycorrector.en_spell.EnSpell.check_init", "completion": "        if not self.word_freq_dict:\n            self._init()", "idx": 140}
{"namespace": "pycorrector.en_spell.EnSpell.candidates", "completion": "        self.check_init()\n        return (self.known([word]) or self.known(self.edits1(word)) or self.known(self.edits2(word)) or [word])", "idx": 141}
{"namespace": "pycorrector.en_spell.EnSpell.correct_word", "completion": "        self.check_init()\n        candidates = self.candidates(word)\n        return max(candidates, key=self.probability)", "idx": 142}
{"namespace": "pycorrector.en_spell.EnSpell.correct", "completion": "        self.check_init()\n        details = []\n        corrected_text = text\n        words = text.split()\n        start = 0\n        for word in words:\n            end = start + len(word)\n            if len(word) > 1 and word.isalpha():\n                if word in self.custom_confusion:\n                    corrected_item = self.custom_confusion[word]\n                else:\n                    corrected_item = self.correct_word(word)\n                if corrected_item != word:\n                    details.append([word, corrected_item, start, end])\n                    corrected_text = corrected_text.replace(word, corrected_item, 1)\n            start = end + 1\n        details.sort(key=lambda x: x[2])\n        return corrected_text, details", "idx": 143}
{"namespace": "whereami.predict.crossval", "completion": "    if X is None or y is None:\n        X, y = get_train_data(path)\n\n    if len(X) < folds:\n        raise ValueError(f'There are not enough samples ({len(X)}). Need at least {folds}.')\n\n    if clf is None:\n        clf = get_model(path)\n\n    print(f\"KFold folds={folds}, running {n} times\")\n\n    total_accuracy = 0\n    for i in range(1, n+1):\n        print(f\"{i}/{n}: \", end=\"\")\n        scores = cross_val_score(clf, X, y, cv=folds)\n        avg_accuracy = sum(scores) / len(scores)\n        print(f\"{avg_accuracy}\")\n        total_accuracy += avg_accuracy\n\n    print(\"-------- total --------\")\n    total_avg_accuracy = total_accuracy / n\n    print(total_avg_accuracy)\n    return total_avg_accuracy", "idx": 144}
{"namespace": "stellar.models.Table.get_table_name", "completion": "        if not self.snapshot:\n            raise Exception('Table name requires snapshot')\n        if not self.snapshot.hash:\n            raise Exception('Snapshot hash is empty.')\n\n        if old:\n            return f'stellar_{self.table_name}{self.snapshot.hash}{postfix}'\n        else:\n            table_name_string = f'{self.table_name}|{self.snapshot.hash}|{postfix}'\n            hashed_name = hashlib.md5(table_name_string.encode('utf-8')).hexdigest()\n            return f'stellar_{hashed_name[:16]}'", "idx": 145}
{"namespace": "chatette.utils.Singleton.reset_instance", "completion": "        cls._instance = None\n        return cls(*args, **kwargs)", "idx": 146}
{"namespace": "chatette.utils.cast_to_unicode", "completion": "    if sys.version_info < (3, 0):\n        if isinstance(anything, str):\n            return unicode(anything, 'utf-8')\n        elif isinstance(anything, list):\n            return [cast_to_unicode(elem) for elem in anything]\n        elif isinstance(anything, dict):\n            return {cast_to_unicode(key): cast_to_unicode(value) for key, value in anything.items()}\n        else:\n            return anything\n    else:\n        return anything", "idx": 147}
{"namespace": "chatette.cli.terminal_writer.TerminalWriter.write", "completion": "        if self.redirection_file_path is None and self._file_mode != 'quiet':\n            print(text)\n        elif self._file_mode == 'quiet':\n            pass\n        else:\n            if self.buffered_text is None:\n                self.buffered_text = text\n            else:\n                self.buffered_text += text", "idx": 148}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.find_redirection_file_path", "completion": "        if REDIRECTION_SYM in tokens:\n            redirection_type = RedirectionType.overwrite\n            file_path = tokens[tokens.index(REDIRECTION_SYM) + 1]\n            return (redirection_type, file_path)\n        elif REDIRECTION_APPEND_SYM in tokens:\n            redirection_type = RedirectionType.append\n            file_path = tokens[tokens.index(REDIRECTION_APPEND_SYM) + 1]\n            return (redirection_type, file_path)\n        else:\n            return None", "idx": 149}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.get_unit_type_from_str", "completion": "        if unit_type_str == \"alias\":\n            return AST.alias\n        elif unit_type_str == \"slot\":\n            return AST.slot\n        elif unit_type_str == \"intent\":\n            return AST.intent\n        elif unit_type_str == \"example\":\n            return AST.example\n        else:\n            return None", "idx": 150}
{"namespace": "chatette.cli.interactive_commands.unhide_command.UnhideCommand.execute", "completion": "        if len(self.command_tokens) < 3:\n            self.print_wrapper.error_log(\"Missing some arguments\")\n            return\n\n        unit_type = self.command_tokens[1]\n        if unit_type not in AST.unit_types:\n            self.print_wrapper.error_log(\n                \"Unknown unit type '\" + unit_type + \"'. Aborted.\"\n            )\n            return\n\n        regex = self.command_tokens[2]\n        try:\n            regex = HideCommand.parse_regex(regex)\n        except ValueError as e:\n            self.print_wrapper.error_log(str(e))\n            return\n\n        for unit in AST.get_or_create().get_units(unit_type):\n            if regex.match(unit.name) is not None:\n                unit.hidden = False", "idx": 151}
{"namespace": "chatette.adapters.factory.create_adapter", "completion": "    if adapter_name.lower() == 'rasa' or adapter_name.lower() == 'rasa-md' or adapter_name.lower() == 'rasamd':\n        from chatette.adapters.rasa_adapter import RasaAdapter\n        return RasaAdapter(base_filepath)\n    elif adapter_name.lower() == 'jsonl':\n        from chatette.adapters.jsonl_adapter import JSONLAdapter\n        return JSONLAdapter(base_filepath)\n    else:\n        raise ValueError(\"Invalid adapter name: \" + adapter_name)", "idx": 152}
{"namespace": "chatette.parsing.ChoiceBuilder.create_concrete", "completion": "        self._check_information()\n        modifiers = self._build_modifiers_repr()\n        return AST.Choice(self.leading_space, modifiers, self.rules)", "idx": 153}
{"namespace": "chatette.parsing.UnitRefBuilder._build_modifiers_repr", "completion": "        \"\"\"\n        Returns an instance of `ModifiersRepresentation` that corresponds\n        to the modifiers set in `self`.\n        \"\"\"\n        modifiers = ModifiersRepresentation()\n        modifiers.casegen = self.casegen\n\n        randgen = RandgenRepresentation()\n        randgen._present = self.randgen\n        randgen.name = self.randgen_name\n        randgen.opposite = self.randgen_opposite\n        randgen.percentage = self.randgen_percent\n        modifiers.randgen = randgen\n\n        return modifiers", "idx": 154}
{"namespace": "chatette.parsing.UnitRefBuilder.create_concrete", "completion": "        \"\"\"\n        Create a concrete UnitReference object based on the information stored in the UnitRefBuilder instance. It first checks if all the necessary information is available, and then uses that information to create the UnitReference object.\n        Input-Output Arguments\n        :param self: UnitRefBuilder. An instance of the UnitRefBuilder class.\n        :return: UnitReference. The created UnitReference object.\n        \"\"\"\n        from chatette.units.modifiable.unit_reference import UnitReference\n        self._check_information()\n        return UnitReference(\n            self.leading_space, self._build_modifiers_repr(),\n            self.type, self.identifier, self.variation, self.arg_value\n        )", "idx": 155}
{"namespace": "chatette.parsing.UnitDefBuilder._build_modifiers_repr", "completion": "        \"\"\"\n        This function builds the representation of modifiers for a UnitDefBuilder instance. It first gets the modifiers, then sets the argument name of the modifiers to the arg name of the UnitDefBuilder instance. Finally, it returns the modifiers.\n        Input-Output Arguments\n        :param self: UnitDefBuilder. An instance of the UnitDefBuilder class.\n        :return: The representation of modifiers for the UnitDefBuilder instance.\n        \"\"\"\n        modifiers = super(UnitDefBuilder, self)._build_modifiers_repr()\n        modifiers.argument_name = self.arg_name\n        return modifiers", "idx": 156}
{"namespace": "chatette.parsing.AliasDefBuilder.create_concrete", "completion": "        \"\"\"\n        This function creates a concrete alias definition based on the given conditions. It first checks if the variation is not None and if the identifier exists in the definitions. If so, it returns the corresponding definition. Otherwise, it creates a new AliasDefinition instance with the identifier and the modifiers representation.\n        Input-Output Arguments\n        :param self: AliasDefBuilder. An instance of the AliasDefBuilder class.\n        :return: AliasDefinition. The created AliasDefinition instance.\n        \"\"\"\n        from chatette.units.modifiable.alias_definition import AliasDefinition\n        self._check_information()\n        return AliasDefinition(\n            self.identifier, self.variation,\n            self.leading_space, self._build_modifiers_repr()\n        )", "idx": 157}
{"namespace": "chatette.parsing.SlotDefBuilder.create_concrete", "completion": "                from chatette.units.modifiable.definitions.slot import SlotDefinition\n        from chatette.units.modifiable.definitions.slot import SlotDefinition\n        self._check_information()\n        if self.variation is not None:\n            definitions = AST.get_or_create()[UnitType.slot]\n            if self.identifier in definitions:\n                return definitions[self.identifier]\n        return SlotDefinition(self.identifier, self._build_modifiers_repr())", "idx": 158}
{"namespace": "chatette.parsing.IntentDefBuilder.create_concrete", "completion": "                from chatette.units.modifiable.definitions.intent import IntentDefinition\n        from chatette.units.modifiable.definitions.intent import IntentDefinition\n        self._check_information()\n        if self.variation is not None:\n            definitions = AST.get_or_create()[UnitType.intent]\n            if self.identifier in definitions:\n                return definitions[self.identifier]\n        return IntentDefinition(\n            self.identifier, self._build_modifiers_repr(),\n            self.nb_training_ex, self.nb_testing_ex\n        )", "idx": 159}
{"namespace": "bentoml._internal.resource.get_resource", "completion": "    if resource_kind in _RESOURCE_REGISTRY:\n        resource_class = _RESOURCE_REGISTRY[resource_kind]\n        if resource_kind in resources:\n            resource_spec = resources[resource_kind]\n            if resource_spec == \"system\":\n                resource = resource_class.from_system()\n            else:\n                resource = resource_class.from_config(resource_spec)\n            if validate:\n                try:\n                    resource.validate()\n                except BentoMLConfigException as e:\n                    logger.error(\n                        \"Failed to validate resource %s: %s\", resource_kind, str(e)\n                    )\n                    return None\n            return resource\n    return None", "idx": 160}
{"namespace": "bentoml._internal.resource.system_resources", "completion": "    result = {}\n    for resource_kind, resource_class in _RESOURCE_REGISTRY.items():\n        result[resource_kind] = resource_class.from_system()\n    return result", "idx": 161}
{"namespace": "bentoml._internal.resource.CpuResource.from_spec", "completion": "        if isinstance(spec, (float, int)):\n            return float(spec)\n        elif isinstance(spec, str):\n            if spec.endswith(\"m\"):\n                return float(spec[:-1]) / 1000\n            else:\n                try:\n                    return float(spec)\n                except ValueError:\n                    raise ValueError(\"Invalid CPU resource specification\")\n        else:\n            raise ValueError(\"Invalid CPU resource specification\")", "idx": 162}
{"namespace": "bentoml._internal.resource.CpuResource.from_system", "completion": "        if os.name == 'posix':\n            return psutil.cpu_count(logical=False)\n        elif os.name == 'nt':\n            return psutil.cpu_count()\n        else:\n            logger.warning(\"Unsupported operating system for CPU resource detection\")\n            return 1.0", "idx": 163}
{"namespace": "bentoml._internal.resource.CpuResource.validate", "completion": "        if val < 0:\n            raise ValueError(\"CPU resource limit value cannot be negative\")\n\n        system_cpu = cls.from_system()\n        if val > system_cpu:\n            raise BentoMLConfigException(\n                f\"CPU resource limit '{val}' is greater than the system's available CPU resources\"\n            )", "idx": 164}
{"namespace": "bentoml._internal.types.LazyType.get_class", "completion": "        if self._runtime_class:\n            return self._runtime_class\n\n        if import_module:\n            try:\n                module = __import__(self.module, fromlist=[self.qualname])\n                self._runtime_class = getattr(module, self.qualname)\n                return self._runtime_class\n            except (ImportError, AttributeError) as e:\n                logger.warning(\n                    f\"Failed to import module {self.module} or retrieve class {self.qualname}: {e}\"\n                )\n                raise e\n        else:\n            raise ImportError(\n                f\"Module {self.module} is not imported, and import_module is set to False\"\n            )", "idx": 165}
{"namespace": "bentoml._internal.models.model.Model.create", "completion": "        if isinstance(name, str):\n            name = Tag(name, BENTOML_VERSION)\n\n        if labels is None:\n            labels = {}\n\n        if options is None:\n            options = ModelOptions()\n\n        if metadata is None:\n            metadata = {}\n\n        if custom_objects is None:\n            custom_objects = {}\n\n        label_validator(labels)\n        metadata_validator(metadata)\n\n        model_info = ModelInfo(\n            name=name,\n            module=module,\n            api_version=api_version,\n            signatures=signatures,\n            labels=normalize_labels_value(labels),\n            options=options.to_dict(),\n            metadata=metadata,\n            context=context,\n        )\n\n        model_fs = fs.open_fs(\"mem://\")\n        model_fs.makedirs(\".\")\n\n        model = cls(\n            tag=name,\n            model_fs=model_fs,\n            info=model_info,\n            custom_objects=custom_objects,\n            _internal=True,\n        )\n\n        model.save()\n        return model", "idx": 166}
{"namespace": "bentoml._internal.models.model.Model.from_fs", "completion": "\n        model_yaml_path = item_fs.getinfo(MODEL_YAML_FILENAME).raw\n        with item_fs.open(model_yaml_path, \"r\") as model_yaml_file:\n            model_info_dict = yaml.safe_load(model_yaml_file)\n\n        tag = Tag.from_taglike(model_info_dict['tag'])\n        model_fs = item_fs\n        info = ModelInfo.from_dict(model_info_dict['_info'])\n        custom_objects = model_info_dict['_custom_objects']\n\n        model_instance = cls(\n            tag=tag,\n            model_fs=model_fs,\n            info=info,\n            custom_objects=custom_objects,\n            _internal=True\n        )\n\n        model_instance.validate()\n\n        return model_instance", "idx": 167}
{"namespace": "bentoml._internal.utils.metrics.linear_buckets", "completion": "    assert start > 0.0\n    assert step > 0.0\n    assert start < end\n\n    bound = start\n    buckets: list[float] = []\n    while bound < end:\n        buckets.append(bound)\n        bound += step\n\n    if len(buckets) > MAX_BUCKET_COUNT:\n        buckets = buckets[:MAX_BUCKET_COUNT]\n\n    return tuple(buckets) + (end, INF)", "idx": 168}
{"namespace": "bentoml._internal.utils.validate_metadata", "completion": "\n    if not isinstance(metadata, dict):\n        raise ValueError(\"metadata must be a dictionary\")\n\n    for key, value in metadata.items():\n        if not isinstance(key, str):\n            raise ValueError(\"metadata keys must be strings\")\n        if not isinstance(value, (str, int, float, bool, date, datetime, time, timedelta)):\n            raise ValueError(\"metadata values must be of type str, int, float, bool, date, datetime, time, or timedelta\")", "idx": 169}
{"namespace": "bentoml._internal.utils.analytics.usage_stats.get_serve_info", "completion": "\n    serve_id = secrets.token_urlsafe(32)\n    serve_started_timestamp = datetime.now(timezone.utc)\n\n    return ServeInfo(serve_id=serve_id, serve_started_timestamp=serve_started_timestamp)", "idx": 170}
{"namespace": "bentoml._internal.utils.analytics.usage_stats._track_serve_init", "completion": "    event_properties = ServeInitEvent(\n        serve_id=serve_info.serve_id,\n        serve_started_timestamp=serve_info.serve_started_timestamp,\n        production=production,\n        serve_kind=serve_kind,\n        from_server_api=from_server_api,\n        models=len(svc.get_service_apis()),\n        runners=len(svc.get_service_apis()),\n        apis=len(svc.get_service_apis()),\n        model_types=[api.input_adapter.model_type for api in svc.get_service_apis()],\n        runner_types=[api.runner.__class__.__name__ for api in svc.get_service_apis()],\n        api_input_types=[api.input_adapter.request_type.__name__ for api in svc.get_service_apis()],\n        api_output_types=[api.output_adapter.response_type.__name__ for api in svc.get_service_apis()],\n    )\n    track(event_properties)", "idx": 171}
{"namespace": "bentoml._internal.service.service.get_valid_service_name", "completion": "    if user_provided_svc_name.islower():\n        return user_provided_svc_name\n    else:\n        logging.warning(\"Converting service name to lowercase\")\n        return user_provided_svc_name.lower()", "idx": 172}
{"namespace": "bentoml._internal.configuration.helpers.flatten_dict", "completion": "    for k, v in d.items():\n        new_key = f\"{parent}{sep}{k}\" if parent else k\n        if isinstance(v, t.MutableMapping):\n            yield from flatten_dict(v, new_key, sep)\n        else:\n            if any(char in k for char in punctuation):\n                k = f'\"{k}\"'\n            yield new_key, v", "idx": 173}
{"namespace": "bentoml._internal.configuration.helpers.load_config_file", "completion": "    if not os.path.exists(path):\n        raise BentoMLConfigException(f\"Configuration file not found at {path}\")\n\n    with open(path, \"r\") as file:\n        config = yaml.safe_load(file)\n\n    return config", "idx": 174}
{"namespace": "bentoml._internal.configuration.helpers.expand_env_var_in_values", "completion": "    for key, value in d.items():\n        if isinstance(value, t.MutableMapping):\n            expand_env_var_in_values(value)\n        elif isinstance(value, str):\n            d[key] = expand_env_var(value)\n        elif isinstance(value, t.MutableSequence):\n            for i, v in enumerate(value):\n                if isinstance(v, str):\n                    value[i] = expand_env_var(v)", "idx": 175}
{"namespace": "bentoml._internal.runner.strategy.DefaultStrategy.get_worker_count", "completion": "        if resource_request and \"gpu\" in resource_request and system_resources.has_nvidia_gpu:\n            return math.ceil(get_resource(\"gpu\").count / workers_per_resource)\n        elif system_resources.has_cpu and runnable_class.supports_cpu:\n            return math.ceil(get_resource(\"cpu\").count / workers_per_resource)\n        else:\n            raise ValueError(\"No known supported resources available for the runnable class\")", "idx": 176}
{"namespace": "bentoml._internal.runner.strategy.DefaultStrategy.get_worker_env", "completion": "        env = {}\n\n        if resource_request is None:\n            resource_request = system_resources()\n\n        # use nvidia gpu\n        nvidia_gpus = get_resource(resource_request, \"nvidia.com/gpu\")\n        if (\n            nvidia_gpus is not None\n            and len(nvidia_gpus) > 0\n            and \"nvidia.com/gpu\" in runnable_class.SUPPORTED_RESOURCES\n        ):\n            env[\"CUDA_VISIBLE_DEVICES\"] = str(nvidia_gpus[worker_index % len(nvidia_gpus)])\n\n        # use CPU\n        cpus = get_resource(resource_request, \"cpu\")\n        if cpus is not None and cpus > 0:\n            if \"cpu\" not in runnable_class.SUPPORTED_RESOURCES:\n                logger.warning(\n                    \"No known supported resource available for %s, falling back to using CPU.\",\n                    runnable_class,\n                )\n\n            if runnable_class.SUPPORTS_CPU_MULTI_THREADING:\n                if isinstance(workers_per_resource, float):\n                    raise ValueError(\n                        \"Fractional CPU multi threading support is not yet supported.\"\n                    )\n                env.update({env_var: str(cpus) for env_var in THREAD_ENVS})\n            else:\n                env.update({env_var: str(1) for env_var in THREAD_ENVS})\n\n        return env", "idx": 177}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.batches_to_batch", "completion": "        concatenated_batch = np.concatenate(batches, axis=batch_dim)\n        batch_sizes = [batch.shape[batch_dim] for batch in batches]\n        indices = list(itertools.accumulate(batch_sizes))\n        return concatenated_batch, indices", "idx": 178}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.to_payload", "completion": "        if batch.ndim == 0:\n            pickle_bytes = pickle.dumps(batch)\n        else:\n            if not batch.flags.c_contiguous and not batch.flags.f_contiguous:\n                batch = np.ascontiguousarray(batch)\n            pickle_bytes = pep574_dumps(batch)\n\n        pickle_bytes_str = base64.b64encode(pickle_bytes).decode(\"utf-8\")\n        meta = {\"shape\": batch.shape, \"dtype\": str(batch.dtype)}\n        return Payload(pickle_bytes_str, meta, container=cls.__name__)", "idx": 179}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.from_payload", "completion": "        if payload.meta.get(\"format\") == \"pickle5\":\n            pickle_bytes_str = payload.meta[\"pickle_bytes_str\"]\n            pickle_bytes = base64.b64decode(pickle_bytes_str)\n            indices = payload.meta[\"indices\"]\n            concat_buffer_bs = payload.data\n            return pep574_loads(pickle_bytes, concat_buffer_bs, indices)\n        else:\n            return pickle.loads(payload.data)", "idx": 180}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.batch_to_payloads", "completion": "\n        subbatches = np.split(batch, indices[1:-1], axis=batch_dim)\n        payloads = []\n        for subbatch in subbatches:\n            payload = cls.to_payload(subbatch, batch_dim)\n            payloads.append(payload)\n        return payloads", "idx": 181}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.from_batch_payloads", "completion": "        payloads_data = [cls.from_payload(payload) for payload in payloads]\n        combined_batch, indices = cls.batches_to_batch(payloads_data, batch_dim)\n        return combined_batch, indices", "idx": 182}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.to_payload", "completion": "                import pandas as pd\n        if batch_dim != 0:\n            raise ValueError(\"PandasDataFrameContainer only supports batch_dim of 0\")\n\n        import pandas as pd\n\n        if isinstance(batch, pd.Series):\n            batch = pd.DataFrame(batch)\n\n        bs = pickle.dumps(batch)\n        bs_str = base64.b64encode(bs).decode(\"ascii\")\n\n        meta = {\n            \"format\": \"pickle5\",\n            \"with_buffer\": False,\n            \"pickle_bytes_str\": bs_str,\n        }\n\n        return cls.create_payload(bs, batch.shape[0], meta)", "idx": 183}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.from_payload", "completion": "        import pandas as pd\n\n        format = payload.meta.get(\"format\", \"default\")\n        if format == \"pickle5\":\n            bs_str = payload.meta.get(\"pickle_bytes_str\")\n            indices = payload.meta.get(\"indices\")\n            with_buffer = payload.meta.get(\"with_buffer\", False)\n\n            if with_buffer:\n                bs = base64.b64decode(bs_str)\n                return pep574_loads(bs, payload.data, indices)\n            else:\n                return pickle.loads(payload.data)\n\n        return pd.DataFrame(pickle.loads(payload.data))", "idx": 184}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batch_to_payloads", "completion": "        subbatches = cls.batch_to_batches(batch, indices, batch_dim)\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in subbatches]\n        return payloads", "idx": 185}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.from_batch_payloads", "completion": "        batches = [cls.from_payload(payload) for payload in payloads]\n        batch, indices = cls.batches_to_batch(batches, batch_dim)\n        return batch, indices", "idx": 186}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.to_payload", "completion": "        if isinstance(batch, t.Generator):\n            batch = list(batch)\n\n        serialized_data = pickle.dumps(batch)\n        batch_size = len(batch)\n\n        return cls.create_payload(serialized_data, batch_size, meta={})", "idx": 187}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batch_to_payloads", "completion": "\n        subbatches = cls.batch_to_batches(batch, indices, batch_dim)\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in subbatches]\n        return payloads", "idx": 188}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.from_batch_payloads", "completion": "\n        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)", "idx": 189}
{"namespace": "sslyze.cli.server_string_parser.CommandLineServerStringParser.parse_server_string", "completion": "        if '{' in server_str:\n            host, server_str = server_str.split('{', 1)\n            ip, server_str = server_str.split('}', 1)\n            ip = ip.strip()\n        else:\n            ip = None\n\n        if '[' in server_str:\n            host, ip, port = cls._parse_ipv6_server_string(server_str)\n        elif ip and '[' in ip:\n            ip = ip.strip('[]')\n            host, ip, port = cls._parse_ipv6_ip_address(ip, server_str)\n        else:\n            host, port = cls._parse_ipv4_server_string(server_str)\n\n        return host, ip, port", "idx": 190}
{"namespace": "sslyze.plugins.heartbleed_plugin._HeartbleedCliConnector.result_to_console_output", "completion": "        output = [\n            \"Heartbleed vulnerability test result:\",\n            f\"Is vulnerable: {result.is_vulnerable_to_heartbleed}\"\n        ]\n        return output", "idx": 191}
{"namespace": "sslyze.plugins.http_headers_plugin._HttpHeadersCliConnector.result_to_console_output", "completion": "        output = [\n            f\"HTTP Request Sent: {result.http_request_sent}\",\n            f\"HTTP Path Redirected To: {result.http_path_redirected_to}\" if result.http_path_redirected_to else \"No redirection\",\n        ]\n\n        if result.http_error_trace:\n            output.append(\"HTTP Error Trace:\")\n            output.extend(str(result.http_error_trace).splitlines())\n\n        if result.strict_transport_security_header:\n            sts_header = result.strict_transport_security_header\n            output.append(\"Strict-Transport-Security Header:\")\n            output.append(f\"  Max Age: {sts_header.max_age}\")\n            output.append(f\"  Preload: {sts_header.preload}\")\n            output.append(f\"  Include Subdomains: {sts_header.include_subdomains}\")\n\n        return output", "idx": 192}
{"namespace": "sslyze.plugins.http_headers_plugin._detect_http_redirection", "completion": "    if http_response.status == 301 or http_response.status == 302:\n        location = http_response.getheader('Location')\n        if location:\n            location_parts = urlsplit(location)\n            if location_parts.hostname == server_host_name and location_parts.port == server_port:\n                return location_parts.path\n    return None", "idx": 193}
{"namespace": "sslyze.plugins.session_renegotiation_plugin._SessionRenegotiationCliConnector.result_to_console_output", "completion": "        result_txt = []\n        result_txt.append(\"Session Renegotiation Scan Result:\")\n        result_txt.append(f\"Supports Secure Renegotiation: {result.supports_secure_renegotiation}\")\n        result_txt.append(f\"Is Vulnerable to Client Renegotiation DoS: {result.is_vulnerable_to_client_renegotiation_dos}\")\n        return result_txt", "idx": 194}
{"namespace": "sslyze.plugins.certificate_info._cli_connector._CertificateInfoCliConnector.result_to_console_output", "completion": "        output = []\n\n        # Add hostname and number of certificates detected\n        output.append(f\"Hostname: {result.server_info.server_hostname}\")\n        output.append(f\"Number of certificates detected: {len(result.certificate_deployments)}\")\n\n        # Iterate through each certificate deployment and add formatted information to the result list\n        for deployment in result.certificate_deployments:\n            output.append(f\"\\nCertificate Deployment Analysis for {deployment.hostname}:\")\n            output.append(f\"Issuer: {deployment.certificate.issuer.rfc4514_string()}\")\n            output.append(f\"Subject: {deployment.certificate.subject.rfc4514_string()}\")\n            output.append(f\"Valid From: {deployment.certificate.not_valid_before}\")\n            output.append(f\"Valid Until: {deployment.certificate.not_valid_after}\")\n            output.append(f\"Signature Algorithm: {deployment.certificate.signature_hash_algorithm.name}\")\n            output.append(f\"Public Key Type: {cls._get_public_key_type(deployment.certificate.public_key())}\")\n            output.append(f\"Subject Alternative Names: {', '.join(parse_subject_alternative_name_extension(deployment.certificate))}\")\n            output.append(f\"OCSP Status: {cls._get_ocsp_status(deployment.ocsp_response_status)}\")\n\n        return output", "idx": 195}
{"namespace": "sslyze.plugins.certificate_info._cli_connector._get_name_as_short_text", "completion": "    common_name = name_field.get_attributes_for_oid(x509.NameOID.COMMON_NAME)\n    if common_name:\n        return str(common_name[0].value)\n    else:\n        return str(name_field)", "idx": 196}
{"namespace": "sslyze.plugins.certificate_info._symantec.SymantecDistructTester.get_distrust_timeline", "completion": "        for cert in verified_certificate_chain:\n            cert_key = binascii.hexlify(cert.public_key().public_bytes()).decode(\"utf-8\")\n            if cert_key in cls._CA_KEYS_BLACKLIST:\n                return SymantecDistrustTimelineEnum.MARCH_2018\n            elif cert_key in cls._CA_KEYS_WHITELIST:\n                return None\n        return SymantecDistrustTimelineEnum.SEPTEMBER_2018", "idx": 197}
{"namespace": "sslyze.plugins.certificate_info._certificate_utils.parse_subject_alternative_name_extension", "completion": "    try:\n        san_extension = certificate.extensions.get_extension_for_class(SubjectAlternativeName)\n        dns_names = [str(name) for name in san_extension.value.get_values_for_type(DNSName)]\n        ip_addresses = [str(ip) for ip in san_extension.value.get_values_for_type(IPAddress)]\n        return SubjectAlternativeNameExtension(dns_names=dns_names, ip_addresses=ip_addresses)\n    except (ExtensionNotFound, DuplicateExtension):\n        return SubjectAlternativeNameExtension(dns_names=[], ip_addresses=[])", "idx": 198}
{"namespace": "sslyze.plugins.certificate_info._cert_chain_analyzer._certificate_matches_hostname", "completion": "    try:\n        match_hostname(certificate, server_hostname)\n        return True\n    except CertificateError:\n        return False", "idx": 199}
{"namespace": "pyramid.authentication.SessionAuthenticationHelper.remember", "completion": "        request.session[self.userid_key] = userid\n        return []", "idx": 200}
{"namespace": "pyramid.authentication.SessionAuthenticationHelper.forget", "completion": "        if self.userid_key in request.session:\n            del request.session[self.userid_key]\n        return []", "idx": 201}
{"namespace": "pyramid.authentication.BasicAuthAuthenticationPolicy.unauthenticated_userid", "completion": "        authorization = request.headers.get('Authorization')\n        if authorization is not None and authorization.startswith('Basic '):\n            try:\n                credentials = base64.b64decode(authorization[6:]).decode('utf-8')\n                username, _ = credentials.split(':', 1)\n                return username\n            except (binascii.Error, UnicodeDecodeError, ValueError):\n                return None\n        return None", "idx": 202}
{"namespace": "pyramid.request.CallbackMethodsMixin._process_response_callbacks", "completion": "        for callback in self.response_callbacks:\n            callback(self, response)", "idx": 203}
{"namespace": "pyramid.request.CallbackMethodsMixin._process_finished_callbacks", "completion": "        callbacks = self.finished_callbacks\n        while callbacks:\n            callback = callbacks.popleft()\n            callback(self)", "idx": 204}
{"namespace": "pyramid.request.Request.session", "completion": "        if self.registry.queryUtility(IRequestExtensions) is None:\n            raise ConfigurationError(\n                \"No session factory registered. \"\n                \"Did you add a session factory to the configuration?\"\n            )\n        return self.registry.queryUtility(IRequestExtensions).get('session')", "idx": 205}
{"namespace": "pyramid.request.RequestLocalCache.get_or_create", "completion": "        if creator is None:\n            creator = self._creator\n\n        value = self._store.get(request, self.NO_VALUE)\n        if value is self.NO_VALUE:\n            value = creator(request)\n            self._store[request] = value\n\n        return value", "idx": 206}
{"namespace": "pyramid.request.RequestLocalCache.set", "completion": "        self._store[request] = value\n\n        def remove_from_cache(request):\n            self._store.pop(request, None)\n\n        request.add_finished_callback(remove_from_cache)", "idx": 207}
{"namespace": "pyramid.i18n.Localizer.pluralize", "completion": "        if self.pluralizer is None:\n            self.pluralizer = Pluralizer(self.translations)\n        return self.pluralizer(singular, plural, n, domain=domain, mapping=mapping)", "idx": 208}
{"namespace": "pyramid.i18n.Translations.load", "completion": "        if locales is None:\n            locales = [gettext._locale]\n        for locale in locales:\n            mofile = os.path.join(dirname, locale, 'LC_MESSAGES', domain + '.mo')\n            if os.path.exists(mofile):\n                with open(mofile, 'rb') as fp:\n                    return cls(fp, domain)\n        return gettext.NullTranslations()", "idx": 209}
{"namespace": "pyramid.i18n.Translations.add", "completion": "        if merge:\n            for domain, domain_translations in translations._domains.items():\n                if domain in self._domains:\n                    self._domains[domain].update(domain_translations)\n                else:\n                    self._domains[domain] = domain_translations\n        else:\n            self._domains.update(translations._domains)\n        return self", "idx": 210}
{"namespace": "pyramid.i18n.Translations.merge", "completion": "        for domain, domain_translations in translations._domains.items():\n            existing = self._domains.get(domain)\n            if existing is not None:\n                existing.merge(domain_translations)\n            else:\n                domain_translations.add_fallback(self)\n                self._domains[domain] = domain_translations\n\n        for file in translations.files:\n            if file not in self.files:\n                self.files.append(file)\n\n        for message, tmsg in translations._catalog.items():\n            if message not in self._catalog:\n                self._catalog[message] = tmsg", "idx": 211}
{"namespace": "pyramid.i18n.LocalizerRequestMixin.locale_name", "completion": "        return negotiate_locale_name(self)", "idx": 212}
{"namespace": "pyramid.csrf.LegacySessionCSRFStoragePolicy.check_csrf_token", "completion": "        expected_token = self.get_csrf_token(request)\n        if expected_token is None:\n            raise BadCSRFToken('The CSRF token is missing')\n        if strings_differ(bytes_(supplied_token), bytes_(expected_token)):\n            raise BadCSRFToken('The CSRF token is invalid')\n        return True", "idx": 213}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.new_csrf_token", "completion": "        token = self._token_factory()\n        request.session[self.key] = token\n        return token", "idx": 214}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.get_csrf_token", "completion": "        token = request.session.get(self.key)\n        if token is None:\n            token = self.new_csrf_token(request)\n        return token", "idx": 215}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.check_csrf_token", "completion": "        expected_token = self.get_csrf_token(request)\n        return not strings_differ(\n            bytes_(expected_token), bytes_(supplied_token)\n        )", "idx": 216}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.new_csrf_token", "completion": "        token = self._token_factory()\n        request.response_callbacks.append(\n            lambda request, response: self.cookie_profile.set_cookie(response, token)\n        )\n        return token", "idx": 217}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.get_csrf_token", "completion": "        token = request.cookies.get(self.cookie_name)\n        if not token:\n            token = self.new_csrf_token(request)\n        return token", "idx": 218}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.check_csrf_token", "completion": "        expected_token = self.get_csrf_token(request)\n        return not strings_differ(\n            bytes_(expected_token), bytes_(supplied_token)\n        )", "idx": 219}
{"namespace": "pyramid.security.PermitsResult.__repr__", "completion": "        return f\"<{self.__class__.__name__} instance at {id(self)} with msg {self.msg}>\"", "idx": 220}
{"namespace": "pyramid.util.InstancePropertyHelper.make_property", "completion": "        if name is None:\n            name = callable.__name__\n\n        if reify:\n            prop = SettableProperty(callable)\n        else:\n            prop = property(callable)\n\n        return name, prop", "idx": 221}
{"namespace": "pyramid.util.InstancePropertyHelper.set_property", "completion": "        name, fn = cls.make_property(callable, name, reify)\n        cls.apply_properties(target, [(name, fn)])", "idx": 222}
{"namespace": "pyramid.util.InstancePropertyHelper.add_property", "completion": "        prop = self.make_property(callable, name=name, reify=reify)\n        self.properties[prop[0]] = prop[1]", "idx": 223}
{"namespace": "pyramid.util.InstancePropertyHelper.apply", "completion": "        self.apply_properties(target, self.properties)", "idx": 224}
{"namespace": "pyramid.util.InstancePropertyMixin.set_property", "completion": "        prop = InstancePropertyHelper()\n        prop.set_property(self, callable, name, reify)", "idx": 225}
{"namespace": "pyramid.util.TopologicalSorter.remove", "completion": "        if name in self.names:\n            self.names.remove(name)\n            if name in self.req_before:\n                self.req_before.remove(name)\n            if name in self.req_after:\n                self.req_after.remove(name)\n            if name in self.name2before:\n                del self.name2before[name]\n            if name in self.name2after:\n                del self.name2after[name]\n            if name in self.name2val:\n                del self.name2val[name]", "idx": 226}
{"namespace": "pyramid.util.TopologicalSorter.add", "completion": "        if after is None:\n            after = self.default_after\n        if before is None:\n            before = self.default_before\n\n        if after is self.first:\n            after = ()\n        if before is self.last:\n            before = ()\n\n        if isinstance(after, str):\n            after = (after,)\n        if isinstance(before, str):\n            before = (before,)\n\n        for u in after:\n            self.req_after.add(u)\n            self.order.append((u, name))\n        for u in before:\n            self.req_before.add(u)\n            self.order.append((name, u))\n\n        self.names.append(name)\n        self.name2val[name] = val\n        self.name2before[name] = before\n        self.name2after[name] = after", "idx": 227}
{"namespace": "pyramid.traversal.find_resource", "completion": "    if isinstance(path, str):\n        if not path.startswith('/'):\n            path = '/' + path\n        path = unquote_to_bytes(path)\n        path = path.decode('utf-8', 'replace')\n        path = path.lstrip('/')\n        path = path.split('/')\n    else:\n        if not path[0].startswith('/'):\n            path = ('',) + path\n    if not path or path == ('',):\n        return find_root(resource)\n    if path[0] == '':\n        resource = find_root(resource)\n        path = path[1:]\n    for segment in path:\n        if not segment:\n            continue\n        if segment == '.':\n            continue\n        if segment == '..':\n            resource = resource.__parent__\n            continue\n        resource = resource[segment]\n    return resource", "idx": 228}
{"namespace": "pyramid.static.ManifestCacheBuster.manifest", "completion": "        if self.reload:\n            if self.exists(self.manifest_path):\n                mtime = self.getmtime(self.manifest_path)\n                if self._mtime is None or mtime > self._mtime:\n                    self._manifest = self.get_manifest()\n                    self._mtime = mtime\n        return self._manifest", "idx": 229}
{"namespace": "pyramid.registry.Registry.registerSubscriptionAdapter", "completion": "        result = super(Registry, self).registerSubscriptionAdapter(*arg, **kw)\n        self.has_listeners = True\n        return result", "idx": 230}
{"namespace": "pyramid.registry.Registry.registerHandler", "completion": "        result = Components.registerHandler(self, *arg, **kw)\n        self.has_listeners = True\n        return result", "idx": 231}
{"namespace": "pyramid.registry.Registry.notify", "completion": "        if not self.has_listeners:\n            return\n\n        for event in events:\n            subscribers = self.adapters.subscribers((event,), None)\n            for subscriber in subscribers:\n                subscriber(event)", "idx": 232}
{"namespace": "pyramid.registry.Introspector.add", "completion": "        category_name = intr.category_name\n        discriminator = intr.discriminator\n        order = self._counter\n        self._counter += 1\n\n        if category_name not in self._categories:\n            self._categories[category_name] = {}\n\n        if discriminator not in self._refs:\n            self._refs[discriminator] = []\n\n        self._refs[discriminator].append(intr)\n        self._categories[category_name][discriminator] = (intr, order)", "idx": 233}
{"namespace": "pyramid.registry.Introspector.get", "completion": "        category = self._categories.get(category_name, {})\n        return category.get(discriminator, default)", "idx": 234}
{"namespace": "pyramid.registry.Introspector.get_category", "completion": "        category = self._categories.get(category_name, default)\n        if category is not None:\n            introspectable_values = list(category.values())\n            if sort_key is not None:\n                introspectable_values.sort(key=sort_key)\n            else:\n                introspectable_values.sort(key=operator.attrgetter('order'))\n            return [{'introspectable': introspectable, 'related': category[introspectable.discriminator_hash]} for introspectable in introspectable_values]\n        return []", "idx": 235}
{"namespace": "pyramid.registry.Introspector.categorized", "completion": "        categories = self._categories.keys()\n        categorized_data = []\n        for category_name in categories:\n            category_data = self.get_category(category_name, sort_key=sort_key)\n            categorized_data.append((category_name, category_data))\n        return categorized_data", "idx": 236}
{"namespace": "pyramid.registry.Introspector.remove", "completion": "        category = self._categories.get(category_name)\n        if category:\n            introspection_object = category.pop(discriminator, None)\n            if introspection_object:\n                del self._refs[introspection_object]", "idx": 237}
{"namespace": "pyramid.registry.Introspector.relate", "completion": "        introspectables = self._get_intrs_by_pairs(pairs)\n        for i, intr in enumerate(introspectables):\n            for other in introspectables[i + 1:]:\n                intr.related.add(other)\n                other.related.add(intr)", "idx": 238}
{"namespace": "pyramid.registry.Introspector.related", "completion": "        category_name = intr.category_name\n        discriminator = intr.discriminator\n        category = self._categories.get(category_name, {})\n        introspectable = category.get(discriminator)\n        if introspectable:\n            return self._refs.get(introspectable, [])\n        else:\n            raise KeyError((category_name, discriminator))", "idx": 239}
{"namespace": "pyramid.registry.Introspectable.discriminator_hash", "completion": "        return hash((self.category_name, self.discriminator))", "idx": 240}
{"namespace": "pyramid.registry.Introspectable.__repr__", "completion": "        return f'<{self.type_name} category {self.category_name}, discriminator {self.discriminator}>'", "idx": 241}
{"namespace": "pyramid.scripts.proutes.PRoutesCommand._get_mapper", "completion": "        return registry.queryUtility(IRoutesMapper)", "idx": 242}
{"namespace": "pyramid.scripts.pshell.PShellCommand.make_shell", "completion": "        if self.args.python_shell:\n            shell = self.args.python_shell\n            if shell in self.find_all_shells():\n                return self.find_all_shells()[shell]\n            else:\n                raise ValueError('could not find a shell named \"%s\"' % shell)\n        elif self.preferred_shells:\n            for shell in self.preferred_shells:\n                if shell in self.find_all_shells():\n                    return self.find_all_shells()[shell]\n        else:\n            for shell in self.find_all_shells():\n                return self.find_all_shells()[shell]\n        return self.default_runner", "idx": 243}
{"namespace": "pyramid.config.assets.PackageOverrides.insert", "completion": "        override = Override(path, source)\n        self.overrides.insert(0, override)\n        return override", "idx": 244}
{"namespace": "pyramid.config.assets.PackageOverrides.filtered_sources", "completion": "        for override in self.overrides:\n            if override.has_resource(resource_name):\n                yield override.get_resource_stream(self, resource_name)", "idx": 245}
{"namespace": "pyramid.config.assets.PackageOverrides.real_loader", "completion": "        if self._real_loader is None:\n            raise NotImplementedError(\"Real loader is not set\")\n        return self._real_loader", "idx": 246}
{"namespace": "pyramid.config.views.MultiView.add", "completion": "        if phash is None:\n            phash = DEFAULT_PHASH\n\n        if accept is None:\n            self.views.append((order, phash, view))\n            self.views.sort(key=operator.itemgetter(0))\n        else:\n            if accept not in self.accepts:\n                self.accepts.append(accept)\n            if accept_order is not None:\n                self.accepts.sort(key=accept_order)\n            if accept not in self.media_views:\n                self.media_views[accept] = []\n            self.media_views[accept].append((order, phash, view))\n            self.media_views[accept].sort(key=operator.itemgetter(0))", "idx": 247}
{"namespace": "pyramid.config.views.MultiView.get_views", "completion": "        if request.accept and self.accepts:\n            views = []\n            for offer in request.accept:\n                if offer in self.media_views:\n                    views.extend(view for _, view, _ in self.media_views[offer])\n            views.extend(view for _, view, _ in self.views)\n            return views\n        else:\n            return [view for _, view, _ in self.views]", "idx": 248}
{"namespace": "pyramid.config.views.MultiView.match", "completion": "        views = self.get_views(request)\n        for _, view, _ in views:\n            if not hasattr(view, '__predicated__') or view.__predicated__(context, request):\n                return view\n        raise PredicateMismatch('No predicate matched')", "idx": 249}
{"namespace": "pyramid.config.views.MultiView.__permitted__", "completion": "        view = self.match(context, request)\n        if not hasattr(view, '__permitted__'):\n            return True\n        return view.__permitted__(context, request)", "idx": 250}
{"namespace": "pyramid.config.views.MultiView.__call_permissive__", "completion": "    view = self.match(context, request)\n    if hasattr(view, '__call_permissive__'):\n        return view.__call_permissive__(context, request)\n    return view(context, request)", "idx": 251}
{"namespace": "pyramid.config.actions.ActionState.processSpec", "completion": "        if spec in self._seen_files:\n            return False\n        else:\n            self._seen_files.add(spec)\n            return True", "idx": 252}
{"namespace": "pyramid.config.actions.ActionState.action", "completion": "        action = extra\n        action.update(\n            dict(\n                discriminator=discriminator,\n                callable=callable,\n                args=args,\n                kw=kw,\n                order=order,\n                info=info,\n                includepath=includepath,\n                introspectables=introspectables,\n            )\n        )\n        self.action_state.actions.append(action)", "idx": 253}
{"namespace": "pyramid.config.actions.ActionInfo.__str__", "completion": "        return f'Line {self.line} of file {self.file}:\\n{self.src}'", "idx": 254}
{"namespace": "pyramid.config.Configurator.__getattr__", "completion": "        if name in self.registry._directives:\n            directive, action_wrap = self.registry._directives[name]\n            if action_wrap:\n                def action(*args, **kw):\n                    return self.action(directive, (name,) + args, kw)\n                return action\n            else:\n                return directive\n        raise AttributeError(name)", "idx": 255}
{"namespace": "pyramid.config.Configurator.with_package", "completion": "        new_configurator = self.__class__(\n            registry=self.registry,\n            package=package,\n            root_package=self.root_package,\n            autocommit=self.autocommit,\n            route_prefix=self.route_prefix,\n        )\n        new_configurator.basepath = self.basepath\n        new_configurator.includepath = self.includepath\n        return new_configurator", "idx": 256}
{"namespace": "pyramid.config.Configurator.absolute_asset_spec", "completion": "    return self._make_spec(relative_spec)", "idx": 257}
{"namespace": "pyramid.config.Configurator.begin", "completion": "        if request is _marker:\n            registry = self.registry\n            if registry is not None:\n                request = registry.queryUtility(IRequest, default=None)\n        manager.push({'registry': self.registry, 'request': request})", "idx": 258}
{"namespace": "pyramid.config.Configurator.scan", "completion": "        scanner = venusian.Scanner(\n            config=self,\n            categories=categories,\n            onerror=onerror,\n            ignore=ignore,\n            **kw\n        )\n        if package is None:\n            package = caller_package()\n        scanner.scan(package)", "idx": 259}
{"namespace": "pyramid.config.Configurator.make_wsgi_app", "completion": "        self.commit()\n        self.registry.notify(ApplicationCreated(self.registry))\n        return Router(self.registry)", "idx": 260}
{"namespace": "aiohappybase._util.camel_case_to_pep8", "completion": "    return re.sub(r'(?<!^)([A-Z])', r'_\\1', name).lower()", "idx": 261}
{"namespace": "kinto.authorization._relative_object_uri", "completion": "    obj_parts = object_uri.split(\"/\")\n    parent_uri = \"\"\n    for i in range(len(obj_parts) - 1, 0, -1):\n        if obj_parts[i] == resource_name:\n            parent_uri = \"/\".join(obj_parts[:i+1])\n            return parent_uri\n    raise ValueError(f\"No parent URI found for resource name {resource_name} in object URI {object_uri}\")", "idx": 262}
{"namespace": "kinto.core.openapi.OpenAPI.expose_authentication_method", "completion": "        cls.security_definitions[method_name] = definition\n        if \"scopes\" in definition:\n            cls.security_roles[method_name] = definition[\"scopes\"]", "idx": 263}
{"namespace": "kinto.core.openapi.OpenAPI.generate", "completion": "        base_spec = {\n            \"host\": self.request.host,\n            \"schemes\": [self.request.scheme],\n            \"securityDefinitions\": self.security_definitions\n        }\n\n        return super().generate(swagger=base_spec)", "idx": 264}
{"namespace": "kinto.core.testing.get_user_headers", "completion": "    user_pass = f\"{user}:{password}\"\n    encoded_user_pass = base64.b64encode(user_pass.encode()).decode(\"utf-8\")\n    return {\"Authorization\": f\"Basic {encoded_user_pass}\"}", "idx": 265}
{"namespace": "kinto.core.authorization.RouteFactory.fetch_shared_objects", "completion": "        shared_objects = self._get_accessible_objects(principals, perm, get_bound_permissions)\n        if shared_objects:\n            self.shared_ids = [obj[\"id\"] for obj in shared_objects]\n            return self.shared_ids\n        return None", "idx": 266}
{"namespace": "kinto.core.authorization.RouteFactory.get_permission_object_id", "completion": "        if object_id:\n            return f\"{request.current_resource_name}/{object_id}\"\n\n        # If the request is on a plural endpoint, find the object URI by inspecting the \"plural\" service and its sibling \"object\" service.\n        if self.on_plural_endpoint:\n            plural_service = utils.current_service(request)\n            object_service = plural_service.sibling(\"object\")\n            return f\"{plural_service.path}/{object_service.path}\"\n\n        # Otherwise, return the object URI based on the request path.\n        return request.path", "idx": 267}
{"namespace": "kinto.core.utils.recursive_update_dict", "completion": "    for k, v in changes.items():\n        if v in ignores:\n            root.pop(k, None)\n        elif isinstance(v, collections_abc.Mapping):\n            root[k] = recursive_update_dict(root.get(k, {}), v, ignores)\n        else:\n            root[k] = v", "idx": 268}
{"namespace": "kinto.core.utils.native_value", "completion": "    try:\n        return json.loads(value)\n    except (ValueError, TypeError):\n        return value", "idx": 269}
{"namespace": "kinto.core.utils.dict_subset", "completion": "    result = {}\n    for key in keys:\n        if '.' in key:\n            nested_keys = key.split('.')\n            temp = d\n            for nested_key in nested_keys:\n                if nested_key in temp:\n                    temp = temp[nested_key]\n                else:\n                    temp = None\n                    break\n            if temp is not None:\n                result[key] = temp\n        else:\n            if key in d:\n                result[key] = d[key]\n    return result", "idx": 270}
{"namespace": "kinto.core.utils.dict_merge", "completion": "    for k, v in b.items():\n        if isinstance(v, dict):\n            merge_dicts(a.setdefault(k, {}), v)\n        else:\n            a.setdefault(k, v)\n    return a", "idx": 271}
{"namespace": "kinto.core.utils.find_nested_value", "completion": "    if path in d:\n        return d[path]\n    else:\n        parts = path.split(\".\")\n        root_keys = [parts[:i] for i in range(len(parts), 0, -1)]\n        for root_key in root_keys:\n            root_key_str = \".\".join(root_key)\n            if root_key_str in d and isinstance(d[root_key_str], dict):\n                subpath = \".\".join(parts[len(root_key):])\n                return find_nested_value(d[root_key_str], subpath, default)\n        return default", "idx": 272}
{"namespace": "kinto.core.utils.instance_uri_registry", "completion": "    api_prefix = f\"/{registry.route_prefix}\"\n    path = api_prefix + f\"/{resource_name}/\"  # Construct the path for the resource.\n\n    # Create a dummy request object and set the registry attribute to the given registry.\n    dummy_request = Request.blank(path=path)\n    dummy_request.registry = registry\n\n    # Use the dummy request object to find the URI for the given resource.\n    return strip_uri_prefix(dummy_request.route_path(f\"{resource_name}-object\", **params))", "idx": 273}
{"namespace": "kinto.core.statsd.load_from_config", "completion": "    if statsd_module is None:\n        raise ConfigurationError(\"The statsd module is not installed.\")\n\n    settings = config.get_settings()\n    statsd_url = settings.get(\"statsd_url\")\n    if not statsd_url:\n        raise ConfigurationError(\"No statsd_url setting found.\")\n\n    parsed_url = urlparse(statsd_url)\n    host = parsed_url.hostname\n    port = parsed_url.port\n    prefix = parsed_url.path.lstrip(\"/\")\n    return Client(host, port, prefix)", "idx": 274}
{"namespace": "kinto.core.errors.http_error", "completion": "    if code is None:\n        code = httpexception.code\n\n    if errno is None:\n        errno = ERRORS.UNDEFINED.value\n\n    if error is None:\n        error = httpexception.title\n\n    response = {\n        \"code\": code,\n        \"errno\": errno,\n        \"error\": error,\n        \"message\": message,\n        \"info\": info,\n        \"details\": details,\n    }\n\n    # Create a JSON response.\n    response = httpexceptions.JSONHTTPError(\n        code=code, detail=response, content_type=\"application/json\"\n    )\n\n    # Reapply CORS headers.\n    reapply_cors(response)\n\n    return response", "idx": 275}
{"namespace": "kinto.core.resource.schema.ResourceReponses.get_and_bind", "completion": "        # Clone the default schemas.\n        responses = self.default_schemas.copy()\n\n        # Update with endpoint-specific schemas.\n        if endpoint_type == \"object\":\n            responses.update(self.default_object_schemas)\n        elif endpoint_type == \"plural\":\n            responses.update(self.default_plural_schemas)\n\n        # Update with method-specific schemas.\n        if method == \"GET\":\n            responses.update(self.default_get_schemas)\n        elif method == \"POST\":\n            responses.update(self.default_post_schemas)\n        elif method == \"PUT\":\n            responses.update(self.default_put_schemas)\n        elif method == \"PATCH\":\n            responses.update(self.default_patch_schemas)\n        elif method == \"DELETE\":\n            responses.update(self.default_delete_schemas)\n\n        return responses", "idx": 276}
{"namespace": "kinto.core.resource.Resource.timestamp", "completion": "        try:\n            return self.model.timestamp()\n        except storage_exceptions.ReadOnly as e:\n            error_info = ERRORS[\"readonly\"]\n            error_info[\"details\"] = str(e)\n            error_info[\"location\"] = \"query\"\n            error_info[\"name\"] = \"timestamp\"\n            error_info[\"description\"] = \"This resource does not support timestamps.\"\n            error_info[\"code\"] = 400\n            error_info[\"error\"] = \"Bad Request\"\n            error_info[\"errno\"] = 109\n            error_info[\"message\"] = \"This resource does not support timestamps.\"\n            raise_invalid(self.request, **error_info)", "idx": 277}
{"namespace": "kinto.core.resource.Resource.plural_post", "completion": "        self._add_timestamp_header(self.request.response)\n        self._add_cache_header(self.request.response)\n\n        # If-Match header is provided and the objects have been modified in the meantime.\n        self._raise_412_if_modified(obj={})\n\n        # If the object id is specified, it is added to the posted body and the existing object is looked up.\n        if self.object_id:\n            existing_object = self.model.get_object(self.object_id)\n            if existing_object is not None:\n                return self.postprocess(existing_object, status=200)\n\n        # The new object is processed, created, and returned with a status code of 201.\n        new_object = self.model.create_object(self.object_id, self.request.validated[\"data\"])\n        return self.postprocess(new_object, status=201)", "idx": 278}
{"namespace": "kinto.core.resource.Resource.get", "completion": "        self._add_timestamp_header(self.request.response)\n        self._add_cache_header(self.request.response)\n        self._raise_304_if_not_modified()\n\n        obj = self._get_object_or_404(self.object_id)\n        partial_fields = self._extract_partial_fields()\n        if partial_fields:\n            obj = dict_subset(obj, partial_fields)\n\n        return self.postprocess(obj)", "idx": 279}
{"namespace": "kinto.core.resource.Resource.delete", "completion": "        self._raise_400_if_invalid_id(self.object_id)\n        existing = self._get_object_or_404(self.object_id)\n        self._raise_412_if_modified(existing)\n\n        deleted_object = self.model.delete_object(self.object_id)\n\n        timestamp = deleted_object[self.model.modified_field]\n        self._add_timestamp_header(self.request.response, timestamp=timestamp)\n\n        return self.postprocess(deleted_object, action=ACTIONS.DELETE)", "idx": 280}
{"namespace": "kinto.core.permission.memory.Permission.add_principal_to_ace", "completion": "        ace_key = f\"ace:{object_id}:{permission}\"\n        ace_principals = self._store.get(ace_key, set())\n        ace_principals.add(principal)\n        self._store[ace_key] = ace_principals", "idx": 281}
{"namespace": "kinto.core.permission.memory.Permission.get_object_permission_principals", "completion": "        permission_key = f\"permission:{object_id}:{permission}\"\n        return self._store.get(permission_key, set())", "idx": 282}
{"namespace": "kinto.core.storage.generators.Generator.match", "completion": "        if not self._regexp:\n            self._regexp = re.compile(self.regexp)\n        return bool(self._regexp.match(object_id))", "idx": 283}
{"namespace": "kinto.core.storage.postgresql.migrator.MigratorMixin.create_or_migrate_schema", "completion": "        current_version = self.get_installed_version()\n\n        if current_version is None:\n            logger.info(f\"No existing {self.name} schema found, creating new schema\")\n            # Create new schema logic here\n        elif current_version == self.schema_version:\n            logger.info(f\"{self.name} schema is up-to-date at version {self.schema_version}\")\n        else:\n            logger.info(f\"Migrating {self.name} schema from version {current_version} to {self.schema_version}\")\n            # Migration logic here", "idx": 284}
{"namespace": "kinto.core.views.batch.BatchPayloadSchema.deserialize", "completion": "        deserialized = super().deserialize(cstruct)\n\n        # Merge defaults with requests.\n        defaults = deserialized.get(\"defaults\", {})\n        requests = deserialized.get(\"requests\", [])\n\n        for request in requests:\n            for key, value in defaults.items():\n                if key not in request:\n                    request[key] = value\n\n        return deserialized", "idx": 285}
{"namespace": "kinto.plugins.accounts.utils.get_cached_reset_password", "completion": "    settings = registry.settings\n    hmac_secret = settings[\"userid_hmac_secret\"]\n    cache_key = utils.hmac_digest(hmac_secret, ACCOUNT_RESET_PASSWORD_CACHE_KEY.format(username))\n\n    cache = registry.cache\n    reset_password = cache.get(cache_key)\n\n    return reset_password", "idx": 286}
{"namespace": "kinto.plugins.accounts.utils.get_cached_validation_key", "completion": "    hmac_secret = registry.settings[\"userid_hmac_secret\"]\n    cache_key = utils.hmac_digest(hmac_secret, ACCOUNT_VALIDATION_CACHE_KEY.format(username))\n\n    cache = registry.cache\n    cache_result = cache.get(cache_key)\n    return cache_result", "idx": 287}
{"namespace": "kinto.plugins.accounts.views.validation.on_account_activated", "completion": "    settings = event.request.registry.settings\n    account_validation_enabled = settings.get(\"account_validation.enabled\", False)\n\n    if account_validation_enabled:\n        for change in event.impacted_objects:\n            old_account_validated = change.get(\"old\", {}).get(\"validated\", False)\n            new_account_validated = change.get(\"new\", {}).get(\"validated\", False)\n\n            if old_account_validated or not new_account_validated:\n                continue\n\n            # Send confirmation email to the account.\n            user_id = change[\"new\"][\"id\"]\n            user = event.request.registry.storage.get(\n                parent_id=user_id, resource_name=\"account\", object_id=user_id\n            )\n            Emailer(event.request, user).send_account_activation_confirmation()", "idx": 288}
{"namespace": "kinto.plugins.openid.OpenIDConnectPolicy._verify_token", "completion": "        userinfo_endpoint = self.oid_config.get(\"userinfo_endpoint\")\n        headers = {\n            \"Authorization\": f\"{self.header_type} {access_token}\"\n        }\n        try:\n            response = requests.get(userinfo_endpoint, headers=headers)\n            response.raise_for_status()\n            return response.json()\n        except Exception as e:\n            # Log debug message and return None if there is an error.\n            print(f\"Error verifying access token: {e}\")\n            return None", "idx": 289}
{"namespace": "kinto.plugins.quotas.scripts.rebuild_quotas", "completion": "    # Get all the buckets from the storage.\n    buckets = list(storage.list_all(\"bucket\", parent_id=\"\", sorting=OLDEST_FIRST))\n\n    # Iterate through each bucket and calculate the total record count, storage size, and collection count.\n    for bucket in buckets:\n        bucket_id = bucket[\"id\"]\n        records = list(storage.list_all(\"record\", parent_id=bucket_id))\n        collections = list(storage.list_all(\"collection\", parent_id=bucket_id))\n\n        total_record_count = len(records)\n        total_storage_size = sum(record_size(record) for record in records)\n        total_collection_count = len(collections)\n\n        # Update the quota information for the bucket in the storage.\n        quota = {\n            \"id\": COLLECTION_QUOTA_OBJECT_ID.format(bucket_id),\n            \"bucket_id\": bucket_id,\n            \"record_count\": total_record_count,\n            \"storage_size\": total_storage_size,\n            \"collection_count\": total_collection_count,\n        }\n\n        if not dry_run:\n            storage.update(\"quota\", quota[\"id\"], quota)\n        else:\n            logger.info(f\"Quota for bucket {bucket_id} would be updated with: {quota}\")\n\n        # Log the final size of each bucket.\n        logger.info(\n            f\"Bucket {bucket_id} - Record count: {total_record_count}, Storage size: {total_storage_size}, Collection count: {total_collection_count}\"\n        )", "idx": 290}
{"namespace": "kinto.config.render_template", "completion": "    with codecs.open(template, 'r', encoding='utf-8') as f:\n        content = f.read()\n        content = content.format(**kwargs)\n\n    with codecs.open(destination, 'w', encoding='utf-8') as f:\n        f.write(content)", "idx": 291}
{"namespace": "trafilatura.sitemaps.SitemapObject.extract_sitemap_langlinks", "completion": "        hreflang_links = HREFLANG_REGEX.findall(self.content)\n        for link in hreflang_links:\n            if self.target_lang in link:\n                self.handle_link(link)\n        LOGGER.debug('%s sitemaps and %s links with hreflang found for %s', len(self.sitemap_urls), len(self.urls), self.target_lang)", "idx": 292}
{"namespace": "imapclient.datetime_util.format_criteria_date", "completion": "    return dt.strftime(\"%d-%b-%Y\").encode(\"ascii\")", "idx": 293}
{"namespace": "imapclient.util.assert_imap_protocol", "completion": "    if not condition:\n        error_message = b\"Server replied with a response that violates the IMAP protocol\"\n        if message:\n            error_message += b\": \" + message\n        raise ProtocolError(to_unicode(error_message))", "idx": 294}
{"namespace": "ehforwarderbot.utils.get_config_path", "completion": "    profile = coordinator.profile\n    if module_id:\n        config_path = get_base_path() / 'profiles' / profile / module_id / f'config.{ext}'\n    else:\n        config_path = get_base_path() / 'profiles' / profile / f'config.{ext}'\n    if not config_path.exists():\n        config_path.parent.mkdir(parents=True, exist_ok=True)\n    return config_path", "idx": 295}
{"namespace": "ehforwarderbot.utils.get_custom_modules_path", "completion": "    base_path = get_base_path()\n    custom_modules_path = base_path / \"modules\"\n    if not custom_modules_path.exists():\n        custom_modules_path.mkdir(parents=True)\n    return custom_modules_path", "idx": 296}
{"namespace": "ehforwarderbot.chat.Chat.add_member", "completion": "        member = ChatMember(self, name=name, alias=alias, uid=uid, id=id, vendor_specific=vendor_specific, description=description, middleware=middleware)\n        self.members.append(member)\n        return member", "idx": 297}
{"namespace": "ehforwarderbot.chat.Chat.add_system_member", "completion": "        member = SystemChatMember(self, name=name, alias=alias, id=id, uid=uid,\n                                  vendor_specific=vendor_specific, description=description,\n                                  middleware=middleware)\n        self.members.append(member)\n        return member", "idx": 298}
{"namespace": "ehforwarderbot.chat.PrivateChat.verify", "completion": "        super().verify()\n        for member in self.members:\n            assert isinstance(member, ChatMember), f\"Invalid chat member: {member}\"", "idx": 299}
{"namespace": "telethon.utils.get_inner_text", "completion": "    result = []\n    for entity in entities:\n        start = entity.offset\n        end = entity.offset + entity.length\n        result.append(text[start:end])\n    return result", "idx": 300}
{"namespace": "telethon.extensions.html.parse", "completion": "    parser = HTMLToTelegramParser()\n    parser.feed(html)\n    parser.close()\n    return parser.text, parser.entities", "idx": 301}
{"namespace": "telethon.extensions.html.unparse", "completion": "    if not text or not entities:\n        return escape(text)\n\n    result = ''\n    last_offset = 0\n    for entity in entities:\n        result += escape(text[last_offset:entity.offset])\n        last_offset = entity.offset + entity.length\n        formatter = ENTITY_TO_FORMATTER.get(type(entity))\n        if formatter:\n            if callable(formatter):\n                result += formatter(entity, text[entity.offset:last_offset])\n            else:\n                result += formatter[0]\n                result += escape(text[entity.offset:last_offset])\n                result += formatter[1]\n\n    result += escape(text[last_offset:])\n    return result", "idx": 302}
{"namespace": "telethon.crypto.rsa.encrypt", "completion": "    global _server_keys\n    if fingerprint in _server_keys:\n        key, old = _server_keys[fingerprint]\n        if use_old and old:\n            key = old\n        data_hash = sha1(data).digest()\n        padded_data = b'\\x00' + data_hash + data + os.urandom(255 - len(data) - len(data_hash))\n        return rsa.encrypt(padded_data, key)\n    else:\n        return None", "idx": 303}
{"namespace": "hbmqtt.codecs.encode_string", "completion": "    encoded_string = string.encode('utf-8')\n    length_bytes = int_to_bytes(len(encoded_string), 2)\n    return length_bytes + encoded_string", "idx": 304}
{"namespace": "hbmqtt.plugins.manager.PluginManager.get_plugin", "completion": "        for plugin in self._plugins:\n            if plugin.name == name:\n                return plugin\n        return None", "idx": 305}
{"namespace": "pysimplesoap.simplexml.SimpleXMLElement.add_child", "completion": "        if ns is True:\n            namespace = self.__ns\n        elif ns is False or self.__ns is None:\n            namespace = None\n        else:\n            namespace = ns\n\n        if namespace and self.__prefix:\n            name = \"{}:{}\".format(self.__prefix, name)\n\n        new_element = self.__document.createElement(name)\n\n        if text is not None:\n            if isinstance(text, xml.dom.minidom.CDATASection):\n                new_element.appendChild(text)\n            else:\n                new_element.appendChild(self.__document.createTextNode(text))\n\n        self.__elements[-1].appendChild(new_element)\n\n        return SimpleXMLElement(elements=self.__elements, document=self.__document, namespace=self.__ns, prefix=self.__prefix, namespaces_map=self.__namespaces_map, jetty=self.__jetty)", "idx": 306}
{"namespace": "pysimplesoap.simplexml.SimpleXMLElement.as_xml", "completion": "        if pretty:\n            xml_str = self.__document.toprettyxml(indent=\"  \")\n        else:\n            xml_str = self.__document.toxml()\n\n        if filename:\n            with open(filename, 'w') as f:\n                f.write(xml_str)\n\n        return xml_str", "idx": 307}
{"namespace": "twilio.base.deserialize.iso8601_date", "completion": "    try:\n        date_obj = datetime.datetime.strptime(s, ISO8601_DATE_FORMAT).date()\n        return date_obj\n    except ValueError:\n        return s", "idx": 308}
{"namespace": "twilio.base.deserialize.iso8601_datetime", "completion": "    try:\n        return datetime.datetime.strptime(s, ISO8601_DATETIME_FORMAT).replace(tzinfo=datetime.timezone.utc)\n    except (TypeError, ValueError):\n        return s", "idx": 309}
{"namespace": "twilio.base.serialize.iso8601_date", "completion": "    if isinstance(d, str):\n        try:\n            date_obj = datetime.datetime.strptime(d, '%Y-%m-%d').date()\n            return date_obj.strftime('%Y-%m-%d')\n        except ValueError:\n            return None\n    elif isinstance(d, (datetime.datetime, datetime.date)):\n        return d.strftime('%Y-%m-%d')\n    else:\n        return None", "idx": 310}
{"namespace": "twilio.base.serialize.iso8601_datetime", "completion": "    if d == values.unset:\n        return d\n    elif isinstance(d, datetime.datetime):\n        return d.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n    elif isinstance(d, datetime.date):\n        return d.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n    elif isinstance(d, str):\n        return d\n    else:\n        return None", "idx": 311}
{"namespace": "twilio.base.serialize.prefixed_collapsible_map", "completion": "    return {prefix + k: v for k, v in m.items()}", "idx": 312}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.dial", "completion": "        return self.nest(Dial(\n            number=number,\n            action=action,\n            method=method,\n            timeout=timeout,\n            hangup_on_star=hangup_on_star,\n            time_limit=time_limit,\n            caller_id=caller_id,\n            record=record,\n            trim=trim,\n            recording_status_callback=recording_status_callback,\n            recording_status_callback_method=recording_status_callback_method,\n            recording_status_callback_event=recording_status_callback_event,\n            answer_on_bridge=answer_on_bridge,\n            ring_tone=ring_tone,\n            recording_track=recording_track,\n            sequential=sequential,\n            refer_url=refer_url,\n            refer_method=refer_method,\n            **kwargs\n        ))", "idx": 313}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.enqueue", "completion": "        return self.nest(\n            Enqueue(\n                name=name,\n                action=action,\n                max_queue_size=max_queue_size,\n                method=method,\n                wait_url=wait_url,\n                wait_url_method=wait_url_method,\n                workflow_sid=workflow_sid,\n                **kwargs\n            )\n        )", "idx": 314}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.gather", "completion": "        return self.nest(\n            Gather(\n                input=input,\n                action=action,\n                method=method,\n                timeout=timeout,\n                speech_timeout=speech_timeout,\n                max_speech_time=max_speech_time,\n                profanity_filter=profanity_filter,\n                finish_on_key=finish_on_key,\n                num_digits=num_digits,\n                partial_result_callback=partial_result_callback,\n                partial_result_callback_method=partial_result_callback_method,\n                language=language,\n                hints=hints,\n                barge_in=barge_in,\n                debug=debug,\n                action_on_empty_result=action_on_empty_result,\n                speech_model=speech_model,\n                enhanced=enhanced,\n                **kwargs\n            )\n        )", "idx": 315}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.say", "completion": "        return self.nest(Say(message=message, voice=voice, loop=loop, language=language, **kwargs))", "idx": 316}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.sms", "completion": "        return self.nest(\n            Sms(\n                message=message,\n                to=to,\n                from_=from_,\n                action=action,\n                method=method,\n                status_callback=status_callback,\n                **kwargs\n            )\n        )", "idx": 317}
{"namespace": "twilio.twiml.voice_response.Gather.say", "completion": "        return self.nest(\n            Say(message=message, voice=voice, loop=loop, language=language, **kwargs)\n        )", "idx": 318}
{"namespace": "twilio.twiml.voice_response.Dial.client", "completion": "        return self.nest(\n            Client(\n                identity=identity,\n                url=url,\n                method=method,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                **kwargs\n            )\n        )", "idx": 319}
{"namespace": "twilio.twiml.voice_response.Dial.conference", "completion": "        return self.nest(\n            Conference(\n                name=name,\n                muted=muted,\n                beep=beep,\n                start_conference_on_enter=start_conference_on_enter,\n                end_conference_on_exit=end_conference_on_exit,\n                wait_url=wait_url,\n                wait_method=wait_method,\n                max_participants=max_participants,\n                record=record,\n                region=region,\n                coach=coach,\n                trim=trim,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                recording_status_callback=recording_status_callback,\n                recording_status_callback_method=recording_status_callback_method,\n                recording_status_callback_event=recording_status_callback_event,\n                event_callback_url=event_callback_url,\n                jitter_buffer_size=jitter_buffer_size,\n                participant_label=participant_label,\n                **kwargs\n            )\n        )", "idx": 320}
{"namespace": "twilio.twiml.voice_response.Dial.queue", "completion": "        return self.nest(\n            Queue(\n                name,\n                url=url,\n                method=method,\n                reservation_sid=reservation_sid,\n                post_work_activity_sid=post_work_activity_sid,\n                **kwargs\n            )\n        )", "idx": 321}
{"namespace": "twilio.twiml.voice_response.Dial.sip", "completion": "        return self.nest(\n            Sip(\n                sip_url,\n                username=username,\n                password=password", "idx": 322}
{"namespace": "twilio.twiml.messaging_response.MessagingResponse.message", "completion": "        message = self.append(\n            self.xml.createElement(\"Message\"),\n            body,\n            to=to,\n            from_=from_,\n            action=action,\n            method=method,\n            statusCallback=status_callback,\n            **kwargs\n        )\n        return message", "idx": 323}
{"namespace": "twilio.twiml.TwiML.append", "completion": "        self.verbs.append(verb)\n        return self", "idx": 324}
{"namespace": "twilio.jwt.Jwt.to_jwt", "completion": "        ttl = ttl or self.ttl\n        payload = self.payload.copy()\n        payload[\"exp\"] = int(time.time()) + ttl\n        headers = self.headers.copy()\n        encoded_jwt = jwt_lib.encode(payload, self.secret_key, algorithm=self.algorithm, headers=headers)\n        return encoded_jwt", "idx": 325}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_client_outgoing", "completion": "        scope = \"scope:client:outgoing?appSid={}\".format(application_sid)\n        if kwargs:\n            scope += \"&\" + urlencode(kwargs)\n        self.capabilities[\"outgoing\"] = scope", "idx": 326}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_client_incoming", "completion": "        scope = ScopeURI(\"client\", \"incoming\", {\"clientName\": client_name})\n        self.capabilities[\"incoming\"] = scope", "idx": 327}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_event_stream", "completion": "        scope = ScopeURI(\"stream\", \"subscribe\", kwargs)\n        self.capabilities[\"event_stream\"] = scope", "idx": 328}
{"namespace": "twilio.jwt.client.ClientCapabilityToken._generate_payload", "completion": "        payload_values = []\n        for capability in self.capabilities.values():\n            if capability.scope_uri == \"client/outgoing\" and self.client_name is not None:\n                capability.add_param(\"clientName\", self.client_name)\n            payload_values.append(capability.to_payload())\n\n        payload = {\"scope\": \" \".join(payload_values)}\n        return payload", "idx": 329}
{"namespace": "twilio.jwt.client.ScopeURI.to_payload", "completion": "        if self.params:\n            sorted_params = sorted(self.params.items(), key=lambda x: x[0])\n            encoded_params = urlencode(sorted_params, doseq=True)\n            param_string = f\"?{encoded_params}\" if encoded_params else \"\"\n        else:\n            param_string = \"\"\n\n        return f\"scope:{self.service}:{self.privilege}{param_string}\"", "idx": 330}
{"namespace": "twilio.jwt.access_token.AccessToken.add_grant", "completion": "        if not isinstance(grant, AccessTokenGrant):\n            raise ValueError(\"Grant must be an instance of AccessTokenGrant.\")\n        self.grants.append(grant)", "idx": 331}
{"namespace": "twilio.jwt.taskrouter.capabilities.WorkerCapabilityToken.allow_update_activities", "completion": "        self._make_policy(self.workspace_url + \"/Activities\", \"POST\", {\"ActivitySid\": {\"required\": True}})", "idx": 332}
{"namespace": "zulipterminal.platform_code.successful_GUI_return_code", "completion": "    return 1 if PLATFORM == \"WSL\" else 0", "idx": 333}
{"namespace": "zulipterminal.platform_code.normalized_file_path", "completion": "    if PLATFORM == \"WSL\":\n        return path.replace(\"/\", \"\\\\\")\n    else:\n        return path", "idx": 334}
{"namespace": "zulipterminal.helper.canonicalize_color", "completion": "    if len(color) == 7:\n        return color.lower()\n    elif len(color) == 4:\n        return \"#\" + color[1] * 2 + color[2] * 2 + color[3] * 2\n    else:\n        raise ValueError(\"Invalid color format. Color should be in the format '#xxxxxx' or '#xxx'\")", "idx": 335}
{"namespace": "zulipterminal.helper.get_unused_fence", "completion": "        import re\n    import re\n\n    # Find all continuous back-ticks in the content\n    matches = re.findall(r'`+', content)\n\n    # Calculate the maximum length of the fence\n    max_length = max(len(match) for match in matches) + 1\n\n    # Generate the fence using back-ticks\n    fence = \"`\" * max_length\n\n    return fence", "idx": 336}
{"namespace": "zulipterminal.helper.open_media", "completion": "    try:\n        subprocess.run([tool, media_path], check=True)\n    except subprocess.CalledProcessError as e:\n        controller.report_error(f\"Error opening media: {e}\")", "idx": 337}
{"namespace": "zulipterminal.server_url.encode_stream", "completion": "    encoded_stream_name = hash_util_encode(stream_name.replace(\" \", \"-\"))\n    return f\"{stream_name}-{encoded_stream_name}\"", "idx": 338}
{"namespace": "zulipterminal.server_url.near_message_url", "completion": "    if message[\"type\"] == \"stream\":\n        return near_stream_message_url(server_url, message)\n    else:\n        return near_pm_message_url(server_url, message)", "idx": 339}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.update_recipients", "completion": "        input_text = write_box.get_edit_text()\n\n        # Use regex to extract recipient emails from the input text\n        recipient_emails = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', input_text)\n\n        # Map recipient emails to user IDs using the user_id_email_dict\n        recipient_user_ids = [\n            self.model.email_user_dict[email]\n            for email in recipient_emails\n            if email in self.model.email_user_dict\n        ]\n\n        # Update the recipient_user_ids in the WriteBox instance\n        self._set_regular_and_typing_recipient_user_ids(recipient_user_ids)", "idx": 340}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.stream_box_view", "completion": "\n        self._setup_common_stream_compose(stream_id, caption, title)\n\n        def set_stream_marker(edit: object, new_edit_text: str) -> None:\n            self.model.set_narrow(\n                stream_id=stream_id,\n                topic=title,\n            )\n\n        urwid.connect_signal(self.msg_write_box, \"change\", set_stream_marker)", "idx": 341}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.stream_box_edit_view", "completion": "\n        self.stream_write_box = ReadlineEdit(\n            edit_text=caption, max_char=self.model.max_stream_name_length\n        )\n        self.stream_write_box.enable_autocomplete(\n            func=self._stream_box_autocomplete,\n            key=primary_key_for_command(\"AUTOCOMPLETE\"),\n            key_reverse=primary_key_for_command(\"AUTOCOMPLETE_REVERSE\"),\n        )\n        self.stream_write_box.set_completer_delims(\"\")\n        self._setup_common_stream_compose(stream_id, caption, title)\n\n        # Add an edit mode button to the header write box\n        edit_mode_button = EditModeButton(self)\n        self.header_write_box.contents.insert(0, (edit_mode_button, self.header_write_box.options()))\n\n        # Use and set a callback to set the stream marker\n        self._set_stream_write_box_style(None, caption)\n        urwid.connect_signal(\n            self.stream_write_box, \"change\", self._set_stream_write_box_style\n        )", "idx": 342}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._set_stream_write_box_style", "completion": "\n        # Check if the input text is a valid stream name\n        if new_text in self.model.stream_id_dict:\n            stream_id = self.model.stream_id_dict[new_text]\n            stream_color = self.model.stream_dict[stream_id][\"color\"]\n            stream_marker = self.model.stream_dict[stream_id][\"name\"][0].upper()\n        else:\n            # If the input text is not a valid stream name, set default values\n            stream_color = \"default\"\n            stream_marker = \"?\"\n\n        # Set the color and stream marker in the header write box\n        self.header_write_box.widget_list[1].set_attr_map({None: stream_color})\n        self.header_write_box.widget_list[0].set_text(stream_marker)", "idx": 343}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._to_box_autocomplete", "completion": "        users = self.view.users\n        recipients = text.split(\",\")\n        latest_recipient = recipients[-1].strip()\n        matching_users = [user for user in users if latest_recipient.lower() in user[\"email\"].lower()]\n        autocompleted_recipients = \", \".join(recipients[:-1] + [matching_users[0][\"email\"]])\n        user_names = [user[\"full_name\"] for user in matching_users]\n        \n        return self.process_typeaheads(autocompleted_recipients, state, user_names)", "idx": 344}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._topic_box_autocomplete", "completion": "        topics_list = self.model.topics_in_stream(self.stream_id)\n        matching_topics = [topic for topic in topics_list if text.lower() in topic.lower()]\n\n        return self._process_typeaheads(matching_topics, state, matching_topics)", "idx": 345}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._stream_box_autocomplete", "completion": "        stream_names = self.view.streams\n        matching_streams = [stream for stream in stream_names if match_stream(stream, text)]\n        return self._process_typeaheads(matching_streams, state, matching_streams)", "idx": 346}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.generic_autocomplete", "completion": "        if text.startswith(\"@\"):\n            return self._to_box_autocomplete(text, state)\n        elif text.startswith(\"#\"):\n            return self._topic_box_autocomplete(text, state)\n        elif text.startswith(\"```\"):\n            return get_unused_fence(text)\n        elif text.startswith(\"```\\n\"):\n            return \"python\\n\"\n        elif text.startswith(\"```python\\n\"):\n            return \"```\"\n        elif text.startswith(\"```python\\n```\"):\n            return \"\\n\"\n        elif text.startswith(\"```python\\n\\n\"):\n            return \"```\"\n        elif text.startswith(\"```python\\n\\n```\"):\n            return \"\\n\"\n        elif text.startswith(\"```python\\n\\n\\n\"):\n            return \"```\"\n        elif text.startswith(\"```python\\n\\n\\n```\"):\n            return \"\\n\"\n        elif text.startswith(\"```python\\n\\n\\n\\n\"):\n            return \"```\"\n        elif text.startswith(\"```python\\n\\n\\n\\n```\"):\n            return \"\\n\"\n        elif text.startswith(\"```python\\n\\n\\n\\n\\n\"):\n            return \"```\"\n        elif text.startswith(\"```python\\n\\n\\n\\n\\n```\"):\n            return \"\\n\"\n        elif text.startswith(\"```python\\n\\n\\n\\n\\n\\n\"):\n            return \"```\"\n        elif text.startswith(\"```python\\n\\n\\n\\n\\n\\n```\"):\n            return \"\\n\"\n        elif text.startswith(\"```python\\n\\n\\n\\n\\n\\n\\n\"):\n            return \"```\"\n        elif text.startswith(\"```python\\n\\n\\n\\n\\n\\n\\n```\"):\n            return \"\\n\"\n        elif text.startswith(\"```python\\n\\n\\n\\n\\n\\n\\n\\n\"):\n            return \"```\"\n        elif text.startswith(\"```python\\n\\n\\n\\n\\n\\n\\n\\n```\"):\n            return \"\\n\"\n        elif text.startswith(\"```python\\n\\n\\n\\n\\n\\n\\n\\n\\n\"):\n            return \"```\"\n        elif text.startswith(\"```python\\n\\n\\n\\n\\n\\n\\n\\n\\n```\"):\n            return \"\\n\"\n        else:\n            return None", "idx": 347}
{"namespace": "zulipterminal.ui_tools.boxes.PanelSearchBox.reset_search_text", "completion": "        self.set_edit_text(\"\")  # Clear the edit text\n        self.set_caption(self.search_text)  # Set the caption to the current search text", "idx": 348}
{"namespace": "zulipterminal.ui_tools.boxes.PanelSearchBox.valid_char", "completion": "        if self.edit_text == \"\":\n            # Check if the character is a valid unicode character and not a control character or space separator\n            return ch.isprintable() and ch != \" \"\n        else:\n            # Use the regular validation method\n            return super().valid_char(ch)", "idx": 349}
{"namespace": "zulipterminal.ui_tools.utils.is_muted", "completion": "    if msg[\"type\"] == \"private\":\n        return False\n    # Check if the message is in a topic narrow\n    if model.narrow_topic(msg[\"stream_id\"], msg[\"subject\"]):\n        return False\n    # Check if the message's stream or topic is muted in the model\n    if msg[\"stream_id\"] in model.muted_streams:\n        return True\n    if (msg[\"stream_id\"], msg[\"subject\"]) in model.muted_topics:\n        return True\n    return False", "idx": 350}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.update_count", "completion": "        if text_color is None:\n            text_color = self.original_color\n\n        self.count = count\n        self.count_style = text_color\n\n        count_text = f\" {self.prefix_character}{count}\" if count > 0 else \"\"\n        self.update_widget(count_text, text_color)", "idx": 351}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.update_widget", "completion": "        self.count_style, count_text = count_text\n        self.button_prefix.set_text(self.prefix_character)\n        self._label.set_text(self._caption)\n        self.button_suffix.set_text(count_text)\n        if text_color:\n            self._w.set_attr_map({None: urwid.AttrSpec(text_color)})", "idx": 352}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.keypress", "completion": "        if key == \"enter\":\n            self.activate(key)\n            return None\n        else:\n            return super().keypress(size, key)", "idx": 353}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._parse_narrow_link", "completion": "\n        parsed_link: ParsedNarrowLink = {}\n\n        # Extract the fragment from the link\n        parsed_url = urlparse(link)\n        fragment = parsed_url.fragment\n\n        # Parse the fragment based on the supported formats\n        if fragment.startswith(\"narrow/stream/\"):\n            parts = fragment.split(\"/\")\n            if len(parts) >= 3:\n                stream_data = cls._decode_stream_data(parts[2])\n                parsed_link[\"stream\"] = stream_data\n                if len(parts) >= 5:\n                    if parts[3] == \"topic\":\n                        parsed_link[\"topic_name\"] = parts[4].replace(\".20\", \" \")\n                        if len(parts) == 7 and parts[5] == \"near\":\n                            parsed_link[\"message_id\"] = cls._decode_message_id(parts[6])\n\n        return parsed_link", "idx": 354}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._validate_and_patch_stream_data", "completion": "        stream_data = parsed_link.get(\"stream\")\n        if stream_data:\n            stream_id = stream_data.get(\"stream_id\")\n            stream_name = stream_data.get(\"stream_name\")\n\n            # Check if stream ID or name is missing\n            if stream_id is None and stream_name is None:\n                return \"Stream ID or name is missing in the link.\"\n\n            # Check if the stream is subscribed by the user\n            if stream_id is not None and stream_id not in self.model.stream_dict:\n                return \"Stream ID is not valid or not subscribed by the user.\"\n\n            if stream_name is not None and stream_name not in self.model.stream_dict_by_name:\n                return \"Stream name is not valid or not subscribed by the user.\"\n\n            # Patch the stream data in the parsed link if necessary\n            if stream_id is None and stream_name in self.model.stream_dict_by_name:\n                stream_data[\"stream_id\"] = self.model.stream_dict_by_name[stream_name][\"id\"]\n            elif stream_name is None and stream_id in self.model.stream_dict:\n                stream_data[\"stream_name\"] = self.model.stream_dict[stream_id][\"name\"]\n\n        return \"\"  # Return empty string if stream data is valid and patched successfully", "idx": 355}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._validate_narrow_link", "completion": "        if parsed_link[\"narrow\"] not in [\"stream\", \"stream:topic\", \"stream:near\", \"stream:topic:near\"]:\n            return \"Invalid narrow type\"\n\n        if parsed_link[\"narrow\"] in [\"stream\", \"stream:topic\", \"stream:near\", \"stream:topic:near\"]:\n            return self._validate_and_patch_stream_data(parsed_link)", "idx": 356}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._switch_narrow_to", "completion": "        narrow_type = parsed_link.get(\"narrow\")\n        stream_data = parsed_link.get(\"stream\")\n        topic_name = parsed_link.get(\"topic_name\")\n        message_id = parsed_link.get(\"message_id\")\n\n        if narrow_type == \"stream\":\n            stream_id = stream_data.get(\"stream_id\")\n            stream_name = stream_data.get(\"stream_name\")\n            if stream_id is not None:\n                self.controller.narrow_to_stream(stream_id=stream_id)\n            elif stream_name is not None:\n                self.controller.narrow_to_stream(stream_name=stream_name)\n        elif narrow_type == \"stream:topic\":\n            stream_id = stream_data.get(\"stream_id\")\n            stream_name = stream_data.get(\"stream_name\")\n            if stream_id is not None and topic_name is not None:\n                self.controller.narrow_to_topic(stream_id=stream_id, topic_name=topic_name)\n            elif stream_name is not None and topic_name is not None:\n                self.controller.narrow_to_topic(stream_name=stream_name, topic_name=topic_name)\n        elif narrow_type == \"stream:near\":\n            stream_id = stream_data.get(\"stream_id\")\n            message_id = message_id\n            if stream_id is not None and message_id is not None:\n                self.controller.narrow_to_stream(stream_id=stream_id, message_id=message_id)\n        elif narrow_type == \"stream:topic:near\":\n            stream_id = stream_data.get(\"stream_id\")\n            topic_name = topic_name\n            message_id = message_id\n            if stream_id is not None and topic_name is not None and message_id is not None:\n                self.controller.narrow_to_topic(stream_id=stream_id, topic_name=topic_name, message_id=message_id)", "idx": 357}
{"namespace": "zulipterminal.config.themes.complete_and_incomplete_themes", "completion": "    complete_themes = []\n    incomplete_themes = []\n\n    for theme_name, theme_data in THEMES.items():\n        if all(style in theme_data for style in REQUIRED_STYLES) and all(meta in theme_data.get(\"pygments\", {}) for meta in REQUIRED_META[\"pygments\"]):\n            complete_themes.append(theme_name)\n        else:\n            incomplete_themes.append(theme_name)\n\n    complete_themes.sort()\n    incomplete_themes.sort()\n\n    return complete_themes, incomplete_themes", "idx": 358}
{"namespace": "zulipterminal.config.themes.validate_colors", "completion": "    theme = THEMES[theme_name]\n    for style, color in theme.STYLES.items():\n        if color not in valid_16_color_codes:\n            raise InvalidThemeColorCode(f\"Invalid color code '{color}' for style '{style}' in theme '{theme_name}'\")\n    if color_depth != 16:\n        raise ValueError(\"Color depth must be 16 for theme validation\")", "idx": 359}
{"namespace": "zulipterminal.config.themes.parse_themefile", "completion": "    theme_spec = []\n    for style, (fg_color, bg_color) in theme_styles.items():\n        if color_depth == 1:\n            fg_color = \"default\"\n            bg_color = \"default\"\n        elif color_depth == 16:\n            fg_color = fg_color if fg_color in valid_16_color_codes else \"default\"\n            bg_color = bg_color if bg_color in valid_16_color_codes else \"default\"\n        elif color_depth == 256:\n            fg_color = fg_color if isinstance(fg_color, int) else 0\n            bg_color = bg_color if isinstance(bg_color, int) else 0\n        elif color_depth == 2 ** 24:\n            fg_color = fg_color if isinstance(fg_color, int) else 0\n            bg_color = bg_color if isinstance(bg_color, int) else 0\n        theme_spec.append((style, fg_color, bg_color))\n    return theme_spec", "idx": 360}
{"namespace": "zulipterminal.config.themes.add_pygments_style", "completion": "    pygments_styles = theme_meta.get(\"pygments\", {}).get(\"styles\")\n    if pygments_styles:\n        for style in pygments_styles:\n            if style[0] in STANDARD_TYPES:\n                urwid_theme.append(style)", "idx": 361}
{"namespace": "zulipterminal.config.keys.is_command_key", "completion": "    for binding in KEY_BINDINGS.values():\n        if key in binding['keys']:\n            if command in ZT_TO_URWID_CMD_MAPPING:\n                return ZT_TO_URWID_CMD_MAPPING[command] == command_map[key]\n            else:\n                raise InvalidCommand(f\"Invalid command: {command}\")\n    return False", "idx": 362}
{"namespace": "zulipterminal.config.keys.keys_for_command", "completion": "    try:\n        return KEY_BINDINGS[command][\"keys\"]\n    except KeyError as exception:\n        raise InvalidCommand(command)", "idx": 363}
{"namespace": "zulipterminal.config.keys.commands_for_random_tips", "completion": "    return [binding for binding in KEY_BINDINGS.values() if not binding.get(\"excluded_from_random_tips\", False)]", "idx": 364}
{"namespace": "hypertools.datageometry.DataGeometry.transform", "completion": "\n        if data is None:\n            return self.xform_data\n        else:\n            # Perform transformation on the input data using the specified model\n            # Store the transformed data in self.xform_data\n            # Return the transformed data\n            pass  # Replace 'pass' with the actual transformation code", "idx": 365}
{"namespace": "hypertools.datageometry.DataGeometry.plot", "completion": "        if data is None:\n            data = self.xform_data\n        else:\n            data = format_data(\n                data,\n                semantic=self.semantic,\n                vectorizer=self.vectorizer,\n                corpus=self.corpus,\n                ppca=True)\n        from .plot.plot import plot as plotter\n        return plotter(data, **kwargs)", "idx": 366}
{"namespace": "awesome_autodl.autodl_topic2papers", "completion": "        import yaml\n        from autodl_paper import AutoDLpaper\n    from autodl_paper import AutoDLpaper\n    import yaml\n\n    topic2path = autodl_topic2path()\n    topic2papers = OrderedDict()\n\n    for topic, path in topic2path.items():\n        with open(path, 'r') as file:\n            data = yaml.safe_load(file)\n            papers = [AutoDLpaper(**paper) for paper in data]\n            topic2papers[topic] = papers\n\n    return topic2papers", "idx": 367}
{"namespace": "awesome_autodl.get_bib_abbrv_obj", "completion": "    from awesome_autodl.bib import BibAbbreviations\n    bib_abbrv_file = get_bib_abbrv_file()\n    return BibAbbreviations(bib_abbrv_file)", "idx": 368}
{"namespace": "mssqlcli.localized_strings.translation", "completion": "    if languages is None:\n        languages = LANGUAGES\n    translation = gettext.translation(domain, localedir, languages=languages)\n    return translation", "idx": 369}
{"namespace": "mssqlcli.mssqlbuffer._is_query_executable", "completion": "\n    # Remove comments\n    sql = re.sub(r'/\\*.*?\\*/', '', sql)\n    sql = re.sub(r'--.*?\\n', '', sql)\n\n    # Check for open quotes\n    open_quote = False\n    for char in sql:\n        if char == \"'\":\n            open_quote = not open_quote\n        if char == 'G' and not open_quote:\n            if sql.startswith('GO'):\n                return True\n    return False", "idx": 370}
{"namespace": "mssqlcli.telemetry.conclude", "completion": "\n    _session.end_time = datetime.now()\n    payload = _session.generate_payload()\n\n    telemetry_file = MSSQL_CLI_TELEMETRY_FILE\n    with open(telemetry_file, 'w') as f:\n        f.write(payload)\n\n    if separate_process:\n        result = subprocess.run(['curl', '-X', 'POST', service_endpoint_uri, '-d', telemetry_file])\n    else:\n        result = subprocess.run(['curl', '-X', 'POST', service_endpoint_uri, '-d', telemetry_file], capture_output=True)\n\n    return result", "idx": 371}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.start", "completion": "        request_thread = threading.Thread(name=self.REQUEST_THREAD_NAME, target=self._request_thread)\n        response_thread = threading.Thread(name=self.RESPONSE_THREAD_NAME, target=self._response_thread)\n\n        request_thread.start()\n        response_thread.start()", "idx": 372}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.submit_request", "completion": "        if method is None or params is None:\n            raise ValueError(\"Method and params cannot be None\")\n\n        request = {\n            \"jsonrpc\": \"2.0\",\n            \"method\": method,\n            \"params\": params,\n            \"id\": request_id\n        }\n\n        self.request_queue.put(request)", "idx": 373}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.get_response", "completion": "        if request_id in self.response_map:\n            response_queue = self.response_map[request_id]\n            try:\n                response = response_queue.get_nowait()\n                return response\n            except:\n                pass\n\n        if owner_uri in self.response_map:\n            response_queue = self.response_map[owner_uri]\n            try:\n                response = response_queue.get_nowait()\n                return response\n            except:\n                pass\n\n        if not self.exception_queue.empty():\n            exception = self.exception_queue.get_nowait()\n            raise exception\n\n        return None", "idx": 374}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.shutdown", "completion": "        self.cancel = True\n        self.request_queue.put(None)\n        self.request_thread.join()\n        self.writer.close()\n        logger.debug('Json Rpc client shutdown.')", "idx": 375}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcWriter.send_request", "completion": "        content = {\n            u'method': method,\n            u'params': params,\n            u'id': request_id\n        }\n        content_json = json.dumps(content, ensure_ascii=False).encode(self.encoding)\n        header = self.HEADER.format(len(content_json)).encode(self.encoding)\n\n        try:\n            self.stream.write(header)\n            self.stream.write(content_json)\n            self.stream.flush()\n        except ValueError as error:\n            raise ValueError(u'Stream is closed. Unable to send request.') from error", "idx": 376}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.read_response", "completion": "\n        while self.needs_more_data:\n            if self.read_state == ReadState.Header:\n                self._read_header()\n            elif self.read_state == ReadState.Content:\n                self._read_content()\n\n        try:\n            response = json.loads(self.buffer[:self.expected_content_length].decode(self.encoding))\n            self._trim_buffer(self.expected_content_length)\n            return response\n        except ValueError as error:\n            logger.error('Error parsing JSON response: %s', error)\n            raise ValueError('Error parsing JSON response')", "idx": 377}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.read_next_chunk", "completion": "        try:\n            # Check if buffer needs to be resized\n            if self.buffer_end_offset >= len(self.buffer) * self.BUFFER_RESIZE_TRIGGER:\n                self.buffer = bytearray(self.buffer + bytearray(self.DEFAULT_BUFFER_SIZE))\n\n            # Read data from the stream into the buffer\n            data = self.stream.read(self.DEFAULT_BUFFER_SIZE)\n            if not data:\n                raise ValueError(\"Stream is empty or closed externally\")\n\n            # Update buffer offset\n            self.buffer[self.buffer_end_offset:self.buffer_end_offset + len(data)] = data\n            self.buffer_end_offset += len(data)\n\n            return True\n        except ValueError as ex:\n            logger.debug(u'Read Next Chunk encountered exception %s', ex)\n            return False", "idx": 378}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.try_read_headers", "completion": "        header_end = self.buffer.rfind(b'\\r\\n\\r\\n')\n        if header_end == -1:\n            return False\n\n        headers = self.buffer[:header_end].decode(self.encoding).split('\\r\\n')\n        for header in headers:\n            key, value = header.split(': ')\n            self.headers[key] = value\n\n        if 'Content-Length' in self.headers:\n            self.expected_content_length = int(self.headers['Content-Length'])\n\n        self.read_state = ReadState.Content\n        return True", "idx": 379}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.close", "completion": "        try:\n            self.stream.close()\n        except AttributeError:\n            raise AttributeError(\"Failed to close the stream\")", "idx": 380}
{"namespace": "mssqlcli.packages.prioritization.PrevalenceCounter.update", "completion": "        # Update keyword counts\n        for keyword, regex in keyword_regexs.items():\n            self.keyword_counts[keyword] += len(regex.findall(text))\n\n        # Update name counts\n        parsed = sqlparse.parse(text)\n        for token in parsed:\n            if token.ttype is Name:\n                name = token.value.lower()\n                self.name_counts[name] += 1", "idx": 381}
{"namespace": "mssqlcli.packages.sqlcompletion.suggest_type", "completion": "    stmt = SqlStatement(full_text, text_before_cursor)\n\n    if stmt.word_before_cursor and stmt.word_before_cursor[0] == '\\\\':\n        return Path(),\n\n    if stmt.parsed:\n        special_command = parse_special_command(stmt)\n        if special_command:\n            return special_command\n\n        last_token = stmt.last_token\n        if last_token.value.lower() == 'from':\n            return FromClauseItem(),\n        elif last_token.value.lower() == 'join':\n            return Join(),\n        elif last_token.value.lower() == 'on':\n            return JoinCondition(),\n        elif last_token.value.lower() == 'select':\n            return Column(),\n        elif last_token.value.lower() == 'insert':\n            return Table(),\n        elif last_token.value.lower() == 'update':\n            return Table(),\n        elif last_token.value.lower() == 'delete':\n            return Table(),\n        elif last_token.value.lower() == 'create':\n            return Table(),\n        elif last_token.value.lower() == 'alter':\n            return Table(),\n        elif last_token.value.lower() == 'drop':\n            return Table(),\n        elif last_token.value.lower() == 'truncate':\n            return Table(),\n        elif last_token.value.lower() == 'use':\n            return Database(),\n        elif last_token.value.lower() == 'set':\n            return NamedQuery(),\n        elif last_token.value.lower() == 'declare':\n            return NamedQuery(),\n        elif last_token.value.lower() == 'with':\n            return NamedQuery(),\n        elif last_token.value.lower() == 'exec':\n            return Function(),\n        elif last_token.value.lower() == 'where':\n            return Comparison(),\n        elif last_token.value.lower() == 'order by':\n            return Column(),\n        elif last_token.value.lower() == 'group by':\n            return Column(),\n        elif last_token.value.lower() == 'having':\n            return Comparison(),\n        elif last_token.value.lower() == 'union':\n            return Keyword(),\n        elif last_token.value.lower() == 'intersect':\n            return Keyword(),\n        elif last_token.value.lower() == 'except':\n            return Keyword(),\n        elif last_token.value.lower() == 'case':\n            return Keyword(),\n        elif last_token.value.lower() == 'when':\n            return Keyword(),\n        elif last_token.value.lower() == 'else':\n            return Keyword(),\n        elif last_token.value.lower() == 'and':\n            return Keyword(),\n        elif last_token.value.lower() == 'or':\n            return Keyword(),\n        elif last_token.value.lower() == 'not':\n            return Keyword(),\n        elif last_token.value.lower() == 'in':\n            return Keyword(),\n        elif last_token.value.lower() == 'exists':\n            return Keyword(),\n        elif last_token.value.lower() == 'like':\n            return Keyword(),\n        elif last_token.value.lower() == 'between':\n            return Keyword(),\n        elif last_token.value.lower() == 'is':\n            return Keyword(),\n        elif last_token.value.lower() == 'null':\n            return Keyword(),\n        elif last_token.value.lower() == 'distinct':\n            return Keyword(),\n        elif last_token.value.lower() == 'top':\n            return Keyword(),\n        elif last_token.value.lower() == 'limit':\n            return Keyword(),\n        elif last_token.value.lower() == 'offset':\n            return Keyword(),\n        elif last_token.value.lower() == 'fetch':\n            return Keyword(),\n        elif last_token.value.lower() == 'first':\n            return Keyword(),\n        elif last_token.value.lower() == 'next':\n            return Keyword(),\n        elif last_token.value.lower() == 'rows':\n            return Keyword(),\n        elif last_token.value.lower() == 'only':\n            return Keyword(),\n        elif last_token.value.lower() == 'fetch':\n            return Keyword(),\n        elif last_token.value.lower() == 'next':\n            return Keyword(),\n        elif last_token.value.lower() == 'row':\n            return Keyword(),\n        elif last_token.value.lower() == 'rows':\n            return Keyword(),\n        elif last_token.value.lower() == 'percent':\n            return Keyword(),\n        elif last_token.value.lower() == 'with':\n            return Keyword(),\n        elif last_token.value.lower() == 'ties':\n            return Keyword(),\n        elif last_token.value.lower() == 'current':\n            return Keyword(),\n        elif last_token.value.lower() == 'of':\n            return Keyword(),\n        elif last_token.value.lower() == 'now':\n            return Keyword(),\n        elif last_token.value.lower() == 'only':\n            return Keyword(),\n        elif last_token.value.lower() == 'row':\n            return Keyword(),\n        elif last_token.value.lower() == 'rows':\n            return Keyword(),\n        elif last_token.value.lower() == 'fetch':\n            return Keyword(),\n        elif last_token.value.lower() == 'next':\n            return Keyword(),\n        elif last_token.value.lower() == 'row':\n            return Keyword(),\n        elif last_token.value.lower() == 'rows':\n            return Keyword(),\n        elif last_token.value.lower() == 'percent':\n            return Keyword(),\n        elif last_token.value.lower() == 'with':\n            return Keyword(),\n        elif last_token.value.lower() == 'ties':\n            return Keyword(),\n        elif last_token.value.lower() == 'current':\n            return Keyword(),\n        elif last_token.value.lower() == 'of':\n            return Keyword(),\n        elif last_token.value.lower() == 'now':\n            return Keyword(),\n        elif last_token.value.lower() == 'only':\n            return Keyword(),\n        elif last_token.value.lower() == 'row':\n            return Keyword(),\n        elif last_token.value.lower() == 'rows':\n            return Keyword(),\n        elif last_token.value.lower() == 'fetch':\n            return Keyword(),\n        elif last_token.value.lower() == 'next':\n            return Keyword(),\n        elif last_token.value.lower() == 'row':\n            return Keyword(),\n        elif last_token.value.lower() == 'rows':\n            return Keyword(),\n        elif last_token.value.lower() == 'percent':\n            return Keyword(),\n        elif last_token.value.lower() == 'with':\n            return Keyword(),\n        elif last_token.value.lower() == 'ties':\n            return Keyword(),\n        elif last_token.value.lower() == 'current':\n            return Keyword(),\n        elif last_token.value.lower() == 'of':\n            return Keyword(),\n        elif last_token.value.lower() == 'now':\n            return Keyword(),\n        elif last_token.value.lower() == 'only':\n            return Keyword(),\n        elif last_token.value.lower() == 'row':\n            return Keyword(),\n        elif last_token.value.lower() == 'rows':\n            return Keyword(),\n        elif last_token.value.lower() == 'fetch':\n            return Keyword(),\n        elif last_token.value.lower() == 'next':\n            return Keyword(),\n        elif last_token.value.lower() == 'row':\n            return Keyword(),\n        elif last_token.value.lower() == 'rows':\n            return Keyword(),\n        elif last_token.value.lower() == 'percent':\n            return Keyword(),\n        elif last_token.value.lower() == 'with':\n            return Keyword(),\n        elif last_token.value.lower() == 'ties':\n            return Keyword(),\n        elif last_token.value.lower() == 'current':\n            return Keyword(),\n        elif last_token.value.lower() == 'of':\n            return Keyword(),\n        elif last_token.value.lower() == 'now':\n            return Keyword(),\n        elif last_token.value.lower() == 'only':\n            return Keyword(),\n        elif last_token.value.lower() == 'row':\n            return Keyword(),\n        elif last_token.value.lower() == 'rows':\n            return Keyword(),\n        elif last_token.value.lower() == 'fetch':\n            return Keyword(),\n        elif last_token.value.lower() == 'next':\n            return Keyword(),\n        elif last_token.value.lower() == 'row':\n            return Keyword(),\n        elif last_token.value.lower() == 'rows':\n            return Keyword(),\n        elif last_token.value.lower() == 'percent':\n            return Keyword(),\n        elif last_token.value.lower() == 'with':\n            return Keyword(),\n        elif last_token.value.lower() == 'ties':\n            return Keyword(),\n        elif last_token.value.lower() == 'current':\n            return Keyword(),\n        elif last_token.value.lower() == 'of':\n            return Keyword(),\n        elif last_token.value.lower() == 'now':\n            return Keyword(),\n        elif last_token.value.lower() == 'only':\n            return Keyword(),\n        elif last_token.value.lower() == 'row':\n            return Keyword(),\n        elif last_token.value.lower() == 'rows':\n            return Keyword(),\n        elif last_token.value.lower() == 'fetch':\n            return Keyword(),\n        elif last_token.value.lower() == 'next':\n            return Keyword(),\n        elif last_token.value.lower() == 'row':\n            return Keyword(),\n        elif last_token.value.lower() == 'rows':\n            return Keyword(),\n        elif last_token.value.lower() == 'percent':\n            return Keyword(),\n        elif last_token.value.lower() == 'with':\n            return Keyword(),\n        elif last_token.value.lower() == 'ties':\n            return Keyword(),\n        elif last_token.value.lower() == 'current':\n            return Keyword(),\n        elif last_token.value.lower() == 'of':\n            return Keyword(),\n        elif last_token.value.lower() == 'now':\n            return Keyword(),\n        elif last_token.value.lower() == 'only':\n            return Keyword(),\n        elif last_token.value.lower() == 'row':\n            return Keyword(),\n        elif last_token.value.lower() == 'rows':\n            return Keyword(),\n        elif last_token.value.lower() == 'fetch':\n            return Keyword(),\n        elif last_token.value.lower() == 'next':\n            return Keyword(),\n        elif last_token.value.lower() == 'row':\n            return Keyword(),\n        elif last_token.value.lower() == 'rows':\n            return Keyword(),\n        elif last_token.value.lower() == 'percent':\n            return Keyword(),\n        elif last_token.value.lower() == 'with':\n            return Keyword(),\n        elif last_token.value.lower() == 'ties':\n            return Keyword(),\n        elif last_token.value.lower() == 'current':\n            return Keyword(),\n        elif last_token.value.lower() == 'of':\n            return Keyword(),\n        elif last_token.value.lower() == 'now':\n            return Keyword(),\n        elif last_token.value.lower() == 'only':\n            return Keyword(),\n        elif last_token.value.lower() == 'row':\n            return Keyword(),\n        elif last_token.value.lower() == 'rows':\n            return Keyword(),\n        elif last_token.value.lower() == 'fetch':\n            return Keyword(),\n        elif last_token.value.lower() == 'next':\n            return Keyword(),\n        elif last_token.value.lower() == 'row':\n            return Keyword(),\n        elif last_token.value.lower() == 'rows':\n            return Keyword(),\n        elif last_token.value.lower() == 'percent':\n            return Keyword(),\n        elif last_token.value.lower() == 'with':\n            return Keyword(),\n        elif last_token.value.lower() == 'ties':\n            return Keyword(),\n        elif last_token.value.lower() == 'current':\n            return Keyword(),\n        elif last_token.value.lower() == 'of':\n            return Keyword(),\n        elif last_token.value.lower() == 'now':\n            return Keyword(),\n        elif last_token.value.lower() == 'only':\n            return Keyword(),\n        elif last_token.value.lower() == 'row':\n            return Keyword(),\n        elif last_token.value.lower() == 'rows':\n            return Keyword(),\n        elif last_token.value.lower() == 'fetch':\n            return Keyword(),\n        elif last_token.value.lower() == 'next':\n            return Keyword(),\n        elif last_token.value.lower() == 'row':\n            return Keyword(),\n        elif last_token.value.lower() == 'rows':\n            return Keyword(),\n        elif last_token.value.lower() == 'percent':\n            return Keyword(),\n        elif last_token.value.lower() == 'with':\n            return Keyword(),\n        elif last_token.value.lower() == 'ties':\n            return Keyword(),\n        elif last_token.value.lower() == 'current':\n            return Keyword(),\n        elif last_token.value.lower() == 'of':\n            return Keyword(),\n        elif last_token.value.lower() == 'now':\n            return Keyword(),\n        elif last_token.value.lower() == 'only':\n            return Keyword(),\n        elif last_token.value.lower() == 'row':\n            return Keyword(),\n        elif last_token.value.lower() == 'rows':\n            return Keyword(),\n        elif last_token.value.lower() == 'fetch':\n            return Keyword(),\n        elif last_token.value.lower() == 'next':\n            return Keyword(),\n        elif last_token.value.lower() == 'row':\n            return Keyword(),\n        elif last_token.value.lower() == 'rows':\n            return Keyword(),\n        elif last_token.value.lower() == 'percent':\n            return Keyword(),\n        elif last_token.value.lower() == 'with':\n            return Keyword(),\n        elif last_token.value.lower() == 'ties':\n            return Keyword(),\n        elif last_token.value.lower() == 'current':\n            return Keyword(),\n        elif last_token.value.lower() == 'of':\n            return Keyword(),\n        elif last_token.value.lower() == 'now':\n            return Keyword(),\n        elif last_token.value.lower() == 'only':\n            return Keyword(),\n        elif last_token.value.lower() == 'row':\n            return Keyword(),\n        elif last_token.value.lower() == 'rows':\n            return Keyword(),\n        elif last_token.value.lower() == 'fetch':\n            return Keyword(),\n        elif last_token.value.lower() == 'next':\n            return Keyword(),\n        elif last_token.value.lower() == 'row':\n            return Keyword(),\n        elif last_token.value.lower() == 'rows':\n            return Keyword(),\n        elif last_token.value.lower() == 'percent':\n            return Keyword(),\n        elif last_token.value.lower() == 'with':\n            return Keyword(),\n        elif last_token.value.lower() == 'ties':\n            return Keyword(),\n        elif last_token.value.lower() == 'current':\n            return Keyword(),\n        elif last_token.value.lower() == 'of':\n            return Keyword(),\n        elif last_token.value.lower() == 'now':\n            return Keyword(),\n        elif last_token.value.lower() == 'only':\n            return Keyword(),\n        elif last_token.value.lower() == 'row':\n            return Keyword(),\n        elif last_token.value.lower() == 'rows':\n            return Keyword(),\n        elif last_token.value.lower() == 'fetch':\n            return Keyword(),\n        elif last_token.value.lower() == 'next':\n            return Keyword(),\n        elif last_token.value.lower() == 'row':\n            return Keyword(),\n        elif last_token.value.lower() == 'rows':\n            return Keyword(),\n        elif last_token.value.lower() == 'percent':\n            return Keyword(),\n        elif last_token.value.lower() == 'with':\n            return Keyword(),\n        elif last_token.value.lower() == 'ties':\n            return Keyword(),\n        elif last_token.value.lower() == 'current':\n            return Keyword(),\n        elif last_token.value.lower() == 'of':\n            return Keyword(),\n        elif last_token.value.lower() == 'now':\n            return Keyword(),\n        elif last_token.value.lower() == 'only':\n            return Keyword(),\n        elif last_token.value.lower() == 'row':\n            return Keyword(),\n        elif last_token.value.lower() == 'rows':\n            return Keyword(),\n        elif last_token.value.lower() == 'fetch':\n            return Keyword(),\n        elif last_token.value.lower() == 'next':\n            return Keyword(),\n        elif last_token.value.lower() == 'row':\n            return Keyword(),\n        elif last_token.value.lower() == 'rows':\n            return Keyword(),\n        elif last_token.value.lower() == 'percent':\n            return Keyword(),\n        elif last_token.value.lower() == 'with':\n            return Keyword(),\n        elif last_token.value.lower() == 'ties':\n            return Keyword(),\n        elif last_token.value.lower() == 'current':\n            return Keyword(),\n        elif last_token.value.lower() == 'of':\n            return Keyword(),\n        elif last_token.value.lower() == 'now':\n            return Keyword(),\n        elif last_token.value.lower() == 'only':\n            return Keyword(),\n        elif last_token.value.lower() == 'row':\n            return Keyword(),\n        elif last_token.value.lower() == 'rows':\n            return Keyword(),\n        elif last_token.value.lower() == 'fetch':\n            return Keyword(),\n        elif last_token.value.lower() == 'next':\n            return Keyword(),\n        elif last_token.value.lower() == 'row':\n            return Keyword(),\n        elif last_token.value.lower() == 'rows':\n            return Keyword(),\n        elif last_token.value.lower() == 'percent':\n            return Keyword(),\n        elif last_token.value.lower() == 'with':\n            return Keyword(),\n        elif last_token.value.lower() == 'ties':\n            return Keyword(),\n        elif last_token.value.lower() == 'current':\n            return Keyword(),\n        elif last_token.value.lower() == 'of':\n            return Keyword(),\n        elif last_token.value.lower() == 'now':\n            return Keyword(),\n        elif last_token.value.lower() == 'only':\n            return Keyword(),\n        elif last_token.value.lower() == 'row':\n            return Keyword(),\n        elif last_token.value.lower() == 'rows':\n            return Keyword(),\n        elif last_token.value.lower() == 'fetch':\n            return Keyword(),\n        elif last_token.value.lower() == 'next':\n            return Keyword(),\n        elif last_token.value.lower() == 'row':\n            return Keyword(),\n        elif last_token.value.lower() == 'rows':\n            return Keyword(),\n        elif last_token.value.lower() == 'percent':\n            return Keyword(),\n        elif last_token.value.lower() == 'with':\n            return Keyword(),\n        elif last_token.value.lower() == 'ties':\n            return Keyword(),\n        elif last_token.value.lower() == 'current':\n            return Keyword(),\n        elif last_token.value.lower() == 'of':\n            return Keyword(),\n        elif last_token.value.lower() == 'now':\n            return Keyword(),\n        elif last_token.value.lower() == 'only':\n            return Keyword(),\n        elif last_token.value.lower() == 'row':\n            return Keyword(),\n        elif last_token.value.lower() == 'rows':\n            return Keyword(),\n        elif last_token.value.lower() == 'fetch':\n            return Keyword(),\n        elif last_token.value.lower() == 'next':\n            return Keyword(),\n        elif last_token.value.lower() == 'row':\n            return Keyword(),\n        elif last_token.value.lower() == 'rows':\n            return Keyword(),\n        elif last_token.value.lower() == 'percent':\n            return Keyword(),\n        elif last_token.value.lower() == 'with':\n            return Keyword(),\n        elif last_token.value.lower() == 'ties':\n            return Keyword(),\n        elif last_token.value.lower() == 'current':\n            return Keyword(),\n        elif last_token.value.lower() == 'of':\n            return Keyword(),\n        elif last_token.value.lower() == 'now':\n            return Keyword(),\n        elif last_token.value.lower() == 'only':\n            return Keyword(),\n        elif last_token.value.lower() == 'row':\n            return Keyword(),\n        elif last_token.value.lower() == 'rows':\n            return Keyword(),\n        elif last_token.value.lower() == 'fetch':\n            return Keyword(),\n        elif last_token.value.lower() == 'next':\n            return Keyword(),\n        elif last_token.value.lower() == 'row':\n            return Keyword(),\n        elif last_token.value.lower() == 'rows':\n            return Keyword(),\n        elif last_token.value.lower() == 'percent':\n            return Keyword(),\n        elif last_token.value.lower() == 'with':\n            return Keyword(),\n        elif last_token.value.lower() == 'ties':\n            return Keyword(),\n        elif last_token.value.lower() == 'current':\n            return Keyword(),\n        elif last_token.value.lower() == 'of':\n            return Keyword(),\n        elif last_token.value.lower() == 'now':\n            return Keyword(),\n        elif last_token.value.lower() == 'only':\n            return Keyword(),\n        elif last_token.value.lower() == 'row':\n            return Keyword(),\n        elif last_token.value.lower() == 'rows':\n            return Keyword(),\n        elif last_token.value.lower() == 'fetch':\n            return Keyword(),\n        elif last_token.value.lower() == 'next':\n            return Keyword(),\n        elif last_token.value.lower() == 'row':\n            return Keyword", "idx": 382}
{"namespace": "mssqlcli.packages.parseutils.ctes.extract_ctes", "completion": "    parsed = parse(sql)\n    ctes = []\n    for statement in parsed:\n        if statement.get_type() == 'SELECT':\n            for token in statement.tokens:\n                if token.ttype is Keyword and token.value.upper() == 'WITH':\n                    cte_start = token._get_first_name().ttype\n                    cte_end = token._get_last_name().ttype\n                    cte_name = token._get_first_name().value\n                    cte_columns = []\n                    for subtoken in token.tokens:\n                        if subtoken.ttype is CTE:\n                            cte_start = subtoken._get_first_name().ttype\n                            cte_end = subtoken._get_last_name().ttype\n                            cte_name = subtoken._get_first_name().value\n                        elif subtoken.ttype is DML and subtoken.value.upper() == 'SELECT':\n                            for subsubtoken in subtoken.tokens:\n                                if isinstance(subsubtoken, IdentifierList):\n                                    cte_columns = [x.value for x in subsubtoken.get_identifiers()]\n                    ctes.append(TableExpression(cte_name, cte_columns, cte_start, cte_end))\n    return ctes, sql[cte_end:]", "idx": 383}
{"namespace": "mssqlcli.packages.parseutils.tables.extract_tables", "completion": "\n    parsed = sqlparse.parse(sql)[0]\n    stream = extract_from_part(parsed)\n    tables = list(extract_table_identifiers(stream))\n    return tuple(tables)", "idx": 384}
{"namespace": "googleapiclient.channel.Channel.body", "completion": "        body_dict = {\n            \"id\": self.id,\n            \"token\": self.token,\n            \"type\": self.type,\n            \"address\": self.address\n        }\n        if self.expiration:\n            body_dict[\"expiration\"] = self.expiration\n        if self.params:\n            body_dict[\"params\"] = self.params\n        if self.resource_id:\n            body_dict[\"resourceId\"] = self.resource_id\n        if self.resource_uri:\n            body_dict[\"resourceUri\"] = self.resource_uri\n\n        return body_dict", "idx": 385}
{"namespace": "sqlitedict.SqliteDict.update", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to update a read-only SqliteDict')\n\n        for key, value in items:\n            self[key] = value\n\n        for key, value in kwds.items():\n            self[key] = value\n\n        if self.autocommit:\n            self.commit()", "idx": 386}
{"namespace": "sqlitedict.SqliteDict.clear", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to clear a read-only SqliteDict')\n\n        CLEAR_TABLE = 'DELETE FROM \"%s\"' % self.tablename\n        self.conn.execute(CLEAR_TABLE)\n        if self.autocommit:\n            self.commit()", "idx": 387}
{"namespace": "sqlitedict.SqliteDict.commit", "completion": "        if blocking:\n            self.conn.commit()\n        else:\n            self.conn.execute(_REQUEST_COMMIT)", "idx": 388}
{"namespace": "sqlitedict.SqliteDict.terminate", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to delete database file from read-only SqliteDict')\n        \n        self.close(do_log=False, force=True)\n        if self.filename != \":memory:\":\n            try:\n                os.remove(self.filename)\n            except Exception as e:\n                logger.error(\"Error deleting database file: %s\" % str(e))", "idx": 389}
{"namespace": "boto.utils.retry_url", "completion": "        import urllib.error\n        import urllib.request\n    import urllib.request\n    import urllib.error\n\n    for _ in range(num_retries):\n        try:\n            response = urllib.request.urlopen(url, timeout=timeout)\n            return response.read().decode('utf-8')\n        except urllib.error.HTTPError as e:\n            if e.code == 404 and not retry_on_404:\n                raise\n        except urllib.error.URLError as e:\n            pass\n    raise Exception(\"Failed to retrieve URL after {} retries\".format(num_retries))", "idx": 390}
{"namespace": "boto.utils.LazyLoadMetadata.values", "completion": "        self._materialize()\n        return super(LazyLoadMetadata, self).values()", "idx": 391}
{"namespace": "boto.utils.get_instance_userdata", "completion": "    try:\n        userdata_url = _build_instance_metadata_url(url, version, 'user-data')\n        userdata = retry_url(userdata_url, num_retries=num_retries, timeout=timeout)\n        if sep:\n            userdata = dict(item.split(sep) for item in userdata.split('\\n'))\n        return userdata\n    except urllib.error.URLError:\n        boto.log.exception(\"Exception caught when trying to retrieve instance user data\")\n        return None", "idx": 392}
{"namespace": "boto.utils.pythonize_name", "completion": "    s1 = _first_cap_regex.sub(r'\\1_\\2', name)\n    s2 = _number_cap_regex.sub(r'\\1_\\2', s1)\n    return _end_cap_regex.sub(r'\\1_\\2', s2).lower()", "idx": 393}
{"namespace": "boto.cloudsearchdomain.connect_to_region", "completion": "        from boto.cloudsearchdomain.layer1 import CloudSearchDomainConnection\n    from boto.cloudsearchdomain.layer1 import CloudSearchDomainConnection\n    region = RegionInfo(name=region_name, endpoint='cloudsearchdomain.' + region_name + '.amazonaws.com')\n    return CloudSearchDomainConnection(region=region, **kw_params)", "idx": 394}
{"namespace": "boto.redshift.connect_to_region", "completion": "        from boto.redshift.layer1 import RedshiftConnection\n    from boto.redshift.layer1 import RedshiftConnection\n    region = RegionInfo(name=region_name, endpoint='redshift.' + region_name + '.amazonaws.com')\n    return RedshiftConnection(region=region, **kw_params)", "idx": 395}
{"namespace": "boto.support.connect_to_region", "completion": "        from boto.support.layer1 import SupportConnection\n    from boto.support.layer1 import SupportConnection\n    return SupportConnection(region=region_name, **kw_params)", "idx": 396}
{"namespace": "boto.configservice.connect_to_region", "completion": "    from boto.configservice.layer1 import ConfigServiceConnection\n    return ConfigServiceConnection(region=RegionInfo(name=region_name, endpoint='config.amazonaws.com'), **kw_params)", "idx": 397}
{"namespace": "boto.cloudhsm.connect_to_region", "completion": "    from boto.cloudhsm.layer1 import CloudHSMConnection\n    region = RegionInfo(name=region_name, endpoint='cloudhsm.amazonaws.com')\n    return CloudHSMConnection(region=region, **kw_params)", "idx": 398}
{"namespace": "boto.logs.connect_to_region", "completion": "    from boto.logs.layer1 import CloudWatchLogsConnection\n    from boto.regioninfo import get_regions\n    region = [r for r in get_regions('logs', connection_cls=CloudWatchLogsConnection) if r.name == region_name][0]\n    return region.connect(**kw_params)", "idx": 399}
{"namespace": "boto.cloudsearch.connect_to_region", "completion": "    from boto.cloudsearch.layer1 import Layer1\n    regions = get_regions('cloudsearch', connection_cls=Layer1)\n    for reg in regions:\n        if reg.name == region_name:\n            return reg.connect(**kw_params)\n    raise ValueError(\"Invalid region name: %s\" % region_name)", "idx": 400}
{"namespace": "boto.glacier.job.Job.download_to_fileobj", "completion": "        num_chunks = self._calc_num_chunks(chunk_size)\n        self._download_to_fileob(output_file, num_chunks, chunk_size,\n                                 verify_hashes, retry_exceptions)", "idx": 401}
{"namespace": "boto.glacier.utils.minimum_part_size", "completion": "    if size_in_bytes <= default_part_size:\n        return default_part_size\n    else:\n        num_parts = math.ceil(size_in_bytes / default_part_size)\n        min_part_size = math.ceil(size_in_bytes / MAXIMUM_NUMBER_OF_PARTS)\n        return min_part_size", "idx": 402}
{"namespace": "boto.glacier.utils.chunk_hashes", "completion": "    if not bytestring:\n        return [hashlib.sha256(b'').digest()]\n\n    chunked_data = [bytestring[i:i + chunk_size] for i in range(0, len(bytestring), chunk_size)]\n    return [hashlib.sha256(chunk).digest() for chunk in chunked_data]", "idx": 403}
{"namespace": "boto.glacier.utils.compute_hashes_from_fileobj", "completion": "    linear_hash = hashlib.sha256()\n    tree_hashes = []\n\n    while True:\n        chunk = fileobj.read(chunk_size)\n        if not chunk:\n            break\n        linear_hash.update(chunk)\n        tree_hashes.extend(chunk_hashes(chunk, chunk_size))\n\n    linear_hash_hex = linear_hash.hexdigest()\n    tree_hash_hex = binascii.hexlify(tree_hash(tree_hashes)).decode('utf-8')\n\n    return linear_hash_hex, tree_hash_hex", "idx": 404}
{"namespace": "boto.glacier.concurrent.ConcurrentTransferer._calculate_required_part_size", "completion": "        min_part_size = 1 << 20  # 1MB\n        num_parts = int(math.ceil(total_size / float(self._part_size)))\n        final_part_size = int(math.ceil(total_size / float(num_parts)))\n\n        if final_part_size < min_part_size:\n            final_part_size = min_part_size\n            num_parts = int(math.ceil(total_size / float(final_part_size)))\n\n        return num_parts, final_part_size", "idx": 405}
{"namespace": "boto.glacier.connect_to_region", "completion": "    from boto.glacier.layer2 import Layer2\n    region = RegionInfo(name=region_name, endpoint='glacier.%s.amazonaws.com' % region_name)\n    return Layer2(region=region, **kw_params)", "idx": 406}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.update", "completion": "        if self.id is None:\n            raise BotoClientError('Attempting to update NetworkInterface but id is None')\n        new_obj = self.connection.get_all_network_interfaces([self.id])\n        if len(new_obj) > 0:\n            self._update(new_obj[0])\n            return self.status\n        elif validate:\n            raise ValueError('No data returned when updating NetworkInterface')\n        else:\n            return self.status", "idx": 407}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.attach", "completion": "        if self.id is None:\n            raise BotoClientError('An ENI must have an id before it can be attached')\n        return self.connection.attach_network_interface(\n            self.id, instance_id, device_index, dry_run=dry_run)", "idx": 408}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.detach", "completion": "        return self.connection.detach_network_interface(\n            self.id,\n            force,\n            dry_run\n        )", "idx": 409}
{"namespace": "boto.ec2.address.Address.release", "completion": "        if self.allocation_id:\n            return self.connection.release_address(allocation_id=self.allocation_id, dry_run=dry_run)\n        else:\n            return self.connection.release_address(public_ip=self.public_ip, dry_run=dry_run)", "idx": 410}
{"namespace": "boto.ec2.address.Address.associate", "completion": "        if self.allocation_id:\n            return self.connection.associate_address(\n                allocation_id=self.allocation_id,\n                instance_id=instance_id,\n                network_interface_id=network_interface_id,\n                private_ip_address=private_ip_address,\n                allow_reassociation=allow_reassociation,\n                dry_run=dry_run\n            )\n        else:\n            return self.connection.associate_address(\n                public_ip=self.public_ip,\n                instance_id=instance_id,\n                network_interface_id=network_interface_id,\n                private_ip_address=private_ip_address,\n                allow_reassociation=allow_reassociation,\n                dry_run=dry_run\n            )", "idx": 411}
{"namespace": "boto.ec2.address.Address.disassociate", "completion": "        if self.association_id:\n            return self.connection.disassociate_address(\n                association_id=self.association_id,\n                dry_run=dry_run\n            )\n        else:\n            return self.connection.disassociate_address(\n                public_ip=self.public_ip,\n                dry_run=dry_run\n            )", "idx": 412}
{"namespace": "boto.ec2.ec2object.TaggedEC2Object.add_tags", "completion": "        for key, value in tags.items():\n            self.add_tag(key, value, dry_run)", "idx": 413}
{"namespace": "boto.ec2.ec2object.TaggedEC2Object.remove_tags", "completion": "        status = self.connection.delete_tags(\n            [self.id],\n            list(tags.keys()),\n            dry_run=dry_run\n        )\n        for key in tags.keys():\n            if key in self.tags:\n                del self.tags[key]", "idx": 414}
{"namespace": "boto.ec2.connection.EC2Connection.get_all_instance_status", "completion": "        params = {}\n        if instance_ids:\n            self.build_list_params(params, instance_ids, 'InstanceId')\n        if filters:\n            self.build_filter_params(params, filters)\n        if dry_run:\n            params['DryRun'] = 'true'\n        if max_results is not None:\n            params['MaxResults'] = max_results\n        if next_token:\n            params['NextToken'] = next_token\n        if include_all_instances:\n            params['IncludeAllInstances'] = 'true'\n        return self.get_list('DescribeInstanceStatus', params,\n                             [('item', InstanceStatus)], verb='POST')", "idx": 415}
{"namespace": "boto.ec2.volume.Volume.update", "completion": "        if self.id:\n            if validate:\n                self.connection.get_all_volumes([self.id])\n            updated = self.connection.get_all_volumes([self.id])[0]\n            self._update(updated)\n            return self.status\n        else:\n            if validate:\n                raise ValueError('Volume does not exist')", "idx": 416}
{"namespace": "boto.ec2.volume.Volume.attach", "completion": "        return self.connection.attach_volume(self.id, instance_id, device, dry_run=dry_run)", "idx": 417}
{"namespace": "boto.ec2.volume.Volume.detach", "completion": "        return self.connection.detach_volume(\n            self.id,\n            instance_id,\n            device,\n            force=force,\n            dry_run=dry_run\n        )", "idx": 418}
{"namespace": "boto.ec2.volume.Volume.create_snapshot", "completion": "        return self.connection.create_snapshot(self.id, description, dry_run=dry_run)", "idx": 419}
{"namespace": "boto.ec2.volume.Volume.attachment_state", "completion": "        if self.attach_data:\n            return self.attach_data.status\n        else:\n            return None", "idx": 420}
{"namespace": "boto.ec2.securitygroup.SecurityGroup.add_rule", "completion": "        perm = IPPermissions(self)\n        perm.ip_protocol = ip_protocol\n        perm.from_port = from_port\n        perm.to_port = to_port\n        if src_group_name:\n            grant = GroupOrCIDR(src_group_name, src_group_owner_id, src_group_group_id)\n            perm.groups.append(grant)\n        if cidr_ip:\n            grant = GroupOrCIDR(cidr_ip)\n            perm.grants.append(grant)\n        self.rules.append(perm)", "idx": 421}
{"namespace": "boto.ec2.connect_to_region", "completion": "    region = RegionData.get(region_name)\n    if region:\n        return region.connect(**kw_params)\n    else:\n        return None", "idx": 422}
{"namespace": "boto.ec2.cloudwatch.connect_to_region", "completion": "    if region_name in RegionData:\n        region = RegionInfo(name=region_name, endpoint=RegionData[region_name])\n        return CloudWatchConnection(region=region, **kw_params)\n    else:\n        return None", "idx": 423}
{"namespace": "boto.ec2.autoscale.connect_to_region", "completion": "    region_info = RegionData.get(region_name)\n    if region_info:\n        return AutoScaleConnection(region=region_info, **kw_params)\n    else:\n        return None", "idx": 424}
{"namespace": "boto.ec2.elb.connect_to_region", "completion": "    region = RegionData.get(region_name)\n    if region:\n        return ELBConnection(region=region, **kw_params)\n    else:\n        return None", "idx": 425}
{"namespace": "boto.ec2.elb.ELBConnection.get_all_load_balancers", "completion": "        params = {}\n        if load_balancer_names:\n            self.build_list_params(params, load_balancer_names, 'LoadBalancerNames.member.%d')\n        if marker:\n            params['Marker'] = marker\n        return self.get_list('DescribeLoadBalancers', params, [('member', LoadBalancer)])", "idx": 426}
{"namespace": "boto.ec2.elb.ELBConnection.disable_availability_zones", "completion": "        params = {'LoadBalancerName': load_balancer_name}\n        self.build_list_params(params, zones_to_remove,\n                               'AvailabilityZones.member.%d')\n        obj = self.get_object('DisableAvailabilityZonesForLoadBalancer',\n                              params, LoadBalancerZones)\n        return obj.zones", "idx": 427}
{"namespace": "boto.awslambda.connect_to_region", "completion": "        from boto.awslambda.layer1 import AWSLambdaConnection\n    from boto.awslambda.layer1 import AWSLambdaConnection\n    region = RegionInfo(name=region_name, endpoint='lambda.' + region_name + '.amazonaws.com')\n    return AWSLambdaConnection(region=region, **kw_params)", "idx": 428}
{"namespace": "boto.cognito.identity.connect_to_region", "completion": "        from boto.cognito.identity.layer1 import CognitoIdentityConnection\n    from boto.cognito.identity.layer1 import CognitoIdentityConnection\n    region = RegionInfo(name=region_name, endpoint='cognito-identity.amazonaws.com')\n    return CognitoIdentityConnection(region=region, **kw_params)", "idx": 429}
{"namespace": "boto.cognito.sync.connect_to_region", "completion": "        from boto.cognito.sync.layer1 import CognitoSyncConnection\n    from boto.cognito.sync.layer1 import CognitoSyncConnection\n    return CognitoSyncConnection(region=RegionInfo(name=region_name, endpoint='cognito-sync.amazonaws.com'), **kw_params)", "idx": 430}
{"namespace": "boto.cloudformation.connect_to_region", "completion": "    region = RegionInfo(name=region_name, endpoint=RegionData.get(region_name))\n    if region is not None:\n        return CloudFormationConnection(region=region, **kw_params)\n    else:\n        return None", "idx": 431}
{"namespace": "boto.route53.zone.Zone.find_records", "completion": "        records = self.route53connection.get_all_rrsets(self.id, name, type)\n        matching_records = []\n        for record in records:\n            if record.name == name and record.type == type:\n                if identifier:\n                    if (record.identifier == identifier[0] and\n                            record.weight == identifier[1]):\n                        matching_records.append(record)\n                else:\n                    matching_records.append(record)\n\n        if len(matching_records) == 0:\n            return None\n        elif len(matching_records) == 1:\n            return matching_records[0]\n        else:\n            if not all:\n                if len(matching_records) > desired:\n                    raise TooManyRecordsException\n                else:\n                    return matching_records[0]\n            else:\n                return matching_records", "idx": 432}
{"namespace": "boto.route53.domains.connect_to_region", "completion": "        from boto.route53.domains.layer1 import Route53DomainsConnection\n    from boto.route53.domains.layer1 import Route53DomainsConnection\n    region = RegionInfo(name=region_name, endpoint='route53domains.amazonaws.com')\n    return Route53DomainsConnection(region=region, **kw_params)", "idx": 433}
{"namespace": "boto.s3.key.Key.get_contents_to_filename", "completion": "        if self.bucket is not None:\n            if res_download_handler:\n                res_download_handler.get_file(self, filename, headers, cb, num_cb,\n                                             torrent=torrent,\n                                             version_id=version_id)\n            else:\n                with open(filename, 'wb') as fp:\n                    self.get_file(fp, headers, cb, num_cb, torrent=torrent,\n                                  version_id=version_id,\n                                  response_headers=response_headers)", "idx": 434}
{"namespace": "boto.s3.cors.CORSConfiguration.add_rule", "completion": "        rule = CORSRule(allowed_method, allowed_origin, id, allowed_header, max_age_seconds, expose_header)\n        self.append(rule)", "idx": 435}
{"namespace": "boto.s3.bucket.Bucket.get_key", "completion": "        if validate:\n            return self.connection.head_key(self.name, key_name, headers, version_id, response_headers)\n        else:\n            return self.key_class(bucket=self, name=key_name)", "idx": 436}
{"namespace": "boto.s3.bucket.Bucket.new_key", "completion": "        if key_name is None:\n            raise ValueError(\"Key name cannot be None\")\n        return self.key_class(self, key_name)", "idx": 437}
{"namespace": "boto.s3.bucket.Bucket.delete_key", "completion": "        if version_id:\n            key_name += '?versionId=' + version_id\n        response = self.connection.make_request('DELETE', self.name, key_name,\n                                               headers=headers)\n        body = response.read()\n        if response.status == 204:\n            k = self.key_class(self)\n            k.name = key_name\n            k.handle_version_headers(response)\n            return k\n        else:\n            raise self.connection.provider.storage_response_error(\n                response.status, response.reason, body)", "idx": 438}
{"namespace": "boto.s3.bucket.Bucket.get_tags", "completion": "        response = self.connection.make_request('GET', self.name,\n                                                query_args='tagging',\n                                                headers=headers)\n        body = response.read()\n        if response.status == 200:\n            tags = {}\n            h = handler.XmlHandler(tags, self)\n            if not isinstance(body, bytes):\n                body = body.encode('utf-8')\n            xml.sax.parseString(body, h)\n            return tags\n        else:\n            raise self.connection.provider.storage_response_error(\n                response.status, response.reason, body)", "idx": 439}
{"namespace": "boto.s3.connection.S3Connection._required_auth_capability", "completion": "        return ['hmac-v4-s3']", "idx": 440}
{"namespace": "boto.s3.connection.S3Connection.generate_url_sigv4", "completion": "        if iso_date is None:\n            iso_date = boto.utils.ISO8601\n        if headers is None:\n            headers = {}\n        if response_headers is None:\n            response_headers = {}\n        if version_id is not None:\n            key += '?versionId=' + version_id\n        if force_http:\n            protocol = 'http'\n        else:\n            protocol = 'https' if self.is_secure else 'http'\n        url_base = self.calling_format.build_url_base(self, protocol, self.server_name(), bucket, key)\n        if '?' in url_base:\n            url_base += '&'\n        else:\n            url_base += '?'\n        url_base += 'X-Amz-Expires=%d' % expires_in\n        url_base += '&X-Amz-Algorithm=AWS4-HMAC-SHA256'\n        url_base += '&X-Amz-Credential=%s' % self._auth_handler.scope\n        url_base += '&X-Amz-Date=%s' % iso_date\n        url_base += '&X-Amz-SignedHeaders=host'\n        url_base += '&X-Amz-Signature=%s' % self._auth_handler.sign_string(self._auth_handler.canonical_request(method, url_base, headers, '', 'host'))\n        return url_base", "idx": 441}
{"namespace": "boto.s3.lifecycle.Lifecycle.add_rule", "completion": "        rule = Rule(id, prefix, status, expiration, transition)\n        self.append(rule)", "idx": 442}
{"namespace": "boto.s3.website.WebsiteConfiguration.to_xml", "completion": "        xml = '<WebsiteConfiguration>'\n        if self.suffix:\n            xml += tag('IndexDocument', tag('Suffix', self.suffix))\n        if self.error_key:\n            xml += tag('ErrorDocument', tag('Key', self.error_key))\n        if self.redirect_all_requests_to:\n            xml += self.redirect_all_requests_to.to_xml()\n        if self.routing_rules:\n            xml += self.routing_rules.to_xml()\n        xml += '</WebsiteConfiguration>'\n        return xml", "idx": 443}
{"namespace": "boto.s3.website.RoutingRules.to_xml", "completion": "        parts = ['<RoutingRules>']\n        for rule in self:\n            parts.append(rule.to_xml())\n        parts.append('</RoutingRules>')\n        return ''.join(parts)", "idx": 444}
{"namespace": "boto.s3.website.RoutingRule.when", "completion": "        condition = Condition(key_prefix, http_error_code)\n        return cls(condition=condition)", "idx": 445}
{"namespace": "boto.s3.website.RoutingRule.then_redirect", "completion": "        self.redirect = Redirect(hostname=hostname, protocol=protocol,\n                                  replace_key=replace_key,\n                                  replace_key_prefix=replace_key_prefix,\n                                  http_redirect_code=http_redirect_code)\n        return self", "idx": 446}
{"namespace": "boto.s3.connect_to_region", "completion": "    custom_host = kw_params.get('host')\n    if custom_host:\n        custom_region = S3RegionInfo(name=region_name, endpoint=custom_host, connection_cls=S3Connection)\n        return custom_region.connect(**kw_params)\n    else:\n        for region in regions():\n            if region.name == region_name:\n                return region.connect(**kw_params)", "idx": 447}
{"namespace": "boto.directconnect.connect_to_region", "completion": "        from boto.directconnect.layer1 import DirectConnectConnection\n    from boto.directconnect.layer1 import DirectConnectConnection\n    region = RegionInfo(name=region_name, endpoint='directconnect.amazonaws.com')\n    return DirectConnectConnection(region=region, **kw_params)", "idx": 448}
{"namespace": "boto.rds.connect_to_region", "completion": "    regions = get_regions('rds', region_cls=RDSRegionInfo, connection_cls=RDSConnection)\n    for reg in regions:\n        if reg.name == region_name:\n            return reg.connect(**kw_params)\n    return None", "idx": 449}
{"namespace": "boto.datapipeline.connect_to_region", "completion": "    from boto.datapipeline.layer1 import DataPipelineConnection\n    return DataPipelineConnection(region=region_name, **kw_params)", "idx": 450}
{"namespace": "boto.dynamodb.batch.Batch.to_dict", "completion": "    data = {'table_name': self.table.name,\n            'keys': [key if isinstance(key, (list, tuple)) else [key] for key in self.keys]}\n    if self.attributes_to_get:\n        data['attributes_to_get'] = self.attributes_to_get\n    if self.consistent_read:\n        data['consistent_read'] = 'true'\n    return data", "idx": 451}
{"namespace": "boto.dynamodb.batch.BatchList.to_dict", "completion": "        batch_list_dict = []\n        for batch in self:\n            batch_list_dict.append(batch.to_dict())\n        return batch_list_dict", "idx": 452}
{"namespace": "boto.dynamodb.types.Dynamizer.encode", "completion": "        dynamodb_type = self._get_dynamodb_type(attr)\n        if dynamodb_type == 'N':\n            return self._encode_n(attr)\n        elif dynamodb_type == 'S':\n            return self._encode_s(attr)\n        elif dynamodb_type == 'NS':\n            return self._encode_ns(attr)\n        elif dynamodb_type == 'SS':\n            return self._encode_ss(attr)\n        elif dynamodb_type == 'B':\n            return self._encode_b(attr)\n        elif dynamodb_type == 'BS':\n            return self._encode_bs(attr)\n        elif dynamodb_type == 'BOOL':\n            return self._encode_bool(attr)\n        elif dynamodb_type == 'NULL':\n            return self._encode_null(attr)\n        elif dynamodb_type == 'M':\n            return self._encode_m(attr)\n        elif dynamodb_type == 'L':\n            return self._encode_l(attr)\n        else:\n            raise TypeError('Unsupported type \"%s\" for value \"%s\"' % (type(attr), attr))", "idx": 453}
{"namespace": "boto.dynamodb.types.Dynamizer.decode", "completion": "        if len(attr) == 1 and isinstance(attr, dict):\n            dynamodb_type, value = next(iter(attr.items()))\n            try:\n                decoder = getattr(self, '_decode_%s' % dynamodb_type.lower())\n            except AttributeError:\n                raise ValueError(\"Unable to decode dynamodb type: %s\" %\n                                 dynamodb_type)\n            return decoder({dynamodb_type: value})\n        else:\n            return attr", "idx": 454}
{"namespace": "boto.dynamodb.connect_to_region", "completion": "        from boto.dynamodb.layer2 import Layer2\n    from boto.dynamodb.layer2 import Layer2\n    region = RegionInfo(name=region_name, endpoint='dynamodb.' + region_name + '.amazonaws.com')\n    return Layer2(region=region, **kw_params)", "idx": 455}
{"namespace": "boto.beanstalk.connect_to_region", "completion": "    from boto.beanstalk.layer1 import Layer1\n    return Layer1(region_name, **kw_params)", "idx": 456}
{"namespace": "boto.swf.connect_to_region", "completion": "    region = RegionInfo(name=region_name, endpoint=REGION_ENDPOINTS.get(region_name))\n    return boto.swf.layer1.Layer1(region=region, **kw_params)", "idx": 457}
{"namespace": "boto.opsworks.regions", "completion": "    return [\n        RegionInfo(name='us-east-1', endpoint='opsworks.us-east-1.amazonaws.com'),\n        RegionInfo(name='us-west-1', endpoint='opsworks.us-west-1.amazonaws.com'),\n        RegionInfo(name='us-west-2', endpoint='opsworks.us-west-2.amazonaws.com'),\n        RegionInfo(name='eu-west-1', endpoint='opsworks.eu-west-1.amazonaws.com'),\n        RegionInfo(name='eu-central-1', endpoint='opsworks.eu-central-1.amazonaws.com'),\n        RegionInfo(name='ap-northeast-1', endpoint='opsworks.ap-northeast-1.amazonaws.com'),\n        RegionInfo(name='ap-southeast-1', endpoint='opsworks.ap-southeast-1.amazonaws.com'),\n        RegionInfo(name='ap-southeast-2', endpoint='opsworks.ap-southeast-2.amazonaws.com'),\n        RegionInfo(name='sa-east-1', endpoint='opsworks.sa-east-1.amazonaws.com')\n    ]", "idx": 458}
{"namespace": "boto.opsworks.connect_to_region", "completion": "    region = RegionInfo(name=region_name, endpoint='opsworks.us-east-1.amazonaws.com')\n    from boto.opsworks.layer1 import OpsWorksConnection\n    return OpsWorksConnection(region=region, **kw_params)", "idx": 459}
{"namespace": "boto.sqs.connect_to_region", "completion": "        from boto.sqs.connection import SQSConnection\n    from boto.sqs.connection import SQSConnection\n    region = SQSRegionInfo(name=region_name, endpoint='sqs.' + region_name + '.amazonaws.com')\n    return SQSConnection(region=region, **kw_params)", "idx": 460}
{"namespace": "boto.rds2.connect_to_region", "completion": "    region = [r for r in regions() if r.name == region_name]\n    if region:\n        return region[0].connect(**kw_params)\n    else:\n        return None", "idx": 461}
{"namespace": "boto.cloudsearch2.connect_to_region", "completion": "        from boto.cloudsearch2.layer1 import CloudSearchConnection\n    from boto.cloudsearch2.layer1 import CloudSearchConnection\n    return CloudSearchConnection(region=region_name, **kw_params)", "idx": 462}
{"namespace": "boto.cloudtrail.connect_to_region", "completion": "    from boto.cloudtrail.layer1 import CloudTrailConnection\n    return CloudTrailConnection(region=region_name, **kw_params)", "idx": 463}
{"namespace": "boto.elasticache.connect_to_region", "completion": "        from boto.elasticache.layer1 import ElastiCacheConnection\n    from boto.elasticache.layer1 import ElastiCacheConnection\n    region = RegionInfo(name=region_name, endpoint='elasticache.amazonaws.com')\n    return ElastiCacheConnection(region=region, **kw_params)", "idx": 464}
{"namespace": "boto.ses.connect_to_region", "completion": "    region = RegionInfo(name=region_name, endpoint='email.' + region_name + '.amazonaws.com')\n    if region in regions():\n        return SESConnection(region=region, **kw_params)\n    else:\n        return None", "idx": 465}
{"namespace": "boto.codedeploy.connect_to_region", "completion": "        from boto.codedeploy.layer1 import CodeDeployConnection\n    from boto.codedeploy.layer1 import CodeDeployConnection\n    return CodeDeployConnection(region=region_name, **kw_params)", "idx": 466}
{"namespace": "boto.sts.credentials.Credentials.to_dict", "completion": "        return {\n            'access_key': self.access_key,\n            'secret_key': self.secret_key,\n            'session_token': self.session_token,\n            'expiration': self.expiration,\n            'request_id': self.request_id\n        }", "idx": 467}
{"namespace": "boto.sts.connect_to_region", "completion": "    region = RegionInfo(name=region_name, endpoint='sts.amazonaws.com')\n    if region.name in [r.name for r in regions()]:\n        return STSConnection(region=region, **kw_params)\n    else:\n        return None", "idx": 468}
{"namespace": "boto.machinelearning.connect_to_region", "completion": "        from boto.machinelearning.layer1 import MachineLearningConnection\n    from boto.machinelearning.layer1 import MachineLearningConnection\n    region = RegionInfo(name=region_name, endpoint='machinelearning.' + region_name + '.amazonaws.com')\n    return MachineLearningConnection(region=region, **kw_params)", "idx": 469}
{"namespace": "boto.vpc.connect_to_region", "completion": "    regions = get_regions('ec2', connection_cls=EC2Connection)\n    for reg in regions:\n        if reg.name == region_name:\n            return EC2Connection(region=reg, **kw_params)\n    return None", "idx": 470}
{"namespace": "boto.vpc.VPCConnection.get_all_vpc_peering_connections", "completion": "        params = {}\n        if vpc_peering_connection_ids:\n            self.build_list_params(params, vpc_peering_connection_ids, 'VpcPeeringConnectionId')\n        if filters:\n            self.build_filter_params(params, filters)\n        if dry_run:\n            params['DryRun'] = 'true'\n        return self.get_list('DescribeVpcPeeringConnections', params, [('item', VpcPeeringConnection)])", "idx": 471}
{"namespace": "boto.kinesis.connect_to_region", "completion": "        from boto.kinesis.layer1 import KinesisConnection\n    from boto.kinesis.layer1 import KinesisConnection\n    region = RegionInfo(name=region_name, endpoint='kinesis.' + region_name + '.amazonaws.com')\n    return KinesisConnection(region=region, **kw_params)", "idx": 472}
{"namespace": "boto.ec2containerservice.connect_to_region", "completion": "    from boto.ec2containerservice.layer1 import EC2ContainerServiceConnection\n    return EC2ContainerServiceConnection(region=region_name, **kw_params)", "idx": 473}
{"namespace": "boto.dynamodb2.table.Table._introspect_indexes", "completion": "        map_indexes_projection = self._PROJECTION_TYPE_TO_INDEX['local_indexes']\n        indexes = self._introspect_all_indexes(raw_indexes, map_indexes_projection)\n        return indexes", "idx": 474}
{"namespace": "boto.dynamodb2.table.Table.describe", "completion": "        response = self.connection.describe_table(self.table_name)\n        self.schema = self._introspect_schema(response['Table']['KeySchema'], response['Table']['AttributeDefinitions'])\n        self.indexes = self._introspect_indexes(response['Table'].get('LocalSecondaryIndexes', []))\n        self.global_indexes = self._introspect_global_indexes(response['Table'].get('GlobalSecondaryIndexes', []))\n        self.throughput = {\n            'read': response['Table']['ProvisionedThroughput']['ReadCapacityUnits'],\n            'write': response['Table']['ProvisionedThroughput']['WriteCapacityUnits']\n        }\n        return response", "idx": 475}
{"namespace": "boto.dynamodb2.table.Table.update", "completion": "        if throughput:\n            self.throughput['read'] = throughput.get('read', self.throughput['read'])\n            self.throughput['write'] = throughput.get('write', self.throughput['write'])\n\n        if global_indexes:\n            for index_name, values in global_indexes.items():\n                for index in self.global_indexes:\n                    if index.name == index_name:\n                        index.throughput['read'] = values.get('read', index.throughput['read'])\n                        index.throughput['write'] = values.get('write', index.throughput['write'])\n\n        return True", "idx": 476}
{"namespace": "boto.dynamodb2.table.Table.create_global_secondary_index", "completion": "        if global_index:\n            if not self.global_indexes:\n                self.global_indexes = []\n            self.global_indexes.append(global_index)\n            self.describe()\n            return True\n        else:\n            return False", "idx": 477}
{"namespace": "boto.dynamodb2.table.Table.delete_global_secondary_index", "completion": "\n        if global_index_name:\n            gsi_data = []\n\n            gsi_data.append({\n                \"Delete\": {\n                    \"IndexName\": global_index_name\n                }\n            })\n\n            self.connection.update_table(\n                self.table_name,\n                global_secondary_index_updates=gsi_data\n            )\n\n            return True\n        else:\n            msg = 'You need to provide the global index name to delete_global_secondary_index method'\n            boto.log.error(msg)\n            return False", "idx": 478}
{"namespace": "boltons.tbutils.print_exception", "completion": "    if file is None:\n        file = sys.stderr\n    if limit is not None:\n        if limit < 0:\n            limit = 0\n    if limit is not None:\n        stack = TracebackInfo.from_traceback(tb, limit=limit)\n    else:\n        stack = TracebackInfo.from_traceback(tb)\n    exc_info = ExceptionInfo.from_exc_info(etype, value, tb)\n    file.write(exc_info.get_formatted())\n    file.write('\\n')\n    file.write(stack.get_formatted())", "idx": 479}
{"namespace": "boltons.tbutils.ParsedException.to_string", "completion": "        formatted_exception = f\"{self.exc_type}: {self.exc_msg}\\n\"\n        for frame in self.frames:\n            if 'funcname' in frame:\n                formatted_exception += f\"  File \\\"{frame['filepath']}\\\", line {frame['lineno']}, in {frame['funcname']}\\n\"\n            else:\n                formatted_exception += f\"  File \\\"{frame['filepath']}\\\", line {frame['lineno']}\\n\"\n        return formatted_exception", "idx": 480}
{"namespace": "boltons.tbutils.ParsedException.from_string", "completion": "        if not isinstance(tb_str, text):\n            tb_str = tb_str.decode('utf-8')\n\n        lines = tb_str.splitlines()\n        frames = []\n        exc_type = None\n        exc_msg = None\n\n        for line in lines:\n            frame_match = _frame_re.match(line)\n            if frame_match:\n                frame_info = frame_match.groupdict()\n                frame_info['lineno'] = int(frame_info['lineno'])\n                frames.append(frame_info)\n            else:\n                se_frame_match = _se_frame_re.match(line)\n                if se_frame_match:\n                    frame_info = se_frame_match.groupdict()\n                    frame_info['lineno'] = int(frame_info['lineno'])\n                    frames.append(frame_info)\n                else:\n                    if exc_type is None:\n                        exc_type = line.split(':')[0]\n                        exc_msg = line[len(exc_type) + 2:]\n\n        return cls(exc_type, exc_msg, frames)", "idx": 481}
{"namespace": "boltons.tableutils.Table.extend", "completion": "        if not data:\n            return\n\n        self._data.extend(data)\n        self._width = max(self._width, max(len(row) for row in self._data))\n        for row in self._data:\n            row.extend([''] * (self._width - len(row)))", "idx": 482}
{"namespace": "boltons.tableutils.Table.from_object", "completion": "        for input_type in cls._input_types:\n            if input_type.check_type(data):\n                return cls.from_data(data=data, headers=headers, max_depth=max_depth, _data_type=input_type, metadata=metadata)\n        raise UnsupportedData(\"Unsupported data type: %s\" % type(data))", "idx": 483}
{"namespace": "boltons.tableutils.Table.__repr__", "completion": "        if self.headers:\n            return f'Table(headers={self.headers!r}, data={self._data!r})'\n        else:\n            return f'Table({self._data!r})'", "idx": 484}
{"namespace": "boltons.tableutils.Table.to_text", "completion": "        text_rows = []\n        if with_headers and self.headers:\n            header_row = ['{:^{}}'.format(str(h), maxlen) for h in self.headers]\n            text_rows.append(' | '.join(header_row))\n            separator_row = ['-|-' * len(self.headers)]\n            text_rows.append(''.join(separator_row))\n\n        for row in self._data:\n            text_row = ['{:^{}}'.format(to_text(cell, maxlen), maxlen) for cell in row]\n            text_rows.append(' | '.join(text_row))\n\n        return '\\n'.join(text_rows)", "idx": 485}
{"namespace": "boltons.statsutils.Stats.get_histogram_counts", "completion": "        if bins is None:\n            bins = self._get_bin_bounds(**kw)\n        elif isinstance(bins, int):\n            bins = self._get_bin_bounds(bins, **kw)\n\n        bin_counts = [0] * (len(bins) - 1)\n        for value in self.data:\n            for i, (lower, upper) in enumerate(zip(bins[:-1], bins[1:])):\n                if lower <= value < upper:\n                    bin_counts[i] += 1\n                    break\n        return list(zip(bins[:-1], bin_counts))", "idx": 486}
{"namespace": "boltons.setutils.IndexedSet.add", "completion": "        if item not in self.item_index_map:\n            real_index = len(self.item_list)\n            self.item_index_map[item] = real_index\n            self.item_list.append(item)\n        self._cull()", "idx": 487}
{"namespace": "boltons.setutils.IndexedSet.pop", "completion": "        if index is None:\n            index = -1\n        real_index = self._get_real_index(index)\n        item = self.item_list[real_index]\n        self.item_list[real_index] = _MISSING\n        self.item_index_map.pop(item)\n        self._add_dead(real_index)\n        self._cull()\n        return item", "idx": 488}
{"namespace": "boltons.setutils.IndexedSet.index", "completion": "        if val not in self.item_index_map:\n            raise ValueError(f\"{val!r} is not in {self.__class__.__name__}\")\n        return self.item_index_map[val]", "idx": 489}
{"namespace": "boltons.setutils.complement", "completion": "    class _ComplementSet(IndexedSet):\n        def __init__(self, other=None):\n            super().__init__(other)\n            self.complement_set = set()\n\n        def add(self, item):\n            self.complement_set.discard(item)\n            super().add(item)\n\n        def remove(self, item):\n            self.complement_set.add(item)\n            super().remove(item)\n\n        def discard(self, item):\n            self.complement_set.add(item)\n            super().discard(item)\n\n        def clear(self):\n            self.complement_set.update(self)\n            super().clear()\n\n        def update(self, *others):\n            for other in others:\n                self.complement_set.difference_update(other)\n            super().update(*others)\n\n        def intersection_update(self, *others):\n            for other in others:\n                self.complement_set.update(other)\n            super().intersection_update(*others)\n\n        def difference_update(self, *others):\n            for other in others:\n                self.complement_set.update(other)\n            super().difference_update(*others)\n\n        def symmetric_difference_update(self, other):\n            self.complement_set.symmetric_difference_update(other)\n            super().symmetric_difference_update(other)\n\n    return _ComplementSet(wrapped)", "idx": 490}
{"namespace": "boltons.strutils.strip_ansi", "completion": "    if isinstance(text, str):\n        return ANSI_SEQUENCES.sub('', text)\n    elif isinstance(text, bytes):\n        return ANSI_SEQUENCES.sub(b'', text).decode('utf-8', 'ignore')\n    elif isinstance(text, bytearray):\n        return ANSI_SEQUENCES.sub(b'', bytes(text)).decode('utf-8', 'ignore')\n    else:\n        raise TypeError(\"Input text must be of type str, bytes, or bytearray\")", "idx": 491}
{"namespace": "boltons.strutils.asciify", "completion": "    if isinstance(text, str):\n        text = text.encode('ascii', 'ignore' if ignore else 'replace')\n    return text", "idx": 492}
{"namespace": "boltons.strutils.indent", "completion": "    lines = text.split(newline)\n    indented_lines = [margin + line if key(line) else line for line in lines]\n    return newline.join(indented_lines)", "idx": 493}
{"namespace": "boltons.strutils.multi_replace", "completion": "    mr = MultiReplace(sub_map, **kwargs)\n    return mr.sub(text)", "idx": 494}
{"namespace": "boltons.cacheutils.LRI._get_flattened_ll", "completion": "        result = []\n        current = self._anchor[NEXT]\n        while current is not self._anchor:\n            result.append((current[KEY], current[VALUE]))\n            current = current[NEXT]\n        return result", "idx": 495}
{"namespace": "boltons.cacheutils.LRI.pop", "completion": "        with self._lock:\n            try:\n                link = self._link_lookup.pop(key)\n            except KeyError:\n                if default is _MISSING:\n                    raise\n                return default\n            else:\n                value = link[VALUE]\n                self._remove_from_ll(key)\n                super(LRI, self).__delitem__(key)\n                return value", "idx": 496}
{"namespace": "boltons.cacheutils.LRI.popitem", "completion": "        with self._lock:\n            if not self:\n                raise KeyError('popitem(): dictionary is empty')\n\n            # Get the oldest link in the linked list\n            oldest_link = self._anchor[NEXT]\n\n            # Remove the oldest link from the linked list\n            self._remove_from_ll(oldest_link[KEY])\n\n            # Remove the oldest key from the dictionary\n            value = super(LRI, self).pop(oldest_link[KEY])\n\n            return oldest_link[KEY], value", "idx": 497}
{"namespace": "boltons.cacheutils.LRI.clear", "completion": "        with self._lock:\n            super(LRI, self).clear()\n            self._init_ll()", "idx": 498}
{"namespace": "boltons.cacheutils.LRI.setdefault", "completion": "        with self._lock:\n            try:\n                link = self._link_lookup[key]\n            except KeyError:\n                self.miss_count += 1\n                if not self.on_miss:\n                    self[key] = default\n                    return default\n                ret = self[key] = self.on_miss(key)\n                return ret\n            else:\n                return link[VALUE]", "idx": 499}
{"namespace": "boltons.cacheutils.LRI.update", "completion": "        if hasattr(E, 'keys'):\n            for k in E.keys():\n                self[k] = E[k]\n        else:\n            for k, v in E:\n                self[k] = v\n        for k, v in F.items():\n            self[k] = v", "idx": 500}
{"namespace": "boltons.cacheutils.LRI.__repr__", "completion": "        return f\"{self.__class__.__name__}(max_size={self.max_size}, on_miss={self.on_miss}, values={dict(self)})\"", "idx": 501}
{"namespace": "boltons.cacheutils.CachedFunction.__repr__", "completion": "        cn = self.__class__.__name__\n        if not self.scoped and not self.typed:\n            return '%s(func=%r)' % (cn, self.func)\n        else:\n            return '%s(func=%r, scoped=%r, typed=%r)' % (cn, self.func, self.scoped, self.typed)", "idx": 502}
{"namespace": "boltons.cacheutils.ThresholdCounter.elements", "completion": "        for key, (count, _) in self._count_map.items():\n            for _ in range(count):\n                yield key", "idx": 503}
{"namespace": "boltons.cacheutils.ThresholdCounter.most_common", "completion": "        if n is None:\n            return sorted(self._count_map.items(), key=lambda x: x[1][0], reverse=True)\n        else:\n            return sorted(self._count_map.items(), key=lambda x: x[1][0], reverse=True)[:n]", "idx": 504}
{"namespace": "boltons.cacheutils.ThresholdCounter.update", "completion": "        if hasattr(iterable, 'items'):\n            iterable = iterable.items()\n        for elem, count in iterable:\n            self._count_map[elem] = [self._count_map.get(elem, [0, self._cur_bucket - 1])[0] + count, self._cur_bucket - 1]\n        for elem, count in kwargs.items():\n            self._count_map[elem] = [self._count_map.get(elem, [0, self._cur_bucket - 1])[0] + count, self._cur_bucket - 1]", "idx": 505}
{"namespace": "boltons.cacheutils.MinIDMap.get", "completion": "        if a in self.mapping:\n            return self.mapping[a]\n        else:\n            if self.free:\n                new_id = self.free.pop()\n            else:\n                new_id = len(self.ref_map)\n            self.mapping[a] = new_id\n            self.ref_map[new_id] = weakref.ref(a)\n            return new_id", "idx": 506}
{"namespace": "boltons.iterutils.chunked", "completion": "    if count is not None:\n        chunks = [list(chunk) for chunk in itertools.zip_longest(*[iter(src)]*size, fillvalue=kw.get('fill'))][:count]\n    else:\n        chunks = [list(chunk) for chunk in itertools.zip_longest(*[iter(src)]*size, fillvalue=kw.get('fill'))]\n    return chunks", "idx": 507}
{"namespace": "boltons.iterutils.chunk_ranges", "completion": "    if input_size <= 0:\n        raise ValueError('input_size must be a positive integer')\n    if chunk_size <= 0:\n        raise ValueError('chunk_size must be a positive integer')\n    if overlap_size < 0:\n        raise ValueError('overlap_size must be a non-negative integer')\n    if input_offset < 0:\n        raise ValueError('input_offset must be a non-negative integer')\n\n    if align:\n        start = input_offset\n        while start < input_size:\n            end = min(start + chunk_size, input_size)\n            yield (start, end)\n            start += chunk_size - overlap_size\n    else:\n        for start in range(input_offset, input_size, chunk_size - overlap_size):\n            end = min(start + chunk_size, input_size)\n            yield (start, end)", "idx": 508}
{"namespace": "boltons.iterutils.remap", "completion": "    def _remap(path, key, value):\n        new_key, new_value = visit(path, key, value)\n        if new_value is False:\n            return None\n        if new_value is True:\n            new_value = value\n        new_parent, children = enter(path, new_key, new_value)\n        if children is False:\n            return new_value\n        if children is True:\n            children = value\n        new_items = [(_remap(path + (new_key,), k, v) if isinstance(v, Iterable) else v)\n                     for k, v in children]\n        return exit(path, new_key, value, new_parent, new_items)\n\n    return _remap((), None, root)", "idx": 509}
{"namespace": "boltons.iterutils.get_path", "completion": "    try:\n        value = root\n        for key in path:\n            value = value[key]\n        return value\n    except (KeyError, IndexError, TypeError) as exc:\n        raise PathAccessError(exc, key, path)\n    except PathAccessError as pae:\n        if default is not _UNSET:\n            return default\n        else:\n            raise pae", "idx": 510}
{"namespace": "boltons.iterutils.research", "completion": "    results = []\n\n    def search(root, path=()):\n        if isinstance(root, Mapping):\n            for key, value in root.items():\n                new_path = path + (key,)\n                if query(new_path, key, value):\n                    results.append((new_path, value))\n                if isinstance(value, (Mapping, Sequence, Set)):\n                    search(value, new_path)\n        elif isinstance(root, Sequence) and not isinstance(root, basestring):\n            for i, value in enumerate(root):\n                new_path = path + (i,)\n                if query(new_path, i, value):\n                    results.append((new_path, value))\n                if isinstance(value, (Mapping, Sequence, Set)):\n                    search(value, new_path)\n        elif isinstance(root, Set):\n            for value in root:\n                new_path = path + (value,)\n                if query(new_path, value, value):\n                    results.append((new_path, value))\n                if isinstance(value, (Mapping, Sequence, Set)):\n                    search(value, new_path)\n\n    search(root)\n    return results", "idx": 511}
{"namespace": "boltons.socketutils.BufferedSocket.getrecvbuffer", "completion": "        return self.rbuf", "idx": 512}
{"namespace": "boltons.socketutils.BufferedSocket.getsendbuffer", "completion": "        with self._send_lock:\n            return b''.join(self.sbuf)", "idx": 513}
{"namespace": "boltons.socketutils.BufferedSocket.recv", "completion": "        if flags != 0:\n            raise ValueError(f'non-zero flags not supported: {flags!r}')\n\n        with self._recv_lock:\n            if len(self.rbuf) >= size:\n                data = self.rbuf[:size]\n                self.rbuf = self.rbuf[size:]\n                return data\n            elif self.rbuf:\n                data = self.rbuf\n                self.rbuf = b''\n                return data\n            else:\n                if timeout is _UNSET:\n                    timeout = self.timeout\n\n                with self._send_lock:\n                    self.sock.settimeout(timeout)\n                    try:\n                        data = self.sock.recv(size)\n                    except socket.timeout:\n                        raise Timeout(f'timed out after {timeout} seconds')\n                    finally:\n                        self.sock.settimeout(self.timeout)\n\n                if len(data) > size:\n                    self.rbuf = data[size:]\n                    return data[:size]\n                else:\n                    return data", "idx": 514}
{"namespace": "boltons.socketutils.BufferedSocket.recv_close", "completion": "        with self._recv_lock:\n            if timeout is _UNSET:\n                timeout = self.timeout\n            if maxsize is _UNSET:\n                maxsize = self.maxsize\n            self.sock.settimeout(timeout)\n            data = b''\n            while True:\n                chunk = self.sock.recv(self._recvsize)\n                if not chunk:\n                    break\n                data += chunk\n                if len(data) > maxsize:\n                    raise MessageTooLong(maxsize)\n        return data", "idx": 515}
{"namespace": "boltons.socketutils.BufferedSocket.flush", "completion": "        with self._send_lock:\n            self.sock.sendall(b'')", "idx": 516}
{"namespace": "boltons.socketutils.BufferedSocket.buffer", "completion": "        with self._send_lock:\n            self.sbuf.append(data)", "idx": 517}
{"namespace": "boltons.socketutils.BufferedSocket.close", "completion": "        with self._send_lock:\n            with self._recv_lock:\n                self.sock.close()\n                self.rbuf = b''\n                self.sbuf = []", "idx": 518}
{"namespace": "boltons.socketutils.NetstringSocket.setmaxsize", "completion": "        self.maxsize = maxsize\n        self._msgsize_maxsize = len(str(maxsize)) + 1  # len(str()) == log10", "idx": 519}
{"namespace": "boltons.socketutils.NetstringSocket.write_ns", "completion": "        payload_size = str(len(payload))\n        if len(payload_size) > self._msgsize_maxsize:\n            raise NetstringMessageTooLong(len(payload), self.maxsize)\n        data = payload_size.encode('ascii') + b':' + payload + b','\n        self.bsock.sendall(data, timeout=self.timeout)", "idx": 520}
{"namespace": "boltons.fileutils.FilePerms.__repr__", "completion": "        return '%s(user=%r, group=%r, other=%r)' % (self.__class__.__name__, self.user, self.group, self.other)", "idx": 521}
{"namespace": "boltons.mathutils.Bits.as_hex", "completion": "        template = '{{0:0{0}x}}'.format((self.len + 3) // 4)\n        return template.format(self.val)", "idx": 522}
{"namespace": "boltons.mathutils.Bits.from_hex", "completion": "        if isinstance(hex, bytes):\n            hex = hex.decode('utf-8')\n        if not hex.startswith('0x'):\n            hex = '0x' + hex\n        return cls(int(hex, 16))", "idx": 523}
{"namespace": "boltons.formatutils.split_format_str", "completion": "    formatter = Formatter()\n    parts = []\n    for literal_text, field_name, format_spec, conversion in formatter.parse(fstr):\n        parts.append((literal_text, construct_format_field_str(field_name, format_spec, conversion)))\n    return parts", "idx": 524}
{"namespace": "boltons.formatutils.infer_positional_format_args", "completion": "    pos_args = []\n    def repl(mo):\n        if mo.group(1):\n            return '{{'\n        elif mo.group(2):\n            return '}}'\n        else:\n            pos_args.append(mo.group(0))\n            return '{%s}' % (len(pos_args) - 1)\n    return _pos_farg_re.sub(repl, fstr)", "idx": 525}
{"namespace": "boltons.formatutils.tokenize_format_str", "completion": "    ret = []\n    formatter = Formatter()\n    prev_end = 0\n\n    for lit, fname, fspec, conv in formatter.parse(fstr):\n        if resolve_pos and fname is not None:\n            fname_list = re.split('[.[]', fname)\n            if len(fname_list) > 1:\n                raise ValueError('encountered compound format arg: %r' % fname)\n            try:\n                base_fname = fname_list[0]\n                assert base_fname\n            except (IndexError, AssertionError):\n                raise ValueError('encountered anonymous positional argument')\n            ret.append((fstr[prev_end:lit], BaseFormatField(fname, fspec, conv)))\n            prev_end = lit + len(fname)\n        else:\n            ret.append((fstr[prev_end:lit], None))\n            prev_end = lit\n\n    ret.append((fstr[prev_end:], None))\n    return ret", "idx": 526}
{"namespace": "boltons.dictutils.OneToOne.clear", "completion": "        dict.clear(self)\n        dict.clear(self.inv)", "idx": 527}
{"namespace": "boltons.dictutils.OneToOne.pop", "completion": "        if key in self:\n            value = self[key]\n            del self[key]\n            return value\n        elif default is not _MISSING:\n            return default\n        else:\n            raise KeyError(key)", "idx": 528}
{"namespace": "boltons.dictutils.OneToOne.popitem", "completion": "        key, value = dict.popitem(self)\n        del self.inv[value]\n        return key, value", "idx": 529}
{"namespace": "boltons.dictutils.ManyToMany.update", "completion": "        if isinstance(iterable, ManyToMany):\n            for key, vals in iterable.items():\n                self[key] = vals\n        elif hasattr(iterable, 'items'):\n            for key, vals in iterable.items():\n                self[key] = vals\n        else:\n            for key, val in iterable:\n                self[key] = val", "idx": 530}
{"namespace": "boltons.dictutils.ManyToMany.add", "completion": "        self.data.setdefault(key, set()).add(val)\n        self.inv.data.setdefault(val, set()).add(key)", "idx": 531}
{"namespace": "boltons.dictutils.ManyToMany.remove", "completion": "        if key in self.data and val in self.data[key]:\n            self.data[key].remove(val)\n            if not self.data[key]:\n                del self.data[key]\n            if val in self.inv.data and key in self.inv.data[val]:\n                self.inv.data[val].remove(key)\n                if not self.inv.data[val]:\n                    del self.inv.data[val]", "idx": 532}
{"namespace": "boltons.dictutils.ManyToMany.replace", "completion": "        if key in self.data:\n            # Replace key with newkey in the data dictionary\n            self.data[newkey] = self.data.pop(key)\n            # Update the inverse dictionary\n            for val in self.data[newkey]:\n                self.inv.data[val].remove(key)\n                self.inv.data[val].add(newkey)\n            # Remove the old key from the inverse dictionary\n            del self.inv.data[key]", "idx": 533}
{"namespace": "boltons.dictutils.ManyToMany.iteritems", "completion": "        for key, values in self.data.items():\n            for value in values:\n                yield key, value", "idx": 534}
{"namespace": "gunicorn.config.Config.__str__", "completion": "        lines = []\n        for key, value in self.settings.items():\n            if callable(value):\n                value = \"<{0}()>\".format(value.__qualname__)\n            lines.append(\"{0:{1}} = {2}\".format(key, len(max(self.settings.keys(), key=len)), value))\n        return '\\n'.join(lines)", "idx": 535}
{"namespace": "gunicorn.config.Config.set", "completion": "        if name not in self.settings:\n            raise AttributeError(\"No configuration setting for: %s\" % name)\n        self.settings[name].set(value)", "idx": 536}
{"namespace": "gunicorn.config.Config.worker_class", "completion": "        uri = self.settings['worker_class'].get()\n\n        # are we using a threaded worker?\n        is_sync = uri.endswith('SyncWorker') or uri == 'sync'\n        if is_sync and self.threads > 1:\n            return \"gthread\"\n        return uri", "idx": 537}
{"namespace": "gunicorn.config.Config.address", "completion": "        bind_address = self.settings['bind'].get()\n        return util.parse_address(bind_address)", "idx": 538}
{"namespace": "gunicorn.config.Config.logger_class", "completion": "        logger_class = self.settings['logger_class'].get()\n\n        if logger_class == \"simple\":\n            return util.load_class(\"gunicorn.glogging.Logger\")\n\n        if logger_class == \"statsd\" and self.statsd_on:\n            return util.load_class(\"gunicorn.instrument.statsd.Statsd\")\n\n        logger_class = self.settings['logger_class'].get(section=\"gunicorn.loggers\", default=\"gunicorn.glogging.Logger\")\n        logger_class = util.load_class(logger_class)\n\n        if hasattr(logger_class, \"install\"):\n            logger_class.install()\n\n        return logger_class", "idx": 539}
{"namespace": "gunicorn.sock.create_sockets", "completion": "\n    sockets = []\n    for addr in conf.bind:\n        sock_type = _sock_type(addr)\n        if fds:\n            fd = fds.pop(0)\n            sockets.append(sock_type(addr, conf, log, fd=fd))\n        else:\n            sockets.append(sock_type(addr, conf, log))\n    return sockets", "idx": 540}
{"namespace": "gunicorn.http.body.LengthReader.read", "completion": "        if not isinstance(size, int):\n            raise TypeError(\"size must be an integer type\")\n        if size < 0:\n            raise ValueError(\"Size must be positive.\")\n        if size == 0:\n            return b\"\"\n\n        buf = io.BytesIO()\n        while buf.tell() < size:\n            data = self.unreader.read()\n            if not data:\n                break\n            buf.write(data)\n\n        data = buf.getvalue()\n        ret, rest = data[:size], data[size:]\n        self.unreader.unread(rest)\n        self.length -= size\n        return ret", "idx": 541}
{"namespace": "gunicorn.http.body.EOFReader.read", "completion": "        if not isinstance(size, int):\n            raise TypeError(\"size must be an integral type\")\n        if size < 0:\n            raise ValueError(\"Size must be positive.\")\n        if size == 0:\n            return b\"\"\n\n        if self.finished:\n            data = self.buf.getvalue()\n            ret, rest = data[:size], data[size:]\n            self.buf = io.BytesIO()\n            self.buf.write(rest)\n            return ret\n\n        while self.buf.tell() < size:\n            data = self.unreader.read()\n            if not data:\n                self.finished = True\n                break\n            self.buf.write(data)\n        \n        data = self.buf.getvalue()\n        ret, rest = data[:size], data[size:]\n        self.buf = io.BytesIO()\n        self.buf.write(rest)\n        return ret", "idx": 542}
{"namespace": "gunicorn.http.body.Body.read", "completion": "        if size is None:\n            size = -1\n\n        if not isinstance(size, int):\n            raise TypeError(\"size must be an integral type\")\n        if size < 0:\n            raise ValueError(\"Size must be positive.\")\n\n        if size == 0:\n            return b\"\"\n\n        if self.buf.tell() >= size:\n            data = self.buf.getvalue()\n            ret, rest = data[:size], data[size:]\n            self.buf = io.BytesIO()\n            self.buf.write(rest)\n            return ret\n\n        buf = io.BytesIO()\n        data = self.reader.read(size)\n        while data:\n            buf.write(data)\n            if buf.tell() >= size:\n                break\n            data = self.reader.read(size - buf.tell())\n\n        buf = buf.getvalue()\n        ret, rest = buf[:size], buf[size:]\n        self.buf.write(rest)\n        return ret", "idx": 543}
{"namespace": "gunicorn.http.unreader.Unreader.read", "completion": "        if not isinstance(size, (int, long)):\n            raise TypeError(\"size parameter must be an int or long.\")\n        \n        if size is None:\n            size = -1\n        \n        self.buf.seek(0, os.SEEK_END)\n        \n        if size is None and self.buf.tell():\n            data = self.buf.getvalue()\n            self.buf = io.BytesIO()\n            return data\n        elif size is None:\n            return self.chunk()\n        \n        while self.buf.tell() < size:\n            chunk_data = self.chunk()\n            if chunk_data:\n                self.buf.write(chunk_data)\n            else:\n                data = self.buf.getvalue()\n                self.buf = io.BytesIO()\n                return data\n        \n        data = self.buf.read(size)\n        remaining_data = self.buf.read()\n        self.buf = io.BytesIO()\n        self.buf.write(remaining_data)\n        return data", "idx": 544}
{"namespace": "gunicorn.http.unreader.Unreader.unread", "completion": "        \"\"\"\n        This function appends the given data to the end of the buffer in the Unreader instance.\n        Input-Output Arguments\n        :param self: Unreader. An instance of the Unreader class.\n        :param data: The data to be appended to the buffer.\n        :return: No return values.\n        \"\"\"\n        if not isinstance(data, bytes):\n            raise TypeError(\"data parameter must be of type bytes.\")\n\n        self.buf.write(data)", "idx": 545}
{"namespace": "gunicorn.http.unreader.IterUnreader.chunk", "completion": "        \"\"\"\n        This function returns the next chunk of data from the iterator. If the iterator is empty, it returns an empty byte string. If the iterator is exhausted, it sets the iterator to None and returns an empty byte string.\n        Input-Output Arguments\n        :param self: IterUnreader. An instance of the IterUnreader class.\n        :return: bytes. The next chunk of data from the iterator.\n        \"\"\"\n        try:\n            return next(self.iter)\n        except StopIteration:\n            self.iter = None\n            return b\"\"", "idx": 546}
{"namespace": "gunicorn.instrument.statsd.Statsd.critical", "completion": "        Logger.critical(self, msg, *args, **kwargs)\n        self._send_stat(\"gunicorn.log.critical\", 1, COUNTER_TYPE)", "idx": 547}
{"namespace": "gunicorn.instrument.statsd.Statsd.access", "completion": "        status_code = int(str(resp.status).split(None, 1)[0])\n        self.histogram(\"gunicorn.request.duration\", request_time.total_seconds() * 1000)\n        self.increment(\"gunicorn.request.total\", 1)\n        self.increment(f\"gunicorn.request.status.{status_code}\", 1)", "idx": 548}
{"namespace": "praw.exceptions.RedditErrorItem.error_message", "completion": "        error_message = f\"{self.type}: {self.message}\" \n        if self.field:\n            error_message += f\" on field {self.field}\"\n        return error_message", "idx": 549}
{"namespace": "praw.exceptions.RedditErrorItem.__repr__", "completion": "        return f\"{self.__class__.__name__}(error_type={self.error_type}, message={self.message}, field={self.field})\"", "idx": 550}
{"namespace": "praw.models.util.BoundedSet.add", "completion": "        if item in self._set:\n            self._set.move_to_end(item)\n        else:\n            if len(self._set) >= self.max_items:\n                self._set.popitem(last=False)\n            self._set[item] = None", "idx": 551}
{"namespace": "praw.models.util.ExponentialCounter.counter", "completion": "        jitter = random.uniform(-self._base / 32, self._base / 32)\n        value = self._base + jitter\n        self._base = min(self._base * 2, self._max / 2) if self._base < self._max / 2 else self._max\n        return value", "idx": 552}
{"namespace": "praw.models.listing.generator.ListingGenerator._extract_sublist", "completion": "        if isinstance(listing, list):\n            if len(listing) >= 2 and isinstance(listing[1], (FlairListing, ModNoteListing)):\n                return listing[1]\n            else:\n                raise ValueError(\"The generator returned a list with unrecognized elements. File a bug report at PRAW.\")\n        elif isinstance(listing, dict):\n            if \"kind\" in listing and \"data\" in listing:\n                if listing[\"kind\"] == \"Listing\":\n                    return listing[\"data\"]\n                elif listing[\"kind\"] == \"UserList\":\n                    return listing[\"data\"][\"children\"]\n                else:\n                    raise ValueError(\"The generator returned a dictionary with unrecognized kind. File a bug report at PRAW.\")\n            else:\n                raise ValueError(\"The generator returned a dictionary PRAW didn't recognize. File a bug report at PRAW.\")\n        else:\n            raise ValueError(\"The generator returned a type PRAW didn't recognize. File a bug report at PRAW.\")", "idx": 553}
{"namespace": "praw.util.token_manager.FileTokenManager.post_refresh_callback", "completion": "        with open(self._filename, 'w') as file:\n            file.write(authorizer.refresh_token)", "idx": 554}
{"namespace": "praw.util.token_manager.FileTokenManager.pre_refresh_callback", "completion": "        \"\"\"\n        This function loads the refresh token from the file and assigns it to the authorizer if it is not already set.\n        Input-Output Arguments\n        :param self: FileTokenManager. An instance of the FileTokenManager class.\n        :param authorizer: The authorizer object that needs to be refreshed.\n        :return: No return values.\n        \"\"\"\n        if authorizer.refresh_token is None:\n            with open(self._filename, \"r\") as fp:\n                authorizer.refresh_token = fp.read().strip()", "idx": 555}
{"namespace": "praw.util.token_manager.SQLiteTokenManager._get", "completion": "        cursor = self._connection.cursor()\n        cursor.execute(\"SELECT refresh_token FROM tokens WHERE id = ?\", (self.key,))\n        result = cursor.fetchone()\n        cursor.close()\n        if result is None:\n            raise KeyError(\"No refresh token found for the provided key\")\n        return result[0]", "idx": 556}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.is_registered", "completion": "        try:\n            self._get()\n            return True\n        except KeyError:\n            return False", "idx": 557}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.post_refresh_callback", "completion": "        self._set(authorizer.refresh_token)\n        authorizer.refresh_token = None", "idx": 558}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.pre_refresh_callback", "completion": "        \"\"\"Load the refresh token from the database.\"\"\"\n        try:\n            authorizer.refresh_token = self._get()\n        except KeyError:\n            pass", "idx": 559}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.register", "completion": "        if self.is_registered():\n            return False\n        else:\n            self._set(refresh_token)\n            return True", "idx": 560}
{"namespace": "jc.cli.JcCli.about_jc", "completion": "\n        about_info: JSONDictType = {\n            \"library_name\": \"jc\",\n            \"version\": info.version,\n            \"description\": info.description,\n            \"author\": info.author,\n            \"author_email\": info.author_email,\n            \"website\": info.website,\n            \"copyright\": info.copyright,\n            \"license\": info.license,\n            \"python_version\": sys.version,\n            \"python_path\": sys.executable,\n            \"parser_count\": len(all_parser_info()),\n            \"standard_parser_count\": len([p for p in all_parser_info() if 'standard' in p.get('tags', [])]),\n            \"streaming_parser_count\": len([p for p in all_parser_info() if p.get('streaming')]),\n            \"plugin_parser_count\": len([p for p in all_parser_info() if 'plugin' in p.get('tags', [])]),\n            \"all_parser_info\": all_parser_info()\n        }\n\n        return about_info", "idx": 561}
{"namespace": "jc.cli.JcCli.yaml_out", "completion": "                    from ruamel.yaml import YAML\n                    import ruamel.yaml\n\n        if self.data_out is None:\n            return ''\n\n        try:\n            import ruamel.yaml\n            from ruamel.yaml import YAML\n            yaml = YAML()\n            yaml.default_flow_style = False\n            yaml.indent(mapping=2, sequence=4, offset=2)\n            yaml_str = yaml.dump(self.data_out)\n            return yaml_str\n        except ImportError:\n            utils.warning_message(['ruamel.yaml library not installed. Falling back to JSON formatting.'])\n            return utils.json_format(self.data_out)", "idx": 562}
{"namespace": "jc.parsers.os_release.parse", "completion": "\n    if raw:\n        return jc.utils.process_raw(data)\n    else:\n        return _process(jc.parsers.kv.parse(data, quiet=quiet))", "idx": 563}
{"namespace": "jc.parsers.xrandr._parse_screen", "completion": "    screen_match = re.match(_screen_pattern, next_lines[0])\n    if not screen_match:\n        return None\n\n    screen = {\n        \"screen_number\": int(screen_match.group(\"screen_number\")),\n        \"minimum_width\": int(screen_match.group(\"minimum_width\")),\n        \"minimum_height\": int(screen_match.group(\"minimum_height\")),\n        \"current_width\": int(screen_match.group(\"current_width\")),\n        \"current_height\": int(screen_match.group(\"current_height\")),\n        \"maximum_width\": int(screen_match.group(\"maximum_width\")),\n        \"maximum_height\": int(screen_match.group(\"maximum_height\")),\n        \"devices\": []\n    }\n\n    for line in next_lines[1:]:\n        device = _parse_device([line])\n        if device:\n            screen[\"devices\"].append(device)\n\n    return screen", "idx": 564}
{"namespace": "jc.parsers.xrandr._parse_model", "completion": "    if not next_lines:\n        return None\n\n    next_line = next_lines.pop()\n    result = re.match(_edid_head_pattern, next_line)\n    if not result:\n        next_lines.append(next_line)\n        return None\n\n    edid_lines = []\n    while next_lines:\n        next_line = next_lines.pop()\n        result = re.match(_edid_line_pattern, next_line)\n        if result:\n            edid_lines.append(result.group(\"edid_line\"))\n        else:\n            next_lines.append(next_line)\n            break\n\n    if not edid_lines:\n        return None\n\n    edid_bytes = bytes.fromhex(\"\".join(edid_lines))\n\n    model: Model = {\n        \"name\": edid_bytes[0x14:0x36].decode(\"utf-8\").strip(),\n        \"product_id\": edid_bytes[0x8:0x10].hex(),\n        \"serial_number\": edid_bytes[0x10:0x18].hex(),\n    }\n\n    return model", "idx": 565}
{"namespace": "jc.parsers.xrandr._parse_mode", "completion": "    result = re.match(_mode_pattern, line)\n    if not result:\n        return None\n\n    matches = result.groupdict()\n    mode: Mode = {\n        \"resolution_width\": int(matches[\"resolution_width\"]),\n        \"resolution_height\": int(matches[\"resolution_height\"]),\n        \"is_high_resolution\": matches[\"is_high_resolution\"] == \"i\",\n        \"frequencies\": []\n    }\n\n    frequencies = re.finditer(_frequencies_pattern, matches[\"rest\"])\n    for frequency in frequencies:\n        frequency_matches = frequency.groupdict()\n        if frequency_matches[\"frequency\"]:\n            freq: Frequency = {\n                \"frequency\": float(frequency_matches[\"frequency\"]),\n                \"is_current\": frequency_matches[\"star\"] == \"*\",\n                \"is_preferred\": frequency_matches[\"plus\"] == \"+\"\n            }\n            mode[\"frequencies\"].append(freq)\n\n    return mode", "idx": 566}
{"namespace": "pythonforandroid.archs.Arch.include_dirs", "completion": "        return [\n            self.ctx.ndk.sysroot_include_dir,\n            self.python_includes\n        ]", "idx": 567}
{"namespace": "pythonforandroid.archs.Arch.target", "completion": "        return f\"{self.command_prefix}-android{self.ctx.ndk_api}\"", "idx": 568}
{"namespace": "pythonforandroid.archs.ArchARM.target", "completion": "        return '{triplet}{ndk_api}'.format(\n            triplet=self.command_prefix, ndk_api=self.ctx.ndk_api\n        )", "idx": 569}
{"namespace": "pythonforandroid.recipe.Recipe.get_recipe", "completion": "\n        recipe_module = None\n        for recipes_dir in cls.recipe_dirs(ctx):\n            recipe_file = join(recipes_dir, name, '__init__.py')\n            if exists(recipe_file):\n                recipe_module = split(recipe_file)[0]\n                break\n\n        if recipe_module:\n            recipe_name = 'pythonforandroid.recipes.{}'.format(name)\n            recipe = __import__(recipe_name, fromlist=[''])\n            return recipe.Recipe\n        else:\n            raise ValueError(f\"Recipe '{name}' not found\")", "idx": 570}
{"namespace": "pythonforandroid.prerequisites.HomebrewPrerequisite.darwin_helper", "completion": "        info(f\"Installer for Homebrew is not supported on macOS. Please follow the instructions at https://brew.sh/ to install Homebrew.\")", "idx": 571}
{"namespace": "msticpy.config.query_editor.MetadataEditWidget.save_metadata", "completion": "        self.metadata.version = self.version_widget.value\n        self.metadata.description = self.description_widget.value\n        self.metadata.data_environments = list(self.data_env_widget.value)\n        self.metadata.data_families = [\n            family.strip() for family in self.data_families_widget.value.split(\",\")\n        ]\n        self.metadata.database = self.database_widget.value\n        self.metadata.cluster = self.cluster_widget.value\n        self.metadata.clusters = [\n            cluster.strip() for cluster in self.clusters_widget.value.split(\"\\n\")\n        ]\n        self.metadata.cluster_groups = [\n            group.strip() for group in self.cluster_groups_widget.value.split(\"\\n\")\n        ]\n        self.metadata.tags = [\n            tag.strip() for tag in self.tags_widget.value.split(\",\")\n        ]\n        self.metadata.data_source = self.data_source_widget.value\n        self._changed_data = True", "idx": 572}
{"namespace": "msticpy.config.query_editor.QueryEditor._save_file", "completion": "        if self.ignore_changes.value:\n            return\n        if not self.query_collection:\n            return\n        file_path = self.filename_widget.value\n        with open(file_path, \"w\") as file:\n            yaml.dump(asdict(self.query_collection), file)", "idx": 573}
{"namespace": "msticpy.config.query_editor.QueryEditor._unsaved_changes", "completion": "        return (\n            self.default_param_editor.changed_data\n            or self.metadata_editor.changed_data\n            or self.query_editor.changed_data\n        )", "idx": 574}
{"namespace": "msticpy.config.query_editor.load_queries_from_yaml", "completion": "    with open(yaml_file, \"r\") as file:\n        yaml_data = yaml.safe_load(file)\n\n    metadata = yaml_data.get(\"metadata\", {})\n    defaults = yaml_data.get(\"defaults\", {})\n    sources = yaml_data.get(\"sources\", {})\n\n    query_metadata = QueryMetadata(\n        version=metadata.get(\"version\", 1),\n        description=metadata.get(\"description\", \"Query collection description\"),\n        data_environments=metadata.get(\"data_environments\", []),\n        data_families=metadata.get(\"data_families\", []),\n        database=metadata.get(\"database\", \"\"),\n        cluster=metadata.get(\"cluster\", \"\"),\n        clusters=metadata.get(\"clusters\", []),\n        cluster_groups=metadata.get(\"cluster_groups\", []),\n        tags=metadata.get(\"tags\", []),\n        data_source=metadata.get(\"data_source\", \"\"),\n    )\n\n    query_defaults = QueryDefaults(\n        metadata=defaults.get(\"metadata\", {}),\n        parameters=defaults.get(\"parameters\", {}),\n    )\n\n    query_collection = QueryCollection(\n        file_name=yaml_file,\n        metadata=query_metadata,\n        defaults=query_defaults,\n        sources=sources,\n    )\n\n    return query_collection", "idx": 575}
{"namespace": "zxcvbn.time_estimates.estimate_attack_times", "completion": "    ctx = Context(prec=28, rounding=Inexact)\n    crack_times_seconds = {\n        \"online_throttling_100_per_hour\": ctx.divide(Decimal(guesses), Decimal(100)) * 3600,\n        \"online_no_throttling_10_per_second\": ctx.divide(Decimal(guesses), Decimal(10)),\n        \"offline_slow_hashing_1e4_per_second\": ctx.divide(Decimal(guesses), Decimal(1e4)),\n        \"offline_fast_hashing_1e10_per_second\": ctx.divide(Decimal(guesses), Decimal(1e10))\n    }\n\n    crack_times_readable = {\n        \"online_throttling_100_per_hour\": \"Centuries\",\n        \"online_no_throttling_10_per_second\": \"Centuries\",\n        \"offline_slow_hashing_1e4_per_second\": \"Centuries\",\n        \"offline_fast_hashing_1e10_per_second\": \"Centuries\"\n    }\n\n    score = 0\n    if guesses < 1e3:\n        score = 0\n    elif guesses < 1e6:\n        score = 1\n    elif guesses < 1e8:\n        score = 2\n    elif guesses < 1e10:\n        score = 3\n    elif guesses < 1e12:\n        score = 4\n    else:\n        score = 5\n\n    return {\n        \"crack_times_seconds\": crack_times_seconds,\n        \"crack_times_readable\": crack_times_readable,\n        \"score\": score\n    }", "idx": 576}
{"namespace": "zxcvbn.scoring.estimate_guesses", "completion": "    if 'guesses' in match:\n        return match['guesses']\n\n    token = match['token']\n    if len(token) == 0:\n        match['guesses'] = 1\n        return match['guesses']\n\n    if len(token) == 1:\n        match['guesses'] = MIN_SUBMATCH_GUESSES_SINGLE_CHAR\n        return match['guesses']\n\n    if len(token) == 2:\n        match['guesses'] = MIN_SUBMATCH_GUESSES_MULTI_CHAR\n        return match['guesses']\n\n    if len(token) < len(password):\n        match['guesses'] = nCk(len(password), len(token))\n        return match['guesses']\n\n    match['guesses'] = MIN_GUESSES_BEFORE_GROWING_SEQUENCE ** (len(token) - 1)\n    return match['guesses']", "idx": 577}
{"namespace": "zxcvbn.scoring.dictionary_guesses", "completion": "    base_guesses = match['rank']\n    uppercase_variations = 1\n    l33t_variations = 1\n    reversed_variations = 1\n\n    if match.get('reversed', False):\n        reversed_variations = 2\n\n    if match.get('l33t', False):\n        l33t_variations = match['sub']\n        # uppercase + l33t uppercase + l33t lowercase + l33t title\n        uppercase_variations = 2 ** match['l33t'] + 1\n\n    return base_guesses * uppercase_variations * l33t_variations * reversed_variations", "idx": 578}
{"namespace": "zxcvbn.scoring.regex_guesses", "completion": "    character_class_bases = {\n        'alpha_lower': 26,  # lowercase alphabets\n        'alpha_upper': 26,  # uppercase alphabets\n        'alpha': 52,  # both lowercase and uppercase alphabets\n        'alphanumeric': 62,  # alphanumeric characters\n        'digits': 10,  # numeric digits\n        'symbols': 33  # symbols and special characters\n    }\n\n    if match['pattern'] == 'regex':\n        # handle regular expression matches\n        token = match['token']\n        if token.startswith('\\\\d'):\n            # match for digits\n            return character_class_bases['digits'] ** len(token)\n        elif token.startswith('\\\\w'):\n            # match for alphanumeric characters\n            return character_class_bases['alphanumeric'] ** len(token)\n        elif token.startswith('\\\\s'):\n            # match for whitespace characters\n            return 33 ** len(token)  # assuming 33 possible symbols for whitespace\n        elif token.startswith('\\\\p{Ll}'):\n            # match for lowercase alphabets\n            return character_class_bases['alpha_lower'] ** len(token)\n        elif token.startswith('\\\\p{Lu}'):\n            # match for uppercase alphabets\n            return character_class_bases['alpha_upper'] ** len(token)\n        elif token.startswith('\\\\p{L}'):\n            # match for alphabets (both lowercase and uppercase)\n            return character_class_bases['alpha'] ** len(token)\n        elif token.startswith('\\\\p{N}'):\n            # match for any kind of numeric character\n            return 62 ** len(token)  # assuming 62 possible alphanumeric characters\n        else:\n            # default case for other regular expression matches\n            return 33 ** len(token)  # assuming 33 possible symbols for other characters\n    else:\n        # handle other types of matches\n        return 33 ** len(match['token'])  # assuming 33 possible symbols for other characters", "idx": 579}
{"namespace": "zxcvbn.scoring.date_guesses", "completion": "    year_space = abs(int(match['year']) - REFERENCE_YEAR)\n    year_space = max(year_space, MIN_YEAR_SPACE)\n\n    separator_variations = 1\n    if match['separator']:\n        separator_variations = 2  # either the separator is present or not\n\n    return year_space * 12 * 31 * separator_variations", "idx": 580}
{"namespace": "zxcvbn.scoring.spatial_guesses", "completion": "    graph_type = match['graph']\n    token_length = len(match['token'])\n    turns = match['turns']\n    shifted_count = match['shifted_count']\n\n    if graph_type == 'qwerty':\n        average_degree = KEYBOARD_AVERAGE_DEGREE\n        starting_positions = KEYBOARD_STARTING_POSITIONS\n    elif graph_type == 'dvorak':\n        average_degree = calc_average_degree(ADJACENCY_GRAPHS['dvorak'])\n        starting_positions = len(ADJACENCY_GRAPHS['dvorak'].keys())\n    elif graph_type == 'keypad':\n        average_degree = KEYPAD_AVERAGE_DEGREE\n        starting_positions = KEYPAD_STARTING_POSITIONS\n    else:\n        raise ValueError(\"Invalid graph type\")\n\n    possibilities = 0\n    for i in range(1, turns + 1):\n        possibilities += nCk(token_length - 1, i - 1) * starting_positions * (average_degree ** i)\n\n    shifted_possibilities = 0\n    if shifted_count > 0:\n        for i in range(1, turns + 1):\n            shifted_possibilities += nCk(shifted_count, i - 1) * starting_positions * (average_degree ** i)\n\n    return int(possibilities + shifted_possibilities)", "idx": 581}
{"namespace": "zxcvbn.scoring.uppercase_variations", "completion": "    word = match['token']\n\n    if ALL_LOWER.match(word) or word.islower():\n        return 1\n    elif START_UPPER.match(word) or END_UPPER.match(word) or ALL_UPPER.match(word):\n        return 2\n    else:\n        upper_count = sum(1 for c in word if c.isupper())\n        lower_count = sum(1 for c in word if c.islower())\n        return 2 ** min(upper_count, lower_count)", "idx": 582}
{"namespace": "zxcvbn.matching.dictionary_match", "completion": "    matches = []\n    for i in range(len(password)):\n        for j in range(i, len(password)):\n            word = password[i:j + 1]\n            if word in _ranked_dictionaries:\n                rank = _ranked_dictionaries[word]\n                matches.append({\n                    'pattern': 'dictionary',\n                    'i': i,\n                    'j': j,\n                    'token': password[i:j + 1],\n                    'matched_word': word,\n                    'rank': rank,\n                    'dictionary_name': 'user',\n                    'reversed': False,\n                    'l33t': False,\n                })\n    return sorted(matches, key=lambda x: (x['i'], x['j']))", "idx": 583}
{"namespace": "zxcvbn.matching.reverse_dictionary_match", "completion": "    matches = []\n    length = len(password)\n    password_lower = password.lower()\n    for dictionary_name, ranked_dict in _ranked_dictionaries.items():\n        for i in range(length):\n            for j in range(i, length):\n                if password_lower[i:j + 1][::-1] in ranked_dict:\n                    word = password_lower[i:j + 1][::-1]\n                    rank = ranked_dict[word]\n                    matches.append({\n                        'pattern': 'dictionary',\n                        'i': i,\n                        'j': j,\n                        'token': password[i:j + 1],\n                        'matched_word': word[::-1],\n                        'rank': rank,\n                        'dictionary_name': dictionary_name,\n                        'reversed': True,\n                        'l33t': False,\n                    })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))", "idx": 584}
{"namespace": "zxcvbn.matching.l33t_match", "completion": "    matches = []\n    password_lower = password.lower()\n    for sub in relevant_l33t_subtable(password, _l33t_table):\n        subbed_password = translate(password_lower, sub)\n        for match in dictionary_match(subbed_password, _ranked_dictionaries):\n            token = password[match['i']:match['j'] + 1]\n            if token.lower() == match['matched_word']:\n                continue\n            match['l33t'] = True\n            match['token'] = token\n            match['sub'] = sub\n            matches.append(match)\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))", "idx": 585}
{"namespace": "zxcvbn.matching.repeat_match", "completion": "    matches = []\n    greedy = re.compile(r'(.+)\\1+')\n    lazy = re.compile(r'(.+?)\\1+')\n    length = len(password)\n\n    for i in range(length):\n        for j in range(i, length):\n            greedy_match = greedy.match(password[i:j + 1])\n            lazy_match = lazy.match(password[i:j + 1])\n\n            if greedy_match:\n                base_token = greedy_match.group(1)\n                repeat_count = len(password[i:j + 1]) // len(base_token)\n                matches.append({\n                    'pattern': 'repeat',\n                    'i': i,\n                    'j': j,\n                    'token': password[i:j + 1],\n                    'base_token': base_token,\n                    'base_guesses': scoring.most_guessable_match_sequence(\n                        base_token, _ranked_dictionaries),\n                    'base_matches': dictionary_match(base_token,\n                                                     _ranked_dictionaries),\n                    'repeat_count': repeat_count,\n                })\n\n            if lazy_match:\n                base_token = lazy_match.group(1)\n                repeat_count = len(password[i:j + 1]) // len(base_token)\n                matches.append({\n                    'pattern': 'repeat',\n                    'i': i,\n                    'j': j,\n                    'token': password[i:j + 1],\n                    'base_token': base_token,\n                    'base_guesses': scoring.most_guessable_match_sequence(\n                        base_token, _ranked_dictionaries),\n                    'base_matches': dictionary_match(base_token,\n                                                     _ranked_dictionaries),\n                    'repeat_count': repeat_count,\n                })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))", "idx": 586}
{"namespace": "zxcvbn.matching.spatial_match", "completion": "    matches = []\n    for graph_name, graph in _graphs.items():\n        matches.extend(spatial_match_helper(password, graph, graph_name, _ranked_dictionaries))\n    return sorted(matches, key=lambda x: (x['i'], x['j']))", "idx": 587}
{"namespace": "zxcvbn.matching.sequence_match", "completion": "    matches = []\n    i = 0\n    while i < len(password) - 1:\n        j = i + 1\n        delta = ord(password[j]) - ord(password[i])\n        if delta == 1 or delta == -1:\n            sequence_name = 'lower'\n            sequence_space = 25\n        elif delta == 2 or delta == -2:\n            sequence_name = 'upper'\n            sequence_space = 25\n        elif delta == 9 or delta == -9:\n            sequence_name = 'digits'\n            sequence_space = 8\n        else:\n            sequence_name = 'unicode'\n            sequence_space = 0\n\n        while j < len(password):\n            delta = ord(password[j]) - ord(password[j - 1])\n            if delta == 0:\n                j += 1\n            else:\n                if delta == 1 or delta == -1:\n                    if sequence_name != 'lower':\n                        break\n                elif delta == 2 or delta == -2:\n                    if sequence_name != 'upper':\n                        break\n                elif delta == 9 or delta == -9:\n                    if sequence_name != 'digits':\n                        break\n                elif sequence_name == 'unicode':\n                    sequence_space += 1\n                else:\n                    break\n\n                if j - i > 2:\n                    matches.append({\n                        'pattern': 'sequence',\n                        'i': i,\n                        'j': j - 1,\n                        'token': password[i:j],\n                        'sequence_name': sequence_name,\n                        'sequence_space': sequence_space,\n                        'ascending': delta > 0\n                    })\n\n                i = j - 1\n                break\n\n    return matches", "idx": 588}
{"namespace": "zxcvbn.matching.regex_match", "completion": "    matches = []\n    for regex_name, regex_pattern in _regexen.items():\n        for match in regex_pattern.finditer(password):\n            matches.append({\n                'pattern': 'regex',\n                'i': match.start(),\n                'j': match.end() - 1,\n                'token': match.group(),\n                'regex_name': regex_name,\n                'regex_match': match,\n            })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))", "idx": 589}
{"namespace": "OpenSSL.rand.add", "completion": "    _lib.RAND_add(buffer, entropy)", "idx": 590}
{"namespace": "asyncssh.kex.register_kex_alg", "completion": "\n    global _kex_algs, _default_kex_algs, _kex_handlers\n\n    _kex_algs.append(alg)\n    _kex_handlers[alg] = (handler, hash_alg, args)\n\n    if default:\n        _default_kex_algs.append(alg)", "idx": 591}
{"namespace": "asyncssh.auth.get_supported_server_auth_methods", "completion": "\n    supported_methods = []\n    for method in _auth_methods:\n        if _server_auth_handlers.get(method) and \\\n                _server_auth_handlers[method].supported(conn):\n            supported_methods.append(method)\n    return supported_methods", "idx": 592}
{"namespace": "asyncssh.mac.get_mac", "completion": "    if mac_alg in _mac_handler:\n        handler, hash_size, args = _mac_handler[mac_alg]\n        return handler(key, hash_size, *args)\n    else:\n        raise ValueError(\"Unsupported MAC algorithm\")", "idx": 593}
{"namespace": "asyncssh.auth_keys.SSHAuthorizedKeys.validate", "completion": "        entries = self._ca_entries if ca else self._user_entries\n        entries = entries + self._x509_entries\n\n        for entry in entries:\n            if entry.key == key:\n                if entry.match_options(client_host, client_addr, cert_principals):\n                    return entry.options\n\n        return None", "idx": 594}
{"namespace": "asyncssh.saslprep.saslprep", "completion": "\n    return _stringprep(s, True, _map_saslprep, 'NFKC', [stringprep.in_table_c12, stringprep.in_table_c21, stringprep.in_table_c22, stringprep.in_table_c3, stringprep.in_table_c4, stringprep.in_table_c5, stringprep.in_table_c6, stringprep.in_table_c7, stringprep.in_table_c8, stringprep.in_table_c9], True)", "idx": 595}
{"namespace": "asyncssh.asn1.der_decode", "completion": "    value, end = der_decode_partial(data)\n    if end < len(data):\n        raise ASN1DecodeError('Data contains unexpected bytes at end')\n    return value", "idx": 596}
{"namespace": "asyncssh.packet.SSHPacket.check_end", "completion": "        if self._idx != self._len:\n            raise PacketDecodeError(f\"Leftover data at end of packet: {self._len - self._idx} byte(s)\")", "idx": 597}
{"namespace": "asyncssh.public_key.SSHKey.verify", "completion": "        packet = SSHPacket(sig)\n        sig_algorithm = packet.get_string()\n        \n        if sig_algorithm not in self.all_sig_algorithms:\n            return False  # Signature algorithm not supported\n        \n        return self.verify_ssh(data, sig_algorithm, packet)", "idx": 598}
{"namespace": "asyncssh.public_key.SSHKey.convert_to_public", "completion": "\n        public_key = SSHKey()\n        public_key.algorithm = self.algorithm\n        public_key.sig_algorithms = self.sig_algorithms\n        public_key.cert_algorithms = self.cert_algorithms\n        public_key.x509_algorithms = self.x509_algorithms\n        public_key.all_sig_algorithms = self.all_sig_algorithms\n        public_key.default_x509_hash = self.default_x509_hash\n        public_key.pem_name = self.pem_name\n        public_key.pkcs8_oid = self.pkcs8_oid\n        public_key.use_executor = self.use_executor\n\n        # Set the comment and filename for the public key\n        public_key.set_comment(self.get_comment_bytes())\n        public_key.set_filename(self.get_filename())\n\n        return public_key", "idx": 599}
{"namespace": "asyncssh.public_key.SSHKey.generate_x509_user_certificate", "completion": "        if not _x509_available: # pragma: no cover\n            raise KeyGenerationError('X.509 certificate generation requires PyOpenSSL')\n\n        if not self.x509_algorithms:\n            raise KeyGenerationError('X.509 certificate generation not supported for ' + self.get_algorithm() + ' keys')\n\n        if hash_alg == ():\n            hash_alg = self.default_x509_hash\n\n        if comment == ():\n            comment = user_key.get_comment_bytes()\n\n        return SSHX509Certificate.generate(self, user_key, subject, issuer, serial, valid_after, valid_before, False, None, purposes, principals, (), hash_alg, comment)", "idx": 600}
{"namespace": "asyncssh.misc.write_file", "completion": "    with open_file(filename, mode) as f:\n        return f.write(data)", "idx": 601}
{"namespace": "diffprivlib.accountant.BudgetAccountant.__repr__", "completion": "        epsilon_str = f\"epsilon={self.epsilon}\" if self.epsilon != float(\"inf\") else \"\"\n        delta_str = f\"delta={self.delta}\" if self.delta != 1.0 else \"\"\n        slack_str = f\"slack={self.slack}\" if self.slack > 0 else \"\"\n        \n        spent_budget_str = \"\"\n        if len(self.spent_budget) > n_budget_max:\n            spent_budget_str = \", \".join([f\"({epsilon}, {delta})\" for epsilon, delta in self.spent_budget[:n_budget_max]])\n            spent_budget_str += \", ...\"\n        else:\n            spent_budget_str = \", \".join([f\"({epsilon}, {delta})\" for epsilon, delta in self.spent_budget])\n        \n        return f\"BudgetAccountant({epsilon_str}, {delta_str}, {slack_str}, spent_budget=[{spent_budget_str}])\"", "idx": 602}
{"namespace": "diffprivlib.accountant.BudgetAccountant.check", "completion": "        if not isinstance(epsilon, Integral) and epsilon < 0:\n            raise ValueError(\"Epsilon must be a non-negative integer\")\n\n        if not isinstance(delta, float) and delta < 0:\n            raise ValueError(\"Delta must be a non-negative float\")\n\n        total_epsilon, total_delta = self.total()\n\n        if epsilon > total_epsilon or delta > total_delta:\n            raise BudgetError(f\"Privacy spend of ({epsilon},{delta}) not permissible; will exceed remaining privacy budget. Use {self.__class__.__name__}.{self.remaining.__name__}() to check remaining budget.\")\n\n        return True", "idx": 603}
{"namespace": "diffprivlib.accountant.BudgetAccountant.spend", "completion": "        if not self.check(epsilon, delta):\n            raise BudgetError(f\"Privacy spend of ({epsilon},{delta}) not permissible; will exceed remaining privacy budget.\"\n                              f\" Use {self.__class__.__name__}.{self.remaining.__name__}() to check remaining budget.\")\n\n        self.__spent_budget.append((epsilon, delta))\n        return self", "idx": 604}
{"namespace": "diffprivlib.accountant.BudgetAccountant.load_default", "completion": "        if accountant is None:\n            if BudgetAccountant._default is None:\n                BudgetAccountant._default = BudgetAccountant()\n            return BudgetAccountant._default\n        elif isinstance(accountant, BudgetAccountant):\n            return accountant\n        else:\n            raise TypeError(\"The supplied accountant must be an instance of the BudgetAccountant class\")", "idx": 605}
{"namespace": "diffprivlib.accountant.BudgetAccountant.set_default", "completion": "        BudgetAccountant._default = self\n        return self", "idx": 606}
{"namespace": "diffprivlib.accountant.BudgetAccountant.pop_default", "completion": "        default = BudgetAccountant._default\n        BudgetAccountant._default = None\n        return default", "idx": 607}
{"namespace": "diffprivlib.validation.clip_to_bounds", "completion": "    if not isinstance(array, np.ndarray):\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\n    if array.ndim != 2:\n        raise ValueError(f\"input array must be 2-dimensional, got {array.ndim} dimensions.\")\n\n    lower, upper = bounds\n\n    if np.asarray(lower).size == 1 or np.asarray(upper).size == 1:\n        lower = np.ravel(lower).astype(array.dtype)\n        upper = np.ravel(upper).astype(array.dtype)\n    else:\n        lower = np.asarray(lower, dtype=array.dtype)\n        upper = np.asarray(upper, dtype=array.dtype)\n\n    if lower.shape != upper.shape:\n        raise ValueError(\"lower and upper bounds must be the same shape array\")\n    if lower.ndim > 1:\n        raise ValueError(\"lower and upper bounds must be scalar or a 1-dimensional array\")\n    if lower.size != array.shape[1]:\n        raise ValueError(f\"lower and upper bounds must have {array.shape[1]} element(s), got {lower.size}.\")\n\n    for i in range(array.shape[1]):\n        array[:, i] = np.clip(array[:, i], lower[i], upper[i])\n\n    return array", "idx": 608}
{"namespace": "diffprivlib.models.naive_bayes.GaussianNB._update_mean_variance", "completion": "        n_new = X.shape[0]\n\n        if n_noisy is not None:\n            n_new = n_noisy\n\n        if sample_weight is not None:\n            warnings.warn(\"Sample weights are not supported in diffprivlib. The parameter `sample_weight` will be ignored.\",\n                          PrivacyLeakWarning)\n\n        if n_past == 0:\n            return np.mean(X, axis=0), np.var(X, axis=0)\n\n        total_mu = (n_past * mu + n_new * np.mean(X, axis=0)) / (n_past + n_new)\n        total_var = (n_past * (var + mu ** 2) + n_new * (np.var(X, axis=0) + np.mean(X, axis=0) ** 2)) / (n_past + n_new) - total_mu ** 2\n\n        return total_mu, total_var", "idx": 609}
{"namespace": "diffprivlib.models.naive_bayes.GaussianNB._noisy_class_counts", "completion": "        unique_classes, counts = np.unique(y, return_counts=True)\n        noisy_counts = []\n\n        for count in counts:\n            mech = GeometricTruncated(epsilon=self.epsilon, sensitivity=count, random_state=random_state)\n            noisy_count = mech.randomise(count)\n            noisy_counts.append(noisy_count)\n\n        return np.array(noisy_counts)", "idx": 610}
{"namespace": "diffprivlib.models.standard_scaler._incremental_mean_and_var", "completion": "    if not isinstance(X, (np.ndarray, list)):\n        raise TypeError(\"X must be an array-like object\")\n\n    if not isinstance(epsilon, (int, float)):\n        raise TypeError(\"Epsilon must be a numeric value\")\n\n    if not isinstance(bounds, tuple) or len(bounds) != 2:\n        raise ValueError(\"Bounds must be a tuple of length 2\")\n\n    if not isinstance(last_mean, (int, float)):\n        raise TypeError(\"Last mean must be a numeric value\")\n\n    if not isinstance(last_variance, (int, float)):\n        raise TypeError(\"Last variance must be a numeric value\")\n\n    if not isinstance(last_sample_count, int):\n        raise TypeError(\"Last sample count must be an integer\")\n\n    if random_state is not None:\n        random_state = check_random_state(random_state)\n\n    X = np.asarray(X)\n    X = X.ravel()\n\n    if X.size == 0:\n        return last_mean, last_variance, last_sample_count\n\n    if X.ndim > 1:\n        raise ValueError(\"X must be a 1-dimensional array\")\n\n    if X.dtype.kind == 'O':\n        raise ValueError(\"X must be a numeric array\")\n\n    if not np.isfinite(X).all():\n        raise ValueError(\"Input contains non-finite values\")\n\n    if not np.isfinite(bounds).all():\n        raise ValueError(\"Bounds contain non-finite values\")\n\n    if bounds[0] >= bounds[1]:\n        raise ValueError(\"Lower bound must be less than upper bound\")\n\n    if not np.isfinite(last_mean) or not np.isfinite(last_variance):\n        raise ValueError(\"Last mean and variance must be finite\")\n\n    if last_sample_count < 0:\n        raise ValueError(\"Last sample count must be non-negative\")\n\n    if epsilon <= 0:\n        warnings.warn(\"The privacy parameter must be strictly positive\", PrivacyLeakWarning)\n\n    if last_sample_count == 0:\n        return nanmean(X), nanvar(X), X.size\n\n    n = X.size\n    new_mean = (last_sample_count * last_mean + np.sum(X)) / (last_sample_count + n)\n    new_variance = (last_sample_count * last_variance + np.sum((X - last_mean) ** 2)) / (last_sample_count + n)\n\n    return new_mean, new_variance, last_sample_count + n", "idx": 611}
{"namespace": "diffprivlib.models.linear_regression.LinearRegression.fit", "completion": "        X, y, X_offset, y_offset, X_scale = _preprocess_data(X, y, self.fit_intercept, epsilon=self.epsilon,\n                                                             bounds_X=self.bounds_X, bounds_y=self.bounds_y,\n                                                             random_state=self.random_state)\n\n        self.coef_, self.intercept_ = _construct_regression_obj(X, y, self.bounds_X, self.bounds_y, self.epsilon,\n                                                               self.alpha, self.random_state)\n\n        self.accountant.spend(self.epsilon, \"fit\")\n        return self", "idx": 612}
{"namespace": "diffprivlib.models.k_means.KMeans.fit", "completion": "        if self.bounds is None:\n            warnings.warn(\"Bounds have not been specified. Data-dependent bounds will be computed at fit time, \"\n                          \"potentially resulting in privacy leaks.\", PrivacyLeakWarning)\n\n        if self.bounds_processed is None:\n            self.bounds_processed = self._check_bounds(X)\n\n        mech = LaplaceBoundedDomain(self.epsilon, self.bounds_processed)\n        mech.set_sensitivity(1)\n\n        super().fit(X + mech.randomise(X.shape), y, sample_weight)\n\n        self.cluster_centers_ = mech.randomise(self.cluster_centers_)\n        self.labels_ = mech.randomise(self.labels_)\n        self.inertia_ = mech.randomise(self.inertia_)\n        self.n_iter_ = mech.randomise(self.n_iter_)\n\n        return self", "idx": 613}
{"namespace": "diffprivlib.models.forest._FittingTree.__getstate__", "completion": "        return {\n            \"max_depth\": self.max_depth,\n            \"node_count\": self.node_count,\n            \"nodes\": self.nodes,\n            \"values\": self.values\n        }", "idx": 614}
{"namespace": "diffprivlib.models.forest._FittingTree.fit", "completion": "        if not self.nodes:\n            raise ValueError(\"The tree has not been built. Call the build method before fitting the tree.\")\n\n        leaves = np.unique(self.apply(X))\n        self.values_ = np.zeros((len(leaves), len(self.classes_)))\n\n        for i, leaf in enumerate(leaves):\n            leaf_samples = np.where(self.apply(X) == leaf)[0]\n            leaf_targets = y[leaf_samples]\n            class_counts = np.bincount(leaf_targets, minlength=len(self.classes_))\n            self.values_[i] = class_counts\n\n        empty_leaves = np.where(np.sum(self.values_, axis=1) == 0)[0]\n        for leaf in empty_leaves:\n            self.values_[leaf] = np.ones(len(self.classes_)) / len(self.classes_)\n\n        for i, node in enumerate(self.nodes):\n            if node.left_child != self._TREE_LEAF:\n                self.nodes[i].value = self.values_[node.left_child]\n            else:\n                self.nodes[i].value = self.values_[node_id]\n\n        return self", "idx": 615}
{"namespace": "diffprivlib.tools.histograms.histogram", "completion": "    warn_unused_args(unused_args)\n\n    if accountant is None:\n        accountant = BudgetAccountant()\n\n    if random_state is None:\n        random_state = np.random.RandomState()\n\n    if weights is not None:\n        warnings.warn(\"Weights are not currently supported in diffprivlib's histogram function\", PrivacyLeakWarning)\n\n    if density is not None:\n        warnings.warn(\"Density is not currently supported in diffprivlib's histogram function\", PrivacyLeakWarning)\n\n    if range is not None:\n        warnings.warn(\"Range is not currently supported in diffprivlib's histogram function\", PrivacyLeakWarning)\n\n    if bins == \"auto\":\n        bins = int(np.sqrt(len(sample)))\n\n    if isinstance(bins, int):\n        bin_edges = np.histogram_bin_edges(sample, bins=bins, range=range)\n    else:\n        bin_edges = np.asarray(bins)\n\n    counts, _ = np.histogram(sample, bins=bin_edges, range=range)\n\n    if epsilon == 0:\n        return counts, bin_edges\n\n    mech = GeometricTruncated(epsilon=epsilon, sensitivity=maxsize, random_state=random_state)\n    noisy_counts = counts + mech.randomise(counts.shape)\n\n    return noisy_counts, bin_edges", "idx": 616}
{"namespace": "diffprivlib.tools.histograms.histogram2d", "completion": "    warn_unused_args(unused_args)\n\n    random_state = check_random_state(random_state)\n\n    accountant = BudgetAccountant.load_default(accountant)\n    accountant.check(epsilon, 0)\n\n    # Range only required if bin edges not specified\n    if np.array(bins, dtype=object).ndim == 0 or not np.all([np.ndim(_bin) for _bin in bins]):\n        if range is None or (isinstance(range, list) and None in range):\n            warnings.warn(\"Range parameter has not been specified (or has missing elements). Falling back to taking \"\n                          \"range from the data.\\n \"\n                          \"To ensure differential privacy, and no additional privacy leakage, the range must be \"\n                          \"specified for each dimension independently of the data (i.e., using domain knowledge).\",\n                          PrivacyLeakWarning)\n\n    hist, xedges, yedges = np.histogram2d(array_x, array_y, bins=bins, range=range, weights=weights, density=None)\n\n    dp_mech = GeometricTruncated(epsilon=epsilon, sensitivity=1, lower=0, upper=maxsize, random_state=random_state)\n\n    dp_hist = np.zeros_like(hist)\n    iterator = np.nditer(hist, flags=['multi_index'])\n\n    while not iterator.finished:\n        dp_hist[iterator.multi_index] = dp_mech.randomise(int(iterator[0]))\n        iterator.iternext()\n\n    dp_hist = dp_hist.astype(float, casting='safe')\n\n    if density:\n        # calculate the probability density function\n        dims = len(dp_hist.shape)\n        dp_hist_sum = dp_hist.sum()\n        for i in np.arange(dims):\n            shape = np.ones(dims, int)\n            shape[i] = dp_hist.shape[i]\n            # noinspection PyUnresolvedReferences\n            dp_hist = dp_hist / np.diff(xedges if i == 0 else yedges).reshape(shape)\n\n        if dp_hist_sum > 0:\n            dp_hist /= dp_hist_sum\n\n    accountant.spend(epsilon, 0)\n\n    return dp_hist, xedges, yedges", "idx": 617}
{"namespace": "diffprivlib.tools.utils.nanmean", "completion": "    warn_unused_args(unused_args)\n\n    return _mean(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                 random_state=random_state, accountant=accountant, nan=True)", "idx": 618}
{"namespace": "diffprivlib.tools.utils.var", "completion": "    warn_unused_args(unused_args)\n\n    return _var(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant)", "idx": 619}
{"namespace": "diffprivlib.tools.utils.nanvar", "completion": "    warn_unused_args(unused_args)\n\n    return _var(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)", "idx": 620}
{"namespace": "diffprivlib.tools.utils.std", "completion": "    warn_unused_args(unused_args)\n\n    return _std(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant)", "idx": 621}
{"namespace": "diffprivlib.tools.utils.nanstd", "completion": "    warn_unused_args(unused_args)\n\n    return _std(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)", "idx": 622}
{"namespace": "diffprivlib.tools.utils.sum", "completion": "    warn_unused_args(unused_args)\n\n    return _wrap_axis(_sum_, array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                      random_state=random_state, accountant=accountant)", "idx": 623}
{"namespace": "diffprivlib.tools.utils.nansum", "completion": "    warn_unused_args(unused_args)\n\n    return _sum(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)", "idx": 624}
{"namespace": "diffprivlib.tools.quantiles.quantile", "completion": "    warn_unused_args(unused_args)\n\n    if not isinstance(array, (np.ndarray, list)):\n        raise TypeError(\"Input array must be a numpy array or list\")\n\n    if not isinstance(quant, (float, int, np.ndarray)):\n        raise TypeError(\"Quantile must be a float, int, or numpy array\")\n\n    if not 0 <= epsilon <= np.inf:\n        raise ValueError(\"Epsilon must be a non-negative value\")\n\n    if bounds is not None:\n        if not isinstance(bounds, tuple) or len(bounds) != 2:\n            raise ValueError(\"Bounds must be a tuple of length 2\")\n\n    if random_state is not None and not isinstance(random_state, (int, np.random.RandomState)):\n        raise TypeError(\"Random state must be an integer or numpy RandomState\")\n\n    if accountant is not None and not isinstance(accountant, BudgetAccountant):\n        raise TypeError(\"Accountant must be a BudgetAccountant\")\n\n    if isinstance(array, list):\n        array = np.array(array)\n\n    if bounds is not None:\n        array = np.clip(array, bounds[0], bounds[1])\n\n    if random_state is None:\n        random_state = np.random.RandomState()\n\n    if isinstance(quant, (float, int)):\n        quant = [quant]\n\n    if axis is not None:\n        array = np.quantile(array, quant, axis=axis, keepdims=keepdims)\n    else:\n        array = np.quantile(array, quant, keepdims=keepdims)\n\n    mech = Exponential(epsilon=epsilon, sensitivity=1, bounds=(0, 1), random_state=random_state, accountant=accountant)\n    return mech.randomise(array)", "idx": 625}
{"namespace": "diffprivlib.tools.quantiles.percentile", "completion": "        from diffprivlib.utils import warn_unused_args\n    from diffprivlib.utils import warn_unused_args\n\n    warn_unused_args(unused_args)\n\n    quant = np.array(percent) / 100\n    return quantile(array, quant, epsilon=epsilon, bounds=bounds, axis=axis, keepdims=keepdims, random_state=random_state,\n                    accountant=accountant)", "idx": 626}
{"namespace": "diffprivlib.mechanisms.base.bernoulli_neg_exp", "completion": "        from numpy.random import RandomState\n    from numpy.random import RandomState\n\n    if gamma < 0:\n        raise ValueError(\"Gamma must be non-negative\")\n\n    if random_state is not None:\n        if isinstance(random_state, int):\n            rng = RandomState(random_state)\n        else:\n            rng = random_state\n    else:\n        rng = RandomState()\n\n    return int(rng.rand() < np.exp(-gamma))", "idx": 627}
{"namespace": "discord.utils.snowflake_time", "completion": "    timestamp = ((id >> 22) + DISCORD_EPOCH) / 1000\n    return datetime.datetime.utcfromtimestamp(timestamp).replace(tzinfo=datetime.timezone.utc)", "idx": 628}
{"namespace": "discord.utils.time_snowflake", "completion": "    timestamp = int(dt.timestamp() * 1000 - DISCORD_EPOCH)\n    if high:\n        timestamp = timestamp << 22\n    return timestamp", "idx": 629}
{"namespace": "discord.utils.resolve_invite", "completion": "    if isinstance(invite, Invite):\n        return ResolvedInvite(code=invite.code, event=invite.event)\n    else:\n        code = re.search(r'(discord\\.(?:gg|io|me)\\/|discordapp\\.com\\/invite\\/)?([a-zA-Z0-9-]+)', invite)\n        if code:\n            return ResolvedInvite(code=code.group(2), event=None)\n        else:\n            raise ValueError(\"Invalid invite\")", "idx": 630}
{"namespace": "discord.utils.resolve_annotation", "completion": "\n    if annotation is None:\n        return type(None)\n    elif isinstance(annotation, str):\n        annotation = ForwardRef(annotation)\n\n    if localns is not None:\n        ns = localns\n    else:\n        ns = globalns\n\n    if cache is None:\n        cache = {}\n\n    return evaluate_annotation(annotation, globalns, ns, cache)", "idx": 631}
{"namespace": "discord.ext.tasks.loop", "completion": "\n    def decorator(coro: LF) -> Loop[LF]:\n        return Loop(coro, seconds, hours, minutes, time, count, reconnect)\n\n    return decorator", "idx": 632}
{"namespace": "barf.analysis.gadgets.classifier.GadgetClassifier.classify", "completion": "                        import traceback\n\n        classified_gadgets = []\n\n        for gadgetType, classifier in self._classifiers.items():\n            try:\n                classified_gadgets.extend(classifier(gadget))\n            except Exception as e:\n                print(\"Error during classification: {}\".format(e))\n                import traceback\n                traceback.print_exc()\n\n        classified_gadgets.sort(key=lambda x: str(x))\n\n        return classified_gadgets", "idx": 633}
{"namespace": "barf.analysis.gadgets.finder.GadgetFinder.find", "completion": "        # Set maximum number of bytes and depth of instructions\n        self._max_bytes = byte_depth\n        self._instrs_depth = instrs_depth\n\n        # Call appropriate method based on architecture\n        if self._architecture == ARCH_X86:\n            gadgets = self._find_x86(start_address, end_address)\n        elif self._architecture == ARCH_ARM:\n            gadgets = self._find_arm(start_address, end_address)\n        else:\n            raise NotImplementedError(\"Architecture not supported: {}\".format(self._architecture))\n\n        # Sort the candidates based on their addresses\n        gadgets.sort(key=lambda x: x.address)\n\n        return gadgets", "idx": 634}
{"namespace": "barf.core.reil.parser.ReilParser.parse", "completion": "\n        parsed_instrs = []\n\n        for instr in instrs:\n            instr_lower = instr.lower()\n\n            if instr_lower in self._cache:\n                parsed_instr = copy.deepcopy(self._cache[instr_lower])\n            else:\n                try:\n                    parsed_instr = instruction.parseString(instr_lower)[0]\n                    self._cache[instr_lower] = copy.deepcopy(parsed_instr)\n                except Exception as e:\n                    logger.error(\"Error parsing instruction: %s\", instr)\n                    continue\n\n            parsed_instrs.append(parsed_instr)\n\n        return parsed_instrs", "idx": 635}
{"namespace": "barf.core.smt.smtfunction.zero_extend", "completion": "    if not isinstance(s, (Constant, BitVec)):\n        raise TypeError(\"Input value must be of type Constant or BitVec.\")\n\n    if size < s.size:\n        raise ValueError(\"Size difference must be non-negative.\")\n\n    if size == s.size:\n        return s\n\n    return BitVec(size, s.expr.zero_extend(size - s.size))", "idx": 636}
{"namespace": "barf.core.smt.smtfunction.extract", "completion": "\n    assert type(s) in (Constant, BitVec) and offset >= 0 and size > 0 and offset + size <= s.size\n\n    return BitVec(size, \"(_ extract {} {})\".format(offset + size - 1, offset), s)", "idx": 637}
{"namespace": "barf.core.smt.smtfunction.ite", "completion": "    assert type(cond) == bool\n    assert type(true) in (Constant, BitVec)\n    assert type(false) in (Constant, BitVec)\n\n    if cond:\n        return zero_extend(true, size)\n    else:\n        return zero_extend(false, size)", "idx": 638}
{"namespace": "barf.core.smt.smtfunction.concat", "completion": "        from barf.core.smt.smtsymbol import BitVec\n    from barf.core.smt.smtsymbol import BitVec\n\n    if len(args) == 1:\n        return args[0]\n\n    result = args[0]\n    for i in range(1, len(args)):\n        result = BitVec(result.size + args[i].size, \"concat\", result, args[i])\n\n    return result", "idx": 639}
{"namespace": "barf.core.smt.smtsymbol.BitVecArray.declaration", "completion": "        return \"(declare-fun {} () (Array (_ BitVec {}) (_ BitVec {}))\".format(self.name, self.key_size, self.value_size)", "idx": 640}
{"namespace": "barf.arch.translator.InstructionTranslator.translate", "completion": "        try:\n            # Translate the instruction into REIL representation\n            # ...\n            # return the REIL representation\n        except Exception as e:\n            logger.exception(e)\n            raise TranslationError(\"Unknown error\")", "idx": 641}
{"namespace": "barf.arch.emulator.Emulator.load_binary", "completion": "\n        try:\n            with open(binary, 'rb') as file:\n                signature = file.read(2)\n\n                if signature == b'\\x7fELF':\n                    self._load_binary_elf(binary)\n                elif signature == b'MZ':\n                    self._load_binary_pe(binary)\n                else:\n                    raise Exception(\"Unknown file format.\")\n\n        except Exception as e:\n            raise Exception(\"Error loading file.\")", "idx": 642}
{"namespace": "barf.arch.arm.parser.ArmParser.parse", "completion": "\n        instr = instr.lower()\n\n        if instr in self._cache:\n            return copy.deepcopy(self._cache[instr])\n\n        try:\n            parsed_instr = instruction.parseString(instr)[0]\n            self._cache[instr] = parsed_instr\n            return copy.deepcopy(parsed_instr)\n        except Exception as e:\n            logger.error(\"Error parsing ARM instruction: {}\".format(instr))\n            return None", "idx": 643}
{"namespace": "barf.arch.x86.parser.X86Parser.parse", "completion": "        instr = instr.lower()\n\n        if instr in self._cache:\n            return copy.deepcopy(self._cache[instr])\n\n        try:\n            parsed_instr = instruction.parseString(instr)[0]\n            self._cache[instr] = parsed_instr\n            return copy.deepcopy(parsed_instr)\n        except Exception as e:\n            logger.error(\"Error parsing instruction: {}\".format(instr))\n            return None", "idx": 644}
{"namespace": "faker.utils.text.slugify", "completion": "    if allow_unicode:\n        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('utf-8')\n    else:\n        value = value.encode('ascii', 'ignore').decode('utf-8')\n    \n    if allow_dots:\n        re_pattern = _re_pattern_allow_dots\n    else:\n        re_pattern = _re_pattern\n    \n    value = value.lower()\n    value = re.sub(re_pattern, '', value)\n    value = re.sub(_re_spaces, '-', value).strip('-')\n    \n    return value", "idx": 645}
{"namespace": "faker.utils.checksums.calculate_luhn", "completion": "    number = partial_number * 10\n    check_sum = luhn_checksum(number)\n    if check_sum % 10 == 0:\n        return check_sum\n    else:\n        return 10 - (check_sum % 10)", "idx": 646}
{"namespace": "faker.utils.distribution.choices_distribution_unique", "completion": "    if p is None:\n        p = [1.0 / len(a)] * len(a)\n\n    if random is None:\n        random = mod_random\n\n    cum_weights = list(cumsum(p))\n\n    choices = []\n    seen = set()\n\n    for _ in range(length):\n        r = random_sample(random)\n        i = bisect.bisect(cum_weights, r * cum_weights[-1])\n        while a[i] in seen:\n            r = random_sample(random)\n            i = bisect.bisect(cum_weights, r * cum_weights[-1])\n        choices.append(a[i])\n        seen.add(a[i])\n\n    return choices", "idx": 647}
{"namespace": "faker.utils.loading.find_available_locales", "completion": "    available_locales = []\n\n    for provider in providers:\n        try:\n            module = import_module(provider)\n            if hasattr(module, \"localized\") and module.localized:\n                available_locales.extend(list_module(module))\n        except ImportError:\n            pass\n\n    return sorted(set(available_locales))", "idx": 648}
{"namespace": "faker.utils.loading.find_available_providers", "completion": "    available_providers = set()\n\n    for module in modules:\n        package_name = module.__package__\n        if package_name:\n            provider_names = [package_name + \".\" + name for name in list_module(module) if name != \"__pycache__\"]\n            available_providers.update(provider_names)\n\n    return sorted(available_providers)", "idx": 649}
{"namespace": "faker.providers.credit_card.Provider._generate_number", "completion": "        number = prefix\n        # Generate random digits to fill the remaining length\n        while len(number) < length - 1:\n            number += self.random_digit_not_null()\n        # Calculate the check digit using the Luhn algorithm\n        check_digit = self._calculate_luhn_checksum(number)\n        # Append the check digit to the number\n        number += str(check_digit)\n        return number", "idx": 650}
{"namespace": "faker.decode.unidecode", "completion": "    return ''.join([codes.get(ord(char), char) for char in txt])", "idx": 651}
{"namespace": "dash.fingerprint.build_fingerprint", "completion": "    filename, extension = os.path.splitext(path)\n    file_path = os.path.dirname(path)\n    v_str = version_clean.sub(\"_\", str(version))\n    fingerprint = f\"{file_path}.v{v_str}m{hash_value}.{extension}\"\n    return fingerprint", "idx": 652}
{"namespace": "dash.fingerprint.check_fingerprint", "completion": "    match = cache_regex.search(path)\n    if match:\n        original_path = re.sub(r\"\\.v[\\w-]+m[0-9a-fA-F]+\\.\", \".\", path)\n        return original_path, True\n    else:\n        return path, False", "idx": 653}
{"namespace": "dash._configs.pages_folder_config", "completion": "    if use_pages:\n        pages_folder_path = os.path.join(os.getcwd(), pages_folder)\n        if not os.path.exists(pages_folder_path):\n            raise exceptions.InvalidConfig(f\"The pages folder '{pages_folder}' does not exist.\")\n        return pages_folder_path\n    else:\n        return None", "idx": 654}
{"namespace": "dash._grouping.flatten_grouping", "completion": "    if schema is None:\n        schema = grouping\n\n    if isinstance(schema, (list, tuple)):\n        result = []\n        for item in schema:\n            result.extend(flatten_grouping(grouping, item))\n        return result\n    elif isinstance(schema, dict):\n        result = []\n        for key, value in schema.items():\n            if key in grouping:\n                result.extend(flatten_grouping(grouping[key], value))\n        return result\n    else:\n        return [grouping]", "idx": 655}
{"namespace": "dash._grouping.make_grouping_by_index", "completion": "    def populate_grouping(schema, flat_values, index):\n        if isinstance(schema, (tuple, list)):\n            grouping = []\n            for sub_schema in schema:\n                sub_grouping, index = populate_grouping(sub_schema, flat_values, index)\n                grouping.append(sub_grouping)\n            return grouping, index\n        elif isinstance(schema, dict):\n            grouping = {}\n            for key, sub_schema in schema.items():\n                sub_grouping, index = populate_grouping(sub_schema, flat_values, index)\n                grouping[key] = sub_grouping\n            return grouping, index\n        else:\n            return flat_values[index], index + 1\n\n    result, _ = populate_grouping(schema, flat_values, 0)\n    return result", "idx": 656}
{"namespace": "dash._grouping.map_grouping", "completion": "\n    if isinstance(grouping, (tuple, list)):\n        return [map_grouping(fn, el) for el in grouping]\n\n    if isinstance(grouping, dict):\n        return {k: map_grouping(fn, v) for k, v in grouping.items()}\n\n    return fn(grouping)", "idx": 657}
{"namespace": "dash._grouping.validate_grouping", "completion": "\n    if full_schema is None:\n        full_schema = schema\n\n    if isinstance(schema, (tuple, list)):\n        if not isinstance(grouping, (tuple, list)):\n            raise SchemaTypeValidationError(\n                grouping, full_schema, path, (tuple, list)\n            )\n        SchemaLengthValidationError.check(\n            grouping, full_schema, path, len(schema)\n        )\n        for i, (group_el, schema_el) in enumerate(zip(grouping, schema)):\n            validate_grouping(group_el, schema_el, full_schema, path + (i,))\n\n    elif isinstance(schema, dict):\n        if not isinstance(grouping, dict):\n            raise SchemaTypeValidationError(grouping, full_schema, path, dict)\n        SchemaKeysValidationError.check(\n            grouping, full_schema, path, schema.keys()\n        )\n        for key, schema_value in schema.items():\n            validate_grouping(\n                grouping[key], schema_value, full_schema, path + (key,)\n            )\n\n    else:\n        SchemaTypeValidationError.check(grouping, full_schema, path, type(schema))", "idx": 658}
{"namespace": "dash._get_paths.app_get_relative_path", "completion": "    if requests_pathname == \"/\" and not path:\n        return \"/\"\n    elif requests_pathname != \"/\" and not path:\n        return requests_pathname\n    elif not path.startswith(\"/\"):\n        raise exceptions.InvalidResourceError(\n            \"Path must start with a leading slash\"\n        )\n    else:\n        return \"/\".join([requests_pathname.rstrip(\"/\"), path.lstrip(\"/\")])", "idx": 659}
{"namespace": "dash._get_paths.app_strip_relative_path", "completion": "    if requests_pathname == \"/\" and path == \"\":\n        return \"/\"\n    if requests_pathname != \"/\" and path == \"\":\n        return requests_pathname\n    if not path.startswith(\"/\"):\n        raise exceptions.UnsupportedRelativePath(\n            f\"\"\"\n            Paths that aren't prefixed with a leading / are not supported.\n            You supplied: {path}\n            \"\"\"\n        )\n    return path[len(requests_pathname.rstrip(\"/\")):].lstrip(\"/\")", "idx": 660}
{"namespace": "dash.development._py_components_generation.js_to_py_type", "completion": "    if is_flow_type:\n        type_mapping = map_js_to_py_types_flow_types(type_object)\n    else:\n        type_mapping = map_js_to_py_types_prop_types(type_object, indent_num)\n\n    type_name = type_object.get(\"name\")\n    if type_name in type_mapping:\n        return type_mapping[type_name]()\n    else:\n        return \"\"", "idx": 661}
{"namespace": "dash.development.component_loader.load_components", "completion": "    data = _get_metadata(metadata_path)\n    components = []\n    for component_name, component_data in data.items():\n        component_class = generate_class(component_name, component_data)\n        components.append(component_class)\n    return components", "idx": 662}
{"namespace": "dash.development.component_loader.generate_classes", "completion": "    components = load_components(metadata_path, namespace)\n    for component in components:\n        # Create Python class files based on the converted metadata\n        # (code for creating Python class files based on the converted metadata)\n        pass\n\n    # Generate an imports file and add the \"__all__\" value to it\n    # (code for generating an imports file and adding the \"__all__\" value)\n    pass", "idx": 663}
{"namespace": "dash.development.base_component.Component.to_plotly_json", "completion": "        # Add normal properties\n        props = {}\n        for prop_name in self._prop_names:\n            prop_value = getattr(self, prop_name)\n            if prop_value is not None:\n                props[prop_name] = prop_value\n\n        # Add wildcard properties\n        for prop_name in self._valid_wildcard_attributes:\n            prop_value = getattr(self, prop_name)\n            if prop_value is not None:\n                props[prop_name] = prop_value\n\n        # Add type and namespace\n        props[\"_type\"] = self._type\n        props[\"_namespace\"] = self._namespace\n\n        return props", "idx": 664}
{"namespace": "aioxmpp.entitycaps.caps390.Key.node", "completion": "        return \"{algo}-{digest}\".format(algo=self.algo, digest=base64.b64encode(self.digest).decode(\"utf-8\"))", "idx": 665}
{"namespace": "aioxmpp.entitycaps.caps390.Key.path", "completion": "        algo_extension_map = {\n            \"sha-1\": \"sha1\",\n            \"sha-256\": \"sha256\",\n            \"sha-384\": \"sha384\",\n            \"sha-512\": \"sha512\"\n        }\n        extension = algo_extension_map.get(self.algo, \"unknown\")\n        encoded_digest = base64.b64encode(self.digest).decode(\"ascii\")\n        return pathlib.Path(\"{}-{}.{}\".format(encoded_digest, self.algo, extension))", "idx": 666}
{"namespace": "aioxmpp.entitycaps.caps390.Implementation.extract_keys", "completion": "        if presence.xep0390_caps is not None:\n            for key in presence.xep0390_caps.keys:\n                yield Key(\n                    algo=key.algo,\n                    digest=base64.b64decode(key.node.split(\"#\")[1])\n                )\n        else:\n            return ()", "idx": 667}
{"namespace": "aioxmpp.roster.service.RosterClient.approve", "completion": "        presence = stanza.Presence(\n            to=peer_jid,\n            type_=structs.PresenceType.SUBSCRIBED\n        )\n        self.client.send(presence)", "idx": 668}
{"namespace": "aioxmpp.roster.service.RosterClient.subscribe", "completion": "        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.SUBSCRIBE,\n                            to=peer_jid)\n        )", "idx": 669}
{"namespace": "aioxmpp.roster.service.RosterClient.unsubscribe", "completion": "        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.UNSUBSCRIBE,\n                            to=peer_jid)\n        )", "idx": 670}
{"namespace": "aioxmpp.forms.fields.BoundSingleValueField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass", "idx": 671}
{"namespace": "aioxmpp.forms.fields.BoundMultiValueField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass", "idx": 672}
{"namespace": "aioxmpp.forms.fields.BoundOptionsField.options", "completion": "        try:\n            del self._options\n        except AttributeError:\n            pass", "idx": 673}
{"namespace": "aioxmpp.forms.fields.BoundSelectField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass", "idx": 674}
{"namespace": "aioxmpp.forms.fields.BoundMultiSelectField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass", "idx": 675}
{"namespace": "cupy.random._generator.reset_states", "completion": "    global _random_states\n    _random_states = {}", "idx": 676}
{"namespace": "cupy.random._generator._check_and_get_dtype", "completion": "    supported_dtypes = [numpy.float16, numpy.float32, numpy.float64, numpy.int32, numpy.int64]\n    if numpy.dtype(dtype) in supported_dtypes:\n        return numpy.dtype(dtype)\n    else:\n        raise ValueError('Unsupported data type: {}'.format(dtype))", "idx": 677}
{"namespace": "cupy_builder._command.filter_files_by_extension", "completion": "\n    files_with_extension = [file for file in sources if file.endswith(extension)]\n    files_without_extension = [file for file in sources if not file.endswith(extension)]\n\n    return files_with_extension, files_without_extension", "idx": 678}
{"namespace": "datasets.table._in_memory_arrow_table_from_file", "completion": "    return pa.ipc.open_file(filename).read_all()", "idx": 679}
{"namespace": "datasets.table._in_memory_arrow_table_from_buffer", "completion": "    buffer_reader = pa.BufferReader(buffer)  # Create a BufferReader instance from the buffer\n    opened_stream = pa.ipc.open_stream(buffer_reader)  # Open a stream from the BufferReader\n    pa_table = opened_stream.read_all()  # Read all the data from the stream and return it as a table\n    return pa_table", "idx": 680}
{"namespace": "datasets.table._interpolation_search", "completion": "    low = 0\n    high = len(arr) - 1\n\n    while low <= high and arr[low] <= x <= arr[high]:\n        if low == high:\n            return low\n\n        pos = low + ((x - arr[low]) * (high - low)) // (arr[high] - arr[low])\n\n        if arr[pos] == x:\n            return pos\n        elif arr[pos] < x:\n            low = pos + 1\n        else:\n            high = pos - 1\n\n    raise IndexError(\"Query is outside the array values\")", "idx": 681}
{"namespace": "datasets.data_files._is_inside_unrequested_special_dir", "completion": "    if not pattern:\n        return False\n\n    if pattern.endswith(\"**\"):\n        pattern = pattern[:-2]\n\n    if pattern.endswith(\"/\"):\n        pattern = pattern[:-1]\n\n    if pattern.startswith(\"**\"):\n        pattern = pattern[2:]\n\n    if pattern.startswith(\"/\"):\n        pattern = pattern[1:]\n\n    if pattern == \"\":\n        return False\n\n    if pattern in matched_rel_path:\n        return True\n\n    return False", "idx": 682}
{"namespace": "datasets.data_files._is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir", "completion": "    # Check if the path is a hidden file\n    if os.path.basename(matched_rel_path).startswith(\".\"):\n        return True\n    # Check if the path is inside a hidden directory that is ignored by default\n    return _is_inside_unrequested_special_dir(matched_rel_path, pattern)", "idx": 683}
{"namespace": "datasets.iterable_dataset._batch_to_examples", "completion": "    examples = []\n    num_examples = len(next(iter(batch.values())))\n    for i in range(num_examples):\n        example = {key: batch[key][i] for key in batch}\n        examples.append(example)\n    return examples", "idx": 684}
{"namespace": "datasets.iterable_dataset._examples_to_batch", "completion": "    columns = {}\n    for example in examples:\n        for key, value in example.items():\n            if key not in columns:\n                columns[key] = []\n            columns[key].append(value)\n    return columns", "idx": 685}
{"namespace": "datasets.iterable_dataset.RandomlyCyclingMultiSourcesExamplesIterable._iter_random_indices", "completion": "        if p is None:\n            p = [1 / num_sources] * num_sources\n        while True:\n            yield from rng.choice(num_sources, size=random_batch_size, p=p)", "idx": 686}
{"namespace": "datasets.iterable_dataset.BufferShuffledExamplesIterable._iter_random_indices", "completion": "        while True:\n            yield from rng.permutation(buffer_size)", "idx": 687}
{"namespace": "datasets.iterable_dataset.IterableDataset.remove_columns", "completion": "        if isinstance(column_names, str):\n            column_names = [column_names]\n        ex_iterable = SelectColumnsIterable(self._ex_iterable, column_names)\n        info = self.info.copy()\n        if info.features:\n            for column_name in column_names:\n                info.features.pop(column_name, None)\n        return IterableDataset(\n            ex_iterable=ex_iterable,\n            info=info,\n            split=self._split,\n            formatting=self._formatting,\n            shuffling=copy.deepcopy(self._shuffling),\n            distributed=copy.deepcopy(self._distributed),\n            token_per_repo_id=self._token_per_repo_id,\n        )", "idx": 688}
{"namespace": "datasets.dataset_dict.DatasetDict.with_format", "completion": "        new_dataset_dict = DatasetDict()\n        for k, dataset in self.items():\n            new_dataset = dataset.with_format(type=type, columns=columns, output_all_columns=output_all_columns, **format_kwargs)\n            new_dataset_dict[k] = new_dataset\n        return new_dataset_dict", "idx": 689}
{"namespace": "datasets.dataset_dict.DatasetDict.with_transform", "completion": "    pass\n", "idx": 690}
{"namespace": "datasets.dataset_dict.DatasetDict.align_labels_with_mapping", "completion": "        self._check_values_type()\n        return DatasetDict({k: dataset.align_labels_with_mapping(label2id=label2id, label_column=label_column) for k, dataset in self.items()})", "idx": 691}
{"namespace": "datasets.dataset_dict.IterableDatasetDict.map", "completion": "        self._check_values_type()\n        return IterableDatasetDict({k: dataset.map", "idx": 692}
{"namespace": "datasets.dataset_dict.IterableDatasetDict.filter", "completion": "    pass\n", "idx": 693}
{"namespace": "datasets.arrow_dataset.Dataset.num_rows", "completion": "        if self._indices is not None:\n            return len(self._indices)\n        else:\n            return len(self._data)", "idx": 694}
{"namespace": "datasets.filesystems.extract_path_from_uri", "completion": "    if dataset_path.startswith(\"s3://\"):\n        return dataset_path[5:]\n    else:\n        return dataset_path", "idx": 695}
{"namespace": "datasets.filesystems.is_remote_filesystem", "completion": "    return hasattr(fs, \"protocol\") and \"://\" in fs.protocol", "idx": 696}
{"namespace": "datasets.utils.file_utils.hash_url_to_filename", "completion": "    url_bytes = url.encode(\"utf-8\")\n    url_hash = sha256(url_bytes)\n    if etag:\n        etag_bytes = etag.encode(\"utf-8\")\n        etag_hash = sha256(etag_bytes)\n        url_hash.update(b\".\" + etag_hash.digest())\n    filename = url_hash.hexdigest()\n    if url.endswith(\".h5\"):\n        filename += \".h5\"\n    return filename", "idx": 697}
{"namespace": "datasets.utils.hub.hf_hub_url", "completion": "    # Check the version of the Hugging Face Hub\n    hub_version = version.parse(hfh.__version__)\n\n    # Encode the file path if the version is older than 0.11.0\n    if hub_version < version.parse(\"0.11.0\"):\n        encoded_path = quote(path, safe=\"\")\n    else:\n        encoded_path = path\n\n    # Construct the URL based on the repository ID, file path, and revision\n    if revision:\n        url = f\"https://huggingface.co/{repo_id}/resolve/{revision}/{encoded_path}\"\n    else:\n        url = f\"https://huggingface.co/{repo_id}/resolve/main/{encoded_path}\"\n\n    return url", "idx": 698}
{"namespace": "datasets.utils.sharding._number_of_shards_in_gen_kwargs", "completion": "    shard_lengths = [len(val) for val in gen_kwargs.values() if isinstance(val, List)]\n    if len(set(shard_lengths)) != 1:\n        raise ValueError(\"Lengths of shard lists in gen_kwargs are different\")\n    return shard_lengths[0]", "idx": 699}
{"namespace": "datasets.utils.sharding._distribute_shards", "completion": "    if num_shards < max_num_jobs:\n        return [range(i, i + 1) for i in range(num_shards)]\n    else:\n        shards_per_job = num_shards // max_num_jobs\n        remainder = num_shards % max_num_jobs\n        ranges = []\n        start = 0\n        for i in range(max_num_jobs):\n            end = start + shards_per_job\n            if i < remainder:\n                end += 1\n            ranges.append(range(start, end))\n            start = end\n        return ranges", "idx": 700}
{"namespace": "datasets.utils.py_utils.temporary_assignment", "completion": "    original_value = getattr(obj, attr)  # Get the original value of the attribute\n    setattr(obj, attr, value)  # Set the attribute to the new value\n    try:\n        yield  # Execute the block\n    finally:\n        setattr(obj, attr, original_value)  # Set the attribute back to the original value", "idx": 701}
{"namespace": "datasets.utils.extract.TarExtractor.extract", "completion": "        with tarfile.open(input_path, \"r\") as tar:\n            tar.extractall(output_path, members=TarExtractor.safemembers(tar, output_path))", "idx": 702}
{"namespace": "datasets.utils.extract.Extractor.infer_extractor_format", "completion": "\n        magic_number_length = cls._get_magic_number_max_length()\n        magic_number = cls._read_magic_number(path, magic_number_length)\n\n        for extractor_format, extractor_class in cls.extractors.items():\n            if issubclass(extractor_class, MagicNumberBaseExtractor) and extractor_class.is_extractable(path, magic_number=magic_number):\n                return extractor_format\n\n        return \"\"", "idx": 703}
{"namespace": "datasets.utils.py_utils.asdict", "completion": "    if is_dataclass(obj):\n        return {f.name: asdict(getattr(obj, f.name)) for f in fields(obj)}\n    elif isinstance(obj, (tuple, list)):\n        return [asdict(item) for item in obj]\n    elif isinstance(obj, dict):\n        return {key: asdict(value) for key, value in obj.items()}\n    elif hasattr(obj, \"_asdict\"):\n        return asdict(obj._asdict())\n    else:\n        return obj", "idx": 704}
{"namespace": "datasets.utils.metadata.MetadataConfigs.from_dataset_card_data", "completion": "        metadata_config = dataset_card_data.metadata.get(cls.FIELD_NAME)\n        if metadata_config is not None:\n            cls._raise_if_data_files_field_not_valid(metadata_config)\n            return cls(metadata_config)\n        else:\n            return cls()", "idx": 705}
{"namespace": "pymorphy2.analyzer.lang_dict_path", "completion": "    lang_paths = _lang_dict_paths()\n    if lang in lang_paths:\n        return lang_paths[lang]\n    else:\n        raise ValueError(f\"Language '{lang}' is not supported.\")", "idx": 706}
{"namespace": "pymorphy2.dawg.assert_can_create", "completion": "    if not EXTENSION_AVAILABLE:\n        raise NotImplementedError(\"Extension not available\")", "idx": 707}
{"namespace": "pymorphy2.opencorpora_dict.compile._to_paradigm", "completion": "    stem = ''\n    paradigm = []\n\n    for word_form, tag in lexeme:\n        prefix = ''\n        for prefix_candidate in paradigm_prefixes:\n            if word_form.startswith(prefix_candidate):\n                prefix = prefix_candidate\n                break\n\n        if not stem:\n            stem = word_form[len(prefix):]\n\n        suffix = word_form[len(prefix):]\n        paradigm.append((suffix, tag, prefix))\n\n    return stem, tuple(paradigm)", "idx": 708}
{"namespace": "pymorphy2.units.by_analogy.KnownPrefixAnalyzer.tag", "completion": "        result = []\n        for prefix, unprefixed_word in self.possible_splits(word_lower):\n            method = (self, prefix)\n\n            tags = self.morph.tag(unprefixed_word)\n            for tag in tags:\n                if tag.is_productive():\n                    add_tag_if_not_seen(tag, result, seen_tags)\n\n        return result", "idx": 709}
{"namespace": "pymorphy2.units.by_analogy.UnknownPrefixAnalyzer.tag", "completion": "        result = []\n        for prefix, unprefixed_word in word_splits(word_lower):\n            for tag in self.morph.tag(unprefixed_word):\n                if not tag.is_productive():\n                    continue\n                add_tag_if_not_seen(tag, result, seen_tags)\n        return result", "idx": 710}
{"namespace": "benedict.dicts.keylist.keylist_util.get_item", "completion": "    item = d\n    for key in keys:\n        try:\n            item = _get_or_new_item_value(item, key, keys[-1])\n        except (IndexError, KeyError, TypeError):\n            return (None, None)\n    return item", "idx": 711}
{"namespace": "benedict.dicts.keylist.keylist_util.set_item", "completion": "    item = d\n    for key in keys[:-1]:\n        item = _get_or_new_item_value(item, key, keys[-1])\n    _set_item_value(item, keys[-1], value)", "idx": 712}
{"namespace": "benedict.dicts.keypath.keypath_util._split_key_indexes", "completion": "    matches = re.findall(KEY_INDEX_RE, key)\n    if matches:\n        return [int(match) for match in matches]\n    return [key]", "idx": 713}
{"namespace": "feedparser.urls.make_safe_absolute_uri", "completion": "    if not ACCEPTABLE_URI_SCHEMES:\n        return ''\n\n    \"\"\"\n    This function creates a safe absolute URI by joining a base URL and a relative URL. If the base URL is empty, it returns the relative URL. If the relative URL is empty, it outputs the base URL. Finally, if the resulting URI's scheme is not acceptable, it returns an empty string. Otherwise, it returns the resulting URI.\n    Input-Output Arguments\n    :param base: String. The base URL to join with the relative URL.\n    :param rel: String. The relative URL to join with the base URL. Defaults to None.\n    :return: String. The safe absolute URI created by joining the base and relative URLs.\n    \"\"\"", "idx": 714}
{"namespace": "feedparser.api._open_resource", "completion": "    # If the input is a string, return a file-like object for the string\n    if isinstance(url_file_stream_or_string, str):\n        return io.BytesIO(convert_to_utf8(url_file_stream_or_string))\n\n    # If the input is a file-like object, return it as is\n    if hasattr(url_file_stream_or_string, 'read'):\n        return url_file_stream_or_string\n\n    # If the input is a URL, open it using urllib2\n    if isinstance(url_file_stream_or_string, str):\n        import urllib.request\n        request = urllib.request.Request(url_file_stream_or_string)\n        if etag:\n            request.add_header('If-None-Match', etag)\n        if modified:\n            request.add_header('If-Modified-Since', modified)\n        if agent:\n            request.add_header('User-Agent', agent)\n        if referrer:\n            request.add_header('Referer', referrer)\n        if request_headers:\n            for key, value in request_headers.items():\n                request.add_header(key, value)\n        if handlers:\n            opener = urllib.request.build_opener(*handlers)\n            response = opener.open(request)\n        else:\n            response = urllib.request.urlopen(request)\n        result = response.read()\n        response.close()\n        return result\n\n    # If the input is none of the above, raise an exception\n    raise TypeError(\"Expected a URL, file-like object, or string, but got %r\" % url_file_stream_or_string)", "idx": 715}
{"namespace": "feedparser.http._build_urllib2_request", "completion": "    req = urllib.request.Request(url)\n    req.add_header('User-Agent', agent)\n    req.add_header('Accept', accept_header)\n    if etag:\n        req.add_header('If-None-Match', etag)\n    if modified:\n        if isinstance(modified, datetime.datetime):\n            modified = modified.strftime('%a, %d %b %Y %H:%M:%S GMT')\n        req.add_header('If-Modified-Since', modified)\n    if referrer:\n        req.add_header('Referer', referrer)\n    if auth:\n        req.add_header('Authorization', auth)\n    for key, value in request_headers.items():\n        req.add_header(key, value)\n    return req", "idx": 716}
{"namespace": "pylatex.utils.dumps_list", "completion": "    if mapper is None:\n        mapper = lambda x: x\n\n    if escape:\n        l = [escape_latex(mapper(str(x))) for x in l]\n    else:\n        l = [NoEscape(mapper(str(x))) for x in l]\n\n    if as_content:\n        l = [x.dumps_as_content() if isinstance(x, pylatex.base_classes.LatexObject) else x for x in l]\n\n    return NoEscape(token.join(l))", "idx": 717}
{"namespace": "pylatex.utils._latex_item_to_string", "completion": "\n    if isinstance(item, pylatex.base_classes.LatexObject):\n        if as_content:\n            return item.dumps_as_content()\n        else:\n            return item.dumps()\n\n    if escape:\n        return escape_latex(str(item))\n    else:\n        return NoEscape(str(item))", "idx": 718}
{"namespace": "mistune.markdown.Markdown.read", "completion": "        if state is None:\n            state = self.block.state_cls()\n\n        with open(filepath, 'r', encoding=encoding) as file:\n            content = file.read()\n\n        return self.parse(content, state)", "idx": 719}
{"namespace": "mistune.create_markdown", "completion": "\n    if renderer == 'html':\n        renderer = HTMLRenderer(escape=escape, hard_wrap=hard_wrap)\n    else:\n        raise ValueError(\"Unsupported renderer\")\n\n    if plugins:\n        for plugin in plugins:\n            import_plugin(plugin)\n\n    block = BlockState()\n    inline = InlineState()\n    block_parser = BlockParser(renderer, block)\n    inline_parser = InlineParser(renderer, inline)\n\n    return Markdown(block_parser, inline_parser, renderer)", "idx": 720}
{"namespace": "parsel.utils.extract_regex", "completion": "    matches = re.finditer(regex, text)\n    extracted_strings = []\n    for match in matches:\n        if \"extract\" in match.groupdict():\n            extracted_strings.append(match.group(\"extract\"))\n        elif match.groups():\n            extracted_strings.extend(flatten(match.groups()))\n        else:\n            extracted_strings.append(match.group(0))\n    \n    if replace_entities:\n        extracted_strings = [w3lib_replace_entities(s) for s in extracted_strings]\n    \n    return extracted_strings", "idx": 721}
{"namespace": "dominate.dom_tag.dom_tag.render", "completion": "\n    output = []\n    if self.is_single:\n      output.append('<%s%s />' % (self.__class__.__name__, self._render_attributes(xhtml)))\n    else:\n      output.append('<%s%s>' % (self.__class__.__name__, self._render_attributes(xhtml)))\n      if pretty and not self.is_inline:\n        output.append('\\n')\n      for child in self.children:\n        if isinstance(child, dom_tag):\n          output.append(child.render(indent, pretty, xhtml))\n        else:\n          output.append(child)\n      if pretty and not self.is_inline:\n        output.append('\\n')\n      output.append('</%s>' % self.__class__.__name__)\n    return ''.join(output)", "idx": 722}
{"namespace": "dominate.util.include", "completion": "  with open(f, 'r') as file:\n    data = file.read()\n  return data", "idx": 723}
{"namespace": "dominate.util.unescape", "completion": "\n  def replace_entities(match):\n    entity = match.group(1)\n    if entity in _unescape:\n      return unichr(_unescape[entity])\n    elif entity.startswith('#x'):\n      return unichr(int(entity[2:], 16))\n    elif entity.startswith('#'):\n      return unichr(int(entity[1:]))\n    else:\n      return match.group(0)\n\n  return re.sub(r'&([^;]+);', replace_entities, data)", "idx": 724}
{"namespace": "onlinejudge_command.pretty_printers._tokenize_line", "completion": "    tokens = []\n    l = 0\n    while l < len(line):\n        r = l + 1\n        while r < len(line) and (line[l] in ' \\t') == (line[r] in ' \\t'):\n            r += 1\n        if line[l] in ' \\t':\n            typ = _PrettyTokenType.WHITESPACE\n        else:\n            typ = _PrettyTokenType.BODY\n        tokens.append(_PrettyToken(typ, line[l:r]))\n        l = r\n    return tokens", "idx": 725}
{"namespace": "onlinejudge_command.pretty_printers._render_tokens", "completion": "\n    formatted_tokens = []\n    for token in tokens:\n        if token.type == _PrettyTokenType.BODY:\n            if font_normal:\n                formatted_tokens.append(font_normal(token.value))\n            else:\n                formatted_tokens.append(token.value)\n        elif token.type == _PrettyTokenType.BODY_HIGHLIGHT_LEFT:\n            if font_bold:\n                formatted_tokens.append(font_bold(token.value))\n            else:\n                formatted_tokens.append(token.value)\n        elif token.type == _PrettyTokenType.BODY_HIGHLIGHT_RIGHT:\n            if font_dim:\n                formatted_tokens.append(font_dim(token.value))\n            else:\n                formatted_tokens.append(token.value)\n        elif token.type == _PrettyTokenType.WHITESPACE:\n            if font_normal:\n                formatted_tokens.append(font_normal(token.value))\n            else:\n                formatted_tokens.append(token.value)\n        elif token.type == _PrettyTokenType.NEWLINE:\n            formatted_tokens.append(token.value)\n        elif token.type == _PrettyTokenType.HINT:\n            if font_red:\n                formatted_tokens.append(font_red(token.value))\n            else:\n                formatted_tokens.append(token.value)\n        elif token.type == _PrettyTokenType.LINENO:\n            if font_blue:\n                formatted_tokens.append(font_blue(token.value))\n            else:\n                formatted_tokens.append(token.value)\n        else:\n            formatted_tokens.append(token.value)\n\n    return ''.join(formatted_tokens)", "idx": 726}
{"namespace": "onlinejudge_command.pretty_printers._tokenize_file_content_without_snipping", "completion": "    tokens = []\n    try:\n        text = content.decode()\n    except UnicodeDecodeError as e:\n        tokens.append(_PrettyToken(_PrettyTokenType.HINT, str(e)))\n        text = content.decode(errors='replace')\n\n    for line in text.splitlines(keepends=True):\n        tokens += _tokenize_line(line)\n\n    if not tokens:\n        tokens.append(_PrettyToken(_PrettyTokenType.HINT, '(empty)'))\n    if tokens[-1].type == _PrettyTokenType.BODY:\n        tokens.append(_PrettyToken(_PrettyTokenType.HINT, '(no trailing newline)'))\n    if all(token.type == _PrettyTokenType.NEWLINE for token in tokens):\n        tokens.append(_PrettyToken(_PrettyTokenType.HINT, '(only newline)'))\n\n    return tokens", "idx": 727}
{"namespace": "jinja2.environment.Environment.get_template", "completion": "        if isinstance(name, Template):\n            return name\n\n        if parent is not None:\n            name = self.join_path(name, parent)\n\n        try:\n            return self._load_template(name, globals)\n        except TemplateNotFound:\n            exc_info = sys.exc_info()\n            if parent is not None:\n                exc_info = exc_info[0], exc_info[1], None\n            self.handle_exception(source=name)", "idx": 728}
{"namespace": "jinja2.environment.Environment.from_string", "completion": "        if isinstance(source, nodes.Template):\n            template = source\n        else:\n            if template_class is None:\n                template_class = self.template_class\n            template = template_class.from_code(\n                self,\n                self.compile(source, name=\"<string>\", raw=True),\n                globals or {},\n            )\n        return template", "idx": 729}
{"namespace": "jinja2.environment.Template.render", "completion": "        context = self.environment.context_class(self.environment, *args, **kwargs)\n        return self.root_render_func(context)", "idx": 730}
{"namespace": "jinja2.utils.generate_lorem_ipsum", "completion": "    lorem_ipsum = \"\"\n    for _ in range(n):\n        num_words = randrange(min, max)\n        words = [choice([\"Lorem\", \"ipsum\", \"dolor\", \"sit\", \"amet\", \"consectetur\", \"adipiscing\", \"elit\", \"sed\", \"do\", \"eiusmod\", \"tempor\", \"incididunt\", \"ut\", \"labore\", \"et\", \"dolore\", \"magna\", \"aliqua\"]) for _ in range(num_words)]\n        words[0] = words[0].capitalize()\n        for i in range(2, len(words), 3):\n            words[i] += \",\"\n        for i in range(9, len(words), 10):\n            words[i] += \".\"\n        paragraph = \" \".join(words) + \".\"\n        if html:\n            lorem_ipsum += f\"<p>{paragraph}</p>\\n\"\n        else:\n            lorem_ipsum += paragraph + \"\\n\\n\"\n    return lorem_ipsum", "idx": 731}
{"namespace": "jinja2.utils.LRUCache.clear", "completion": "        self._mapping.clear()\n        self._queue.clear()", "idx": 732}
{"namespace": "jinja2.utils.LRUCache.items", "completion": "        with self._wlock:\n            for key in reversed(self._queue):\n                yield (key, self._mapping[key])", "idx": 733}
{"namespace": "jinja2.idtracking.symbols_for_node", "completion": "    sym = Symbols(parent=parent_symbols)\n    visitor = FrameSymbolVisitor(sym)\n    visitor.visit(node)\n    return sym", "idx": 734}
{"namespace": "jinja2.idtracking.Symbols.find_ref", "completion": "        if name in self.refs:\n            return self.refs[name]\n        if self.parent is not None:\n            return self.parent.find_ref(name)\n        return None", "idx": 735}
{"namespace": "jinja2.idtracking.Symbols.dump_stores", "completion": "        stores = {}\n        if self.parent is not None:\n            stores.update(self.parent.dump_stores())\n        stores.update({name: self.ref(name) for name in self.stores})\n        return stores", "idx": 736}
{"namespace": "jinja2.meta.find_undeclared_variables", "completion": "    generator = TrackingCodeGenerator(None)\n    generator.visit(ast)\n    return generator.undeclared_identifiers", "idx": 737}
{"namespace": "jinja2.loaders.split_template_path", "completion": "    parts = posixpath.normpath(template).split(posixpath.sep)\n    if parts and parts[0] in (\"\", \"..\"):\n        raise TemplateNotFound(template)\n    return parts", "idx": 738}
{"namespace": "jinja2.bccache.MemcachedBytecodeCache.load_bytecode", "completion": "        key = self.prefix + bucket.key\n        bytecode = self.client.get(key)\n        if bytecode is not None:\n            bucket.bytecode_from_string(bytecode)\n        elif not self.ignore_memcache_errors:\n            raise RuntimeError(\"Failed to load bytecode from memcache\")", "idx": 739}
{"namespace": "jinja2.bccache.MemcachedBytecodeCache.dump_bytecode", "completion": "        key = self.prefix + bucket.key\n        bytecode_string = bucket.bytecode_to_string()\n        try:\n            if self.timeout is not None:\n                self.client.set(key, bytecode_string, timeout=self.timeout)\n            else:\n                self.client.set(key, bytecode_string)\n        except Exception:\n            if not self.ignore_memcache_errors:\n                raise", "idx": 740}
{"namespace": "sumy.utils.get_stop_words", "completion": "    language = normalize_language(language)\n    resource_path = \"stopwords/%s.txt\" % language\n\n    try:\n        content = pkgutil.get_data(\"sumy\", resource_path)\n        if content is None:\n            raise LookupError(\"Stop words for language %s are not available\" % language)\n        content = to_unicode(content.decode(\"utf-8\"))\n        return frozenset(content.splitlines())\n    except IOError:\n        raise LookupError(\"Stop words for language %s are not available\" % language)", "idx": 741}
{"namespace": "sumy._compat.to_bytes", "completion": "\n    if isinstance(object, bytes):\n        return object\n    elif isinstance(object, unicode):\n        return object.encode('utf-8')\n    else:\n        try:\n            return str(object).encode('utf-8')\n        except:\n            return bytes(object)", "idx": 742}
{"namespace": "sumy._compat.to_unicode", "completion": "    if isinstance(object, unicode):\n        return object\n    elif isinstance(object, bytes):\n        return object.decode(\"utf-8\")\n    else:\n        # call custom function to decode to Unicode string\n        return custom_decode_function(object)", "idx": 743}
{"namespace": "sumy.summarizers.lsa.LsaSummarizer._create_dictionary", "completion": "        normalized_words = [self.normalize_word(word) for word in document.words if self.normalize_word(word) not in self.stop_words]\n        unique_words = list(set(normalized_words))\n        dictionary = {word: index for index, word in enumerate(unique_words)}\n        return dictionary", "idx": 744}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._get_content_words_in_sentence", "completion": "        content_words = []\n        for word in sentence.words:\n            normalized_word = self.normalize_word(word)\n            if normalized_word not in self.stop_words:\n                content_words.append(self.stem_word(normalized_word))\n        return content_words", "idx": 745}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._get_all_content_words_in_doc", "completion": "        all_words = self._get_all_words_in_doc(sentences)\n        content_words = self._get_content_words_in_sentence(all_words)\n        return content_words", "idx": 746}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_tf", "completion": "        content_words = self._get_all_content_words_in_doc(sentences)\n        word_freq = self._compute_word_freq(content_words)\n        total_content_words = len(content_words)\n        tf = {word: freq / total_content_words for word, freq in word_freq.items()}\n        return tf", "idx": 747}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_ratings", "completion": "        word_freq = self._compute_tf(sentences)\n        sentences_as_words = [self._get_content_words_in_sentence(s) for s in sentences]\n        ratings = {}\n        for _ in range(len(sentences)):\n            best_sentence_index = self._find_index_of_best_sentence(word_freq, sentences_as_words)\n            best_sentence = sentences[best_sentence_index]\n            ratings[best_sentence] = -(_ + 1)\n            word_freq = self._update_tf(word_freq, sentences_as_words[best_sentence_index])\n            del sentences_as_words[best_sentence_index]\n            del sentences[best_sentence_index]\n        return ratings", "idx": 748}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.cue_method", "completion": "        method = self._build_cue_method_instance()\n        method.bonus_word_value = bonus_word_value\n        method.stigma_word_value = stigma_word_value\n        ratings = self._update_ratings(ratings, method.rate_sentences(document))\n        return self._get_best_sentences(document.sentences, sentences_count, ratings)", "idx": 749}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.key_method", "completion": "        summarization_method = self._build_key_method_instance()\n        return summarization_method(document, sentences_count, weight)", "idx": 750}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.title_method", "completion": "        summarization_method = self._build_title_method_instance()\n        return summarization_method(document, sentences_count)", "idx": 751}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.location_method", "completion": "        summarization_method = self._build_location_method_instance()\n        return summarization_method(document, sentences_count, w_h, w_p1, w_p2, w_s1, w_s2)", "idx": 752}
{"namespace": "sumy.summarizers.reduction.ReductionSummarizer.rate_sentences", "completion": "        ratings = defaultdict(int)\n        sentences = document.sentences\n\n        for (sentence1, sentence2) in combinations(sentences, 2):\n            similarity = self._calculate_similarity(sentence1, sentence2)\n            ratings[sentence1] += similarity\n            ratings[sentence2] += similarity\n\n        return ratings", "idx": 753}
{"namespace": "sumy.summarizers.reduction.ReductionSummarizer._to_words_set", "completion": "        words = sentence.split()\n        words = [self.normalize_word(word) for word in words if word.lower() not in self._stop_words]\n        return set(words)", "idx": 754}
{"namespace": "sumy.summarizers.text_rank.TextRankSummarizer._to_words_set", "completion": "        words = sentence.words\n        words = [self.normalize_word(word) for word in words]\n        words = [word for word in words if word not in self.stop_words]\n        return set(words)", "idx": 755}
{"namespace": "sumy.summarizers.kl.KLSummarizer.compute_tf", "completion": "        content_words = self._get_all_content_words_in_doc(sentences)\n        word_freq = self._compute_word_freq(content_words)\n        total_content_words = len(content_words)\n        \n        tf = {word: freq / total_content_words for word, freq in word_freq.items()}\n        \n        return tf", "idx": 756}
{"namespace": "sumy.evaluation.rouge._get_word_ngrams", "completion": "    ngram_set = set()\n    full_text_words = _split_into_words(sentences)\n    text_length = len(full_text_words)\n    max_index_ngram_start = text_length - n\n    for i in range(max_index_ngram_start + 1):\n        ngram_set.add(tuple(full_text_words[i:i + n]))\n    return ngram_set", "idx": 757}
{"namespace": "kinto.plugins.accounts.views.on_account_created", "completion": "    request = event.request\n    settings = request.registry.settings\n\n    # Check if account validation is enabled in the settings.\n    if settings.get(\"account_validation\", False):\n        for obj in event.impacted_objects:\n            # Retrieve account information.\n            user_email = obj[\"new\"][\"id\"]\n            activation_key = get_cached_validation_key(user_email, request.registry)\n\n            # If activation key is not found, skip to the next impacted object.\n            if activation_key is None:\n                continue\n\n            # Send an email to the user with the activation link.\n            emailer = Emailer(request)\n            emailer.send_activation(user_email, activation_key)", "idx": 758}
{"namespace": "kinto.plugins.accounts.utils.hash_password", "completion": "    hashed_password = bcrypt.hashpw(password.encode('utf-8'), bcrypt.gensalt())\n    return hashed_password.decode('utf-8')", "idx": 759}
{"namespace": "kinto.views.admin.get_parent_uri", "completion": "    # Split the object URI by \"/\"\n    path = object_uri.split(\"/\")\n    \n    # If the path length is less than 3, return an empty string\n    if len(path) < 3:\n        return \"\"\n    \n    # Otherwise, return the first element of the path as the parent URI\n    return \"/\".join(path[:3])", "idx": 760}
{"namespace": "alembic.script.write_hooks.register", "completion": "    def decorator(func: Callable) -> Callable:\n        _registry[name] = func\n        return func\n    return decorator", "idx": 761}
{"namespace": "mongo_connector.namespace_config.match_replace_regex", "completion": "    match = regex.match(src_namespace)\n    if match:\n        return dest_namespace.replace(\"*\", match.group(1))\n    else:\n        return None", "idx": 762}
{"namespace": "mongo_connector.namespace_config.namespace_to_regex", "completion": "    db_name, coll_name = namespace.split(\".\", 1)\n    db_name = db_name.replace(\"*\", \"([^.]*)\")\n    coll_name = coll_name.replace(\"*\", \"([^.]*)\")\n    regex_str = \"^\" + db_name + \"\\\\.\" + coll_name + \"$\"\n    return re.compile(regex_str)", "idx": 763}
{"namespace": "mongo_connector.util.long_to_bson_ts", "completion": "    time = val >> 32\n    inc = val & 0xFFFFFFFF\n    return Timestamp(time, inc)", "idx": 764}
{"namespace": "mongo_connector.doc_managers.formatters.DocumentFlattener.format_document", "completion": "        def _flatten(doc, parent_key='', sep='.'):\n            items = []\n            for k, v in doc.items():\n                new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n                if isinstance(v, dict):\n                    items.extend(_flatten(v, new_key, sep=sep))\n                else:\n                    items.append((new_key, v))\n            return items\n\n        return dict(_flatten(document))", "idx": 765}
{"namespace": "bplustree.memory.open_file_in_dir", "completion": "    try:\n        file_descriptor = io.open(path, 'rb')\n        if platform.system() != 'Windows':\n            dir_descriptor = os.open(os.path.dirname(path), os.O_RDONLY)\n        else:\n            dir_descriptor = None\n        return file_descriptor, dir_descriptor\n    except Exception as e:\n        logger.error(f\"Error opening file in directory: {e}\")\n        return None, None", "idx": 766}
{"namespace": "bplustree.memory.FileMemory.read_transaction", "completion": "        return ReadTransaction(self._lock, 'r')", "idx": 767}
{"namespace": "bplustree.utils.pairwise", "completion": "    return zip(iterable, iterable[1:])", "idx": 768}
{"namespace": "bplustree.utils.iter_slice", "completion": "    it = iter(iterable)\n    while True:\n        slice = bytes(itertools.islice(it, n))\n        if not slice:\n            break\n        yield slice, len(slice) < n", "idx": 769}
{"namespace": "bplustree.serializer.StrSerializer.serialize", "completion": "        serialized_bytes = obj.encode('utf-8')\n        assert len(serialized_bytes) <= key_size, \"Serialized bytes length exceeds key size\"\n        return serialized_bytes", "idx": 770}
{"namespace": "psd_tools.utils.pack", "completion": "    try:\n        return struct.pack(fmt, *args)\n    except struct.error as e:\n        logger.error(\"Error packing data: %s\", e)\n        return b''", "idx": 771}
{"namespace": "psd_tools.utils.unpack", "completion": "    fmt = str(\">\" + fmt)\n    return struct.unpack(fmt, data)", "idx": 772}
{"namespace": "psd_tools.api.numpy_io.get_pattern", "completion": "\n    height, width = pattern.data[2], pattern.data[3]\n    pattern_array = np.zeros((height, width, 1), dtype=np.float32)\n\n    for channel in pattern.data:\n        if channel == 'mask' or (channel == 'shape' and not has_transparency(pattern)):\n            pattern_array = np.ones((height, width, 1), dtype=np.float32)\n        else:\n            pattern_array = get_layer_data(pattern, channel)\n\n    return pattern_array", "idx": 773}
{"namespace": "sqlite_utils.utils.maximize_csv_field_size_limit", "completion": "    max_size = sys.maxsize\n    csv.field_size_limit(max_size)\n    while True:\n        try:\n            csv.field_size_limit(max_size)\n            break\n        except OverflowError:\n            max_size = int(max_size / 2)", "idx": 774}
{"namespace": "sqlite_utils.utils.column_affinity", "completion": "\n    if column_type in (\"INT\", \"INTEGER\", \"TINYINT\", \"SMALLINT\", \"MEDIUMINT\", \"BIGINT\", \"UNSIGNED BIG INT\", \"INT2\", \"INT8\"):\n        return \"INTEGER\"\n    elif column_type in (\"CHAR\", \"CLOB\", \"TEXT\"):\n        return \"TEXT\"\n    elif column_type in (\"BLOB\"):\n        return \"BLOB\"\n    elif column_type in (\"REAL\", \"FLOA\", \"DOUB\", \"DOUBLE\", \"DOUBLE PRECISION\", \"FLOAT\"):\n        return \"REAL\"\n    else:\n        return \"NUMERIC\"", "idx": 775}
{"namespace": "sqlite_utils.utils.decode_base64_values", "completion": "    for key, value in doc.items():\n        if isinstance(value, dict) and value.get(\"$base64\"):\n            encoded_value = value.get(\"encoded\")\n            if encoded_value:\n                decoded_value = base64.b64decode(encoded_value).decode(\"utf-8\")\n                doc[key] = decoded_value\n    return doc", "idx": 776}
{"namespace": "sqlite_utils.utils.chunks", "completion": "    it = iter(sequence)\n    chunk = list(itertools.islice(it, size))\n    while chunk:\n        yield chunk\n        chunk = list(itertools.islice(it, size))", "idx": 777}
{"namespace": "sqlite_utils.utils.hash_record", "completion": "    if keys:\n        data = {k: record[k] for k in keys if k in record}\n    else:\n        data = record\n    data_str = json.dumps(data, sort_keys=True)\n    return hashlib.sha1(data_str.encode()).hexdigest()", "idx": 778}
{"namespace": "arctic.decorators._get_host", "completion": "    host_info = {}\n    if store:\n        if isinstance(store, (list, tuple)):\n            store = store[0]\n        host_info['library_name'] = store.library_name\n        host_info['mongo_nodes'] = store.get_mongo_nodes()\n        host_info['mongo_host'] = store.get_mongo_host()\n    return host_info", "idx": 779}
{"namespace": "arctic.decorators.mongo_retry", "completion": "    @wraps(f)\n    def wrapper(*args, **kwargs):\n        global _in_retry\n        global _retry_count\n        _in_retry = True\n        while _retry_count < _MAX_RETRIES:\n            try:\n                result = f(*args, **kwargs)\n                _in_retry = False\n                _retry_count = 0\n                return result\n            except (AutoReconnect, OperationFailure, DuplicateKeyError, ServerSelectionTimeoutError, BulkWriteError) as e:\n                _log_exception(logger, e, extra=_get_host(args[0]))\n                _retry_count += 1\n                sleep(1)\n        _in_retry = False\n        _retry_count = 0\n        raise\n    return wrapper", "idx": 780}
{"namespace": "arctic._util.are_equals", "completion": "    if isinstance(o1, DataFrame) and isinstance(o2, DataFrame):\n        try:\n            assert_frame_equal(o1, o2, **kwargs)\n            return True\n        except:\n            return False\n    else:\n        return o1 == o2", "idx": 781}
{"namespace": "arctic.hooks.register_resolve_mongodb_hook", "completion": "    global _resolve_mongodb_hook\n    _resolve_mongodb_hook = hook", "idx": 782}
{"namespace": "arctic.hooks.register_log_exception_hook", "completion": "    global _log_exception_hook\n    _log_exception_hook = hook", "idx": 783}
{"namespace": "arctic.hooks.register_get_auth_hook", "completion": "    global _get_auth_hook\n    _get_auth_hook = hook", "idx": 784}
{"namespace": "arctic.store._version_store_utils._split_arrs", "completion": "    result = []\n    start = 0\n    for end in slices:\n        result.append(array_2d[start:end])\n        start = end\n    return result", "idx": 785}
{"namespace": "arctic.store._version_store_utils.checksum", "completion": "\n    # Encode symbol and doc as bytes\n    symbol_bytes = symbol.encode('utf-8')\n    doc_bytes = pickle.dumps(doc, protocol=pickle.HIGHEST_PROTOCOL)\n\n    # Calculate checksum using SHA1 algorithm\n    sha1 = hashlib.sha1()\n    sha1.update(symbol_bytes)\n    sha1.update(doc_bytes)\n    checksum_result = sha1.digest()\n\n    # Return the result as a Binary object\n    return Binary(checksum_result)", "idx": 786}
{"namespace": "arctic.store.versioned_item.VersionedItem.__str__", "completion": "        return f\"VersionedItem(symbol={self.symbol},library={self.library},data={self.data},version={self.version},metadata={self.metadata},host={self.host})\"", "idx": 787}
{"namespace": "arctic.store._ndarray_store.NdarrayStore._dtype", "completion": "        if metadata is None:\n            metadata = {}\n        return np.dtype(string, metadata)", "idx": 788}
{"namespace": "arctic.store._ndarray_store._promote_struct_dtypes", "completion": "    # Check if the fields of dtype1 are a superset of dtype2\n    if set(dtype2.names).issubset(set(dtype1.names)):\n        # Promote the data types of the two structured arrays\n        new_dtype = []\n        for field in dtype1.names:\n            if field in dtype2.names:\n                # Promote the data type of the field\n                new_dtype.append((field, np.find_common_type([dtype1[field], dtype2[field]], [])))\n            else:\n                new_dtype.append((field, dtype1[field]))\n        return np.dtype(new_dtype)\n    else:\n        raise UnhandledDtypeException(\"Fields of dtype1 are not a superset of dtype2\")", "idx": 789}
{"namespace": "arctic.chunkstore.passthrough_chunker.PassthroughChunker.exclude", "completion": "        return DataFrame() if isinstance(data, DataFrame) else Series()", "idx": 790}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.to_chunks", "completion": "        if isinstance(df, pd.Series):\n            df = df.to_frame()\n\n        if not isinstance(df.index, pd.DatetimeIndex):\n            raise ValueError(\"Index of the dataframe/series must be a DatetimeIndex\")\n\n        for start, end in zip(df.index.to_period(chunk_size).to_timestamp(), df.index.to_period(chunk_size).to_timestamp() + pd.Timedelta(1, unit=chunk_size)):\n            chunk = df[(df.index >= start) & (df.index < end)]\n            yield (start, end, chunk_size, chunk)", "idx": 791}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.exclude", "completion": "        if isinstance(range_obj, (pd.DatetimeIndex, tuple)):\n            range_obj = DateRange(range_obj[0], range_obj[-1])\n\n        range_obj = to_pandas_closed_closed(range_obj, add_tz=False)\n        start = range_obj.start\n        end = range_obj.end\n\n        if 'date' in data.index.names:\n            return data[(data.index.get_level_values('date') < start) | (data.index.get_level_values('date') > end)]\n        elif 'date' in data.columns:\n            return data[(data.date < start) | (data.date > end)]\n        else:\n            return data", "idx": 792}
{"namespace": "mopidy.httpclient.format_proxy", "completion": "    if proxy_config:\n        scheme = proxy_config.get('scheme', 'http')\n        host = proxy_config.get('hostname', 'localhost')\n        port = proxy_config.get('port', '8080')\n        user = proxy_config.get('username', '')\n        password = proxy_config.get('password', '')\n\n        if auth and user and password:\n            return f\"{scheme}://{user}:{password}@{host}:{port}\"\n        else:\n            return f\"{scheme}://{host}:{port}\"\n    else:\n        return None", "idx": 793}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.filter", "completion": "\n        if isinstance(range_obj, (pd.DatetimeIndex, tuple)):\n            range_obj = DateRange(range_obj[0], range_obj[-1])\n\n        filtered_data = data[(data.index >= range_obj.start) & (data.index <= range_obj.end)]\n        return filtered_data", "idx": 794}
{"namespace": "mopidy.config.validators.validate_required", "completion": "    if required and not value:\n        raise ValueError(\"Value is required but not set\")", "idx": 795}
{"namespace": "mopidy.config.validators.validate_choice", "completion": "    if value not in choices:\n        raise ValueError(f\"must be one of {', '.join(choices)}, not {value}.\")", "idx": 796}
{"namespace": "mopidy.config.validators.validate_minimum", "completion": "    if value < minimum:\n        raise ValueError(f\"{value!r} must be larger than {minimum!r}.\")", "idx": 797}
{"namespace": "mopidy.config.validators.validate_maximum", "completion": "    if maximum is not None and value > maximum:\n        raise ValueError(f\"{value!r} must be smaller than {maximum!r}.\")", "idx": 798}
{"namespace": "mopidy.config.schemas._did_you_mean", "completion": "    if not choices:\n        return None\n    \n    # Calculate Levenshtein distance between input name and each choice\n    distances = {choice: _levenshtein(name, choice) for choice in choices}\n    \n    # Sort the results based on distance\n    sorted_distances = sorted(distances.items(), key=lambda x: x[1])\n    \n    # Return the most likely setting if the distance is less than or equal to 3\n    if sorted_distances[0][1] <= 3:\n        return sorted_distances[0][0]\n    else:\n        return None", "idx": 799}
{"namespace": "mopidy.config.types.encode", "completion": "    if isinstance(value, str):\n        for char in (\"\\\\\", \"\\n\", \"\\t\"):\n            value = value.replace(char, \"\\\\\" + char)\n        return value.encode(encoding=\"unicode-escape\")\n    else:\n        return value", "idx": 800}
{"namespace": "mopidy.config.types.decode", "completion": "    if isinstance(value, bytes):\n        return value.decode('unicode_escape').encode().decode('unicode_escape')\n    else:\n        return value", "idx": 801}
{"namespace": "mopidy.config.types.ConfigValue.serialize", "completion": "        if value is None:\n            return \"\"\n        return encode(str(value))", "idx": 802}
{"namespace": "mopidy.config.types.Boolean.serialize", "completion": "        if value is True:\n            return \"true\"\n        elif value is False or value is None:\n            return \"false\"\n        else:\n            raise ValueError(f\"{value!r} is not a boolean\")", "idx": 803}
{"namespace": "hypertools.tools.df2mat.df2mat", "completion": "    # Get dummies for text columns\n    data = pd.get_dummies(data)\n    \n    if return_labels:\n        return data.values, list(data.columns)\n    else:\n        return data.values", "idx": 804}
{"namespace": "hypertools._shared.helpers.center", "completion": "\n    assert isinstance(x, list), \"Input must be a list\"\n    mean_x = np.mean(x)\n    centered_x = [i - mean_x for i in x]\n    return centered_x", "idx": 805}
{"namespace": "hypertools._shared.helpers.group_by_category", "completion": "\n    if any(isinstance(i, list) for i in vals):\n        vals = list(itertools.chain.from_iterable(vals))\n    \n    unique_vals = sorted(set(vals))\n    return [unique_vals.index(i) for i in vals]", "idx": 806}
{"namespace": "hypertools._shared.helpers.vals2colors", "completion": "    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n    palette = sns.color_palette(cmap, res)\n    val_set = list(sorted(set(vals), key=list(vals).index))\n    color_mapping = {val: palette[i % res] for i, val in enumerate(val_set)}\n    return [color_mapping[val] for val in vals]", "idx": 807}
{"namespace": "hypertools._shared.helpers.vals2bins", "completion": "    # flatten if list of lists\n    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n\n    # map values to bins based on resolution\n    bins = np.digitize(vals, np.linspace(np.min(vals), np.max(vals)+1, res+1)) - 1\n    return bins", "idx": 808}
{"namespace": "hypertools._shared.helpers.interp_array", "completion": "    x = np.arange(len(arr))\n    y = arr\n    x_new = np.linspace(0, len(arr) - 1, interp_val)\n    interpolator = pchip(x, y)\n    return interpolator(x_new)", "idx": 809}
{"namespace": "hypertools._shared.helpers.parse_args", "completion": "    result = []\n    for elem in x:\n        if isinstance(args, (list, tuple)):\n            if len(args) != len(x):\n                print(\"Error: Length of args must be the same as x\")\n                sys.exit(1)\n            result.append(tuple([elem] + list(args)))\n        else:\n            result.append((elem, args))\n    return result", "idx": 810}
{"namespace": "hypertools._shared.helpers.parse_kwargs", "completion": "\n    args_list = []\n    for i, item in enumerate(x):\n        args_list.append({key: value[i] if isinstance(value, (tuple, list)) and len(value) == len(x) else value for key, value in kwargs.items()})\n    return args_list", "idx": 811}
{"namespace": "gif_for_cli.utils._get_default_display_mode", "completion": "    if \"truecolor\" in environ.get(\"TERM\", \"\") or \"truecolor\" in environ.get(\"COLORTERM\", \"\"):\n        return \"truecolor\"\n    elif \"256\" in environ.get(\"TERM\", \"\") or \"256\" in environ.get(\"COLORTERM\", \"\"):\n        return \"256fgbg\"\n    else:\n        return \"nocolor\"", "idx": 812}
{"namespace": "gif_for_cli.utils._pool_type", "completion": "    try:\n        value = int(val)\n        if value <= 0:\n            raise argparse.ArgumentTypeError(\"%s is an invalid positive int value\" % val)\n        return value\n    except ValueError:\n        raise argparse.ArgumentTypeError(\"%s is an invalid positive int value\" % val)", "idx": 813}
{"namespace": "gif_for_cli.generate.utils.get_avg_for_em", "completion": "    total_r, total_g, total_b = 0, 0, 0\n    count = 0\n    for i in range(y, y + cell_height):\n        for j in range(x, x + cell_width):\n            if i < len(px) and j < len(px[i]):\n                r, g, b = px[i][j]\n                total_r += r\n                total_g += g\n                total_b += b\n                count += 1\n    if count == 0:\n        return [0, 0, 0]\n    else:\n        avg_r = total_r // count\n        avg_g = total_g // count\n        avg_b = total_b // count\n        return [avg_r, avg_g, avg_b]", "idx": 814}
{"namespace": "gif_for_cli.generate.utils.process_input_source", "completion": "    if input_source.startswith(\"https://tenor.com/view/\"):\n        gif_id = input_source.split(\"/\")[-1]\n        gif_url = f\"https://tenor.com/o{gif_id}\"\n    else:\n        params = {\n            \"q\": input_source,\n            \"key\": api_key,\n            \"limit\": 1\n        }\n        response = requests.get(\"https://api.tenor.com/v1/search\", params=params)\n        try:\n            gif_url = response.json()[\"results\"][0][\"media\"][0][\"gif\"][\"url\"]\n        except (JSONDecodeError, KeyError, IndexError):\n            gif_url = \"\"\n    return gif_url", "idx": 815}
{"namespace": "hypertools._shared.helpers.reshape_data", "completion": "    if labels is not None:\n        reshaped_data = []\n        reshaped_labels = []\n        for category in np.unique(hue):\n            category_indices = np.where(hue == category)[0]\n            category_data = [x[i] for i in category_indices]\n            category_labels = [labels[i] for i in category_indices]\n            reshaped_data.append(category_data)\n            reshaped_labels.append(category_labels)\n        return reshaped_data, reshaped_labels\n    else:\n        reshaped_data = []\n        for category in np.unique(hue):\n            category_indices = np.where(hue == category)[0]\n            category_data = [x[i] for i in category_indices]\n            reshaped_data.append(category_data)\n        return reshaped_data", "idx": 816}
{"namespace": "mingus.extra.lilypond.from_Note", "completion": "    if process_octaves:\n        return note.name\n    else:\n        return note.name[0]", "idx": 817}
{"namespace": "mingus.extra.tablature._get_qsize", "completion": "    # Calculate the number of strings in the tuning\n    num_strings = len(tuning.tuning)\n\n    # Calculate the quarter note size based on the number of strings and the width\n    qsize = width // (num_strings * 2)\n\n    return qsize", "idx": 818}
{"namespace": "mingus.core.notes.augment", "completion": "    if note[-1] != \"b\":\n        return note + \"#\"\n    else:\n        return note[:-1]", "idx": 819}
{"namespace": "mingus.core.meter.valid_beat_duration", "completion": "        import math\n    import math\n    return math.log2(duration).is_integer()", "idx": 820}
{"namespace": "mingus.core.notes.diminish", "completion": "    if note[-1] != \"#\":\n        return note + \"b\"\n    else:\n        return note[:-1]", "idx": 821}
{"namespace": "mingus.core.intervals.invert", "completion": "    return interval[::-1]", "idx": 822}
{"namespace": "mingus.core.progressions.parse_string", "completion": "\n    # Initialize variables\n    roman_numeral = \"\"\n    acc = 0\n    suffix = \"\"\n\n    # Check for accidentals and suffix\n    if progression[0] in [\"b\", \"#\"]:\n        acc = 1\n        roman_numeral = progression[1]\n        suffix = progression[2:]\n    else:\n        roman_numeral = progression[0]\n        suffix = progression[1:]\n\n    return (roman_numeral, acc, suffix)", "idx": 823}
{"namespace": "exodus_bundler.bundling.bytes_to_int", "completion": "    return int.from_bytes(bytes, byteorder)", "idx": 824}
{"namespace": "exodus_bundler.templating.render_template", "completion": "    for key, value in context.items():\n        string = string.replace('{{' + key + '}}', str(value))\n    return string", "idx": 825}
{"namespace": "exodus_bundler.input_parsing.strip_pid_prefix", "completion": "    return re.sub(r'^\\[pid \\d+\\] ', '', line)", "idx": 826}
{"namespace": "fs.path.abspath", "completion": "    if not path.startswith(\"/\"):\n        return \"/\" + path\n    return path", "idx": 827}
{"namespace": "fs.path.combine", "completion": "    if not path2:\n        return path1\n    if path2.startswith(\"/\"):\n        return path2\n    if not path1 or path1.endswith(\"/\"):\n        return path1 + path2\n    return path1 + \"/\" + path2", "idx": 828}
{"namespace": "fs.path.split", "completion": "    components = normpath(path).split(\"/\")\n    head = \"/\".join(components[:-1])\n    tail = components[-1]\n    return head, tail", "idx": 829}
{"namespace": "fs.path.isparent", "completion": "    return path2.startswith(path1.rstrip(\"/\") + \"/\")", "idx": 830}
{"namespace": "fs.path.forcedir", "completion": "    if not path.endswith(\"/\"):\n        return path + \"/\"\n    return path", "idx": 831}
{"namespace": "fs.wildcard.match_any", "completion": "    if not patterns:\n        return True\n    for pattern in patterns:\n        if match(pattern, name):\n            return True\n    return False", "idx": 832}
{"namespace": "fs.wildcard.imatch_any", "completion": "    if not patterns:\n        return True\n    return any(imatch(pattern, name) for pattern in patterns)", "idx": 833}
{"namespace": "wal_e.cmd.parse_boolean_envvar", "completion": "\n    if val.lower() in ('false', '0'):\n        return False\n    elif val.lower() in ('true', '1'):\n        return True\n    else:\n        raise ValueError(\"Invalid boolean value: {}\".format(val))", "idx": 834}
{"namespace": "wal_e.log_help.get_log_destinations", "completion": "    log_destinations = os.getenv(\"WALE_LOG_DESTINATION\", \"stderr,syslog\")\n    return log_destinations.split(',')", "idx": 835}
{"namespace": "wal_e.log_help.WalELogger._fmt_structured", "completion": "        time = datetime.datetime.utcnow().strftime('time=%Y-%m-%dT%H:%M:%S.%f-00')\n        pid = 'pid={}'.format(os.getpid())\n        formatted_dict = ' '.join(['{}={}'.format(k, v) for k, v in sorted(d.items())])\n        return '{} {} {}'.format(time, pid, formatted_dict)", "idx": 836}
{"namespace": "wal_e.tar_partition._fsync_files", "completion": "    for filename in filenames:\n        try:\n            with open(filename, 'r') as file:\n                os.fsync(file.fileno())\n        except OSError as e:\n            if e.errno == errno.EACCES:\n                logger.warning('Could not fsync file {0} due to permissions error'.format(filename))\n            else:\n                raise\n        try:\n            directory = os.path.dirname(filename)\n            with os.scandir(directory) as dir:\n                os.fsync(dir.fileno())\n        except OSError as e:\n            if e.errno == errno.EACCES:\n                logger.warning('Could not fsync directory {0} due to permissions error'.format(directory))\n            else:\n                raise", "idx": 837}
{"namespace": "wal_e.blobstore.file.calling_format.Bucket.list", "completion": "        path = os.path.join(\"/\", prefix.strip(\"/\"))\n        file_keys = []\n        for root, dirs, files in os.walk(path):\n            for f in files:\n                file_path = os.path.join(root, f)\n                file_keys.append(FileKey(bucket=self, name=file_path))\n        return file_keys", "idx": 838}
{"namespace": "pyinfra.operations.util.files.unix_path_join", "completion": "    # Remove trailing slashes from all parts except the last one\n    path_parts = [part.rstrip('/') for part in path_parts[:-1]] + [path_parts[-1]]\n\n    # Join the path parts using a forward slash\n    joined_path = '/'.join(path_parts)\n\n    return joined_path", "idx": 839}
{"namespace": "pyinfra.operations.server.shell", "completion": "    if isinstance(commands, str):\n        commands = [commands]\n\n    for command in commands:\n        yield command", "idx": 840}
{"namespace": "pyinfra.api.util.try_int", "completion": "    try:\n        return int(value)\n    except (ValueError, TypeError):\n        return value", "idx": 841}
{"namespace": "mrjob.job.MRJob.mr_job_script", "completion": "        try:\n            return inspect.getsourcefile(cls)\n        except TypeError:\n            return None", "idx": 842}
{"namespace": "mrjob.compat.map_version", "completion": "    if isinstance(version_map, list):\n        version_map = dict(version_map)\n\n    for ver in sorted(version_map.keys(), key=LooseVersion):\n        if LooseVersion(version) >= LooseVersion(ver):\n            return version_map[ver]\n\n    return None", "idx": 843}
{"namespace": "mrjob.conf.combine_values", "completion": "    for value in reversed(values):\n        if value is not None:\n            return value\n    return None", "idx": 844}
{"namespace": "mrjob.protocol.BytesProtocol.read", "completion": "        parts = line.split(b'\\t', 1)\n        if len(parts) == 1:\n            return (parts[0], None)\n        else:\n            return (parts[0], parts[1])", "idx": 845}
{"namespace": "mrjob.protocol.TextProtocol.write", "completion": "        if key is not None and value is not None:\n            return (key.encode('utf_8') + b'\\t' + value.encode('utf_8'))\n        elif key is not None:\n            return key.encode('utf_8')\n        elif value is not None:\n            return value.encode('utf_8')\n        else:\n            return b''", "idx": 846}
{"namespace": "mrjob.protocol.TextProtocol.read", "completion": "\n        try:\n            decoded_line = line.decode('utf_8')\n        except UnicodeDecodeError:\n            decoded_line = line.decode('latin_1')\n\n        key_value = decoded_line.split('\\t', 1)\n        if len(key_value) == 1:\n            key_value.append(None)\n\n        return tuple(key_value)", "idx": 847}
{"namespace": "mrjob.protocol.TextValueProtocol.read", "completion": "        try:\n            line = line.decode('utf_8')\n        except UnicodeDecodeError:\n            line = line.decode('latin_1')\n\n        return (None, line)", "idx": 848}
{"namespace": "mrjob.util.file_ext", "completion": "    if \".\" in filename:\n        return \".\" + filename.split(\".\")[-1]\n    else:\n        return \"\"", "idx": 849}
{"namespace": "mrjob.util.cmd_line", "completion": "    return ' '.join(pipes.quote(arg) for arg in args)", "idx": 850}
{"namespace": "trafilatura.sitemaps.SitemapObject.extract_sitemap_links", "completion": "        matches = re.finditer(LINK_REGEX, self.content)\n        for match in matches:\n            link = match.group(1)\n            self.handle_link(link)\n        LOGGER.debug('%s sitemaps and %s links found for %s', len(self.sitemap_urls), len(self.urls), self.sitemap_url)", "idx": 851}
{"namespace": "trafilatura.sitemaps.SitemapObject.process", "completion": "\n        # Check if the sitemap is plausible\n        if not SITEMAP_FORMAT.search(self.content):\n            LOGGER.warning('not a sitemap: %s', self.sitemap_url)\n            return\n\n        # Try to extract links from a TXT file\n        if not self.extract_sitemap_langlinks():\n            # If the content does not match the format of a sitemap, iterate through the content to find links and handle each link\n            self.extract_sitemap_links()\n\n        # If the content matches the format of an XML sitemap and a target language is specified, extract language links from the sitemap\n        if SITEMAP_FORMAT.search(self.content) and self.target_lang:\n            self.extract_sitemap_langlinks()\n\n        # If there are sitemap URLs or URLs extracted from the sitemap, return\n        if self.sitemap_urls or self.urls:\n            return\n\n        # Otherwise, extract the links from the sitemap\n        self.extract_sitemap_links()", "idx": 852}
{"namespace": "trafilatura.sitemaps.is_plausible_sitemap", "completion": "    if contents is None:\n        return False  # If the sitemap is empty, it is not plausible\n\n    if SITEMAP_FORMAT.search(contents):\n        return True  # If the contents match the expected sitemap format, it is plausible\n    else:\n        return False  # If the contents do not match the expected sitemap format, it is not plausible", "idx": 853}
{"namespace": "trafilatura.sitemaps.extract_robots_sitemaps", "completion": "\n    sitemap_urls = []\n    if robotstxt:\n        lines = robotstxt.split('\\n')\n        for line in lines:\n            line = line.strip()\n            if line and not line.startswith('#'):  # remove comments and empty lines\n                if line.lower().startswith('sitemap:'):\n                    parts = line.split(':')\n                    if len(parts) > 1:\n                        sitemap_url = parts[1].strip()\n                        sitemap_url = fix_relative_urls(baseurl, sitemap_url)  # resolve relative URLs\n                        sitemap_urls.append(sitemap_url)\n    return sitemap_urls", "idx": 854}
{"namespace": "trafilatura.feeds.handle_link_list", "completion": "    filtered_links = []\n    for link in linklist:\n        if not link:\n            continue\n        link = clean_url(link)\n        if not link:\n            continue\n        if not validate_url(link):\n            continue\n        if not check_url(link):\n            continue\n        if domainname not in link:\n            continue\n        if target_lang and target_lang not in link:\n            continue\n        filtered_links.append(fix_relative_urls(link, baseurl))\n        if len(filtered_links) >= MAX_LINKS:\n            break\n    return filtered_links", "idx": 855}
{"namespace": "trafilatura.feeds.find_feed_urls", "completion": "    domainname, baseurl = get_hostinfo(url)\n    htmlstring = load_html(url)\n    if htmlstring is None:\n        LOGGER.debug('Invalid HTML/Feed page: %s', url)\n        return []\n\n    feed_urls = determine_feed(htmlstring, baseurl, url)\n    if not feed_urls:\n        feed_string = htmlstring\n        feed_urls = extract_links(feed_string, domainname, baseurl, url, target_lang)\n\n    return sorted(set(feed_urls))", "idx": 856}
{"namespace": "trafilatura.hashing.generate_hash_filename", "completion": "    clean_content = re.sub(CLEAN_XML, '', content)  # Remove potential XML tags\n    hash_bytes = generate_bow_hash(clean_content, length=12)  # Generate bag-of-words hash of length 12\n    encoded_hash = urlsafe_b64encode(hash_bytes).decode()  # Encode the hash using urlsafe_b64encode\n    return encoded_hash", "idx": 857}
{"namespace": "trafilatura.cli_utils.download_queue_processing", "completion": "    errors = []\n    with ThreadPoolExecutor(max_workers=args.threads) as executor:\n        futures = []\n        while not url_store.empty():\n            url = url_store.get()\n            try:\n                future = executor.submit(buffered_downloads, url, args, config)\n                futures.append(future)\n            except Exception as e:\n                errors.append(str(e))\n        for future in as_completed(futures):\n            try:\n                result = future.result()\n                counter = process_result(result, args, url, counter, config)\n            except Exception as e:\n                errors.append(str(e))\n    return errors, counter", "idx": 858}
{"namespace": "trafilatura.utils.decode_response", "completion": "    if isinstance(response, HTTPResponse):\n        filecontent = response.data\n    elif isinstance(response, bytes):\n        filecontent = response\n    else:\n        raise ValueError(\"Invalid input type. Expected urllib3 response object or bytes.\")\n\n    filecontent = handle_compressed_file(filecontent)\n    encodings = detect_encoding(filecontent)\n    for encoding in encodings:\n        try:\n            decoded_response = filecontent.decode(encoding)\n            return decoded_response\n        except UnicodeDecodeError:\n            pass\n    raise ValueError(\"Unable to decode the response using detected encodings.\")", "idx": 859}
{"namespace": "trafilatura.utils.txttocsv", "completion": "    csv_data = [\n        \"URL\\tFingerprint\\tHostname\\tTitle\\tImage\\tDate\\tLicense\\tPagetype\\tID\",\n        f\"{docmeta.get('URL', '')}\\t{docmeta.get('fingerprint', '')}\\t{docmeta.get('hostname', '')}\\t{docmeta.get('title', '')}\\t{docmeta.get('image', '')}\\t{docmeta.get('date', '')}\\t{docmeta.get('license', '')}\\t{docmeta.get('pagetype', '')}\\t{docmeta.get('ID', '')}\",\n        f\"\\n{text}\\n\",\n        f\"\\n{comments}\\n\"\n    ]\n    return '\\n'.join(csv_data)", "idx": 860}
{"namespace": "trafilatura.utils.is_similar_domain", "completion": "    # Remove file extension from both strings\n    reference = STRIP_EXTENSION.sub('', reference)\n    new_string = STRIP_EXTENSION.sub('', new_string)\n\n    # Calculate the similarity ratio using SequenceMatcher\n    similarity_ratio = SequenceMatcher(None, reference, new_string).ratio()\n\n    # Check if the similarity ratio is above the threshold\n    if similarity_ratio >= threshold:\n        return True\n    else:\n        return False", "idx": 861}
{"namespace": "trafilatura.xml.remove_empty_elements", "completion": "    for elem in tree.iter():\n        if len(elem) == 0 and not elem.text and not elem.tail:\n            parent = elem.getparent()\n            if parent is not None:\n                parent.remove(elem)\n    return tree", "idx": 862}
{"namespace": "trafilatura.xml.strip_double_tags", "completion": "    for tag in NESTING_WHITELIST:\n        elements = tree.findall('.//' + tag)\n        for element in elements:\n            for child in element:\n                if child.tag in NESTING_WHITELIST:\n                    element.text = (element.text or '') + (child.text or '')\n                    element.tail = (element.tail or '') + (child.tail or '')\n                    element.remove(child)\n    return tree", "idx": 863}
{"namespace": "trafilatura.xml.check_tei", "completion": "\n    # Remove tail from specific elements\n    for element in xmldoc.iter():\n        if element.tag in TEI_REMOVE_TAIL:\n            element.tail = None\n\n    # Strip double tags\n    strip_double_tags(xmldoc)\n\n    # Remove empty elements\n    remove_empty_elements(xmldoc)\n\n    return xmldoc", "idx": 864}
{"namespace": "trafilatura.xml.validate_tei", "completion": "    global TEI_RELAXNG\n    if TEI_RELAXNG is None:\n        with lzma.open(TEI_SCHEMA, 'rb') as file:\n            TEI_RELAXNG = load_pickle(file)\n    relaxng = RelaxNG(TEI_RELAXNG)\n    return relaxng.validate(xmldoc)", "idx": 865}
{"namespace": "trafilatura.xml.merge_with_parent", "completion": "    parent = element.getparent()\n    if parent is not None:\n        if element.text:\n            if parent.text:\n                parent.text += element.text\n            else:\n                parent.text = element.text\n        if element.tail:\n            if parent.tail:\n                parent.tail += element.tail\n            else:\n                parent.tail = element.tail\n        parent.remove(element)", "idx": 866}
{"namespace": "trafilatura.downloads._determine_headers", "completion": "    myagents, mycookie = _parse_config(config)\n\n    if myagents:\n        headers['User-Agent'] = random.choice(myagents)\n\n    if mycookie:\n        headers['Cookie'] = mycookie\n\n    return headers or DEFAULT_HEADERS", "idx": 867}
{"namespace": "trafilatura.meta.reset_caches", "completion": "    reset_caches_courlan()  # Reset caches for courlan module\n    reset_caches_htmldate()  # Reset caches for htmldate module\n    define_stoplist.cache_clear()  # Reset cache for define_stoplist function\n    gc.collect()  # Perform garbage collection to release memory", "idx": 868}
{"namespace": "trafilatura.core.handle_table", "completion": "    newtable = Element('table')\n    newrow = Element('row')\n    for child in table_elem.iter('*'):\n        if child.tag in {'thead', 'tbody', 'tfoot'}:\n            continue\n        if child.tag == 'tr':\n            if len(newrow) > 0:\n                newtable.append(newrow)\n                newrow = Element('row')\n        if child.tag in {'td', 'th'}:\n            cell_element = define_cell_type(child)\n            if len(child) == 0:\n                processed_child = process_node(child, options)\n                if processed_child is not None:\n                    cell_element.text = processed_child.text\n            else:\n                for subelem in child.iterdescendants('*'):\n                    if subelem.tag in potential_tags:\n                        processed_subchild = process_node(subelem, options)\n                        if processed_subchild is not None:\n                            cell_element.append(processed_subchild)\n            newrow.append(cell_element)\n    if len(newrow) > 0:\n        newtable.append(newrow)\n    if len(newtable) > 0:\n        return newtable\n    else:\n        return None", "idx": 869}
{"namespace": "trafilatura.filters.language_filter", "completion": "\n    # Check if the target language is specified\n    if not target_language:\n        LOGGER.warning('Target language not specified')\n        return True, docmeta\n\n    # Detect the language of the text using a language classifier\n    detected_language = language_classifier(temp_text, temp_comments)\n\n    # Compare the detected language with the target language\n    if detected_language and detected_language != target_language:\n        LOGGER.warning('Detected language (%s) is different from the target language (%s)', detected_language, target_language)\n        return True, docmeta\n    else:\n        return False, docmeta", "idx": 870}
{"namespace": "trafilatura.filters.textfilter", "completion": "\n    # Check for duplicate text\n    if duplicate_test(element, config):\n        return True\n\n    # Add more filtering conditions here if needed\n\n    return False", "idx": 871}
{"namespace": "trafilatura.metadata.extract_meta_json", "completion": "    for element in tree.xpath('//script[@type=\"application/ld+json\" or @type=\"application/settings+json\"]'):\n        if element.text:\n            try:\n                json_data = json.loads(element.text)\n                metadata.update(json_data)\n            except json.JSONDecodeError:\n                json_text = re.sub(JSON_MINIFY, lambda m: m.group(1) or '', element.text)\n                metadata.update(json.loads(json_text))\n    return metadata", "idx": 872}
{"namespace": "trafilatura.external.try_justext", "completion": "    global JT_STOPLIST\n    if JT_STOPLIST is None:\n        JT_STOPLIST = jt_stoplist_init()\n\n    if target_language not in JT_STOPLIST:\n        stoplist = JT_STOPLIST['default']\n    else:\n        stoplist = JT_STOPLIST[target_language]\n\n    try:\n        paragraphs = custom_justext(tree, stoplist)\n        body = Element('body')\n        for paragraph in paragraphs:\n            if not paragraph.is_boilerplate:\n                p = Element('p')\n                p.text = trim(paragraph.text)\n                body.append(p)\n        return body\n    except Exception as err:\n        LOGGER.warning('justext failed: %s', err)\n        return None", "idx": 873}
{"namespace": "mongoengine.base.datastructures.BaseDict.get", "completion": "        if key in self:\n            return self[key]\n        else:\n            return default", "idx": 874}
{"namespace": "sqlite_utils.utils.suggest_column_types", "completion": "    column_types = {}\n    for record in records:\n        for key, value in record.items():\n            if key not in column_types:\n                column_types[key] = set()\n            column_types[key].add(type(value))\n    return column_types", "idx": 875}
{"namespace": "sqlite_utils.plugins.get_plugins", "completion": "    plugins = []\n    for plugin_name in pm.list_name_plugin():\n        plugin = {\"name\": plugin_name, \"hooks\": []}\n        for hook_name in pm.hook._name_to_ep[plugin_name]:\n            plugin[\"hooks\"].append(hook_name)\n        dist = pm.get_distribution(plugin_name)\n        if dist is not None:\n            plugin[\"version\"] = dist.version\n            plugin[\"project_name\"] = dist.project_name\n        plugins.append(plugin)\n    return plugins", "idx": 876}
{"namespace": "alembic.config.Config.print_stdout", "completion": "        if not self.cmd_opts or not getattr(self.cmd_opts, \"quiet\", False):\n            if arg:\n                self.stdout.write(text % arg)\n            else:\n                self.stdout.write(text)", "idx": 877}
{"namespace": "alembic.config.Config.set_section_option", "completion": "        if not self.file_config.has_section(section):\n            self.file_config.add_section(section)\n        self.file_config.set(section, name, value)", "idx": 878}
{"namespace": "alembic.command.merge", "completion": "\n    script_directory = ScriptDirectory.from_config(config)\n\n    command_args = dict(\n        message=message,\n        branch_label=branch_label,\n        rev_id=rev_id,\n    )\n    merge_context = autogen.MergeContext(\n        config,\n        script_directory,\n        command_args,\n    )\n\n    def retrieve_migrations(rev, context):\n        merge_context.run(rev, context)\n        return []\n\n    with EnvironmentContext(\n        config,\n        script_directory,\n        fn=retrieve_migrations,\n        as_sql=False,\n        template_args=merge_context.template_args,\n        merge_context=merge_context,\n    ):\n        script_directory.run_env()\n\n    # the merge_context now has MigrationScript structure(s) present.\n\n    scripts = merge_context.generate_scripts()\n    if scripts:\n        return scripts[0]\n    else:\n        return None", "idx": 879}
{"namespace": "alembic.command.upgrade", "completion": "\n    script_directory = ScriptDirectory.from_config(config)\n\n    def upgrade(rev, context):\n        return script_directory._upgrade_revs(revision, rev)\n\n    with EnvironmentContext(\n        config,\n        script_directory,\n        fn=upgrade,\n        as_sql=sql,\n        starting_rev=None,\n        destination_rev=revision,\n        tag=tag,\n    ):\n        script_directory.run_env()", "idx": 880}
{"namespace": "alembic.command.downgrade", "completion": "    script = ScriptDirectory.from_config(config)\n\n    starting_rev = None\n    if \":\" in revision:\n        if not sql:\n            raise util.CommandError(\"Range revision not allowed\")\n        starting_rev, revision = revision.split(\":\", 2)\n\n    def downgrade(rev, context):\n        return script._downgrade_revs(revision, rev)\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=downgrade,\n        as_sql=sql,\n        starting_rev=starting_rev,\n        destination_rev=revision,\n        tag=tag,\n    ):\n        script.run_env()", "idx": 881}
{"namespace": "alembic.command.history", "completion": "\n    script = ScriptDirectory.from_config(config)\n\n    def show_history(rev, context):\n        for sc in script.walk_revisions(rev_range, verbose, indicate_current):\n            config.print_stdout(sc.log_entry)\n\n    with EnvironmentContext(config, script, fn=show_history):\n        script.run_env()", "idx": 882}
{"namespace": "alembic.command.stamp", "completion": "    script = ScriptDirectory.from_config(config)\n\n    def do_stamp(rev, context):\n        return script._stamp_revs(revision, purge)\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=do_stamp,\n        as_sql=sql,\n        tag=tag,\n    ):\n        script.run_env()", "idx": 883}
{"namespace": "alembic.command.ensure_version", "completion": "    script = ScriptDirectory.from_config(config)\n\n    def do_ensure_version(rev, context):\n        return script._ensure_version_table(rev)\n\n    with EnvironmentContext(config, script, fn=do_ensure_version, as_sql=sql):\n        script.run_env()", "idx": 884}
{"namespace": "alembic.autogenerate.compare._compare_server_default", "completion": "    conn_default = _render_server_default_for_compare(\n        conn_col.server_default, autogen_context\n    )\n    metadata_default = _render_server_default_for_compare(\n        metadata_col.server_default, autogen_context\n    )\n\n    if conn_default is not None and metadata_default is not None:\n        if conn_default != metadata_default:\n            alter_column_op.modify_server_default = metadata_default\n            log.info(\n                \"Detected server default change from %r to %r on '%s.%s'\",\n                conn_default,\n                metadata_default,\n                tname,\n                cname,\n            )", "idx": 885}
{"namespace": "alembic.autogenerate.render._render_server_default", "completion": "    rendered = _user_defined_render(\"server_default\", default, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    if sqla_compat._server_default_is_computed(default):\n        return \"Computed()\"\n\n    if sqla_compat._server_default_is_identity(default):\n        return \"Identity()\"\n\n    if isinstance(default, sql.text):\n        return default.text\n\n    if isinstance(default, str) and repr_:\n        return default.strip(\"'\")\n\n    return None", "idx": 886}
{"namespace": "alembic.autogenerate.render._render_constraint", "completion": "    renderer = _constraint_renderers.dispatch(constraint)\n    if renderer:\n        return renderer(constraint, autogen_context, namespace_metadata)\n    else:\n        return f\"Unknown Python object: {type(constraint).__name__}\"", "idx": 887}
{"namespace": "alembic.autogenerate.render._render_unique_constraint", "completion": "    rendered = _user_defined_render(\"unique_constraint\", constraint, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    opts = []\n    if constraint.name:\n        opts.append(\n            (\"name\", repr(_render_gen_name(autogen_context, constraint.name))\n        )\n\n    return _uq_constraint(constraint, autogen_context, False)", "idx": 888}
{"namespace": "alembic.autogenerate.render._render_check_constraint", "completion": "\n    rendered = _user_defined_render(\"check\", constraint, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    if namespace_metadata is not None and constraint in namespace_metadata:\n        return None\n\n    opts = []\n    if constraint.name:\n        opts.append(\n            (\"name\", repr(_render_gen_name(autogen_context, constraint.name)))\n        )\n\n    return (\n        \"%(prefix)sCheckConstraint(%(sql)s, %(args)s)\"\n        % {\n            \"prefix\": _sqlalchemy_autogenerate_prefix(autogen_context),\n            \"sql\": _render_potential_expr(constraint.sqltext, autogen_context),\n            \"args\": \", \".join(\n                [\"%s=%s\" % (kwname, val) for kwname, val in opts]\n            ),\n        }\n    )", "idx": 889}
{"namespace": "alembic.autogenerate.api.compare_metadata", "completion": "    with context.begin_transaction():\n        connection = context.bind\n        insp = inspect(connection)\n        autogen_context = {\n            'imports': set(),\n            'connection': connection,\n            'dialect': connection.dialect,\n            'context': context,\n            'metadata': metadata,\n            'inspector': insp,\n            'change_map': {}\n        }\n\n        autogen_context['opts'] = {\n            'compare_type': compare._compare_type,\n            'compare_server_default': compare._compare_server_default\n        }\n\n        autogen_context['opts'].update(context.opts)\n\n        diffs = compare._compare_tables(autogen_context, metadata.tables, insp.get_table_names())\n\n        return diffs", "idx": 890}
{"namespace": "alembic.autogenerate.api.AutogenContext._within_batch", "completion": "        self._has_batch = True\n        yield\n        self._has_batch = False", "idx": 891}
{"namespace": "alembic.util.sqla_compat._connectable_has_table", "completion": "    if sqla_14:\n        insp = inspect(connectable)\n        return insp.has_table(tablename, schema=schemaname)\n    else:\n        insp = inspect(connectable)\n        return tablename in insp.get_table_names(schema=schemaname)", "idx": 892}
{"namespace": "alembic.util.sqla_compat._get_constraint_final_name", "completion": "\n    if constraint.name is not None:\n        return quoted_name(constraint.name, quote=True, dialect=dialect)\n    elif isinstance(constraint, Index):\n        if sqla_14:\n            return constraint.name\n        else:\n            return \"_\".join(\n                [\n                    quoted_name(\n                        c.name, quote=True, dialect=dialect\n                    )\n                    for c in constraint.columns\n                ]\n            )\n    else:\n        return None", "idx": 893}
{"namespace": "alembic.testing.env.env_file_fixture", "completion": "    dir_ = os.path.join(_get_staging_directory(), \"scripts\")\n    path = os.path.join(dir_, \"env.py\")\n    with open(path, \"w\") as f:\n        f.write(txt)", "idx": 894}
{"namespace": "alembic.testing.env._no_sql_testing_config", "completion": "    dir_ = os.path.join(_get_staging_directory(), \"scripts\")\n    url = f\"{dialect}:///{dir_}/foo.db\"\n\n    return _write_config_file(\n        \"\"\"\n[alembic]\nscript_location = %s\nsqlalchemy.url = %s\n%s\n\n[loggers]\nkeys = root,sqlalchemy\n\n[handlers]\nkeys = console\n\n[logger_root]\nlevel = WARN\nhandlers = console\nqualname =\n\n[logger_sqlalchemy]\nlevel = DEBUG\nhandlers =\nqualname = sqlalchemy.engine\n\n[handler_console]\nclass = StreamHandler\nargs = (sys.stderr,)\nlevel = NOTSET\nformatter = generic\n\n[formatters]\nkeys = generic\n\n[formatter_generic]\nformat = %%(levelname)-5.5s [%%(name)s] %%(message)s\ndatefmt = %%H:%%M:%%S\n        \"\"\"\n        % (dir_, url, directives)\n    )", "idx": 895}
{"namespace": "alembic.testing.env._write_config_file", "completion": "    cfg = _testing_config()\n    with open(\"alembic.ini\", \"w\") as file:\n        file.write(text)\n    return cfg", "idx": 896}
{"namespace": "alembic.testing.env.three_rev_fixture", "completion": "\n    scriptdir = script.ScriptDirectory.from_config(cfg)\n\n    a = util.rev_id()\n    b = util.rev_id()\n    c = util.rev_id()\n\n    write_script(scriptdir, a, \"\"\"\n    # upgrade function\n    def upgrade():\n        pass\n\n    # downgrade function\n    def downgrade():\n        pass\n    \"\"\")\n\n    write_script(scriptdir, b, \"\"\"\n    # upgrade function\n    def upgrade():\n        pass\n\n    # downgrade function\n    def downgrade():\n        pass\n    \"\"\")\n\n    write_script(scriptdir, c, \"\"\"\n    # upgrade function\n    def upgrade():\n        pass\n\n    # downgrade function\n    def downgrade():\n        pass\n    \"\"\")\n\n    return a, b, c", "idx": 897}
{"namespace": "alembic.testing.env.multi_heads_fixture", "completion": "\n    d = util.rev_id()\n    e = util.rev_id()\n    f = util.rev_id()\n\n    script = ScriptDirectory.from_config(cfg)\n    script.generate_revision(d, \"revision d\", refresh=True, head=a)\n    write_script(\n        script,\n        d,\n        \"\"\"\\\n\"Rev D\"\nrevision = '%s'\ndown_revision = '%s'\n\nfrom alembic import op\n\n\ndef upgrade():\n    op.execute(\"CREATE STEP 4\")\n\n\ndef downgrade():\n    op.execute(\"DROP STEP 4\")\n\n\"\"\"\n        % (d, a),\n    )\n\n    script.generate_revision(e, \"revision e\", refresh=True, head=b)\n    write_script(\n        script,\n        e,\n        \"\"\"\\\n\"Rev E\"\nrevision = '%s'\ndown_revision = '%s'\n\nfrom alembic import op\n\n\ndef upgrade():\n    op.execute(\"CREATE STEP 5\")\n\n\ndef downgrade():\n    op.execute(\"DROP STEP 5\")\n\n\"\"\"\n        % (e, b),\n    )\n\n    script.generate_revision(f, \"revision f\", refresh=True, head=c)\n    write_script(\n        script,\n        f,\n        \"\"\"\\\n\"Rev F\"\nrevision = '%s'\ndown_revision = '%s'\n\nfrom alembic import op\n\n\ndef upgrade():\n    op.execute(\"CREATE STEP 6\")\n\n\ndef downgrade():\n    op.execute(\"DROP STEP 6\")\n\n\"\"\"\n        % (f, c),\n    )\n\n    return d, e, f", "idx": 898}
{"namespace": "alembic.testing.fixtures.capture_db", "completion": "    engine = mock.Mock()\n    engine.dialect.name = dialect\n    buffer = io.StringIO()\n    engine.execute = buffer.write\n    return engine, buffer", "idx": 899}
{"namespace": "alembic.testing.fixtures.capture_engine_context_buffer", "completion": "\n    if kw.pop(\"bytes_io\", False):\n        buf = io.BytesIO()\n    else:\n        buf = io.StringIO()\n\n    dialect_name = kw.get(\"dialect_name\", \"sqlite\")\n    output_buffer = buf\n\n    def dump(sql, *multiparams, **params):\n        output_buffer.write(str(sql.compile(dialect=dialect_name)) + \"\\n\")\n\n    engine = testing_config.db\n    event.listen(engine, \"before_cursor_execute\", dump)\n\n    yield buf\n\n    event.remove(engine, \"before_cursor_execute\", dump)", "idx": 900}
{"namespace": "alembic.operations.schemaobj.SchemaObjects.unique_constraint", "completion": "        m = self.metadata()\n        columns = [sa_schema.Column(n, NULLTYPE) for n in local_cols]\n        t = sa_schema.Table(source, m, *columns, schema=schema)\n        u = sa_schema.UniqueConstraint(*[t.c[n] for n in local_cols], name=name, **kw)\n        t.append_constraint(u)\n        return u", "idx": 901}
{"namespace": "alembic.operations.schemaobj.SchemaObjects.index", "completion": "        m = self.metadata()\n        t = sa_schema.Table(tablename, m, schema=schema)\n        idx = sa_schema.Index(name, *columns, table=t, **kw)\n        return idx", "idx": 902}
{"namespace": "alembic.operations.ops.DropConstraintOp.from_constraint", "completion": "        if isinstance(constraint, ForeignKeyConstraint):\n            return DropConstraintOp(\n                constraint_name=constraint.name,\n                table_name=constraint.parent.name,\n                type_=\"foreignkey\",\n                schema=constraint.parent.schema,\n            )\n        elif isinstance(constraint, CheckConstraint):\n            return DropConstraintOp(\n                constraint_name=constraint.name,\n                table_name=constraint.table.name,\n                type_=\"check\",\n                schema=constraint.table.schema,\n            )\n        elif isinstance(constraint, UniqueConstraint):\n            return DropConstraintOp(\n                constraint_name=constraint.name,\n                table_name=constraint.table.name,\n                type_=\"unique\",\n                schema=constraint.table.schema,\n            )\n        elif isinstance(constraint, PrimaryKeyConstraint):\n            return DropConstraintOp(\n                constraint_name=None,\n                table_name=constraint.table.name,\n                type_=\"primary_key\",\n                schema=constraint.table.schema,\n            )\n        else:\n            raise NotImplementedError(f\"Constraint type {type(constraint)} not supported\")", "idx": 903}
{"namespace": "alembic.operations.ops.DropConstraintOp.to_constraint", "completion": "        if self._reverse is not None:\n            constraint = self._reverse.to_constraint()\n            constraint.name = self.constraint_name\n            constraint.table.name = self.table_name\n            constraint.table.schema = self.schema\n            return constraint\n        else:\n            raise ValueError(\"Reverse operation is not present.\")", "idx": 904}
{"namespace": "alembic.operations.ops.CreatePrimaryKeyOp.to_constraint", "completion": "        if migration_context is not None:\n            schema_obj = schemaobj.SchemaObjects(migration_context)\n            table = schema_obj.table(self.table_name, schema=self.schema)\n            return schema_obj.primary_key_constraint(\n                self.constraint_name, table, *self.columns, **self.kw\n            )\n        else:\n            raise ValueError(\"Migration context is required to create the primary key constraint.\")", "idx": 905}
{"namespace": "alembic.operations.ops.CreateIndexOp.from_index", "completion": "        return cls(\n            index.name,\n            index.table.name,\n            [c.name if isinstance(c, Column) else c for c in index.columns],\n            schema=index.table.schema,\n            unique=index.unique,\n            **index.dialect_kwargs,\n        )", "idx": 906}
{"namespace": "alembic.operations.ops.DropIndexOp.from_index", "completion": "        return cls(\n            index.name,\n            index.table.name if index.table is not None else None,\n            schema=index.table.schema if index.table is not None else None,\n            if_exists=None,\n            _reverse=CreateIndexOp.from_index(index),\n            **index.kwargs,\n        )", "idx": 907}
{"namespace": "alembic.operations.ops.DropIndexOp.to_index", "completion": "        schema_obj = schemaobj.SchemaObjects(migration_context)\n        idx = schema_obj.index(\n            self.index_name,\n            self.table_name,\n            schema=self.schema,\n            **self.kw,\n        )\n        return idx", "idx": 908}
{"namespace": "alembic.operations.ops.CreateTableOp.from_table", "completion": "        return cls(\n            table.name,\n            [c.copy() for c in table.columns],\n            schema=table.schema,\n            _namespace_metadata=_namespace_metadata or table.metadata,\n            _constraints_included=True,\n            comment=table.comment,\n            info=table.info,\n            prefixes=table.prefixes,\n            **table.kwargs,\n        )", "idx": 909}
{"namespace": "alembic.operations.ops.DropTableOp.from_table", "completion": "        if _namespace_metadata is None:\n            _namespace_metadata = table.metadata\n\n        return cls(\n            table.name,\n            schema=table.schema,\n            table_kw=dict(\n                comment=table.comment,\n                info=dict(table.info),\n                prefixes=list(table._prefixes) if table._prefixes else [],\n                **table.kwargs,\n            ),\n            _reverse=CreateTableOp.from_table(\n                table, _namespace_metadata=_namespace_metadata\n            ),\n        )", "idx": 910}
{"namespace": "alembic.operations.ops.DropTableOp.to_table", "completion": "\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.table(\n            self.table_name,\n            schema=self.schema,\n            prefixes=list(self.prefixes) if self.prefixes else [],\n            comment=self.comment,\n            info=self.info.copy() if self.info else {},\n            _constraints_included=False,\n            **self.table_kw,\n        )", "idx": 911}
{"namespace": "alembic.operations.ops.AlterColumnOp.to_diff_tuple", "completion": "        diff_tuple = (\n            \"modify_column\",\n            self.table_name,\n            self.column_name,\n            self.existing_type,\n            self.modify_type,\n            self.existing_nullable,\n            self.modify_nullable,\n            self.existing_server_default,\n            self.modify_server_default,\n            self.existing_comment,\n            self.modify_comment,\n            self.modify_name,\n        )\n        return diff_tuple", "idx": 912}
{"namespace": "alembic.operations.ops.AddColumnOp.reverse", "completion": "        \"\"\"\n        This function reverses the operation performed by the AddColumnOp.\n        Input-Output Arguments\n        :param self: AddColumnOp. An instance of the AddColumnOp class.\n        :return: DropColumnOp.\n        \"\"\"\n        return DropColumnOp(self.table_name, self.column.name, schema=self.schema)", "idx": 913}
{"namespace": "alembic.operations.ops.DropColumnOp.reverse", "completion": "        if self._reverse is None:\n            raise ValueError(\"Reverse operation not available\")\n        return self._reverse", "idx": 914}
{"namespace": "alembic.operations.ops.DropColumnOp.from_column_and_tablename", "completion": "\n        return cls(tname, col.name, schema=schema, _reverse=AddColumnOp.from_column(col))", "idx": 915}
{"namespace": "alembic.operations.ops.DropColumnOp.to_column", "completion": "        if self._reverse is not None:\n            return self._reverse.column\n        else:\n            schema_obj = schemaobj.SchemaObjects(migration_context)\n            return schema_obj.column(self.column_name, NULLTYPE)", "idx": 916}
{"namespace": "alembic.script.revision.RevisionMap.heads", "completion": "        rev_map = self._revision_map\n        heads = [rev for rev in rev_map if not rev_map[rev]]\n        return tuple(heads)", "idx": 917}
{"namespace": "alembic.script.revision.RevisionMap.add_revision", "completion": "        if not _replace and revision.revision in self._revision_map:\n            raise RevisionError(\n                f\"Revision {revision.revision} already exists in the map\"\n            )\n\n        self._generator = lambda: [revision] + list(self._generator())\n        self._revision_map = None  # clear the memoized property", "idx": 918}
{"namespace": "alembic.script.revision.RevisionMap.get_revisions", "completion": "        if id_ is None:\n            return ()\n        elif isinstance(id_, str):\n            return (self._revision_map.get(id_, None),)\n        else:\n            return tuple(self._revision_map.get(i, None) for i in id_)", "idx": 919}
{"namespace": "alembic.script.revision.RevisionMap.get_revision", "completion": "        resolved_id, branch_label = self._resolve_revision_number(id_)\n\n        if resolved_id == \"head\":\n            return self.get_current_head(branch_label)\n        elif resolved_id == \"base\":\n            bases = self._get_base_revisions(branch_label)\n            if len(bases) > 1:\n                raise MultipleHeads(bases, \"base\")\n            return self._revision_for_ident(bases[0], branch_label)\n        else:\n            revisions = self.get_revisions(resolved_id)\n            if len(revisions) > 1:\n                raise MultipleHeads(revisions, id_)\n            return revisions[0]", "idx": 920}
{"namespace": "alembic.script.revision.RevisionMap.filter_for_lineage", "completion": "\n        if check_against is None:\n            return tuple(targets)\n\n        lineage = self._get_descendant_nodes(\n            [check_against], include_dependencies=include_dependencies\n        )\n\n        return tuple(\n            target\n            for target in targets\n            if target in lineage\n        )", "idx": 921}
{"namespace": "alembic.script.revision.RevisionMap.iterate_revisions", "completion": "        upper_rev = self.get_revision(upper)\n        lower_rev = self.get_revision(lower)\n\n        if upper_rev is None:\n            raise RevisionError(f\"Upper revision {upper} not found\")\n        if lower_rev is None:\n            raise RevisionError(f\"Lower revision {lower} not found\")\n\n        if implicit_base:\n            base_revs = self._get_base_revisions(upper)\n            if base_revs:\n                yield from (self.get_revision(base) for base in base_revs)\n\n        current_rev = upper_rev\n        while current_rev != lower_rev:\n            if current_rev is None:\n                raise RevisionError(\n                    f\"Revision {upper} is not an ancestor of revision {lower}\"\n                )\n            yield current_rev\n            if select_for_downgrade:\n                next_revs = current_rev._all_down_revisions\n            else:\n                next_revs = current_rev.nextrev\n            if not next_revs:\n                raise RevisionError(\n                    f\"No next revision found for {current_rev.revision}\"\n                )\n            if len(next_revs) > 1:\n                raise RevisionError(\n                    f\"Multiple next revisions found for {current_rev.revision}\"\n                )\n            current_rev = self.get_revision(next_revs[0])\n\n        if inclusive:\n            yield lower_rev\n\n        if assert_relative_length:\n            num_revisions = len(list(self.iterate_revisions(upper, lower)))\n            if num_revisions != len(list(self.iterate_revisions(lower, upper))):\n                raise RevisionError(\n                    \"Number of revisions between upper and lower is not the same in both directions\"\n                )", "idx": 922}
{"namespace": "alembic.script.revision.RevisionMap._topological_sort", "completion": "\n        sorted_revisions: List[str] = []\n        visited: Set[str] = set()\n\n        def visit(rev: Revision, ancestors: Set[str]) -> None:\n            if rev.revision in ancestors:\n                raise DependencyCycleDetected(list(ancestors))\n\n            if rev.revision not in visited:\n                ancestors.add(rev.revision)\n                for next_rev in rev.nextrev:\n                    visit(self.get_revision(next_rev), ancestors)\n                ancestors.remove(rev.revision)\n                visited.add(rev.revision)\n                sorted_revisions.append(rev.revision)\n\n        for head in heads:\n            visit(self.get_revision(head), set())\n\n        return sorted_revisions", "idx": 923}
{"namespace": "alembic.script.revision.Revision._all_down_revisions", "completion": "        down_revisions = set(self.down_revision or ())\n        dependencies = set(self._resolved_dependencies or ())\n        all_down_revisions = tuple(sorted(down_revisions.union(dependencies)))\n        return all_down_revisions", "idx": 924}
{"namespace": "alembic.script.revision.Revision._normalized_down_revisions", "completion": "        return self._versioned_down_revisions", "idx": 925}
{"namespace": "alembic.script.write_hooks._invoke", "completion": "    formatter = _registry.get(name)\n    if formatter is None:\n        raise CommandError(f\"No formatter with name '{name}' registered\")\n    return formatter(revision, **options)", "idx": 926}
{"namespace": "bplustree.memory.FileMemory.get_node", "completion": "        with self._lock.gen_rlock():\n            node_data = self._cache.get(page)\n            if node_data is not None:\n                return Node.from_bytes(node_data)\n\n            start = page * self._tree_conf.page_size\n            stop = start + self._tree_conf.page_size\n            node_data = read_from_file(self._fd, start, stop)\n            node = Node.from_bytes(node_data)\n            self._cache[page] = node_data\n            return node", "idx": 927}
{"namespace": "bplustree.memory.FileMemory.next_available_page", "completion": "        with self.read_transaction:\n            self._fd.seek(0, io.SEEK_END)\n            last_byte = self._fd.tell()\n            last_page = int(last_byte / self._tree_conf.page_size)\n            return last_page + 1", "idx": 928}
{"namespace": "bplustree.memory.FileMemory.get_metadata", "completion": "        first_page_data = self.get_page(0)\n        root_node_page = int.from_bytes(first_page_data[:PAGE_REFERENCE_BYTES], byteorder=ENDIAN)\n        page_size = int.from_bytes(first_page_data[PAGE_REFERENCE_BYTES:PAGE_REFERENCE_BYTES*2], byteorder=ENDIAN)\n        order = int.from_bytes(first_page_data[PAGE_REFERENCE_BYTES*2:PAGE_REFERENCE_BYTES*2+1], byteorder=ENDIAN)\n        key_size = int.from_bytes(first_page_data[PAGE_REFERENCE_BYTES*2+1:PAGE_REFERENCE_BYTES*2+3], byteorder=ENDIAN)\n        value_size = int.from_bytes(first_page_data[PAGE_REFERENCE_BYTES*2+3:PAGE_REFERENCE_BYTES*2+5], byteorder=ENDIAN)\n        tree_conf = TreeConf(page_size=page_size, order=order, key_size=key_size, value_size=value_size)\n        return root_node_page, tree_conf", "idx": 929}
{"namespace": "bplustree.memory.FileMemory.set_metadata", "completion": "        metadata = (\n            root_node_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN) +\n            tree_conf.page_size.to_bytes(OTHERS_BYTES, ENDIAN) +\n            tree_conf.order.to_bytes(OTHERS_BYTES, ENDIAN) +\n            tree_conf.key_size.to_bytes(OTHERS_BYTES, ENDIAN) +\n            tree_conf.value_size.to_bytes(OTHERS_BYTES, ENDIAN)\n        )\n        write_to_file(self._fd, self._dir_fd, metadata)", "idx": 930}
{"namespace": "bplustree.memory.WAL.checkpoint", "completion": "        if self._not_committed_pages:\n            logger.warning('There are uncommitted pages in the WAL')\n\n        fsync_file_and_dir(self._fd.fileno(), self._dir_fd)\n\n        for page, page_data in self._committed_pages.items():\n            yield page, page_data\n\n        self._fd.close()\n        os.remove(self.filename)\n\n        if self._dir_fd is not None:\n            os.fsync(self._dir_fd)", "idx": 931}
{"namespace": "bplustree.memory.WAL.commit", "completion": "        if self._not_committed_pages:\n            self._add_frame(FrameType.COMMIT)", "idx": 932}
{"namespace": "bplustree.memory.WAL.rollback", "completion": "        if self._not_committed_pages:\n            self._add_frame(FrameType.ROLLBACK)", "idx": 933}
{"namespace": "bplustree.entry.Record.dump", "completion": "        used_key_length = len(self._tree_conf.serializer.serialize(self.key))\n        used_value_length = len(self.value) if self.value else 0\n\n        data = (\n            used_key_length.to_bytes(USED_KEY_LENGTH_BYTES, ENDIAN) +\n            self._tree_conf.serializer.serialize(self.key) +\n            used_value_length.to_bytes(USED_VALUE_LENGTH_BYTES, ENDIAN) +\n            (self.value if self.value else b'\\x00' * self._tree_conf.value_size) +\n            (self.overflow_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN) if self.overflow_page else b'\\x00' * PAGE_REFERENCE_BYTES)\n        )\n\n        return data", "idx": 934}
{"namespace": "bplustree.entry.Reference.__repr__", "completion": "        return '<Reference: key={} before={} after={}>'.format(self.key, self.before, self.after)", "idx": 935}
{"namespace": "bplustree.node.Node.dump", "completion": "        data = bytearray()\n        entry_length = self._entry_class(self._tree_conf).length\n        for entry in self.entries:\n            data.extend(entry.dump())\n\n        used_page_length = NODE_TYPE_BYTES + USED_PAGE_LENGTH_BYTES + len(data) + PAGE_REFERENCE_BYTES\n        data[:NODE_TYPE_BYTES] = self._node_type_int.to_bytes(NODE_TYPE_BYTES, ENDIAN)\n        data[NODE_TYPE_BYTES:end_used_page_length] = used_page_length.to_bytes(USED_PAGE_LENGTH_BYTES, ENDIAN)\n        data[end_used_page_length:end_header] = (self.next_page or 0).to_bytes(PAGE_REFERENCE_BYTES, ENDIAN)\n\n        padding = b'\\x00' * (self._tree_conf.page_size - len(data))\n        data.extend(padding)\n\n        return data", "idx": 936}
{"namespace": "bplustree.node.Node._find_entry_index", "completion": "        entry = self._entry_class(self._tree_conf, key=key)\n        return bisect.bisect_left(self.entries, entry)", "idx": 937}
{"namespace": "bplustree.node.Node.from_page_data", "completion": "        node_type = int.from_bytes(data[:NODE_TYPE_BYTES], ENDIAN)\n        if node_type == cls._node_type_int:\n            return cls(tree_conf, data, page=page)\n        else:\n            raise ValueError(f\"Invalid node type: {node_type}\")", "idx": 938}
{"namespace": "bplustree.tree.BPlusTree._root_node", "completion": "        with self._mem.read_transaction:\n            return self._mem.get_node(self._root_node_page)", "idx": 939}
{"namespace": "bplustree.tree.BPlusTree._left_record_node", "completion": "        node = self._root_node\n        while not isinstance(node, (LonelyRootNode, LeafNode)):\n            node = self._mem.get_node(node.children[0])\n        return node", "idx": 940}
{"namespace": "mopidy.ext.Extension.get_cache_dir", "completion": "        cache_dir = path.get_or_create_dir(\n            config[\"core\"][\"cache_dir\"], cls.ext_name\n        )\n        return cache_dir", "idx": 941}
{"namespace": "mopidy.ext.Extension.get_config_dir", "completion": "        if cls.ext_name is None:\n            raise AssertionError\n        config_dir_path = path.expand_path(config[\"core\"][\"config_dir\"]) / cls.ext_name\n        path.get_or_create_dir(config_dir_path)\n        return config_dir_path", "idx": 942}
{"namespace": "mopidy.ext.Extension.get_data_dir", "completion": "        if cls.ext_name is None:\n            raise AssertionError\n        data_dir_path = (\n            path.expand_path(config[\"core\"][\"data_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(data_dir_path)\n        return data_dir_path", "idx": 943}
{"namespace": "fs.time.epoch_to_datetime", "completion": "    if t is None:\n        return None\n    else:\n        return datetime.utcfromtimestamp(t)", "idx": 944}
{"namespace": "fs.path.normpath", "completion": "\n    parts = path.split(\"/\")\n    new_parts = []\n    for part in parts:\n        if part == \"..\":\n            if new_parts and new_parts[-1] != \"..\":\n                new_parts.pop()\n            else:\n                new_parts.append(part)\n        elif part and part != \".\":\n            new_parts.append(part)\n    new_path = \"/\".join(new_parts)\n    if path.startswith(\"/\") and not new_path.startswith(\"/\"):\n        new_path = \"/\" + new_path\n    return new_path", "idx": 945}
{"namespace": "fs.path.iteratepath", "completion": "    return [component for component in normpath(path).split(\"/\") if component]", "idx": 946}
{"namespace": "fs.path.recursepath", "completion": "    path = normpath(path)\n    components = iteratepath(path)\n    paths = [\"/\" + \"/\".join(components[:i+1]) for i in range(len(components))]\n    if reverse:\n        paths.reverse()\n    return paths", "idx": 947}
{"namespace": "fs.path.join", "completion": "    return normpath(\"/\".join(paths))", "idx": 948}
{"namespace": "fs.path.parts", "completion": "    path = relpath(normpath(path))\n    if not path:\n        return []\n    return path.split(\"/\")", "idx": 949}
{"namespace": "fs.path.splitext", "completion": "    if \".\" not in path:\n        return path, \"\"\n    split = path.rsplit(\".\", 1)\n    return (split[0], \".\" + split[1])", "idx": 950}
{"namespace": "fs.path.isbase", "completion": "    return abspath(path2).startswith(abspath(path1))", "idx": 951}
{"namespace": "fs.path.frombase", "completion": "    if not isparent(path1, path2):\n        raise ValueError(f\"{path1} is not a parent directory of {path2}\")\n    return normpath(path2[len(path1):])", "idx": 952}
{"namespace": "fs.path.relativefrom", "completion": "\n    base_parts = iteratepath(normpath(base))\n    path_parts = iteratepath(normpath(path))\n\n    # Find the common prefix\n    i = 0\n    for i, (base_part, path_part) in enumerate(zip(base_parts, path_parts)):\n        if base_part != path_part:\n            break\n    else:\n        i += 1\n\n    # Add backrefs for the remaining parts of the base path\n    rel_parts = [\"..\"] * (len(base_parts) - i)\n    rel_parts.extend(path_parts[i:])\n\n    return \"/\".join(rel_parts)", "idx": 953}
{"namespace": "fs.path.iswildcard", "completion": "    return any(char in _WILD_CHARS for char in path)", "idx": 954}
{"namespace": "fs.wildcard.match", "completion": "    pattern = pattern + '\\Z(?ms)' if _is_wildcard_pattern(pattern) else re.escape(pattern)\n    if pattern not in _PATTERN_CACHE:\n        _PATTERN_CACHE[pattern] = re.compile(pattern)\n    return bool(_PATTERN_CACHE[pattern].match(name))", "idx": 955}
{"namespace": "fs.wildcard.imatch", "completion": "    try:\n        re_pat = _PATTERN_CACHE[(pattern, True)]\n    except KeyError:\n        res = \"(?i)(?ms)\" + _translate(pattern) + r\"\\Z\"  # Case-insensitive pattern\n        _PATTERN_CACHE[(pattern, True)] = re_pat = re.compile(res)\n    return re_pat.match(name) is not None", "idx": 956}
{"namespace": "fs.wildcard.get_matcher", "completion": "    if case_sensitive:\n        return partial(match_any, patterns)\n    else:\n        return partial(imatch_any, patterns)", "idx": 957}
{"namespace": "fs._url_tools.url_quote", "completion": "\n    if _WINDOWS_PLATFORM:\n        drive, tail = os.path.splitdrive(path_snippet)\n        quoted_path = urllib.parse.quote(tail)\n        return drive + quoted_path\n    else:\n        return urllib.request.pathname2url(path_snippet)", "idx": 958}
{"namespace": "fs._ftp_parse.parse", "completion": "    decoders = get_decoders()\n    parsed_info = []\n    for line in lines:\n        if line.strip():  # Check if the line is not blank\n            for regex, decoder in decoders:\n                match = regex.match(line)\n                if match:\n                    parsed_info.append(decoder(match))\n                    break\n    return parsed_info", "idx": 959}
{"namespace": "fs._ftp_parse._parse_time", "completion": "    for fmt in formats:\n        try:\n            dt = datetime.strptime(t, fmt)\n            return (dt - EPOCH_DT).total_seconds()\n        except ValueError:\n            pass\n    return None", "idx": 960}
{"namespace": "fs.permissions.Permissions.parse", "completion": "        user = ls[0:3]\n        group = ls[3:6]\n        other = ls[6:9]\n        return cls(user=user, group=group, other=other)", "idx": 961}
{"namespace": "fs.permissions.Permissions.create", "completion": "        return Permissions(init)", "idx": 962}
{"namespace": "fs.info.Info.suffix", "completion": "        name = self.name\n        if \".\" in name:\n            return name[name.rindex(\".\"):]\n        else:\n            return \"\"", "idx": 963}
{"namespace": "fs.info.Info.suffixes", "completion": "        name = self.get(\"basic\", \"name\")\n        if name.startswith(\".\") and name.count(\".\") == 1:\n            return []\n        else:\n            return name.split(\".\")[1:]", "idx": 964}
{"namespace": "fs.info.Info.stem", "completion": "        name = self.get(\"basic\", \"name\")\n        stem, _, _ = name.partition(\".\")\n        return stem", "idx": 965}
{"namespace": "fs.info.Info.type", "completion": "            self._require_namespace(\"details\")\n            return cast(ResourceType, self.get(\"details\", \"type\"))", "idx": 966}
{"namespace": "fs.info.Info.created", "completion": "        \"\"\"`~datetime.datetime`: the resource creation time, or `None`.\n\n        Requires the ``\"details\"`` namespace.\n\n        Raises:\n            ~fs.errors.MissingInfoNamespace: if the ``\"details\"``\n                namespace is not in the Info.\n\n        \"\"\"\n        self._require_namespace(\"details\")\n        _time = self._make_datetime(self.get(\"details\", \"created\"))\n        return _time", "idx": 967}
{"namespace": "pyinfra.connectors.mech.MechInventoryConnector.make_names_data", "completion": "\n        mech_config = get_mech_config(limit)\n        mech_options = get_mech_options()\n\n        names_data = []\n\n        for line in mech_config:\n            if line.startswith(\"Host \"):\n                host = {\n                    \"Host\": line.split()[1],\n                    \"HostName\": \"\",\n                    \"Port\": \"\",\n                    \"User\": \"\",\n                    \"IdentityFile\": \"\",\n                }\n                for config_line in mech_config:\n                    if config_line.startswith(\"HostName \"):\n                        host[\"HostName\"] = config_line.split()[1]\n                    elif config_line.startswith(\"Port \"):\n                        host[\"Port\"] = config_line.split()[1]\n                    elif config_line.startswith(\"User \"):\n                        host[\"User\"] = config_line.split()[1]\n                    elif config_line.startswith(\"IdentityFile \"):\n                        host[\"IdentityFile\"] = config_line.split()[1]\n\n                data = {\n                    \"ssh_hostname\": host[\"HostName\"],\n                    \"ssh_port\": host[\"Port\"],\n                    \"ssh_user\": host[\"User\"],\n                    \"ssh_key\": host[\"IdentityFile\"],\n                }\n\n                mech_host = host[\"Host\"]\n                if mech_host in mech_options.get(\"data\", {}):\n                    data.update(mech_options[\"data\"][mech_host])\n\n                groups = mech_options.get(\"groups\", {}).get(mech_host, [])\n                if \"@mech\" not in groups:\n                    groups.append(\"@mech\")\n\n                names_data.append({\"name\": f\"@mech/{host['Host']}\", \"data\": data, \"groups\": groups})\n\n        return names_data", "idx": 968}
{"namespace": "pyinfra.connectors.ansible.AnsibleInventoryConnector.make_names_data", "completion": "\n        if inventory_filename is None:\n            raise InventoryError(\"No Ansible inventory filename provided!\")\n\n        if not path.exists(inventory_filename):\n            raise InventoryError(f\"Could not find Ansible inventory file: {inventory_filename}\")\n\n        with open(inventory_filename, \"r\") as inventory_file:\n            inventory_data = inventory_file.read()\n\n        # Parse the inventory data (pseudocode)\n        # parsed_data = parse_inventory_data(inventory_data)\n\n        # Return the parsed data\n        # return parsed_data", "idx": 969}
{"namespace": "pyinfra.operations.files.rsync", "completion": "\n    show_rsync_warning()\n    yield RsyncCommand(src, dest, flags=flags)", "idx": 970}
{"namespace": "pyinfra.operations.files.get", "completion": "\n    # Add deploy directory?\n    if add_deploy_dir and state.cwd:\n        src = os.path.join(state.cwd, src)\n\n    # Ensure the destination directory exists\n    if create_local_dir:\n        os.makedirs(os.path.dirname(dest), exist_ok=True)\n\n    # Do we download the file? Force by default\n    download = force\n\n    # Destination file exists?\n    if os.path.isfile(dest):\n        # Check if force is False and the local file matches the remote file\n        if not force and get_file_sha1(dest) == host.get_fact(Sha1File, path=src):\n            download = False\n\n    # Download the file if needed\n    if download:\n        with open(dest, 'wb') as f:\n            f.write(host.get_file(src))\n\n    # Example\n    files.get(\n        src=\"remote_file.txt\",\n        dest=\"/local/directory/file.txt\",\n        add_deploy_dir=True,\n        create_local_dir=False,\n        force=False,\n    )\n\n    # Note: This operation is suitable for small to medium-sized files. For large files, consider using the `files.download` operation.", "idx": 971}
{"namespace": "pyinfra.operations.files.put", "completion": "\n    if add_deploy_dir and state.cwd:\n        src = os.path.join(state.cwd, src)\n\n    if not assume_exists and not os.path.exists(src):\n        raise IOError(\"No such file: {0}\".format(src))\n\n    if create_remote_dir:\n        yield from _create_remote_dir(state, host, dest, user, group)\n\n    remote_file = host.get_fact(File, path=dest)\n\n    # No remote file, so assume exists and upload it \"blind\"\n    if not remote_file or force:\n        yield FileUploadCommand(src, dest, local_temp_filename=state.get_temp_filename(src))\n\n    # No local file, so always upload\n    elif not os.path.exists(src):\n        yield FileUploadCommand(src, dest, local_temp_filename=state.get_temp_filename(src))\n\n    # Local file exists - check if it matches our remote\n    else:\n        local_sum = get_file_sha1(src)\n        remote_sum = host.get_fact(Sha1File, path=dest)\n\n        # Check sha1sum, upload if needed\n        if local_sum != remote_sum:\n            yield FileUploadCommand(src, dest, local_temp_filename=state.get_temp_filename(src))", "idx": 972}
{"namespace": "pyinfra.operations.files.file", "completion": "\n    path = _validate_path(path)\n\n    info = host.get_fact(File, path=path)\n\n    if info is False:  # not a file\n        yield from _raise_or_remove_invalid_path(\n            \"file\",\n            path,\n            force,\n            force_backup,\n            force_backup_dir,\n        )\n        info = None\n\n    if not present:\n        if info:\n            yield from file_utils.remove(path, force=force, force_backup=force_backup, force_backup_dir=force_backup_dir)\n        else:\n            host.noop(\"file {0} does not exist\".format(path))\n        return\n\n    if info is None:  # create\n        if create_remote_dir:\n            yield from _create_remote_dir(state, host, path, user, group)\n\n        if touch:\n            yield StringCommand(\"touch\", QuoteString(path))\n\n        if user or group:\n            yield file_utils.chown(path, user, group)\n\n        if mode:\n            yield file_utils.chmod(path, mode)\n\n    else:  # edit\n        changed = False\n\n        # Check user/group\n        if (user and info[\"user\"] != user) or (group and info[\"group\"] != group):\n            yield file_utils.chown(path, user, group)\n            changed = True\n\n        # Check mode\n        if mode and info[\"mode\"] != mode:\n            yield file_utils.chmod(path, mode)\n            changed = True\n\n        if not changed:\n            host.noop(\"file {0} already exists\".format(path))", "idx": 973}
{"namespace": "pyinfra.operations.python.call", "completion": "\n    return FunctionCommand(\n        function,\n        args=args,\n        kwargs=kwargs,\n    )", "idx": 974}
{"namespace": "pyinfra.api.operation.add_op", "completion": "\n    # Get the operation meta\n    op_meta = OperationMeta()\n\n    # Get the operation order from the stack\n    op_order = get_operation_order_from_stack()\n\n    # Get the call location\n    call_location = get_call_location()\n\n    # Get the file SHA1\n    file_sha1 = get_file_sha1(call_location)\n\n    # Make a hash\n    op_hash = make_hash(op_order, file_sha1)\n\n    # Create a new state operation meta\n    state_op_meta = StateOperationMeta(\n        op_order,\n        op_hash,\n        op_func,\n        op_meta,\n    )\n\n    # Get the execution keyword arguments\n    execution_kwargs = get_execution_kwarg_keys(kwargs)\n\n    # Pop global arguments\n    global_args = pop_global_arguments(kwargs)\n\n    # Iterate over each host in the state\n    for host in state.inventory:\n        # Create a new state operation host data\n        state_op_host_data = StateOperationHostData(\n            host,\n            op_func,\n            args,\n            kwargs,\n            global_args,\n            execution_kwargs,\n        )\n\n        # Add the state operation host data to the state operation meta\n        state_op_meta.add_host_data(state_op_host_data)\n\n    # Add the state operation meta to the state\n    state.add_op_meta(state_op_meta)", "idx": 975}
{"namespace": "pyinfra.api.facts.get_facts", "completion": "\n    # Create a dictionary to store the retrieved facts\n    facts = {}\n\n    # Iterate over the active hosts in the state's inventory\n    for host in state.inventory:\n        # Spawn a greenlet for each host to retrieve the facts\n        greenlet = gevent.spawn(get_fact, state, host, *args, **kwargs)\n        # Store the greenlet in the facts dictionary with the host as the key\n        facts[host] = greenlet\n\n    # Wait for the greenlets to complete\n    gevent.joinall(facts.values())\n\n    # Store the results in a dictionary\n    results = {host: greenlet.value for host, greenlet in facts.items()}\n\n    return results", "idx": 976}
{"namespace": "pyinfra.api.operations.run_ops", "completion": "\n    if serial:\n        _run_serial_ops(state)\n    elif no_wait:\n        _run_no_wait_ops(state)\n    else:\n        _run_single_op(state, op_hash)", "idx": 977}
{"namespace": "pyinfra.api.connect.connect_all", "completion": "\n    def connect_host(host):\n        # Connect to the host\n        # Update the state accordingly\n\n    # Create a list of greenlets for connecting to each host\n    greenlets = [gevent.spawn(connect_host, host) for host in state.inventory]\n\n    # Wait for all greenlets to complete\n    gevent.joinall(greenlets)", "idx": 978}
{"namespace": "pyinfra.api.arguments.pop_global_arguments", "completion": "\n    if keys_to_check is None:\n        keys_to_check = all_argument_meta.keys()\n\n    found_keys = []\n    popped_arguments = AllArguments()\n\n    for key in keys_to_check:\n        if key in kwargs:\n            found_keys.append(key)\n            setattr(popped_arguments, key, kwargs.pop(key))\n\n    return popped_arguments, found_keys", "idx": 979}
{"namespace": "pyinfra_cli.commands.get_func_and_args", "completion": "    # Extract the operation name from the commands list\n    operation_name = commands[0]\n\n    # Import the corresponding module attribute\n    operation_func = try_import_module_attribute('pyinfra.operations', operation_name)\n\n    # Parse the arguments\n    args = commands[1:]\n\n    # Return the operation function and its arguments\n    return operation_func, args", "idx": 980}
{"namespace": "viztracer.tracer._VizTracer.start", "completion": "        self.enable = True\n        self.parsed = False\n        if self.log_print:\n            builtins.print = self._tracer.log_print\n        if self.include_files and self.exclude_files:\n            raise ValueError(\"Both include_files and exclude_files are specified, which is not allowed.\")\n        self.config()\n        self._tracer.start()", "idx": 981}
{"namespace": "viztracer.tracer._VizTracer.stop", "completion": "        self.enable = False\n        if self.log_print:\n            self.restore_print()\n        self._tracer.stop()", "idx": 982}
{"namespace": "viztracer.report_builder.ReportBuilder.save", "completion": "        if isinstance(output_file, str):\n            file_extension = os.path.splitext(output_file)[1][1:]\n            if file_extension.lower() == \"html\":\n                with open(output_file, \"w\", encoding=\"utf-8\") as f:\n                    self.generate_report(f, \"html\", file_info)\n            elif file_extension.lower() == \"json\":\n                with open(output_file, \"w\", encoding=\"utf-8\") as f:\n                    self.generate_report(f, \"json\", file_info)\n            elif file_extension.lower() == \"gz\":\n                with gzip.open(output_file, \"wt\", encoding=\"utf-8\") as f:\n                    self.generate_report(f, \"json\", file_info)\n            else:\n                raise ValueError(\"Unsupported file format\")\n        elif isinstance(output_file, TextIO):\n            file_extension = os.path.splitext(output_file.name)[1][1:]\n            if file_extension.lower() == \"html\":\n                self.generate_report(output_file, \"html\", file_info)\n            elif file_extension.lower() == \"json\":\n                self.generate_report(output_file, \"json\", file_info)\n            elif file_extension.lower() == \"gz\":\n                with gzip.open(output_file, \"wt\", encoding=\"utf-8\") as f:\n                    self.generate_report(f, \"json\", file_info)\n            else:\n                raise ValueError(\"Unsupported file format\")\n        else:\n            raise ValueError(\"Invalid output file type\")\n        self.final_messages.append((\"view_command\", {\"output_file\": os.path.abspath(output_file)}))\n        for message, data in self.final_messages:\n            print(message, data)", "idx": 983}
{"namespace": "viztracer.code_monkey.AstTransformer.get_assign_targets_with_attr", "completion": "        if isinstance(node, ast.Attribute):\n            return [node]\n        elif isinstance(node, (ast.Name, ast.Subscript, ast.Starred)):\n            return []\n        elif isinstance(node, (ast.Tuple, ast.List)):\n            result = []\n            for elt in node.elts:\n                result += self.get_assign_targets_with_attr(elt)\n            return result\n        else:\n            color_print(\"WARNING\", f\"Unexpected node type {type(node)} for ast.Assign. Please report to the author github.com/gaogaotiantian/viztracer\")\n            return []", "idx": 984}
{"namespace": "viztracer.code_monkey.SourceProcessor.process", "completion": "        if isinstance(source, bytes):\n            source = source.decode('utf-8')\n        elif not isinstance(source, str):\n            return source\n\n        processed_lines = []\n        for line in source.split('\\n'):\n            transformed_line = self.apply_transformations(line)\n            processed_lines.append(transformed_line)\n\n        return '\\n'.join(processed_lines)", "idx": 985}
{"namespace": "wal_e.log_help.WalELogger.fmt_logline", "completion": "        log_list = ['MSG: {message}'.format(message=msg)]\n        if detail:\n            log_list.append('DETAIL: {detail}'.format(detail=detail))\n        if hint:\n            log_list.append('HINT: {hint}'.format(hint=hint))\n        if structured:\n            log_list.append('STRUCTURED: {structured data}'.format(structured=structured))\n\n        return '\\n'.join(log_list)", "idx": 986}
{"namespace": "wal_e.blobstore.file.calling_format.Bucket.delete_keys", "completion": "        for key in keys:\n            file_key = self.get_key(key)\n            if os.path.exists(file_key.path):\n                os.remove(file_key.path)\n                remove_empty_dirs(os.path.dirname(file_key.path))", "idx": 987}
{"namespace": "wal_e.worker.upload_pool.TarUploadPool.put", "completion": "        if self.closed:\n            raise ValueError('put on closed pool')\n\n        if self.member_burden + len(tpart) > self.max_members:\n            raise ValueError('too much work outstanding')\n\n        if self.concurrency_burden >= self.max_concurrency:\n            raise ValueError('too much concurrency')\n\n        self._start(tpart)", "idx": 988}
{"namespace": "wal_e.worker.pg.wal_transfer.WalSegment.from_ready_archive_status", "completion": "        status_dir = path.join(xlog_dir, 'archive_status')\n        for file in os.listdir(status_dir):\n            if file.endswith('.ready'):\n                seg_path = path.join(xlog_dir, file[:-6])  # Remove the '.ready' extension\n                yield WalSegment(seg_path)", "idx": 989}
{"namespace": "wal_e.worker.pg.wal_transfer.WalTransferGroup.join", "completion": "        # Wait for the transfer to exit\n        self.wait_change.get()\n\n        # Raise any errors that occur during the process\n        if self.transferer.error is not None:\n            raise self.transferer.error\n\n        # Close the input WalTransferGroup instance\n        self.closed = True\n\n        # Wait for all running greenlets to exit\n        gevent.joinall(self.greenlets, timeout=30)\n\n        # Attempt to force them to exit if they haven't already\n        for greenlet in self.greenlets:\n            greenlet.kill()", "idx": 990}
{"namespace": "wal_e.worker.pg.wal_transfer.WalTransferGroup.start", "completion": "        greenlet = gevent.spawn(self.transferer, segment)\n        self.greenlets.add(greenlet)\n        greenlet.start()", "idx": 991}
{"namespace": "mrjob.py2.to_unicode", "completion": "    if isinstance(s, bytes):\n        try:\n            return s.decode('utf-8')\n        except UnicodeDecodeError:\n            return s.decode('latin-1')\n    else:\n        return s", "idx": 992}
{"namespace": "mrjob.job.MRJob.steps", "completion": "        steps = []\n\n        # create a dictionary of redefined methods\n        methods = {\n            'mapper': self.mapper,\n            'reducer': self.reducer,\n            'combiner': self.combiner,\n            'mapper_init': self.mapper_init,\n            'mapper_final': self.mapper_final,\n            'mapper_cmd': self.mapper_cmd,\n            'mapper_pre_filter': self.mapper_pre_filter,\n            'mapper_raw': self.mapper_raw,\n            'reducer_init': self.reducer_init,\n            'reducer_final': self.reducer_final,\n            'reducer_cmd': self.reducer_cmd,\n            'reducer_pre_filter': self.reducer_pre_filter,\n            'combiner_init': self.combiner_init,\n            'combiner_final': self.combiner_final,\n            'combiner_cmd': self.combiner_cmd,\n            'combiner_pre_filter': self.combiner_pre_filter,\n            'spark': self.spark,\n            'spark_args': self.spark_args\n        }\n\n        # create a list of MRStep objects constructed with the updated kwargs\n        for method, func in methods.items():\n            if func is not NotImplementedError:\n                kwargs = self._kwargs_from_switches(\n                    _im_func(getattr(self, method)).__code__.co_varnames)\n                kwargs['step_num'] = len(steps) + 1\n\n                if method == 'spark':\n                    steps.append(SparkStep(**kwargs))\n                else:\n                    kwargs['mapper'] = self.mapper_cmd() if method == 'mapper' else None\n                    kwargs['reducer'] = self.reducer_cmd() if method == 'reducer' else None\n                    kwargs['combiner'] = self.combiner_cmd() if method == 'combiner' else None\n                    steps.append(MRStep(**kwargs))\n\n        return steps", "idx": 993}
{"namespace": "mrjob.job.MRJob.increment_counter", "completion": "        group = group.replace(',', ';')  # replace commas with semicolons\n        counter = counter.replace(',', ';')  # replace commas with semicolons\n        line = \"reporter:counter:{},{},{}\\n\".format(group, counter, amount)\n        self.stderr.write(line)  # output the line through the standard error stream", "idx": 994}
{"namespace": "mrjob.job.MRJob.set_status", "completion": "        line = 'reporter:status:%s\\n' % msg\n        if not isinstance(line, bytes):\n            line = line.encode('utf_8')\n\n        self.stderr.write(line)\n        self.stderr.flush()", "idx": 995}
{"namespace": "mrjob.job.MRJob.run_job", "completion": "\n        log.info('writing to /tmp directory')\n        with self.make_runner() as runner:\n            runner.run()\n            for line in runner.stream_output():\n                sys.stdout.write(line)", "idx": 996}
{"namespace": "mrjob.job.MRJob.set_up_logging", "completion": "        # set up logging for mrjob\n        if quiet:\n            log.setLevel(logging.ERROR)\n        elif verbose:\n            log.setLevel(logging.DEBUG)\n        else:\n            log.setLevel(logging.INFO)\n\n        # set up logging for __main__\n        if quiet:\n            logging.getLogger().setLevel(logging.ERROR)\n        elif verbose:\n            logging.getLogger().setLevel(logging.DEBUG)\n        else:\n            logging.getLogger().setLevel(logging.INFO)\n\n        # set the stream to log to\n        if stream:\n            logging.basicConfig(stream=stream)\n        else:\n            logging.basicConfig(stream=sys.stderr)", "idx": 997}
{"namespace": "mrjob.job.MRJob.execute", "completion": "\n        if self.options.mapper:\n            self.run_mapper()\n        elif self.options.combiner:\n            self.run_combiner()\n        elif self.options.reducer:\n            self.run_reducer()\n        else:\n            self.run_job()", "idx": 998}
{"namespace": "mrjob.job.MRJob._runner_kwargs", "completion": "        non_option_kwargs = self._non_option_kwargs()\n\n        # keyword arguments from switches\n        switch_keys = ['conf_paths', 'extra_args', 'hadoop_input_format',\n                       'hadoop_output_format', 'input_paths', 'max_output_files',\n                       'mr_job_script', 'output_dir', 'partitioner',\n                       'step_output_dir']\n        switch_kwargs = self._kwargs_from_switches(switch_keys)\n\n        # keyword arguments from the job\n        job_kwargs = self._job_kwargs()\n\n        # include the MRJob class in the keyword arguments if the runner class is \"inline\" or \"spark\"\n        runner_class = self._runner_class()\n        if runner_class in ('inline', 'spark'):\n            job_kwargs['mr_job'] = self\n\n        # include the steps description in the keyword arguments\n        job_kwargs['steps'] = self.steps()\n\n        # combine all the keyword arguments\n        return combine_dicts(non_option_kwargs, switch_kwargs, job_kwargs)", "idx": 999}
{"namespace": "mrjob.job.MRJob.run_mapper", "completion": "\n        step = self._get_step(step_num, MRStep)\n\n        input_protocol = step.input_protocol or JSONProtocol\n        output_protocol = step.output_protocol or JSONProtocol\n\n        for key, value in input_protocol.read(self.stdin):\n            for out_key, out_value in self.mapper(key, value):\n                output_protocol.write(out_key, out_value, self.stdout)", "idx": 1000}
{"namespace": "mrjob.job.MRJob.run_combiner", "completion": "        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'combiner')\n\n        for k, v in self.combine_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)", "idx": 1001}
{"namespace": "mrjob.job.MRJob.add_passthru_arg", "completion": "        pass_opt = self.arg_parser.add_argument(*args, **kwargs)\n        self._passthru_arg_dests.add(pass_opt.dest)", "idx": 1002}
{"namespace": "mrjob.job.MRJob.is_task", "completion": "        return hasattr(self, 'mapper') or hasattr(self, 'combiner') or hasattr(self, 'reducer') or hasattr(self, 'spark')", "idx": 1003}
{"namespace": "mrjob.job.MRJob.parse_output", "completion": "\n        output_protocol = self.output_protocol()\n\n        for chunk in chunks:\n            for line in to_lines(chunk):\n                key, value = output_protocol.read(line.rstrip(b'\\r\\n'))\n                yield key, value", "idx": 1004}
{"namespace": "mrjob.job.MRJob.sandbox", "completion": "        if stdin is not None:\n            self._stdin = stdin\n        else:\n            self._stdin = BytesIO()\n\n        if stdout is not None:\n            self._stdout = stdout\n        else:\n            self._stdout = BytesIO()\n\n        if stderr is not None:\n            self._stderr = stderr\n        else:\n            self._stderr = BytesIO()\n\n        return self", "idx": 1005}
{"namespace": "mrjob.hadoop.fully_qualify_hdfs_path", "completion": "    if path.startswith('hdfs://'):\n        return path\n    elif path.startswith('/'):\n        return 'hdfs://' + path\n    else:\n        username = getpass.getuser()\n        return 'hdfs:///user/{}/{}'.format(username, path)", "idx": 1006}
{"namespace": "mrjob.hadoop.HadoopJobRunner.fs", "completion": "        if not hasattr(self, '_filesystem'):\n            self._filesystem = CompositeFilesystem()\n            self._filesystem.add_fs('hdfs', HadoopFilesystem())\n            self._filesystem.add_fs('local', LocalFilesystem())\n        return self._filesystem", "idx": 1007}
{"namespace": "mrjob.hadoop.HadoopJobRunner._find_hadoop_streaming_jar", "completion": "        for directory in _EMR_HADOOP_STREAMING_JAR_DIRS:\n            log.info('Looking for Hadoop streaming jar in %s...' % directory)\n            for root, dirs, files in os.walk(directory):\n                for file in files:\n                    if _HADOOP_STREAMING_JAR_RE.match(file):\n                        return os.path.join(root, file)\n        return None", "idx": 1008}
{"namespace": "mrjob.hadoop.HadoopJobRunner._find_binaries_and_jars", "completion": "\n        # Load the Hadoop binary\n        self.get_hadoop_bin()\n\n        # Check if there are Hadoop streaming steps or Spark steps in the job\n        if any(_is_spark_step_type(step['type']) for step in self._get_steps()):\n            # Load the Spark submit binary\n            self._spark_submit_bin = self._opts['spark_submit_bin']\n\n        if any(step['type'] == 'streaming' for step in self._get_steps()):\n            # Load the Hadoop streaming jar\n            self.get_hadoop_streaming_jar()", "idx": 1009}
{"namespace": "mrjob.hadoop.HadoopJobRunner._args_for_streaming_step", "completion": "        # check if the Hadoop streaming jar is available\n        hadoop_streaming_jar = self.get_hadoop_streaming_jar()\n        if not hadoop_streaming_jar:\n            raise Exception('no Hadoop streaming jar')\n\n        # construct the command line arguments for the Hadoop streaming step\n        step = self._get_step(step_num)\n        step_args = [\n            self.get_hadoop_bin(),\n            'jar',\n            hadoop_streaming_jar,\n            '-D', 'mapreduce.job.reduces=%d' % self._get_num_reducers(step_num),\n            '-D', 'mapreduce.job.name=%s' % self._job_name,\n            '-D', 'mapreduce.job.output.key.comparator.class=%s' % self._opts['hadoop_output_key_comparator'],\n            '-D', 'mapreduce.job.output.group.comparator.class=%s' % self._opts['hadoop_output_group_comparator'],\n            '-D', 'stream.num.map.output.key.fields=%d' % self._opts['hadoop_streaming_num_map_output_key_fields'],\n            '-D', 'stream.map.output.field.separator=%s' % self._opts['hadoop_streaming_map_output_field_separator'],\n            '-D', 'stream.reduce.output.field.separator=%s' % self._opts['hadoop_streaming_reduce_output_field_separator'],\n            '-D', 'stream.map.input.field.separator=%s' % self._opts['hadoop_streaming_map_input_field_separator'],\n            '-D', 'stream.reduce.input.field.separator=%s' % self._opts['hadoop_streaming_reduce_input_field_separator'],\n            '-D', 'stream.map.output.key.field.separator=%s' % self._opts['hadoop_streaming_map_output_key_field_separator'],\n            '-D', 'stream.reduce.output.key.field.separator=%s' % self._opts['hadoop_streaming_reduce_output_key_field_separator'],\n            '-files', self._upload_mgr.uri(step_num),\n            '-input', self._step_input_uris(step_num),\n            '-output', self._step_output_uri(step_num),\n            '-mapper', self._opts['hadoop_streaming_mapper'],\n            '-reducer', self._opts['hadoop_streaming_reducer'],\n            '-jobconf', 'mapred.reduce.tasks=%d' % self._get_num_reducers(step_num),\n            '-jobconf', 'mapred.job.name=%s' % self._job_name,\n        ]\n\n        return step_args", "idx": 1010}
{"namespace": "mrjob.hadoop.HadoopJobRunner._stream_history_log_dirs", "completion": "        for log_dir in unique(self._hadoop_log_dirs(output_dir)):\n            if self.fs.exists(log_dir):\n                log.info('Looking for history log in %s...' % log_dir)\n                yield [log_dir]", "idx": 1011}
{"namespace": "mrjob.hadoop.HadoopJobRunner._stream_task_log_dirs", "completion": "        if not self._read_logs():\n            return\n\n        for log_dir in unique(self._hadoop_log_dirs(output_dir=output_dir)):\n            if application_id:\n                task_log_dir = posixpath.join(log_dir, 'userlogs', application_id)\n            else:\n                task_log_dir = posixpath.join(log_dir, 'userlogs')\n\n            log.info('Looking for task logs in %s...' % task_log_dir)\n            yield [task_log_dir]", "idx": 1012}
{"namespace": "mrjob.setup.UploadDirManager.add", "completion": "        if is_uri(path):\n            return path\n\n        if path in self._path_to_name:\n            return posixpath.join(self.prefix, self._path_to_name[path])\n\n        name = name_uniquely(\n            path, self._names_taken, unhide=True, strip_ext=True)\n        self._path_to_name[path] = name\n        self._names_taken.add(name)\n        return posixpath.join(self.prefix, name)", "idx": 1013}
{"namespace": "mrjob.setup.UploadDirManager.uri", "completion": "        if is_uri(path):\n            return path\n        elif path in self._path_to_name:\n            return posixpath.join(self.prefix, self._path_to_name[path])\n        else:\n            raise ValueError('%r is not a URI or a known local file' % path)", "idx": 1014}
{"namespace": "mrjob.setup.UploadDirManager.path_to_uri", "completion": "        path_to_uri_dict = {}\n        for path in self._path_to_name:\n            uri = self.uri(path)\n            path_to_uri_dict[path] = uri\n        return path_to_uri_dict", "idx": 1015}
{"namespace": "mrjob.setup.WorkingDirManager.name_to_path", "completion": "        if type and type not in self._SUPPORTED_TYPES:\n            raise ValueError('bad path type %r, must be one of %s' % (\n                type, ', '.join(sorted(self._SUPPORTED_TYPES))))\n\n        if type:\n            return dict((name, path) for name, (t, path) in self._name_to_typed_path.items() if t == type)\n        else:\n            return dict(self._name_to_typed_path)", "idx": 1016}
{"namespace": "mrjob.setup.WorkingDirManager.paths", "completion": "        tracked_paths = set()\n        for path_type, path in self._typed_path_to_auto_name:\n            if type is None or path_type == type:\n                tracked_paths.add(path)\n        \n        return tracked_paths", "idx": 1017}
{"namespace": "mrjob.compat.jobconf_from_env", "completion": "    if variable in os.environ:\n        return os.environ[variable]\n    else:\n        # Try alternative variable names based on a mapping dictionary\n        if variable in _JOBCONF_MAP:\n            version_dict = _JOBCONF_MAP[variable]\n            for version in ['2.0', '1.0', '0.21', '0.20']:\n                if version in version_dict:\n                    alt_variable = version_dict[version]\n                    if alt_variable in os.environ:\n                        return os.environ[alt_variable]\n        # Return the default value if the jobconf variable is not set\n        return default", "idx": 1018}
{"namespace": "mrjob.compat.jobconf_from_dict", "completion": "    if name in jobconf:\n        return jobconf[name]\n\n    for var in _JOBCONF_MAP.get(name, {}).values():\n        if var in jobconf:\n            return jobconf[var]\n\n    return default", "idx": 1019}
{"namespace": "mrjob.compat.translate_jobconf", "completion": "    if variable in _JOBCONF_MAP:\n        return map_version(version, _JOBCONF_MAP[variable])\n    else:\n        return variable", "idx": 1020}
{"namespace": "mrjob.compat.translate_jobconf_for_all_versions", "completion": "    variants = _JOBCONF_MAP.get(variable, {}).values()\n    return sorted(variants)", "idx": 1021}
{"namespace": "mrjob.compat.translate_jobconf_dict", "completion": "    translated_jobconf = {}\n    warnings = []\n\n    for key, value in jobconf.items():\n        translated_key = translate_jobconf(key, hadoop_version)\n        if translated_key != key:\n            warnings.append(f\"{key} has been translated to {translated_key}\")\n\n        translated_jobconf[translated_key] = value\n\n    if warnings:\n        warning_message = (f\"Detected hadoop configuration property names that do not match version {hadoop_version}:\\n\"\n                           f\"The have been translated to the following names:\\n\"\n                           f\"{sorted(warnings)}\")\n        print(warning_message)\n\n    return translated_jobconf", "idx": 1022}
{"namespace": "mrjob.compat.uses_yarn", "completion": "    pass\n", "idx": 1023}
{"namespace": "mrjob.local.LocalMRJobRunner._spark_master", "completion": "        num_executors = self._opts['num_executors']\n        cores_per_executor = self._opts['cores_per_executor']\n        executor_memory = self._opts['executor_memory']\n\n        # calculate executor memory in MB (rounded up)\n        executor_memory_mb = int(math.ceil(float(executor_memory[:-1]) * 1024))\n\n        return 'local-cluster[%d,%d,%d]' % (num_executors, cores_per_executor, executor_memory_mb)", "idx": 1024}
{"namespace": "mrjob.runner.MRJobRunner._bootstrap_mrjob", "completion": "        return self._opts.get('bootstrap_mrjob', True)", "idx": 1025}
{"namespace": "mrjob.conf._fix_clear_tags", "completion": "    if isinstance(x, list):\n        return [_fix_clear_tags(item) for item in x]\n    elif isinstance(x, dict):\n        result = {}\n        for key, value in x.items():\n            if isinstance(key, ClearedValue):\n                key = key.value\n            if isinstance(value, ClearedValue):\n                value = value.value\n            result[key] = _fix_clear_tags(value)\n        return result\n    elif isinstance(x, ClearedValue):\n        return x.value\n    else:\n        return x", "idx": 1026}
{"namespace": "mrjob.conf.load_opts_from_mrjob_conf", "completion": "\n    if already_loaded is None:\n        already_loaded = []\n\n    conf_path = _expanded_mrjob_conf_path(conf_path)\n\n    if conf_path in already_loaded:\n        log.debug('Already loaded %s' % conf_path)\n        return []\n\n    log.debug('Loading options from %s' % conf_path)\n\n    conf = _conf_object_at_path(conf_path)\n\n    if conf is None:\n        return [(None, {})]\n\n    if 'include' in conf:\n        include_paths = conf['include']\n        if isinstance(include_paths, string_types):\n            include_paths = [include_paths]\n\n        for include_path in include_paths:\n            include_conf_path = _expanded_mrjob_conf_path(include_path)\n\n            if include_conf_path in already_loaded:\n                log.debug('Already loaded %s' % include_conf_path)\n                continue\n\n            log.debug('Loading included options from %s' % include_conf_path)\n\n            include_conf = _conf_object_at_path(include_conf_path)\n\n            if include_conf is not None:\n                conf.update(include_conf)\n\n    already_loaded.append(conf_path)\n\n    return [(conf_path, conf)]", "idx": 1027}
{"namespace": "mrjob.conf.load_opts_from_mrjob_confs", "completion": "    if conf_paths is None:\n        conf_paths = [find_mrjob_conf()]\n\n    if not conf_paths:\n        log.warning('No config specified for %s runner' % runner_alias)\n\n    return sum((_load_opts_from_mrjob_conf(runner_alias, conf_path)\n                for conf_path in conf_paths), [])", "idx": 1028}
{"namespace": "mrjob.conf.dump_mrjob_conf", "completion": "    if yaml:\n        _dump_yaml_with_clear_tags(conf, f, default_flow_style=False)\n    else:\n        json.dump(conf, f, indent=2)", "idx": 1029}
{"namespace": "mrjob.conf.combine_lists", "completion": "    combined_list = []\n    for seq in seqs:\n        if seq is None:\n            continue\n        elif isinstance(seq, (str, bytes)) or not hasattr(seq, '__iter__'):\n            combined_list.append(seq)\n        else:\n            combined_list.extend([item for item in seq if item is not None])\n    return combined_list", "idx": 1030}
{"namespace": "mrjob.conf.combine_cmds", "completion": "    last_cmd = None\n    for cmd in cmds:\n        if cmd is not None:\n            if isinstance(cmd, list):\n                last_cmd = cmd\n            else:\n                last_cmd = shlex_split(cmd)\n    return last_cmd", "idx": 1031}
{"namespace": "mrjob.conf.combine_dicts", "completion": "    combined_dict = {}\n    for d in dicts:\n        if d is not None:\n            for key, value in d.items():\n                if isinstance(value, ClearedValue) and value.value is None:\n                    combined_dict.pop(key, None)\n                else:\n                    combined_dict[key] = value\n    return combined_dict", "idx": 1032}
{"namespace": "mrjob.conf.combine_jobconfs", "completion": "    combined_jobconf = {}\n\n    for jobconf in jobconfs:\n        if jobconf:\n            for key, value in jobconf.items():\n                if value is not None:\n                    if not isinstance(value, str):\n                        value = str(value)\n                    combined_jobconf[key] = value\n\n    return combined_jobconf", "idx": 1033}
{"namespace": "mrjob.conf.combine_path_lists", "completion": "    combined_paths = []\n\n    for path_seq in path_seqs:\n        if isinstance(path_seq, str):\n            path_seq = [path_seq]\n\n        for path in path_seq:\n            expanded_path = expand_path(path)\n            if '*' in expanded_path:\n                glob_expansion = glob.glob(expanded_path)\n                combined_paths.extend(glob_expansion)\n            else:\n                combined_paths.append(expanded_path)\n\n    return combined_paths", "idx": 1034}
{"namespace": "mrjob.conf.combine_opts", "completion": "    combined_opts = {}\n\n    for opts in opts_list:\n        for key, value in opts.items():\n            if not isinstance(value, ClearedValue):\n                combiner = combiners.get(key, combine_values)\n                combined_opts[key] = combiner(combined_opts.get(key), value)\n\n    return combined_opts", "idx": 1035}
{"namespace": "mrjob.bin.MRJobBinRunner._task_python_bin", "completion": "\n        # If the task python binary option is set, return its value\n        if self._opts['task_python_bin']:\n            return self._opts['task_python_bin']\n        else:\n            # Otherwise, return the default Python binary\n            return self._default_python_bin()", "idx": 1036}
{"namespace": "django.utils.ipv6.is_valid_ipv6_address", "completion": "    try:\n        ipaddress.IPv6Address(ip_str)\n        return True\n    except ValueError:\n        return False", "idx": 1037}
{"namespace": "django.utils.http.urlsafe_base64_decode", "completion": "    # Add back any trailing equal signs\n    s += '=' * (-len(s) % 4)\n    return base64.urlsafe_b64decode(s.encode('ascii')).decode('utf-8')", "idx": 1038}
{"namespace": "django.utils.http.parse_etags", "completion": "    if etag_str.strip() == \"*\":\n        return [\"*\"]\n    etags = []\n    for match in ETAG_MATCH.finditer(etag_str):\n        etags.append(match.group(1))\n    return etags", "idx": 1039}
{"namespace": "django.utils.http.is_same_domain", "completion": "    if host == pattern:\n        return True\n    if pattern.startswith(\"*.\") and host.endswith(pattern[1:]):\n        return True\n    return False", "idx": 1040}
{"namespace": "django.utils.http.content_disposition_header", "completion": "    if as_attachment:\n        disposition = 'attachment'\n    else:\n        disposition = 'inline'\n    header_value = f'{disposition}; filename=\"{filename}\"'\n    return header_value", "idx": 1041}
{"namespace": "pysnooper.utils.truncate", "completion": "    if len(string) <= max_length:\n        return string\n    else:\n        return f'{string[:max_length-3]}...'  # Truncate the string and add ellipsis", "idx": 1042}
{"namespace": "pysnooper.variables.needs_parentheses", "completion": "    # Implementation goes here", "idx": 1043}
{"namespace": "django.test.utils.extend_sys_path", "completion": "    class ExtendSysPathContext:\n        def __enter__(self):\n            self.original_sys_path = sys.path.copy()\n            sys.path.extend(paths)\n\n        def __exit__(self, exc_type, exc_value, traceback):\n            sys.path = self.original_sys_path\n\n    return ExtendSysPathContext()", "idx": 1044}
{"namespace": "albumentations.augmentations.functional.normalize_cv2", "completion": "    if not isinstance(img, np.ndarray):\n        raise TypeError(\"Input image should be a numpy array\")\n\n    if not isinstance(mean, np.ndarray) or not isinstance(denominator, np.ndarray):\n        raise TypeError(\"Mean and denominator should be numpy arrays\")\n\n    if mean.shape != img.shape or denominator.shape != img.shape:\n        raise ValueError(\"Mean and denominator should have the same shape as the input image\")\n\n    normalized_img = (img - mean) / denominator\n    return normalized_img", "idx": 1045}
{"namespace": "albumentations.augmentations.functional.normalize_numpy", "completion": "    if mean.shape and len(mean) != 4 and mean.shape != img.shape:\n        mean = np.array(mean.tolist() + [0] * (4 - len(mean)), dtype=np.float64)\n    if not denominator.shape:\n        denominator = np.array([denominator.tolist()] * 4, dtype=np.float64)\n    elif len(denominator) != 4 and denominator.shape != img.shape:\n        denominator = np.array(denominator.tolist() + [1] * (4 - len(denominator)), dtype=np.float64)\n\n    img = np.ascontiguousarray(img.astype(\"float32\"))\n    np.subtract(img, mean.astype(np.float64), img)\n    np.multiply(img, denominator.astype(np.float64), img)\n    return img", "idx": 1046}
{"namespace": "albumentations.augmentations.functional.gamma_transform", "completion": "    if img.dtype == np.uint8:\n        lut = np.array([((i / 255.0) ** (1.0 / gamma)) * 255 for i in range(256)], dtype=np.uint8)\n        return cv2.LUT(img, lut)\n    else:\n        return np.power(img, 1.0 / gamma)", "idx": 1047}
{"namespace": "albumentations.augmentations.functional.swap_tiles_on_image", "completion": "    output_image = image.copy()\n    for tile in tiles:\n        current_x, current_y, old_x, old_y, height, width = tile\n        output_image[current_y:current_y + height, current_x:current_x + width], output_image[old_y:old_y + height, old_x:old_x + width] = output_image[old_y:old_y + height, old_x:old_x + width], output_image[current_y:current_y + height, current_x:current_x + width].copy()\n    return output_image", "idx": 1048}
{"namespace": "albumentations.augmentations.geometric.functional.keypoint_rotate", "completion": "    x, y, angle, scale = keypoint[:4]\n\n    angle = angle_2pi_range(angle + angle)\n\n    x_c = cols / 2\n    y_c = rows / 2\n\n    x_new = (x - x_c) * math.cos(angle) - (y - y_c) * math.sin(angle) + x_c\n    y_new = (x - x_c) * math.sin(angle) + (y - y_c) * math.cos(angle) + y_c\n\n    return x_new, y_new, angle, scale", "idx": 1049}
{"namespace": "albumentations.augmentations.geometric.functional.keypoint_shift_scale_rotate", "completion": "    center = (cols - 1) * 0.5, (rows - 1) * 0.5\n    matrix = cv2.getRotationMatrix2D(center, angle, scale)\n    x, y, a, s = keypoint[:4]\n    x, y = cv2.transform(np.array([[[x, y]]]), matrix).squeeze()\n    return x + dx, y + dy, a, s", "idx": 1050}
{"namespace": "albumentations.core.keypoints_utils.angle_to_2pi_range", "completion": "    while angle < 0:\n        angle += 2 * math.pi\n    while angle >= 2 * math.pi:\n        angle -= 2 * math.pi\n    return angle", "idx": 1051}
{"namespace": "albumentations.augmentations.geometric.functional.rot90", "completion": "    if factor not in {0, 1, 2, 3}:\n        raise ValueError(\"Parameter factor must be in set {0, 1, 2, 3}\")\n    return np.rot90(img, k=factor)", "idx": 1052}
{"namespace": "albumentations.core.keypoints_utils.convert_keypoints_to_albumentations", "completion": "    albumentations_keypoints = []\n    for keypoint in keypoints:\n        albumentations_keypoint = convert_keypoint_to_albumentations(\n            keypoint, source_format, rows, cols, check_validity, angle_in_degrees\n        )\n        albumentations_keypoints.append(albumentations_keypoint)\n    return albumentations_keypoints", "idx": 1053}
{"namespace": "albumentations.core.keypoints_utils.convert_keypoints_from_albumentations", "completion": "    return [\n        convert_keypoint_from_albumentations(kp, target_format, rows, cols, check_validity, angle_in_degrees)\n        for kp in keypoints\n    ]", "idx": 1054}
{"namespace": "albumentations.core.transforms_interface.to_tuple", "completion": "    if isinstance(param, (int, float)):\n        if low is not None:\n            return (param - low, param + low)\n        else:\n            return (param - bias, param + bias)\n    elif isinstance(param, (tuple, list)) and len(param) >= 2:\n        if low is not None:\n            return (param[0] + low, param[1] + low)\n        else:\n            return (param[0] + bias, param[1] + bias)\n    else:\n        raise ValueError(\"Input argument must be a scalar, tuple, or list of 2+ elements.\")", "idx": 1055}
{"namespace": "albumentations.core.composition.ReplayCompose.replay", "completion": "        replayed_data = kwargs\n        for key, value in saved_augmentations.items():\n            if isinstance(value, dict):\n                replayed_data[key] = replay(saved_augmentations[key], **replayed_data)\n            else:\n                replayed_data = value(**replayed_data)\n        return replayed_data", "idx": 1056}
{"namespace": "albumentations.core.serialization.shorten_class_name", "completion": "    if class_fullname.startswith(\"albumentations.\"):\n        return class_fullname[len(\"albumentations.\"):]\n    return class_fullname", "idx": 1057}
{"namespace": "wandb.util.to_forward_slash_path", "completion": "    if platform.system() == \"Windows\":\n        return path.replace(\"\\\\\", \"/\")\n    else:\n        return path", "idx": 1058}
{"namespace": "wandb.util.make_artifact_name_safe", "completion": "    cleaned_name = re.sub(r\"[^a-zA-Z0-9\\-\\._]\", \"_\", name)\n    # If the length of the cleaned name is greater than 128, truncate the name with dots in the middle using regex\n    if len(cleaned_name) > 128:\n        prefix_len = 64\n        suffix_len = 64\n        truncated_name = cleaned_name[:prefix_len] + \"...\" + cleaned_name[-suffix_len:]\n        return truncated_name\n    else:\n        return cleaned_name", "idx": 1059}
{"namespace": "wandb.sdk.wandb_settings._redact_dict", "completion": "    redacted_dict = d.copy()\n    for key in unsafe_keys:\n        if key in redacted_dict:\n            redacted_dict[key] = redact_str\n    return redacted_dict", "idx": 1060}
{"namespace": "wandb.sdk.launch.builder.build.get_current_python_version", "completion": "    full_version = sys.version\n    major_version = sys.version_info.major\n    return full_version, str(major_version)", "idx": 1061}
{"namespace": "wandb.sdk.artifacts.storage_policy.StoragePolicy.lookup_by_name", "completion": "        for subclass in cls.__subclasses__():\n            if subclass.__name__ == name:\n                return subclass\n        raise NotImplementedError(f\"No storage policy with name {name} found\")", "idx": 1062}
{"namespace": "wandb.sdk.lib.runid.generate_id", "completion": "    characters = string.ascii_lowercase + string.digits\n    return ''.join(secrets.choice(characters) for _ in range(length))", "idx": 1063}
{"namespace": "wandb.sdk.internal.file_stream.CRDedupeFilePolicy.get_consecutive_offsets", "completion": "        intervals = []\n        start = None\n        end = None\n        for offset in sorted(console.keys()):\n            if start is None:\n                start = offset\n                end = offset\n            elif offset == end + 1:\n                end = offset\n            else:\n                intervals.append([start, end])\n                start = offset\n                end = offset\n        if start is not None:\n            intervals.append([start, end])\n        return intervals", "idx": 1064}
{"namespace": "wandb.sdk.internal.system.assets.ipu.IPUStats.sample", "completion": "        devices = self._gc_ipu_info.getDevices()\n        for device in devices:\n            device_id = device.getDeviceId()\n            if device_id in self._devices_called:\n                continue\n            metrics = device.getMetrics()\n            filtered_metrics = {\n                key: value\n                for key, value in metrics.items()\n                if key.startswith(str(self._pid)) or key in self.variable_metric_keys\n            }\n            if filtered_metrics:\n                wandb.log({self.name.format(device_id, key): value for key, value in filtered_metrics.items()})\n                self._devices_called.add(device_id)", "idx": 1065}
{"namespace": "csvkit.cleanup.join_rows", "completion": "    try:\n        joined_row = [joiner.join(row) for row in rows]\n        return joined_row\n    except Exception as e:\n        raise CSVTestException(f\"Error joining rows: {e}\")", "idx": 1066}
{"namespace": "csvkit.convert.guess_format", "completion": "    if filename.endswith('.csv'):\n        return 'csv'\n    elif filename.endswith('.dbf'):\n        return 'dbf'\n    elif filename.endswith('.fixed'):\n        return 'fixed'\n    elif filename.endswith('.xls'):\n        return 'xls'\n    elif filename.endswith('.xlsx'):\n        return 'xlsx'\n    elif filename.endswith('.js'):\n        return 'json'\n    else:\n        return None", "idx": 1067}
{"namespace": "folium.utilities.normalize", "completion": "    # Remove non-functional spaces and newlines\n    return re.sub(r'\\s+', ' ', rendered).strip()", "idx": 1068}
{"namespace": "tpot.gp_deap.initialize_stats_dict", "completion": "    individual.stats = defaultdict(int)\n    individual.stats['generation'] = 0\n    individual.stats['mutation_count'] = 0\n    individual.stats['crossover_count'] = 0\n    individual.stats['predecessor'] = None", "idx": 1069}
{"namespace": "bentoml_cli.env_manager.remove_env_arg", "completion": "    pattern = re.compile(r'--env(=|\\s)\\S+')\n\n    # Filter out the `--env <env_name>` or `--env=<env_name>` from the input list of command line arguments\n    filtered_args = [arg for arg in cmd_args if not pattern.match(arg)]\n\n    return filtered_args", "idx": 1070}
{"namespace": "bentoml._internal.utils.uri.path_to_uri", "completion": "    abs_path = os.path.abspath(path)\n    if os.name == 'nt':  # Windows\n        return urlparse(abs_path).geturl()\n    else:  # Unix-like\n        return 'file://' + quote(abs_path)", "idx": 1071}
{"namespace": "bentoml._internal.utils.uri.uri_to_path", "completion": "    parsed_uri = urlparse(uri)\n    if parsed_uri.scheme == 'file':\n        path = url2pathname(parsed_uri.path)\n        return unquote(path)\n    else:\n        raise ValueError(\"Unsupported URI scheme\")", "idx": 1072}
{"namespace": "bentoml._internal.utils.validate_labels", "completion": "    if not isinstance(labels, dict):\n        raise ValueError(\"Input 'labels' must be a dictionary\")\n\n    if any(not isinstance(key, str) for key in labels.keys()):\n        raise ValueError(\"Keys in 'labels' must be strings\")\n\n    if any(not isinstance(value, str) for value in labels.values()):\n        raise ValueError(\"Values in 'labels' must be strings\")", "idx": 1073}
{"namespace": "bentoml._internal.configuration.helpers.is_valid_ip_address", "completion": "    try:\n        ipaddress.ip_address(addr)\n        return True\n    except ValueError:\n        return False", "idx": 1074}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batches_to_batch", "completion": "        # Concatenate the batches along the specified batch dimension\n        batch: ext.PdDataFrame = pd.concat(batches, axis=batch_dim)\n        \n        # Calculate the indices of the subbatches\n        indices = list(itertools.accumulate(subbatch.shape[0] for subbatch in batches))\n        indices = [0] + indices\n        \n        return batch, indices", "idx": 1075}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batch_to_batches", "completion": "\n        return [batch.iloc[indices[i]:indices[i+1]] for i in range(len(indices)-1)]", "idx": 1076}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batches_to_batch", "completion": "        batch: list[t.Any] = list(itertools.chain.from_iterable(batches))\n        indices = list(itertools.accumulate(len(subbatch) for subbatch in batches))\n        indices = [0] + indices\n        return batch, indices", "idx": 1077}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batch_to_batches", "completion": "        return [batch[indices[i]:indices[i+1]] for i in range(len(indices)-1)]", "idx": 1078}
{"namespace": "jwt.utils.force_bytes", "completion": "    if isinstance(value, bytes):\n        return value\n    elif isinstance(value, str):\n        return value.encode('utf-8')\n    else:\n        raise TypeError(\"Input value must be bytes or string\")", "idx": 1079}
{"namespace": "pytube.cli.display_progress_bar", "completion": "    percent = float(bytes_received) / filesize\n    progress = ch * int(scale * percent * 100)\n    remaining = ' ' * (int(100 * scale) - len(progress))\n    sys.stdout.write(f\"\\r|{progress}{remaining}| {percent:.2%}\")\n    sys.stdout.flush()", "idx": 1080}
{"namespace": "pytube.cli._download", "completion": "    if not target:\n        target = os.getcwd()\n    if not filename:\n        filename = stream.default_filename\n\n    file_size_mb = round(stream.filesize / (1024 * 1024), 2)\n    print(f\"Downloading {filename} ({file_size_mb} MB) to {target}...\")\n\n    stream.download(output_path=target, filename=filename)", "idx": 1081}
{"namespace": "pytube.cli.display_streams", "completion": "    streams = youtube.streams\n    for stream in streams:\n        print(stream)", "idx": 1082}
{"namespace": "pytube.cli._unique_name", "completion": "    filename = f\"{base}.{subtype}\"\n    if media_type == \"audio\":\n        filename = f\"{base}_audio.{subtype}\"\n    elif media_type == \"video\":\n        filename = f\"{base}_video.{subtype}\"\n\n    file_path = os.path.join(target, filename)\n    if not os.path.exists(file_path):\n        return filename\n\n    count = 1\n    while True:\n        new_filename = f\"{base}_{count}.{subtype}\"\n        if media_type == \"audio\":\n            new_filename = f\"{base}_audio_{count}.{subtype}\"\n        elif media_type == \"video\":\n            new_filename = f\"{base}_video_{count}.{subtype}\"\n\n        new_file_path = os.path.join(target, new_filename)\n        if not os.path.exists(new_file_path):\n            return new_filename\n        count += 1", "idx": 1083}
{"namespace": "pytube.cli._print_available_captions", "completion": "    print(\"Available caption codes:\")\n    for caption in captions:\n        print(f\"Language: {caption.code}, Name: {caption.name}\")", "idx": 1084}
{"namespace": "pytube.cipher.throttling_reverse", "completion": "    arr.reverse()", "idx": 1085}
{"namespace": "pytube.helpers.setup_logger", "completion": "    logger.setLevel(level)\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    \n    stream_handler = logging.StreamHandler()\n    stream_handler.setFormatter(formatter)\n    logger.addHandler(stream_handler)\n    \n    if log_filename:\n        file_handler = logging.FileHandler(log_filename)\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)", "idx": 1086}
{"namespace": "pytube.helpers.deprecated", "completion": "    def decorator(func: Callable) -> Callable:\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            warnings.warn(f\"Call to deprecated function {func.__name__}. Reason: {reason}\", category=DeprecationWarning, stacklevel=2)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator", "idx": 1087}
{"namespace": "pytube.helpers.uniqueify", "completion": "    seen = set()\n    result = []\n    for item in duped_list:\n        if item not in seen:\n            seen.add(item)\n            result.append(item)\n    return result", "idx": 1088}
{"namespace": "pytube.helpers.target_directory", "completion": "    if output_path is None:\n        return os.getcwd()\n    elif not os.path.isabs(output_path):\n        return os.path.abspath(output_path)\n    else:\n        if not os.path.exists(output_path):\n            os.makedirs(output_path)\n        return output_path", "idx": 1089}
{"namespace": "pytube.extract.is_private", "completion": "    private_strings = [\n        'This video is private',\n        'This live stream is private'\n    ]\n    for string in private_strings:\n        if string in watch_html:\n            return True\n    return False", "idx": 1090}
{"namespace": "pymc.math.cartesian", "completion": "    return np.array(np.meshgrid(*arrays)).T.reshape(-1, len(arrays))", "idx": 1091}
{"namespace": "pymc.math.log1mexp", "completion": "    if negative_input:\n        return np.log1p(-np.exp(x))\n    else:\n        if x <= 0:\n            return np.log1p(-np.exp(x))\n        else:\n            return np.log(-np.expm1(-x))", "idx": 1092}
{"namespace": "pymc.math.log1mexp_numpy", "completion": "    if not negative_input:\n        warnings.warn(\n            \"pymc.math.log1mexp will expect a negative input in a future \"\n            \"version of PyMC.\\n To suppress this warning set `negative_input=True`\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        x = -x\n\n    return np.log1p(-np.exp(x))", "idx": 1093}
{"namespace": "pymc.util.drop_warning_stat", "completion": "    if \"sample_stats\" in idata:\n        if \"warning\" in idata.sample_stats:\n            idata.sample_stats.drop_vars(\"warning\")\n    return idata", "idx": 1094}
{"namespace": "pymc.pytensorf.walk_model", "completion": "    if stop_at_vars is None:\n        stop_at_vars = set()\n\n    seen = set()\n    stack = list(graphs)\n\n    while stack:\n        var = stack.pop()\n        if var not in seen:\n            seen.add(var)\n            yield var\n            if var not in stop_at_vars:\n                stack.extend(expand_fn(var))", "idx": 1095}
{"namespace": "pymc.testing.select_by_precision", "completion": "\n    if pytensor.config.floatX == \"float64\":\n        return float64\n    elif pytensor.config.floatX == \"float32\":\n        return float32\n    else:\n        raise ValueError(\"Unsupported floatX mode\")", "idx": 1096}
{"namespace": "pymc.gp.cov.handle_args", "completion": "    @wraps(func)\n    def wrapper(X, args=None):\n        if args is None:\n            return func(X)\n        else:\n            return func(X, *args)\n    return wrapper", "idx": 1097}
{"namespace": "pymc.gp.util.kmeans_inducing_points", "completion": "    # Perform K-means clustering to find the initial locations of the inducing points\n    centroids, _ = kmeans(X, n_inducing, **kmeans_kwargs)\n    \n    return centroids", "idx": 1098}
{"namespace": "pymc.pytensorf.floatX", "completion": "\n    if isinstance(X, pt.TensorVariable):\n        return pt.tensor.cast(X, pt.config.floatX)\n    elif isinstance(X, np.ndarray):\n        return X.astype(pt.config.floatX)\n    else:\n        raise TypeError(\"Unsupported input type. Only PyTensor tensor or numpy array is allowed.\")", "idx": 1099}
{"namespace": "pymc.distributions.multivariate.posdef", "completion": "    try:\n        np.linalg.cholesky(AA)\n        return True\n    except np.linalg.LinAlgError:\n        return False", "idx": 1100}
{"namespace": "pymc.distributions.dist_math.multigammaln", "completion": "\n    half = p * (p - 1) / 4.0 * np.log(np.pi)\n    out = half * p * (p - 1) * np.log(np.pi)\n    for i in range(1, p + 1):\n        out += gammaln(a + (1 - i) / 2.0)\n    return out", "idx": 1101}
{"namespace": "pymc.distributions.dist_math.incomplete_beta", "completion": "    return pt.betainc(a, b, value)", "idx": 1102}
{"namespace": "pymc.sampling.forward.observed_dependent_deterministics", "completion": "    observed_rvs = [rv for rv in model.observed_RVs]\n    basic_rvs = model.basic_RVs\n    deterministics = model.deterministics\n\n    dependent_deterministics = []\n    for det in deterministics:\n        if any(rv in observed_rvs for rv in ancestors([det], blockers=basic_rvs)):\n            dependent_deterministics.append(det)\n\n    return dependent_deterministics", "idx": 1103}
{"namespace": "pymc.smc.kernels.systematic_resampling", "completion": "\n    N = len(weights)\n    cumulative_weights = np.cumsum(weights)\n    new_indices = np.zeros(N, dtype=int)\n    u = (np.arange(N) + rng.random()) / N\n    j = 0\n    for i in range(N):\n        while u[i] > cumulative_weights[j]:\n            j += 1\n        new_indices[i] = j\n\n    return new_indices", "idx": 1104}
{"namespace": "pymc.backends.base._squeeze_cat", "completion": "\n    if combine:\n        if squeeze:\n            return [np.squeeze(np.concatenate(results))]\n        else:\n            return [np.concatenate(results)]\n    else:\n        if squeeze:\n            return np.squeeze(results)\n        else:\n            return results", "idx": 1105}
{"namespace": "pymc.logprob.transforms.SimplexTransform.forward", "completion": "        log_value = pt.log(value)\n        sum_log_values = pt.sum(log_value, axis=-1, keepdims=True)\n        return log_value - sum_log_values", "idx": 1106}
{"namespace": "pymc.logprob.transforms.SimplexTransform.backward", "completion": "        value = pt.as_tensor(value)\n        N = value.shape[-1].astype(value.dtype)\n        log_value = value + pt.log(pt.sum(pt.exp(value), -1, keepdims=True))\n        return pt.concatenate([log_value, pt.zeros_like(log_value[..., :1])], -1)", "idx": 1107}
{"namespace": "pymc.logprob.utils.walk_model", "completion": "    for graph in graphs:\n        if not isinstance(graph, FunctionGraph):\n            raise ValueError(\"All graphs must be instances of FunctionGraph.\")\n\n        seen = set()\n        stack = list(graph_inputs(graph))\n\n        while stack:\n            node = stack.pop()\n            if node not in seen:\n                seen.add(node)\n                yield node\n\n                if isinstance(node.owner, Apply):\n                    for inp in node.owner.inputs:\n                        stack.append(inp)\n\n                if isinstance(node, MeasurableVariable) and not walk_past_rvs:\n                    continue\n\n                if stop_at_vars and node in stop_at_vars:\n                    continue\n\n                for var in expand_fn(node):\n                    stack.append(var)", "idx": 1108}
{"namespace": "sacred.metrics_logger.linearize_metrics", "completion": "    metrics_dict = {}\n    for entry in logged_metrics:\n        if entry.name not in metrics_dict:\n            metrics_dict[entry.name] = {\"steps\": [], \"values\": [], \"timestamps\": []}\n        metrics_dict[entry.name][\"steps\"].append(entry.step)\n        metrics_dict[entry.name][\"values\"].append(entry.value)\n        metrics_dict[entry.name][\"timestamps\"].append(entry.timestamp)\n    return metrics_dict", "idx": 1109}
{"namespace": "sacred.utils.set_by_dotted_path", "completion": "    keys = path.split('.')\n    for key in keys[:-1]:\n        d = d.setdefault(key, {})\n    d[keys[-1]] = value", "idx": 1110}
{"namespace": "sacred.utils.get_by_dotted_path", "completion": "    split_path = path.split(\".\")\n    current_option = d\n    for p in split_path:\n        if isinstance(current_option, dict) and p in current_option:\n            current_option = current_option[p]\n        else:\n            return default\n    return current_option", "idx": 1111}
{"namespace": "pymc.logprob.scan.construct_scan", "completion": "\n    # Construct the scan operation\n    scan_op = Scan(**kwargs)\n\n    # Create a node based on the input and output variables\n    scan_node = scan_op.make_node(**scan_args)\n\n    # Return the node outputs and updates\n    return scan_node.outputs, scan_node.op.get_updates()", "idx": 1112}
{"namespace": "sacred.utils.is_prefix", "completion": "    return path.startswith(pre_path)", "idx": 1113}
{"namespace": "sacred.utils.get_inheritors", "completion": "    subclasses = set()\n    work = [cls]\n    while work:\n        parent = work.pop()\n        for child in parent.__subclasses__():\n            if child not in subclasses:\n                subclasses.add(child)\n                work.append(child)\n    return subclasses", "idx": 1114}
{"namespace": "sacred.utils.convert_camel_case_to_snake_case", "completion": "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()", "idx": 1115}
{"namespace": "sacred.utils.module_exists", "completion": "    loader = pkgutil.find_loader(modname)\n    return loader is not None", "idx": 1116}
{"namespace": "sacred.utils.apply_backspaces_and_linefeeds", "completion": "    lines = []\n    for char in text:\n        if char == \"\\b\":\n            if lines:\n                lines[-1] = lines[-1][:-1]\n        elif char == \"\\n\":\n            lines.append(\"\")\n        else:\n            lines[-1] += char\n    return \"\\n\".join(lines)", "idx": 1117}
{"namespace": "sacred.commands.help_for_command", "completion": "\n    return pydoc.getdoc(command)", "idx": 1118}
{"namespace": "sacred.optional.optional_import", "completion": "    for package_name in package_names:\n        if modules_exist(package_name):\n            imported_package = importlib.import_module(package_name)\n            return True, imported_package\n    return False, None", "idx": 1119}
{"namespace": "sacred.dependencies.get_py_file_if_possible", "completion": "    if pyc_name.endswith((\".py\", \".so\", \".pyd\", \".ipynb\")):\n        return pyc_name\n    py_name = pyc_name[:-1] if pyc_name.endswith(\".pyc\") else pyc_name + \"c\"\n    if os.path.exists(py_name):\n        return py_name\n    return pyc_name", "idx": 1120}
{"namespace": "sacred.config.custom_containers.DogmaticDict.update", "completion": "        if iterable is not None:\n            if hasattr(iterable, 'keys'):\n                for key in iterable:\n                    self[key] = iterable[key]\n            else:\n                for item in iterable:\n                    self[item[0]] = item[1]\n\n        for key, value in kwargs.items():\n            self[key] = value", "idx": 1121}
{"namespace": "sacred.config.config_scope.is_empty_or_comment", "completion": "    line = line.strip()\n    return line == '' or line.startswith('#')", "idx": 1122}
{"namespace": "boltons.funcutils.copy_function", "completion": "    if not isinstance(orig, FunctionType):\n        raise TypeError(\"The input 'orig' must be a function\")\n\n    if _IS_PY2:\n        # Python 2\n        new_func = FunctionType(orig.func_code, orig.func_globals, orig.func_name, orig.func_defaults, orig.func_closure)\n    else:\n        # Python 3\n        new_func = FunctionType(orig.__code__, orig.__globals__, orig.__name__, orig.__defaults__, orig.__closure__)\n\n    if copy_dict:\n        new_func.__dict__.update(orig.__dict__)\n\n    return new_func", "idx": 1123}
{"namespace": "sacred.config.config_scope.dedent_line", "completion": "    if line.startswith(indent):\n        return line[len(indent):]\n    else:\n        return line", "idx": 1124}
{"namespace": "boltons.funcutils.format_invocation", "completion": "    if kwargs is None:\n        kwargs = {}\n    args_str = ', '.join(map(repr, args))\n    kwargs_str = ', '.join('%s=%r' % item for item in kwargs.items())\n    if args_str and kwargs_str:\n        args_str += ', '\n    return '%s(%s%s%s)' % (name, args_str, kwargs_str, ', '.join('%s=%r' % item for item in kw.items()))", "idx": 1125}
{"namespace": "boltons.listutils.SplayList.shift", "completion": "        item = self.pop(item_index)\n        self.insert(dest_index, item)", "idx": 1126}
{"namespace": "boltons.strutils.gzip_bytes", "completion": "    out = BytesIO()\n    with GzipFile(fileobj=out, mode='wb', compresslevel=level) as f:\n        f.write(bytestring)\n    return out.getvalue()", "idx": 1127}
{"namespace": "boltons.strutils.is_uuid", "completion": "    if isinstance(obj, uuid.UUID):\n        if version == 0:\n            return True\n        else:\n            return obj.version == version\n    elif isinstance(obj, str):\n        try:\n            uuid_obj = uuid.UUID(obj)\n            if version == 0:\n                return True\n            else:\n                return uuid_obj.version == version\n        except ValueError:\n            return False\n    else:\n        return False", "idx": 1128}
{"namespace": "boltons.strutils.parse_int_list", "completion": "    result = []\n    for part in range_string.split(delim):\n        if range_delim in part:\n            start, end = map(int, part.split(range_delim))\n            result.extend(range(start, end + 1))\n        else:\n            result.append(int(part))\n    return sorted(result)", "idx": 1129}
{"namespace": "dash.development.base_component.Component._traverse", "completion": "        if isinstance(self.children, Component):\n            yield self.children\n            yield from self.children._traverse()\n        elif isinstance(self.children, (tuple, MutableSequence)):\n            for child in self.children:\n                if isinstance(child, Component):\n                    yield child\n                    yield from child._traverse()", "idx": 1130}
{"namespace": "dash.development._r_components_generation.make_namespace_exports", "completion": "    export_string = \"\"\n    for component in components:\n        if prefix:\n            export_string += f\"export({prefix}, {component})\\n\"\n        export_string += f\"export({component})\\n\"\n    return export_string", "idx": 1131}
{"namespace": "dash.development._collect_nodes.collect_nodes", "completion": "    if nodes is None:\n        nodes = []\n\n    for key, value in metadata.items():\n        if is_node(value.get(\"name\")):\n            nodes.append(base + key)\n        elif is_shape(value.get(\"name\")):\n            nodes = collect_nodes(value.get(\"value\", {}), base + key + \".\", nodes)\n        elif value.get(\"name\") == \"union\":\n            nodes = collect_union(value.get(\"value\"), base + key + \".\", nodes)\n        elif value.get(\"name\") == \"arrayOf\":\n            nodes = collect_array(value, base + key + \".\", nodes)\n        elif value.get(\"name\") == \"objectOf\":\n            nodes = collect_object(value, base + key + \".\", nodes)\n\n    return nodes", "idx": 1132}
{"namespace": "peewee.Index.where", "completion": "        if self._where is not None:\n            expressions = (self._where,) + expressions\n        self._where = reduce(operator.and_, expressions)", "idx": 1133}
{"namespace": "playhouse.dataset.DataSet.tables", "completion": "        tables = self._introspector.get_tables()\n        if self._include_views:\n            tables.extend(self._introspector.get_views())\n        return tables", "idx": 1134}
{"namespace": "playhouse.dataset.DataSet.update_cache", "completion": "        if table:\n            if table in self._models:\n                del self._models[table]\n            if table in self.tables:\n                self._models[table] = self._introspector.generate_models(\n                    tables=[table],\n                    skip_invalid=True,\n                    literal_column_names=True,\n                    include_views=self._include_views)[table]\n        else:\n            self._models = self._introspector.generate_models(\n                skip_invalid=True,\n                literal_column_names=True,\n                include_views=self._include_views)", "idx": 1135}
{"namespace": "playhouse.dataset.DataSet.freeze", "completion": "        self._check_arguments(filename, file_obj, format, self._export_formats)\n        exporter_class = self._export_formats[format]\n        exporter = exporter_class(self, query, **kwargs)\n\n        if filename:\n            with open_file(filename, 'w', encoding=encoding) as f:\n                exporter.export(f)\n        elif file_obj:\n            exporter.export(file_obj)", "idx": 1136}
{"namespace": "playhouse.db_url.parse", "completion": "    parsed = urlparse(url)\n    return parseresult_to_dict(parsed, unquote_password)", "idx": 1137}
{"namespace": "playhouse.db_url.connect", "completion": "    parsed = parse(url, unquote_password)\n    connect_kwargs = parsed.copy()\n    connect_kwargs.update(connect_params)\n    scheme = connect_kwargs.pop('scheme', 'sqlite')\n    db_class = schemes[scheme]\n    return db_class(**connect_kwargs)", "idx": 1138}
{"namespace": "playhouse.sqlite_changelog.ChangeLog.install", "completion": "        if create_table:\n            self.model.create_table()\n\n        if drop:\n            for action in self._actions:\n                self.db.execute_sql(self.drop_trigger_sql(model, action))\n\n        if insert:\n            self.db.execute_sql(self.trigger_sql(model, 'INSERT', skip_fields))\n\n        if update:\n            self.db.execute_sql(self.trigger_sql(model, 'UPDATE', skip_fields))\n\n        if delete:\n            self.db.execute_sql(self.trigger_sql(model, 'DELETE', skip_fields))", "idx": 1139}
{"namespace": "playhouse.kv.KeyValue.pop", "completion": "        with self._database.atomic():\n            try:\n                value = self[key]\n                del self[key]\n                return value\n            except KeyError:\n                if default is not Sentinel:\n                    return default\n                else:\n                    raise KeyError(key)", "idx": 1140}
{"namespace": "playhouse.signals.Signal.connect", "completion": "        if name is None:\n            name = receiver.__name__\n\n        if sender is not None:\n            key = (name, sender)\n        else:\n            key = name\n\n        if key in self._receivers:\n            raise ValueError('Receiver with name {} and sender {} already exists'.format(name, sender))\n\n        self._receivers.add(key)\n        self._receiver_list.append((receiver, name, sender))", "idx": 1141}
{"namespace": "playhouse.signals.Signal.disconnect", "completion": "        if receiver is not None:\n            name = name or receiver.__name__\n            key = (name, sender)\n            if key in self._receivers:\n                self._receivers.remove(key)\n                self._receiver_list = [(n, r, s) for n, r, s in self._receiver_list if (n, s) != key]", "idx": 1142}
{"namespace": "backtrader.trade.Trade.update", "completion": "        # Increase commissions\n        self.commission += commission\n\n        # Update size\n        self.size += size\n\n        # Check if currently opened\n        if size != 0:\n            self.isopen = True\n            self.justopened = True\n\n        # Update current trade length\n        self.barlen += 1\n\n        # Record if position was closed (set size to null)\n        if self.size == 0:\n            self.isclosed = True\n\n        # Record last bar for the trade\n        if self.size != 0:\n            self.barclose = self.data.barnum\n        else:\n            self.baropen = self.data.barnum\n\n        # Update average price if the absolute size is bigger than the absolute old size\n        if abs(self.size) >= abs(self.size - size):\n            self.price = price\n        # Reduce or close position if that condition is not met\n        else:\n            self.price = (self.price * abs(self.size - size) + price * abs(size)) / abs(self.size)\n\n        # Update the attributes of the trade object and history if needed\n        if self.historyon:\n            self.history.append(TradeHistory(\n                self.status, self.data.datetime(), self.barlen, self.size,\n                self.price, self.value, self.pnl, self.pnlcomm, self.data._tz))\n\n        self.justopened = False", "idx": 1143}
{"namespace": "ydata_profiling.profile_report.ProfileReport.typeset", "completion": "        if self._typeset is None:\n            self._typeset = VisionsTypeset(\n                type_schema=self._type_schema, config=self.config\n            )\n        return self._typeset", "idx": 1144}
{"namespace": "ydata_profiling.report.presentation.flavours.html.frequency_table.HTMLFrequencyTable.render", "completion": "        if isinstance(self.content, list):\n            rows = \"\"\n            for row in self.content:\n                rows += \"<tr>\"\n                for cell in row:\n                    rows += f\"<td>{cell}</td>\"\n                rows += \"</tr>\"\n            html = f\"\"\"\n            <table>\n                {rows}\n            </table>\n            \"\"\"\n        else:\n            html = f\"\"\"\n            <table>\n                <tr>\n                    <td>{self.content}</td>\n                </tr>\n            </table>\n            \"\"\"\n        return html", "idx": 1145}
{"namespace": "ydata_profiling.report.presentation.flavours.html.image.HTMLImage.render", "completion": "        return f'<img src=\"{self.content}\" alt=\"{self.alt}\" width=\"{self.width}\" height=\"{self.height}\">'", "idx": 1146}
{"namespace": "ydata_profiling.model.summary_algorithms.histogram_compute", "completion": "    max_bins = config.max_histogram_bins\n    bins = min(max_bins, n_unique)\n    hist, bin_edges = np.histogram(finite_values, bins=bins, weights=weights)\n\n    return {\n        \"name\": name,\n        \"histogram\": hist.tolist(),\n        \"bin_edges\": bin_edges.tolist(),\n        \"chi_squared\": _chi_squared(hist, weights),\n    }", "idx": 1147}
{"namespace": "ydata_profiling.model.summarizer.BaseSummarizer.summarize", "completion": "        # Get the appropriate summarization algorithm based on the data type\n        summarization_algorithm = self._get_summarization_algorithm(dtype)\n\n        # Apply the summarization algorithm to the series\n        summary = summarization_algorithm(config, series)\n\n        return summary", "idx": 1148}
{"namespace": "ydata_profiling.model.pandas.discretize_pandas.Discretizer.discretize_dataframe", "completion": "        df_copy = dataframe.copy()\n        numerical_columns = df_copy.select_dtypes(include=[np.number]).columns\n\n        for column in numerical_columns:\n            if self.discretization_type == DiscretizationType.UNIFORM:\n                df_copy[column] = pd.cut(df_copy[column], bins=self.n_bins, labels=False)\n            elif self.discretization_type == DiscretizationType.QUANTILE:\n                df_copy[column] = pd.qcut(df_copy[column], q=self.n_bins, labels=False)\n\n        if self.reset_index:\n            df_copy.reset_index(drop=True, inplace=True)\n\n        return df_copy", "idx": 1149}
{"namespace": "ydata_profiling.model.pandas.correlations_pandas.pandas_cramers_compute", "completion": "\n    categorical_variables = [\n        var_name\n        for var_name, var_summary in summary.items()\n        if var_summary[\"type\"] == \"categorical\"\n    ]\n\n    if len(categorical_variables) <= 1:\n        return None\n\n    correlation_matrix = pd.DataFrame(index=categorical_variables, columns=categorical_variables)\n\n    for var1, var2 in itertools.combinations(categorical_variables, 2):\n        confusion_matrix = pd.crosstab(df[var1], df[var2])\n        correlation = _cramers_corrected_stat(confusion_matrix, correction=True)\n        correlation_matrix.loc[var1, var2] = correlation\n        correlation_matrix.loc[var2, var1] = correlation\n\n    return correlation_matrix", "idx": 1150}
{"namespace": "ydata_profiling.model.pandas.correlations_pandas.pandas_auto_compute", "completion": "\n    numerical_cols = {\n        key\n        for key, value in summary.items()\n        if value[\"type\"] == \"Numeric\" and value[\"n_distinct\"] > 1\n    }\n\n    categorical_cols = {\n        key\n        for key, value in summary.items()\n        if value[\"type\"] in {\"Categorical\", \"Boolean\"} and 1 < value[\"n_distinct\"] <= config.categorical_maximum_correlation_distinct\n    }\n\n    if len(numerical_cols) <= 1 and len(categorical_cols) <= 1:\n        return None\n\n    discretized_df = df.copy()\n    for col in numerical_cols:\n        discretized_df[col] = pd.qcut(df[col], q=10, duplicates=\"drop\")\n\n    correlation_matrix = pd.DataFrame(index=numerical_cols | categorical_cols, columns=numerical_cols | categorical_cols)\n\n    for col1, col2 in itertools.product(numerical_cols, repeat=2):\n        if col1 != col2:\n            correlation_matrix.loc[col1, col2] = _pairwise_spearman(discretized_df[col1], discretized_df[col2])\n\n    for col1, col2 in itertools.product(categorical_cols, numerical_cols):\n        correlation_matrix.loc[col1, col2] = _pairwise_cramers(discretized_df[col1], discretized_df[col2])\n\n    for col1, col2 in itertools.product(categorical_cols, repeat=2):\n        if col1 != col2:\n            confusion_matrix = pd.crosstab(discretized_df[col1], discretized_df[col2])\n            if not confusion_matrix.empty:\n                correlation_matrix.loc[col1, col2] = _cramers_corrected_stat(confusion_matrix, correction=True)\n\n    return correlation_matrix", "idx": 1151}
{"namespace": "ydata_profiling.controller.console.main", "completion": "\n    # Parse the command line arguments\n    parsed_args = parse_args(args)\n\n    # Generate a profile report\n    profile = ProfileReport(\n        Path(parsed_args.input_file),\n        title=parsed_args.title,\n        pool_size=parsed_args.pool_size,\n        infer_dtypes=parsed_args.infer_dtypes,\n        config_file=parsed_args.config_file,\n    )\n\n    # Save the report to the output file\n    if parsed_args.output_file:\n        profile.to_file(parsed_args.output_file)\n    else:\n        profile.to_file(Path(parsed_args.input_file).with_suffix(\".html\"))\n\n    # Open the report if not in silent mode\n    if not parsed_args.silent:\n        profile.to_notebook_iframe()", "idx": 1152}
{"namespace": "ydata_profiling.utils.cache.cache_file", "completion": "    data_path = get_data_path()\n    file_path = data_path / file_name\n\n    if not file_path.exists():\n        with request.urlopen(url) as response, open(file_path, 'wb') as out_file:\n            data = response.read()\n            out_file.write(data)\n\n    return file_path", "idx": 1153}
{"namespace": "ydata_profiling.utils.dataframe.expand_mixed", "completion": "    if types is None:\n        types = [list, dict, tuple]\n\n    for column in df.columns:\n        if df[column].apply(lambda x: any(isinstance(x, t) for t in types)).any():\n            expanded = pd.json_normalize(df[column])\n            expanded.columns = [f\"{column}_{col}\" for col in expanded.columns]\n            df = pd.concat([df, expanded], axis=1)\n            df = df.drop(columns=column)\n\n    return df", "idx": 1154}
{"namespace": "pysnooper.utils.ensure_tuple", "completion": "    if isinstance(x, str):\n        return (x,)\n    try:\n        return tuple(x)\n    except TypeError:\n        return (x,)", "idx": 1155}
{"namespace": "rq.serializers.resolve_serializer", "completion": "    if serializer is None:\n        return DefaultSerializer\n    elif isinstance(serializer, str):\n        # Load the serializer from the provided string path\n        module, _, name = serializer.rpartition('.')\n        serializer_module = __import__(module, fromlist=[name])\n        return getattr(serializer_module, name)\n    else:\n        # Check if the provided serializer has 'dumps' and 'loads' methods\n        if not hasattr(serializer, 'dumps') or not hasattr(serializer, 'loads'):\n            raise NotImplementedError(\"The serializer must implement 'dumps' and 'loads' methods\")\n        return serializer", "idx": 1156}
{"namespace": "lux.vis.Vis.Vis.get_attr_by_channel", "completion": "        return list(filter(lambda x: x.channel == channel, self._inferred_intent))", "idx": 1157}
{"namespace": "lux.action.default.register_default_actions", "completion": "        from lux.utils import utils\n        from lux.vis.Vis import Vis\n        from lux.actions import bar, scatter, line, histogram, heatmap, density, count, mean, correlation, boxplot, trend, time_series\n    from lux.actions import bar, scatter, line, histogram, heatmap, density, count, mean, correlation, boxplot, trend, time_series\n    from lux.vis.Vis import Vis\n    from lux.utils import utils\n\n    # Define display conditions for each action\n    bar.displayCondition = lambda visList: utils.get_agg_col_datatype(visList)[\"quantitative\"] and len(visList) > 1\n    scatter.displayCondition = lambda visList: utils.get_agg_col_datatype(visList)[\"quantitative\"] and len(visList) > 1\n    line.displayCondition = lambda visList: utils.get_agg_col_datatype(visList)[\"temporal\"] and len(visList) > 1\n    histogram.displayCondition = lambda visList: utils.get_agg_col_datatype(visList)[\"quantitative\"] and len(visList) > 1\n    heatmap.displayCondition = lambda visList: utils.get_agg_col_datatype(visList)[\"quantitative\"] and len(visList) > 1\n    density.displayCondition = lambda visList: utils.get_agg_col_datatype(visList)[\"quantitative\"] and len(visList) > 1\n    count.displayCondition = lambda visList: len(visList) > 1\n    mean.displayCondition = lambda visList: utils.get_agg_col_datatype(visList)[\"quantitative\"] and len(visList) > 1\n    correlation.displayCondition = lambda visList: utils.get_agg_col_datatype(visList)[\"quantitative\"] and len(visList) > 1\n    boxplot.displayCondition = lambda visList: utils.get_agg_col_datatype(visList)[\"quantitative\"] and len(visList) > 1\n    trend.displayCondition = lambda visList: utils.get_agg_col_datatype(visList)[\"temporal\"] and len(visList) > 1\n    time_series.displayCondition = lambda visList: utils.get_agg_col_datatype(visList)[\"temporal\"] and len(visList) > 1\n\n    # Globally register each action with its corresponding display condition\n    Vis.register_action(bar)\n    Vis.register_action(scatter)\n    Vis.register_action(line)\n    Vis.register_action(histogram)\n    Vis.register_action(heatmap)\n    Vis.register_action(density)\n    Vis.register_action(count)\n    Vis.register_action(mean)\n    Vis.register_action(correlation)\n    Vis.register_action(boxplot)\n    Vis.register_action(trend)\n    Vis.register_action(time_series)", "idx": 1158}
{"namespace": "folium.utilities.get_bounds", "completion": "\n    lat_min = None\n    lon_min = None\n    lat_max = None\n    lon_max = None\n\n    for location in locations:\n        if lonlat:\n            lon, lat = location\n        else:\n            lat, lon = location\n\n        lat_min = none_min(lat_min, lat)\n        lon_min = none_min(lon_min, lon)\n        lat_max = none_max(lat_max, lat)\n        lon_max = none_max(lon_max, lon)\n\n    return [[lat_min, lon_min], [lat_max, lon_max]]", "idx": 1159}
{"namespace": "folium.features.VegaLite.vegalite_major_version", "completion": "\n        schema = self.data.get(\"$schema\", \"\")\n        if schema:\n            version = int(schema.split(\"/\")[-1].split(\".\")[0])\n            return version\n        else:\n            return 0", "idx": 1160}
{"namespace": "music_dl.utils.colorize", "completion": "    if platform.system() == \"Windows\" or color not in colors:\n        return string\n    return f\"{colors[color]}{string}\\033[0m\"", "idx": 1161}
{"namespace": "music_dl.source.MusicSource.search", "completion": "        search_results = []\n\n        def search_source(source):\n            try:\n                module = importlib.import_module(f\".{source}\", package=\"music_dl.sources\")\n                source_class = getattr(module, source.capitalize())\n                source_instance = source_class()\n                results = source_instance.search(keyword)\n                search_results.extend(results)\n            except Exception as e:\n                self.logger.error(f\"Error searching {source}: {e}\")\n                self.logger.debug(traceback.format_exc())\n\n        threads = []\n        for source in sources_list:\n            thread = threading.Thread(target=search_source, args=(source,))\n            threads.append(thread)\n            thread.start()\n\n        for thread in threads:\n            thread.join()\n\n        # Sort and remove duplicates from search results\n        search_results.sort(key=lambda x: (x['title'], x['singer'], x['size']))\n        unique_results = [search_results[i] for i in range(len(search_results)) if i == 0 or search_results[i] != search_results[i-1]]\n\n        return unique_results", "idx": 1162}
{"namespace": "jwt.utils.base64url_decode", "completion": "    input_bytes = force_bytes(input)\n    padded_input = input_bytes + b'=' * (-len(input_bytes) % 4)\n    return base64.urlsafe_b64decode(padded_input)", "idx": 1163}
{"namespace": "jwt.utils.to_base64url_uint", "completion": "    if val < 0:\n        raise ValueError(\"Input value must be a positive integer\")\n\n    val_bytes = val.to_bytes((val.bit_length() + 7) // 8, byteorder=\"big\")\n\n    if not val_bytes:\n        val_bytes = b\"\\x00\"\n\n    return base64url_encode(val_bytes)", "idx": 1164}
{"namespace": "jwt.algorithms.HMACAlgorithm.prepare_key", "completion": "        if isinstance(key, str):\n            key = key.encode(\"utf-8\")\n        if key.startswith(b\"-----BEGIN \") or key.startswith(b\"ssh-\"):\n            raise InvalidKeyError(\"The specified key is an asymmetric key or x509 certificate and should not be used as an HMAC secret.\")\n        return key", "idx": 1165}
{"namespace": "jwt.algorithms.HMACAlgorithm.to_jwk", "completion": "        if isinstance(key_obj, str):\n            key_obj = key_obj.encode(\"utf-8\")\n        if as_dict:\n            return {\n                \"kty\": \"oct\",\n                \"k\": base64url_encode(key_obj).decode(\"utf-8\"),\n            }\n        else:\n            return json.dumps(\n                {\n                    \"kty\": \"oct\",\n                    \"k\": base64url_encode(key_obj).decode(\"utf-8\"),\n                }\n            )", "idx": 1166}
{"namespace": "jwt.algorithms.HMACAlgorithm.from_jwk", "completion": "        if isinstance(jwk, str):\n            jwk_dict = json.loads(jwk)\n        elif isinstance(jwk, dict):\n            jwk_dict = jwk\n        else:\n            raise ValueError(\"Invalid JWK format. Must be a JSON string or dictionary.\")\n\n        if jwk_dict.get(\"kty\") != \"oct\":\n            raise ValueError(\"Invalid JWK type. Must be 'oct' for HMAC key.\")\n\n        return base64url_decode(jwk_dict[\"k\"])", "idx": 1167}
{"namespace": "sacred.arg_parser._convert_value", "completion": "    try:\n        return ast.literal_eval(value)\n    except (ValueError, SyntaxError):\n        return value", "idx": 1168}
{"namespace": "sacred.utils.recursive_update", "completion": "    for k, v in u.items():\n        if isinstance(v, collections.abc.Mapping):\n            d[k] = recursive_update(d.get(k, {}), v)\n        else:\n            d[k] = v\n    return d", "idx": 1169}
{"namespace": "sacred.utils.iterate_flattened_separately", "completion": "    if manually_sorted_keys is None:\n        manually_sorted_keys = []\n\n    for key in manually_sorted_keys:\n        if key in dictionary:\n            yield key, dictionary[key]\n\n    non_dict_items = {}\n    dict_items = {}\n    for key, value in sorted(dictionary.items()):\n        if isinstance(value, dict):\n            dict_items[key] = value\n        else:\n            non_dict_items[key] = value\n\n    for key, value in sorted(non_dict_items.items()):\n        yield key, value\n\n    for key, value in sorted(dict_items.items()):\n        yield key, PATHCHANGE\n        for subkey, subvalue in iterate_flattened_separately(value):\n            yield f\"{key}.{subkey}\", subvalue", "idx": 1170}
{"namespace": "sacred.utils.iterate_flattened", "completion": "    for key, value in iterate_flattened_separately(d):\n        if value is PATHCHANGE:\n            continue\n        yield key, value", "idx": 1171}
{"namespace": "sacred.utils.iter_prefixes", "completion": "    parts = path.split(\".\")\n    for i in range(1, len(parts) + 1):\n        yield \".\".join(parts[:i])", "idx": 1172}
{"namespace": "sacred.utils.rel_path", "completion": "    if is_prefix(base, path):\n        return path[len(base) + 1:]\n    else:\n        assert False, f\"{base} not a prefix of {path}\"", "idx": 1173}
{"namespace": "sacred.utils.convert_to_nested_dict", "completion": "    nested_dict = {}\n    for key, value in dotted_dict.items():\n        set_by_dotted_path(nested_dict, key, value)\n    return nested_dict", "idx": 1174}
{"namespace": "sacred.utils.format_sacred_error", "completion": "    lines = [short_usage]\n    if e.print_traceback:\n        lines.append(format_filtered_stacktrace(e.filter_traceback))\n    else:\n        lines.append(f\"{e.__class__.__name__}: {str(e)}\")\n    return \"\\n\".join(lines)", "idx": 1175}
{"namespace": "sacred.utils.get_package_version", "completion": "    try:\n        package = importlib.import_module(name)\n        version_string = getattr(package, \"__version__\", None)\n        if version_string:\n            return version.parse(version_string)\n        else:\n            return None\n    except (ModuleNotFoundError, AttributeError):\n        return None", "idx": 1176}
{"namespace": "sacred.experiment.Experiment.main", "completion": "        self.default_command = function\n        return function", "idx": 1177}
{"namespace": "sacred.experiment.Experiment.run", "completion": "        options = options or {}\n        config_updates = config_updates or {}\n        named_configs = named_configs or []\n\n        if command_name is None:\n            command_name = self.default_command\n\n        if command_name is None:\n            raise ValueError(\"No command_name given and no default command set.\")\n\n        if command_name not in self.commands:\n            raise ValueError(f\"Command '{command_name}' not found.\")\n\n        config_updates = get_config_updates(config_updates, named_configs)\n\n        run = create_run(\n            self,\n            command_name,\n            config_updates,\n            info,\n            meta_info,\n            options,\n        )\n\n        run()\n        return run", "idx": 1178}
{"namespace": "sacred.host_info.host_info_getter", "completion": "    if name is None:\n        name = func.__name__\n    host_info_gatherers[name] = HostInfoGetter(func, name)\n    return func", "idx": 1179}
{"namespace": "sacred.ingredient.Ingredient.command", "completion": "            if function in self.commands:\n                return function\n            captured_function = create_captured_function(function, prefix=prefix, unobserved=unobserved)\n            self.commands[function.__name__] = captured_function\n            return captured_function", "idx": 1180}
{"namespace": "sacred.ingredient.Ingredient.config", "completion": "        config_scope = ConfigScope(function)\n        self.config_hooks.append(config_scope)\n        return config_scope", "idx": 1181}
{"namespace": "sacred.ingredient.Ingredient.named_config", "completion": "        config_scope = ConfigScope(func)\n        self.named_configs[func.__name__] = config_scope\n        return config_scope", "idx": 1182}
{"namespace": "sacred.ingredient.Ingredient.gather_commands", "completion": "        for cmd_name, cmd in self.commands.items():\n            yield cmd_name, cmd\n        for ingredient in self.ingredients:\n            for cmd_name, cmd in ingredient.gather_commands():\n                yield cmd_name, cmd", "idx": 1183}
{"namespace": "sacred.ingredient.Ingredient.gather_named_configs", "completion": "        for ingredient, _ in self.traverse_ingredients():\n            for name, config in ingredient.named_configs.items():\n                full_name = join_paths(ingredient.path, name)\n                yield full_name, config", "idx": 1184}
{"namespace": "sacred.dependencies.Source.create", "completion": "        if not os.path.isfile(filename):\n            raise ValueError(f\"invalid filename or file not found {filename}\")\n\n        main_file = get_py_file_if_possible(filename)\n        digest = get_digest(main_file)\n        repo, commit, is_dirty = get_commit_if_possible(main_file, save_git_info)\n\n        return Source(main_file, digest, repo, commit, is_dirty)", "idx": 1185}
{"namespace": "sacred.dependencies.Source.to_json", "completion": "        if base_dir:\n            relative_path = os.path.relpath(self.filename, base_dir)\n            return relative_path, self.digest\n        else:\n            return self.filename, self.digest", "idx": 1186}
{"namespace": "sacred.dependencies.PackageDependency.create", "completion": "        if mod in cls.modname_to_dist:\n            return cls(mod, cls.modname_to_dist[mod].version)\n\n        for dist in pkg_resources.working_set:\n            if mod in getattr(dist, \"get_metadata_lines\", lambda x: [])(\"top_level.txt\"):\n                cls.modname_to_dist[mod] = dist\n                return cls(mod, dist.version)\n\n        return cls(mod, None)", "idx": 1187}
{"namespace": "sacred.dependencies.is_local_source", "completion": "    if modname in MODULE_BLACKLIST:\n        return False\n    if filename.startswith(experiment_path):\n        return True\n    return False", "idx": 1188}
{"namespace": "sacred.dependencies.gather_sources_and_dependencies", "completion": "            import numpy\n\n    if base_dir is None:\n        experiment_path, main = get_main_file(globs, save_git_info)\n    else:\n        experiment_path = base_dir\n        main = None\n\n    source_discovery_strategy = SETTINGS.SOURCE_DISCOVERY_STRATEGY\n    dependency_discovery_strategy = SETTINGS.DEPENDENCY_DISCOVERY_STRATEGY\n\n    sources = source_discovery_strategies[source_discovery_strategy](\n        globs, experiment_path, save_git_info\n    )\n\n    dependencies = dependency_discovery_strategies[dependency_discovery_strategy](\n        globs, experiment_path\n    )\n\n    if main is not None:\n        sources.add(main)\n\n    # Add numpy as a dependency if available\n    try:\n        import numpy\n        dependencies.add(PackageDependency(\"numpy\", numpy.__version__))\n    except ImportError:\n        pass\n\n    return main, sources, dependencies", "idx": 1189}
{"namespace": "sacred.observers.file_storage.FileStorageObserver.resource_event", "completion": "        store_path = self.find_or_save(filename, self.resource_dir)\n        self.run_entry[\"resources\"].append(str(store_path))\n        self.save_json(self.run_entry, \"run.json\")", "idx": 1190}
{"namespace": "sacred.config.signature.Signature.get_free_parameters", "completion": "        if bound:\n            free_params = [p for p in self.arguments if p not in args]\n        else:\n            free_params = [p for p in self.arguments if p not in kwargs]\n\n        return free_params", "idx": 1191}
{"namespace": "sacred.config.signature.Signature.construct_arguments", "completion": "        expected_args = self._get_expected_args(bound)\n        free_params = self.get_free_parameters(args, kwargs, bound)\n\n        # Check for unexpected arguments\n        unexpected_args = [a for a in args[len(expected_args) :] if a not in free_params]\n        if unexpected_args:\n            raise SignatureError(\n                \"Unexpected arguments: {}\".format(\", \".join(unexpected_args))\n            )\n\n        # Fill in missing arguments using options\n        for param in free_params:\n            if param in options:\n                kwargs[param] = options[param]\n            elif param in self.kwargs:\n                kwargs[param] = self.kwargs[param]\n            else:\n                raise MissingConfigError(\n                    \"Missing configuration for parameter '{}'\".format(param)\n                )\n\n        # Override default arguments with options\n        for param, value in options.items():\n            if param in self.kwargs:\n                kwargs[param] = value\n\n        # Check for conflicting values for a parameter in both args and kwargs\n        for param in args:\n            if param in kwargs and args.index(param) < expected_args.index(param):\n                raise SignatureError(\n                    \"Conflicting values for parameter '{}' in both args and kwargs\".format(\n                        param\n                    )\n                )\n\n        # Check for unfilled parameters\n        unfilled_params = [p for p in expected_args[len(args) :] if p not in kwargs]\n        if unfilled_params:\n            raise MissingConfigError(\n                \"Unfilled parameters: {}\".format(\", \".join(unfilled_params))\n            )\n\n        return args, kwargs", "idx": 1192}
{"namespace": "sacred.config.config_files.load_config_file", "completion": "    handler = get_handler(filename)\n    with open(filename, \"r\" + handler.mode) as file:\n        return handler.load(file)", "idx": 1193}
{"namespace": "sacred.config.custom_containers.DogmaticDict.get", "completion": "        if k in self:\n            return self[k]\n        elif self.fallback and k in self.fallback:\n            return self.fallback[k]\n        else:\n            return d", "idx": 1194}
{"namespace": "sacred.config.custom_containers.DogmaticDict.revelation", "completion": "        missing_keys = set(self.fixed.keys()) - set(self.keys())\n        for key in missing_keys:\n            if isinstance(self.fixed[key], DogmaticDict):\n                subkeys = {f\"{key}.{subkey}\" for subkey in self.fixed[key].revelation()}\n                missing_keys |= subkeys\n            self[key] = self.fixed[key]\n        return missing_keys", "idx": 1195}
{"namespace": "sacred.config.custom_containers.make_read_only", "completion": "    if isinstance(o, dict):\n        return ReadOnlyDict({k: make_read_only(v) for k, v in o.items()})\n    elif isinstance(o, list):\n        return ReadOnlyList([make_read_only(v) for v in o])\n    elif isinstance(o, tuple):\n        return tuple(make_read_only(v) for v in o)\n    else:\n        return o", "idx": 1196}
{"namespace": "sacred.config.config_scope.dedent_function_body", "completion": "    lines = body.split('\\n')\n    non_empty_lines = [line for line in lines if not is_empty_or_comment(line)]\n    indent = re.match(r'^\\s*', non_empty_lines[0]).group(0)\n    dedented_lines = [dedent_line(line, indent) for line in lines]\n    return '\\n'.join(dedented_lines)", "idx": 1197}
{"namespace": "boltons.funcutils.FunctionBuilder.get_sig_str", "completion": "            if with_annotations:\n                return inspect_formatargspec(self.args, self.varargs,\n                                             self.varkw, self.defaults,\n                                             self.kwonlyargs, self.kwonlydefaults,\n                                             self.annotations)\n            else:\n                return inspect_formatargspec(self.args, self.varargs,\n                                             self.varkw, self.defaults)", "idx": 1198}
{"namespace": "boltons.funcutils.FunctionBuilder.get_invocation_str", "completion": "            args = self.args\n            varargs = self.varargs\n            varkw = self.varkw\n            kwonlyargs = self.kwonlyargs\n\n            arg_strs = [arg for arg in args]\n            if varargs:\n                arg_strs.append('*' + varargs)\n            if kwonlyargs:\n                arg_strs.append('*')\n            if varkw:\n                arg_strs.append('**' + varkw)\n\n            return ', '.join(arg_strs)", "idx": 1199}
{"namespace": "boltons.funcutils.FunctionBuilder.from_func", "completion": "        fb = cls(name=func.__name__,\n                 doc=func.__doc__,\n                 module=func.__module__,\n                 **cls._argspec_to_dict(func))\n        if isinstance(func, functools.partial):\n            fb.body = 'return _func(%s)' % fb.get_invocation_str()\n        return fb", "idx": 1200}
{"namespace": "boltons.funcutils.FunctionBuilder.get_defaults_dict", "completion": "        defaults_dict = {}\n        for arg, default in zip(self.args[-len(self.defaults):], self.defaults):\n            defaults_dict[arg] = default\n        return defaults_dict", "idx": 1201}
{"namespace": "boltons.funcutils.FunctionBuilder.get_arg_names", "completion": "        if only_required:\n            return tuple(arg for arg in self.args if arg not in self.defaults)\n        else:\n            return tuple(self.args + self.kwonlyargs)", "idx": 1202}
{"namespace": "boltons.ioutils.SpooledIOBase.writelines", "completion": "        for line in lines:\n            self.write(line)", "idx": 1203}
{"namespace": "boltons.ioutils.SpooledBytesIO.write", "completion": "        self._checkClosed()\n        if not isinstance(s, binary_type):\n            raise TypeError(\"bytes expected, got {}\".format(type(s)))\n        if self.len + len(s) > self._max_size:\n            self.rollover()\n        self.buffer.write(s)", "idx": 1204}
{"namespace": "boltons.ioutils.SpooledBytesIO.seek", "completion": "        self._checkClosed()\n        return self.buffer.seek(pos, mode)", "idx": 1205}
{"namespace": "boltons.ioutils.SpooledBytesIO.len", "completion": "        if self._rolled:\n            return len(self.buffer.getvalue())\n        else:\n            return self.buffer.tell()", "idx": 1206}
{"namespace": "boltons.ioutils.SpooledStringIO.read", "completion": "        self._checkClosed()\n        return self.buffer.read(n)", "idx": 1207}
{"namespace": "boltons.ioutils.SpooledStringIO.write", "completion": "        self._checkClosed()\n        if not isinstance(s, text_type):\n            raise TypeError(\"{} expected, got {}\".format(\n                text_type.__name__,\n                type(s).__name__\n            ))\n\n        if self.tell() + len(s) >= self._max_size:\n            self.rollover()\n        self.buffer.write(s.encode('utf-8'))\n        self._tell += len(s)", "idx": 1208}
{"namespace": "boltons.ioutils.SpooledStringIO.seek", "completion": "        self._checkClosed()\n        if mode == 0:\n            self._tell = pos\n        elif mode == 1:\n            self._tell += pos\n        elif mode == 2:\n            self._tell = len(self.getvalue()) + pos\n        else:\n            raise ValueError(f'Invalid whence ({mode}, should be 0, 1, or 2)')\n        return self._tell", "idx": 1209}
{"namespace": "boltons.ioutils.SpooledStringIO.tell", "completion": "        \"\"\"\n        Return the codepoint position in the SpooledStringIO instance.\n\n        Input-Output Arguments\n        :param self: SpooledStringIO, an instance of SpooledStringIO class.\n        :return: Int, the codepoint position.\n\n        \"\"\"\n        self._checkClosed()\n        return self._tell", "idx": 1210}
{"namespace": "boltons.ioutils.SpooledStringIO.len", "completion": "        codepoint_count = 0\n        chunk_size = 1024  # Define the chunk size for reading the file\n        current_position = self.tell()  # Get the current position in the file\n        self.seek(0)  # Move to the beginning of the file\n        while True:\n            chunk = self.read(chunk_size)  # Read a chunk of data from the file\n            if not chunk:  # If there is no more data to read, break the loop\n                break\n            codepoint_count += len(chunk)  # Count the number of codepoints in the chunk\n        self.seek(current_position)  # Return to the original position in the file\n        return codepoint_count  # Return the total number of codepoints in the file", "idx": 1211}
{"namespace": "boltons.ioutils.MultiFileReader.read", "completion": "        if amt is None:\n            amt = -1\n        data = b'' if self._joiner == b'' else u''\n        while amt != 0 and self._index < len(self._fileobjs):\n            fileobj = self._fileobjs[self._index]\n            chunk = fileobj.read(amt)\n            if not chunk:\n                self._index += 1\n            else:\n                data += chunk\n                if amt > 0:\n                    amt -= len(chunk)\n        return data", "idx": 1212}
{"namespace": "boltons.ioutils.MultiFileReader.seek", "completion": "        if whence != os.SEEK_SET:\n            raise NotImplementedError(\"MultiFileReader.seek() only supports os.SEEK_SET\")\n        if offset != 0:\n            raise NotImplementedError(\"MultiFileReader only supports seeking to start at this time\")\n        for fileobj in self._fileobjs:\n            fileobj.seek(0)", "idx": 1213}
{"namespace": "boltons.listutils.BarrelList.insert", "completion": "        list_idx, rel_idx = self._translate_index(index)\n        if list_idx is None:\n            raise IndexError(\"list index out of range\")\n        self.lists[list_idx].insert(rel_idx, item)\n        self._balance_list(list_idx)", "idx": 1214}
{"namespace": "boltons.listutils.BarrelList.pop", "completion": "        if not a:\n            if not self.lists:\n                raise IndexError(\"pop from empty list\")\n            return self.lists[-1].pop()\n        list_idx, rel_idx = self._translate_index(a[0])\n        if list_idx is None:\n            raise IndexError(\"pop index out of range\")\n        return self.lists[list_idx].pop(rel_idx)", "idx": 1215}
{"namespace": "boltons.listutils.BarrelList.sort", "completion": "\n        sorted_lists = [sorted(sublist) for sublist in self.lists]\n        merged_list = list(heapq.merge(*sorted_lists))\n        self.lists = [merged_list]\n        self._balance_list(0)", "idx": 1216}
{"namespace": "boltons.urlutils.URL.path", "completion": "        self.path_parts = tuple([unquote(p) if '%' in p else p for p in path_text.split('/')])", "idx": 1217}
{"namespace": "boltons.urlutils.URL.navigate", "completion": "        if isinstance(dest, URL):\n            new_url = dest\n        else:\n            new_url = URL(dest)\n\n        new_url.normalize()\n        return new_url", "idx": 1218}
{"namespace": "boltons.urlutils.URL.to_text", "completion": "        parts = []\n        _add = parts.append\n\n        if self.scheme:\n            _add(self.scheme)\n            _add(':')\n\n        if self.uses_netloc:\n            _add(self._netloc_sep)\n            _add(self.get_authority(full_quote=full_quote))\n\n        if self.path:\n            _add('/')\n            _add(self.path)\n\n        if self.query_params:\n            _add('?')\n            _add(self.query_params.to_text(full_quote=full_quote))\n\n        if self.fragment:\n            _add('#')\n            _add(quote_fragment_part(self.fragment, full_quote=full_quote))\n\n        return u''.join(parts)", "idx": 1219}
{"namespace": "boltons.urlutils.QueryParamDict.to_text", "completion": "        query_string = ''\n        for key, value in self.items(multi=True):\n            if isinstance(value, list):\n                for v in value:\n                    if full_quote:\n                        query_string += quote(key) + '=' + quote(v) + '&'\n                    else:\n                        query_string += key + '=' + v + '&'\n            else:\n                if full_quote:\n                    query_string += quote(key) + '=' + quote(value) + '&'\n                else:\n                    query_string += key + '=' + value + '&'\n        return query_string.rstrip('&')", "idx": 1220}
{"namespace": "boltons.tbutils.TracebackInfo.from_traceback", "completion": "        if tb is None:\n            exc_type, exc_value, tb = sys.exc_info()\n            if tb is None:\n                raise ValueError('no tb set and no exception being handled')\n        return cls.from_frame(tb.tb_frame, limit=limit)", "idx": 1221}
{"namespace": "boltons.tbutils.ExceptionInfo.get_formatted", "completion": "        formatted_tb = self.tb_info.get_formatted()\n        return f\"{self.exc_type}: {self.exc_msg}\\n{formatted_tb}\"", "idx": 1222}
{"namespace": "boltons.cacheutils.ThresholdCounter.get", "completion": "        return self._count_map.get(key, [default, 0])[0]", "idx": 1223}
{"namespace": "boltons.iterutils.backoff_iter", "completion": "    if start <= 0 or stop <= 0:\n        raise ValueError(\"start and stop must be positive numbers\")\n    if factor <= 1.0:\n        raise ValueError(\"factor must be greater than 1.0\")\n    if count == 0:\n        return\n\n    cur = start\n    i = 0\n    while cur < stop:\n        yield cur\n        i += 1\n        if count is not None and i >= count:\n            return\n        cur *= factor\n        if jitter:\n            if jitter is True:\n                jitter = 1.0\n            cur += cur * random.uniform(-jitter, jitter)\n        if cur > stop:\n            yield stop\n            return", "idx": 1224}
{"namespace": "boltons.cacheutils.cached", "completion": "    def decorator(func):\n        return CachedFunction(func, cache, scoped, typed, key)\n    return decorator", "idx": 1225}
{"namespace": "boltons.timeutils.total_seconds", "completion": "    return td.days * 24 * 3600 + td.seconds + td.microseconds / 1e6", "idx": 1226}
{"namespace": "boltons.gcutils.get_all", "completion": "    instances = []\n    for obj in gc.get_objects():\n        if isinstance(obj, type_obj):\n            if include_subtypes or type(obj) == type_obj:\n                instances.append(obj)\n    return instances", "idx": 1227}
{"namespace": "boltons.timeutils.daterange", "completion": "    current = start\n    while stop is None or (current < stop if step > 0 else current > stop) or (inclusive and current == stop):\n        yield current\n        if step > 0:\n            current += timedelta(days=step)\n        else:\n            current -= timedelta(days=abs(step))", "idx": 1228}
{"namespace": "boltons.mathutils.clamp", "completion": "    return max(min(x, upper), lower)", "idx": 1229}
{"namespace": "boltons.mathutils.ceil", "completion": "    if options is not None:\n        return min(filter(lambda y: y >= x, options))\n    else:\n        return _ceil(x)", "idx": 1230}
{"namespace": "boltons.formatutils.get_format_args", "completion": "    positional_args = []\n    named_args = []\n\n    for lit, fname, fspec, conv in Formatter().parse(fstr):\n        if fname is not None:\n            if fname.isdigit():\n                positional_args.append((fname, _TYPE_MAP.get(fspec, str)))\n            else:\n                named_args.append((fname, _TYPE_MAP.get(fspec, str)))\n\n    return positional_args, named_args", "idx": 1231}
{"namespace": "boltons.mathutils.floor", "completion": "    if options is None:\n        return _floor(x)\n    options = sorted(options)\n    i = bisect.bisect_right(options, x)\n    if i == 0:\n        raise ValueError(\"no floor options less than or equal to: %r\" % x)\n    return options[i-1]", "idx": 1232}
{"namespace": "boltons.dictutils.OneToOne.setdefault", "completion": "        if key not in self:\n            self[key] = default\n        return self[key]", "idx": 1233}
{"namespace": "boltons.dictutils.OneToOne.update", "completion": "        if isinstance(dict_or_iterable, dict):\n            for key, value in dict_or_iterable.items():\n                self[key] = value\n        else:\n            for item in dict_or_iterable:\n                if len(item) != 2:\n                    raise ValueError(\"Iterable items must be key-value pairs\")\n                key, value = item\n                self[key] = value\n        for key, value in kw.items():\n            self[key] = value", "idx": 1234}
{"namespace": "boltons.dictutils.ManyToMany.get", "completion": "        return self.data.get(key, default)", "idx": 1235}
{"namespace": "boltons.dictutils.FrozenDict.updated", "completion": "        new_dict = self.copy()\n        new_dict.update(*a, **kw)\n        return new_dict", "idx": 1236}
{"namespace": "boltons.dictutils.subdict", "completion": "    if keep is None:\n        keep = list(d.keys())\n    if drop is None:\n        drop = []\n\n    sub_d = {k: v for k, v in d.items() if k in keep and k not in drop}\n    return sub_d", "idx": 1237}
{"namespace": "boltons.dictutils.FrozenDict.__repr__", "completion": "        cn = self.__class__.__name__\n        dict_repr = dict.__repr__(self)\n        return \"%s(%s)\" % (cn, dict_repr)", "idx": 1238}
{"namespace": "gunicorn.config.validate_callable", "completion": "    def _validate_callable(val):\n        if val is None:\n            return None\n        if isinstance(val, str):\n            val = util.load_class(val)\n        if not callable(val):\n            raise TypeError(\"Not a callable object: %s\" % val)\n        if arity != -1 and arity != inspect.getfullargspec(val).args:\n            raise TypeError(\"Callable object does not have the specified arity: %s\" % val)\n        return val\n    return _validate_callable", "idx": 1239}
{"namespace": "gunicorn.config.get_default_config_file", "completion": "    default_config_file = os.path.join(os.getcwd(), 'gunicorn.conf.py')\n    if os.path.exists(default_config_file):\n        return default_config_file\n    else:\n        return None", "idx": 1240}
{"namespace": "gunicorn.util.is_ipv6", "completion": "    try:\n        socket.inet_pton(socket.AF_INET6, addr)\n        return True\n    except socket.error:\n        return False", "idx": 1241}
{"namespace": "gunicorn.systemd.listen_fds", "completion": "    if \"LISTEN_PID\" in os.environ and int(os.environ.get(\"LISTEN_PID\")) == os.getpid():\n        if \"LISTEN_FDS\" in os.environ:\n            listen_fds = int(os.environ.get(\"LISTEN_FDS\"))\n            if unset_environment:\n                os.unsetenv(\"LISTEN_PID\")\n                os.unsetenv(\"LISTEN_FDS\")\n            return listen_fds\n    return 0", "idx": 1242}
{"namespace": "gunicorn.util.http_date", "completion": "    if timestamp is None:\n        timestamp = time.time()\n    return email.utils.formatdate(timestamp, usegmt=True)", "idx": 1243}
{"namespace": "gunicorn.util.parse_address", "completion": "    if netloc.startswith('[') and ']' in netloc:\n        # IPv6\n        host, port = netloc[1:].split(']')\n        if ':' in port:\n            port = port.split(':')[1]\n        else:\n            port = default_port\n        return host, int(port)\n    elif netloc.startswith('fd://'):\n        # File descriptor\n        return netloc, None\n    elif netloc.startswith('/'):\n        # Unix socket\n        return netloc, None\n    else:\n        # TCP address\n        parts = netloc.split(':')\n        host = parts[0]\n        port = parts[1] if len(parts) > 1 else default_port\n        return host, int(port)", "idx": 1244}
{"namespace": "gunicorn.util.to_bytestring", "completion": "    if isinstance(value, str):\n        return value.encode(encoding)\n    elif isinstance(value, bytes):\n        return value\n    else:\n        raise TypeError(\"Expected a string or bytes, but got {}\".format(type(value)))", "idx": 1245}
{"namespace": "gunicorn.util.warn", "completion": "    sys.stderr.write(\"WARNING: %s\\n\" % msg)", "idx": 1246}
{"namespace": "gunicorn.util.split_request_uri", "completion": "    if uri.startswith(\"//\"):\n        uri = \".\" + uri\n    return urllib.parse.urlsplit(uri)", "idx": 1247}
{"namespace": "praw.models.listing.listing.ModNoteListing.after", "completion": "        if getattr(self, \"has_next_page\", False):\n            return getattr(self, \"end_cursor\", None)\n        else:\n            return None", "idx": 1248}
{"namespace": "praw.models.util.permissions_string", "completion": "    if permissions is None:\n        return \"+all\"\n    \n    permission_changes = []\n    for permission in permissions:\n        if permission.startswith(\"+\") or permission.startswith(\"-\"):\n            permission_changes.append(permission)\n        else:\n            if permission not in known_permissions:\n                permission_changes.append(\"+\" + permission)\n    \n    return \",\".join(permission_changes)", "idx": 1249}
{"namespace": "jc.cli.JcCli.json_out", "completion": "        if self.pretty:\n            return json.dumps(self.data_out, indent=self.json_indent, separators=self.json_separators)\n        else:\n            return json.dumps(self.data_out)", "idx": 1250}
{"namespace": "pythonforandroid.pythonpackage.transform_dep_for_pip", "completion": "\n    if \"@\" in dependency and \"://\" in dependency:\n        return dependency\n    else:\n        return f\"git+{dependency}\"", "idx": 1251}
{"namespace": "pythonforandroid.graph.fix_deplist", "completion": "    modified_deps = []\n    for dep in deps:\n        if isinstance(dep, str):\n            modified_deps.append((dep.lower(),))\n        else:\n            modified_deps.append(tuple(d.lower() for d in dep))\n    return modified_deps", "idx": 1252}
{"namespace": "pythonforandroid.util.walk_valid_filens", "completion": "    for dirpath, dirnames, filenames in walk(base_dir):\n        # Remove invalid directory names\n        dirnames[:] = [d for d in dirnames if d not in invalid_dir_names]\n\n        for filename in filenames:\n            full_path = join(dirpath, filename)\n            # Check if the file matches any of the invalid file patterns\n            if not any(fnmatch(full_path, pattern) for pattern in invalid_file_patterns):\n                yield full_path", "idx": 1253}
{"namespace": "pythonforandroid.bootstrap._cmp_bootstraps_by_priority", "completion": "    priority_order = {name: index for index, name in enumerate(default_recipe_priorities)}\n    priority_a = priority_order.get(a.name, len(default_recipe_priorities))\n    priority_b = priority_order.get(b.name, len(default_recipe_priorities)\n    return priority_a - priority_b", "idx": 1254}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.all_bootstraps", "completion": "        bootstrap_dirs = [\n            join(cls.ctx.root_dir, 'bootstraps', bootstrap_name)\n            for bootstrap_name in reversed(cls.bootstrap_names)\n        ]\n        available_bootstraps = set()\n        for bootstrap_dir in bootstrap_dirs:\n            if isdir(bootstrap_dir):\n                for entry in listdir(bootstrap_dir):\n                    if isdir(join(bootstrap_dir, entry)):\n                        available_bootstraps.add(entry)\n        return available_bootstraps", "idx": 1255}
{"namespace": "mmcv.image.colorspace._convert_input_type_range", "completion": "    if img.dtype == np.uint8:\n        img = img.astype(np.float32) / 255.0\n    elif img.dtype == np.float32:\n        if np.max(img) > 1.0:\n            img = img / 255.0\n    else:\n        raise TypeError(f\"The input image type should be np.uint8 or np.float32, but got {img.dtype}.\")\n    return img", "idx": 1256}
{"namespace": "mackup.utils.error", "completion": "    print(\"Error: \" + message)\n    sys.exit(1)", "idx": 1257}
{"namespace": "mmcv.image.colorspace._convert_output_type_range", "completion": "    if dst_type == np.float32:\n        img = _convert_input_type_range(img)\n    elif dst_type == np.uint8:\n        img = (img * 255).clip(0, 255).astype(np.uint8)\n    else:\n        raise TypeError('The dst_type should be np.float32 or np.uint8, '\n                        f'but got {dst_type}')\n    return img", "idx": 1258}
{"namespace": "mackup.utils.is_process_running", "completion": "    try:\n        subprocess.check_output([\"pgrep\", process_name])\n        return True\n    except subprocess.CalledProcessError:\n        return False", "idx": 1259}
{"namespace": "stellar.operations._get_pid_column", "completion": "    server_version = raw_conn.connection.connection.server_version\n    if raw_conn.engine.dialect.name not in SUPPORTED_DIALECTS:\n        raise NotSupportedDatabase(\"Database dialect not supported\")\n\n    if raw_conn.engine.dialect.name == 'postgresql':\n        if server_version >= 90200:\n            return 'pid'\n        else:\n            return 'procpid'\n    elif raw_conn.engine.dialect.name == 'mysql':\n        return 'id'\n    else:\n        raise NotSupportedDatabase(\"Database dialect not supported\")", "idx": 1260}
{"namespace": "imapclient.imap_utf7.encode", "completion": "    if isinstance(s, str):\n        s = s.encode('utf-7')\n        s = s.replace(b'+', b'&')\n        return s\n    else:\n        return s", "idx": 1261}
{"namespace": "imapclient.version._imapclient_version_string", "completion": "    major, minor, micro, releaselevel = vinfo\n    version_string = f\"{major}.{minor}.{micro}\"\n    if releaselevel != \"final\":\n        version_string += f\"-{releaselevel}\"\n    return version_string", "idx": 1262}
{"namespace": "telethon.helpers.generate_key_data_from_nonce", "completion": "    server_nonce_bytes = server_nonce.to_bytes(16, byteorder='little', signed=True)\n    new_nonce_bytes = new_nonce.to_bytes(32, byteorder='little', signed=True)\n\n    hash1 = sha1(new_nonce_bytes + server_nonce_bytes).digest()\n    hash2 = sha1(server_nonce_bytes + new_nonce_bytes).digest()\n    hash3 = sha1(new_nonce_bytes + new_nonce_bytes).digest()\n\n    key = hash1 + hash2[:12]\n    iv = hash2[12:] + hash3 + new_nonce_bytes[:4]\n\n    return key, iv", "idx": 1263}
{"namespace": "hbmqtt.codecs.bytes_to_int", "completion": "    return int.from_bytes(data, byteorder='big')", "idx": 1264}
{"namespace": "zulipterminal.helper.display_error_if_present", "completion": "\n    if \"result\" in response and response[\"result\"] == \"error\" and hasattr(controller, \"view\"):\n        controller.view.write_box(\n            f\"Error: {response['msg']}\",\n            \"error\"\n        )", "idx": 1265}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._decode_message_id", "completion": "        try:\n            decoded_id = int(message_id)\n            return decoded_id\n        except ValueError:\n            return None", "idx": 1266}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton.handle_narrow_link", "completion": "        parsed_link = self._parse_narrow_link(self.link)\n        validation_error = self._validate_narrow_link(parsed_link)\n\n        if validation_error:\n            self.view.set_footer_text(validation_error, \"error\")\n        else:\n            self._switch_narrow_to(parsed_link)", "idx": 1267}
{"namespace": "zulipterminal.config.color.color_properties", "completion": "    ColorWithProperties = Enum(\n        'ColorWithProperties',\n        {color.name: color.value + ' ' + ' '.join(prop) for color in colors}\n    )\n    return ColorWithProperties", "idx": 1268}
{"namespace": "twilio.base.deserialize.decimal", "completion": "    if d is None or d == \"\":\n        return \"\"\n    try:\n        return Decimal(d, context=BasicContext)\n    except InvalidOperation:\n        return d", "idx": 1269}
{"namespace": "twilio.base.deserialize.integer", "completion": "    try:\n        return int(i)\n    except (TypeError, ValueError):\n        return i", "idx": 1270}
{"namespace": "twilio.base.serialize.object", "completion": "    try:\n        return json.dumps(obj)\n    except (TypeError, OverflowError):\n        return obj", "idx": 1271}
{"namespace": "twilio.base.serialize.map", "completion": "    return [serialize_func(item) for item in lst]", "idx": 1272}
{"namespace": "twilio.base.obsolete.deprecated_method", "completion": "\n    def deprecated_method_wrapper(func):\n        @functools.wraps(func)\n        def new_func(*args, **kwargs):\n            warnings.simplefilter('always', DeprecationWarning)  # turn off filter\n            warnings.warn(\n                \"Call to deprecated method {}.\".format(func.__name__),\n                category=DeprecationWarning,\n                stacklevel=2\n            )\n            warnings.simplefilter('default', DeprecationWarning)  # reset filter\n            return func(*args, **kwargs)\n\n        return new_func\n\n    return deprecated_method_wrapper", "idx": 1273}
{"namespace": "chatette.utils.sample_indulgent", "completion": "    if nb_items >= len(array):\n        return array.copy()\n    else:\n        return sample(array, nb_items)", "idx": 1274}
{"namespace": "chatette.utils.rchop", "completion": "    if string.endswith(ending):\n        return string[:-len(ending)]\n    return string", "idx": 1275}
{"namespace": "chatette.utils.str_to_bool", "completion": "    if text.lower() == 'true':\n        return True\n    elif text.lower() == 'false':\n        return False\n    else:\n        raise ValueError(\"Input text must be 'True' or 'False'\")", "idx": 1276}
{"namespace": "chatette.utils.min_if_exist", "completion": "    if n1 is not None and n2 is not None:\n        return min(n1, n2)\n    elif n1 is not None:\n        return n1\n    elif n2 is not None:\n        return n2\n    else:\n        return None", "idx": 1277}
{"namespace": "chatette.utils.append_to_list_in_dict", "completion": "    if key in dict_of_lists:\n        dict_of_lists[key].append(value)\n    else:\n        dict_of_lists[key] = [value]", "idx": 1278}
{"namespace": "chatette.utils.extend_list_in_dict", "completion": "    if key not in dict_of_lists:\n        dict_of_lists[key] = values\n    else:\n        dict_of_lists[key].extend(values)", "idx": 1279}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy._is_end_regex", "completion": "        return re.match(r'\\/(g?i?|i?g?)$', word) is not None", "idx": 1280}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.execute", "completion": "        raise NotImplementedError(\"This function should be overridden by subclasses.\")", "idx": 1281}
{"namespace": "aioxmpp.network.group_and_order_srv_records", "completion": "    all_records.sort(key=lambda x: (x[0], x[1]))  # Sort by priority and weight\n\n    if rng is None:\n        rng = random\n\n    groups = itertools.groupby(all_records, key=lambda x: x[0])  # Group by priority\n\n    for _, group in groups:\n        records = list(group)\n        records.sort(key=lambda x: x[1])  # Sort by weight\n        total_weight = sum(record[1] for record in records)\n\n        while records:\n            pick = rng.randint(0, total_weight - 1)\n            for record in records:\n                pick -= record[1]\n                if pick < 0:\n                    yield record[2]\n                    break\n            records.remove(record)\n            total_weight -= record[1]", "idx": 1282}
{"namespace": "aioxmpp.nonza.StreamFeatures.get_feature", "completion": "        try:\n            return self.features[feature_cls.TAG][0]\n        except (IndexError, KeyError):\n            return default", "idx": 1283}
{"namespace": "aioxmpp.connector.XMPPOverTLSConnector._context_factory_factory", "completion": "        def context_factory():\n            ssl_context = metadata.ssl_context_factory()\n            if hasattr(ssl_context, \"set_alpn_protos\"):\n                ssl_context.set_alpn_protos([b'xmpp-client'])\n            verifier.setup_context(ssl_context, None)\n            return ssl_context\n        return context_factory", "idx": 1284}
{"namespace": "aioxmpp.xmltestutils.element_path", "completion": "    path = []\n    while el is not None and el != upto:\n        path.append(el.tag)\n        el = el.getparent()\n    return '/'.join(reversed(path))", "idx": 1285}
{"namespace": "aioxmpp.structs.JID.fromstr", "completion": "        parts = s.split(\"@\", 1)\n        domain = parts[-1]\n        localpart = parts[0] if len(parts) > 1 else None\n        resource = None\n        if \"/\" in domain:\n            domain, resource = domain.split(\"/\", 1)\n        return cls(localpart, domain, resource, strict=strict)", "idx": 1286}
{"namespace": "aioxmpp.security_layer.extract_python_dict_from_x509", "completion": "    result = {}\n    result[\"subject\"] = x509.get_subject().get_components()\n    result[\"subjectAltName\"] = [x509.get_extension(i).get_data().decode() for i in range(x509.get_extension_count()) if x509.get_extension(i).get_short_name() == b'subjectAltName']\n    return result", "idx": 1287}
{"namespace": "aioxmpp.security_layer.extract_blob", "completion": "    return x509.to_cryptography().public_bytes(serialization.Encoding.DER)", "idx": 1288}
{"namespace": "aioxmpp.security_layer.blob_to_pyasn1", "completion": "    return pyasn1.codec.der.decoder.decode(blob)", "idx": 1289}
{"namespace": "aioxmpp.security_layer.extract_pk_blob_from_pyasn1", "completion": "    # Assuming the public key is located in the tbsCertificate -> subjectPublicKeyInfo -> subjectPublicKey field\n    subject_public_key_info = pyasn1_struct['tbsCertificate']['subjectPublicKeyInfo']\n    public_key_blob = pyasn1.codec.der.encoder.encode(subject_public_key_info['subjectPublicKey'])\n    return public_key_blob", "idx": 1290}
{"namespace": "aioxmpp.callbacks.AdHocSignal.ASYNC_WITH_LOOP", "completion": "        if loop is None:\n            loop = asyncio.get_event_loop()\n\n        def wrapper(f):\n            async def async_wrapper(*args, **kwargs):\n                return await f(*args, **kwargs)\n\n            return async_wrapper\n\n        return wrapper", "idx": 1291}
{"namespace": "aioxmpp.callbacks.AdHocSignal.SPAWN_WITH_LOOP", "completion": "        if loop is None:\n            loop = asyncio.get_event_loop()\n\n        def create_spawn_function(corofunc):\n            if not asyncio.iscoroutinefunction(corofunc):\n                raise TypeError(\"must be a coroutine function, got {!r}\".format(corofunc))\n\n            async def spawn_and_log(*args, **kwargs):\n                task = asyncio.ensure_future(corofunc(*args, **kwargs), loop=loop)\n                task.add_done_callback(lambda fut: log_spawned(cls.logger, fut))\n\n            return spawn_and_log\n\n        return create_spawn_function", "idx": 1292}
{"namespace": "aioxmpp.callbacks.first_signal", "completion": "    async def awaitable():\n        fut = asyncio.Future()\n\n        def handler(data):\n            if isinstance(data, Exception):\n                fut.set_exception(data)\n            else:\n                fut.set_result(data)\n\n        for signal in signals:\n            signal.connect(handler)\n\n        result = await fut\n        return result\n\n    return awaitable()", "idx": 1293}
{"namespace": "aioxmpp.tasks.TaskPool.spawn", "completion": "        for group in __groups:\n            limit = self.get_limit(group)\n            if limit is not None and self.get_task_count(group) >= limit:\n                raise RuntimeError(f\"Limit exhausted for group {group}\")\n\n        total_limit = self.get_limit(())\n        if total_limit is not None and self.get_task_count(()) >= total_limit:\n            raise RuntimeError(\"Total limit exhausted\")\n\n        task = asyncio.ensure_future(__coro_fun(*args, **kwargs))\n        self._group_tasks.setdefault((), set()).add(task)\n        for group in __groups:\n            self._group_tasks.setdefault(group, set()).add(task)\n\n        return task", "idx": 1294}
{"namespace": "aioxmpp.protocol.send_and_wait_for", "completion": "\n    async def _wait_for_response():\n        try:\n            if timeout is not None:\n                return await asyncio.wait_for(cb, timeout)\n            else:\n                return await cb\n        except asyncio.TimeoutError:\n            raise TimeoutError(\"Timeout waiting for response\")\n\n    xmlstream.send_xso(send)\n    if cb is not None:\n        response = await _wait_for_response()\n    else:\n        response = None\n    return response", "idx": 1295}
{"namespace": "aioxmpp.testutils.run_coroutine_with_peer", "completion": "    if not loop:\n        loop = asyncio.get_event_loop()\n\n    local_future = asyncio.ensure_future(coroutine, loop=loop)\n    peer_future = asyncio.ensure_future(peer_coroutine, loop=loop)\n\n    try:\n        result, _ = loop.run_until_complete(\n            asyncio.wait(\n                [local_future, peer_future],\n                timeout=timeout))\n    except asyncio.TimeoutError:\n        raise TimeoutError(\"Timeout reached while waiting for coroutines to complete\")\n\n    return result.pop().result()", "idx": 1296}
{"namespace": "aioxmpp.testutils.make_listener", "completion": "    listener = unittest.mock.Mock()\n\n    for signal_name, signal in instance.__class__.__dict__.items():\n        if isinstance(signal, callbacks.Signal):\n            listener.attach_mock(unittest.mock.Mock(), signal_name)\n\n    return listener", "idx": 1297}
{"namespace": "aioxmpp.vcard.service.VCardService.set_vcard", "completion": "        iq = aioxmpp.IQ(\n            type_=aioxmpp.IQType.SET,\n            to=jid,\n            payload=vcard,\n        )\n\n        await self.client.send(iq)", "idx": 1298}
{"namespace": "aioxmpp.rsm.xso.ResultSetMetadata.limit", "completion": "        result = copy.deepcopy(self)\n        result.max_ = max_\n        return result", "idx": 1299}
{"namespace": "aioxmpp.muc.service.Room.features", "completion": "        return {\n            \"urn:xmpp:muc\",\n            \"urn:xmpp:muc:admin\",\n            \"urn:xmpp:muc:invite\",\n            \"urn:xmpp:muc:config\",\n            \"urn:xmpp:muc:history\",\n            \"urn:xmpp:muc#user\",\n            \"urn:xmpp:muc#traffic\",\n            \"urn:xmpp:muc#filter\"\n        }", "idx": 1300}
{"namespace": "aioxmpp.xso.query.EvaluationContext.eval_bool", "completion": "        result = self.eval(expr)\n        return bool(result)", "idx": 1301}
{"namespace": "aioxmpp.xso.query._BoolOpMixin.eval", "completion": "        raise NotImplementedError(\"Subclasses must implement this method\")", "idx": 1302}
{"namespace": "aioxmpp.xso.model.drop_handler", "completion": "    depth = 0\n    while True:\n        ev_type, *ev_args = yield\n        if ev_type == \"start\":\n            depth += 1\n        elif ev_type == \"end\":\n            depth -= 1\n            if depth == 0:\n                break", "idx": 1303}
{"namespace": "aioxmpp.xso.model.guard", "completion": "    depth = 0\n    try:\n        while True:\n            ev = yield\n            if ev[0] == \"start\":\n                depth += 1\n            elif ev[0] == \"end\":\n                depth -= 1\n            dest.send(ev)\n    except StopIteration as e:\n        return e.value\n    except Exception as e:\n        if depth != 0:\n            while depth:\n                ev = yield\n                if ev[0] == \"start\":\n                    depth += 1\n                elif ev[0] == \"end\":\n                    depth -= 1\n        raise", "idx": 1304}
{"namespace": "aioxmpp.xso.model.capture_events", "completion": "    try:\n        while True:\n            value = yield\n            dest.append(value)\n            receiver.send(value)\n    except GeneratorExit:\n        dest.clear()", "idx": 1305}
{"namespace": "aioxmpp.xso.model.events_to_sax", "completion": "    for event in events:\n        if event[0] == \"start\":\n            dest.startElementNS(event[1], None, event[3])\n        elif event[0] == \"text\":\n            dest.characters(event[1])\n        elif event[0] == \"end\":\n            dest.endElementNS(event[1], None)", "idx": 1306}
{"namespace": "aioxmpp.adhoc.service.AdHocClient.get_command_info", "completion": "        disco = self.dependencies[aioxmpp.disco.DiscoClient]\n        response = await disco.query_info(\n            peer_jid,\n            node=command_name,\n        )\n        return response", "idx": 1307}
{"namespace": "aioxmpp.entitycaps.caps115.build_identities_string", "completion": "    # Process each identity and encode it into a byte string\n    identity_strings = [f\"{identity.category}/{identity.type}/{identity.lang}\" for identity in identities]\n    encoded_identities = [identity.encode('utf-8') for identity in identity_strings]\n\n    # Check for duplicate identities and sort the identities\n    unique_identities = sorted(set(encoded_identities))\n\n    # Join the unique identities into a single byte string separated by '<'\n    identities_string = b'<'.join(unique_identities)\n\n    return identities_string", "idx": 1308}
{"namespace": "aioxmpp.entitycaps.caps115.build_features_string", "completion": "    features = [\n        escape(feature).encode(\"utf-8\")\n        for feature in features\n    ]\n\n    if len(set(features)) != len(features):\n        raise ValueError(\"duplicate feature\")\n\n    features.sort()\n    features.append(b\"\")\n    return b\"<\".join(features)", "idx": 1309}
{"namespace": "aioxmpp.entitycaps.caps115.build_forms_string", "completion": "    forms = [\n        b\"/\".join([\n            escape(form.type_).encode(\"utf-8\"),\n            escape(form.label or \"\").encode(\"utf-8\"),\n            escape(form.var or \"\").encode(\"utf-8\"),\n            b\"\".join(\n                b\"<\".join(\n                    escape(field.value).encode(\"utf-8\")\n                    for field in form.fields\n                )\n            ),\n        ])\n        for form in forms\n    ]\n\n    if len(set(forms)) != len(forms):\n        raise ValueError(\"duplicate form\")\n\n    forms.sort()\n    forms.append(b\"\")\n    return b\"<\".join(forms)", "idx": 1310}
{"namespace": "aioxmpp.entitycaps.caps115.Key.path", "completion": "        quoted_node = urllib.parse.quote(self.node)\n        return pathlib.Path(\"hashes\") / self.algo / quoted_node", "idx": 1311}
{"namespace": "aioxmpp.entitycaps.caps390._process_features", "completion": "    features_string = ''.join(features).encode('utf-8')\n    return features_string", "idx": 1312}
{"namespace": "aioxmpp.entitycaps.caps390._process_identities", "completion": "    parts = [\n        _process_identity(identity)\n        for identity in identities\n    ]\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1c\"", "idx": 1313}
{"namespace": "aioxmpp.entitycaps.caps390._process_extensions", "completion": "    parts = [\n        _process_form(ext)\n        for ext in exts\n    ]\n    parts.sort()\n    return b\"\".join(parts) + b\"\\x1d\"", "idx": 1314}
{"namespace": "aioxmpp.entitycaps.caps390._calculate_hash", "completion": "    if algo == \"sha1\":\n        return aioxmpp.hashes.hash_sha1(hash_input)\n    elif algo == \"sha256\":\n        return aioxmpp.hashes.hash_sha256(hash_input)\n    elif algo == \"sha512\":\n        return aioxmpp.hashes.hash_sha512(hash_input)\n    else:\n        raise ValueError(\"Unsupported hash algorithm: {}\".format(algo))", "idx": 1315}
{"namespace": "sumy.evaluation.rouge._len_lcs", "completion": "    m = len(x)\n    n = len(y)\n    table = [[0] * (n + 1) for _ in range(m + 1)]\n\n    for i in range(m + 1):\n        for j in range(n + 1):\n            if i == 0 or j == 0:\n                table[i][j] = 0\n            elif x[i - 1] == y[j - 1]:\n                table[i][j] = table[i - 1][j - 1] + 1\n            else:\n                table[i][j] = max(table[i - 1][j], table[i][j - 1])\n\n    return table[m][n]", "idx": 1316}
{"namespace": "sumy.evaluation.rouge._recon_lcs", "completion": "    def _recon_lcs_helper(table, x, y, i, j):\n        if i == 0 or j == 0:\n            return []\n        elif x[i - 1] == y[j - 1]:\n            return _recon_lcs_helper(table, x, y, i - 1, j - 1) + [x[i - 1]]\n        else:\n            if table[i - 1, j] > table[i, j - 1]:\n                return _recon_lcs_helper(table, x, y, i - 1, j)\n            else:\n                return _recon_lcs_helper(table, x, y, i, j - 1)\n\n    table = _lcs(x, y)\n    n, m = _get_index_of_lcs(x, y)\n    lcs = _recon_lcs_helper(table, x, y, n, m)\n    return lcs", "idx": 1317}
{"namespace": "sumy.evaluation.rouge._union_lcs", "completion": "    reference_words = reference_sentence.words\n    combined_lcs = set()\n    for sentence in evaluated_sentences:\n        evaluated_words = sentence.words\n        lcs = _recon_lcs(evaluated_words, reference_words)\n        combined_lcs.update(lcs)\n    return len(combined_lcs) / len(reference_words)", "idx": 1318}
{"namespace": "sumy.parsers.html.HtmlParser.from_file", "completion": "        with open(file_path, 'r', encoding='utf-8') as file:\n            file_contents = file.read()\n        return cls(file_contents, tokenizer, url)", "idx": 1319}
{"namespace": "sumy.parsers.plaintext.PlaintextParser.document", "completion": "        paragraphs = []\n        current_heading = None\n        current_paragraph = None\n\n        for line in self._text.split('\\n'):\n            line = line.strip()\n            if line.isupper():\n                if current_paragraph:\n                    paragraphs.append(current_paragraph)\n                current_heading = line\n                current_paragraph = Paragraph(heading=current_heading, sentences=[])\n            elif line:\n                sentences = self.tokenizer.tokenize_sentences(line)\n                for sentence in sentences:\n                    current_paragraph.sentences.append(Sentence(text=sentence))\n        if current_paragraph:\n            paragraphs.append(current_paragraph)\n\n        return ObjectDocumentModel(paragraphs=paragraphs)", "idx": 1320}
{"namespace": "sumy.nlp.tokenizers.Tokenizer.to_sentences", "completion": "        # Update abbreviations based on language\n        if self._language in self.LANGUAGE_EXTRA_ABREVS:\n            for abrev in self.LANGUAGE_EXTRA_ABREVS[self._language]:\n                nltk.data.load('tokenizers/punkt/{0}.pickle'.format(self._language))\n                self._sentence_tokenizer._params.abbrev_types.add(abrev)\n\n        # Tokenize the paragraph into sentences\n        sentences = self._sentence_tokenizer.tokenize(paragraph)\n\n        return tuple(sentences)", "idx": 1321}
{"namespace": "sumy.nlp.stemmers.null_stemmer", "completion": "    if isinstance(object, str):\n        return object.lower()\n    else:\n        return str(object).lower()", "idx": 1322}
{"namespace": "rows.fields.BinaryField.serialize", "completion": "        \"\"\"\n        Serialize a binary value into a string representation. If the value is not None, it checks if the value is of type binary. If it is, it encodes the binary value using base64 encoding and decodes it by ascii. If there is an error during encoding, it returns the original binary value. If the value is not of type binary, execute error processing. If the value is None, it returns an empty string.\n        Input-Output Arguments\n        :param cls: BinaryField. The class itself.\n        :param value: Any. The binary value to be serialized.\n        :param *args: Any. Additional positional arguments.\n        :param **kwargs: Any. Additional keyword arguments.\n        :return: str. The serialized string representation of the binary value.\n        \"\"\"\n        if value is not None:\n            if isinstance(value, six.binary_type):\n                try:\n                    return b64encode(value).decode('ascii')\n                except Exception:\n                    return value\n            else:\n                value_error(value, cls)\n        else:\n            return \"\"", "idx": 1323}
{"namespace": "rows.fields.BoolField.deserialize", "completion": "        if isinstance(value, cls.TYPE) or value is None:\n            return value\n        else:\n            value = str(value).lower()\n            if value in cls.TRUE_VALUES:\n                return True\n            elif value in cls.FALSE_VALUES:\n                return False\n            else:\n                raise ValueError(\"Value is not boolean\")", "idx": 1324}
{"namespace": "rows.fields.DateField.serialize", "completion": "        if value is None:\n            return \"\"\n        else:\n            return value.strftime(cls.OUTPUT_FORMAT)", "idx": 1325}
{"namespace": "rows.fields.DateField.deserialize", "completion": "        value = super(DateField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            value = as_string(value)\n            try:\n                date_obj = datetime.datetime.strptime(value, cls.INPUT_FORMAT)\n                return datetime.date(date_obj.year, date_obj.month, date_obj.day)\n            except ValueError:\n                value_error(value, cls)", "idx": 1326}
{"namespace": "rows.fields.TextField.deserialize", "completion": "        value = super(TextField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            return six.text_type(value)", "idx": 1327}
{"namespace": "rows.fields.EmailField.deserialize", "completion": "        value = super(EmailField, cls).deserialize(value)\n        if value is None or value == \"\":\n            return None\n        else:\n            value = as_string(value)\n            match = cls.EMAIL_REGEXP.match(value)\n            if match:\n                return match.group()\n            else:\n                raise ValueError(\"Invalid email format\")", "idx": 1328}
{"namespace": "rows.fields.JSONField.deserialize", "completion": "        value = super(JSONField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            return json.loads(value)", "idx": 1329}
{"namespace": "falcon.http_error.HTTPError.to_json", "completion": "                import json\n\n        error_dict = self.to_dict()\n\n        if handler:\n            return handler.serialize(error_dict)\n\n        # Default to using the built-in JSON library\n        import json\n        return json.dumps(error_dict).encode('utf-8')", "idx": 1330}
{"namespace": "falcon.inspect.inspect_app", "completion": "    router = app._router\n    compiled_router = CompiledRouter()\n    router.compiled = compiled_router\n\n    routes = []\n    for route in compiled_router._roots:\n        routes.append(_inspect_route(route))\n\n    static_routes = _inspect_static_routes(compiled_router._static_routes)\n    sinks = _inspect_sinks(app._sinks)\n    error_handlers = _inspect_error_handlers(app._error_handlers)\n    middleware = _inspect_middleware(app._middleware)\n\n    return AppInfo(\n        routes=routes,\n        static_routes=static_routes,\n        sinks=sinks,\n        error_handlers=error_handlers,\n        middleware=middleware\n    )", "idx": 1331}
{"namespace": "falcon.inspect.inspect_routes", "completion": "    router = app._router\n    compiled_router = CompiledRouter()\n    router.find('GET', '/dummy', compiled_router)\n    routes = []\n    for route in compiled_router._roots:\n        route_info = RouteInfo(\n            uri_template=route.uri_template,\n            resource=route.resource.__class__,\n            method=route.method,\n            middleware=route._middleware,\n            sink=route.sink\n        )\n        routes.append(route_info)\n    return routes", "idx": 1332}
{"namespace": "falcon.inspect.inspect_static_routes", "completion": "    static_routes = []\n    for route in app._router._static_routes:\n        static_routes.append(StaticRouteInfo(route.uri_template, route.resource.__class__.__name__))\n    return static_routes", "idx": 1333}
{"namespace": "falcon.inspect.inspect_sinks", "completion": "    sinks = []\n    for sink in app._sinks:\n        sink_info = SinkInfo(sink.__name__, inspect.getsourcefile(sink), inspect.getsourcelines(sink))\n        sinks.append(sink_info)\n    return sinks", "idx": 1334}
{"namespace": "falcon.inspect.inspect_error_handlers", "completion": "    error_handlers = []\n    for ex, handler in app._error_handlers.items():\n        error_handlers.append(ErrorHandlerInfo(ex, handler.__name__))\n    return error_handlers", "idx": 1335}
{"namespace": "falcon.inspect.inspect_middleware", "completion": "    middleware = app._middleware\n    middleware_tree = _build_middleware_tree(middleware)\n    middleware_classes = _get_middleware_classes(middleware)\n    return MiddlewareInfo(middleware_tree, middleware_classes)", "idx": 1336}
{"namespace": "falcon.inspect.InspectVisitor.process", "completion": "\n        method_name = 'visit_' + instance.__visit_name__\n        visitor_method = getattr(self, method_name, None)\n        if visitor_method:\n            return visitor_method(instance)\n        else:\n            raise RuntimeError(f'No visit method found for {_Traversable.__visit_name__}')", "idx": 1337}
{"namespace": "falcon.request.Request.forwarded", "completion": "        if self._cached_forwarded is None:\n            forwarded_header = self.env.get('HTTP_FORWARDED')\n            if forwarded_header:\n                self._cached_forwarded = Forwarded.parse(forwarded_header)\n            else:\n                self._cached_forwarded = None\n\n        return self._cached_forwarded", "idx": 1338}
{"namespace": "falcon.request.Request.client_accepts_msgpack", "completion": "        accept_header = self.get_header('Accept')\n        if accept_header:\n            accept_media_types = mimeparse.quality_parsed(accept_header)\n            for media_type, _ in accept_media_types:\n                if media_type.lower() == 'application/x-msgpack' or media_type.lower() == 'application/msgpack':\n                    return True\n        return False", "idx": 1339}
{"namespace": "falcon.request.Request.content_length", "completion": "        try:\n            content_length = int(self.env.get('CONTENT_LENGTH', 0))\n            if content_length < 0:\n                raise ValueError(\"Invalid value for 'CONTENT_LENGTH'\")\n            return content_length\n        except (ValueError, TypeError):\n            return None", "idx": 1340}
{"namespace": "falcon.request.Request.bounded_stream", "completion": "\n        if self._bounded_stream is None:\n            self._bounded_stream = BoundedStream(self.stream, self.content_length)\n\n        return self._bounded_stream", "idx": 1341}
{"namespace": "falcon.request.Request.uri", "completion": "        if self._cached_uri is None:\n            self._cached_uri = f\"{self.scheme}://{self.netloc}{self.relative_uri}\"\n\n        return self._cached_uri", "idx": 1342}
{"namespace": "falcon.request.Request.forwarded_uri", "completion": "        if self._cached_forwarded_uri is None:\n            forwarded_scheme = self.forwarded_scheme or self.scheme\n            forwarded_host = self.forwarded_host or self.host\n            value = f\"{forwarded_scheme}://{forwarded_host}{self.relative_uri}\"\n            self._cached_forwarded_uri = value\n\n        return self._cached_forwarded_uri", "idx": 1343}
{"namespace": "falcon.request.Request.relative_uri", "completion": "        if self._cached_relative_uri is None:\n            if self.query_string:\n                self._cached_relative_uri = self.app + self.path + '?' + self.query_string\n            else:\n                self._cached_relative_uri = self.app + self.path\n\n        return self._cached_relative_uri", "idx": 1344}
{"namespace": "falcon.request.Request.prefix", "completion": "        if self._cached_prefix is None:\n            # PERF: For small numbers of items, '+' is faster\n            # than ''.join(...). Concatenation is also generally\n            # faster than formatting.\n            value = self.scheme + '://' + self.netloc + self.app\n\n            self._cached_prefix = value\n\n        return self._cached_prefix", "idx": 1345}
{"namespace": "falcon.request.Request.forwarded_prefix", "completion": "        if self._cached_forwarded_prefix is None:\n            self._cached_forwarded_prefix = self.forwarded_scheme + '://' + self.forwarded_host + self.app\n\n        return self._cached_forwarded_prefix", "idx": 1346}
{"namespace": "falcon.request.Request.host", "completion": "        try:\n            return self.env['HTTP_HOST']\n        except KeyError:\n            return self.env['SERVER_NAME']", "idx": 1347}
{"namespace": "falcon.request.Request.subdomain", "completion": "        parts = self.host.partition('.')\n        if len(parts) > 2:\n            return parts[0]\n        else:\n            return None", "idx": 1348}
{"namespace": "falcon.request.Request.headers", "completion": "        if self._cached_headers is None:\n            self._cached_headers = {\n                key[5:].replace('_', '-').title(): value\n                for key, value in self.env.items()\n                if key in WSGI_CONTENT_HEADERS or key.startswith('HTTP_')\n            }\n\n        return self._cached_headers", "idx": 1349}
{"namespace": "falcon.request.Request.remote_addr", "completion": "        return self.env.get('REMOTE_ADDR', '127.0.0.1')", "idx": 1350}
{"namespace": "falcon.request.Request.client_accepts", "completion": "\n        # Check if the Accept header is present\n        accept_header = self.accept\n        if accept_header == '*/*':\n            return True\n\n        # Parse the Accept header and compare with the specified media type\n        parsed_accept = mimeparse.parse_mime_type(accept_header)\n        parsed_media_type = mimeparse.parse_media_type(media_type)\n\n        if parsed_accept == parsed_media_type:\n            return True\n        else:\n            return False", "idx": 1351}
{"namespace": "falcon.request.Request.client_prefers", "completion": "\n        # Get the Accept header from the request\n        accept_header = self.accept\n\n        # Parse the Accept header to determine the client's preferred media type\n        preferred_type = mimeparse.best_match(media_types, accept_header)\n\n        return preferred_type", "idx": 1352}
{"namespace": "falcon.request.Request.get_header", "completion": "        header_name = name.upper().replace('-', '_')\n        try:\n            return self.env['HTTP_' + header_name]\n        except KeyError:\n            if required:\n                raise errors.HTTPBadRequest(\n                    title='Missing Header',\n                    description=f\"The '{name}' header is required but was not found.\"\n                )\n            else:\n                return default", "idx": 1353}
{"namespace": "falcon.request.Request.get_cookie_values", "completion": "        if self._cookies is None:\n            header_value = self.get_header('Cookie')\n            if header_value:\n                self._cookies = helpers.parse_cookie_header(header_value)\n            else:\n                self._cookies = {}\n\n        return self._cookies.get(name)", "idx": 1354}
{"namespace": "falcon.response.Response.unset_cookie", "completion": "        if self._cookies is not None and name in self._cookies:\n            self._cookies[name]['max-age'] = 0\n            self._cookies[name]['expires'] = 'Thu, 01 Jan 1970 00:00:00 GMT'\n            self._cookies[name]['path'] = path if path else self._cookies[name]['path']\n            self._cookies[name]['domain'] = domain if domain else self._cookies[name]['domain']", "idx": 1355}
{"namespace": "falcon.response.Response.get_header", "completion": "    name = name.lower()\n    if name == 'set-cookie':\n        raise HeaderNotSupported('Set-Cookie header is not supported')\n\n    return self._headers.get(name, default)", "idx": 1356}
{"namespace": "falcon.response.Response.set_header", "completion": "\n        if not is_ascii_encodable(name):\n            raise ValueError('Header name is not ascii encodable')\n\n        if not is_ascii_encodable(value):\n            raise ValueError('Header value is not ascii encodable')\n\n        self._headers[name.lower()] = value", "idx": 1357}
{"namespace": "falcon.response.Response.delete_header", "completion": "        # Normalize header name by lowercasing it\n        name = name.lower()\n\n        # Check if the header exists and delete it\n        if name in self._headers:\n            del self._headers[name]", "idx": 1358}
{"namespace": "falcon.cmd.inspect_app.route_main", "completion": "    print('The \"falcon-print-routes\" command is deprecated.')\n    print('Please use \"falcon-inspect-app\"')\n    parser = make_parser()\n    args = parser.parse_args()\n    app = load_app(parser, args)\n    main(app, args)", "idx": 1359}
{"namespace": "falcon.util.uri.decode", "completion": "    if _cy_decode is not None:\n        return _cy_decode(encoded_uri, unquote_plus)\n\n    # PERF(kgriffs): This is faster than using a lambda\n    def decode_char(match):\n        return chr(int(match.group(1), 16))\n\n    # PERF(kgriffs): This is faster than using a lambda\n    def decode_char_plus(match):\n        return ' ' if match.group(0) == '+' else chr(int(match.group(1), 16))\n\n    if unquote_plus:\n        return _join_tokens(\n            re.sub(r'%([0-9A-Fa-f]{2})', decode_char, encoded_uri).split('%'),\n        )\n    else:\n        return _join_tokens(\n            re.sub(r'%([0-9A-Fa-f]{2})', decode_char_plus, encoded_uri).split('%'),\n        )", "idx": 1360}
{"namespace": "falcon.util.structures.ETag.dumps", "completion": "        if self.is_weak:\n            return 'W/\"{}\"'.format(self)\n        else:\n            return str(self)", "idx": 1361}
{"namespace": "falcon.util.structures.ETag.loads", "completion": "        is_weak = False\n        if etag_str.startswith('W/'):\n            is_weak = True\n            etag_str = etag_str[2:]\n\n        # Remove quotes\n        etag_str = etag_str.strip('\"')\n\n        etag_instance = ETag(etag_str)\n        etag_instance.is_weak = is_weak\n\n        return etag_instance", "idx": 1362}
{"namespace": "falcon.util.misc.secure_filename", "completion": "\n    # Normalize the filename to the Unicode NKFD form\n    filename = unicodedata.normalize('NFKD', filename).encode('ascii', 'ignore').decode('utf-8')\n\n    # Replace non-ASCII characters with an underscore\n    filename = _UNSAFE_CHARS.sub('_', filename)\n\n    # If the filename starts with a period, replace the first period with an underscore\n    if filename.startswith('.'):\n        filename = '_' + filename[1:]\n\n    return filename", "idx": 1363}
{"namespace": "falcon.asgi.reader.BufferedReader.peek", "completion": "        if size < -1:\n            raise ValueError('size must be a non-negative integer or -1')\n\n        if size == 0:\n            return b''\n\n        if size == -1:\n            size = self._buffer_len\n\n        if size <= self._buffer_len - self._buffer_pos:\n            return self._buffer[self._buffer_pos : self._buffer_pos + size]\n\n        result = [self._buffer[self._buffer_pos : self._buffer_len]]\n        remaining = size - (self._buffer_len - self._buffer_pos)\n\n        async for chunk in self._source:\n            chunk_len = len(chunk)\n            if remaining < chunk_len:\n                result.append(chunk[:remaining])\n                self._prepend_buffer(chunk[remaining:])\n                break\n\n            result.append(chunk)\n            remaining -= chunk_len\n            if remaining == 0:  # pragma: no py39,py310 cover\n                break\n\n        return b''.join(result)", "idx": 1364}
{"namespace": "falcon.asgi.reader.BufferedReader.read_until", "completion": "            if size == 0:\n                return b''\n\n            if size < 0:\n                return await self._read_from(self._iter_delimited(delimiter), size)\n\n            return await self._read_from(self._iter_delimited(delimiter), size)", "idx": 1365}
{"namespace": "falcon.routing.converters.IntConverter.convert", "completion": "        if not isinstance(value, str):\n            return None\n\n        # Remove leading and trailing whitespace\n        value = value.strip()\n\n        # Check if the value contains any whitespace characters\n        if ' ' in value:\n            return None\n\n        # Check if the number of digits matches the specified number of digits\n        if self._num_digits is not None and len(value) != self._num_digits:\n            return None\n\n        try:\n            # Convert the value to an integer\n            int_value = int(value)\n        except ValueError:\n            return None\n\n        # Check if the value is within the specified minimum and maximum range\n        if (self._min is not None and int_value < self._min) or (self._max is not None and int_value > self._max):\n            return None\n\n        return int_value", "idx": 1366}
{"namespace": "falcon.routing.converters.DateTimeConverter.convert", "completion": "        try:\n            return strptime(value, self._format_string)\n        except ValueError:\n            return None", "idx": 1367}
{"namespace": "falcon.routing.util.map_http_methods", "completion": "    methods = {}\n\n    for method in constants.HTTP_METHODS:\n        method_name = 'on_' + method.lower()\n\n        if suffix:\n            method_name += '_' + suffix\n\n        responder = getattr(resource, method_name, None)\n\n        if responder is not None:\n            methods[method] = responder\n\n    return methods", "idx": 1368}
{"namespace": "falcon.routing.static._BoundedFile.read", "completion": "        if size < 0 or size > self.remaining:\n            size = self.remaining\n        data = self.fh.read(size)\n        self.remaining -= len(data)\n        return data", "idx": 1369}
{"namespace": "authlib.oauth2.rfc6749.util.list_to_scope", "completion": "    if scope is None:\n        return None\n    elif isinstance(scope, (set, tuple, list)):\n        return ' '.join(map(to_unicode, scope))\n    else:\n        raise ValueError(\"Invalid scope type, must be set, tuple, or list\")", "idx": 1370}
{"namespace": "authlib.oauth2.rfc6749.util.extract_basic_authorization", "completion": "    auth_header = headers.get('Authorization')\n    if not auth_header or ' ' not in auth_header:\n        return None, None\n\n    auth_type, auth_token = auth_header.split(' ', 1)\n    if auth_type.lower() != 'basic':\n        return None, None\n\n    try:\n        auth_token = base64.b64decode(auth_token).decode('utf-8')\n    except (TypeError, binascii.Error):\n        return None, None\n\n    if ':' not in auth_token:\n        return auth_token, None\n\n    username, password = auth_token.split(':', 1)\n    return username, password", "idx": 1371}
{"namespace": "authlib.oauth2.rfc6749.parameters.prepare_grant_uri", "completion": "    params = {\n        'client_id': client_id,\n        'response_type': response_type,\n    }\n\n    if redirect_uri:\n        params['redirect_uri'] = redirect_uri\n\n    if scope:\n        params['scope'] = list_to_scope(scope)\n\n    if state:\n        params['state'] = state\n\n    params.update(kwargs)\n\n    parsed_uri = urlparse(uri)\n    query = add_params_to_qs(parsed_uri.query, params)\n    return to_unicode(parsed_uri._replace(query=query).geturl())", "idx": 1372}
{"namespace": "authlib.oauth2.rfc6749.parameters.parse_authorization_code_response", "completion": "    parsed_uri = urlparse.urlparse(uri)\n    query_params = urlparse.parse_qs(parsed_uri.query)\n\n    if 'code' not in query_params:\n        raise MissingCodeException()\n\n    if state and 'state' in query_params and query_params['state'][0] != state:\n        raise MismatchingStateException()\n\n    return {\n        'code': query_params['code'][0],\n        'state': query_params.get('state', [None])[0]\n    }", "idx": 1373}
{"namespace": "authlib.oauth2.rfc6749.parameters.parse_implicit_response", "completion": "    query = urlparse.urlparse(uri).fragment\n    params = dict(urlparse.parse_qsl(query))\n\n    if 'access_token' not in params:\n        raise MissingCodeException()\n\n    if 'token_type' not in params:\n        raise MissingCodeException()\n\n    if 'expires_in' not in params:\n        raise MissingCodeException()\n\n    if 'state' not in params and state:\n        raise MismatchingStateException()\n\n    return params", "idx": 1374}
{"namespace": "authlib.common.encoding.json_b64encode", "completion": "    if isinstance(text, dict):\n        text = json_dumps(text)\n    else:\n        text = to_unicode(text)\n    encoded_text = urlsafe_b64encode(to_bytes(text))\n    return to_native(encoded_text)", "idx": 1375}
{"namespace": "authlib.jose.util.extract_header", "completion": "    header_data = urlsafe_b64decode(to_unicode(header_segment))\n    try:\n        header = json_loads(header_data)\n    except DecodeError:\n        raise error_cls('Invalid header data')\n    if not isinstance(header, dict):\n        raise error_cls('Invalid header data')\n    return header", "idx": 1376}
{"namespace": "twitter.models.TwitterModel.AsDict", "completion": "        result = {}\n        for key, value in self.__dict__.items():\n            if isinstance(value, (list, tuple, set)):\n                result[key] = [v.AsDict() if isinstance(v, TwitterModel) else v for v in value]\n            elif isinstance(value, TwitterModel):\n                result[key] = value.AsDict()\n            else:\n                result[key] = value\n        return result", "idx": 1377}
{"namespace": "twitter.models.TwitterModel.NewFromJsonDict", "completion": "        # Create a new instance of the class\n        instance = cls()\n\n        # Update the instance with the data from the JSON dictionary\n        for key, value in data.items():\n            setattr(instance, key, value)\n\n        return instance", "idx": 1378}
{"namespace": "twitter.api.Api._TweetTextWrap", "completion": "        words = status.split()\n        tweets = []\n        line = \"\"\n        for word in words:\n            if len(word) > char_lim:\n                raise ValueError(\"Word exceeds character limit\")\n            if len(line) + len(word) + 1 <= char_lim:\n                if line:\n                    line += \" \" + word\n                else:\n                    line = word\n            else:\n                tweets.append(line)\n                line = word\n        if line:\n            tweets.append(line)\n        return tweets", "idx": 1379}
{"namespace": "databases.importer.import_from_string", "completion": "    module_name, attribute_name = import_str.split(\":\")\n    try:\n        module = importlib.import_module(module_name)\n        attribute = getattr(module, attribute_name)\n        return attribute\n    except (ImportError, AttributeError) as e:\n        raise ImportFromStringError(f\"Error importing {module_name} or retrieving {attribute_name}: {e}\")", "idx": 1380}
{"namespace": "rest_framework.reverse.reverse", "completion": "    if format is not None:\n        kwargs = kwargs or {}\n        kwargs['format'] = format\n\n    url = django_reverse(viewname, args=args, kwargs=kwargs, **extra)\n\n    if request is not None:\n        url = preserve_builtin_query_params(url, request)\n\n    return url", "idx": 1381}
{"namespace": "rest_framework.serializers.Serializer.fields", "completion": "        \"\"\"\n        This function returns a dictionary of field names and their corresponding field instances. It lazily evaluates the fields to avoid import issues with modules that use ModelSerializers as fields before Django's app-loading stage has run.\n        Input-Output Arguments\n        :param: self: Serializer. An instance of the Serializer class.\n        :return: Dictionary. A dictionary of {field_name: field_instance}.\n        \"\"\"\n        # Lazily evaluate the fields to avoid import issues with modules that use ModelSerializers as fields before Django's app-loading stage has run\n        if not hasattr(self, '_fields'):\n            self._fields = self.get_fields()\n        return self._fields", "idx": 1382}
{"namespace": "rest_framework.parsers.JSONParser.parse", "completion": "        try:\n            encoding = parser_context.get('encoding', settings.DEFAULT_CHARSET)\n            decoded_stream = codecs.getreader(encoding)(stream)\n            parsed_data = self.parse_json(decoded_stream, parser_context)\n            return parsed_data\n        except ParseError:\n            raise\n        except Exception as exc:\n            raise ParseError('JSON parse error - %s' % str(exc))", "idx": 1383}
{"namespace": "rest_framework.parsers.FileUploadParser.get_filename", "completion": "        request = parser_context['request']\n        content_type = parser_context.get('content_type', '')\n        content_disposition = parser_context.get('content_disposition', '')\n\n        if 'filename' in request.GET:\n            return request.GET['filename']\n        elif content_disposition and 'filename' in content_disposition:\n            filename = content_disposition.split('filename=')[1]\n            return filename.strip('\"')\n        else:\n            return None", "idx": 1384}
{"namespace": "rest_framework.fields.is_simple_callable", "completion": "    if not callable(obj):\n        return False\n    if inspect.isbuiltin(obj):\n        raise BuiltinSignatureError(\"Built-in function signatures are not inspectable\")\n    if isinstance(obj, (types.FunctionType, types.MethodType, functools.partial)):\n        signature = inspect.signature(obj)\n        for param in signature.parameters.values():\n            if (param.default is param.empty and\n                    param.kind not in (param.VAR_POSITIONAL, param.VAR_KEYWORD)):\n                return False\n        return True\n    return False", "idx": 1385}
{"namespace": "rest_framework.fields.Field.bind", "completion": "        self.field_name = field_name\n        self.parent = parent", "idx": 1386}
{"namespace": "rest_framework.fields.Field.run_validation", "completion": "        if data is empty:\n            if getattr(self.root, 'partial', False):\n                raise SkipField()\n            return self.get_default()\n\n        if html.is_html_input(data):\n            data = html.parse_html_list(data)\n\n        if isinstance(data, str) and html.is_html_input(data):\n            self.fail('html')\n\n        if self.source == '*':\n            return data\n\n        if self.source_attrs:\n            try:\n                return self.get_value(data)\n            except (KeyError, AttributeError):\n                self.fail('required')\n\n        return data", "idx": 1387}
{"namespace": "rest_framework.fields.Field.root", "completion": "        root = self\n        while root.parent is not None:\n            root = root.parent\n        return root", "idx": 1388}
{"namespace": "rest_framework.fields.CharField.run_validation", "completion": "        if data == '' and not self.allow_blank:\n            self.fail('blank')\n\n        # Check if the data consists only of whitespace characters\n        if data.strip() == '' and not self.allow_blank:\n            self.fail('blank')\n\n        # If the data is not empty, call the parent class's run_validation() method\n        return super().run_validation(data)", "idx": 1389}
{"namespace": "rest_framework.fields.CharField.to_internal_value", "completion": "        if isinstance(data, (str, int, float)):\n            if isinstance(data, bool) or isinstance(data, list):\n                self.fail('invalid')\n            return str(data).strip() if self.trim_whitespace else str(data)\n        self.fail('invalid', input=data)", "idx": 1390}
{"namespace": "rest_framework.fields.DecimalField.to_internal_value", "completion": "        if isinstance(data, str) and len(data) > self.MAX_STRING_LENGTH:\n            self.fail('max_string_length')\n\n        try:\n            value = decimal.Decimal(data)\n        except decimal.InvalidOperation:\n            self.fail('invalid')\n\n        return value", "idx": 1391}
{"namespace": "rest_framework.fields.DateTimeField.to_representation", "completion": "        if value in (None, ''):\n            return None\n\n        output_format = getattr(self, 'format', api_settings.DATETIME_FORMAT)\n        if output_format is None or isinstance(value, str):\n            return value\n\n        value = self.enforce_timezone(value)\n        return value.strftime(output_format)", "idx": 1392}
{"namespace": "rest_framework.fields.ChoiceField.iter_options", "completion": "        class StartOptionGroup:\n            start_option_group = True\n            end_option_group = False\n\n            def __init__(self, label):\n                self.label = label\n\n        class EndOptionGroup:\n            start_option_group = False\n            end_option_group = True\n\n        class Option:\n            start_option_group = False\n            end_option_group = False\n\n            def __init__(self, value, display_text, disabled=False):\n                self.value = value\n                self.display_text = display_text\n                self.disabled = disabled\n\n        count = 0\n\n        for key, value in self.choices.items():\n            if self.html_cutoff and count >= self.html_cutoff:\n                break\n\n            if isinstance(value, dict):\n                yield StartOptionGroup(label=key)\n                for sub_key, sub_value in value.items():\n                    if self.html_cutoff and count >= self.html_cutoff:\n                        break\n                    yield Option(value=sub_key, display_text=sub_value)\n                    count += 1\n                yield EndOptionGroup()\n            else:\n                yield Option(value=key, display_text=value)\n                count += 1\n\n        if self.html_cutoff and count >= self.html_cutoff and self.html_cutoff_text:\n            cutoff_text = self.html_cutoff_text.format(count=self.html_cutoff)\n            yield Option(value='n/a', display_text=cutoff_text, disabled=True)", "idx": 1393}
{"namespace": "rest_framework.fields.MultipleChoiceField.get_value", "completion": "        if self.field_name not in dictionary:\n            if getattr(self.root, 'partial', False):\n                return self.default_empty_html\n            return empty\n\n        if html.is_html_input(dictionary):\n            return dictionary.getlist(self.field_name, self.default_empty_html)\n\n        return dictionary[self.field_name]", "idx": 1394}
{"namespace": "rest_framework.exceptions._get_error_details", "completion": "    if isinstance(data, dict):\n        return {key: _get_error_details(value, default_code) for key, value in data.items()}\n    elif isinstance(data, (list, tuple)):\n        return [_get_error_details(item, default_code) for item in data]\n    elif isinstance(data, str):\n        return ErrorDetail(force_str(data), code=default_code) if default_code else ErrorDetail(force_str(data))\n    else:\n        return data", "idx": 1395}
{"namespace": "rest_framework.exceptions.server_error", "completion": "    data = {'error': 'A server error occurred.'}\n    return JsonResponse(data, status=500)", "idx": 1396}
{"namespace": "rest_framework.exceptions.bad_request", "completion": "    data = {\n        'error': 'Bad Request (400)'\n    }\n    return JsonResponse(data, status=status.HTTP_400_BAD_REQUEST)", "idx": 1397}
{"namespace": "rest_framework.relations.RelatedField.iter_options", "completion": "        queryset = self.get_queryset()\n        if queryset is not None:\n            for item in queryset:\n                yield self.to_representation(item), self.display_value(item)", "idx": 1398}
{"namespace": "rest_framework.relations.PrimaryKeyRelatedField.to_internal_value", "completion": "        try:\n            data = smart_str(data)\n            return self.queryset.get(**{self.pk_field: data})\n        except (ValueError, ObjectDoesNotExist):\n            self.fail('does_not_exist', pk_value=data)\n        except TypeError:\n            self.fail('incorrect_type', data_type=type(data).__name__)", "idx": 1399}
{"namespace": "rest_framework.relations.PrimaryKeyRelatedField.to_representation", "completion": "        if self.pk_field is not None:\n            return self.pk_field.to_representation(value)\n        return value", "idx": 1400}
{"namespace": "rest_framework.relations.SlugRelatedField.to_internal_value", "completion": "        queryset = self.get_queryset()\n        try:\n            return queryset.get(**{self.slug_field: data})\n        except ObjectDoesNotExist:\n            self.fail('does_not_exist', slug_name=self.slug_field, value=data)\n        except (TypeError, ValueError):\n            self.fail('invalid')", "idx": 1401}
{"namespace": "rest_framework.templatetags.rest_framework.add_query_param", "completion": "    full_path = request.get_full_path()\n    uri = iri_to_uri(full_path)\n    new_uri = re.sub(r'([&?])' + key + '=[^&]*', '', uri)\n    new_uri += ('&' if '?' in new_uri else '?') + key + '=' + smart_urlquote(val)\n    return new_uri", "idx": 1402}
{"namespace": "rest_framework.utils.mediatypes._MediaType.match", "completion": "        if self.main_type != '*' and other.main_type != '*' and self.main_type != other.main_type:\n            return False\n\n        if self.sub_type != '*' and other.sub_type != '*' and self.sub_type != other.sub_type:\n            return False\n\n        if self.params != other.params:\n            return False\n\n        return True", "idx": 1403}
{"namespace": "rest_framework.utils.mediatypes._MediaType.precedence", "completion": "        if self.full_type.count('/') == 1:\n            if not self.params:\n                return 2\n            else:\n                return 3\n        elif self.full_type.count('/') == 0:\n            return 0\n        else:\n            return 1", "idx": 1404}
{"namespace": "rest_framework.utils.mediatypes._MediaType.__str__", "completion": "        params_str = '; '.join([f\"{key}={value}\" for key, value in self.params.items()])\n        return f\"{self.full_type}; {params_str}\" if params_str else self.full_type", "idx": 1405}
{"namespace": "asyncpg._testbase.TestCase.assertLoopErrorHandlerCalled", "completion": "        def error_handler(loop, context):\n            self.__unhandled_exceptions.append(context)\n\n        self.loop.set_exception_handler(error_handler)\n\n        try:\n            yield\n        finally:\n            self.loop.set_exception_handler(self.loop_exception_handler)\n\n        for i, context in enumerate(self.__unhandled_exceptions):\n            if re.search(msg_re, str(context['message'])):\n                return\n\n        raise AssertionError(f\"No matching message found for regular expression: {msg_re}\")", "idx": 1406}
{"namespace": "csvs_to_sqlite.utils.refactor_dataframes", "completion": "\n    for df in dataframes:\n        for column, (table_name, value_column) in foreign_keys.items():\n            lookup_table = LookupTable(conn, table_name, value_column, index_fts)\n            df[column] = df[column].apply(lambda x: lookup_table.id_for_value(x))\n    return dataframes", "idx": 1407}
{"namespace": "sqlitedict.SqliteDict.iteritems", "completion": "        GET_ITEMS = 'SELECT key, value FROM \"%s\" ORDER BY rowid' % self.tablename\n        for row in self.conn.select(GET_ITEMS):\n            yield (self.decode_key(row[0]), self.decode(row[1]))", "idx": 1408}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_checker", "completion": "        return shutil.which(\"brew\") is not None and self._darwin_get_brew_formula_location_prefix(self.homebrew_formula_name, installed=True) is not None", "idx": 1409}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_pkg_config_location", "completion": "        openssl_prefix = self._darwin_get_brew_formula_location_prefix(self.homebrew_formula_name, installed=True)\n        if openssl_prefix:\n            return os.path.join(openssl_prefix, \"lib\", \"pkgconfig\")\n        else:\n            return \"\"", "idx": 1410}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_installer", "completion": "        if self.darwin_checker():\n            info(f\"OpenSSL is already installed on macOS\")\n        else:\n            if shutil.which(\"brew\") is None:\n                error(\"Homebrew is not installed on macOS, please install it first\")\n                return\n\n            info(f\"Installing OpenSSL using Homebrew on macOS\")\n            subprocess.check_call([\"brew\", \"install\", self.homebrew_formula_name])", "idx": 1411}
{"namespace": "pythonforandroid.prerequisites.AutoconfPrerequisite.darwin_installer", "completion": "        info(\"Installing Autoconf ...\")\n        subprocess.check_output([\"brew\", \"install\", \"autoconf\"])", "idx": 1412}
{"namespace": "pythonforandroid.prerequisites.AutomakePrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"automake\", installed=True)\n            is not None\n        )", "idx": 1413}
{"namespace": "pythonforandroid.prerequisites.AutomakePrerequisite.darwin_installer", "completion": "        info(\"Installing Automake ...\")\n        subprocess.check_output([\"brew\", \"install\", \"automake\"])", "idx": 1414}
{"namespace": "pythonforandroid.prerequisites.LibtoolPrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"libtool\", installed=True)\n            is not None\n        )", "idx": 1415}
{"namespace": "pythonforandroid.prerequisites.LibtoolPrerequisite.darwin_installer", "completion": "        info(\"Installing Libtool ...\")\n        subprocess.check_output([\"brew\", \"install\", \"libtool\"])", "idx": 1416}
{"namespace": "pythonforandroid.prerequisites.PkgConfigPrerequisite.darwin_checker", "completion": "        \"\"\"\n        Check if the \"pkg-config\" formula is installed on a macOS system using Homebrew.\n        Input-Output Arguments\n        :param self: PkgConfigPrerequisite. An instance of the PkgConfigPrerequisite class.\n        :return: bool. True if the \"pkg-config\" formula is installed, False otherwise.\n        \"\"\"\n        return (\n            self._darwin_get_brew_formula_location_prefix(\"pkg-config\", installed=True)\n            is not None\n        )", "idx": 1417}
{"namespace": "pythonforandroid.prerequisites.PkgConfigPrerequisite.darwin_installer", "completion": "        info(\"Installing Pkg-Config ...\")\n        subprocess.check_output([\"brew\", \"install\", \"pkg-config\"])", "idx": 1418}
{"namespace": "pythonforandroid.prerequisites.CmakePrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"cmake\", installed=True)\n            is not None\n        )", "idx": 1419}
{"namespace": "pythonforandroid.prerequisites.CmakePrerequisite.darwin_installer", "completion": "        info(\"Installing CMake ...\")\n        subprocess.check_output([\"brew\", \"install\", \"cmake\"])", "idx": 1420}
{"namespace": "pythonforandroid.prerequisites.get_required_prerequisites", "completion": "    prerequisites = [\n        HomebrewPrerequisite(),\n        JDKPrerequisite(),\n        OpenSSLPrerequisite(),\n        AutoconfPrerequisite(),\n        AutomakePrerequisite(),\n        LibtoolPrerequisite(),\n        PkgConfigPrerequisite(),\n        CmakePrerequisite()\n    ]\n\n    if platform == \"linux\":\n        prerequisites = [prerequisite for prerequisite in prerequisites if prerequisite.mandatory[platform]]\n    elif platform == \"darwin\":\n        prerequisites = [prerequisite for prerequisite in prerequisites if prerequisite.mandatory[platform]]\n\n    return prerequisites", "idx": 1421}
{"namespace": "pythonforandroid.pythonpackage.parse_as_folder_reference", "completion": "    if dep.startswith(\"file://\"):\n        return urlparse(dep).path\n    elif os.path.isdir(dep):\n        return dep\n    else:\n        return None", "idx": 1422}
{"namespace": "pythonforandroid.pythonpackage.get_package_name", "completion": "\n    if use_cache and dependency in package_name_cache:\n        return package_name_cache[dependency]\n\n    package_name = _extract_info_from_package(dependency, extract_type=\"name\")\n\n    package_name_cache[dependency] = package_name\n\n    return package_name", "idx": 1423}
{"namespace": "pythonforandroid.recommendations.read_ndk_version", "completion": "\n    source_properties_path = join(ndk_dir, 'source.properties')\n    try:\n        with open(source_properties_path, 'r') as source_properties_file:\n            for line in source_properties_file:\n                if line.startswith('Pkg.Revision'):\n                    version_string = line.split('=')[1].strip()\n                    return LooseVersion(version_string)\n    except FileNotFoundError:\n        warning(UNKNOWN_NDK_MESSAGE)\n    except Exception as e:\n        warning(PARSE_ERROR_NDK_MESSAGE)\n        warning(str(e))\n    return None", "idx": 1424}
{"namespace": "pythonforandroid.recommendations.check_target_api", "completion": "    if api < MIN_TARGET_API:\n        warning(OLD_API_MESSAGE)\n    if arch == 'armeabi' and api > ARMEABI_MAX_TARGET_API:\n        warning(UNSUPPORTED_NDK_API_FOR_ARMEABI_MESSAGE.format(req_ndk_api=api, max_ndk_api=ARMEABI_MAX_TARGET_API))", "idx": 1425}
{"namespace": "pythonforandroid.recommendations.check_ndk_api", "completion": "\n    if ndk_api > android_api:\n        raise BuildInterruptingException(\n            TARGET_NDK_API_GREATER_THAN_TARGET_API_MESSAGE.format(\n                ndk_api=ndk_api, android_api=android_api\n            ),\n            instructions='Please make sure the NDK API version is compatible with the target Android API version.'\n        )\n    elif ndk_api < MIN_NDK_API:\n        warning(OLD_NDK_API_MESSAGE)", "idx": 1426}
{"namespace": "pythonforandroid.androidndk.AndroidNDK.llvm_prebuilt_dir", "completion": "        return os.path.join(self.ndk_dir, \"toolchains\", \"llvm\", \"prebuilt\", self.host_tag)", "idx": 1427}
{"namespace": "pythonforandroid.build.Context.setup_dirs", "completion": "\n        self.storage_dir = abspath(expanduser(storage_dir))\n        self.build_dir = join(self.storage_dir, 'build')\n        self.dist_dir = join(self.storage_dir, 'dist')\n\n        ensure_dir(self.storage_dir)\n        ensure_dir(self.build_dir)\n        ensure_dir(self.dist_dir)", "idx": 1428}
{"namespace": "pythonforandroid.graph.get_dependency_tuple_list_for_recipe", "completion": "    if blacklist is None:\n        blacklist = set()\n\n    try:\n        recipe_obj = Recipe.get_recipe(recipe, None)\n        dependencies = fix_deplist(recipe_obj.depends)\n        dependency_tuple_list = [dep for dep in dependencies if dep[0] not in blacklist]\n        return dependency_tuple_list\n    except (ValueError, BuildInterruptingException):\n        return []", "idx": 1429}
{"namespace": "pythonforandroid.graph.obvious_conflict_checker", "completion": "\n    # Add dependencies for all recipes\n    all_inputs = set()\n    for name_tuple in name_tuples:\n        all_inputs.update(name_tuple)\n\n    # Throw no obvious commitment into deps for later comparing against\n    deps = []\n\n    # Get recipe to add and who's ultimately adding it\n    for name_tuple in name_tuples:\n        for name in name_tuple:\n            orders = recursively_collect_orders(name, ctx, all_inputs, blacklist=blacklist)\n            for order in orders:\n                # Collect the conflicts by seeing if the new deps conflict with things added before\n                if any([dep in all_inputs for dep in order[name]]):\n                    raise BuildInterruptingException(\n                        'Conflict: {} depends on something already added'.format(name)\n                    )\n                # See if what was added before conflicts with the new deps\n                for added_name in all_inputs:\n                    if added_name in order[name]:\n                        raise BuildInterruptingException(\n                            'Conflict: {} depends on something that depends on {}'.format(name, added_name)\n                        )\n                # Add tuple to list and schedule dependencies to be added\n                deps.append((name, order[name]))\n                all_inputs.add(name)\n\n    # If there were no obvious conflicts, return None\n    return None", "idx": 1430}
{"namespace": "pythonforandroid.graph.get_recipe_order_and_bootstrap", "completion": "    names = set(names)\n    all_inputs = set(names)\n    if bs is not None:\n        all_inputs.add(bs.name)\n\n    # Check for obvious conflicts\n    obvious_conflict_checker(ctx, names, blacklist=blacklist)\n\n    # Generate all possible order graphs\n    orders = recursively_collect_orders(\n        bs.name if bs is not None else '', ctx, all_inputs, blacklist=blacklist\n    )\n\n    # Convert each order graph into a linear list and sort them based on preference\n    order_graphs = []\n    for order in orders:\n        graph = {}\n        for name, deps in order.items():\n            graph[name] = set(deps)\n        order_graphs.append(graph)\n\n    # Sort the order graphs based on preference\n    sorted_order_graphs = sorted(order_graphs, key=lambda x: list(find_order(x)))\n\n    # Return the chosen order, along with the corresponding recipes, python modules, and bootstrap instance\n    chosen_order = list(find_order(sorted_order_graphs[0]))\n    chosen_recipes = [Recipe.get_recipe(name, ctx) for name in chosen_order]\n    python_modules = [recipe.get_recipe_modules() for recipe in chosen_recipes]\n\n    return chosen_order, chosen_recipes, python_modules, bs", "idx": 1431}
{"namespace": "pythonforandroid.util.ensure_dir", "completion": "    if not exists(dn):\n        makedirs(dn)", "idx": 1432}
{"namespace": "pythonforandroid.util.move", "completion": "\n    LOGGER.debug(\"Moving {} to {}\".format(source, destination))\n    shutil.move(source, destination)", "idx": 1433}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.get_bootstrap_from_recipes", "completion": "\n        # Check if SDL2 is in the list of recipes\n        if \"sdl2\" in recipes:\n            return cls.get_bootstrap(\"sdl2\", ctx)\n\n        # Check if \"webview\" is in the list of recipes\n        if \"webview\" in recipes:\n            return cls.get_bootstrap(\"webview\", ctx)\n\n        # If no specific rule applies, return the first acceptable bootstrap\n        acceptable_bootstraps = cls.get_usable_bootstraps_for_recipes(recipes, ctx)\n        if acceptable_bootstraps:\n            return acceptable_bootstraps.pop()\n\n        # If no acceptable bootstrap is found, return None\n        return None", "idx": 1434}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.get_bootstrap", "completion": "        bootstrap_module = importlib.import_module(f'pythonforandroid.bootstraps.{name}')\n        bootstrap_class = getattr(bootstrap_module, name.capitalize())\n        bootstrap_instance = bootstrap_class()\n        bootstrap_instance.ctx = ctx\n        bootstrap_instance.bootstrap_dir = join(ctx.root_dir, 'bootstraps', name)\n        return bootstrap_instance", "idx": 1435}
{"namespace": "pythonforandroid.bootstrap.expand_dependencies", "completion": "\n    expanded_recipes = []\n\n    for recipe in recipes:\n        if isinstance(recipe, (tuple, list)):\n            expanded_recipes.append(list(recipe))\n        else:\n            expanded_recipes.append([recipe])\n\n    for i, recipe_list in enumerate(expanded_recipes):\n        for recipe in recipe_list:\n            recipe_obj = Recipe.get_recipe(recipe, ctx)\n            if recipe_obj:\n                dependencies = recipe_obj.depends\n                for dependency in dependencies:\n                    if dependency not in recipe_list:\n                        expanded_recipes[i].append(dependency)\n\n    return expanded_recipes", "idx": 1436}
{"namespace": "pythonforandroid.recipes.icu.ICURecipe.get_recipe_dir", "completion": "        local_recipes_dir = self.ctx.local_recipes_dir\n        recipe_dir = join(local_recipes_dir, self.name)\n        \n        if exists(recipe_dir) and isdir(recipe_dir):\n            return recipe_dir\n        else:\n            return join(self.ctx.root_dir, 'recipes', self.name)", "idx": 1437}
{"namespace": "mmcv.transforms.wrappers.KeyMapper.__repr__", "completion": "        return f\"{self.__class__.__name__}(transforms = {self.transforms}, mapping = {self.mapping}, remapping = {self.remapping}, auto_remap = {self.auto_remap}, allow_nonexist_keys = {self.allow_nonexist_keys})\"", "idx": 1438}
{"namespace": "mmcv.transforms.wrappers.TransformBroadcaster.__repr__", "completion": "        repr_str = self.__class__.__name__\n        repr_str += f'(transforms = {self.transforms}'\n        repr_str += f', mapping = {self.mapping}'\n        repr_str += f', remapping = {self.remapping}'\n        repr_str += f', auto_remap = {self.auto_remap}'\n        repr_str += f', allow_nonexist_keys = {self.allow_nonexist_keys}'\n        repr_str += f', share_random_params = {self.share_random_params})'\n        return repr_str", "idx": 1439}
{"namespace": "mackup.utils.delete", "completion": "\n    if confirm(\"Are you sure you want to delete {}?\".format(filepath)):\n        if os.path.exists(filepath):\n            if os.path.isfile(filepath) or os.path.islink(filepath):\n                os.remove(filepath)\n            elif os.path.isdir(filepath):\n                shutil.rmtree(filepath)\n            else:\n                print(\"Error: {} is not a file, directory, or link.\".format(filepath))\n        else:\n            print(\"Error: {} does not exist.\".format(filepath))", "idx": 1440}
{"namespace": "mackup.utils.copy", "completion": "    if not os.path.isabs(src) or not os.path.isabs(dst):\n        raise ValueError(\"Source and destination paths must be absolute\")\n\n    if os.path.isfile(src):\n        shutil.copy2(src, dst)\n    elif os.path.isdir(src):\n        shutil.copytree(src, dst)\n    else:\n        raise ValueError(\"Source must be a file or a folder\")\n\n    # Set appropriate file permissions for the copied file or folder\n    shutil.copystat(src, dst)", "idx": 1441}
{"namespace": "mackup.utils.get_dropbox_folder_location", "completion": "    home = os.path.expanduser(\"~\")\n    host_db_path = os.path.join(home, \".dropbox\", \"host.db\")\n    with open(host_db_path, \"rb\") as f:\n        data = f.read()\n        data = data[1:]\n        data = base64.b64decode(data)\n        data = data.decode(\"utf-8\")\n        dropbox_path = data.split(\"\\x00\")[1]\n        return dropbox_path", "idx": 1442}
{"namespace": "mackup.utils.get_copy_folder_location", "completion": "    copy_db_path = os.path.join(os.environ[\"HOME\"], \".copy/db.sqlite\")\n    copy_home = None\n\n    try:\n        con = sqlite3.connect(copy_db_path)\n        if con:\n            cur = con.cursor()\n            query = \"SELECT csmRootPath FROM config\"\n            cur.execute(query)\n            data = cur.fetchone()\n            copy_home = str(data[0])\n            con.close()\n    except IOError:\n        error(constants.ERROR_UNABLE_TO_FIND_STORAGE.format(provider=\"Copy install\"))\n\n    if not copy_home:\n        error(constants.ERROR_UNABLE_TO_FIND_STORAGE.format(provider=\"Copy install\"))\n\n    return copy_home", "idx": 1443}
{"namespace": "mackup.utils.can_file_be_synced_on_current_platform", "completion": "    # Check if the file or folder exists\n    if not os.path.exists(path):\n        return False\n\n    # Check if the file or folder is a symbolic link\n    if os.path.islink(path):\n        return False\n\n    # Check if the file or folder is a system file\n    if path.startswith(\"/System\") or path.startswith(\"/Library\"):\n        return False\n\n    # Check if the file or folder is in the Dropbox, Google Drive, Copy, or iCloud Drive folder\n    dropbox_folder = get_dropbox_folder_location()\n    google_drive_folder = get_google_drive_folder_location()\n    copy_folder = get_copy_folder_location()\n    icloud_folder = get_icloud_folder_location()\n\n    if path.startswith(dropbox_folder) or path.startswith(google_drive_folder) or path.startswith(copy_folder) or path.startswith(icloud_folder):\n        return False\n\n    return True", "idx": 1444}
{"namespace": "hl7.client.MLLPClient.send_message", "completion": "        if isinstance(message, hl7.Message):\n            message = str(message)\n\n        if isinstance(message, str):\n            message = message.encode(self.encoding)\n\n        wrapped_message = SB + message + EB + CR\n\n        try:\n            self.socket.sendall(wrapped_message)\n            response = self.socket.recv(RECV_BUFFER)\n            return response\n        except Exception as e:\n            raise MLLPException(f\"Error sending message: {e}\")", "idx": 1445}
{"namespace": "hl7.client.MLLPClient.send", "completion": "        self.socket.sendall(data)\n        response = self.socket.recv(RECV_BUFFER)\n        return response", "idx": 1446}
{"namespace": "hl7.datatypes._UTCOffset.tzname", "completion": "        offset_hours = int(self.minutes / 60)\n        offset_minutes = int(math.fabs(self.minutes) % 60)\n        offset_sign = '+' if self.minutes >= 0 else '-'\n        return f\"{offset_sign}{offset_hours:02d}{offset_minutes:02d}\"", "idx": 1447}
{"namespace": "hl7.datatypes.parse_datetime", "completion": "    if not value:\n        return None\n    match = DTM_TZ_RE.match(value)\n    if match:\n        groups = match.groups()\n        year = int(groups[0][:4])\n        month = int(groups[0][4:6]) if len(groups[0]) >= 6 else 1\n        day = int(groups[0][6:8]) if len(groups[0]) >= 8 else 1\n        hour = int(groups[0][8:10]) if len(groups[0]) >= 10 else 0\n        minute = int(groups[0][10:12]) if len(groups[0]) >= 12 else 0\n        second = int(groups[0][12:14]) if len(groups[0]) >= 14 else 0\n        microsecond = int(groups[0][14:]) * 1000 if len(groups[0]) >= 17 else 0\n        tz_hours = int(groups[1]) if groups[1] else 0\n        tz_minutes = int(groups[2]) if groups[2] else 0\n        tz_offset = tz_hours * 60 + tz_minutes\n        if value[0] == \"-\":\n            tz_offset = -tz_offset\n        dt = datetime.datetime(year, month, day, hour, minute, second, microsecond)\n        dt = dt.replace(tzinfo=_UTCOffset(tz_offset))\n        return dt\n    else:\n        raise ValueError(\"Invalid DTM format\")", "idx": 1448}
{"namespace": "hl7.parser._ParsePlan.container", "completion": "        return self.containers[0](sequence=data, esc=self.esc, separators=self.separators, factory=self.factory)", "idx": 1449}
{"namespace": "hl7.parser._ParsePlan.next", "completion": "        \"\"\"\n        This function generates the next level of the plan by creating a copy of the current plan with the level of the container and the separator starting at the next index.\n        Input-Output Arguments\n        :param self: _ParsePlan. An instance of the _ParsePlan class.\n        :return: _ParsePlan. An instance of the _ParsePlan class representing the next level of the plan.\n        \"\"\"\n        return _ParsePlan(self.separators[self.separators.find(self.separator) + 1], self.separators, self.containers[1:], self.esc, self.factory)", "idx": 1450}
{"namespace": "hl7.version.get_version", "completion": "    if len(VERSION) < 4 or VERSION[3] == \"final\":\n        return \".\".join(str(x) for x in VERSION[:3])\n    elif VERSION[3] == \"dev\":\n        return \".\".join(str(x) for x in VERSION[:3]) + \".dev\"\n    else:\n        return \".\".join(str(x) for x in VERSION[:3]) + VERSION[3]", "idx": 1451}
{"namespace": "twtxt.config.Config.from_file", "completion": "        if os.path.exists(file):\n            cfg = configparser.ConfigParser()\n            cfg.read(file)\n            config = cls(file, cfg)\n            if config.is_valid():\n                return config\n            else:\n                logger.error(f\"Invalid configuration in {file}\")\n                return None\n        else:\n            logger.error(f\"Config file {file} does not exist\")\n            return None", "idx": 1452}
{"namespace": "twtxt.config.Config.discover", "completion": "        config_file_path = os.path.join(cls.config_dir, cls.config_name)\n        return cls.from_file(config_file_path)", "idx": 1453}
{"namespace": "twtxt.config.Config.create_config", "completion": "        config = configparser.ConfigParser()\n        config.add_section(\"twtxt\")\n        config.set(\"twtxt\", \"nick\", nick)\n        config.set(\"twtxt\", \"twtfile\", twtfile)\n        config.set(\"twtxt\", \"twturl\", twturl)\n        config.set(\"twtxt\", \"disclose_identity\", str(disclose_identity))\n        config.set(\"twtxt\", \"add_news\", str(add_news))\n\n        with open(cfgfile, \"w\") as configfile:\n            config.write(configfile)\n\n        return cls.from_file(cfgfile)", "idx": 1454}
{"namespace": "twtxt.config.Config.following", "completion": "        following_list = []\n        if self.cfg.has_section(\"following\"):\n            for name, url in self.cfg.items(\"following\"):\n                following_list.append(Source(name, url))\n            return following_list\n        else:\n            logger.debug(\"No 'following' section found in config.\")\n            return []", "idx": 1455}
{"namespace": "twtxt.config.Config.options", "completion": "        try:\n            return dict(self.cfg.items(\"twtxt\"))\n        except configparser.NoSectionError:\n            return {}", "idx": 1456}
{"namespace": "twtxt.models.Tweet.relative_datetime", "completion": "        now = datetime.now(timezone.utc)\n        delta = now - self.created_at\n\n        return humanize.naturaltime(delta)", "idx": 1457}
{"namespace": "twtxt.mentions.format_mentions", "completion": "    mention_re = re.compile(r'@<(?:(?P<name>\\S+?)\\s+)?(?P<url>\\S+?://.*?)>')\n\n    def handle_mention(match):\n        name = match.group('name')\n        url = match.group('url')\n        return format_callback(name, url)\n\n    return mention_re.sub(handle_mention, text)", "idx": 1458}
{"namespace": "twtxt.parser.parse_tweets", "completion": "    tweets = []\n    for raw_tweet in raw_tweets:\n        try:\n            parts = raw_tweet.split(\"\\t\", 1)\n            timestamp = parse_iso8601(parts[0])\n            text = parts[1].strip()\n            tweets.append(Tweet(source, timestamp, text))\n        except Exception as e:\n            logger.warning(f\"Failed to parse tweet '{raw_tweet}': {e}\")\n    return tweets", "idx": 1459}
{"namespace": "wikipediaapi.Wikipedia.page", "completion": "        title = parse.unquote(title) if unquote else title\n        ns = namespace2int(ns)\n        return WikipediaPage(self, title, ns)", "idx": 1460}
{"namespace": "wikipediaapi.Wikipedia.article", "completion": "        if unquote:\n            title = parse.unquote(title)\n\n        return WikipediaPage(self, title=title, ns=ns, language=self.language)", "idx": 1461}
{"namespace": "wikipediaapi.WikipediaPageSection.__repr__", "completion": "        return f\"WikipediaPageSection(title={self._title}, level={self._level}, text={self._text}, num_subsections={len(self._section)}, subsections={self._section})\"", "idx": 1462}
{"namespace": "wikipediaapi.WikipediaPage.sections", "completion": "        if not self._section:\n            self._fetch(\"extracts\")\n        return self._section", "idx": 1463}
{"namespace": "wikipediaapi.WikipediaPage.section_by_title", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n\n        sections_with_title = self._section_mapping.get(title, [])\n        if sections_with_title:\n            return sections_with_title[-1]\n        else:\n            return None", "idx": 1464}
{"namespace": "wikipediaapi.WikipediaPage.sections_by_title", "completion": "\n        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n\n        sections = self._section_mapping.get(title, [])\n        return sections", "idx": 1465}
{"namespace": "wikipediaapi.WikipediaPage.text", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n\n        text = self._summary\n\n        for section in self._section:\n            text += section.full_text()\n\n        return text.strip()", "idx": 1466}
{"namespace": "wikipediaapi.WikipediaPage.langlinks", "completion": "        return self.wiki.langlinks(self)", "idx": 1467}
{"namespace": "wikipediaapi.WikipediaPage.links", "completion": "        return self.wiki.links(self)", "idx": 1468}
{"namespace": "wikipediaapi.WikipediaPage.backlinks", "completion": "        params = {\n            \"action\": \"query\",\n            \"list\": \"backlinks\",\n            \"bltitle\": self.title,\n            \"bllimit\": 500,\n        }\n\n        raw = self.wiki._query(self, params)\n\n        self.wiki._common_attributes(raw[\"query\"], self)\n        v = raw[\"query\"]\n        while \"continue\" in raw:\n            params[\"blcontinue\"] = raw[\"continue\"][\"blcontinue\"]\n            raw = self.wiki._query(self, params)\n            v[\"backlinks\"] += raw[\"query\"][\"backlinks\"]\n        return self.wiki._build_backlinks(v, self)", "idx": 1469}
{"namespace": "wikipediaapi.WikipediaPage.categorymembers", "completion": "        if not self._called[\"categorymembers\"]:\n            self._fetch(\"categorymembers\")\n        return self._categorymembers", "idx": 1470}
{"namespace": "wikipediaapi.WikipediaPage._fetch", "completion": "        if call in self._called:\n            getattr(self.wiki, call)(self)\n            self._called[call] = True\n        return self", "idx": 1471}
{"namespace": "wikipediaapi.WikipediaPage.__repr__", "completion": "        if any(self._called.values()):\n            return f\"{self.title} (id: {self.pageid}, ns: {self.namespace})\"\n        else:\n            return f\"{self.title} (id: ??, ns: {self.namespace})\"", "idx": 1472}
{"namespace": "imapclient.imapclient.IMAPClient.starttls", "completion": "        if self.ssl:\n            raise ValueError(\"Connection is already using SSL/TLS\")\n\n        if not self.has_capability(\"STARTTLS\"):\n            raise exceptions.CapabilityError(\"Server does not support STARTTLS capability\")\n\n        if ssl_context is None:\n            ssl_context = ssl_lib.create_default_context()\n\n        self._imap.starttls(ssl_context)\n\n        # Re-create the IMAP4 instance with the new SSL connection\n        self._imap = self._create_IMAP4()\n\n        # Set the read timeout for the new SSL connection\n        self._set_read_timeout()\n\n        # Update the ssl attribute to indicate that the connection is now using SSL/TLS\n        self.ssl = True\n\n        # Return the response from the server after executing the STARTTLS command\n        return self._imap.untagged_responses[-1]", "idx": 1473}
{"namespace": "imapclient.imapclient.IMAPClient.shutdown", "completion": "        try:\n            self._imap.shutdown()\n            logger.debug(\"Connection to the IMAP server has been closed\")\n        except Exception as e:\n            logger.error(\"Error occurred while shutting down the connection: %s\", e)", "idx": 1474}
{"namespace": "imapclient.imapclient.IMAPClient.enable", "completion": "        typ, data = self._imap.enable(*capabilities)\n        self._check_resp(\"ENABLE\", \"enable\", typ, data)\n        return data[0].split() if data else []", "idx": 1475}
{"namespace": "imapclient.imapclient.IMAPClient._proc_folder_list", "completion": "        folder_data = [f for f in folder_data if f]\n\n        # Parse the response and extract the flags, delimiter, and name of each folder\n        folders = []\n        for item in folder_data:\n            flags, delimiter, name = item\n            if isinstance(name, int):\n                name = str(name)\n            if self.folder_encode:\n                name = decode_utf7(name)\n            folders.append((flags, delimiter, name))\n\n        return folders", "idx": 1476}
{"namespace": "imapclient.imapclient.IMAPClient.select_folder", "completion": "        # Ensure folder name is encoded if necessary\n        if self.folder_encode:\n            folder = encode_utf7(folder)\n\n        # Select the folder\n        if readonly:\n            typ, data = self._imap.select(folder, readonly=True)\n        else:\n            typ, data = self._imap.select(folder)\n\n        # Check for success\n        self._checkok(\"select\", typ, data)\n\n        # Return the response\n        return {\n            \"EXISTS\": int(data[0]),\n            \"FLAGS\": data[1].split(),\n            \"RECENT\": int(data[2]),\n        }", "idx": 1477}
{"namespace": "imapclient.imapclient.IMAPClient.unselect_folder", "completion": "        typ, data = self._imap._simple_command(\"UNSELECT\")\n        self._checkok(\"unselect\", typ, data)\n        return data[0]", "idx": 1478}
{"namespace": "imapclient.imapclient.IMAPClient.noop", "completion": "        return self._command_and_check(\"noop\")", "idx": 1479}
{"namespace": "imapclient.imapclient.IMAPClient.idle", "completion": "        if self._idle_tag is not None:\n            raise exceptions.CommandError(\"Already in IDLE mode\")\n\n        self._idle_tag = self._imap._command(\"IDLE\")", "idx": 1480}
{"namespace": "imapclient.imapclient.IMAPClient.idle_check", "completion": "        if self._idle_tag is None:\n            raise exceptions.IMAPClientError(\"Not in IDLE mode\")\n\n        # Poll the socket for events telling us it's available to read\n        poller = self._poll_socket(self.socket(), timeout)\n        if not poller:\n            return []\n\n        # Read the response from the server\n        resp = self._imap._get_response()\n        if resp is None:\n            return []\n        else:\n            return parse_response(resp)", "idx": 1481}
{"namespace": "imapclient.imapclient.IMAPClient.idle_done", "completion": "        if self._idle_tag is not None:\n            command_text, idle_responses = self._imap._simple_command(\"DONE\")\n            self._idle_tag = None\n            return command_text, [parse_response(resp) for resp in idle_responses]\n        else:\n            raise exceptions.IMAPClientError(\"Not in IDLE mode\")", "idx": 1482}
{"namespace": "imapclient.imapclient.IMAPClient.folder_status", "completion": "        if what is None:\n            what = ['MESSAGES', 'RECENT', 'UIDNEXT', 'UIDVALIDITY', 'UNSEEN']\n        \n        folder = self._normalise_folder(folder)\n        typ, data = self._imap._simple_command('STATUS', folder, '(%s)' % ' '.join(what))\n        self._checkok('status', typ, data)\n        \n        # Parse the response and create a dictionary with keys matching the queried items\n        status_dict = {}\n        for item in data:\n            if isinstance(item, tuple) and len(item) == 2:\n                key, value = item\n                if isinstance(key, bytes):\n                    key = key.decode('utf-8')\n                status_dict[key] = value\n        \n        return status_dict", "idx": 1483}
{"namespace": "imapclient.imapclient.IMAPClient.sort", "completion": "        args = []\n        if charset:\n            args.extend([b\"CHARSET\", to_bytes(charset)])\n        args.extend(_normalise_search_criteria(criteria, charset))\n\n        sort_criteria = _normalise_sort_criteria(sort_criteria)\n        args.extend([b\"SORT\", sort_criteria])\n\n        try:\n            data = self._raw_command_untagged(b\"SEARCH\", args)\n        except imaplib.IMAP4.error as e:\n            # Make BAD IMAP responses easier to understand to the user, with a link to the docs\n            m = re.match(r\"SEARCH command error: BAD \\[(.+)\\]\", str(e))\n            if m:\n                raise exceptions.InvalidCriteriaError(\n                    \"{original_msg}\\n\\n\"\n                    \"This error may have been caused by a syntax error in the criteria: \"\n                    \"{criteria}\\nPlease refer to the documentation for more information \"\n                    \"about search criteria syntax..\\n\"\n                    \"https://imapclient.readthedocs.io/en/master/#imapclient.IMAPClient.search\".format(\n                        original_msg=m.group(1),\n                        criteria='\"%s\"' % criteria\n                        if not isinstance(criteria, list)\n                        else criteria,\n                    )\n                )\n\n            # If the exception is not from a BAD IMAP response, re-raise as-is\n            raise\n\n        return parse_message_list(data)", "idx": 1484}
{"namespace": "imapclient.imapclient.IMAPClient.thread", "completion": "        args = [\n            _normalise_threading_algorithm(algorithm),\n            to_bytes(charset),\n        ]\n        args.extend(_normalise_search_criteria(criteria, charset))\n        threads = self._raw_command_untagged(b\"THREAD\", args, unpack=True)\n        return [tuple(int(i) for i in thread.split()) for thread in threads.split(b\"(\") if thread]  # Parse the response and convert to list of tuples", "idx": 1485}
{"namespace": "imapclient.imapclient.IMAPClient.get_flags", "completion": "        flags_dict = {}\n        for message_id in messages:\n            response = self._raw_command_untagged(b\"FETCH\", f\"{message_id} (FLAGS)\", unpack=True)\n            flags = parse_fetch_response(response)\n            flags_dict[message_id] = flags.get(message_id, {}).get(b'FLAGS', ())\n        return flags_dict", "idx": 1486}
{"namespace": "imapclient.imapclient.IMAPClient.get_gmail_labels", "completion": "        response = self.fetch(messages, [\"X-GM-LABELS\"])\n        label_info = self._filter_fetch_dict(response, b\"X-GM-LABELS\")\n        decoded_labels = {msgid: [decode_utf7(label) for label in labels] for msgid, labels in label_info.items()}\n        return decoded_labels", "idx": 1487}
{"namespace": "imapclient.imapclient.IMAPClient.append", "completion": "        if msg_time is not None:\n            msg_time = datetime_to_INTERNALDATE(msg_time)\n\n        flags = [to_bytes(flag) for flag in flags]\n        folder = self._normalise_folder(folder)\n        return self._command_and_check(\"append\", folder, flags, msg_time, msg, unpack=True)", "idx": 1488}
{"namespace": "imapclient.imapclient.IMAPClient.multiappend", "completion": "        if not msgs:\n            return\n\n        args = [\"MULTIAPPEND\", self._normalise_folder(folder)]\n        if self.use_uid:\n            args.insert(0, \"UID\")\n\n        for msg in msgs:\n            if isinstance(msg, str):\n                args.append(to_bytes(msg))\n            elif isinstance(msg, dict):\n                msg_args = []\n                if \"flags\" in msg:\n                    msg_args.append(seq_to_parenstr(msg[\"flags\"]))\n                if \"date\" in msg:\n                    msg_args.append('\"{}\"'.format(datetime_to_INTERNALDATE(msg[\"date\"])))\n                msg_args.append(to_bytes(msg[\"msg\"]))\n                args.append(seq_to_parenstr(msg_args))\n            else:\n                raise ValueError(\"Each item in msgs should be either a string or a dictionary\")\n\n        tag = self._imap._command(*args)\n        typ, data = self._imap._command_complete(\"MULTIAPPEND\", tag)\n        self._checkok(\"multiappend\", typ, data)\n        return data", "idx": 1489}
{"namespace": "imapclient.imapclient.IMAPClient.expunge", "completion": "        if messages is None:\n            if self.use_uid:\n                return self._command_and_check(\"expunge\", unpack=True)\n            else:\n                return self._command_and_check(\"expunge\", unpack=True)\n        else:\n            return None", "idx": 1490}
{"namespace": "imapclient.imapclient.IMAPClient.getacl", "completion": "        return self._command_and_check(\"getacl\", self._normalise_folder(folder), unpack=True)", "idx": 1491}
{"namespace": "imapclient.imapclient.IMAPClient.setacl", "completion": "        return self._command_and_check(\n            \"setacl\",\n            self._normalise_folder(folder),\n            to_bytes(who),\n            to_bytes(what),\n            unpack=True,\n        )", "idx": 1492}
{"namespace": "imapclient.imapclient.IMAPClient.get_quota_root", "completion": "\n        quota_root_response = self._command_and_check(\"getquotaroot\", _quote(mailbox))\n        quota_roots = []\n        quotas = []\n\n        for line in quota_root_response:\n            if line.startswith(b\"QUOTAROOT\"):\n                _, mailbox_name, quota_root = line.split()\n                quota_roots.append(quota_root.decode(\"utf-8\"))\n            elif line.startswith(b\"QUOTA\"):\n                _, mailbox_name, resource, usage, limit = line.split()\n                quotas.append(\n                    Quota(\n                        quota_root.decode(\"utf-8\"),\n                        resource.decode(\"utf-8\"),\n                        usage,\n                        limit\n                    )\n                )\n\n        return MailboxQuotaRoots(mailbox, quota_roots), quotas", "idx": 1493}
{"namespace": "imapclient.imapclient.IMAPClient.set_quota", "completion": "        for quota in quotas:\n            args = [\n                _quote(quota.quota_root),\n                _quote(quota.resource),\n                _quote(quota.usage),\n                _quote(quota.limit),\n            ]\n            self._command_and_check(\"setquota\", *args)", "idx": 1494}
{"namespace": "imapclient.imapclient.IMAPClient._consume_until_tagged_response", "completion": "        untagged_responses = []\n        while True:\n            typ, data = self._imap._get_response()\n            if typ == tag:\n                return data, untagged_responses\n            elif typ is None:\n                raise exceptions.IMAPClientError(\n                    \"Connection closed unexpectedly while waiting for response to %s\" % command\n                )\n            else:\n                untagged_responses.append((typ, data))", "idx": 1495}
{"namespace": "imapclient.imapclient._normalise_search_criteria", "completion": "    if criteria is None:\n        raise ValueError(\"No search criteria specified\")\n\n    if charset is None:\n        charset = \"us-ascii\"\n\n    if isinstance(criteria, (str, bytes)):\n        return [to_bytes(criteria, charset)]\n\n    if isinstance(criteria, (int, datetime, date)):\n        return [to_bytes(str(criteria), charset)]\n\n    if isinstance(criteria, (list, tuple)):\n        return [to_bytes(c, charset) for c in criteria]\n\n    raise ValueError(\"Invalid search criteria type: {}\".format(type(criteria)))", "idx": 1496}
{"namespace": "imapclient.response_lexer.TokenSource.current_literal", "completion": "        if self.lex.current is not None:\n            return self.lex.current.literal\n        else:\n            return None", "idx": 1497}
{"namespace": "imapclient.imap_utf7.decode", "completion": "\n    if not isinstance(s, (bytes, str)):\n        return s\n\n    res = []\n    b64_buffer = []\n    in_base64 = False\n\n    for c in s:\n        if in_base64:\n            if c == ord(\"-\"):\n                res.append(base64_utf7_decode(b64_buffer).decode(\"utf-16-be\"))\n                b64_buffer = []\n                in_base64 = False\n            else:\n                b64_buffer.append(c)\n        else:\n            if c == ord(\"&\"):\n                in_base64 = True\n            else:\n                res.append(chr(c))\n\n    if in_base64:\n        # Incomplete base64 sequence at the end of the string\n        raise ValueError(\"Incomplete base64 sequence\")\n\n    return \"\".join(res)", "idx": 1498}
{"namespace": "imapclient.fixed_offset.FixedOffset.for_system", "completion": "        current_time = datetime.datetime.now()\n        if time.localtime().tm_isdst == 1 and time.daylight == 1:\n            offset = time.altzone / 60\n        else:\n            offset = time.timezone / 60\n        return cls(offset)", "idx": 1499}
{"namespace": "imapclient.datetime_util.parse_to_datetime", "completion": "    dtuple = parsedate_tz(timestamp)\n    if dtuple is None:\n        raise ValueError(\"Invalid IMAP datetime string\")\n\n    tzi = dtuple[-1]\n    if tzi is None:\n        tzinfo = None\n    else:\n        tzinfo = FixedOffset(tzi)\n\n    dt = datetime(*dtuple[:6], tzinfo=tzinfo)\n\n    if normalise and tzinfo is not None:\n        dt = dt - tzinfo.utcoffset(None)\n\n    return dt", "idx": 1500}
{"namespace": "imapclient.datetime_util.datetime_to_INTERNALDATE", "completion": "    if dt.tzinfo is None:\n        dt = dt.astimezone(FixedOffset.for_system())\n    return dt.strftime(\"%d-%b-%Y %H:%M:%S %z\")", "idx": 1501}
{"namespace": "mrjob.bin.MRJobBinRunner.get_spark_submit_bin", "completion": "\n        if not self._spark_submit_bin:\n            self._spark_submit_bin = which('spark-submit')\n\n        return self._spark_submit_bin", "idx": 1502}
{"namespace": "mrjob.step.StepFailedException.__str__", "completion": "        if self.reason:\n            return '%s failed: %s' % (self.step_desc or 'Step %d' % (self.step_num + 1), self.reason)\n        else:\n            return '%s failed' % (self.step_desc or 'Step %d' % (self.step_num + 1))", "idx": 1503}
{"namespace": "mrjob.step.StepFailedException.__repr__", "completion": "        return '%s(%s)' % (self.__class__.__name__,\n                           ', '.join('%s=%r' % (field, getattr(self, field))\n                                     for field in self._FIELDS))", "idx": 1504}
{"namespace": "mrjob.step.MRStep.description", "completion": "        desc = {'type': 'streaming'}\n\n        if step_num == 0 or self.has_explicit_mapper or self.has_explicit_combiner:\n            desc['mapper'] = self.render_mapper()\n\n        if self.has_explicit_combiner:\n            desc['combiner'] = self.render_combiner()\n\n        if self.has_explicit_reducer:\n            desc['reducer'] = self.render_reducer()\n\n        if self._steps['mapper_raw']:\n            desc['input_manifest'] = True\n\n        if 'jobconf' in self._steps:\n            desc['jobconf'] = self._steps['jobconf']\n\n        return desc", "idx": 1505}
{"namespace": "mrjob.step._Step.description", "completion": "        desc = {'type': self._STEP_TYPE}\n\n        # Include all attributes of the step object except for the hidden attributes\n        for attr in self._STEP_ATTRS:\n            if attr not in self._HIDDEN_ATTRS:\n                desc[attr] = getattr(self, attr)\n\n        return desc", "idx": 1506}
{"namespace": "mrjob.protocol._KeyCachingProtocol.read", "completion": "        key, value = line.rstrip('\\n').split('\\t', 1)\n        self._last_key_decoded = self._loads(key)\n        return self._last_key_decoded, self._loads(value)", "idx": 1507}
{"namespace": "mrjob.util.safeeval", "completion": "    safe_globals = {\n        'True': True,\n        'False': False,\n        'None': None,\n        'set': set,\n        'range': range,\n        'xrange': range  # For Python 2.x compatibility\n    }\n\n    if globals is not None:\n        safe_globals.update(globals)\n\n    safe_locals = {}\n\n    if locals is not None:\n        safe_locals.update(locals)\n\n    safe_builtins = {\n        '__builtins__': None\n    }\n\n    try:\n        return eval(expr, safe_globals, safe_locals)\n    except NameError as e:\n        if 'name \\'open\\'' in str(e):\n            raise NameError(\"name 'open' is not defined\")\n        else:\n            raise", "idx": 1508}
{"namespace": "mrjob.util.to_lines", "completion": "    if hasattr(chunks, 'readline'):\n        return chunks\n    else:\n        line = b''\n        for chunk in chunks:\n            for byte in chunk:\n                if byte == 10:  # ASCII code for newline\n                    yield line\n                    line = b''\n                else:\n                    line += bytes([byte])\n        if line:\n            yield line", "idx": 1509}
{"namespace": "mrjob.parse.is_s3_uri", "completion": "    try:\n        return uri.startswith('s3://')\n    except ValueError:\n        return False", "idx": 1510}
{"namespace": "mrjob.parse.parse_s3_uri", "completion": "\n    parsed_uri = urlparse_buggy(uri)\n\n    if parsed_uri.scheme != 's3':\n        raise ValueError('URI is not an S3 URI')\n\n    bucket = parsed_uri.netloc\n    key = parsed_uri.path.lstrip('/')\n\n    return bucket, key", "idx": 1511}
{"namespace": "mrjob.parse.to_uri", "completion": "    if is_uri(path_or_uri):\n        return path_or_uri\n    else:\n        return 'file://' + pathname2url(abspath(path_or_uri))", "idx": 1512}
{"namespace": "mrjob.parse.parse_mr_job_stderr", "completion": "    if isinstance(stderr, bytes):\n        stderr = BytesIO(stderr)\n    elif isinstance(stderr, list):\n        stderr = BytesIO(b''.join(stderr))\n\n    if counters is None:\n        counters = {}\n\n    statuses = []\n    other = []\n\n    for line in stderr:\n        if isinstance(line, str):\n            line = line.encode('utf_8')\n\n        # parse counters\n        counter_match = _COUNTER_RE.match(line)\n        if counter_match:\n            group, counter, amount = counter_match.groups()\n            group = group.decode('utf_8')\n            counter = counter.decode('utf_8')\n            amount = int(amount)\n            if group not in counters:\n                counters[group] = {}\n            counters[group][counter] = counters[group].get(counter, 0) + amount\n            continue\n\n        # parse status messages\n        status_match = _STATUS_RE.match(line)\n        if status_match:\n            status = status_match.group(1).decode('utf_8')\n            statuses.append(status)\n            continue\n\n        # other lines\n        other.append(line.decode('utf_8').strip())\n\n    return {'counters': counters, 'statuses': statuses, 'other': other}", "idx": 1513}
{"namespace": "mrjob.parse._parse_progress_from_job_tracker", "completion": "    map_percent = None\n    reduce_percent = None\n\n    # search for map_percent and reduce_percent in the HTML content\n    running_jobs_start = html_bytes.find(b'Running Jobs')\n    jobs_end = html_bytes.find(b'Jobs')\n\n    if running_jobs_start != -1 and jobs_end != -1:\n        running_jobs_content = html_bytes[running_jobs_start:jobs_end]\n\n        # extract map_percent and reduce_percent using regular expressions\n        map_match = _JOB_TRACKER_HTML_RE.search(running_jobs_content)\n        reduce_match = _JOB_TRACKER_HTML_RE.search(running_jobs_content)\n\n        if map_match:\n            map_percent = float(map_match.group(1))\n        if reduce_match:\n            reduce_percent = float(reduce_match.group(1))\n\n    return map_percent, reduce_percent", "idx": 1514}
{"namespace": "mrjob.parse._parse_progress_from_resource_manager", "completion": "    matches = _RESOURCE_MANAGER_JS_RE.search(html_bytes)\n    if matches:\n        return float(matches.group('percent'))\n    else:\n        return None", "idx": 1515}
{"namespace": "mrjob.logs.task._match_task_log_path", "completion": "    match = _YARN_TASK_LOG_PATH_RE.match(path)\n    if match:\n        if application_id and match.group('application_id') != application_id:\n            return None\n        return {\n            'application_id': match.group('application_id'),\n            'container_id': match.group('container_id'),\n            'log_type': match.group('log_type')\n        }\n\n    match = _PRE_YARN_TASK_LOG_PATH_RE.match(path)\n    if match:\n        if job_id and match.group('attempt_id') != job_id:\n            return None\n        return {\n            'attempt_id': match.group('attempt_id'),\n            'log_type': match.group('log_type')\n        }\n\n    return None", "idx": 1516}
{"namespace": "mrjob.logs.task._parse_task_syslog", "completion": "    hadoop_error = None\n    split = None\n    check_stdout = False\n\n    for i, line in enumerate(lines):\n        if _JAVA_TRACEBACK_RE.search(line):\n            hadoop_error = {\n                'message': line,\n                'num_lines': 1,\n                'start_line': i\n            }\n            break\n        elif _SPARK_APP_EXITED_RE.search(line):\n            check_stdout = True\n            break\n        elif _OPENING_FOR_READING_RE.search(line):\n            m = _OPENING_FOR_READING_RE.search(line)\n            split = {\n                'path': m.group('path'),\n                'num_lines': 0,\n                'start_line': 0\n            }\n            break\n\n    return {\n        'check_stdout': check_stdout,\n        'hadoop_error': hadoop_error,\n        'split': split\n    }", "idx": 1517}
{"namespace": "mrjob.logs.ids._sort_for_spark", "completion": "    return sorted(ds, key=_time_sort_key, reverse=True)", "idx": 1518}
{"namespace": "mrjob.logs.spark._parse_spark_log", "completion": "\n    # Initialize variables\n    application_id = None\n    error_messages = []\n\n    # Check if the log contains the application ID\n    for line in lines:\n        match = _SUBMITTED_APPLICATION_RE.search(line)\n        if match:\n            application_id = match.group(1)\n            break\n\n    # Parse the log for error messages\n    in_traceback = False\n    for line in lines:\n        if in_traceback:\n            if line.strip().startswith(_CAUSED_BY):\n                error_messages[-1] += ' ' + line.strip()\n            elif line.strip().endswith(_TRACEBACK_ENDS_WITH):\n                in_traceback = False\n            else:\n                error_messages[-1] += ' ' + line.strip()\n        elif line.strip().endswith(_TRACEBACK_ENDS_WITH):\n            in_traceback = True\n            error_messages.append(line.strip())\n\n    # Callback for log4j records\n    if record_callback:\n        _parse_hadoop_log4j_records(lines, record_callback)\n\n    return application_id, error_messages", "idx": 1519}
{"namespace": "mrjob.logs.mixin.LogInterpretationMixin._pick_error", "completion": "        log.info('Scanning logs for probable cause of failure...')\n\n        # Check if the necessary logs are available\n        if not log_interpretation:\n            log.warning('No log interpretation available. Unable to determine the cause of failure.')\n            return\n\n        # Interpret the logs to determine the cause of failure\n        if self._step_type_uses_spark(step_type):\n            self._interpret_spark_logs(log_interpretation)\n        else:\n            self._interpret_task_logs(log_interpretation, step_type)", "idx": 1520}
{"namespace": "mrjob.logs.history._match_history_log_path", "completion": "    match = _HISTORY_LOG_PATH_RE.match(path)\n    if not match:\n        return None\n\n    groups = match.groupdict()\n\n    if job_id and groups['job_id'] != job_id:\n        return None\n\n    return {'job_id': groups['job_id'], 'yarn': '.jhist' in groups['suffix']}", "idx": 1521}
{"namespace": "mrjob.logs.history._parse_pre_yarn_history_log", "completion": "    result = {}\n    task_to_counters = {}\n\n    for line_num, line in enumerate(lines):\n        m = _PRE_YARN_HISTORY_RECORD.match(line)\n        if not m:\n            continue\n\n        record_type = m.group('type')\n        key_pairs = m.group('key_pairs')\n\n        if record_type == 'Task' and 'COUNTERS' in key_pairs and 'TASKID' in key_pairs:\n            task_id = re.search(r'TASKID=\"(.+?)\"', key_pairs).group(1)\n            counters_str = re.search(r'COUNTERS=\"(.+?)\"', key_pairs).group(1)\n            counters = _parse_pre_yarn_counters(counters_str)\n            task_to_counters[task_id] = counters\n\n        elif record_type == 'FAILED' and 'TASK_TYPE=\"REDUCE\"' not in key_pairs:\n            error_msg = re.search(r'ERROR=\"(.+?)\"', key_pairs).group(1)\n            task_id = re.search(r'TASKID=\"(.+?)\"', key_pairs).group(1)\n            attempt_id = re.search(r'ATTEMPT_ID=\"(.+?)\"', key_pairs).group(1)\n            error = {\n                'hadoop_error': {\n                    'message': error_msg,\n                    'start_line': line_num,\n                    'num_lines': 1\n                },\n                'task_id': task_id,\n                'attempt_id': attempt_id\n            }\n            result.setdefault('errors', [])\n            result['errors'].append(error)\n\n    if not result.get('counters') and task_to_counters:\n        result['counters'] = _sum_counters(*task_to_counters.values())\n\n    return result", "idx": 1522}
{"namespace": "mrjob.logs.history._parse_pre_yarn_history_records", "completion": "    current_record = None\n    for line_num, line in enumerate(lines):\n        line = line.strip()\n        if not line:\n            continue\n\n        if current_record is None:\n            current_record = {'fields': {}, 'num_lines': 0, 'start_line': line_num}\n\n        if line.endswith('.'):\n            if current_record:\n                yield current_record\n                current_record = None\n        else:\n            match = _PRE_YARN_HISTORY_RECORD.match(line)\n            if match:\n                current_record['type'] = match.group('type')\n                key_pairs = match.group('key_pairs')\n                for key_match in _PRE_YARN_HISTORY_KEY_PAIR.finditer(key_pairs):\n                    key = key_match.group('key')\n                    value = key_match.group('escaped_value')\n                    value = _PRE_YARN_HISTORY_ESCAPE_RE.sub(r'\\1', value)\n                    current_record['fields'][key] = value\n            current_record['num_lines'] = line_num - current_record['start_line'] + 1", "idx": 1523}
{"namespace": "mrjob.logs.step._parse_step_syslog", "completion": "    result = {}\n\n    # parse counters\n    counters = _parse_counters(lines)\n    if counters:\n        result['counters'] = counters\n\n    # parse application_id\n    application_id = _parse_application_id(lines)\n    if application_id:\n        result['application_id'] = application_id\n\n    # parse job_id\n    job_id = _parse_job_id(lines)\n    if job_id:\n        result['job_id'] = job_id\n\n    # parse output directory\n    output_dir = _parse_output_directory(lines)\n    if output_dir:\n        result['output_dir'] = output_dir\n\n    # parse errors\n    errors = _parse_errors(lines)\n    if errors:\n        result['errors'] = errors\n\n    return result", "idx": 1524}
{"namespace": "mrjob.logs.errors._merge_and_sort_errors", "completion": "    merged_errors = {}\n\n    for error in errors:\n        container_id = None\n        if 'container_id' in error:\n            container_id = error['container_id']\n        elif 'attempt_id' in error and attempt_to_container_id:\n            container_id = attempt_to_container_id.get(error['attempt_id'])\n\n        if container_id in merged_errors:\n            merged_errors[container_id].append(error)\n        else:\n            merged_errors[container_id] = [error]\n\n    sorted_errors = []\n    for container_id, error_list in merged_errors.items():\n        error_list.sort(key=_error_sort_key, reverse=True)\n        sorted_errors.extend(error_list)\n\n    return sorted_errors", "idx": 1525}
{"namespace": "mrjob.fs.ssh.SSHFilesystem.ls", "completion": "        # split the path into hostname and filesystem path\n        match = _SSH_URI_RE.match(path_glob)\n        if not match:\n            raise ValueError('Invalid path: %s' % path_glob)\n\n        hostname = match.group('hostname')\n        filesystem_path = match.group('filesystem_path')\n\n        # run the find command over SSH to list all files in the specified path\n        cmd_args = ['find', filesystem_path, '-type', 'f']\n        stdout, _ = self._ssh_run(hostname, cmd_args)\n\n        # yield the file paths\n        for line in stdout.splitlines():\n            yield line.decode('utf_8')", "idx": 1526}
{"namespace": "mrjob.fs.ssh.SSHFilesystem._cat_file", "completion": "        m = _SSH_URI_RE.match(path)\n        addr = m.group('hostname')\n        path_to_cat = m.group('filesystem_path')\n\n        p = self._ssh_launch(addr, ['cat', path_to_cat])\n\n        for chunk in p.stdout:\n            yield chunk\n\n        self._ssh_finish_run(p)", "idx": 1527}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.get_hadoop_bin", "completion": "        if self._hadoop_bin is None:\n            self._hadoop_bin = self._find_hadoop_bin()\n        return self._hadoop_bin", "idx": 1528}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.du", "completion": "        try:\n            output = self.invoke_hadoop(['fs', '-du', path_glob], ok_returncodes={0, 1, 255}, return_stdout=True)\n            lines = output.split(b'\\n')\n            total_size = 0\n            for line in lines:\n                parts = line.split()\n                if len(parts) == 2:\n                    size = int(parts[0])\n                    total_size += size\n            return total_size\n        except Exception as e:\n            raise IOError(f'Unexpected output from Hadoop fs -du: {output!r}')", "idx": 1529}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.mkdir", "completion": "\n        version = self.get_hadoop_version()\n\n        if uses_yarn(version):\n            args = ['fs', '-mkdir', '-p', path]\n        else:\n            args = ['fs', '-mkdir', path]\n\n        try:\n            self.invoke_hadoop(args, ok_stderr=[_HADOOP_FILE_EXISTS_RE])\n        except CalledProcessError:\n            raise IOError(\"Could not mkdir %s\" % path)", "idx": 1530}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.exists", "completion": "        try:\n            self.invoke_hadoop(['fs', '-ls', path_glob], ok_returncodes=[0, -1, 255], ok_stderr=[_HADOOP_LS_NO_SUCH_FILE])\n            return True\n        except CalledProcessError:\n            return False\n        except IOError:\n            raise IOError(\"Could not check path %s\" % path_glob)", "idx": 1531}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.rm", "completion": "        if not is_uri(path_glob):\n            super(HadoopFilesystem, self).rm(path_glob)\n            return\n\n        version = self.get_hadoop_version()\n\n        # use -r on Hadoop 2 (see #991, #845)\n        if uses_yarn(version):\n            args = ['fs', '-rm', '-r', path_glob]\n        else:\n            args = ['fs', '-rmr', path_glob]\n\n        try:\n            self.invoke_hadoop(args, ok_stderr=[_HADOOP_RM_NO_SUCH_FILE])\n        except CalledProcessError:\n            raise IOError(\"Could not remove %s\" % path_glob)", "idx": 1532}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.touchz", "completion": "        try:\n            self.invoke_hadoop(['fs', '-touchz', path])\n        except CalledProcessError:\n            raise IOError(\"Could not touchz %s\" % path)", "idx": 1533}
{"namespace": "mrjob.fs.local.LocalFilesystem.du", "completion": "        total_size = 0\n        local_path = self._to_real_path(path_glob)\n\n        for file_path in glob.glob(local_path):\n            if os.path.isfile(file_path):\n                total_size += os.path.getsize(file_path)\n\n        return total_size", "idx": 1534}
{"namespace": "mrjob.fs.local.LocalFilesystem.ls", "completion": "        path_glob = _from_file_uri(path_glob)\n        if os.path.isdir(path_glob):\n            for root, dirs, files in os.walk(path_glob):\n                for file in files:\n                    yield _to_file_uri(os.path.join(root, file))\n        else:\n            yield _to_file_uri(path_glob)", "idx": 1535}
{"namespace": "mrjob.fs.local.LocalFilesystem._cat_file", "completion": "        path = _from_file_uri(path)\n        with open(path, 'rb') as f:\n            while True:\n                chunk = f.read(8192)  # read 8KB at a time\n                if not chunk:\n                    break\n                yield chunk", "idx": 1536}
{"namespace": "mrjob.fs.local.LocalFilesystem.exists", "completion": "        path_glob = _from_file_uri(path_glob)\n        return any(os.path.exists(path) for path in glob.glob(path_glob))", "idx": 1537}
{"namespace": "mrjob.fs.local.LocalFilesystem.mkdir", "completion": "        path = _from_file_uri(path)\n        if not os.path.exists(path):\n            os.makedirs(path)", "idx": 1538}
{"namespace": "mrjob.fs.local.LocalFilesystem.put", "completion": "        src = _from_file_uri(src)\n        path = _from_file_uri(path)\n        shutil.copy(src, path)", "idx": 1539}
{"namespace": "mrjob.fs.local.LocalFilesystem.rm", "completion": "        path_glob = _from_file_uri(path_glob)\n        for path in glob.glob(path_glob):\n            if os.path.isdir(path):\n                shutil.rmtree(path)\n            else:\n                os.remove(path)", "idx": 1540}
{"namespace": "mrjob.fs.local.LocalFilesystem.touchz", "completion": "        path = _from_file_uri(path)\n        if os.path.exists(path) and os.path.getsize(path) > 0:\n            raise OSError(\"File already exists and is not empty\")\n        with open(path, 'w'):\n            pass", "idx": 1541}
{"namespace": "mrjob.fs.local.LocalFilesystem.md5sum", "completion": "        path = _from_file_uri(path)\n        with open(path, 'rb') as f:\n            return self._md5sum_file(f)", "idx": 1542}
{"namespace": "mrjob.fs.composite.CompositeFilesystem.add_fs", "completion": "        setattr(self, name, fs)\n        self._fs_names.append(name)\n        if disable_if:\n            self._disable_if[name] = disable_if", "idx": 1543}
{"namespace": "mrjob.fs.base.Filesystem.cat", "completion": "        pass  # implementation of the cat function will go here", "idx": 1544}
{"namespace": "mrjob.fs.base.Filesystem.join", "completion": "        if '://' in path:\n            scheme, netloc, base_path, _, _ = urlparse(path)\n            joined_path = posixpath.join(base_path, *paths)\n            return urlunparse((scheme, netloc, joined_path, '', '', ''))\n        else:\n            return posixpath.join(path, *paths)", "idx": 1545}
{"namespace": "mrjob.examples.mr_text_classifier.parse_doc_filename", "completion": "    filename = posixpath.basename(input_uri)\n\n    # extract the id and categories from the filename using regular expressions\n    match = re.match(r'(?P<id>.*?)-(?P<categories>.*).txt', filename)\n\n    # create a dictionary to store the parsed information\n    parsed_info = {'id': match.group('id'), 'cats': defaultdict(bool)}\n\n    # extract the categories and their corresponding boolean values\n    for category in match.group('categories').split('-'):\n        if category.startswith('not_'):\n            parsed_info['cats'][category[4:]] = False\n        else:\n            parsed_info['cats'][category] = True\n\n    return parsed_info", "idx": 1546}
{"namespace": "prometheus_client.mmap_dict.MmapedDict.read_value", "completion": "        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        _, value, _ = next(_read_all_values(self._m, self._used, pos))\n        return value", "idx": 1547}
{"namespace": "prometheus_client.mmap_dict.MmapedDict.write_value", "completion": "        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        _pack_two_doubles(self._m, pos, value, timestamp)", "idx": 1548}
{"namespace": "prometheus_client.multiprocess.MultiProcessCollector.merge", "completion": "        metrics = defaultdict(list)\n        for file in files:\n            with MmapedDict(file) as mmaped:\n                for name, metric_data in mmaped.items():\n                    metric = Metric.from_dict(json.loads(metric_data))\n                    if isinstance(metric, Gauge):\n                        metrics[name] = metric\n                    else:\n                        metrics[name].append(metric)\n        merged_metrics = {}\n        for name, metric_list in metrics.items():\n            if accumulate and isinstance(metric_list[0], Sample):\n                merged_metrics[name] = metric_list[0].add(*metric_list[1:])\n            else:\n                merged_metrics[name] = metric_list[-1]\n        return merged_metrics", "idx": 1549}
{"namespace": "prometheus_client.multiprocess.MultiProcessCollector.collect", "completion": "        files = glob.glob(os.path.join(self._path, '*.db'))\n        return self.merge(files, accumulate=True)", "idx": 1550}
{"namespace": "prometheus_client.exposition.choose_encoder", "completion": "    if accept_header and \"application/openmetrics-text\" in accept_header:\n        return generate_latest, CONTENT_TYPE_LATEST\n    else:\n        return generate_latest, CONTENT_TYPE_LATEST", "idx": 1551}
{"namespace": "flower.command.apply_options", "completion": "\n    # Filter options specific to the application\n    filtered_argv = [arg for arg in argv if not arg.startswith('--app_specific_option')]\n\n    # Parse the command line to get the \"--conf\" option\n    conf_file = None\n    for i, arg in enumerate(filtered_argv):\n        if arg == '--conf':\n            try:\n                conf_file = filtered_argv[i + 1]\n            except IndexError:\n                pass\n\n    # Parse the configuration file and update the options\n    if conf_file:\n        try:\n            parse_config_file(conf_file, final=False)\n        except IOError as e:\n            if conf_file == default_options['conf'].default:\n                logger.warning(\"Error parsing configuration file %s: %s\", conf_file, e)\n            else:\n                raise\n\n    # Parse the command line again to update the options\n    parse_command_line(filtered_argv)", "idx": 1552}
{"namespace": "trackerjacker.ieee_mac_vendor_db.MacVendorDB.lookup", "completion": "        mac = mac.upper().replace(':', '')\n        prefix = mac[:6]\n        return self.db.get(prefix, '')", "idx": 1553}
{"namespace": "trailscraper.iam.Statement.merge", "completion": "\n        if self.Effect != other.Effect:\n            raise ValueError(f\"Trying to combine two statements with differing effects: {self.Effect} {other.Effect}\")\n\n        merged_actions = sorted(list(set(self.Action + other.Action)))\n        merged_resources = sorted(list(set(self.Resource + other.Resource)))\n\n        return Statement(Action=merged_actions, Effect=self.Effect, Resource=merged_resources)", "idx": 1554}
{"namespace": "trailscraper.iam.parse_policy_document", "completion": "    if isinstance(stream, six.string_types):\n        json_data = json.loads(stream)\n    else:\n        json_data = json.load(stream)\n\n    statements = _parse_statements(json_data['Statement'])\n    version = json_data.get('Version', \"2012-10-17\")\n\n    return PolicyDocument(Statement=statements, Version=version)", "idx": 1555}
{"namespace": "trailscraper.iam.known_iam_actions", "completion": "    all_actions = all_known_iam_permissions()\n    actions_by_prefix = pipe(\n        all_actions,\n        mapz(lambda action: _parse_action(action)),\n        groupbyz(lambda action: action.prefix)\n    )\n    return [action.json_repr() for action in actions_by_prefix.get(prefix, [])]", "idx": 1556}
{"namespace": "trailscraper.boto_service_definitions.service_definition_file", "completion": "    files = boto_service_definition_files()\n    filtered_files = [file for file in files if fnmatch.fnmatch(file, '**/' + servicename + '/*/service-*.json')]\n    filtered_files.sort()\n    return filtered_files[-1]", "idx": 1557}
{"namespace": "trailscraper.boto_service_definitions.operation_definition", "completion": "\n    service_definition_path = service_definition_file(servicename)\n    with open(service_definition_path, 'r') as file:\n        service_definition = json.load(file)\n        return service_definition['operations'][operationname]", "idx": 1558}
{"namespace": "trailscraper.cloudtrail.Record.to_statement", "completion": "\n        if self.event_source == \"sts.amazonaws.com\" and self.event_name == \"GetCallerIdentity\":\n            return None\n\n        if self.event_source == \"apigateway.amazonaws.com\":\n            return self._to_api_gateway_statement()\n\n        # Add more conditions for other event sources and event names\n\n        return None  # Return None if no matching IAM Policy Statement is found", "idx": 1559}
{"namespace": "trailscraper.cloudtrail.filter_records", "completion": "    return list(filterz(_by_timeframe(from_date, to_date), filterz(_by_role_arns(arns_to_filter_for), records)))", "idx": 1560}
{"namespace": "trailscraper.record_sources.local_directory_record_source.LocalDirectoryRecordSource.load_from_dir", "completion": "        valid_log_files = self._valid_log_files()\n        records = []\n\n        for log_file in valid_log_files:\n            if log_file.contains_events_in_date_range(from_date, to_date):\n                records.extend(log_file.records())\n\n        return records", "idx": 1561}
{"namespace": "pyt.__main__.discover_files", "completion": "    included_files = []\n    excluded_files = excluded_files.split(',')\n    \n    for target in targets:\n        if os.path.isfile(target):\n            if target.endswith('.py') and target not in excluded_files:\n                included_files.append(target)\n                log.debug('Discovered file: %s' % target)\n        elif os.path.isdir(target):\n            if recursive:\n                for root, dirs, files in os.walk(target):\n                    for file in files:\n                        if file.endswith('.py') and file not in excluded_files:\n                            included_files.append(os.path.join(root, file))\n                            log.debug('Discovered file: %s' % os.path.join(root, file))\n            else:\n                for file in os.listdir(target):\n                    if file.endswith('.py') and file not in excluded_files:\n                        included_files.append(os.path.join(target, file))\n                        log.debug('Discovered file: %s' % os.path.join(target, file))\n    \n    return included_files", "idx": 1562}
{"namespace": "pyt.core.project_handler.get_directory_modules", "completion": "    global _local_modules\n\n    if _local_modules and directory == os.path.dirname(_local_modules[0][1]):\n        return _local_modules\n\n    if not os.path.isdir(directory):\n        directory = os.path.dirname(directory)\n\n    for file in os.listdir(directory):\n        if file.endswith(\".py\"):\n            module_name = os.path.splitext(file)[0]\n            module_path = os.path.join(directory, file)\n            _local_modules.append((module_name, module_path))\n\n    return _local_modules", "idx": 1563}
{"namespace": "pyt.vulnerabilities.vulnerabilities.find_triggers", "completion": "    trigger_nodes = []\n    for node in nodes:\n        if node.lineno not in nosec_lines:\n            for word in trigger_words:\n                if word.label in node.label:\n                    trigger_nodes.append(TriggerNode(word, cfg_node=node))\n    return trigger_nodes", "idx": 1564}
{"namespace": "pyt.vulnerabilities.vulnerabilities.label_contains", "completion": "    for trigger in triggers:\n        if trigger.label in node.label:\n            yield TriggerNode(trigger, node)", "idx": 1565}
{"namespace": "pyt.vulnerabilities.vulnerabilities.build_sanitiser_node_dict", "completion": "    sanitiser_node_dict = defaultdict(list)\n    for sink_node in sinks_in_file:\n        sanitiser = sink_node.trigger_word.sanitiser\n        if sanitiser:\n            sanitiser_nodes = find_sanitiser_nodes(cfg, sanitiser)\n            sanitiser_node_dict[sanitiser].extend(sanitiser_nodes)\n    return sanitiser_node_dict\n\n", "idx": 1566}
{"namespace": "pyt.vulnerabilities.trigger_definitions_parser.parse", "completion": "    with open(trigger_word_file, 'r') as file:\n        data = json.load(file)\n\n    sources = []\n    sinks = []\n\n    for key, value in data.items():\n        if 'trigger' in value:\n            source = Source(value['trigger'])\n            sources.append(source)\n        else:\n            sink = Sink.from_json(key, value)\n            sinks.append(sink)\n\n    return Definitions(sources, sinks)", "idx": 1567}
{"namespace": "principalmapper.querying.local_policy_simulation._statement_matches_resource", "completion": "\n    if 'Resource' in statement:\n        for item in _listify_string(statement['Resource']):\n            if _matches_after_expansion(resource, item, condition_keys):\n                return True\n        return False\n    elif 'NotResource' in statement:\n        for item in _listify_string(statement['NotResource']):\n            if _matches_after_expansion(resource, item, condition_keys):\n                return False\n        return True\n    else:\n        return True", "idx": 1568}
{"namespace": "principalmapper.querying.local_policy_simulation._matches_after_expansion", "completion": "    \"\"\"\n    This function is a helper function that checks if a given string matches another string based on certain conditions. It handles matching with respect to wildcards, variables, and regular expressions, like replace a '${' + key + '}' pattern to value in condition_keys.\n    Input-Output Arguments\n    :param string_to_check: str. The string that needs to be checked.\n    :param string_to_check_against: str. The string that the first string is checked against.\n    :param condition_keys: Optional[CaseInsensitiveDict]. A dictionary of condition keys and their corresponding values. These values can be used for variable substitution in the second string. Defaults to None.\n    :return: bool. True if the first string matches the second string based on the conditions, False otherwise.\n    \"\"\"\n    if condition_keys is not None:\n        for key, value in condition_keys.items():\n            string_to_check_against = string_to_check_against.replace('${' + key + '}', value)\n\n    pattern = _compose_pattern(string_to_check_against)\n    return bool(pattern.match(string_to_check))", "idx": 1569}
{"namespace": "passpie.database.PasspieStorage.delete", "completion": "        for credential in credentials:\n            credpath = self.make_credpath(credential[\"name\"], credential[\"login\"])\n            if os.path.exists(credpath):\n                os.remove(credpath)\n                dirname = os.path.dirname(credpath)\n                if not os.listdir(dirname):\n                    os.rmdir(dirname)", "idx": 1570}
{"namespace": "passpie.database.PasspieStorage.read", "completion": "        data = {}\n        for root, dirs, files in os.walk(self.path):\n            for file in files:\n                if file.endswith(self.extension):\n                    filepath = os.path.join(root, file)\n                    with open(filepath, 'r') as f:\n                        content = f.read()\n                        try:\n                            parsed_content = yaml.safe_load(content)\n                            data[len(data) + 1] = parsed_content\n                        except yaml.YAMLError as exc:\n                            logging.error(f\"Error reading file {filepath}: {exc}\")\n        return data", "idx": 1571}
{"namespace": "threatingestor.state.State.save_state", "completion": "        try:\n            self.cursor.execute('INSERT OR REPLACE INTO states (name, state) VALUES (?, ?)', (name, state))\n            self.conn.commit()\n        except sqlite3.Error:\n            logger.error(\"Failed to save state to database\")\n            raise threatingestor.exceptions.IngestorError(\"Failed to save state to database\")", "idx": 1572}
{"namespace": "threatingestor.state.State.get_state", "completion": "        try:\n            self.cursor.execute('SELECT state FROM states WHERE name = ?', (name,))\n            result = self.cursor.fetchone()\n            if result:\n                return result[0]\n            else:\n                return None\n        except sqlite3.Error:\n            raise threatingestor.exceptions.IngestorError(\"Error retrieving state from database\")", "idx": 1573}
{"namespace": "threatingestor.Ingestor.run", "completion": "        if self.config.run_forever():\n            while True:\n                self._run_once()\n                time.sleep(self.config.polling_interval())\n        else:\n            self._run_once()", "idx": 1574}
{"namespace": "msticpy.analysis.anomalous_sequence.model.Model.compute_scores", "completion": "        if use_start_end_tokens:\n            sessions = [[self.start_token] + ses + [self.end_token] for ses in self.sessions]\n        else:\n            sessions = self.sessions\n\n        self.session_likelihoods = []\n        self.session_geomean_likelihoods = []\n\n        self.rare_windows = defaultdict(list)\n        self.rare_window_likelihoods = defaultdict(list)\n\n        self.rare_windows_geo = defaultdict(list)\n        self.rare_window_likelihoods_geo = defaultdict(list)\n\n        for ses in sessions:\n            likelihoods = []\n            for i in range(len(ses) - 1):\n                seq1 = (ses[i],)\n                seq2 = (ses[i], ses[i + 1])\n                likelihood = self.trans_probs.get(seq2, 0) / self.seq1_counts.get(seq1, 1)\n                likelihoods.append(likelihood)\n\n                if i < len(ses) - 2:\n                    seq3 = (ses[i], ses[i + 1], ses[i + 2])\n                    rare_window_likelihood = self.trans_probs.get(seq3, 0) / self.seq2_counts.get(seq2, 1)\n                    self.rare_windows[len(seq3)].append(seq3)\n                    self.rare_window_likelihoods[len(seq3)].append(rare_window_likelihood)\n\n            self.session_likelihoods.append(likelihoods)\n            self.session_geomean_likelihoods.append(np.exp(np.mean(np.log(likelihoods))))\n\n        for window_len in self.rare_windows:\n            self.rare_windows_geo[window_len] = self.rare_windows[window_len]\n            self.rare_window_likelihoods_geo[window_len] = np.exp(np.mean(np.log(self.rare_window_likelihoods[window_len])))", "idx": 1575}
{"namespace": "msticpy.analysis.anomalous_sequence.model.Model.compute_rarest_windows", "completion": "        if self.session_likelihoods is None:\n            self.compute_likelihoods_of_sessions(use_start_end_tokens=use_start_end_tokens)\n\n        result_windows = {}\n        result_likelihoods = {}\n\n        for idx, session in enumerate(self.sessions):\n            if use_start_end_tokens:\n                session = [self.start_token] + session + [self.end_token]\n            rarest_window, rarest_likelihood = self._find_rarest_window(\n                session, window_len, use_geo_mean, idx\n            )\n            result_windows[idx] = rarest_window\n            result_likelihoods[idx] = rarest_likelihood\n\n        if use_geo_mean:\n            self.rare_windows_geo[window_len] = result_windows\n            self.rare_window_likelihoods_geo[window_len] = result_likelihoods\n        else:\n            self.rare_windows[window_len] = result_windows\n            self.rare_window_likelihoods[window_len] = result_likelihoods", "idx": 1576}
{"namespace": "msticpy.analysis.anomalous_sequence.anomalous.score_sessions", "completion": "    # Group the data by session and create a list of all the transitions within each session\n    transitions = data.groupby(session_column).apply(lambda x: list(zip(x[session_column].values, x[session_column].shift(-1).values)))\n\n    # Create a transition matrix for each session\n    transition_matrices = transitions.apply(lambda x: pd.crosstab(pd.Series(x).apply(lambda x: x[0]), pd.Series(x).apply(lambda x: x[1])))\n\n    # Compute the likelihood metrics for each session based on the specified window length\n    likelihood_metrics = transition_matrices.apply(lambda x: x.rolling(window=window_length, min_periods=1).sum().max(axis=1))\n\n    # Append the computed likelihood and the rarest window to the input DataFrame\n    data['likelihood'] = likelihood_metrics.max(axis=1)\n    data['rarest_window'] = likelihood_metrics.idxmax(axis=1)\n\n    return data", "idx": 1577}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.laplace_smooth_counts", "completion": "    # Apply Laplace smoothing to the counts of commands and parameters\n    # Add 1 to each count to shift some probability mass from very probable commands/parameters\n    # to unseen and unlikely commands/parameters\n    # Handle unseen commands, sequences of commands, and parameters using the `unk_token`\n\n    # Create a deep copy of the original counts\n    seq1_counts_sm = copy.deepcopy(seq1_counts)\n    seq2_counts_sm = copy.deepcopy(seq2_counts)\n    param_counts_sm = copy.deepcopy(param_counts)\n    cmd_param_counts_sm = copy.deepcopy(cmd_param_counts)\n\n    # Add 1 to each count to apply Laplace smoothing\n    for key in seq1_counts_sm:\n        seq1_counts_sm[key] += 1\n    for key1 in seq2_counts_sm:\n        for key2 in seq2_counts_sm[key1]:\n            seq2_counts_sm[key1][key2] += 1\n    for key in param_counts_sm:\n        param_counts_sm[key] += 1\n    for key1 in cmd_param_counts_sm:\n        for key2 in cmd_param_counts_sm[key1]:\n            cmd_param_counts_sm[key1][key2] += 1\n\n    # Handle unseen commands, sequences of commands, and parameters using the `unk_token`\n    seq1_counts_sm[unk_token] = 1\n    seq2_counts_sm[unk_token] = defaultdict(lambda: 1)\n    param_counts_sm[unk_token] = 1\n    for key in cmd_param_counts_sm:\n        cmd_param_counts_sm[key][unk_token] = 1\n\n    return seq1_counts_sm, seq2_counts_sm, param_counts_sm, cmd_param_counts_sm", "idx": 1578}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.compute_likelihood_window", "completion": "\n    likelihood = 1.0\n    prev_cmd = start_token if use_start_token else None\n\n    for cmd in window:\n        if prev_cmd is not None:\n            trans_prob = trans_probs[prev_cmd][cmd.name]\n            likelihood *= trans_prob\n        cmd_prob = prior_probs[cmd.name]\n        likelihood *= cmd_prob\n        param_likelihood = compute_prob_setofparams_given_cmd(\n            cmd.name, cmd.params, param_cond_cmd_probs\n        )\n        likelihood *= param_likelihood\n        prev_cmd = cmd.name\n\n    if use_end_token:\n        end_trans_prob = trans_probs[prev_cmd][end_token]\n        likelihood *= end_trans_prob\n\n    return likelihood", "idx": 1579}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.compute_likelihood_windows_in_session", "completion": "    likelihoods = []\n    if use_start_end_tokens:\n        if start_token is None or end_token is None:\n            raise ValueError(\"Both start_token and end_token must be provided when use_start_end_tokens is True.\")\n        session = [Cmd(name=start_token, params=set())] + session + [Cmd(name=end_token, params=set())]\n\n    for i in range(len(session) - window_len + 1):\n        window = session[i:i + window_len]\n        likelihood = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            use_start_token=False,\n            use_end_token=False,\n            start_token=None,\n            end_token=None\n        )\n        if use_geo_mean:\n            likelihood = likelihood ** (1 / window_len)\n        likelihoods.append(likelihood)\n\n    return likelihoods", "idx": 1580}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.rarest_window_session", "completion": "\n    likelihoods = compute_likelihood_windows_in_session(\n        session=session,\n        prior_probs=prior_probs,\n        trans_probs=trans_probs,\n        param_cond_cmd_probs=param_cond_cmd_probs,\n        window_len=window_len,\n        use_start_end_tokens=use_start_end_tokens,\n        start_token=start_token,\n        end_token=end_token,\n        use_geo_mean=use_geo_mean,\n    )\n\n    rarest_window_index = likelihoods.index(min(likelihoods))\n    rarest_window = session[rarest_window_index : rarest_window_index + window_len]\n\n    return rarest_window, likelihoods[rarest_window_index]", "idx": 1581}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.compute_likelihood_window", "completion": "    if start_token is None or end_token is None:\n        raise ValueError(\"start_token and end_token must be provided.\")\n\n    if use_start_token:\n        window = [start_token] + window\n    if use_end_token:\n        window = window + [end_token]\n\n    likelihood = 1.0\n    for i in range(len(window) - 1):\n        cmd1 = window[i]\n        cmd2 = window[i + 1]\n\n        if isinstance(prior_probs, dict):\n            prior_prob_cmd1 = prior_probs.get(cmd1, 0)\n            prior_prob_cmd2 = prior_probs.get(cmd2, 0)\n        else:\n            prior_prob_cmd1 = prior_probs.get_state_prob(cmd1)\n            prior_prob_cmd2 = prior_probs.get_state_prob(cmd2)\n\n        if isinstance(trans_probs, dict):\n            trans_prob = trans_probs.get(cmd1, {}).get(cmd2, 0)\n        else:\n            trans_prob = trans_probs.get_state_prob(cmd1, cmd2)\n\n        likelihood *= prior_prob_cmd1 * trans_prob\n\n    return likelihood", "idx": 1582}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.compute_likelihood_windows_in_session", "completion": "    likelihoods = []\n    for i in range(len(session) - window_len + 1):\n        window = session[i:i + window_len]\n        likelihood = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            use_start_token=use_start_end_tokens,\n            use_end_token=use_start_end_tokens,\n            start_token=start_token,\n            end_token=end_token\n        )\n        if use_geo_mean:\n            likelihood = likelihood ** (1 / window_len)\n        likelihoods.append(likelihood)\n    return likelihoods", "idx": 1583}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.rarest_window_session", "completion": "\n    likelihoods = compute_likelihood_windows_in_session(\n        session=session,\n        prior_probs=prior_probs,\n        trans_probs=trans_probs,\n        window_len=window_len,\n        use_start_end_tokens=use_start_end_tokens,\n        start_token=start_token,\n        end_token=end_token,\n        use_geo_mean=use_geo_mean,\n    )\n\n    rarest_window_idx = np.argmin(likelihoods)\n    rarest_window = session[rarest_window_idx : rarest_window_idx + window_len]\n\n    return rarest_window, likelihoods[rarest_window_idx]", "idx": 1584}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.get_params_to_model_values", "completion": "    if isinstance(param_counts, dict):\n        param_counts = StateMatrix(states=param_counts)\n    if isinstance(param_value_counts, dict):\n        param_value_counts = StateMatrix(states=param_value_counts)\n\n    params_to_model = set()\n    for param, value_counts in param_value_counts.states.items():\n        if len(value_counts) > 1 and len(value_counts) < 10:\n            params_to_model.add(param)\n\n    return params_to_model", "idx": 1585}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_prob_setofparams_given_cmd", "completion": "    # Initialize the likelihood\n    likelihood = 1.0\n\n    # Loop through each parameter and its value\n    for param, val in params_with_vals.items():\n        # Check if the parameter is modellable\n        if param in modellable_params:\n            # If the parameter is modellable, calculate the probability of the value given the parameter\n            val_prob = value_cond_param_probs.get(param, {}).get(val, 0)\n            # Multiply the likelihood by the probability of the value given the parameter\n            likelihood *= val_prob\n\n        # Calculate the probability of the parameter given the command\n        param_prob = param_cond_cmd_probs.get(cmd, {}).get(param, 0)\n        # Multiply the likelihood by the probability of the parameter given the command\n        likelihood *= param_prob\n\n    # If use_geo_mean is True, raise the likelihood to the power of (1/K)\n    if use_geo_mean:\n        # Calculate the number of distinct parameters that appeared for the given command across the training set\n        num_distinct_params = len(param_cond_cmd_probs.get(cmd, {}))\n        # Calculate the number of values included in the modeling for this command\n        num_values_in_model = sum(len(value_cond_param_probs.get(param, {})) for param in modellable_params)\n        # Calculate the exponent\n        exponent = 1 / (num_distinct_params + num_values_in_model)\n        # Raise the likelihood to the power of the exponent\n        likelihood = likelihood ** exponent\n\n    return likelihood", "idx": 1586}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_likelihood_window", "completion": "    if use_start_token:\n        window = [Cmd(name=start_token, params={})] + window\n    if use_end_token:\n        window = window + [Cmd(name=end_token, params={})]\n\n    likelihood = 1.0\n    for idx, cmd in enumerate(window):\n        if idx == 0:\n            continue\n        prior_prob = prior_probs[window[idx - 1].name]\n        trans_prob = trans_probs[window[idx - 1].name][cmd.name]\n        param_cond_cmd_prob = compute_prob_setofparams_given_cmd(\n            cmd.name,\n            cmd.params,\n            param_cond_cmd_probs,\n            value_cond_param_probs,\n            modellable_params,\n        )\n        likelihood *= prior_prob * trans_prob * param_cond_cmd_prob\n\n    return likelihood", "idx": 1587}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_likelihood_windows_in_session", "completion": "    likelihoods = []\n    if use_start_end_tokens:\n        if start_token is None or end_token is None:\n            raise MsticpyException(\n                \"start_token and end_token should not be None, when use_start_end_tokens is True\"\n            )\n\n    if window_len <= 0:\n        raise MsticpyException(\"window_len should be greater than 0\")\n\n    if len(session) < window_len:\n        return likelihoods\n\n    for i in range(len(session) - window_len + 1):\n        window = session[i : i + window_len]\n        likelihood = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            value_cond_param_probs=value_cond_param_probs,\n            modellable_params=modellable_params,\n            use_start_token=use_start_end_tokens,\n            use_end_token=use_start_end_tokens,\n            start_token=start_token,\n            end_token=end_token,\n        )\n        if use_geo_mean:\n            likelihood = likelihood ** (1 / window_len)\n        likelihoods.append(likelihood)\n\n    return likelihoods", "idx": 1588}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.rarest_window_session", "completion": "\n    likelihoods = compute_likelihood_windows_in_session(\n        session=session,\n        prior_probs=prior_probs,\n        trans_probs=trans_probs,\n        param_cond_cmd_probs=param_cond_cmd_probs,\n        value_cond_param_probs=value_cond_param_probs,\n        modellable_params=modellable_params,\n        window_len=window_len,\n        use_start_end_tokens=use_start_end_tokens,\n        start_token=start_token,\n        end_token=end_token,\n        use_geo_mean=use_geo_mean,\n    )\n\n    rarest_window_index = likelihoods.index(min(likelihoods))\n    rarest_window = session[rarest_window_index : rarest_window_index + window_len]\n    rarest_likelihood = likelihoods[rarest_window_index]\n\n    return rarest_window, rarest_likelihood", "idx": 1589}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.probabilities.compute_cmds_probs", "completion": "    if isinstance(seq1_counts, dict):\n        seq1_counts = StateMatrix(seq1_counts)\n    if isinstance(seq2_counts, dict):\n        seq2_counts = StateMatrix(seq2_counts)\n\n    cmds_probs = seq1_counts.normalize(unk_token)\n    cmds_trans_probs = seq2_counts.normalize(unk_token)\n\n    return cmds_probs, cmds_trans_probs", "idx": 1590}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.probabilities.compute_values_probs", "completion": "    value_probs: DefaultDict[str, float] = defaultdict(lambda: 0)\n    value_cond_param_probs: DefaultDict[str, DefaultDict[str, float]] = defaultdict(\n        lambda: defaultdict(lambda: 0)\n    )\n\n    for param, values in param_value_counts.items():\n        n_param = sum(values.values())\n        for value, count in values.items():\n            value_cond_param_probs[param][value] = count / n_param\n\n    tot_values = sum(value_counts.values())\n    for value, count in value_counts.items():\n        value_probs[value] = count / tot_values\n\n    value_probs_sm = StateMatrix(states=value_probs, unk_token=unk_token)\n    value_cond_param_probs_sm = StateMatrix(\n        states=value_cond_param_probs, unk_token=unk_token\n    )\n\n    return value_probs_sm, value_cond_param_probs_sm", "idx": 1591}
{"namespace": "msticpy.auth.msal_auth.MSALDelegatedAuth.get_token", "completion": "\n        chosen_account = self.app.get_accounts(username=self.username)\n        if chosen_account:\n            self.result = self.app.acquire_token_silent(self.scopes, account=chosen_account[0])\n        else:\n            if self.auth_type == \"interactive\":\n                self.result = self.app.acquire_token_interactive(scopes=self.scopes, username=self.username)\n            elif self.auth_type == \"client_secret\":\n                # Add code to authenticate using client secret\n                pass\n\n        if \"access_token\" in self.result:\n            self.result = self.app.acquire_token_silent(scopes=self.scopes, account=chosen_account[0])\n        else:\n            self.result = self.app.acquire_token_silent(scopes=self.scopes, account=chosen_account[0], force_refresh=True)", "idx": 1592}
{"namespace": "msticpy.config.query_editor.QueryParameterEditWidget.save_parameter", "completion": "        param_name = self.parameter_name_widget.value\n        param_description = self.description_widget.value\n        param_datatype = self.type_widget.value\n        param_default = self.default_widget.value if self.default_reqd_widget.value else None\n\n        new_parameter = QueryParameter(\n            name=param_name,\n            description=param_description,\n            datatype=param_datatype,\n            default=param_default,\n        )\n\n        if isinstance(self.param_container, Query):\n            self.param_container.parameters[param_name] = new_parameter\n        elif isinstance(self.param_container, QueryDefaults):\n            self.param_container.parameters[param_name] = new_parameter\n\n        self.parameter_dropdown.options = list(self.param_container.parameters.keys())\n        self.parameter_dropdown.value = param_name\n        self._changed_data = True", "idx": 1593}
{"namespace": "msticpy.config.query_editor.QueryParameterEditWidget.delete_parameter", "completion": "        del button\n        param_name = self.parameter_name_widget.value\n        if param_name in self.param_container.parameters:\n            del self.param_container.parameters[param_name]\n            self.parameter_dropdown.options = list(self.param_container.parameters.keys())\n            self._blank_parameter()\n            self._changed_data = True", "idx": 1594}
{"namespace": "mrjob.util.save_cwd", "completion": "    original_cwd = os.getcwd()\n\n    try:\n        yield\n    finally:\n        os.chdir(original_cwd)", "idx": 1595}
{"namespace": "mrjob.util.save_sys_std", "completion": "    original_stdin = sys.stdin\n    original_stdout = sys.stdout\n    original_stderr = sys.stderr\n\n    try:\n        yield\n    finally:\n        sys.stdin.flush()\n        sys.stdout.flush()\n        sys.stderr.flush()\n        sys.stdin = original_stdin\n        sys.stdout = original_stdout\n        sys.stderr = original_stderr", "idx": 1596}
{"namespace": "mrjob.util.unarchive", "completion": "    if tarfile.is_tarfile(archive_path):\n        with tarfile.open(archive_path, 'r:*') as tar:\n            tar.extractall(dest)\n    elif zipfile.is_zipfile(archive_path):\n        with zipfile.ZipFile(archive_path, 'r') as zip_ref:\n            zip_ref.extractall(dest)", "idx": 1597}
{"namespace": "mrjob.util.unique", "completion": "    seen = set()\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            yield item", "idx": 1598}
{"namespace": "mrjob.parse.urlparse", "completion": "    return urlparse_buggy(urlstring, scheme, allow_fragments, *args, **kwargs)", "idx": 1599}
{"namespace": "mrjob.util.which", "completion": "    if path is None:\n        path = os.environ.get('PATH', '')\n\n    paths = path.split(os.pathsep)\n\n    for p in paths:\n        p = os.path.join(p, cmd)\n        if os.path.exists(p) and os.access(p, os.X_OK):\n            return p\n\n    return None", "idx": 1600}
{"namespace": "sshuttle.ssh.parse_hostport", "completion": "    if not rhostport:\n        return (None, None, None, None)\n\n    # Parse the rhostport\n    pattern = re.compile(r'^(?:(?P<username>[^:]+)(?::(?P<password>[^@]+))?@)?(?P<host>[^:]+)(?::(?P<port>\\d+))?$')\n    match = pattern.match(rhostport)\n\n    if match:\n        username = match.group('username')\n        password = match.group('password')\n        host = match.group('host')\n        port = match.group('port')\n        return (username, password, port, host)\n    else:\n        return (None, None, None, None)", "idx": 1601}
{"namespace": "flower.utils.search.stringified_dict_contains_value", "completion": "    try:\n        # Convert the string representation of the dictionary to an actual dictionary\n        dict_obj = eval(str_dict)\n        # Check if the key and value pair exists in the dictionary\n        return key in dict_obj and dict_obj[key] == value\n    except (SyntaxError, NameError):\n        return False", "idx": 1602}
{"namespace": "flower.utils.abs_path", "completion": "    expanded_path = os.path.expanduser(path)\n    if not os.path.isabs(expanded_path):\n        abs_path = os.path.abspath(expanded_path)\n    else:\n        abs_path = expanded_path\n    return abs_path", "idx": 1603}
{"namespace": "flower.utils.strtobool", "completion": "    true_values = ['y', 'yes', 't', 'true', 'on', '1']\n    false_values = ['n', 'no', 'f', 'false', 'off', '0']\n\n    if val.lower() in true_values:\n        return 1\n    elif val.lower() in false_values:\n        return 0\n    else:\n        raise ValueError(\"Input string is neither a true value nor a false value\")", "idx": 1604}
{"namespace": "sshuttle.methods.get_method", "completion": "    try:\n        module = importlib.import_module(f\"sshuttle.methods.{method_name}\")\n        Method = getattr(module, \"Method\")\n        return Method()\n    except (ImportError, AttributeError):\n        raise Fatal(f\"Method {method_name} not found or does not contain a Method class.\")", "idx": 1605}
{"namespace": "trailscraper.iam.all_known_iam_permissions", "completion": "    current_dir = os.path.dirname(os.path.realpath(__file__))\n    file_path = os.path.join(current_dir, 'known-iam-actions.txt')\n    with open(file_path, 'r') as file:\n        return set(file.readlines())", "idx": 1606}
{"namespace": "trailscraper.cloudtrail.parse_records", "completion": "    return list(filterz(lambda x: x is not None, map(_parse_record, json_records)))", "idx": 1607}
{"namespace": "pycoin.satoshi.IntStreamer.IntStreamer.int_to_script_bytes", "completion": "        if v == 0:\n            return b''\n        is_negative = v < 0\n        if is_negative:\n            v = -v\n        b = bytearray()\n        while v:\n            b.append(v & 0xff)\n            v >>= 8\n        if b[-1] & 0x80:\n            if is_negative:\n                b.append(0x80)\n            else:\n                b.append(0)\n        elif is_negative:\n            b[-1] |= 0x80\n        return bytes(b)", "idx": 1608}
{"namespace": "pycoin.satoshi.stackops.do_OP_2DROP", "completion": "    if len(stack) < 2:\n        raise ScriptError(\"OP_2DROP requires at least two items on the stack\", errno.INVALID_STACK_OPERATION)\n    stack.pop()\n    stack.pop()", "idx": 1609}
{"namespace": "pycoin.satoshi.stackops.do_OP_2DUP", "completion": "    if len(stack) < 2:\n        raise ScriptError(\"OP_2DUP requires at least 2 elements on the stack\", errno.INVALID_STACK_OPERATION)\n    x1, x2 = stack[-2], stack[-1]\n    stack.extend([x1, x2])", "idx": 1610}
{"namespace": "pycoin.satoshi.stackops.do_OP_3DUP", "completion": "    stack.append(stack[-3])\n    stack.append(stack[-3])\n    stack.append(stack[-3])", "idx": 1611}
{"namespace": "trailscraper.s3_download._s3_key_prefixes", "completion": "    delta = to_date - from_date\n    date_list = [from_date + datetime.timedelta(days=i) for i in range(delta.days + 1)]\n    s3_key_prefixes = []\n\n    for org_id in org_ids:\n        for account_id in account_ids:\n            for region in regions:\n                for date in date_list:\n                    if org_id:\n                        s3_key_prefixes.append(_s3_key_prefix_for_org_trails(prefix, date, org_id, account_id, region))\n                    else:\n                        s3_key_prefixes.append(_s3_key_prefix(prefix, date, account_id, region))\n\n    return s3_key_prefixes", "idx": 1612}
{"namespace": "pycoin.satoshi.stackops.do_OP_2OVER", "completion": "    stack.append(stack[-4])\n    stack.append(stack[-3])", "idx": 1613}
{"namespace": "pycoin.satoshi.stackops.do_OP_2SWAP", "completion": "    stack[-4], stack[-3], stack[-2], stack[-1] = stack[-2], stack[-1], stack[-4], stack[-3]", "idx": 1614}
{"namespace": "pycoin.satoshi.stackops.do_OP_IFDUP", "completion": "    if stack and stack[-1] != 0:\n        stack.append(stack[-1])", "idx": 1615}
{"namespace": "pycoin.satoshi.stackops.do_OP_NIP", "completion": "    stack.pop(-2)", "idx": 1616}
{"namespace": "pycoin.satoshi.stackops.do_OP_TUCK", "completion": "    top = stack.pop()\n    second = stack.pop()\n    stack.append(top)\n    stack.append(second)\n    stack.append(top)", "idx": 1617}
{"namespace": "pycoin.satoshi.stackops.do_OP_CAT", "completion": "    v1 = stack.pop()\n    v2 = stack.pop()\n    stack.append(v2 + v1)", "idx": 1618}
{"namespace": "pycoin.crack.ecdsa.crack_secret_exponent_from_k", "completion": "    from Crypto.Util.number import inverse\n    secret_exponent = ((sig - signed_value) * inverse(k, generator-1)) % (generator-1)\n    return secret_exponent", "idx": 1619}
{"namespace": "pycoin.crack.ecdsa.crack_k_from_sigs", "completion": "    r1, s1 = sig1\n    r2, s2 = sig2\n    k = ((val1 - val2) * generator.inverse(s1 - s2)) % generator.order()\n    return k", "idx": 1620}
{"namespace": "pycoin.message.make_parser_and_packer.standard_streamer", "completion": "    streamer = Streamer(parse_satoshi_int=parse_satoshi_int)\n    for type_code, (parse_f, stream_f) in parsing_functions:\n        streamer.register(type_code, parse_f, stream_f)\n    return streamer", "idx": 1621}
{"namespace": "pycoin.key.subpaths.subpaths_for_path_range", "completion": "    path_parts = path_range.split('/')\n    ranges = path_parts[-1].split('-')\n    for i in range(int(ranges[0]), int(ranges[1])+1):\n        yield '/'.join(path_parts[:-1] + [str(i)])", "idx": 1622}
{"namespace": "pyt.core.project_handler._is_python_file", "completion": "    return path.endswith('.py')", "idx": 1623}
{"namespace": "pycoin.encoding.hexbytes.h2b", "completion": "    return binascii.unhexlify(h)", "idx": 1624}
{"namespace": "zxcvbn.scoring.calc_average_degree", "completion": "    total_degree = 0\n    num_nodes = len(graph)\n\n    for node in graph:\n        total_degree += len(graph[node])\n\n    average_degree = total_degree / num_nodes\n    return average_degree", "idx": 1625}
{"namespace": "zxcvbn.scoring.nCk", "completion": "    if k < 0 or k > n:\n        return 0\n    return factorial(n) // (factorial(k) * factorial(n - k))", "idx": 1626}
{"namespace": "zxcvbn.matching.relevant_l33t_subtable", "completion": "    relevant_subs = {}\n    for letter in password:\n        if letter in table:\n            relevant_subs[letter] = table[letter]\n    return relevant_subs", "idx": 1627}
{"namespace": "zxcvbn.matching.translate", "completion": "    translated_string = \"\"\n    for char in string:\n        if char in chr_map:\n            translated_string += chr_map[char]\n        else:\n            translated_string += char\n    return translated_string", "idx": 1628}
{"namespace": "tools.cgrep.get_nets", "completion": "  results = []\n  for obj in objects:\n    try:\n      nets = db.GetNetParents(obj)\n      results.append((obj, nets))\n    except naming.UndefinedAddressError:\n      logging.info(\"Network group '%s' is not defined!\", obj)\n  return results", "idx": 1629}
{"namespace": "tools.cgrep.get_ports", "completion": "  results = []\n  for svc in svc_group:\n    ports = db.GetService(svc)\n    for port in ports:\n      results.append((svc, f\"{port.port}/{port.protocol}\"))\n  return results", "idx": 1630}
{"namespace": "tools.cgrep.compare_ip_token", "completion": "  results = []\n  for ip in options.ip:\n    try:\n      nets = get_nets([options.token], db)\n      for net in nets:\n        if ip in net[1]:\n          results.append(f\"IP {ip} is in network object {options.token}\")\n        else:\n          results.append(f\"IP {ip} is not in network object {options.token}\")\n    except naming.UndefinedAddressError:\n      results.append(f\"Network group '{options.token}' is not defined!\")\n  return '\\n'.join(results)", "idx": 1631}
{"namespace": "tools.cgrep.get_services", "completion": "  port, protocol = options.port\n  results = []\n  for svc in db.service:\n    if port in db.service[svc] and protocol in db.service[svc][port]:\n      results.append(svc)\n  return port, protocol, results", "idx": 1632}
{"namespace": "asyncssh.packet.String", "completion": "    if isinstance(value, str):\n        value = value.encode('utf-8')\n    length = len(value)\n    return UInt32(length) + value", "idx": 1633}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_cmd_counts", "completion": "    # Create deep copies of the input counts to avoid modifying the original data\n    smoothed_seq1_counts = copy.deepcopy(seq1_counts)\n    smoothed_seq2_counts = copy.deepcopy(seq2_counts)\n\n    # Apply Laplace smoothing by adding 1 to each count, including the unk_token\n    for cmd in smoothed_seq1_counts:\n        smoothed_seq1_counts[cmd] += 1\n    for cmd1 in smoothed_seq2_counts:\n        for cmd2 in smoothed_seq2_counts[cmd1]:\n            smoothed_seq2_counts[cmd1][cmd2] += 1\n\n    # Add Laplace smoothing for start_token and end_token\n    smoothed_seq1_counts[start_token] = 1\n    smoothed_seq1_counts[end_token] = 1\n    for cmd in smoothed_seq2_counts:\n        smoothed_seq2_counts[start_token][cmd] = 1\n        smoothed_seq2_counts[cmd][end_token] = 1\n\n    return smoothed_seq1_counts, smoothed_seq2_counts", "idx": 1634}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_param_counts", "completion": "    param_counts_ls = copy.deepcopy(param_counts)\n    cmd_param_counts_ls = copy.deepcopy(cmd_param_counts)\n\n    for cmd in cmds:\n        for param in cmd_param_counts_ls[cmd]:\n            param_counts_ls[param] += 1\n            cmd_param_counts_ls[cmd][param] += 1\n\n    param_counts_ls[unk_token] += 1\n    for param in param_counts_ls:\n        param_counts_ls[param] += 1\n\n    return param_counts_ls, cmd_param_counts_ls", "idx": 1635}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_value_counts", "completion": "    value_counts_ls = copy.deepcopy(value_counts)\n    param_value_counts_ls = copy.deepcopy(param_value_counts)\n\n    values: List[str] = list(value_counts.keys()) + [unk_token]\n    for param in params:\n        for value in values:\n            if value in param_value_counts_ls[param] or value == unk_token:\n                value_counts_ls[value] += 1\n                param_value_counts_ls[param][value] += 1\n\n    return value_counts_ls, param_value_counts_ls", "idx": 1636}
{"namespace": "diffprivlib.validation.check_epsilon_delta", "completion": "    if not isinstance(epsilon, Real) or not isinstance(delta, Real):\n        raise TypeError(\"Epsilon and delta must be numeric\")\n\n    if epsilon < 0:\n        raise ValueError(\"Epsilon must be non-negative\")\n\n    if not 0 <= delta <= 1:\n        raise ValueError(\"Delta must be in [0, 1]\")\n\n    if epsilon == 0 and delta == 0 and not allow_zero:\n        raise ValueError(\"Epsilon and Delta cannot both be zero\")", "idx": 1637}
{"namespace": "diffprivlib.utils.check_random_state", "completion": "    if seed is None:\n        return secrets.SystemRandom() if secure else np.random.mtrand._rand\n    if isinstance(seed, (int, np.integer)):\n        return np.random.RandomState(seed)\n    if isinstance(seed, np.random.RandomState):\n        return seed\n    if isinstance(seed, secrets.SystemRandom):\n        return seed\n    raise ValueError(\"Invalid seed\")", "idx": 1638}
{"namespace": "diffprivlib.validation.clip_to_norm", "completion": "    if not isinstance(array, np.ndarray):\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\n\n    if array.ndim != 2:\n        raise ValueError(f\"Input array must be 2-dimensional, got {array.ndim} dimensions.\")\n\n    if not isinstance(clip, Real):\n        raise TypeError(f\"Clip value must be numeric, got {type(clip)}.\")\n\n    if clip <= 0:\n        raise ValueError(f\"Clip value must be strictly positive, got {clip}.\")\n\n    norms = np.linalg.norm(array, axis=1)\n    scale = np.minimum(1.0, clip / norms)\n    return array * scale[:, np.newaxis]", "idx": 1639}
{"namespace": "diffprivlib.models.pca.PCA.fit_transform", "completion": "        X = self._validate_data(X, dtype=[np.float64, np.float32], ensure_2d=True, copy=self.copy)\n\n        if self.centered:\n            X -= self.mean_\n        else:\n            if self.bounds is None:\n                warnings.warn(\n                    \"Bounds parameter hasn't been specified, so falling back to determining range from the data.\\n\"\n                    \"This will result in additional privacy leakage. To ensure differential privacy with no \"\n                    \"additional privacy loss, specify `range` for each valued returned by np.mean().\",\n                    PrivacyLeakWarning)\n\n                self.bounds = (np.min(X, axis=0), np.max(X, axis=0))\n\n            self.bounds = self._check_bounds(self.bounds, X.shape[1])\n            X = mean(X, epsilon=self.epsilon / 2, bounds=self.bounds, axis=0, random_state=check_random_state(self.random_state),\n                     accountant=BudgetAccountant())\n\n        X = self._clip_to_norm(X, self.data_norm)\n\n        if self.n_components is None:\n            n_components = X.shape[1] - 1\n        else:\n            n_components = self.n_components\n\n        _, sigma, u = self._fit_full(X, n_components)\n\n        return np.dot(X, u.T) * sigma", "idx": 1640}
{"namespace": "discord.utils.get_slots", "completion": "    for c in cls.mro():\n        if hasattr(c, '__slots__'):\n            for slot in c.__slots__:\n                yield slot", "idx": 1641}
{"namespace": "discord.utils.is_inside_class", "completion": "    return '.' in getattr(func, '__qualname__', '')", "idx": 1642}
{"namespace": "faker.utils.decorators.slugify", "completion": "    @wraps(fn)\n    def wrapper(*args, **kwargs) -> str:\n        result = fn(*args, **kwargs)\n        return text.slugify(result)\n    return wrapper", "idx": 1643}
{"namespace": "faker.utils.decorators.slugify_domain", "completion": "    @wraps(fn)\n    def wrapper(*args: Tuple[T, ...], **kwargs: Dict[str, T]) -> str:\n        result = fn(*args, **kwargs)\n        return text.slugify(result, allow_dots=True)\n\n    return wrapper", "idx": 1644}
{"namespace": "faker.utils.decorators.slugify_unicode", "completion": "    @wraps(fn)\n    def wrapper(*args: Tuple[T, ...], **kwargs: Dict[str, T]) -> str:\n        return text.slugify_unicode(fn(*args, **kwargs))\n\n    return wrapper", "idx": 1645}
{"namespace": "faker.utils.loading.get_path", "completion": "    if getattr(sys, 'frozen', False):\n        if hasattr(sys, '_MEIPASS'):\n            return Path(sys._MEIPASS) / module.__file__\n        else:\n            return Path(sys.executable).parent / module.__file__\n    else:\n        if module.__file__:\n            return Path(module.__file__).resolve()\n        else:\n            raise RuntimeError(f\"Can't find path from module `{module}`.\")", "idx": 1646}
{"namespace": "faker.utils.checksums.luhn_checksum", "completion": "    digits = [int(x) for x in str(number)]\n    odd_digits = digits[-1::-2]\n    even_digits = digits[-2::-2]\n    total = sum(odd_digits)\n    for digit in even_digits:\n        total += sum(divmod(digit * 2, 10))\n    return (10 - total % 10) % 10", "idx": 1647}
{"namespace": "faker.utils.datasets.add_ordereddicts", "completion": "    combined_dict = OrderedDict()\n    for odict in odicts:\n        combined_dict.update(odict)\n    return combined_dict", "idx": 1648}
{"namespace": "faker.providers.person.pl_PL.checksum_identity_card_number", "completion": "    weights = [7, 3, 1, 0, 7, 3, 1, 7, 3]\n    total = 0\n    for i in range(len(characters)):\n        if isinstance(characters[i], str):\n            total += int(characters[i]) * weights[i]\n        else:\n            total += characters[i] * weights[i]\n    control_digit = total % 10\n    return control_digit", "idx": 1649}
{"namespace": "faker.providers.company.pl_PL.regon_checksum", "completion": "    weights = [8, 9, 2, 3, 4, 5, 6, 7]\n    total = sum(w * d for w, d in zip(weights, digits)) % 11\n    control_digit = total if total != 10 else 0\n    return control_digit", "idx": 1650}
{"namespace": "faker.providers.company.ru_RU.calculate_checksum", "completion": "    factors = [3, 7, 2, 4, 10, 3, 5, 9, 4, 6, 8]\n    checksum = 0\n    for i in range(len(value)):\n        checksum += int(value[i]) * factors[i % len(factors)]\n    return str(checksum % 11)", "idx": 1651}
{"namespace": "faker.providers.company.pl_PL.local_regon_checksum", "completion": "    weights_for_check_digit = [2, 4, 8, 5, 0, 9, 7, 3, 6, 1, 2, 4, 8]\n    check_digit = 0\n\n    for i in range(0, 13):\n        check_digit += weights_for_check_digit[i] * digits[i]\n\n    check_digit %= 11\n\n    if check_digit == 10:\n        check_digit = 0\n\n    return check_digit", "idx": 1652}
{"namespace": "faker.providers.company.pl_PL.company_vat_checksum", "completion": "    weights_for_check_digit = [6, 5, 7, 2, 3, 4, 5, 6, 7]\n    check_digit = 0\n\n    for i in range(0, 9):\n        check_digit += weights_for_check_digit[i] * digits[i]\n\n    check_digit %= 11\n\n    if check_digit == 10:\n        check_digit = 0\n\n    return check_digit", "idx": 1653}
{"namespace": "faker.providers.company.pt_BR.company_id_checksum", "completion": "    weights = [6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 2]\n    checksum = sum([a*b for a, b in zip(digits, weights)]) % 11\n    return digits + [checksum]", "idx": 1654}
{"namespace": "faker.providers.misc.Provider.binary", "completion": "        return os.urandom(length)", "idx": 1655}
{"namespace": "faker.providers.python.Provider.pystr", "completion": "        if min_chars is None:\n            min_chars = 0\n        if min_chars > max_chars:\n            raise ValueError(\"min_chars cannot be greater than max_chars\")\n\n        random_length = self.random_int(min_chars, max_chars)\n        random_string = \"\".join(self.random_choices(string.ascii_letters, k=random_length))\n        return f\"{prefix}{random_string}{suffix}\"", "idx": 1656}
{"namespace": "dash._utils.AttributeDict.set_read_only", "completion": "        for name in names:\n            setattr(self, f\"_{name}_read_only\", msg)", "idx": 1657}
{"namespace": "dash._utils.AttributeDict.first", "completion": "        if not names:\n            return next(iter(self.values()), None)\n        for name in names:\n            value = self.get(name)\n            if value:\n                return value\n        return None", "idx": 1658}
{"namespace": "dash._get_paths.app_get_asset_url", "completion": "    if 'assets_external_path' in config:\n        return config.assets_external_path + path\n    else:\n        return config.requests_pathname_prefix + '/_assets/' + path", "idx": 1659}
{"namespace": "peewee.sort_models", "completion": "    def dfs(node, visited, stack):\n        visited.add(node)\n        for neighbor in node.get_rel_for_model(node.__class__)[0]:\n            if neighbor not in visited:\n                dfs(neighbor, visited, stack)\n        stack.append(node)\n\n    visited = set()\n    stack = []\n    for model in models:\n        if model not in visited:\n            dfs(model, visited, stack)\n    return stack", "idx": 1660}
{"namespace": "dash._grouping.grouping_len", "completion": "    flattened_grouping = flatten_grouping(grouping)\n    return len(flattened_grouping)", "idx": 1661}
{"namespace": "playhouse.kv.KeyValue.get", "completion": "        try:\n            return self[key]\n        except KeyError:\n            return default", "idx": 1662}
{"namespace": "playhouse.kv.KeyValue.setdefault", "completion": "        try:\n            return self[key]\n        except KeyError:\n            self[key] = default\n            return default", "idx": 1663}
{"namespace": "sslyze.plugins.certificate_info._certificate_utils.get_public_key_sha256", "completion": "    public_key = certificate.public_key()\n    public_key_bytes = public_key.public_bytes(Encoding.DER, PublicFormat.SubjectPublicKeyInfo)\n    sha256_hash = sha256(public_key_bytes).digest()\n    return sha256_hash", "idx": 1664}
{"namespace": "ydata_profiling.compare_reports._compare_title", "completion": "    if all(title == titles[0] for title in titles):\n        return titles[0]\n    else:\n        return \"Titles are not the same: \" + \", \".join(titles)", "idx": 1665}
{"namespace": "ydata_profiling.report.formatters.fmt_bytesize", "completion": "    for unit in [\"\", \"Ki\", \"Mi\", \"Gi\", \"Ti\", \"Pi\", \"Ei\", \"Zi\"]:\n        if abs(num) < 1024.0:\n            return f\"{num:.1f} {unit}{suffix}\"\n        num /= 1024.0\n    return f\"{num:.1f} Yi{suffix}\"", "idx": 1666}
{"namespace": "ydata_profiling.report.formatters.fmt_percent", "completion": "    if edge_cases:\n        if math.isclose(value, 0.0):\n            return \"0.0%\"\n        elif math.isclose(value, 1.0):\n            return \"100.0%\"\n    return f\"{value*100:.1f}%\"", "idx": 1667}
{"namespace": "ydata_profiling.report.formatters.fmt_numeric", "completion": "    return f\"{value:.{precision}f}\"", "idx": 1668}
{"namespace": "ydata_profiling.report.formatters.fmt_array", "completion": "    if threshold is np.nan:\n        return np.array2string(value, threshold=np.inf)\n    else:\n        return np.array2string(value, threshold=threshold)", "idx": 1669}
{"namespace": "ydata_profiling.report.formatters.fmt_monotonic", "completion": "    if value > 0:\n        return \"Positive\"\n    elif value < 0:\n        return \"Negative\"\n    else:\n        return \"Zero\"", "idx": 1670}
{"namespace": "ydata_profiling.visualisation.plot._plot_pie_chart", "completion": "\n    # Plot\n    _, ax = plt.subplots(figsize=(7, 7))\n    ax.axis(\"equal\")\n\n    wedges, texts, autotexts = ax.pie(\n        data,\n        labels=data.index,\n        colors=colors,\n        autopct=\"%1.1f%%\",\n        startangle=140,\n        wedgeprops=dict(width=0.4),\n    )\n\n    for text in texts:\n        text.set_fontsize(12)\n    for autotext in autotexts:\n        autotext.set_fontsize(12)\n\n    if not hide_legend:\n        legend = ax.legend(\n            wedges,\n            data.index,\n            title=\"Categories\",\n            loc=\"center left\",\n            bbox_to_anchor=(1, 0, 0.5, 1),\n            fontsize=\"large\",\n        )\n    else:\n        legend = None\n\n    return ax, legend", "idx": 1671}
{"namespace": "ydata_profiling.visualisation.plot._prepare_heatmap_data", "completion": "    if selected_entities:\n        dataframe = dataframe[dataframe[entity_column].isin(selected_entities)]\n\n    if sortby:\n        if isinstance(sortby, str):\n            sortby = [sortby]\n        dataframe = dataframe.sort_values(by=sortby)\n\n    unique_entities = dataframe[entity_column].unique()[:max_entities]\n    heatmap_data = dataframe[dataframe[entity_column].isin(unique_entities)]\n\n    return heatmap_data.pivot(index=entity_column, columns=dataframe.columns.difference([entity_column])[0], values=dataframe.columns.difference([entity_column])[1])", "idx": 1672}
{"namespace": "ydata_profiling.visualisation.plot._create_timeseries_heatmap", "completion": "\n    fig, ax = plt.subplots(figsize=figsize)\n    sns.heatmap(df, cmap=color, ax=ax)\n    return ax", "idx": 1673}
{"namespace": "ydata_profiling.model.expectation_algorithms.generic_expectations", "completion": "    # Check if the column exists in the batch\n    if name not in batch.columns:\n        raise ValueError(f\"Column '{name}' does not exist in the batch.\")\n\n    # Check for missing values\n    if summary[\"missing_count\"] > 0:\n        raise ValueError(f\"Column '{name}' has {summary['missing_count']} missing values.\")\n\n    # Check if all values are unique\n    if summary[\"unique_count\"] != summary[\"count\"]:\n        raise ValueError(f\"Column '{name}' has non-unique values.\")\n\n    return name, summary, batch", "idx": 1674}
{"namespace": "ydata_profiling.model.expectation_algorithms.numeric_expectations", "completion": "    batch.expect_column_values_to_be_of_type(name, \"int64\")\n\n    batch.expect_column_min_to_be_between(name, min_value=args[0], max_value=args[1])\n    batch.expect_column_max_to_be_between(name, min_value=args[0], max_value=args[1])\n\n    return name, summary, batch", "idx": 1675}
{"namespace": "ydata_profiling.model.expectation_algorithms.categorical_expectations", "completion": "    \n    if summary[\"n_distinct\"] <= 10 or summary[\"p_distinct\"] <= 0.1:\n        batch.expect_column_values_to_be_in_set(name, set(summary[\"value_counts\"].keys()))\n\n    return name, summary, batch", "idx": 1676}
{"namespace": "ydata_profiling.model.expectation_algorithms.datetime_expectations", "completion": "    if any(k in summary for k in [\"min\", \"max\"]):\n        batch.expect_column_values_to_be_between(\n            name, min_value=summary.get(\"min\"), max_value=summary.get(\"max\")\n        )\n\n    return name, summary, batch", "idx": 1677}
{"namespace": "ydata_profiling.model.expectation_algorithms.file_expectations", "completion": "    return name, summary, batch", "idx": 1678}
{"namespace": "ydata_profiling.model.pandas.describe_categorical_pandas.word_summary_vc", "completion": "    # Convert the index of the Series to a list of words\n    words = vc.index.tolist()\n\n    # Remove stop words from the list of words\n    words = [word for word in words if word not in stop_words]\n\n    # Count the occurrences of each word\n    word_counts = Counter(words)\n\n    # Convert the Counter to a Series\n    word_counts_series = pd.Series(word_counts)\n\n    # Sort the Series by the computed frequency\n    word_counts_series = word_counts_series.sort_values(ascending=False)\n\n    # Create the summary dictionary\n    summary = {\n        \"n_words_distinct\": len(word_counts_series),\n        \"n_words\": np.sum(word_counts_series.values),\n        \"word_counts\": word_counts_series,\n    }\n\n    return summary", "idx": 1679}
{"namespace": "ydata_profiling.model.pandas.imbalance_pandas.column_imbalance_score", "completion": "\n    # Calculate the probability of each class\n    class_probabilities = value_counts / value_counts.sum()\n\n    # Calculate the entropy\n    entropy_score = entropy(class_probabilities, base=2)\n\n    # Calculate the imbalance score\n    imbalance_score = entropy_score / log2(n_classes)\n\n    return imbalance_score", "idx": 1680}
{"namespace": "django.core.exceptions.ValidationError.messages", "completion": "        if hasattr(self, \"error_dict\"):\n            return sum(self.error_dict.values(), [])\n        else:\n            return [error.message for error in self.error_list]", "idx": 1681}
{"namespace": "django.utils.module_loading.module_has_submodule", "completion": "    # Check if the package is a valid package\n    if not hasattr(package, \"__path__\"):\n        return False\n\n    # Try to import the module\n    try:\n        import_module(module_name, package.__name__)\n    except ImportError:\n        return False\n    else:\n        return True", "idx": 1682}
{"namespace": "django.utils.timezone.get_fixed_timezone", "completion": "    if isinstance(offset, timedelta):\n        offset = offset.total_seconds() // 60\n    return timezone(timedelta(minutes=offset))", "idx": 1683}
{"namespace": "django.utils.encoding.filepath_to_uri", "completion": "    return escape_uri_path(path)", "idx": 1684}
{"namespace": "django.utils._os.to_path", "completion": "    if isinstance(value, Path):\n        return value\n    elif isinstance(value, str):\n        return Path(value)\n    else:\n        raise ValueError(\"Input value must be a string or a pathlib.Path instance\")", "idx": 1685}
{"namespace": "django.utils.lorem_ipsum.sentence", "completion": "    sentence_length = random.randint(4, 15)\n    sentence = [random.choice(WORDS) for _ in range(sentence_length)]\n    sentence[0] = sentence[0].capitalize()\n    sentence = ' '.join(sentence)\n    if random.random() > 0.5:\n        sentence += ','\n    sentence += random.choice(['.', '?'])\n    return sentence", "idx": 1686}
{"namespace": "ydata_profiling.utils.dataframe.sort_column_names", "completion": "    if sort == \"ascending\":\n        return {k: v for k, v in sorted(dct.items())}\n    elif sort == \"descending\":\n        return {k: v for k, v in sorted(dct.items(), reverse=True)}\n    else:\n        return dct", "idx": 1687}
{"namespace": "benedict.utils.type_util.is_json_serializable", "completion": "    return (\n        is_bool(val)\n        or is_collection(val)\n        or is_datetime(val)\n        or is_decimal(val)\n        or is_dict(val)\n        or is_float(val)\n        or is_function(val)\n        or is_integer(val)\n        or isinstance(val, str)\n        or isinstance(val, int)\n        or isinstance(val, float)\n        or isinstance(val, bool)\n        or isinstance(val, (list, dict))\n        or isinstance(val, (list, tuple))\n        or isinstance(val, (str, int, float, bool))\n        or isinstance(val, (str, int, float, bool, list, dict))\n        or isinstance(val, (str, int, float, bool, list, tuple))\n        or isinstance(val, (str, int, float, bool, list, dict, tuple))\n        or isinstance(val, (pathlib.Path, regex, uuid_re))\n    )", "idx": 1688}
{"namespace": "feedparser.urls.convert_to_idn", "completion": "    try:\n        scheme, netloc, path, params, query, fragment = urllib.parse.urlparse(url)\n        netloc = netloc.encode('idna').decode('utf-8')\n        return urllib.parse.urlunparse((scheme, netloc, path, params, query, fragment))\n    except:\n        return url", "idx": 1689}
{"namespace": "mistune.toc.add_toc_hook", "completion": "    def save_toc(items, level=0):\n        if not hasattr(md, 'toc_tree'):\n            md.toc_tree = []\n        if level == 0:\n            md.toc_tree = []\n        for item in items:\n            if item['level'] >= min_level and item['level'] <= max_level:\n                if heading_id:\n                    item['id'] = heading_id(item['title'])\n                md.toc_tree.append({'level': item['level'], 'id': item.get('id'), 'title': striptags(item['title'])})\n            if item.get('children'):\n                save_toc(item['children'], level + 1)\n\n    md.toc_tokens = []\n    md.toc_tree = []\n    md.toc_tokens.append({'save_toc': save_toc})", "idx": 1690}
{"namespace": "mistune.plugins.table.table_in_quote", "completion": "    md.block.register('table', TABLE_PATTERN, parse_table, before='block_quote')\n    md.block.register('nptable', NP_TABLE_PATTERN, parse_nptable, before='block_quote')\n\n    if md.renderer and md.renderer.NAME == 'html':\n        md.renderer.register('table', render_table)\n        md.renderer.register('table_head', render_table_head)\n        md.renderer.register('table_body', render_table_body)\n        md.renderer.register('table_row', render_table_row)\n        md.renderer.register('table_cell', render_table_cell)", "idx": 1691}
{"namespace": "mistune.plugins.table.table_in_list", "completion": "    md.block.insert_rule(md.block.list_rules, 'table', before='paragraph')\n    md.block.insert_rule(md.block.list_rules, 'nptable', before='paragraph')", "idx": 1692}
{"namespace": "xmnlp.utils.parallel_handler", "completion": "    if not isinstance(texts, list):\n        raise ValueError(\"You should pass a list of texts\")\n\n    with futures.ThreadPoolExecutor(max_workers=n_jobs) as executor:\n        results = executor.map(partial(callback, **kwargs), texts)\n        for result in results:\n            yield result", "idx": 1693}
{"namespace": "parsel.utils.shorten", "completion": "    if width < 0:\n        raise ValueError(\"width must be equal or greater than 0\")\n    if len(text) <= width:\n        return text\n    if width >= len(suffix):\n        return text[:width - len(suffix)] + suffix\n    return suffix", "idx": 1694}
{"namespace": "parsel.xpathfuncs.set_xpathfunc", "completion": "    if func is not None:\n        etree.XPathEvaluator._namespace[fname] = func\n    else:\n        etree.XPathEvaluator._namespace.pop(fname, None)", "idx": 1695}
{"namespace": "dominate.dom_tag._get_thread_context", "completion": "  context = [threading.current_thread()]\n  if greenlet is not None:\n    current_greenlet = greenlet.getcurrent()\n    if current_greenlet:\n      context.append(current_greenlet)\n  return hash(tuple(context))", "idx": 1696}
{"namespace": "dominate.util.system", "completion": "    import subprocess\n    if data:\n        p = subprocess.Popen(cmd, shell=True, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        out, err = p.communicate(data)\n    else:\n        p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        out, err = p.communicate()\n    return out.decode('utf-8')", "idx": 1697}
{"namespace": "dominate.util.url_unescape", "completion": "    from urllib.parse import unquote\n    import re\n  import re\n  from urllib.parse import unquote\n  return unquote(data)", "idx": 1698}
{"namespace": "rows.fields.DatetimeField.serialize", "completion": "        \"\"\"\n        Serialize the given datetime value into a string in ISO 8601 format.\n        Input-Output Arguments\n        :param cls: Class. The class instance.\n        :param value: Datetime. The datetime value to be serialized.\n        :param *args: Additional positional arguments.\n        :param **kwargs: Additional keyword arguments.\n        :return: String. The serialized datetime value in ISO 8601 format.\n        \"\"\"\n        if value is None:\n            return \"\"\n\n        return value.isoformat()", "idx": 1699}
{"namespace": "rows.fields.Field.serialize", "completion": "        if value is None:\n            return u\"\"\n\n        if isinstance(value, cls.TYPE):\n            return six.text_type(value)\n\n        value_error(value, cls)", "idx": 1700}
{"namespace": "rows.fields.EmailField.serialize", "completion": "        if value is None:\n            return \"\"\n        return six.text_type(value)", "idx": 1701}
{"namespace": "rows.fields.as_string", "completion": "    if isinstance(value, six.binary_type):\n        raise ValueError(\"Value '{}' can't be {}\".format(repr(value), \"string\"))\n    return six.text_type(value)", "idx": 1702}
{"namespace": "rows.fields.get_items", "completion": "    return lambda obj: tuple(obj[index] if index < len(obj) else None for index in indexes)", "idx": 1703}
{"namespace": "pycorrector.proper_corrector.load_dict_file", "completion": "    dictionary = {}\n    if path and os.path.exists(path):\n        with open(path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line.startswith('#'):\n                    continue\n                if line:\n                    key, value = line.split('\\t')\n                    dictionary[key] = value\n    return dictionary", "idx": 1704}
{"namespace": "natasha.span.envelop_spans", "completion": "    for envelope in envelopes:\n        enveloped_spans = [span for span in spans if envelope.start <= span.start and envelope.stop >= span.stop]\n        yield enveloped_spans", "idx": 1705}
{"namespace": "googleapiclient._helpers.parse_unique_urlencoded", "completion": "    parsed_content = urllib.parse.parse_qs(content)\n    result = {}\n    for key, value in parsed_content.items():\n        if len(value) > 1:\n            raise ValueError(\"Repeated key found: {}\".format(key))\n        result[key] = value[0]\n    return result", "idx": 1706}
{"namespace": "jinja2.async_utils.auto_aiter", "completion": "\n    if hasattr(iterable, \"__aiter__\"):\n        async for item in iterable:\n            yield item\n    else:\n        for item in iterable:\n            yield item", "idx": 1707}
{"namespace": "jinja2.utils.consume", "completion": "    for _ in iterable:\n        pass", "idx": 1708}
{"namespace": "pycorrector.utils.tokenizer.segment", "completion": "    if cut_type == 'word':\n        if pos:\n            return posseg.lcut(sentence)\n        else:\n            return jieba.lcut(sentence)\n    elif cut_type == 'char':\n        if pos:\n            return [(char, 'x') for char in sentence]\n        else:\n            return list(sentence)\n    else:\n        raise ValueError(\"Invalid cut_type, it should be 'word' or 'char'.\")", "idx": 1709}
{"namespace": "jinja2.utils.object_type_repr", "completion": "    if obj is None:\n        return \"None\"\n    elif obj is Ellipsis:\n        return \"Ellipsis\"\n    elif obj is NotImplemented:\n        return \"NotImplemented\"\n    elif isinstance(obj, abc.Iterable):\n        return f\"{type(obj).__module__} {type(obj).__name__} object\"\n    else:\n        return f\"{type(obj).__name__} object\"", "idx": 1710}
{"namespace": "jinja2.utils.LRUCache.setdefault", "completion": "        with self._wlock:\n            if key in self._mapping:\n                return self._mapping[key]\n            self._append(key)\n            self._mapping[key] = default\n            if len(self._mapping) > self.capacity:\n                old_key = self._popleft()\n                del self._mapping[old_key]\n            return default", "idx": 1711}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_word_freq", "completion": "        word_freq = {}\n        for word in list_of_words:\n            if word in word_freq:\n                word_freq[word] += 1\n            else:\n                word_freq[word] = 1\n        return word_freq", "idx": 1712}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_average_probability_of_words", "completion": "        total_probability = 0\n        content_words_count = len(content_words_in_sentence)\n        \n        if content_words_count == 0:\n            return 0\n        \n        for word in content_words_in_sentence:\n            total_probability += word_freq_in_doc.get(word, 0) / content_words_count\n        \n        return total_probability / content_words_count", "idx": 1713}
{"namespace": "sumy.summarizers.lex_rank.LexRankSummarizer._compute_idf", "completion": "        idf_metrics = {}\n        total_sentences = len(sentences)\n\n        # Create a set of all unique words in the sentences\n        unique_words = set()\n        for sentence in sentences:\n            unique_words.update(sentence)\n\n        # Calculate the IDF for each unique word\n        for word in unique_words:\n            word_count = sum(1 for s in sentences if word in s)\n            idf_metrics[word] = math.log10(total_sentences / (1 + word_count))\n\n        return idf_metrics", "idx": 1714}
{"namespace": "sumy.summarizers.lex_rank.LexRankSummarizer.cosine_similarity", "completion": "        common_words = set(sentence1) & set(sentence2)\n\n        numerator = sum((tf1[word] * tf2[word] * idf_metrics[word]**2) for word in common_words)\n        denominator1 = sum((tf1[word] * idf_metrics[word])**2 for word in sentence1)\n        denominator2 = sum((tf2[word] * idf_metrics[word])**2 for word in sentence2)\n\n        if denominator1 == 0 or denominator2 == 0:\n            return 0.0\n\n        return numerator / (math.sqrt(denominator1) * math.sqrt(denominator2))", "idx": 1715}
{"namespace": "sumy.evaluation.rouge._get_ngrams", "completion": "    ngrams = set()\n    words = text.split()\n    for i in range(len(words) - n + 1):\n        ngram = ' '.join(words[i:i + n])\n        ngrams.add(ngram)\n    return ngrams", "idx": 1716}
{"namespace": "sumy.evaluation.rouge._split_into_words", "completion": "    if not all(isinstance(sentence, Sentence) for sentence in sentences):\n        raise ValueError(\"Object in collection must be of type Sentence\")\n    \n    words = []\n    for sentence in sentences:\n        words.extend(sentence.split())\n    \n    return words", "idx": 1717}
{"namespace": "falcon.inspect.register_router", "completion": "    if router_class in _supported_routers:\n        raise ValueError(f\"The router class {router_class} is already registered\")\n\n    def decorator(inspect_function: Callable[[CompiledRouter], List[RouteInfo]]) -> Callable[[CompiledRouter], List[RouteInfo]]:\n        _supported_routers[router_class] = inspect_function\n        return inspect_function\n\n    return decorator", "idx": 1718}
{"namespace": "falcon.inspect.inspect_compiled_router", "completion": "    routes = []\n    for route in router._roots:\n        route_info = RouteInfo(\n            route.resource.__class__.__name__,\n            route.method_map.keys(),\n            route.uri_template,\n            route._suffix\n        )\n        routes.append(route_info)\n    return routes", "idx": 1719}
{"namespace": "falcon.inspect._is_internal", "completion": "    if inspect.ismodule(obj):\n        return obj.__name__.startswith('falcon.')\n    else:\n        return inspect.getmodule(obj).__name__.startswith('falcon.')", "idx": 1720}
{"namespace": "falcon.cmd.inspect_app.load_app", "completion": "    module_name, instance_name = args.app_module.split(':')\n\n    # Import the module\n    module = importlib.import_module(module_name)\n\n    # Get the app instance from the module\n    app = getattr(module, instance_name)\n\n    # If the app is not an instance of falcon.App, try to create an instance from the callable app\n    if not isinstance(app, falcon.App):\n        app = app()\n\n    # If the app is still not an instance of falcon.App, raise an error\n    if not isinstance(app, falcon.App):\n        raise ValueError('The app is not an instance of falcon.App')\n\n    return app", "idx": 1721}
{"namespace": "falcon.cmd.inspect_app.make_parser", "completion": "    parser = argparse.ArgumentParser(description='Prints out the routes of an App instance')\n    parser.add_argument('-r', '--router', help='The router to use', default='default')\n    parser.add_argument('-v', '--verbose', help='Print verbose output', action='store_true')\n    parser.add_argument('-i', '--internal', help='Include internal routes', action='store_true')\n    parser.add_argument('app_module', help='The Python module that contains the application instance')\n    return parser", "idx": 1722}
{"namespace": "falcon.util.uri.unquote_string", "completion": "\n    if not isinstance(quoted, str):\n        raise TypeError('Input must be a string')\n\n    if quoted.startswith('\"') and quoted.endswith('\"'):\n        return quoted[1:-1]\n\n    return quoted", "idx": 1723}
{"namespace": "falcon.util.misc.get_argnames", "completion": "    if not is_python_func(func):\n        raise TypeError('func is not a Python function')\n\n    sig = inspect.signature(func)\n    arg_names = [param.name for param in sig.parameters.values() if param.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD]\n\n    return arg_names", "idx": 1724}
{"namespace": "falcon.testing.client._is_asgi_app", "completion": "        import inspect\n    import inspect\n\n    if inspect.isfunction(app):\n        signature = inspect.signature(app)\n        return len(signature.parameters) == 3\n    else:\n        return False", "idx": 1725}
{"namespace": "falcon.routing.converters.UUIDConverter.convert", "completion": "        try:\n            return uuid.UUID(value)\n        except ValueError:\n            return None", "idx": 1726}
{"namespace": "rest_framework_simplejwt.utils.make_utc", "completion": "    if settings.USE_TZ and is_naive(dt):\n        return make_aware(dt, timezone.utc)\n    else:\n        return dt", "idx": 1727}
{"namespace": "boto.sdb.db.sequence.fib", "completion": "    if cv is None:\n        return 1\n    return cv + lv", "idx": 1728}
{"namespace": "boto.s3.website.RoutingRules.add_rule", "completion": "        self.append(rule)\n        return self", "idx": 1729}
{"namespace": "boto.cloudfront.distribution.Distribution._canned_policy", "completion": "        policy = '{\"Statement\":[{\"Resource\":\"%s\",\"Condition\":{\"DateLessThan\":{\"AWS:EpochTime\":%s}}}]}' % (resource, expires)\n        return policy", "idx": 1730}
{"namespace": "boto.cloudfront.invalidation.InvalidationBatch.escape", "completion": "        p = urllib.quote(p, safe='/*')\n        if not p.startswith('/'):\n            p = '/' + p\n        return p", "idx": 1731}
{"namespace": "proxybroker.utils.get_status_code", "completion": "    try:\n        status_code = int(resp[start:stop])\n        return status_code\n    except (ValueError, IndexError):\n        return 400", "idx": 1732}
{"namespace": "authlib.oauth2.rfc6749.util.scope_to_list", "completion": "    if isinstance(scope, (set, tuple, list)):\n        return [to_unicode(s) for s in scope]\n    if scope is None:\n        return scope\n    return [to_unicode(s) for s in scope.split()]", "idx": 1733}
{"namespace": "authlib.common.encoding.to_unicode", "completion": "    if x is None:\n        return None\n    if isinstance(x, str):\n        return x\n    if isinstance(x, bytes):\n        return x.decode(charset, errors)\n    if isinstance(x, (int, float)):\n        return str(x)\n    return str(x)", "idx": 1734}
{"namespace": "authlib.common.encoding.to_bytes", "completion": "    if x is None:\n        return None\n    elif isinstance(x, bytes):\n        return x\n    elif isinstance(x, str):\n        return x.encode(charset, errors)\n    elif isinstance(x, int) or isinstance(x, float):\n        return struct.pack('!f', x)\n    else:\n        return json.dumps(x).encode(charset, errors)", "idx": 1735}
{"namespace": "authlib.common.encoding.urlsafe_b64decode", "completion": "    s += b'=' * (-len(s) % 4)\n    return base64.urlsafe_b64decode(s)", "idx": 1736}
{"namespace": "csvs_to_sqlite.utils.table_exists", "completion": "    result = conn.execute(\n        \"\"\"\n        SELECT name\n        FROM sqlite_master\n        WHERE type='table'\n        AND name=?\n    \"\"\",\n        (table,),\n    ).fetchall()\n    return bool(result)", "idx": 1737}
{"namespace": "sqlitedict.SqliteDict.get_tablenames", "completion": "        if not os.path.exists(filename):\n            raise IOError('file {} does not exist'.format(filename))\n\n        conn = sqlite3.connect(filename)\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n        tables = cursor.fetchall()\n        table_names = [table[0] for table in tables]\n        conn.close()\n\n        return table_names", "idx": 1738}
{"namespace": "litecli.packages.parseutils.query_starts_with", "completion": "    query = query.lower().strip()\n    query = sqlparse.format(query, strip_comments=True)\n    for prefix in prefixes:\n        if query.startswith(prefix.lower()):\n            return True\n    return False", "idx": 1739}
{"namespace": "rest_framework.negotiation.DefaultContentNegotiation.filter_renderers", "completion": "        filtered_renderers = []\n        for renderer in renderers:\n            if media_type_matches(renderer.media_type, format):\n                filtered_renderers.append(renderer)\n        \n        if not filtered_renderers:\n            raise Http404(\"No renderer found for the given format\")\n        \n        return filtered_renderers", "idx": 1740}
{"namespace": "rest_framework.templatetags.rest_framework.as_string", "completion": "    if value is None:\n        return ''\n    return str(value)", "idx": 1741}
{"namespace": "rest_framework.templatetags.rest_framework.add_nested_class", "completion": "    if isinstance(value, dict) or (isinstance(value, list) and any(isinstance(item, (dict, list)) for item in value)):\n        return 'class=nested'\n    else:\n        return ''", "idx": 1742}
{"namespace": "pyramid.session.PickleSerializer.loads", "completion": "        try:\n            return pickle.loads(bstruct)\n        except Exception as e:\n            raise ValueError(f\"Error deserializing byte stream: {e}\")", "idx": 1743}
{"namespace": "pyramid.testing.DummySession.flash", "completion": "        if 'flash' not in self:\n            self['flash'] = []\n        if allow_duplicate or msg not in self['flash']:\n            self['flash'].append(msg)", "idx": 1744}
{"namespace": "pyramid.testing.DummySession.pop_flash", "completion": "        return self.pop('_f_' + queue, [])", "idx": 1745}
{"namespace": "pyramid.testing.DummySession.peek_flash", "completion": "        return self.get('_f_' + queue, [])", "idx": 1746}
{"namespace": "pyramid.testing.DummySession.new_csrf_token", "completion": "        csrf_token = '0123456789012345678901234567890123456789'\n        self['_csrft_'] = csrf_token\n        return csrf_token", "idx": 1747}
{"namespace": "pyramid.view.view_defaults", "completion": "    def decorator(cls):\n        if not hasattr(cls, '__view_defaults__'):\n            cls.__view_defaults__ = {}\n        cls.__view_defaults__.update(settings)\n        return cls\n    return decorator", "idx": 1748}
{"namespace": "pyramid.util.bytes_", "completion": "    if isinstance(s, str):\n        return s.encode(encoding, errors)\n    return s", "idx": 1749}
{"namespace": "pyramid.scripts.common.parse_vars", "completion": "    parsed_dict = {}\n    for arg in args:\n        key, value = arg.split('=')\n        parsed_dict[key] = value\n    return parsed_dict", "idx": 1750}
{"namespace": "pyramid.scripts.pviews.PViewsCommand._find_multi_routes", "completion": "        infos = []\n        for route in mapper.get_routes():\n            match = route.match(request.path)\n            if match is not None:\n                infos.append({'route': route, 'match': match})\n        return infos", "idx": 1751}
{"namespace": "pyramid.scripts.pserve.PServeCommand.guess_server_url", "completion": "        server_name = server_name or 'main'\n        server_settings = loader.get_settings('server:' + server_name, global_conf)\n        port = server_settings.get('port')\n        if port:\n            return f'http://127.0.0.1:{port}'\n        else:\n            return ''", "idx": 1752}
{"namespace": "aiohappybase._util.pep8_to_camel_case", "completion": "    if initial:\n        return ''.join(word.capitalize() for word in name.split('_'))\n    else:\n        words = name.split('_')\n        return words[0] + ''.join(word.capitalize() for word in words[1:])", "idx": 1753}
{"namespace": "aiohappybase._util.bytes_increment", "completion": "    if b == b'\\xFF' * len(b):\n        return None\n    else:\n        return b[:-1] + bytes([b[-1] + 1])", "idx": 1754}
{"namespace": "mssqlcli.config.ensure_dir_exists", "completion": "    directory = dirname(path)\n    if not exists(directory):\n        os.makedirs(directory)", "idx": 1755}
{"namespace": "mssqlcli.telemetry._user_id_file_is_old", "completion": "    if os.path.exists(id_file_path):\n        file_modified_time = os.path.getmtime(id_file_path)\n        current_time = datetime.now().timestamp()\n        if current_time - file_modified_time > 24 * 60 * 60:  # 24 hours in seconds\n            return True\n    return False", "idx": 1756}
{"namespace": "mssqlcli.util.is_command_valid", "completion": "    if not command:\n        return False\n    try:\n        with open(devnull, 'w') as fnull:\n            subprocess.call([command], stdout=fnull, stderr=fnull)\n    except OSError:\n        return False\n    return True", "idx": 1757}
{"namespace": "mssqlcli.packages.parseutils.utils.find_prev_keyword", "completion": "\n    parsed = sqlparse.parse(sql)[0]\n    tokens = list(parsed.flatten())\n    tokens.reverse()\n\n    keyword = None\n    for token in tokens:\n        if token.ttype is Token.Keyword:\n            if n_skip == 0:\n                keyword = token.value\n                break\n            else:\n                n_skip -= 1\n\n    if keyword:\n        idx = sql.rfind(keyword)\n        return keyword, sql[:idx + len(keyword)].strip()\n    else:\n        return None, sql.strip()", "idx": 1758}
{"namespace": "trafilatura.settings.use_config", "completion": "    if config:\n        return config\n    else:\n        if not filename:\n            filename = \"settings.cfg\"\n        config = ConfigParser()\n        config.read(filename)\n        return config", "idx": 1759}
{"namespace": "pyramid.util.text_", "completion": "    if isinstance(s, bytes):\n        return s.decode(encoding, errors)\n    else:\n        return s", "idx": 1760}
{"namespace": "trafilatura.downloads._parse_config", "completion": "    user_agents = config.get('http', 'user_agents').split('\\n')\n    cookies = config.get('http', 'cookies')\n    return user_agents, cookies", "idx": 1761}
{"namespace": "trafilatura.downloads.load_download_buffer", "completion": "    bufferlist = url_store.get_urls()\n    if not bufferlist:\n        sleep(sleep_time)\n    return bufferlist, url_store", "idx": 1762}
{"namespace": "trafilatura.metadata.check_authors", "completion": "    author_blacklist = {author.lower() for author in author_blacklist}\n    authors_list = authors.split(';')\n    new_authors = [author.strip() for author in authors_list if author.strip().lower() not in author_blacklist]\n    if new_authors:\n        return '; '.join(new_authors)\n    else:\n        return None", "idx": 1763}
{"namespace": "datasette.filters.where_filters", "completion": "    where_clauses = []\n    extra_wheres_for_ui = []\n    \n    if \"_where\" in request.args:\n        if not datasette.permission_allowed(\n            request.actor, \"sql\", database=database, default_allow=True\n        ):\n            raise DatasetteError(403, \"Permission denied for SQL\")\n        \n        where_clauses.append(request.args[\"_where\"])\n        extra_wheres_for_ui.append(request.args[\"_where\"])\n    \n    class FilterArguments:\n        def __init__(self, where_clauses, extra_wheres_for_ui):\n            self.where_clauses = where_clauses\n            self.extra_wheres_for_ui = extra_wheres_for_ui\n    \n    return FilterArguments(where_clauses, extra_wheres_for_ui)", "idx": 1764}
{"namespace": "datasette.utils.path_with_added_args", "completion": "    if path is None:\n        path = request.path\n    if args:\n        query_string = urllib.parse.urlencode(args)\n        path = append_querystring(path, query_string)\n    return path", "idx": 1765}
{"namespace": "datasette.utils.path_with_replaced_args", "completion": "    path = path or request.path\n    if isinstance(args, dict):\n        args = args.items()\n    args_to_remove = {k for k, v in args if v is None}\n    current = []\n    for key, value in urllib.parse.parse_qsl(request.query_string):\n        if key not in args_to_remove:\n            current.append((key, value))\n    current.extend([(key, value) for key, value in args if value is not None])\n    query_string = urllib.parse.urlencode(current)\n    if query_string:\n        query_string = f\"?{query_string}\"\n    return path + query_string", "idx": 1766}
{"namespace": "datasette.utils.format_bytes", "completion": "    if bytes < 1024:\n        return f\"{bytes} bytes\"\n    elif bytes < 1024 ** 2:\n        return f\"{bytes / 1024:.2f} KB\"\n    elif bytes < 1024 ** 3:\n        return f\"{bytes / (1024 ** 2):.2f} MB\"\n    elif bytes < 1024 ** 4:\n        return f\"{bytes / (1024 ** 3):.2f} GB\"\n    else:\n        return f\"{bytes / (1024 ** 4):.2f} TB\"", "idx": 1767}
{"namespace": "datasette.utils.actor_matches_allow", "completion": "    if isinstance(allow, (list, tuple)):\n        return actor in allow\n    elif isinstance(allow, dict):\n        return all(\n            getattr(actor, key, None) == value for key, value in allow.items()\n        )\n    else:\n        return actor == allow", "idx": 1768}
{"namespace": "datasette.utils.resolve_env_secrets", "completion": "    if isinstance(config, dict):\n        return {\n            key: resolve_env_secrets(value, environ) for key, value in config.items()\n        }\n    elif isinstance(config, list):\n        return [resolve_env_secrets(item, environ) for item in config]\n    elif isinstance(config, str):\n        if config.startswith(\"$env:\"):\n            env_var = config.split(\"$env:\")[1]\n            return environ.get(env_var, config)\n        elif config.startswith(\"$file:\"):\n            file_path = config.split(\"$file:\")[1]\n            try:\n                with open(file_path, \"r\") as file:\n                    return file.read()\n            except FileNotFoundError:\n                return f\"File not found: {file_path}\"\n        else:\n            return config\n    else:\n        return config", "idx": 1769}
{"namespace": "datasette.utils.display_actor", "completion": "    if \"display_name\" in actor:\n        return actor[\"display_name\"]\n    elif \"name\" in actor:\n        return actor[\"name\"]\n    elif \"username\" in actor:\n        return actor[\"username\"]\n    elif \"login\" in actor:\n        return actor[\"login\"]\n    elif \"id\" in actor:\n        return actor[\"id\"]\n    else:\n        return str(actor)", "idx": 1770}
{"namespace": "datasette.utils.initial_path_for_datasette", "completion": "    databases = datasette.databases\n    if len(databases) == 1:\n        db_name = databases[0]\n        tables = datasette.inspect().get(db_name, {}).get(\"tables\", [])\n        if len(tables) == 1:\n            return f\"/{db_name}/{tables[0]}\"\n        else:\n            return f\"/{db_name}\"\n    else:\n        return \"/\"", "idx": 1771}
{"namespace": "datasette.utils.tilde_decode", "completion": "    return urllib.parse.unquote(s.replace(\"%\", \"TEMP_PERCENT\").replace(\"~\", \"%\").replace(\"TEMP_PERCENT\", \"%\"))", "idx": 1772}
{"namespace": "datasette.utils.resolve_routes", "completion": "    for route in routes:\n        regex, view = route\n        if re.match(regex, path):\n            return route\n    return None", "idx": 1773}
{"namespace": "datasette.utils.truncate_url", "completion": "    if len(url) <= length:\n        return url\n    else:\n        # Check if the URL ends with a file extension\n        if \".\" in url:\n            parts = url.split(\".\")\n            extension = parts[-1]\n            if len(extension) >= 1 and len(extension) <= 4 and \"/\" not in extension:\n                # Truncate the URL to the specified length and add ellipsis and the extension at the end\n                truncated_url = url[:length - len(extension) - 3] + \"...\" + extension\n                return truncated_url\n        # If the URL does not end with a file extension, simply truncate it and add ellipsis\n        return url[:length - 3] + \"...\"", "idx": 1774}
{"namespace": "kinto.core.authorization.groupfinder", "completion": "    settings = request.registry.settings\n    permission_backend = settings.get(\"permission_backend\")\n\n    if permission_backend:\n        # If the permission backend is configured, fetch the principals from the backend.\n        principals = request.registry.permission.get_principals(userid)\n        return principals\n    else:\n        # If the permission backend is not configured, return an empty list.\n        return []", "idx": 1775}
{"namespace": "kinto.core.utils.json.dumps", "completion": "        return rapidjson.dumps(v, bytes_mode=rapidjson.BM_NONE, **kw)", "idx": 1776}
{"namespace": "kinto.core.utils.json.loads", "completion": "        kw.setdefault(\"number_mode\", rapidjson.NM_NATIVE)\n        return rapidjson.loads(v, **kw)", "idx": 1777}
{"namespace": "kinto.core.utils.hmac_digest", "completion": "    if isinstance(secret, str):\n        secret = secret.encode(encoding)\n    h = hmac.new(secret, message.encode(encoding), hashlib.sha256)\n    return h.hexdigest()", "idx": 1778}
{"namespace": "kinto.core.utils.current_service", "completion": "    # Get the route mapper from the request's registry.\n    route_mapper = request.registry.queryUtility(IRoutesMapper)\n\n    # If the route mapper is not found, return None.\n    if route_mapper is None:\n        return None\n\n    # Get the route info for the request.\n    route_info = route_mapper(request)\n\n    # If the route info is not found, return None.\n    if route_info is None:\n        return None\n\n    # Get the service name from the route info.\n    service_name = route_info.get(\"matchdict\", {}).get(\"service\")\n\n    # If the service name is not found, return None.\n    if service_name is None:\n        return None\n\n    # Get the service from the request's registry based on the service name.\n    service = request.registry.getUtility(collections_abc.Mapping, name=service_name)\n\n    # Return the service.\n    return service", "idx": 1779}
{"namespace": "kinto.core.utils.prefixed_principals", "completion": "    principals = request.effective_principals\n    if Authenticated in principals:\n        prefixed_user_id = prefixed_userid(request)\n        # Remove the unprefixed user id from the effective principals.\n        principals = [p for p in principals if not p.startswith(\"userid:\")]\n        # Add the prefixed user id to the beginning of the list.\n        principals.insert(0, prefixed_user_id)\n    return principals", "idx": 1780}
{"namespace": "mopidy.ext.load_extensions", "completion": "    extensions = []\n    for entry_point in pkg_resources.iter_entry_points(\"mopidy.ext\"):\n        try:\n            extension_class = entry_point.load()\n            if issubclass(extension_class, Extension):\n                extension = extension_class()\n                config_schema = extension.get_config_schema()\n                config_defaults = extension.get_default_config()\n                command = extension.get_command()\n                extension_data = ExtensionData(\n                    extension=extension,\n                    entry_point=entry_point,\n                    config_schema=config_schema,\n                    config_defaults=config_defaults,\n                    command=command,\n                )\n                extensions.append(extension_data)\n        except Exception as exc:\n            logger.warning(\n                \"Loading extension %s failed: %s\", entry_point.name, exc\n            )\n    return extensions", "idx": 1781}
{"namespace": "mopidy.ext.validate_extension_data", "completion": "    try:\n        # Check if the extension's entry point name matches its extension name\n        if data.entry_point.name != data.extension.ext_name:\n            logger.error(\n                \"Entry point name '%s' does not match extension name '%s'\",\n                data.entry_point.name,\n                data.extension.ext_name,\n            )\n            return False\n\n        # Check if the required dependencies are installed\n        data.extension.validate_environment()\n\n        # Check if the extension has a valid config schema and default config\n        if not isinstance(data.config_schema, config_lib.ConfigSchema):\n            logger.error(\n                \"Extension '%s' has an invalid config schema\",\n                data.extension.ext_name,\n            )\n            return False\n        if not isinstance(data.config_defaults, str):\n            logger.error(\n                \"Extension '%s' has an invalid default config\",\n                data.extension.ext_name,\n            )\n            return False\n\n        return True\n    except Exception as e:\n        logger.exception(\n            \"Validation of extension '%s' failed: %s\",\n            data.extension.ext_name,\n            e,\n        )\n        return False", "idx": 1782}
{"namespace": "mopidy.httpclient.format_user_agent", "completion": "    mopidy_version = \"3.1.1\"  # Replace with actual Mopidy version\n    python_version = platform.python_version()\n    user_agent = f\"Mopidy/{mopidy_version} Python/{python_version}\"\n    if name:\n        user_agent += f\" {name}\"\n    return user_agent", "idx": 1783}
{"namespace": "mopidy.models.immutable.ValidatedImmutableObject.replace", "completion": "        other = copy.copy(self)\n        for key, value in kwargs.items():\n            if not self._is_valid_field(key):\n                raise TypeError(\n                    f\"replace() got an unexpected keyword argument {key!r}\"\n                )\n            other._set_field(key, value)\n        return other", "idx": 1784}
{"namespace": "mopidy.http.Extension.get_default_config", "completion": "        return {\n            \"http\": {\n                \"enabled\": \"true\",\n                \"hostname\": \"127.0.0.1\",\n                \"port\": 6680,\n                \"static_dir\": \"\",\n                \"zeroconf\": \"\",\n                \"allowed_origins\": \"\",\n                \"csrf_protection\": \"true\",\n                \"default_app\": \"mopidy\",\n                \"max_content_length\": 0,\n                \"hostname\": \"0.0.0.0\",\n                \"allowed_origins\": \"*\",\n                \"output\": \"autoaudiosink\",\n            }\n        }", "idx": 1785}
{"namespace": "mopidy.http.Extension.get_config_schema", "completion": "        schema = super().get_config_schema()\n        schema[\"hostname\"] = config_lib.String()\n        schema[\"port\"] = config_lib.Integer(minimum=0, maximum=65535)\n        schema[\"static_dir\"] = config_lib.String(optional=True)\n        return schema", "idx": 1786}
{"namespace": "mopidy.internal.network.try_ipv6_socket", "completion": "    try:\n        socket.socket(socket.AF_INET6, socket.SOCK_STREAM)\n        return True\n    except:\n        logger.debug(\"IPv6 is not supported on this system\")\n        return False", "idx": 1787}
{"namespace": "mopidy.internal.network.format_hostname", "completion": "    if re.match(r'^([0-9a-fA-F]{1,4}:){7}[0-9a-fA-F]{1,4}$', hostname):\n        return f\"::ffff:{socket.inet_ntop(socket.AF_INET6, socket.inet_pton(socket.AF_INET6, hostname))}\"\n    return hostname", "idx": 1788}
{"namespace": "mopidy.internal.xdg.get_dirs", "completion": "    xdg_dirs = {}\n    xdg_vars = {\n        \"XDG_CACHE_HOME\": \"XDG_CACHE_DIR\",\n        \"XDG_CONFIG_HOME\": \"XDG_CONFIG_DIR\",\n        \"XDG_DATA_HOME\": \"XDG_DATA_DIR\",\n        \"XDG_DOCUMENTS_DIR\": \"XDG_DOCUMENTS_DIR\",\n        \"XDG_DOWNLOAD_DIR\": \"XDG_DOWNLOAD_DIR\",\n        \"XDG_MUSIC_DIR\": \"XDG_MUSIC_DIR\",\n        \"XDG_PICTURES_DIR\": \"XDG_PICTURES_DIR\",\n        \"XDG_PUBLICSHARE_DIR\": \"XDG_PUBLICSHARE_DIR\",\n        \"XDG_TEMPLATES_DIR\": \"XDG_TEMPLATES_DIR\",\n        \"XDG_VIDEOS_DIR\": \"XDG_VIDEOS_DIR\"\n    }\n\n    for var, name in xdg_vars.items():\n        value = os.environ.get(var, None)\n        if value:\n            xdg_dirs[name] = pathlib.Path(value).expanduser()\n\n    user_dirs_file = os.path.expanduser(\"~/.config/user-dirs.dirs\")\n    if os.path.exists(user_dirs_file):\n        config = configparser.ConfigParser()\n        config.read(user_dirs_file)\n        for key, value in config[\"XDG\"].items():\n            if key in xdg_vars.values():\n                xdg_dirs[key] = pathlib.Path(value.strip('\"')).expanduser()\n\n    return xdg_dirs", "idx": 1789}
{"namespace": "mopidy.internal.log.get_verbosity_level", "completion": "    verbosity_level = args_verbosity_level if args_verbosity_level is not None else logging_config[\"verbosity\"]\n    verbosity_level += base_verbosity_level\n\n    min_level = min(LOG_LEVELS.keys())\n    max_level = max(LOG_LEVELS.keys())\n\n    if verbosity_level < min_level:\n        return min_level\n    elif verbosity_level > max_level:\n        return max_level\n    else:\n        return verbosity_level", "idx": 1790}
{"namespace": "mopidy.internal.validation.check_instance", "completion": "    if not isinstance(arg, cls):\n        raise exceptions.ValidationError(msg.format(name=cls.__name__, arg=arg))", "idx": 1791}
{"namespace": "mopidy.internal.validation.check_instances", "completion": "    _check_iterable(arg, msg, name=cls.__name__)\n    for a in arg:\n        check_instance(a, cls, msg=msg)", "idx": 1792}
{"namespace": "mopidy.internal.validation.check_uri", "completion": "    if not isinstance(arg, str):\n        raise exceptions.ValidationError(msg.format(arg=arg))\n    elif not urllib.parse.urlparse(arg).scheme:\n        raise exceptions.ValidationError(msg.format(arg=arg))", "idx": 1793}
{"namespace": "mopidy.internal.validation.check_uris", "completion": "    _check_iterable(arg, msg)\n    for uri in arg:\n        check_uri(uri)", "idx": 1794}
{"namespace": "mopidy.internal.playlists.parse", "completion": "    # Add code to parse the data and return a list of parsed items\n    pass", "idx": 1795}
{"namespace": "mopidy.config.schemas.ConfigSchema.deserialize", "completion": "        result = {}\n        errors = {}\n\n        for key, value in values.items():\n            if key in self:\n                try:\n                    result[key] = self[key].deserialize(value)\n                except Exception as e:\n                    errors[key] = str(e)\n                    result[key] = None\n            else:\n                suggestion = _did_you_mean(key, list(self.keys()))\n                if suggestion:\n                    errors[key] = f\"Unknown key '{key}'. Did you mean '{suggestion}'?\"\n                else:\n                    errors[key] = f\"Unknown key '{key}'.\"\n\n        deprecated_keys = [key for key in result if self[key].deprecated]\n        for key in deprecated_keys:\n            del result[key]\n\n        return result, errors", "idx": 1796}
{"namespace": "mopidy.config.types.String.deserialize", "completion": "        value = value.strip()\n        \n        if not value and self._required:\n            raise ValueError(\"Value is required\")\n\n        if not value:\n            return None\n\n        if self._transformer:\n            value = self._transformer(value)\n\n        if self._choices and value not in self._choices:\n            raise ValueError(\"Value is not in the list of choices\")\n\n        return value", "idx": 1797}
{"namespace": "mopidy.config.types.String.serialize", "completion": "        if value is None:\n            return \"\"\n        if isinstance(value, _TransformedValue):\n            value = value.original\n        return encode(value)", "idx": 1798}
{"namespace": "mopidy.config.types.Secret.serialize", "completion": "        if value is not None and display:\n            return \"********\"\n        else:\n            return super().serialize(value, display)", "idx": 1799}
{"namespace": "mopidy.config.types.Integer.deserialize", "completion": "        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        try:\n            value = int(value)\n        except ValueError:\n            raise ValueError(f\"Invalid integer value: {value}\")\n\n        if self._minimum is not None and value < self._minimum:\n            raise ValueError(\n                f\"Value {value} is less than minimum {self._minimum}\"\n            )\n\n        if self._maximum is not None and value > self._maximum:\n            raise ValueError(\n                f\"Value {value} is greater than maximum {self._maximum}\"\n            )\n\n        validators.validate_choice(value, self._choices)\n        return value", "idx": 1800}
{"namespace": "mopidy.config.types.Float.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = float(value)\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        return value", "idx": 1801}
{"namespace": "mopidy.config.types.Boolean.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = value.lower()\n        if value in self.true_values:\n            return True\n        elif value in self.false_values:\n            return False\n        else:\n            raise ValueError(f\"Invalid boolean value: {value}\")", "idx": 1802}
{"namespace": "mopidy.config.types.Pair.deserialize", "completion": "        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        if self._separator in value:\n            parts = value.split(self._separator, 1)\n            if len(parts) == 2:\n                return (\n                    self._subtypes[0].deserialize(parts[0]),\n                    self._subtypes[1].deserialize(parts[1]),\n                )\n            elif self._optional_pair:\n                return (self._subtypes[0].deserialize(value), self._subtypes[1].deserialize(value))\n            else:\n                raise ValueError(f\"config value must include {self._separator}\")\n        else:\n            return (self._subtypes[0].deserialize(value), self._subtypes[1].deserialize(value))", "idx": 1803}
{"namespace": "mopidy.config.types.Pair.serialize", "completion": "        if value is None:\n            return \"\"\n\n        serialized_first = self._subtypes[0].serialize(value[0], display)\n        serialized_second = self._subtypes[1].serialize(value[1], display)\n\n        if not display and self._optional_pair and serialized_first == serialized_second:\n            return serialized_first\n        else:\n            return f\"{serialized_first}{self._separator}{serialized_second}\"", "idx": 1804}
{"namespace": "mopidy.config.types.List.serialize", "completion": "    serialized_values = [self._subtype.serialize(item, display) for item in value]\n    return \"\\n\".join(serialized_values)", "idx": 1805}
{"namespace": "mopidy.config.types.LogColor.deserialize", "completion": "        value = decode(value).strip().lower()\n        validators.validate_choice(value, (\"black\", \"red\", \"green\", \"yellow\", \"blue\", \"magenta\", \"cyan\", \"white\"))\n        return value", "idx": 1806}
{"namespace": "mopidy.config.types.LogColor.serialize", "completion": "        if value is None:\n            return \"\"\n        if display:\n            return log.COLORS.get(value, \"\")\n        else:\n            return value", "idx": 1807}
{"namespace": "mopidy.config.types.LogLevel.deserialize", "completion": "        value = decode(value)\n        value = value.lower()\n        validators.validate_choice(value, self.levels.keys())\n        return self.levels[value]", "idx": 1808}
{"namespace": "mopidy.config.types.LogLevel.serialize", "completion": "        if value in self.levels.values():\n            return next((k for k, v in self.levels.items() if v == value), \"\")\n        return \"\"", "idx": 1809}
{"namespace": "mopidy.config.types.Hostname.deserialize", "completion": "        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        if value.startswith(\"unix:\"):\n            return value\n\n        try:\n            socket.getaddrinfo(value, None)\n            return value\n        except socket.gaierror:\n            raise ValueError(f\"invalid value for hostname: {value!r}\")", "idx": 1810}
{"namespace": "mopidy.config.load", "completion": "    config_dir = os.path.dirname(files[0])\n    defaults = [read(os.path.join(config_dir, \"default.conf\"))]\n    defaults.extend(ext_defaults)\n\n    raw_config = configparser.ConfigParser()\n    raw_config.read_string(\"\\n\".join(defaults))\n    raw_config.read(files)\n\n    for override in overrides:\n        section, key_value = override.split(\"=\", 1)\n        key, value = key_value.split(\"=\", 1)\n        if not raw_config.has_section(section):\n            raw_config.add_section(section)\n        raw_config.set(section, key, value)\n\n    schemas = _schemas + ext_schemas\n    for schema in schemas:\n        schema.validate(raw_config)\n\n    return raw_config", "idx": 1811}
{"namespace": "mopidy.config.format_initial", "completion": "        from mopidy.config import keyring\n\n    ext_schemas = []\n    ext_defaults = []\n    overrides = []\n\n    for extension_data in extensions_data:\n        ext_schemas.append(extension_data[\"schema\"])\n        ext_defaults.append(extension_data[\"default\"])\n        overrides.extend(extension_data[\"overrides\"])\n\n    from mopidy.config import keyring\n    config_dir = pathlib.Path(__file__).parent\n    defaults = [read(config_dir / \"default.conf\")]\n    defaults.extend(ext_defaults)\n    raw_config = _load([], defaults, keyring.fetch() + (overrides or []))\n\n    schemas = _schemas[:]\n    schemas.extend(ext_schemas)\n    return _validate(raw_config, schemas)", "idx": 1812}
{"namespace": "mopidy.config._load", "completion": "    config = configparser.RawConfigParser(comment_prefixes=(\"#\", \";\"))\n    raw_config = {}\n\n    for default in defaults:\n        config.read_string(default)\n\n    for file in files:\n        if os.path.isdir(file):\n            for filename in os.listdir(file):\n                if filename.endswith(\".conf\"):\n                    with open(os.path.join(file, filename), \"r\") as f:\n                        config.read_file(f)\n        else:\n            with open(file, \"r\") as f:\n                config.read_file(f)\n\n    for section in config.sections():\n        raw_config[section] = dict(config.items(section))\n\n    for section, key, value in overrides:\n        if section not in raw_config:\n            raw_config[section] = {}\n        raw_config[section][key] = value\n\n    return raw_config", "idx": 1813}
{"namespace": "mopidy.config._validate", "completion": "    validated_config = {}\n    errors = {}\n\n    for schema in schemas:\n        for section, options in raw_config.items():\n            if section == schema.name:\n                try:\n                    validated_config[section], section_errors = schema.deserialize(options)\n                    if section_errors:\n                        errors[section] = section_errors\n                except Exception as e:\n                    errors[section] = str(e)\n\n    for section in raw_config.keys():\n        if section not in validated_config:\n            logger.warning(f\"Ignoring unknown config section '{section}'\")\n\n    return validated_config, errors", "idx": 1814}
{"namespace": "mingus.extra.tunings.get_tunings", "completion": "    matching_tunings = []\n    keys = list(_known.keys())\n    for x in keys:\n        if instrument is None or str.upper(x).startswith(str.upper(instrument)):\n            for (desc, tun) in six.iteritems(_known[x][1]):\n                if (nr_of_strings is None or tun.count_strings() == nr_of_strings) and (nr_of_courses is None or tun.count_courses() == nr_of_courses):\n                    matching_tunings.append(tun)\n    return matching_tunings", "idx": 1815}
{"namespace": "mingus.containers.instrument.Instrument.note_in_range", "completion": "        if isinstance(note, six.string_types):\n            note = Note(note)\n        if not hasattr(note, \"name\"):\n            raise UnexpectedObjectError(\n                \"Unexpected object '%s'. \"\n                \"Expecting a mingus.containers.Note object\" % note\n            )\n        return self.range[0] <= note <= self.range[1]", "idx": 1816}
{"namespace": "mingus.containers.instrument.Guitar.can_play_notes", "completion": "        if len(notes) > 6:\n            return False\n        else:\n            return super().can_play_notes(notes)", "idx": 1817}
{"namespace": "mingus.containers.bar.Bar.get_range", "completion": "        all_notes = []\n        for entry in self.bar:\n            notes = entry[2].notes\n            all_notes.extend(notes)\n        all_notes.sort()\n        return all_notes[-1], all_notes[0]", "idx": 1818}
{"namespace": "mingus.containers.bar.Bar.transpose", "completion": "        for cont in self.bar:\n            cont[2].transpose(interval, up)", "idx": 1819}
{"namespace": "mingus.containers.bar.Bar.determine_chords", "completion": "        possible_chords = []\n        for i in range(int(self.length * self.meter[1])):\n            possible_chords.append([i, self.key.get_chord_at(i)])\n        return possible_chords", "idx": 1820}
{"namespace": "mingus.containers.note.Note.transpose", "completion": "        if up:\n            direction = 1\n        else:\n            direction = -1\n\n        # Define the intervals\n        intervals = {\n            \"P1\": 0,\n            \"m2\": 1,\n            \"M2\": 2,\n            \"m3\": 3,\n            \"M3\": 4,\n            \"P4\": 5,\n            \"A4\": 6,\n            \"d5\": 6,\n            \"P5\": 7,\n            \"m6\": 8,\n            \"M6\": 9,\n            \"m7\": 10,\n            \"M7\": 11,\n            \"P8\": 12\n        }\n\n        # Transpose the note\n        current_index = notes.note_to_int(self.name)\n        new_index = (current_index + (intervals[interval] * direction)) % 12\n        self.name = notes.int_to_note(new_index)", "idx": 1821}
{"namespace": "mingus.containers.note.Note.from_int", "completion": "\n        # Define the list of note names\n        note_names = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n\n        # Calculate the octave and note name based on the integer value\n        octave = (integer // 12) - 1\n        note_name = note_names[integer % 12]\n\n        # Set the note instance with the calculated name and octave\n        self.set_note(note_name, octave)\n\n        return self", "idx": 1822}
{"namespace": "mingus.containers.note.Note.to_hertz", "completion": "        # Calculate the number of semitones from A-4 to the current note\n        semitones_from_a4 = self.measure(Note('A', 4))\n\n        # Calculate the frequency using the formula: frequency = standard_pitch * (2 ** (semitones_from_a4 / 12))\n        frequency = standard_pitch * (2 ** (semitones_from_a4 / 12))\n\n        return frequency", "idx": 1823}
{"namespace": "mingus.containers.note.Note.from_hertz", "completion": "        # Calculate the difference in semitones from A-4\n        diff = 12 * log(hertz / standard_pitch, 2)\n        # Calculate the note and octave based on the difference\n        note_int = 57 + round(diff)\n        octave = note_int // 12\n        note_name = notes.int_to_note(note_int % 12)\n        # Set the note and octave\n        self.set_note(note_name, octave)\n        return self", "idx": 1824}
{"namespace": "mingus.containers.note.Note.to_shorthand", "completion": "        return self.name.upper() + (\",\" * (4 - self.octave)) + (\"'\" * (self.octave - 4))", "idx": 1825}
{"namespace": "mingus.containers.note_container.NoteContainer.from_chord_shorthand", "completion": "        \"\"\"\n        This function clears the NoteContainer and adds the notes corresponding to the shorthand notation.\n\n        Input-Output Arguments\n        :param self: NoteContainer. An instance of the NoteContainer class.\n        :param shorthand: str. The shorthand notation representing the chords.\n        :return: NoteContainer. The updated NoteContainer instance.\n\n        \"\"\"\n        self.empty()  # Clear the NoteContainer\n        chord_notes = chords.from_shorthand(shorthand)  # Get the notes corresponding to the shorthand notation\n        for note in chord_notes:\n            self.add_note(note)  # Add each note to the NoteContainer\n        return self  # Return the updated NoteContainer instance", "idx": 1826}
{"namespace": "mingus.containers.note_container.NoteContainer.from_interval_shorthand", "completion": "        self.empty()\n        if isinstance(startnote, str):\n            startnote = Note(startnote)\n        interval = intervals.from_shorthand(shorthand)\n        if not up:\n            interval = -interval\n        new_note = startnote + interval\n        self.add_note(new_note)\n        return self", "idx": 1827}
{"namespace": "mingus.containers.note_container.NoteContainer.from_progression_shorthand", "completion": "        self.empty()\n        chord_progression = chords.from_shorthand(shorthand, key)\n        for chord in chord_progression:\n            self.add_notes(chord)\n        return self", "idx": 1828}
{"namespace": "mingus.containers.note_container.NoteContainer.transpose", "completion": "        for note in self.notes:\n            note.transpose(interval, up)\n        return self", "idx": 1829}
{"namespace": "mingus.containers.note_container.NoteContainer.get_note_names", "completion": "        unique_note_names = []\n        for note in self.notes:\n            if note.name not in unique_note_names:\n                unique_note_names.append(note.name)\n        return unique_note_names", "idx": 1830}
{"namespace": "mingus.core.notes.int_to_note", "completion": "    if note_int < 0 or note_int > 11:\n        raise RangeError(\"Note integer must be in the range 0-11\")\n\n    note_name = fifths[note_int % 7]\n    if accidentals == \"#\":\n        note_name += \"#\" * (note_int // 7)\n    else:\n        note_name += \"b\" * (note_int // 7)\n\n    return note_name", "idx": 1831}
{"namespace": "mingus.core.notes.is_valid_note", "completion": "    return note in _note_dict", "idx": 1832}
{"namespace": "mingus.core.notes.reduce_accidentals", "completion": "    note_int = note_to_int(note)\n    return int_to_note(note_int)", "idx": 1833}
{"namespace": "mingus.core.notes.remove_redundant_accidentals", "completion": "    val = note_to_int(note[0])\n    for token in note[1:]:\n        if token == \"b\":\n            val -= 1\n        elif token == \"#\":\n            val += 1\n        else:\n            raise NoteFormatError(\"Unknown note format '%s'\" % note)\n    return int_to_note(val % 12)", "idx": 1834}
{"namespace": "mingus.core.intervals.minor_second", "completion": "    return notes.augment(note)", "idx": 1835}
{"namespace": "mingus.core.intervals.major_second", "completion": "    sec = second(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sec, 2)", "idx": 1836}
{"namespace": "mingus.core.intervals.minor_third", "completion": "    third_note = third(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, third_note, 3)", "idx": 1837}
{"namespace": "mingus.core.intervals.minor_fourth", "completion": "    frth = fourth(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, frth, 5)", "idx": 1838}
{"namespace": "mingus.core.intervals.minor_seventh", "completion": "    svt = seventh(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, svt, 10)", "idx": 1839}
{"namespace": "mingus.core.intervals.major_seventh", "completion": "    sth = seventh(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sth, 11)", "idx": 1840}
{"namespace": "mingus.core.intervals.measure", "completion": "\n    return (notes.note_to_int(note2) - notes.note_to_int(note1)) % 12", "idx": 1841}
{"namespace": "mingus.core.intervals.determine", "completion": "\n    # Determine the interval between note1 and note2\n    interval_name = \"\"\n    if shorthand:\n        # Use shorthand notation for the interval name\n        interval_name = get_shorthand_interval_name(note1, note2)\n    else:\n        # Use full notation for the interval name\n        interval_name = get_full_interval_name(note1, note2)\n\n    return interval_name", "idx": 1842}
{"namespace": "mingus.core.intervals.from_shorthand", "completion": "\n    # Check if the input note is valid\n    if not notes.is_valid_note(note):\n        return False\n\n    # Convert the interval to a number\n    interval_number = int(interval[0])\n\n    # Adjust the interval based on the sharp or flat symbols\n    for i in range(1, len(interval)):\n        if interval[i] == '#':\n            interval_number += 1\n        elif interval[i] == 'b':\n            interval_number -= 1\n\n    # Determine the key based on the starting note\n    key = note[0]\n\n    # Move up or down based on the 'up' parameter\n    if up:\n        return get_interval(note, interval_number, key)\n    else:\n        return get_interval(note, -interval_number, key)", "idx": 1843}
{"namespace": "mingus.core.intervals.is_consonant", "completion": "\n    # Determine the interval between note1 and note2\n    interval_name = determine(note1, note2, shorthand=True)\n\n    # List of consonant intervals\n    consonant_intervals = [\"unison\", \"third\", \"fifth\", \"sixth\", \"octave\"]\n\n    # Check if the interval is in the list of consonant intervals\n    if interval_name in consonant_intervals:\n        return True\n    # Check if perfect fourth is included and if the interval is a perfect fourth\n    elif include_fourths and interval_name == \"fourth\":\n        return True\n    else:\n        return False", "idx": 1844}
{"namespace": "mingus.core.intervals.is_perfect_consonant", "completion": "    interval = determine(note1, note2, shorthand=True)\n    if interval in [\"unison\", \"fifth\", \"octave\"]:\n        return True\n    elif include_fourths and interval == \"fourth\":\n        return True\n    else:\n        return False", "idx": 1845}
{"namespace": "mingus.core.keys.get_key", "completion": "    if accidentals < -7 or accidentals > 7:\n        raise ValueError(\"Accidentals must be between -7 and 7\")\n\n    key_index = 7 + accidentals\n    major_key = keys[key_index][0]\n    minor_key = keys[key_index][1]\n\n    return major_key, minor_key", "idx": 1846}
{"namespace": "mingus.core.keys.get_key_signature", "completion": "    if key in major_keys:\n        return major_keys.index(key) - 7\n    elif key in minor_keys:\n        return minor_keys.index(key) + 1\n    else:\n        raise NoteFormatError(\"Invalid key format\")", "idx": 1847}
{"namespace": "mingus.core.keys.get_key_signature_accidentals", "completion": "    accidentals = get_key_signature(key)\n    if accidentals > 0:\n        return [\"#\" * accidentals]\n    elif accidentals < 0:\n        return [\"b\" * abs(accidentals)]\n    else:\n        return []", "idx": 1848}
{"namespace": "mingus.core.keys.get_notes", "completion": "        from mingus.core import notes\n    from mingus.core import notes\n\n    if not is_valid_key(key):\n        raise NoteFormatError(\"unrecognized format for key '%s'\" % key)\n\n    accidentals = get_key_signature(key)\n    scale = base_scale\n\n    if accidentals < 0:\n        scale = list(reversed(notes.fifths))[-accidentals:] + scale\n    elif accidentals > 0:\n        scale = scale + notes.fifths[:accidentals]\n\n    return scale", "idx": 1849}
{"namespace": "mingus.core.keys.relative_major", "completion": "    if key not in minor_keys:\n        raise NoteFormatError(\"unrecognized format for key '%s'\" % key)\n\n    minor_index = minor_keys.index(key)\n    return major_keys[minor_index]", "idx": 1850}
{"namespace": "mingus.core.chords.augmented_triad", "completion": "    return [note, intervals.major_third(note), intervals.augmented_fifth(note)]", "idx": 1851}
{"namespace": "mingus.core.chords.determine", "completion": "\n    if len(chord) == 3:\n        if no_inversions:\n            return \"Triad\"\n        else:\n            return \"Root Position Triad\"\n    elif len(chord) == 4:\n        if no_inversions:\n            return \"Seventh Chord\"\n        else:\n            return \"Root Position Seventh Chord\"\n    elif len(chord) > 4:\n        if no_polychords:\n            return \"Extended Chord\"\n        else:\n            return \"Polychord\"\n    else:\n        return \"Unknown Chord\"", "idx": 1852}
{"namespace": "mingus.core.value.septuplet", "completion": "    if in_fourths:\n        return tuplet(value, 7, 4)\n    else:\n        return tuplet(value, 7, 8)", "idx": 1853}
{"namespace": "mingus.core.value.determine", "completion": "\n    if value in base_values:\n        return (value, 0, 1)\n    elif value in base_triplets:\n        return (value * 2, 0, 3)\n    elif value in base_quintuplets:\n        return (value * 4, 0, 5)\n    elif value in base_septuplets:\n        return (value * 4, 0, 7)\n    else:\n        dots = 0\n        while value % 2 == 0:\n            value /= 2\n            dots += 1\n        return (value, dots, 1)", "idx": 1854}
{"namespace": "mingus.core.progressions.substitute_major_for_minor", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Major to minor substitution\n    if (\n        suff == \"M\"\n        or suff == \"M7\"\n        or suff == \"\"\n        and roman in [\"I\", \"IV\", \"V\"]\n        or ignore_suffix\n    ):\n        n = skip(roman, -2)\n        a = interval_diff(roman, n, -3) + acc\n        if suff == \"M\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"m\")))\n        elif suff == \"M7\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"m7\")))\n        elif suff == \"\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"\")))\n    return res", "idx": 1855}
{"namespace": "mingus.core.progressions.substitute_diminished_for_diminished", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Diminished to diminished substitution\n    if (\n        suff == \"dim7\"\n        or suff == \"dim\"\n        or suff == \"\"\n        and roman == \"VII\"\n        or ignore_suffix\n    ):\n        for i in range(3):\n            n = skip(roman, 7)\n            a = interval_diff(roman, n, 6) + acc\n            res.append(tuple_to_string((n, a, \"dim\")))\n            roman = n\n            acc = a\n\n    return res", "idx": 1856}
{"namespace": "mingus.core.progressions.substitute_diminished_for_dominant", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Diminished to dominant substitution\n    if (\n        suff == \"dim7\"\n        or suff == \"dim\"\n        or suff == \"\"\n        and roman in [\"VII\"]\n        or ignore_suffix\n    ):\n        if suff == \"\":\n            suff = \"dim\"\n\n        # Add dominant chord\n        last = roman\n        for x in range(3):\n            next = skip(last, 5)\n            acc += interval_diff(last, next, 9)\n            res.append(tuple_to_string((next, acc, \"7\")))\n            last = next\n    return res", "idx": 1857}
{"namespace": "mingus.core.progressions.substitute", "completion": "    if depth == 0:\n        return substitute_harmonic(progression, substitute_index)\n    else:\n        substitutions = []\n        substitutions.extend(substitute_harmonic(progression, substitute_index))\n        substitutions.extend(substitute_minor_for_major(progression, substitute_index))\n        substitutions.extend(substitute_major_for_minor(progression, substitute_index))\n        substitutions.extend(substitute_diminished_for_diminished(progression, substitute_index))\n        substitutions.extend(substitute_diminished_for_dominant(progression, substitute_index))\n        \n        if depth > 1:\n            new_progression = progression.copy()\n            for sub in substitutions:\n                new_progression[substitute_index] = sub\n                next_substitutions = substitute(new_progression, substitute_index, depth-1)\n                substitutions.extend(next_substitutions)\n        \n        return substitutions", "idx": 1858}
{"namespace": "mingus.core.progressions.skip", "completion": "    index = numerals.index(roman_numeral)\n    new_index = (index - skip_count) % 7\n    return numerals[new_index]", "idx": 1859}
{"namespace": "exodus_bundler.cli.configure_logging", "completion": "    if quiet:\n        level = logging.ERROR\n    elif verbose:\n        level = logging.INFO\n    else:\n        level = logging.WARNING\n\n    # Set up the logger\n    logger.setLevel(level)\n\n    # Set up the stderr handler\n    stderr_handler = logging.StreamHandler(sys.stderr)\n    stderr_handler.setLevel(level)\n    formatter = logging.Formatter('%(levelname)s: %(message)s')\n    stderr_handler.setFormatter(formatter)\n    logger.addHandler(stderr_handler)\n\n    # Set up the stdout handler if suppress_stdout is False\n    if not suppress_stdout:\n        stdout_handler = logging.StreamHandler(sys.stdout)\n        stdout_handler.setLevel(logging.DEBUG)\n        formatter = logging.Formatter('%(levelname)s: %(message)s')\n        stdout_handler.setFormatter(formatter)\n        logger.addHandler(stdout_handler)", "idx": 1860}
{"namespace": "exodus_bundler.bundling.create_unpackaged_bundle", "completion": "\n    # Create a temporary directory to hold the unpackaged bundle.\n    root_directory = tempfile.mkdtemp()\n\n    try:\n        # Copy the main executable files and their dependencies to the temporary directory.\n        for executable in executables:\n            # Copy the main executable file.\n            shutil.copy2(executable, root_directory)\n\n            # Copy the dependencies of the main executable file.\n            dependencies = find_dependencies(executable, chroot=chroot, detect=detect)\n            for dependency in dependencies:\n                shutil.copy2(dependency, root_directory)\n\n        # Add additional files to the bundle.\n        for additional_file in add:\n            shutil.copy2(additional_file, root_directory)\n\n        # Create shell launchers for the executables if required.\n        if shell_launchers:\n            for executable in executables:\n                create_shell_launcher(executable, root_directory, rename=rename)\n\n        # Return the path to the temporary directory containing the unpackaged bundle.\n        return root_directory\n\n    except Exception as e:\n        # Handle any exceptions and log the error.\n        logger.error(f\"Error creating unpackaged bundle: {e}\")\n        raise", "idx": 1861}
{"namespace": "exodus_bundler.bundling.detect_elf_binary", "completion": "    with open(filename, 'rb') as file:\n        header = file.read(4)\n        return header == b'\\x7fELF'", "idx": 1862}
{"namespace": "exodus_bundler.bundling.resolve_binary", "completion": "    if os.path.exists(binary):\n        return os.path.abspath(binary)\n    else:\n        for path in os.environ[\"PATH\"].split(os.pathsep):\n            full_path = os.path.join(path, binary)\n            if os.path.exists(full_path):\n                return os.path.abspath(full_path)\n    raise MissingFileError('The \"%s\" binary could not be found in $PATH.' % binary)", "idx": 1863}
{"namespace": "exodus_bundler.bundling.resolve_file_path", "completion": "    if search_environment_path:\n        return resolve_binary(path)\n\n    absolute_path = os.path.normpath(os.path.abspath(path))\n    if not os.path.exists(absolute_path):\n        raise MissingFileError('The file \"%s\" could not be found.' % path)\n    if os.path.isdir(absolute_path):\n        raise UnexpectedDirectoryError('The file \"%s\" is a directory.' % path)\n    return absolute_path", "idx": 1864}
{"namespace": "exodus_bundler.bundling.run_ldd", "completion": "    if not detect_elf_binary(binary):\n        raise InvalidElfBinaryError('The provided file is not a valid ELF binary.')\n\n    try:\n        process = Popen([ldd, binary], stdout=PIPE, stderr=PIPE)\n        stdout, stderr = process.communicate()\n        output = stdout.decode('utf-8').split('\\n') + stderr.decode('utf-8').split('\\n')\n        return [line for line in output if line.strip()]\n    except Exception as e:\n        raise RuntimeError(f'Error running ldd command: {e}')", "idx": 1865}
{"namespace": "exodus_bundler.bundling.Elf.dependencies", "completion": "        direct_dependencies = self.find_direct_dependencies()\n        all_dependencies = set(direct_dependencies)\n        new_dependencies = set(direct_dependencies)\n\n        while new_dependencies:\n            current_dependencies = set(new_dependencies)\n            new_dependencies = set()\n            for dependency in current_dependencies:\n                new_dependencies |= dependency.find_direct_dependencies()\n            new_dependencies -= all_dependencies\n            all_dependencies |= new_dependencies\n\n        return all_dependencies", "idx": 1866}
{"namespace": "exodus_bundler.bundling.File.hash", "completion": "        sha256_hash = hashlib.sha256()\n        with open(self.path, \"rb\") as f:\n            # Read and update hash string value in blocks of 4K\n            for byte_block in iter(lambda: f.read(4096), b\"\"):\n                sha256_hash.update(byte_block)\n        return sha256_hash.hexdigest()", "idx": 1867}
{"namespace": "exodus_bundler.bundling.Bundle.add_file", "completion": "        file = None\n        if os.path.isdir(path):\n            for root, dirs, files in os.walk(path):\n                for file_name in files:\n                    file_path = os.path.join(root, file_name)\n                    self.add_file(file_path)\n            return None\n        else:\n            file = File(path, entry_point=entry_point, chroot=self.chroot, file_factory=self.add_file)\n            self.files.add(file)\n            if file.elf:\n                for dependency in file.elf.dependencies:\n                    self.add_file(dependency.path)\n        return file", "idx": 1868}
{"namespace": "exodus_bundler.bundling.Bundle.bundle_root", "completion": "        bundle_hash = hashlib.sha256(''.join(sorted(file.path for file in self.files)).encode('utf-8')).hexdigest()\n        root_directory = os.path.join(self.working_directory, 'bundles', bundle_hash)\n        return os.path.normpath(os.path.abspath(root_directory))", "idx": 1869}
{"namespace": "exodus_bundler.bundling.Bundle.hash", "completion": "        file_hashes = [file.hash for file in sorted(self.files, key=lambda x: x.path)]\n        combined_hash_string = ''.join(file_hashes).encode('utf-8')\n        return hashlib.sha256(combined_hash_string).hexdigest()", "idx": 1870}
{"namespace": "exodus_bundler.launchers.construct_bash_launcher", "completion": "    template_path = os.path.join(parent_directory, 'templates', 'launcher.sh')\n    return render_template_file(template_path, linker=linker, library_path=library_path, executable=executable, full_linker=full_linker)", "idx": 1871}
{"namespace": "exodus_bundler.input_parsing.extract_open_path", "completion": "    line = strip_pid_prefix(line)\n    if line.startswith(\"open(\"):\n        parts = re.findall(r'\"(.*?)\"', line)\n        if len(parts) > 0:\n            return parts[0]\n    return None", "idx": 1872}
{"namespace": "exodus_bundler.input_parsing.extract_paths", "completion": "    paths = []\n    if isinstance(content, str):\n        content = content.split('\\n')\n    for line in content:\n        path = extract_exec_path(line) or extract_open_path(line) or extract_stat_path(line)\n        if path and os.path.exists(path) and not os.path.isdir(path):\n            paths.append(path)\n    return paths", "idx": 1873}
