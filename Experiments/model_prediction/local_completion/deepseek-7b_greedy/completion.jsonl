{"namespace": "benedict.utils.type_util.is_json_serializable", "completion": "    if isinstance(val, (str, int, float, bool, type(None))):\n        return True\n    elif isinstance(val, dict):\n        return all(is_json_serializable(v) for v in val.values())\n    elif isinstance(val, (list, tuple)):\n        return all(is_json_serializable(v) for v in val)\n    elif isinstance(val, set):\n        return all(is_json_serializable(v) for v in val)\n    elif isinstance(val, pathlib.Path):\n        return True\n    elif isinstance(val, datetime):\n        return True\n    elif isinstance(val, Decimal):\n        return True\n    else:\n        return False", "idx": 0}
{"namespace": "feedparser.urls.convert_to_idn", "completion": "    if not url:\n        return url\n\n    parsed_url = urllib.parse.urlparse(url)\n    if not parsed_url.hostname:\n        return url\n\n    try:\n        hostname = parsed_url.hostname.encode('idna')\n    except UnicodeError:\n        hostname = parsed_url.hostname\n\n    return parsed_url._replace(hostname=hostname).geturl()", "idx": 1}
{"namespace": "mistune.toc.add_toc_hook", "completion": "    def _add_toc_hook(self, tokens, idx, **kwargs):\n        if tokens[idx]['type'] == 'heading_open':\n            level = int(tokens[idx]['tag'][1:])\n            if level >= min_level and level <= max_level:\n                title = tokens[idx + 1]['content']\n                if heading_id:\n                    heading_id(title)\n                self.env.toc.append({\n                    'level': level,\n                    'title': title,\n                    'id': self.env.toc_id,\n                })\n                self.env.toc_id += 1\n\n    md.pre_render.append(_add_toc_hook)\n\n", "idx": 2}
{"namespace": "mistune.plugins.table.table_in_quote", "completion": "    md.block.rules.insert(md.block.rules.index('paragraph') + 1, 'table')\n    md.block.rules.insert(md.block.rules.index('paragraph') + 1, 'nptable')", "idx": 3}
{"namespace": "mistune.plugins.table.table_in_list", "completion": "    md.block.insert_rule(md.block.list_rules, 'table', before='paragraph')\n    md.block.insert_rule(md.block.list_rules, 'nptable', before='paragraph')", "idx": 4}
{"namespace": "xmnlp.utils.parallel_handler", "completion": "    if not isinstance(texts, list):\n        raise ValueError(\"You should pass a list of texts\")\n\n    with futures.ThreadPoolExecutor(max_workers=n_jobs) as executor:\n        for result in executor.map(partial(callback, **kwargs), texts):\n            yield result", "idx": 5}
{"namespace": "parsel.utils.shorten", "completion": "    if width < 0:\n        raise ValueError(\"width must be equal or greater than 0\")\n    if width == 0:\n        return suffix\n    if len(text) <= width:\n        return text\n    return text[: width - len(suffix)] + suffix", "idx": 6}
{"namespace": "parsel.xpathfuncs.set_xpathfunc", "completion": "    if func is None:\n        etree.XPath.unregister_function(fname)\n    else:\n        etree.XPath.register_function(fname, func)", "idx": 7}
{"namespace": "dominate.dom_tag._get_thread_context", "completion": "  context = []\n  if greenlet:\n    context.append(greenlet.getcurrent())\n  context.append(threading.current_thread())\n  return hash(tuple(context))\n\n", "idx": 8}
{"namespace": "dominate.util.system", "completion": "  if data is None:\n    p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n  else:\n    p = subprocess.Popen(cmd, shell=True, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n  out, err = p.communicate(data)\n  return out.decode('utf-8')\n\n", "idx": 9}
{"namespace": "dominate.util.url_unescape", "completion": "  return data.replace('%2F', '/').replace('%3F', '?').replace('%3A', ':').replace('%3B', ';').replace('%2C', ',').replace('%2B', '+').replace('%20', ' ').replace('%21', '!')\\\n    .replace('%22', '\"')\\\n    .replace('%23', '#').replace('%24', '$').replace('%25', '%').replace('%26', '&').replace('%27', '\\'').replace('%28', '(').replace('%29', ')').replace('%2A', '*')\\\n    .replace('%2C', ',').replace('%2D', '-').replace('%2E', '.').replace('%2F', '/').replace('%3A', ':').replace('%3B', ';').replace('%3C', '<').replace('%3D', '=')\\\n    .replace('%3E', '>').replace('%3F', '?').replace('%40', '@').replace('%5B', '[').replace('%5C', '\\\\').replace('%5D', ']').replace('%5E', '^').replace('%5F', '_')\\\n    .replace('%60', '`').replace('%7B', '{').replace('%7C', '|').replace('%7D', '}').replace('%7E', '~')", "idx": 10}
{"namespace": "rows.fields.DatetimeField.serialize", "completion": "        if value is None:\n            return \"\"\n\n        return value.isoformat()\n", "idx": 11}
{"namespace": "rows.fields.Field.serialize", "completion": "        if value is None:\n            return None\n        return six.text_type(value)", "idx": 12}
{"namespace": "rows.fields.EmailField.serialize", "completion": "        if value is None:\n            return \"\"\n        else:\n            return six.text_type(value)\n", "idx": 13}
{"namespace": "rows.fields.as_string", "completion": "    if isinstance(value, six.text_type):\n        return value\n    elif isinstance(value, six.binary_type):\n        raise ValueError(\"Can't convert binary to string\")\n    else:\n        return six.text_type(value)", "idx": 14}
{"namespace": "rows.fields.get_items", "completion": "    def getter(obj):\n        return tuple(obj[i] if i < len(obj) else None for i in indexes)\n\n    return getter", "idx": 15}
{"namespace": "pycorrector.proper_corrector.load_dict_file", "completion": "    if path and os.path.exists(path):\n        with open(path, 'r', encoding='utf-8') as f:\n            return {line.split('\\t')[0]: line.split('\\t')[1].strip() for line in f}\n    else:\n        return {}\n\n", "idx": 16}
{"namespace": "natasha.span.envelop_spans", "completion": "    for envelope in envelopes:\n        yield [span for span in spans if span.start >= envelope.start and span.stop <= envelope.stop]", "idx": 17}
{"namespace": "googleapiclient._helpers.parse_unique_urlencoded", "completion": "    # Parse the URL-encoded content.\n    parsed_content = urllib.parse.parse_qs(content)\n\n    # Check for repeated keys.\n    repeated_keys = [\n        key for key, value in parsed_content.items() if len(value) > 1\n    ]\n\n    # Raise a ValueError if any repeated keys are found.\n    if repeated_keys:\n        raise ValueError(\n            \"Repeated keys found in URL-encoded content: {}\".format(\n                \", \".join(repeated_keys)\n            )\n        )\n\n    return parsed_content", "idx": 18}
{"namespace": "jinja2.async_utils.auto_aiter", "completion": "    if hasattr(iterable, \"__aiter__\"):\n        async for item in iterable:\n            yield item\n    else:\n        for item in iterable:\n            yield item", "idx": 19}
{"namespace": "jinja2.utils.consume", "completion": "    deque(iterable, maxlen=0)", "idx": 20}
{"namespace": "pycorrector.utils.tokenizer.segment", "completion": "    if cut_type == 'word':\n        if pos:\n            return posseg.cut(sentence)\n        else:\n            return jieba.lcut(sentence)\n    elif cut_type == 'char':\n        if pos:\n            return posseg.lcut(sentence)\n        else:\n            return list(sentence)\n    else:\n        raise ValueError('Unknown cut_type: %s' % cut_type)\n\n", "idx": 21}
{"namespace": "jinja2.utils.object_type_repr", "completion": "    from .runtime import Undefined\n\n    if obj is None:\n        return \"None\"\n    elif obj is Ellipsis:\n        return \"Ellipsis\"\n    elif obj is True:\n        return \"True\"\n    elif obj is False:\n        return \"False\"\n    elif obj is Undefined:\n        return \"Undefined\"\n    elif isinstance(obj, type):\n        if obj.__module__ == \"builtins\":\n            return obj.__name__\n        return f\"{obj.__module__} {obj.__name__}\"\n    elif isinstance(obj, abc.Generator):\n        return f\"{obj.__module__} {obj.__class__.__name__}\"\n    elif isinstance(obj, abc.Iterable):\n        return f\"{obj.__module__} {obj.__class__.__name__}\"\n    elif isinstance(obj, abc.Iterator):\n        return f\"{obj.__module__} {obj.__class__.__name__}\"\n    elif isinstance(obj, abc.Callable):\n        return f\"{obj.__module__} {obj.__class__.__name__}\"\n    elif isinstance(obj, abc.Awaitable):\n        return f\"{obj.__module__} {obj.__class__.__name__}\"\n    elif isinstance(obj, abc.Coroutine):\n        return f\"{obj.__module__} {obj.__class__.__name__}\"\n    elif isinstance(obj, abc.AsyncIterable):\n        return f\"{obj.__module__} {obj.__class__.__name__}\"\n    elif isinstance(obj, abc.AsyncIterator):\n        return f\"{obj.__module__} {obj.__class__.__name__}\"\n    elif isinstance(obj, abc.AsyncGenerator):\n        return f\"{obj.__module__} {obj.__class__.__name__}\"\n    elif isinstance(obj, abc.Collection):\n        return f\"{obj.__module__} {obj.__class__.__name__}\"\n    elif isinstance(obj, abc.Sized):\n        return f\"{obj.__module__} {obj.__class__", "idx": 22}
{"namespace": "jinja2.utils.LRUCache.setdefault", "completion": "        if key not in self:\n            self[key] = default\n        return self[key]\n", "idx": 23}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_word_freq", "completion": "        word_freq = {}\n        for word in list_of_words:\n            if word in word_freq:\n                word_freq[word] += 1\n            else:\n                word_freq[word] = 1\n        return word_freq\n", "idx": 24}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_average_probability_of_words", "completion": "        content_words_count = len(content_words_in_sentence)\n        if content_words_count == 0:\n            return 0\n        else:\n            return sum(word_freq_in_doc[w] for w in content_words_in_sentence) / content_words_count\n", "idx": 25}
{"namespace": "sumy.summarizers.lex_rank.LexRankSummarizer._compute_idf", "completion": "        idf_metrics = {}\n        sentences_count = len(sentences)\n\n        for sentence in sentences:\n            for term in sentence:\n                if term in idf_metrics:\n                    idf_metrics[term] += 1\n                else:\n                    idf_metrics[term] = 1\n\n        for term, count in idf_metrics.items():\n            idf_metrics[term] = math.log(sentences_count / count)\n\n        return idf_metrics\n", "idx": 26}
{"namespace": "sumy.summarizers.lex_rank.LexRankSummarizer.cosine_similarity", "completion": "        numerator = 0.0\n        denominator1 = 0.0\n        denominator2 = 0.0\n\n        for term in sentence1:\n            tfidf1 = tf1[term] * idf_metrics[term]\n            tfidf2 = tf2[term] * idf_metrics[term]\n            numerator += tfidf1 * tfidf2\n            denominator1 += tfidf1 ** 2\n            denominator2 += tfidf2 ** 2\n\n        return numerator / math.sqrt(denominator1 * denominator2)\n", "idx": 27}
{"namespace": "sumy.evaluation.rouge._get_ngrams", "completion": "    text_length = len(text)\n    return set(\n        [\n            text[i:i + n]\n            for i in range(text_length - n + 1)\n        ]\n    )\n\n", "idx": 28}
{"namespace": "sumy.evaluation.rouge._split_into_words", "completion": "    if not sentences:\n        return []\n    if not all(isinstance(abjad_obj, Sentence) for abjad_obj in sentences):\n        raise ValueError(\"Object in collection must be of type Sentence\")\n    return [word for sentence in sentences for word in sentence.words]\n\n", "idx": 29}
{"namespace": "falcon.inspect.register_router", "completion": "    if router_class in _supported_routers:\n        raise ValueError(\n            'Router class {} is already registered'.format(router_class)\n        )\n\n    def decorator(inspect_function):\n        _supported_routers[router_class] = inspect_function\n        return inspect_function\n\n    return decorator", "idx": 30}
{"namespace": "falcon.inspect.inspect_compiled_router", "completion": "    routes = []\n    for route in router._routes:\n        for method in route.methods:\n            source_info, name = _get_source_info_and_name(route.resource)\n            info = RouteInfo(method, name, source_info, route.uri_template)\n            routes.append(info)\n    return routes", "idx": 31}
{"namespace": "falcon.inspect._is_internal", "completion": "    source_info = _get_source_info(obj)\n    if source_info is None:\n        return False\n    return source_info.startswith('falcon')\n\n", "idx": 32}
{"namespace": "falcon.cmd.inspect_app.load_app", "completion": "    module_name, app_name = args.app_module.split(':')\n    module = importlib.import_module(module_name)\n    app = getattr(module, app_name, None)\n    if not isinstance(app, falcon.App):\n        app = app()\n    if not isinstance(app, falcon.App):\n        parser.error('The app is not a falcon.App instance')\n    return app\n\n", "idx": 33}
{"namespace": "falcon.cmd.inspect_app.make_parser", "completion": "    parser = argparse.ArgumentParser(description='Prints out the routes of an App instance.')\n    parser.add_argument('-r', '--router', action='store_true', help='Prints out the router of the app instance.')\n    parser.add_argument('-v', '--verbose', action='store_true', help='Prints out the routes of the app instance in a verbose format.')\n    parser.add_argument('-i', '--internal', action='store_true', help='Prints out the internal routes of the app instance.')\n    parser.add_argument('app_module', help='The module containing the app instance.')\n    return parser\n\n", "idx": 34}
{"namespace": "falcon.util.uri.unquote_string", "completion": "    if not isinstance(quoted, str):\n        raise TypeError('unquote_string() expects a string')\n\n    if quoted[0] != '\"' or quoted[-1] != '\"':\n        raise ValueError('unquote_string() expects a quoted string')\n\n    unquoted = quoted[1:-1]\n\n    if _cy_decode:\n        return _cy_decode(unquoted)\n\n    # NOTE(kgriffs): This is a faster implementation of the\n    # Python 3.5+ urllib.parse.unquote() function.\n    #\n    # PERF(kgriffs): This was found to be faster than using\n    # a regex sub call or list comprehension with a join.\n    tokens = unquoted.split('%')\n    # PERF(vytas): Just use in-place add for a low number of items:\n    if len(tokens) < 8:\n        unquoted = tokens[0]\n        for token in tokens[1:]:\n            token_partial = token[:2]\n            try:\n                unquoted += _HEX_TO_BYTE[token_partial] + token[2:]\n            except KeyError:\n                # malformed percentage like \"x=%\" or \"y=%+\"\n                unquoted += '%' + token\n\n        # Convert back to str\n        return unquoted\n\n    # NOTE(vytas): Decode percent-encoded bytestring fragments and join them\n    # back to a string using the platform-dependent method.\n    return _join_tokens(tokens)\n\n", "idx": 35}
{"namespace": "falcon.util.misc.get_argnames", "completion": "    argspec = inspect.getargspec(func)\n    argnames = argspec.args\n    if argspec.varargs:\n        argnames.remove(argspec.varargs)\n    if argspec.keywords:\n        argnames.remove(argspec.keywords)\n    return argnames", "idx": 36}
{"namespace": "falcon.testing.client._is_asgi_app", "completion": "    return len(inspect.signature(app).parameters) == 3", "idx": 37}
{"namespace": "falcon.routing.converters.UUIDConverter.convert", "completion": "        try:\n            return uuid.UUID(value)\n        except ValueError:\n            return None", "idx": 38}
{"namespace": "rest_framework_simplejwt.utils.make_utc", "completion": "    if settings.USE_TZ and is_naive(dt):\n        return make_aware(dt, timezone.utc)\n    return dt", "idx": 39}
{"namespace": "boto.sdb.db.sequence.fib", "completion": "    if cv is None:\n        return 0\n    return cv + lv", "idx": 40}
{"namespace": "boto.s3.website.RoutingRules.add_rule", "completion": "        self.append(rule)\n        return self\n", "idx": 41}
{"namespace": "boto.cloudfront.distribution.Distribution._canned_policy", "completion": "        policy = '{\"Statement\":[{\"Resource\":\"%s\",\"Condition\":{\"DateLessThan\":{\"AWS:EpochTime\":%s}}}]}' % (resource, expires)\n        return policy\n", "idx": 42}
{"namespace": "boto.cloudfront.invalidation.InvalidationBatch.escape", "completion": "        if p[0] != '/':\n            p = '/' + p\n        p = urllib.quote(p, safe='/')\n        return p", "idx": 43}
{"namespace": "proxybroker.utils.get_status_code", "completion": "    try:\n        return int(resp[start:stop])\n    except (ValueError, IndexError):\n        return 400\n\n", "idx": 44}
{"namespace": "authlib.oauth2.rfc6749.util.scope_to_list", "completion": "    if isinstance(scope, (set, tuple, list)):\n        return [to_unicode(s) for s in scope]\n    if scope is None:\n        return scope\n    return [to_unicode(s) for s in scope.split(\" \")]", "idx": 45}
{"namespace": "authlib.common.encoding.to_unicode", "completion": "    if x is None:\n        return None\n    if isinstance(x, str):\n        return x\n    if isinstance(x, bytes):\n        return x.decode(charset, errors)\n    return str(x)\n\n", "idx": 46}
{"namespace": "authlib.common.encoding.to_bytes", "completion": "    if x is None:\n        return None\n    elif isinstance(x, bytes):\n        return x\n    elif isinstance(x, str):\n        return x.encode(charset, errors)\n    elif isinstance(x, int):\n        return x.to_bytes((x.bit_length() + 7) // 8, 'big')\n    elif isinstance(x, float):\n        return struct.pack('>d', x)\n    else:\n        raise TypeError('Unsupported type: {}'.format(type(x)))\n\n", "idx": 47}
{"namespace": "authlib.common.encoding.urlsafe_b64decode", "completion": "    s = to_bytes(s)\n    s += b'=' * (4 - len(s) % 4)\n    return base64.urlsafe_b64decode(s)\n\n", "idx": 48}
{"namespace": "csvs_to_sqlite.utils.table_exists", "completion": "    cursor = conn.cursor()\n    cursor.execute(\n        \"\"\"\n        SELECT count(*)\n        FROM sqlite_master\n        WHERE type='table'\n        AND name=?\n    \"\"\",\n        (table,),\n    )\n    return cursor.fetchone()[0] == 1", "idx": 49}
{"namespace": "sqlitedict.SqliteDict.get_tablenames", "completion": "        if not os.path.exists(filename):\n            raise IOError('file {} does not exist'.format(filename))\n        conn = sqlite3.connect(filename)\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n        tables = cursor.fetchall()\n        cursor.close()\n        conn.close()\n        return [table[0] for table in tables]\n", "idx": 50}
{"namespace": "litecli.packages.parseutils.query_starts_with", "completion": "    prefixes = [prefix.lower() for prefix in prefixes]\n    query = query.lower()\n    query = sqlparse.format(query, strip_comments=True)\n    if not query.strip():\n        return False\n    return last_word(query) in prefixes", "idx": 51}
{"namespace": "rest_framework.negotiation.DefaultContentNegotiation.filter_renderers", "completion": "        filtered_renderers = [renderer for renderer in renderers if renderer.format == format]\n        if not filtered_renderers:\n            raise Http404", "idx": 52}
{"namespace": "rest_framework.templatetags.rest_framework.as_string", "completion": "    return str(value)", "idx": 53}
{"namespace": "rest_framework.templatetags.rest_framework.add_nested_class", "completion": "    if isinstance(value, dict) or (isinstance(value, list) and isinstance(value[0], dict)):\n        return 'class=nested'\n    return ''", "idx": 54}
{"namespace": "pyramid.session.PickleSerializer.loads", "completion": "        return pickle.loads(bstruct)\n", "idx": 55}
{"namespace": "pyramid.testing.DummySession.flash", "completion": "        if allow_duplicate:\n            self.setdefault(queue, []).append(msg)\n        else:\n            if msg not in self.setdefault(queue, []):\n                self.setdefault(queue, []).append(msg)\n\n", "idx": 56}
{"namespace": "pyramid.testing.DummySession.pop_flash", "completion": "        storage = self.setdefault('_f_' + queue, [])\n        return storage", "idx": 57}
{"namespace": "pyramid.testing.DummySession.peek_flash", "completion": "        storage = self.get('_f_' + queue, [])\n        return storage", "idx": 58}
{"namespace": "pyramid.testing.DummySession.new_csrf_token", "completion": "        self.csrf_token = '0123456789012345678901234567890123456789'\n        return self.csrf_token\n\n", "idx": 59}
{"namespace": "pyramid.view.view_defaults", "completion": "    def decorator(wrapped):\n        if not hasattr(wrapped, '__view_defaults__'):\n            wrapped.__view_defaults__ = {}\n        wrapped.__view_defaults__.update(settings)\n        return wrapped\n\n    return decorator", "idx": 60}
{"namespace": "pyramid.util.bytes_", "completion": "    if isinstance(s, str):\n        return s.encode(encoding, errors)\n    return s", "idx": 61}
{"namespace": "pyramid.scripts.common.parse_vars", "completion": "    vars = {}\n    for arg in args:\n        if '=' in arg:\n            key, value = arg.split('=')\n            vars[key] = value\n    return vars\n\n", "idx": 62}
{"namespace": "pyramid.scripts.pviews.PViewsCommand._find_multi_routes", "completion": "        infos = []\n        for route in mapper.routes:\n            match = route.match(request.path_info)\n            if match:\n                infos.append({'match': match, 'route': route})\n        return infos", "idx": 63}
{"namespace": "pyramid.scripts.pserve.PServeCommand.guess_server_url", "completion": "        if not server_name:\n            server_name = 'main'\n        settings = loader.get_settings('server:' + server_name, global_conf)\n        port = settings.get('port')\n        if port:\n            return 'http://127.0.0.1:%s' % port\n", "idx": 64}
{"namespace": "aiohappybase._util.pep8_to_camel_case", "completion": "    if initial:\n        return ''.join(word.capitalize() for word in name.split('_'))\n    else:\n        return name[0].lower() + ''.join(word.capitalize() for word in name.split('_')[1:])", "idx": 65}
{"namespace": "aiohappybase._util.bytes_increment", "completion": "    if b == b'\\xff' * len(b):\n        return None\n    i = len(b) - 1\n    while i >= 0 and b[i] == 0xff:\n        i -= 1\n    if i < 0:\n        return b'\\x00' * (len(b) + 1)\n    return b[:i] + bytes([b[i] + 1]) + b[i + 1:]", "idx": 66}
{"namespace": "mssqlcli.config.ensure_dir_exists", "completion": "    if not exists(dirname(path)):\n        os.makedirs(dirname(path))", "idx": 67}
{"namespace": "mssqlcli.telemetry._user_id_file_is_old", "completion": "    if os.path.exists(id_file_path):\n        file_modified_time = datetime.fromtimestamp(os.path.getmtime(id_file_path))\n        current_time = datetime.now()\n        time_difference = current_time - file_modified_time\n        if time_difference > timedelta(hours=24):\n            return True\n        else:\n            return False", "idx": 68}
{"namespace": "mssqlcli.util.is_command_valid", "completion": "    try:\n        subprocess.call(encode(command), stdout=open(devnull, 'wb'), stderr=open(devnull, 'wb'))\n    except OSError:\n        return False\n    return True", "idx": 69}
{"namespace": "mssqlcli.packages.parseutils.utils.find_prev_keyword", "completion": "    # Parse the SQL statement\n    parsed = sqlparse.parse(sql)[0]\n\n    # Find the last keyword\n    last_keyword = None\n    for token in parsed.tokens:\n        if isinstance(token, Identifier) and token.get_type() == 'keyword':\n            last_keyword = token.value\n\n    # If no keyword found, return None\n    if last_keyword is None:\n        return None, sql\n\n    # Find the index of the last keyword\n    last_keyword_index = parsed.value.rfind(last_keyword)\n\n    # If the last keyword is not found, return None\n    if last_keyword_index == -1:\n        return None, sql\n\n    # Strip everything after the last keyword\n    sql = sql[:last_keyword_index]\n\n    # Return the last keyword and the stripped SQL statement\n    return last_keyword, sql", "idx": 70}
{"namespace": "trafilatura.settings.use_config", "completion": "    if config:\n        return config\n\n    if not filename:\n        filename = Path(__file__).parent / \"settings.cfg\"\n\n    config = ConfigParser()\n    config.read(filename)\n\n    return config", "idx": 71}
{"namespace": "pyramid.util.text_", "completion": "    if isinstance(s, bytes):\n        return s.decode(encoding, errors)\n    return s\n\n", "idx": 72}
{"namespace": "trafilatura.downloads._parse_config", "completion": "    user_agents = config.get('http', 'user_agents').split('\\n')\n    cookies = config.get('http', 'cookies')\n    return user_agents, cookies", "idx": 73}
{"namespace": "trafilatura.downloads.load_download_buffer", "completion": "    # Determine the threading strategy\n    if url_store.compressed:\n        # If the url_store is compressed, use a single thread\n        bufferlist = url_store.get_urls()\n        url_store.clear()\n    else:\n        # If the url_store is not compressed, use multiple threads\n        bufferlist = url_store.get_urls(100)\n        url_store.remove_urls(bufferlist)\n\n    # If the bufferlist is empty, sleep for the specified time\n    if not bufferlist:\n        time.sleep(sleep_time)\n        bufferlist = url_store.get_urls(100)\n        url_store.remove_urls(bufferlist)\n\n    # Return the bufferlist and the url_store object\n    return bufferlist, url_store", "idx": 74}
{"namespace": "trafilatura.metadata.check_authors", "completion": "    new_authors = []\n    for author in authors.lower().split(';'):\n        if author not in author_blacklist:\n            new_authors.append(author)\n    if new_authors:\n        return '; '.join(new_authors)\n    return None", "idx": 75}
{"namespace": "datasette.filters.where_filters", "completion": "    where_clauses = []\n    extra_wheres_for_ui = []\n    if \"_where\" in request.args:\n        if not database.permission_allow_sql:\n            raise DatasetteError(\n                \"SQL queries are not allowed for this database\",\n                status=403,\n            )\n        for where in request.args.getlist(\"_where\"):\n            if not where:\n                continue\n            where_clauses.append(where)\n            extra_wheres_for_ui.append(where)\n        return FilterArguments(where_clauses, extra_wheres_for_ui)\n\n", "idx": 76}
{"namespace": "datasette.utils.path_with_added_args", "completion": "    if path is None:\n        path = request.path\n    if args:\n        path = append_querystring(path, urllib.parse.urlencode(args))\n    return path\n\n", "idx": 77}
{"namespace": "datasette.utils.path_with_replaced_args", "completion": "    path = path or request.path\n    if isinstance(args, dict):\n        args = args.items()\n    current = []\n    for key, value in urllib.parse.parse_qsl(request.query_string):\n        if key not in args:\n            current.append((key, value))\n    current.extend([(key, value) for key, value in args if value is not None])\n    query_string = urllib.parse.urlencode(current)\n    if query_string:\n        query_string = f\"?{query_string}\"\n    return path + query_string", "idx": 78}
{"namespace": "datasette.utils.format_bytes", "completion": "    if bytes == 0:\n        return \"0B\"\n    size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n    i = int(math.floor(math.log(bytes, 1024)))\n    p = math.pow(1024, i)\n    s = round(bytes / p, 2)\n    return \"%s %s\" % (s, size_name[i])", "idx": 79}
{"namespace": "datasette.utils.actor_matches_allow", "completion": "    if allow is None:\n        return True\n    if isinstance(allow, str):\n        return actor == allow\n    if isinstance(allow, list):\n        return actor in allow\n    if isinstance(allow, dict):\n        return actor in allow.keys()\n    return False", "idx": 80}
{"namespace": "datasette.utils.resolve_env_secrets", "completion": "    if isinstance(config, dict):\n        return {\n            key: resolve_env_secrets(value, environ)\n            for key, value in config.items()\n        }\n    elif isinstance(config, list):\n        return [resolve_env_secrets(item, environ) for item in config]\n    elif isinstance(config, str):\n        if config.startswith(\"$env:\"):\n            return environ.get(config[5:])\n        elif config.startswith(\"$file:\"):\n            with open(config[6:]) as fp:\n                return fp.read()\n        else:\n            return config\n    else:\n        return config", "idx": 81}
{"namespace": "datasette.utils.display_actor", "completion": "    if actor is None:\n        return \"Anonymous\"\n    if \"display_name\" in actor:\n        return actor[\"display_name\"]\n    if \"name\" in actor:\n        return actor[\"name\"]\n    if \"username\" in actor:\n        return actor[\"username\"]\n    if \"login\" in actor:\n        return actor[\"login\"]\n    if \"id\" in actor:\n        return actor[\"id\"]\n    return str(actor)", "idx": 82}
{"namespace": "datasette.utils.initial_path_for_datasette", "completion": "    databases = datasette.databases()\n    if len(databases) == 1:\n        database = list(databases.keys())[0]\n        tables = datasette.table_names(database)\n        if len(tables) == 1:\n            return f\"/{database}/{tables[0]}\"\n        else:\n            return f\"/{database}\"\n    else:\n        return \"/\"", "idx": 83}
{"namespace": "datasette.utils.tilde_decode", "completion": "    s = s.replace(\"%\", \"~p\")\n    s = urllib.parse.unquote(s)\n    s = s.replace(\"~p\", \"%\")\n    return s", "idx": 84}
{"namespace": "datasette.utils.resolve_routes", "completion": "    for regex, view in routes:\n        match = re.match(regex, path)\n        if match:\n            return match, view\n    return None", "idx": 85}
{"namespace": "datasette.utils.truncate_url", "completion": "    if len(url) <= length:\n        return url\n    if url.endswith(\".html\"):\n        url = url[:-5]\n    if url.endswith(\".htm\"):\n        url = url[:-4]\n    if url.endswith(\".xml\"):\n        url = url[:-4]\n    if url.endswith(\".json\"):\n        url = url[:-5]\n    if url.endswith(\".csv\"):\n        url = url[:-4]\n    if url.endswith(\".txt\"):\n        url = url[:-4]\n    if url.endswith(\".pdf\"):\n        url = url[:-4]\n    if url.endswith(\".png\"):\n        url = url[:-4]\n    if url.endswith(\".jpg\"):\n        url = url[:-4]\n    if url.endswith(\".jpeg\"):\n        url = url[:-5]\n    if url.endswith(\".gif\"):\n        url = url[:-4]\n    if url.endswith(\".svg\"):\n        url = url[:-4]\n    if url.endswith(\".ico\"):\n        url = url[:-4]\n    if url.endswith(\".bmp\"):\n        url = url[:-4]\n    if url.endswith(\".tiff\"):\n        url = url[:-5]\n    if url.endswith(\".tif\"):\n        url = url[:-4]\n    if url.endswith(\".webp\"):\n        url = url[:-5]\n    if url.endswith(\".mp4\"):\n        url = url[:-4]\n    if url.endswith(\".mov\"):\n        url = url[:-4]\n    if url.endswith(\".avi\"):\n        url = url[:-4]\n    if url.endswith(\".wmv\"):\n        url = url[:-4]\n    if url.endswith(\".mp3\"):\n        url = url[:-4]\n    if url.endswith(\".wav\"):\n       ", "idx": 86}
{"namespace": "kinto.core.authorization.groupfinder", "completion": "    principals = []\n    permission_backend = request.registry.settings.get(\"permission_backend\")\n    if permission_backend:\n        permission_backend = utils.resolve_permission_backend(permission_backend)\n        try:\n            principals = permission_backend.get_principals(userid)\n        except storage_exceptions.BackendError as e:\n            logger.warning(\"Could not fetch principals from permission backend: %s\", e)\n    return principals", "idx": 87}
{"namespace": "kinto.core.utils.json.dumps", "completion": "        kw.setdefault(\"bytes_mode\", rapidjson.BM_NONE)\n        return rapidjson.dumps(v, **kw)\n", "idx": 88}
{"namespace": "kinto.core.utils.json.loads", "completion": "        kw.setdefault(\"number_mode\", rapidjson.NM_NATIVE)\n        return rapidjson.loads(v, **kw)", "idx": 89}
{"namespace": "kinto.core.utils.hmac_digest", "completion": "    if isinstance(secret, str):\n        secret = secret.encode(encoding)\n    return hmac.new(secret, message.encode(encoding), hashlib.sha256).hexdigest()", "idx": 90}
{"namespace": "kinto.core.utils.current_service", "completion": "    if not request.matched_route:\n        return None\n\n    route_name = request.matched_route.name\n    routes = request.registry.introspector.get_routes()\n    for route in routes:\n        if route.name == route_name:\n            return route.service\n\n    return None", "idx": 91}
{"namespace": "kinto.core.utils.prefixed_principals", "completion": "    principals = request.effective_principals\n    if \"Authenticated\" not in principals:\n        return principals\n    principals = list(principals)\n    principals.remove(request.selected_userid)\n    principals.insert(0, prefixed_userid(request))\n    return principals", "idx": 92}
{"namespace": "kinto.plugins.accounts.views.on_account_created", "completion": "    request = event.request\n    settings = request.registry.settings\n\n    if not settings.get(\"account_validation\", False):\n        return\n\n    for obj in event.impacted_objects:\n        account = obj[\"new\"]\n        activation_key = get_cached_validation_key(account[\"id\"], request.registry)\n        if activation_key is None:\n            continue\n\n        emailer = Emailer(request)\n        emailer.send_activation(account, activation_key)", "idx": 93}
{"namespace": "kinto.plugins.accounts.utils.hash_password", "completion": "    return bcrypt.hashpw(password.encode(\"utf-8\"), bcrypt.gensalt()).decode(\"utf-8\")\n\n", "idx": 94}
{"namespace": "kinto.views.admin.get_parent_uri", "completion": "    path = object_uri.split(\"/\")\n    if len(path) < 3:\n        return \"\"\n    return path[0] + \"/\" + path[1]\n\n", "idx": 95}
{"namespace": "alembic.script.write_hooks.register", "completion": "    def decorator(func: Callable) -> Callable:\n        _registry[name] = func\n        return func\n\n    return decorator\n\n", "idx": 96}
{"namespace": "mongo_connector.namespace_config.match_replace_regex", "completion": "    match = regex.match(src_namespace)\n    if match:\n        return dest_namespace.format(**match.groupdict())\n    return None", "idx": 97}
{"namespace": "mongo_connector.namespace_config.namespace_to_regex", "completion": "    database_name, collection_name = namespace.split(\".\", 1)\n    database_regex = re.compile(database_name.replace(\"*\", \".*\"))\n    collection_regex = re.compile(collection_name.replace(\"*\", \".*\"))\n    return re.compile(\n        \"^\" + database_regex.pattern + \"\\\\.\" + collection_regex.pattern + \"$\"\n    )", "idx": 98}
{"namespace": "mongo_connector.util.long_to_bson_ts", "completion": "    return Timestamp(val >> 32, val & 0xffffffff)", "idx": 99}
{"namespace": "mongo_connector.doc_managers.formatters.DocumentFlattener.format_document", "completion": "        # Initialize an empty dictionary to store the flattened keys and values\n        flattened_document = {}\n\n        # Recursively flatten the document\n        def flatten(document, prefix=\"\"):\n            for key, value in document.items():\n                if isinstance(value, dict):\n                    flatten(value, prefix + key + \".\")\n                elif isinstance(value, list):\n                    for i, item in enumerate(value):\n                        flatten({str(i): item}, prefix + key + \".\")\n                else:\n                    flattened_document[prefix + key] = value\n\n        # Call the flatten function with the input document\n        flatten(document)\n\n        # Return the flattened document\n        return flattened_document", "idx": 100}
{"namespace": "bplustree.memory.open_file_in_dir", "completion": "    if platform.system() == \"Windows\":\n        fd = io.FileIO(path, \"rb+\")\n        return fd, None\n\n    dir_path = os.path.dirname(path)\n    if not os.path.exists(dir_path):\n        os.makedirs(dir_path)\n\n    fd = io.FileIO(path, \"rb+\")\n    dir_fd = os.open(dir_path, os.O_RDONLY)\n    return fd, dir_fd\n\n", "idx": 101}
{"namespace": "bplustree.memory.FileMemory.read_transaction", "completion": "        return ReadTransaction(self)\n", "idx": 102}
{"namespace": "bplustree.utils.pairwise", "completion": "    a, b = itertools.tee(iterable)\n    next(b, None)\n    return zip(a, b)", "idx": 103}
{"namespace": "bplustree.utils.iter_slice", "completion": "    it = iter(iterable)\n    while True:\n        chunk = tuple(itertools.islice(it, n))\n        if not chunk:\n            return\n        yield chunk, not bool(chunk)", "idx": 104}
{"namespace": "bplustree.serializer.StrSerializer.serialize", "completion": "        data = obj.encode('utf-8')\n        assert len(data) <= key_size, 'Key size is too small'\n        return data\n", "idx": 105}
{"namespace": "psd_tools.utils.pack", "completion": "    return struct.pack(\">\" + fmt, *args)\n\n", "idx": 106}
{"namespace": "psd_tools.utils.unpack", "completion": "    fmt = str(\">\" + fmt)\n    return struct.unpack(fmt, data)\n\n", "idx": 107}
{"namespace": "psd_tools.api.numpy_io.get_pattern", "completion": "    height = pattern.data[3]\n    width = pattern.data[4]\n    pattern_array = np.zeros((height, width, 4), dtype=np.float32)\n    for channel in pattern.channels:\n        if channel.id == 0:\n            pattern_array[:, :, 0] = _parse_array(channel.data, channel.depth)\n        elif channel.id == 1:\n            pattern_array[:, :, 1] = _parse_array(channel.data, channel.depth)\n        elif channel.id == 2:\n            pattern_array[:, :, 2] = _parse_array(channel.data, channel.depth)\n        elif channel.id == 3:\n            pattern_array[:, :, 3] = _parse_array(channel.data, channel.depth)\n    return pattern_array", "idx": 108}
{"namespace": "sqlite_utils.utils.maximize_csv_field_size_limit", "completion": "    csv.field_size_limit(sys.maxsize)\n    while True:\n        try:\n            csv.field_size_limit(sys.maxsize)\n            break\n        except OverflowError:\n            csv.field_size_limit(csv.field_size_limit() // 2)", "idx": 109}
{"namespace": "sqlite_utils.utils.column_affinity", "completion": "    if column_type in (int, float, bool):\n        return \"NUM\"\n    elif column_type == str:\n        return \"TEXT\"\n    elif column_type == bytes:\n        return \"BLOB\"\n    elif column_type == list:\n        return \"TEXT\"\n    elif column_type == dict:\n        return \"TEXT\"\n    elif column_type == tuple:\n        return \"TEXT\"\n    else:\n        return \"NOCAST\"", "idx": 110}
{"namespace": "sqlite_utils.utils.decode_base64_values", "completion": "    if isinstance(doc, dict):\n        for key, value in doc.items():\n            if isinstance(value, dict) and value.get(\"$base64\", False):\n                doc[key] = base64.b64decode(value[\"encoded\"])\n            elif isinstance(value, list):\n                for i, item in enumerate(value):\n                    if isinstance(item, dict) and item.get(\"$base64\", False):\n                        value[i] = base64.b64decode(item[\"encoded\"])\n    elif isinstance(doc, list):\n        for i, item in enumerate(doc):\n            if isinstance(item, dict) and item.get(\"$base64\", False):\n                doc[i] = base64.b64decode(item[\"encoded\"])\n    return doc", "idx": 111}
{"namespace": "sqlite_utils.utils.chunks", "completion": "    for i in range(0, len(sequence), size):\n        yield sequence[i : i + size]", "idx": 112}
{"namespace": "sqlite_utils.utils.hash_record", "completion": "    if keys is None:\n        keys = record.keys()\n    return hashlib.sha1(\n        json.dumps(\n            {key: record[key] for key in keys},\n            sort_keys=True,\n        ).encode(\"utf-8\")\n    ).hexdigest()", "idx": 113}
{"namespace": "arctic.decorators._get_host", "completion": "    if not store:\n        raise ValueError(\"Store is empty\")\n\n    if isinstance(store, (list, tuple)):\n        store = store[0]\n\n    library_name = store.library\n    nodes = store.nodes\n    host = store.host\n\n    return {\"library_name\": library_name, \"nodes\": nodes, \"host\": host}\n\n", "idx": 114}
{"namespace": "arctic.decorators.mongo_retry", "completion": "    @wraps(f)\n    def wrapper(*args, **kwargs):\n        global _retry_count, _in_retry\n        _retry_count = 0\n        _in_retry = False\n        while True:\n            try:\n                return f(*args, **kwargs)\n            except (AutoReconnect, OperationFailure, DuplicateKeyError, ServerSelectionTimeoutError, BulkWriteError) as e:\n                _retry_count += 1\n                _in_retry = True\n                logger.warning(\"MongoDB error: %s\", e)\n                if _retry_count > _MAX_RETRIES:\n                    _log_exception(e, logger)\n                    raise\n                sleep(0.1)\n            except Exception as e:\n                _log_exception(e, logger)\n                raise\n\n    return wrapper", "idx": 115}
{"namespace": "arctic._util.are_equals", "completion": "    if isinstance(o1, DataFrame) and isinstance(o2, DataFrame):\n        try:\n            assert_frame_equal(o1, o2, **kwargs)\n            return True\n        except AssertionError:\n            return False\n    else:\n        return o1 == o2\n\n", "idx": 116}
{"namespace": "arctic.hooks.register_resolve_mongodb_hook", "completion": "    global _resolve_mongodb_hook\n    _resolve_mongodb_hook = hook", "idx": 117}
{"namespace": "arctic.hooks.register_log_exception_hook", "completion": "    global _log_exception_hook\n    _log_exception_hook = hook", "idx": 118}
{"namespace": "arctic.hooks.register_get_auth_hook", "completion": "    global _get_auth_hook\n    _get_auth_hook = hook", "idx": 119}
{"namespace": "arctic.store._version_store_utils._split_arrs", "completion": "    slices = np.array(slices)\n    slices = np.insert(slices, 0, 0)\n    slices = np.append(slices, len(array_2d))\n    return np.array([array_2d[slices[i]:slices[i + 1]] for i in range(len(slices) - 1)])\n\n", "idx": 120}
{"namespace": "arctic.store._version_store_utils.checksum", "completion": "    pickle_compat.Pickler(None, None)\n    pickle_compat.Pickler.dump(doc)\n    pickle_compat.Pickler.dumps()\n    pickle_compat.Pickler.dumps()\n    pickle_compat.Pickler.dumps()\n    pickle_compat.Pickler.dumps()\n    pickle_compat.Pickler.dumps()\n    pickle_compat.Pickler.dumps()\n    pickle_compat.Pickler.dumps()\n    pickle_compat.Pickler.dumps()\n    pickle_compat.Pickler.dumps()\n    pickle_compat.Pickler.dumps()\n    pickle_compat.Pickler.dumps()\n    pickle_compat.Pickler.dumps()\n    pickle_compat.Pickler.dumps()\n    pickle_compat.Pickler.dumps()\n    pickle_compat.Pickler.dumps()\n    pickle_compat.Pickler.dumps()\n    pickle_compat.Pickler.dumps()\n    pickle_compat.Pickler.dumps()\n    pickle_compat.Pickler.dumps()\n    pickle_compat.Pickler.dumps()\n    pickle_compat.Pickler.dumps()\n    pickle_compat.Pickler.dumps()\n    pickle_compat.Pickler.dumps()\n    pickle_compat.Pickler.dumps()\n    pickle_compat.Pickler.dumps()\n    pickle_compat.Pickler.dumps()\n    pickle_compat.Pickler.dumps()\n    pickle_compat.Pickler.dumps()\n    pickle_compat.Pickler.dumps()\n    pickle_compat.Pickler.dumps()\n    pickle_compat.Pickler.dumps()\n    pickle_compat.Pickler.dumps()\n    pickle_compat.Pickler.dumps()\n    pickle_compat.P", "idx": 121}
{"namespace": "arctic.store.versioned_item.VersionedItem.__str__", "completion": "        return \"VersionedItem(symbol={symbol},library={library},data={data},version={version},metadata={metadata},host={host})\".format(\n            symbol=self.symbol,\n            library=self.library,\n            data=self.data,\n            version=self.version,\n            metadata=self.metadata,\n            host=self.host\n        )", "idx": 122}
{"namespace": "arctic.store._ndarray_store.NdarrayStore._dtype", "completion": "        if metadata is None:\n            metadata = {}\n        if string.startswith(\"[\"):\n            return np.dtype(string)\n        elif string.startswith(\"(\"):\n            return np.dtype(string)\n        elif string.startswith(\"{\"):\n            return np.dtype(string)\n        elif string.startswith(\"<\"):\n            return np.dtype(string)\n        elif string.startswith(\"|\"):\n            return np.dtype(string)\n        elif string.startswith(\"U\"):\n            return np.dtype(string)\n        elif string.startswith(\"S\"):\n            return np.dtype(string)\n        elif string.startswith(\"V\"):\n            return np.dtype(string)\n        elif string.startswith(\"O\"):\n            return np.dtype(string)\n        elif string.startswith(\"?\"):\n            return np.dtype(string)\n        elif string.startswith(\"b\"):\n            return np.dtype(string)\n        elif string.startswith(\"B\"):\n            return np.dtype(string)\n        elif string.startswith(\"i\"):\n            return np.dtype(string)\n        elif string.startswith(\"u\"):\n            return np.dtype(string)\n        elif string.startswith(\"f\"):\n            return np.dtype(string)\n        elif string.startswith(\"c\"):\n            return np.dtype(string)\n        elif string.startswith(\"m\"):\n            return np.dtype(string)\n        elif string.startswith(\"M\"):\n            return np.dtype(string)\n        elif string.startswith(\"?\"):\n            return np.dtype(string)\n        elif string.startswith(\"S\"):\n            return np.dtype(string)\n        elif string.startswith(\"U\"):\n            return np.dtype(string)\n        elif string.startswith(\"V\"):\n            return np.dtype(string)\n        elif string.startswith(\"", "idx": 123}
{"namespace": "arctic.store._ndarray_store._promote_struct_dtypes", "completion": "    if not np.issubdtype(dtype1, np.dtype(dtype2)):\n        raise ValueError(f\"dtype1 {dtype1} is not a superset of dtype2 {dtype2}\")\n\n    dtype1_fields = dtype1.fields\n    dtype2_fields = dtype2.fields\n\n    promoted_fields = {}\n    for field_name, field_dtype in dtype2_fields.items():\n        if field_name in dtype1_fields:\n            promoted_fields[field_name] = (dtype1_fields[field_name][0], field_dtype)\n        else:\n            promoted_fields[field_name] = (dtype1_fields[field_name][0], field_dtype)\n\n    return np.dtype(promoted_fields)", "idx": 124}
{"namespace": "arctic.chunkstore.passthrough_chunker.PassthroughChunker.exclude", "completion": "        return data", "idx": 125}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.to_chunks", "completion": "        if func is None:\n            raise ValueError('func cannot be None')\n\n        if not isinstance(df, pd.DataFrame):\n            df = pd.DataFrame(df)\n\n        if df.empty:\n            return\n\n        if df.index.name != 'date':\n            raise ValueError('df must have a date index')\n\n        if df.index.tz is not None:\n            raise ValueError('df must have a timezone-naive index')\n\n        if df.index.dtype != 'datetime64[ns]':\n            raise ValueError('df must have a datetime64[ns] index')\n\n        if df.index.is_monotonic_increasing:\n            df = df.sort_index()\n\n        if df.index.is_unique:\n            df = df.reset_index()\n\n        if df.index.name != 'date':\n            raise ValueError('df must have a date index')\n\n        if df.index.tz is not None:\n            raise ValueError('df must have a timezone-naive index')\n\n        if df.index.dtype != 'datetime64[ns]':\n            raise ValueError('df must have a datetime64[ns] index')\n\n        if df.index.is_monotonic_increasing:\n            df = df.sort_index()\n\n        if df.index.is_unique:\n            df = df.reset_index()\n\n        if df.index.name != 'date':\n            raise ValueError('df must have a date index')\n\n        if df.index.tz is not None:\n            raise ValueError('df must have a timezone-naive index')\n\n        if df.index.dtype != 'datetime64[ns]':\n            raise ValueError('df must have a datetime64[ns] index')\n\n        if df.index.is_monotonic_increasing:\n            df = df.sort_index()\n\n        if df.index.is_unique:\n            df = df.reset_index()\n\n        if df.index.name !", "idx": 126}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.exclude", "completion": "        if isinstance(range_obj, (pd.DatetimeIndex, tuple)):\n            range_obj = DateRange(range_obj[0], range_obj[-1])\n\n        range_obj = to_pandas_closed_closed(range_obj, add_tz=False)\n        start = range_obj.start\n        end = range_obj.end\n\n        if 'date' in data.index.names:\n            return data[~((data.index.get_level_values('date') >= start) & (data.index.get_level_values('date') <= end))]\n        elif 'date' in data.columns:\n            return data[~((data.date >= start) & (data.date <= end))]\n        else:\n            return data", "idx": 127}
{"namespace": "mopidy.httpclient.format_proxy", "completion": "    if proxy_config is None:\n        return None\n\n    scheme = proxy_config.get(\"scheme\", \"http\")\n    host = proxy_config.get(\"hostname\", \"\")\n    port = proxy_config.get(\"port\", \"\")\n    username = proxy_config.get(\"username\", \"\")\n    password = proxy_config.get(\"password\", \"\")\n\n    if not auth:\n        return f\"{scheme}://{host}:{port}\"\n\n    return f\"{scheme}://{username}:{password}@{host}:{port}\"", "idx": 128}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.filter", "completion": "        if isinstance(range_obj, (pd.DatetimeIndex, tuple)):\n            range_obj = DateRange(range_obj[0], range_obj[-1])\n        if range_obj.start and range_obj.end:\n            return data[to_pandas_closed_closed(range_obj)]\n        elif range_obj.start:\n            return data[data.index >= range_obj.start]\n        elif range_obj.end:\n            return data[data.index <= range_obj.end]\n        else:\n            return data", "idx": 129}
{"namespace": "mopidy.config.validators.validate_required", "completion": "    if required and not value:\n        raise ValueError('Required value is missing')\n\n", "idx": 130}
{"namespace": "mopidy.config.validators.validate_choice", "completion": "    if value not in choices:\n        raise ValueError(f\"must be one of {choices}, not {value}.\")", "idx": 131}
{"namespace": "mopidy.config.validators.validate_minimum", "completion": "    if value < minimum:\n        raise ValueError(f\"{value} must be larger than {minimum}.\")", "idx": 132}
{"namespace": "mopidy.config.validators.validate_maximum", "completion": "    if maximum is not None and value > maximum:\n        raise ValueError(f\"{value} must be smaller than {maximum}.\")", "idx": 133}
{"namespace": "mopidy.config.schemas._did_you_mean", "completion": "    if not choices:\n        return None\n\n    distances = collections.defaultdict(list)\n    for choice in choices:\n        distance = levenshtein_distance(name, choice)\n        distances[distance].append(choice)\n\n    min_distance = min(distances.keys())\n    if min_distance <= 3:\n        return distances[min_distance][0]\n    else:\n        return None\n\n\n", "idx": 134}
{"namespace": "mopidy.config.types.encode", "completion": "    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(char, \"\\\\\" + char)\n\n    return value", "idx": 135}
{"namespace": "mopidy.config.types.decode", "completion": "    if isinstance(value, bytes):\n        value = value.decode(\"surrogateescape\")\n    value = re.sub(r\"\\\\\\\\\", \"\\\\\", value)\n    value = re.sub(r\"\\\\n\", \"\\n\", value)\n    value = re.sub(r\"\\\\t\", \"\\t\", value)\n    return value\n\n", "idx": 136}
{"namespace": "mopidy.config.types.ConfigValue.serialize", "completion": "        if value is None:\n            return \"\"\n        return encode(value)\n", "idx": 137}
{"namespace": "mopidy.config.types.Boolean.serialize", "completion": "        if value is None:\n            return \"false\"\n        if value is True:\n            return \"true\"\n        if value is False:\n            return \"false\"\n        raise ValueError(f\"{value} is not a boolean\")", "idx": 138}
{"namespace": "hypertools.tools.df2mat.df2mat", "completion": "    # Get the column names\n    cols = data.columns\n\n    # Get the column names that are of type object\n    obj_cols = data.select_dtypes(include=['object']).columns\n\n    # Get the column names that are of type int\n    int_cols = data.select_dtypes(include=['int']).columns\n\n    # Get the column names that are of type float\n    float_cols = data.select_dtypes(include=['float']).columns\n\n    # Get the column names that are of type bool\n    bool_cols = data.select_dtypes(include=['bool']).columns\n\n    # Get the column names that are of type datetime\n    datetime_cols = data.select_dtypes(include=['datetime']).columns\n\n    # Get the column names that are of type category\n    category_cols = data.select_dtypes(include=['category']).columns\n\n    # Get the column names that are of type string\n    string_cols = data.select_dtypes(include=['string']).columns\n\n    # Get the column names that are of type timedelta\n    timedelta_cols = data.select_dtypes(include=['timedelta']).columns\n\n    # Get the column names that are of type complex\n    complex_cols = data.select_dtypes(include=['complex']).columns\n\n    # Get the column names that are of type sparse\n    sparse_cols = data.select_dtypes(include=['sparse']).columns\n\n    # Get the column names that are of type SparseArray\n    sparse_array_cols = data.select_dtypes(include=['SparseArray']).columns\n\n    # Get the column names that are of type SparseDtype\n    sparse_dtype_cols = data.select_dtypes(include=['SparseDtype']).columns\n\n    # Get the column names that are of type SparseFrame\n    sparse_frame_cols = data.select_dtypes(include=['SparseFrame']).columns\n\n    # Get the column names that are of type Sparse", "idx": 139}
{"namespace": "hypertools._shared.helpers.center", "completion": "    assert type(x) == list, \"Input must be a list\"\n    x_mean = np.mean(x)\n    x_centered = [i - x_mean for i in x]\n    return x_centered\n\n", "idx": 140}
{"namespace": "hypertools._shared.helpers.group_by_category", "completion": "    if any(isinstance(i, list) for i in vals):\n        vals = list(itertools.chain.from_iterable(vals))\n    vals = sorted(set(vals))\n    return [vals.index(i) for i in vals]", "idx": 141}
{"namespace": "hypertools._shared.helpers.vals2colors", "completion": "    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n    palette = sns.color_palette(cmap, res)\n    return [palette[int(i)] for i in vals]\n\n", "idx": 142}
{"namespace": "hypertools._shared.helpers.vals2bins", "completion": "    # flatten if list of lists\n    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n\n    # get bins\n    bins = np.linspace(np.min(vals), np.max(vals)+1, res+1)\n    return [np.digitize(val, bins) for val in vals]", "idx": 143}
{"namespace": "hypertools._shared.helpers.interp_array", "completion": "    ##PACKAGES##\n    import numpy as np\n    from scipy.interpolate import PchipInterpolator as pchip\n\n    ##VARIABLES##\n    x = np.arange(0, len(arr))\n    y = arr\n    x_new = np.linspace(0, len(arr), interp_val)\n    f = pchip(x, y)\n    return f(x_new)", "idx": 144}
{"namespace": "hypertools._shared.helpers.parse_args", "completion": "    if len(args) != len(x):\n        print(\"Error: args must be same length as x\")\n        sys.exit()\n    if any(isinstance(el, list) for el in args):\n        if any(len(el) != len(x) for el in args):\n            print(\"Error: args must be same length as x\")\n            sys.exit()\n    return [tuple(i) for i in zip(x, *args)]", "idx": 145}
{"namespace": "hypertools._shared.helpers.parse_kwargs", "completion": "    kwargs_list = []\n    for i, item in enumerate(x):\n        tmp = {}\n        for key, value in kwargs.items():\n            if isinstance(value, (tuple, list)):\n                if len(value) == len(x):\n                    tmp[key] = value[i]\n                else:\n                    print('Error: arguments must be a list of the same length as x')\n                    sys.exit(1)\n            else:\n                tmp[key] = value\n        kwargs_list.append(tmp)\n    return kwargs_list", "idx": 146}
{"namespace": "gif_for_cli.utils._get_default_display_mode", "completion": "    term = environ.get(\"TERM\", \"\")\n    colorterm = environ.get(\"COLORTERM\", \"\")\n\n    if \"truecolor\" in term or \"truecolor\" in colorterm:\n        return \"truecolor\"\n    elif \"256\" in term or \"256\" in colorterm:\n        return \"256fgbg\"\n    else:\n        return \"nocolor\"\n\n", "idx": 147}
{"namespace": "gif_for_cli.utils._pool_type", "completion": "    ival = int(val)\n    if ival <= 0:\n        raise argparse.ArgumentTypeError(\n            'Pool size must be greater than 0, got {}'.format(ival))\n    return ival", "idx": 148}
{"namespace": "gif_for_cli.generate.utils.get_avg_for_em", "completion": "    r_sum = 0\n    g_sum = 0\n    b_sum = 0\n    for i in range(cell_height):\n        for j in range(cell_width):\n            r_sum += px[x + j][y + i][0]\n            g_sum += px[x + j][y + i][1]\n            b_sum += px[x + j][y + i][2]\n    return [r_sum / (cell_height * cell_width), g_sum / (cell_height * cell_width),\n            b_sum / (cell_height * cell_width)]", "idx": 149}
{"namespace": "gif_for_cli.generate.utils.process_input_source", "completion": "    if input_source.startswith('https://tenor.com/view/'):\n        gif_id = input_source.split('/')[-1]\n        gif_url = f'https://tenor.com/gifapi/gifs?ids={gif_id}'\n    else:\n        if input_source.startswith('http'):\n            gif_url = input_source\n        else:\n            gif_url = f'https://tenor.googleapis.com/v2/search?q={input_source}&key={api_key}&client_key=gif.js&limit=1'\n\n    return gif_url\n\n", "idx": 150}
{"namespace": "hypertools._shared.helpers.reshape_data", "completion": "    # Flatten the input data and the labels if they are lists\n    if any(isinstance(el, list) for el in x):\n        x = list(itertools.chain(*x))\n    if labels is not None:\n        if any(isinstance(el, list) for el in labels):\n            labels = list(itertools.chain(*labels))\n\n    # Get the unique categories in the hue\n    hue_categories = list(sorted(set(hue), key=list(hue).index))\n\n    # Reshape the input data based on the categories in the hue\n    x_reshaped = [[] for _ in range(len(hue_categories))]\n    for i, category in enumerate(hue_categories):\n        x_reshaped[i] = [x[j] for j, hue_val in enumerate(hue) if hue_val == category]\n\n    # Reshape the labels based on the categories in the hue\n    if labels is not None:\n        labels_reshaped = [[] for _ in range(len(hue_categories))]\n        for i, category in enumerate(hue_categories):\n            labels_reshaped[i] = [labels[j] for j, hue_val in enumerate(hue) if hue_val == category]\n    else:\n        labels_reshaped = None\n\n    return x_reshaped, labels_reshaped", "idx": 151}
{"namespace": "mingus.extra.lilypond.from_Note", "completion": "    if process_octaves:\n        if note.octave == 0:\n            octave = \"\"\n        elif note.octave > 0:\n            octave = \"'\" * note.octave\n        else:\n            octave = \",\" * abs(note.octave)\n    else:\n        octave = \"\"\n\n    if note.is_rest():\n        return \"r\" + octave\n    else:\n        return note.name + octave\n\n", "idx": 152}
{"namespace": "mingus.extra.tablature._get_qsize", "completion": "    # Size of a quarter note\n    qsize = (width - len(begin_track(tuning)[0])) / 4\n    return qsize", "idx": 153}
{"namespace": "mingus.core.notes.augment", "completion": "    if note[-1] == \"b\":\n        return note[:-1]\n    else:\n        return note + \"#\"\n\n", "idx": 154}
{"namespace": "mingus.core.meter.valid_beat_duration", "completion": "    return duration & (duration - 1) == 0\n\n", "idx": 155}
{"namespace": "mingus.core.notes.diminish", "completion": "    if note[-1] != \"#\":\n        return note + \"b\"\n    else:\n        return note[:-1]", "idx": 156}
{"namespace": "mingus.core.intervals.invert", "completion": "    return interval[::-1]", "idx": 157}
{"namespace": "mingus.core.progressions.parse_string", "completion": "    # Initialize variables\n    roman_numeral = \"\"\n    accidentals = 0\n    suffix = \"\"\n\n    # Parse the input string\n    for i in range(len(progression)):\n        if progression[i] == \"b\":\n            accidentals -= 1\n        elif progression[i] == \"#\":\n            accidentals += 1\n        elif progression[i] in numerals:\n            roman_numeral = progression[i]\n        elif progression[i] in chords.chord_shorthand:\n            suffix = progression[i]\n\n    # Return the parsed result\n    return roman_numeral, accidentals, suffix", "idx": 158}
{"namespace": "exodus_bundler.bundling.bytes_to_int", "completion": "    return int.from_bytes(bytes, byteorder)\n\n", "idx": 159}
{"namespace": "exodus_bundler.templating.render_template", "completion": "    for key, value in context.items():\n        string = string.replace('{{' + key + '}}', str(value))\n\n    return string", "idx": 160}
{"namespace": "exodus_bundler.input_parsing.strip_pid_prefix", "completion": "    return re.sub(r'^\\[pid \\d+\\] ', '', line)", "idx": 161}
{"namespace": "fs.path.abspath", "completion": "    if path.startswith(\"/\"):\n        return path\n    else:\n        return \"/\" + path", "idx": 162}
{"namespace": "fs.path.combine", "completion": "    if path1.endswith(\"/\"):\n        return path1 + path2\n    else:\n        return path1 + \"/\" + path2", "idx": 163}
{"namespace": "fs.path.split", "completion": "    if path == \"/\":\n        return \"/\", \"\"\n    if path == \"\":\n        return \".\", \"\"\n    if path == \".\":\n        return \".\", \"\"\n    if path == \"..\":\n        return \"..\", \"\"\n\n    if path.startswith(\"/\"):\n        head = \"/\"\n        tail = path[1:]\n    else:\n        head = \".\"\n        tail = path\n\n    if tail.endswith(\"/\"):\n        tail = tail[:-1]\n\n    if tail == \"\":\n        return head, tail\n\n    if tail.startswith(\"/\"):\n        tail = tail[1:]\n\n    if tail.endswith(\"/\"):\n        tail = tail[:-1]\n\n    if tail == \"\":\n        return head, tail\n\n    if tail.startswith(\".\"):\n        tail = tail[1:]\n\n    if tail == \"\":\n        return head, tail\n\n    if tail.startswith(\"/\"):\n        tail = tail[1:]\n\n    if tail == \"\":\n        return head, tail\n\n    if tail.startswith(\".\"):\n        tail = tail[1:]\n\n    if tail == \"\":\n        return head, tail\n\n    if tail.startswith(\"/\"):\n        tail = tail[1:]\n\n    if tail == \"\":\n        return head, tail\n\n    if tail.startswith(\".\"):\n        tail = tail[1:]\n\n    if tail == \"\":\n        return head, tail\n\n    if tail.startswith(\"/\"):\n        tail = tail[1:]\n\n    if tail == \"\":\n        return head, tail\n\n    if tail.startswith(\".\"):\n        tail = tail[1:]\n\n    if tail == \"\":\n        return head, tail\n\n    if tail.startswith(\"/\"):\n        tail = tail[1:]\n\n    if tail == \"\":\n        return head, tail\n\n    if tail.startswith(\".\"):\n        tail = tail[1:]\n\n    if tail == \"\":\n        return head, tail\n\n    if tail.startsw", "idx": 164}
{"namespace": "fs.path.isparent", "completion": "    _path1 = forcedir(abspath(path1))\n    _path2 = forcedir(abspath(path2))\n    return _path1 == _path2[: len(_path1)]", "idx": 165}
{"namespace": "fs.path.forcedir", "completion": "    if path.endswith(\"/\"):\n        return path\n    return path + \"/\"", "idx": 166}
{"namespace": "fs.wildcard.match_any", "completion": "    return any(match(pattern, name) for pattern in patterns)", "idx": 167}
{"namespace": "fs.wildcard.imatch_any", "completion": "    if not patterns:\n        return True\n    return any(imatch(pattern, name) for pattern in patterns)", "idx": 168}
{"namespace": "wal_e.cmd.parse_boolean_envvar", "completion": "    if val.lower() in ['false', '0']:\n        return False\n    elif val.lower() in ['true', '1']:\n        return True\n    else:\n        raise UserCritical(\n            'Invalid value for environment variable: ' + val +\n            '; must be one of: false, 0, true, 1')\n\n", "idx": 169}
{"namespace": "wal_e.log_help.get_log_destinations", "completion": "    log_destinations = os.environ.get('WALE_LOG_DESTINATION', 'stderr,syslog')\n    return log_destinations.split(',')\n\n", "idx": 170}
{"namespace": "wal_e.log_help.WalELogger._fmt_structured", "completion": "        # Sort the dictionary lexically, except for the time and pid.\n        sorted_keys = sorted(d.keys())\n        sorted_keys.remove('time')\n        sorted_keys.remove('pid')\n        sorted_keys.insert(0, 'time')\n        sorted_keys.insert(1, 'pid')\n\n        # Format the dictionary into a string.\n        s = ''\n        for k in sorted_keys:\n            s += ' ' + k + '=' + str(d[k])\n\n        # Format the time.\n        s = s.replace('time=', 'time=' + datetime.datetime.utcnow().isoformat() + '-00')\n\n        return s", "idx": 171}
{"namespace": "wal_e.tar_partition._fsync_files", "completion": "    for filename in filenames:\n        with open(filename, 'a') as f:\n            os.fsync(f.fileno())\n\n    os.fsync(os.path.dirname(filenames[0]))\n\n", "idx": 172}
{"namespace": "wal_e.blobstore.file.calling_format.Bucket.list", "completion": "        path = os.path.join(\"/\", prefix.strip(\"/\"))\n        if not os.path.exists(path):\n            return []\n        file_paths = [os.path.join(path, f) for f in os.listdir(path)]\n        return [FileKey(bucket=self, name=f) for f in file_paths]", "idx": 173}
{"namespace": "pyinfra.operations.util.files.unix_path_join", "completion": "    return \"/\".join(\n        [\n            re.sub(r\"/+$\", \"\", part)\n            for part in path_parts\n        ]\n    )", "idx": 174}
{"namespace": "pyinfra.operations.server.shell", "completion": "    if isinstance(commands, str):\n        commands = [commands]\n\n    for command in commands:\n        yield command", "idx": 175}
{"namespace": "pyinfra.api.util.try_int", "completion": "    try:\n        return int(value)\n    except ValueError:\n        return value", "idx": 176}
{"namespace": "mrjob.job.MRJob.mr_job_script", "completion": "        try:\n            return inspect.getsourcefile(cls)\n        except TypeError:\n            return None\n", "idx": 177}
{"namespace": "mrjob.compat.map_version", "completion": "    if isinstance(version_map, list):\n        version_map = dict(version_map)\n    if isinstance(version, string_types):\n        version = LooseVersion(version)\n    for version_key in sorted(version_map.keys()):\n        if version < version_key:\n            return version_map[version_key]\n    return version_map[sorted(version_map.keys())[0]]", "idx": 178}
{"namespace": "mrjob.conf.combine_values", "completion": "    for value in values:\n        if value is not None:\n            return value\n    return None", "idx": 179}
{"namespace": "mrjob.protocol.BytesProtocol.read", "completion": "        if b'\\t' in line:\n            key, value = line.split(b'\\t', 1)\n        else:\n            key = line\n            value = None\n        return (key, value)\n", "idx": 180}
{"namespace": "mrjob.protocol.TextProtocol.write", "completion": "        if key is None:\n            if value is None:\n                return b''\n            else:\n                return value.encode('utf_8')\n        else:\n            if value is None:\n                return key.encode('utf_8')\n            else:\n                return key.encode('utf_8') + b'\\t' + value.encode('utf_8')", "idx": 181}
{"namespace": "mrjob.protocol.TextProtocol.read", "completion": "        try:\n            key_value = line.decode('utf_8').split('\\t', 1)\n        except UnicodeDecodeError:\n            key_value = line.decode('latin_1').split('\\t', 1)\n        if len(key_value) == 1:\n            key_value.append(None)\n\n        return tuple(key_value)\n", "idx": 182}
{"namespace": "mrjob.protocol.TextValueProtocol.read", "completion": "        try:\n            line = line.decode('utf_8')\n        except UnicodeDecodeError:\n            line = line.decode('latin_1')\n\n        return (None, line)\n", "idx": 183}
{"namespace": "mrjob.util.file_ext", "completion": "    filename = filename.lstrip(\".\")\n    index = filename.find(\".\")\n    if index == -1:\n        return \"\"\n    else:\n        return filename[index:]\n\n", "idx": 184}
{"namespace": "mrjob.util.cmd_line", "completion": "    return \" \".join(pipes.quote(str(arg)) for arg in args)\n\n", "idx": 185}
{"namespace": "mrjob.util.save_cwd", "completion": "    original_cwd = os.getcwd()\n\n    try:\n        yield\n\n    finally:\n        os.chdir(original_cwd)\n\n", "idx": 186}
{"namespace": "mrjob.util.save_sys_std", "completion": "    original_stdin = sys.stdin\n    original_stdout = sys.stdout\n    original_stderr = sys.stderr\n\n    try:\n        yield\n\n    finally:\n        sys.stdin = original_stdin\n        sys.stdout = original_stdout\n        sys.stderr = original_stderr\n\n", "idx": 187}
{"namespace": "mrjob.util.unarchive", "completion": "    if is_zipfile(archive_path):\n        with ZipFile(archive_path, 'r') as zip_file:\n            zip_file.extractall(dest)\n    else:\n        with tarfile.open(archive_path, 'r') as tar_file:\n            tar_file.extractall(dest)", "idx": 188}
{"namespace": "mrjob.util.unique", "completion": "    seen = set()\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            yield item", "idx": 189}
{"namespace": "mrjob.parse.urlparse", "completion": "    if allow_fragments:\n        # This is a workaround for a bug in urlparse.urlparse.\n        # See https://bugs.python.org/issue16033\n        # This bug is fixed in Python 3.7.\n        # This workaround is only needed for Python 2.7.\n        # This workaround is not needed for Python 3.\n        # This workaround is not needed for Python 3.7.\n        # This workaround is not needed for Python 3.8.\n        # This workaround is not needed for Python 3.9.\n        # This workaround is not needed for Python 3.10.\n        # This workaround is not needed for Python 3.11.\n        # This workaround is not needed for Python 3.12.\n        # This workaround is not needed for Python 3.13.\n        # This workaround is not needed for Python 3.14.\n        # This workaround is not needed for Python 3.15.\n        # This workaround is not needed for Python 3.16.\n        # This workaround is not needed for Python 3.17.\n        # This workaround is not needed for Python 3.18.\n        # This workaround is not needed for Python 3.19.\n        # This workaround is not needed for Python 3.20.\n        # This workaround is not needed for Python 3.21.\n        # This workaround is not needed for Python 3.22.\n        # This workaround is not needed for Python 3.23.\n        # This workaround is not needed for Python 3.24.\n        # This workaround is not needed for Python 3.25.\n        # This workaround is not needed for Python 3.26.\n        # This workaround is not needed for Python 3.27.\n        # This workaround is not needed for Python 3.28.\n        # This workaround is not needed for Python 3.29.\n        # This workaround is not needed for Python 3.30.\n        # This workaround is not needed for Python 3.31.\n        # This workaround is not needed for Python 3.32.", "idx": 190}
{"namespace": "mrjob.util.which", "completion": "    if path is None:\n        path = os.environ.get('PATH', '')\n\n    if os.path.isabs(cmd):\n        if os.path.exists(cmd):\n            return cmd\n        else:\n            return None\n\n    if os.path.sep in cmd:\n        return None\n\n    for p in path.split(os.pathsep):\n        p = os.path.join(p, cmd)\n        if os.path.exists(p) and os.access(p, os.X_OK):\n            return p\n\n    return None", "idx": 191}
{"namespace": "sshuttle.ssh.parse_hostport", "completion": "    if not rhostport:\n        return None, None, None, None\n\n    # Parse the rhostport variable\n    rhostport = rhostport.strip()\n    if \"@\" in rhostport:\n        username, rhostport = rhostport.split(\"@\", 1)\n    else:\n        username = None\n\n    if \":\" in rhostport:\n        rhost, rport = rhostport.split(\":\", 1)\n    else:\n        rhost = rhostport\n        rport = None\n\n    if rport:\n        try:\n            rport = int(rport)\n        except ValueError:\n            raise Fatal(\"Invalid port number: %s\" % rport)\n\n    return username, None, rport, rhost\n\n", "idx": 192}
{"namespace": "flower.utils.search.stringified_dict_contains_value", "completion": "    if not str_dict:\n        return False\n    if not isinstance(str_dict, str):\n        raise TypeError('str_dict must be a string')\n    if not isinstance(key, str):\n        raise TypeError('key must be a string')\n    if not isinstance(value, str):\n        raise TypeError('value must be a string')\n    if not str_dict.startswith('{') or not str_dict.endswith('}'):\n        raise ValueError('str_dict must be a dictionary in the form of a string')\n    str_dict = str_dict[1:-1]\n    if not str_dict:\n        return False\n    str_dict = str_dict.split(',')\n    for item in str_dict:\n        item = item.strip()\n        if not item:\n            continue\n        if ':' not in item:\n            continue\n        k, v = item.split(':')\n        k = k.strip()\n        v = v.strip()\n        if k == key and v == value:\n            return True\n    return False", "idx": 193}
{"namespace": "flower.utils.abs_path", "completion": "    path = os.path.expanduser(path)\n    if not os.path.isabs(path):\n        path = os.path.join(os.getcwd(), path)\n    return path", "idx": 194}
{"namespace": "flower.utils.strtobool", "completion": "    val = val.lower()\n    if val in ('y', 'yes', 't', 'true', 'on', '1'):\n        return 1\n    if val in ('n', 'no', 'f', 'false', 'off', '0'):\n        return 0\n    raise ValueError(\"invalid truth value %r\" % (val,))", "idx": 195}
{"namespace": "sshuttle.methods.get_method", "completion": "    try:\n        module = importlib.import_module(\"sshuttle.methods.%s\" % method_name)\n    except ImportError:\n        raise Fatal(\"Method %s not found.\" % method_name)\n    try:\n        method = module.Method()\n    except AttributeError:\n        raise Fatal(\"Method %s does not have a Method class.\" % method_name)\n    if not isinstance(method, BaseMethod):\n        raise Fatal(\"Method %s does not inherit from BaseMethod.\" % method_name)\n    return method", "idx": 196}
{"namespace": "trailscraper.iam.all_known_iam_permissions", "completion": "    script_dir = os.path.dirname(os.path.realpath(__file__))\n    with open(os.path.join(script_dir, 'known-iam-actions.txt'), 'r') as f:\n        return set(f.read().splitlines())", "idx": 197}
{"namespace": "trailscraper.cloudtrail.parse_records", "completion": "    return list(filterz(None, map(_parse_record, json_records)))", "idx": 198}
{"namespace": "pycoin.satoshi.IntStreamer.IntStreamer.int_to_script_bytes", "completion": "        if v == 0:\n            return b\"\"\n        if v < 0:\n            v = -v\n            is_negative = True\n        else:\n            is_negative = False\n        s = bytearray()\n        while True:\n            s.append(v & 0xff)\n            v >>= 8\n            if v == 0:\n                break\n        if is_negative:\n            s[0] |= 0x80\n        return bytes(s)", "idx": 199}
{"namespace": "pycoin.satoshi.stackops.do_OP_2DROP", "completion": "    stack.pop()\n    stack.pop()\n\n", "idx": 200}
{"namespace": "pycoin.satoshi.stackops.do_OP_2DUP", "completion": "    stack.append(stack[-2])\n    stack.append(stack[-1])\n\n", "idx": 201}
{"namespace": "pycoin.satoshi.stackops.do_OP_3DUP", "completion": "    stack.append(stack[-3])\n    stack.append(stack[-3])\n    stack.append(stack[-3])\n\n", "idx": 202}
{"namespace": "trailscraper.s3_download._s3_key_prefixes", "completion": "    delta = to_date - from_date\n    dates = [from_date + datetime.timedelta(days=i) for i in range(delta.days + 1)]\n    return [\n        _s3_key_prefix_for_org_trails(prefix, date, org_id, account_id, region)\n        for org_id in org_ids\n        for account_id in account_ids\n        for region in regions\n        for date in dates\n    ]", "idx": 203}
{"namespace": "pycoin.satoshi.stackops.do_OP_2OVER", "completion": "    stack.append(stack[-3])\n    stack.append(stack[-4])\n\n", "idx": 204}
{"namespace": "pycoin.satoshi.stackops.do_OP_2SWAP", "completion": "    stack.append(stack.pop(-3))\n    stack.append(stack.pop(-3))\n\n", "idx": 205}
{"namespace": "pycoin.satoshi.stackops.do_OP_IFDUP", "completion": "    if stack[-1]:\n        stack.append(stack[-1])\n\n", "idx": 206}
{"namespace": "pycoin.satoshi.stackops.do_OP_NIP", "completion": "    stack.append(stack.pop(-2))\n\n", "idx": 207}
{"namespace": "pycoin.satoshi.stackops.do_OP_TUCK", "completion": "    stack.append(stack.pop(-3))\n    stack.append(stack.pop(-3))\n    stack.append(stack[-1])\n\n", "idx": 208}
{"namespace": "pycoin.satoshi.stackops.do_OP_CAT", "completion": "    v1 = stack.pop()\n    v2 = stack.pop()\n    stack.append(v1 + v2)\n\n", "idx": 209}
{"namespace": "pycoin.crack.ecdsa.crack_secret_exponent_from_k", "completion": "    # Calculate the signature of the signed value.\n    sig_of_signed_value = pow(signed_value, k, generator.n)\n\n    # Calculate the secret exponent.\n    secret_exponent = (sig - sig_of_signed_value) % generator.n\n\n    return secret_exponent", "idx": 210}
{"namespace": "pycoin.crack.ecdsa.crack_k_from_sigs", "completion": "    r1, s1 = sig1\n    r2, s2 = sig2\n    k = (val1 * s2 - val2 * s1) * generator.inverse(r1 - r2) % generator.order()\n    return k", "idx": 211}
{"namespace": "pycoin.message.make_parser_and_packer.standard_streamer", "completion": "    streamer = Streamer(parse_satoshi_int)\n    for name, func in parsing_functions:\n        streamer.register_parsing_function(name, func)\n    return streamer", "idx": 212}
{"namespace": "pycoin.key.subpaths.subpaths_for_path_range", "completion": "    # Split the path range into parts\n    parts = path_range.split('/')\n\n    # Initialize the output paths list\n    output_paths = []\n\n    # Iterate over the parts\n    for i, part in enumerate(parts):\n\n        # Check if the part contains a range\n        if '-' in part:\n\n            # Split the part into start and end\n            start, end = part.split('-')\n\n            # Convert the start and end to integers\n            start = int(start)\n            end = int(end)\n\n            # Generate the range of integers\n            range_ints = range(start, end + 1)\n\n            # Convert the range of integers to strings\n            range_strs = [str(x) for x in range_ints]\n\n            # Replace the part with the range of strings\n            parts[i] = range_strs\n\n    # Generate all possible combinations of parts\n    combinations = itertools.product(*parts)\n\n    # Iterate over the combinations\n    for combination in combinations:\n\n        # Join the parts to form a path\n        path = '/'.join(combination)\n\n        # Check if the path contains hardening characters\n        if any(char in path for char in hardening_chars):\n\n            # Add the path to the output paths list\n            output_paths.append(path)\n\n    # Return the output paths list\n    return output_paths", "idx": 213}
{"namespace": "pyt.core.project_handler._is_python_file", "completion": "    return path.endswith('.py')", "idx": 214}
{"namespace": "pycoin.encoding.hexbytes.h2b", "completion": "    return binascii.unhexlify(h)\n\n", "idx": 215}
{"namespace": "zxcvbn.scoring.calc_average_degree", "completion": "    total_degree = 0\n    for node in graph:\n        total_degree += len(graph[node])\n    return total_degree / len(graph)\n\n", "idx": 216}
{"namespace": "zxcvbn.scoring.nCk", "completion": "    return factorial(n) / (factorial(k) * factorial(n - k))\n\n", "idx": 217}
{"namespace": "zxcvbn.matching.relevant_l33t_subtable", "completion": "    # Create a dictionary of characters in the password\n    password_chars = {}\n    for char in password:\n        if char in password_chars:\n            password_chars[char] += 1\n        else:\n            password_chars[char] = 1\n\n    # Create a subtable based on the relevant substitutions in the table\n    subtable = {}\n    for char, substitutions in table.items():\n        if char in password_chars:\n            subtable[char] = substitutions\n\n    return subtable\n\n", "idx": 218}
{"namespace": "zxcvbn.matching.translate", "completion": "    translated_string = ''\n    for char in string:\n        if char in chr_map:\n            translated_string += chr_map[char] + ' '\n        else:\n            translated_string += char + ' '\n\n    return translated_string.strip()\n\n", "idx": 219}
{"namespace": "tools.cgrep.get_nets", "completion": "  results = []\n  for obj in objects:\n    # get the networks inside the object\n    networks = db.GetNetParents(obj)\n    # if the object is a network, then add it to the results\n    if networks:\n      results.append((obj, networks))\n    # if the object is a service, then get the networks inside the service\n    else:\n      networks = db.GetServiceParents(obj)\n      # if the object is a service, then add it to the results\n      if networks:\n        results.append((obj, networks))\n  return results", "idx": 220}
{"namespace": "tools.cgrep.get_ports", "completion": "  results = []\n  for svc in svc_group:\n    svc_def = db.GetSvc(svc)\n    for port in svc_def:\n      results.append((svc, '%s/%s' % (port.port, port.protocol)))\n  return results", "idx": 221}
{"namespace": "tools.cgrep.compare_ip_token", "completion": "  results = []\n  for ip in options.ip:\n    ip = nacaddr.IP(ip)\n    for token in db.GetIpParents(ip):\n      if ip in db.GetNet(token):\n        results.append(token)\n  if results:\n    return 'IP: %s is in %s' % (ip, results)\n  else:\n    return 'IP: %s is not in any network' % ip", "idx": 222}
{"namespace": "tools.cgrep.get_services", "completion": "  port = options.port[0]\n  protocol = options.port[1]\n  results = []\n  for svc in db.GetServices():\n    if port in db.GetService(svc) and protocol in db.GetService(svc):\n      results.append(svc)\n  return port, protocol, results", "idx": 223}
{"namespace": "asyncssh.packet.String", "completion": "    if isinstance(value, str):\n        value = value.encode('utf-8')\n    return UInt32(len(value)) + value\n\n", "idx": 224}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_cmd_counts", "completion": "    # Add 1 to each count, including the unk_token, to handle unseen commands.\n    seq1_counts[unk_token] += 1\n    for seq2_count in seq2_counts.values():\n        seq2_count[unk_token] += 1\n\n    # Add 1 to the start and end tokens.\n    seq1_counts[start_token] += 1\n    seq1_counts[end_token] += 1\n    for seq2_count in seq2_counts.values():\n        seq2_count[start_token] += 1\n        seq2_count[end_token] += 1\n\n    return seq1_counts, seq2_counts\n\n", "idx": 225}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_param_counts", "completion": "    param_counts_ls = copy.deepcopy(param_counts)\n    cmd_param_counts_ls = copy.deepcopy(cmd_param_counts)\n\n    for cmd in cmds:\n        for param in cmd_param_counts_ls[cmd].keys():\n            cmd_param_counts_ls[cmd][param] += 1\n        param_counts_ls[param] += 1\n\n    return param_counts_ls, cmd_param_counts_ls", "idx": 226}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_value_counts", "completion": "    value_counts_ls = copy.deepcopy(value_counts)\n    param_value_counts_ls = copy.deepcopy(param_value_counts)\n\n    values: List[str] = list(value_counts.keys()) + [unk_token]\n    for param in params:\n        for value in values:\n            if value in param_value_counts_ls[param] or value == unk_token:\n                value_counts_ls[value] += 1\n                param_value_counts_ls[param][value] += 1\n\n    return value_counts_ls, param_value_counts_ls", "idx": 227}
{"namespace": "diffprivlib.validation.check_epsilon_delta", "completion": "    if not isinstance(epsilon, Real):\n        raise TypeError(\"Epsilon and delta must be numeric\")\n    if not isinstance(delta, Real):\n        raise TypeError(\"Epsilon and delta must be numeric\")\n    if epsilon < 0:\n        raise ValueError(\"Epsilon must be non-negative\")\n    if not 0 <= delta <= 1:\n        raise ValueError(\"Delta must be in [0, 1]\")\n    if epsilon == 0 and delta == 0 and not allow_zero:\n        raise ValueError(\"Epsilon and Delta cannot both be zero\")\n\n", "idx": 228}
{"namespace": "diffprivlib.utils.check_random_state", "completion": "    if seed is None:\n        if secure:\n            return secrets.SystemRandom()\n        else:\n            return np.random.mtrand._rand\n    elif isinstance(seed, int):\n        return np.random.RandomState(seed)\n    elif isinstance(seed, np.random.RandomState):\n        return seed\n    elif isinstance(seed, secrets.SystemRandom):\n        return seed\n    else:\n        raise ValueError(\"{} cannot be used to seed a numpy.random.RandomState\".format(seed))", "idx": 229}
{"namespace": "diffprivlib.validation.clip_to_norm", "completion": "    if not isinstance(array, np.ndarray):\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\n    if array.ndim != 2:\n        raise ValueError(f\"input array must be 2-dimensional, got {array.ndim} dimensions.\")\n    if not isinstance(clip, Real):\n        raise TypeError(f\"Clip value must be numeric, got {type(clip)}.\")\n    if clip <= 0:\n        raise ValueError(f\"Clip value must be strictly positive, got {clip}.\")\n\n    norms = np.linalg.norm(array, axis=1)\n    norms[norms > clip] = clip\n    return array / norms[:, np.newaxis]", "idx": 230}
{"namespace": "diffprivlib.models.pca.PCA.fit_transform", "completion": "        self.fit(X, y)\n        return self.transform(X)", "idx": 231}
{"namespace": "discord.utils.get_slots", "completion": "    for base in cls.__mro__:\n        for key in base.__dict__:\n            if isinstance(base.__dict__[key], classproperty):\n                yield key\n\n", "idx": 232}
{"namespace": "discord.utils.is_inside_class", "completion": "    if not hasattr(func, '__qualname__'):\n        return False\n\n    qualname = func.__qualname__\n    if '.' not in qualname:\n        return False\n\n    # If the function is defined inside a class, the qualname has a dotted path\n    # denoting which class it belongs to. So, e.g. for A.foo the qualname\n    # would be A.foo while a global foo() would just be foo.\n    #\n    # Unfortunately, for nested functions this breaks. So inside an outer\n    # function named outer, those two would end up having a qualname with\n    # outer.<locals>.A.foo and outer.<locals>.foo\n    #\n    # This is a workaround for that.\n    #\n    # If the function is defined inside a class, the qualname has a dotted path\n    # denoting which class it belongs to. So, e.g. for A.foo the qualname\n    # would be A.foo while a global foo() would just be foo.\n    #\n    # Unfortunately, for nested functions this breaks. So inside an outer\n    # function named outer, those two would end up having a qualname with\n    # outer.<locals>.A.foo and outer.<locals>.foo\n    #\n    # This is a workaround for that.\n    #\n    # If the function is defined inside a class, the qualname has a dotted path\n    # denoting which class it belongs to. So, e.g. for A.foo the qualname\n    # would be A.foo while a global foo() would just be foo.\n    #\n    # Unfortunately, for nested functions this breaks. So inside an outer\n    # function named outer, those two would end up having a qualname with\n    # outer.<locals>.A.foo and outer.<locals>.foo\n    #\n    # This is a workaround for that.\n    #\n    # If the function is defined inside a class, the qualname has a dotted path\n    # denoting which class it belongs to. So, e.g. for A.foo the qualname\n    # would be A.foo while", "idx": 233}
{"namespace": "faker.utils.decorators.slugify", "completion": "    def wrapper(*args, **kwargs) -> str:\n        return text.slugify(fn(*args, **kwargs))\n\n    return wrapper", "idx": 234}
{"namespace": "faker.utils.decorators.slugify_domain", "completion": "    def wrapper(*args: Tuple[T, ...], **kwargs: Dict[str, T]) -> str:\n        return text.slugify(fn(*args, **kwargs), allow_dots=True)\n\n    return wrapper", "idx": 235}
{"namespace": "faker.utils.decorators.slugify_unicode", "completion": "    def wrapper(*args: Tuple[T, ...], **kwargs: Dict[str, T]) -> str:\n        return text.slugify_unicode(fn(*args, **kwargs))\n\n    return wrapper", "idx": 236}
{"namespace": "faker.utils.loading.get_path", "completion": "    if getattr(sys, \"frozen\", False):\n        if hasattr(sys, \"_MEIPASS\"):\n            return Path(sys._MEIPASS)\n        else:\n            return Path(sys.executable).parent\n    else:\n        return Path(module.__file__).parent\n\n", "idx": 237}
{"namespace": "faker.utils.checksums.luhn_checksum", "completion": "    # Convert the number to a list of integers\n    digits = [int(digit) for digit in str(number)]\n\n    # Double every second digit, starting from the right\n    for i in range(len(digits) - 2, -1, -2):\n        digits[i] *= 2\n\n    # Subtract 9 from any digit that is greater than 9\n    for i in range(len(digits)):\n        if digits[i] > 9:\n            digits[i] -= 9\n\n    # Sum the digits\n    checksum = sum(digits)\n\n    # Calculate the check digit\n    check_digit = (10 - (checksum % 10)) % 10\n\n    return check_digit\n\n", "idx": 238}
{"namespace": "faker.utils.datasets.add_ordereddicts", "completion": "    return OrderedDict(chain(*odicts.items()))", "idx": 239}
{"namespace": "faker.providers.person.pl_PL.checksum_identity_card_number", "completion": "    weights = [7, 3, 1, 0, 7, 3, 1, 7, 3]\n    sum = 0\n    for i in range(len(characters)):\n        sum += int(characters[i]) * weights[i]\n    return sum % 10\n\n", "idx": 240}
{"namespace": "faker.providers.company.pl_PL.regon_checksum", "completion": "    weights = [8, 9, 2, 3, 4, 5, 6, 7]\n    sum = 0\n    for i in range(len(digits)):\n        sum += digits[i] * weights[i]\n    return sum % 11\n\n", "idx": 241}
{"namespace": "faker.providers.company.ru_RU.calculate_checksum", "completion": "    factors = [3, 7, 2, 4, 10, 3, 5, 9, 4, 6, 8]\n    checksum = 0\n    for i, char in enumerate(value):\n        checksum += int(char) * factors[i % len(factors)]\n    return str(checksum)[-1]", "idx": 242}
{"namespace": "faker.providers.company.pl_PL.local_regon_checksum", "completion": "    weights_for_check_digit = [2, 4, 8, 5, 0, 9, 7, 3, 6, 1, 2, 4, 8]\n    check_digit = 0\n\n    for i in range(0, 13):\n        check_digit += weights_for_check_digit[i] * digits[i]\n\n    check_digit %= 11\n\n    if check_digit == 10:\n        check_digit = 0\n\n    return check_digit", "idx": 243}
{"namespace": "faker.providers.company.pl_PL.company_vat_checksum", "completion": "    weights_for_check_digit = [6, 5, 7, 2, 3, 4, 5, 6, 7]\n    check_digit = 0\n\n    for i in range(0, 9):\n        check_digit += weights_for_check_digit[i] * digits[i]\n\n    check_digit %= 11\n\n    if check_digit == 10:\n        check_digit = 0\n\n    return check_digit", "idx": 244}
{"namespace": "faker.providers.company.pt_BR.company_id_checksum", "completion": "    weights = [6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 2]\n    checksum = sum(digits[i] * weights[i] for i in range(12))\n    checksum %= 11\n    checksum = 11 - checksum\n    if checksum == 10:\n        checksum = 0\n    return digits + [checksum]", "idx": 245}
{"namespace": "faker.providers.misc.Provider.binary", "completion": "        return self.generator.random.getrandbits(length * 8).to_bytes(length, \"big\")", "idx": 246}
{"namespace": "faker.providers.python.Provider.pystr", "completion": "        if min_chars is None:\n            min_chars = max_chars\n        return prefix + self.random_sample(string.ascii_letters, self.random_int(min_chars, max_chars)) + suffix", "idx": 247}
{"namespace": "dash._utils.AttributeDict.set_read_only", "completion": "        if not isinstance(names, list):\n            names = [names]\n        for name in names:\n            if name not in self._read_only:\n                self._read_only[name] = msg\n            else:\n                self._read_only[name] += f\"\\n{msg}\"\n\n", "idx": 248}
{"namespace": "dash._utils.AttributeDict.first", "completion": "        if len(names) == 0:\n            for key in self:\n                if self[key] is not None:\n                    return self[key]\n        else:\n            for name in names:\n                if name in self:\n                    if self[name] is not None:\n                        return self[name]\n", "idx": 249}
{"namespace": "dash._get_paths.app_get_asset_url", "completion": "    if config.assets_external_path:\n        return config.assets_external_path + path\n    elif config.requests_pathname_prefix:\n        return config.requests_pathname_prefix + path\n    else:\n        raise exceptions.AssetsNotFoundException(\n            \"Could not find assets URL. \"\n            \"Please specify either the 'assets_external_path' \"\n            \"or 'requests_pathname_prefix' configuration options.\"\n        )", "idx": 250}
{"namespace": "peewee.sort_models", "completion": "    # Create a dictionary to store the dependencies of each model.\n    dependencies = {}\n    for model in models:\n        dependencies[model] = set()\n        for _, fk in model.dependencies():\n            dependencies[model].add(fk.model)\n\n    # Create a dictionary to store the sorted models.\n    sorted_models = {}\n\n    # Create a set to store the visited models.\n    visited = set()\n\n    # Create a set to store the models that are currently being visited.\n    visiting = set()\n\n    # Define a recursive function to sort the models.\n    def sort_models_recursive(model):\n        # If the model is already in the sorted models, return it.\n        if model in sorted_models:\n            return sorted_models[model]\n\n        # If the model is already in the visiting set, it means that there is a circular dependency.\n        if model in visiting:\n            raise ValueError(\"Circular dependency detected in models.\")\n\n        # Add the model to the visiting set.\n        visiting.add(model)\n\n        # Sort the dependencies of the model.\n        for dependency in dependencies[model]:\n            sort_models_recursive(dependency)\n\n        # Remove the model from the visiting set.\n        visiting.remove(model)\n\n        # Add the model to the sorted models.\n        sorted_models[model] = model\n\n        # Return the model.\n        return model\n\n    # Sort the models.\n    for model in models:\n        sort_models_recursive(model)\n\n    # Return the sorted models.\n    return list(sorted_models.values())", "idx": 251}
{"namespace": "dash._grouping.grouping_len", "completion": "    return len(flatten_grouping(grouping))", "idx": 252}
{"namespace": "playhouse.kv.KeyValue.get", "completion": "        try:\n            return self[key]\n        except KeyError:\n            return default\n", "idx": 253}
{"namespace": "playhouse.kv.KeyValue.setdefault", "completion": "        if key not in self:\n            self[key] = default\n        return self[key]", "idx": 254}
{"namespace": "sslyze.plugins.certificate_info._certificate_utils.get_public_key_sha256", "completion": "    public_key = certificate.public_key()\n    public_key_bytes = public_key.public_bytes(Encoding.DER, PublicFormat.SubjectPublicKeyInfo)\n    return sha256(public_key_bytes).digest()", "idx": 255}
{"namespace": "ydata_profiling.compare_reports._compare_title", "completion": "    if len(set(titles)) == 1:\n        return titles[0]\n    else:\n        return \" and \".join(titles)", "idx": 256}
{"namespace": "ydata_profiling.report.formatters.fmt_bytesize", "completion": "    for unit in [\"\", \"Ki\", \"Mi\", \"Gi\", \"Ti\", \"Pi\", \"Ei\", \"Zi\"]:\n        if abs(num) < 1024.0:\n            return f\"{num:3.1f}{unit}{suffix}\"\n        num /= 1024.0\n    return f\"{num:.1f}Yi{suffix}\"", "idx": 257}
{"namespace": "ydata_profiling.report.formatters.fmt_percent", "completion": "    if edge_cases:\n        if value == 0:\n            return \"0%\"\n        if value == 1:\n            return \"100%\"\n    return f\"{value:.1%}\"", "idx": 258}
{"namespace": "ydata_profiling.report.formatters.fmt_numeric", "completion": "    return f\"{value:.{precision}f}\"", "idx": 259}
{"namespace": "ydata_profiling.report.formatters.fmt_array", "completion": "    if np.isnan(threshold):\n        return str(value)\n    if len(value) > threshold:\n        return f\"{value[:threshold]} ... {value[-threshold:]}\"\n    return str(value)", "idx": 260}
{"namespace": "ydata_profiling.report.formatters.fmt_monotonic", "completion": "    if value == 0:\n        return \"0\"\n    elif value == 1:\n        return \"1st\"\n    elif value == 2:\n        return \"2nd\"\n    elif value == 3:\n        return \"3rd\"\n    else:\n        return f\"{value}th\"", "idx": 261}
{"namespace": "ydata_profiling.visualisation.plot._plot_pie_chart", "completion": "    # Use the pd.Series indices as category names\n    labels = data.index.values.astype(str)\n\n    # Plot\n    _, ax = plt.subplots(figsize=(7, 2))\n    ax.axis(\"off\")\n\n    ax.pie(\n        data,\n        labels=labels,\n        colors=colors,\n        autopct=\"%1.1f%%\",\n        pctdistance=0.8,\n        labeldistance=1.1,\n        startangle=90,\n    )\n\n    legend = None\n    if not hide_legend:\n        legend = ax.legend(\n            ncol=1, bbox_to_anchor=(0, 0), fontsize=\"xx-large\", loc=\"upper left\"\n        )\n\n    return ax, legend", "idx": 262}
{"namespace": "ydata_profiling.visualisation.plot._prepare_heatmap_data", "completion": "    if selected_entities is not None:\n        dataframe = dataframe[dataframe[entity_column].isin(selected_entities)]\n\n    if sortby is not None:\n        dataframe = dataframe.sort_values(by=sortby, ascending=False)\n\n    dataframe = dataframe.groupby(entity_column).size().reset_index(name=\"count\")\n    dataframe = dataframe.sort_values(by=\"count\", ascending=False)\n    dataframe = dataframe.head(max_entities)\n\n    return dataframe", "idx": 263}
{"namespace": "ydata_profiling.visualisation.plot._create_timeseries_heatmap", "completion": "    fig, ax = plt.subplots(figsize=figsize)\n    sns.heatmap(\n        df,\n        cmap=color,\n        cbar=False,\n        linewidths=0.5,\n        linecolor=\"white\",\n        ax=ax,\n        yticklabels=1,\n    )\n    ax.set_ylabel(\"\")\n    ax.set_xlabel(\"\")\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n    return ax", "idx": 264}
{"namespace": "ydata_profiling.model.expectation_algorithms.generic_expectations", "completion": "    if name not in batch.columns:\n        raise ValueError(f\"Column {name} not found in batch\")\n\n    if batch[name].isnull().any():\n        raise ValueError(f\"Column {name} contains missing values\")\n\n    if len(batch[name].unique()) != len(batch[name]):\n        raise ValueError(f\"Column {name} contains duplicate values\")\n\n    return name, summary, batch", "idx": 265}
{"namespace": "ydata_profiling.model.expectation_algorithms.numeric_expectations", "completion": "    batch.expect_column_to_exist(name)\n\n    if summary[\"n_missing\"] == 0:\n        batch.expect_column_values_to_not_be_null(name)\n\n    if summary[\"p_unique\"] == 1.0:\n        batch.expect_column_values_to_be_unique(name)\n\n    if summary[\"min\"] >= 0:\n        batch.expect_column_values_to_be_between(\n            name, min_value=0, strict_min=False\n        )\n\n    return name, summary, batch", "idx": 266}
{"namespace": "ydata_profiling.model.expectation_algorithms.categorical_expectations", "completion": "    batch.expect_column_values_to_be_in_set(\n        name,\n        value_set=summary[\"value_counts_without_nan\"].keys(),\n        meta={\n            \"notes\": {\n                \"format\": \"markdown\",\n                \"content\": [\n                    \"The column values should be in the set of value counts without NaN.\"\n                ],\n            }\n        },\n    )\n\n    if summary[\"n_distinct\"] < summary[\"n_distinct_threshold\"]:\n        batch.expect_column_values_to_be_in_set(\n            name,\n            value_set=summary[\"value_counts_without_nan\"].keys(),\n            meta={\n                \"notes\": {\n                    \"format\": \"markdown\",\n                    \"content\": [\n                        \"The column values should be in the set of value counts without NaN.\"\n                    ],\n                }\n            },\n        )\n\n    if summary[\"p_distinct\"] < summary[\"p_distinct_threshold\"]:\n        batch.expect_column_values_to_be_in_set(\n            name,\n            value_set=summary[\"value_counts_without_nan\"].keys(),\n            meta={\n                \"notes\": {\n                    \"format\": \"markdown\",\n                    \"content\": [\n                        \"The column values should be in the set of value counts without NaN.\"\n                    ],\n                }\n            },\n        )\n\n    return name, summary, batch", "idx": 267}
{"namespace": "ydata_profiling.model.expectation_algorithms.datetime_expectations", "completion": "    if any(k in summary for k in [\"min\", \"max\"]):\n        batch.expect_column_values_to_be_between(\n            name, min_value=summary.get(\"min\"), max_value=summary.get(\"max\")\n        )\n\n    return name, summary, batch", "idx": 268}
{"namespace": "ydata_profiling.model.expectation_algorithms.file_expectations", "completion": "    return name, summary, batch", "idx": 269}
{"namespace": "ydata_profiling.model.pandas.describe_categorical_pandas.word_summary_vc", "completion": "    # Unicode Character Summaries (category and script name)\n    character_counts = get_character_counts_vc(vc)\n\n    character_counts_series = character_counts\n    summary = {\n        \"n_characters_distinct\": len(character_counts_series),\n        \"n_characters\": np.sum(character_counts_series.values),\n        \"character_counts\": character_counts_series,\n    }\n\n    char_to_block = {key: block(key) for key in character_counts.keys()}\n    char_to_category_short = {key: category(key) for key in character_counts.keys()}\n    char_to_script = {key: script(key) for key in character_counts.keys()}\n\n    summary.update(\n        {\n            \"category_alias_values\": {\n                key: category_long(value)\n                for key, value in char_to_category_short.items()\n            },\n            \"block_alias_values\": {\n                key: block_abbr(value) for key, value in char_to_block.items()\n            },\n        }\n    )\n\n    # Retrieve original distribution\n    block_alias_counts: Counter = Counter()\n    per_block_char_counts: dict = {\n        k: Counter() for k in summary[\"block_alias_values\"].values()\n    }\n    for char, n_char in character_counts.items():\n        block_name = summary[\"block_alias_values\"][char]\n        block_alias_counts[block_name] += n_char\n        per_block_char_counts[block_name][char] = n_char\n    summary[\"block_alias_counts\"] = counter_to_series(block_alias_counts)\n    summary[\"n_block_alias\"] = len(summary[\"block_alias_counts\"])\n    summary[\"block_alias_char_counts\"] = {\n        k: counter_to_series(v) for k, v in per_block", "idx": 270}
{"namespace": "ydata_profiling.model.pandas.imbalance_pandas.column_imbalance_score", "completion": "    if n_classes == 1:\n        return 0\n    elif n_classes == 2:\n        return 1 - value_counts.max() / value_counts.sum()\n    else:\n        return entropy(value_counts / value_counts.sum(), base=2) / log2(n_classes)", "idx": 271}
{"namespace": "django.core.exceptions.ValidationError.messages", "completion": "        if hasattr(self, \"error_dict\"):\n            return sum(self.error_dict.values(), [])\n        else:\n            return self.error_list\n", "idx": 272}
{"namespace": "django.utils.module_loading.module_has_submodule", "completion": "    if not isinstance(package, str):\n        package = package.__name__\n\n    try:\n        package = importlib_find(package)\n    except AttributeError:\n        return False\n\n    if not package:\n        return False\n\n    try:\n        import_module(f\"{package}.{module_name}\")\n    except ImportError:\n        return False\n\n    return True", "idx": 273}
{"namespace": "django.utils.timezone.get_fixed_timezone", "completion": "    if isinstance(offset, timedelta):\n        offset = offset.total_seconds() / 60\n    offset = int(offset)\n    if offset == 0:\n        return timezone.utc\n    sign = \"-\" if offset < 0 else \"+\"\n    hhmm = \"%02d%02d\" % divmod(abs(offset), 60)\n    name = \"UTC%s%s\" % (sign, hhmm)\n    return timezone(timedelta(minutes=offset), name)", "idx": 274}
{"namespace": "django.utils.encoding.filepath_to_uri", "completion": "    # The list of safe characters here is constructed from the \"reserved\" and\n    # \"unreserved\" characters specified in RFC 3986 Sections 2.2 and 2.3:\n    #     reserved    = gen-delims / sub-delims\n    #     gen-delims  = \":\" / \"/\" / \"?\" / \"#\" / \"[\" / \"]\" / \"@\"\n    #     sub-delims  = \"!\" / \"$\" / \"&\" / \"'\" / \"(\" / \")\"\n    #                   / \"*\" / \"+\" / \",\" / \";\" / \"=\"\n    #     unreserved  = ALPHA / DIGIT / \"-\" / \".\" / \"_\" / \"~\"\n    # Of the unreserved characters, urllib.parse.quote() already considers all\n    # but the ~ safe.\n    # The % character is also added to the list of safe characters here, as the\n    # end of RFC 3987 Section 3.1 specifically mentions that % must not be\n    # converted.\n    # The ' character is not added to the list of safe characters here, as it\n    # is a valid character within URIs.\n    return quote(path, safe=\"/#%[]=:;$&()+,;=@'~\")", "idx": 275}
{"namespace": "django.utils._os.to_path", "completion": "    if isinstance(value, Path):\n        return value\n    elif isinstance(value, str):\n        return Path(value)\n    else:\n        raise TypeError(f\"Invalid type for value: {type(value)}\")", "idx": 276}
{"namespace": "django.utils.lorem_ipsum.sentence", "completion": "    sentence = \"\"\n    sentence += random.choice(COMMON_WORDS).capitalize()\n    sentence += \" \"\n    sentence += random.choice(WORDS)\n    sentence += \" \"\n    sentence += random.choice(WORDS)\n    sentence += \" \"\n    sentence += random.choice(WORDS)\n    sentence += \" \"\n    sentence += random.choice(WORDS)\n    sentence += \" \"\n    sentence += random.choice(WORDS)\n    sentence += \" \"\n    sentence += random.choice(WORDS)\n    sentence += \" \"\n    sentence += random.choice(WORDS)\n    sentence += \" \"\n    sentence += random.choice(WORDS)\n    sentence += \" \"\n    sentence += random.choice(WORDS)\n    sentence += \" \"\n    sentence += random.choice(WORDS)\n    sentence += \" \"\n    sentence += random.choice(WORDS)\n    sentence += \" \"\n    sentence += random.choice(WORDS)\n    sentence += \" \"\n    sentence += random.choice(WORDS)\n    sentence += \" \"\n    sentence += random.choice(WORDS)\n    sentence += \" \"\n    sentence += random.choice(WORDS)\n    sentence += \" \"\n    sentence += random.choice(WORDS)\n    sentence += \" \"\n    sentence += random.choice(WORDS)\n    sentence += \" \"\n    sentence += random.choice(WORDS)\n    sentence += \" \"\n    sentence += random.choice(WORDS)\n    sentence += \" \"\n    sentence += random.choice(WORDS)\n    sentence += \" \"\n    sentence += random.choice(WORDS)\n    sentence += \" \"\n    sentence += random.choice(WORDS)\n    sentence += \" \"\n    sentence += random.choice(WORDS)\n    sentence += \" \"\n    sentence += random.choice(WORDS)\n    sentence += \" \"\n    sentence += random.choice(WORDS)\n    sentence += \" \"\n    sentence += random.choice(WORDS)\n    sentence += \" \"\n    sentence += random.choice(WORDS)\n    sentence += \" \"\n    sentence += random.choice(WORDS)", "idx": 277}
{"namespace": "ydata_profiling.utils.dataframe.sort_column_names", "completion": "    if sort is None:\n        return dct\n    if sort == \"ascending\":\n        return dict(sorted(dct.items(), key=lambda item: item[0]))\n    if sort == \"descending\":\n        return dict(sorted(dct.items(), key=lambda item: item[0], reverse=True))\n    raise ValueError(f\"Invalid sorting order: {sort}\")", "idx": 278}
{"namespace": "django.utils.ipv6.is_valid_ipv6_address", "completion": "    try:\n        ipaddress.IPv6Address(ip_str)\n        return True\n    except ValueError:\n        return False", "idx": 279}
{"namespace": "django.utils.http.urlsafe_base64_decode", "completion": "    s = s.encode(\"ascii\")\n    s += b\"=\" * (4 - len(s) % 4)\n    return base64.urlsafe_b64decode(s)", "idx": 280}
{"namespace": "django.utils.http.parse_etags", "completion": "    if etag_str == \"*\":\n        return [\"*\"]\n    etags = etag_str.split(\",\")\n    return [quote(etag.strip(), safe=RFC3986_GENDELIMS + RFC3986_SUBDELIMS) for etag in etags]\n\n", "idx": 281}
{"namespace": "django.utils.http.is_same_domain", "completion": "    if pattern == \"*\":\n        return True\n    if pattern.startswith(\"*.\"):\n        pattern = pattern[2:]\n        return host.endswith(pattern)\n    return host == pattern", "idx": 282}
{"namespace": "django.utils.http.content_disposition_header", "completion": "    if as_attachment:\n        return 'attachment; filename=\"%s\"' % filename\n    else:\n        return 'inline; filename=\"%s\"' % filename", "idx": 283}
{"namespace": "pysnooper.utils.truncate", "completion": "    if len(string) <= max_length:\n        return string\n    else:\n        return '...' + string[-(max_length - 3):] + '...'", "idx": 284}
{"namespace": "pysnooper.variables.needs_parentheses", "completion": "    source_without_parentheses = source.replace('(', '').replace(')', '')\n    source_with_parentheses = '(' + source + ')'\n\n    if utils.get_bytecode(source_without_parentheses) == utils.get_bytecode(source_with_parentheses):\n        return False\n    else:\n        return True\n\n", "idx": 285}
{"namespace": "django.test.utils.extend_sys_path", "completion": "    original_sys_path = list(sys.path)\n    sys.path.extend(paths)\n    try:\n        yield\n    finally:\n        sys.path = original_sys_path\n\n", "idx": 286}
{"namespace": "albumentations.augmentations.functional.normalize_cv2", "completion": "    if mean.shape != img.shape:\n        mean = mean.reshape(img.shape)\n    if denominator.shape != img.shape:\n        denominator = denominator.reshape(img.shape)\n    img = img.astype(np.float32)\n    img -= mean\n    img /= denominator\n    return img\n\n", "idx": 287}
{"namespace": "albumentations.augmentations.functional.normalize_numpy", "completion": "    img = img.astype(\"float32\")\n    img -= mean\n    img /= denominator\n    return img", "idx": 288}
{"namespace": "albumentations.augmentations.functional.gamma_transform", "completion": "    input_dtype = img.dtype\n    needs_float = False\n\n    if input_dtype == np.float32:\n        img = from_float(img, dtype=np.dtype(\"uint8\"))\n        needs_float = True\n    elif input_dtype not in (np.uint8, np.float32):\n        raise ValueError(\"Unexpected dtype {} for gamma_transform augmentation\".format(input_dtype))\n\n    img = img.astype(np.float32)\n    img = img / 255.0\n    img = img ** gamma\n    img = img * 255.0\n    img = img.astype(np.uint8)\n\n    if needs_float:\n        img = to_float(img, max_value=255)\n\n    return img", "idx": 289}
{"namespace": "albumentations.augmentations.functional.swap_tiles_on_image", "completion": "    # Get the image dimensions\n    height, width, channels = image.shape\n\n    # Create a copy of the input image\n    output_image = image.copy()\n\n    # Iterate over the tiles\n    for tile in tiles:\n        # Get the current and old left-up corner coordinates, height, and width of the tile\n        current_left, current_top, current_height, current_width = tile\n        old_left, old_top, old_height, old_width = tile\n\n        # Calculate the new left-up corner coordinates of the tile\n        new_left = old_left\n        new_top = old_top\n\n        # Calculate the new height and width of the tile\n        new_height = current_height\n        new_width = current_width\n\n        # Calculate the new right-down corner coordinates of the tile\n        new_right = new_left + new_width\n        new_bottom = new_top + new_height\n\n        # Calculate the old right-down corner coordinates of the tile\n        old_right = old_left + old_width\n        old_bottom = old_top + old_height\n\n        # Check if the new tile is within the image boundaries\n        if (\n            new_left >= 0\n            and new_top >= 0\n            and new_right <= width\n            and new_bottom <= height\n            and old_left >= 0\n            and old_top >= 0\n            and old_right <= width\n            and old_bottom <= height\n        ):\n            # Extract the tile from the input image\n            tile_image = image[old_top:old_bottom, old_left:old_right]\n\n            # Replace the tile in the output image with the new tile\n            output_image[new_top:new_bottom, new_left:new_right] = tile_image\n\n    # Return the output image\n    return output_image", "idx": 290}
{"namespace": "albumentations.augmentations.geometric.functional.keypoint_rotate", "completion": "    x, y, angle, scale = keypoint[:4]\n    angle = np.deg2rad(angle)\n    x_t = (np.cos(angle) * x - np.sin(angle) * y) / scale\n    y_t = (np.sin(angle) * x + np.cos(angle) * y) / scale\n    x_t = x_t + 0.5\n    y_t = y_t + 0.5\n    return x_t, y_t, angle, scale\n\n", "idx": 291}
{"namespace": "albumentations.augmentations.geometric.functional.keypoint_shift_scale_rotate", "completion": "    x, y, angle, scale = keypoint[:4]\n    center = (cols - 1) * 0.5, (rows - 1) * 0.5\n    matrix = cv2.getRotationMatrix2D(center, angle, scale)\n    x, y = cv2.transform(np.array([[[x, y]]]), matrix).squeeze()\n    x += dx * cols\n    y += dy * rows\n    return x, y, angle + math.radians(angle), scale\n\n", "idx": 292}
{"namespace": "albumentations.core.keypoints_utils.angle_to_2pi_range", "completion": "    return angle % (2 * math.pi)\n\n", "idx": 293}
{"namespace": "albumentations.augmentations.geometric.functional.rot90", "completion": "    return np.rot90(img, factor)", "idx": 294}
{"namespace": "albumentations.core.keypoints_utils.convert_keypoints_to_albumentations", "completion": "    if source_format not in keypoint_formats:\n        raise ValueError(\"Unknown target_format {}. Supported formats are: {}\".format(source_format, keypoint_formats))\n\n    return [\n        convert_keypoint_to_albumentations(kp, source_format, rows, cols, check_validity, angle_in_degrees)\n        for kp in keypoints\n    ]\n\n", "idx": 295}
{"namespace": "albumentations.core.keypoints_utils.convert_keypoints_from_albumentations", "completion": "    return [\n        convert_keypoint_from_albumentations(kp, target_format, rows, cols, check_validity, angle_in_degrees)\n        for kp in keypoints\n    ]", "idx": 296}
{"namespace": "albumentations.core.transforms_interface.to_tuple", "completion": "    if low is not None:\n        if isinstance(low, (int, float)):\n            low = (low,)\n        if len(low) == 1:\n            low = (low[0], low[0])\n        if len(low) == 2:\n            low = cast(Tuple[float, float], low)\n        else:\n            raise ValueError(\n                \"Invalid low value. Must be a scalar or a tuple of 2 elements.\"\n            )\n    else:\n        low = (0.0, 0.0)\n\n    if bias is not None:\n        if isinstance(bias, (int, float)):\n            bias = (bias,)\n        if len(bias) == 1:\n            bias = (bias[0], bias[0])\n        if len(bias) == 2:\n            bias = cast(Tuple[float, float], bias)\n        else:\n            raise ValueError(\n                \"Invalid bias value. Must be a scalar or a tuple of 2 elements.\"\n            )\n    else:\n        bias = (0.0, 0.0)\n\n    if isinstance(param, (int, float)):\n        return (param - bias[0], param + bias[1])\n    elif isinstance(param, (list, tuple)):\n        if len(param) == 1:\n            return (param[0] - bias[0], param[0] + bias[1])\n        elif len(param) == 2:\n            return (param[0] - bias[0], param[1] + bias[1])\n        else:\n            raise ValueError(\n                \"Invalid param value. Must be a scalar or a tuple of 2 elements.\"\n            )\n    else:\n        raise ValueError(\n            \"Invalid param value. Must be a scalar or a tuple of 2 elements.\"\n        )\n\n", "idx": 297}
{"namespace": "albumentations.core.composition.ReplayCompose.replay", "completion": "        result = {}\n        for key, value in saved_augmentations.items():\n            if key == \"__class_fullname__\":\n                continue\n            if key == \"id\":\n                continue\n            if key == \"params\":\n                continue\n            if key == \"transforms\":\n                for transform in value:\n                    if transform[\"__class_fullname__\"] == \"ReplayCompose\":\n                        result.update(ReplayCompose.replay(transform, **kwargs))\n                    else:\n                        result.update(instantiate_nonserializable(transform, **kwargs))\n                continue\n            if key == \"applied\":\n                continue\n            if key == \"additional_targets\":\n                continue\n            if key == \"is_check_shapes\":\n                continue\n            if key == \"bbox_params\":\n                continue\n            if key == \"keypoint_params\":\n                continue\n            if key == \"replay_mode\":\n                continue\n            if key == \"applied_in_replay\":\n                continue\n            if key == \"save_key\":\n                continue\n            if key == \"p\":\n                continue\n            if key == \"always_apply\":\n                continue\n            if key == \"check_each_transform\":\n                continue\n            if key == \"add_targets\":\n                continue\n            if key == \"deterministic\":\n                continue\n            if key == \"deterministic_key\":\n                continue\n            if key == \"replay_mode\":\n                continue\n            if key == \"applied_in_replay\":\n                continue\n            if key == \"save_key\":\n                continue\n            if key == \"p\":\n                continue\n            if key == \"always_apply\":\n                continue\n            if key == \"check_each_transform\":\n                continue\n            if key == \"add_targets\":\n                continue\n            if key == \"deterministic\":\n                continue\n            if key == \"deterministic_key\":\n                continue\n            if key == \"replay_mode\":\n                continue\n            if key == \"applied_in_replay\":\n                continue\n            if key == \"save_", "idx": 298}
{"namespace": "albumentations.core.serialization.shorten_class_name", "completion": "    if class_fullname.startswith(\"albumentations.\"):\n        return class_fullname[len(\"albumentations.\") :]\n    return class_fullname\n\n", "idx": 299}
{"namespace": "wandb.util.to_forward_slash_path", "completion": "    if platform.system() == \"Windows\":\n        return path.replace(\"\\\\\", \"/\")\n    else:\n        return path", "idx": 300}
{"namespace": "wandb.util.make_artifact_name_safe", "completion": "    # Replace any characters that are not alphanumeric, dashes, underscores, or dots with underscores\n    cleaned_name = re.sub(r\"[^\\w\\-\\.]\", \"_\", name)\n\n    # If the length of the cleaned name is greater than 128, truncate the name with dots in the middle\n    if len(cleaned_name) > 128:\n        cleaned_name = re.sub(r\"\\.(?=[^.]*$)\", \"_\", cleaned_name[:125]) + \"...\"\n\n    return cleaned_name", "idx": 301}
{"namespace": "wandb.sdk.wandb_settings._redact_dict", "completion": "    return {k: redact_str if k in unsafe_keys else v for k, v in d.items()}", "idx": 302}
{"namespace": "wandb.sdk.launch.builder.build.get_current_python_version", "completion": "    import sys\n\n    py_version = sys.version_info\n    py_version_full = f\"{py_version.major}.{py_version.minor}.{py_version.micro}\"\n    py_version_major = f\"{py_version.major}.{py_version.minor}\"\n    return py_version_full, py_version_major", "idx": 303}
{"namespace": "wandb.sdk.artifacts.storage_policy.StoragePolicy.lookup_by_name", "completion": "        for subclass in cls.__subclasses__():\n            if subclass.__name__ == name:\n                return subclass\n        raise NotImplementedError(f\"No storage policy with name {name}\")\n", "idx": 304}
{"namespace": "wandb.sdk.lib.runid.generate_id", "completion": "    alphabet = string.ascii_lowercase + string.digits\n    return \"\".join(secrets.choice(alphabet) for _ in range(length))", "idx": 305}
{"namespace": "wandb.sdk.internal.file_stream.CRDedupeFilePolicy.get_consecutive_offsets", "completion": "        offsets = sorted(console.keys())\n        if not offsets:\n            return []\n        intervals = []\n        start = offsets[0]\n        for i in range(1, len(offsets)):\n            if offsets[i] - offsets[i - 1] > 1:\n                intervals.append([start, offsets[i - 1]])\n                start = offsets[i]\n        intervals.append([start, offsets[-1]])\n        return intervals\n", "idx": 306}
{"namespace": "wandb.sdk.internal.system.assets.ipu.IPUStats.sample", "completion": "        devices = self._gc_ipu_info.getDevices()\n        for device in devices:\n            device_name = device[\"name\"]\n            metrics = device[\"metrics\"]\n            for key, value in metrics.items():\n                parsed_metric = self.parse_metric(key, value)\n                if parsed_metric is None:\n                    continue\n                key, value = parsed_metric\n                if self._pid is not None and device_name not in self._devices_called:\n                    continue\n                if key in self.variable_metric_keys:\n                    self._devices_called.add(device_name)\n                self.samples.append({\"device\": device_name, key: value})\n", "idx": 307}
{"namespace": "csvkit.cleanup.join_rows", "completion": "    if not rows:\n        raise CSVTestException('No rows to join.')\n\n    if len(rows) == 1:\n        return rows[0]\n\n    if len(rows) == 2:\n        return join_two_rows(rows[0], rows[1], joiner)\n\n    if len(rows) > 2:\n        return join_two_rows(rows[0], join_rows(rows[1:], joiner), joiner)\n\n", "idx": 308}
{"namespace": "csvkit.convert.guess_format", "completion": "    import os\n\n    ext = os.path.splitext(filename)[1]\n    if ext == '.csv':\n        return 'csv'\n    elif ext == '.dbf':\n        return 'dbf'\n    elif ext == '.fixed':\n        return 'fixed'\n    elif ext == '.xls':\n        return 'xls'\n    elif ext == '.xlsx':\n        return 'xlsx'\n    elif ext == '.js':\n        return 'json'\n    else:\n        return None\n\n", "idx": 309}
{"namespace": "folium.utilities.normalize", "completion": "    # Remove non-functional spaces and newlines\n    rendered = rendered.replace(\" \", \"\").replace(\"\\n\", \"\")\n\n    # Remove non-functional spaces and newlines\n    rendered = rendered.replace(\" \", \"\").replace(\"\\n\", \"\")\n\n    # Remove non-functional spaces and newlines\n    rendered = rendered.replace(\" \", \"\").replace(\"\\n\", \"\")\n\n    # Remove non-functional spaces and newlines\n    rendered = rendered.replace(\" \", \"\").replace(\"\\n\", \"\")\n\n    # Remove non-functional spaces and newlines\n    rendered = rendered.replace(\" \", \"\").replace(\"\\n\", \"\")\n\n    # Remove non-functional spaces and newlines\n    rendered = rendered.replace(\" \", \"\").replace(\"\\n\", \"\")\n\n    # Remove non-functional spaces and newlines\n    rendered = rendered.replace(\" \", \"\").replace(\"\\n\", \"\")\n\n    # Remove non-functional spaces and newlines\n    rendered = rendered.replace(\" \", \"\").replace(\"\\n\", \"\")\n\n    # Remove non-functional spaces and newlines\n    rendered = rendered.replace(\" \", \"\").replace(\"\\n\", \"\")\n\n    # Remove non-functional spaces and newlines\n    rendered = rendered.replace(\" \", \"\").replace(\"\\n\", \"\")\n\n    # Remove non-functional spaces and newlines\n    rendered = rendered.replace(\" \", \"\").replace(\"\\n\", \"\")\n\n    # Remove non-functional spaces and newlines\n    rendered = rendered.replace(\" \", \"\").replace(\"\\n\", \"\")\n\n    # Remove non-functional spaces and newlines\n    rendered = rendered.replace(\" \", \"\").replace(\"\\n\", \"\")\n\n    # Remove non-functional spaces and newlines\n    rendered = rendered.replace(\" \", \"\").replace(\"\\n\", \"\")\n\n    # Remove non-functional spaces and newlines\n    rendered = rendered.replace(\" \", \"\").replace(\"\\n\", \"\")\n\n    # Remove non-functional spaces and newlines\n    rendered = rendered.replace(\" \", \"\").replace(\"\\n\", \"\")\n\n    # Remove non-functional spaces and newlines\n    rendered = rendered.replace", "idx": 310}
{"namespace": "tpot.gp_deap.initialize_stats_dict", "completion": "    individual.stats = {'generation': 0, 'mutation_count': 0, 'crossover_count': 0, 'predecessor': None}", "idx": 311}
{"namespace": "bentoml_cli.env_manager.remove_env_arg", "completion": "    new_cmd_args = []\n    for arg in cmd_args:\n        if arg.startswith(\"--env\") or arg.startswith(\"--env=\"):\n            continue\n        new_cmd_args.append(arg)\n    return new_cmd_args\n\n", "idx": 312}
{"namespace": "bentoml._internal.utils.uri.path_to_uri", "completion": "    path = pathlib.Path(path).resolve()\n    if os.name == \"nt\":\n        return \"file:///\" + quote(str(path))\n    else:\n        return \"file://\" + quote(str(path))\n\n", "idx": 313}
{"namespace": "bentoml._internal.utils.uri.uri_to_path", "completion": "    parsed = urlparse(uri)\n    if parsed.scheme not in [\"file\", \"http\", \"https\"]:\n        raise ValueError(\"Unsupported scheme\")\n    if psutil.WINDOWS:\n        return url2pathname(unquote(parsed.path))\n    if psutil.POSIX:\n        return unquote(parsed.path)", "idx": 314}
{"namespace": "bentoml._internal.utils.validate_labels", "completion": "    if not isinstance(labels, dict):\n        raise ValueError(f\"labels must be a dictionary, got {type(labels)}\")\n    if not all(isinstance(k, str) for k in labels.keys()):\n        raise ValueError(\"labels keys must be strings\")\n    if not all(isinstance(v, str) for v in labels.values()):\n        raise ValueError(\"labels values must be strings\")\n\n", "idx": 315}
{"namespace": "bentoml._internal.configuration.helpers.is_valid_ip_address", "completion": "    try:\n        ipaddress.ip_address(addr)\n        return True\n    except ValueError:\n        return False", "idx": 316}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batches_to_batch", "completion": "        # numpy.concatenate may consume lots of memory, need optimization later\n        batch: ext.PdDataFrame = pd.concat(batches, axis=batch_dim)\n        indices = list(\n            itertools.accumulate(subbatch.shape[batch_dim] for subbatch in batches)\n        )\n        indices = [0] + indices\n        return batch, indices\n", "idx": 317}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batch_to_batches", "completion": "        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n        return [batch.iloc[indices[i] : indices[i + 1]] for i in range(len(indices) - 1)]\n", "idx": 318}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batches_to_batch", "completion": "        batch = list(itertools.chain.from_iterable(batches))\n        indices = list(\n            itertools.accumulate(len(subbatch) for subbatch in batches)\n        )\n        indices = [0] + indices\n        return batch, indices\n", "idx": 319}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batch_to_batches", "completion": "        assert (\n            batch_dim == 0\n        ), \"Default Runner DataContainer does not support batch_dim other than 0\"\n        return [batch[indices[i] : indices[i + 1]] for i in range(len(indices) - 1)]\n", "idx": 320}
{"namespace": "jwt.utils.force_bytes", "completion": "    if isinstance(value, str):\n        return value.encode(\"utf-8\")\n    elif isinstance(value, bytes):\n        return value\n    else:\n        raise TypeError(\"Input value must be a string or bytes\")\n\n", "idx": 321}
{"namespace": "pytube.cli.display_progress_bar", "completion": "    if filesize == 0:\n        return\n\n    percent = int(bytes_received / filesize * 100)\n    filled_length = int(scale * percent // 2)\n    bar = ch * filled_length + \" \" * (50 - filled_length)\n    print(f\"\\rProgress: |{bar}| {percent}%\", end=\"\\r\")\n    if percent == 100:\n        print()", "idx": 322}
{"namespace": "pytube.cli._download", "completion": "    filesize = stream.filesize\n    if filesize is None:\n        logger.warning(\"Could not determine filesize. Skipping.\")\n        return\n\n    if target is None:\n        target = os.getcwd()\n\n    if filename is None:\n        filename = stream.default_filename\n\n    if not os.path.exists(target):\n        os.makedirs(target)\n\n    target = os.path.join(target, filename)\n\n    if os.path.exists(target):\n        if os.path.getsize(target) == filesize:\n            logger.info(\"File already exists. Skipping.\")\n            return\n        logger.info(\"File already exists but is incomplete. Re-downloading.\")\n\n    logger.info(f\"Downloading {filename} ({filesize / 1024 / 1024:.2f} MB)\")\n\n    stream.download(\n        target,\n        on_progress_callback=on_progress,\n        skip_existing=True,\n        filename=filename,\n    )", "idx": 323}
{"namespace": "pytube.cli.display_streams", "completion": "    print(\"Available formats:\")\n    for stream in youtube.streams.filter(progressive=True):\n        print(\n            f\"{stream.itag} | {stream.mime_type} | {stream.resolution} | {stream.fps}\"\n        )", "idx": 324}
{"namespace": "pytube.cli._unique_name", "completion": "    if not os.path.exists(target):\n        os.makedirs(target)\n    if media_type == \"audio\":\n        file_format = \"mp3\"\n    else:\n        file_format = subtype\n    file_name = f\"{base}.{file_format}\"\n    file_path = os.path.join(target, file_name)\n    if os.path.exists(file_path):\n        file_name = f\"{base} (1).{file_format}\"\n        file_path = os.path.join(target, file_name)\n        if os.path.exists(file_path):\n            file_name = f\"{base} (2).{file_format}\"\n            file_path = os.path.join(target, file_name)\n            if os.path.exists(file_path):\n                file_name = f\"{base} (3).{file_format}\"\n                file_path = os.path.join(target, file_name)\n                if os.path.exists(file_path):\n                    file_name = f\"{base} (4).{file_format}\"\n                    file_path = os.path.join(target, file_name)\n                    if os.path.exists(file_path):\n                        file_name = f\"{base} (5).{file_format}\"\n                        file_path = os.path.join(target, file_name)\n                        if os.path.exists(file_path):\n                            file_name = f\"{base} (6).{file_format}\"\n                            file_path = os.path.join(target, file_name)\n                            if os.path.exists(file_path):\n                                file_name = f\"{base} (7).{file_format}\"\n                                file_path = os.path.join(target, file_name)\n                                if os.path.exists(file_path):\n                                    file_name = f\"{base} (8).{file_format}\"\n                                    file_path = os.path.join(target, file_name)\n                                    if os.path.exists(file_path", "idx": 325}
{"namespace": "pytube.cli._print_available_captions", "completion": "    for caption in captions:\n        print(caption.code)", "idx": 326}
{"namespace": "pytube.cipher.throttling_reverse", "completion": "    arr.reverse()", "idx": 327}
{"namespace": "pytube.helpers.setup_logger", "completion": "    logger = logging.getLogger()\n    logger.setLevel(level)\n    formatter = logging.Formatter(\n        \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n    )\n    stream_handler = logging.StreamHandler()\n    stream_handler.setFormatter(formatter)\n    logger.addHandler(stream_handler)\n    if log_filename:\n        file_handler = logging.FileHandler(log_filename)\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n\n", "idx": 328}
{"namespace": "pytube.helpers.deprecated", "completion": "    def decorator(func1: Callable[..., GenericType]) -> GenericType:\n        if isinstance(reason, Exception):\n            raise reason\n\n        @functools.wraps(func1)\n        def wrapper(*args: Any, **kwargs: Any) -> GenericType:\n            warnings.simplefilter('always', DeprecationWarning)\n            warnings.warn(\n                \"Call to deprecated function {}. ({})\".format(func1.__name__, reason),\n                category=DeprecationWarning,\n                stacklevel=2\n            )\n            warnings.simplefilter('default', DeprecationWarning)\n            return func1(*args, **kwargs)\n        return wrapper\n    return decorator", "idx": 329}
{"namespace": "pytube.helpers.uniqueify", "completion": "    return list(dict.fromkeys(duped_list))", "idx": 330}
{"namespace": "pytube.helpers.target_directory", "completion": "    if output_path is None:\n        return os.getcwd()\n    elif os.path.isabs(output_path):\n        return output_path\n    else:\n        return os.path.join(os.getcwd(), output_path)", "idx": 331}
{"namespace": "pytube.extract.is_private", "completion": "    private_strings = [\n        'This video is private.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This live stream recording is not available.',\n        'This", "idx": 332}
{"namespace": "pymc.math.cartesian", "completion": "    arrays = [np.asarray(a) for a in arrays]\n    shape = [a.shape[0] for a in arrays]\n    ndim = len(shape)\n    dtype = arrays[0].dtype\n    out = np.empty(shape, dtype=dtype)\n    for i in range(ndim):\n        out[..., i] = arrays[i].flatten()\n    for i in range(1, ndim):\n        out = out.reshape(shape[i] * shape[:i].prod(), -1)\n    return out\n\n", "idx": 333}
{"namespace": "pymc.math.log1mexp", "completion": "    if negative_input:\n        return -x - pt.log1p(pt.exp(-x))\n    else:\n        return pt.log1p(-pt.exp(x))", "idx": 334}
{"namespace": "pymc.math.log1mexp_numpy", "completion": "    if not negative_input:\n        warnings.warn(\n            \"pymc.math.log1mexp_numpy will expect a negative input in a future \"\n            \"version of PyMC.\\n To suppress this warning set `negative_input=True`\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        x = -x\n\n    if x >= 0:\n        return np.log(1 - np.exp(-x))\n    else:\n        return np.log1p(-np.exp(x))", "idx": 335}
{"namespace": "pymc.util.drop_warning_stat", "completion": "    # Create a new InferenceData object with the same groups as the input object\n    new_idata = arviz.InferenceData()\n    for group_name, group_data in idata.groups.items():\n        # Create a new group with the same data as the input group, except for the \"warning\" stat\n        new_group_data = {\n            stat_name: stat_data\n            for stat_name, stat_data in group_data.items()\n            if stat_name != \"warning\"\n        }\n        # Add the new group to the new InferenceData object\n        new_idata.add_groups(**{group_name: new_group_data})\n    # Return the new InferenceData object\n    return new_idata", "idx": 336}
{"namespace": "pymc.pytensorf.walk_model", "completion": "    if stop_at_vars is None:\n        stop_at_vars = set()\n    visited = set()\n    for graph in graphs:\n        for node in walk(graph, stop_at_vars=stop_at_vars, expand_fn=expand_fn):\n            if node not in visited:\n                visited.add(node)\n                yield node\n\n", "idx": 337}
{"namespace": "pymc.testing.select_by_precision", "completion": "    if pytensor.config.floatX == \"float64\":\n        return float64\n    elif pytensor.config.floatX == \"float32\":\n        return float32\n    else:\n        raise ValueError(\"floatX must be either float32 or float64\")\n\n", "idx": 338}
{"namespace": "pymc.gp.cov.handle_args", "completion": "    def new_func(X, args=None):\n        if args is None:\n            return func(X)\n        else:\n            return func(X, *args)\n\n    return new_func", "idx": 339}
{"namespace": "pymc.gp.util.kmeans_inducing_points", "completion": "    fu, _ = kmeans(X, n_inducing, **kmeans_kwargs)\n    return fu", "idx": 340}
{"namespace": "pymc.pytensorf.floatX", "completion": "    if isinstance(X, np.ndarray):\n        return X.astype(pytensor.config.floatX)\n    elif isinstance(X, TensorVariable):\n        return X.astype(pytensor.config.floatX)\n    elif isinstance(X, TensorConstant):\n        return X.astype(pytensor.config.floatX)\n    elif isinstance(X, SharedVariable):\n        return X.astype(pytensor.config.floatX)\n    elif isinstance(X, RandomVariable):\n        return X.astype(pytensor.config.floatX)\n    elif isinstance(X, RandomGeneratorSharedVariable):\n        return X.astype(pytensor.config.floatX)\n    elif isinstance(X, RandomStateSharedVariable):\n        return X.astype(pytensor.config.floatX)\n    elif isinstance(X, TensorSharedVariable):\n        return X.astype(pytensor.config.floatX)\n    elif isinstance(X, AdvancedIncSubtensor):\n        return X.astype(pytensor.config.floatX)\n    elif isinstance(X, AdvancedIncSubtensor1):\n        return X.astype(pytensor.config.floatX)\n    elif isinstance(X, Scan):\n        return X.astype(pytensor.config.floatX)\n    elif isinstance(X, Elemwise):\n        return X.astype(pytensor.config.floatX)\n    elif isinstance(X, Apply):\n        return X.astype(pytensor.config.floatX)\n    elif isinstance(X, Constant):\n        return X.astype(pytensor.config.floatX)\n    elif isinstance(X, Variable):\n        return X.astype(pytensor.config.floatX)\n    elif isinstance(X, ShapeFeature):\n        return X.astype(pytensor.config.floatX)\n    elif isinstance(X, CheckParameterValue):\n        return X.astype(pytensor.config.floatX)\n    elif isinstance(X, Cast):\n        return X.astype(pytensor.config.", "idx": 341}
{"namespace": "pymc.distributions.multivariate.posdef", "completion": "    try:\n        chol = cholesky(AA)\n        return True\n    except ValueError:\n        return False\n\n", "idx": 342}
{"namespace": "pymc.distributions.dist_math.multigammaln", "completion": "    if p <= 0:\n        raise ValueError(\"p should be greater than 0.\")\n    if p == 1:\n        return gammaln(a)\n    else:\n        return gammaln(a) + (p - 1) * np.log(a)\n\n", "idx": 343}
{"namespace": "pymc.distributions.dist_math.incomplete_beta", "completion": "    return pt.betainc(a, b, value)\n\n", "idx": 344}
{"namespace": "pymc.sampling.forward.observed_dependent_deterministics", "completion": "    deterministics = model.deterministics\n    observed_rvs = model.observed_RVs\n    basic_rvs = model.basic_RVs\n\n    # Find deterministics that depend directly on observed variables\n    observed_dependent_deterministics = []\n    for deterministic in deterministics:\n        if deterministic.owner is None:\n            continue\n        if deterministic.owner.inputs[0] in observed_rvs:\n            observed_dependent_deterministics.append(deterministic)\n\n    return observed_dependent_deterministics", "idx": 345}
{"namespace": "pymc.smc.kernels.systematic_resampling", "completion": "    normalized_weights = weights / np.sum(weights)\n    new_indices = np.zeros(len(normalized_weights), dtype=int)\n    cumulative_sum = np.cumsum(normalized_weights)\n    r = rng.random() / len(normalized_weights)\n    i = 0\n    for j in range(len(normalized_weights)):\n        while cumulative_sum[i] < r:\n            i += 1\n        new_indices[j] = i\n        r += 1 / len(normalized_weights)\n    return new_indices", "idx": 346}
{"namespace": "pymc.backends.base._squeeze_cat", "completion": "    if combine:\n        results = np.concatenate(results)\n    if squeeze:\n        results = [results]\n    return results", "idx": 347}
{"namespace": "pymc.logprob.transforms.SimplexTransform.forward", "completion": "        log_value = pt.log(value)\n        log_sum = pt.sum(log_value)\n        return log_value - log_sum\n", "idx": 348}
{"namespace": "pymc.logprob.transforms.SimplexTransform.backward", "completion": "        value = pt.as_tensor(value)\n        N = value.shape[-1].astype(value.dtype)\n        shift = pt.sum(value, -1, keepdims=True) / N\n        return pt.exp(value + shift)\n", "idx": 349}
{"namespace": "pymc.logprob.utils.walk_model", "completion": "    if stop_at_vars is None:\n        stop_at_vars = set()\n\n    def _walk(graph: TensorVariable) -> Generator[TensorVariable, None, None]:\n        for node in walk(graph):\n            if isinstance(node, MeasurableVariable):\n                if not walk_past_rvs:\n                    continue\n            if node in stop_at_vars:\n                return\n            yield node\n            yield from _walk(node)\n\n    for graph in graphs:\n        yield from _walk(graph)\n\n", "idx": 350}
{"namespace": "sacred.metrics_logger.linearize_metrics", "completion": "    # Group metrics by name\n    grouped_metrics = {}\n    for metric in logged_metrics:\n        if metric.name not in grouped_metrics:\n            grouped_metrics[metric.name] = {\n                \"steps\": [],\n                \"values\": [],\n                \"timestamps\": [],\n            }\n        grouped_metrics[metric.name][\"steps\"].append(metric.step)\n        grouped_metrics[metric.name][\"values\"].append(metric.value)\n        grouped_metrics[metric.name][\"timestamps\"].append(metric.timestamp)\n\n    return grouped_metrics", "idx": 351}
{"namespace": "sacred.utils.set_by_dotted_path", "completion": "    if \".\" not in path:\n        d[path] = value\n        return\n    head, tail = path.split(\".\", 1)\n    if head not in d:\n        d[head] = {}\n    set_by_dotted_path(d[head], tail, value)", "idx": 352}
{"namespace": "sacred.utils.get_by_dotted_path", "completion": "    split_path = path.split(\".\")\n    current_option = d\n    for p in split_path:\n        if p not in current_option:\n            return default\n        current_option = current_option[p]\n    return current_option", "idx": 353}
{"namespace": "pymc.logprob.scan.construct_scan", "completion": "    # Create a scan operation based on the given scan arguments and additional keyword arguments\n    scan_op = Scan(\n        inner_out_seqs=scan_args.inner_out_seqs,\n        inner_out_mit_sot=scan_args.inner_out_mit_sot,\n        inner_out_sit_sot=scan_args.inner_out_sit_sot,\n        inner_out_nit_sot=scan_args.inner_out_nit_sot,\n        outer_in_seqs=scan_args.outer_in_seqs,\n        outer_in_mit_sot=scan_args.outer_in_mit_sot,\n        outer_in_sit_sot=scan_args.outer_in_sit_sot,\n        outer_in_nit_sot=scan_args.outer_in_nit_sot,\n        n_steps=scan_args.n_steps,\n        **kwargs,\n    )\n\n    # Create a node based on the input and output variables of the scan operation\n    node = scan_op.make_node(\n        *scan_args.inner_in_seqs,\n        *scan_args.outer_in_seqs,\n        *scan_args.inner_out_seqs,\n        *scan_args.outer_outputs,\n    )\n\n    # Return the node outputs and updates\n    return node.outputs, node.updates\n\n", "idx": 354}
{"namespace": "sacred.utils.is_prefix", "completion": "    split_pre_path = pre_path.split(\".\")\n    split_path = path.split(\".\")\n    if len(split_pre_path) > len(split_path):\n        return False\n    for i in range(len(split_pre_path)):\n        if split_pre_path[i] != split_path[i]:\n            return False\n    return True", "idx": 355}
{"namespace": "sacred.utils.get_inheritors", "completion": "    subclasses = set()\n    for subclass in cls.__subclasses__():\n        subclasses.add(subclass)\n        subclasses.update(get_inheritors(subclass))\n    return subclasses", "idx": 356}
{"namespace": "sacred.utils.convert_camel_case_to_snake_case", "completion": "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()", "idx": 357}
{"namespace": "sacred.utils.module_exists", "completion": "    try:\n        pkgutil.find_loader(modname)\n        return True\n    except ImportError:\n        return False", "idx": 358}
{"namespace": "sacred.utils.apply_backspaces_and_linefeeds", "completion": "    lines = text.split(\"\\n\")\n    for i in range(len(lines)):\n        lines[i] = lines[i].replace(\"\\b\", \"\")\n        lines[i] = lines[i].replace(\"\\r\", \"\")\n        lines[i] = lines[i].replace(\"\\t\", \"    \")\n        lines[i] = lines[i].replace(\"\\f\", \"\")\n        lines[i] = lines[i].replace(\"\\v\", \"\")\n        lines[i] = lines[i].replace(\"\\a\", \"\")\n        lines[i] = lines[i].replace(\"\\0\", \"\")\n        lines[i] = lines[i].replace(\"\\x0b\", \"\")\n        lines[i] = lines[i].replace(\"\\x0c\", \"\")\n        lines[i] = lines[i].replace(\"\\x0e\", \"\")\n        lines[i] = lines[i].replace(\"\\x0f\", \"\")\n        lines[i] = lines[i].replace(\"\\x1b\", \"\")\n        lines[i] = lines[i].replace(\"\\x7f\", \"\")\n        lines[i] = lines[i].replace(\"\\x80\", \"\")\n        lines[i] = lines[i].replace(\"\\x81\", \"\")\n        lines[i] = lines[i].replace(\"\\x82\", \"\")\n        lines[i] = lines[i].replace(\"\\x83\", \"\")\n        lines[i] = lines[i].replace(\"\\x84\", \"\")\n        lines[i] = lines[i].replace(\"\\x85\", \"\")\n        lines[i] = lines[i].replace(\"\\x86\", \"\")\n        lines[i] = lines[i].replace(\"\\x87\", \"\")\n        lines[i] = lines[i].replace(\"\\x88\", \"\")\n        lines[i] = lines[i].replace(\"\\x89\", \"\")\n        lines[i] = lines[i].replace(\"\\x8a\", \"\")\n        lines[i] = lines[i].replace(\"\\x8b\", \"\")\n        lines[i] = lines[i].replace(\"\\x8c", "idx": 359}
{"namespace": "sacred.commands.help_for_command", "completion": "    help_text = pydoc.render_doc(command)\n    help_text = re.sub(r\"\\x08\", \"\", help_text)\n    return help_text", "idx": 360}
{"namespace": "sacred.optional.optional_import", "completion": "    for package_name in package_names:\n        if modules_exist(package_name):\n            return True, importlib.import_module(package_name)\n    return False, None\n\n", "idx": 361}
{"namespace": "sacred.dependencies.get_py_file_if_possible", "completion": "    if pyc_name.endswith(\".py\"):\n        return pyc_name\n    elif pyc_name.endswith(\".so\") or pyc_name.endswith(\".pyd\") or pyc_name.endswith(\n        \".ipynb\"\n    ):\n        return pyc_name\n    else:\n        py_name = pyc_name[:-1]\n        if os.path.exists(py_name):\n            return py_name\n        else:\n            return pyc_name\n\n", "idx": 362}
{"namespace": "sacred.config.custom_containers.DogmaticDict.update", "completion": "        if iterable is not None:\n            if hasattr(iterable, \"keys\"):\n                for k, v in iterable.items():\n                    self[k] = v\n            else:\n                for k, v in iterable:\n                    self[k] = v\n        for k, v in kwargs.items():\n            self[k] = v", "idx": 363}
{"namespace": "sacred.config.config_scope.is_empty_or_comment", "completion": "    line = line.strip()\n    return not line or line.startswith(\"#\")", "idx": 364}
{"namespace": "boltons.funcutils.copy_function", "completion": "    if not isinstance(orig, FunctionType):\n        raise TypeError('orig must be a function, not %s' % type(orig))\n    if not _IS_PY35:\n        # Python 3.5+ has a copy_function() method on FunctionType\n        # objects, but we're not there yet.\n        # TODO: use copy.copy() instead?\n        # TODO: use copy.copy() instead?\n        # TODO: use copy.copy() instead?\n        # TODO: use copy.copy() instead?\n        # TODO: use copy.copy() instead?\n        # TODO: use copy.copy() instead?\n        # TODO: use copy.copy() instead?\n        # TODO: use copy.copy() instead?\n        # TODO: use copy.copy() instead?\n        # TODO: use copy.copy() instead?\n        # TODO: use copy.copy() instead?\n        # TODO: use copy.copy() instead?\n        # TODO: use copy.copy() instead?\n        # TODO: use copy.copy() instead?\n        # TODO: use copy.copy() instead?\n        # TODO: use copy.copy() instead?\n        # TODO: use copy.copy() instead?\n        # TODO: use copy.copy() instead?\n        # TODO: use copy.copy() instead?\n        # TODO: use copy.copy() instead?\n        # TODO: use copy.copy() instead?\n        # TODO: use copy.copy() instead?\n        # TODO: use copy.copy() instead?\n        # TODO: use copy.copy() instead?\n        # TODO: use copy.copy() instead?\n        # TODO: use copy.copy() instead?\n        # TODO: use copy.copy() instead?\n        # TODO: use copy.copy() instead?\n        # TODO: use copy.copy() instead?\n        # TODO: use copy.copy() instead?\n        # TODO: use copy.copy() instead?\n        # TODO: use copy.copy() instead?\n        # TODO: use copy.copy() instead?\n        # TODO: use copy.copy() instead?\n        # TODO: use copy.copy() instead?\n        # TODO", "idx": 365}
{"namespace": "sacred.config.config_scope.dedent_line", "completion": "    if line.startswith(indent):\n        return line[len(indent) :]\n    else:\n        return line\n\n", "idx": 366}
{"namespace": "boltons.funcutils.format_invocation", "completion": "    if kwargs is None:\n        kwargs = {}\n    if args:\n        args = ', '.join(map(repr, args))\n    else:\n        args = ''\n    if kwargs:\n        kwargs = ', '.join('%s=%r' % (k, v) for k, v in kwargs.items())\n    else:\n        kwargs = ''\n    if args and kwargs:\n        args = ', ' + args\n    if kwargs and kw:\n        kwargs = ', ' + kwargs\n    if kw:\n        kwargs = ', ' + ', '.join('%s=%r' % (k, v) for k, v in kw.items())\n    return '%s(%s%s%s)' % (name, args, kwargs, kw)", "idx": 367}
{"namespace": "boltons.listutils.SplayList.shift", "completion": "        if item_index < 0:\n            item_index += len(self)\n        if dest_index < 0:\n            dest_index += len(self)\n        if item_index == dest_index:\n            return\n        if item_index < 0 or item_index >= len(self):\n            raise IndexError('item index out of range')\n        if dest_index < 0 or dest_index >= len(self):\n            raise IndexError('destination index out of range')\n        if item_index < dest_index:\n            self.insert(dest_index, self.pop(item_index))\n        else:\n            self.insert(dest_index, self.pop(item_index - 1))\n", "idx": 368}
{"namespace": "boltons.strutils.gzip_bytes", "completion": "    return zlib.compress(bytestring, level)", "idx": 369}
{"namespace": "boltons.strutils.is_uuid", "completion": "    if isinstance(obj, uuid.UUID):\n        return obj.version == version\n    elif isinstance(obj, basestring):\n        try:\n            uuid.UUID(obj)\n            return True\n        except ValueError:\n            return False\n    else:\n        return False", "idx": 370}
{"namespace": "boltons.strutils.parse_int_list", "completion": "    # Split the range string into a list of integers and ranges\n    range_list = range_string.split(delim)\n\n    # Initialize an empty list to store the final list of integers\n    final_list = []\n\n    # Iterate over each element in the range list\n    for element in range_list:\n        # Check if the element is a range of integers\n        if range_delim in element:\n            # Split the range into start and end integers\n            start, end = map(int, element.split(range_delim))\n            # Add the integers in the range to the final list\n            final_list.extend(range(start, end + 1))\n        else:\n            # If the element is not a range, add it to the final list as an integer\n            final_list.append(int(element))\n\n    # Sort the final list of integers\n    final_list.sort()\n\n    # Return the final list of integers\n    return final_list", "idx": 371}
{"namespace": "boltons.cacheutils.ThresholdCounter.get", "completion": "        return self._count_map.get(key, [default, 0])[0]\n", "idx": 372}
{"namespace": "boltons.iterutils.backoff_iter", "completion": "    if count == 'repeat':\n        count = None\n    if count is not None:\n        count = int(count)\n    if start <= 0 or stop <= 0:\n        raise ValueError('start and stop must be positive')\n    if factor <= 1.0:\n        raise ValueError('factor must be greater than 1.0')\n    if jitter:\n        if jitter is True:\n            jitter = 1.0\n        elif jitter < -1.0 or jitter > 1.0:\n            raise ValueError('jitter must be between -1.0 and 1.0')\n        jitter_factor = (1.0 - jitter) / 2.0\n        jitter_range = jitter * jitter_factor\n    else:\n        jitter_factor = jitter_range = 0.0\n    if count is None:\n        count = int(math.ceil(math.log(stop / start, factor)))\n    if count <= 0:\n        return\n    cur = start\n    yield cur\n    for i in xrange(1, count):\n        cur *= factor\n        if jitter_range:\n            cur += random.uniform(jitter_factor, jitter_factor + jitter_range)\n        yield cur\n        if cur >= stop:\n            break\n    return", "idx": 373}
{"namespace": "boltons.cacheutils.cached", "completion": "    def decorator(func):\n        return CachedFunction(func, cache, scoped=scoped, typed=typed, key=key)\n    return decorator", "idx": 374}
{"namespace": "boltons.timeutils.total_seconds", "completion": "    return (td.microseconds + (td.seconds + td.days * 24 * 3600) * 10**6) / 10**6\n\n", "idx": 375}
{"namespace": "boltons.gcutils.get_all", "completion": "    if include_subtypes:\n        return [obj for obj in gc.get_objects() if isinstance(obj, type_obj)]\n    else:\n        return [obj for obj in gc.get_objects() if type(obj) is type_obj]", "idx": 376}
{"namespace": "boltons.timeutils.daterange", "completion": "    if isinstance(step, timedelta):\n        step = total_seconds(step)\n    elif isinstance(step, tuple):\n        step = timedelta(*step).total_seconds()\n    if isinstance(start, datetime):\n        start = start.date()\n    if isinstance(stop, datetime):\n        stop = stop.date()\n    if step < 0:\n        start, stop = stop, start\n        step = -step\n    if stop is None:\n        while True:\n            yield start\n            start += timedelta(seconds=step)\n    else:\n        if inclusive:\n            stop += timedelta(seconds=step)\n        while start < stop:\n            yield start\n            start += timedelta(seconds=step)", "idx": 377}
{"namespace": "boltons.mathutils.clamp", "completion": "    return max(lower, min(x, upper))\n\n", "idx": 378}
{"namespace": "boltons.mathutils.ceil", "completion": "    if options is None:\n        return _ceil(x)\n    else:\n        return options[bisect.bisect_right(options, x) - 1]", "idx": 379}
{"namespace": "boltons.formatutils.get_format_args", "completion": "    pos_args, named_args = [], []\n    for lit, field_str in split_format_str(fstr):\n        if field_str is None:\n            continue\n        fname, fspec, conv = field_str[1:-1].partition(':')\n        if conv:\n            conv = conv[1:]\n        if fname.startswith('!'):\n            fname = fname[1:]\n        if fname.startswith('.'):\n            fname = fname[1:]\n        if fname.startswith('['):\n            fname = fname[1:]\n        if fname.startswith('*'):\n            fname = fname[1:]\n        if fname.startswith('$'):\n            fname = fname[1:]\n        if fname.startswith('@'):\n            fname = fname[1:]\n        if fname.startswith('!'):\n            fname = fname[1:]\n        if fname.startswith('.'):\n            fname = fname[1:]\n        if fname.startswith('['):\n            fname = fname[1:]\n        if fname.startswith('*'):\n            fname = fname[1:]\n        if fname.startswith('$'):\n            fname = fname[1:]\n        if fname.startswith('@'):\n            fname = fname[1:]\n        if fname.startswith('!'):\n            fname = fname[1:]\n        if fname.startswith('.'):\n            fname = fname[1:]\n        if fname.startswith('['):\n            fname = fname[1:]\n        if fname.startswith('*'):\n            fname = fname[1:]\n        if fname.startswith('$'):\n            fname = fname[1:]\n        if fname.startswith('@'):\n            fname = fname[1:]\n        if fname.startswith('!'):\n            fname = f", "idx": 380}
{"namespace": "boltons.mathutils.floor", "completion": "    if options is None:\n        return _floor(x)\n    options = sorted(options)\n    i = bisect.bisect_right(options, x)\n    if i == 0:\n        raise ValueError(\"no floor options less than or equal to: %r\" % x)\n    return options[i - 1]", "idx": 381}
{"namespace": "boltons.dictutils.OneToOne.setdefault", "completion": "        if key not in self:\n            self[key] = default\n        return self[key]", "idx": 382}
{"namespace": "boltons.dictutils.OneToOne.update", "completion": "        if isinstance(dict_or_iterable, dict):\n            for key, value in dict_or_iterable.items():\n                self[key] = value\n        else:\n            for key, value in dict_or_iterable:\n                self[key] = value\n        for key, value in kw.items():\n            self[key] = value\n", "idx": 383}
{"namespace": "boltons.dictutils.ManyToMany.get", "completion": "        return self.data.get(key, default)\n", "idx": 384}
{"namespace": "boltons.dictutils.FrozenDict.updated", "completion": "        if a:\n            if a[0] is _OTO_INV_MARKER:\n                self.inv = a[1]\n                dict.__init__(self, [(v, k) for k, v in self.inv.items()])\n                return\n            elif a[0] is _OTO_UNIQUE_MARKER:\n                a, raise_on_dupe = a[1:], True\n\n        dict.__init__(self, *a, **kw)\n        self.inv = self.__class__(_OTO_INV_MARKER, self)\n\n        if len(self) == len(self.inv):\n            # if lengths match, that means everything's unique\n            return\n\n        if not raise_on_dupe:\n            dict.clear(self)\n            dict.update(self, [(v, k) for k, v in self.inv.items()])\n            return\n\n        # generate an error message if the values aren't 1:1\n\n        val_multidict = {}\n        for k, v in self.items():\n            val_multidict.setdefault(v, []).append(k)\n\n        dupes = dict([(v, k_list) for v, k_list in\n                      val_multidict.items() if len(k_list) > 1])\n\n        raise ValueError('expected unique values, got multiple keys for'\n                         ' the following values: %r' % dupes)\n", "idx": 385}
{"namespace": "boltons.dictutils.subdict", "completion": "    if keep is None:\n        keep = d.keys()\n    if drop is None:\n        drop = []\n    return dict((k, d[k]) for k in keep if k not in drop)", "idx": 386}
{"namespace": "boltons.dictutils.FrozenDict.__repr__", "completion": "        cn = self.__class__.__name__\n        dict_repr = dict.__repr__(self)\n        return \"%s(%s)\" % (cn, dict_repr)\n", "idx": 387}
{"namespace": "gunicorn.config.validate_callable", "completion": "    def decorator(val):\n        if isinstance(val, str):\n            try:\n                val = util.load_class(val)\n            except ImportError:\n                raise ValueError(\"Invalid callable: %s\" % val)\n        if not callable(val):\n            raise TypeError(\"Invalid callable: %s\" % val)\n        if arity != -1 and inspect.getargspec(val).varargs is not None:\n            raise TypeError(\"Invalid callable: %s\" % val)\n        return val\n\n    return decorator\n\n", "idx": 388}
{"namespace": "gunicorn.config.get_default_config_file", "completion": "    config_file = os.path.join(os.getcwd(), 'gunicorn.conf.py')\n    if os.path.exists(config_file):\n        return config_file\n\n", "idx": 389}
{"namespace": "gunicorn.util.is_ipv6", "completion": "    try:\n        socket.inet_pton(socket.AF_INET6, addr)\n    except socket.error:\n        return False\n    return True", "idx": 390}
{"namespace": "gunicorn.systemd.listen_fds", "completion": "    if os.environ.get(\"LISTEN_PID\") != str(os.getpid()):\n        return 0\n\n    fds = int(os.environ.get(\"LISTEN_FDS\", 0))\n\n    if unset_environment:\n        for key in (\"LISTEN_PID\", \"LISTEN_FDS\"):\n            os.environ.pop(key, None)\n\n    return fds", "idx": 391}
{"namespace": "gunicorn.util.http_date", "completion": "    if timestamp is None:\n        timestamp = time.time()\n\n    return email.utils.formatdate(timestamp, usegmt=True)", "idx": 392}
{"namespace": "gunicorn.util.parse_address", "completion": "    if netloc.startswith('unix:'):\n        return netloc[5:], None\n    elif netloc.startswith('fd://'):\n        return netloc[5:], None\n    else:\n        host, port = netloc.split(':')\n        if not port:\n            port = default_port\n        return host, port", "idx": 393}
{"namespace": "gunicorn.util.to_bytestring", "completion": "    if isinstance(value, bytes):\n        return value\n    elif isinstance(value, str):\n        return value.encode(encoding)\n    else:\n        raise TypeError(\"to_bytestring() argument must be str or bytes, not %r\" % type(value))", "idx": 394}
{"namespace": "gunicorn.util.warn", "completion": "    print(\"WARNING: %s\" % msg, file=sys.stderr)", "idx": 395}
{"namespace": "gunicorn.util.split_request_uri", "completion": "    if uri.startswith(\"//\"):\n        uri = \".\" + uri\n\n    scheme, netloc, path, query, fragment = urllib.parse.urlsplit(uri)\n\n    if scheme == \"\":\n        scheme = \"http\"\n\n    if netloc == \"\":\n        netloc = \"localhost\"\n\n    return scheme, netloc, path, query, fragment", "idx": 396}
{"namespace": "praw.models.listing.listing.ModNoteListing.after", "completion": "        if self.has_next_page:\n            return self.end_cursor\n        return None\n\n", "idx": 397}
{"namespace": "praw.models.util.permissions_string", "completion": "    if permissions is None:\n        return \"+all\"\n    if not permissions:\n        return \"\"\n    if all(p.startswith(\"+\") or p.startswith(\"-\") for p in permissions):\n        return \",\".join(permissions)\n    return \",\".join(\n        f\"+{p}\" if p not in known_permissions else f\"-{p}\" for p in permissions\n    )\n\n", "idx": 398}
{"namespace": "jc.cli.JcCli.json_out", "completion": "        if self.mono:\n            return utils.json_dumps(\n                self.data_out,\n                indent=self.json_indent,\n                separators=self.json_separators,\n                ascii=self.ascii_only\n            )\n\n        class JcStyle(Style):\n            styles: CustomColorType = self.custom_colors\n\n        return str(highlight(\n            utils.json_dumps(\n                self.data_out,\n                indent=self.json_indent,\n                separators=self.json_separators,\n                ascii=self.ascii_only\n            ),\n            JsonLexer(),\n            Terminal256Formatter(style=JcStyle)\n        ))", "idx": 399}
{"namespace": "pythonforandroid.pythonpackage.transform_dep_for_pip", "completion": "    if \"@\" in dependency and \"://\" in dependency:\n        dependency = dependency.split(\"@\")[0]\n        dependency = dependency.split(\"://\")[1]\n        dependency = \"https://\" + dependency\n    return dependency", "idx": 400}
{"namespace": "pythonforandroid.graph.fix_deplist", "completion": "    deps = [dep.lower() if isinstance(dep, str) else dep for dep in deps]\n    deps = [(dep,) if isinstance(dep, str) else dep for dep in deps]\n    return deps\n\n", "idx": 401}
{"namespace": "pythonforandroid.util.walk_valid_filens", "completion": "    for dirpath, dirnames, filenames in walk(base_dir):\n        # Ignore the directories that match the invalid directory names\n        dirnames[:] = [d for d in dirnames if d not in invalid_dir_names]\n        # Yield the full path of the valid files\n        for filename in filenames:\n            file_path = join(dirpath, filename)\n            if not any(fnmatch(file_path, pattern) for pattern in invalid_file_patterns):\n                yield file_path", "idx": 402}
{"namespace": "pythonforandroid.bootstrap._cmp_bootstraps_by_priority", "completion": "    a_priority = default_recipe_priorities.index(a.name)\n    b_priority = default_recipe_priorities.index(b.name)\n    if a_priority == b_priority:\n        return cmp(a.name, b.name)\n    else:\n        return cmp(a_priority, b_priority)", "idx": 403}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.all_bootstraps", "completion": "        bootstraps_dir = join(cls.ctx.root_dir, 'bootstraps')\n        bootstraps = set()\n        for file in listdir(bootstraps_dir):\n            if isdir(join(bootstraps_dir, file)):\n                bootstraps.add(file)\n        return bootstraps\n", "idx": 404}
{"namespace": "mmcv.image.colorspace._convert_input_type_range", "completion": "    img_type = img.dtype\n    img = img.astype(np.float32)\n    if img_type == np.float32:\n        pass\n    elif img_type == np.uint8:\n        img /= 255.\n    else:\n        raise TypeError('The img type should be np.float32 or np.uint8, '\n                        f'but got {img_type}')\n    return img", "idx": 405}
{"namespace": "mackup.utils.error", "completion": "    sys.stderr.write(message + \"\\n\")\n    sys.exit(1)", "idx": 406}
{"namespace": "mmcv.image.colorspace._convert_output_type_range", "completion": "    if dst_type not in (np.uint8, np.float32):\n        raise TypeError('The dst_type should be np.uint8 or np.float32, '\n                        f'but got {dst_type}')\n    if dst_type == np.uint8:\n        img = img.round()\n    else:\n        img /= 255.\n    return img", "idx": 407}
{"namespace": "mackup.utils.is_process_running", "completion": "    try:\n        subprocess.check_output([\"pgrep\", process_name])\n        return True\n    except subprocess.CalledProcessError:\n        return False", "idx": 408}
{"namespace": "stellar.operations._get_pid_column", "completion": "    server_version = raw_conn.engine.dialect.server_version_info\n    if server_version[0] == 10:\n        return 'pid'\n    elif server_version[0] == 9:\n        return 'procpid'\n    elif server_version[0] == 8:\n        return 'procpid'\n    elif server_version[0] == 7:\n        return 'procpid'\n    elif server_version[0] == 6:\n        return 'procpid'\n    elif server_version[0] == 5:\n        return 'procpid'\n    elif server_version[0] == 4:\n        return 'procpid'\n    elif server_version[0] == 3:\n        return 'procpid'\n    elif server_version[0] == 2:\n        return 'procpid'\n    elif server_version[0] == 1:\n        return 'procpid'\n    else:\n        return 'procpid'\n\n", "idx": 409}
{"namespace": "imapclient.imap_utf7.encode", "completion": "    if isinstance(s, str):\n        s = s.encode(\"utf-8\")\n    if not isinstance(s, bytes):\n        return s\n\n    def encode_chunk(chunk: bytes) -> bytes:\n        if len(chunk) == 0:\n            return b\"\"\n        if len(chunk) == 1:\n            return b\"+\" + chunk\n        if len(chunk) == 2:\n            return b\"+\" + chunk + b\"-\"\n        if len(chunk) == 3:\n            return b\"+\" + chunk + b\"=\"\n        if len(chunk) == 4:\n            return b\"+\" + chunk + b\"==\"\n        if len(chunk) == 5:\n            return b\"+\" + chunk + b\"=-\"\n        if len(chunk) == 6:\n            return b\"+\" + chunk + b\"==-\"\n        if len(chunk) == 7:\n            return b\"+\" + chunk + b\"===\"\n        if len(chunk) == 8:\n            return b\"+\" + chunk + b\"====\"\n        if len(chunk) == 9:\n            return b\"+\" + chunk + b\"=-=\"\n        if len(chunk) == 10:\n            return b\"+\" + chunk + b\"==-=\"\n        if len(chunk) == 11:\n            return b\"+\" + chunk + b\"===-\"\n        if len(chunk) == 12:\n            return b\"+\" + chunk + b\"====-\"\n        if len(chunk) == 13:\n            return b\"+\" + chunk + b\"=====\"\n        if len(chunk) == 14:\n            return b\"+\" + chunk + b\"=====-\"\n        if len(chunk) == 15:\n            return b\"+\" + chunk + b\"=====-=\"\n        if len(chunk) == 16:\n            return b\"+\" + chunk + b\"=====-==\"\n        if len(chunk) == 17:\n            return", "idx": 410}
{"namespace": "imapclient.version._imapclient_version_string", "completion": "    major, minor, micro, releaselevel = vinfo\n    version = f\"{major}.{minor}.{micro}\"\n    if releaselevel != \"final\":\n        version += f\"{releaselevel[0]}\"\n    return version", "idx": 411}
{"namespace": "telethon.helpers.generate_key_data_from_nonce", "completion": "    server_nonce_bytes = server_nonce.to_bytes(8, byteorder='little')\n    new_nonce_bytes = new_nonce.to_bytes(8, byteorder='little')\n\n    hash1 = sha1(server_nonce_bytes).digest()\n    hash2 = sha1(new_nonce_bytes).digest()\n    hash3 = sha1(hash1 + hash2).digest()\n\n    key = hash1 + hash2[:12]\n    iv = hash2[12:] + hash3[:4] + new_nonce_bytes[:4]\n\n    return key, iv", "idx": 412}
{"namespace": "hbmqtt.codecs.bytes_to_int", "completion": "    return unpack('>I', data)[0]\n\n", "idx": 413}
{"namespace": "zulipterminal.helper.display_error_if_present", "completion": "    if \"error\" in response:\n        if hasattr(controller, \"view\"):\n            controller.view.display_error(response[\"error\"])", "idx": 414}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._decode_message_id", "completion": "        from zulipterminal.config.regexes import REGEX_INTERNAL_LINK_MESSAGE_ID\n        if re.match(REGEX_INTERNAL_LINK_MESSAGE_ID, message_id):\n            return int(message_id)\n        else:\n            return None", "idx": 415}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton.handle_narrow_link", "completion": "        parsed_link = self._parse_narrow_link(self.link)\n        error = self._validate_narrow_link(parsed_link)\n        if error:\n            self.view.footer.update_footer(error)\n        else:\n            self._switch_narrow_to(parsed_link)\n\n", "idx": 416}
{"namespace": "zulipterminal.config.color.color_properties", "completion": "    for p in prop:\n        for color in colors:\n            setattr(color, p, f\"{p} {color.value}\")\n    return colors", "idx": 417}
{"namespace": "twilio.base.deserialize.decimal", "completion": "    if d is None:\n        return d\n    if d == \"\":\n        return d\n    return Decimal(d, BasicContext)", "idx": 418}
{"namespace": "twilio.base.deserialize.integer", "completion": "    try:\n        return int(i)\n    except ValueError:\n        return i", "idx": 419}
{"namespace": "twilio.base.serialize.object", "completion": "    if obj == values.unset:\n        return obj\n\n    if isinstance(obj, dict):\n        return json.dumps(obj)\n\n    return obj", "idx": 420}
{"namespace": "twilio.base.serialize.map", "completion": "    return [serialize_func(x) for x in lst]", "idx": 421}
{"namespace": "twilio.base.obsolete.deprecated_method", "completion": "    def deprecated_method_wrapper(func):\n        @functools.wraps(func)\n        def new_func(*args, **kwargs):\n            warnings.warn(\n                \"{} has been deprecated and will be removed in a future version of the library. \"\n                \"Please refer to current documentation for guidance.\".format(func.__name__),\n                DeprecationWarning,\n                stacklevel=2,\n            )\n            return func(*args, **kwargs)\n\n        return new_func\n\n    if new_func is None:\n        return deprecated_method_wrapper\n    else:\n        return deprecated_method_wrapper(new_func)", "idx": 422}
{"namespace": "chatette.utils.sample_indulgent", "completion": "    if nb_items > len(array):\n        return deepcopy(array)\n    return sample(array, nb_items)\n\n", "idx": 423}
{"namespace": "chatette.utils.rchop", "completion": "    if string.endswith(ending):\n        return string[:-len(ending)]\n    return string", "idx": 424}
{"namespace": "chatette.utils.str_to_bool", "completion": "    if text == \"True\":\n        return True\n    elif text == \"False\":\n        return False\n    else:\n        raise ValueError(\"The string '{}' cannot be transformed into a boolean.\".format(text))", "idx": 425}
{"namespace": "chatette.utils.min_if_exist", "completion": "    if n1 is None and n2 is None:\n        return None\n    if n1 is None:\n        return n2\n    if n2 is None:\n        return n1\n    return min(n1, n2)", "idx": 426}
{"namespace": "chatette.utils.append_to_list_in_dict", "completion": "    if key in dict_of_lists:\n        dict_of_lists[key].append(value)\n    else:\n        dict_of_lists[key] = [value]", "idx": 427}
{"namespace": "chatette.utils.extend_list_in_dict", "completion": "    if key not in dict_of_lists:\n        dict_of_lists[key] = values\n    else:\n        dict_of_lists[key].extend(values)", "idx": 428}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy._is_end_regex", "completion": "        if word.endswith(\"/\"):\n            word = rchop(word)\n        if word.endswith(\"g\") or word.endswith(\"i\"):\n            word = rchop(word)\n        if word.endswith(\"g\") or word.endswith(\"i\"):\n            word = rchop(word)\n        if word.endswith(\"g\") or word.endswith(\"i\"):\n            return False\n        return True\n", "idx": 429}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.execute", "completion": "        pass", "idx": 430}
{"namespace": "aioxmpp.network.group_and_order_srv_records", "completion": "    if rng is None:\n        rng = random.Random()\n\n    def key(record):\n        return record[0], record[1]\n\n    def group_key(record):\n        return record[0]\n\n    def sort_key(record):\n        return record[1], record[2]\n\n    all_records.sort(key=sort_key)\n    groups = itertools.groupby(all_records, key=group_key)\n\n    for _, group in groups:\n        group = list(group)\n        group.sort(key=sort_key)\n        for record in group:\n            yield record[2]\n\n", "idx": 431}
{"namespace": "aioxmpp.nonza.StreamFeatures.get_feature", "completion": "        try:\n            return self[feature_cls]\n        except KeyError:\n            return default", "idx": 432}
{"namespace": "aioxmpp.connector.XMPPOverTLSConnector._context_factory_factory", "completion": "        ssl_context = metadata.ssl_context_factory()\n        verifier.setup_context(ssl_context, None)\n        if hasattr(ssl_context, \"set_alpn_protos\"):\n            ssl_context.set_alpn_protos([\"xmpp-client\"])\n        return ssl_context\n", "idx": 433}
{"namespace": "aioxmpp.xmltestutils.element_path", "completion": "    path = []\n    while el is not upto:\n        if el.parent is None:\n            raise ValueError(\"Element is not in the tree\")\n        path.append(el.tag)\n        if el.parent.children.index(el) == 0:\n            path.append(\"[0]\")\n        else:\n            path.append(\"[%d]\" % el.parent.children.index(el))\n        el = el.parent\n    path.reverse()\n    return \"/\".join(path)", "idx": 434}
{"namespace": "aioxmpp.structs.JID.fromstr", "completion": "        if not s:\n            raise ValueError(\"empty JID\")\n\n        if s[0] == \"@\":\n            raise ValueError(\"JID must not start with @\")\n\n        if s[-1] == \"/\":\n            raise ValueError(\"JID must not end with /\")\n\n        if \"@\" in s:\n            localpart, s = s.split(\"@\", 1)\n        else:\n            localpart = None\n\n        if \"/\" in s:\n            domain, resource = s.split(\"/\", 1)\n        else:\n            domain = s\n            resource = None\n\n        return cls(localpart, domain, resource, strict=strict)\n\n", "idx": 435}
{"namespace": "aioxmpp.security_layer.extract_python_dict_from_x509", "completion": "    result = {}\n    subject = x509.get_subject()\n    for key, value in subject.get_components():\n        result[key.decode()] = value.decode()\n    subjectAltName = x509.get_extension('subjectAltName')\n    if subjectAltName:\n        result['subjectAltName'] = subjectAltName.get_data().decode()\n    return result\n\n", "idx": 436}
{"namespace": "aioxmpp.security_layer.extract_blob", "completion": "    return pyasn1.codec.der.encoder.encode(x509)", "idx": 437}
{"namespace": "aioxmpp.security_layer.blob_to_pyasn1", "completion": "    return pyasn1.codec.der.decoder.decode(blob, asn1Spec=pyasn1_modules.rfc2459.Certificate())[0]", "idx": 438}
{"namespace": "aioxmpp.security_layer.extract_pk_blob_from_pyasn1", "completion": "    tbs_certificate = pyasn1_struct[0]\n    subject_public_key_info = tbs_certificate[6]\n    return subject_public_key_info.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1Spec.subtypeSpec.asn1", "idx": 439}
{"namespace": "aioxmpp.callbacks.AdHocSignal.ASYNC_WITH_LOOP", "completion": "        if loop is None:\n            loop = asyncio.get_event_loop()\n\n        def wrapper(f):\n            if not hasattr(f, \"__call__\"):\n                raise TypeError(\"must be callable, got {0}\".format(f))\n            return functools.partial(cls._async_wrapper, f, loop)\n        return wrapper\n", "idx": 440}
{"namespace": "aioxmpp.callbacks.AdHocSignal.SPAWN_WITH_LOOP", "completion": "        if loop is None:\n            loop = asyncio.get_event_loop()\n\n        def create_wrapper(f):\n            if not asyncio.iscoroutinefunction(f):\n                raise TypeError(\"must be a coroutine function, got {}\"\n                                .format(f))\n            return functools.partial(cls._spawn_wrapper,\n                                     f,\n                                     loop)\n\n        return create_wrapper\n", "idx": 441}
{"namespace": "aioxmpp.callbacks.first_signal", "completion": "    fut = asyncio.Future()\n    for signal in signals:\n        signal.connect(fut.set_result)\n    return fut", "idx": 442}
{"namespace": "aioxmpp.tasks.TaskPool.spawn", "completion": "        if not isinstance(__groups, set):\n            raise TypeError(\"groups must be a set\")\n        if not callable(__coro_fun):\n            raise TypeError(\"coro_fun must be callable\")\n\n        # Check if the groups have free slots available for the coroutine to be spawned\n        for group in __groups:\n            if self.get_limit(group) is not None and self.get_task_count(group) >= self.get_limit(group):\n                raise RuntimeError(\"Limit on group {} is exhausted\".format(group))\n\n        # Check if the total limit is exhausted\n        if self.default_limit is not None and self.get_task_count(()) >= self.default_limit:\n            raise RuntimeError(\"Total limit is exhausted\")\n\n        # Start the coroutine and add it to the pool\n        coro = __coro_fun(*args, **kwargs)\n        task = asyncio.create_task(coro)\n        for group in __groups:\n            self._group_tasks.setdefault(group, set()).add(task)\n        return task", "idx": 443}
{"namespace": "aioxmpp.protocol.send_and_wait_for", "completion": "    if timeout is not None:\n        timeout = asyncio.wait_for(xmlstream.send_xso(send), timeout)\n    else:\n        timeout = xmlstream.send_xso(send)\n    try:\n        await timeout\n    except asyncio.TimeoutError:\n        raise TimeoutError(\"Timeout while waiting for response\")\n    if cb is not None:\n        cb(wait_for)\n    return wait_for\n\n", "idx": 444}
{"namespace": "aioxmpp.testutils.run_coroutine_with_peer", "completion": "    if not loop:\n        loop = asyncio.get_event_loop()\n\n    local_future = asyncio.Future(loop=loop)\n    peer_future = asyncio.Future(loop=loop)\n\n    def _set_result(future, result):\n        future.set_result(result)\n\n    coroutine.add_done_callback(functools.partial(_set_result, local_future))\n    peer_coroutine.add_done_callback(functools.partial(_set_result, peer_future))\n\n    try:\n        loop.run_until_complete(asyncio.wait([local_future, peer_future], loop=loop, timeout=timeout))\n    except asyncio.TimeoutError:\n        local_future.cancel()\n        peer_future.cancel()\n        raise\n\n    return local_future.result()", "idx": 445}
{"namespace": "aioxmpp.testutils.make_listener", "completion": "    listener = unittest.mock.Mock()\n    for name, signal in callbacks.get_signals(instance).items():\n        setattr(listener, name, signal.connect(listener.event))\n    return listener", "idx": 446}
{"namespace": "aioxmpp.vcard.service.VCardService.set_vcard", "completion": "        iq = aioxmpp.IQ(\n            type_=aioxmpp.IQType.SET,\n            to=jid,\n            payload=vcard,\n        )\n\n        try:\n            await self.client.send(iq)\n        except aioxmpp.XMPPCancelError as e:\n            if e.condition in (\n                    aioxmpp.ErrorCondition.FEATURE_NOT_IMPLEMENTED,\n                    aioxmpp.ErrorCondition.ITEM_NOT_FOUND):\n                return vcard_xso.VCard()\n            else:\n                raise", "idx": 447}
{"namespace": "aioxmpp.rsm.xso.ResultSetMetadata.limit", "completion": "        result = copy.copy(self)\n        result.max_ = max_\n        return result\n", "idx": 448}
{"namespace": "aioxmpp.muc.service.Room.features", "completion": "        return self._service.features\n", "idx": 449}
{"namespace": "aioxmpp.xso.query.EvaluationContext.eval_bool", "completion": "        return bool(self.eval(expr))", "idx": 450}
{"namespace": "aioxmpp.xso.query._BoolOpMixin.eval", "completion": "        for leaf in self.leafs:\n            if ec.eval_bool(leaf):\n                yield True\n                break\n        else:\n            yield False\n\n", "idx": 451}
{"namespace": "aioxmpp.xso.model.drop_handler", "completion": "    depth = 0\n    while True:\n        ev_type, *ev_args = yield\n        if ev_type == \"start\":\n            depth += 1\n        elif ev_type == \"end\":\n            depth -= 1\n            if depth == 0:\n                break\n\n", "idx": 452}
{"namespace": "aioxmpp.xso.model.guard", "completion": "    depth = 1\n    while depth:\n        ev = yield\n        if ev[0] == \"start\":\n            depth += 1\n        elif ev[0] == \"end\":\n            depth -= 1\n        dest.send(ev)\n    return (yield from dest)", "idx": 453}
{"namespace": "aioxmpp.xso.model.capture_events", "completion": "    try:\n        while True:\n            value = yield\n            dest.append(value)\n            receiver.send(value)\n    except GeneratorExit:\n        dest.clear()\n        raise\n    except Exception:\n        dest.clear()\n        raise\n\n", "idx": 454}
{"namespace": "aioxmpp.xso.model.events_to_sax", "completion": "    for ev_type, *ev_args in events:\n        if ev_type == \"start\":\n            dest.startElementNS(ev_args[0], None, ev_args[2])\n        elif ev_type == \"text\":\n            dest.characters(ev_args[0])\n        elif ev_type == \"end\":\n            dest.endElementNS(ev_args[0], None)\n\n", "idx": 455}
{"namespace": "aioxmpp.adhoc.service.AdHocClient.get_command_info", "completion": "        disco = self.dependencies[aioxmpp.disco.DiscoClient]\n        response = await disco.query_items(\n            peer_jid,\n            node=namespaces.xep0050_commands,\n            item_node=command_name,\n        )\n        return response.items[0]", "idx": 456}
{"namespace": "aioxmpp.entitycaps.caps115.build_identities_string", "completion": "    # Process each identity and encode it into a byte string\n    identity_bytes = [identity.encode() for identity in identities]\n\n    # Check for duplicate identities and sort the identities\n    identity_bytes = list(set(identity_bytes))\n    identity_bytes.sort()\n\n    # Join the identities into a single byte string\n    return b\"\".join(identity_bytes)\n\n", "idx": 457}
{"namespace": "aioxmpp.entitycaps.caps115.build_features_string", "completion": "    features = [escape(feature).encode(\"utf-8\") for feature in features]\n\n    if len(set(features)) != len(features):\n        raise ValueError(\"duplicate feature\")\n\n    features.sort()\n    features.append(b\"\")\n    return b\"<\".join(features)", "idx": 458}
{"namespace": "aioxmpp.entitycaps.caps115.build_forms_string", "completion": "    forms = [\n        b\"/\".join([\n            escape(form.type_).encode(\"utf-8\"),\n            escape(form.name or \"\").encode(\"utf-8\"),\n            escape(form.label or \"\").encode(\"utf-8\"),\n            escape(form.instructions or \"\").encode(\"utf-8\"),\n            escape(form.x_data_type or \"\").encode(\"utf-8\"),\n            escape(form.reported or \"\").encode(\"utf-8\"),\n            escape(form.reportable or \"\").encode(\"utf-8\"),\n            escape(form.desc or \"\").encode(\"utf-8\"),\n            escape(form.values or \"\").encode(\"utf-8\"),\n            escape(form.x_data_values or \"\").encode(\"utf-8\"),\n            escape(form.x_data_mediatype or \"\").encode(\"utf-8\"),\n            escape(form.x_data_uri or \"\").encode(\"utf-8\"),\n            escape(form.x_data_desc or \"\").encode(\"utf-8\"),\n            escape(form.x_data_label or \"\").encode(\"utf-8\"),\n            escape(form.x_data_required or \"\").encode(\"utf-8\"),\n            escape(form.x_data_value or \"\").encode(\"utf-8\"),\n            escape(form.x_data_options or \"\").encode(\"utf-8\"),\n            escape(form.x_data_option or \"\").encode(\"utf-8\"),\n            escape(form.x_data_option_label or \"\").encode(\"utf-8\"),\n            escape(form.x_data_option_value or \"\").encode(\"utf-8\"),\n            escape(form.x_data_option_desc or \"\").encode(\"utf-8\"),\n            escape(form.x_data_option_selected or \"\").encode(\"utf-8\"),\n            escape(form.x_data_option_params or \"\").encode(\"utf-8\"),\n            escape(form.x_data_option_param or \"\").encode(\"utf-8\"),\n            escape(form.x_data_option_param_name or \"\").encode(\"utf-8", "idx": 459}
{"namespace": "aioxmpp.entitycaps.caps115.Key.path", "completion": "        return pathlib.Path(\"hashes\") / self.algo / urllib.parse.quote(self.node)\n\n", "idx": 460}
{"namespace": "aioxmpp.entitycaps.caps390._process_features", "completion": "    features_bytes = b''\n    for feature in features:\n        features_bytes += feature.encode('utf-8')\n    return features_bytes\n\n", "idx": 461}
{"namespace": "aioxmpp.entitycaps.caps390._process_identities", "completion": "    parts = [\n        _process_identity(identity)\n        for identity in identities\n    ]\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1c\"\n\n", "idx": 462}
{"namespace": "aioxmpp.entitycaps.caps390._process_extensions", "completion": "    parts = [\n        _process_form(ext)\n        for ext in exts\n    ]\n\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1c\"", "idx": 463}
{"namespace": "aioxmpp.entitycaps.caps390._calculate_hash", "completion": "    if algo == \"sha-1\":\n        return aioxmpp.hashes.sha1(hash_input)\n    elif algo == \"sha-224\":\n        return aioxmpp.hashes.sha224(hash_input)\n    elif algo == \"sha-256\":\n        return aioxmpp.hashes.sha256(hash_input)\n    elif algo == \"sha-384\":\n        return aioxmpp.hashes.sha384(hash_input)\n    elif algo == \"sha-512\":\n        return aioxmpp.hashes.sha512(hash_input)\n    elif algo == \"sha3-224\":\n        return aioxmpp.hashes.sha3_224(hash_input)\n    elif algo == \"sha3-256\":\n        return aioxmpp.hashes.sha3_256(hash_input)\n    elif algo == \"sha3-384\":\n        return aioxmpp.hashes.sha3_384(hash_input)\n    elif algo == \"sha3-512\":\n        return aioxmpp.hashes.sha3_512(hash_input)\n    elif algo == \"blake2b\":\n        return aioxmpp.hashes.blake2b(hash_input)\n    elif algo == \"blake2s\":\n        return aioxmpp.hashes.blake2s(hash_input)\n    else:\n        raise ValueError(\"Unknown hash algorithm: {}\".format(algo))\n\n", "idx": 464}
{"namespace": "aioxmpp.entitycaps.caps390.Key.node", "completion": "        return \"{}-{}\".format(self.algo, base64.b64encode(self.digest).decode(\"utf-8\"))\n\n", "idx": 465}
{"namespace": "aioxmpp.entitycaps.caps390.Key.path", "completion": "        return pathlib.PurePosixPath(\n            \"caps\",\n            base64.b64encode(self.digest).decode(\"ascii\") + \".\" + self.algo\n        )\n", "idx": 466}
{"namespace": "aioxmpp.entitycaps.caps390.Implementation.extract_keys", "completion": "        if presence.xep0390_caps is not None:\n            for algo in self.__algorithms:\n                for node in presence.xep0390_caps.nodes:\n                    if node.startswith(\"urn:xmpp:caps#\"):\n                        algo, digest = node.split(\"#\")\n                        algo = algo.split(\":\")[2]\n                        digest = base64.b64decode(digest)\n                        yield Key(algo, digest)\n", "idx": 467}
{"namespace": "aioxmpp.roster.service.RosterClient.approve", "completion": "        self.client.send(\n            stanza.Presence(\n                to=peer_jid,\n                type_=structs.PresenceType.SUBSCRIBED\n            )\n        )", "idx": 468}
{"namespace": "aioxmpp.roster.service.RosterClient.subscribe", "completion": "        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.SUBSCRIBE,\n                            to=peer_jid)\n        )", "idx": 469}
{"namespace": "aioxmpp.roster.service.RosterClient.unsubscribe", "completion": "        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.UNSUBSCRIBE,\n                            to=peer_jid)\n        )", "idx": 470}
{"namespace": "aioxmpp.forms.fields.BoundSingleValueField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass\n", "idx": 471}
{"namespace": "aioxmpp.forms.fields.BoundMultiValueField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass\n", "idx": 472}
{"namespace": "aioxmpp.forms.fields.BoundOptionsField.options", "completion": "        try:\n            del self._options\n        except AttributeError:\n            pass\n", "idx": 473}
{"namespace": "aioxmpp.forms.fields.BoundSelectField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass\n", "idx": 474}
{"namespace": "aioxmpp.forms.fields.BoundMultiSelectField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass\n", "idx": 475}
{"namespace": "cupy.random._generator.reset_states", "completion": "    global _random_states\n    _random_states = {}\n\n", "idx": 476}
{"namespace": "cupy.random._generator._check_and_get_dtype", "completion": "    if dtype is None:\n        dtype = float\n    elif isinstance(dtype, numpy.dtype):\n        dtype = dtype.type\n    elif isinstance(dtype, numpy.generic):\n        dtype = dtype.dtype.type\n    else:\n        dtype = numpy.dtype(dtype)\n\n    if dtype not in (numpy.float32, numpy.float64):\n        raise TypeError('cupy.random does not support dtype={}'.format(dtype))\n\n    return dtype", "idx": 477}
{"namespace": "cupy_builder._command.filter_files_by_extension", "completion": "    sources_with_extension = []\n    sources_without_extension = []\n    for source in sources:\n        if source.endswith(extension):\n            sources_with_extension.append(source)\n        else:\n            sources_without_extension.append(source)\n    return sources_with_extension, sources_without_extension\n\n", "idx": 478}
{"namespace": "datasets.table._in_memory_arrow_table_from_file", "completion": "    with pa.memory_map(filename, \"r\") as source:\n        return pa.ipc.open_stream(source).read_all()\n\n", "idx": 479}
{"namespace": "datasets.table._in_memory_arrow_table_from_buffer", "completion": "    buffer_reader = pa.BufferReader(buffer)\n    opened_stream = pa.ipc.open_stream(buffer_reader)\n    pa_table = opened_stream.read_all()\n    return pa_table\n\n", "idx": 480}
{"namespace": "datasets.table._interpolation_search", "completion": "    if x < arr[0] or x > arr[-1]:\n        raise IndexError(f\"{x} is outside the array values\")\n\n    i, j = 0, len(arr) - 1\n    while i <= j:\n        mid = (i + j) // 2\n        if arr[mid] <= x < arr[mid + 1]:\n            return mid\n        elif x < arr[mid]:\n            j = mid - 1\n        else:\n            i = mid + 1\n\n    raise IndexError(f\"{x} is outside the array values\")", "idx": 481}
{"namespace": "datasets.data_files._is_inside_unrequested_special_dir", "completion": "    if not contains_wildcards(pattern):\n        return False\n    if not is_relative_path(matched_rel_path):\n        return False\n    if not is_local_path(matched_rel_path):\n        return False\n    if not is_local_path(pattern):\n        return False\n    if not is_relative_path(pattern):\n        return False\n    if not matched_rel_path.startswith(pattern):\n        return False\n    if matched_rel_path == pattern:\n        return False\n    if matched_rel_path.startswith(pattern + \"/\"):\n        return False\n    if matched_rel_path.startswith(pattern + \"\\\\\"):\n        return False\n    if matched_rel_path.startswith(pattern + os.sep):\n        return False\n    if matched_rel_path.startswith(pattern + os.altsep):\n        return False\n    if matched_rel_path.startswith(pattern + os.pathsep):\n        return False\n    if matched_rel_path.startswith(pattern + os.sep + os.altsep):\n        return False\n    if matched_rel_path.startswith(pattern + os.sep + os.pathsep):\n        return False\n    if matched_rel_path.startswith(pattern + os.altsep + os.sep):\n        return False\n    if matched_rel_path.startswith(pattern + os.altsep + os.pathsep):\n        return False\n    if matched_rel_path.startswith(pattern + os.pathsep + os.sep):\n        return False\n    if matched_rel_path.startswith(pattern + os.pathsep + os.altsep):\n        return False\n    if matched_rel_path.startswith(pattern + os.altsep + os.sep + os.pathsep):\n        return False\n    if matched_rel_path.startswith(pattern + os.altsep + os.pathsep + os.sep):\n        return False\n    if matched_rel_path.startswith(pattern + os.pathsep + os.alt", "idx": 482}
{"namespace": "datasets.data_files._is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir", "completion": "    # We just need to check if every special directories from the path is present explicly in the pattern.\n    # Since we assume that the path matches the pattern, it's equivalent to counting that both\n    # the parent path and the parent pattern have the same number of special directories.\n    data_dirs_to_ignore_in_path = [part for part in PurePath(matched_rel_path).parent.parts if part.startswith(\"__\")]\n    data_dirs_to_ignore_in_pattern = [part for part in PurePath(pattern).parent.parts if part.startswith(\"__\")]\n    return len(data_dirs_to_ignore_in_path) != len(data_dirs_to_ignore_in_pattern)", "idx": 483}
{"namespace": "datasets.iterable_dataset._batch_to_examples", "completion": "    # we order the columns by order of appearance\n    # to do so, we use a dict as an ordered set\n    cols = {col: None for col in batch}\n    # when an example is missing a column, we set the value to None with .get()\n    arrays = [[example.get(col) for example in batch] for col in cols]\n    return dict(zip(cols, arrays))", "idx": 484}
{"namespace": "datasets.iterable_dataset._examples_to_batch", "completion": "    columns = set()\n    for example in examples:\n        columns.update(example.keys())\n    columns = list(columns)\n    arrays = [[] for _ in range(len(columns))]\n    for example in examples:\n        for i, column in enumerate(columns):\n            arrays[i].append(example[column])\n    return dict(zip(columns, arrays))", "idx": 485}
{"namespace": "datasets.iterable_dataset.RandomlyCyclingMultiSourcesExamplesIterable._iter_random_indices", "completion": "        if p is None:\n            p = [1 / num_sources] * num_sources\n        while True:\n            yield from rng.choice(num_sources, size=random_batch_size, p=p, replace=True)", "idx": 486}
{"namespace": "datasets.iterable_dataset.BufferShuffledExamplesIterable._iter_random_indices", "completion": "        while True:\n            yield from (int(i) for i in rng.integers(0, buffer_size, size=random_batch_size))\n", "idx": 487}
{"namespace": "datasets.iterable_dataset.IterableDataset.remove_columns", "completion": "        if isinstance(column_names, str):\n            column_names = [column_names]\n        return self.map(partial(_remove_columns_fn, column_names=column_names), remove_columns=column_names)", "idx": 488}
{"namespace": "datasets.dataset_dict.DatasetDict.with_format", "completion": "        self._check_values_type()\n        return DatasetDict(\n            {\n                k: dataset.with_format(\n                    type=type, columns=columns, output_all_columns=output_all_columns, **format_kwargs\n                )\n                for k, dataset in self.items()\n            }\n        )", "idx": 489}
{"namespace": "datasets.dataset_dict.DatasetDict.with_transform", "completion": "        dataset = copy.deepcopy(self)\n        dataset.set_format(\"custom\", columns=columns, output_all_columns=output_all_columns, transform=transform)\n        return dataset", "idx": 490}
{"namespace": "datasets.dataset_dict.DatasetDict.align_labels_with_mapping", "completion": "        self._check_values_type()\n        return DatasetDict({k: dataset.align_labels_with_mapping(label2id=label2id, label_column=label_column) for k, dataset in self.items()})", "idx": 491}
{"namespace": "datasets.dataset_dict.IterableDatasetDict.map", "completion": "        return IterableDatasetDict({k: dataset.map(function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, fn_kwargs) for k, dataset in self.items()})\n", "idx": 492}
{"namespace": "datasets.dataset_dict.IterableDatasetDict.filter", "completion": "        return IterableDatasetDict(\n            {\n                k: dataset.filter(\n                    function=function,\n                    with_indices=with_indices,\n                    input_columns=input_columns,\n                    batched=batched,\n                    batch_size=batch_size,\n                    fn_kwargs=fn_kwargs,\n                )\n                for k, dataset in self.items()\n            }\n        )", "idx": 493}
{"namespace": "datasets.arrow_dataset.Dataset.num_rows", "completion": "        if self._indices is not None:\n            return len(self._indices)\n        else:\n            return len(self._data)\n", "idx": 494}
{"namespace": "datasets.filesystems.extract_path_from_uri", "completion": "    if dataset_path.startswith(\"s3://\"):\n        dataset_path = dataset_path[5:]\n    return dataset_path", "idx": 495}
{"namespace": "datasets.filesystems.is_remote_filesystem", "completion": "    if isinstance(fs, fsspec.asyn.AsyncFileSystem):\n        fs = fs.fs\n    return fs.protocol in [\"s3\", \"gs\", \"abfs\", \"az\", \"gcs\", \"http\", \"https\", \"s3a\", \"s3n\", \"webdav\", \"webdavs\"]", "idx": 496}
{"namespace": "datasets.utils.file_utils.hash_url_to_filename", "completion": "    url_hash = sha256(url.encode(\"utf-8\")).hexdigest()\n    if etag:\n        etag_hash = sha256(etag.encode(\"utf-8\")).hexdigest()\n        url_hash = url_hash + \".\" + etag_hash\n    if url.endswith(\".h5\"):\n        url_hash = url_hash + \".h5\"\n    return url_hash", "idx": 497}
{"namespace": "datasets.utils.hub.hf_hub_url", "completion": "    if version.parse(hfh.__version__) < version.parse(\"0.11.0\"):\n        path = quote(path, safe=\"\")\n    if revision is None:\n        return f\"https://huggingface.co/{repo_id}/resolve/{path}\"\n    else:\n        return f\"https://huggingface.co/{repo_id}/resolve/{revision}/{path}\"", "idx": 498}
{"namespace": "datasets.utils.sharding._number_of_shards_in_gen_kwargs", "completion": "    if \"shard_id\" in gen_kwargs:\n        return 1\n\n    if \"shard_id\" not in gen_kwargs:\n        shard_id_list = gen_kwargs[\"shard_id\"]\n        shard_count_list = gen_kwargs[\"shard_count\"]\n        if len(shard_id_list) != len(shard_count_list):\n            raise ValueError(\n                \"The length of shard_id and shard_count lists must be the same.\"\n            )\n        return len(shard_id_list)\n\n", "idx": 499}
{"namespace": "datasets.utils.sharding._distribute_shards", "completion": "    if num_shards <= max_num_jobs:\n        return [range(num_shards)]\n    else:\n        shard_indices = np.arange(num_shards)\n        shard_indices = np.array_split(shard_indices, max_num_jobs)\n        return [range(shard_indices[i][0], shard_indices[i][-1] + 1) for i in range(max_num_jobs)]", "idx": 500}
{"namespace": "datasets.utils.py_utils.temporary_assignment", "completion": "    original_value = getattr(obj, attr)\n    setattr(obj, attr, value)\n    try:\n        yield\n    finally:\n        setattr(obj, attr, original_value", "idx": 501}
{"namespace": "datasets.utils.extract.TarExtractor.extract", "completion": "        if not os.path.exists(output_path):\n            os.makedirs(output_path)\n        with tarfile.open(input_path, \"r:*\") as tar:\n            tar.extractall(output_path, members=TarExtractor.safemembers(tar, output_path))\n\n", "idx": 502}
{"namespace": "datasets.utils.extract.Extractor.infer_extractor_format", "completion": "        magic_number_length = cls._get_magic_number_max_length()\n        magic_number = cls._read_magic_number(path, magic_number_length=magic_number_length)\n        for extractor_format, extractor in cls.extractors.items():\n            if extractor.is_extractable(path, magic_number=magic_number):\n                return extractor_format\n        return \"\"\n", "idx": 503}
{"namespace": "datasets.utils.py_utils.asdict", "completion": "    if is_dataclass(obj):\n        return {field.name: asdict(getattr(obj, field.name)) for field in fields(obj)}\n    elif isinstance(obj, tuple) and hasattr(obj, \"_asdict\"):\n        return obj._asdict()\n    elif isinstance(obj, (list, tuple)):\n        return [asdict(item) for item in obj]\n    elif isinstance(obj, dict):\n        return {key: asdict(value) for key, value in obj.items()}\n    else:\n        return obj", "idx": 504}
{"namespace": "datasets.utils.metadata.MetadataConfigs.from_dataset_card_data", "completion": "        if cls.FIELD_NAME in dataset_card_data:\n            metadata_configs = dataset_card_data[cls.FIELD_NAME]\n            if not isinstance(metadata_configs, dict):\n                raise ValueError(\n                    f\"Expected {cls.FIELD_NAME} to be a dict, but got {type(metadata_configs)}\"\n                )\n            for metadata_config_name, metadata_config in metadata_configs.items():\n                if not isinstance(metadata_config, dict):\n                    raise ValueError(\n                        f\"Expected {cls.FIELD_NAME} to be a dict of dicts, but got {type(metadata_config)}\"\n                    )\n                cls._raise_if_data_files_field_not_valid(metadata_config)\n            return cls(metadata_configs)\n        else:\n            return cls()", "idx": 505}
{"namespace": "pymorphy2.analyzer.lang_dict_path", "completion": "    lang_dict_paths = _lang_dict_paths()\n    if lang not in lang_dict_paths:\n        raise ValueError(\n            \"Language '{}' is not supported. \"\n            \"Supported languages: {}\".format(\n                lang, ', '.join(sorted(lang_dict_paths.keys()))\n            )\n        )\n    return lang_dict_paths[lang]\n\n", "idx": 506}
{"namespace": "pymorphy2.dawg.assert_can_create", "completion": "    if not EXTENSION_AVAILABLE:\n        raise NotImplementedError(\"DAWG extension is not available\")\n\n", "idx": 507}
{"namespace": "pymorphy2.opencorpora_dict.compile._to_paradigm", "completion": "    stem = ''\n    paradigm = []\n    for word_form, tag in lexeme:\n        prefix = ''\n        for p in paradigm_prefixes:\n            if word_form.startswith(p):\n                prefix = p\n                word_form = word_form[len(p):]\n                break\n\n        if not prefix:\n            stem = ''\n            paradigm = []\n            break\n\n        suffix = ''\n        for s in word_form:\n            if s.isalpha():\n                break\n            suffix += s\n\n        paradigm.append((suffix, tag, prefix))\n\n    return stem, tuple(paradigm)\n\n", "idx": 508}
{"namespace": "pymorphy2.units.by_analogy.KnownPrefixAnalyzer.tag", "completion": "        result = []\n        for prefix, unprefixed_word in self.possible_splits(word_lower):\n            method = (self, prefix)\n\n            tags = self.morph.tag(unprefixed_word)\n            for tag, score, methods_stack in tags:\n\n                if not tag.is_productive():\n                    continue\n\n                tag = (\n                    prefix + tag,\n                    score * self.score_multiplier,\n                    methods_stack + (method,)\n                )\n\n                add_tag_if_not_seen(tag, result, seen_tags)\n\n        return result\n", "idx": 509}
{"namespace": "pymorphy2.units.by_analogy.UnknownPrefixAnalyzer.tag", "completion": "        result = []\n        for prefix, unprefixed_word in word_splits(word_lower):\n            for tag in self.morph.tag(unprefixed_word):\n                if not tag.is_productive():\n                    continue\n                add_tag_if_not_seen(tag, result, seen_tags)\n        return result", "idx": 510}
{"namespace": "benedict.dicts.keylist.keylist_util.get_item", "completion": "    if not keys:\n        return None\n\n    item = d\n    for key in keys:\n        try:\n            item = item[key]\n        except (KeyError, IndexError, TypeError):\n            return None\n\n    return item", "idx": 511}
{"namespace": "benedict.dicts.keylist.keylist_util.set_item", "completion": "    item = d\n    for key in keys[:-1]:\n        item = _get_or_new_item_value(item, key, keys[keys.index(key) + 1])\n    _set_item_value(item, keys[-1], value)", "idx": 512}
{"namespace": "benedict.dicts.keypath.keypath_util._split_key_indexes", "completion": "    if not key:\n        return []\n\n    if not key.endswith(\"]\"):\n        return [key]\n\n    indexes = []\n    index_str = \"\"\n    for char in key:\n        if char == \"[\":\n            index_str = \"\"\n        elif char == \"]\":\n            indexes.append(index_str)\n        else:\n            index_str += char\n\n    return indexes\n\n", "idx": 513}
{"namespace": "feedparser.urls.make_safe_absolute_uri", "completion": "    if not rel:\n        return base\n    if not base:\n        return rel\n    uri = _urljoin(base, rel)\n    if not uri:\n        return ''\n    scheme = urllib.parse.urlparse(uri).scheme\n    if scheme not in ACCEPTABLE_URI_SCHEMES:\n        return ''\n    return uri", "idx": 514}
{"namespace": "feedparser.api._open_resource", "completion": "    if isinstance(url_file_stream_or_string, str):\n        if url_file_stream_or_string.startswith('http://') or url_file_stream_or_string.startswith('https://'):\n            # URL\n            if handlers is None:\n                handlers = []\n            opener = urllib.request.build_opener(*handlers)\n            if agent is not None:\n                opener.addheaders = [('User-Agent', agent)]\n            if referrer is not None:\n                opener.addheaders = [('Referer', referrer)]\n            if request_headers is not None:\n                opener.addheaders = request_headers\n            if etag is not None:\n                opener.addheaders = [('If-None-Match', etag)]\n            if modified is not None:\n                if isinstance(modified, tuple):\n                    modified = time.strftime('%a, %d %b %Y %H:%M:%S GMT', modified)\n                opener.addheaders = [('If-Modified-Since', modified)]\n            result.data = opener.open(url_file_stream_or_string).read()\n        else:\n            # File\n            result.data = open(url_file_stream_or_string, 'rb').read()\n    elif isinstance(url_file_stream_or_string, io.IOBase):\n        # Stream\n        result.data = url_file_stream_or_string.read()\n    else:\n        # String\n        result.data = url_file_stream_or_string.encode('utf-8')\n\n    return result.data", "idx": 515}
{"namespace": "feedparser.http._build_urllib2_request", "completion": "    # Create a request object\n    request = urllib.request.Request(url)\n\n    # Add headers to the request\n    request.add_header('User-Agent', agent)\n    if accept_header:\n        request.add_header('Accept', accept_header)\n    if etag:\n        request.add_header('If-None-Match', etag)\n    if modified:\n        if isinstance(modified, datetime.datetime):\n            modified = modified.strftime('%a, %d %b %Y %H:%M:%S GMT')\n        request.add_header('If-Modified-Since', modified)\n    if referrer:\n        request.add_header('Referer', referrer)\n    if auth:\n        request.add_header('Authorization', auth)\n    if request_headers:\n        for key, value in request_headers.items():\n            request.add_header(key, value)\n\n    return request\n\n", "idx": 516}
{"namespace": "pylatex.utils.dumps_list", "completion": "    if mapper is None:\n        mapper = []\n\n    if not isinstance(mapper, list):\n        mapper = [mapper]\n\n    if not isinstance(l, list):\n        l = [l]\n\n    if escape:\n        l = [escape_latex(item) for item in l]\n\n    for m in mapper:\n        l = [m(item) for item in l]\n\n    if as_content:\n        l = [item.dumps_as_content() for item in l]\n\n    return NoEscape(token.join(l))\n\n", "idx": 517}
{"namespace": "pylatex.utils._latex_item_to_string", "completion": "    if isinstance(item, pylatex.base_classes.LatexObject):\n        if as_content:\n            return item.dumps_as_content()\n        return item.dumps()\n\n    if escape:\n        return escape_latex(str(item))\n\n    return str(item)", "idx": 518}
{"namespace": "mistune.markdown.Markdown.read", "completion": "        if state is None:\n            state = self.block.state_cls()\n\n        with open(filepath, 'rb') as f:\n            content = f.read()\n\n        content = content.decode(encoding)\n        return self.parse(content, state)", "idx": 519}
{"namespace": "mistune.create_markdown", "completion": "    if plugins is None:\n        plugins = []\n\n    if isinstance(renderer, str):\n        renderer = import_plugin(renderer)()\n\n    if escape:\n        renderer.escape = escape\n\n    if hard_wrap:\n        renderer.hard_wrap = hard_wrap\n\n    return Markdown(renderer, plugins)", "idx": 520}
{"namespace": "parsel.utils.extract_regex", "completion": "    if isinstance(regex, str):\n        regex = re.compile(regex)\n    if replace_entities:\n        text = w3lib_replace_entities(text)\n    match = regex.search(text)\n    if match is None:\n        return []\n    if \"extract\" in match.groupdict():\n        return [match.group(\"extract\")]\n    return flatten(match.groups())", "idx": 521}
{"namespace": "dominate.dom_tag.dom_tag.render", "completion": "    # If the tag is a single tag, render it as a self-closing tag\n    if self.is_single:\n      return self.render_single(indent, pretty, xhtml)\n\n    # If the tag is an inline tag, render it as an inline tag\n    if self.is_inline:\n      return self.render_inline(indent, pretty, xhtml)\n\n    # If the tag is a pretty tag, render it as a pretty tag\n    if self.is_pretty:\n      return self.render_pretty(indent, pretty, xhtml)\n\n    # If the tag is not a pretty tag, render it as a normal tag\n    return self.render_normal(indent, pretty, xhtml)\n\n", "idx": 522}
{"namespace": "dominate.util.include", "completion": "  with open(f, 'r') as f:\n    return f.read()", "idx": 523}
{"namespace": "dominate.util.unescape", "completion": "  # This function unescapes HTML entities in the given data. It searches for HTML entities in the data and replaces them with their corresponding characters.\n  # Input-Output Arguments\n  # :param data: String. The input data containing HTML entities to be unescaped.\n  # :return: String. The unescaped data.\n  # \"\"\"\n  # This function unescapes HTML entities in the given data. It searches for HTML entities in the data and replaces them with their corresponding characters.\n  # Input-Output Arguments\n  # :param data: String. The input data containing HTML entities to be unescaped.\n  # :return: String. The unescaped data.\n  # \"\"\"\n  # This function unescapes HTML entities in the given data. It searches for HTML entities in the data and replaces them with their corresponding characters.\n  # Input-Output Arguments\n  # :param data: String. The input data containing HTML entities to be unescaped.\n  # :return: String. The unescaped data.\n  # \"\"\"\n  # This function unescapes HTML entities in the given data. It searches for HTML entities in the data and replaces them with their corresponding characters.\n  # Input-Output Arguments\n  # :param data: String. The input data containing HTML entities to be unescaped.\n  # :return: String. The unescaped data.\n  # \"\"\"\n  # This function unescapes HTML entities in the given data. It searches for HTML entities in the data and replaces them with their corresponding characters.\n  # Input-Output Arguments\n  # :param data: String. The input data containing HTML entities to be unescaped.\n  # :return: String. The unescaped data.\n  # \"\"\"\n  # This function unescapes HTML entities in the given data. It searches for HTML entities in the data and replaces them with their corresponding characters.\n  # Input-Output Arguments\n  # :param data: String. The input data containing HTML entities to be unescaped.\n  # :return: String. The unescaped data.\n  # \"\"\"\n  # This function unescapes HTML entities in the given data. It searches for HTML entities in the data and replaces", "idx": 524}
{"namespace": "onlinejudge_command.pretty_printers._tokenize_line", "completion": "    tokens = []\n    l = 0\n    while l < len(line):\n        r = l + 1\n        while r < len(line) and (line[l] in ' \\t') == (line[r] in ' \\t'):\n            r += 1\n        if line[l] in ' \\t':\n            typ = _PrettyTokenType.WHITESPACE\n        elif line[l] == '\\n':\n            typ = _PrettyTokenType.NEWLINE\n        else:\n            typ = _PrettyTokenType.BODY\n        tokens.append(_PrettyToken(typ, line[l:r]))\n        l = r\n    return tokens\n\n", "idx": 525}
{"namespace": "onlinejudge_command.pretty_printers._render_tokens", "completion": "    result = []\n    for token in tokens:\n        if token.type == _PrettyTokenType.BODY:\n            if font_bold:\n                result.append(font_bold(token.value))\n            else:\n                result.append(token.value)\n        elif token.type == _PrettyTokenType.BODY_HIGHLIGHT_LEFT:\n            if font_bold:\n                result.append(font_bold(token.value))\n            else:\n                result.append(token.value)\n        elif token.type == _PrettyTokenType.BODY_HIGHLIGHT_RIGHT:\n            if font_bold:\n                result.append(font_bold(token.value))\n            else:\n                result.append(token.value)\n        elif token.type == _PrettyTokenType.WHITESPACE:\n            if font_dim:\n                result.append(font_dim(token.value))\n            else:\n                result.append(token.value)\n        elif token.type == _PrettyTokenType.NEWLINE:\n            result.append(token.value)\n        elif token.type == _PrettyTokenType.HINT:\n            if font_red:\n                result.append(font_red(token.value))\n            else:\n                result.append(token.value)\n        elif token.type == _PrettyTokenType.LINENO:\n            if font_blue:\n                result.append(font_blue(token.value))\n            else:\n                result.append(token.value)\n        elif token.type == _PrettyTokenType.OTHERS:\n            if font_normal:\n                result.append(font_normal(token.value))\n            else:\n                result.append(token.value)\n        else:\n            raise ValueError('unknown token type: {}'.format(token.type))\n    return ''.join(result)\n\n", "idx": 526}
{"namespace": "onlinejudge_command.pretty_printers._tokenize_file_content_without_snipping", "completion": "    tokens, text = _decode_with_recovery(content)\n    if text:\n        for line in text.splitlines(keepends=True):\n            tokens += _tokenize_line(line)\n    tokens = _warn_if_empty(tokens)\n    return tokens", "idx": 527}
{"namespace": "jinja2.environment.Environment.get_template", "completion": "        if isinstance(name, Template):\n            template = name\n        else:\n            if parent is not None:\n                name = self.join_path(name, parent)\n            template = self._load_template(name, globals)\n\n        if template.parent is None:\n            template.parent = parent\n\n        return template\n", "idx": 528}
{"namespace": "jinja2.environment.Environment.from_string", "completion": "        if isinstance(source, str):\n            source = self._parse(source, None, None)\n        if template_class is None:\n            template_class = self.template_class\n        return template_class(self, source, globals)", "idx": 529}
{"namespace": "jinja2.environment.Template.render", "completion": "        if self.environment.is_async:\n            return self.render_async(*args, **kwargs)\n        return self.render_sync(*args, **kwargs)\n", "idx": 530}
{"namespace": "jinja2.utils.generate_lorem_ipsum", "completion": "    import random\n\n    words = [\n        \"lorem\",\n        \"ipsum\",\n        \"dolor\",\n        \"sit\",\n        \"amet\",\n        \"consectetur\",\n        \"adipiscing\",\n        \"elit\",\n        \"sed\",\n        \"do\",\n        \"eiusmod\",\n        \"tempor\",\n        \"incididunt\",\n        \"ut\",\n        \"labore\",\n        \"et\",\n        \"dolore\",\n        \"magna\",\n        \"aliqua\",\n        \"ut\",\n        \"enim\",\n        \"ad\",\n        \"minim\",\n        \"veniam\",\n        \"quis\",\n        \"nostrud\",\n        \"exercitation\",\n        \"ullamco\",\n        \"laboris\",\n        \"nisi\",\n        \"ut\",\n        \"aliquip\",\n        \"ex\",\n        \"ea\",\n        \"commodo\",\n        \"consequat\",\n        \"duis\",\n        \"aute\",\n        \"irure\",\n        \"dolor\",\n        \"in\",\n        \"reprehenderit\",\n        \"in\",\n        \"voluptate\",\n        \"velit\",\n        \"esse\",\n        \"cillum\",\n        \"dolore\",\n        \"eu\",\n        \"fugiat\",\n        \"nulla\",\n        \"pariatur\",\n        \"excepteur\",\n        \"sint\",\n        \"occaecat\",\n        \"cupidatat\",\n        \"non\",\n        \"proident\",\n        \"sunt\",\n        \"in\",\n        \"culpa\",\n        \"qui\",\n        \"officia\",\n        \"deserunt\",\n        \"mollit\",\n        \"anim\",\n        \"id\",\n        \"est\",\n        \"laborum\",\n    ]\n\n    paragraphs = []\n\n    for i in range(n):\n        num_words = random.randint(min, max)\n        paragraph = []\n\n        for j in range(num_words):\n            word = random.choice(words", "idx": 531}
{"namespace": "jinja2.utils.LRUCache.clear", "completion": "        self._mapping.clear()\n        self._queue.clear()\n", "idx": 532}
{"namespace": "jinja2.utils.LRUCache.items", "completion": "        return self._mapping.items()\n", "idx": 533}
{"namespace": "jinja2.idtracking.symbols_for_node", "completion": "    sym = Symbols(parent=parent_symbols)\n    visitor = FrameSymbolVisitor(sym)\n    visitor.visit(node)\n    return sym\n\n", "idx": 534}
{"namespace": "jinja2.idtracking.Symbols.find_ref", "completion": "        if name in self.refs:\n            return self.refs[name]\n\n        if self.parent is not None:\n            return self.parent.find_ref(name)\n\n        return None", "idx": 535}
{"namespace": "jinja2.idtracking.Symbols.dump_stores", "completion": "        rv = {}\n        for sym in self.stores:\n            rv[sym] = self.ref(sym)\n        if self.parent is not None:\n            rv.update(self.parent.dump_stores())\n        return rv\n\n", "idx": 536}
{"namespace": "jinja2.meta.find_undeclared_variables", "completion": "    generator = TrackingCodeGenerator(ast.environment)\n    generator.visit(ast)\n    return generator.undeclared_identifiers", "idx": 537}
{"namespace": "jinja2.loaders.split_template_path", "completion": "    if os.path.sep in template:\n        raise TemplateNotFound(template, \"Illegal path separators in '%s'\" % template)\n    if os.path.altsep and os.path.altsep in template:\n        raise TemplateNotFound(template, \"Illegal path separators in '%s'\" % template)\n    if \"..\" in template:\n        raise TemplateNotFound(template, \"Relative path up not allowed in '%s'\" % template)\n    return template.split(os.path.sep)\n\n", "idx": 538}
{"namespace": "jinja2.bccache.MemcachedBytecodeCache.load_bytecode", "completion": "        key = self.prefix + bucket.key\n        try:\n            bytecode = self.client.get(key)\n        except Exception as e:\n            if self.ignore_memcache_errors:\n                return\n            raise e\n        if bytecode is not None:\n            bucket.bytecode_from_string(bytecode)\n", "idx": 539}
{"namespace": "jinja2.bccache.MemcachedBytecodeCache.dump_bytecode", "completion": "        try:\n            self.client.set(\n                self.prefix + bucket.key, bucket.bytecode_to_string(), self.timeout\n            )\n        except Exception:\n            if not self.ignore_memcache_errors:\n                raise", "idx": 540}
{"namespace": "sumy.utils.get_stop_words", "completion": "    language = normalize_language(language)\n    path = \"stopwords/%s.txt\" % language\n    data = pkgutil.get_data(\"sumy\", path)\n\n    if data is None:\n        raise LookupError(\"Stop words for language '%s' not found\" % language)\n\n    return frozenset(to_unicode(line).strip() for line in data.splitlines())", "idx": 541}
{"namespace": "sumy._compat.to_bytes", "completion": "    if isinstance(object, bytes):\n        return object\n    elif isinstance(object, unicode):\n        return object.encode(\"utf-8\")\n    else:\n        return object.encode()", "idx": 542}
{"namespace": "sumy._compat.to_unicode", "completion": "    if isinstance(object, unicode):\n        return object\n    elif isinstance(object, bytes):\n        return object.decode(\"utf-8\")\n    else:\n        # try decode instance to unicode\n        return instance_to_unicode(object)", "idx": 543}
{"namespace": "sumy.summarizers.lsa.LsaSummarizer._create_dictionary", "completion": "        # Create a dictionary that maps each unique word in the document to its corresponding row index\n        dictionary = {}\n        for sentence in document.sentences:\n            for word in sentence.words:\n                normalized_word = self.normalize_word(word)\n                if not self.is_stop_word(normalized_word):\n                    if normalized_word not in dictionary:\n                        dictionary[normalized_word] = len(dictionary)\n        return dictionary\n", "idx": 544}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._get_content_words_in_sentence", "completion": "        words = [self.normalize_word(w) for w in sentence.words]\n        words = [w for w in words if w not in self.stop_words]\n        words = self._stem_words(words)\n        return words\n", "idx": 545}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._get_all_content_words_in_doc", "completion": "        all_words = self._get_all_words_in_doc(sentences)\n        normalized_words = self._normalize_words(all_words)\n        normalized_content_words = self._filter_out_stop_words(normalized_words)\n        return normalized_content_words\n", "idx": 546}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_tf", "completion": "        content_words = self._get_all_content_words_in_doc(sentences)\n        word_freq = self._compute_word_freq(content_words)\n        total_content_words = len(content_words)\n        tf = {}\n        for w in word_freq:\n            tf[w] = word_freq[w] / total_content_words\n        return tf\n", "idx": 547}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_ratings", "completion": "        sentences_as_words = [self._get_content_words_in_sentence(s) for s in sentences]\n        word_freq = self._compute_tf(sentences)\n        ratings = {}\n        for i in range(len(sentences)):\n            best_sentence_index = self._find_index_of_best_sentence(word_freq, sentences_as_words)\n            ratings[sentences[best_sentence_index]] = i * -1\n            word_freq = self._update_tf(word_freq, sentences_as_words[best_sentence_index])\n            del sentences[best_sentence_index]\n            del sentences_as_words[best_sentence_index]\n        return ratings", "idx": 548}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.cue_method", "completion": "        method = self._build_cue_method_instance(bonus_word_value, stigma_word_value)\n        ratings = method.rate_sentences(document)\n        return self._get_best_sentences(document.sentences, sentences_count, ratings)\n", "idx": 549}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.key_method", "completion": "        summarization_method = self._build_key_method_instance()\n        return summarization_method(document, sentences_count, weight)\n", "idx": 550}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.title_method", "completion": "        summarization_method = self._build_title_method_instance()\n        return summarization_method(document, sentences_count)\n", "idx": 551}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.location_method", "completion": "        summarization_method = self._build_location_method_instance()\n        return summarization_method(document, sentences_count, w_h, w_p1, w_p2, w_s1, w_s2)\n", "idx": 552}
{"namespace": "sumy.summarizers.reduction.ReductionSummarizer.rate_sentences", "completion": "        ratings = defaultdict(int)\n        for s1, s2 in combinations(document.sentences, 2):\n            if s1.sentence_number == s2.sentence_number:\n                continue\n            rating = self.rate_sentence_pair(s1, s2)\n            ratings[s1] += rating\n            ratings[s2] += rating\n        return ratings\n", "idx": 553}
{"namespace": "sumy.summarizers.reduction.ReductionSummarizer._to_words_set", "completion": "        words = [self.normalize_word(w) for w in sentence.words]\n        return frozenset(w for w in words if w not in self.stop_words)\n", "idx": 554}
{"namespace": "sumy.summarizers.text_rank.TextRankSummarizer._to_words_set", "completion": "        return set(self.normalize_word(word) for word in sentence.words if word not in self.stop_words)\n", "idx": 555}
{"namespace": "sumy.summarizers.kl.KLSummarizer.compute_tf", "completion": "        all_content_words = self._get_all_content_words_in_doc(sentences)\n        word_freq = self._compute_word_freq(all_content_words)\n        tf = {}\n        for w in word_freq:\n            tf[w] = word_freq[w] / len(all_content_words)\n        return tf\n", "idx": 556}
{"namespace": "sumy.evaluation.rouge._get_word_ngrams", "completion": "    assert type(n) == int and n > 0\n    assert len(sentences) > 0 and type(sentences) == list\n    words = _split_into_words(sentences)\n    return _get_ngrams(n, words)", "idx": 557}
{"namespace": "sumy.evaluation.rouge._len_lcs", "completion": "    table = [[0 for _ in range(len(y) + 1)] for _ in range(len(x) + 1)]\n\n    for i, x_i in enumerate(x):\n        for j, y_j in enumerate(y):\n            if x_i == y_j:\n                table[i + 1][j + 1] = table[i][j] + 1\n            else:\n                table[i + 1][j + 1] = max(table[i + 1][j], table[i][j + 1])\n\n    i, j = _get_index_of_lcs(x, y)\n    return table[i][j]\n\n", "idx": 558}
{"namespace": "sumy.evaluation.rouge._recon_lcs", "completion": "    table = _lcs(x, y)\n    n, m = _get_index_of_lcs(x, y)\n    return _recon_lcs_helper(x, y, n, m, table)\n\n", "idx": 559}
{"namespace": "sumy.evaluation.rouge._union_lcs", "completion": "    reference_words = _split_into_words([reference_sentence])\n    evaluated_words = _split_into_words(evaluated_sentences)\n    m = len(reference_words)\n    n = len(evaluated_words)\n    lcs = _len_lcs(evaluated_words, reference_words)\n    return lcs / m\n\n", "idx": 560}
{"namespace": "sumy.parsers.html.HtmlParser.from_file", "completion": "        with open(file_path, \"r\") as f:\n            return cls(f.read(), tokenizer, url)\n", "idx": 561}
{"namespace": "sumy.parsers.plaintext.PlaintextParser.document", "completion": "        document = ObjectDocumentModel()\n        paragraph = Paragraph()\n        document.paragraphs.append(paragraph)\n        for line in self._text.splitlines():\n            line = line.strip()\n            if not line:\n                paragraph = Paragraph()\n                document.paragraphs.append(paragraph)\n            elif line.isupper():\n                heading = Sentence(line)\n                paragraph.headings.append(heading)\n            else:\n                sentence = Sentence(line)\n                paragraph.sentences.append(sentence)\n        return document", "idx": 562}
{"namespace": "sumy.nlp.tokenizers.Tokenizer.to_sentences", "completion": "        # update abbreviations\n        self._sentence_tokenizer.abbrev_types.update(self.LANGUAGE_EXTRA_ABREVS.get(self._language, []))\n        return self._sentence_tokenizer.tokenize(paragraph)\n", "idx": 563}
{"namespace": "sumy.nlp.stemmers.null_stemmer", "completion": "    return unicode(object).lower()\n\n", "idx": 564}
{"namespace": "rows.fields.BinaryField.serialize", "completion": "        if value is None:\n            return \"\"\n        elif not isinstance(value, six.binary_type):\n            value_error(value, cls)\n        try:\n            return b64encode(value).decode(\"ascii\")\n        except binascii.Error:\n            return value", "idx": 565}
{"namespace": "rows.fields.BoolField.deserialize", "completion": "        value = super(BoolField, cls).deserialize(value, *args, **kwargs)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        elif isinstance(value, six.text_type):\n            value = value.lower()\n            if value in cls.TRUE_VALUES:\n                return True\n            elif value in cls.FALSE_VALUES:\n                return False\n            else:\n                raise ValueError(\"Value is not boolean\")\n        else:\n            value_error(value, cls)\n\n", "idx": 566}
{"namespace": "rows.fields.DateField.serialize", "completion": "        if value is None:\n            return \"\"\n        elif isinstance(value, cls.TYPE):\n            return value.strftime(cls.OUTPUT_FORMAT)\n        else:\n            value_error(value, cls)\n", "idx": 567}
{"namespace": "rows.fields.DateField.deserialize", "completion": "        value = super(DateField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n        try:\n            value = datetime.datetime.strptime(value, cls.INPUT_FORMAT).date()\n        except ValueError:\n            value_error(value, cls)\n        return value\n\n", "idx": 568}
{"namespace": "rows.fields.TextField.deserialize", "completion": "        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            return six.text_type(value)\n\n", "idx": 569}
{"namespace": "rows.fields.EmailField.deserialize", "completion": "        value = super(EmailField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n        if not cls.EMAIL_REGEXP.match(value):\n            value_error(value, cls)\n        return value\n\n", "idx": 570}
{"namespace": "rows.fields.JSONField.deserialize", "completion": "        value = super(JSONField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        try:\n            return json.loads(value)\n        except ValueError:\n            value_error(value, cls)\n\n", "idx": 571}
{"namespace": "falcon.http_error.HTTPError.to_json", "completion": "        if handler is None:\n            import json\n\n            handler = json\n\n        return handler.dumps(self.to_dict())", "idx": 572}
{"namespace": "falcon.inspect.inspect_app", "completion": "    # Get the routes\n    routes = inspect_routes(app)\n\n    # Get the static routes\n    static_routes = inspect_static_routes(app)\n\n    # Get the sinks\n    sinks = inspect_sinks(app)\n\n    # Get the error handlers\n    error_handlers = inspect_error_handlers(app)\n\n    # Get the middleware\n    middleware = inspect_middleware(app)\n\n    # Create the AppInfo object\n    app_info = AppInfo(routes, static_routes, sinks, error_handlers, middleware)\n\n    return app_info\n\n", "idx": 573}
{"namespace": "falcon.inspect.inspect_routes", "completion": "    routes = []\n    for route in app._router._routes:\n        if isinstance(route, CompiledRouter):\n            for r in route._routes:\n                routes.append(RouteInfo(r.uri_template, r.method_map))\n        else:\n            routes.append(RouteInfo(route.uri_template, route.method_map))\n    return routes\n\n", "idx": 574}
{"namespace": "falcon.inspect.inspect_static_routes", "completion": "    router = app._router\n    inspect_function = _supported_routers.get(type(router))\n    if inspect_function is None:\n        raise TypeError(\n            'Unsupported router class {}. Use \"register_router\" '\n            'to register a function that can inspect the router '\n            'used by the provided application'.format(type(router))\n        )\n    return inspect_function(router)", "idx": 575}
{"namespace": "falcon.inspect.inspect_sinks", "completion": "    sinks = []\n    for sink in app._sinks:\n        info = SinkInfo(sink._name, sink._stream)\n        sinks.append(info)\n    return sinks", "idx": 576}
{"namespace": "falcon.inspect.inspect_error_handlers", "completion": "    error_handlers = []\n    for status_code, handler in app._error_handlers.items():\n        source_info, name = _get_source_info_and_name(handler)\n        info = ErrorHandlerInfo(status_code, name, source_info)\n        error_handlers.append(info)\n    return error_handlers\n\n", "idx": 577}
{"namespace": "falcon.inspect.inspect_middleware", "completion": "    middleware = []\n    for fn in app._middleware:\n        source_info, name = _get_source_info_and_name(fn)\n        info = MiddlewareInfo(name, source_info, _is_internal(fn))\n        middleware.append(info)\n    return MiddlewareInfo(middleware)", "idx": 578}
{"namespace": "falcon.inspect.InspectVisitor.process", "completion": "        visit_name = instance.__visit_name__\n        visit_method = getattr(self, 'visit_' + visit_name, None)\n        if visit_method is None:\n            raise RuntimeError(\n                'No visit method found for {}'.format(instance.__visit_name__)\n            )\n        return visit_method(instance)\n\n", "idx": 579}
{"namespace": "falcon.request.Request.forwarded", "completion": "        if self._cached_forwarded is None:\n            self._cached_forwarded = helpers.parse_forwarded_header(\n                self.env.get('HTTP_FORWARDED', ''),\n            )\n\n        return self._cached_forwarded\n", "idx": 580}
{"namespace": "falcon.request.Request.client_accepts_msgpack", "completion": "        return self.client_accepts('application/x-msgpack') or self.client_accepts('application/msgpack')\n", "idx": 581}
{"namespace": "falcon.request.Request.content_length", "completion": "        try:\n            return int(self.env['CONTENT_LENGTH'])\n        except (KeyError, ValueError):\n            return None\n", "idx": 582}
{"namespace": "falcon.request.Request.bounded_stream", "completion": "        if self._bounded_stream is None:\n            self._bounded_stream = BoundedStream(\n                self.stream, self.content_length, self.env\n            )\n        return self._bounded_stream\n", "idx": 583}
{"namespace": "falcon.request.Request.uri", "completion": "        if self._cached_uri is None:\n            self._cached_uri = self.scheme + '://' + self.netloc + self.relative_uri\n        return self._cached_uri\n", "idx": 584}
{"namespace": "falcon.request.Request.forwarded_uri", "completion": "        if self._cached_forwarded_uri is None:\n            # PERF: For small numbers of items, '+' is faster\n            # than ''.join(...). Concatenation is also generally\n            # faster than formatting.\n            value = (\n                self.forwarded_scheme\n                + '://'\n                + self.forwarded_host\n                + self.relative_uri\n            )\n\n            self._cached_forwarded_uri = value\n\n        return self._cached_forwarded_uri\n", "idx": 585}
{"namespace": "falcon.request.Request.relative_uri", "completion": "        if self._cached_relative_uri is None:\n            # PERF: For small numbers of items, '+' is faster\n            # than ''.join(...). Concatenation is also generally\n            # faster than formatting.\n            value = self.app + self.path\n            if self.query_string:\n                value += '?' + self.query_string\n\n            self._cached_relative_uri = value\n\n        return self._cached_relative_uri\n", "idx": 586}
{"namespace": "falcon.request.Request.prefix", "completion": "        if self._cached_prefix is None:\n            # PERF: For small numbers of items, '+' is faster\n            # than ''.join(...). Concatenation is also generally\n            # faster than formatting.\n            value = self.scheme + '://' + self.netloc + self.app\n\n            self._cached_prefix = value\n\n        return self._cached_prefix\n", "idx": 587}
{"namespace": "falcon.request.Request.forwarded_prefix", "completion": "        if self._cached_forwarded_prefix is None:\n            self._cached_forwarded_prefix = (\n                self.forwarded_scheme + '://' + self.forwarded_host + self.app\n            )\n\n        return self._cached_forwarded_prefix\n", "idx": 588}
{"namespace": "falcon.request.Request.host", "completion": "        try:\n            host = self.env['HTTP_HOST']\n        except KeyError:\n            host = self.env['SERVER_NAME']\n\n        return host\n", "idx": 589}
{"namespace": "falcon.request.Request.subdomain", "completion": "        # PERF(kgriffs): .partition is slightly faster than .split\n        host = self.host\n        if '.' in host:\n            subdomain, sep, remainder = host.partition('.')\n            return subdomain\n        else:\n            return None\n", "idx": 590}
{"namespace": "falcon.request.Request.headers", "completion": "        if self._cached_headers is None:\n            self._cached_headers = {}\n            for key, value in self.env.items():\n                if key.startswith('HTTP_'):\n                    key = key[5:].replace('_', '-')\n                    self._cached_headers[key] = value\n\n        return self._cached_headers\n", "idx": 591}
{"namespace": "falcon.request.Request.remote_addr", "completion": "        try:\n            return self.env['REMOTE_ADDR']\n        except KeyError:\n            return '127.0.0.1'\n", "idx": 592}
{"namespace": "falcon.request.Request.client_accepts", "completion": "        # NOTE(kgriffs): Per RFC, a missing accept header is\n        # equivalent to '*/*'\n        try:\n            accept = self.env['HTTP_ACCEPT'] or '*/*'\n        except KeyError:\n            accept = '*/*'\n\n        # NOTE(kgriffs): Per RFC, a missing accept header is\n        # equivalent to '*/*'\n        if accept == '*/*':\n            return True\n\n        # NOTE(kgriffs): Per RFC, a missing accept header is\n        # equivalent to '*/*'\n        if accept == '*':\n            return True\n\n        # NOTE(kgriffs): Per RFC, a missing accept header is\n        # equivalent to '*/*'\n        if accept == '':\n            return True\n\n        # NOTE(kgriffs): Per RFC, a missing accept header is\n        # equivalent to '*/*'\n        if accept == 'text/*':\n            return True\n\n        # NOTE(kgriffs): Per RFC, a missing accept header is\n        # equivalent to '*/*'\n        if accept == 'text/plain':\n            return True\n\n        # NOTE(kgriffs): Per RFC, a missing accept header is\n        # equivalent to '*/*'\n        if accept == 'text/html':\n            return True\n\n        # NOTE(kgriffs): Per RFC, a missing accept header is\n        # equivalent to '*/*'\n        if accept == 'text/css':\n            return True\n\n        # NOTE(kgriffs): Per RFC, a missing accept header is\n        # equivalent to '*/*'\n        if accept == 'text/javascript':\n            return True\n\n        # NOTE(kgriffs): Per RFC, a missing accept header is\n        # equivalent to '*/*'\n        if accept == 'application/json':\n            return True\n\n        # NOTE(kgriffs): Per RFC, a missing accept header is\n        # equivalent to '*/*'\n        if accept == 'application/xml':\n            return True\n\n        # NOTE(kgriffs): Per R", "idx": 593}
{"namespace": "falcon.request.Request.client_prefers", "completion": "        accept = self.accept\n\n        # PERF(kgriffs): Usually the following will be true, so\n        # try it first.\n        if accept == '*/*':\n            return media_types[0]\n\n        # Fall back to full-blown parsing\n        try:\n            return mimeparse.best_match(media_types, accept)\n        except ValueError:\n            return None", "idx": 594}
{"namespace": "falcon.request.Request.get_header", "completion": "        # Convert the header name to uppercase and replace hyphens with underscores\n        name = name.upper().replace('-', '_')\n\n        # Try to retrieve the header value from the request environment\n        try:\n            value = self.env['HTTP_' + name]\n        except KeyError:\n            if required:\n                raise errors.HTTPBadRequest(\n                    'Missing required header', name, href='https://tools.ietf.org/html/rfc7230#section-3.2.2'\n                )\n            else:\n                return default\n\n        return value\n", "idx": 595}
{"namespace": "falcon.request.Request.get_cookie_values", "completion": "        if self._cookies is None:\n            header_value = self.get_header('Cookie')\n            if header_value:\n                self._cookies = helpers.parse_cookie_header(header_value)\n            else:\n                self._cookies = {}\n\n        return self._cookies.get(name)", "idx": 596}
{"namespace": "falcon.response.Response.unset_cookie", "completion": "        if not is_ascii_encodable(name):\n            raise KeyError('name is not ascii encodable')\n\n        if self._cookies is None:\n            self._cookies = http_cookies.SimpleCookie()\n\n        try:\n            self._cookies[name] = ''\n        except http_cookies.CookieError as e:  # pragma: no cover\n            # NOTE(tbug): we raise a KeyError here, to avoid leaking\n            # the CookieError to the user. SimpleCookie (well, BaseCookie)\n            # only throws CookieError on issues with the cookie key\n            raise KeyError(str(e))\n\n        if domain:\n            self._cookies[name]['domain'] = domain\n\n        if path:\n            self._cookies[name]['path'] = path\n\n        self._cookies[name]['expires'] = 'Thu, 01 Jan 1970 00:00:00 GMT'\n\n        self._cookies[name]['max-age'] = 0\n\n        self._cookies[name]['httponly'] = True\n\n        self._cookies[name]['secure'] = True\n\n        self._cookies[name]['samesite'] = 'None'", "idx": 597}
{"namespace": "falcon.response.Response.get_header", "completion": "        name = name.lower()\n        if name == 'set-cookie':\n            raise HeaderNotSupported(\n                'The \"Set-Cookie\" header is not supported by this function.'\n            )\n\n        if name in self._headers:\n            return self._headers[name]\n        else:\n            return default\n", "idx": 598}
{"namespace": "falcon.response.Response.set_header", "completion": "        # NOTE(kgriffs): normalize name by lowercasing it\n        name = name.lower()\n\n        if name == 'set-cookie':\n            raise HeaderNotSupported('Setting Set-Cookie is not currently supported.')\n\n        if self._extra_headers is None:\n            self._extra_headers = []\n\n        # NOTE(kgriffs): We need to do some conversions to ensure that\n        #   the header is set correctly.\n        if name == 'content-type':\n            # NOTE(kgriffs): If the value is a string, we need to\n            #   convert it to a tuple.\n            if isinstance(value, str):\n                value = (value, None)\n\n            # NOTE(kgriffs): If the value is a tuple, we need to\n            #   convert it to a string.\n            if isinstance(value, tuple):\n                value = value[0]\n\n            # NOTE(kgriffs): If the value is a string, we need to\n            #   convert it to a tuple.\n            if isinstance(value, str):\n                value = (value, None)\n\n            # NOTE(kgriffs): If the value is a tuple, we need to\n            #   convert it to a string.\n            if isinstance(value, tuple):\n                value = value[0]\n\n            # NOTE(kgriffs): If the value is a string, we need to\n            #   convert it to a tuple.\n            if isinstance(value, str):\n                value = (value, None)\n\n            # NOTE(kgriffs): If the value is a tuple, we need to\n            #   convert it to a string.\n            if isinstance(value, tuple):\n                value = value[0]\n\n            # NOTE(kgriffs): If the value is a string, we need to\n            #   convert it to a tuple.\n            if isinstance(value, str):\n                value = (value, None)\n\n            # NOTE(kgriffs): If the value is a tuple, we need to\n            #   convert it to a string.\n           ", "idx": 599}
{"namespace": "falcon.response.Response.delete_header", "completion": "        # NOTE(kgriffs): normalize name by lowercasing it\n        name = name.lower()\n\n        if name == 'set-cookie':\n            raise HeaderNotSupported('This method cannot be used to delete cookies')\n\n        self._headers.pop(name, None)", "idx": 600}
{"namespace": "falcon.cmd.inspect_app.route_main", "completion": "    parser = make_parser()\n    args = parser.parse_args()\n    app = load_app(parser, args)\n    if args.route_only:\n        inspect_routes(app, StringVisitor(args.verbose))\n    else:\n        inspect_app(app, StringVisitor(args.verbose, args.internal))", "idx": 601}
{"namespace": "falcon.util.uri.decode", "completion": "    if not encoded_uri:\n        return encoded_uri\n\n    if _cy_decode:\n        return _cy_decode(encoded_uri, unquote_plus)\n\n    tokens = encoded_uri.split('%')\n    if len(tokens) == 1:\n        return encoded_uri\n\n    return _join_tokens(tokens)", "idx": 602}
{"namespace": "falcon.util.structures.ETag.dumps", "completion": "        if self.is_weak:\n            return \"W/\" + str(self)\n        else:\n            return str(self)", "idx": 603}
{"namespace": "falcon.util.structures.ETag.loads", "completion": "        if etag_str.startswith('W/'):\n            etag_str = etag_str[2:]\n            etag = cls(etag_str[1:-1])\n            etag.is_weak = True\n        else:\n            etag = cls(etag_str[1:-1])\n\n        return etag", "idx": 604}
{"namespace": "falcon.util.misc.secure_filename", "completion": "    # Normalize the filename to the Unicode NKFD form.\n    filename = unicodedata.normalize('NFKD', filename)\n\n    # Replace any non-ASCII characters with an underscore.\n    filename = _UNSAFE_CHARS.sub('_', filename)\n\n    # If the filename starts with a period, replace the first period with an underscore.\n    if filename.startswith('.'):\n        filename = '_' + filename[1:]\n\n    return filename", "idx": 605}
{"namespace": "falcon.asgi.reader.BufferedReader.peek", "completion": "        if size == -1 or size is None:\n            return await self.pipe()\n\n        if size <= 0:\n            return b''\n\n        if size <= self._max_join_size:\n            if size <= self._buffer_len - self._buffer_pos:\n                return self._buffer[self._buffer_pos : self._buffer_pos + size]\n\n            # PERF(vytas): Do not join unless necessary.\n            result = []\n            async for chunk in self._iter_with_buffer(size):\n                result.append(chunk)\n            return b''.join(result)\n\n        # NOTE(vytas): size > self._max_join_size\n        if size <= self._buffer_len:\n            return self._buffer[self._buffer_pos : self._buffer_pos + size]\n\n        # PERF(vytas): Do not join unless necessary.\n        result = []\n        async for chunk in self._iter_with_buffer(size):\n            result.append(chunk)\n        return b''.join(result)\n", "idx": 606}
{"namespace": "falcon.asgi.reader.BufferedReader.read_until", "completion": "        return await self._read_from(\n            self._iter_delimited(delimiter, size_hint=size or 0), size\n        )\n", "idx": 607}
{"namespace": "falcon.routing.converters.IntConverter.convert", "completion": "        if self._num_digits is not None and len(value) != self._num_digits:\n            return None\n\n        if value.strip() != value:\n            return None\n\n        try:\n            value = int(value)\n        except ValueError:\n            return None\n\n        if self._min is not None and value < self._min:\n            return None\n\n        if self._max is not None and value > self._max:\n            return None\n\n        return value\n\n", "idx": 608}
{"namespace": "falcon.routing.converters.DateTimeConverter.convert", "completion": "        try:\n            return strptime(value, self._format_string)\n        except ValueError:\n            return None\n\n", "idx": 609}
{"namespace": "falcon.routing.util.map_http_methods", "completion": "    methods = {}\n    for method in constants.HTTP_METHODS:\n        if suffix is not None:\n            method_name = 'on_' + method.lower() + '_' + suffix\n        else:\n            method_name = 'on_' + method.lower()\n        if hasattr(resource, method_name):\n            methods[method] = getattr(resource, method_name)\n    return methods", "idx": 610}
{"namespace": "falcon.routing.static._BoundedFile.read", "completion": "        if size < 0:\n            size = self.remaining\n        data = self.fh.read(size)\n        self.remaining -= len(data)\n        return data", "idx": 611}
{"namespace": "authlib.oauth2.rfc6749.util.list_to_scope", "completion": "    if scope is None:\n        return None\n    if isinstance(scope, (set, tuple, list)):\n        scope = \" \".join(scope)\n    return to_unicode(scope)", "idx": 612}
{"namespace": "authlib.oauth2.rfc6749.util.extract_basic_authorization", "completion": "    auth_header = headers.get(\"Authorization\")\n    if not auth_header:\n        return None, None\n    auth_header = auth_header.strip()\n    if not auth_header.startswith(\"Basic \"):\n        return None, None\n    auth_token = auth_header[6:]\n    if \":\" not in auth_token:\n        return auth_token, None\n    try:\n        auth_token = base64.b64decode(auth_token)\n    except binascii.Error:\n        return None, None\n    username, password = auth_token.split(\":\", 1)\n    return username, password", "idx": 613}
{"namespace": "authlib.oauth2.rfc6749.parameters.prepare_grant_uri", "completion": "    params = {\n        'client_id': client_id,\n        'response_type': response_type,\n    }\n\n    if redirect_uri:\n        params['redirect_uri'] = redirect_uri\n\n    if scope:\n        params['scope'] = list_to_scope(scope)\n\n    if state:\n        params['state'] = state\n\n    params.update(kwargs)\n\n    uri = add_params_to_qs(uri, params)\n    return uri\n\n", "idx": 614}
{"namespace": "authlib.oauth2.rfc6749.parameters.parse_authorization_code_response", "completion": "    params = parse_qs(uri)\n    if 'code' not in params:\n        raise MissingCodeException()\n    if state and 'state' not in params:\n        raise MismatchingStateException()\n    if state and params['state'] != state:\n        raise MismatchingStateException()\n    return params", "idx": 615}
{"namespace": "authlib.oauth2.rfc6749.parameters.parse_implicit_response", "completion": "    fragment = urlparse.urlparse(uri).fragment\n    params = dict(urlparse.parse_qsl(fragment))\n\n    if 'access_token' not in params:\n        raise MissingException('access_token')\n\n    params_state = params.get('state')\n    if state and params_state != state:\n        raise MismatchingStateException()\n\n    return params", "idx": 616}
{"namespace": "authlib.common.encoding.json_b64encode", "completion": "    if isinstance(text, dict):\n        text = json_dumps(text)\n\n    return urlsafe_b64encode(to_bytes(text, charset='ascii'))\n\n", "idx": 617}
{"namespace": "authlib.jose.util.extract_header", "completion": "    header_segment = header_segment.strip()\n    if not header_segment:\n        raise error_cls('Invalid header string.')\n\n    try:\n        header_data = urlsafe_b64decode(header_segment)\n    except (TypeError, binascii.Error):\n        raise error_cls('Invalid header string.')\n\n    try:\n        header = json_loads(to_unicode(header_data))\n    except ValueError:\n        raise error_cls('Invalid header string.')\n\n    if not isinstance(header, dict):\n        raise error_cls('Invalid header string.')\n\n    return header", "idx": 618}
{"namespace": "twitter.models.TwitterModel.AsDict", "completion": "        d = {}\n        for k, v in self.__dict__.items():\n            if k.startswith('_'):\n                continue\n            if isinstance(v, list):\n                d[k] = [x.AsDict() if hasattr(x, 'AsDict') else x for x in v]\n            elif isinstance(v, tuple):\n                d[k] = tuple(x.AsDict() if hasattr(x, 'AsDict') else x for x in v)\n            elif isinstance(v, set):\n                d[k] = set(x.AsDict() if hasattr(x, 'AsDict') else x for x in v)\n            elif isinstance(v, TwitterModel):\n                d[k] = v.AsDict()\n            else:\n                d[k] = v\n        return d", "idx": 619}
{"namespace": "twitter.models.TwitterModel.NewFromJsonDict", "completion": "        return cls(**data)", "idx": 620}
{"namespace": "twitter.api.Api._TweetTextWrap", "completion": "        # Split the status into words\n        words = status.split()\n\n        # Check if any word exceeds the character limit\n        for word in words:\n            if len(word) > char_lim:\n                raise TwitterError(\"Word '{0}' exceeds the character limit of {1}\".format(word, char_lim))\n\n        # Initialize the list of tweets\n        tweets = []\n\n        # Initialize the current line\n        line = ''\n\n        # Loop through the words\n        for word in words:\n            # Calculate the length of the line by adding the length of the current word\n            line_len = len(line) + len(word)\n\n            # If the line length exceeds the character limit, append the line to the list of tweets and start a new line with the current word\n            if line_len > char_lim:\n                tweets.append(line)\n                line = word\n\n            # If the line length does not exceed the character limit, add the word to the line\n            else:\n                line += ' ' + word\n\n        # Append the last line to the list of tweets\n        tweets.append(line)\n\n        # Return the list of tweets\n        return tweets", "idx": 621}
{"namespace": "databases.importer.import_from_string", "completion": "    module_str, _, attrs_str = import_str.partition(\":\")\n    if not module_str or not attrs_str:\n        raise ImportFromStringError(\n            f\"Invalid import string '{import_str}'. Must be in format '<module>:<attribute>'\"\n        )\n\n    module = importlib.import_module(module_str)\n    return getattr(module, attrs_str)", "idx": 622}
{"namespace": "rest_framework.reverse.reverse", "completion": "    if viewname is None:\n        return None\n\n    if args is None:\n        args = []\n    if kwargs is None:\n        kwargs = {}\n\n    url = django_reverse(viewname, args=args, kwargs=kwargs, **extra)\n\n    return preserve_builtin_query_params(url, request=request)", "idx": 623}
{"namespace": "rest_framework.serializers.Serializer.fields", "completion": "        # We're using a cached_property here because the fields are only\n        # accessed once, and then cached for the lifetime of the instance.\n        # This means that we can avoid the overhead of calling\n        # `get_fields()` on every access.\n        return self.get_fields()\n", "idx": 624}
{"namespace": "rest_framework.parsers.JSONParser.parse", "completion": "        parser_context = parser_context or {}\n        encoding = parser_context.get('encoding', settings.DEFAULT_CHARSET)\n        try:\n            data = stream.read().decode(encoding)\n            return self.renderer_class().parse(data)\n        except ValueError as exc:\n            raise ParseError('JSON parse error - %s' % str(exc))", "idx": 625}
{"namespace": "rest_framework.parsers.FileUploadParser.get_filename", "completion": "        parser_context = parser_context or {}\n        request = parser_context['request']\n        filename = request.GET.get('filename')\n        if filename:\n            return filename\n        content_disposition = request.META.get('HTTP_CONTENT_DISPOSITION')\n        if content_disposition:\n            filename = content_disposition.split(';')[1].split('=')[1].strip('\"')\n            return filename\n        return None", "idx": 626}
{"namespace": "rest_framework.fields.is_simple_callable", "completion": "    if not callable(obj):\n        return False\n\n    if isinstance(obj, (staticmethod, classmethod)):\n        obj = obj.__func__\n\n    if isinstance(obj, functools.partial):\n        obj = obj.func\n\n    if isinstance(obj, type(lambda: None)):\n        return True\n\n    if isinstance(obj, type(lambda: None).__func__):\n        return True\n\n    if isinstance(obj, type(lambda: None).__func__.__func__):\n        return True\n\n    if isinstance(obj, type(lambda: None).__func__.__func__.__func__):\n        return True\n\n    if isinstance(obj, type(lambda: None).__func__.__func__.__func__.__func__):\n        return True\n\n    if isinstance(obj, type(lambda: None).__func__.__func__.__func__.__func__.__func__):\n        return True\n\n    if isinstance(obj, type(lambda: None).__func__.__func__.__func__.__func__.__func__.__func__):\n        return True\n\n    if isinstance(obj, type(lambda: None).__func__.__func__.__func__.__func__.__func__.__func__.__func__):\n        return True\n\n    if isinstance(obj, type(lambda: None).__func__.__func__.__func__.__func__.__func__.__func__.__func__.__func__):\n        return True\n\n    if isinstance(obj, type(lambda: None).__func__.__func__.__func__.__func__.__func__.__func__.__func__.__func__.__func__):\n        return True\n\n    if isinstance(obj, type(lambda: None).__func__.__func__.__func__.__func__.__func__.__func__.__func__.__func__.__func__.__func__):\n        return True\n\n    if isinstance(obj, type(lambda: None).__func__.__func__.__func__.__func__.__func__.__func__.__func__.__func__.__func__.__func__.__func__):\n        return True", "idx": 627}
{"namespace": "rest_framework.fields.Field.bind", "completion": "        self.field_name = field_name\n        self.parent = parent\n", "idx": 628}
{"namespace": "rest_framework.fields.Field.run_validation", "completion": "        if data is empty:\n            return data\n\n        value = self.to_internal_value(data)\n        self.run_validators(value)\n        return value\n", "idx": 629}
{"namespace": "rest_framework.fields.Field.root", "completion": "        s = self\n        while s.parent:\n            s = s.parent\n        return s\n", "idx": 630}
{"namespace": "rest_framework.fields.CharField.run_validation", "completion": "        if data == '' and not self.allow_blank:\n            self.fail('blank')\n        return super().run_validation(data)\n", "idx": 631}
{"namespace": "rest_framework.fields.CharField.to_internal_value", "completion": "        if isinstance(data, bool):\n            raise ValidationError('Boolean values are not allowed.')\n        if not isinstance(data, (str, int, float)):\n            raise ValidationError('Invalid input type.')\n        if self.trim_whitespace:\n            data = str(data).strip()\n        return str(data)", "idx": 632}
{"namespace": "rest_framework.fields.DecimalField.to_internal_value", "completion": "        if isinstance(data, str) and len(data) > self.MAX_STRING_LENGTH:\n            self.fail('max_string_length')\n\n        try:\n            if self.coerce_to_string:\n                data = str(data)\n            else:\n                data = decimal.Decimal(data)\n        except (TypeError, ValueError):\n            self.fail('invalid')\n\n        if self.max_digits is not None and self.decimal_places is not None:\n            if data.as_tuple().exponent > self.max_digits - self.decimal_places:\n                self.fail('max_decimal_places')\n            if data.as_tuple().digits > self.max_whole_digits:\n                self.fail('max_whole_digits')\n\n        if self.rounding is not None:\n            data = data.quantize(decimal.Decimal('.1') ** self.decimal_places, rounding=self.rounding)\n\n        return data\n", "idx": 633}
{"namespace": "rest_framework.fields.DateTimeField.to_representation", "completion": "        output_format = getattr(self, 'format', api_settings.DATETIME_OUTPUT_FORMAT)\n\n        if value is None:\n            return None\n\n        if output_format is None or isinstance(value, str):\n            return value\n\n        value = self.enforce_timezone(value)\n        return value.strftime(output_format)", "idx": 634}
{"namespace": "rest_framework.fields.ChoiceField.iter_options", "completion": "        return iter_options(self.choice_strings_to_values, self.html_cutoff, self.html_cutoff_text)", "idx": 635}
{"namespace": "rest_framework.fields.MultipleChoiceField.get_value", "completion": "        if self.field_name not in dictionary:\n            if getattr(self.root, 'partial', False):\n                return empty\n            return self.default_empty_html\n        ret = dictionary[self.field_name]\n        if html.is_html_input(dictionary):\n            if ret == '' and self.allow_empty:\n                return []\n            if ret == '' and not self.required:\n                return empty\n            if isinstance(ret, list):\n                return ret\n            if isinstance(ret, str):\n                return ret.split(',')\n            self.fail('not_a_list', input_type=type(ret).__name__)\n        return ret\n", "idx": 636}
{"namespace": "rest_framework.exceptions._get_error_details", "completion": "    if isinstance(data, list):\n        ret = [_get_error_details(item, default_code) for item in data]\n        if isinstance(data, list) and len(ret) == 1:\n            return ret[0]\n        return ret\n    elif isinstance(data, dict):\n        ret = {\n            key: _get_error_details(value, default_code)\n            for key, value in data.items()\n        }\n        if isinstance(data, dict) and len(ret) == 1:\n            return list(ret.values())[0]\n        return ret\n    elif isinstance(data, Exception):\n        return _get_error_details(str(data), default_code)\n    return _get_error_details_from_error_string(data, default_code)\n\n", "idx": 637}
{"namespace": "rest_framework.exceptions.server_error", "completion": "    return JsonResponse({'error': 'Server Error'}, status=500)", "idx": 638}
{"namespace": "rest_framework.exceptions.bad_request", "completion": "    data = {\n        'error': 'Bad Request (400)'\n    }\n    return JsonResponse(data, status=status.HTTP_400_BAD_REQUEST)", "idx": 639}
{"namespace": "rest_framework.relations.RelatedField.iter_options", "completion": "        queryset = self.get_queryset()\n        if queryset is None:\n            # Ensure that field.choices returns something sensible\n            # even when accessed with a read-only field.\n            return\n\n        if self.html_cutoff is not None:\n            queryset = queryset[:self.html_cutoff]\n\n        for item in queryset:\n            yield self.display_value(item), self.to_representation(item)\n", "idx": 640}
{"namespace": "rest_framework.relations.PrimaryKeyRelatedField.to_internal_value", "completion": "        if data is None:\n            self.fail('required')\n\n        if not isinstance(data, (int, str)):\n            self.fail('incorrect_type', data_type=type(data).__name__)\n\n        try:\n            pk = self.pk_field.to_internal_value(data)\n        except ValidationError as exc:\n            raise ValidationError({self.source_field_name: exc.detail})\n\n        queryset = self.get_queryset()\n        filter_kwargs = {self.pk_field.source_field_name: pk}\n        try:\n            return queryset.get(**filter_kwargs)\n        except ObjectDoesNotExist:\n            self.fail('does_not_exist', pk_value=pk)\n        except (ValueError, TypeError) as exc:\n            raise ValidationError({self.source_field_name: str(exc)})\n", "idx": 641}
{"namespace": "rest_framework.relations.PrimaryKeyRelatedField.to_representation", "completion": "        if self.pk_field is not None:\n            return self.pk_field.to_representation(value.pk)\n        return value.pk", "idx": 642}
{"namespace": "rest_framework.relations.SlugRelatedField.to_internal_value", "completion": "        queryset = self.get_queryset()\n        try:\n            return queryset.get(**{self.slug_field: data})\n        except ObjectDoesNotExist:\n            self.fail('does_not_exist', slug_name=self.slug_field, value=data)\n        except (TypeError, ValueError):\n            self.fail('invalid')\n", "idx": 643}
{"namespace": "rest_framework.templatetags.rest_framework.add_query_param", "completion": "    url = request.get_full_path()\n    url = iri_to_uri(url)\n    url = re.sub(r'%s=[^&]+&' % key, '', url)\n    url = re.sub(r'%s=[^&]+' % key, '', url)\n    url = re.sub(r'&%s=[^&]+' % key, '', url)\n    url = re.sub(r'&%s=[^&]+&' % key, '', url)\n    url = re.sub(r'&%s=[^&]+$' % key, '', url)\n    url = re.sub(r'%s=[^&]+$' % key, '', url)\n    url = re.sub(r'&$', '', url)\n    url = re.sub(r'^&', '', url)\n    if url and url[-1] == '?':\n        url = url[:-1]\n    if url and url[-1] == '&':\n        url = url[:-1]\n    if url and url[-1] == '&':\n        url = url[:-1]\n    if url and url[-1] == '?':\n        url = url[:-1]\n    if url and url[-1] == '&':\n        url = url[:-1]\n    if url and url[-1] == '&':\n        url = url[:-1]\n    if url and url[-1] == '?':\n        url = url[:-1]\n    if url and url[-1] == '&':\n        url = url[:-1]\n    if url and url[-1] == '?':\n        url = url[:-1]\n    if url and url[-1] == '&':\n        url = url[:-1]\n    if url and url[-1] == '?':\n        url = url[:-1]\n    if url and url[-1] == '&':\n        url = url[:-1]\n    if url and url[-1] == '?':\n        url = url[:-1]\n   ", "idx": 644}
{"namespace": "rest_framework.utils.mediatypes._MediaType.match", "completion": "        if self.main_type != other.main_type:\n            return False\n        if self.sub_type != other.sub_type:\n            if self.sub_type != '*' and other.sub_type != '*':\n                return False\n        if self.params != other.params:\n            return False\n        return True\n", "idx": 645}
{"namespace": "rest_framework.utils.mediatypes._MediaType.precedence", "completion": "        if self.main_type == '*' and self.sub_type == '*':\n            return 0\n        elif self.main_type == '*':\n            return 1\n        elif self.sub_type == '*':\n            return 2\n        else:\n            return 3", "idx": 646}
{"namespace": "rest_framework.utils.mediatypes._MediaType.__str__", "completion": "        ret = self.main_type + '/' + self.sub_type\n        if self.params:\n            ret += '; ' + '; '.join(\n                '%s=%s' % (key, val) for key, val in sorted(self.params.items())\n            )\n        return ret", "idx": 647}
{"namespace": "asyncpg._testbase.TestCase.assertLoopErrorHandlerCalled", "completion": "        def handler(loop, context):\n            self.__unhandled_exceptions.append(context)\n\n        self.loop.set_exception_handler(handler)\n        try:\n            yield\n        finally:\n            self.loop.set_exception_handler(None)\n\n        for context in self.__unhandled_exceptions:\n            if re.search(msg_re, self._format_loop_exception(context, 1)):\n                return\n\n        raise AssertionError(\n            'no exception logged matching \"{}\"'.format(msg_re))\n", "idx": 648}
{"namespace": "csvs_to_sqlite.utils.refactor_dataframes", "completion": "    # Create a dictionary of lookup tables for each foreign key\n    lookup_tables = {}\n    for column, (table_name, value_column) in foreign_keys.items():\n        lookup_tables[column] = LookupTable(conn, table_name, value_column, index_fts)\n\n    # Iterate over the dataframes and apply the lookup tables to each foreign key column\n    for df in dataframes:\n        for column, (table_name, value_column) in foreign_keys.items():\n            if column in df.columns:\n                df[column] = df[column].apply(lookup_tables[column].id_for_value)\n\n    return dataframes", "idx": 649}
{"namespace": "sqlitedict.SqliteDict.iteritems", "completion": "        GET_ITEMS = 'SELECT key, value FROM \"%s\" ORDER BY rowid' % self.tablename\n        for key, value in self.conn.select(GET_ITEMS):\n            yield self.decode_key(key), self.decode(value)\n", "idx": 650}
{"namespace": "sqlitedict.SqliteDict.update", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to update read-only SqliteDict')\n\n        if isinstance(items, dict):\n            items = items.items()\n\n        for key, value in items:\n            self[key] = value\n\n        for key, value in kwds.items():\n            self[key] = value\n\n        if self.autocommit:\n            self.commit()", "idx": 651}
{"namespace": "sqlitedict.SqliteDict.clear", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to clear read-only SqliteDict')\n\n        CLEAR_TABLE = 'DELETE FROM \"%s\"' % self.tablename\n        self.conn.execute(CLEAR_TABLE)\n        if self.autocommit:\n            self.commit()\n", "idx": 652}
{"namespace": "sqlitedict.SqliteDict.commit", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to commit read-only SqliteDict')\n\n        if blocking:\n            self.conn.commit()\n        else:\n            self.conn.queue_commit()", "idx": 653}
{"namespace": "sqlitedict.SqliteDict.terminate", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to delete read-only SqliteDict')\n        self.close()\n        if self.filename != ':memory:':\n            try:\n                os.remove(self.filename)\n            except Exception:\n                pass", "idx": 654}
{"namespace": "boto.utils.retry_url", "completion": "    if not timeout:\n        timeout = 10\n\n    for i in range(num_retries):\n        try:\n            proxy_handler = urllib.request.ProxyHandler({})\n            opener = urllib.request.build_opener(proxy_handler)\n            opener.addheaders = [('User-agent', 'boto')]\n            response = opener.open(url, timeout=timeout)\n            return response.read()\n        except urllib.error.HTTPError as e:\n            if e.code == 404 and retry_on_404:\n                continue\n            else:\n                raise\n\n", "idx": 655}
{"namespace": "boto.utils.LazyLoadMetadata.values", "completion": "        self._materialize()\n        return super(LazyLoadMetadata, self).values()\n\n", "idx": 656}
{"namespace": "boto.utils.get_instance_userdata", "completion": "    try:\n        userdata_url = _build_instance_metadata_url(url, version, 'user-data')\n        userdata = retry_url(userdata_url, num_retries=num_retries, timeout=timeout)\n        if sep:\n            userdata = dict(item.split(sep, 1) for item in userdata.split('\\n') if item)\n        return userdata\n    except urllib.error.URLError:\n        boto.log.exception(\"Exception caught when trying to retrieve \"\n                           \"instance userdata\")\n        return None", "idx": 657}
{"namespace": "boto.utils.pythonize_name", "completion": "    s1 = _first_cap_regex.sub(r'\\1_\\2', name)\n    return _number_cap_regex.sub(r'\\1_\\2', s1).lower()", "idx": 658}
{"namespace": "boto.cloudsearchdomain.connect_to_region", "completion": "    region = RegionInfo('cloudsearchdomain', region_name)\n    return region.connect(**kw_params)", "idx": 659}
{"namespace": "boto.redshift.connect_to_region", "completion": "    from boto.redshift.layer1 import RedshiftConnection\n    return RedshiftConnection(region=region_name, **kw_params)", "idx": 660}
{"namespace": "boto.support.connect_to_region", "completion": "    from boto.support.layer1 import SupportConnection\n    return SupportConnection(region=region_name, **kw_params)", "idx": 661}
{"namespace": "boto.configservice.connect_to_region", "completion": "    from boto.configservice.layer1 import ConfigServiceConnection\n    return ConfigServiceConnection(region=region_name, **kw_params)", "idx": 662}
{"namespace": "boto.cloudhsm.connect_to_region", "completion": "    from boto.cloudhsm.layer1 import CloudHSMConnection\n    return CloudHSMConnection(region_name, **kw_params)", "idx": 663}
{"namespace": "boto.logs.connect_to_region", "completion": "    from boto.logs.layer1 import CloudWatchLogsConnection\n    return CloudWatchLogsConnection(region_name, **kw_params)", "idx": 664}
{"namespace": "boto.cloudsearch.connect_to_region", "completion": "    from boto.cloudsearch.layer1 import Layer1\n    return Layer1.connect_to_region(region_name, **kw_params)", "idx": 665}
{"namespace": "boto.glacier.job.Job.download_to_fileobj", "completion": "        num_chunks = self._calc_num_chunks(chunk_size)\n        self._download_to_fileob(output_file, num_chunks, chunk_size,\n                                 verify_hashes, retry_exceptions)\n", "idx": 666}
{"namespace": "boto.glacier.utils.minimum_part_size", "completion": "    if size_in_bytes <= default_part_size:\n        return default_part_size\n\n    if size_in_bytes > MAXIMUM_NUMBER_OF_PARTS * default_part_size:\n        raise ValueError(\"File size exceeds the maximum allowed archive size of 10,000 * 4GB\")\n\n    return int(math.ceil(size_in_bytes / float(MAXIMUM_NUMBER_OF_PARTS)))\n\n", "idx": 667}
{"namespace": "boto.glacier.utils.chunk_hashes", "completion": "    if not isinstance(bytestring, six.binary_type):\n        raise TypeError(\"bytestring must be a binary string\")\n    if not isinstance(chunk_size, int):\n        raise TypeError(\"chunk_size must be an integer\")\n    if chunk_size <= 0:\n        raise ValueError(\"chunk_size must be greater than 0\")\n    if chunk_size > len(bytestring):\n        raise ValueError(\"chunk_size must be less than or equal to the length of the bytestring\")\n    if len(bytestring) == 0:\n        return [hashlib.sha256(b\"\").digest()]\n    hashes = []\n    for i in range(0, len(bytestring), chunk_size):\n        hashes.append(hashlib.sha256(bytestring[i:i + chunk_size]).digest())\n    return hashes", "idx": 668}
{"namespace": "boto.glacier.utils.compute_hashes_from_fileobj", "completion": "    linear_hash = hashlib.sha256()\n    tree_hash = hashlib.sha256()\n    tree_hash_chunks = []\n    while True:\n        data = fileobj.read(chunk_size)\n        if not data:\n            break\n        linear_hash.update(data)\n        tree_hash_chunks.append(hashlib.sha256(data).digest())\n    tree_hash.update(tree_hash_chunks[0])\n    for chunk in tree_hash_chunks[1:]:\n        tree_hash.update(chunk)\n    return linear_hash.hexdigest(), tree_hash.hexdigest()", "idx": 669}
{"namespace": "boto.glacier.concurrent.ConcurrentTransferer._calculate_required_part_size", "completion": "        # Calculate the number of parts required for the data\n        num_parts = int(math.ceil(total_size / float(self._part_size)))\n\n        # Calculate the final part size to be used for concurrent transfer\n        final_part_size = int(math.ceil(total_size / float(num_parts)))\n\n        return num_parts, final_part_size\n", "idx": 670}
{"namespace": "boto.glacier.connect_to_region", "completion": "    from boto.glacier.layer2 import Layer2\n    return Layer2(region_name, **kw_params)", "idx": 671}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.update", "completion": "        if self.id is None:\n            raise ValueError('NetworkInterface.update() requires an ENI ID')\n\n        params = {'NetworkInterfaceId': self.id}\n        if dry_run:\n            params['DryRun'] = True\n\n        try:\n            response = self.connection.get_all_network_interfaces(**params)\n        except BotoClientError, e:\n            if e.error_code == 'InvalidNetworkInterfaceID.NotFound':\n                if validate:\n                    raise ValueError('NetworkInterface.update() requires a valid ENI ID')\n                else:\n                    return\n            else:\n                raise\n\n        if len(response) == 0:\n            if validate:\n                raise ValueError('NetworkInterface.update() requires a valid ENI ID')\n            else:\n                return\n\n        updated = response[0]\n        self._update(updated)\n        return self.status", "idx": 672}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.attach", "completion": "        try:\n            self.connection.attach_network_interface(\n                self.id,\n                instance_id,\n                device_index,\n                dry_run=dry_run\n            )\n            return True\n        except BotoClientError as e:\n            if e.error_code == 'InvalidNetworkInterfaceID.NotFound':\n                raise ValueError('%s is not a valid ENI ID' % self.id)\n            elif e.error_code == 'InvalidInstanceID.NotFound':\n                raise ValueError('%s is not a valid instance ID' % instance_id)\n            elif e.error_code == 'InvalidNetworkInterfaceID.IncorrectState':\n                raise ValueError('%s is not in available state' % self.id)\n            elif e.error_code == 'InvalidInstanceID.Malformed':\n                raise ValueError('%s is not a valid instance ID' % instance_id)\n            elif e.error_code == 'InvalidDeviceIndex.Malformed':\n                raise ValueError('%s is not a valid device index' % device_index)\n            elif e.error_code == 'InvalidDeviceIndex.NotFound':\n                raise ValueError('%s is not a valid device index' % device_index)\n            elif e.error_code == 'InvalidNetworkInterfaceID.IncorrectState':\n                raise ValueError('%s is not in available state' % self.id)\n            elif e.error_code == 'InvalidNetworkInterfaceID.IncorrectState':\n                raise ValueError('%s is not in available state' % self.id)\n            elif e.error_code == 'InvalidNetworkInterfaceID.IncorrectState':\n                raise ValueError('%s is not in available state' % self.id)\n            elif e.error_code == 'InvalidNetworkInterfaceID.IncorrectState':\n                raise ValueError('%s is not in available state' % self.id)\n            elif e.error_code == 'InvalidNetworkInterfaceID.IncorrectState':\n                raise ValueError('%s is not in available state' % self.id)\n            elif e.error_code == 'InvalidNetworkInterfaceID.IncorrectState':\n                raise ValueError('%", "idx": 673}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.detach", "completion": "        return self.connection.detach_network_interface(self.id, force, dry_run)", "idx": 674}
{"namespace": "boto.ec2.address.Address.release", "completion": "        params = {}\n        if dry_run:\n            params['DryRun'] = 'true'\n        return self.connection.release_address(self.public_ip, **params)", "idx": 675}
{"namespace": "boto.ec2.address.Address.associate", "completion": "        if self.allocation_id:\n            return self.connection.associate_address(\n                allocation_id=self.allocation_id,\n                instance_id=instance_id,\n                network_interface_id=network_interface_id,\n                private_ip_address=private_ip_address,\n                allow_reassociation=allow_reassociation,\n                dry_run=dry_run)\n        else:\n            return self.connection.associate_address(\n                public_ip=self.public_ip,\n                instance_id=instance_id,\n                network_interface_id=network_interface_id,\n                private_ip_address=private_ip_address,\n                allow_reassociation=allow_reassociation,\n                dry_run=dry_run)\n", "idx": 676}
{"namespace": "boto.ec2.address.Address.disassociate", "completion": "        if self.allocation_id:\n            return self.connection.disassociate_address(\n                allocation_id=self.allocation_id,\n                dry_run=dry_run\n            )\n        else:\n            return self.connection.disassociate_address(\n                public_ip=self.public_ip,\n                dry_run=dry_run\n            )", "idx": 677}
{"namespace": "boto.ec2.ec2object.TaggedEC2Object.add_tags", "completion": "        self.tags.add_tags(tags, dry_run)", "idx": 678}
{"namespace": "boto.ec2.ec2object.TaggedEC2Object.remove_tags", "completion": "        status = self.connection.delete_tags(\n            [self.id],\n            tags,\n            dry_run=dry_run\n        )\n        if self.tags is None:\n            self.tags = TagSet()\n        self.tags.remove(tags)", "idx": 679}
{"namespace": "boto.ec2.connection.EC2Connection.get_all_instance_status", "completion": "        params = {}\n        if instance_ids:\n            self.build_list_params(params, instance_ids, 'InstanceId')\n        if filters:\n            self.build_filter_params(params, filters)\n        if dry_run:\n            params['DryRun'] = 'true'\n        if max_results is not None:\n            params['MaxResults'] = max_results\n        if next_token:\n            params['NextToken'] = next_token\n        if include_all_instances:\n            params['IncludeAllInstances'] = 'true'\n        return self.get_list('DescribeInstanceStatus', params,\n                             [('item', Instance)], verb='POST')", "idx": 680}
{"namespace": "boto.ec2.volume.Volume.update", "completion": "        if self.id is None:\n            raise ValueError('Cannot update a volume that does not exist')\n\n        params = {'VolumeId': self.id}\n        if dry_run:\n            params['DryRun'] = 'true'\n\n        result = self.connection.get_all_volumes(volume_ids=[self.id],\n                                                 dry_run=dry_run)\n        if len(result) == 0:\n            if validate:\n                raise ValueError('Volume %s does not exist' % self.id)\n            else:\n                return\n\n        self._update(result[0])\n        return self.status", "idx": 681}
{"namespace": "boto.ec2.volume.Volume.attach", "completion": "        return self.connection.attach_volume(self.id, instance_id, device, dry_run=dry_run)\n", "idx": 682}
{"namespace": "boto.ec2.volume.Volume.detach", "completion": "        return self.connection.detach_volume(self.id, force, dry_run)", "idx": 683}
{"namespace": "boto.ec2.volume.Volume.create_snapshot", "completion": "        return self.connection.create_snapshot(self.id, description, dry_run)", "idx": 684}
{"namespace": "boto.ec2.volume.Volume.attachment_state", "completion": "        if self.attach_data:\n            return self.attach_data.state\n        else:\n            return None\n", "idx": 685}
{"namespace": "boto.ec2.securitygroup.SecurityGroup.add_rule", "completion": "        if self.vpc_id:\n            return self.connection.authorize_security_group_ingress(\n                group_id=self.id,\n                ip_protocol=ip_protocol,\n                from_port=from_port,\n                to_port=to_port,\n                src_group_name=src_group_name,\n                src_group_owner_id=src_group_owner_id,\n                cidr_ip=cidr_ip,\n                src_group_group_id=src_group_group_id,\n                dry_run=dry_run\n            )\n        else:\n            return self.connection.authorize_security_group_ingress(\n                group_name=self.name,\n                ip_protocol=ip_protocol,\n                from_port=from_port,\n                to_port=to_port,\n                src_group_name=src_group_name,\n                src_group_owner_id=src_group_owner_id,\n                cidr_ip=cidr_ip,\n                src_group_group_id=src_group_group_id,\n                dry_run=dry_run\n            )\n", "idx": 686}
{"namespace": "boto.ec2.connect_to_region", "completion": "    region = RegionData.get(region_name)\n    if region is None:\n        return None\n    return region.connect(**kw_params)", "idx": 687}
{"namespace": "boto.ec2.cloudwatch.connect_to_region", "completion": "    region = RegionInfo(name=region_name, connection_cls=CloudWatchConnection, **RegionData[region_name])\n    return region.connect(**kw_params)", "idx": 688}
{"namespace": "boto.ec2.autoscale.connect_to_region", "completion": "    region = RegionInfo(name=region_name, connection_cls=AutoScaleConnection, **RegionData[region_name])\n    return region.connect(**kw_params)", "idx": 689}
{"namespace": "boto.ec2.elb.connect_to_region", "completion": "    region = RegionInfo(name=region_name, connection_cls=ELBConnection, **RegionData[region_name])\n    return region.connect(**kw_params)", "idx": 690}
{"namespace": "boto.ec2.elb.ELBConnection.get_all_load_balancers", "completion": "        params = {}\n        if load_balancer_names:\n            self.build_list_params(params, load_balancer_names, 'LoadBalancerNames.member.%s')\n        if marker:\n            params['Marker'] = marker\n        return self.get_all('DescribeLoadBalancers', params,\n                            key='DescribeLoadBalancersResult.LoadBalancerDescriptions',\n                            obj=LoadBalancer)\n", "idx": 691}
{"namespace": "boto.ec2.elb.ELBConnection.disable_availability_zones", "completion": "        params = {'LoadBalancerName': load_balancer_name}\n        self.build_list_params(params, zones_to_remove,\n                               'AvailabilityZones.member.%d')\n        obj = self.get_object('DisableAvailabilityZonesForLoadBalancer',\n                              params, LoadBalancerZones)\n        return obj.zones", "idx": 692}
{"namespace": "boto.awslambda.connect_to_region", "completion": "    from boto.awslambda.layer1 import AWSLambdaConnection\n    return AWSLambdaConnection(region=region_name, **kw_params)", "idx": 693}
{"namespace": "boto.cognito.identity.connect_to_region", "completion": "    from boto.cognito.identity.layer1 import CognitoIdentityConnection\n    return CognitoIdentityConnection(region_name, **kw_params)", "idx": 694}
{"namespace": "boto.cognito.sync.connect_to_region", "completion": "    from boto.cognito.sync.layer1 import CognitoSyncConnection\n    return CognitoSyncConnection(region_name, **kw_params)", "idx": 695}
{"namespace": "boto.cloudformation.connect_to_region", "completion": "    region = RegionInfo(**RegionData[region_name])\n    return region.connect(**kw_params)", "idx": 696}
{"namespace": "boto.route53.zone.Zone.find_records", "completion": "        if identifier is not None:\n            try:\n                int(identifier[1])\n                weight = identifier[1]\n                identifier = identifier[0]\n            except:\n                region = identifier[1]\n                identifier = identifier[0]\n        else:\n            weight = None\n            region = None\n        if all:\n            return self.route53connection.get_all_rrsets(self.id, name, type,\n                                                        identifier=identifier,\n                                                        weight=weight,\n                                                        region=region)\n        else:\n            return self.route53connection.get_rrset(self.id, name, type,\n                                                   identifier=identifier,\n                                                   weight=weight,\n                                                   region=region)", "idx": 697}
{"namespace": "boto.route53.domains.connect_to_region", "completion": "    from boto.route53.domains.layer1 import Route53DomainsConnection\n    return Route53DomainsConnection(region=RegionInfo(name=region_name, **kw_params))", "idx": 698}
{"namespace": "boto.s3.key.Key.get_contents_to_filename", "completion": "        with open(filename, 'wb') as fp:\n            self.get_contents_to_file(fp, headers, cb, num_cb, torrent=torrent,\n                                      version_id=version_id,\n                                      res_download_handler=res_download_handler,\n                                      response_headers=response_headers)", "idx": 699}
{"namespace": "boto.s3.cors.CORSConfiguration.add_rule", "completion": "        rule = CORSRule(allowed_method, allowed_origin, id, allowed_header, max_age_seconds, expose_header)\n        self.append(rule)", "idx": 700}
{"namespace": "boto.s3.bucket.Bucket.get_key", "completion": "        if validate:\n            if not self.connection.validate_bucket_name(self.name):\n                raise ValueError('Invalid bucket name: %s' % self.name)\n            if not self.connection.validate_key_name(key_name):\n                raise ValueError('Invalid key name: %s' % key_name)\n        if version_id:\n            headers = headers or {}\n            headers['x-amz-version-id'] = version_id\n        response = self.connection.make_request('HEAD', self.name, key_name,\n                                                headers=headers)\n        if response.status == 200:\n            return self.key_class(self.name, key_name, self.connection,\n                                  response_headers=response_headers)\n        elif response.status == 404:\n            return None\n        else:\n            response.error_from_status_code(response.status)\n", "idx": 701}
{"namespace": "boto.s3.bucket.Bucket.new_key", "completion": "        return self.key_class(self, key_name)\n", "idx": 702}
{"namespace": "boto.s3.bucket.Bucket.delete_key", "completion": "        if version_id:\n            query_args = 'versionId=%s' % version_id\n        else:\n            query_args = None\n        if mfa_token:\n            provider = self.connection.provider\n            headers[provider.mfa_header] = ' '.join(mfa_token)\n        response = self.connection.make_request('DELETE', self.name, key_name,\n                                                headers=headers,\n                                                query_args=query_args)\n        response.read()\n        if response.status == 204:\n            k = self.key_class(self)\n            k.name = key_name\n            k.version_id = version_id\n            return k\n        else:\n            raise self.connection.provider.storage_response_error(\n                response.status, response.reason, '')", "idx": 703}
{"namespace": "boto.s3.bucket.Bucket.get_tags", "completion": "        response = self.connection.make_request('GET', self.name,\n                                                query_args='tagging',\n                                                headers=headers)\n        body = response.read()\n        boto.log.debug(body)\n        if response.status == 200:\n            tags = Tags()\n            h = handler.XmlHandler(tags, self)\n            if not isinstance(body, bytes):\n                body = body.encode('utf-8')\n            xml.sax.parseString(body, h)\n            return tags\n        else:\n            raise self.connection.provider.storage_response_error(\n                response.status, response.reason, body)\n", "idx": 704}
{"namespace": "boto.s3.connection.S3Connection._required_auth_capability", "completion": "        if self.anon:\n            return ['hmac-v4-s3']\n        else:\n            return ['hmac-v4']\n", "idx": 705}
{"namespace": "boto.s3.connection.S3Connection.generate_url_sigv4", "completion": "        if iso_date is None:\n            iso_date = boto.utils.iso8601_date()\n        if headers is None:\n            headers = {}\n        if response_headers is None:\n            response_headers = {}\n        if bucket == '':\n            raise BotoClientError(\"Bucket name must be specified.\")\n        if key == '':\n            raise BotoClientError(\"Key name must be specified.\")\n        if not isinstance(expires_in, int):\n            raise BotoClientError(\"Expires must be specified as an integer.\")\n        if expires_in < 0:\n            raise BotoClientError(\"Expires must be greater than 0.\")\n        if expires_in > 604800:\n            raise BotoClientError(\"Expires must be less than 604800.\")\n        if method not in ['GET', 'PUT', 'POST', 'HEAD', 'DELETE']:\n            raise BotoClientError(\"Method must be one of GET, PUT, POST, HEAD, or DELETE.\")\n        if not isinstance(headers, dict):\n            raise BotoClientError(\"Headers must be specified as a dictionary.\")\n        if not isinstance(response_headers, dict):\n            raise BotoClientError(\"Response headers must be specified as a dictionary.\")\n        if not isinstance(force_http, bool):\n            raise BotoClientError(\"Force HTTP must be specified as a boolean.\")\n        if not isinstance(version_id, six.string_types):\n            raise BotoClientError(\"Version ID must be specified as a string.\")\n        if not isinstance(iso_date, six.string_types):\n            raise BotoClientError(\"ISO date must be specified as a string.\")\n        if version_id != '':\n            key += '?versionId=' + version_id\n        if force_http:\n            protocol = 'http'\n        else:\n            protocol = 'https'\n        url_base = self.calling_format.build_url_base(self, protocol, self.server_name(), bucket, key)\n        if ':' in url_base:\n            url_base = url", "idx": 706}
{"namespace": "boto.s3.lifecycle.Lifecycle.add_rule", "completion": "        rule = Rule(id, prefix, status, expiration, transition)\n        self.append(rule)", "idx": 707}
{"namespace": "boto.s3.website.WebsiteConfiguration.to_xml", "completion": "        xml = ''\n        if self.suffix is not None:\n            xml += tag('Suffix', self.suffix)\n        if self.error_key is not None:\n            xml += tag('Key', self.error_key)\n        if self.redirect_all_requests_to is not None:\n            xml += self.redirect_all_requests_to.to_xml()\n        if self.routing_rules is not None:\n            xml += self.routing_rules.to_xml()\n        return xml", "idx": 708}
{"namespace": "boto.s3.website.RoutingRules.to_xml", "completion": "        parts = []\n        for rule in self:\n            parts.append(rule.to_xml())\n        return ''.join(parts)", "idx": 709}
{"namespace": "boto.s3.website.RoutingRule.when", "completion": "        condition = Condition(key_prefix=key_prefix, http_error_code=http_error_code)\n        return cls(condition=condition)\n", "idx": 710}
{"namespace": "boto.s3.website.RoutingRule.then_redirect", "completion": "        self.redirect = Redirect(hostname=hostname, protocol=protocol, replace_key=replace_key, replace_key_prefix=replace_key_prefix, http_redirect_code=http_redirect_code)\n        return self", "idx": 711}
{"namespace": "boto.s3.connect_to_region", "completion": "    if 'host' in kw_params:\n        region = S3RegionInfo(name=region_name, connection_cls=S3Connection, **kw_params)\n        return region.connect(**kw_params)\n    else:\n        return S3Connection(region_name, **kw_params)", "idx": 712}
{"namespace": "boto.directconnect.connect_to_region", "completion": "    from boto.directconnect.layer1 import DirectConnectConnection\n    return DirectConnectConnection(region_name, **kw_params)", "idx": 713}
{"namespace": "boto.rds.connect_to_region", "completion": "    region = get_region(region_name)\n    if region:\n        return region.connect(**kw_params)\n    else:\n        return None", "idx": 714}
{"namespace": "boto.datapipeline.connect_to_region", "completion": "    from boto.datapipeline.layer1 import DataPipelineConnection\n    return DataPipelineConnection(region_name, **kw_params)", "idx": 715}
{"namespace": "boto.dynamodb.batch.Batch.to_dict", "completion": "        batch_dict = {}\n        batch_dict['TableName'] = self.table.name\n        batch_dict['Keys'] = self.keys\n        if self.attributes_to_get is not None:\n            batch_dict['AttributesToGet'] = self.attributes_to_get\n        if self.consistent_read:\n            batch_dict['ConsistentRead'] = self.consistent_read\n        return batch_dict", "idx": 716}
{"namespace": "boto.dynamodb.batch.BatchList.to_dict", "completion": "        batch_list = []\n        for batch in self:\n            batch_list.append(batch.to_dict())\n        return {'RequestItems': {self.table.name: batch_list}}", "idx": 717}
{"namespace": "boto.dynamodb.types.Dynamizer.encode", "completion": "        dynamodb_type = self._get_dynamodb_type(attr)\n        if dynamodb_type == 'N':\n            return self._encode_n(attr)\n        elif dynamodb_type == 'S':\n            return self._encode_s(attr)\n        elif dynamodb_type == 'NS':\n            return self._encode_ns(attr)\n        elif dynamodb_type == 'SS':\n            return self._encode_ss(attr)\n        elif dynamodb_type == 'B':\n            return self._encode_b(attr)\n        elif dynamodb_type == 'BS':\n            return self._encode_bs(attr)\n        elif dynamodb_type == 'M':\n            return self._encode_m(attr)\n        elif dynamodb_type == 'L':\n            return self._encode_l(attr)\n        elif dynamodb_type == 'NULL':\n            return self._encode_null(attr)\n        else:\n            raise TypeError('Unsupported type \"%s\" for value \"%s\"' % (dynamodb_type, attr))\n", "idx": 718}
{"namespace": "boto.dynamodb.types.Dynamizer.decode", "completion": "        if len(attr.keys()) > 1:\n            return attr\n        if 'S' in attr:\n            return attr['S']\n        if 'N' in attr:\n            return convert_num(attr['N'])\n        if 'SS' in attr:\n            return set(attr['SS'])\n        if 'NS' in attr:\n            return set(map(convert_num, attr['NS']))\n        if 'B' in attr:\n            return convert_binary(attr['B'])\n        if 'BS' in attr:\n            return set(map(convert_binary, attr['BS']))\n        if 'NULL' in attr:\n            return None\n        if 'BOOL' in attr:\n            return attr['BOOL']\n        if 'M' in attr:\n            return dict([(k, self.decode(v)) for k, v in attr['M'].items()])\n        if 'L' in attr:\n            return [self.decode(i) for i in attr['L']]\n        return attr", "idx": 719}
{"namespace": "boto.dynamodb.connect_to_region", "completion": "    from boto.dynamodb.layer2 import Layer2\n    return Layer2(region_name, **kw_params)", "idx": 720}
{"namespace": "boto.beanstalk.connect_to_region", "completion": "    from boto.beanstalk.layer1 import Layer1\n    return Layer1(region_name, **kw_params)", "idx": 721}
{"namespace": "boto.swf.connect_to_region", "completion": "    region = RegionInfo(None, region_name, 'swf', 'swf.%s.amazonaws.com' % region_name,\n                        'swf.%s.amazonaws.com' % region_name,\n                        'swf.%s.amazonaws.com' % region_name)\n    return region.connect(**kw_params)", "idx": 722}
{"namespace": "boto.opsworks.regions", "completion": "    regions = [\n        RegionInfo(name='us-east-1', endpoint='opsworks.us-east-1.amazonaws.com'),\n        RegionInfo(name='us-west-2', endpoint='opsworks.us-west-2.amazonaws.com'),\n        RegionInfo(name='eu-west-1', endpoint='opsworks.eu-west-1.amazonaws.com'),\n        RegionInfo(name='ap-southeast-1', endpoint='opsworks.ap-southeast-1.amazonaws.com'),\n        RegionInfo(name='ap-northeast-1', endpoint='opsworks.ap-northeast-1.amazonaws.com'),\n        RegionInfo(name='sa-east-1', endpoint='opsworks.sa-east-1.amazonaws.com'),\n    ]\n\n    return regions", "idx": 723}
{"namespace": "boto.opsworks.connect_to_region", "completion": "    region = RegionInfo(name=region_name, endpoint='opsworks.%s.amazonaws.com' % region_name)\n    return OpsWorksConnection(region=region, **kw_params)", "idx": 724}
{"namespace": "boto.sqs.connect_to_region", "completion": "    from boto.sqs.connection import SQSConnection\n    return SQSConnection(region_name, **kw_params)", "idx": 725}
{"namespace": "boto.rds2.connect_to_region", "completion": "    region = get_region(region_name)\n    if region is None:\n        return None\n    return region.connect(**kw_params)", "idx": 726}
{"namespace": "boto.cloudsearch2.connect_to_region", "completion": "    from boto.cloudsearch2.layer1 import CloudSearchConnection\n    return CloudSearchConnection(region=region_name, **kw_params)", "idx": 727}
{"namespace": "boto.cloudtrail.connect_to_region", "completion": "    from boto.cloudtrail.layer1 import CloudTrailConnection\n    return CloudTrailConnection(region=region_name, **kw_params)", "idx": 728}
{"namespace": "boto.elasticache.connect_to_region", "completion": "    from boto.elasticache.layer1 import ElastiCacheConnection\n    return ElastiCacheConnection(region=RegionInfo(name=region_name), **kw_params)", "idx": 729}
{"namespace": "boto.ses.connect_to_region", "completion": "    region = RegionInfo.get_region(region_name)\n    if region:\n        return region.connect(**kw_params)\n    else:\n        return None", "idx": 730}
{"namespace": "boto.codedeploy.connect_to_region", "completion": "    from boto.codedeploy.layer1 import CodeDeployConnection\n    return CodeDeployConnection(region=RegionInfo(name=region_name, **kw_params))", "idx": 731}
{"namespace": "boto.sts.credentials.Credentials.to_dict", "completion": "        return {\n            'access_key': self.access_key,\n            'secret_key': self.secret_key,\n            'session_token': self.session_token,\n            'expiration': self.expiration,\n            'request_id': self.request_id\n        }", "idx": 732}
{"namespace": "boto.sts.connect_to_region", "completion": "    region = RegionInfo.get(region_name, 'sts', connection_cls=STSConnection)\n    if region:\n        return region.connect(**kw_params)\n    else:\n        return None", "idx": 733}
{"namespace": "boto.machinelearning.connect_to_region", "completion": "    from boto.machinelearning.layer1 import MachineLearningConnection\n    return MachineLearningConnection(region=region_name, **kw_params)", "idx": 734}
{"namespace": "boto.vpc.connect_to_region", "completion": "    region = RegionInfo(name=region_name, connection_cls=VPCConnection)\n    return region.connect(**kw_params)", "idx": 735}
{"namespace": "boto.vpc.VPCConnection.get_all_vpc_peering_connections", "completion": "        params = {}\n        if vpc_peering_connection_ids:\n            self.build_list_params(params, vpc_peering_connection_ids, 'VpcPeeringConnectionId')\n        if filters:\n            self.build_filter_params(params, filters)\n        if dry_run:\n            params['DryRun'] = 'true'\n        return self.get_list('DescribeVpcPeeringConnections', params, [('item', VPCPeeringConnection)])", "idx": 736}
{"namespace": "boto.kinesis.connect_to_region", "completion": "    from boto.kinesis.layer1 import KinesisConnection\n    return KinesisConnection(region=region_name, **kw_params)", "idx": 737}
{"namespace": "boto.ec2containerservice.connect_to_region", "completion": "    from boto.ec2containerservice.layer1 import EC2ContainerServiceConnection\n    return EC2ContainerServiceConnection(region_name, **kw_params)", "idx": 738}
{"namespace": "boto.dynamodb2.table.Table._introspect_indexes", "completion": "        indexes = []\n        for field in raw_indexes:\n            index_klass = AllIndex\n            kwargs = {\n                'parts': []\n            }\n            if field['Projection']['ProjectionType'] == 'ALL':\n                index_klass = AllIndex\n            elif field['Projection']['ProjectionType'] == 'KEYS_ONLY':\n                index_klass = KeysOnlyIndex\n            elif field['Projection']['ProjectionType'] == 'INCLUDE':\n                index_klass = IncludeIndex\n                kwargs['includes'] = field['Projection']['NonKeyAttributes']\n            else:\n                raise exceptions.UnknownIndexFieldError(\n                    \"%s was seen, but is unknown. Please report this at \"\n                    \"https://github.com/boto/boto/issues.\" % \\\n                    field['Projection']['ProjectionType']\n                )\n            name = field['IndexName']\n            kwargs['parts'] = self._introspect_schema(field['KeySchema'], None)\n            indexes.append(index_klass(name, **kwargs))\n        return indexes", "idx": 739}
{"namespace": "boto.dynamodb2.table.Table.describe", "completion": "        table_description = self.connection.describe_table(self.table_name)\n        self.schema = self._introspect_schema(table_description['KeySchema'])\n        self.indexes = self._introspect_indexes(table_description['LocalSecondaryIndexes'])\n        self.global_indexes = self._introspect_global_indexes(table_description['GlobalSecondaryIndexes'])\n        self.throughput = {\n            'read': table_description['ProvisionedThroughput']['ReadCapacityUnits'],\n            'write': table_description['ProvisionedThroughput']['WriteCapacityUnits']\n        }\n        return table_description", "idx": 740}
{"namespace": "boto.dynamodb2.table.Table.update", "completion": "        if throughput is not None:\n            self.throughput = throughput\n\n        if global_indexes is not None:\n            self.global_indexes = global_indexes\n\n        # Prep the schema.\n        raw_schema = []\n        attr_defs = []\n        seen_attrs = set()\n\n        for field in self.schema:\n            raw_schema.append(field.schema())\n            # Build the attributes off what we know.\n            seen_attrs.add(field.name)\n            attr_defs.append(field.definition())\n\n        raw_throughput = {\n            'ReadCapacityUnits': int(self.throughput['read']),\n            'WriteCapacityUnits': int(self.throughput['write']),\n        }\n        kwargs = {}\n\n        kwarg_map = {\n            'indexes': 'local_secondary_indexes',\n            'global_indexes': 'global_secondary_indexes',\n        }\n        for index_attr in ('indexes', 'global_indexes'):\n            table_indexes = getattr(self, index_attr)\n            if table_indexes:\n                raw_indexes = []\n                for index_field in table_indexes:\n                    raw_indexes.append(index_field.schema())\n                    # Make sure all attributes specified in the indexes are\n                    # added to the definition\n                    for field in index_field.parts:\n                        if field.name not in seen_attrs:\n                            seen_attrs.add(field.name)\n                            attr_defs.append(field.definition())\n\n                kwargs[kwarg_map[index_attr]] = raw_indexes\n\n        self.connection.update_table(\n            table_name=self.table_name,\n            attribute_definitions=attr_defs,\n            key_schema=raw_schema,\n            provisioned_throughput=raw_throughput,\n            **kwargs\n        )\n        return True", "idx": 741}
{"namespace": "boto.dynamodb2.table.Table.create_global_secondary_index", "completion": "        # Check if the global index already exists\n        if global_index.name in [index.name for index in self.global_indexes]:\n            boto.log.error('Global index with name %s already exists' % global_index.name)\n            return False\n\n        # Update the global_indexes information on the Table\n        self.describe()\n\n        # Prep the schema.\n        raw_schema = []\n        attr_defs = []\n        seen_attrs = set()\n\n        for field in self.schema:\n            raw_schema.append(field.schema())\n            # Build the attributes off what we know.\n            seen_attrs.add(field.name)\n            attr_defs.append(field.definition())\n\n        # Make sure all attributes specified in the indexes are added to the definition\n        for field in global_index.parts:\n            if field.name not in seen_attrs:\n                seen_attrs.add(field.name)\n                attr_defs.append(field.definition())\n\n        raw_throughput = {\n            'ReadCapacityUnits': int(self.throughput['read']),\n            'WriteCapacityUnits': int(self.throughput['write']),\n        }\n        kwargs = {}\n\n        kwarg_map = {\n            'indexes': 'local_secondary_indexes',\n            'global_indexes': 'global_secondary_indexes',\n        }\n        for index_attr in ('indexes', 'global_indexes'):\n            table_indexes = getattr(self, index_attr)\n            if table_indexes:\n                raw_indexes = []\n                for index_field in table_indexes:\n                    raw_indexes.append(index_field.schema())\n                    # Make sure all attributes specified in the indexes are added to the definition\n                    for field in index_field.parts:\n                        if field.name not in seen_attrs:\n                            seen_attrs.add(field.name)\n                            attr_defs.append(field.definition())\n\n                kwargs[kwarg_map[index_attr]]", "idx": 742}
{"namespace": "boto.dynamodb2.table.Table.delete_global_secondary_index", "completion": "        if global_index_name:\n            gsi_data = []\n\n            gsi_data.append({\n                \"Delete\": {\n                    \"IndexName\": global_index_name\n                },\n            })\n\n            self.connection.update_table(\n                self.table_name,\n                global_secondary_index_updates=gsi_data,\n            )\n\n            return True\n        else:\n            msg = 'You need to provide the global index name to ' \\\n                  'delete_global_secondary_index method'\n            boto.log.error(msg)\n\n            return False", "idx": 743}
{"namespace": "boto.dynamodb2.table.Table.update_global_secondary_index", "completion": "        if global_indexes:\n            gsi_data = []\n\n            for gsi_name, gsi_throughput in global_indexes.items():\n                gsi_data.append({\n                    \"Update\": {\n                        \"IndexName\": gsi_name,\n                        \"ProvisionedThroughput\": {\n                            \"ReadCapacityUnits\": int(gsi_throughput['read']),\n                            \"WriteCapacityUnits\": int(gsi_throughput['write']),\n                        },\n                    },\n                })\n\n            self.connection.update_table(\n                self.table_name,\n                global_secondary_index_updates=gsi_data,\n            )\n\n            return True\n        else:\n            msg = 'You need to provide the global_indexes to ' \\\n                  'update_global_secondary_index method'\n            boto.log.error(msg)\n\n            return False", "idx": 744}
{"namespace": "boto.dynamodb2.table.Table.delete", "completion": "        self.connection.delete_table(self.table_name)\n        return True", "idx": 745}
{"namespace": "boto.dynamodb2.table.Table.get_item", "completion": "        raw_key = self._encode_keys(kwargs)\n        raw_attributes = None\n\n        if attributes:\n            raw_attributes = [{'Name': attr} for attr in attributes]\n\n        try:\n            result = self.connection.get_item(\n                table_name=self.table_name,\n                consistent_read=consistent,\n                attributes_to_get=raw_attributes,\n                **raw_key\n            )\n        except exceptions.ItemNotFound:\n            raise exceptions.ItemNotFound(\n                \"Item not found in table %s\" % self.table_name\n            )\n\n        return Item(self, result['Item'])\n", "idx": 746}
{"namespace": "boto.dynamodb2.table.Table.has_item", "completion": "        raw_key = self._encode_keys(kwargs)\n        item_data = self.connection.get_item(\n            self.table_name,\n            raw_key,\n            attributes_to_get=attributes,\n            consistent_read=consistent\n        )\n        if 'Item' not in item_data:\n            return False\n        return True", "idx": 747}
{"namespace": "boto.dynamodb2.table.Table._put_item", "completion": "        if expects is None:\n            expects = []\n        if not isinstance(item_data, list):\n            item_data = [item_data]\n        for item in item_data:\n            if not isinstance(item, Item):\n                raise exceptions.InvalidItemError(\n                    \"You must pass an Item instance to Table.put_item\")\n            if not item.table == self:\n                raise exceptions.InvalidItemError(\n                    \"The Item instance must be associated with this Table instance\")\n        if not self.schema:\n            self.describe()\n        raw_item = {}\n        for field in self.schema:\n            raw_item[field.name] = self._dynamizer.encode(item.data[field.name])\n        self.connection.put_item(\n            self.table_name,\n            raw_item,\n            expected=expects\n        )\n        return True", "idx": 748}
{"namespace": "boto.dynamodb2.table.Table.delete_item", "completion": "        raw_key = self._encode_keys(kwargs)\n        kwargs = {}\n\n        if expected is not None:\n            kwargs['expected'] = expected\n\n        if conditional_operator is not None:\n            kwargs['conditional_operator'] = conditional_operator\n\n        self.connection.delete_item(self.table_name, raw_key, **kwargs)\n        return True", "idx": 749}
{"namespace": "boto.dynamodb2.table.Table.get_key_fields", "completion": "        if not self.schema:\n            self.describe()\n        key_fields = []\n        for field in self.schema:\n            if field.key_type == 'HASH':\n                key_fields.append(field.name)\n        return key_fields", "idx": 750}
{"namespace": "boto.dynamodb2.table.Table._build_filters", "completion": "        if not filter_kwargs:\n            return None\n\n        filters = {}\n        for key, value in filter_kwargs.items():\n            if '__' not in key:\n                raise exceptions.InvalidFilterError(\n                    \"Invalid filter key %s. Please use the format \"\n                    \"<attributename>__<comparison_operator>=<value_to_expect>\" % key\n                )\n\n            attr_name, operator = key.split('__')\n\n            if operator not in using:\n                raise exceptions.InvalidFilterError(\n                    \"Invalid filter operator %s. Please use one of %s\" % (\n                        operator, ', '.join(using.keys())\n                    )\n                )\n\n            filters[attr_name] = {\n                'AttributeValueList': [self._dynamizer.encode(value)],\n                'ComparisonOperator': using[operator],\n            }\n\n        return filters", "idx": 751}
{"namespace": "boto.dynamodb2.table.Table.batch_get", "completion": "        results = ResultSet()\n        results.to_call(self._batch_get, keys, consistent, attributes)\n        return results\n", "idx": 752}
{"namespace": "boto.dynamodb2.table.Table.count", "completion": "        return self.connection.describe_table(self.table_name)['Table']['ItemCount']", "idx": 753}
{"namespace": "boto.dynamodb2.table.BatchTable.put_item", "completion": "        self._to_put.append({\n            'PutRequest': {\n                'Item': data,\n            },\n        })\n\n        if len(self._to_put) >= self.table.max_batch_get:\n            self.flush()\n", "idx": 754}
{"namespace": "boto.dynamodb2.table.BatchTable.delete_item", "completion": "        self._to_delete.append(kwargs)\n\n        if self.should_flush():\n            self.flush()\n", "idx": 755}
{"namespace": "boto.dynamodb2.table.BatchTable.flush", "completion": "        if self._to_put:\n            self.table._batch_put(self._to_put)\n            self._to_put = []\n\n        if self._to_delete:\n            self.table._batch_delete(self._to_delete)\n            self._to_delete = []\n\n        if self._unprocessed:\n            self.resend_unprocessed()\n\n        return True\n", "idx": 756}
{"namespace": "boto.dynamodb2.table.BatchTable.resend_unprocessed", "completion": "        while self._unprocessed:\n            batch_data = {\n                self.table.table_name: [\n                    # We'll insert data here shortly.\n                ],\n            }\n\n            for unprocessed in self._unprocessed:\n                if unprocessed.get('PutRequest'):\n                    item = Item(self.table, data=unprocessed['PutRequest']['Item'])\n                    batch_data[self.table.table_name].append({\n                        'PutRequest': {\n                            'Item': item.prepare_full(),\n                        }\n                    })\n                elif unprocessed.get('DeleteRequest'):\n                    batch_data[self.table.table_name].append({\n                        'DeleteRequest': {\n                            'Key': unprocessed['DeleteRequest']['Key'],\n                        }\n                    })\n\n            resp = self.table.connection.batch_write_item(batch_data)\n            self.handle_unprocessed(resp)\n\n        self._unprocessed = []", "idx": 757}
{"namespace": "boto.dynamodb2.fields.BaseSchemaField.definition", "completion": "        return {\n            'AttributeName': self.name,\n            'AttributeType': self.data_type\n        }", "idx": 758}
{"namespace": "boto.dynamodb2.fields.BaseIndexField.definition", "completion": "        return [{'AttributeName': part.name, 'AttributeType': part.data_type} for part in self.parts]\n", "idx": 759}
{"namespace": "boto.dynamodb2.fields.BaseIndexField.schema", "completion": "        schema = {\n            'IndexName': self.name,\n            'KeySchema': [],\n            'Projection': {\n                'ProjectionType': self.projection_type,\n            },\n        }\n\n        for part in self.parts:\n            schema['KeySchema'].append({\n                'AttributeName': part.name,\n                'KeyType': part.attr_type,\n            })\n\n        return schema", "idx": 760}
{"namespace": "boto.dynamodb2.fields.GlobalBaseIndexField.schema", "completion": "        schema_data = super(GlobalBaseIndexField, self).schema()\n        schema_data['ProvisionedThroughput'] = self.throughput\n        return schema_data", "idx": 761}
{"namespace": "boto.dynamodb2.fields.GlobalIncludeIndex.schema", "completion": "        schema_data = super(GlobalIncludeIndex, self).schema()\n        schema_data['Projection']['NonKeyAttributes'] = self.includes_fields\n        schema_data['ProvisionedThroughput'] = {\n            'ReadCapacityUnits': int(self.throughput['read']),\n            'WriteCapacityUnits': int(self.throughput['write']),\n        }\n        return schema_data", "idx": 762}
{"namespace": "boto.dynamodb2.items.Item.get_keys", "completion": "        keys = {}\n        for key in self.table.key_fields:\n            keys[key] = self[key]\n        return keys", "idx": 763}
{"namespace": "boto.dynamodb2.items.Item.get_raw_keys", "completion": "        key_fields = self.table.get_key_fields()\n        key_data = {}\n\n        for key in key_fields:\n            key_data[key] = self._dynamizer.encode(self[key])\n\n        return key_data", "idx": 764}
{"namespace": "boto.dynamodb2.items.Item.build_expects", "completion": "        if fields is None:\n            fields = self.table.get_fields()\n\n        expects = {}\n\n        for field in fields:\n            if field in self._data:\n                if field in self._orig_data:\n                    if self._data[field] != self._orig_data[field]:\n                        expects[field] = self._dynamizer.encode(self._data[field])\n                else:\n                    expects[field] = self._dynamizer.encode(self._data[field])\n            elif field in self._orig_data:\n                expects[field] = {\n                    'Action': 'DELETE',\n                }\n\n        return expects", "idx": 765}
{"namespace": "boto.dynamodb2.items.Item.prepare_full", "completion": "        data = {}\n        for key, value in self._data.items():\n            data[key] = self._dynamizer.encode(value)\n        return data", "idx": 766}
{"namespace": "boto.dynamodb2.items.Item.prepare_partial", "completion": "        # This doesn't save on its own. Rather, we prepare the datastructure\n        # and hand-off to the table to handle creation/update.\n        final_data = {}\n        altered_fields = set()\n\n        for key, value in self._data.items():\n            if not self._is_storable(value):\n                continue\n\n            if key in self._orig_data:\n                if self._data[key] != self._orig_data[key]:\n                    final_data[key] = self._dynamizer.encode(value)\n                    altered_fields.add(key)\n            else:\n                final_data[key] = self._dynamizer.encode(value)\n                altered_fields.add(key)\n\n        return final_data, altered_fields", "idx": 767}
{"namespace": "boto.dynamodb2.items.Item.partial_save", "completion": "        # Check if the item needs to be saved.\n        if not self.needs_save():\n            return False\n\n        # Prepare the data to be saved.\n        data, fields = self.prepare_partial()\n\n        # Save the data.\n        self.table.update_item(data, self.get_raw_keys(), fields)\n\n        # Mark the item as clean.\n        self.mark_clean()\n\n        return True", "idx": 768}
{"namespace": "boto.dynamodb2.items.Item.save", "completion": "        if not self.needs_save():\n            return False\n\n        if not overwrite:\n            return self.partial_save()\n\n        key = self.get_keys()\n        final_data = self.prepare_full()\n        expects = self.build_expects()\n        returned = self.table._put_item(key, final_data, expects=expects)\n        self.mark_clean()\n        return returned", "idx": 769}
{"namespace": "boto.dynamodb2.items.Item.delete", "completion": "        key = self.get_keys()\n        self.table._delete_item(key)\n        return True", "idx": 770}
{"namespace": "boto.dynamodb2.connect_to_region", "completion": "    from boto.dynamodb2.layer1 import DynamoDBConnection\n    return DynamoDBConnection(region=RegionInfo(name=region_name), **kw_params)", "idx": 771}
{"namespace": "litecli.packages.parseutils.extract_tables", "completion": "    # Parse the SQL statement using sqlparse\n    parsed = sqlparse.parse(sql)[0]\n\n    # Extract the table identifiers from the parsed statement\n    table_identifiers = extract_table_identifiers(extract_from_part(parsed))\n\n    # Return the table identifiers as a list of tuples\n    return list(table_identifiers)", "idx": 772}
{"namespace": "litecli.packages.parseutils.queries_start_with", "completion": "    for query in queries:\n        if query_starts_with(query, prefixes):\n            return True\n    return False", "idx": 773}
{"namespace": "litecli.packages.parseutils.is_destructive", "completion": "    destructive_keywords = [\n        \"DROP\",\n        \"TRUNCATE\",\n        \"DELETE\",\n        \"RENAME\",\n        \"ALTER\",\n        \"REPLACE\",\n        \"UPDATE\",\n        \"INSERT\",\n        \"CREATE\",\n        \"DROP\",\n        \"TRUNCATE\",\n        \"DELETE\",\n        \"RENAME\",\n        \"ALTER\",\n        \"REPLACE\",\n        \"UPDATE\",\n        \"INSERT\",\n        \"CREATE\",\n        \"DROP\",\n        \"TRUNCATE\",\n        \"DELETE\",\n        \"RENAME\",\n        \"ALTER\",\n        \"REPLACE\",\n        \"UPDATE\",\n        \"INSERT\",\n        \"CREATE\",\n        \"DROP\",\n        \"TRUNCATE\",\n        \"DELETE\",\n        \"RENAME\",\n        \"ALTER\",\n        \"REPLACE\",\n        \"UPDATE\",\n        \"INSERT\",\n        \"CREATE\",\n        \"DROP\",\n        \"TRUNCATE\",\n        \"DELETE\",\n        \"RENAME\",\n        \"ALTER\",\n        \"REPLACE\",\n        \"UPDATE\",\n        \"INSERT\",\n        \"CREATE\",\n        \"DROP\",\n        \"TRUNCATE\",\n        \"DELETE\",\n        \"RENAME\",\n        \"ALTER\",\n        \"REPLACE\",\n        \"UPDATE\",\n        \"INSERT\",\n        \"CREATE\",\n        \"DROP\",\n        \"TRUNCATE\",\n        \"DELETE\",\n        \"RENAME\",\n        \"ALTER\",\n        \"REPLACE\",\n        \"UPDATE\",\n        \"INSERT\",\n        \"CREATE\",\n        \"DROP\",\n        \"TRUNCATE\",\n        \"DELETE\",\n        \"RENAME\",\n        \"ALTER\",\n        \"REPLACE\",\n        \"UPDATE\",\n        \"INSERT\",\n        \"CREATE\",\n        \"DROP\",\n        \"TRUNCATE\",\n        \"DELETE\",\n        \"RENAME\",\n        \"ALTER\",\n        \"REPLACE\",\n        \"UPDATE\",\n        \"INSERT\",\n        \"CREATE\",\n        \"DROP\",\n        \"TRUNCATE\",", "idx": 774}
{"namespace": "litecli.packages.completion_engine.suggest_type", "completion": "    # If the text before the cursor is a special command, return the corresponding scope.\n    scope = parse_special_command(text_before_cursor)\n    if scope:\n        return [{'type': 'special', 'scope': scope}]\n\n    # If the text before the cursor is a comparison operator, return the corresponding scope.\n    if text_before_cursor in ['=', '<', '>', '<=', '>=', '<>', 'LIKE', 'IN', 'BETWEEN', 'IS', 'IS NOT']:\n        return [{'type': 'comparison', 'scope': 'comparison'}]\n\n    # If the text before the cursor is a keyword, return the corresponding scope.\n    if text_before_cursor in ['SELECT', 'FROM', 'WHERE', 'GROUP BY', 'HAVING', 'ORDER BY', 'LIMIT', 'OFFSET', 'UNION', 'INTERSECT', 'EXCEPT', 'ALL', 'DISTINCT', 'AS', 'INNER JOIN', 'LEFT JOIN', 'RIGHT JOIN', 'FULL JOIN', 'CROSS JOIN', 'NATURAL JOIN', 'ON', 'USING', 'CASE', 'WHEN', 'THEN', 'ELSE', 'END', 'CAST', 'CONVERT', 'TRY_CAST', 'TRY_CONVERT', 'COALESCE', 'NULLIF', 'IFNULL', 'ISNULL', 'NVL', 'NVL2', 'IIF', 'CASE WHEN', 'END CASE', 'EXISTS', 'NOT EXISTS', 'ANY', 'ALL', 'SOME', 'IN', 'BETWEEN', 'LIKE', 'ILIKE', 'NOT LIKE', 'ILIKE', 'NOT ILIKE', 'AND', 'OR', 'NOT', 'XOR', '=', '<>', '<', '>', '<=', '>=', 'IS', 'IS NOT', 'IS NULL', 'IS NOT NULL', 'IS TRUE', 'IS NOT TRUE', 'IS FALSE', 'IS NOT FALSE', 'IS UNKNOWN', 'IS NOT UNKNOWN', 'IS DISTINCT FROM', 'IS NOT DISTINCT FROM", "idx": 775}
{"namespace": "datasette.plugins.get_plugins", "completion": "    plugins = []\n    for ep in pkg_resources.iter_entry_points(\"datasette.plugins\"):\n        mod = ep.load()\n        plugin = {\n            \"name\": ep.name,\n            \"static_path\": getattr(mod, \"static_path\", None),\n            \"templates_path\": getattr(mod, \"templates_path\", None),\n            \"hooks\": [\n                {\n                    \"name\": hook.name,\n                    \"args\": hook.args,\n                    \"kwargs\": hook.kwargs,\n                }\n                for hook in pm.get_hooks(ep.name)\n            ],\n        }\n        if hasattr(mod, \"__version__\"):\n            plugin[\"version\"] = mod.__version__\n        if hasattr(mod, \"__project_name__\"):\n            plugin[\"project_name\"] = mod.__project_name__\n        plugins.append(plugin)\n    return plugins", "idx": 776}
{"namespace": "datasette.facets.ColumnFacet.suggest", "completion": "        # Get the row count and columns from the dataset\n        row_count = await self.get_row_count()\n        columns = await self.get_columns(self.sql, self.params)\n\n        # Determine the facet size\n        facet_size = self.get_facet_size()\n\n        # Initialize a list to store the suggested facets\n        suggested_facets = []\n\n        # Iterate through each column\n        for column in columns:\n            # Construct a SQL query to retrieve distinct values and their counts\n            sql = f\"select {column}, count(*) as count from ({self.sql}) group by {column}\"\n\n            # Execute the SQL query\n            results = await self.ds.execute(self.database, sql, self.params)\n\n            # Check if the number of distinct values is between 1 and the row count,\n            # and the number of distinct values is less than or equal to the facet size,\n            # and at least one distinct value has a count greater than 1\n            if (\n                len(results.rows) > 1\n                and len(results.rows) <= row_count\n                and len(results.rows) <= facet_size\n                and any(row[1] > 1 for row in results.rows)\n            ):\n                # Add the column as a suggested facet\n                suggested_facets.append(\n                    {\n                        \"name\": column,\n                        \"toggle_url\": path_with_added_args(\n                            self.request.path,\n                            {\"_facet\": column},\n                            self.request.args,\n                        ),\n                    }\n                )\n\n        # Return the list of suggested facets\n        return suggested_facets\n\n", "idx": 777}
{"namespace": "datasette.facets.ColumnFacet.facet_results", "completion": "        facet_size = self.get_facet_size()\n        facet_configs = self.get_configs()\n        facet_results = []\n        facets_timed_out = []\n        for facet_config in facet_configs:\n            config = facet_config[\"config\"]\n            facet_sql = \"\"\"\n                select value, label, count(*) as n from (\n                    {sql}\n                ) where value is not null\n                group by value\n                order by n desc\n                limit {limit}\n            \"\"\".format(\n                sql=self.sql, limit=facet_size + 1\n            )\n            try:\n                facet_results_raw = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n                facet_results_raw = list(facet_results_raw)\n                if len(facet_results_raw) > facet_size:\n                    facet_results_raw = facet_results_raw[:facet_size]\n                    truncated = True\n                else:\n                    truncated = False\n                for row in facet_results_raw:\n                    facet_results.append(\n                        {\n                            \"value\": row[\"value\"],\n                            \"label\": row[\"label\"] or row[\"value\"],\n                            \"count\": row[\"n\"],\n                            \"toggle_url\": self.ds.absolute_url(\n                                self.request,\n                                self.ds.urls.path(\n                                    path_with_removed_args(\n                                        self.request, {\"_facet\": config[\"simple\"]}\n                                    )\n                                ),\n                            ),\n                            \"selected\": config[\"simple\"] == row[\"value\"],\n                        }\n                    )\n            except QueryInterrupted:\n                facets_timed_out.append(config[\"simple\"])\n        return facet_results, facets_timed_out", "idx": 778}
{"namespace": "datasette.facets.ArrayFacet.suggest", "completion": "        suggested_facets = []\n        already_enabled = [c[\"config\"][\"simple\"] for c in self.get_configs()]\n        columns = await self.get_columns(self.sql, self.params)\n        for column in columns:\n            if column in already_enabled:\n                continue\n            facet_sql = \"\"\"\n                select {column} as value, count(*) as n from (\n                    {sql}\n                ) where value is not null\n                group by value\n                limit 100\n            \"\"\".format(\n                column=escape_sqlite(column), sql=self.sql\n            )\n            distinct_values = None\n            try:\n                distinct_values = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_suggest_time_limit_ms\"),\n                )\n                num_distinct_values = len(distinct_values)\n                if (\n                    1 < num_distinct_values < 100\n                    and num_distinct_values <= 100\n                    # And at least one has n > 1\n                    and any(r[\"n\"] > 1 for r in distinct_values)\n                ):\n                    # Check that the first 100 values are all JSON arrays of strings\n                    for row in distinct_values[:100]:\n                        if not self._is_json_array_of_strings(row[\"value\"]):\n                            break\n                    else:\n                        suggested_facets.append(\n                            {\n                                \"name\": column,\n                                \"toggle_url\": self.ds.absolute_url(\n                                    self.request,\n                                    self.ds.urls.path(\n                                        path_with_added_args(\n                                            self.request, {\"_facet\": column}\n                                        )\n                                    ),\n                                ),\n                            }\n                        )\n            except QueryInterrupted:\n                continue\n        return suggested_facets\n", "idx": 779}
{"namespace": "datasette.facets.ArrayFacet.facet_results", "completion": "        facet_results = []\n        facets_timed_out = []\n\n        qs_pairs = self.get_querystring_pairs()\n\n        facet_size = self.get_facet_size()\n        for source_and_config in self.get_configs():\n            config = source_and_config[\"config\"]\n            source = source_and_config[\"source\"]\n            column = config.get(\"column\") or config[\"simple\"]\n            facet_sql = \"\"\"\n                select value, count(*) as count from (\n                    select json_extract(json_each({column}), '$.value') as value\n                    from ({sql})\n                    where {column} is not null and {column} != ''\n                )\n                group by value order by count desc, value limit {limit}\n            \"\"\".format(\n                column=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            try:\n                facet_rows_results = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n                facet_results_values = []\n                facet_results.append(\n                    {\n                        \"name\": column,\n                        \"type\": self.type,\n                        \"hideable\": source != \"metadata\",\n                        \"toggle_url\": self.ds.urls.path(\n                            path_with_removed_args(self.request, {\"_facet_array\": column})\n                        ),\n                        \"results\": facet_results_values,\n                        \"truncated\": len(facet_rows_results) > facet_size,\n                    }\n                )\n                facet_rows = facet_rows_results.rows[:facet_size]\n                for row in facet_rows:\n                    column_qs = column\n                    if column.startswith(\"_\"):\n                        column_qs = \"{}__exact\".format(column)\n                   ", "idx": 780}
{"namespace": "datasette.facets.DateFacet.facet_results", "completion": "        facet_results = []\n        facets_timed_out = []\n\n        facet_size = self.get_facet_size()\n        for source_and_config in self.get_configs():\n            config = source_and_config[\"config\"]\n            source = source_and_config[\"source\"]\n            column = config.get(\"column\") or config[\"simple\"]\n            facet_sql = \"\"\"\n                select\n                    date({column}) as value,\n                    count(*) as count\n                from ({sql})\n                where {column} glob \"????-??-*\"\n                group by value\n                order by count desc, value limit {limit}\n            \"\"\".format(\n                column=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            try:\n                facet_rows_results = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n                facet_results_values = []\n                facet_results.append(\n                    {\n                        \"name\": column,\n                        \"type\": self.type,\n                        \"results\": facet_results_values,\n                        \"hideable\": source != \"metadata\",\n                        \"toggle_url\": self.ds.urls.path(\n                            path_with_removed_args(\n                                self.request, {\"_facet_date\": column}\n                            )\n                        ),\n                        \"truncated\": len(facet_rows_results) > facet_size,\n                    }\n                )\n                facet_rows = facet_rows_results.rows[:facet_size]\n                pairs = self.get_querystring_pairs()\n                for row in facet_rows:\n                    value = str(row[\"value\"])\n                    selected = (f\"{column}__date\", value) in pairs\n                    if selected:\n                        toggle_path = path_with_removed_args(\n                            self.request, {", "idx": 781}
{"namespace": "datasette.app.Datasette.invoke_startup", "completion": "        if self._startup_invoked:\n            return\n        self._startup_invoked = True\n        await self.refresh_schemas()\n        await self.client.refresh_permissions()\n        await self.client.refresh_messages()\n        await self.client.refresh_plugins()\n        await self.client.refresh_settings()\n        await self.client.refresh_metadata()\n        await self.client.refresh_inspect_data()\n        await self.client.refresh_spatialite_functions()\n        await self.client.refresh_spatialite_extensions()\n        await self.client.refresh_spatialite_functions_by_extension()\n        await self.client.refresh_spatialite_extensions_by_function()\n        await self.client.refresh_spatialite_functions_by_extension_and_function()\n        await self.client.refresh_spatialite_extensions_by_function_and_extension()\n        await self.client.refresh_spatialite_functions_by_extension_and_function_and_extension()\n        await self.client.refresh_spatialite_extensions_by_function_and_extension_and_extension()\n        await self.client.refresh_spatialite_extensions_by_function_and_extension_and_extension_and_extension()\n        await self.client.refresh_spatialite_extensions_by_function_and_extension_and_extension_and_extension_and_extension()\n        await self.client.refresh_spatialite_extensions_by_function_and_extension_and_extension_and_extension_and_extension_and_extension()\n        await self.client.refresh_spatialite_extensions_by_function_and_extension_and_extension_and_extension_and_extension_and_extension_and_extension()\n        await self.client.refresh_spatialite_extensions_by_function_and_extension_and_extension_and_extension_and_extension_and_extension_and_extension_and_extension()\n        await self.client.refresh_spatialite", "idx": 782}
{"namespace": "datasette.app.Datasette.get_database", "completion": "        if route:\n            for db in self.databases.values():\n                if db.route == route:\n                    return db\n        if name:\n            return self.databases.get(name)\n        for db in self.databases.values():\n            if not db.is_memory:\n                return db\n        return None\n", "idx": 783}
{"namespace": "datasette.app.Datasette.add_database", "completion": "        if name is None:\n            name = secrets.token_hex(8)\n        if route is None:\n            route = name\n        if name in self.databases:\n            i = 1\n            while f\"{name}-{i}\" in self.databases:\n                i += 1\n            name = f\"{name}-{i}\"\n        if route in self.databases:\n            i = 1\n            while f\"{route}-{i}\" in self.databases:\n                i += 1\n            route = f\"{route}-{i}\"\n        copied_databases = self.databases.copy()\n        copied_databases[name] = db\n        db.name = name\n        db.route = route\n        self.databases = copied_databases\n        return db\n", "idx": 784}
{"namespace": "datasette.app.Datasette.ensure_permissions", "completion": "        for permission in permissions:\n            if isinstance(permission, str):\n                action = permission\n                resource = None\n            else:\n                action, resource = permission\n            if not await self.permission_allowed(actor, action, resource):\n                raise Forbidden(\n                    \"You do not have permission to {} {}\".format(\n                        action, resource or \"this resource\"\n                    )\n                )", "idx": 785}
{"namespace": "datasette.app.Datasette.check_visibility", "completion": "        assert actor is None or isinstance(actor, dict), \"actor must be None or a dict\"\n        if permissions is None:\n            permissions = []\n        if action is not None:\n            permissions.append(action)\n        if resource is not None:\n            permissions.append((\"view\", resource))\n        await self.ensure_permissions(actor, permissions)\n        return True, False", "idx": 786}
{"namespace": "datasette.app.Datasette.render_template", "completion": "        if not self._startup_invoked:\n            raise StartupError(\"Datasette.render_template() called before startup\")\n        if not isinstance(templates, (Template, str)):\n            raise ValueError(\"templates must be a Template or str\")\n        if isinstance(templates, str):\n            templates = self.jinja_env.get_template(templates)\n        if context is None:\n            context = {}\n        context[\"datasette\"] = self\n        context[\"request\"] = request\n        context[\"view_name\"] = view_name\n        context[\"settings\"] = self.settings_dict()\n        context[\"messages\"] = self._show_messages(request)\n        context[\"crumb_items\"] = await self._crumb_items(request)\n        context[\"actor\"] = request.actor if request else None\n        context[\"csrf_token\"] = asgi_csrf.generate_csrf()\n        context[\"csrf_token_input\"] = Markup(\n            '<input type=\"hidden\" name=\"csrf_token\" value=\"{}\">'.format(\n                context[\"csrf_token\"]\n            )\n        )\n        context[\"csrf_token_meta\"] = Markup(\n            '<meta name=\"csrf-token\" content=\"{}\">'.format(context[\"csrf_token\"])\n        )\n        context[\"csrf_token_header\"] = \"X-CSRFToken: {}\".format(context[\"csrf_token\"])\n        context[\"csrf_token_cookie\"] = \"csrf_token={}\".format(context[\"csrf_token\"])\n        context[\"csrf_token_javascript\"] = Markup(\n            '<script>window.csrfToken = \"{}\";</script>'.format(context[\"csrf_token\"])\n        )\n        context[\"csrf_token_javascript_var\"] = Markup(\n            '<script>var csrfToken = \"{}\";</script>'.format(context[\"csrf_token\"])\n        )\n        context[\"csrf_token_javascript_var_name\"] = \"cs", "idx": 787}
{"namespace": "datasette.app.DatasetteClient.get", "completion": "        path = self._fix(path)\n        async with httpx.AsyncClient() as client:\n            response = await client.get(path, **kwargs)\n        return response\n", "idx": 788}
{"namespace": "datasette.utils.asgi.Request.full_path", "completion": "        if self.query_string:\n            return f\"{self.path}?{self.query_string}\"\n        else:\n            return self.path\n", "idx": 789}
{"namespace": "datasette.utils.asgi.Request.post_body", "completion": "        body = b\"\"\n        more_body = True\n        while more_body:\n            message = await self.receive()\n            body += message.get(\"body\", b\"\")\n            more_body = message.get(\"more_body\", False)\n        return body\n", "idx": 790}
{"namespace": "datasette.utils.asgi.Request.fake", "completion": "        url_vars = url_vars or {}\n        path, query_string = path_with_query_string.split(\"?\")\n        return cls(\n            {\n                \"method\": method,\n                \"scheme\": scheme,\n                \"path\": path,\n                \"raw_path\": path.encode(\"latin-1\"),\n                \"query_string\": query_string.encode(\"latin-1\"),\n                \"headers\": [\n                    (b\"host\", b\"localhost\"),\n                    (b\"cookie\", b\"\"),\n                    (b\"content-type\", b\"application/x-www-form-urlencoded\"),\n                ],\n                \"url_route\": {\"kwargs\": url_vars},\n            },\n            lambda: None,\n        )\n\n", "idx": 791}
{"namespace": "datasette.utils.asgi.Response.asgi_send", "completion": "        # Remove any existing content-type header\n        headers = {k: v for k, v in self.headers.items() if k.lower() != \"content-type\"}\n        headers[\"content-type\"] = self.content_type\n        await send(\n            {\n                \"type\": \"http.response.start\",\n                \"status\": self.status,\n                \"headers\": [\n                    [key.encode(\"utf-8\"), value.encode(\"utf-8\")]\n                    for key, value in headers.items()\n                ],\n            }\n        )\n        if self.body:\n            await send(\n                {\n                    \"type\": \"http.response.body\",\n                    \"body\": self.body.encode(\"utf-8\"),\n                }\n            )\n\n", "idx": 792}
{"namespace": "datasette.utils.asgi.Response.set_cookie", "completion": "        if samesite not in SAMESITE_VALUES:\n            raise ValueError(\n                f\"samesite should be one of {SAMESITE_VALUES}, not {samesite}\"\n            )\n        cookie = Morsel()\n        cookie.set(key, value)\n        if max_age is not None:\n            cookie[\"max-age\"] = max_age\n        if expires is not None:\n            cookie[\"expires\"] = expires\n        if path is not None:\n            cookie[\"path\"] = path\n        if domain is not None:\n            cookie[\"domain\"] = domain\n        if secure:\n            cookie[\"secure\"] = \"\"\n        if httponly:\n            cookie[\"httponly\"] = \"\"\n        if samesite != \"none\":\n            cookie[\"samesite\"] = samesite\n        self._set_cookie_headers.append(cookie.OutputString())", "idx": 793}
{"namespace": "datasette.utils.asgi.Response.html", "completion": "        return cls(body, status, headers, content_type=\"text/html; charset=utf-8\")\n", "idx": 794}
{"namespace": "datasette.utils.asgi.Response.text", "completion": "        return cls(\n            body,\n            status=status,\n            headers=headers,\n            content_type=\"text/plain; charset=utf-8\",\n        )\n", "idx": 795}
{"namespace": "datasette.utils.asgi.Response.json", "completion": "        return cls(\n            json.dumps(body, default=default),\n            status=status,\n            headers=headers,\n            content_type=\"application/json; charset=utf-8\",\n        )\n", "idx": 796}
{"namespace": "datasette.utils.asgi.Response.redirect", "completion": "        headers = headers or {}\n        headers[\"Location\"] = path\n        return cls(\"\", status=status, headers=headers, content_type=\"text/html; charset=utf-8\")\n\n", "idx": 797}
{"namespace": "datasette.utils.validate_sql_select", "completion": "    # Remove comments from SQL statement\n    sql = re.sub(comment_re, \"\", sql)\n\n    # Convert SQL statement to lowercase\n    sql = sql.lower()\n\n    # Check if SQL statement matches any of the allowed SQL patterns\n    for pattern in allowed_sql_res:\n        if pattern.match(sql):\n            return\n\n    # Check if SQL statement matches any of the disallowed SQL patterns\n    for pattern, error_message in disallawed_sql_res:\n        if pattern.match(sql):\n            raise InvalidSql(error_message)\n\n    # If SQL statement does not match any of the allowed or disallowed patterns, raise an invalid sql exception\n    raise InvalidSql(\"Statement did not match any allowed SQL pattern\")", "idx": 798}
{"namespace": "datasette.utils.escape_css_string", "completion": "    s = s.replace(\"\\r\\n\", \"\\n\")\n    s = s.replace(\"\\r\", \"\\n\")\n    s = s.replace(\"\\n\", \"\\\\A\")\n    s = _css_re.sub(lambda m: \"\\\\\" + m.group(0), s)\n    return s", "idx": 799}
{"namespace": "datasette.utils.detect_fts", "completion": "    # Check if the table has a corresponding FTS virtual table\n    fts_table = f\"{table}_fts\"\n    fts_table_exists = conn.execute(\n        \"SELECT name FROM sqlite_master WHERE type='table' AND name=?\", (fts_table,)\n    ).fetchone()\n    if fts_table_exists:\n        return fts_table\n    else:\n        return None\n\n", "idx": 800}
{"namespace": "datasette.utils.is_url", "completion": "    return isinstance(value, str) and value.startswith((\"http://\", \"https://\")) and not whitespace_re.search(value)", "idx": 801}
{"namespace": "datasette.utils.to_css_class", "completion": "    if css_class_re.match(s):\n        return s\n    s = css_invalid_chars_re.sub(\"\", s)\n    return f\"{s}-{hashlib.md5(s.encode('utf8')).hexdigest()[:6]}\"", "idx": 802}
{"namespace": "datasette.utils.escape_fts", "completion": "    if query.count('\"') % 2 == 1:\n        query += '\"'\n    return ' '.join(\n        [\n            f'\"{term}\"'\n            for term in _escape_fts_re.split(query)\n            if term and term != '\"'\n        ]\n    )", "idx": 803}
{"namespace": "datasette.utils.check_connection", "completion": "    try:\n        conn.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n    except sqlite3.OperationalError as e:\n        if \"no such table\" in str(e):\n            raise ConnectionProblem(\"No such table\")\n        elif \"unable to open database file\" in str(e):\n            raise ConnectionProblem(\"Unable to open database file\")\n        elif \"database is locked\" in str(e):\n            raise ConnectionProblem(\"Database is locked\")\n        elif \"attempt to write a readonly database\" in str(e):\n            raise ConnectionProblem(\"Attempt to write a readonly database\")\n        elif \"unable to open database file\" in str(e):\n            raise ConnectionProblem(\"Unable to open database file\")\n        elif \"unable to open database file\" in str(e):\n            raise ConnectionProblem(\"Unable to open database file\")\n        elif \"unable to open database file\" in str(e):\n            raise ConnectionProblem(\"Unable to open database file\")\n        elif \"unable to open database file\" in str(e):\n            raise ConnectionProblem(\"Unable to open database file\")\n        elif \"unable to open database file\" in str(e):\n            raise ConnectionProblem(\"Unable to open database file\")\n        elif \"unable to open database file\" in str(e):\n            raise ConnectionProblem(\"Unable to open database file\")\n        elif \"unable to open database file\" in str(e):\n            raise ConnectionProblem(\"Unable to open database file\")\n        elif \"unable to open database file\" in str(e):\n            raise ConnectionProblem(\"Unable to open database file\")\n        elif \"unable to open database file\" in str(e):\n            raise ConnectionProblem(\"Unable to open database file\")\n        elif \"unable to open database file\" in str(e):\n            raise ConnectionProblem(\"Unable to open database file\")\n        elif \"unable to open database file\" in str(e):\n            raise ConnectionProblem(\"Unable to open database file\")\n        elif \"unable to open database file\" in str(e):\n            raise ConnectionProblem", "idx": 804}
{"namespace": "datasette.utils.parse_metadata", "completion": "    if isinstance(content, str):\n        try:\n            return json.loads(content)\n        except json.JSONDecodeError:\n            pass\n        try:\n            return yaml.safe_load(content)\n        except yaml.YAMLError:\n            pass\n    raise BadMetadataError(\n        \"Metadata must be valid JSON or YAML. Got {}\".format(type(content))\n    )", "idx": 805}
{"namespace": "datasette.utils.call_with_supported_arguments", "completion": "    call_with = _gather_arguments(fn, kwargs)\n    return fn(*call_with)", "idx": 806}
{"namespace": "datasette.utils.derive_named_parameters", "completion": "    sql = sql.strip().rstrip(\";\")\n    possible_named_parameters = _re_named_parameter.findall(sql)\n    try:\n        explain_sql = f\"explain {sql}\"\n        explain_results = await db.execute(explain_sql, {\"__named_parameters__\": None})\n        variables = [\n            row[\"variable\"]\n            for row in explain_results\n            if row[\"variable\"] and row[\"variable\"].startswith(\":\")\n        ]\n        return [variable[1:] for variable in variables]\n    except Exception as e:\n        return possible_named_parameters", "idx": 807}
{"namespace": "pyramid.path.Resolver.get_package_name", "completion": "        if self.package is CALLER_PACKAGE:\n            return caller_package().__name__\n        return self.package.__name__\n", "idx": 808}
{"namespace": "pyramid.path.Resolver.get_package", "completion": "        if self.package is CALLER_PACKAGE:\n            package = caller_package()\n        else:\n            package = self.package\n        return package\n", "idx": 809}
{"namespace": "pyramid.path.DottedNameResolver.resolve", "completion": "        if ':' in dotted:\n            package_name, dotted = dotted.split(':', 1)\n            return self.resolve_pkg_resources(package_name, dotted)\n        else:\n            return self.resolve_zope_dottedname(dotted)\n", "idx": 810}
{"namespace": "pyramid.path.DottedNameResolver.maybe_resolve", "completion": "        if not isinstance(dotted, str):\n            return dotted\n        package_name = self.get_package_name()\n        package = self.get_package()\n        if dotted.startswith('.'):\n            dotted = package_name + dotted\n        elif ':' in dotted:\n            package_name, dotted = dotted.split(':', 1)\n        else:\n            if self.package is CALLER_PACKAGE:\n                package_name = caller_package().__name__\n            else:\n                package_name = getattr(self.package, '__name__', None)\n            if package_name is None:\n                raise ValueError(\n                    'relative spec %r irresolveable without package' % (dotted,)\n                )\n        return package.resolve(dotted)", "idx": 811}
{"namespace": "pyramid.path.PkgResourcesAssetDescriptor.abspath", "completion": "        return self.pkg_resources.resource_filename(self.pkg_name, self.path)", "idx": 812}
{"namespace": "pyramid.renderers.render_to_response", "completion": "    try:\n        registry = request.registry\n    except AttributeError:\n        registry = None\n    if package is None:\n        package = caller_package()\n    helper = RendererHelper(\n        name=renderer_name, package=package, registry=registry\n    )\n\n    with hide_attrs(request, 'response'):\n        result = helper.render(value, response, request=request)\n\n    return result", "idx": 813}
{"namespace": "pyramid.renderers.JSON.add_adapter", "completion": "        self.components.registerAdapter(adapter, providedBy(type_or_iface), IJSONAdapter)", "idx": 814}
{"namespace": "pyramid.renderers.RendererHelper.settings", "completion": "        try:\n            return self.registry.settings\n        except AttributeError:\n            return {}\n", "idx": 815}
{"namespace": "pyramid.renderers.RendererHelper.render_view", "completion": "        system = {\n            'view': view,\n            'renderer_name': self.name,\n            'renderer_info': self,\n            'context': context,\n            'request': request,\n            'csrf_token': get_csrf_token(request),\n        }\n        return self.render(view, system, request=request, response=response)\n", "idx": 816}
{"namespace": "pyramid.renderers.RendererHelper.render", "completion": "        system = {}\n        system.update(system_values)\n        if request is not None:\n            system['request'] = request\n            system['req'] = request\n            system['get_csrf_token'] = partial(get_csrf_token, request)\n        self.registry.notify(system)\n        return self.renderer(value, system)\n", "idx": 817}
{"namespace": "pyramid.renderers.RendererHelper.render_to_response", "completion": "        from pyramid.events import BeforeRender\n\n        renderer = self.renderer\n        if system_values is None:\n            system_values = {\n                'view': None,\n                'renderer_name': self.name,  # b/c\n                'renderer_info': self,\n                'context': getattr(request, 'context', None),\n                'request': request,\n                'req': request,\n                'get_csrf_token': partial(get_csrf_token, request),\n            }\n\n        system_values = BeforeRender(system_values, value)\n\n        registry = self.registry\n        registry.notify(system_values)\n        result = renderer(value, system_values)\n        response = request.response\n        response.body = result\n        return response", "idx": 818}
{"namespace": "pyramid.renderers.RendererHelper.clone", "completion": "        if name is None:\n            name = self.name\n        if package is None:\n            package = self.package\n        if registry is None:\n            registry = self.registry\n\n        return RendererHelper(name=name, package=package, registry=registry)", "idx": 819}
{"namespace": "pyramid.urldispatch.RoutesMapper.get_routes", "completion": "        if include_static:\n            return self.routelist\n        else:\n            return [route for route in self.routelist if not route.static]", "idx": 820}
{"namespace": "pyramid.urldispatch.RoutesMapper.connect", "completion": "        route = Route(name, pattern, factory, predicates, pregenerator)\n        self.routes[name] = route\n        if static is True:\n            self.static_routes.append(route)\n        else:\n            self.routelist.append(route)\n        return route", "idx": 821}
{"namespace": "pyramid.testing.DummyTemplateRenderer.assert_", "completion": "        for k, v in kw.items():\n            val = self._received.get(k, _marker)\n            if val is _marker:\n                val = self._implementation._received.get(k, _marker)\n                if val is _marker:\n                    raise AssertionError(\n                        '{} not received by renderer'.format(k)\n                    )\n            if val != v:\n                raise AssertionError(\n                    '{} received by renderer is {} but expected {}'.format(\n                        k, val, v\n                    )\n                )\n        return True", "idx": 822}
{"namespace": "pyramid.testing.DummyResource.__getitem__", "completion": "        return self.subs[name]", "idx": 823}
{"namespace": "pyramid.testing.DummyResource.clone", "completion": "        if __name__ is _marker:\n            __name__ = self.__name__\n        if __parent__ is _marker:\n            __parent__ = self.__parent__\n        kw.update(self.kw)\n        return DummyResource(__name__=__name__, __parent__=__parent__, **kw)", "idx": 824}
{"namespace": "pyramid.testing.DummySession.get_csrf_token", "completion": "        token = self.get('_csrft_')\n        if token is None:\n            token = self.new_csrf_token()\n        return token", "idx": 825}
{"namespace": "pyramid.testing.DummyRequest.response", "completion": "        return self.response_factory(self)\n", "idx": 826}
{"namespace": "pyramid.testing.DummyRendererFactory.add", "completion": "        self.renderers[spec] = renderer\n", "idx": 827}
{"namespace": "pyramid.authorization.ACLAuthorizationPolicy.principals_allowed_by_permission", "completion": "        return self.helper.principals_allowed_by_permission(context, permission)", "idx": 828}
{"namespace": "pyramid.url.URLMethodsMixin.route_url", "completion": "        app_url, qs, anchor = parse_url_overrides(self, kw)\n        route = self.registry.getUtility(IResourceURL, route_name)\n        return route.static_url(self, *elements, **kw) + qs + anchor\n", "idx": 829}
{"namespace": "pyramid.predicates.CustomPredicate.text", "completion": "        if hasattr(self.func, '__text__'):\n            return self.func.__text__()\n        else:\n            return 'custom = %s' % self.func.__doc__\n", "idx": 830}
{"namespace": "pyramid.threadlocal.ThreadLocalManager.pop", "completion": "        return self.stack.pop()\n", "idx": 831}
{"namespace": "pyramid.threadlocal.ThreadLocalManager.get", "completion": "        if self.stack:\n            return self.stack[-1]\n        else:\n            return self.default()", "idx": 832}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.authenticated_userid", "completion": "        identity = self._get_identity(request)\n        if identity is None:\n            return None\n        userid = identity.get('repoze.who.userid')\n        if userid is None:\n            return None\n        if self._clean_principal(userid) is None:\n            return None\n        if self.callback is None:\n            return userid\n        callback_ok = self.callback(userid, request)\n        if callback_ok is not None:  # is not None\n            return userid\n        return None\n", "idx": 833}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.unauthenticated_userid", "completion": "        identity = self._get_identity(request)\n        if identity is None:\n            self.debug and self._log(\n                'repoze.who identity is None, returning None',\n                'unauthenticated_userid',\n                request,\n            )\n            return None\n        userid = identity['repoze.who.userid']\n        if userid is None:\n            self.debug and self._log(\n                'repoze.who.userid is None, returning None' % userid,\n                'unauthenticated_userid',\n                request,\n            )\n            return None\n        if self._clean_principal(userid) is None:\n            self.debug and self._log(\n                (\n                    'use of userid %r is disallowed by any built-in Pyramid '\n                    'security policy, returning None' % userid\n                ),\n                'unauthenticated_userid',\n                request,\n            )\n            return None\n        return userid", "idx": 834}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.forget", "completion": "        identifier = self._get_identifier(request)\n        if identifier is None:\n            return []\n        environ = request.environ\n        return identifier.forget(environ)", "idx": 835}
{"namespace": "pyramid.authentication.AuthTktAuthenticationPolicy.unauthenticated_userid", "completion": "        cookie = self.cookie.cookie_name\n        if cookie not in request.cookies:\n            return None\n        try:\n            userid = self.cookie.deserialize(request.cookies[cookie])\n        except binascii.Error:\n            return None\n        return userid\n", "idx": 836}
{"namespace": "pyramid.authentication.SessionAuthenticationHelper.remember", "completion": "        request.session[self.userid_key] = userid\n        return []\n", "idx": 837}
{"namespace": "pyramid.authentication.SessionAuthenticationHelper.forget", "completion": "        if self.userid_key in request.session:\n            del request.session[self.userid_key]\n        return []", "idx": 838}
{"namespace": "pyramid.authentication.BasicAuthAuthenticationPolicy.unauthenticated_userid", "completion": "        auth = request.authorization\n        if auth is not None:\n            return auth.username\n", "idx": 839}
{"namespace": "pyramid.request.CallbackMethodsMixin._process_response_callbacks", "completion": "        for callback in self.response_callbacks:\n            callback(self, response)", "idx": 840}
{"namespace": "pyramid.request.CallbackMethodsMixin._process_finished_callbacks", "completion": "        callbacks = self.finished_callbacks\n        while callbacks:\n            callback = callbacks.popleft()\n            callback(self)", "idx": 841}
{"namespace": "pyramid.request.Request.session", "completion": "        if self.session_factory is None:\n            raise ConfigurationError(\n                \"No session factory has been registered. \"\n                \"Did you forget to call config.set_session_factory()?\"\n            )\n        return self.session_factory(self)\n", "idx": 842}
{"namespace": "pyramid.request.RequestLocalCache.get_or_create", "completion": "        if creator is None:\n            creator = self._creator\n        if creator is None:\n            raise ValueError('No creator function bound to cache')\n        try:\n            value = self._store[request]\n        except KeyError:\n            value = creator(request)\n            self._store[request] = value\n        return value\n", "idx": 843}
{"namespace": "pyramid.request.RequestLocalCache.set", "completion": "        self._store[request] = value\n        request.add_finished_callback(self.clear)\n", "idx": 844}
{"namespace": "pyramid.i18n.Localizer.pluralize", "completion": "        if self.pluralizer is None:\n            self.pluralizer = Pluralizer(self.translations)\n        return self.pluralizer(singular, plural, n, domain=domain, mapping=mapping)", "idx": 845}
{"namespace": "pyramid.i18n.Translations.load", "completion": "        if dirname is None:\n            dirname = os.path.join(os.path.dirname(__file__), 'locale')\n        if locales is None:\n            locales = [Locale.getdefaultlocale()[0]]\n        if not isinstance(locales, list):\n            locales = [locales]\n        catalog = cls()\n        for locale in locales:\n            if isinstance(locale, Locale):\n                locale = locale.language\n            if locale is None:\n                continue\n            locale = locale.replace('-', '_')\n            filename = os.path.join(dirname, locale, 'LC_MESSAGES', domain + '.mo')\n            if os.path.exists(filename):\n                with open(filename, 'rb') as fp:\n                    catalog.add(cls(fp, domain))\n                break\n        return catalog\n", "idx": 846}
{"namespace": "pyramid.i18n.Translations.add", "completion": "        if translations.domain != self.domain:\n            if translations.domain not in self._domains:\n                self._domains[translations.domain] = Translations()\n            self._domains[translations.domain].add(translations, merge=merge)\n            return self\n        if not merge:\n            self._catalog.update(translations._catalog)\n            self.files.extend(translations.files)\n            return self\n        for msgid, msgstr in translations._catalog.items():\n            if msgid not in self._catalog:\n                self._catalog[msgid] = msgstr\n        self.files.extend(translations.files)\n        return self\n", "idx": 847}
{"namespace": "pyramid.i18n.Translations.merge", "completion": "        for msgid in translations.catalog:\n            if msgid in self.catalog:\n                self.catalog[msgid] = translations.catalog[msgid]\n                self.files.extend(translations.files)\n            else:\n                self.add(translations, merge=False)\n        return self", "idx": 848}
{"namespace": "pyramid.i18n.LocalizerRequestMixin.locale_name", "completion": "        from pyramid.interfaces import ILocalizer\n        from pyramid.interfaces import ITranslationDirectories\n        registry = self.registry\n\n        current_locale_name = self.locale_name\n        localizer = registry.queryUtility(ILocalizer, name=current_locale_name)\n\n        if localizer is None:\n            # no localizer utility registered yet\n            tdirs = registry.queryUtility(ITranslationDirectories, default=[])\n            localizer = make_localizer(current_locale_name, tdirs)\n\n            registry.registerUtility(\n                localizer, ILocalizer, name=current_locale_name\n            )\n\n        return localizer", "idx": 849}
{"namespace": "pyramid.csrf.LegacySessionCSRFStoragePolicy.check_csrf_token", "completion": "        expected_token = self.get_csrf_token(request)\n        if not expected_token:\n            raise BadCSRFToken(\"No CSRF token found in session\")\n        if not supplied_token:\n            raise BadCSRFToken(\"No CSRF token found in request\")\n        if not isinstance(expected_token, bytes_):\n            expected_token = text_(expected_token).encode(\"utf-8\")\n        if not isinstance(supplied_token, bytes_):\n            supplied_token = text_(supplied_token).encode(\"utf-8\")\n        if not strings_differ(expected_token, supplied_token):\n            return True\n        raise BadCSRFToken(\"CSRF token mismatch\")", "idx": 850}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.new_csrf_token", "completion": "        token = self._token_factory()\n        request.session[self.key] = token\n        return token\n", "idx": 851}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.get_csrf_token", "completion": "        token = request.session.get(self.key)\n        if token is None:\n            token = self.new_csrf_token(request)\n        return token\n", "idx": 852}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.check_csrf_token", "completion": "        expected_token = self.get_csrf_token(request)\n        return not strings_differ(\n            bytes_(expected_token), bytes_(supplied_token)\n        )", "idx": 853}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.new_csrf_token", "completion": "        token = self._token_factory()\n        request.add_response_callback(self.set_csrf_token_cookie, token)\n        return token\n", "idx": 854}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.get_csrf_token", "completion": "        token = request.cookies.get(self.cookie_name, None)\n        if not token:\n            token = self.new_csrf_token(request)\n        return token\n", "idx": 855}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.check_csrf_token", "completion": "        expected_token = self.get_csrf_token(request)\n        return not strings_differ(\n            bytes_(expected_token), bytes_(supplied_token)\n        )", "idx": 856}
{"namespace": "pyramid.security.PermitsResult.__repr__", "completion": "        return f\"<{self.__class__.__name__} instance at {id(self)} with msg {self.msg}>\"\n\n", "idx": 857}
{"namespace": "pyramid.util.InstancePropertyHelper.make_property", "completion": "        if name is None:\n            name = callable.__name__\n        if reify:\n            property = SettableProperty(callable)\n        else:\n            property = property(callable)\n        return name, property\n", "idx": 858}
{"namespace": "pyramid.util.InstancePropertyHelper.set_property", "completion": "        name, fn = cls.make_property(callable, name, reify)\n        setattr(target, name, fn)", "idx": 859}
{"namespace": "pyramid.util.InstancePropertyHelper.add_property", "completion": "        prop = self.make_property(callable, name=name, reify=reify)\n        self.properties[prop[0]] = prop[1]", "idx": 860}
{"namespace": "pyramid.util.InstancePropertyHelper.apply", "completion": "        self.apply_properties(target, self.properties)", "idx": 861}
{"namespace": "pyramid.util.InstancePropertyMixin.set_property", "completion": "        InstancePropertyHelper.set_property(self, callable, name=name, reify=reify)", "idx": 862}
{"namespace": "pyramid.util.TopologicalSorter.remove", "completion": "        if name in self.names:\n            self.names.remove(name)\n            self.req_before.discard(name)\n            self.req_after.discard(name)\n            self.name2before.pop(name, None)\n            self.name2after.pop(name, None)\n            self.name2val.pop(name, None)\n", "idx": 863}
{"namespace": "pyramid.util.TopologicalSorter.add", "completion": "        if name in self.names:\n            raise ValueError('duplicate name %r' % name)\n        self.names.append(name)\n        self.name2val[name] = val\n        if after is None:\n            after = self.default_after\n        if before is None:\n            before = self.default_before\n        if after is not None:\n            if isinstance(after, str):\n                after = [after]\n            for u in after:\n                if u not in self.names:\n                    raise ValueError('unknown name %r' % u)\n                self.order.append((u, name))\n                self.req_after.add(u)\n            self.name2after[name] = after\n        if before is not None:\n            if isinstance(before, str):\n                before = [before]\n            for u in before:\n                if u not in self.names:\n                    raise ValueError('unknown name %r' % u)\n                self.order.append((name, u))\n                self.req_before.add(u)\n            self.name2before[name] = before\n", "idx": 864}
{"namespace": "pyramid.traversal.find_resource", "completion": "    if isinstance(path, str):\n        if path.startswith(\"/\"):\n            path = path[1:]\n            absolute = True\n        else:\n            absolute = False\n        path = path.split(\"/\")\n    else:\n        if path[0] == \"\":\n            path = path[1:]\n            absolute = True\n        else:\n            absolute = False\n    if absolute:\n        resource = find_root(resource)\n    for segment in path:\n        segment = unquote_to_bytes(segment)\n        if segment == b\"\":\n            continue\n        try:\n            resource = resource[segment]\n        except KeyError:\n            raise KeyError(segment)\n    return resource", "idx": 865}
{"namespace": "pyramid.static.ManifestCacheBuster.manifest", "completion": "        if self.reload:\n            if self.exists(self.manifest_path):\n                mtime = self.getmtime(self.manifest_path)\n                if mtime != self._mtime:\n                    self._mtime = mtime\n                    self._manifest = self.get_manifest()\n        return self._manifest\n", "idx": 866}
{"namespace": "pyramid.registry.Registry.registerSubscriptionAdapter", "completion": "        self.has_listeners = True\n        return super().registerSubscriptionAdapter(*arg, **kw)", "idx": 867}
{"namespace": "pyramid.registry.Registry.registerHandler", "completion": "        result = Components.registerHandler(self, *arg, **kw)\n        self.has_listeners = True\n        return result", "idx": 868}
{"namespace": "pyramid.registry.Registry.notify", "completion": "        if not self.has_listeners:\n            return\n\n        for event in events:\n            self.notifyObservers(event)", "idx": 869}
{"namespace": "pyramid.registry.Introspector.add", "completion": "        if intr.category not in self._categories:\n            self._categories[intr.category] = {}\n        if intr.discriminator not in self._categories[intr.category]:\n            self._categories[intr.category][intr.discriminator] = []\n        self._categories[intr.category][intr.discriminator].append(intr)\n        self._refs[intr.name] = intr\n        self._counter += 1\n        intr.order = self._counter\n", "idx": 870}
{"namespace": "pyramid.registry.Introspector.get", "completion": "        category = self._categories.get(category_name, {})\n        if discriminator in category:\n            return category[discriminator]\n        return default\n", "idx": 871}
{"namespace": "pyramid.registry.Introspector.get_category", "completion": "        category = self._categories.get(category_name, default)\n        if sort_key is not None:\n            category = sorted(category.values(), key=sort_key)\n        return [\n            {'introspectable': intr, 'related': self._refs.get(intr, [])}\n            for intr in category\n        ]", "idx": 872}
{"namespace": "pyramid.registry.Introspector.categorized", "completion": "        if sort_key is None:\n            sort_key = operator.attrgetter('order')\n        categories = self._categories.items()\n        categories = sorted(categories, key=lambda x: sort_key(x[1]))\n        return [\n            (category_name, sorted(introspectables, key=sort_key))\n            for category_name, introspectables in categories\n        ]", "idx": 873}
{"namespace": "pyramid.registry.Introspector.remove", "completion": "        # Retrieve the introspection object based on the category name and discriminator\n        intr = self.get(category_name, discriminator)\n\n        # If the object is found, remove all references to it and delete it from the category dictionary\n        if intr is not None:\n            for ref in self._refs.get(intr, []):\n                del self._refs[ref]\n            del self._categories[category_name][discriminator]\n            del self._categories[category_name][intr.discriminator_hash]\n", "idx": 874}
{"namespace": "pyramid.registry.Introspector.relate", "completion": "        introspectables = self._get_intrs_by_pairs(pairs)\n        for intr in introspectables:\n            self._refs.setdefault(intr, []).extend(introspectables)\n", "idx": 875}
{"namespace": "pyramid.registry.Introspector.related", "completion": "        category_name = intr.category_name\n        discriminator = intr.discriminator\n        category = self._categories.get(category_name)\n        if category is None:\n            raise KeyError((category_name, discriminator))\n        return self._refs.get(intr, [])", "idx": 876}
{"namespace": "pyramid.registry.Introspectable.discriminator_hash", "completion": "        self._assert_resolved()\n        return hash(s)\n", "idx": 877}
{"namespace": "pyramid.registry.Introspectable.__repr__", "completion": "        return '<%s category %r, discriminator %r>' % (\n            self.type_name,\n            self.category_name,\n            self.discriminator,\n        )", "idx": 878}
{"namespace": "pyramid.scripts.proutes.PRoutesCommand._get_mapper", "completion": "        mapper = registry.getUtility(registry.introspector.get('routes'))\n        return mapper", "idx": 879}
{"namespace": "pyramid.scripts.pshell.PShellCommand.make_shell", "completion": "        shells = self.find_all_shells()\n        if self.args.python_shell:\n            if self.args.python_shell in shells:\n                return shells[self.args.python_shell]\n            else:\n                raise ValueError(\n                    'could not find a shell named \"%s\"' % self.args.python_shell\n                )\n        else:\n            if self.preferred_shells:\n                for shell in self.preferred_shells:\n                    if shell in shells:\n                        return shells[shell]\n            else:\n                if shells:\n                    return list(shells.values())[0]\n        return self.default_runner", "idx": 880}
{"namespace": "pyramid.config.assets.PackageOverrides.insert", "completion": "        if path.endswith('/'):\n            override = DirectoryOverride(path, source)\n        else:\n            override = FileOverride(path, source)\n        self.overrides.insert(0, override)\n        return override", "idx": 881}
{"namespace": "pyramid.config.assets.PackageOverrides.filtered_sources", "completion": "        for override in self.overrides:\n            yield from override.filtered_sources(resource_name)", "idx": 882}
{"namespace": "pyramid.config.assets.PackageOverrides.real_loader", "completion": "        if self._real_loader is None:\n            raise NotImplementedError(\n                \"PackageOverrides.real_loader is not set\"\n            )\n        return self._real_loader\n\n", "idx": 883}
{"namespace": "pyramid.config.views.MultiView.add", "completion": "        if phash is None:\n            phash = DEFAULT_PHASH\n        if accept is None:\n            self.views.append((view, order, phash))\n            self.views.sort(key=lambda x: x[1])\n        else:\n            if accept_order is None:\n                accept_order = DEFAULT_PHASH\n            if phash not in self.media_views:\n                self.media_views[phash] = []\n            self.media_views[phash].append((view, order, accept_order))\n            self.media_views[phash].sort(key=lambda x: x[2])\n            self.accepts.append((accept, phash))\n            self.accepts.sort(key=lambda x: x[1])\n", "idx": 884}
{"namespace": "pyramid.config.views.MultiView.get_views", "completion": "        if request.accept and self.accepts:\n            views = []\n            for accept in request.accept:\n                for offer in self.accepts:\n                    if accept.accepts(offer):\n                        views.extend(self.media_views.get(offer, []))\n            views.extend(self.views)\n            return views\n        else:\n            return self.views\n", "idx": 885}
{"namespace": "pyramid.config.views.MultiView.match", "completion": "        views = self.get_views(request)\n        for order, view, phash in views:\n            if hasattr(view, '__predicated__'):\n                if view.__predicated__(context, request):\n                    return view\n            else:\n                return view\n        raise PredicateMismatch(self, context, request)\n\n", "idx": 886}
{"namespace": "pyramid.config.views.MultiView.__permitted__", "completion": "        view = self.match(context, request)\n        if not hasattr(view, '__permitted__'):\n            return True\n        return view.__permitted__(context, request)\n", "idx": 887}
{"namespace": "pyramid.config.views.MultiView.__call_permissive__", "completion": "        view = self.match(context, request)\n        if hasattr(view, '__call_permissive__'):\n            return view.__call_permissive__(context, request)\n        return view(context, request)\n\n", "idx": 888}
{"namespace": "pyramid.config.actions.ActionState.processSpec", "completion": "        if spec in self._seen_files:\n            return False\n        self._seen_files.add(spec)\n        return True", "idx": 889}
{"namespace": "pyramid.config.actions.ActionState.action", "completion": "        if kw is None:\n            kw = {}\n        action = extra\n        action.update(\n            dict(\n                discriminator=discriminator,\n                callable=callable,\n                args=args,\n                kw=kw,\n                order=order,\n                includepath=includepath,\n                info=info,\n                introspectables=introspectables,\n            )\n        )\n        self.actions.append(action)\n", "idx": 890}
{"namespace": "pyramid.config.actions.ActionInfo.__str__", "completion": "        if self.file is None:\n            return \"<no file>\"\n        if self.line is None:\n            return \"<no line>\"\n        if self.function is None:\n            return \"<no function>\"\n        if self.src is None:\n            return \"<no src>\"\n        return \"Line %s of file %s:\\n%s\" % (\n            self.line,\n            self.file,\n            \"\\n\".join([\"  \" + line for line in self.src.split(\"\\n\")]),\n        )\n", "idx": 891}
{"namespace": "pyramid.config.Configurator.__getattr__", "completion": "        if hasattr(self.registry, '_directives'):\n            if name in self.registry._directives:\n                c, action_wrap = self.registry._directives[name]\n                if action_wrap:\n                    c = self.action(c)\n                return c.__get__(self, self.__class__)\n        raise AttributeError(name)", "idx": 892}
{"namespace": "pyramid.config.Configurator.with_package", "completion": "        return self.__class__(\n            registry=self.registry,\n            package=package,\n            settings=self.registry.settings,\n            root_factory=self.registry.root_factory,\n            security_policy=self.registry.security_policy,\n            authentication_policy=self.registry.authentication_policy,\n            authorization_policy=self.registry.authorization_policy,\n            renderers=self.registry.renderers,\n            debug_logger=self.registry.debug_logger,\n            locale_negotiator=self.registry.locale_negotiator,\n            request_factory=self.registry.request_factory,\n            response_factory=self.registry.response_factory,\n            default_permission=self.registry.default_permission,\n            session_factory=self.registry.session_factory,\n            default_view_mapper=self.registry.default_view_mapper,\n            autocommit=self.autocommit,\n            exceptionresponse_view=self.registry.exceptionresponse_view,\n            route_prefix=self.route_prefix,\n            introspection=self.introspection,\n            root_package=self.root_package,\n        )", "idx": 893}
{"namespace": "pyramid.config.Configurator.absolute_asset_spec", "completion": "        if isinstance(relative_spec, str):\n            if relative_spec.startswith(':'):\n                return relative_spec\n            return self._make_spec(relative_spec)\n        return relative_spec", "idx": 894}
{"namespace": "pyramid.config.Configurator.begin", "completion": "        if request is _marker:\n            request = self.registry.getUtility(IRequestFactory)()\n        self.manager.push({'registry': self.registry, 'request': request})\n", "idx": 895}
{"namespace": "pyramid.config.Configurator.scan", "completion": "        if package is None:\n            package = caller_package()\n        scanner = venusian.Scanner(\n            categories=categories, onerror=onerror, ignore=ignore, **kw\n        )\n        scanner.scan(package)", "idx": 896}
{"namespace": "pyramid.config.Configurator.make_wsgi_app", "completion": "        self.commit()\n        self.registry.notify(ApplicationCreated(self.registry))\n        self.registry.set_global()\n        return self.registry.get_wsgi_app()\n", "idx": 897}
{"namespace": "aiohappybase._util.camel_case_to_pep8", "completion": "    return CAPITALS.sub(r'_\\1', name).lower()\n\n", "idx": 898}
{"namespace": "kinto.authorization._relative_object_uri", "completion": "    # Split the object URI into parts\n    parts = object_uri.split(\"/\")\n\n    # Iterate through each part of the object URI\n    for i in range(len(parts) - 1, 0, -1):\n        # Get the parent URI\n        parent_uri = \"/\".join(parts[:i])\n\n        # Get the parent resource name\n        parent_resource_name, _ = _resource_endpoint(parent_uri)\n\n        # Check if the resource name matches the parent resource name\n        if resource_name == parent_resource_name:\n            # Return the parent URI\n            return parent_uri\n\n    # If no match is found, raise a ValueError\n    raise ValueError(\n        \"The resource name does not match the parent resource name. \"\n        \"Please check the resource name and try again.\"\n    )", "idx": 899}
{"namespace": "kinto.core.openapi.OpenAPI.expose_authentication_method", "completion": "        cls.security_definitions[method_name] = definition\n        for scope in definition[\"scopes\"]:\n            cls.security_roles[scope] = method_name", "idx": 900}
{"namespace": "kinto.core.openapi.OpenAPI.generate", "completion": "        base_spec = {\n            \"host\": self.settings[\"http_host\"],\n            \"schemes\": [self.settings[\"http_scheme\"]],\n            \"securityDefinitions\": self.security_definitions,\n        }\n\n        return super().generate(swagger=base_spec)", "idx": 901}
{"namespace": "kinto.core.testing.get_user_headers", "completion": "    import base64\n\n    auth = f\"{user}:{password}\"\n    auth_encoded = base64.b64encode(auth.encode(\"utf-8\")).decode(\"utf-8\")\n    return {\"Authorization\": f\"Basic {auth_encoded}\"}", "idx": 902}
{"namespace": "kinto.core.authorization.RouteFactory.fetch_shared_objects", "completion": "        if self.shared_ids is not None:\n            return self.shared_ids\n\n        if self.on_plural_endpoint:\n            # If the permission is not dynamic, we can use the permission backend to get the shared objects.\n            if perm != DYNAMIC:\n                self.shared_ids = self._get_accessible_objects(\n                    principals, self._object_id_match, perm\n                )\n                return self.shared_ids\n\n            # If the permission is dynamic, we need to get the bound permissions for the object ID match.\n            if get_bound_permissions is None:\n                return None\n\n            # Get the bound permissions for the object ID match.\n            bound_perms = get_bound_permissions(self._object_id_match, perm)\n\n            # Get the shared objects for the bound permissions.\n            self.shared_ids = self._get_accessible_objects(principals, bound_perms)\n            return self.shared_ids\n\n        # If the permission is not dynamic, we can use the permission backend to get the shared objects.\n        if perm != DYNAMIC:\n            self.shared_ids = self._get_accessible_objects(\n                principals, self.permission_object_id, perm\n            )\n            return self.shared_ids\n\n        # If the permission is dynamic, we need to get the bound permissions for the object ID.\n        if get_bound_permissions is None:\n            return None\n\n        # Get the bound permissions for the object ID.\n        bound_perms = get_bound_permissions(self.permission_object_id, perm)\n\n        # Get the shared objects for the bound permissions.\n        self.shared_ids = self._get_accessible_objects(principals, bound_perms)\n        return self.shared_ids\n", "idx": 903}
{"namespace": "kinto.core.authorization.RouteFactory.get_permission_object_id", "completion": "        if object_id is None:\n            object_id = request.matchdict.get(\"id\", None)\n        if object_id is None:\n            return None\n        if self.on_plural_endpoint:\n            # If the request is on a plural endpoint, we need to find the object URI.\n            # We do this by inspecting the \"plural\" service and its sibling \"object\" service.\n            plural_service = request.registry.get(self.resource_name, None)\n            if plural_service is None:\n                return None\n            object_service = request.registry.get(plural_service.object_name, None)\n            if object_service is None:\n                return None\n            object_uri = object_service.get_uri(object_id)\n        else:\n            object_uri = self.get_uri(object_id)\n        return object_uri\n", "idx": 904}
{"namespace": "kinto.core.utils.recursive_update_dict", "completion": "    for k, v in changes.items():\n        if isinstance(v, dict):\n            if k not in root:\n                root[k] = {}\n            recursive_update_dict(root[k], v, ignores)\n        elif k not in ignores:\n            root[k] = v", "idx": 905}
{"namespace": "kinto.core.utils.native_value", "completion": "    try:\n        return json.loads(value)\n    except ValueError:\n        return value", "idx": 906}
{"namespace": "kinto.core.utils.dict_subset", "completion": "    result = {}\n    for key in keys:\n        if \".\" in key:\n            parts = key.split(\".\")\n            value = d\n            for part in parts:\n                if part in value:\n                    value = value[part]\n                else:\n                    break\n            else:\n                result[key] = value\n        else:\n            if key in d:\n                result[key] = d[key]\n    return result", "idx": 907}
{"namespace": "kinto.core.utils.dict_merge", "completion": "    result = a.copy()\n    for key, value in b.items():\n        if key in result and isinstance(value, dict):\n            result[key] = dict_merge(result[key], value)\n        else:\n            result[key] = value\n    return result", "idx": 908}
{"namespace": "kinto.core.utils.find_nested_value", "completion": "    if path in d:\n        return d[path]\n\n    root_keys = path.split(\".\")\n    for i in range(len(root_keys) - 1, -1, -1):\n        root_key = \".\".join(root_keys[: i + 1])\n        if root_key in d:\n            subpath = \".\".join(root_keys[i + 1 :])\n            if isinstance(d[root_key], dict):\n                return find_nested_value(d[root_key], subpath, default)\n            else:\n                return default\n\n    return default", "idx": 909}
{"namespace": "kinto.core.utils.instance_uri_registry", "completion": "    request = Request.blank(\"/\")\n    request.registry = registry\n    return strip_uri_prefix(request.route_path(f\"{resource_name}-object\", **params))", "idx": 910}
{"namespace": "kinto.core.statsd.load_from_config", "completion": "    if statsd_module is None:\n        raise ConfigurationError(\"statsd module is not installed\")\n\n    statsd_url = config.get_settings().get(\"statsd_url\")\n    if statsd_url is None:\n        raise ConfigurationError(\"statsd_url is not set\")\n\n    parsed_url = urlparse(statsd_url)\n    hostname = parsed_url.hostname\n    port = parsed_url.port\n    prefix = parsed_url.path.strip(\"/\")\n\n    return Client(hostname, port, prefix)", "idx": 911}
{"namespace": "kinto.core.errors.http_error", "completion": "    if errno is None:\n        errno = ERRORS.UNDEFINED.value\n\n    if code is None:\n        code = httpexception.code\n\n    if error is None:\n        error = httpexception.title\n\n    response = httpexception(\n        json_body={\n            \"code\": code,\n            \"errno\": errno,\n            \"error\": error,\n            \"message\": message,\n            \"info\": info,\n            \"details\": details,\n        }\n    )\n    response.content_type = \"application/json\"\n    return response", "idx": 912}
{"namespace": "kinto.core.resource.schema.ResourceReponses.get_and_bind", "completion": "        # Get the default schemas for the endpoint type and method\n        default_schemas = self.default_schemas.copy()\n        default_schemas.update(self.default_object_schemas)\n        default_schemas.update(self.default_plural_schemas)\n        default_schemas.update(self.default_get_schemas)\n        default_schemas.update(self.default_post_schemas)\n        default_schemas.update(self.default_put_schemas)\n        default_schemas.update(self.default_patch_schemas)\n        default_schemas.update(self.default_delete_schemas)\n        default_schemas.update(self.object_get_schemas)\n        default_schemas.update(self.object_patch_schemas)\n        default_schemas.update(self.object_delete_schemas)\n\n        # Get the endpoint-specific schemas for the endpoint type and method\n        endpoint_schemas = getattr(self, endpoint_type + \"_schemas\", {})\n        endpoint_schemas = endpoint_schemas.get(method, {})\n\n        # Get the method-specific schemas for the endpoint type and method\n        method_schemas = getattr(self, method + \"_schemas\", {})\n        method_schemas = method_schemas.get(endpoint_type, {})\n\n        # Merge the default, endpoint, and method schemas\n        schemas = default_schemas.copy()\n        schemas.update(endpoint_schemas)\n        schemas.update(method_schemas)\n\n        # Clone and bind the schemas\n        cloned_schemas = {}\n        for status_code, schema in schemas.items():\n            cloned_schemas[status_code] = schema.bind(**kwargs)\n\n        return cloned_schemas", "idx": 913}
{"namespace": "kinto.core.resource.Resource.timestamp", "completion": "        try:\n            return self.model.timestamp\n        except AttributeError:\n            raise_invalid(\n                http_error(\n                    HTTPNotModified,\n                    code=ERRORS.INVALID_RESOURCE_ID.code,\n                    message=ERRORS.INVALID_RESOURCE_ID.message,\n                )\n            )\n", "idx": 914}
{"namespace": "kinto.core.resource.Resource.plural_post", "completion": "        self._raise_412_if_modified(obj={})\n\n        if self.object_id:\n            self._raise_400_if_invalid_id(self.object_id)\n            self._raise_409_if_exists()\n\n        data = self.request.validated[\"data\"]\n        data[self.model.id_field] = self.object_id or self.id_generator()\n\n        if self.model.modified_field in data:\n            self._raise_400_if_invalid_modified(data[self.model.modified_field])\n\n        if self.model.deleted_field in data:\n            self._raise_400_if_invalid_deleted(data[self.model.deleted_field])\n\n        self._raise_400_if_invalid_data(data)\n\n        # If the object is not created, it means it already exists.\n        # We return it with a 200 status code.\n        try:\n            obj = self.model.create_object(data)\n        except storage_exceptions.ObjectConflictError:\n            obj = self.model.get_object(self.object_id)\n            return self.postprocess(obj, status=200)\n\n        self.request.response.status = 201\n        return self.postprocess(obj)\n", "idx": 915}
{"namespace": "kinto.core.resource.Resource.get", "completion": "        self._add_timestamp_header(self.request.response)\n        self._add_cache_header(self.request.response)\n        self._raise_304_if_not_modified()\n        self._raise_412_if_modified()\n\n        partial_fields = self._extract_partial_fields()\n        obj = self._get_object_or_404(self.object_id)\n\n        if partial_fields:\n            obj = dict_subset(obj, partial_fields)\n\n        return self.postprocess(obj)\n", "idx": 916}
{"namespace": "kinto.core.resource.Resource.delete", "completion": "        self._raise_400_if_invalid_id(self.object_id)\n        existing = self._get_object_or_404(self.object_id)\n        self._raise_412_if_modified(existing)\n\n        deleted = self.model.delete_object(self.object_id)\n        timestamp = deleted[self.model.modified_field]\n        self._add_timestamp_header(self.request.response, timestamp=timestamp)\n\n        return self.postprocess(deleted, action=ACTIONS.DELETE, old=existing)", "idx": 917}
{"namespace": "kinto.core.permission.memory.Permission.add_principal_to_ace", "completion": "        # Retrieve the set of principals associated with the given object and permission from the store.\n        ace_key = f\"{object_id}:{permission}\"\n        principals = self._store.get(ace_key, set())\n\n        # Add the new principal to the set.\n        principals.add(principal)\n\n        # Update the store with the modified set.\n        self._store[ace_key] = principals\n", "idx": 918}
{"namespace": "kinto.core.permission.memory.Permission.get_object_permission_principals", "completion": "        permission_key = f\"permission:{object_id}:{permission}\"\n        return self._store.get(permission_key, set())", "idx": 919}
{"namespace": "kinto.core.storage.generators.Generator.match", "completion": "        if self._regexp is None:\n            self._regexp = re.compile(self.regexp)\n\n        return self._regexp.match(object_id)\n", "idx": 920}
{"namespace": "kinto.core.storage.postgresql.migrator.MigratorMixin.create_or_migrate_schema", "completion": "        installed_version = self.get_installed_version()\n        if installed_version is None:\n            logger.info(f\"Creating new {self.name} schema\")\n            self.create_schema(dry_run=dry_run)\n        elif installed_version == self.schema_version:\n            logger.info(f\"{self.name} schema is up-to-date\")\n        else:\n            logger.info(f\"Migrating {self.name} schema from {installed_version} to {self.schema_version}\")\n            self.migrate_schema(dry_run=dry_run)\n", "idx": 921}
{"namespace": "kinto.core.views.batch.BatchPayloadSchema.deserialize", "completion": "        if cstruct is colander.null:\n            return cstruct\n\n        # Deserialize the data.\n        deserialized = super().deserialize(cstruct)\n\n        # Merge the defaults with the requests.\n        for request in deserialized[\"requests\"]:\n            for key, value in deserialized[\"defaults\"].items():\n                if key not in request:\n                    request[key] = value\n\n        return deserialized", "idx": 922}
{"namespace": "kinto.plugins.accounts.utils.get_cached_reset_password", "completion": "    settings = registry.settings\n    hmac_secret = settings[\"userid_hmac_secret\"]\n    cache_key = utils.hmac_digest(hmac_secret, ACCOUNT_RESET_PASSWORD_CACHE_KEY.format(username))\n    cache = registry.cache\n    return cache.get(cache_key)", "idx": 923}
{"namespace": "kinto.plugins.accounts.utils.get_cached_validation_key", "completion": "    hmac_secret = registry.settings[\"userid_hmac_secret\"]\n    cache_key = utils.hmac_digest(hmac_secret, ACCOUNT_VALIDATION_CACHE_KEY.format(username))\n\n    cache = registry.cache\n    cache_result = cache.get(cache_key)\n    return cache_result", "idx": 924}
{"namespace": "kinto.plugins.accounts.views.validation.on_account_activated", "completion": "    settings = event.request.registry.settings\n    account_validation_enabled = settings.get(\"account_validation.enabled\", False)\n    if not account_validation_enabled:\n        return\n\n    for impacted_object in event.impacted_objects:\n        if impacted_object.get(\"old\", {}).get(\"validated\", False):\n            # The old account was validated, so we don't need to send a confirmation email.\n            continue\n        if not impacted_object.get(\"new\", {}).get(\"validated\", False):\n            # The new account is not validated, so we don't need to send a confirmation email.\n            continue\n\n        # Send a confirmation email to the account.\n        Emailer(event.request, impacted_object[\"new\"]).send_confirmation()", "idx": 925}
{"namespace": "kinto.plugins.openid.OpenIDConnectPolicy._verify_token", "completion": "        # Fetch userinfo\n        userinfo_endpoint = self.oid_config[\"userinfo_endpoint\"]\n        headers = {\"Authorization\": f\"Bearer {access_token}\"}\n        response = requests.get(userinfo_endpoint, headers=headers)\n        if response.status_code != 200:\n            self.request.logger.debug(\n                f\"Error fetching userinfo: {response.status_code} {response.text}\"\n            )\n            return None\n        return response.json()", "idx": 926}
{"namespace": "kinto.plugins.quotas.scripts.rebuild_quotas", "completion": "    for bucket in storage.list_buckets():\n        logger.info(\"Rebuilding quotas for bucket %s\", bucket[\"id\"])\n        quota = storage.get_quota(bucket[\"id\"])\n        if quota is None:\n            logger.info(\"No quota found for bucket %s\", bucket[\"id\"])\n            continue\n\n        # Get the total record count, storage size, and collection count for the bucket.\n        record_count, storage_size, collection_count = get_bucket_stats(storage, bucket[\"id\"])\n\n        # Update the quota information for the bucket.\n        quota[\"record_count\"] = record_count\n        quota[\"storage_size\"] = storage_size\n        quota[\"collection_count\"] = collection_count\n        if not dry_run:\n            storage.update_quota(bucket[\"id\"], quota)\n\n        # Log the final size of the bucket.\n        logger.info(\"Bucket %s has %s records, %s bytes of storage, and %s collections\", bucket[\"id\"], record_count, storage_size, collection_count)\n\n", "idx": 927}
{"namespace": "kinto.config.render_template", "completion": "    with codecs.open(template, encoding='utf-8') as f:\n        content = f.read()\n\n    for key, value in kwargs.items():\n        content = content.replace('{{%s}}' % key, value)\n\n    with codecs.open(destination, 'w', encoding='utf-8') as f:\n        f.write(content)\n\n", "idx": 928}
{"namespace": "trafilatura.sitemaps.SitemapObject.extract_sitemap_langlinks", "completion": "        # extract hreflang links\n        hreflang_links = re.findall(HREFLANG_REGEX, self.content)\n        for attr in hreflang_links:\n            if attr.startswith(self.target_lang):\n                self.handle_link(attr)\n        LOGGER.debug('%s sitemaps and %s links with hreflang found for %s', len(self.sitemap_urls), len(self.urls), self.target_lang)", "idx": 929}
{"namespace": "trafilatura.sitemaps.SitemapObject.extract_sitemap_links", "completion": "        # compile regex here for modularity and efficiency\n        sitemap_regex = re.compile(r'<loc>(?:<\\!\\[CDATA\\[)?(http.+?)(?:\\]\\]>)?</loc>', re.DOTALL)\n        # extract\n        for match in islice(sitemap_regex.finditer(self.content), MAX_LINKS):\n            self.handle_link(match[1])\n        LOGGER.debug('%s sitemaps and %s links found for %s', len(self.sitemap_urls), len(self.urls), self.sitemap_url)", "idx": 930}
{"namespace": "trafilatura.sitemaps.SitemapObject.process", "completion": "        # check if sitemap is plausible\n        if not SITEMAP_FORMAT.match(self.content):\n            LOGGER.warning('sitemap not plausible: %s', self.sitemap_url)\n            return\n\n        # try to extract links from a TXT file\n        if self.content.startswith('http'):\n            self.urls = filter_urls(self.content)\n            LOGGER.debug('%s links found in %s', len(self.urls), self.sitemap_url)\n            return\n\n        # iterate through the content to find links\n        for link in (m[0] for m in islice(DETECT_LINKS.finditer(self.content), MAX_LINKS)):\n            self.handle_link(link)\n\n        # if there are sitemap URLs or URLs extracted from the sitemap, return\n        if self.sitemap_urls or self.urls:\n            return\n\n        # extract the links from the sitemap\n        if self.target_lang:\n            self.extract_sitemap_langlinks()\n        else:\n            self.extract_sitemap_links()", "idx": 931}
{"namespace": "trafilatura.sitemaps.is_plausible_sitemap", "completion": "    # Check if the URL ends with .txt or .xml\n    if not url.endswith(('.txt', '.xml')):\n        LOGGER.warning('sitemap URL not in expected format: %s', url)\n        return False\n\n    # Check if the URL is a valid URL\n    if not is_valid_url(url):\n        LOGGER.warning('sitemap URL not valid: %s', url)\n        return False\n\n    # Check if the contents of the sitemap are in the expected format\n    if contents is not None and not SITEMAP_FORMAT.match(contents):\n        LOGGER.warning('sitemap contents not in expected format: %s', url)\n        return False\n\n    # If all checks pass, the sitemap is plausible and in the expected format\n    return True", "idx": 932}
{"namespace": "trafilatura.sitemaps.extract_robots_sitemaps", "completion": "    # Initialize an empty list to store the sitemap URLs\n    sitemaps = []\n\n    # Split the input text into lines\n    lines = robotstxt.split('\\n')\n\n    # Iterate over the lines\n    for line in lines:\n        # Remove comments and empty lines\n        line = line.split('#')[0].strip()\n        if not line:\n            continue\n\n        # Check if the line contains the word \"sitemap\"\n        if 'sitemap' in line.lower():\n            # Extract the URL from the line\n            url = line.split(':')[1].strip()\n\n            # Resolve relative URLs\n            if not url.startswith('http'):\n                url = baseurl + url\n\n            # Add the URL to the list of sitemap URLs\n            sitemaps.append(url)\n\n    # Return the list of sitemap URLs\n    return sitemaps", "idx": 933}
{"namespace": "trafilatura.feeds.handle_link_list", "completion": "    ## This function is available from https://github.com/adbar/trafilatura\n    ## under GNU GPL v3 license\n\n    ## TODO: add support for target_lang\n\n    ## TODO: add support for baseurl\n\n    ## TODO: add support for domainname\n\n    ## TODO: add support for linklist\n\n    ## TODO: add support for linklist\n\n    ## TODO: add support for linklist\n\n    ## TODO: add support for linklist\n\n    ## TODO: add support for linklist\n\n    ## TODO: add support for linklist\n\n    ## TODO: add support for linklist\n\n    ## TODO: add support for linklist\n\n    ## TODO: add support for linklist\n\n    ## TODO: add support for linklist\n\n    ## TODO: add support for linklist\n\n    ## TODO: add support for linklist\n\n    ## TODO: add support for linklist\n\n    ## TODO: add support for linklist\n\n    ## TODO: add support for linklist\n\n    ## TODO: add support for linklist\n\n    ## TODO: add support for linklist\n\n    ## TODO: add support for linklist\n\n    ## TODO: add support for linklist\n\n    ## TODO: add support for linklist\n\n    ## TODO: add support for linklist\n\n    ## TODO: add support for linklist\n\n    ## TODO: add support for linklist\n\n    ## TODO: add support for linklist\n\n    ## TODO: add support for linklist\n\n    ## TODO: add support for linklist\n\n    ## TODO: add support for linklist\n\n    ## TODO: add support for linklist\n\n    ## TODO: add support for linklist\n\n    ## TODO: add support for linklist\n\n    ## TODO: add support for linklist\n\n    ## TODO: add support for linklist\n\n    ## TODO: add support for linklist\n\n    ## TODO: add support for linklist\n\n    ## TODO: add support for linklist\n\n    ## TODO: add support", "idx": 934}
{"namespace": "trafilatura.feeds.find_feed_urls", "completion": "    # get domain name and base URL\n    domainname, baseurl = get_hostinfo(url)\n    # fetch webpage content\n    htmlstring = fetch_url(url)\n    # check if it's a feed\n    if htmlstring is None:\n        LOGGER.debug('Invalid feed page: %s', url)\n        return []\n    # check if it's a feed\n    if htmlstring.startswith('<?xml'):\n        # extract feed links\n        feed_links = extract_links(htmlstring, domainname, baseurl, url, target_lang)\n        # filter URLs\n        feed_links = filter_urls(feed_links, target_lang)\n        # return feed links\n        return feed_links\n    # determine feed\n    feed_urls = determine_feed(htmlstring, baseurl, url)\n    # filter URLs\n    feed_urls = filter_urls(feed_urls, target_lang)\n    # return feed URLs\n    return feed_urls", "idx": 935}
{"namespace": "trafilatura.hashing.generate_hash_filename", "completion": "    # Remove any potential XML tags from the content\n    content = re.sub(CLEAN_XML, \"\", content)\n\n    # Generate a bag-of-word hashing of length 12\n    bow_hash = generate_bow_hash(content, 12)\n\n    # Encode the hash using urlsafe_b64encode and return as a decoded string\n    return urlsafe_b64encode(bow_hash).decode()", "idx": 936}
{"namespace": "trafilatura.cli_utils.download_queue_processing", "completion": "    # load download buffer\n    download_buffer = load_download_buffer(args.download_buffer)\n    # process downloads\n    if args.threads > 1:\n        with ThreadPoolExecutor(max_workers=args.threads) as executor:\n            futures = {executor.submit(download_buffer.get_download, url): url for url in url_store}\n            for future in as_completed(futures):\n                url = futures[future]\n                try:\n                    htmlstring = future.result()\n                except Exception as exc:\n                    LOGGER.error('%r generated an exception: %s', url, exc)\n                    continue\n                # process result\n                counter = process_result(htmlstring, args, url, counter, config)\n    else:\n        for url in url_store:\n            htmlstring = download_buffer.get_download(url)\n            # process result\n            counter = process_result(htmlstring, args, url, counter, config)\n    # return errors and updated counter\n    return [], counter", "idx": 937}
{"namespace": "trafilatura.utils.decode_response", "completion": "    # if response is already a string, return it\n    if isinstance(response, str):\n        return response\n    # if response is a bytes object, try to decode it\n    if isinstance(response, bytes):\n        # source: https://stackoverflow.com/questions/3703276/how-to-tell-if-a-file-is-gzip-compressed\n        if response[:2] == b'\\x1f\\x8b':\n            # decode GZipped data\n            try:\n                response = decompress(response)\n            except (EOFError, OSError):\n                logging.warning('invalid GZ file')\n        # try brotli\n        elif brotli is not None:\n            try:\n                response = brotli.decompress(response)\n            except brotli.error:\n                pass  # logging.debug('invalid Brotli file')\n        # try to guess encoding\n        encoding = detect_encoding(response)\n        if encoding:\n            try:\n                response = response.decode(encoding[0])\n            except UnicodeDecodeError:\n                pass\n    # if response is a urllib3 response object, try to decode it\n    elif isinstance(response, HTTPResponse):\n        # try to guess encoding\n        encoding = detect_encoding(response.data)\n        if encoding:\n            try:\n                response = response.data.decode(encoding[0])\n            except UnicodeDecodeError:\n                pass\n    # return the response\n    return response", "idx": 938}
{"namespace": "trafilatura.utils.txttocsv", "completion": "    # Initialize the CSV string with the header row\n    csv_string = \"URL\\tFingerprint\\tHostname\\tTitle\\tImage\\tDate\\tLicense\\tPagetype\\tID\\n\"\n\n    # Add the main text content to the CSV string\n    csv_string += f\"{text}\\n\"\n\n    # Add the comments to the CSV string\n    csv_string += f\"{comments}\\n\"\n\n    # Add the metadata information to the CSV string\n    csv_string += f\"{docmeta.url}\\t{docmeta.fingerprint}\\t{docmeta.hostname}\\t{docmeta.title}\\t{docmeta.image}\\t{docmeta.date}\\t{docmeta.license}\\t{docmeta.pagetype}\\t{docmeta.id}\\n\"\n\n    # Return the CSV string\n    return csv_string", "idx": 939}
{"namespace": "trafilatura.utils.is_similar_domain", "completion": "    # remove file extension\n    reference = STRIP_EXTENSION.sub('', reference)\n    new_string = STRIP_EXTENSION.sub('', new_string)\n    # remove trailing slash\n    reference = reference.rstrip('/')\n    new_string = new_string.rstrip('/')\n    # remove www.\n    reference = reference.replace('www.', '')\n    new_string = new_string.replace('www.', '')\n    # remove http(s)\n    reference = URL_BLACKLIST_REGEX.sub('', reference)\n    new_string = URL_BLACKLIST_REGEX.sub('', new_string)\n    # remove trailing slash\n    reference = reference.rstrip('/')\n    new_string = new_string.rstrip('/')\n    # remove trailing www\n    reference = reference.replace('www.', '')\n    new_string = new_string.replace('www.', '')\n    # remove trailing slash\n    reference = reference.rstrip('/')\n    new_string = new_string.rstrip('/')\n    # remove trailing www\n    reference = reference.replace('www.', '')\n    new_string = new_string.replace('www.', '')\n    # remove trailing slash\n    reference = reference.rstrip('/')\n    new_string = new_string.rstrip('/')\n    # remove trailing www\n    reference = reference.replace('www.', '')\n    new_string = new_string.replace('www.', '')\n    # remove trailing slash\n    reference = reference.rstrip('/')\n    new_string = new_string.rstrip('/')\n    # remove trailing www\n    reference = reference.replace('www.', '')\n    new_string = new_string.replace('www.', '')\n    # remove trailing slash\n    reference = reference.rstrip('/')\n    new_string = new_string.rstrip('/')\n    # remove trailing www\n    reference = reference.replace('www.', '')\n    new_string = new_string.replace('www.", "idx": 940}
{"namespace": "trafilatura.xml.remove_empty_elements", "completion": "    for elem in tree.iter():\n        if elem.getchildren() and elem.text and elem.tail:\n            continue\n        if elem.getchildren():\n            continue\n        if elem.text:\n            continue\n        if elem.tail:\n            continue\n        elem.getparent().remove(elem)", "idx": 941}
{"namespace": "trafilatura.xml.strip_double_tags", "completion": "    for element in reversed(tree.iter()):\n        if element.tag in NESTING_WHITELIST:\n            for child in element.getchildren():\n                if child.tag in NESTING_WHITELIST:\n                    element.insert(0, child)\n                    element.remove(child)\n    return tree", "idx": 942}
{"namespace": "trafilatura.xml.check_tei", "completion": "    # check for missing <head>\n    if xmldoc.find('head') is None:\n        head = SubElement(xmldoc, 'head')\n        head.text = 'Title'\n\n    # check for missing <title>\n    if xmldoc.find('title') is None:\n        title = SubElement(xmldoc, 'title')\n        title.text = 'Title'\n\n    # check for missing <author>\n    if xmldoc.find('author') is None:\n        author = SubElement(xmldoc, 'author')\n        author.text = 'Author'\n\n    # check for missing <date>\n    if xmldoc.find('date') is None:\n        date = SubElement(xmldoc, 'date')\n        date.text = 'Date'\n\n    # check for missing <source>\n    if xmldoc.find('source') is None:\n        source = SubElement(xmldoc, 'source')\n        source.text = url\n\n    # check for missing <text>\n    if xmldoc.find('text') is None:\n        text = SubElement(xmldoc, 'text')\n        text.text = 'Text'\n\n    # check for missing <body>\n    if xmldoc.find('body') is None:\n        body = SubElement(xmldoc, 'body')\n        body.text = 'Body'\n\n    # check for missing <back>\n    if xmldoc.find('back') is None:\n        back = SubElement(xmldoc, 'back')\n        back.text = 'Back'\n\n    # check for missing <front>\n    if xmldoc.find('front') is None:\n        front = SubElement(xmldoc, 'front')\n        front.text = 'Front'\n\n    # check for missing <div>\n    if xmldoc.find('div') is None:\n        div = SubElement(xmldoc, 'div')\n        div.text = 'Div'\n\n    # check for missing <p>", "idx": 943}
{"namespace": "trafilatura.xml.validate_tei", "completion": "    global TEI_RELAXNG\n    if TEI_RELAXNG is None:\n        TEI_RELAXNG = RelaxNG(lzma.open(TEI_SCHEMA))\n    return TEI_RELAXNG.validate(xmldoc)", "idx": 944}
{"namespace": "trafilatura.xml.merge_with_parent", "completion": "    parent = element.getparent()\n    if parent is not None:\n        parent.text = (parent.text or '') + replace_element_text(element, include_formatting)\n        parent.remove(element)", "idx": 945}
{"namespace": "trafilatura.downloads._determine_headers", "completion": "    myagents, mycookie = _parse_config(config)\n    if myagents is not None:\n        headers['User-Agent'] = random.choice(myagents)\n    if mycookie is not None:\n        headers['Cookie'] = mycookie\n    if headers is None:\n        headers = DEFAULT_HEADERS\n    return headers", "idx": 946}
{"namespace": "trafilatura.meta.reset_caches", "completion": "    # Reset the caches for the htmldate module.\n    reset_caches_htmldate()\n\n    # Reset the caches for the courlan module.\n    reset_caches_courlan()\n\n    # Clear the stoplist.\n    define_stoplist([])\n\n    # Release some memory.\n    gc.collect()", "idx": 947}
{"namespace": "trafilatura.core.handle_table", "completion": "    # create new table\n    newtable = Element('table')\n    newrow = Element('row')\n    # iterate through sub-elements\n    for child in table_elem.iter('*'):\n        # check for row\n        if child.tag == 'tr':\n            # check if row is complete\n            if len(newrow) > 0:\n                # append row to table\n                newtable.append(newrow)\n                # start new row\n                newrow = Element('row')\n        # check for cell\n        elif child.tag in TABLE_ELEMS:\n            # define cell type\n            cell_element = define_cell_type(child)\n            # iterate through descendants\n            for subchild in child.iterdescendants('*'):\n                # check for nested table\n                if subchild.tag == 'table':\n                    # ignore nested table\n                    continue\n                # process node\n                processed_subchild = process_node(subchild, options)\n                # add processed node to cell\n                if processed_subchild is not None:\n                    cell_element.append(processed_subchild)\n                # remove subchild from tree\n                subchild.tag = 'done'\n            # add cell to row\n            newrow.append(cell_element)\n        # remove child from tree\n        child.tag = 'done'\n    # append last row to table\n    if len(newrow) > 0:\n        newtable.append(newrow)\n    # return table if it contains rows\n    if len(newtable) > 0:\n        return newtable\n    return None", "idx": 948}
{"namespace": "trafilatura.filters.language_filter", "completion": "    if target_language:\n        detected_language = language_classifier(temp_text, temp_comments)\n        if detected_language != target_language:\n            LOGGER.warning('Language %s detected, expected %s', detected_language, target_language)\n            docmeta.language_detected = detected_language\n            return True, docmeta\n    return False, docmeta", "idx": 949}
{"namespace": "trafilatura.filters.textfilter", "completion": "    # Check if the element is a text element\n    if element.text is None:\n        return True\n\n    # Check if the text contains any of the blacklisted words\n    if any(word in element.text for word in COMMENTS_BLACKLIST):\n        return True\n\n    # Check if the text contains any of the blacklisted words\n    if any(word in element.text for word in COMMENTS_BLACKLIST):\n        return True\n\n    # Check if the text contains any of the blacklisted words\n    if any(word in element.text for word in COMMENTS_BLACKLIST):\n        return True\n\n    # Check if the text contains any of the blacklisted words\n    if any(word in element.text for word in COMMENTS_BLACKLIST):\n        return True\n\n    # Check if the text contains any of the blacklisted words\n    if any(word in element.text for word in COMMENTS_BLACKLIST):\n        return True\n\n    # Check if the text contains any of the blacklisted words\n    if any(word in element.text for word in COMMENTS_BLACKLIST):\n        return True\n\n    # Check if the text contains any of the blacklisted words\n    if any(word in element.text for word in COMMENTS_BLACKLIST):\n        return True\n\n    # Check if the text contains any of the blacklisted words\n    if any(word in element.text for word in COMMENTS_BLACKLIST):\n        return True\n\n    # Check if the text contains any of the blacklisted words\n    if any(word in element.text for word in COMMENTS_BLACKLIST):\n        return True\n\n    # Check if the text contains any of the blacklisted words\n    if any(word in element.text for word in COMMENTS_BLACKLIST):\n        return True\n\n    # Check if the text contains any of the blacklisted words\n    if any(word in element.text for word in COMMENTS_BLACKLIST):\n        return True\n\n    # Check if the text contains any of the blacklisted words\n    if any(word in element.text for word in COMMENTS_BLACKLIST", "idx": 950}
{"namespace": "trafilatura.metadata.extract_meta_json", "completion": "    for element in tree.iter():\n        if element.tag == 'script' and element.get('type') in ('application/ld+json', 'application/settings+json'):\n            if element.text:\n                try:\n                    json_data = json.loads(element.text)\n                    if isinstance(json_data, dict):\n                        metadata.update(json_data)\n                except json.JSONDecodeError:\n                    metadata.update(json.loads(element.text))\n    return metadata", "idx": 951}
{"namespace": "trafilatura.external.try_justext", "completion": "    # Initialize the body element\n    body = Element('body')\n\n    # Determine the language of the webpage\n    language = get_language(url)\n\n    # Use the default stoplist if the language is not found in JUSTEXT_LANGUAGES\n    if language not in JUSTEXT_LANGUAGES:\n        language = 'en'\n\n    # Get the stoplist for the language\n    stoplist = JUSTEXT_LANGUAGES[language]\n\n    # Extract paragraphs from the HTML tree\n    try:\n        paragraphs = custom_justext(tree, stoplist)\n    except Exception as e:\n        LOGGER.warning('justext failed: %s', e)\n        return None\n\n    # Populate the body element with non-boilerplate paragraphs\n    for paragraph in paragraphs:\n        if paragraph.is_boilerplate:\n            continue\n        p = Element('p')\n        p.text = paragraph.text\n        body.append(p)\n\n    return body", "idx": 952}
{"namespace": "mongoengine.base.datastructures.BaseDict.get", "completion": "        try:\n            return self[key]\n        except KeyError:\n            return default\n", "idx": 953}
{"namespace": "sqlite_utils.utils.suggest_column_types", "completion": "    column_types = {}\n    for record in records:\n        for key, value in record.items():\n            if key not in column_types:\n                column_types[key] = []\n            column_types[key].append(value)\n\n    for key, values in column_types.items():\n        column_types[key] = suggest_column_type(values)\n\n    return column_types", "idx": 954}
{"namespace": "sqlite_utils.plugins.get_plugins", "completion": "    plugins = pm.get_plugins()\n    plugin_list = []\n    for plugin in plugins:\n        plugin_dict = {\"name\": plugin.name, \"hooks\": plugin.hook.get_hookcaller_names()}\n        if plugin.dist_name:\n            plugin_dict[\"version\"] = plugin.dist.version\n            plugin_dict[\"project_name\"] = plugin.dist.project_name\n        plugin_list.append(plugin_dict)\n    return plugin_list", "idx": 955}
{"namespace": "alembic.config.Config.print_stdout", "completion": "        if not self.cmd_opts or not self.cmd_opts.quiet:\n            print(text % arg, file=self.stdout)\n", "idx": 956}
{"namespace": "alembic.config.Config.set_section_option", "completion": "        self.file_config.set(section, name, value)\n", "idx": 957}
{"namespace": "alembic.command.merge", "completion": "    script_directory = ScriptDirectory.from_config(config)\n\n    command_args = dict(\n        message=message,\n        autogenerate=False,\n        sql=False,\n        head=\"head\",\n        splice=False,\n        branch_label=branch_label,\n        version_path=None,\n        rev_id=rev_id,\n        depends_on=None,\n    )\n    revision_context = autogen.RevisionContext(\n        config,\n        script_directory,\n        command_args,\n    )\n\n    def retrieve_migrations(rev, context):\n        revision_context.run_no_autogenerate(rev, context)\n        return []\n\n    with EnvironmentContext(\n        config,\n        script_directory,\n        fn=retrieve_migrations,\n        as_sql=False,\n        template_args=revision_context.template_args,\n        revision_context=revision_context,\n    ):\n        script_directory.run_env()\n\n    # the revision_context now has MigrationScript structure(s) present.\n\n    migration_script = revision_context.generated_revisions[-1]\n    diffs = migration_script.upgrade_ops.as_diffs()\n    if diffs:\n        raise util.AutogenerateDiffsDetected(\n            f\"New upgrade operations detected: {diffs}\"\n        )\n    else:\n        config.print_stdout(\"No new upgrade operations detected.\")\n\n    return migration_script", "idx": 958}
{"namespace": "alembic.command.upgrade", "completion": "    script = ScriptDirectory.from_config(config)\n    script.run_env(revision, sql=sql, tag=tag)", "idx": 959}
{"namespace": "alembic.command.downgrade", "completion": "    script = ScriptDirectory.from_config(config)\n\n    starting_rev = None\n    if \":\" in revision:\n        if not sql:\n            raise util.CommandError(\"Range revision not allowed\")\n        starting_rev, revision = revision.split(\":\", 2)\n\n    def downgrade(rev, context):\n        return script._downgrade_revs(revision, rev)\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=downgrade,\n        as_sql=sql,\n        starting_rev=starting_rev,\n        destination_rev=revision,\n        tag=tag,\n    ):\n        script.run_env()", "idx": 960}
{"namespace": "alembic.command.history", "completion": "    script = ScriptDirectory.from_config(config)\n\n    def history(rev, context):\n        for sc in script.get_revisions(rev_range):\n            if indicate_current and sc.is_head():\n                config.print_stdout(\"  (head)\")\n            if verbose:\n                config.print_stdout(sc.log_entry)\n            else:\n                config.print_stdout(sc.revision)\n        return []\n\n    with EnvironmentContext(config, script, fn=history):\n        script.run_env()", "idx": 961}
{"namespace": "alembic.command.stamp", "completion": "    script = ScriptDirectory.from_config(config)\n\n    def stamp_revision(rev, context):\n        return script._stamp_revision(revision, rev, purge)\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=stamp_revision,\n        as_sql=sql,\n        tag=tag,\n    ):\n        script.run_env()", "idx": 962}
{"namespace": "alembic.command.ensure_version", "completion": "    script = ScriptDirectory.from_config(config)\n\n    def ensure_version(rev, context):\n        return script._ensure_version(rev, context)\n\n    with EnvironmentContext(config, script, fn=ensure_version, as_sql=sql):\n        script.run_env()", "idx": 963}
{"namespace": "alembic.autogenerate.compare._compare_server_default", "completion": "    metadata_default = metadata_col.server_default\n    conn_default = conn_col.server_default\n\n    if metadata_default is None:\n        if conn_default is not None:\n            alter_column_op.modify_server_default = None\n            log.info(\n                \"Detected removal of server default on '%s.%s'\",\n                tname,\n                cname,\n            )\n            return True\n        else:\n            return None\n\n    if conn_default is None:\n        alter_column_op.modify_server_default = metadata_default\n        log.info(\n            \"Detected addition of server default on '%s.%s'\",\n            tname,\n            cname,\n        )\n        return True\n\n    if isinstance(metadata_default, sa_schema.DefaultClause):\n        metadata_default = metadata_default.arg\n\n    if isinstance(conn_default, sa_schema.DefaultClause):\n        conn_default = conn_default.arg\n\n    if isinstance(metadata_default, sa_schema.Computed):\n        return _compare_computed_default(\n            autogen_context,\n            alter_column_op,\n            schema,\n            tname,\n            cname,\n            conn_col,\n            metadata_col,\n        )\n\n    if isinstance(metadata_default, sa_schema.Identity):\n        return _compare_identity_default(\n            autogen_context,\n            alter_column_op,\n            schema,\n            tname,\n            cname,\n            conn_col,\n            metadata_col,\n        )\n\n    if metadata_default != conn_default:\n        rendered_metadata_default = _render_server_default_for_compare(\n            metadata_default, autogen_context\n        )\n        rendered_conn_default = _render_server_default_for_compare(\n            conn_default, autogen_context\n        )\n        if rendered_metadata_default != rendered_conn_default:\n            alter_column_op.modify_server_default = metadata_default\n            log.info(", "idx": 964}
{"namespace": "alembic.autogenerate.render._render_server_default", "completion": "    rendered = _user_defined_render(\"server_default\", default, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    if isinstance(default, Computed):\n        return \"%s(%s)\" % (\n            _sqlalchemy_autogenerate_prefix(autogen_context),\n            _render_potential_expr(default.text, autogen_context),\n        )\n    elif isinstance(default, Identity):\n        return \"%s(%s)\" % (\n            _sqlalchemy_autogenerate_prefix(autogen_context),\n            _render_potential_expr(default.text, autogen_context),\n        )\n    elif isinstance(default, DefaultClause):\n        if isinstance(default.arg, str):\n            return \"%s(%s)\" % (\n                _sqlalchemy_autogenerate_prefix(autogen_context),\n                _render_potential_expr(default.arg, autogen_context),\n            )\n        else:\n            return \"%s(%s)\" % (\n                _sqlalchemy_autogenerate_prefix(autogen_context),\n                _render_potential_expr(default.arg, autogen_context),\n            )\n    elif isinstance(default, str) and repr_:\n        return default.strip(\"'\")\n    else:\n        return None", "idx": 965}
{"namespace": "alembic.autogenerate.render._render_constraint", "completion": "    renderer = _constraint_renderers.dispatch(constraint)\n    if renderer:\n        return renderer(constraint, autogen_context, namespace_metadata)\n    else:\n        return \"Python object %r is unknown\" % constraint", "idx": 966}
{"namespace": "alembic.autogenerate.render._render_unique_constraint", "completion": "    rendered = _user_defined_render(\"unique_constraint\", constraint, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    return _uq_constraint(constraint, autogen_context, alter=False)", "idx": 967}
{"namespace": "alembic.autogenerate.render._render_check_constraint", "completion": "    rendered = _user_defined_render(\"check\", constraint, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    if constraint.parent is not None:\n        return None\n\n    opts = []\n    if constraint.name:\n        opts.append(\n            (\"name\", repr(_render_gen_name(autogen_context, constraint.name)))\n        )\n\n    return \"%(prefix)sCheckConstraint(%(sqltext)s, %(args)s)\" % {\n        \"prefix\": _sqlalchemy_autogenerate_prefix(autogen_context),\n        \"sqltext\": _render_potential_expr(\n            constraint.sqltext, autogen_context, wrap_in_text=False\n        ),\n        \"args\": \", \".join(\n            [\"%s=%s\" % (kwname, val) for kwname, val in opts]\n        ),\n    }", "idx": 968}
{"namespace": "alembic.autogenerate.api.compare_metadata", "completion": "    # Create a list to store the diff directives\n    diffs: List[Any] = []\n\n    # Get the current database schema using the MigrationContext\n    current_schema = context.get_current_schema()\n\n    # Iterate over the tables in the MetaData instance\n    for table in metadata.tables.values():\n        # Check if the table exists in the current database schema\n        if table.name not in current_schema:\n            # If the table does not exist, add a \"create_table\" diff directive\n            diffs.append(ops.create_table(table))\n        else:\n            # If the table exists, compare its columns\n            current_table = current_schema[table.name]\n            diffs.extend(compare.compare_columns(table, current_table))\n\n    # Return the list of diff directives\n    return diffs\n\n", "idx": 969}
{"namespace": "alembic.autogenerate.api.AutogenContext._within_batch", "completion": "        self._has_batch = True\n        try:\n            yield\n        finally:\n            self._has_batch = False", "idx": 970}
{"namespace": "alembic.util.sqla_compat._connectable_has_table", "completion": "    if sqla_14:\n        return connectable.has_table(tablename, schema=schemaname)\n    else:\n        return connectable.has_table(tablename, schemaname)\n\n", "idx": 971}
{"namespace": "alembic.util.sqla_compat._get_constraint_final_name", "completion": "    if constraint.name is not None:\n        if sqla_14:\n            return constraint.name.compile(dialect=dialect)\n        else:\n            return constraint.name.compile(dialect=dialect, compile_kwargs={})\n    else:\n        return None\n\n", "idx": 972}
{"namespace": "alembic.testing.env.env_file_fixture", "completion": "    dir_ = os.path.join(_get_staging_directory(), \"scripts\")\n    path = os.path.join(dir_, \"env.py\")\n    with open(path, \"w\") as f:\n        f.write(txt)", "idx": 973}
{"namespace": "alembic.testing.env._no_sql_testing_config", "completion": "    dir_ = os.path.join(_get_staging_directory(), \"scripts\")\n    url = \"sqlite:///%s/foo.db\" % dir_\n\n    return _write_config_file(\n        \"\"\"", "idx": 974}
{"namespace": "alembic.testing.env._write_config_file", "completion": "    cfg = _testing_config()\n    with open(cfg.config_file_name, \"w\") as f:\n        f.write(text)\n    return cfg", "idx": 975}
{"namespace": "alembic.testing.env.three_rev_fixture", "completion": "    sc = ScriptDirectory.from_config(cfg)\n    a = sc.generate_revision_id()\n    b = sc.generate_revision_id()\n    c = sc.generate_revision_id()\n\n    sc.write_revision_file(a, \"a\", \"upgrade\", \"sql\", \"\")\n    sc.write_revision_file(b, \"b\", \"upgrade\", \"sql\", \"\")\n    sc.write_revision_file(c, \"c\", \"upgrade\", \"sql\", \"\")\n\n    return a, b, c", "idx": 976}
{"namespace": "alembic.testing.env.multi_heads_fixture", "completion": "    script = ScriptDirectory.from_config(cfg)\n    d = util.rev_id()\n    e = util.rev_id()\n    f = util.rev_id()\n\n    script.generate_revision(d, \"revision d\", refresh=True, head=a)\n    write_script(\n        script,\n        d,\n        \"\"\"\\", "idx": 977}
{"namespace": "alembic.testing.fixtures.capture_db", "completion": "    engine = mock.create_engine(dialect)\n    buffer = io.StringIO()\n    engine.execute = lambda sql, *multiparams, **params: buffer.write(sql)\n    return engine, buffer", "idx": 978}
{"namespace": "alembic.testing.fixtures.capture_engine_context_buffer", "completion": "    dialect = _get_dialect(kw.pop(\"dialect_name\", \"sqlite\"))\n    buf = kw.pop(\"output_buffer\", io.StringIO())\n    engine = dialect.create_engine(**kw)\n    engine.connect().close()\n    engine.listener.on_connect = lambda *args, **kw: buf.write(\n        \"-- connected to %s\\n\" % engine.url\n    )\n    engine.listener.on_execute = lambda *args, **kw: buf.write(\n        \"-- %s\\n\" % re.sub(r\"\\s+\", \" \", kw[\"sql\"].strip())\n    )\n    kw.update({\"dialect_name\": \"sqlite\", \"output_buffer\": buf})\n    conf = EnvironmentContext.configure\n\n    def configure(*arg, **opt):\n        opt.update(**kw)\n        return conf(*arg, **opt)\n\n    with mock.patch.object(EnvironmentContext, \"configure\", configure):\n        yield buf", "idx": 979}
{"namespace": "alembic.operations.schemaobj.SchemaObjects.unique_constraint", "completion": "        m = self.metadata()\n        t = sa_schema.Table(source, m, schema=schema)\n        u = sa_schema.UniqueConstraint(*local_cols, name=name, **kw)\n        t.append_constraint(u)\n        return u\n", "idx": 980}
{"namespace": "alembic.operations.schemaobj.SchemaObjects.index", "completion": "        m = self.metadata()\n        t = sa_schema.Table(\n            tablename,\n            m,\n            *[sa_schema.Column(n, NULLTYPE) for n in columns],\n            schema=schema,\n        )\n        i = sa_schema.Index(name, *[t.c[n] for n in columns], **kw)\n        t.append_constraint(i)\n        return i", "idx": 981}
{"namespace": "alembic.operations.ops.DropConstraintOp.from_constraint", "completion": "        if isinstance(constraint, ForeignKeyConstraint):\n            return cls(\n                constraint.name,\n                constraint.elements[0].table.name,\n                \"foreignkey\",\n                schema=constraint.elements[0].table.schema,\n            )\n        elif isinstance(constraint, CheckConstraint):\n            return cls(\n                constraint.name,\n                constraint.table.name,\n                \"check\",\n                schema=constraint.table.schema,\n            )\n        elif isinstance(constraint, UniqueConstraint):\n            return cls(\n                constraint.name,\n                constraint.table.name,\n                \"unique\",\n                schema=constraint.table.schema,\n            )\n        elif isinstance(constraint, PrimaryKeyConstraint):\n            return cls(\n                constraint.name,\n                constraint.table.name,\n                \"primary_key\",\n                schema=constraint.table.schema,\n            )\n        elif isinstance(constraint, Index):\n            return cls(\n                constraint.name,\n                constraint.table.name,\n                \"index\",\n                schema=constraint.table.schema,\n            )\n        else:\n            raise NotImplementedError(\n                f\"DropConstraintOp.from_constraint() not implemented for {constraint}\"\n            )\n", "idx": 982}
{"namespace": "alembic.operations.ops.DropConstraintOp.to_constraint", "completion": "        if self._reverse:\n            constraint = self._reverse.to_constraint()\n            constraint.name = self.constraint_name\n            constraint.table.name = self.table_name\n            constraint.table.schema = self.schema\n            return constraint\n        else:\n            raise ValueError(\"Reverse operation not present\")", "idx": 983}
{"namespace": "alembic.operations.ops.CreatePrimaryKeyOp.to_constraint", "completion": "        schema = migration_context.schema if migration_context else None\n        return PrimaryKeyConstraint(\n            *self.columns,\n            name=self.constraint_name,\n            schema=self.schema,\n            **self.kw,\n        )", "idx": 984}
{"namespace": "alembic.operations.ops.CreateIndexOp.from_index", "completion": "        return cls(\n            index.name,\n            index.table.name,\n            index.columns,\n            schema=index.table.schema,\n            unique=index.unique,\n            if_not_exists=index.if_not_exists,\n            **index.dialect_kwargs,\n        )\n", "idx": 985}
{"namespace": "alembic.operations.ops.DropIndexOp.from_index", "completion": "        assert index.table is not None\n        return cls(\n            index.name,  # type: ignore[arg-type]\n            index.table.name,\n            schema=index.table.schema,\n            if_exists=index.if_exists,\n            _reverse=CreateIndexOp.from_index(index),\n            **index.kwargs,\n        )\n", "idx": 986}
{"namespace": "alembic.operations.ops.DropIndexOp.to_index", "completion": "        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        idx = schema_obj.index(\n            self.index_name,\n            self.table_name,\n            self._reverse.columns,\n            schema=self.schema,\n            unique=self._reverse.unique,\n            **self.kw,\n        )\n        return idx", "idx": 987}
{"namespace": "alembic.operations.ops.CreateTableOp.from_table", "completion": "        columns = []\n        for column in table.columns:\n            if column.foreign_keys:\n                columns.append(\n                    ForeignKeyConstraint(\n                        column.foreign_keys[0].constraint.columns,\n                        column.foreign_keys[0].referred_columns,\n                        name=column.foreign_keys[0].constraint.name,\n                        onupdate=column.foreign_keys[0].onupdate,\n                        ondelete=column.foreign_keys[0].ondelete,\n                        initially=column.foreign_keys[0].initially,\n                        deferrable=column.foreign_keys[0].deferrable,\n                        use_alter=column.foreign_keys[0].use_alter,\n                        match=column.foreign_keys[0].match,\n                        **column.foreign_keys[0].dialect_kwargs,\n                    )\n                )\n            else:\n                columns.append(column)\n\n        return cls(\n            table.name,\n            columns,\n            schema=table.schema,\n            _namespace_metadata=_namespace_metadata,\n            _constraints_included=True,\n            comment=table.comment,\n            info=table.info,\n            prefixes=table.prefixes,\n            **table.kwargs,\n        )\n", "idx": 988}
{"namespace": "alembic.operations.ops.DropTableOp.from_table", "completion": "        if _namespace_metadata is None:\n            _namespace_metadata = table.metadata\n\n        return cls(\n            table.name,\n            schema=table.schema,\n            table_kw=dict(\n                table.kwargs,\n                comment=table.comment,\n                info=dict(table.info),\n                prefixes=list(table._prefixes),\n            ),\n            _reverse=CreateTableOp.from_table(table, _namespace_metadata=_namespace_metadata),\n        )\n", "idx": 989}
{"namespace": "alembic.operations.ops.DropTableOp.to_table", "completion": "        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.table(\n            self.table_name,\n            *self._reverse.columns,\n            schema=self.schema,\n            comment=self.comment,\n            info=self.info,\n            prefixes=list(self.prefixes) if self.prefixes else [],\n            **self.table_kw,\n        )", "idx": 990}
{"namespace": "alembic.operations.ops.AlterColumnOp.to_diff_tuple", "completion": "        if self.modify_type is not None:\n            return (\n                \"modify_column\",\n                self.to_column(),\n                self.existing_type,\n                self.existing_nullable,\n                self.existing_server_default,\n                self.existing_comment,\n            )\n        elif self.modify_nullable is not None:\n            return (\n                \"modify_column\",\n                self.to_column(),\n                self.existing_type,\n                self.existing_nullable,\n                self.existing_server_default,\n                self.existing_comment,\n            )\n        elif self.modify_server_default is not None:\n            return (\n                \"modify_column\",\n                self.to_column(),\n                self.existing_type,\n                self.existing_nullable,\n                self.existing_server_default,\n                self.existing_comment,\n            )\n        elif self.modify_comment is not None:\n            return (\n                \"modify_column\",\n                self.to_column(),\n                self.existing_type,\n                self.existing_nullable,\n                self.existing_server_default,\n                self.existing_comment,\n            )\n        elif self.modify_name is not None:\n            return (\n                \"modify_column\",\n                self.to_column(),\n                self.existing_type,\n                self.existing_nullable,\n                self.existing_server_default,\n                self.existing_comment,\n            )\n        else:\n            raise ValueError(\"No modifications found\")\n", "idx": 991}
{"namespace": "alembic.operations.ops.AddColumnOp.reverse", "completion": "        return DropColumnOp(\n            self.table_name,\n            self.column.name,\n            schema=self.schema,\n            existing_type=self.column.type,\n            existing_server_default=self.column.server_default,\n            existing_nullable=self.column.nullable,\n            existing_comment=self.column.comment,\n            **self.kw,\n        )", "idx": 992}
{"namespace": "alembic.operations.ops.DropColumnOp.reverse", "completion": "        if self._reverse is None:\n            raise ValueError(\n                \"Cannot reverse operation. Reverse operation is not available.\"\n            )\n        return self._reverse", "idx": 993}
{"namespace": "alembic.operations.ops.DropColumnOp.from_column_and_tablename", "completion": "        return cls(tname, col.name, schema=schema)\n", "idx": 994}
{"namespace": "alembic.operations.ops.DropColumnOp.to_column", "completion": "        if self._reverse:\n            return self._reverse.to_column()\n        else:\n            schema_obj = schemaobj.SchemaObjects(migration_context)\n            return schema_obj.column(self.column_name, nulltype=True)", "idx": 995}
{"namespace": "alembic.script.revision.RevisionMap.heads", "completion": "        self.initialize()\n        return tuple(self._heads)\n", "idx": 996}
{"namespace": "alembic.script.revision.RevisionMap.add_revision", "completion": "        if revision.revision in self._revision_map:\n            if _replace:\n                self._revision_map[revision.revision] = revision\n            else:\n                raise RevisionError(\n                    \"Revision %s is present more than once\"\n                    % revision.revision\n                )\n        else:\n            self._revision_map[revision.revision] = revision\n\n        if revision.branch_labels:\n            self._map_branch_labels(\n                [revision], cast(_RevisionMapType, self._revision_map)\n            )\n\n        if revision.dependencies:\n            self._add_depends_on(\n                [revision], cast(_RevisionMapType, self._revision_map)\n            )\n\n        if revision.is_base:\n            self.bases += (revision,)\n        if revision._is_real_base:\n            self._real_bases += (revision,)\n\n        if revision.down_revision:\n            self.heads.discard(revision)\n        if revision._is_real_head:\n            self._real_heads.discard(revision)\n\n        self._normalize_depends_on(\n            [revision], cast(_RevisionMapType, self._revision_map)\n        )\n\n        if revision.down_revision:\n            down_revision = self._revision_map[revision.down_revision]\n            down_revision.add_nextrev(revision)\n            if revision.down_revision in down_revision._versioned_down_revisions:\n                self.heads.discard(down_revision)\n            self._real_heads.discard(down_revision)\n\n        self._add_branches(\n            [revision], cast(_RevisionMapType, self._revision_map)\n        )\n\n        self.heads = tuple(rev.revision for rev in self.heads)\n        self._real_heads = tuple(rev.revision for rev in self._real_heads)\n        self.", "idx": 997}
{"namespace": "alembic.script.revision.RevisionMap.get_revisions", "completion": "        if id_ is None:\n            return ()\n        elif isinstance(id_, str):\n            if id_ == \"head\":\n                return (self.heads,)\n            elif id_ == \"base\":\n                return (self.bases,)\n            elif id_ == \"heads\":\n                return self.heads\n            elif id_ == \"bases\":\n                return self.bases\n            elif id_ == \"current\":\n                return self.get_current_head()\n            elif id_ == \"current_head\":\n                return self.get_current_head()\n            elif id_ == \"current_base\":\n                return self.get_current_base()\n            elif id_ == \"current_heads\":\n                return self.get_current_heads()\n            elif id_ == \"current_bases\":\n                return self.get_current_bases()\n            elif id_ == \"heads_and_bases\":\n                return self.heads + self.bases\n            elif id_ == \"heads_and_current_heads\":\n                return self.heads + self.get_current_heads()\n            elif id_ == \"bases_and_current_bases\":\n                return self.bases + self.get_current_bases()\n            elif id_ == \"heads_and_bases_and_current_heads_and_current_bases\":\n                return (\n                    self.heads\n                    + self.bases\n                    + self.get_current_heads()\n                    + self.get_current_bases()\n                )\n            elif id_ == \"heads_and_current_heads_and_bases_and_current_bases\":\n                return (\n                    self.heads\n                    + self.get_current_heads()\n                    + self.bases\n                    + self.get_current_bases()\n                )\n            elif id_ == \"current_heads_and_heads_and_current_bases_and_bases\":\n                return (\n                    self.get_current_heads()\n                    + self.heads\n                    + self", "idx": 998}
{"namespace": "alembic.script.revision.RevisionMap.get_revision", "completion": "        if id_ is None:\n            return None\n        resolved_id, branch_label = self._resolve_revision_number(id_)\n        if len(resolved_id) == 1:\n            try:\n                rint = int(resolved_id[0])\n                if rint < 0:\n                    # branch@-n -> walk down from heads\n                    select_heads = self.get_revisions(\"heads\")\n                    if branch_label is not None:\n                        select_heads = tuple(\n                            head\n                            for head in select_heads\n                            if branch_label\n                            in is_revision(head).branch_labels\n                        )\n                    return tuple(\n                        self._walk(head, steps=rint)\n                        for head in select_heads\n                    )\n            except ValueError:\n                # couldn't resolve as integer\n                pass\n        return tuple(\n            self._revision_for_ident(rev_id, branch_label)\n            for rev_id in resolved_id\n        )\n", "idx": 999}
{"namespace": "alembic.script.revision.RevisionMap.filter_for_lineage", "completion": "        if not check_against:\n            return tuple(targets)\n\n        if include_dependencies:\n            targets = self._get_descendant_nodes(\n                targets, include_dependencies=True\n            )\n        else:\n            targets = self._get_descendant_nodes(targets)\n\n        return tuple(\n            target\n            for target in targets\n            if self._shares_lineage(target.revision, check_against)\n        )", "idx": 1000}
{"namespace": "alembic.script.revision.RevisionMap.iterate_revisions", "completion": "        if upper is None:\n            upper = ()\n        if lower is None:\n            lower = ()\n\n        if isinstance(upper, str):\n            upper = (upper,)\n        if isinstance(lower, str):\n            lower = (lower,)\n\n        if not isinstance(upper, tuple):\n            raise RevisionError(\n                \"upper revision identifier %r is not a string; ensure database \"\n                \"driver settings are correct\" % (upper,)\n            )\n        if not isinstance(lower, tuple):\n            raise RevisionError(\n                \"lower revision identifier %r is not a string; ensure database \"\n                \"driver settings are correct\" % (lower,)\n            )\n\n        if assert_relative_length:\n            if len(upper) != len(lower):\n                raise RevisionError(\n                    \"upper and lower revision identifiers are not of the same length\"\n                )\n\n        if select_for_downgrade:\n            upper, lower = lower, upper\n\n        if not isinstance(upper, tuple):\n            upper = (upper,)\n        if not isinstance(lower, tuple):\n            lower = (lower,)\n\n        if not upper:\n            upper = ()\n        if not lower:\n            lower = ()\n\n        if not upper and not lower:\n            return\n\n        if not upper:\n            upper = self.bases\n        if not lower:\n            lower = self.heads\n\n        if not upper:\n            raise RangeNotAncestorError(upper, lower)\n        if not lower:\n            raise RangeNotAncestorError(lower, upper)\n\n        if not self._shares_lineage(upper, lower):\n            raise RangeNotAncestorError(upper, lower)\n\n        if not self._shares_lineage(lower, upper):\n            raise RangeNotAncestorError(lower, upper)\n\n        if not self._shares_lineage(upper, lower, include_dependencies=True):\n            raise RangeNotAncestorError(upper, lower)\n\n        if not self._shares_lineage(lower, upper, include_dependencies=True):", "idx": 1001}
{"namespace": "alembic.script.revision.RevisionMap._topological_sort", "completion": "        # Create a dictionary to store the dependencies of each revision.\n        dependencies: Dict[str, Set[str]] = {}\n        for revision in revisions:\n            dependencies[revision.revision] = set(\n                revision._normalized_down_revisions\n            )\n\n        # Create a set to store the revisions that have no dependencies.\n        no_dependencies: Set[str] = set()\n        for revision in revisions:\n            if not dependencies[revision.revision]:\n                no_dependencies.add(revision.revision)\n\n        # Create a list to store the sorted revisions.\n        sorted_revisions: List[str] = []\n\n        # Perform the topological sort.\n        while no_dependencies:\n            # Get the next revision with no dependencies.\n            revision = no_dependencies.pop()\n\n            # Add the revision to the sorted list.\n            sorted_revisions.append(revision)\n\n            # Remove the revision from the dependencies of other revisions.\n            for other_revision in revisions:\n                if revision in dependencies[other_revision.revision]:\n                    dependencies[other_revision.revision].remove(revision)\n                    if not dependencies[other_revision.revision]:\n                        no_dependencies.add(other_revision.revision)\n\n        # Check if there are any revisions with remaining dependencies.\n        if any(dependencies.values()):\n            raise CycleDetected(\n                [\n                    rev\n                    for rev in revisions\n                    if dependencies[rev.revision]\n                ]\n            )\n\n        # Return the sorted list of revision ids.\n        return sorted_revisions", "idx": 1002}
{"namespace": "alembic.script.revision.Revision._all_down_revisions", "completion": "        return tuple(\n            set(self.down_revision).union(self._resolved_dependencies)\n        )\n", "idx": 1003}
{"namespace": "alembic.script.revision.Revision._normalized_down_revisions", "completion": "        return util.dedupe_tuple(\n            util.to_tuple(self.down_revision, default=())\n            + self._normalized_resolved_dependencies\n        )\n", "idx": 1004}
{"namespace": "alembic.script.write_hooks._invoke", "completion": "    formatter = _registry.get(name)\n    if not formatter:\n        raise util.CommandError(f\"No formatter with name '{name}' registered\")\n    return formatter(revision, **options)", "idx": 1005}
{"namespace": "bplustree.memory.FileMemory.get_node", "completion": "        with self._lock.read_lock:\n            node = self._cache.get(page)\n            if node is not None:\n                return node\n\n            data = read_from_file(self._fd, page * self._tree_conf.page_size,\n                                  (page + 1) * self._tree_conf.page_size)\n            node = Node.from_bytes(data, self._tree_conf)\n            self._cache[page] = node\n            return node\n", "idx": 1006}
{"namespace": "bplustree.memory.FileMemory.next_available_page", "completion": "        self.last_page += 1\n        return self.last_page\n", "idx": 1007}
{"namespace": "bplustree.memory.FileMemory.get_metadata", "completion": "        # Read the first page of the file\n        data = self._read_page(0)\n\n        # Extract the root node page, page size, order, key size, and value size from the data\n        root_node_page = int.from_bytes(data[:PAGE_REFERENCE_BYTES], ENDIAN)\n        page_size = int.from_bytes(data[PAGE_REFERENCE_BYTES:PAGE_REFERENCE_BYTES + OTHERS_BYTES], ENDIAN)\n        order = int.from_bytes(data[PAGE_REFERENCE_BYTES + OTHERS_BYTES:PAGE_REFERENCE_BYTES + OTHERS_BYTES * 2], ENDIAN)\n        key_size = int.from_bytes(data[PAGE_REFERENCE_BYTES + OTHERS_BYTES * 2:PAGE_REFERENCE_BYTES + OTHERS_BYTES * 3], ENDIAN)\n        value_size = int.from_bytes(data[PAGE_REFERENCE_BYTES + OTHERS_BYTES * 3:PAGE_REFERENCE_BYTES + OTHERS_BYTES * 4], ENDIAN)\n\n        # Create a TreeConf object with the extracted values\n        tree_conf = TreeConf(page_size=page_size, order=order, key_size=key_size, value_size=value_size)\n\n        # Return the root node page and the TreeConf object\n        return root_node_page, tree_conf", "idx": 1008}
{"namespace": "bplustree.memory.FileMemory.set_metadata", "completion": "        self._tree_conf = tree_conf\n        data = root_node_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN)\n        data += tree_conf.page_size.to_bytes(OTHERS_BYTES, ENDIAN)\n        data += tree_conf.order.to_bytes(OTHERS_BYTES, ENDIAN)\n        data += tree_conf.key_size.to_bytes(OTHERS_BYTES, ENDIAN)\n        data += tree_conf.value_size.to_bytes(OTHERS_BYTES, ENDIAN)\n        self._wal.set_page(0, data)\n", "idx": 1009}
{"namespace": "bplustree.memory.WAL.checkpoint", "completion": "        if self._not_committed_pages:\n            logger.warning('There are uncommitted pages, '\n                           'checkpointing them')\n        fsync_file_and_dir(self._fd.fileno(), self._dir_fd)\n        self._fd.seek(0)\n        for page, data in self._committed_pages.items():\n            yield page, data\n        self._fd.close()\n        os.remove(self.filename)\n        if self._dir_fd is not None:\n            os.fsync(self._dir_fd)\n", "idx": 1010}
{"namespace": "bplustree.memory.WAL.commit", "completion": "        if self._not_committed_pages:\n            self._add_frame(FrameType.COMMIT)\n", "idx": 1011}
{"namespace": "bplustree.memory.WAL.rollback", "completion": "        if self._not_committed_pages:\n            self._add_frame(FrameType.ROLLBACK)", "idx": 1012}
{"namespace": "bplustree.entry.Record.dump", "completion": "        key_data = self._tree_conf.serializer.serialize(self.key)\n        used_key_length = len(key_data)\n        used_value_length = len(self.value) if self.value else 0\n        overflow_page_data = (\n            self.overflow_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN)\n            if self.overflow_page else b'\\x00' * PAGE_REFERENCE_BYTES\n        )\n\n        return (\n            used_key_length.to_bytes(USED_KEY_LENGTH_BYTES, ENDIAN) +\n            key_data +\n            used_value_length.to_bytes(USED_VALUE_LENGTH_BYTES, ENDIAN) +\n            self.value +\n            overflow_page_data\n        )", "idx": 1013}
{"namespace": "bplustree.entry.Reference.__repr__", "completion": "        return '<Reference: key={} before={} after={}>'.format(\n            self.key, self.before, self.after\n        )", "idx": 1014}
{"namespace": "bplustree.node.Node.dump", "completion": "        data = bytearray()\n        for entry in self.entries:\n            data.extend(entry.dump())\n        used_page_length = len(data)\n        data.extend(\n            used_page_length.to_bytes(USED_PAGE_LENGTH_BYTES, ENDIAN)\n        )\n        data.extend(\n            self.next_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN)\n        )\n        data.extend(\n            self._node_type_int.to_bytes(NODE_TYPE_BYTES, ENDIAN)\n        )\n        data.extend(\n            b'\\x00' * (self._tree_conf.page_size - len(data))\n        )\n        return data\n", "idx": 1015}
{"namespace": "bplustree.node.Node._find_entry_index", "completion": "        entry = self._entry_class(self._tree_conf, key=key)\n        return bisect.bisect_left(self.entries, entry)\n", "idx": 1016}
{"namespace": "bplustree.node.Node.from_page_data", "completion": "        node_type = data[0]\n        if node_type == 0:\n            return LeafNode(tree_conf, data, page)\n        elif node_type == 1:\n            return IndexNode(tree_conf, data, page)\n        else:\n            raise ValueError('Unknown node type {}'.format(node_type))\n\n", "idx": 1017}
{"namespace": "bplustree.tree.BPlusTree._root_node", "completion": "        return self._mem.get_node(self._root_node_page)\n", "idx": 1018}
{"namespace": "bplustree.tree.BPlusTree._left_record_node", "completion": "        node = self._root_node\n        while True:\n            if isinstance(node, LonelyRootNode):\n                return node\n            elif isinstance(node, RootNode):\n                node = self._mem.get_node(node.left_child)\n            elif isinstance(node, LeafNode):\n                return node\n            else:\n                raise TypeError('Unexpected node type: {}'.format(type(node)))\n", "idx": 1019}
{"namespace": "mopidy.ext.Extension.get_cache_dir", "completion": "        cache_dir = path.get_cache_dir(config) / cls.ext_name\n        cache_dir.mkdir(parents=True, exist_ok=True)\n        return cache_dir\n", "idx": 1020}
{"namespace": "mopidy.ext.Extension.get_config_dir", "completion": "        if cls.ext_name is None:\n            raise AssertionError\n        config_dir_path = (\n            path.expand_path(config[\"core\"][\"config_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(config_dir_path)\n        return config_dir_path\n", "idx": 1021}
{"namespace": "mopidy.ext.Extension.get_data_dir", "completion": "        if cls.ext_name is None:\n            raise AssertionError\n        data_dir_path = (\n            path.expand_path(config[\"core\"][\"data_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(data_dir_path)\n        return data_dir_path\n", "idx": 1022}
{"namespace": "mopidy.ext.load_extensions", "completion": "    installed_extensions: List[ExtensionData] = []\n    for entry_point in pkg_resources.iter_entry_points(\"mopidy.ext\"):\n        try:\n            extension = entry_point.load()\n        except Exception as e:\n            logger.warning(\n                \"Failed to load extension %s: %s\", entry_point.name, e\n            )\n            continue\n        if not issubclass(extension, Extension):\n            logger.warning(\n                \"Extension %s is not a valid extension class\", entry_point.name\n            )\n            continue\n        config_schema = extension.get_config_schema()\n        config_defaults = extension.get_default_config()\n        command = extension.get_command()\n        installed_extensions.append(\n            ExtensionData(\n                extension=extension,\n                entry_point=entry_point,\n                config_schema=config_schema,\n                config_defaults=config_defaults,\n                command=command,\n            )\n        )\n    return installed_extensions", "idx": 1023}
{"namespace": "mopidy.ext.validate_extension_data", "completion": "    if data.extension.ext_name != data.entry_point.name:\n        logger.error(\n            \"Extension entry point name %s does not match extension name %s\",\n            data.entry_point.name,\n            data.extension.ext_name,\n        )\n        return False\n\n    try:\n        data.extension.validate_environment()\n    except Exception as e:\n        logger.error(\n            \"Extension %s failed to validate environment: %s\",\n            data.extension.ext_name,\n            e,\n        )\n        return False\n\n    if not data.config_schema:\n        logger.error(\n            \"Extension %s has no config schema\", data.extension.ext_name\n        )\n        return False\n\n    if not data.config_defaults:\n        logger.error(\n            \"Extension %s has no default config\", data.extension.ext_name\n        )\n        return False\n\n    return True", "idx": 1024}
{"namespace": "mopidy.httpclient.format_user_agent", "completion": "    if name is None:\n        name = \"Mopidy\"\n    return f\"{name}/{mopidy.__version__} ({platform.python_implementation()} {platform.python_version()})\"", "idx": 1025}
{"namespace": "mopidy.models.immutable.ValidatedImmutableObject.replace", "completion": "        other = copy.copy(self)\n        for key, value in kwargs.items():\n            if not self._is_valid_field(key):\n                raise TypeError(\n                    f\"replace() got an unexpected keyword argument {key}\"\n                )\n            other._set_field(key, value)\n        return other", "idx": 1026}
{"namespace": "mopidy.http.Extension.get_default_config", "completion": "        conf_file = os.path.join(os.path.dirname(__file__), \"ext.conf\")\n        return config_lib.read(conf_file)\n", "idx": 1027}
{"namespace": "mopidy.http.Extension.get_config_schema", "completion": "        schema = super().get_config_schema()\n        schema[\"hostname\"] = config_lib.Hostname()\n        schema[\"port\"] = config_lib.Port()\n        schema[\"static_dir\"] = config_lib.Path()\n        schema[\"debug\"] = config_lib.Boolean()\n        schema[\"allowed_origins\"] = config_lib.List()\n        schema[\"auth_basic\"] = config_lib.String()\n        schema[\"auth_digest\"] = config_lib.String()\n        schema[\"auth_digest_users\"] = config_lib.List()\n        schema[\"auth_digest_realm\"] = config_lib.String()\n        schema[\"auth_digest_nonce_count\"] = config_lib.Integer()\n        schema[\"auth_digest_opaque\"] = config_lib.String()\n        schema[\"auth_digest_method\"] = config_lib.String()\n        schema[\"auth_digest_qop\"] = config_lib.String()\n        schema[\"auth_digest_nonce_max_age\"] = config_lib.Integer()\n        schema[\"auth_digest_stale\"] = config_lib.Boolean()\n        schema[\"auth_digest_stale_while_revalidate\"] = config_lib.Integer()\n        schema[\"auth_digest_revalidate_override\"] = config_lib.Integer()\n        schema[\"auth_digest_proxy_revalidate\"] = config_lib.Boolean()\n        schema[\"auth_digest_proxy_revalidate_override\"] = config_lib.Integer()\n        schema[\"auth_digest_cache\"] = config_lib.String()\n        schema[\"auth_digest_cache_size\"] = config_lib.Integer()\n        schema[\"auth_digest_cache_expiration\"] = config_lib.Integer()\n        schema[\"auth_digest_cache_file\"] = config_lib.Path()\n        schema[\"auth_digest_cache_file_mode\"] = config_lib.Integer()\n        schema[\"auth_digest_cache_file_owner\"] = config_lib.String()\n        schema[\"auth_digest_cache_file_group", "idx": 1028}
{"namespace": "mopidy.internal.network.try_ipv6_socket", "completion": "    try:\n        socket.socket(socket.AF_INET6, socket.SOCK_STREAM)\n        return True\n    except socket.error:\n        logger.debug(\"IPv6 not supported\")\n        return False\n\n", "idx": 1029}
{"namespace": "mopidy.internal.network.format_hostname", "completion": "    if not hostname:\n        return hostname\n\n    if \":\" in hostname:\n        # IPv6 address\n        if hostname.count(\":\") == 7:\n            # IPv4-mapped IPv6 address\n            if hostname.startswith(\"::ffff:\"):\n                return hostname\n            # Convert IPv6 address to IPv4-mapped IPv6 address\n            hostname = \"::ffff:\" + hostname\n        # Convert IPv6 address to IPv4-mapped IPv6 address\n        elif hostname.startswith(\"::\"):\n            hostname = \"::ffff:\" + hostname[2:]\n        # Convert IPv6 address to IPv4-mapped IPv6 address\n        elif hostname.endswith(\"::\"):\n            hostname = hostname[:-2] + \"::ffff:\"\n        # Convert IPv6 address to IPv4-mapped IPv6 address\n        elif hostname.count(\"::\") == 1:\n            hostname = hostname.replace(\"::\", \"::ffff:\")\n\n    return hostname", "idx": 1030}
{"namespace": "mopidy.internal.xdg.get_dirs", "completion": "    dirs = {\n        \"XDG_DATA_HOME\": pathlib.Path(os.environ.get(\"XDG_DATA_HOME\", \"~/.local/share\")).expanduser(),\n        \"XDG_CONFIG_HOME\": pathlib.Path(os.environ.get(\"XDG_CONFIG_HOME\", \"~/.config\")).expanduser(),\n        \"XDG_CACHE_HOME\": pathlib.Path(os.environ.get(\"XDG_CACHE_HOME\", \"~/.cache\")).expanduser(),\n        \"XDG_RUNTIME_DIR\": pathlib.Path(os.environ.get(\"XDG_RUNTIME_DIR\", \"~/.xdg-runtime-dir\")).expanduser(),\n        \"XDG_DATA_DIRS\": [pathlib.Path(p).expanduser() for p in os.environ.get(\"XDG_DATA_DIRS\", \"/usr/local/share/:/usr/share/\").split(\":\")],\n        \"XDG_CONFIG_DIRS\": [pathlib.Path(p).expanduser() for p in os.environ.get(\"XDG_CONFIG_DIRS\", \"/etc/xdg\").split(\":\")],\n        \"XDG_STATE_HOME\": pathlib.Path(os.environ.get(\"XDG_STATE_HOME\", \"~/.local/state\")).expanduser(),\n        \"XDG_DESKTOP_DIR\": pathlib.Path(os.environ.get(\"XDG_DESKTOP_DIR\", \"~/Desktop\")).expanduser(),\n        \"XDG_DOCUMENTS_DIR\": pathlib.Path(os.environ.get(\"XDG_DOCUMENTS_DIR\", \"~/Documents\")).expanduser(),\n        \"XDG_DOWNLOAD_DIR\": pathlib.Path(os.environ.get(\"XDG_DOWNLOAD_DIR\", \"~/Downloads\")).expanduser(),\n        \"XDG_MUSIC_DIR\": pathlib.Path(os.environ.get(\"X", "idx": 1031}
{"namespace": "mopidy.internal.log.get_verbosity_level", "completion": "    verbosity_level = base_verbosity_level + args_verbosity_level\n    if verbosity_level < min(LOG_LEVELS):\n        verbosity_level = min(LOG_LEVELS)\n    if verbosity_level > max(LOG_LEVELS):\n        verbosity_level = max(LOG_LEVELS)\n    return verbosity_level", "idx": 1032}
{"namespace": "mopidy.internal.validation.check_instance", "completion": "    if not isinstance(arg, cls):\n        raise exceptions.ValidationError(\n            msg.format(arg=arg, name=cls.__name__)\n        )", "idx": 1033}
{"namespace": "mopidy.internal.validation.check_instances", "completion": "    if not isinstance(arg, list):\n        raise exceptions.ValidationError(msg.format(arg=arg, name=cls.__name__))\n    for a in arg:\n        if not isinstance(a, cls):\n            raise exceptions.ValidationError(msg.format(arg=arg, name=cls.__name__))", "idx": 1034}
{"namespace": "mopidy.internal.validation.check_uri", "completion": "    if not isinstance(arg, str):\n        raise exceptions.ValidationError(msg.format(arg=arg))\n    try:\n        parsed = urllib.parse.urlparse(arg)\n    except ValueError:\n        raise exceptions.ValidationError(msg.format(arg=arg))\n    if not parsed.scheme:\n        raise exceptions.ValidationError(msg.format(arg=arg))", "idx": 1035}
{"namespace": "mopidy.internal.validation.check_uris", "completion": "    _check_iterable(arg, msg)\n    [check_uri(uri) for uri in arg]", "idx": 1036}
{"namespace": "mopidy.internal.playlists.parse", "completion": "    handlers = {\n        \"m3u\": _parse_m3u,\n        \"m3u8\": _parse_m3u8,\n        \"pls\": _parse_pls,\n        \"xspf\": _parse_xspf,\n        \"opml\": _parse_opml,\n        \"yml\": _parse_yml,\n        \"yaml\": _parse_yml,\n        \"json\": _parse_json,\n        \"uri\": _parse_uri,\n    }\n\n    for handler, detector in handlers.items():\n        if detector(data):\n            return handlers[handler](data)\n\n    return _parse_uri(data)\n\n", "idx": 1037}
{"namespace": "mopidy.config.schemas.ConfigSchema.deserialize", "completion": "        result = {}\n        errors = {}\n        for key, schema in self.items():\n            if key in values:\n                value = values[key]\n                try:\n                    result[key] = schema.deserialize(value)\n                except ValueError as e:\n                    errors[key] = str(e)\n                    result[key] = None\n            else:\n                errors[key] = \"Missing\"\n\n        for key in list(result.keys()):\n            if key not in self:\n                del result[key]\n\n        return result, errors\n", "idx": 1038}
{"namespace": "mopidy.config.types.String.deserialize", "completion": "        value = decode(value)\n        value = value.strip()\n\n        if self._required and not value:\n            raise ValueError(\"Value is required\")\n\n        if self._transformer:\n            value = self._transformer(value)\n\n        if self._choices:\n            if value not in self._choices:\n                raise ValueError(\n                    f\"Value must be one of {', '.join(self._choices)}\"\n                )\n\n        return value\n", "idx": 1039}
{"namespace": "mopidy.config.types.String.serialize", "completion": "        if value is None:\n            return \"\"\n        if isinstance(value, _TransformedValue):\n            value = value.original\n        return encode(value)", "idx": 1040}
{"namespace": "mopidy.config.types.Secret.serialize", "completion": "        if value is not None and display:\n            return \"********\"\n        return super().serialize(value, display)", "idx": 1041}
{"namespace": "mopidy.config.types.Integer.deserialize", "completion": "        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        try:\n            value = int(value)\n        except ValueError:\n            raise ValueError(\n                f\"Invalid integer value: {value}. \"\n                \"Integer values must be base 10 integers.\"\n            )\n\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        validators.validate_choice(value, self._choices)\n        return value\n", "idx": 1042}
{"namespace": "mopidy.config.types.Float.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = float(value)\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        return value", "idx": 1043}
{"namespace": "mopidy.config.types.Boolean.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = value.lower()\n        if value in self.true_values:\n            return True\n        elif value in self.false_values:\n            return False\n        raise ValueError(\n            f\"Invalid boolean value: {value}. \"\n            f\"Must be one of {self.true_values} or {self.false_values}\"\n        )\n", "idx": 1044}
{"namespace": "mopidy.config.types.Pair.deserialize", "completion": "        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        if self._separator in value:\n            parts = value.split(self._separator, 1)\n            if len(parts) == 1:\n                if self._optional_pair:\n                    parts = [value, value]\n                else:\n                    raise ValueError(\n                        f\"config value must include separator {self._separator}\"\n                    )\n        else:\n            parts = [value, value]\n\n        return tuple(\n            subtype.deserialize(encode(part)) for subtype, part in zip(self._subtypes, parts)\n        )\n", "idx": 1045}
{"namespace": "mopidy.config.types.Pair.serialize", "completion": "        if value is None:\n            return \"\"\n\n        if display and self._optional_pair and value[0] == value[1]:\n            return self._subtypes[0].serialize(value[0], display)\n\n        return (\n            self._subtypes[0].serialize(value[0], display)\n            + self._separator\n            + self._subtypes[1].serialize(value[1], display)\n        )", "idx": 1046}
{"namespace": "mopidy.config.types.List.serialize", "completion": "        # This is necessary for backwards-compatibility, in case subclasses\n        # aren't calling their parent constructor.\n        subtype = getattr(self, \"_subtype\", String())\n\n        if not value:\n            return \"\"\n\n        if self._unique:\n            values = list(value)\n        else:\n            values = value\n\n        serialized_values = [\n            subtype.serialize(v, display=display) for v in values\n        ]\n        return \"\\n\".join(serialized_values)", "idx": 1047}
{"namespace": "mopidy.config.types.LogColor.deserialize", "completion": "        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = value.lower()\n        validators.validate_choice(value, self._choices)\n        return value", "idx": 1048}
{"namespace": "mopidy.config.types.LogColor.serialize", "completion": "        if value.lower() in log.COLORS:\n            return value.lower()\n        else:\n            return \"\"", "idx": 1049}
{"namespace": "mopidy.config.types.LogLevel.deserialize", "completion": "        value = decode(value)\n        validators.validate_choice(value.lower(), self.levels)\n        return self.levels[value.lower()]\n", "idx": 1050}
{"namespace": "mopidy.config.types.LogLevel.serialize", "completion": "        for key, val in self.levels.items():\n            if val == value:\n                return encode(key)\n        return \"\"", "idx": 1051}
{"namespace": "mopidy.config.types.Hostname.deserialize", "completion": "        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        if value.startswith(\"unix://\"):\n            return str(value)\n\n        try:\n            socket.getaddrinfo(value, 0, socket.AF_UNSPEC, socket.SOCK_STREAM)\n        except socket.gaierror:\n            raise ValueError(f\"invalid hostname: {value}\")\n\n        return value\n", "idx": 1052}
{"namespace": "mopidy.config.load", "completion": "    # Determine the configuration directory based on the current file path.\n    config_dir = os.path.dirname(files[0])\n\n    # Read the default configuration file and append it to an empty list.\n    default_config = []\n    default_config.append(read(os.path.join(config_dir, \"mopidy.conf\")))\n\n    # Extend the list using ext_defaults.\n    default_config.extend(read(os.path.join(config_dir, f)) for f in ext_defaults)\n\n    # Load the configuration files, combine them with the default configurations and any overrides, and store the result in the variable \"raw_config\".\n    raw_config = configparser.ConfigParser()\n    raw_config.read_file(itertools.chain(default_config, files, overrides))\n\n    # Append the external schemas to the list of schemas and validate the \"raw_config\" against the schemas.\n    schemas = _schemas.copy()\n    schemas.extend(ext_schemas)\n    return _validate(raw_config, schemas)", "idx": 1053}
{"namespace": "mopidy.config.format_initial", "completion": "    config_dir = pathlib.Path(__file__).parent\n    defaults = [read(config_dir / \"default.conf\")]\n    raw_config = _load(defaults, [])\n    schemas = _schemas[:]\n    for extension_data in extensions_data:\n        extension_name = extension_data[\"name\"]\n        extension_version = extension_data[\"version\"]\n        extension_default = read(config_dir / f\"{extension_name}.conf\")\n        defaults.append(extension_default)\n        schemas.append(ConfigSchema(extension_name))\n        raw_config[extension_name] = _load_raw_config(extension_default)\n        raw_config[extension_name][\"version\"] = extension_version\n    config = _validate(raw_config, schemas)\n    comments = {}\n    for extension_data in extensions_data:\n        extension_name = extension_data[\"name\"]\n        extension_version = extension_data[\"version\"]\n        comments[extension_name] = f\"# {extension_name} {extension_version}\"\n    return _format(config, comments, schemas, False, True)", "idx": 1054}
{"namespace": "mopidy.config._load", "completion": "    config = configparser.RawConfigParser()\n    config.set(\"DEFAULT\", \"__file__\", __file__)\n    config.set(\"DEFAULT\", \"__name__\", __name__)\n    config.set(\"DEFAULT\", \"__package__\", __package__)\n    config.set(\"DEFAULT\", \"__version__\", __version__)\n    config.set(\"DEFAULT\", \"__author__\", __author__)\n    config.set(\"DEFAULT\", \"__license__\", __license__)\n    config.set(\"DEFAULT\", \"__copyright__\", __copyright__)\n    config.set(\"DEFAULT\", \"__doc__\", __doc__)\n    config.set(\"DEFAULT\", \"__url__\", __url__)\n    config.set(\"DEFAULT\", \"__title__\", __title__)\n    config.set(\"DEFAULT\", \"__summary__\", __summary__)\n    config.set(\"DEFAULT\", \"__email__\", __email__)\n    config.set(\"DEFAULT\", \"__keywords__\", __keywords__)\n    config.set(\"DEFAULT\", \"__classifiers__\", __classifiers__)\n    config.set(\"DEFAULT\", \"__requires_python__\", __requires_python__)\n    config.set(\"DEFAULT\", \"__project_urls__\", __project_urls__)\n    config.set(\"DEFAULT\", \"__download_url__\", __download_url__)\n    config.set(\"DEFAULT\", \"__bugtrack_url__\", __bugtrack_url__)\n    config.set(\"DEFAULT\", \"__docs_url__\", __docs_url__)\n    config.set(\"DEFAULT\", \"__source_url__\", __source_url__)\n    config.set(\"DEFAULT\", \"__issue_url__\", __issue_url__)\n    config.set(\"DEFAULT\", \"__discussion_url__\", __discussion_url__)\n    config.set(\"DEFAULT\", \"__version_url__\", __version_url__)\n    config.set(\"DEFAULT\", \"__download_url__\", __download_url__)\n    config.set(\"DEFAULT\", \"__docs_url__\", __docs_url__)\n    config.set(\"DEFAULT\", \"__source_url__\", __source_url__)\n    config.set(\"DEFAULT\", \"__", "idx": 1055}
{"namespace": "mopidy.config._validate", "completion": "    validated_config = {}\n    errors = {}\n    for schema in schemas:\n        section = schema.name\n        if section not in raw_config:\n            continue\n        try:\n            validated_config[section] = schema.deserialize(raw_config[section])\n        except ValueError as e:\n            errors[section] = e\n            logger.warning(\n                f\"Config section [{section}] has errors; \"\n                f\"{e.args[0]} ({e.args[1]})\"\n            )\n        except Exception as e:\n            errors[section] = e\n            logger.warning(\n                f\"Config section [{section}] has errors; \"\n                f\"{e.args[0]} ({e.args[1]})\"\n            )\n    for section in raw_config:\n        if section not in validated_config:\n            logger.warning(\n                f\"Config section [{section}] is not defined in the config \"\n                f\"schema; it will be ignored\"\n            )\n    return validated_config, errors\n\n", "idx": 1056}
{"namespace": "mingus.extra.tunings.get_tunings", "completion": "    result = []\n    if instrument is None:\n        for x in _known.values():\n            for y in x[1].values():\n                if nr_of_strings is None and nr_of_courses is None:\n                    result.append(y)\n                elif nr_of_strings is not None and nr_of_courses is None:\n                    if y.count_strings() == nr_of_strings:\n                        result.append(y)\n                elif nr_of_strings is None and nr_of_courses is not None:\n                    if y.count_courses() == nr_of_courses:\n                        result.append(y)\n                else:\n                    if y.count_courses() == nr_of_courses and y.count_strings() == nr_of_strings:\n                        result.append(y)\n    else:\n        searchi = str.upper(instrument)\n        keys = list(_known.keys())\n        for x in keys:\n            if (\n                searchi not in keys\n                and x.find(searchi) == 0\n                or searchi in keys\n                and x == searchi\n            ):\n                for y in _known[x][1].values():\n                    if nr_of_strings is None and nr_of_courses is None:\n                        result.append(y)\n                    elif nr_of_strings is not None and nr_of_courses is None:\n                        if y.count_strings() == nr_of_strings:\n                            result.append(y)\n                    elif nr_of_strings is None and nr_of_courses is not None:\n                        if y.count_courses() == nr_of_courses:\n                            result.append(y)\n                    else:\n                        if y.count_courses() == nr_of_courses and y.count_strings() == nr_of_strings:\n                            result.append(y)\n    return result", "idx": 1057}
{"namespace": "mingus.containers.instrument.Instrument.note_in_range", "completion": "        if isinstance(note, six.string_types):\n            note = Note(note)\n        if not hasattr(note, \"name\"):\n            raise UnexpectedObjectError(\n                \"Unexpected object '%s'. \"\n                \"Expecting a mingus.containers.Note object\" % note\n            )\n        if note < self.range[0] or note > self.range[1]:\n            return False\n        return True", "idx": 1058}
{"namespace": "mingus.containers.instrument.Guitar.can_play_notes", "completion": "        if len(notes) > 6:\n            return False\n        return Instrument.can_play_notes(self, notes)", "idx": 1059}
{"namespace": "mingus.containers.bar.Bar.get_range", "completion": "        highest = -1\n        lowest = 100\n        for x in self.bar:\n            if x[2].notes[0].int_value() > highest:\n                highest = x[2].notes[0].int_value()\n            if x[2].notes[0].int_value() < lowest:\n                lowest = x[2].notes[0].int_value()\n        return (highest, lowest)", "idx": 1060}
{"namespace": "mingus.containers.bar.Bar.transpose", "completion": "        if up:\n            for cont in self.bar:\n                cont[2].transpose(interval)\n        else:\n            for cont in self.bar:\n                cont[2].transpose(-interval)\n", "idx": 1061}
{"namespace": "mingus.containers.bar.Bar.determine_chords", "completion": "        chords = []\n        for i in range(len(self.bar)):\n            chords.append([self.bar[i][0], self.bar[i][2].determine_chords(shorthand)])\n        return chords", "idx": 1062}
{"namespace": "mingus.containers.note.Note.transpose", "completion": "        if up:\n            self.name = notes.transpose(self.name, interval)\n        else:\n            self.name = notes.transpose(self.name, interval, up=False)\n", "idx": 1063}
{"namespace": "mingus.containers.note.Note.from_int", "completion": "        if integer < 0:\n            raise ValueError(\"Integer must be positive\")\n        if integer > 127:\n            raise ValueError(\"Integer must be less than 128\")\n\n        self.name = notes.from_int(integer)\n        self.octave = int(log(integer + 1, 2) / 12)\n        return self", "idx": 1064}
{"namespace": "mingus.containers.note.Note.to_hertz", "completion": "        # TODO: This is a very old function. It should be rewritten.\n        # TODO: This function is not used anywhere.\n        # TODO: This function is not tested.\n        # TODO: This function is not documented.\n        # TODO: This function is not pep8 compliant.\n        # TODO: This function is not PEP8 compliant.\n        # TODO: This function is not PEP8 compliant.\n        # TODO: This function is not PEP8 compliant.\n        # TODO: This function is not PEP8 compliant.\n        # TODO: This function is not PEP8 compliant.\n        # TODO: This function is not PEP8 compliant.\n        # TODO: This function is not PEP8 compliant.\n        # TODO: This function is not PEP8 compliant.\n        # TODO: This function is not PEP8 compliant.\n        # TODO: This function is not PEP8 compliant.\n        # TODO: This function is not PEP8 compliant.\n        # TODO: This function is not PEP8 compliant.\n        # TODO: This function is not PEP8 compliant.\n        # TODO: This function is not PEP8 compliant.\n        # TODO: This function is not PEP8 compliant.\n        # TODO: This function is not PEP8 compliant.\n        # TODO: This function is not PEP8 compliant.\n        # TODO: This function is not PEP8 compliant.\n        # TODO: This function is not PEP8 compliant.\n        # TODO: This function is not PEP8 compliant.\n        # TODO: This function is not PEP8 compliant.\n        # TODO: This function is not PEP8 compliant.\n        # TODO: This function is not PEP8 compliant.\n        # TODO: This function is not PEP8 compliant.\n        # TODO: This function is not PEP8 compliant.\n        # TODO: This function is not PEP8 compliant.\n        # TODO: This function is not PEP8 compliant.\n        # TODO: This function is not PEP8 compliant.\n        # TODO: This function is not PEP8 compliant", "idx": 1065}
{"namespace": "mingus.containers.note.Note.from_hertz", "completion": "        # int(Note(\"A\")) == 57\n        diff = int(round(12 * log(hertz / standard_pitch, 2)))\n        self.from_int(57 + diff)\n        return self", "idx": 1066}
{"namespace": "mingus.containers.note.Note.to_shorthand", "completion": "        if self.name == \"C\":\n            return \"C\"\n        elif self.name == \"C#\" or self.name == \"Db\":\n            return \"C#\"\n        elif self.name == \"D\":\n            return \"D\"\n        elif self.name == \"D#\" or self.name == \"Eb\":\n            return \"D#\"\n        elif self.name == \"E\":\n            return \"E\"\n        elif self.name == \"F\":\n            return \"F\"\n        elif self.name == \"F#\" or self.name == \"Gb\":\n            return \"F#\"\n        elif self.name == \"G\":\n            return \"G\"\n        elif self.name == \"G#\" or self.name == \"Ab\":\n            return \"G#\"\n        elif self.name == \"A\":\n            return \"A\"\n        elif self.name == \"A#\" or self.name == \"Bb\":\n            return \"A#\"\n        elif self.name == \"B\":\n            return \"B\"\n        else:\n            raise NoteFormatError(\"Invalid note representation: %r\" % self.name)", "idx": 1067}
{"namespace": "mingus.containers.note_container.NoteContainer.from_chord_shorthand", "completion": "        self.empty()\n        self.add_notes(chords.from_shorthand(shorthand))\n        return self\n", "idx": 1068}
{"namespace": "mingus.containers.note_container.NoteContainer.from_interval_shorthand", "completion": "        self.empty()\n        if isinstance(startnote, six.string_types):\n            startnote = Note(startnote)\n        if not isinstance(startnote, Note):\n            raise UnexpectedObjectError(\n                \"Object '%s' was not expected. \"\n                \"Expecting a mingus.containers.Note object.\" % startnote\n            )\n        self.add_note(startnote)\n        if up:\n            self.add_notes(intervals.transpose(startnote, shorthand))\n        else:\n            self.add_notes(intervals.transpose(startnote, shorthand, up=False))\n        return self", "idx": 1069}
{"namespace": "mingus.containers.note_container.NoteContainer.from_progression_shorthand", "completion": "        self.empty()\n        self.add_notes(intervals.from_shorthand(shorthand, key))\n        return self", "idx": 1070}
{"namespace": "mingus.containers.note_container.NoteContainer.transpose", "completion": "        for n in self.notes:\n            n.transpose(interval, up)\n        return self", "idx": 1071}
{"namespace": "mingus.containers.note_container.NoteContainer.get_note_names", "completion": "        return [x.name for x in self.notes]\n", "idx": 1072}
{"namespace": "mingus.core.notes.int_to_note", "completion": "    if note_int < 0 or note_int > 11:\n        raise RangeError(\"Note out of range\")\n    note = list(fifths)\n    note.extend(list(fifths))\n    if accidentals == \"#\":\n        return note[note_int]\n    elif accidentals == \"b\":\n        return note[note_int].lower()\n    else:\n        raise ValueError(\"Invalid accidentals\")\n\n", "idx": 1073}
{"namespace": "mingus.core.notes.is_valid_note", "completion": "    from mingus.core.mt_exceptions import NoteFormatError\n    if len(note) == 1:\n        if note in _note_dict:\n            return True\n        else:\n            raise NoteFormatError(\"Note '%s' not recognized\" % note)\n    elif len(note) == 2:\n        if note[0] in _note_dict and note[1] in [\"b\", \"#\"]:\n            return True\n        else:\n            raise NoteFormatError(\"Note '%s' not recognized\" % note)\n    else:\n        raise NoteFormatError(\"Note '%s' not recognized\" % note)", "idx": 1074}
{"namespace": "mingus.core.notes.reduce_accidentals", "completion": "    from mingus.core.mt_exceptions import NoteFormatError\n    if is_valid_note(note):\n        val = _note_dict[note[0]]\n    else:\n        raise NoteFormatError(\"Unknown note format '%s'\" % note)\n\n    # Check for '#' and 'b' postfixes\n    for post in note[1:]:\n        if post == \"b\":\n            val -= 1\n        elif post == \"#\":\n            val += 1\n    return val % 12", "idx": 1075}
{"namespace": "mingus.core.notes.remove_redundant_accidentals", "completion": "    val = note_to_int(note[0])\n    for token in note[1:]:\n        if token == \"b\":\n            val -= 1\n        elif token == \"#\":\n            val += 1\n        else:\n            raise NoteFormatError(\"Unknown note format '%s'\" % note)\n    if val >= note_to_int(note[0]):\n        return int_to_note(val % 12)\n    else:\n        return int_to_note(val % 12, \"b\")", "idx": 1076}
{"namespace": "mingus.core.intervals.minor_second", "completion": "    return notes.diminish(second(note))\n\n", "idx": 1077}
{"namespace": "mingus.core.intervals.major_second", "completion": "    sec = second(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sec, 0)", "idx": 1078}
{"namespace": "mingus.core.intervals.minor_third", "completion": "    third_note = third(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, third_note, 1)\n\n", "idx": 1079}
{"namespace": "mingus.core.intervals.minor_fourth", "completion": "    fth = fourth(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, fth, 5)", "idx": 1080}
{"namespace": "mingus.core.intervals.minor_seventh", "completion": "    svn = seventh(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, svn, 10)\n\n", "idx": 1081}
{"namespace": "mingus.core.intervals.major_seventh", "completion": "    sth = seventh(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sth, 11)", "idx": 1082}
{"namespace": "mingus.core.intervals.measure", "completion": "    intervals = [(notes.note_to_int(key) + x) % 12 for x in [0, 2, 4, 5, 7, 9, 11,]]\n    key_notes = keys.get_notes(key)\n    for x in key_notes:\n        if x[0] == note1[0]:\n            result = (intervals[key_notes.index(x)] + interval) % 12\n    if result in intervals:\n        return key_notes[intervals.index(result)] + note1[1:]\n    else:\n        return notes.diminish(key_notes[intervals.index((result + 1) % 12)] + note1[1:])", "idx": 1083}
{"namespace": "mingus.core.intervals.determine", "completion": "    if note1 == note2:\n        return \"unison\"\n    if shorthand:\n        if note1 == \"C\":\n            if note2 == \"D\":\n                return \"m2\"\n            elif note2 == \"E\":\n                return \"M2\"\n            elif note2 == \"F\":\n                return \"m3\"\n            elif note2 == \"G\":\n                return \"M3\"\n            elif note2 == \"A\":\n                return \"P4\"\n            elif note2 == \"B\":\n                return \"TT\"\n            elif note2 == \"C\":\n                return \"P5\"\n            elif note2 == \"D\":\n                return \"m6\"\n            elif note2 == \"E\":\n                return \"M6\"\n            elif note2 == \"F\":\n                return \"m7\"\n            elif note2 == \"G\":\n                return \"M7\"\n            elif note2 == \"A\":\n                return \"P8\"\n            elif note2 == \"B\":\n                return \"TT\"\n        elif note1 == \"D\":\n            if note2 == \"E\":\n                return \"m2\"\n            elif note2 == \"F\":\n                return \"M2\"\n            elif note2 == \"G\":\n                return \"m3\"\n            elif note2 == \"A\":\n                return \"M3\"\n            elif note2 == \"B\":\n                return \"P4\"\n            elif note2 == \"C\":\n                return \"TT\"\n            elif note2 == \"D\":\n                return \"P5\"\n            elif note2 == \"E\":\n                return \"m6\"\n            elif note2 == \"F\":\n                return \"M6\"\n            elif note2 == \"G\":\n                return \"m7\"\n            elif note2 == \"A\":\n                return \"M7\"\n            elif note2 == \"B\":\n                return \"P8\"\n            elif note2 == \"C\":\n                return \"TT\"\n        elif note1 == \"E\":\n            if note2 == \"F\":\n                return \"m2\"\n            elif note2 == \"G\":\n                return \"M2\"\n            elif note2 ==", "idx": 1084}
{"namespace": "mingus.core.intervals.from_shorthand", "completion": "    # Check if the input is valid\n    if not notes.is_valid_note(note):\n        return False\n    if not interval.isdigit():\n        return False\n    if int(interval) < 1 or int(interval) > 7:\n        return False\n\n    # Get the number of half steps from the starting note to the resulting note\n    half_steps = int(interval)\n    if interval[-1] == \"#\":\n        half_steps += 1\n    elif interval[-1] == \"b\":\n        half_steps -= 1\n\n    # Move up or down by the specified number of half steps\n    if up:\n        result = notes.int_to_note(notes.note_to_int(note) + half_steps)\n    else:\n        result = notes.int_to_note(notes.note_to_int(note) - half_steps)\n\n    # Return the resulting note\n    return result", "idx": 1085}
{"namespace": "mingus.core.intervals.is_consonant", "completion": "    if note1 == note2:\n        return True\n    if note1[0] == note2[0]:\n        return False\n    if note1[0] == \"C\" and note2[0] == \"F\":\n        return True\n    if note1[0] == \"C\" and note2[0] == \"B\":\n        return True\n    if note1[0] == \"C\" and note2[0] == \"E\":\n        return True\n    if note1[0] == \"C\" and note2[0] == \"A\":\n        return True\n    if note1[0] == \"C\" and note2[0] == \"D\":\n        return True\n    if note1[0] == \"C\" and note2[0] == \"G\":\n        return True\n    if note1[0] == \"C\" and note2[0] == \"Bb\":\n        return True\n    if note1[0] == \"C\" and note2[0] == \"Eb\":\n        return True\n    if note1[0] == \"C\" and note2[0] == \"Ab\":\n        return True\n    if note1[0] == \"C\" and note2[0] == \"Db\":\n        return True\n    if note1[0] == \"C\" and note2[0] == \"Gb\":\n        return True\n    if note1[0] == \"C\" and note2[0] == \"F#\":\n        return True\n    if note1[0] == \"C\" and note2[0] == \"B#\":\n        return True\n    if note1[0] == \"C\" and note2[0] == \"E#\":\n        return True\n    if note1[0] == \"C\" and note2[0] == \"A#\":\n        return True\n    if note1[0] == \"C\" and note2[0] == \"D#\":\n        return True\n    if note1[0] == \"C\" and note2[0] == \"G#\":\n        return True\n    if note1[0] == \"C\" and note2[0]", "idx": 1086}
{"namespace": "mingus.core.intervals.is_perfect_consonant", "completion": "    # Check if the interval is a perfect unison\n    if note1 == note2:\n        return True\n\n    # Check if the interval is a perfect fifth\n    if determine(note1, note2) == \"perfect fifth\":\n        return True\n\n    # Check if the interval is a perfect fourth\n    if include_fourths and determine(note1, note2) == \"perfect fourth\":\n        return True\n\n    # Check if the interval is an octave\n    if measure(note1, note2) == 0:\n        return True\n\n    # If none of the above conditions are met, the interval is not a perfect consonant\n    return False\n\n", "idx": 1087}
{"namespace": "mingus.core.keys.get_key", "completion": "    if accidentals in _key_cache:\n        return _key_cache[accidentals]\n\n    if accidentals < 0:\n        raise NoteFormatError(\"Accidentals must be positive.\")\n\n    if accidentals > 7:\n        raise NoteFormatError(\"Accidentals must be less than 8.\")\n\n    major_key = major_keys[accidentals]\n    minor_key = minor_keys[accidentals]\n\n    _key_cache[accidentals] = (major_key, minor_key)\n\n    return major_key, minor_key\n\n", "idx": 1088}
{"namespace": "mingus.core.keys.get_key_signature", "completion": "    if key not in major_keys and key not in minor_keys:\n        raise NoteFormatError(\"Key must be a major or minor key.\")\n    if key in major_keys:\n        return major_keys.index(key) - 7\n    else:\n        return minor_keys.index(key) - 7\n\n", "idx": 1089}
{"namespace": "mingus.core.keys.get_key_signature_accidentals", "completion": "    if not is_valid_key(key):\n        raise NoteFormatError(\"unrecognized format for key '%s'\" % key)\n\n    key_signature_accidentals = []\n    key_signature_accidentals_number = get_key_signature(key)\n\n    if key_signature_accidentals_number > 0:\n        for i in range(key_signature_accidentals_number):\n            key_signature_accidentals.append(\"^\")\n    elif key_signature_accidentals_number < 0:\n        for i in range(abs(key_signature_accidentals_number)):\n            key_signature_accidentals.append(\"_\")\n\n    return key_signature_accidentals\n\n", "idx": 1090}
{"namespace": "mingus.core.keys.get_notes", "completion": "    if key not in major_keys:\n        raise NoteFormatError(\"unrecognized format for key '%s'\" % key)\n\n    if key in _key_cache:\n        return _key_cache[key]\n\n    if key == \"C\":\n        _key_cache[key] = base_scale\n        return base_scale\n\n    if key == \"F\":\n        _key_cache[key] = [\"F\", \"G\", \"A\", \"B\", \"C\", \"D\", \"E\"]\n        return [\"F\", \"G\", \"A\", \"B\", \"C\", \"D\", \"E\"]\n\n    if key == \"Bb\":\n        _key_cache[key] = [\"Bb\", \"C\", \"D\", \"Eb\", \"F\", \"G\", \"A\"]\n        return [\"Bb\", \"C\", \"D\", \"Eb\", \"F\", \"G\", \"A\"]\n\n    if key == \"Eb\":\n        _key_cache[key] = [\"Eb\", \"F\", \"G\", \"Ab\", \"Bb\", \"C\", \"D\"]\n        return [\"Eb\", \"F\", \"G\", \"Ab\", \"Bb\", \"C\", \"D\"]\n\n    if key == \"Ab\":\n        _key_cache[key] = [\"Ab\", \"Bb\", \"C\", \"Db\", \"Eb\", \"F\", \"G\"]\n        return [\"Ab\", \"Bb\", \"C\", \"Db\", \"Eb\", \"F\", \"G\"]\n\n    if key == \"Db\":\n        _key_cache[key] = [\"Db\", \"Eb\", \"F\", \"Gb\", \"Ab\", \"Bb\", \"C\"]\n        return [\"Db\", \"Eb\", \"F\", \"Gb\", \"Ab\", \"Bb\", \"C\"]\n\n    if key == \"Gb\":\n        _key_cache[key] = [\"Gb\", \"Ab\", \"Bb\", \"Cb\", \"Db\", \"Eb\", \"F\"]\n        return [\"Gb\", \"Ab\", \"Bb\", \"Cb\", \"Db\", \"Eb\", \"F\"]\n\n    if key == \"B\":", "idx": 1091}
{"namespace": "mingus.core.keys.relative_major", "completion": "    if not is_valid_key(key):\n        raise NoteFormatError(\"unrecognized format for key '%s'\" % key)\n    if key in minor_keys:\n        return major_keys[minor_keys.index(key)]\n    else:\n        return key", "idx": 1092}
{"namespace": "mingus.core.chords.augmented_triad", "completion": "    return [note, intervals.major_third(note), intervals.augmented_fifth(note)]", "idx": 1093}
{"namespace": "mingus.core.chords.determine", "completion": "    if len(chord) == 3:\n        if chord[0] == chord[1] and chord[1] == chord[2]:\n            if shorthand:\n                return [\"I\"]\n            else:\n                return [\"Tonic\"]\n        elif chord[0] == chord[1] and chord[1] == notes.transpose(chord[2], \"P5\"):\n            if shorthand:\n                return [\"II\"]\n            else:\n                return [\"Supertonic\"]\n        elif chord[0] == chord[1] and chord[1] == notes.transpose(chord[2], \"M3\"):\n            if shorthand:\n                return [\"III\"]\n            else:\n                return [\"Mediant\"]\n        elif chord[0] == chord[1] and chord[1] == notes.transpose(chord[2], \"P8\"):\n            if shorthand:\n                return [\"IV\"]\n            else:\n                return [\"Subdominant\"]\n        elif chord[0] == chord[1] and chord[1] == notes.transpose(chord[2], \"P5\"):\n            if shorthand:\n                return [\"V\"]\n            else:\n                return [\"Dominant\"]\n        elif chord[0] == chord[1] and chord[1] == notes.transpose(chord[2], \"M3\"):\n            if shorthand:\n                return [\"VI\"]\n            else:\n                return [\"Submediant\"]\n        elif chord[0] == chord[1] and chord[1] == notes.transpose(chord[2], \"P8\"):\n            if shorthand:\n                return [\"VII\"]\n            else:\n                return [\"Subtonic\"]\n        elif chord[0] == chord[1] and chord[1] == notes.transpose(chord[2], \"P5\"):\n            if shorthand:\n                return [\"I7\"]\n            else:\n                return [\"Tonic 7th\"]\n        elif chord[0]", "idx": 1094}
{"namespace": "mingus.core.value.septuplet", "completion": "    if in_fourths:\n        return tuplet(value, 7, 4)\n    else:\n        return tuplet(value, 7, 8)", "idx": 1095}
{"namespace": "mingus.core.value.determine", "completion": "    for i in range(len(base_values)):\n        if value == base_values[i]:\n            return (base_values[i], 0, 1)\n        elif value == base_triplets[i]:\n            return (base_values[i], 0, 3)\n        elif value == base_quintuplets[i]:\n            return (base_values[i], 0, 5)\n        elif value == base_septuplets[i]:\n            return (base_values[i], 0, 7)\n    for i in range(len(base_values)):\n        if value == base_values[i] * 1.5:\n            return (base_values[i], 1, 1)\n        elif value == base_triplets[i] * 1.5:\n            return (base_values[i], 1, 3)\n        elif value == base_quintuplets[i] * 1.5:\n            return (base_values[i], 1, 5)\n        elif value == base_septuplets[i] * 1.5:\n            return (base_values[i], 1, 7)\n    for i in range(len(base_values)):\n        if value == base_values[i] * 2:\n            return (base_values[i], 2, 1)\n        elif value == base_triplets[i] * 2:\n            return (base_values[i], 2, 3)\n        elif value == base_quintuplets[i] * 2:\n            return (base_values[i], 2, 5)\n        elif value == base_septuplets[i] * 2:\n            return (base_values[i], 2, 7)\n    for i in range(len(base_values)):\n        if value == base_values[i] * 3:\n            return (base_values[i], 3, 1)\n        elif value == base_triplets[i] * 3:\n            return (base_values[", "idx": 1096}
{"namespace": "mingus.core.progressions.substitute_major_for_minor", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Major to minor substitution\n    if (\n        suff == \"M\"\n        or suff == \"M7\"\n        or suff == \"\"\n        and roman in [\"I\", \"IV\", \"V\"]\n        or ignore_suffix\n    ):\n        n = skip(roman, 2)\n        a = interval_diff(roman, n, 3) + acc\n        if suff == \"M\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"m\")))\n        elif suff == \"M7\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"m7\")))\n        elif suff == \"\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"\")))\n    return res\n\n", "idx": 1097}
{"namespace": "mingus.core.progressions.substitute_diminished_for_diminished", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Diminished to diminished substitution\n    if (\n        suff == \"dim7\"\n        or suff == \"dim\"\n        or suff == \"\"\n        and roman == \"VII\"\n        or ignore_suffix\n    ):\n        for i in range(3):\n            n = skip(roman, 1)\n            a = interval_diff(roman, n, 1) + acc\n            if suff == \"dim7\" or ignore_suffix:\n                res.append(tuple_to_string((n, a, \"dim7\")))\n            elif suff == \"dim\" or ignore_suffix:\n                res.append(tuple_to_string((n, a, \"dim\")))\n            elif suff == \"\" or ignore_suffix:\n                res.append(tuple_to_string((n, a, \"\")))\n            roman = n\n    return res", "idx": 1098}
{"namespace": "mingus.core.progressions.substitute_diminished_for_dominant", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Diminished to dominant substitution\n    if (\n        suff == \"dim7\"\n        or suff == \"dim\"\n        or suff == \"\"\n        and roman in [\"VII\"]\n        or ignore_suffix\n    ):\n        if suff == \"\":\n            suff = \"dim\"\n\n        # Add diminished chord\n        last = roman\n        for x in range(4):\n            next = skip(last, 2)\n            acc += interval_diff(last, next, 3)\n            res.append(tuple_to_string((next, acc, suff)))\n            last = next\n    return res", "idx": 1099}
{"namespace": "mingus.core.progressions.substitute", "completion": "    res = []\n    (roman, acc, suff) = parse_string(progression[substitute_index])\n\n    # Harmonic substitutions\n    if depth > 0:\n        res += substitute_harmonic(progression, substitute_index, True)\n        res += substitute_minor_for_major(progression, substitute_index, True)\n        res += substitute_major_for_minor(progression, substitute_index, True)\n        res += substitute_diminished_for_diminished(\n            progression, substitute_index, True\n        )\n        res += substitute_diminished_for_dominant(\n            progression, substitute_index, True\n        )\n\n    # Suffix substitutions\n    if suff == \"7\":\n        res += substitute_harmonic(progression, substitute_index, False)\n        res += substitute_minor_for_major(progression, substitute_index, False)\n        res += substitute_major_for_minor(progression, substitute_index, False)\n        res += substitute_diminished_for_diminished(\n            progression, substitute_index, False\n        )\n        res += substitute_diminished_for_dominant(\n            progression, substitute_index, False\n        )\n    elif suff == \"m7\":\n        res += substitute_minor_for_major(progression, substitute_index, False)\n    elif suff == \"M7\":\n        res += substitute_major_for_minor(progression, substitute_index, False)\n    elif suff == \"dim7\":\n        res += substitute_diminished_for_diminished(\n            progression, substitute_index, False\n        )\n    elif suff == \"dim\":\n        res += substitute_diminished_for_diminished(\n            progression, substitute_index, False\n        )\n    elif suff == \"dom7\":\n        res += substitute_diminished_for_dominant(\n            progression, substitute_index, False\n        )\n    elif suff == \"\":\n        res += substitute_harmonic(progression, substitute_index", "idx": 1100}
{"namespace": "mingus.core.progressions.skip", "completion": "    return numerals[(numerals.index(roman_numeral) + skip_count) % 7]", "idx": 1101}
{"namespace": "exodus_bundler.cli.configure_logging", "completion": "    if quiet:\n        level = logging.ERROR\n    elif verbose:\n        level = logging.DEBUG\n    else:\n        level = logging.INFO\n\n    # Set up the logger.\n\n    \"\"\"\n    Configures the logger with the specified level and adds a stderr handler to log warning and error messages. If `suppress_stdout` is True, it also adds a stdout handler to log debug and info messages.\n    Input-Output Arguments\n    :param level: The log level to set for the logger.\n    :param suppress_stdout: Bool. Whether to suppress log messages from being printed to stdout.\n    :return: No return values.\n    \"\"\"\n    logger = logging.getLogger()\n    logger.setLevel(level)\n\n    # Set up the stderr handler.\n\n    \"\"\"\n    Configures a stderr handler with the specified level and formatter.\n    Input-Output Arguments\n    :param level: The log level to set for the stderr handler.\n    :return: No return values.\n    \"\"\"\n    handler = logging.StreamHandler(sys.stderr)\n    handler.setLevel(level)\n    handler.setFormatter(logging.Formatter('%(levelname)s: %(message)s'))\n    logger.addHandler(handler)\n\n    # Set up the stdout handler.\n\n    \"\"\"\n    Configures a stdout handler with the specified level and formatter.\n    Input-Output Arguments\n    :param level: The log level to set for the stdout handler.\n    :return: No return values.\n    \"\"\"\n    if not suppress_stdout:\n        handler = logging.StreamHandler(sys.stdout)\n        handler.setLevel(logging.DEBUG)\n        handler.setFormatter(logging.Formatter('%(levelname)s: %(message)s'))\n        logger.addHandler(handler)\n\n", "idx": 1102}
{"namespace": "exodus_bundler.bundling.create_unpackaged_bundle", "completion": "    # Create a temporary directory for the bundle.\n    root_directory = tempfile.mkdtemp()\n\n    # Create a temporary directory for the unpackaged bundle.\n    unpackaged_directory = tempfile.mkdtemp(dir=root_directory)\n\n    # Create a temporary directory for the unpackaged bundle.\n    unpackaged_directory = tempfile.mkdtemp(dir=root_directory)\n\n    # Create a temporary directory for the unpackaged bundle.\n    unpackaged_directory = tempfile.mkdtemp(dir=root_directory)\n\n    # Create a temporary directory for the unpackaged bundle.\n    unpackaged_directory = tempfile.mkdtemp(dir=root_directory)\n\n    # Create a temporary directory for the unpackaged bundle.\n    unpackaged_directory = tempfile.mkdtemp(dir=root_directory)\n\n    # Create a temporary directory for the unpackaged bundle.\n    unpackaged_directory = tempfile.mkdtemp(dir=root_directory)\n\n    # Create a temporary directory for the unpackaged bundle.\n    unpackaged_directory = tempfile.mkdtemp(dir=root_directory)\n\n    # Create a temporary directory for the unpackaged bundle.\n    unpackaged_directory = tempfile.mkdtemp(dir=root_directory)\n\n    # Create a temporary directory for the unpackaged bundle.\n    unpackaged_directory = tempfile.mkdtemp(dir=root_directory)\n\n    # Create a temporary directory for the unpackaged bundle.\n    unpackaged_directory = tempfile.mkdtemp(dir=root_directory)\n\n    # Create a temporary directory for the unpackaged bundle.\n    unpackaged_directory = tempfile.mkdtemp(dir=root_directory)\n\n    # Create a temporary directory for the unpackaged bundle.\n    unpackaged_directory = tempfile.mkdtemp(dir=root_directory)\n\n    # Create a temporary directory for the unpackaged bundle.\n    unpackaged_directory = tempfile.mkdtemp(dir=root_directory)\n\n    # Create a temporary directory for the unpackaged bundle.\n    unpackaged_directory = tempfile.mkdtemp(dir=root_directory)", "idx": 1103}
{"namespace": "exodus_bundler.bundling.detect_elf_binary", "completion": "    with open(filename, 'rb') as f:\n        header = f.read(4)\n    return header == b'\\x7fELF'\n\n", "idx": 1104}
{"namespace": "exodus_bundler.bundling.resolve_binary", "completion": "    if os.path.exists(binary):\n        return binary\n\n    for directory in os.environ['PATH'].split(':'):\n        binary_path = os.path.join(directory, binary)\n        if os.path.exists(binary_path):\n            return binary_path\n\n    raise MissingFileError('The \"%s\" binary could not be found in $PATH.' % binary)", "idx": 1105}
{"namespace": "exodus_bundler.bundling.resolve_file_path", "completion": "    if not os.path.exists(path):\n        if search_environment_path:\n            for path in os.getenv('PATH', '/bin/:/usr/bin/').split(os.pathsep):\n                path = os.path.normpath(os.path.abspath(os.path.join(path, path)))\n                if os.path.exists(path):\n                    break\n            else:\n                raise MissingFileError('The \"%s\" file could not be found in $PATH.' % path)\n        else:\n            raise MissingFileError('The \"%s\" file was not found.' % path)\n    if os.path.isdir(path):\n        raise UnexpectedDirectoryError('The \"%s\" file is a directory.' % path)\n    return os.path.normpath(os.path.abspath(path))", "idx": 1106}
{"namespace": "exodus_bundler.bundling.run_ldd", "completion": "    if not detect_elf_binary(binary):\n        raise InvalidElfBinaryError('The \"%s\" binary is not a valid ELF file.' % binary)\n\n    process = Popen([ldd, binary], stdout=PIPE, stderr=PIPE)\n    stdout, stderr = process.communicate()\n    return stdout.decode('utf-8').split('\\n') + stderr.decode('utf-8').split('\\n')", "idx": 1107}
{"namespace": "exodus_bundler.bundling.Elf.dependencies", "completion": "        # Initialize the set of dependencies with the direct dependencies.\n        dependencies = self.find_direct_dependencies()\n        # Initialize a set of dependencies that have already been found.\n        found_dependencies = set()\n        # Initialize a set of dependencies that have already been found.\n        found_dependencies = set()\n        # Initialize a set of dependencies that have already been found.\n        found_dependencies = set()\n        # Initialize a set of dependencies that have already been found.\n        found_dependencies = set()\n        # Initialize a set of dependencies that have already been found.\n        found_dependencies = set()\n        # Initialize a set of dependencies that have already been found.\n        found_dependencies = set()\n        # Initialize a set of dependencies that have already been found.\n        found_dependencies = set()\n        # Initialize a set of dependencies that have already been found.\n        found_dependencies = set()\n        # Initialize a set of dependencies that have already been found.\n        found_dependencies = set()\n        # Initialize a set of dependencies that have already been found.\n        found_dependencies = set()\n        # Initialize a set of dependencies that have already been found.\n        found_dependencies = set()\n        # Initialize a set of dependencies that have already been found.\n        found_dependencies = set()\n        # Initialize a set of dependencies that have already been found.\n        found_dependencies = set()\n        # Initialize a set of dependencies that have already been found.\n        found_dependencies = set()\n        # Initialize a set of dependencies that have already been found.\n        found_dependencies = set()\n        # Initialize a set of dependencies that have already been found.\n        found_dependencies = set()\n        # Initialize a set of dependencies that have already been found.\n        found_dependencies = set()\n        # Initialize a set of dependencies that have already been found.\n        found_dependencies = set()\n        # Initialize a set of dependencies that have already been found.\n        found_dependencies = set()\n        # Initialize a set of dependencies that have already been found.\n       ", "idx": 1108}
{"namespace": "exodus_bundler.bundling.File.hash", "completion": "        with open(self.path, 'rb') as f:\n            content = f.read()\n        return hashlib.sha256(content).hexdigest()\n", "idx": 1109}
{"namespace": "exodus_bundler.bundling.Bundle.add_file", "completion": "        file = File(path, entry_point=entry_point, chroot=self.chroot, file_factory=self.add_file)\n        if file.path in self.files:\n            return\n        self.files.add(file)\n        if file.elf:\n            self.linker_files.add(file.elf.linker_file)\n        if os.path.isdir(file.path):\n            for root, directories, filenames in os.walk(file.path):\n                for filename in filenames:\n                    self.add_file(os.path.join(root, filename))\n        return file\n", "idx": 1110}
{"namespace": "exodus_bundler.bundling.Bundle.bundle_root", "completion": "        return os.path.join(self.working_directory, 'bundles', self.hash)\n", "idx": 1111}
{"namespace": "exodus_bundler.bundling.Bundle.hash", "completion": "        hashes = [file.hash for file in self.files]\n        hashes.sort()\n        combined_hashes = ''.join(hashes)\n        combined_hashes = combined_hashes.encode('utf-8')\n        return hashlib.sha256(combined_hashes).hexdigest()\n", "idx": 1112}
{"namespace": "exodus_bundler.launchers.construct_bash_launcher", "completion": "    return render_template_file(os.path.join(parent_directory, 'bash_launcher.sh'),\n                                linker=linker,\n                                library_path=library_path,\n                                executable=executable,\n                                full_linker=full_linker)", "idx": 1113}
{"namespace": "exodus_bundler.input_parsing.extract_open_path", "completion": "    line = strip_pid_prefix(line)\n    for prefix in ['open(\"', 'openat(AT_FDCWD, \"']:\n        if line.startswith(prefix):\n            line = line[len(prefix):]\n            parts = line.split('\", ')\n            if len(parts) > 1:\n                return parts[0]\n    return None\n\n", "idx": 1114}
{"namespace": "exodus_bundler.input_parsing.extract_paths", "completion": "    paths = set()\n    for line in content.split('\\n'):\n        line = line.strip()\n        if not line:\n            continue\n        if existing_only:\n            if os.path.isdir(line):\n                continue\n            if not os.path.exists(line):\n                continue\n        paths.add(line)\n    return list(paths)", "idx": 1115}
{"namespace": "fs.time.epoch_to_datetime", "completion": "    if t is None:\n        return None\n    return datetime.utcfromtimestamp(t).replace(tzinfo=timezone.utc)", "idx": 1116}
{"namespace": "fs.path.normpath", "completion": "    if not path:\n        return path\n\n    if _requires_normalization(path):\n        raise ValueError(\"Illegal back reference\")\n\n    parts = path.split(\"/\")\n    new_parts = []\n    for part in parts:\n        if part == \".\":\n            continue\n        elif part == \"..\":\n            if not new_parts:\n                raise ValueError(\"Illegal back reference\")\n            new_parts.pop()\n        else:\n            new_parts.append(part)\n\n    return \"/\".join(new_parts)\n\n", "idx": 1117}
{"namespace": "fs.path.iteratepath", "completion": "    return path.split(\"/\")", "idx": 1118}
{"namespace": "fs.path.recursepath", "completion": "    path = normpath(path)\n    if not path:\n        return []\n    if reverse:\n        return [normpath(path[:i]) for i in range(len(path), 0, -1) if path[i - 1] == \"/\"]\n    else:\n        return [normpath(path[:i]) for i in range(1, len(path) + 1) if path[i - 1] == \"/\"]", "idx": 1119}
{"namespace": "fs.path.join", "completion": "    return normpath(\"/\".join(paths))", "idx": 1120}
{"namespace": "fs.path.parts", "completion": "    path = normpath(path)\n    if not path:\n        return []\n    return path.split(\"/\")", "idx": 1121}
{"namespace": "fs.path.splitext", "completion": "    if \"/\" not in path:\n        return (\"\", path)\n    split = path.rsplit(\".\", 1)\n    return (split[0], split[1])", "idx": 1122}
{"namespace": "fs.path.isbase", "completion": "    path1 = abspath(path1)\n    path2 = abspath(path2)\n    return path2.startswith(path1)", "idx": 1123}
{"namespace": "fs.path.frombase", "completion": "    if not isparent(path1, path2):\n        raise ValueError(\"path1 is not a parent directory of path2\")\n    return path2[len(path1) :]", "idx": 1124}
{"namespace": "fs.path.relativefrom", "completion": "    if not isparent(base, path):\n        raise ValueError(\"path must be a child of base\")\n    return \"../\" * len(base.split(\"/\")) + frombase(base, path)", "idx": 1125}
{"namespace": "fs.path.iswildcard", "completion": "    return any(char in path for char in _WILD_CHARS)", "idx": 1126}
{"namespace": "fs.wildcard.match", "completion": "    # Convert the pattern to a regular expression.\n    regex = _pattern_to_regex(pattern)\n\n    # Test the name against the regular expression.\n    return bool(regex.match(name))\n\n", "idx": 1127}
{"namespace": "fs.wildcard.imatch", "completion": "    try:\n        re_pat = _PATTERN_CACHE[(pattern, False)]\n    except KeyError:\n        res = \"(?ms)\" + _translate(pattern) + r\"\\Z\"\n        _PATTERN_CACHE[(pattern, False)] = re_pat = re.compile(res, re.IGNORECASE)\n    return re_pat.match(name) is not None\n\n", "idx": 1128}
{"namespace": "fs.wildcard.get_matcher", "completion": "    if not patterns:\n        return lambda _: True\n\n    if case_sensitive:\n        return partial(match_any, patterns)\n    else:\n        return partial(imatch_any, patterns)", "idx": 1129}
{"namespace": "fs._url_tools.url_quote", "completion": "    if _WINDOWS_PLATFORM:\n        if path_snippet.startswith(\"/\"):\n            path_snippet = path_snippet[1:]\n        if path_snippet.startswith(\"\\\\\"):\n            path_snippet = path_snippet[1:]\n        if path_snippet.startswith(\"/\"):\n            path_snippet = path_snippet[1:]\n        if path_snippet.startswith(\"\\\\\"):\n            path_snippet = path_snippet[1:]\n        if path_snippet.startswith(\"/\"):\n            path_snippet = path_snippet[1:]\n        if path_snippet.startswith(\"\\\\\"):\n            path_snippet = path_snippet[1:]\n        if path_snippet.startswith(\"/\"):\n            path_snippet = path_snippet[1:]\n        if path_snippet.startswith(\"\\\\\"):\n            path_snippet = path_snippet[1:]\n        if path_snippet.startswith(\"/\"):\n            path_snippet = path_snippet[1:]\n        if path_snippet.startswith(\"\\\\\"):\n            path_snippet = path_snippet[1:]\n        if path_snippet.startswith(\"/\"):\n            path_snippet = path_snippet[1:]\n        if path_snippet.startswith(\"\\\\\"):\n            path_snippet = path_snippet[1:]\n        if path_snippet.startswith(\"/\"):\n            path_snippet = path_snippet[1:]\n        if path_snippet.startswith(\"\\\\\"):\n            path_snippet = path_snippet[1:]\n        if path_snippet.startswith(\"/\"):\n            path_snippet = path_snippet[1:]\n        if path_snippet.startswith(\"\\\\", "idx": 1130}
{"namespace": "fs._ftp_parse.parse", "completion": "    decoders = get_decoders()\n    parsed_lines = []\n    for line in lines:\n        if line.strip():\n            for regex, decoder in decoders:\n                match = regex.match(line)\n                if match:\n                    parsed_lines.append(decoder(match))\n                    break\n    return parsed_lines\n\n", "idx": 1131}
{"namespace": "fs._ftp_parse._parse_time", "completion": "    for fmt in formats:\n        try:\n            return time.mktime(time.strptime(t, fmt))\n        except ValueError:\n            pass\n    return None", "idx": 1132}
{"namespace": "fs.permissions.Permissions.parse", "completion": "        if len(ls) != 10:\n            raise ValueError(\"Invalid permissions string\")\n        perms = cls()\n        for i, c in enumerate(ls):\n            if c == \"-\":\n                continue\n            if i == 0:\n                perms.setuid = True\n            elif i == 1:\n                perms.setguid = True\n            elif i == 2:\n                perms.sticky = True\n            elif i < 6:\n                perms.add(\"u_\" + c)\n            elif i < 9:\n                perms.add(\"g_\" + c)\n            else:\n                perms.add(\"o_\" + c)\n        return perms\n", "idx": 1133}
{"namespace": "fs.permissions.Permissions.create", "completion": "        if init is None:\n            return cls()\n        elif isinstance(init, int):\n            return cls(mode=init)\n        elif isinstance(init, Iterable):\n            return cls(names=init)\n        else:\n            raise ValueError(\"Invalid initial value: {}\".format(init))\n", "idx": 1134}
{"namespace": "fs.info.Info.suffix", "completion": "        return cast(Text, self.get(\"basic\", \"suffix\"))\n", "idx": 1135}
{"namespace": "fs.info.Info.suffixes", "completion": "        name = self.get(\"basic\", \"name\")\n        if name.startswith(\".\") and name.count(\".\") == 1:\n            return []\n        return name.split(\".\")\n", "idx": 1136}
{"namespace": "fs.info.Info.stem", "completion": "        name = self.get(\"basic\", \"name\")\n        if name.startswith(\".\") and name.count(\".\") == 1:\n            return \"\"\n        basename, dot, ext = name.rpartition(\".\")\n        return basename\n", "idx": 1137}
{"namespace": "fs.info.Info.type", "completion": "        self._require_namespace(\"details\")\n        return cast(ResourceType, self.get(\"details\", \"type\"))\n", "idx": 1138}
{"namespace": "fs.info.Info.created", "completion": "        self._require_namespace(\"details\")\n        _time = self._make_datetime(self.get(\"details\", \"created\"))\n        return _time\n", "idx": 1139}
{"namespace": "pyinfra.connectors.mech.MechInventoryConnector.make_names_data", "completion": "        mech_config = get_mech_config(limit)\n\n        # Parse the config into a list of dictionaries\n        config = []\n        for line in mech_config:\n            if line.startswith(\"Host \"):\n                config.append({})\n            else:\n                key, value = line.split(\" \", 1)\n                config[-1][key] = value\n\n        # Create a list of host names and their corresponding data\n        names_data = []\n        for host in config:\n            names_data.append(_make_name_data(host))\n\n        return names_data", "idx": 1140}
{"namespace": "pyinfra.connectors.ansible.AnsibleInventoryConnector.make_names_data", "completion": "        if inventory_filename is None:\n            raise InventoryError(\"No Ansible inventory filename provided\")\n\n        if not path.exists(inventory_filename):\n            raise InventoryError(\n                \"Could not find Ansible inventory file: {0}\".format(\n                    inventory_filename\n                )\n            )\n\n        if inventory_filename.endswith(\".ini\"):\n            return AnsibleInventoryConnector.parse_ini(inventory_filename)\n        elif inventory_filename.endswith(\".yml\"):\n            return AnsibleInventoryConnector.parse_yaml(inventory_filename)\n        elif inventory_filename.endswith(\".json\"):\n            return AnsibleInventoryConnector.parse_json(inventory_filename)\n        else:\n            raise InventoryError(\n                \"Unsupported Ansible inventory file format: {0}\".format(\n                    inventory_filename\n                )\n            )\n", "idx": 1141}
{"namespace": "pyinfra.operations.files.rsync", "completion": "    show_rsync_warning()\n\n    yield RsyncCommand(src, dest, flags=flags)", "idx": 1142}
{"namespace": "pyinfra.operations.files.get", "completion": "    # Add deploy directory?\n    if add_deploy_dir and state.cwd:\n        src = os.path.join(state.cwd, src)\n\n    # Ensure the source file exists\n    if not os.path.isfile(src):\n        raise IOError(\"No such file: {0}\".format(src))\n\n    # Ensure the destination directory - if the destination is a link, ensure\n    # the link target is a directory.\n    dest_to_ensure = dest\n    dest_link_info = host.get_fact(Link, path=dest)\n    if dest_link_info:\n        dest_to_ensure = dest_link_info[\"link_target\"]\n\n    # Ensure the destination directory\n    if create_local_dir:\n        yield from directory(path=os.path.dirname(dest_to_ensure))\n\n    # Ensure the destination directory - if the destination is a link, ensure\n    # the link target is a directory.\n    dest_to_ensure = dest\n    dest_link_info = host.get_fact(Link, path=dest)\n    if dest_link_info:\n        dest_to_ensure = dest_link_info[\"link_target\"]\n\n    # Ensure the destination directory\n    if create_local_dir:\n        yield from directory(path=os.path.dirname(dest_to_ensure))\n\n    # Ensure the destination file\n    yield from file(path=dest_to_ensure)\n\n    # Get the file\n    yield FileDownloadCommand(src, dest)\n\n", "idx": 1143}
{"namespace": "pyinfra.operations.files.put", "completion": "    if add_deploy_dir and state.cwd:\n        src = os.path.join(state.cwd, src)\n\n    # Ensure the source file exists\n    if not os.path.exists(src):\n        raise IOError(\"No such file: {0}\".format(src))\n\n    # Ensure the destination directory - if the destination is a link, ensure\n    # the link target is a directory.\n    dest_to_ensure = dest\n    dest_link_info = host.get_fact(Link, path=dest)\n    if dest_link_info:\n        dest_to_ensure = dest_link_info[\"link_target\"]\n\n    if create_remote_dir:\n        yield from _create_remote_dir(state, host, dest_to_ensure, user, group)\n\n    # Ensure the destination file\n    yield from file(\n        path=dest_to_ensure,\n        user=user,\n        group=group,\n        mode=mode,\n        _no_check_owner_mode=True,  # don't check existing user/mode\n        _no_fail_on_link=True,  # don't fail if the path is a link\n    )\n\n    # Upload the file\n    yield FileUploadCommand(src, dest, remote_temp_filename=state.get_temp_filename(dest))\n\n    # Check sha1sum, upload if needed\n    if not assume_exists:\n        local_sum = get_file_sha1(src)\n        remote_sum = host.get_fact(Sha1File, path=dest)\n\n        if local_sum != remote_sum:\n            yield FileUploadCommand(src, dest, remote_temp_filename=state.get_temp_filename(dest))\n\n", "idx": 1144}
{"namespace": "pyinfra.operations.files.file", "completion": "    path = _validate_path(path)\n\n    info = host.get_fact(File, path=path)\n\n    if info is False:  # not a file\n        yield from _raise_or_remove_invalid_path(\n            \"file\",\n            path,\n            force,\n            force_backup,\n            force_backup_dir,\n        )\n        info = None\n\n    if not present:\n        if info:\n            yield StringCommand(\"rm\", \"-f\", QuoteString(path))\n        else:\n            host.noop(\"file {file} does not exist\")\n        return\n\n    if info is None:  # create\n        if create_remote_dir:\n            yield from _create_remote_dir(state, host, path, user, group)\n\n        if touch:\n            yield StringCommand(\"touch\", QuoteString(path))\n        else:\n            yield StringCommand(\"touch\", QuoteString(path))\n\n        if user or group:\n            yield file_utils.chown(path, user, group)\n\n        if mode:\n            yield file_utils.chmod(path, mode)\n\n    else:  # edit\n        changed = False\n\n        # Check user/group\n        if (user and info[\"user\"] != user) or (group and info[\"group\"] != group):\n            yield file_utils.chown(path, user, group)\n            changed = True\n\n        # Check mode\n        if mode and info[\"mode\"] != mode:\n            yield file_utils.chmod(path, mode)\n            changed = True\n\n        if not changed:\n            host.noop(\"file {0} already exists\".format(path))", "idx": 1145}
{"namespace": "pyinfra.operations.python.call", "completion": "    argspec = getfullargspec(function)\n\n    if argspec.varkw is None:\n        kwargs = {}\n\n    if argspec.varargs is None:\n        args = []\n\n    return FunctionCommand(function, args, kwargs)", "idx": 1146}
{"namespace": "pyinfra.api.operation.add_op", "completion": "    # Get the operation name\n    op_name = op_func.__name__\n\n    # Get the operation arguments\n    op_args = args\n    op_kwargs = kwargs\n\n    # Get the operation meta\n    op_meta = op_kwargs.pop(\"_meta\", op_meta_default)\n\n    # Get the operation order\n    op_order = get_operation_order_from_stack(op_func)\n\n    # Get the operation hash\n    op_hash = make_hash(op_name, op_args, op_kwargs)\n\n    # Get the operation changed status\n    op_changed = op_meta.changed if op_meta is not op_meta_default else False\n\n    # Get the operation success status\n    op_success = op_meta.success if op_meta is not op_meta_default else None\n\n    # Get the operation commands\n    op_commands = op_meta.commands if op_meta is not op_meta_default else None\n\n    # Get the operation combined output lines\n    op_combined_output_lines = (\n        op_meta.combined_output_lines if op_meta is not op_meta_default else None\n    )\n\n    # Add the operation to the state\n    state.add_operation(\n        op_name,\n        op_args,\n        op_kwargs,\n        op_hash,\n        op_changed,\n        op_success,\n        op_order,\n        op_commands,\n        op_combined_output_lines,\n    )\n\n", "idx": 1147}
{"namespace": "pyinfra.api.facts.get_facts", "completion": "    facts = {}\n    greenlets = []\n\n    for host in state.get_active_hosts():\n        greenlets.append(gevent.spawn(get_fact, state, host, *args, **kwargs))\n\n    gevent.joinall(greenlets)\n\n    for greenlet in greenlets:\n        host, fact = greenlet.value\n        facts[host] = fact\n\n    return facts", "idx": 1148}
{"namespace": "pyinfra.api.operations.run_ops", "completion": "    if serial:\n        _run_serial_ops(state)\n    elif no_wait:\n        _run_no_wait_ops(state)\n    else:\n        for op_hash in state.get_op_order():\n            _run_single_op(state, op_hash)", "idx": 1149}
{"namespace": "pyinfra.api.connect.connect_all", "completion": "    # Connect to all hosts in parallel\n    gevent.joinall([gevent.spawn(state.connect, host) for host in state.inventory])", "idx": 1150}
{"namespace": "pyinfra.api.arguments.pop_global_arguments", "completion": "    # Get the keys to check\n    if keys_to_check is None:\n        keys_to_check = list(all_argument_meta.keys())\n\n    # Get the config\n    config = state.config if state else host.config\n\n    # Get the host data\n    host_data = host.data if host else {}\n\n    # Get the deploy data\n    deploy_data = state.deploy_data if state else {}\n\n    # Get the global data\n    global_data = state.global_data if state else {}\n\n    # Get the operation data\n    operation_data = state.operation_data if state else {}\n\n    # Get the operation data\n    operation_data = state.operation_data if state else {}\n\n    # Get the operation data\n    operation_data = state.operation_data if state else {}\n\n    # Get the operation data\n    operation_data = state.operation_data if state else {}\n\n    # Get the operation data\n    operation_data = state.operation_data if state else {}\n\n    # Get the operation data\n    operation_data = state.operation_data if state else {}\n\n    # Get the operation data\n    operation_data = state.operation_data if state else {}\n\n    # Get the operation data\n    operation_data = state.operation_data if state else {}\n\n    # Get the operation data\n    operation_data = state.operation_data if state else {}\n\n    # Get the operation data\n    operation_data = state.operation_data if state else {}\n\n    # Get the operation data\n    operation_data = state.operation_data if state else {}\n\n    # Get the operation data\n    operation_data = state.operation_data if state else {}\n\n    # Get the operation data\n    operation_data = state.operation_data if state else {}\n\n    # Get the operation data\n    operation_data = state.operation_data if state else {}\n\n    # Get the operation data\n    operation_data = state.operation_data if state else {}\n\n    # Get the operation data\n    operation_data = state.operation_data if state else {}\n\n    # Get the", "idx": 1151}
{"namespace": "pyinfra_cli.commands.get_func_and_args", "completion": "    operation_name = commands[0]\n    operation_module = try_import_module_attribute(operation_name)\n\n    if not operation_module:\n        raise CliError(f\"Invalid operation: {operation_name}\")\n\n    operation_func = operation_module.operation\n\n    if not operation_func:\n        raise CliError(f\"Invalid operation: {operation_name}\")\n\n    operation_args = commands[1:]\n\n    return operation_func, operation_args\n\n", "idx": 1152}
{"namespace": "viztracer.tracer._VizTracer.start", "completion": "        if self.log_print:\n            builtins.print = self.print\n\n        if self.include_files and self.exclude_files:\n            raise ValueError(\"Both included files and excluded files are specified.\")\n\n        self.enable = True\n        self.parsed = False\n        self._tracer.enable()\n", "idx": 1153}
{"namespace": "viztracer.tracer._VizTracer.stop", "completion": "        self.enable = False\n        if self.log_print:\n            self.restore_print()\n        self._tracer.stop()\n        self.parsed = True\n", "idx": 1154}
{"namespace": "viztracer.report_builder.ReportBuilder.save", "completion": "        if isinstance(output_file, str):\n            if output_file.endswith(\".html\"):\n                with open(output_file, \"w\", encoding=\"utf-8\") as f:\n                    self.generate_report(f, \"html\", file_info=file_info)\n            elif output_file.endswith(\".json\"):\n                with open(output_file, \"w\", encoding=\"utf-8\") as f:\n                    self.generate_report(f, \"json\", file_info=file_info)\n            elif output_file.endswith(\".gz\"):\n                with gzip.open(output_file, \"wb\") as f:\n                    self.generate_report(f, \"json\", file_info=file_info)\n            else:\n                raise ValueError(\"Invalid output file format\")\n            self.final_messages.append((\"view_command\", {\"output_file\": os.path.abspath(output_file)}))\n        else:\n            self.generate_report(output_file, \"json\", file_info=file_info)\n            self.final_messages.append((\"view_command\", {\"output_file\": os.path.abspath(output_file.name)}))\n        for message in self.final_messages:\n            color_print(message)", "idx": 1155}
{"namespace": "viztracer.code_monkey.AstTransformer.get_assign_targets_with_attr", "completion": "        if isinstance(node, ast.Attribute):\n            return [node]\n        elif isinstance(node, (ast.Name, ast.Subscript, ast.Starred)):\n            return []\n        elif isinstance(node, ast.Tuple) or isinstance(node, ast.List):\n            return reduce(lambda a, b: a + b, [self.get_assign_targets_with_attr(elt) for elt in node.elts])\n        color_print(\"WARNING\", \"Unexpected node type {} for ast.Assign. \\\n            Please report to the author github.com/gaogaotiantian/viztracer\".format(type(node)))\n        return []\n", "idx": 1156}
{"namespace": "viztracer.code_monkey.SourceProcessor.process", "completion": "        if isinstance(source, bytes):\n            source = source.decode(\"utf-8\")\n        if not isinstance(source, str):\n            return source\n        lines = source.splitlines()\n        new_lines = []\n        for line in lines:\n            for pattern, func in self.patterns.items():\n                if re.fullmatch(pattern, line):\n                    new_lines.append(func(line))\n                    break\n            else:\n                new_lines.append(line)\n        return \"\\n\".join(new_lines)\n", "idx": 1157}
{"namespace": "wal_e.log_help.WalELogger.fmt_logline", "completion": "        if structured is not None:\n            structured = WalELogger._fmt_structured(structured)\n\n        log_line = ['MSG: {0}'.format(msg)]\n        if detail is not None:\n            log_line.append('DETAIL: {0}'.format(detail))\n        if hint is not None:\n            log_line.append('HINT: {0}'.format(hint))\n        if structured is not None:\n            log_line.append('STRUCTURED: {0}'.format(structured))\n\n        return '\\n'.join(log_line)\n", "idx": 1158}
{"namespace": "wal_e.blobstore.file.calling_format.Bucket.delete_keys", "completion": "        for key in keys:\n            os.remove(key)\n        remove_empty_dirs(common_dir_path(keys))", "idx": 1159}
{"namespace": "wal_e.worker.upload_pool.TarUploadPool.put", "completion": "        if self.closed:\n            raise tar_partition.TarPartitionError(\"TarUploadPool is closed\")\n\n        if self.member_burden + len(tpart) > self.max_members:\n            raise tar_partition.TarPartitionError(\n                \"TarUploadPool has too much work outstanding\")\n\n        if self.concurrency_burden >= self.max_concurrency:\n            self._wait()\n\n        self._start(tpart)\n", "idx": 1160}
{"namespace": "wal_e.worker.pg.wal_transfer.WalSegment.from_ready_archive_status", "completion": "        # Iterate through the files in the archive_status directory\n        for file_name in os.listdir(path.join(xlog_dir, 'archive_status')):\n            # Check if the file is a segment file\n            if re.match(storage.SEGMENT_REGEXP, file_name):\n                # Create a WalSegment instance for the segment file\n                yield WalSegment(path.join(xlog_dir, 'archive_status', file_name))", "idx": 1161}
{"namespace": "wal_e.worker.pg.wal_transfer.WalTransferGroup.join", "completion": "        self.close()\n        gevent.joinall(self.greenlets, timeout=30)\n        for g in self.greenlets:\n            if g.ready():\n                g.kill()\n        gevent.joinall(self.greenlets)\n        if self.wait_change.exception:\n            raise self.wait_change.exception\n", "idx": 1162}
{"namespace": "wal_e.worker.pg.wal_transfer.WalTransferGroup.start", "completion": "        g = gevent.Greenlet(self.transferer, segment)\n        self.greenlets.add(g)\n        g.start()", "idx": 1163}
{"namespace": "mrjob.py2.to_unicode", "completion": "    if isinstance(s, bytes):\n        try:\n            return s.decode('utf-8')\n        except UnicodeDecodeError:\n            return s.decode('latin-1')\n    else:\n        return s", "idx": 1164}
{"namespace": "mrjob.job.MRJob.steps", "completion": "        # Create a dictionary of redefined methods, excluding those that are not redefined.\n        methods = {\n            'mapper': self.mapper,\n            'reducer': self.reducer,\n            'combiner': self.combiner,\n            'mapper_init': self.mapper_init,\n            'mapper_final': self.mapper_final,\n            'reducer_init': self.reducer_init,\n            'reducer_final': self.reducer_final,\n            'combiner_init': self.combiner_init,\n            'combiner_final': self.combiner_final,\n            'mapper_cmd': self.mapper_cmd,\n            'reducer_cmd': self.reducer_cmd,\n            'combiner_cmd': self.combiner_cmd,\n            'mapper_pre_filter': self.mapper_pre_filter,\n            'reducer_pre_filter': self.reducer_pre_filter,\n            'combiner_pre_filter': self.combiner_pre_filter,\n            'mapper_raw': self.mapper_raw,\n            'spark': self.spark,\n            'spark_args': self.spark_args,\n        }\n        kwargs = {}\n        for key, value in methods.items():\n            if value is not None:\n                kwargs[key] = value\n\n        # Create a list of MRStep objects constructed with the updated kwargs.\n        steps = []\n        if 'spark' in kwargs:\n            steps.append(SparkStep(**kwargs))\n        else:\n            steps.append(MRStep(**kwargs))\n        return steps\n", "idx": 1165}
{"namespace": "mrjob.job.MRJob.increment_counter", "completion": "        group = group.replace(',', ';')\n        counter = counter.replace(',', ';')\n        print('reporter:counter:{group},{counter},{amount}'.format(group=group, counter=counter, amount=amount), file=sys.stderr)", "idx": 1166}
{"namespace": "mrjob.job.MRJob.set_status", "completion": "        line = 'reporter:status:%s\\n' % (msg)\n        if not isinstance(line, bytes):\n            line = line.encode('utf_8')\n\n        self.stderr.write(line)\n        self.stderr.flush()", "idx": 1167}
{"namespace": "mrjob.job.MRJob.run_job", "completion": "        # set up logging\n        self.setup_logging()\n\n        # create a runner\n        runner = self.make_runner()\n\n        # run the job\n        try:\n            runner.run()\n        except Exception as e:\n            log.error('Error running job: %s', e)\n            sys.exit(1)\n\n        # concatenate the output\n        if self.options.output_dir:\n            self.concatenate_output()\n", "idx": 1168}
{"namespace": "mrjob.job.MRJob.set_up_logging", "completion": "        from mrjob.log import log_to_stream\n        from mrjob.log import log_to_syslog\n        from mrjob.log import log_to_stderr\n        from mrjob.log import log_to_stdout\n        from mrjob.log import log_to_file\n        from mrjob.log import log_to_s3\n        from mrjob.log import log_to_s3_streaming\n        from mrjob.log import log_to_s3_streaming_with_rotation\n        from mrjob.log import log_to_s3_with_rotation\n        from mrjob.log import log_to_s3_with_rotation_and_compression\n        from mrjob.log import log_to_s3_with_rotation_and_compression_with_timestamp\n        from mrjob.log import log_to_s3_with_rotation_and_compression_with_timestamp_and_gzip\n        from mrjob.log import log_to_s3_with_rotation_and_compression_with_timestamp_and_gzip_with_timestamp\n        from mrjob.log import log_to_s3_with_rotation_and_compression_with_timestamp_and_gzip_with_timestamp_and_gzip\n        from mrjob.log import log_to_s3_with_rotation_and_compression_with_timestamp_and_gzip_with_timestamp_and_gzip_with_timestamp\n        from mrjob.log import log_to_s3_with_rotation_and_compression_with_timestamp_and_gzip_with_timestamp_and_gzip_with_timestamp_and_gzip\n        from mrjob.log import log_to_s3_with_rotation_and_compression_with_timestamp_and_gzip_with_timestamp_and_gzip_with_timestamp_and_gzip_with_timestamp\n        from mrjob.log import log_to_s3_with_rotation_and_compression_with", "idx": 1169}
{"namespace": "mrjob.job.MRJob.execute", "completion": "        # if we're running as a script, we'll need to load args from sys.argv\n        if self._cl_args == _READ_ARGS_FROM_SYS_ARGV:\n            self._cl_args = sys.argv[1:]\n\n        # if we're running as a script, we'll need to load args from sys.argv\n        if self._cl_args == _READ_ARGS_FROM_SYS_ARGV:\n            self._cl_args = sys.argv[1:]\n\n        # if we're running as a script, we'll need to load args from sys.argv\n        if self._cl_args == _READ_ARGS_FROM_SYS_ARGV:\n            self._cl_args = sys.argv[1:]\n\n        # if we're running as a script, we'll need to load args from sys.argv\n        if self._cl_args == _READ_ARGS_FROM_SYS_ARGV:\n            self._cl_args = sys.argv[1:]\n\n        # if we're running as a script, we'll need to load args from sys.argv\n        if self._cl_args == _READ_ARGS_FROM_SYS_ARGV:\n            self._cl_args = sys.argv[1:]\n\n        # if we're running as a script, we'll need to load args from sys.argv\n        if self._cl_args == _READ_ARGS_FROM_SYS_ARGV:\n            self._cl_args = sys.argv[1:]\n\n        # if we're running as a script, we'll need to load args from sys.argv\n        if self._cl_args == _READ_ARGS_FROM_SYS_ARGV:\n            self._cl_args = sys.argv[1:]\n\n        # if we're running as a script, we'll need to load args from sys.argv\n        if self._cl_args == _READ_ARGS_FROM_SYS_ARGV:\n            self._cl_args = sys.argv", "idx": 1170}
{"namespace": "mrjob.job.MRJob._runner_kwargs", "completion": "        # non-option kwargs\n        kwargs = self._non_option_kwargs()\n\n        # job kwargs\n        kwargs.update(self._job_kwargs())\n\n        # steps\n        kwargs['steps'] = self.steps()\n\n        # if we're running a Spark job, we need to pass the MRJob class\n        # to the Spark job\n        if self.options.runner == 'spark':\n            kwargs['mr_job_class'] = self.__class__\n\n        return kwargs", "idx": 1171}
{"namespace": "mrjob.job.MRJob.run_mapper", "completion": "        step = self._get_step(step_num, MRStep)\n        input_protocol = self.input_protocol()\n        output_protocol = self.output_protocol()\n        for line in to_lines(self.stdin):\n            key, value = input_protocol.read(line)\n            for out_key, out_value in step.mapper(key, value):\n                output_protocol.write(out_key, out_value, self.stdout)\n        for out_key, out_value in step.mapper_final():\n            output_protocol.write(out_key, out_value, self.stdout)\n", "idx": 1172}
{"namespace": "mrjob.job.MRJob.run_combiner", "completion": "        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'combiner')\n\n        for k, v in self.combine_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)\n", "idx": 1173}
{"namespace": "mrjob.job.MRJob.add_passthru_arg", "completion": "        pass_opt = self.arg_parser.add_argument(*args, **kwargs)\n\n        self._passthru_arg_dests.add(pass_opt.dest)\n", "idx": 1174}
{"namespace": "mrjob.job.MRJob.is_task", "completion": "        return self.options.run_mapper or self.options.run_combiner or self.options.run_reducer or self.options.run_spark", "idx": 1175}
{"namespace": "mrjob.job.MRJob.parse_output", "completion": "        for chunk in chunks:\n            yield self.output_protocol().read(chunk)", "idx": 1176}
{"namespace": "mrjob.job.MRJob.sandbox", "completion": "        if stdin is None:\n            stdin = BytesIO()\n        if stdout is None:\n            stdout = BytesIO()\n        if stderr is None:\n            stderr = BytesIO()\n\n        self.stdin = stdin\n        self.stdout = stdout\n        self.stderr = stderr\n\n        return self", "idx": 1177}
{"namespace": "mrjob.hadoop.fully_qualify_hdfs_path", "completion": "    if path.startswith('hdfs://'):\n        return path\n    elif path.startswith('/'):\n        return 'hdfs://' + path\n    else:\n        return 'hdfs:///user/' + getpass.getuser() + '/' + path\n\n", "idx": 1178}
{"namespace": "mrjob.hadoop.HadoopJobRunner.fs", "completion": "        if not hasattr(self, 'fs_obj'):\n            self.fs_obj = CompositeFilesystem()\n            self.fs_obj.add_filesystem(HadoopFilesystem(self))\n            self.fs_obj.add_filesystem(LocalFilesystem(self))\n        return self.fs_obj\n", "idx": 1179}
{"namespace": "mrjob.hadoop.HadoopJobRunner._find_hadoop_streaming_jar", "completion": "        # Search for the jar in the specified directories\n        for dir in self._opts['hadoop_streaming_jar_dirs']:\n            log.info('Looking for Hadoop streaming jar in %s...', dir)\n            for root, dirs, files in os.walk(dir):\n                for file in files:\n                    if _HADOOP_STREAMING_JAR_RE.match(file):\n                        return os.path.join(root, file)\n\n        # If the jar is not found in the specified directories, search for it in the default locations\n        for dir in _EMR_HADOOP_STREAMING_JAR_DIRS:\n            log.info('Looking for Hadoop streaming jar in %s...', dir)\n            for root, dirs, files in os.walk(dir):\n                for file in files:\n                    if _HADOOP_STREAMING_JAR_RE.match(file):\n                        return os.path.join(root, file)\n\n        # If the jar is not found in the default locations, search for it in the HADOOP_HOME directory\n        hadoop_home = self._opts['hadoop_home']\n        if hadoop_home and hadoop_home not in _BAD_HADOOP_HOMES:\n            log.info('Looking for Hadoop streaming jar in %s...', hadoop_home)\n            for root, dirs, files in os.walk(hadoop_home):\n                for file in files:\n                    if _HADOOP_STREAMING_JAR_RE.match(file):\n                        return os.path.join(root, file)\n\n        # If the jar is not found in the HADOOP_HOME directory, search for it in the default locations\n        for dir in _FALLBACK_HADOOP_STREAMING_JAR_DIRS:\n            log.info('Looking for Hadoop streaming jar in %s...', dir)\n            for root, dirs, files in os.walk(dir):\n                for file in files:\n                    if _HADOOP_STREAMING_", "idx": 1180}
{"namespace": "mrjob.hadoop.HadoopJobRunner._find_binaries_and_jars", "completion": "        # Load the Hadoop binary\n        hadoop_bin = self.get_hadoop_bin()\n        self.fs.hadoop.set_hadoop_bin(hadoop_bin)\n\n        # Check if there are Hadoop streaming steps or Spark steps in the job\n        if self._has_hadoop_streaming_steps() or self._has_spark_steps():\n            # Load the Hadoop streaming jar\n            self.get_hadoop_streaming_jar()\n\n            # Load the Spark submit binary\n            self._load_spark_submit_binary()\n", "idx": 1181}
{"namespace": "mrjob.hadoop.HadoopJobRunner._args_for_streaming_step", "completion": "        hadoop_bin = self.get_hadoop_bin()\n        hadoop_streaming_jar = self.get_hadoop_streaming_jar()\n        if not hadoop_streaming_jar:\n            raise Exception('no Hadoop streaming jar')\n        hadoop_args = [\n            hadoop_bin[0],\n            'jar',\n            hadoop_streaming_jar,\n        ]\n        hadoop_args.extend(self._hadoop_streaming_args(step_num))\n        return hadoop_args\n", "idx": 1182}
{"namespace": "mrjob.hadoop.HadoopJobRunner._stream_history_log_dirs", "completion": "        if self._opts['hadoop_log_dirs']:\n            for path in self._opts['hadoop_log_dirs']:\n                yield [path]\n            return\n\n        if output_dir:\n            # Cloudera style of logging\n            yield [posixpath.join(output_dir, '_logs')]\n\n        for hadoop_dir in self._hadoop_dirs():\n            yield [posixpath.join(hadoop_dir, 'logs')]\n\n        # hard-coded fallback paths\n        for path in _FALLBACK_HADOOP_LOG_DIRS:\n            yield [path]\n", "idx": 1183}
{"namespace": "mrjob.hadoop.HadoopJobRunner._stream_task_log_dirs", "completion": "        if not self._read_logs():\n            return\n\n        for log_dir in unique(self._hadoop_log_dirs(output_dir=output_dir)):\n            if _logs_exist(self.fs, log_dir):\n                if application_id:\n                    directory = posixpath.join(log_dir, 'userlogs', application_id)\n                else:\n                    directory = posixpath.join(log_dir, 'userlogs')\n                log.info('Looking for task logs in %s...' % directory)\n                yield [directory]\n", "idx": 1184}
{"namespace": "mrjob.setup.UploadDirManager.add", "completion": "        if is_uri(path):\n            return path\n\n        path = expand_path(path)\n        name = name_uniquely(path, self._names_taken, unhide=True)\n        self._path_to_name[path] = name\n        self._names_taken.add(name)\n        return self.uri(path)\n", "idx": 1185}
{"namespace": "mrjob.setup.UploadDirManager.uri", "completion": "        if is_uri(path):\n            return path\n\n        if path in self._path_to_name:\n            return posixpath.join(self.prefix, self._path_to_name[path])\n\n        raise ValueError('%r is not a URI or a known local file' % path)", "idx": 1186}
{"namespace": "mrjob.setup.UploadDirManager.path_to_uri", "completion": "        return self._path_to_name", "idx": 1187}
{"namespace": "mrjob.setup.WorkingDirManager.name_to_path", "completion": "        if type is not None:\n            if type not in self._SUPPORTED_TYPES:\n                raise ValueError('bad path type %r, must be one of %s' % (\n                    type, ', '.join(sorted(self._SUPPORTED_TYPES))))\n\n            return dict((name, path) for name, (t, path) in\n                        self._name_to_typed_path.items() if t == type)\n        else:\n            return self._name_to_typed_path\n", "idx": 1188}
{"namespace": "mrjob.setup.WorkingDirManager.paths", "completion": "        if type is not None:\n            self._check_type(type)\n\n        paths = set()\n        for path_type, path in self._typed_path_to_auto_name:\n            if type is None or path_type == type:\n                paths.add(path)\n\n        for path in self._name_to_typed_path.values():\n            if type is None or path[0] == type:\n                paths.add(path[1])\n\n        return paths", "idx": 1189}
{"namespace": "mrjob.compat.jobconf_from_env", "completion": "    if variable in os.environ:\n        return os.environ[variable]\n\n    if variable in _JOBCONF_MAP:\n        for version, name in _JOBCONF_MAP[variable].items():\n            if name in os.environ:\n                return os.environ[name]\n\n    return default", "idx": 1190}
{"namespace": "mrjob.compat.jobconf_from_dict", "completion": "    # try variable verbatim first\n    if name in jobconf:\n        return jobconf[name]\n\n    # try alternatives (arbitrary order)\n    for var in _JOBCONF_MAP.get(name, {}).values():\n        if var in jobconf:\n            return jobconf[var]\n\n    return default", "idx": 1191}
{"namespace": "mrjob.compat.translate_jobconf", "completion": "    if variable in _JOBCONF_MAP:\n        return map_version(version, _JOBCONF_MAP[variable])\n    else:\n        return variable", "idx": 1192}
{"namespace": "mrjob.compat.translate_jobconf_for_all_versions", "completion": "    if variable in _JOBCONF_MAP:\n        return sorted(_JOBCONF_MAP[variable].keys())\n    else:\n        return [variable]", "idx": 1193}
{"namespace": "mrjob.compat.translate_jobconf_dict", "completion": "    if hadoop_version is None:\n        raise TypeError\n\n    # get all known variants of the given jobconf variable\n    jobconf_variables = translate_jobconf_for_all_versions(variable)\n\n    # get all known variants of the given jobconf variable\n    jobconf_variables = translate_jobconf_for_all_versions(variable)\n\n    # get all known variants of the given jobconf variable\n    jobconf_variables = translate_jobconf_for_all_versions(variable)\n\n    # get all known variants of the given jobconf variable\n    jobconf_variables = translate_jobconf_for_all_versions(variable)\n\n    # get all known variants of the given jobconf variable\n    jobconf_variables = translate_jobconf_for_all_versions(variable)\n\n    # get all known variants of the given jobconf variable\n    jobconf_variables = translate_jobconf_for_all_versions(variable)\n\n    # get all known variants of the given jobconf variable\n    jobconf_variables = translate_jobconf_for_all_versions(variable)\n\n    # get all known variants of the given jobconf variable\n    jobconf_variables = translate_jobconf_for_all_versions(variable)\n\n    # get all known variants of the given jobconf variable\n    jobconf_variables = translate_jobconf_for_all_versions(variable)\n\n    # get all known variants of the given jobconf variable\n    jobconf_variables = translate_jobconf_for_all_versions(variable)\n\n    # get all known variants of the given jobconf variable\n    jobconf_variables = translate_jobconf_for_all_versions(variable)\n\n    # get all known variants of the given jobconf variable\n    jobconf_variables = translate_jobconf_for_all_versions(variable)\n\n    # get all known variants of the given jobconf variable\n    jobconf_variables = translate_jobconf_for_all_versions(variable)\n\n    # get all known variants of the given jobconf variable\n    jobconf_variables = translate_jobconf_for_", "idx": 1194}
{"namespace": "mrjob.compat.uses_yarn", "completion": "    if version is None:\n        raise TypeError\n\n    if isinstance(version, string_types):\n        version = LooseVersion(version)\n\n    return version >= LooseVersion('2.0.0')", "idx": 1195}
{"namespace": "mrjob.local.LocalMRJobRunner._spark_master", "completion": "        # Calculate the number of executors\n        num_cores = self._opts['num_cores']\n        num_executors = math.ceil(num_cores / self._opts['cores_per_executor'])\n\n        # Calculate the cores per executor\n        cores_per_executor = self._opts['cores_per_executor']\n\n        # Calculate the executor memory\n        executor_memory = self._opts['executor_memory']\n        if executor_memory is None:\n            executor_memory = _DEFAULT_EXECUTOR_MEMORY\n\n        # Return the Spark master URL\n        return f'local-cluster[{num_executors},{cores_per_executor},{executor_memory}]'\n", "idx": 1196}
{"namespace": "mrjob.runner.MRJobRunner._bootstrap_mrjob", "completion": "        return self._opts.get('bootstrap_mrjob', True)", "idx": 1197}
{"namespace": "mrjob.conf._fix_clear_tags", "completion": "    if isinstance(x, list):\n        return [_fix_clear_tags(y) for y in x]\n    elif isinstance(x, dict):\n        new_dict = {}\n        for k, v in x.items():\n            if isinstance(v, ClearedValue):\n                new_dict[k] = _fix_clear_tags(v.value)\n            else:\n                new_dict[k] = _fix_clear_tags(v)\n        return new_dict\n    elif isinstance(x, ClearedValue):\n        return _fix_clear_tags(x.value)\n    else:\n        return x", "idx": 1198}
{"namespace": "mrjob.conf.load_opts_from_mrjob_conf", "completion": "    if already_loaded is None:\n        already_loaded = []\n\n    conf_path = _expanded_mrjob_conf_path(conf_path)\n\n    if conf_path is None:\n        return [(None, {})]\n\n    if conf_path in already_loaded:\n        return [(conf_path, {})]\n\n    already_loaded.append(conf_path)\n\n    conf = _conf_object_at_path(conf_path)\n\n    if conf is None:\n        return [(conf_path, {})]\n\n    if runner_alias not in conf:\n        return [(conf_path, {})]\n\n    runner_conf = conf[runner_alias]\n\n    if 'includes' not in runner_conf:\n        return [(conf_path, runner_conf)]\n\n    includes = runner_conf['includes']\n\n    if not isinstance(includes, list):\n        includes = [includes]\n\n    includes = [expand_path(include) for include in includes]\n\n    includes = [(include, _conf_object_at_path(include)) for include in includes]\n\n    includes = [(include, {}) for include, conf in includes if conf is None] + \\\n               [(include, conf) for include, conf in includes if conf is not None]\n\n    includes = [(include, conf) for include, conf in includes if conf]\n\n    includes = [(include, conf) for include, conf in includes if runner_alias in conf]\n\n    includes = [(include, conf[runner_alias]) for include, conf in includes]\n\n    includes = [(include, conf) for include, conf in includes if 'includes' not in conf]\n\n    includes = [(include, conf) for include, conf in includes if conf]\n\n    includes = [(include, conf) for include, conf in includes if runner_alias in conf]\n\n    includes = [(include, conf[runner_alias]) for include, conf in includes]\n\n    includes = [(include, conf) for include, conf in includes if conf]\n\n    includes = [(include, conf) for", "idx": 1199}
{"namespace": "mrjob.conf.load_opts_from_mrjob_confs", "completion": "    if conf_paths is None:\n        conf_paths = [find_mrjob_conf()]\n\n    conf_paths = [_expanded_mrjob_conf_path(path) for path in conf_paths]\n\n    # don't load same conf file twice\n    already_loaded = []\n\n    # get configs for our runner out of conf file\n    confs = []\n    for conf_path in conf_paths:\n        confs += _load_opts_from_mrjob_conf(runner_alias, conf_path, already_loaded)\n\n    if not confs:\n        log.warning('No config specified for %s runner' % runner_alias)\n\n    return confs", "idx": 1200}
{"namespace": "mrjob.conf.dump_mrjob_conf", "completion": "    if yaml:\n        _dump_yaml_with_clear_tags(conf, f)\n    else:\n        json.dump(conf, f, indent=2, sort_keys=True)", "idx": 1201}
{"namespace": "mrjob.conf.combine_lists", "completion": "    result = []\n    for seq in seqs:\n        if seq is not None:\n            if isinstance(seq, (str, bytes)):\n                result.append(seq)\n            else:\n                result.extend(seq)\n    return result", "idx": 1202}
{"namespace": "mrjob.conf.combine_cmds", "completion": "    for cmd in reversed(cmds):\n        if cmd is not None:\n            if isinstance(cmd, string_types):\n                return shlex_split(cmd)\n            else:\n                return cmd\n    else:\n        return None", "idx": 1203}
{"namespace": "mrjob.conf.combine_dicts", "completion": "    result = {}\n\n    for d in dicts:\n        if d is None:\n            continue\n\n        for k, v in d.items():\n            if isinstance(v, ClearedValue):\n                if v.value is None:\n                    del result[k]\n                else:\n                    result[k] = v.value\n            else:\n                result[k] = v\n\n    return result", "idx": 1204}
{"namespace": "mrjob.conf.combine_jobconfs", "completion": "    result = {}\n    for jobconf in jobconfs:\n        if jobconf:\n            for k, v in jobconf.items():\n                if v is not None:\n                    if isinstance(v, string_types):\n                        result[k] = v\n                    else:\n                        result[k] = str(v)\n    return result", "idx": 1205}
{"namespace": "mrjob.conf.combine_path_lists", "completion": "    result = []\n\n    for path_seq in path_seqs:\n        if path_seq is None:\n            continue\n\n        if isinstance(path_seq, string_types):\n            path_seq = [path_seq]\n\n        for path in path_seq:\n            path = expand_path(path)\n            result.extend(glob.glob(path))\n\n    return result", "idx": 1206}
{"namespace": "mrjob.conf.combine_opts", "completion": "    # collect all keys from the dictionaries that are not wrapped in ClearedValue\n    keys = set()\n    for opts in opts_list:\n        for k in opts.keys():\n            if not isinstance(opts[k], ClearedValue):\n                keys.add(k)\n\n    # iterate through each key and use the sub-combiner specified in the combiners map for that key, or defaults to a function\n    combined_opts = {}\n    for k in keys:\n        # get the sub-combiner for the key\n        combiner = combiners.get(k, combine_values)\n\n        # get the values for the key from each dictionary\n        values = []\n        for opts in opts_list:\n            if k in opts:\n                values.append(opts[k])\n\n        # combine the values using the sub-combiner\n        combined_opts[k] = combiner(*values)\n\n    return combined_opts", "idx": 1207}
{"namespace": "mrjob.bin.MRJobBinRunner._task_python_bin", "completion": "        # task_python_bin isn't an option for inline runners\n        return self._opts['task_python_bin'] or self._python_bin()", "idx": 1208}
{"namespace": "mrjob.bin.MRJobBinRunner.get_spark_submit_bin", "completion": "        if self._spark_submit_bin:\n            return self._spark_submit_bin\n        else:\n            spark_submit_bin = which('spark-submit')\n            if spark_submit_bin:\n                self._spark_submit_bin = spark_submit_bin\n                return spark_submit_bin\n            else:\n                raise Exception(\"Could not find spark-submit binary\")\n", "idx": 1209}
{"namespace": "mrjob.step.StepFailedException.__str__", "completion": "        if self.reason:\n            return '{step_desc} failed: {reason}'.format(\n                step_desc=self.step_desc or self._get_step_desc(),\n                reason=self.reason)\n        else:\n            return '{step_desc} failed'.format(\n                step_desc=self.step_desc or self._get_step_desc())\n", "idx": 1210}
{"namespace": "mrjob.step.StepFailedException.__repr__", "completion": "        return '{}({})'.format(\n            self.__class__.__name__,\n            ', '.join('{}={}'.format(field, getattr(self, field))\n                      for field in self._FIELDS)\n        )", "idx": 1211}
{"namespace": "mrjob.step.MRStep.description", "completion": "        desc = {'type': 'streaming'}\n        if (step_num == 0 or self.has_explicit_mapper or\n                self.has_explicit_combiner):\n            desc['mapper'] = self.render_mapper()\n        if self.has_explicit_combiner:\n            desc['combiner'] = self.render_combiner()\n        if self.has_explicit_reducer:\n            desc['reducer'] = self.render_reducer()\n        if self._steps['mapper_raw']:\n            desc['input_manifest'] = True\n        if 'jobconf' in self._steps:\n            desc['jobconf'] = self._steps['jobconf']\n        return desc", "idx": 1212}
{"namespace": "mrjob.step._Step.description", "completion": "        desc = {'type': self._STEP_TYPE}\n        for k in self._STEP_ATTRS:\n            if k not in self._HIDDEN_ATTRS:\n                desc[k] = getattr(self, k)\n        return desc", "idx": 1213}
{"namespace": "mrjob.protocol._KeyCachingProtocol.read", "completion": "        key, value = line.split('\\t', 1)\n        self._last_key_encoded = key\n        self._last_key_decoded = self._loads(key)\n        return self._last_key_decoded, self._loads(value)\n", "idx": 1214}
{"namespace": "mrjob.util.safeeval", "completion": "    if globals is None:\n        globals = {}\n    if locals is None:\n        locals = {}\n\n    # Create a safe environment\n    safe_globals = {\n        '__builtins__': __builtins__,\n        '__name__': '__main__',\n        '__doc__': None,\n        '__package__': None,\n        'True': True,\n        'False': False,\n        'None': None,\n        'set': set,\n        'range': range,\n        'xrange': xrange,\n    }\n\n    # Add the safe globals to the safe environment\n    safe_globals.update(globals)\n\n    # Add the safe locals to the safe environment\n    safe_globals.update(locals)\n\n    # Raise an exception if open is used\n    if 'open' in safe_globals:\n        raise NameError(\"name 'open' is not defined\")\n\n    # Evaluate the expression in the safe environment\n    return eval(expr, safe_globals, locals)", "idx": 1215}
{"namespace": "mrjob.util.to_lines", "completion": "    if hasattr(chunks, 'readline'):\n        return chunks\n\n    for chunk in chunks:\n        for line in chunk.split(b'\\n'):\n            if line:\n                yield line\n\n", "idx": 1216}
{"namespace": "mrjob.parse.is_s3_uri", "completion": "    try:\n        parsed_uri = urlparse(uri)\n        if parsed_uri.scheme == 's3':\n            return True\n        else:\n            return False\n    except ValueError:\n        return False", "idx": 1217}
{"namespace": "mrjob.parse.parse_s3_uri", "completion": "    # TODO: This is a temporary workaround for the fact that urlparse is broken in Python 2.6.\n    #       Once we drop support for Python 2.6, we can remove this function.\n    #       See https://github.com/Yelp/mrjob/issues/1000\n    #       See https://bugs.python.org/issue16308\n    #       See https://bugs.python.org/issue16309\n    #       See https://bugs.python.org/issue16310\n    #       See https://bugs.python.org/issue16311\n    #       See https://bugs.python.org/issue16312\n    #       See https://bugs.python.org/issue16313\n    #       See https://bugs.python.org/issue16314\n    #       See https://bugs.python.org/issue16315\n    #       See https://bugs.python.org/issue16316\n    #       See https://bugs.python.org/issue16317\n    #       See https://bugs.python.org/issue16318\n    #       See https://bugs.python.org/issue16319\n    #       See https://bugs.python.org/issue16320\n    #       See https://bugs.python.org/issue16321\n    #       See https://bugs.python.org/issue16322\n    #       See https://bugs.python.org/issue16323\n    #       See https://bugs.python.org/issue16324\n    #       See https://bugs.python.org/issue16325\n    #       See https://bugs.python.org/issue16326\n    #       See https://bugs.python.org/issue16327\n    #       See https://bugs.python.org/issue16328\n    #       See https://bugs.", "idx": 1218}
{"namespace": "mrjob.parse.to_uri", "completion": "    if is_uri(path_or_uri):\n        return path_or_uri\n    else:\n        return 'file:///' + pathname2url(abspath(path_or_uri))", "idx": 1219}
{"namespace": "mrjob.parse.parse_mr_job_stderr", "completion": "    if isinstance(stderr, BytesIO):\n        stderr = stderr.getvalue()\n    if isinstance(stderr, list):\n        stderr = b'\\n'.join(stderr)\n    if isinstance(stderr, str):\n        stderr = stderr.encode('utf-8')\n\n    if counters is None:\n        counters = {}\n\n    lines = stderr.split(b'\\n')\n    statuses = []\n    other = []\n    for line in lines:\n        match = _COUNTER_RE.match(line)\n        if match:\n            group, name, count = match.groups()\n            if group not in counters:\n                counters[group] = {}\n            counters[group][name] = int(count)\n        else:\n            match = _STATUS_RE.match(line)\n            if match:\n                statuses.append(match.group(1))\n            else:\n                other.append(line)\n\n    return {'counters': counters, 'statuses': statuses, 'other': other}", "idx": 1220}
{"namespace": "mrjob.parse._parse_progress_from_job_tracker", "completion": "    from mrjob.py2 import to_unicode\n    html_str = to_unicode(html_bytes)\n    html_str = html_str.split('Running Jobs')[1].split('Jobs')[0]\n    map_percent = None\n    reduce_percent = None\n    for line in html_str.split('\\n'):\n        if 'Map%' in line:\n            map_percent = float(line.split(':')[1].split('%')[0])\n        if 'Reduce%' in line:\n            reduce_percent = float(line.split(':')[1].split('%')[0])\n    return map_percent, reduce_percent", "idx": 1221}
{"namespace": "mrjob.parse._parse_progress_from_resource_manager", "completion": "    # snip out the Running Jobs section (ignore the header)\n    start = html_bytes.rfind(b'Running Jobs')\n    if start == -1:\n        return None\n    end = html_bytes.find(b'Jobs', start + len(b'Running Jobs'))\n    if end == -1:\n        end = None\n\n    html_bytes = html_bytes[start:end]\n\n    # search it for percents\n    matches = _RESOURCE_MANAGER_JS_RE.findall(html_bytes)\n    if len(matches) >= 1:\n        return float(matches[0])\n    else:\n        return None", "idx": 1222}
{"namespace": "mrjob.logs.task._match_task_log_path", "completion": "    if application_id is not None and job_id is not None:\n        raise ValueError('application_id and job_id cannot be both set')\n\n    match = _PRE_YARN_TASK_LOG_PATH_RE.match(path)\n    if match:\n        if job_id is not None and match.group('attempt_id') != job_id:\n            return None\n        return {\n            'application_id': None,\n            'container_id': None,\n            'attempt_id': match.group('attempt_id'),\n            'log_type': match.group('log_type'),\n            'path': path,\n        }\n\n    match = _YARN_TASK_LOG_PATH_RE.match(path)\n    if match:\n        if application_id is not None and match.group('application_id') != application_id:\n            return None\n        return {\n            'application_id': match.group('application_id'),\n            'container_id': match.group('container_id'),\n            'attempt_id': None,\n            'log_type': match.group('log_type'),\n            'path': path,\n        }\n\n    return None", "idx": 1223}
{"namespace": "mrjob.logs.task._parse_task_syslog", "completion": "    result = {}\n    hadoop_error = None\n    split = None\n    check_stdout = None\n\n    for line in lines:\n        if _SPARK_APP_EXITED_RE.search(line):\n            check_stdout = True\n            break\n\n        if _OPENING_FOR_READING_RE.search(line):\n            check_stdout = True\n            break\n\n        if _YARN_INPUT_SPLIT_RE.search(line):\n            split = _parse_input_split(line)\n            break\n\n        if _JAVA_TRACEBACK_RE.search(line):\n            hadoop_error = _parse_java_traceback(line)\n            break\n\n    if hadoop_error:\n        result['hadoop_error'] = hadoop_error\n\n    if split:\n        result['split'] = split\n\n    if check_stdout:\n        result['check_stdout'] = check_stdout\n\n    return result\n\n", "idx": 1224}
{"namespace": "mrjob.logs.ids._sort_for_spark", "completion": "    # Sort the list of dictionaries by the 'start_time' key in descending order\n    ds = sorted(ds, key=lambda x: x['start_time'], reverse=True)\n\n    # Sort the list of dictionaries by the 'end_time' key in descending order\n    ds = sorted(ds, key=lambda x: x['end_time'], reverse=True)\n\n    # Sort the list of dictionaries by the 'id' key in descending order\n    ds = sorted(ds, key=lambda x: x['id'], reverse=True)\n\n    return ds", "idx": 1225}
{"namespace": "mrjob.logs.spark._parse_spark_log", "completion": "    # TODO: this is a hack to get around the fact that we don't have a\n    #       way to get the application ID from the log.\n    #       We should probably just get it from the log itself.\n    #       See https://github.com/Yelp/spark-log-parser/issues/1\n    #       for more details.\n    #       This is a temporary solution until we have a way to get the\n    #       application ID from the log.\n    #       This is a temporary solution until we have a way to get the\n    #       application ID from the log.\n    #       This is a temporary solution until we have a way to get the\n    #       application ID from the log.\n    #       This is a temporary solution until we have a way to get the\n    #       application ID from the log.\n    #       This is a temporary solution until we have a way to get the\n    #       application ID from the log.\n    #       This is a temporary solution until we have a way to get the\n    #       application ID from the log.\n    #       This is a temporary solution until we have a way to get the\n    #       application ID from the log.\n    #       This is a temporary solution until we have a way to get the\n    #       application ID from the log.\n    #       This is a temporary solution until we have a way to get the\n    #       application ID from the log.\n    #       This is a temporary solution until we have a way to get the\n    #       application ID from the log.\n    #       This is a temporary solution until we have a way to get the\n    #       application ID from the log.\n    #       This is a temporary solution until we have a way to get the\n    #       application ID from the log.\n    #       This is a temporary solution until we have a way to get the\n    #       application ID from the log.\n    #       This is a temporary solution until we have a way to get the\n    #       application ID from the log.\n    #       This is a temporary solution until we have a way to get the\n    #       application ID from the log.\n    #       This is a temporary solution until we", "idx": 1226}
{"namespace": "mrjob.logs.mixin.LogInterpretationMixin._pick_error", "completion": "        if self._step_type_uses_spark(step_type):\n            return None\n\n        if self._read_logs():\n            log.info('Scanning logs for probable cause of failure...')\n            self._interpret_step_logs(log_interpretation, step_type)\n            self._interpret_history_log(log_interpretation)\n\n        return _format_counters(log_interpretation)", "idx": 1227}
{"namespace": "mrjob.logs.history._match_history_log_path", "completion": "    m = _HISTORY_LOG_PATH_RE.match(path)\n    if m is None:\n        return None\n\n    if job_id is not None and m.group('job_id') != job_id:\n        return None\n\n    return {\n        'job_id': m.group('job_id'),\n        'path': path,\n        'yarn': 'hadoop' in m.group('suffix'),\n    }", "idx": 1228}
{"namespace": "mrjob.logs.history._parse_pre_yarn_history_log", "completion": "    result = {}\n    task_to_counters = {}\n\n    for line_num, line in enumerate(lines):\n        m = _PRE_YARN_HISTORY_RECORD.match(line)\n        if not m:\n            continue\n\n        record_type = m.group('type')\n        key_pairs = m.group('key_pairs')\n\n        if record_type == 'Task':\n            counters_str = None\n            task_id = None\n            attempt_id = None\n\n            for m in _PRE_YARN_HISTORY_KEY_PAIR.finditer(key_pairs):\n                key = m.group('key')\n                value = m.group('escaped_value')\n\n                if key == 'COUNTERS':\n                    counters_str = value\n                elif key == 'TASKID':\n                    task_id = value\n                elif key == 'ATTEMPTID':\n                    attempt_id = value\n\n            if counters_str and task_id:\n                task_to_counters[task_id] = _parse_pre_yarn_counters(\n                    counters_str)\n\n        elif record_type == 'FAILED':\n            for m in _PRE_YARN_HISTORY_KEY_PAIR.finditer(key_pairs):\n                key = m.group('key')\n                value = m.group('escaped_value')\n\n                if key == 'ERROR':\n                    error = dict(\n                        hadoop_error=dict(\n                            message=value,\n                            start_line=line_num,\n                            num_lines=1))\n\n                    if task_id:\n                        error['task_id'] = task_id\n\n                    if attempt_id:\n                        error['attempt_id'] = attempt_id\n\n                    result.setdefault('errors', [])\n                    result['errors'].append(error)\n\n    if 'counters' not in result and task_to_counters:\n        result['counters'] = _sum_counters(*task_to_counters.values())\n\n    return result", "idx": 1229}
{"namespace": "mrjob.logs.history._parse_pre_yarn_history_records", "completion": "    # TODO: this is a mess. It's not clear how to handle multi-line records\n    # (e.g. MapAttempts).\n    #\n    # The format is:\n    #\n    # type field_name=\"field_value\" field_name=\"field_value\" ... .\n    #\n    # The fields are separated by spaces.\n    #\n    # The record ends with a period that ends the line.\n    #\n    # The fields are double escaped, so we need to unescape them.\n    #\n    # The fields are not always present, so we need to handle that.\n    #\n    # The fields are not always strings, so we need to handle that.\n    #\n    # The fields are not always quoted, so we need to handle that.\n    #\n    # The fields are not always quoted, so we need to handle that.\n    #\n    # The fields are not always quoted, so we need to handle that.\n    #\n    # The fields are not always quoted, so we need to handle that.\n    #\n    # The fields are not always quoted, so we need to handle that.\n    #\n    # The fields are not always quoted, so we need to handle that.\n    #\n    # The fields are not always quoted, so we need to handle that.\n    #\n    # The fields are not always quoted, so we need to handle that.\n    #\n    # The fields are not always quoted, so we need to handle that.\n    #\n    # The fields are not always quoted, so we need to handle that.\n    #\n    # The fields are not always quoted, so we need to handle that.\n    #\n    # The fields are not always quoted, so we need to handle that.\n    #\n    # The fields are not always quoted, so we need to handle that.\n    #\n    # The fields are not always quoted, so we need to handle that.\n    #\n    # The fields are not always quoted, so we need to handle that.\n    #\n    # The fields are not always quoted, so we need to handle that.\n    #\n    # The fields are not always quoted, so we need to handle", "idx": 1230}
{"namespace": "mrjob.logs.step._parse_step_syslog", "completion": "    result = {}\n    errors = []\n    counters = {}\n    application_id = None\n    job_id = None\n    output_dir = None\n\n    for line in lines:\n        if _NOT_A_VALID_JAR_RE.match(line):\n            errors.append(dict(hadoop_error=dict(message=line)))\n            continue\n\n        m = _SUBMITTED_APPLICATION_RE.match(line)\n        if m:\n            application_id = m.group('application_id')\n            continue\n\n        m = _RUNNING_JOB_RE.match(line)\n        if m:\n            job_id = m.group('job_id')\n            continue\n\n        m = _OUTPUT_DIRECTORY_RE.match(line)\n        if m:\n            output_dir = m.group('output_dir')\n            continue\n\n        m = _JOB_PROGRESS_RE.match(line)\n        if m:\n            result['map_progress'] = int(m.group('map'))\n            result['reduce_progress'] = int(m.group('reduce'))\n            continue\n\n        m = _INDENTED_COUNTERS_MESSAGE_RE.match(line)\n        if m:\n            num_counters = int(m.group('num_counters'))\n            if num_counters == 0:\n                continue\n\n            # we're going to be parsing a bunch of lines, so\n            # we'll use a generator to avoid having to buffer\n            # the whole thing in memory\n            def parse_counters(lines):\n                for line in lines:\n                    m = _INDENTED_COUNTER_GROUP_RE.match(line)\n                    if m:\n                        group = m.group('group')\n                        indent = m.group('indent')\n                        yield group, indent\n                        continue\n\n                    m = _INDENTED_COUNTER_RE.match(line)\n                    if m:\n                        counter = m.group('counter')\n                        amount = int(m.group('amount'))\n                        yield", "idx": 1231}
{"namespace": "mrjob.logs.errors._merge_and_sort_errors", "completion": "    merged_errors = {}\n\n    for error in errors:\n        if 'container_id' in error:\n            container_id = error['container_id']\n            if container_id not in merged_errors:\n                merged_errors[container_id] = error\n            else:\n                merged_errors[container_id] = _merge_errors(\n                    merged_errors[container_id], error)\n        else:\n            merged_errors[error['time']] = error\n\n    if attempt_to_container_id:\n        for attempt_id, container_id in attempt_to_container_id.items():\n            if container_id not in merged_errors:\n                merged_errors[container_id] = {'attempt_id': attempt_id}\n            else:\n                merged_errors[container_id]['attempt_id'] = attempt_id\n\n    return sorted(merged_errors.values(), key=_sort_key)\n\n", "idx": 1232}
{"namespace": "mrjob.fs.ssh.SSHFilesystem.ls", "completion": "        m = _SSH_URI_RE.match(path_glob)\n        if m is None:\n            raise ValueError('Invalid path: %s' % path_glob)\n\n        hostname = m.group('hostname')\n        filesystem_path = m.group('filesystem_path')\n\n        if hostname is None:\n            hostname = 'localhost'\n\n        if hostname not in self._hosts_with_key_pair_file:\n            self._ssh_add_key()\n            self._hosts_with_key_pair_file.add(hostname)\n\n        args = self._ssh_cmd_args(hostname, ['find', filesystem_path])\n\n        log.debug('  > ' + cmd_line(args))\n\n        try:\n            p = Popen(args, stdout=PIPE, stderr=PIPE)\n        except OSError as ex:\n            raise IOError(ex.strerror)\n\n        for line in p.stdout:\n            yield line.strip()\n\n        self._ssh_finish_run(p)\n", "idx": 1233}
{"namespace": "mrjob.fs.ssh.SSHFilesystem._cat_file", "completion": "        m = _SSH_URI_RE.match(path)\n        addr = m.group('hostname')\n        path_to_cat = m.group('filesystem_path')\n\n        p = self._ssh_launch(addr, ['cat', path_to_cat])\n\n        for line in p.stdout:\n            yield to_unicode(line)\n\n        self._ssh_finish_run(p)", "idx": 1234}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.get_hadoop_bin", "completion": "        if self._hadoop_bin is None:\n            self._hadoop_bin = which('hadoop')\n        return self._hadoop_bin\n", "idx": 1235}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.du", "completion": "        # Check if the path exists\n        if not self.exists(path_glob):\n            return 0\n\n        # Execute the \"fs -du\" command\n        output = self.invoke_hadoop(['-du', path_glob], return_stdout=True)\n\n        # Parse the output\n        try:\n            size = int(output.split()[0])\n        except (ValueError, IndexError):\n            raise IOError(f'Unexpected output from Hadoop fs -du: {output}')\n\n        return size", "idx": 1236}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.mkdir", "completion": "        # use -p on Hadoop 2 (see #1152)\n        if uses_yarn(self.get_hadoop_version()):\n            args = ['fs', '-mkdir', '-p', path]\n        else:\n            args = ['fs', '-mkdir', path]\n\n        try:\n            self.invoke_hadoop(args, ok_returncodes=[0, 1],\n                               ok_stderr=[_HADOOP_FILE_EXISTS_RE])\n        except CalledProcessError:\n            raise IOError(\"Could not mkdir %s\" % path)\n", "idx": 1237}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.exists", "completion": "        try:\n            self.invoke_hadoop(['fs', '-ls', path_glob],\n                               ok_returncodes=[0, 1, 255],\n                               ok_stderr=[_HADOOP_LS_NO_SUCH_FILE])\n        except CalledProcessError:\n            return False\n\n        return True\n", "idx": 1238}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.rm", "completion": "        # check if path is a URI\n        if not is_uri(path_glob):\n            super(HadoopFilesystem, self).rm(path_glob)\n            return\n\n        # get the version of Hadoop\n        version = self.get_hadoop_version()\n\n        # use -r on Hadoop 2 (see #991, #845)\n        if uses_yarn(version):\n            args = ['fs', '-rm', '-r', path_glob]\n        else:\n            args = ['fs', '-rmr', path_glob]\n\n        try:\n            self.invoke_hadoop(args, ok_stderr=[_HADOOP_RM_NO_SUCH_FILE])\n        except CalledProcessError:\n            raise IOError(\"Could not rm %s\" % path_glob)", "idx": 1239}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.touchz", "completion": "        self.invoke_hadoop(['fs', '-touchz', path])", "idx": 1240}
{"namespace": "mrjob.fs.local.LocalFilesystem.du", "completion": "        path_glob = self.to_local_path(path_glob)\n        total_size = 0\n        for file in glob.glob(path_glob):\n            total_size += os.path.getsize(file)\n        return total_size\n", "idx": 1241}
{"namespace": "mrjob.fs.local.LocalFilesystem.ls", "completion": "        path_glob = _from_file_uri(path_glob)\n        if os.path.isdir(path_glob):\n            for root, dirs, files in os.walk(path_glob):\n                for file in files:\n                    yield _to_file_uri(os.path.join(root, file))\n        else:\n            yield _to_file_uri(path_glob)\n", "idx": 1242}
{"namespace": "mrjob.fs.local.LocalFilesystem._cat_file", "completion": "        path = _from_file_uri(path)\n        with open(path, 'rb') as f:\n            while True:\n                chunk = f.read(1024)\n                if not chunk:\n                    break\n                yield chunk\n", "idx": 1243}
{"namespace": "mrjob.fs.local.LocalFilesystem.exists", "completion": "        path_glob = _from_file_uri(path_glob)\n        return any(glob.glob(path_glob))\n", "idx": 1244}
{"namespace": "mrjob.fs.local.LocalFilesystem.mkdir", "completion": "        path = _from_file_uri(path)\n        if not os.path.exists(path):\n            os.makedirs(path)\n", "idx": 1245}
{"namespace": "mrjob.fs.local.LocalFilesystem.put", "completion": "        src = _from_file_uri(src)\n        path = _from_file_uri(path)\n        shutil.copy(src, path)\n", "idx": 1246}
{"namespace": "mrjob.fs.local.LocalFilesystem.rm", "completion": "        path_glob = _from_file_uri(path_glob)\n        for path in glob.glob(path_glob):\n            if os.path.isdir(path):\n                shutil.rmtree(path)\n            else:\n                os.remove(path)", "idx": 1247}
{"namespace": "mrjob.fs.local.LocalFilesystem.touchz", "completion": "        path = _from_file_uri(path)\n        if os.path.exists(path):\n            if os.path.getsize(path) > 0:\n                raise OSError(\"File already exists and is not empty\")\n        else:\n            open(path, 'w').close()", "idx": 1248}
{"namespace": "mrjob.fs.local.LocalFilesystem.md5sum", "completion": "        path = _from_file_uri(path)\n        with open(path, 'rb') as f:\n            return self._md5sum_file(f)", "idx": 1249}
{"namespace": "mrjob.fs.composite.CompositeFilesystem.add_fs", "completion": "        setattr(self, name, fs)\n        self._fs_names.append(name)\n        if disable_if:\n            self._disable_if[name] = disable_if\n", "idx": 1250}
{"namespace": "mrjob.fs.base.Filesystem.cat", "completion": "        raise NotImplementedError\n", "idx": 1251}
{"namespace": "mrjob.fs.base.Filesystem.join", "completion": "        if not paths:\n            return path\n\n        if not path:\n            return posixpath.join(*paths)\n\n        if path.startswith('s3://'):\n            return posixpath.join(path, *paths)\n\n        if path.startswith('gs://'):\n            return posixpath.join(path, *paths)\n\n        if path.startswith('hdfs://'):\n            return posixpath.join(path, *paths)\n\n        if path.startswith('ssh://'):\n            return posixpath.join(path, *paths)\n\n        if path.startswith('file://'):\n            return posixpath.join(path, *paths)\n\n        return posixpath.join(path, *paths)", "idx": 1252}
{"namespace": "mrjob.examples.mr_text_classifier.parse_doc_filename", "completion": "    # Extract the id from the filename\n    id = re.search(r'^([^-]+)-', input_uri).group(1)\n\n    # Extract the categories from the filename\n    cats = re.search(r'-([^.]+)\\.txt$', input_uri).group(1)\n    cats = cats.split('-')\n    cats = dict((cat, True) for cat in cats)\n\n    # Return the parsed information as a dictionary\n    return dict(id=id, cats=cats)\n\n", "idx": 1253}
{"namespace": "prometheus_client.mmap_dict.MmapedDict.read_value", "completion": "        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        return _unpack_two_doubles(self._m, pos + 4)[0]\n", "idx": 1254}
{"namespace": "prometheus_client.mmap_dict.MmapedDict.write_value", "completion": "        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        _pack_two_doubles(self._m, pos, value, timestamp)\n", "idx": 1255}
{"namespace": "prometheus_client.multiprocess.MultiProcessCollector.merge", "completion": "        merged = defaultdict(list)\n        for f in files:\n            with open(f, 'rb') as fp:\n                data = json.load(fp)\n            for k, v in data.items():\n                if k == '__name__':\n                    continue\n                if k == 'type':\n                    continue\n                if k == 'help':\n                    continue\n                if k == 'buckets':\n                    continue\n                if k == 'sum':\n                    continue\n                if k == 'count':\n                    continue\n                if k == 'created':\n                    continue\n                if k == 'labels':\n                    continue\n                if k == 'gauge':\n                    continue\n                if k == 'histogram':\n                    continue\n                if k == 'summary':\n                    continue\n                if k == 'untyped':\n                    continue\n                if k == 'info':\n                    continue\n                if k == 'state':\n                    continue\n                if k == 'timestamp':\n                    continue\n                if k == 'value':\n                    continue\n                if k == 'values':\n                    continue\n                if k == 'quantiles':\n                    continue\n                if k == 'gauge_value':\n                    continue\n                if k == 'gauge_values':\n                    continue\n                if k == 'gauge_quantiles':\n                    continue\n                if k == 'gauge_sum':\n                    continue\n                if k == 'gauge_count':\n                    continue\n                if k == 'gauge_created':\n                    continue\n                if k == 'gauge_labels':\n                    continue\n                if k == 'histogram_buckets':\n                    continue\n                if k == 'histogram_sum':\n                    continue\n                if k == 'histogram_count':\n                    continue\n                if k == 'histogram_created':\n                    continue\n                if k == 'histogram_labels':\n                    continue\n                if k == 'summary_quantiles':\n                    continue\n                if k == 'summary_sum':\n                    continue\n                if k == 'summary_count':\n                    continue\n                if k == 'summary_created':\n                    continue\n                if k == 'summary_labels':\n                    continue\n                if k == '", "idx": 1256}
{"namespace": "prometheus_client.multiprocess.MultiProcessCollector.collect", "completion": "        files = glob.glob(os.path.join(self._path, '*.db'))\n        return self.merge(files)", "idx": 1257}
{"namespace": "prometheus_client.exposition.choose_encoder", "completion": "    if accept_header and 'application/openmetrics-text' in accept_header:\n        return generate_latest, CONTENT_TYPE_LATEST\n    return generate_latest, CONTENT_TYPE_LATEST", "idx": 1258}
{"namespace": "flower.command.apply_options", "completion": "    # Filter the options that are specific to the application\n    argv = [arg for arg in argv if not arg.startswith('--app')]\n    # Parse the command line to get the \"--conf\" option\n    parse_command_line(argv)\n    # Parse the configuration file and the command line again to update the options\n    try:\n        parse_config_file(options.conf, final=False)\n    except IOError:\n        pass\n    # Filter the options that are specific to the application\n    argv = [arg for arg in argv if not arg.startswith('--app')]\n    # Parse the command line again to update the options\n    parse_command_line(argv)\n\n", "idx": 1259}
{"namespace": "trackerjacker.ieee_mac_vendor_db.MacVendorDB.lookup", "completion": "        mac = mac.upper().replace(':', '')\n        for prefix in self.db:\n            if mac.startswith(prefix):\n                return self.db[prefix]\n        return ''", "idx": 1260}
{"namespace": "trailscraper.iam.Statement.merge", "completion": "        if self.Effect != other.Effect:\n            raise ValueError(f\"Trying to combine two statements with differing effects: {self.Effect} {other.Effect}\")\n\n        merged_actions = sorted(set(self.Action + other.Action))\n        merged_resources = sorted(set(self.Resource + other.Resource))\n\n        return Statement(merged_actions, self.Effect, merged_resources)", "idx": 1261}
{"namespace": "trailscraper.iam.parse_policy_document", "completion": "    if isinstance(stream, six.string_types):\n        json_data = json.loads(stream)\n    else:\n        json_data = json.load(stream)\n\n    return PolicyDocument(Statement=_parse_statements(json_data['Statement']),\n                          Version=json_data['Version'])", "idx": 1262}
{"namespace": "trailscraper.iam.known_iam_actions", "completion": "    actions = all_known_iam_permissions()\n    actions_by_prefix = groupbyz(lambda action: action.split(\":\")[0], actions)\n    return actions_by_prefix[prefix]", "idx": 1263}
{"namespace": "trailscraper.boto_service_definitions.service_definition_file", "completion": "    files = boto_service_definition_files()\n    filtered_files = [file for file in files if fnmatch.fnmatch(file, \"**/\" + servicename + \"/*/service-*.json\")]\n    filtered_files.sort()\n    return filtered_files[-1]", "idx": 1264}
{"namespace": "trailscraper.boto_service_definitions.operation_definition", "completion": "    service_definition_file_path = service_definition_file(servicename)\n    with open(service_definition_file_path, 'r') as f:\n        service_definition = json.load(f)\n\n    operations = service_definition['operations']\n    for operation in operations:\n        if operation['name'] == operationname:\n            return operation", "idx": 1265}
{"namespace": "trailscraper.cloudtrail.Record.to_statement", "completion": "        if self.event_source == \"sts.amazonaws.com\" and self.event_name == \"GetCallerIdentity\":\n            return None\n\n        if self.event_source == \"iam.amazonaws.com\":\n            return self._to_iam_statement()\n\n        if self.event_source == \"apigateway.amazonaws.com\":\n            return self._to_api_gateway_statement()\n\n        return Statement(\n            Effect=\"Allow\",\n            Action=[Action(self._source_to_iam_prefix(), self._event_name_to_iam_action())],\n            Resource=self.resource_arns\n        )", "idx": 1266}
{"namespace": "trailscraper.cloudtrail.filter_records", "completion": "    return filterz(_by_timeframe(from_date, to_date),\n                   filterz(_by_role_arns(arns_to_filter_for),\n                           records))", "idx": 1267}
{"namespace": "trailscraper.record_sources.local_directory_record_source.LocalDirectoryRecordSource.load_from_dir", "completion": "        def _load_from_file(log_file):\n            return log_file.load_from_file(from_date, to_date)\n\n        return pipe(self._valid_log_files(),\n                    mapz(_load_from_file),\n                    mapcatz(lambda x: x),\n                    sortedz(key=lambda x: x.event_time))", "idx": 1268}
{"namespace": "pyt.__main__.discover_files", "completion": "    included_files = []\n    for target in targets:\n        if os.path.isdir(target):\n            for root, dirs, files in os.walk(target):\n                if not recursive and root != target:\n                    continue\n                for file in files:\n                    if file.endswith(\".py\") and file not in excluded_files:\n                        included_files.append(os.path.join(root, file))\n                        log.debug(\"Discovered file: %s\", file)\n        elif os.path.isfile(target):\n            included_files.append(target)\n            log.debug(\"Discovered file: %s\", target)\n    return included_files\n\n", "idx": 1269}
{"namespace": "pyt.core.project_handler.get_directory_modules", "completion": "    if _local_modules and _local_modules[0][1] == directory:\n        return _local_modules\n\n    if not os.path.isdir(directory):\n        directory = os.path.dirname(directory)\n\n    if not os.path.isdir(directory):\n        return []\n\n    for file in os.listdir(directory):\n        if file.endswith(\".py\"):\n            module_name = file[:-3]\n            _local_modules.append((module_name, os.path.join(directory, file)))\n\n    return _local_modules", "idx": 1270}
{"namespace": "pyt.vulnerabilities.vulnerabilities.find_triggers", "completion": "    trigger_nodes = []\n    for node in nodes:\n        if node.line_number not in nosec_lines:\n            for trigger_word in trigger_words:\n                if trigger_word.label in node.label:\n                    trigger_nodes.append(TriggerNode(trigger_word, cfg_node=node))\n    return trigger_nodes\n\n", "idx": 1271}
{"namespace": "pyt.vulnerabilities.vulnerabilities.label_contains", "completion": "    for trigger in triggers:\n        if trigger.label in node.label:\n            yield TriggerNode(trigger, cfg_node=node)", "idx": 1272}
{"namespace": "pyt.vulnerabilities.vulnerabilities.build_sanitiser_node_dict", "completion": "    sanitiser_list = [sink.trigger.sanitiser for sink in sinks_in_file]\n    sanitiser_node_dict = dict()\n    for sanitiser in sanitiser_list:\n        sanitiser_node_dict[sanitiser] = list()\n        for node in cfg.nodes:\n            if sanitiser in node.label:\n                sanitiser_node_dict[sanitiser].append(TriggerNode(Source(sanitiser), node))\n    return sanitiser_node_dict", "idx": 1273}
{"namespace": "pyt.vulnerabilities.trigger_definitions_parser.parse", "completion": "    with open(trigger_word_file, 'r') as f:\n        data = json.load(f)\n\n    sources = [Source(trigger_word=trigger_word) for trigger_word in data['sources']]\n    sinks = [Sink.from_json(key, value) for key, value in data['sinks'].items()]\n\n    return Definitions(sources=sources, sinks=sinks)", "idx": 1274}
{"namespace": "principalmapper.querying.local_policy_simulation._statement_matches_resource", "completion": "    if 'Resource' in statement:\n        for item in _listify_string(statement['Resource']):\n            if _matches_after_expansion(resource, item, condition_keys):\n                return True\n        return False\n    elif 'NotResource' in statement:\n        result = True\n        for item in _listify_string(statement['NotResource']):\n            if _matches_after_expansion(resource, item, condition_keys):\n                result = False\n                break\n        return result\n    else:\n        return True", "idx": 1275}
{"namespace": "principalmapper.querying.local_policy_simulation._matches_after_expansion", "completion": "    if condition_keys is not None:\n        for key, value in condition_keys.items():\n            string_to_check = string_to_check.replace('${' + key + '}', value)\n            string_to_check_against = string_to_check_against.replace('${' + key + '}', value)\n\n    if '*' in string_to_check_against:\n        pattern = _compose_pattern(string_to_check_against)\n        return pattern.match(string_to_check) is not None\n    elif '?' in string_to_check_against:\n        pattern = _compose_pattern(string_to_check_against)\n        return pattern.match(string_to_check) is not None\n    else:\n        return string_to_check == string_to_check_against", "idx": 1276}
{"namespace": "passpie.database.PasspieStorage.delete", "completion": "        for cred in credentials:\n            credpath = self.make_credpath(cred[\"name\"], cred[\"login\"])\n            if os.path.exists(credpath):\n                os.remove(credpath)\n                dirname = os.path.dirname(credpath)\n                if not os.listdir(dirname):\n                    os.rmdir(dirname)\n", "idx": 1277}
{"namespace": "passpie.database.PasspieStorage.read", "completion": "        data = []\n        for dirpath, dirnames, filenames in os.walk(self.path):\n            for filename in filenames:\n                if filename.endswith(self.extension):\n                    credpath = os.path.join(dirpath, filename)\n                    with open(credpath, \"r\") as f:\n                        data.append(yaml.load(f))\n        return {i: data[i] for i in range(len(data))}\n", "idx": 1278}
{"namespace": "threatingestor.state.State.save_state", "completion": "        self.cursor.execute('INSERT OR REPLACE INTO states (name, state) VALUES (?, ?)', (name, state))\n        self.conn.commit()\n\n", "idx": 1279}
{"namespace": "threatingestor.state.State.get_state", "completion": "        self.cursor.execute('SELECT state FROM states WHERE name=?', (name,))\n        result = self.cursor.fetchone()\n        if result:\n            return result[0]\n        else:\n            return None", "idx": 1280}
{"namespace": "threatingestor.Ingestor.run", "completion": "        if self.config.daemon():\n            logger.info(\"Running as daemon\")\n            while True:\n                self.run_once()\n                time.sleep(self.config.interval())\n        else:\n            logger.info(\"Running once\")\n            self.run_once()\n", "idx": 1281}
{"namespace": "msticpy.analysis.anomalous_sequence.model.Model.compute_scores", "completion": "        self.session_likelihoods = []\n        self.session_geomean_likelihoods = []\n        self.rare_windows = defaultdict(list)\n        self.rare_window_likelihoods = defaultdict(list)\n        self.rare_windows_geo = defaultdict(list)\n        self.rare_window_likelihoods_geo = defaultdict(list)\n\n        for i, session in enumerate(self.sessions):\n            if use_start_end_tokens:\n                session = [self.start_token] + session + [self.end_token]\n            self.session_likelihoods.append(probabilities(session, self.trans_probs))\n            self.session_geomean_likelihoods.append(\n                probabilities(session, self.trans_probs, geometric_mean=True)\n            )\n            for j in range(2, len(session) + 1):\n                window = session[j - 2 : j]\n                self.rare_windows[j].append(window)\n                self.rare_window_likelihoods[j].append(\n                    probabilities(window, self.trans_probs)\n                )\n                self.rare_windows_geo[j].append(window)\n                self.rare_window_likelihoods_geo[j].append(\n                    probabilities(window, self.trans_probs, geometric_mean=True)\n                )\n\n        self.session_likelihoods = np.array(self.session_likelihoods)\n        self.session_geomean_likelihoods = np.array(self.session_geomean_likelihoods)\n        for j in range(2, len(session) + 1):\n            self.rare_window_likelihoods[j] = np.array(self.rare_window_likelihoods[j])\n            self.rare_window_likelihoods_geo[j] = np.array(\n                self.rare_window_likelihoods_geo[j]\n            )\n", "idx": 1282}
{"namespace": "msticpy.analysis.anomalous_sequence.model.Model.compute_rarest_windows", "completion": "        if self.session_likelihoods is None:\n            self.compute_likelihoods_of_sessions(use_start_end_tokens=use_start_end_tokens)\n\n        result = []\n        for sess in self.sessions:\n            if use_geo_mean:\n                tmp = probabilities.compute_rarest_window_likelihood_geo(\n                    window_len=window_len,\n                    session=sess,\n                    session_likelihoods=self.session_geomean_likelihoods,\n                    use_start_token=use_start_end_tokens,\n                    use_end_token=use_start_end_tokens,\n                    start_token=self.start_token,\n                    end_token=self.end_token,\n                )\n            else:\n                tmp = probabilities.compute_rarest_window_likelihood(\n                    window_len=window_len,\n                    session=sess,\n                    session_likelihoods=self.session_likelihoods,\n                    use_start_token=use_start_end_tokens,\n                    use_end_token=use_start_end_tokens,\n                    start_token=self.start_token,\n                    end_token=self.end_token,\n                )\n            result.append(tmp)\n\n        if use_geo_mean:\n            self.rare_window_likelihoods_geo[window_len] = result\n        else:\n            self.rare_window_likelihoods[window_len] = result\n", "idx": 1283}
{"namespace": "msticpy.analysis.anomalous_sequence.anomalous.score_sessions", "completion": "    # Create a Markov model with the specified window length\n    model = MarkovModel(window_length)\n\n    # Train the model on the sessions data\n    model.fit(data[session_column])\n\n    # Compute the likelihood metrics for each session\n    likelihoods = model.predict(data[session_column])\n\n    # Compute the rarest window for each session\n    rarest_windows = model.rarest_window(data[session_column])\n\n    # Append the likelihood and rarest window columns to the input DataFrame\n    data[\"likelihood\"] = likelihoods\n    data[\"rarest_window\"] = rarest_windows\n\n    return data\n\n", "idx": 1284}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.laplace_smooth_counts", "completion": "    seq1_counts_sm = StateMatrix(\n        {\n            k: v + 1\n            for k, v in seq1_counts.items()\n            if k not in [start_token, end_token, unk_token]\n        }\n    )\n    seq2_counts_sm = StateMatrix(\n        {\n            k1: {\n                k2: v2 + 1\n                for k2, v2 in v1.items()\n                if k2 not in [start_token, end_token, unk_token]\n            }\n            for k1, v1 in seq2_counts.items()\n            if k1 not in [start_token, end_token, unk_token]\n        }\n    )\n    param_counts_sm = StateMatrix(\n        {\n            k: v + 1\n            for k, v in param_counts.items()\n            if k not in [start_token, end_token, unk_token]\n        }\n    )\n    cmd_param_counts_sm = StateMatrix(\n        {\n            k1: {\n                k2: v2 + 1\n                for k2, v2 in v1.items()\n                if k2 not in [start_token, end_token, unk_token]\n            }\n            for k1, v1 in cmd_param_counts.items()\n            if k1 not in [start_token, end_token, unk_token]\n        }\n    )\n\n    return seq1_counts_sm, seq2_counts_sm, param_counts_sm, cmd_param_counts_sm\n\n", "idx": 1285}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.compute_likelihood_window", "completion": "    if use_start_token:\n        window = [Cmd(name=start_token)] + window\n    if use_end_token:\n        window = window + [Cmd(name=end_token)]\n    if len(window) < 2:\n        raise MsticpyException(\n            \"Window must have at least 2 commands to compute likelihood\"\n        )\n    if isinstance(prior_probs, dict):\n        prior_probs = StateMatrix(states=prior_probs)\n    if isinstance(trans_probs, dict):\n        trans_probs = StateMatrix(states=trans_probs)\n    if isinstance(param_cond_cmd_probs, dict):\n        param_cond_cmd_probs = StateMatrix(states=param_cond_cmd_probs)\n\n    # Compute the likelihood of the window\n    likelihood: float = 1\n    for i in range(len(window) - 1):\n        cmd1 = window[i].name\n        cmd2 = window[i + 1].name\n        if cmd1 == start_token:\n            cmd1 = end_token\n        if cmd2 == end_token:\n            cmd2 = start_token\n        likelihood *= prior_probs[cmd1] * trans_probs[cmd1][cmd2]\n        if len(window[i].params) > 0:\n            likelihood *= compute_prob_setofparams_given_cmd(\n                cmd=cmd1,\n                params=window[i].params,\n                param_cond_cmd_probs=param_cond_cmd_probs,\n            )\n    return likelihood", "idx": 1286}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.compute_likelihood_windows_in_session", "completion": "    if use_start_end_tokens:\n        if end_token is None:\n            raise MsticpyException(\n                \"end_token should not be None, when use_end_token is True\"\n            )\n\n        if start_token is None:\n            raise MsticpyException(\n                \"start_token should not be None, when use_start_token is True\"\n            )\n\n    sess_len = len(session)\n    if sess_len == 0:\n        return []\n    if sess_len < window_len:\n        return []\n    likelihoods: List[float] = []\n    for i in range(sess_len - window_len + 1):\n        window = session[i : i + window_len]\n        likelihood = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            use_start_token=use_start_end_tokens,\n            use_end_token=use_start_end_tokens,\n            start_token=start_token,\n            end_token=end_token,\n        )\n        if use_geo_mean:\n            likelihood = likelihood ** (1 / window_len)\n        likelihoods.append(likelihood)\n    return likelihoods", "idx": 1287}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.rarest_window_session", "completion": "    if use_start_end_tokens:\n        if start_token is None or end_token is None:\n            raise MsticpyException(\n                \"start_token and end_token should not be set to None when use_start_end_tokens is set to True\"\n            )\n\n    sess = session.copy()\n    if use_start_end_tokens and end_token:\n        sess += [Cmd(name=str(end_token), params={})]\n    end = len(sess) - window_len\n\n    likelihoods = []\n    for i in range(end + 1):\n        window = sess[i : i + window_len]  # noqa E203\n        if i == 0:\n            use_start = use_start_end_tokens\n        else:\n            use_start = False\n\n        lik = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            use_start_token=use_start,\n            use_end_token=False,\n            start_token=start_token,\n            end_token=end_token,\n        )\n\n        if use_geo_mean:\n            k = window_len\n            lik = lik ** (1 / k)\n        likelihoods.append(lik)\n\n    min_lik = min(likelihoods)\n    min_idx = likelihoods.index(min_lik)\n    rarest_window = sess[min_idx : min_idx + window_len]  # noqa E203\n\n    return rarest_window, min_lik", "idx": 1288}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.compute_likelihood_window", "completion": "    if use_start_token:\n        window = [start_token] + window\n    if use_end_token:\n        window = window + [end_token]\n\n    if isinstance(prior_probs, dict):\n        prior_probs = StateMatrix(states=prior_probs)\n    if isinstance(trans_probs, dict):\n        trans_probs = StateMatrix(states=trans_probs)\n\n    likelihood = 1.0\n    for i in range(len(window) - 1):\n        if window[i] in prior_probs.states:\n            likelihood *= prior_probs.states[window[i]]\n        else:\n            likelihood *= prior_probs.states[prior_probs.unk_token]\n        if window[i] in trans_probs.states:\n            if window[i + 1] in trans_probs.states[window[i]]:\n                likelihood *= trans_probs.states[window[i]][window[i + 1]]\n            else:\n                likelihood *= trans_probs.states[window[i]][trans_probs.unk_token]\n        else:\n            likelihood *= trans_probs.states[trans_probs.unk_token][\n                trans_probs.unk_token\n            ]\n    return likelihood", "idx": 1289}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.compute_likelihood_windows_in_session", "completion": "    if use_start_end_tokens:\n        if start_token is None:\n            raise MsticpyException(\n                \"start_token should not be None, when use_start_end_tokens is True\"\n            )\n        if end_token is None:\n            raise MsticpyException(\n                \"end_token should not be None, when use_start_end_tokens is True\"\n            )\n\n    if window_len < 1:\n        raise MsticpyException(\"window_len should be greater than 0\")\n\n    if window_len == 1:\n        return [prior_probs[cmd] for cmd in session]\n\n    if window_len > len(session):\n        raise MsticpyException(\n            \"window_len should be less than or equal to the length of the session\"\n        )\n\n    if use_start_end_tokens:\n        session = [start_token] + session + [end_token]\n\n    w_len = len(session)\n    window_likelihoods = []\n    for i in range(w_len - window_len + 1):\n        window = session[i : i + window_len]\n        window_likelihood = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            use_start_token=False,\n            use_end_token=False,\n        )\n        if use_geo_mean:\n            window_likelihood = window_likelihood ** (1 / window_len)\n        window_likelihoods.append(window_likelihood)\n\n    return window_likelihoods", "idx": 1290}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.rarest_window_session", "completion": "    if use_start_end_tokens:\n        if start_token is None or end_token is None:\n            raise MsticpyException(\n                \"start_token and end_token should not be set to None when \"\n                \"use_start_end_tokens is set to True\"\n            )\n\n    sess = session.copy()\n    if use_start_end_tokens and end_token:\n        sess += [str(end_token)]\n    end = len(sess) - window_len\n    likelihoods = []\n    for i in range(end + 1):\n        window = sess[i : i + window_len]  # noqa: E203\n\n        if i == 0:\n            use_start = use_start_end_tokens\n        else:\n            use_start = False\n\n        lik = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            use_start_token=use_start,\n            use_end_token=False,\n            start_token=start_token,\n            end_token=end_token,\n        )\n        if use_geo_mean:\n            k = window_len\n            lik = lik ** (1 / k)\n\n        likelihoods.append(lik)\n\n    min_likelihood = min(likelihoods)\n    min_index = likelihoods.index(min_likelihood)\n    rarest_window = sess[min_index : min_index + window_len]  # noqa: E203\n\n    return rarest_window, min_likelihood", "idx": 1291}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.get_params_to_model_values", "completion": "    # Check if the input is a StateMatrix object\n    if isinstance(param_counts, StateMatrix):\n        param_counts = param_counts.states\n    if isinstance(param_value_counts, StateMatrix):\n        param_value_counts = param_value_counts.states\n\n    # Initialize an empty set to store the parameters to model as categorical\n    params_to_model_values = set()\n\n    # Loop through each parameter\n    for param, value_counts in param_value_counts.items():\n        # Calculate the total count of the parameter\n        param_total_count = sum(value_counts.values())\n\n        # Calculate the total count of the values for the parameter\n        values_total_count = sum(value_counts.values())\n\n        # Calculate the probability of each value for the parameter\n        value_probabilities = {\n            value: count / values_total_count for value, count in value_counts.items()\n        }\n\n        # Calculate the entropy of the parameter\n        param_entropy = -sum(\n            prob * np.log2(prob) for prob in value_probabilities.values()\n        )\n\n        # Calculate the entropy of the values for the parameter\n        values_entropy = -sum(\n            prob * np.log2(prob) for prob in value_probabilities.values()\n        )\n\n        # Calculate the information gain of the parameter\n        param_info_gain = param_entropy - values_entropy\n\n        # Calculate the information gain ratio of the parameter\n        param_info_gain_ratio = param_info_gain / np.log2(len(value_probabilities))\n\n        # Check if the parameter meets the criteria for being modeled as categorical\n        if (\n            param_info_gain_ratio > 0.5\n            and param_total_count > 10\n            and values_total_count > 10\n        ):\n            params_to_model_values.add(param)\n\n    return params_to_model_values\n\n", "idx": 1292}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_prob_setofparams_given_cmd", "completion": "    if not isinstance(params_with_vals, dict):\n        params_with_vals = dict.fromkeys(params_with_vals)\n\n    if not isinstance(modellable_params, set):\n        modellable_params = set(modellable_params)\n\n    if not isinstance(param_cond_cmd_probs, dict):\n        param_cond_cmd_probs = param_cond_cmd_probs.states\n\n    if not isinstance(value_cond_param_probs, dict):\n        value_cond_param_probs = value_cond_param_probs.states\n\n    if not isinstance(modellable_params, set):\n        modellable_params = set(modellable_params)\n\n    if not isinstance(params_with_vals, dict):\n        params_with_vals = dict.fromkeys(params_with_vals)\n\n    if not isinstance(param_cond_cmd_probs, dict):\n        param_cond_cmd_probs = param_cond_cmd_probs.states\n\n    if not isinstance(value_cond_param_probs, dict):\n        value_cond_param_probs = value_cond_param_probs.states\n\n    if not isinstance(modellable_params, set):\n        modellable_params = set(modellable_params)\n\n    if not isinstance(params_with_vals, dict):\n        params_with_vals = dict.fromkeys(params_with_vals)\n\n    if not isinstance(param_cond_cmd_probs, dict):\n        param_cond_cmd_probs = param_cond_cmd_probs.states\n\n    if not isinstance(value_cond_param_probs, dict):\n        value_cond_param_probs = value_cond_param_probs.states\n\n    if not isinstance(modellable_params, set):\n        modellable_params = set(modellable_params)\n\n    if not isinstance(params_with_vals, dict):\n        params_with_vals = dict.fromkeys", "idx": 1293}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_likelihood_window", "completion": "    if use_start_token:\n        window = [Cmd(name=start_token)] + window\n    if use_end_token:\n        window = window + [Cmd(name=end_token)]\n    seq1_probs = prior_probs\n    seq2_probs = trans_probs\n    seq1_probs_prev = prior_probs\n    seq2_probs_prev = trans_probs\n    seq1_probs_prev_prev = prior_probs\n    seq2_probs_prev_prev = trans_probs\n    seq1_probs_prev_prev_prev = prior_probs\n    seq2_probs_prev_prev_prev = trans_probs\n    seq1_probs_prev_prev_prev_prev = prior_probs\n    seq2_probs_prev_prev_prev_prev = trans_probs\n    seq1_probs_prev_prev_prev_prev_prev = prior_probs\n    seq2_probs_prev_prev_prev_prev_prev = trans_probs\n    seq1_probs_prev_prev_prev_prev_prev_prev = prior_probs\n    seq2_probs_prev_prev_prev_prev_prev_prev = trans_probs\n    seq1_probs_prev_prev_prev_prev_prev_prev_prev = prior_probs\n    seq2_probs_prev_prev_prev_prev_prev_prev_prev = trans_probs\n    seq1_probs_prev_prev_prev_prev_prev_prev_prev_prev = prior_probs\n    seq2_probs_prev_prev_prev_prev_prev_prev_prev_prev = trans_probs\n    seq1_probs_prev_prev_prev_prev_prev_prev_prev_prev_prev = prior_probs\n    seq2_probs_prev_prev_prev_prev_prev_prev_prev_prev_prev = trans_probs\n    seq1_probs_prev_prev", "idx": 1294}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_likelihood_windows_in_session", "completion": "    if use_start_end_tokens:\n        if start_token is None:\n            raise MsticpyException(\n                \"start_token should not be None, when use_start_end_tokens is True\"\n            )\n        if end_token is None:\n            raise MsticpyException(\n                \"end_token should not be None, when use_start_end_tokens is True\"\n            )\n\n    sess_len = len(session)\n    if sess_len == 0:\n        return []\n    if sess_len < window_len:\n        return [np.nan]\n\n    if use_start_end_tokens:\n        session = [Cmd(name=start_token)] + session + [Cmd(name=end_token)]\n\n    window_likelihoods = []\n    for i in range(sess_len - window_len + 1):\n        window = session[i : i + window_len]\n        window_likelihood = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            value_cond_param_probs=value_cond_param_probs,\n            modellable_params=modellable_params,\n            use_start_token=False,\n            use_end_token=False,\n        )\n        if use_geo_mean:\n            window_likelihood = window_likelihood ** (1 / window_len)\n        window_likelihoods.append(window_likelihood)\n\n    return window_likelihoods", "idx": 1295}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.rarest_window_session", "completion": "    likelihoods = compute_likelihood_windows_in_session(\n        session=session,\n        prior_probs=prior_probs,\n        trans_probs=trans_probs,\n        param_cond_cmd_probs=param_cond_cmd_probs,\n        value_cond_param_probs=value_cond_param_probs,\n        modellable_params=modellable_params,\n        window_len=window_len,\n        use_start_end_tokens=use_start_end_tokens,\n        start_token=start_token,\n        end_token=end_token,\n        use_geo_mean=use_geo_mean,\n    )\n    min_lik = min(likelihoods)\n    min_lik_idx = likelihoods.index(min_lik)\n    rarest_window = session[min_lik_idx : min_lik_idx + window_len]  # noqa E203\n    return rarest_window, min_lik", "idx": 1296}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.probabilities.compute_cmds_probs", "completion": "    seq1_probs = StateMatrix()\n    seq2_probs = StateMatrix()\n\n    if isinstance(seq1_counts, StateMatrix):\n        seq1_counts = seq1_counts.to_dict()\n\n    if isinstance(seq2_counts, StateMatrix):\n        seq2_counts = seq2_counts.to_dict()\n\n    for cmd1, cmd1_counts in seq1_counts.items():\n        seq1_probs[cmd1] = defaultdict(float)\n        for cmd2, cmd2_count in cmd1_counts.items():\n            seq1_probs[cmd1][cmd2] = cmd2_count / sum(cmd1_counts.values())\n\n    for cmd1, cmd1_counts in seq2_counts.items():\n        seq2_probs[cmd1] = defaultdict(float)\n        for cmd2, cmd2_count in cmd1_counts.items():\n            seq2_probs[cmd1][cmd2] = cmd2_count / sum(cmd1_counts.values())\n\n    return seq1_probs, seq2_probs\n\n", "idx": 1297}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.probabilities.compute_values_probs", "completion": "    value_probs: DefaultDict[str, float] = defaultdict(lambda: 0)\n    value_cond_param_probs: DefaultDict[str, DefaultDict[str, float]] = defaultdict(\n        lambda: defaultdict(lambda: 0)\n    )\n\n    for param, values in param_value_counts.items():\n        for value, count in values.items():\n            value_cond_param_probs[param][value] = count / sum(\n                param_value_counts[param].values()\n            )\n\n    for value, count in value_counts.items():\n        value_probs[value] = count / sum(value_counts.values())\n\n    value_probs_sm = StateMatrix(states=value_probs, unk_token=unk_token)\n    value_cond_param_probs_sm = StateMatrix(\n        states=value_cond_param_probs, unk_token=unk_token\n    )\n\n    return value_probs_sm, value_cond_param_probs_sm", "idx": 1298}
{"namespace": "msticpy.auth.msal_auth.MSALDelegatedAuth.get_token", "completion": "        accounts = self.app.get_accounts()\n        if accounts:\n            logging.info(\"Account(s) exists in cache, probably with token too. Let's try.\")\n            self.result = self.app.acquire_token_silent(\n                self.scopes, account=accounts[0]\n            )\n        if not self.result:\n            if self.auth_type == \"interactive\":\n                self.result = self.app.acquire_token_interactive(self.scopes)\n            elif self.auth_type == \"device_code\":\n                self.result = self.app.acquire_token_by_device_flow(self.scopes)\n            elif self.auth_type == \"username_password\":\n                self.result = self.app.acquire_token_by_username_password(\n                    self.username, kwargs[\"password\"], self.scopes\n                )\n            else:\n                raise ValueError(\"Invalid auth_type\")\n\n        if \"access_token\" in self.result:\n            logging.info(\"Access token retrieved: {0}\".format(self.result[\"access_token\"]))\n        else:\n            logging.error(\"No access token retrieved: {0}\".format(self.result))\n", "idx": 1299}
{"namespace": "msticpy.config.query_editor.QueryParameterEditWidget.save_parameter", "completion": "        del button\n        parameter_name = self.parameter_name_widget.value\n        description = self.description_widget.value\n        datatype = self.type_widget.value\n        default = self.default_widget.value\n        default_reqd = self.default_reqd_widget.value\n        if parameter_name:\n            parameter = QueryParameter(\n                description=description,\n                datatype=datatype,\n                default=default if default_reqd else None,\n            )\n            self.param_container.parameters[parameter_name] = parameter\n            self.parameter_dropdown.options = list(self.param_container.parameters.keys())\n            self.parameter_dropdown.value = parameter_name\n            self._changed_data = True\n", "idx": 1300}
{"namespace": "msticpy.config.query_editor.QueryParameterEditWidget.delete_parameter", "completion": "        del button\n        if not self.parameter_name_widget.value:\n            return\n        param_name = self.parameter_name_widget.value\n        del self.param_container.parameters[param_name]\n        self.parameter_dropdown.options = list(self.param_container.parameters.keys())\n        self._blank_parameter()\n        self._changed_data = True", "idx": 1301}
{"namespace": "msticpy.config.query_editor.MetadataEditWidget.save_metadata", "completion": "        del button\n        self.metadata.version = self.version_widget.value\n        self.metadata.description = self.description_widget.value\n        self.metadata.data_environments = self.data_env_widget.value\n        self.metadata.data_families = self.data_families_widget.value.split(\", \")\n        self.metadata.database = self.database_widget.value\n        self.metadata.cluster = self.cluster_widget.value\n        self.metadata.clusters = self.clusters_widget.value.split(\"\\n\")\n        self.metadata.cluster_groups = self.cluster_groups_widget.value.split(\"\\n\")\n        self.metadata.tags = self.tags_widget.value.split(\", \")\n        self.metadata.data_source = self.data_source_widget.value\n        self._changed_data = True", "idx": 1302}
{"namespace": "msticpy.config.query_editor.QueryEditor._save_file", "completion": "        del button\n        if self.ignore_changes.value:\n            self.ignore_changes.value = False\n            return\n        if not self.query_collection.file_name:\n            self.query_collection.file_name = self.current_file\n        self.query_collection.metadata = self.metadata_editor.metadata\n        self.query_collection.defaults = self.default_param_editor.param_container\n        self.query_collection.save_to_file(self.current_file)\n        self.filename_widget.value = self.current_file\n        self.ignore_changes.value = False\n", "idx": 1303}
{"namespace": "msticpy.config.query_editor.QueryEditor._unsaved_changes", "completion": "        return (\n            self.default_param_editor.changed_data\n            or self.metadata_editor.changed_data\n            or self.query_editor.changed_data\n        )", "idx": 1304}
{"namespace": "msticpy.config.query_editor.load_queries_from_yaml", "completion": "    with open(yaml_file, \"r\", encoding=\"utf-8\") as file:\n        yaml_data = yaml.safe_load(file)\n\n    # Extract the necessary data from the YAML file\n    metadata = yaml_data.get(\"metadata\", {})\n    defaults = yaml_data.get(\"defaults\", {})\n    queries = yaml_data.get(\"queries\", {})\n\n    # Create instances of QueryMetadata, QueryDefaults, and Query objects\n    query_metadata = QueryMetadata(**metadata)\n    query_defaults = QueryDefaults(**defaults)\n    query_collection = QueryCollection(\n        file_name=yaml_file,\n        metadata=query_metadata,\n        defaults=query_defaults,\n        sources={},\n    )\n\n    # Create instances of Query objects and add them to the QueryCollection\n    for query_name, query_data in queries.items():\n        query_args = QueryArgs(**query_data.get(\"args\", {}))\n        query_parameters = query_data.get(\"parameters\", {})\n        query_metadata = query_data.get(\"metadata\", {})\n        query = Query(\n            description=query_data.get(\"description\", \"\"),\n            args=query_args,\n            parameters=query_parameters,\n            metadata=query_metadata,\n        )\n        query_collection.sources[query_name] = query\n\n    # Return the QueryCollection object\n    return query_collection", "idx": 1305}
{"namespace": "zxcvbn.time_estimates.estimate_attack_times", "completion": "    # Calculate the crack times in seconds for different scenarios\n    online_throttling_100_per_hour = Decimal(guesses) / Decimal(100) / Decimal(3600)\n    online_no_throttling_10_per_second = Decimal(guesses) / Decimal(10)\n    offline_slow_hashing_1e4_per_second = Decimal(guesses) / Decimal(1e4)\n    offline_fast_hashing_1e10_per_second = Decimal(guesses) / Decimal(1e10)\n\n    # Convert the crack times into a more readable format\n    online_throttling_100_per_hour = str(online_throttling_100_per_hour).split('.')[0] + ':' + str(online_throttling_100_per_hour).split('.')[1][:2] + ':' + str(online_throttling_100_per_hour).split('.')[1][2:]\n    online_no_throttling_10_per_second = str(online_no_throttling_10_per_second).split('.')[0] + ':' + str(online_no_throttling_10_per_second).split('.')[1][:2] + ':' + str(online_no_throttling_10_per_second).split('.')[1][2:]\n    offline_slow_hashing_1e4_per_second = str(offline_slow_hashing_1e4_per_second).split('.')[0] + ':' + str(offline_slow_hashing_1e4_per_second).split('.')[1][:2] + ':' + str(offline_slow_hashing_1e4_per_second).split('.')[1][2:]\n    offline_fast_hashing_1e10_per_second = str(offline_fast_hashing_1", "idx": 1306}
{"namespace": "zxcvbn.scoring.estimate_guesses", "completion": "    # Check if the number of guesses is already calculated\n    if 'guesses' in match:\n        return match['guesses']\n\n    # Calculate the minimum number of guesses based on the length of the match token compared to the password length\n    min_guesses = Decimal(len(match['token'])) / Decimal(len(password))\n\n    # Calculate the number of guesses based on the pattern of the match\n    if match['pattern'] == 'dictionary':\n        # Calculate the number of guesses based on the length of the match token\n        guesses = min_guesses\n    elif match['pattern'] == 'repeat':\n        # Calculate the number of guesses based on the length of the match token\n        guesses = min_guesses\n    elif match['pattern'] == 'sequence':\n        # Calculate the number of guesses based on the length of the match token\n        guesses = min_guesses\n    elif match['pattern'] == 'regex':\n        # Calculate the number of guesses based on the length of the match token\n        guesses = min_guesses\n    elif match['pattern'] == 'date':\n        # Calculate the number of guesses based on the length of the match token\n        guesses = min_guesses\n    elif match['pattern'] == 'bruteforce':\n        # Calculate the number of guesses based on the length of the match token\n        guesses = min_guesses\n    else:\n        # Calculate the number of guesses based on the length of the match token\n        guesses = min_guesses\n\n    # Update the match dictionary with the calculated number of guesses\n    match['guesses'] = guesses\n\n    # Return the estimated number of guesses\n    return guesses", "idx": 1307}
{"namespace": "zxcvbn.scoring.dictionary_guesses", "completion": "    base_guesses = match['dictionary']['guesses']\n    uppercase_variations = 1\n    l33t_variations = 1\n    reversed_variations = 1\n\n    if match['dictionary']['uppercase']:\n        uppercase_variations = match['dictionary']['uppercase_variations']\n\n    if match['dictionary']['l33t']:\n        l33t_variations = match['dictionary']['l33t_variations']\n\n    if match['reversed']:\n        reversed_variations = 2\n\n    return base_guesses * uppercase_variations * l33t_variations * reversed_variations\n\n", "idx": 1308}
{"namespace": "zxcvbn.scoring.regex_guesses", "completion": "    # Define a dictionary of character class bases\n    character_class_bases = {\n        'L': 26,  # lowercase letters\n        'U': 26,  # uppercase letters\n        'N': 10,  # digits\n        'S': 33,  # symbols\n        'X': 36,  # hex digits\n        'Z': 10,  # digits and letters\n        'A': 26 + 10,  # alphanumeric\n        'B': 26 + 10 + 2,  # alphanumeric and symbols\n        'C': 26 + 10 + 2 + 2,  # alphanumeric and symbols and punctuation\n        'D': 26 + 10 + 2 + 2 + 2,  # alphanumeric and symbols and punctuation and whitespace\n        'E': 26 + 10 + 2 + 2 + 2 + 2,  # alphanumeric and symbols and punctuation and whitespace and control characters\n        'F': 26 + 10 + 2 + 2 + 2 + 2 + 2,  # alphanumeric and symbols and punctuation and whitespace and control characters and extended characters\n        'G': 26 + 10 + 2 + 2 + 2 + 2 + 2 + 2,  # alphanumeric and symbols and punctuation and whitespace and control characters and extended characters and graphical characters\n        'H': 26 + 10 + 2 + 2 + 2 + 2 + 2 + 2 + 2,  # alphanumeric and symbols and punctuation and whitespace and control characters and extended characters and graphical characters and private use characters\n        'I': 26 + 10 + 2 + 2 + 2 + 2 + 2 + 2 + 2 + 2,  # alphanumeric and symbols and punctuation and whitespace and control characters and extended characters and graphical characters and private use characters and supplementary characters\n        'J': 26 + 10 + 2 + 2 + 2 + 2 + 2 + 2 + 2 + 2 + 2,  # alphanumeric and symbols and punctuation and whitespace and control characters and extended characters and graphical characters and private use characters and supplementary", "idx": 1309}
{"namespace": "zxcvbn.scoring.date_guesses", "completion": "    # Calculate the year difference between the match year and the reference year.\n    year_diff = abs(int(match['year']) - REFERENCE_YEAR)\n\n    # Calculate the number of possible guesses based on the year difference and the presence of a separator.\n    guesses = year_diff * 365\n    if match['separator']:\n        guesses *= 4\n\n    return guesses", "idx": 1310}
{"namespace": "zxcvbn.scoring.spatial_guesses", "completion": "    # Calculate the average degree of the keyboard or keypad\n    if match['graph'] == 'qwerty':\n        average_degree = KEYBOARD_AVERAGE_DEGREE\n        starting_positions = KEYBOARD_STARTING_POSITIONS\n    else:\n        average_degree = KEYPAD_AVERAGE_DEGREE\n        starting_positions = KEYPAD_STARTING_POSITIONS\n\n    # Calculate the number of possible guesses\n    guesses = starting_positions * average_degree ** len(match['token'])\n\n    # Consider the additional guesses for shifted keys\n    if match['turns'] > 0:\n        guesses *= match['turns'] + 1\n\n    # Consider the additional guesses for shifted keys\n    if match['shifted_count'] > 0:\n        guesses *= match['shifted_count'] + 1\n\n    return guesses\n\n", "idx": 1311}
{"namespace": "zxcvbn.scoring.uppercase_variations", "completion": "    if ALL_LOWER.match(match['token']):\n        return 1\n    if ALL_UPPER.match(match['token']):\n        return 1\n    if START_UPPER.match(match['token']):\n        return 2\n    if END_UPPER.match(match['token']):\n        return 2\n\n    upper = sum(1 for c in match['token'] if c.isupper())\n    lower = len(match['token']) - upper\n    return upper + lower\n\n", "idx": 1312}
{"namespace": "zxcvbn.matching.dictionary_match", "completion": "    matches = []\n    password_len = len(password)\n    for dictionary_name, ranked_dictionary in _ranked_dictionaries.items():\n        for word, rank in ranked_dictionary.items():\n            word_len = len(word)\n            if word_len > password_len:\n                continue\n            for i in range(password_len - word_len + 1):\n                if password[i:i + word_len] == word:\n                    matches.append({\n                        'pattern': 'dictionary',\n                        'i': i,\n                        'j': i + word_len,\n                        'matched_word': word,\n                        'rank': rank,\n                        'dictionary_name': dictionary_name,\n                        'reversed': False,\n                    })\n    return matches\n\n", "idx": 1313}
{"namespace": "zxcvbn.matching.reverse_dictionary_match", "completion": "    matches = []\n    length = len(password)\n    password_lower = password.lower()\n    reversed_password = password_lower[::-1]\n    for dictionary_name, ranked_dict in _ranked_dictionaries.items():\n        for i in range(length):\n            for j in range(i, length):\n                if reversed_password[i:j + 1] in ranked_dict:\n                    word = reversed_password[i:j + 1]\n                    rank = ranked_dict[word]\n                    matches.append({\n                        'pattern': 'dictionary',\n                        'i': length - j - 1,\n                        'j': length - i - 1,\n                        'token': password[length - j - 1:length - i],\n                        'matched_word': word[::-1],\n                        'rank': rank,\n                        'dictionary_name': dictionary_name,\n                        'reversed': True,\n                        'l33t': False,\n                    })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))", "idx": 1314}
{"namespace": "zxcvbn.matching.l33t_match", "completion": "    matches = []\n    length = len(password)\n    password_lower = password.lower()\n    for dictionary_name, ranked_dict in _ranked_dictionaries.items():\n        for sub_dict in enumerate_l33t_subs(_l33t_table):\n            for i in range(length):\n                for j in range(i, length):\n                    subbed = translate(password_lower[i:j + 1], sub_dict)\n                    if subbed in ranked_dict:\n                        word = subbed\n                        rank = ranked_dict[word]\n                        matches.append({\n                            'pattern': 'l33t',\n                            'i': i,\n                            'j': j,\n                            'token': translate(password[i:j + 1], sub_dict),\n                            'matched_word': word,\n                            'rank': rank,\n                            'dictionary_name': dictionary_name,\n                            'reversed': False,\n                            'l33t': True,\n                            'sub': sub_dict,\n                        })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))\n\n", "idx": 1315}
{"namespace": "zxcvbn.matching.repeat_match", "completion": "    matches = []\n    length = len(password)\n    greedy_matches = re.finditer(r'(.+)\\1+', password)\n    lazy_matches = re.finditer(r'(.+?)\\1+', password)\n    for greedy_match in greedy_matches:\n        for lazy_match in lazy_matches:\n            if greedy_match.start() == lazy_match.start() and \\\n                    greedy_match.end() == lazy_match.end():\n                continue\n\n            greedy_match_start = greedy_match.start()\n            greedy_match_end = greedy_match.end()\n            lazy_match_start = lazy_match.start()\n            lazy_match_end = lazy_match.end()\n            if greedy_match_start > lazy_match_start:\n                continue\n\n            if greedy_match_end < lazy_match_end:\n                continue\n\n            if greedy_match_start == lazy_match_start and \\\n                    greedy_match_end == lazy_match_end:\n                continue\n\n            base_token = greedy_match.group(1)\n            base_token_length = len(base_token)\n            if base_token_length < 3:\n                continue\n\n            greedy_match_length = greedy_match_end - greedy_match_start\n            lazy_match_length = lazy_match_end - lazy_match_start\n            if greedy_match_length < lazy_match_length:\n                continue\n\n            repeat_count = greedy_match_length // base_token_length\n            if repeat_count < 3:\n                continue\n\n            token = password[greedy_match_start:greedy_match_end]\n            i = greedy_match_start\n            j = greedy_match_end - 1\n            matches.append({\n                'pattern': 'repeat',\n                'i': i,\n                'j': j,\n                'token': token,\n                'base_token': base_token,\n                'base_token_length': base_token_length", "idx": 1316}
{"namespace": "zxcvbn.matching.spatial_match", "completion": "    matches = []\n    for graph_name, graph in _graphs.items():\n        matches.extend(spatial_helper(password, graph, _ranked_dictionaries))\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))\n\n", "idx": 1317}
{"namespace": "zxcvbn.matching.sequence_match", "completion": "    matches = []\n    length = len(password)\n    for i in range(length):\n        for j in range(i + 2, length):\n            delta = ord(password[j]) - ord(password[j - 1])\n            if abs(delta) > MAX_DELTA:\n                continue\n            for dictionary_name, ranked_dict in _ranked_dictionaries.items():\n                if password[i:j] in ranked_dict:\n                    matches.append({\n                        'pattern': 'sequence',\n                        'i': i,\n                        'j': j - 1,\n                        'token': password[i:j],\n                        'sequence_name': dictionary_name,\n                        'sequence_space': len(ranked_dict),\n                        'ascending': delta > 0,\n                    })\n\n    return matches\n\n", "idx": 1318}
{"namespace": "zxcvbn.matching.regex_match", "completion": "    matches = []\n    for regex_name, regex in _regexen.items():\n        for match in regex.finditer(password):\n            matches.append({\n                'pattern': 'regex',\n                'i': match.start(),\n                'j': match.end() - 1,\n                'token': match.group(),\n                'regex_name': regex_name,\n                'regex_match': match,\n            })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))\n\n", "idx": 1319}
{"namespace": "OpenSSL.rand.add", "completion": "    _lib.RAND_add(buffer, entropy, len(buffer))\n\n", "idx": 1320}
{"namespace": "asyncssh.kex.register_kex_alg", "completion": "    _kex_algs.append(alg)\n    _kex_handlers[alg] = (handler, hash_alg, args)\n    if default:\n        _default_kex_algs.append(alg)\n\n", "idx": 1321}
{"namespace": "asyncssh.auth.get_supported_server_auth_methods", "completion": "    return [method for method in _auth_methods\n            if _server_auth_handlers[method].supported(conn)]\n\n", "idx": 1322}
{"namespace": "asyncssh.mac.get_mac", "completion": "    if mac_alg not in _mac_handler:\n        raise ValueError('unsupported MAC algorithm')\n\n    handler, hash_size, args = _mac_handler[mac_alg]\n    return handler(key, hash_size, *args)\n\n", "idx": 1323}
{"namespace": "asyncssh.auth_keys.SSHAuthorizedKeys.validate", "completion": "        if ca:\n            entries = self._ca_entries\n        else:\n            entries = self._user_entries\n\n        for entry in entries:\n            if entry.key and entry.key.matches(key):\n                if entry.match_options(client_host, client_addr, cert_principals):\n                    return entry.options\n            elif entry.cert and entry.cert.matches(key):\n                if entry.match_options(client_host, client_addr, cert_principals):\n                    return entry.options\n\n        return None\n", "idx": 1324}
{"namespace": "asyncssh.saslprep.saslprep", "completion": "    return _stringprep(s, True, _map_saslprep, 'NFKC', (stringprep.in_table_c2,), True)", "idx": 1325}
{"namespace": "asyncssh.asn1.der_decode", "completion": "    value, end = der_decode_partial(data)\n    if end < len(data):\n        raise ASN1DecodeError('Data contains unexpected bytes at end')\n    return value", "idx": 1326}
{"namespace": "asyncssh.packet.SSHPacket.check_end", "completion": "        if self._idx != self._len:\n            raise PacketDecodeError('unexpected data at end of packet')\n", "idx": 1327}
{"namespace": "asyncssh.public_key.SSHKey.verify", "completion": "        try:\n            sig_algorithm, sig_packet = der_decode(sig)\n        except ASN1DecodeError:\n            return False\n\n        if sig_algorithm not in self.all_sig_algorithms:\n            return False\n\n        return self.verify_ssh(data, sig_algorithm, sig_packet)\n", "idx": 1328}
{"namespace": "asyncssh.public_key.SSHKey.convert_to_public", "completion": "        # pylint: disable=no-self-use\n        raise KeyExportError('Public key export not supported')", "idx": 1329}
{"namespace": "asyncssh.public_key.SSHKey.generate_x509_user_certificate", "completion": "        if not _x509_available: # pragma: no cover\n            raise KeyGenerationError('X.509 certificate generation '\n                                     'requires PyOpenSSL')\n\n        if not self.x509_algorithms:\n            raise KeyGenerationError('X.509 certificate generation not '\n                                     'supported for ' + self.get_algorithm() +\n                                     ' keys')\n\n        if not isinstance(purposes, str):\n            purposes = list(purposes)\n\n        return self._generate_x509_certificate(user_key, subject, issuer,\n                                               serial, valid_after,\n                                               valid_before, False, None,\n                                               purposes, principals, (),\n                                               hash_alg, comment)\n", "idx": 1330}
{"namespace": "asyncssh.misc.write_file", "completion": "    with open_file(filename, mode) as f:\n        return f.write(data)\n\n", "idx": 1331}
{"namespace": "diffprivlib.accountant.BudgetAccountant.__repr__", "completion": "        attributes = []\n        if self.epsilon != float(\"inf\"):\n            attributes.append(f\"epsilon={self.epsilon}\")\n        if self.delta != 1.0:\n            attributes.append(f\"delta={self.delta}\")\n        if self.slack > 0:\n            attributes.append(f\"slack={self.slack}\")\n        if len(self.spent_budget) > n_budget_max:\n            attributes.append(f\"spent_budget={self.spent_budget[:n_budget_max]}\u2026\")\n        else:\n            attributes.append(f\"spent_budget={self.spent_budget}\")\n        return f\"BudgetAccountant({', '.join(attributes)})\"\n", "idx": 1332}
{"namespace": "diffprivlib.accountant.BudgetAccountant.check", "completion": "        if epsilon < 0 or delta < 0:\n            raise ValueError(\"Epsilon and delta must be non-negative.\")\n\n        if epsilon == 0 and delta == 0:\n            return True\n\n        if epsilon == 0:\n            if delta > self.delta:\n                raise BudgetError(f\"Privacy spend of (0,{delta}) not permissible; will exceed remaining privacy budget. \"\n                                  f\"Use {self.__class__.__name__}.remaining() to check remaining budget.\")\n            return True\n\n        if delta == 0:\n            if epsilon > self.epsilon:\n                raise BudgetError(f\"Privacy spend of ({epsilon},0) not permissible; will exceed remaining privacy budget. \"\n                                  f\"Use {self.__class__.__name__}.remaining() to check remaining budget.\")\n            return True\n\n        if epsilon > self.epsilon:\n            raise BudgetError(f\"Privacy spend of ({epsilon},{delta}) not permissible; will exceed remaining privacy budget. \"\n                              f\"Use {self.__class__.__name__}.remaining() to check remaining budget.\")\n\n        if delta > self.delta:\n            raise BudgetError(f\"Privacy spend of ({epsilon},{delta}) not permissible; will exceed remaining privacy budget. \"\n                              f\"Use {self.__class__.__name__}.remaining() to check remaining budget.\")\n\n        return True\n", "idx": 1333}
{"namespace": "diffprivlib.accountant.BudgetAccountant.spend", "completion": "        if not self.check(epsilon, delta):\n            raise BudgetError(f\"Privacy spend of ({epsilon},{delta}) not permissible; will exceed remaining privacy budget.\"\n                              f\" Use {self.__class__.__name__}.{self.remaining.__name__}() to check remaining budget.\")\n\n        self.__spent_budget.append((epsilon, delta))\n        return self", "idx": 1334}
{"namespace": "diffprivlib.accountant.BudgetAccountant.load_default", "completion": "        if accountant is None:\n            if BudgetAccountant._default is None:\n                BudgetAccountant._default = BudgetAccountant()\n            return BudgetAccountant._default\n        elif isinstance(accountant, BudgetAccountant):\n            return accountant\n        else:\n            raise TypeError(\"accountant must be a BudgetAccountant or None\")", "idx": 1335}
{"namespace": "diffprivlib.accountant.BudgetAccountant.set_default", "completion": "        BudgetAccountant._default = self\n        return self", "idx": 1336}
{"namespace": "diffprivlib.accountant.BudgetAccountant.pop_default", "completion": "        default = BudgetAccountant._default\n        BudgetAccountant._default = None\n        return default", "idx": 1337}
{"namespace": "diffprivlib.validation.clip_to_bounds", "completion": "    if not isinstance(array, np.ndarray):\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\n    if not isinstance(bounds, tuple):\n        raise TypeError(f\"Bounds must be specified as a tuple of (min, max), got {type(bounds)}.\")\n    if not isinstance(shape, Integral):\n        raise TypeError(f\"shape parameter must be integer-valued, got {type(shape)}.\")\n\n    lower, upper = bounds\n\n    if np.asarray(lower).size == 1 or np.asarray(upper).size == 1:\n        lower = np.ravel(lower).astype(dtype)\n        upper = np.ravel(upper).astype(dtype)\n    else:\n        lower = np.asarray(lower, dtype=dtype)\n        upper = np.asarray(upper, dtype=dtype)\n\n    if lower.shape != upper.shape:\n        raise ValueError(\"lower and upper bounds must be the same shape array\")\n    if lower.ndim > 1:\n        raise ValueError(\"lower and upper bounds must be scalar or a 1-dimensional array\")\n    if lower.size not in (1, shape):\n        raise ValueError(f\"lower and upper bounds must have {shape or 1} element(s), got {lower.size}.\")\n\n    n_bounds = lower.shape[0]\n\n    for i in range(n_bounds):\n        _lower = lower[i]\n        _upper = upper[i]\n\n        if not isinstance(_lower, Real) or not isinstance(_upper, Real):\n            raise TypeError(f\"Each bound must be numeric, got {_lower} ({type(_lower)}) and {_upper} ({type(_upper)}).\")\n\n        if _lower > _upper:\n            raise ValueError(f\"For each bound, lower bound must be smaller than upper bound, got {lower}, {upper})\")\n\n        if _upper - _lower < min_separation:\n            mid = (_upper + _lower) / 2\n            lower", "idx": 1338}
{"namespace": "diffprivlib.models.naive_bayes.GaussianNB._update_mean_variance", "completion": "        if n_noisy is None:\n            n_noisy = X.shape[0]\n\n        if sample_weight is not None:\n            warn_unused_args(\"sample_weight\")\n\n        n_new = n_noisy\n        n_total = n_past + n_new\n\n        if n_total == 0:\n            return mu, var\n\n        # Calculate the new mean\n        total_mu = (n_past * mu + n_new * X.mean(axis=0)) / n_total\n\n        # Calculate the new variance\n        total_var = (n_past * var + n_new * np.sum((X - mu) ** 2, axis=0) + n_new * (mu - total_mu) ** 2) / n_total\n\n        return total_mu, total_var", "idx": 1339}
{"namespace": "diffprivlib.models.naive_bayes.GaussianNB._noisy_class_counts", "completion": "        unique_y = np.unique(y)\n        unique_y_counts = np.zeros(len(unique_y))\n        for i, y_i in enumerate(unique_y):\n            unique_y_counts[i] = np.sum(y == y_i)\n\n        # Calculate the sensitivity of the class counts\n        sensitivity = np.max(unique_y_counts)\n\n        # Calculate the epsilon for the class counts\n        epsilon = self.epsilon / len(unique_y)\n\n        # Add noise to the class counts using the privacy mechanism\n        noisy_class_counts = GeometricTruncated(epsilon=epsilon, delta=0, sensitivity=sensitivity,\n                                                lower=0, upper=sensitivity, random_state=random_state).randomise(\n            unique_y_counts)\n\n        return noisy_class_counts", "idx": 1340}
{"namespace": "diffprivlib.models.standard_scaler._incremental_mean_and_var", "completion": "    # Initialising new accountant, as budget is tracked in main class. Subject to review in line with GH issue #21\n    accountant = BudgetAccountant(epsilon=epsilon, bounds=bounds)\n\n    # Calculate the mean and variance of the new data increment\n    new_mean = nanmean(X, axis=0)\n    new_variance = nanvar(X, axis=0)\n    new_sample_count = X.shape[0]\n\n    # Update the mean and variance using the previous values and the new data increment\n    mean = (last_sample_count * last_mean + new_sample_count * new_mean) / (last_sample_count + new_sample_count)\n    variance = (last_sample_count * last_variance + new_sample_count * new_variance +\n                last_sample_count * new_sample_count * (last_mean - new_mean) ** 2 / (last_sample_count + new_sample_count))\n\n    # Add the privacy budget to the accountant\n    accountant.add_budget(epsilon)\n\n    return mean, variance, new_sample_count\n\n", "idx": 1341}
{"namespace": "diffprivlib.models.linear_regression.LinearRegression.fit", "completion": "        X, y, X_offset, y_offset, X_scale = _preprocess_data(X, y, self.fit_intercept, self.epsilon, self.bounds_X,\n                                                             self.bounds_y, self.copy_X, True, self.random_state,\n                                                             **self._unused_args)\n\n        if self.bounds_X is None:\n            self.bounds_X = ((X.min(axis=0), X.max(axis=0)))\n        if self.bounds_y is None:\n            self.bounds_y = ((y.min(axis=0), y.max(axis=0)))\n\n        self.accountant.add_budget(self.epsilon)\n\n        if self.accountant.check_budget(self.epsilon):\n            self.coef_, self.intercept_ = self._fit(X, y, X_offset, y_offset, X_scale)\n        else:\n            warnings.warn(\"Insufficient budget to compute linear regression\", PrivacyLeakWarning)\n\n        return self\n", "idx": 1342}
{"namespace": "diffprivlib.models.k_means.KMeans.fit", "completion": "        # Check if the input data is valid\n        self._validate_data(X, reset=True, y=y, multi_output=False)\n\n        # Check if the bounds are provided\n        if self.bounds is None:\n            self.bounds = (X.min(axis=0), X.max(axis=0))\n\n        # Check if the bounds are valid\n        if len(self.bounds) != 2:\n            raise ValueError(\"Bounds must be a tuple of the form (min, max)\")\n        if len(self.bounds[0]) != X.shape[1]:\n            raise ValueError(\"Bounds must have the same number of features as the input data\")\n        if len(self.bounds[1]) != X.shape[1]:\n            raise ValueError(\"Bounds must have the same number of features as the input data\")\n\n        # Check if the bounds are consistent with the data\n        if np.any(self.bounds[0] > X.max(axis=0)) or np.any(self.bounds[1] < X.min(axis=0)):\n            raise ValueError(\"Bounds must be consistent with the data\")\n\n        # Check if the number of clusters is valid\n        if self.n_clusters <= 0:\n            raise ValueError(\"Number of clusters must be greater than 0\")\n\n        # Check if the privacy parameter is valid\n        if self.epsilon <= 0:\n            raise ValueError(\"Privacy parameter must be greater than 0\")\n\n        # Check if the accountant is valid\n        if self.accountant is None:\n            raise ValueError(\"Accountant must be provided\")\n\n        # Check if the accountant has enough budget\n        if self.accountant.get_epsilon() < self.epsilon:\n            raise ValueError(\"Accountant does not have enough budget\")\n\n        # Check if the number of threads is valid\n        if self._n_threads <= 0:\n            raise ValueError(\"Number of threads must be greater than 0\")\n\n        # Check if the number of threads is valid\n        if self._n_threads > X.shape[0]:\n            raise ValueError(\"", "idx": 1343}
{"namespace": "diffprivlib.models.forest._FittingTree.__getstate__", "completion": "        return {\n            \"max_depth\": self.max_depth,\n            \"node_count\": self.node_count,\n            \"nodes\": self.nodes,\n            \"values\": self.values\n        }\n", "idx": 1344}
{"namespace": "diffprivlib.models.forest._FittingTree.fit", "completion": "        if not self.nodes:\n            raise ValueError(\"Tree has not been built. Call .build() first.\")\n\n        # Get leaves\n        leaves = self._get_leaves(X)\n\n        # Get unique leaves\n        unique_leaves = np.unique(leaves)\n\n        # Initialize values\n        self.values_ = np.zeros((unique_leaves.shape[0], self.n_classes_))\n\n        # Populate values for real leaves\n        for leaf, label in zip(leaves, y):\n            self.values_[leaf, label] += 1\n\n        # Populate values for empty leaves\n        for leaf in unique_leaves:\n            if leaf == self._TREE_LEAF:\n                continue\n            if np.sum(self.values_[leaf]) == 0:\n                self.values_[leaf] = self.random_state.multinomial(1, np.ones(self.n_classes_) / self.n_classes_)\n\n        # Assign values to tree\n        for node in self.nodes:\n            node.value = self.values_[node.left_child] + self.values_[node.right_child]\n\n        return self\n", "idx": 1345}
{"namespace": "diffprivlib.tools.histograms.histogram", "completion": "    warn_unused_args(unused_args)\n\n    # Check if the accountant is provided\n    if accountant is None:\n        accountant = BudgetAccountant()\n\n    # Check if the random state is provided\n    if random_state is None:\n        random_state = np.random.RandomState()\n    elif isinstance(random_state, int):\n        random_state = np.random.RandomState(random_state)\n\n    # Check if the sample is a numpy array\n    if not isinstance(sample, np.ndarray):\n        sample = np.array(sample)\n\n    # Check if the sample is a 1-D array\n    if sample.ndim != 1:\n        raise ValueError(\"The sample must be a 1-D array.\")\n\n    # Check if the sample is not empty\n    if sample.size == 0:\n        raise ValueError(\"The sample must not be empty.\")\n\n    # Check if the sample is not a string\n    if sample.dtype.kind == 'U':\n        raise ValueError(\"The sample must not be a string.\")\n\n    # Check if the sample is not a boolean\n    if sample.dtype.kind == 'b':\n        raise ValueError(\"The sample must not be a boolean.\")\n\n    # Check if the sample is not a complex number\n    if sample.dtype.kind == 'c':\n        raise ValueError(\"The sample must not be a complex number.\")\n\n    # Check if the sample is not a floating point number\n    if sample.dtype.kind == 'f':\n        raise ValueError(\"The sample must not be a floating point number.\")\n\n    # Check if the sample is not an integer\n    if sample.dtype.kind == 'i':\n        raise ValueError(\"The sample must not be an integer.\")\n\n    # Check if the sample is not an unsigned integer\n    if sample.dtype.kind == 'u':\n        raise ValueError(\"The sample must not be an unsigned integer.\")\n\n    # Check if the sample is not an object\n    if sample.dtype.kind == 'O':\n        raise ValueError(\"The sample must not be an object.\")", "idx": 1346}
{"namespace": "diffprivlib.tools.histograms.histogram2d", "completion": "    warn_unused_args(unused_args)\n\n    random_state = check_random_state(random_state)\n\n    accountant = BudgetAccountant.load_default(accountant)\n    accountant.check(epsilon, 0)\n\n    if range is None:\n        warnings.warn(\"Range parameter has not been specified. Falling back to taking range from the data.\\n\"\n                      \"To ensure differential privacy, and no additional privacy leakage, the range must be \"\n                      \"specified independently of the data (i.e., using domain knowledge).\", PrivacyLeakWarning)\n\n    hist, xedges, yedges = np.histogram2d(array_x, array_y, bins=bins, range=range, weights=weights, density=None)\n\n    dp_mech = GeometricTruncated(epsilon=epsilon, sensitivity=1, lower=0, upper=maxsize, random_state=random_state)\n\n    dp_hist = np.zeros_like(hist)\n    iterator = np.nditer(hist, flags=['multi_index'])\n\n    while not iterator.finished:\n        dp_hist[iterator.multi_index] = dp_mech.randomise(int(iterator[0]))\n        iterator.iternext()\n\n    dp_hist = dp_hist.astype(float, casting='safe')\n\n    if density:\n        # calculate the probability density function\n        dims = len(dp_hist.shape)\n        dp_hist_sum = dp_hist.sum()\n        for i in np.arange(dims):\n            shape = np.ones(dims, int)\n            shape[i] = dp_hist.shape[i]\n            # noinspection PyUnresolvedReferences\n            dp_hist = dp_hist / np.diff(bin_edges[i]).reshape(shape)\n\n        if dp_hist_sum > 0:\n            dp_hist /= dp_hist_sum\n\n    accountant.spend(epsilon, 0)\n\n    return", "idx": 1347}
{"namespace": "diffprivlib.tools.utils.nanmean", "completion": "    warn_unused_args(unused_args)\n\n    return _mean(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                 random_state=random_state, accountant=accountant, nan=True)", "idx": 1348}
{"namespace": "diffprivlib.tools.utils.var", "completion": "    warn_unused_args(unused_args)\n\n    return _var(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant)\n\n", "idx": 1349}
{"namespace": "diffprivlib.tools.utils.nanvar", "completion": "    warn_unused_args(unused_args)\n\n    return _var(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)", "idx": 1350}
{"namespace": "diffprivlib.tools.utils.std", "completion": "    warn_unused_args(unused_args)\n\n    return _std(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=False)\n\n", "idx": 1351}
{"namespace": "diffprivlib.tools.utils.nanstd", "completion": "    warn_unused_args(unused_args)\n\n    return _std(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)\n\n", "idx": 1352}
{"namespace": "diffprivlib.tools.utils.sum", "completion": "    warn_unused_args(unused_args)\n\n    return _sum_(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                 random_state=random_state, accountant=accountant, nan=False)\n\n", "idx": 1353}
{"namespace": "diffprivlib.tools.utils.nansum", "completion": "    warn_unused_args(unused_args)\n\n    return _sum(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)", "idx": 1354}
{"namespace": "diffprivlib.tools.quantiles.quantile", "completion": "    warn_unused_args(unused_args)\n    if random_state is None:\n        random_state = np.random.RandomState()\n    if not isinstance(random_state, np.random.RandomState):\n        random_state = np.random.RandomState(random_state)\n\n    if accountant is not None:\n        accountant.account_epsilon(epsilon)\n\n    if bounds is None:\n        bounds = (np.min(array), np.max(array))\n\n    if np.any(quant < 0) or np.any(quant > 1):\n        raise ValueError(\"Quantiles must be in the range [0, 1].\")\n\n    if np.isscalar(quant):\n        quant = np.array([quant])\n\n    quant = np.ravel(quant)\n\n    if np.any(quant < 0) or np.any(quant > 1):\n        raise ValueError(\"Quantiles must be in the range [0, 1].\")\n\n    if np.any(quant == 0) or np.any(quant == 1):\n        warnings.warn(\"Quantiles of 0 or 1 may not be accurate.\")\n\n    if np.any(quant[1:] - quant[:-1] < 0):\n        raise ValueError(\"Quantiles must be in ascending order.\")\n\n    if np.any(quant[1:] - quant[:-1] == 0):\n        raise ValueError(\"Quantiles must be unique.\")\n\n    if np.any(quant == 0) or np.any(quant == 1):\n        warnings.warn(\"Quantiles of 0 or 1 may not be accurate.\")\n\n    if np.any(quant[1:] - quant[:-1] < 0):\n        raise ValueError(\"Quantiles must be in ascending order.\")\n\n    if np.any(quant[1:] - quant[:-1] == 0):\n        raise ValueError(\"Quantiles must be unique.\")\n\n    if np.any(quant == 0) or np.any(quant == 1):\n        warnings.warn(\"Quantiles of 0 or 1 may not be accurate.\")\n\n    if np.any(quant", "idx": 1355}
{"namespace": "diffprivlib.tools.quantiles.percentile", "completion": "    from diffprivlib.utils import check_random_state\n    from diffprivlib.utils import PrivacyLeakWarning\n    from diffprivlib.accountant import BudgetAccountant\n    from diffprivlib.validation import clip_to_bounds\n    from diffprivlib.tools.utils import _wrap_axis\n    from diffprivlib.validation import check_bounds\n    warn_unused_args(unused_args)\n\n    random_state = check_random_state(random_state)\n\n    if bounds is None:\n        warnings.warn(\"Bounds have not been specified and will be calculated on the data provided. This will \"\n                      \"result in additional privacy leakage. To ensure differential privacy and no additional \"\n                      \"privacy leakage, specify bounds for each dimension.\", PrivacyLeakWarning)\n        bounds = (np.min(array), np.max(array))\n\n    percent = np.ravel(percent)\n\n    if np.any(percent < 0) or np.any(percent > 100):\n        raise ValueError(\"Percentiles must be in the range [0, 100].\")\n\n    if len(percent) > 1:\n        return np.array([percentile(array, p_i, epsilon=epsilon / len(percent), bounds=bounds, axis=axis, keepdims=keepdims,\n                                    accountant=accountant, random_state=random_state) for p_i in percent])\n\n    # Dealing with a single percent from now on\n    percent = percent.item()\n\n    if axis is not None or keepdims:\n        return _wrap_axis(percentile, array, percent=percent, epsilon=epsilon, bounds=bounds, axis=axis, keepdims=keepdims,\n                          random_state=random_state, accountant=accountant)\n\n    # Dealing with a scalar output from now on\n    bounds = check_bounds(bounds, shape=0, min_separation=1e-5)\n\n    accountant = BudgetAccountant.load_default(accountant)\n    accountant.check(epsilon, 0)\n\n    # Let's ra", "idx": 1356}
{"namespace": "diffprivlib.mechanisms.base.bernoulli_neg_exp", "completion": "    if not isinstance(gamma, Real):\n        raise TypeError(\"Gamma must be numeric\")\n\n    if gamma < 0:\n        raise ValueError(\"Gamma must be non-negative\")\n\n    rng = check_random_state(random_state, True)\n\n    return 0 if rng.random() < np.exp(-gamma) else 1", "idx": 1357}
{"namespace": "discord.utils.snowflake_time", "completion": "    return datetime.datetime.utcfromtimestamp(((id >> 22) + 1420070400000) / 1000)", "idx": 1358}
{"namespace": "discord.utils.time_snowflake", "completion": "    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=datetime.timezone.utc)\n    timestamp = int(dt.timestamp() * 1000) - DISCORD_EPOCH\n    if high:\n        return (timestamp << 22) | 0x3FFF\n    return (timestamp << 22) | 0x1FFF\n\n", "idx": 1359}
{"namespace": "discord.utils.resolve_invite", "completion": "    if isinstance(invite, Invite):\n        return ResolvedInvite(invite.code, invite.event_id)\n    elif isinstance(invite, str):\n        if invite.startswith('https://discord.gg/'):\n            return ResolvedInvite(invite.split('/')[-1], None)\n        elif invite.startswith('https://discord.com/invite/'):\n            return ResolvedInvite(invite.split('/')[-1], None)\n        elif invite.startswith('https://discordapp.com/invite/'):\n            return ResolvedInvite(invite.split('/')[-1], None)\n        elif invite.startswith('https://discord.com/events/'):\n            return ResolvedInvite(invite.split('/')[-1], int(invite.split('/')[-2]))\n        elif invite.startswith('https://discordapp.com/events/'):\n            return ResolvedInvite(invite.split('/')[-1], int(invite.split('/')[-2]))\n        elif invite.startswith('https://discord.gg/'):\n            return ResolvedInvite(invite.split('/')[-1], None)\n        elif invite.startswith('https://discordapp.com/'):\n            return ResolvedInvite(invite.split('/')[-1], None)\n        elif invite.startswith('https://'):\n            return ResolvedInvite(invite.split('/')[-1], None)\n        elif invite.startswith('http://'):\n            return ResolvedInvite(invite.split('/')[-1], None)\n        elif invite.startswith('discord.gg/'):\n            return ResolvedInvite(invite.split('/')[-1], None)\n        elif invite.startswith('discordapp.com/'):\n            return ResolvedInvite(invite.split('/')[-1], None)\n        elif invite.startswith('discord.com/'):\n            return Res", "idx": 1360}
{"namespace": "discord.utils.resolve_annotation", "completion": "    if annotation is None:\n        return type(None)\n\n    if isinstance(annotation, str):\n        annotation = ForwardRef(annotation)\n\n    namespace = localns or globalns\n    if cache is None:\n        cache = {}\n\n    return evaluate_annotation(annotation, globalns, namespace, cache)", "idx": 1361}
{"namespace": "discord.ext.tasks.loop", "completion": "    def decorator(coro: LF) -> Loop[LF]:\n        return Loop(\n            coro,\n            seconds=seconds,\n            minutes=minutes,\n            hours=hours,\n            time=time,\n            count=count,\n            reconnect=reconnect,\n        )\n\n    return decorator", "idx": 1362}
{"namespace": "barf.analysis.gadgets.classifier.GadgetClassifier.classify", "completion": "        # Classified gadgets.\n        classified_gadgets = []\n\n        # Iterate through the classifiers.\n        for gadget_type, classifier in self._classifiers.items():\n\n            # Try to classify the gadget.\n            try:\n                classified_gadgets.extend(classifier(gadget))\n\n            # If an error occurs, print the error message and traceback.\n            except Exception as e:\n                print(\"Error while classifying gadget: {}\".format(e))\n                import traceback\n                traceback.print_exc()\n\n        # Sort the classified gadgets by their string representation.\n        classified_gadgets.sort(key=lambda x: str(x))\n\n        # Return the classified gadgets.\n        return classified_gadgets\n", "idx": 1363}
{"namespace": "barf.analysis.gadgets.finder.GadgetFinder.find", "completion": "        # Set the maximum number of bytes and the depth of instructions to be considered.\n        self._max_bytes = byte_depth\n        self._instrs_depth = instrs_depth\n\n        # Call the appropriate method based on the architecture to find the candidates.\n        if self._architecture == ARCH_X86:\n            candidates = self._find_x86(start_address, end_address)\n        elif self._architecture == ARCH_ARM:\n            candidates = self._find_arm(start_address, end_address)\n        else:\n            raise NotImplementedError(\"Architecture not supported\")\n\n        # Sort the candidates based on their addresses and return the sorted list.\n        return sorted(candidates, key=lambda x: x.address)\n", "idx": 1364}
{"namespace": "barf.core.reil.parser.ReilParser.parse", "completion": "        parsed_instrs = []\n\n        for instr in instrs:\n            instr = instr.lower()\n\n            if instr in self._cache:\n                parsed_instr = copy.deepcopy(self._cache[instr])\n\n                parsed_instrs.append(parsed_instr)\n            else:\n                try:\n                    parsed_instr = instruction.parseString(instr)\n\n                    self._cache[instr] = parsed_instr\n\n                    parsed_instrs.append(parsed_instr)\n                except Exception as e:\n                    logger.error(\"Error parsing instruction: %s\", e)\n\n        return parsed_instrs", "idx": 1365}
{"namespace": "barf.core.smt.smtfunction.zero_extend", "completion": "    if not isinstance(s, (Constant, BitVec)):\n        raise TypeError(\"The input value must be a Constant or BitVec\")\n\n    if not isinstance(size, int):\n        raise TypeError(\"The size must be an integer\")\n\n    if size < 0:\n        raise ValueError(\"The size must be non-negative\")\n\n    if s.size == size:\n        return s\n\n    if s.size > size:\n        raise ValueError(\"The size must be greater than the input value's size\")\n\n    return BitVec(size, \"zero_extend({}, {})\".format(s, size))", "idx": 1366}
{"namespace": "barf.core.smt.smtfunction.extract", "completion": "    assert type(s) in (Constant, BitVec) and offset >= 0 and size >= 0 and offset + size <= s.size\n\n    if offset == 0 and size == s.size:\n        return s\n\n    return BitVec(size, \"(_ extract {} {})\".format(offset, size), s)\n\n", "idx": 1367}
{"namespace": "barf.core.smt.smtfunction.ite", "completion": "    assert type(cond) == Bool\n    assert type(true) == BitVec and type(false) == BitVec\n    assert true.size == false.size == size\n\n    return BitVec(size, \"(_ ite {} {} {})\".format(cond, true, false))\n\n", "idx": 1368}
{"namespace": "barf.core.smt.smtfunction.concat", "completion": "    if len(args) == 1:\n        return args[0]\n\n    return BitVec(size, \"concat\", *args)\n\n", "idx": 1369}
{"namespace": "barf.core.smt.smtsymbol.BitVecArray.declaration", "completion": "        return \"(declare-fun {name} () (Array (_ BitVec {key size}) (_ BitVec {value size})))\".format(name=self.name, key size=self.key size, value size=self.value size)\n", "idx": 1370}
{"namespace": "barf.arch.translator.InstructionTranslator.translate", "completion": "        try:\n            return self._translate(instruction)\n        except Exception as e:\n            logger.exception(\"Unknown error\")\n            raise TranslationError(\"Unknown error\")\n", "idx": 1371}
{"namespace": "barf.arch.emulator.Emulator.load_binary", "completion": "        try:\n            with open(binary, 'rb') as f:\n                file_signature = f.read(2)\n\n            if file_signature == b'\\x7fELF':\n                self._load_binary_elf(binary)\n            elif file_signature == b'MZ':\n                self._load_binary_pe(binary)\n            else:\n                raise Exception(\"Unknown file format.\")\n        except Exception as e:\n            raise Exception(\"Error loading file.\")", "idx": 1372}
{"namespace": "barf.arch.arm.parser.ArmParser.parse", "completion": "        instr = instr.lower()\n\n        if instr in self._cache:\n            return copy.deepcopy(self._cache[instr])\n\n        try:\n            parsed_instr = instruction.parseString(instr, parseAll=True)[0]\n            self._cache[instr] = parsed_instr\n            return copy.deepcopy(parsed_instr)\n        except Exception as e:\n            logger.error(\"Failed to parse instruction: %s\", e)\n            return None", "idx": 1373}
{"namespace": "barf.arch.x86.parser.X86Parser.parse", "completion": "        instr = instr.lower()\n\n        if instr in self._cache:\n            return copy.deepcopy(self._cache[instr])\n\n        try:\n            parsed_instr = instruction.parseString(instr, parseAll=True)[0]\n        except Exception as e:\n            logger.error(\"Error parsing instruction: %s\", e)\n            return None\n\n        self._cache[instr] = parsed_instr\n\n        return copy.deepcopy(parsed_instr)", "idx": 1374}
{"namespace": "faker.utils.text.slugify", "completion": "    if allow_dots:\n        value = _re_pattern_allow_dots.sub(\"\", value)\n    else:\n        value = _re_pattern.sub(\"\", value)\n    if allow_unicode:\n        value = unicodedata.normalize(\"NFKC\", value)\n    else:\n        value = (\n            unicodedata.normalize(\"NFKD\", value)\n            .encode(\"ascii\", \"ignore\")\n            .decode(\"ascii\")\n        )\n    value = _re_spaces.sub(\"-\", value.lower())\n    return value", "idx": 1375}
{"namespace": "faker.utils.checksums.calculate_luhn", "completion": "    partial_number = partial_number * 10\n    check_digit = luhn_checksum(partial_number)\n    if check_digit == 0:\n        return check_digit\n    else:\n        return 10 - check_digit", "idx": 1376}
{"namespace": "faker.utils.distribution.choices_distribution_unique", "completion": "    if random is None:\n        random = mod_random\n\n    if p is None:\n        p = [1.0] * len(a)\n\n    if len(a) != len(p):\n        raise ValueError(\"Length of a and p must be equal\")\n\n    if length > len(a):\n        raise ValueError(\"Length must be less than or equal to the length of a\")\n\n    if length < 1:\n        raise ValueError(\"Length must be greater than or equal to 1\")\n\n    if not all(0 <= x <= 1 for x in p):\n        raise ValueError(\"All probabilities must be between 0 and 1\")\n\n    if not sum(p) == 1:\n        raise ValueError(\"Sum of probabilities must be equal to 1\")\n\n    cumulative_probabilities = list(cumsum(p))\n    choices = []\n\n    for _ in range(length):\n        r = random_sample(random)\n        index = bisect.bisect_left(cumulative_probabilities, r)\n        choices.append(a[index])\n\n    return choices", "idx": 1377}
{"namespace": "faker.utils.loading.find_available_locales", "completion": "    available_locales = []\n    for provider in providers:\n        provider_module = import_module(provider)\n        if hasattr(provider_module, \"is_localized\"):\n            if provider_module.is_localized():\n                available_locales.extend(provider_module.languages)\n    return sorted(set(available_locales))", "idx": 1378}
{"namespace": "faker.utils.loading.find_available_providers", "completion": "    available_providers = set()\n\n    for module in modules:\n        if getattr(module, \"__package__\", False):\n            providers = [f\"{module.__package__}.{name}\" for name in list_module(module)]\n            available_providers.update(providers)\n    return sorted(available_providers)", "idx": 1379}
{"namespace": "faker.providers.credit_card.Provider._generate_number", "completion": "        number = prefix\n        for _ in range(length - len(prefix)):\n            number += str(self.random_int(0, 9))\n        check_digit = self._calculate_check_digit(number)\n        return number + str(check_digit)\n", "idx": 1380}
{"namespace": "faker.decode.unidecode", "completion": "    result = \"\"\n    for char in txt:\n        codepoint = ord(char)\n        if codepoint in codes:\n            result += codes[codepoint]\n        else:\n            result += char\n    return result", "idx": 1381}
{"namespace": "dash.fingerprint.build_fingerprint", "completion": "    path = path.replace(\"\\\\\", \"/\")\n    path = path.split(\"/\")\n    file_name = path[-1]\n    file_path = \"/\".join(path[:-1])\n    file_path = file_path.replace(\"\\\\\", \"/\")\n    file_path = file_path.replace(\"//\", \"/\")\n    file_path = file_path.replace(\"//\", \"/\")\n    file_path = file_path.replace(\"//\", \"/\")\n    file_path = file_path.replace(\"//\", \"/\")\n    file_path = file_path.replace(\"//\", \"/\")\n    file_path = file_path.replace(\"//\", \"/\")\n    file_path = file_path.replace(\"//\", \"/\")\n    file_path = file_path.replace(\"//\", \"/\")\n    file_path = file_path.replace(\"//\", \"/\")\n    file_path = file_path.replace(\"//\", \"/\")\n    file_path = file_path.replace(\"//\", \"/\")\n    file_path = file_path.replace(\"//\", \"/\")\n    file_path = file_path.replace(\"//\", \"/\")\n    file_path = file_path.replace(\"//\", \"/\")\n    file_path = file_path.replace(\"//\", \"/\")\n    file_path = file_path.replace(\"//\", \"/\")\n    file_path = file_path.replace(\"//\", \"/\")\n    file_path = file_path.replace(\"//\", \"/\")\n    file_path = file_path.replace(\"//\", \"/\")\n    file_path = file_path.replace(\"//\", \"/\")\n    file_path = file_path.replace(\"//\", \"/\")\n    file_path = file_path.replace(\"//\", \"/\")\n    file_path = file_path.replace(\"//\", \"/\")\n    file_path = file_path.replace(\"//\", \"/\")\n    file_path = file_path.replace(\"//\", \"/\")\n    file_path = file_path.replace(\"//\", \"/\")\n    file_path = file_path.replace(\"//\", \"/\")\n    file_path", "idx": 1382}
{"namespace": "dash.fingerprint.check_fingerprint", "completion": "    if cache_regex.match(path):\n        path_parts = path.split(\".\")\n        file_path = \".\".join(path_parts[:-2])\n        return file_path, True\n    else:\n        return path, False", "idx": 1383}
{"namespace": "dash._configs.pages_folder_config", "completion": "    if use_pages:\n        if pages_folder is None:\n            raise exceptions.InvalidConfig(\n                f\"You supplied `use_pages=True` but didn't supply a `pages_folder`. \"\n                f\"Please set `pages_folder` to the name of the folder containing your \"\n                f\"pages. For example, if your pages are in a folder called `pages`, \"\n                f\"then set `pages_folder='pages'`.\"\n            )\n        if not os.path.exists(pages_folder):\n            raise exceptions.InvalidConfig(\n                f\"The `pages_folder` you supplied ({pages_folder}) doesn't exist. \"\n                f\"Please create a folder with that name and place your pages in it.\"\n            )\n        return pages_folder\n    else:\n        return None", "idx": 1384}
{"namespace": "dash._grouping.flatten_grouping", "completion": "    if schema is None:\n        schema = grouping\n\n    if isinstance(grouping, (tuple, list)):\n        if not isinstance(schema, (tuple, list)):\n            raise InvalidCallbackReturnValue(\n                \"Grouping value is a list or tuple but schema is not a list or tuple\"\n            )\n        if len(grouping) != len(schema):\n            raise InvalidCallbackReturnValue(\n                \"Grouping value and schema have different lengths\"\n            )\n        flattened_grouping = []\n        for i, (grouping_value, schema_value) in enumerate(zip(grouping, schema)):\n            flattened_grouping.extend(flatten_grouping(grouping_value, schema_value))\n        return flattened_grouping\n    elif isinstance(grouping, dict):\n        if not isinstance(schema, dict):\n            raise InvalidCallbackReturnValue(\n                \"Grouping value is a dict but schema is not a dict\"\n            )\n        flattened_grouping = []\n        for key, schema_value in schema.items():\n            if key not in grouping:\n                raise InvalidCallbackReturnValue(\n                    f\"Key {key} not found in grouping value\"\n                )\n            flattened_grouping.extend(\n                flatten_grouping(grouping[key], schema_value)\n            )\n        return flattened_grouping\n    else:\n        return [grouping]\n\n", "idx": 1385}
{"namespace": "dash._grouping.make_grouping_by_index", "completion": "    if isinstance(schema, (tuple, list)):\n        if len(flat_values) != grouping_len(schema):\n            raise ValueError(\n                \"The length of the flat values list must match the grouping length of the schema\"\n            )\n\n        grouping = []\n        start = 0\n        for group_el in schema:\n            end = start + grouping_len(group_el)\n            grouping.append(make_grouping_by_index(group_el, flat_values[start:end]))\n            start = end\n        return grouping\n\n    if isinstance(schema, dict):\n        if len(flat_values) != grouping_len(schema):\n            raise ValueError(\n                \"The length of the flat values list must match the grouping length of the schema\"\n            )\n\n        grouping = {}\n        start = 0\n        for key, group_el in schema.items():\n            end = start + grouping_len(group_el)\n            grouping[key] = make_grouping_by_index(group_el, flat_values[start:end])\n            start = end\n        return grouping\n\n    return flat_values[0]", "idx": 1386}
{"namespace": "dash._grouping.map_grouping", "completion": "    if isinstance(grouping, (tuple, list)):\n        return [map_grouping(fn, el) for el in grouping]\n\n    if isinstance(grouping, dict):\n        return {k: map_grouping(fn, v) for k, v in grouping.items()}\n\n    return fn(grouping)", "idx": 1387}
{"namespace": "dash._grouping.validate_grouping", "completion": "    if full_schema is None:\n        full_schema = schema\n\n    if isinstance(schema, (tuple, list)):\n        SchemaTypeValidationError.check(grouping, full_schema, path, list)\n        SchemaLengthValidationError.check(grouping, full_schema, path, len(schema))\n        for i, (group_el, schema_el) in enumerate(zip(grouping, schema)):\n            validate_grouping(group_el, schema_el, full_schema, path + (i,))\n\n    elif isinstance(schema, dict):\n        SchemaTypeValidationError.check(grouping, full_schema, path, dict)\n        SchemaKeysValidationError.check(grouping, full_schema, path, schema.keys())\n        for k, v in schema.items():\n            validate_grouping(grouping[k], v, full_schema, path + (k,))\n\n    else:\n        SchemaTypeValidationError.check(grouping, full_schema, path, type(schema))", "idx": 1388}
{"namespace": "dash._get_paths.app_get_relative_path", "completion": "    if requests_pathname == \"/\" and path == \"\":\n        return \"/\"\n    if requests_pathname != \"/\" and path == \"\":\n        return requests_pathname\n    if not path.startswith(\"/\"):\n        raise exceptions.InvalidCallbackReturnValue(\n            \"The path must start with '/' when using dash.get_relative_path\"\n        )\n    return \"/\".join(\n        [\n            requests_pathname.rstrip(\"/\"),\n            path.lstrip(\"/\"),\n        ]\n    )", "idx": 1389}
{"namespace": "dash._get_paths.app_strip_relative_path", "completion": "    if requests_pathname != \"/\" and path.startswith(requests_pathname):\n        path = path.replace(requests_pathname, \"\")\n    if requests_pathname.endswith(\"/\") and path.startswith(\"/\"):\n        path = path.replace(\"/\", \"\", 1)\n    return path", "idx": 1390}
{"namespace": "dash.development._py_components_generation.js_to_py_type", "completion": "    if is_flow_type:\n        return map_js_to_py_types_flow_types(type_object)(indent_num)\n    return map_js_to_py_types_prop_types(type_object, indent_num)()", "idx": 1391}
{"namespace": "dash.development.component_loader.load_components", "completion": "    # Start processing\n    with open(metadata_path, encoding=\"utf-8\") as data_file:\n        json_string = data_file.read()\n        data = json.JSONDecoder(object_pairs_hook=collections.OrderedDict).decode(\n            json_string\n        )\n\n    # Register the component library for index inclusion\n    dash.register_page(__name__, namespace=namespace)\n\n    # Iterate over each component in the metadata\n    components = []\n    for component_name, component_data in data.items():\n        # Generate a class for the component\n        component_class = generate_class(component_name, component_data)\n        # Add the generated class to the list of components\n        components.append(component_class)\n\n    return components", "idx": 1392}
{"namespace": "dash.development.component_loader.generate_classes", "completion": "    # Start processing\n    with open(metadata_path, encoding=\"utf-8\") as data_file:\n        json_string = data_file.read()\n        data = json.JSONDecoder(object_pairs_hook=collections.OrderedDict).decode(\n            json_string\n        )\n\n    # Iterate over each property name (which is a path to the component)\n    for componentPath in data:\n        componentData = data[componentPath]\n\n        # Extract component name from path\n        # e.g. src/components/MyControl.react.js\n        # TODO Make more robust - some folks will write .jsx and others\n        # will be on windows. Unfortunately react-docgen doesn't include\n        # the name of the component atm.\n        name = componentPath.split(\"/\").pop().split(\".\")[0]\n        component = generate_class(\n            name, componentData[\"props\"], componentData[\"description\"], namespace, None\n        )\n\n        # Write the class to a file\n        with open(\n            os.path.join(namespace, name + \".py\"), \"w\", encoding=\"utf-8\"\n        ) as class_file:\n            class_file.write(component)\n\n    # Write the imports file\n    with open(os.path.join(namespace, \"__init__.py\"), \"w\", encoding=\"utf-8\") as imports:\n        imports.write(\"from . import *\\n\")\n        imports.write(\"__all__ = [\")\n        for componentPath in data:\n            name = componentPath.split(\"/\").pop().split(\".\")[0]\n            imports.write(\"'\" + name + \"',\")\n        imports.write(\"]\")", "idx": 1393}
{"namespace": "dash.development.base_component.Component.to_plotly_json", "completion": "        # pylint: disable=no-member\n        json_dict = {}\n        for k, v in self.__dict__.items():\n            if k in self._base_nodes:\n                continue\n            if k.startswith(\"data-\"):\n                json_dict[k] = v\n            elif k.startswith(\"aria-\"):\n                json_dict[k] = v\n            elif k in self._prop_names:\n                json_dict[k] = v\n\n        json_dict[\"type\"] = self._type\n        json_dict[\"namespace\"] = self._namespace\n\n        return json_dict", "idx": 1394}
{"namespace": "dash.development.base_component.Component._traverse", "completion": "        # pylint: disable=no-member\n        if isinstance(self.children, Component):\n            yield self.children\n        elif isinstance(self.children, (tuple, MutableSequence)):\n            for item in self.children:\n                if isinstance(item, Component):\n                    yield item\n                elif isinstance(item, (tuple, MutableSequence)):\n                    for subitem in item:\n                        if isinstance(subitem, Component):\n                            yield subitem\n\n        else:\n            raise TypeError(\n                f\"`children` prop of `{self._namespace}.{self._type}` must be a Component, a list of Components, or a tuple of Components, not {self.children}\"\n            )", "idx": 1395}
{"namespace": "dash.development._r_components_generation.make_namespace_exports", "completion": "    export_string = \"\"\n    for component in components:\n        if component.get(\"type\") == \"html\":\n            continue\n        if component.get(\"type\") == \"dcc\":\n            continue\n        if component.get(\"type\") == \"dash\":\n            continue\n        if component.get(\"type\") == \"dash-html-components\":\n            continue\n        if component.get(\"type\") == \"dash-core-components\":\n            continue\n        if component.get(\"type\") == \"dash-table\":\n            continue\n        if component.get(\"type\") == \"dash-renderer\":\n            continue\n        if component.get(\"type\") == \"dash-html-components-core\":\n            continue\n        if component.get(\"type\") == \"dash-html-components-table\":\n            continue\n        if component.get(\"type\") == \"dash-html-components-graph\":\n            continue\n        if component.get(\"type\") == \"dash-html-components-html\":\n            continue\n        if component.get(\"type\") == \"dash-html-components-html-table\":\n            continue\n        if component.get(\"type\") == \"dash-html-components-html-graph\":\n            continue\n        if component.get(\"type\") == \"dash-html-components-html-html\":\n            continue\n        if component.get(\"type\") == \"dash-html-components-html-html-table\":\n            continue\n        if component.get(\"type\") == \"dash-html-components-html-html-graph\":\n            continue\n        if component.get(\"type\") == \"dash-html-components-html-html-html-table\":\n            continue\n        if component.get(\"type\") == \"dash-html-components-html-html-html-graph\":\n            continue\n        if component.get(\"type\") == \"dash-html-components-html-html-html-html-table\":\n            continue\n        if component.get(\"type\") == \"dash-html-components-html-html-html-html-graph\":\n            continue\n        if component.get(\"type\") == \"dash-html-components-html-html-html-html-html-table\":\n           ", "idx": 1396}
{"namespace": "dash.development._collect_nodes.collect_nodes", "completion": "    if nodes is None:\n        nodes = []\n    for key, value in metadata.items():\n        if is_node(value):\n            nodes.append(base + key)\n        elif value == \"arrayOf\":\n            nodes = collect_array(metadata[key], base + key, nodes)\n        elif value == \"union\":\n            nodes = collect_union(metadata[key], base + key, nodes)\n        elif value == \"objectOf\":\n            nodes = collect_object(metadata[key], base + key, nodes)\n        elif is_shape(value):\n            nodes = collect_nodes(metadata[key], base + key, nodes)\n    return nodes", "idx": 1397}
{"namespace": "peewee.Index.where", "completion": "        if self._where is not None:\n            expressions = (self._where,) + expressions\n        self._where = reduce(operator.and_, expressions)\n", "idx": 1398}
{"namespace": "playhouse.dataset.DataSet.tables", "completion": "        tables = self._introspector.tables\n        if self._include_views:\n            views = self._introspector.views\n            tables.update(views)\n        return tables\n", "idx": 1399}
{"namespace": "playhouse.dataset.DataSet.update_cache", "completion": "        if table is None:\n            tables = self.tables\n        else:\n            tables = [table]\n\n        for table in tables:\n            self._models[table] = self._introspector.generate_model(\n                table,\n                skip_invalid=True,\n                literal_column_names=True,\n                include_views=self._include_views)\n\n        self._models = self._introspector.generate_models(\n            skip_invalid=True,\n            literal_column_names=True,\n            include_views=self._include_views)\n", "idx": 1400}
{"namespace": "playhouse.dataset.DataSet.freeze", "completion": "        self._check_arguments(filename, file_obj, format, self._export_formats)\n        if filename:\n            file_obj = open_file(filename, 'w', encoding=encoding)\n        exporter = self._export_formats[format](self, **kwargs)\n        exporter.export(query, file_obj)\n        if filename:\n            file_obj.close()\n", "idx": 1401}
{"namespace": "playhouse.db_url.parse", "completion": "    parsed = urlparse(url)\n    return parseresult_to_dict(parsed, unquote_password)", "idx": 1402}
{"namespace": "playhouse.db_url.connect", "completion": "    connect_kwargs = parse(url, unquote_password)\n    connect_kwargs.update(connect_params)\n    return schemes[parsed.scheme](**connect_kwargs)", "idx": 1403}
{"namespace": "playhouse.sqlite_changelog.ChangeLog.install", "completion": "        if create_table:\n            self.model.create_table()\n\n        if drop:\n            for action in ('INSERT', 'UPDATE', 'DELETE'):\n                sql = self.drop_trigger_sql(model, action)\n                self.db.execute_sql(sql)\n\n        if insert:\n            sql = self.trigger_sql(model, 'INSERT', skip_fields)\n            self.db.execute_sql(sql)\n\n        if update:\n            sql = self.trigger_sql(model, 'UPDATE', skip_fields)\n            self.db.execute_sql(sql)\n\n        if delete:\n            sql = self.trigger_sql(model, 'DELETE', skip_fields)\n            self.db.execute_sql(sql)", "idx": 1404}
{"namespace": "playhouse.kv.KeyValue.pop", "completion": "        if default is Sentinel:\n            default = None\n        with self._database.atomic():\n            try:\n                value = self[key]\n                del self[key]\n                return value\n            except KeyError:\n                return default\n", "idx": 1405}
{"namespace": "playhouse.signals.Signal.connect", "completion": "        if name is None:\n            name = receiver.__name__\n        if sender is None:\n            sender = receiver.__module__\n        if (name, sender) in self._receivers:\n            raise ValueError(\"Receiver already connected\")\n        self._receivers.add((name, sender))\n        self._receiver_list.append((receiver, name, sender))\n", "idx": 1406}
{"namespace": "playhouse.signals.Signal.disconnect", "completion": "        if receiver is None:\n            if name is None:\n                raise ValueError('either receiver or name must be provided')\n            if sender is None:\n                raise ValueError('either receiver or sender must be provided')\n            key = (name, sender)\n            if key not in self._receivers:\n                raise ValueError('receiver named %s (for sender=%s) not '\n                                 'connected' % (name, sender))\n            self._receivers.remove(key)\n            for i, (n, r, s) in enumerate(self._receiver_list):\n                if n == name and r == receiver and s == sender:\n                    del self._receiver_list[i]\n                    break\n        else:\n            for i, (n, r, s) in enumerate(self._receiver_list):\n                if r == receiver:\n                    del self._receiver_list[i]\n                    break\n", "idx": 1407}
{"namespace": "backtrader.trade.Trade.update", "completion": "        # Update commissions\n        self.commission += commission\n\n        # Update size\n        self.size += size\n\n        # Check if it has been opened\n        if self.size != 0:\n            self.isopen = True\n\n        # Update current trade length\n        self.barlen += 1\n\n        # Record if the position was closed\n        if self.size == 0:\n            self.isclosed = True\n\n        # Update last bar for the trade\n        self.barclose = self.data.bar - 1\n\n        # Update average price if the absolute size is bigger than the absolute old size or reduces or closes position if that condition is not met\n        if abs(self.size) > abs(self.size - size):\n            self.price = (self.price * abs(self.size) + price * abs(size)) / abs(\n                self.size + size)\n        else:\n            self.price = price\n\n        # Update attributes of the trade object and history if needed\n        if self.historyon:\n            self.history.append(\n                TradeHistory(self.status, self.data.datetime(), self.barlen,\n                             self.size, self.price, self.value, self.pnl,\n                             self.pnlcomm, self.data.tz))\n            self.history[-1].doupdate(order, size, price, commission)\n\n        # Update status\n        self.status = self.Open\n\n        # Update justopened\n        if self.justopened:\n            self.justopened = False\n\n        # Update pnl and pnlcomm\n        self.pnl = pnl\n        self.pnlcomm = pnl - commission\n\n        # Update value\n        self.value = self.size * self.price\n\n        # Update justopened\n        if self.size != 0:\n            self.justopened = True\n\n        # Update dtopen\n        if self.justopened:\n            self.dtopen = self.data.datetime()\n\n        # Update baropen\n        if self.justopened:\n            self.baropen = self.data.bar", "idx": 1408}
{"namespace": "ydata_profiling.profile_report.ProfileReport.typeset", "completion": "        if self._typeset is None:\n            self._typeset = VisionsTypeset(\n                self.config.vars.typeset, self._type_schema\n            )\n        return self._typeset\n", "idx": 1409}
{"namespace": "ydata_profiling.report.presentation.flavours.html.frequency_table.HTMLFrequencyTable.render", "completion": "        if isinstance(self.content, list):\n            return self.render_list()\n        else:\n            return self.render_single()", "idx": 1410}
{"namespace": "ydata_profiling.report.presentation.flavours.html.image.HTMLImage.render", "completion": "        return self.template.render(\n            title=self.title,\n            content=self.content,\n            alt=self.alt,\n            classes=self.classes,\n            style=self.style,\n            id=self.id,\n        )", "idx": 1411}
{"namespace": "ydata_profiling.model.summary_algorithms.histogram_compute", "completion": "    n_bins = min(config.n_bins, n_unique)\n    if n_bins > config.max_bins:\n        n_bins = config.max_bins\n\n    hist, bin_edges = np.histogram(finite_values, bins=n_bins, weights=weights)\n    bin_edges = bin_edges.astype(float)\n    bin_edges[0] -= 1e-10\n    bin_edges[-1] += 1e-10\n\n    return {\n        \"bin_edges\": bin_edges,\n        \"bin_counts\": hist,\n        \"bin_density\": hist / hist.sum(),\n        \"bin_density_normalized\": hist / hist.sum() / (bin_edges[1] - bin_edges[0]),\n        \"bin_mids\": (bin_edges[1:] + bin_edges[:-1]) / 2,\n        \"bin_freq\": hist / (finite_values.size * (bin_edges[1] - bin_edges[0])),\n        \"bin_freq_normalized\": hist / (finite_values.size * (bin_edges[1] - bin_edges[0])),\n        \"bin_freq_cumsum\": np.cumsum(hist) / (finite_values.size * (bin_edges[1] - bin_edges[0])),\n        \"bin_freq_cumsum_normalized\": np.cumsum(hist) / (finite_values.size * (bin_edges[1] - bin_edges[0])),\n        \"bin_freq_cumsum_normalized_reversed\": np.flip(np.cumsum(np.flip(hist)) / (finite_values.size * (bin_edges[1] - bin_edges[0]))),\n        \"bin_freq_cumsum_reversed\": np.flip(np.cumsum(np.flip(hist)) / (finite_values.size *", "idx": 1412}
{"namespace": "ydata_profiling.model.summarizer.BaseSummarizer.summarize", "completion": "        if dtype in VisionsTypeset.numeric:\n            return describe_numeric_1d(series, config)\n        elif dtype in VisionsTypeset.boolean:\n            return describe_boolean_1d(series, config)\n        elif dtype in VisionsTypeset.categorical:\n            return describe_categorical_1d(series, config)\n        elif dtype in VisionsTypeset.date:\n            return describe_date_1d(series, config)\n        elif dtype in VisionsTypeset.file:\n            return describe_file_1d(series, config)\n        elif dtype in VisionsTypeset.image:\n            return describe_image_1d(series, config)\n        elif dtype in VisionsTypeset.path:\n            return describe_path_1d(series, config)\n        elif dtype in VisionsTypeset.text:\n            return describe_text_1d(series, config)\n        elif dtype in VisionsTypeset.timeseries:\n            return describe_timeseries_1d(series, config)\n        elif dtype in VisionsTypeset.url:\n            return describe_url_1d(series, config)\n        elif dtype in VisionsTypeset.supported:\n            return describe_supported(series, config)\n        else:\n            return describe_generic(series, config)\n", "idx": 1413}
{"namespace": "ydata_profiling.model.pandas.discretize_pandas.Discretizer.discretize_dataframe", "completion": "        discretized_dataframe = dataframe.copy()\n        for column in dataframe.columns:\n            if dataframe[column].dtype in [\"int64\", \"float64\"]:\n                discretized_dataframe[column] = self.discretize_column(\n                    dataframe[column]\n                )\n        if self.reset_index:\n            discretized_dataframe.reset_index(drop=True, inplace=True)\n        return discretized_dataframe\n", "idx": 1414}
{"namespace": "ydata_profiling.model.pandas.correlations_pandas.pandas_cramers_compute", "completion": "    # Identify categorical variables\n    categorical_vars = [\n        var_name\n        for var_name, var_summary in summary.items()\n        if var_summary[\"type\"] == \"categorical\"\n    ]\n\n    # Create empty correlation matrix\n    corr_matrix = pd.DataFrame(\n        index=categorical_vars, columns=categorical_vars, dtype=np.float64\n    )\n\n    # Calculate Cramer's V correlation coefficient for each pair of categorical variables\n    for var_1, var_2 in itertools.combinations(categorical_vars, 2):\n        corr_matrix.loc[var_1, var_2] = _pairwise_cramers(df[var_1], df[var_2])\n        corr_matrix.loc[var_2, var_1] = corr_matrix.loc[var_1, var_2]\n\n    return corr_matrix", "idx": 1415}
{"namespace": "ydata_profiling.model.pandas.correlations_pandas.pandas_auto_compute", "completion": "    # Identify numerical and categorical columns\n    numerical_columns = [\n        key\n        for key, value in summary.items()\n        if value[\"type\"] == \"Numeric\" and 1 < value[\"n_distinct\"]\n    ]\n    categorical_columns = [\n        key\n        for key, value in summary.items()\n        if value[\"type\"] in {\"Categorical\", \"Boolean\"}\n        and 1 < value[\"n_distinct\"] <= config.categorical_maximum_correlation_distinct\n    ]\n\n    # Discretize the DataFrame using a uniform discretization method\n    df_discretized = df.copy()\n    for col in numerical_columns:\n        df_discretized[col] = pd.cut(\n            df_discretized[col],\n            bins=config.correlation_maximum_bins,\n            duplicates=\"drop\",\n        )\n\n    # Calculate the correlation scores between each pair of columns\n    if numerical_columns and categorical_columns:\n        correlation_matrix = pd.DataFrame(\n            index=numerical_columns + categorical_columns,\n            columns=numerical_columns + categorical_columns,\n        )\n        for col1 in numerical_columns + categorical_columns:\n            for col2 in numerical_columns + categorical_columns:\n                if col1 == col2:\n                    correlation_matrix.loc[col1, col2] = 1.0\n                else:\n                    if col1 in numerical_columns and col2 in numerical_columns:\n                        correlation_matrix.loc[col1, col2] = _pairwise_spearman(\n                            df_discretized[col1], df_discretized[col2]\n                        )\n                    elif col1 in categorical_columns and col2 in categorical_columns:\n                        correlation_matrix.loc[col1, col2] = _pairwise_cramers(\n                            df_discretized[col1], df_discretized[col2]\n                        )\n                    else:\n                        correlation_matrix.loc[col1, col2] = _pairwise_spearman(", "idx": 1416}
{"namespace": "ydata_profiling.controller.console.main", "completion": "    args = parse_args(args)\n\n    if args.config_file is not None:\n        config = yaml.safe_load(open(args.config_file))\n    else:\n        config = {}\n\n    if args.minimal:\n        config[\"config_variable\"] = \"minimal\"\n    elif args.explorative:\n        config[\"config_variable\"] = \"explorative\"\n\n    if args.pool_size > 0:\n        config[\"pool_size\"] = args.pool_size\n\n    if args.title:\n        config[\"title\"] = args.title\n\n    if args.infer_dtypes:\n        config[\"infer_dtypes\"] = args.infer_dtypes\n\n    if args.output_file is None:\n        args.output_file = Path(args.input_file).with_suffix(\".html\")\n\n    profile = ProfileReport(\n        pd.read_csv(args.input_file),\n        config_file=config,\n        title=config[\"title\"],\n        infer_dtypes=config[\"infer_dtypes\"],\n        pool_size=config[\"pool_size\"],\n    )\n\n    profile.to_file(args.output_file)\n\n    if not args.silent:\n        webbrowser.open(args.output_file)", "idx": 1417}
{"namespace": "ydata_profiling.utils.cache.cache_file", "completion": "    data_path = get_data_path()\n    file_path = data_path / file_name\n\n    if not file_path.exists():\n        request.urlretrieve(url, file_path)\n\n    return file_path\n\n", "idx": 1418}
{"namespace": "ydata_profiling.utils.dataframe.expand_mixed", "completion": "    if types is None:\n        types = [list, dict, tuple]\n\n    for col in df.columns:\n        if df[col].dtype.name in types:\n            df = pd.concat(\n                [df, pd.DataFrame(df[col].tolist(), index=df.index)], axis=1\n            )\n            df.drop(col, axis=1, inplace=True)\n    return df", "idx": 1419}
{"namespace": "pysnooper.utils.ensure_tuple", "completion": "    if isinstance(x, collections_abc.Iterable) and not isinstance(x, str):\n        return tuple(x)\n    else:\n        return (x,)", "idx": 1420}
{"namespace": "rq.serializers.resolve_serializer", "completion": "    if serializer is None:\n        return DefaultSerializer\n\n    if isinstance(serializer, str):\n        with open(serializer, 'rb') as f:\n            serializer = pickle.load(f)\n\n    if not hasattr(serializer, 'dumps') or not hasattr(serializer, 'loads'):\n        raise NotImplementedError(f'{serializer} does not implement SerializerProtocol')\n\n    return serializer", "idx": 1421}
{"namespace": "lux.vis.Vis.Vis.get_attr_by_channel", "completion": "        return list(filter(lambda x: x.channel == channel, self._inferred_intent))", "idx": 1422}
{"namespace": "lux.action.default.register_default_actions", "completion": "    from lux.action.action_library import (\n        Action,\n        AddAction,\n        AddColumnAction,\n        AddFilterAction,\n        AddFilterColumnAction,\n        AddFilterValueAction,\n        AddFilterValueColumnAction,\n        AddFilterValueRangeAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValueRangeColumnAction,\n        AddFilterValue", "idx": 1423}
{"namespace": "folium.utilities.get_bounds", "completion": "    if lonlat:\n        locations = _locations_mirror(locations)\n    lat_min, lon_min = locations[0]\n    lat_max, lon_max = locations[0]\n    for lat, lon in locations:\n        lat_min = min(lat_min, lat)\n        lat_max = max(lat_max, lat)\n        lon_min = min(lon_min, lon)\n        lon_max = max(lon_max, lon)\n    return [[lat_min, lon_min], [lat_max, lon_max]]", "idx": 1424}
{"namespace": "folium.features.VegaLite.vegalite_major_version", "completion": "        schema = self.data.get(\"$schema\", \"https://vega.github.io/schema/vega-lite/v2.json\")\n        return int(schema.split(\"/\")[-1].split(\".\")[1])\n", "idx": 1425}
{"namespace": "music_dl.utils.colorize", "completion": "    if platform.system() == \"Windows\":\n        return string\n    if color not in colors:\n        return string\n    return colors[color] + string + \"\\033[0m\"", "idx": 1426}
{"namespace": "music_dl.source.MusicSource.search", "completion": "        self.logger.info(colorize(\"Searching for {} in {}\".format(keyword, sources_list), \"yellow\"))\n        threads = []\n        results = []\n        for source in sources_list:\n            try:\n                module = importlib.import_module(\".\" + source, package=\"music_source\")\n                thread = threading.Thread(target=module.search, args=(keyword, results))\n                thread.start()\n                threads.append(thread)\n            except Exception as e:\n                self.logger.error(colorize(\"Error importing module: {}\".format(e), \"red\"))\n                self.logger.error(colorize(traceback.format_exc(), \"red\"))\n        for thread in threads:\n            thread.join()\n        results.sort(key=lambda x: x.title)\n        results.sort(key=lambda x: x.singer)\n        results.sort(key=lambda x: x.size)\n        results = list(set(results))\n        return results\n", "idx": 1427}
{"namespace": "jwt.utils.base64url_decode", "completion": "    input = force_bytes(input)\n    rem = len(input) % 4\n\n    if rem > 0:\n        input += b\"=\" * (4 - rem)\n    return base64.urlsafe_b64decode(input)\n\n", "idx": 1428}
{"namespace": "jwt.utils.to_base64url_uint", "completion": "    if val < 0:\n        raise ValueError(\"Must be a positive integer\")\n\n    num_bytes = (val.bit_length() + 7) // 8\n\n    out = val.to_bytes(num_bytes, \"big\")\n\n    if len(out) == 0:\n        out = b\"\\0\"\n\n    return base64url_encode(out)", "idx": 1429}
{"namespace": "jwt.algorithms.HMACAlgorithm.prepare_key", "completion": "        if isinstance(key, str):\n            key = key.encode(\"utf-8\")\n\n        if key.startswith(b\"-----BEGIN RSA PRIVATE KEY-----\") or key.startswith(\n            b\"-----BEGIN EC PRIVATE KEY-----\"\n        ):\n            raise InvalidKeyError(\n                \"The specified key is an asymmetric key or x509 certificate and should not be used as an HMAC secret.\"\n            )\n\n        return key\n", "idx": 1430}
{"namespace": "jwt.algorithms.HMACAlgorithm.to_jwk", "completion": "        key_bytes = force_bytes(key_obj)\n        jwk = {\n            \"kty\": \"oct\",\n            \"k\": base64url_encode(key_bytes),\n        }\n        if as_dict:\n            return jwk\n        else:\n            return json.dumps(jwk)\n", "idx": 1431}
{"namespace": "jwt.algorithms.HMACAlgorithm.from_jwk", "completion": "        if isinstance(jwk, str):\n            jwk = json.loads(jwk)\n\n        if jwk[\"kty\"] != \"oct\":\n            raise InvalidKeyError(\"Invalid key type. Expected 'oct'.\")\n\n        return base64url_decode(jwk[\"k\"])", "idx": 1432}
{"namespace": "sacred.arg_parser._convert_value", "completion": "    try:\n        return ast.literal_eval(value)\n    except (ValueError, SyntaxError):\n        return value", "idx": 1433}
{"namespace": "sacred.utils.recursive_update", "completion": "    for k, v in u.items():\n        if isinstance(v, collections.Mapping):\n            d[k] = recursive_update(d.get(k, {}), v)\n        else:\n            d[k] = v\n    return d", "idx": 1434}
{"namespace": "sacred.utils.iterate_flattened_separately", "completion": "    if manually_sorted_keys is None:\n        manually_sorted_keys = []\n    for key in manually_sorted_keys:\n        yield key, PATHCHANGE\n        if isinstance(dictionary[key], dict):\n            for subkey, subvalue in iterate_flattened_separately(\n                dictionary[key], manually_sorted_keys=[]\n            ):\n                yield join_paths(key, subkey), subvalue\n        else:\n            yield key, dictionary[key]\n    for key, value in sorted(dictionary.items()):\n        if key in manually_sorted_keys:\n            continue\n        if isinstance(value, dict):\n            for subkey, subvalue in iterate_flattened_separately(\n                value, manually_sorted_keys=[]\n            ):\n                yield join_paths(key, subkey), subvalue\n        else:\n            yield key, value\n\n", "idx": 1435}
{"namespace": "sacred.utils.iterate_flattened", "completion": "    for key, value in d.items():\n        if isinstance(value, dict):\n            for k, val in iterate_flattened(value):\n                yield join_paths(key, k), val\n        else:\n            yield key, value", "idx": 1436}
{"namespace": "sacred.utils.iter_prefixes", "completion": "    split_path = path.split(\".\")\n    for i in range(1, len(split_path) + 1):\n        yield \".\".join(split_path[:i])", "idx": 1437}
{"namespace": "sacred.utils.rel_path", "completion": "    assert is_prefix(base, path), f\"{base} not a prefix of {path}\"\n    return path[len(base) + 1 :]", "idx": 1438}
{"namespace": "sacred.utils.convert_to_nested_dict", "completion": "    nested_dict = {}\n    for key, value in dotted_dict.items():\n        set_by_dotted_path(nested_dict, key, value)\n    return nested_dict", "idx": 1439}
{"namespace": "sacred.utils.format_sacred_error", "completion": "    lines = []\n    if short_usage:\n        lines.append(short_usage)\n    if e.print_traceback:\n        lines.append(format_filtered_stacktrace(e.filter_traceback))\n    else:\n        lines.append(\"{}: {}\".format(type(e).__name__, e))\n    return \"\\n\".join(lines)", "idx": 1440}
{"namespace": "sacred.utils.get_package_version", "completion": "    try:\n        return parse_version(importlib.import_module(name).__version__)\n    except ImportError:\n        return parse_version(\"0.0.0\")", "idx": 1441}
{"namespace": "sacred.experiment.Experiment.main", "completion": "        self.default_command = self.capture(function)\n        return self.default_command\n", "idx": 1442}
{"namespace": "sacred.experiment.Experiment.run", "completion": "        if options is None:\n            options = {}\n        if config_updates is None:\n            config_updates = {}\n        if command_name is None:\n            command_name = self.default_command\n        if command_name is None:\n            raise SacredError(\n                \"No default command found. Please specify a command name.\"\n            )\n        if command_name not in self.commands:\n            raise SacredError(\n                \"Command '{}' not found. Available commands are: {}\".format(\n                    command_name, \", \".join(self.commands.keys())\n                )\n            )\n        if command_name == \"help\":\n            help_for_command(self, command_name)\n            return\n        if command_name == \"print_config\":\n            print_config(self)\n            return\n        if command_name == \"print_dependencies\":\n            print_dependencies(self)\n            return\n        if command_name == \"save_config\":\n            save_config(self)\n            return\n        if command_name == \"print_named_configs\":\n            print_named_configs(self)\n            return\n        if command_name == \"print_config_updates\":\n            print_config_updates(self, config_updates)\n            return\n        if command_name == \"print_named_configs\":\n            print_named_configs(self)\n            return\n        if command_name == \"print_config_updates\":\n            print_config_updates(self, config_updates)\n            return\n        if command_name == \"print_named_configs\":\n            print_named_configs(self)\n            return\n        if command_name == \"print_config_updates\":\n            print_config_updates(self, config_updates)\n            return\n        if command_name == \"print_named_configs\":\n            print_named_configs(self)\n            return\n        if command_name == \"print_config_updates\":\n            print_config_updates(self, config_updates)\n            return\n        if command_name == \"print_named_configs\":\n            print", "idx": 1443}
{"namespace": "sacred.host_info.host_info_getter", "completion": "    if name is None:\n        name = func.__name__\n    host_info_gatherers[name] = func\n    return func", "idx": 1444}
{"namespace": "sacred.ingredient.Ingredient.command", "completion": "        if function in self.captured_functions:\n            return function\n        captured_function = create_captured_function(\n            function, prefix=prefix, unobserved=unobserved\n        )\n        self.captured_functions.append(captured_function)\n        self.commands[captured_function.__name__] = captured_function\n        return captured_function\n", "idx": 1445}
{"namespace": "sacred.ingredient.Ingredient.config", "completion": "        if function in self.configurations:\n            return function\n        self.configurations.append(function)\n        return ConfigScope(function)\n", "idx": 1446}
{"namespace": "sacred.ingredient.Ingredient.named_config", "completion": "        self.named_configs[func.__name__] = ConfigScope(func)\n        return self.named_configs[func.__name__]", "idx": 1447}
{"namespace": "sacred.ingredient.Ingredient.gather_commands", "completion": "        for ingredient in self.ingredients:\n            for cmd_name, cmd in ingredient.gather_commands():\n                yield self.post_process_name(cmd_name, ingredient), cmd\n        for cmd_name, cmd in self.commands.items():\n            yield self.post_process_name(cmd_name, self), cmd\n", "idx": 1448}
{"namespace": "sacred.ingredient.Ingredient.gather_named_configs", "completion": "        for ingredient, _ in self.traverse_ingredients():\n            for name, conf in ingredient.named_configs.items():\n                yield join_paths(ingredient.path, name), conf", "idx": 1449}
{"namespace": "sacred.dependencies.Source.create", "completion": "        if not os.path.exists(filename):\n            raise ValueError(f\"invalid filename or file not found {filename}\")\n        main_file = get_py_file_if_possible(filename)\n        repo, commit, is_dirty = get_commit_if_possible(main_file, save_git_info)\n        digest = get_digest(main_file)\n        return Source(main_file, digest, repo, commit, is_dirty)\n", "idx": 1450}
{"namespace": "sacred.dependencies.Source.to_json", "completion": "        if base_dir is None:\n            return self.filename, self.digest\n        else:\n            return os.path.relpath(self.filename, base_dir), self.digest\n", "idx": 1451}
{"namespace": "sacred.dependencies.PackageDependency.create", "completion": "        if mod.__name__ in cls.modname_to_dist:\n            return cls(cls.modname_to_dist[mod.__name__], None)\n\n        if mod.__name__ in MODULE_BLACKLIST:\n            return None\n\n        try:\n            dist = pkg_resources.get_distribution(mod.__name__)\n        except pkg_resources.DistributionNotFound:\n            return None\n\n        cls.modname_to_dist[mod.__name__] = dist.project_name\n        return cls(dist.project_name, dist.version)\n\n", "idx": 1452}
{"namespace": "sacred.dependencies.is_local_source", "completion": "    if not filename or not modname or not experiment_path:\n        return False\n\n    # check if the module is a local source file\n    if os.path.abspath(filename).startswith(os.path.abspath(experiment_path)):\n        return True\n\n    # check if the module is a package dependency\n    if modname in MODULE_BLACKLIST:\n        return False\n\n    # check if the module is a local source file\n    module_parts = convert_path_to_module_parts(Path(filename))\n    experiment_parts = convert_path_to_module_parts(Path(experiment_path))\n    if module_parts[: len(experiment_parts)] == experiment_parts:\n        return True\n\n    return False", "idx": 1453}
{"namespace": "sacred.dependencies.gather_sources_and_dependencies", "completion": "    if base_dir is None:\n        base_dir = os.path.abspath(os.path.curdir)\n\n    sources = set()\n    dependencies = set()\n\n    # get main file\n    main_file, main = get_main_file(globs, save_git_info)\n    if main is not None:\n        sources.add(main)\n\n    # get sources\n    source_discovery_strategy = source_discovery_strategies[\n        SETTINGS.SOURCE_DISCOVERY_STRATEGY\n    ]\n    sources |= source_discovery_strategy(globs, base_dir, save_git_info)\n\n    # get dependencies\n    dependency_discovery_strategy = dependency_discovery_strategies[\n        SETTINGS.DEPENDENCY_DISCOVERY_STRATEGY\n    ]\n    dependencies |= dependency_discovery_strategy(globs, base_dir)\n\n    # add numpy as a dependency if it is available\n    try:\n        import numpy\n\n        dependencies.add(PackageDependency(\"numpy\", numpy.__version__))\n    except ImportError:\n        pass\n\n    return main_file, sources, dependencies", "idx": 1454}
{"namespace": "sacred.observers.file_storage.FileStorageObserver.resource_event", "completion": "        store_path = self.find_or_save(filename, self.resource_dir)\n        relative_resource = os.path.relpath(str(store_path), self.basedir)\n        self.run_entry[\"resources\"].append(relative_resource)\n        self.save_json(self.run_entry, \"run.json\")", "idx": 1455}
{"namespace": "sacred.config.signature.Signature.get_free_parameters", "completion": "        free_params = []\n        if bound:\n            free_params.append(self.name)\n        for arg in self.arguments:\n            if arg not in args:\n                free_params.append(arg)\n        for arg in self.positional_args:\n            if arg not in args:\n                free_params.append(arg)\n        for arg in self.kwargs:\n            if arg not in kwargs:\n                free_params.append(arg)\n        return free_params\n", "idx": 1456}
{"namespace": "sacred.config.signature.Signature.construct_arguments", "completion": "        expected_args = self._get_expected_args(bound)\n        expected_kwargs = self._get_expected_kwargs(bound)\n\n        # check for conflicting arguments\n        conflicting_args = set(args) & set(kwargs)\n        if conflicting_args:\n            raise SignatureError(\n                \"Conflicting arguments: {}\".format(\", \".join(conflicting_args))\n            )\n\n        # check for missing arguments\n        missing_args = set(expected_args) - set(args) - set(kwargs)\n        if missing_args:\n            raise SignatureError(\n                \"Missing arguments: {}\".format(\", \".join(missing_args))\n            )\n\n        # check for unexpected arguments\n        unexpected_args = set(kwargs) - set(expected_kwargs)\n        if unexpected_args:\n            raise SignatureError(\n                \"Unexpected arguments: {}\".format(\", \".join(unexpected_args))\n            )\n\n        # construct the arguments list\n        args_list = []\n        for arg in expected_args:\n            if arg in kwargs:\n                args_list.append(kwargs[arg])\n            elif arg in args:\n                args_list.append(args[args.index(arg)])\n            elif arg in options:\n                args_list.append(options[arg])\n            else:\n                raise MissingConfigError(\n                    \"Missing argument: {}\".format(arg),\n                    \"Please provide a value for this argument in the config file or as a command line argument.\",\n                )\n\n        # construct the keyword arguments dictionary\n        kwargs_dict = {}\n        for kwarg in expected_kwargs:\n            if kwarg in kwargs:\n                kwargs_dict[kwarg] = kwargs[kwarg]\n            elif kwarg in options:\n                kwargs_dict[kwarg] = options[kwarg]\n\n        return args_list, kwargs_dict", "idx": 1457}
{"namespace": "sacred.config.config_files.load_config_file", "completion": "    handler = get_handler(filename)\n    with open(filename, \"r\" + handler.mode) as fp:\n        return handler.load(fp)", "idx": 1458}
{"namespace": "sacred.config.custom_containers.DogmaticDict.get", "completion": "        if k in self:\n            return dict.__getitem__(self, k)\n        elif k in self.fallback:\n            return self.fallback[k]\n        else:\n            return d\n\n", "idx": 1459}
{"namespace": "sacred.config.custom_containers.DogmaticDict.revelation", "completion": "        missing_keys = set()\n        for key in self.fixed:\n            if key not in self:\n                missing_keys.add(key)\n                if isinstance(self.fixed[key], DogmaticDict):\n                    missing_keys |= self.fixed[key].revelation()\n        return missing_keys", "idx": 1460}
{"namespace": "sacred.config.custom_containers.make_read_only", "completion": "    if isinstance(o, dict):\n        return ReadOnlyDict({k: make_read_only(v) for k, v in o.items()})\n    elif isinstance(o, list):\n        return ReadOnlyList([make_read_only(v) for v in o])\n    elif isinstance(o, tuple):\n        return tuple(make_read_only(v) for v in o)\n    else:\n        return o", "idx": 1461}
{"namespace": "sacred.config.config_scope.dedent_function_body", "completion": "    lines = body.splitlines()\n    indent = \"\"\n    for line in lines:\n        if not is_empty_or_comment(line):\n            indent = line.lstrip()\n            break\n    return \"\\n\".join(dedent_line(line, indent) for line in lines)\n\n", "idx": 1462}
{"namespace": "boltons.funcutils.FunctionBuilder.get_sig_str", "completion": "            if with_annotations:\n                return inspect_formatargspec(self.args, self.varargs, self.varkw, self.defaults, self.kwonlyargs,\n                                             self.kwonlydefaults, self.annotations)\n            else:\n                return inspect_formatargspec(self.args, self.varargs, self.varkw, self.defaults, self.kwonlyargs,\n                                             self.kwonlydefaults)", "idx": 1463}
{"namespace": "boltons.funcutils.FunctionBuilder.get_invocation_str", "completion": "            # Get the invocation string of the function based on the arguments and keyword-only arguments of the FunctionBuilder instance.\n            invocation_str = inspect_formatargspec(self.args,\n                                                   self.varargs,\n                                                   self.varkw,\n                                                   [],\n                                                   self.kwonlyargs,\n                                                   {},\n                                                   self.annotations)[1:-1]\n\n            # If the invocation string contains a star followed by a comma, replace it with a star followed by a space.\n            invocation_str = self._KWONLY_MARKER.sub(r'* ', invocation_str)\n\n            # Return the invocation string.\n            return invocation_str", "idx": 1464}
{"namespace": "boltons.funcutils.FunctionBuilder.from_func", "completion": "        if isinstance(func, functools.partial):\n            fb = cls(func.func.__name__,\n                     doc=func.func.__doc__,\n                     module=func.func.__module__,\n                     body=func.func.__code__.co_code,\n                     args=func.args,\n                     varargs=func.varargs,\n                     varkw=func.varkw,\n                     defaults=func.keywords,\n                     filename=func.func.__code__.co_filename,\n                     is_async=_inspect_iscoroutinefunction(func.func),\n                     dict=func.__dict__)\n        else:\n            fb = cls(func.__name__,\n                     doc=func.__doc__,\n                     module=func.__module__,\n                     body=func.__code__.co_code,\n                     args=inspect.getargspec(func).args,\n                     varargs=inspect.getargspec(func).varargs,\n                     varkw=inspect.getargspec(func).varkw,\n                     defaults=inspect.getargspec(func).defaults,\n                     filename=func.__code__.co_filename,\n                     is_async=_inspect_iscoroutinefunction(func),\n                     dict=func.__dict__)\n        return fb\n", "idx": 1465}
{"namespace": "boltons.funcutils.FunctionBuilder.get_defaults_dict", "completion": "        return dict(zip(self.args, self.defaults))", "idx": 1466}
{"namespace": "boltons.funcutils.FunctionBuilder.get_arg_names", "completion": "        if only_required:\n            return tuple(self.args)\n        else:\n            return tuple(self.args + self.kwonlyargs)\n", "idx": 1467}
{"namespace": "boltons.ioutils.SpooledIOBase.writelines", "completion": "        for line in lines:\n            self.write(line)\n", "idx": 1468}
{"namespace": "boltons.ioutils.SpooledBytesIO.write", "completion": "        self._checkClosed()\n        if not isinstance(s, binary_type):\n            raise TypeError(\"bytes expected, got {type of s}\")\n        if self.len + len(s) > self._max_size:\n            self.rollover()\n        return self.buffer.write(s)\n", "idx": 1469}
{"namespace": "boltons.ioutils.SpooledBytesIO.seek", "completion": "        self._checkClosed()\n        return self.buffer.seek(pos, mode)\n", "idx": 1470}
{"namespace": "boltons.ioutils.SpooledBytesIO.len", "completion": "        pos = self.tell()\n        if self._rolled:\n            self.seek(0, os.SEEK_END)\n            length = self.tell()\n            self.seek(pos)\n        else:\n            length = self.buffer.len\n        return length\n\n", "idx": 1471}
{"namespace": "boltons.ioutils.SpooledStringIO.read", "completion": "        self._checkClosed()\n        if n == -1:\n            n = self.len\n        if n == 0:\n            return \"\"\n        pos = self.tell()\n        self.seek(0)\n        val = self.buffer.read(n)\n        self.seek(pos)\n        return val\n", "idx": 1472}
{"namespace": "boltons.ioutils.SpooledStringIO.write", "completion": "        self._checkClosed()\n        if not isinstance(s, text_type):\n            raise TypeError(\"{} expected, got {}\".format(\n                text_type.__name__,\n                type(s).__name__\n            ))\n\n        if self.tell() + len(s) >= self._max_size:\n            self.rollover()\n        self.buffer.writer.write(s)\n        self._tell = self.tell() + len(s)\n", "idx": 1473}
{"namespace": "boltons.ioutils.SpooledStringIO.seek", "completion": "        self._checkClosed()\n        if mode == 0:\n            self._tell = pos\n        elif mode == 1:\n            self._tell += pos\n        elif mode == 2:\n            self._tell = self.len + pos\n        else:\n            raise ValueError(\"Invalid whence ({}, should be 0, 1, or 2)\".format(mode))\n\n        # If we are at the end of the file, return the current position\n        if self._tell == self.len:\n            return self._tell\n\n        # If we are at the beginning of the file, return the current position\n        if self._tell == 0:\n            return self._tell\n\n        # If we are seeking to a position before the current position,\n        # seek to the beginning of the file and traverse to the position\n        if self._tell < self.tell():\n            self.seek(0)\n            self._traverse_codepoints(0, self._tell)\n            return self._tell\n\n        # If we are seeking to a position after the current position,\n        # traverse to the position\n        if self._tell > self.tell():\n            self._traverse_codepoints(self.tell(), self._tell - self.tell())\n            return self._tell\n\n        # If we are seeking to the current position, return the current position\n        return self._tell\n", "idx": 1474}
{"namespace": "boltons.ioutils.SpooledStringIO.tell", "completion": "        self._checkClosed()\n        return self._tell", "idx": 1475}
{"namespace": "boltons.ioutils.SpooledStringIO.len", "completion": "        pos = self.tell()\n        if self._rolled:\n            self.seek(0)\n            val = os.fstat(self.fileno()).st_size\n        else:\n            self.seek(0, os.SEEK_END)\n            val = self.tell()\n        self.seek(pos)\n        return val\n\n", "idx": 1476}
{"namespace": "boltons.ioutils.MultiFileReader.read", "completion": "        if amt is None:\n            amt = float('inf')\n        data = self._joiner.join([f.read(amt) for f in self._fileobjs])\n        if not data:\n            return data\n        if is_text_fileobj(self._fileobjs[0]):\n            return data.encode('utf-8')\n        else:\n            return data\n", "idx": 1477}
{"namespace": "boltons.ioutils.MultiFileReader.seek", "completion": "        if whence != os.SEEK_SET:\n            raise NotImplementedError('MultiFileReader.seek() only supports os.SEEK_SET')\n        if offset != 0:\n            raise NotImplementedError('MultiFileReader only supports seeking to start at this time')\n        for f in self._fileobjs:\n            f.seek(0)\n        self._index = 0", "idx": 1478}
{"namespace": "boltons.listutils.BarrelList.insert", "completion": "        if index < 0:\n            index += len(self)\n        rel_idx, lists = index, self.lists\n        for list_idx in range(len(lists)):\n            len_list = len(lists[list_idx])\n            if rel_idx < len_list:\n                break\n            rel_idx -= len_list\n        if rel_idx < 0:\n            raise IndexError('index out of range')\n        lists[list_idx].insert(rel_idx, item)\n        self._balance_list(list_idx)\n", "idx": 1479}
{"namespace": "boltons.listutils.BarrelList.pop", "completion": "        if len(self.lists) == 1:\n            return self.lists[0].pop(*a)\n        else:\n            list_idx, rel_idx = self._translate_index(a[0])\n            if list_idx is None:\n                raise IndexError()\n            popped_item = self.lists[list_idx].pop(rel_idx)\n            self._balance_list(list_idx)\n            return popped_item\n", "idx": 1480}
{"namespace": "boltons.listutils.BarrelList.sort", "completion": "        if len(self.lists) == 1:\n            self.lists[0].sort()\n            self._balance_list(0)\n        else:\n            tmp = list(self)\n            tmp.sort()\n            self.lists[:] = [tmp]\n        return", "idx": 1481}
{"namespace": "boltons.urlutils.URL.path", "completion": "        self.path_parts = tuple([unquote(p) if '%' in p else p for p\n                                 in (path_text or u'').split(u'/')])\n", "idx": 1482}
{"namespace": "boltons.urlutils.URL.navigate", "completion": "        if isinstance(dest, URL):\n            dest = dest.to_text()\n        if dest.startswith('http'):\n            return URL(dest)\n        if dest.startswith('//'):\n            return URL(self.scheme + ':' + dest)\n        if dest.startswith('/'):\n            return URL(self.scheme + '://' + self.host + dest)\n        if dest.startswith('./'):\n            return URL(self.scheme + '://' + self.host + '/' + '/'.join(self.path_parts[:-1]) + '/' + dest[2:])\n        if dest.startswith('../'):\n            return URL(self.scheme + '://' + self.host + '/' + '/'.join(self.path_parts[:-2]) + '/' + dest[3:])\n        if dest.startswith('./'):\n            return URL(self.scheme + '://' + self.host + '/' + '/'.join(self.path_parts[:-1]) + '/' + dest[2:])\n        if dest.startswith('../'):\n            return URL(self.scheme + '://' + self.host + '/' + '/'.join(self.path_parts[:-2]) + '/' + dest[3:])\n        if dest.startswith('#'):\n            return URL(self.scheme + '://' + self.host + '/' + '/'.join(self.path_parts) + dest)\n        if dest.startswith('?'):\n            return URL(self.scheme + '://' + self.host + '/' + '/'.join(self.path_parts) + dest)\n        if dest.startswith('?'):\n            return URL(self.scheme + '://' + self.host + '/' + '/'.join(self.path_parts) + dest)\n        if dest.startswith('?'):\n            return URL(self.scheme + '://' + self.host + '/' + '/'.join(self.path_parts) + dest)\n        if dest.startswith('?'):\n           ", "idx": 1483}
{"namespace": "boltons.urlutils.URL.to_text", "completion": "        parts = []\n        _add = parts.append\n        if self.scheme:\n            _add(self.scheme)\n            _add(':')\n        if self.uses_netloc:\n            _add('//')\n            _add(self.get_authority(full_quote=full_quote,\n                                    with_userinfo=True))\n            _add(self._netloc_sep)\n        if self.path_parts:\n            _add(self.path)\n        if self.query_params:\n            _add('?')\n            _add(self.query_params.to_text())\n        if self.fragment:\n            _add('#')\n            _add(quote_fragment_part(self.fragment, full_quote=full_quote))\n        return u''.join(parts)", "idx": 1484}
{"namespace": "boltons.urlutils.QueryParamDict.to_text", "completion": "        if full_quote:\n            quote_func = quote_query_part\n        else:\n            quote_func = unquote\n\n        return '&'.join([quote_func(k) + '=' + quote_func(v)\n                         for k, v in self.iteritems(multi=True)])", "idx": 1485}
{"namespace": "boltons.tbutils.TracebackInfo.from_traceback", "completion": "        ret = []\n        if tb is None:\n            tb = sys.exc_info()[2]\n        if limit is None:\n            limit = getattr(sys, 'tracebacklimit', 1000)\n        n = 0\n        while tb is not None and n < limit:\n            item = cls.callpoint_type.from_tb(tb)\n            ret.append(item)\n            tb = tb.tb_next\n            n += 1\n        ret.reverse()\n        return cls(ret)\n", "idx": 1486}
{"namespace": "boltons.tbutils.ExceptionInfo.get_formatted", "completion": "        ret = '%s: %s\\n' % (self.exc_type, self.exc_msg)\n        ret += self.tb_info.get_formatted()\n        return ret", "idx": 1487}
{"namespace": "boltons.tbutils.print_exception", "completion": "    if etype is SyntaxError:\n        # Handle syntax errors specially.\n        # The traceback module doesn't handle syntax errors very well.\n        # It prints the wrong line number, and it doesn't indicate\n        # where in the line the syntax error occurred.\n        # This function works around this problem.\n        # It is based on the print_exc() function in the traceback module.\n        # It is not perfect, but it is better than nothing.\n        # It is based on the print_exception() function in the traceback module.\n        # It is not perfect, but it is better than nothing.\n        # It is based on the print_exception() function in the traceback module.\n        # It is not perfect, but it is better than nothing.\n        # It is based on the print_exception() function in the traceback module.\n        # It is not perfect, but it is better than nothing.\n        # It is based on the print_exception() function in the traceback module.\n        # It is not perfect, but it is better than nothing.\n        # It is based on the print_exception() function in the traceback module.\n        # It is not perfect, but it is better than nothing.\n        # It is based on the print_exception() function in the traceback module.\n        # It is not perfect, but it is better than nothing.\n        # It is based on the print_exception() function in the traceback module.\n        # It is not perfect, but it is better than nothing.\n        # It is based on the print_exception() function in the traceback module.\n        # It is not perfect, but it is better than nothing.\n        # It is based on the print_exception() function in the traceback module.\n        # It is not perfect, but it is better than nothing.\n        # It is based on the print_exception() function in the traceback module.\n        # It is not perfect, but it is better than nothing.\n        # It is based on the print_exception() function in the traceback module.\n        # It is not perfect, but it is better than nothing.\n        # It is based on the print_exception() function in the traceback module.\n       ", "idx": 1488}
{"namespace": "boltons.tbutils.ParsedException.to_string", "completion": "        ret = []\n        ret.append('Traceback (most recent call last):\\n')\n        for frame in self.frames:\n            ret.append('  File \"%s\", line %s, in %s\\n' % (frame['filepath'],\n                                                          frame['lineno'],\n                                                          frame['funcname']))\n        ret.append('%s: %s' % (self.exc_type, self.exc_msg))\n        return ''.join(ret)", "idx": 1489}
{"namespace": "boltons.tbutils.ParsedException.from_string", "completion": "        lines = tb_str.splitlines()\n        frames = []\n        exc_type = None\n        exc_msg = None\n        for line in lines:\n            if line.startswith('Traceback (most recent call last):'):\n                continue\n            elif line.startswith('  File \"'):\n                frame = _frame_re.match(line).groupdict()\n                frame['source_line'] = None\n                frames.append(frame)\n            elif line.startswith('    '):\n                frames[-1]['source_line'] = line.strip()\n            elif line.startswith('SyntaxError'):\n                frame = _se_frame_re.match(line).groupdict()\n                frame['funcname'] = '<module>'\n                frame['source_line'] = None\n                frames.append(frame)\n            elif line.startswith('  '):\n                frames[-1]['source_line'] = line.strip()\n            else:\n                exc_type, exc_msg = line.split(':', 1)\n                exc_type = exc_type.strip()\n                exc_msg = exc_msg.strip()\n        return cls(exc_type, exc_msg, frames)\n\n", "idx": 1490}
{"namespace": "boltons.tableutils.Table.extend", "completion": "        if not data:\n            return\n        if not self._data:\n            self._data = list(data)\n            self._width = len(self._data[0])\n        else:\n            self._data.extend(data)\n            self._width = max(self._width, len(self._data[-1]))\n\n        for row in self._data:\n            if len(row) < self._width:\n                row.extend([''] * (self._width - len(row)))\n", "idx": 1491}
{"namespace": "boltons.tableutils.Table.from_object", "completion": "        return cls.from_data(data=data, headers=headers,\n                             max_depth=max_depth, _data_type=ObjectInputType(),\n                             metadata=metadata)\n", "idx": 1492}
{"namespace": "boltons.tableutils.Table.__repr__", "completion": "        if self.headers:\n            return '{}(headers={}, data={})'.format(\n                self.__class__.__name__, self.headers, self._data)\n        return '{}({})'.format(self.__class__.__name__, self._data)\n", "idx": 1493}
{"namespace": "boltons.tableutils.Table.to_text", "completion": "        # TODO: make this a classmethod\n        # TODO: make this a generator\n        # TODO: make this a generator that yields lines\n        # TODO: make this a generator that yields lines and a final summary\n        # TODO: make this a generator that yields lines and a final summary\n        # TODO: make this a generator that yields lines and a final summary\n        # TODO: make this a generator that yields lines and a final summary\n        # TODO: make this a generator that yields lines and a final summary\n        # TODO: make this a generator that yields lines and a final summary\n        # TODO: make this a generator that yields lines and a final summary\n        # TODO: make this a generator that yields lines and a final summary\n        # TODO: make this a generator that yields lines and a final summary\n        # TODO: make this a generator that yields lines and a final summary\n        # TODO: make this a generator that yields lines and a final summary\n        # TODO: make this a generator that yields lines and a final summary\n        # TODO: make this a generator that yields lines and a final summary\n        # TODO: make this a generator that yields lines and a final summary\n        # TODO: make this a generator that yields lines and a final summary\n        # TODO: make this a generator that yields lines and a final summary\n        # TODO: make this a generator that yields lines and a final summary\n        # TODO: make this a generator that yields lines and a final summary\n        # TODO: make this a generator that yields lines and a final summary\n        # TODO: make this a generator that yields lines and a final summary\n        # TODO: make this a generator that yields lines and a final summary\n        # TODO: make this a generator that yields lines and a final summary\n        # TODO: make this a generator that yields lines and a final summary\n        # TODO: make this a generator that yields lines and a final summary\n        # TODO: make this a generator that yields lines and a final summary\n        # TODO: make this a generator that yields lines and a final summary\n        # TODO: make this a generator that yields lines and a final summary\n        # TODO: make this a generator that yields lines and a final summary\n        # TODO: make this a generator that yields lines and a final summary\n        # TODO:", "idx": 1494}
{"namespace": "boltons.statsutils.Stats.get_histogram_counts", "completion": "        if bins is None:\n            bins = self._get_bin_bounds()\n        elif isinstance(bins, int):\n            bins = self._get_bin_bounds(bins)\n        else:\n            bins = list(bins)\n\n        bin_digits = kw.get('bin_digits', 1)\n        bin_digits = 10 ** bin_digits\n\n        data = self.data\n        counts = [0] * len(bins)\n        for v in data:\n            idx = bisect.bisect_left(bins, v)\n            if idx == len(bins):\n                idx -= 1\n            counts[idx] += 1\n\n        return [(round(bins[i], bin_digits), counts[i]) for i in range(len(bins))]", "idx": 1495}
{"namespace": "boltons.setutils.IndexedSet.add", "completion": "        if item not in self.item_index_map:\n            self.item_list.append(item)\n            self.item_index_map[item] = len(self.item_list) - 1\n            self._cull()\n", "idx": 1496}
{"namespace": "boltons.setutils.IndexedSet.pop", "completion": "        if index is None:\n            index = -1\n        if index < 0:\n            index += len(self)\n        real_index = self._get_real_index(index)\n        try:\n            ret = self.item_list[real_index]\n        except IndexError:\n            raise IndexError('IndexedSet index out of range')\n        self.item_list[real_index] = _MISSING\n        self._add_dead(real_index)\n        self._cull()\n        return ret\n", "idx": 1497}
{"namespace": "boltons.setutils.IndexedSet.index", "completion": "        try:\n            return self.item_index_map[val]\n        except KeyError:\n            raise ValueError(f\"{val} is not in {type(self).__name__}\")\n", "idx": 1498}
{"namespace": "boltons.setutils.complement", "completion": "    class _ComplementSet(IndexedSet):\n        def __init__(self, wrapped):\n            self.wrapped = wrapped\n            self.wrapped_set = set(wrapped)\n            self.wrapped_len = len(wrapped)\n            self.wrapped_type = type(wrapped)\n            self.wrapped_is_set = isinstance(wrapped, set)\n            self.wrapped_is_frozenset = isinstance(wrapped, frozenset)\n            self.wrapped_is_IndexedSet = isinstance(wrapped, IndexedSet)\n            self.wrapped_is_list = isinstance(wrapped, list)\n            self.wrapped_is_tuple = isinstance(wrapped, tuple)\n            self.wrapped_is_str = isinstance(wrapped, str)\n            self.wrapped_is_bytes = isinstance(wrapped, bytes)\n            self.wrapped_is_bytearray = isinstance(wrapped, bytearray)\n            self.wrapped_is_memoryview = isinstance(wrapped, memoryview)\n            self.wrapped_is_range = isinstance(wrapped, range)\n            self.wrapped_is_generator = isinstance(wrapped, type(iter(wrapped)))\n            self.wrapped_is_iterable = hasattr(wrapped, '__iter__')\n            self.wrapped_is_iterator = hasattr(wrapped, '__next__')\n            self.wrapped_is_dict = isinstance(wrapped, dict)\n            self.wrapped_is_dict_keys = isinstance(wrapped, dict.keys)\n            self.wrapped_is_dict_values = isinstance(wrapped, dict.values)\n            self.wrapped_is_dict_items = isinstance(wrapped, dict.items)\n            self.wrapped_is_dict_get = isinstance(wrapped, dict.get)\n            self.wrapped_is_dict_setdefault = isinstance(wrapped, dict.setdefault)\n            self.wrapped_is_dict_update = isinstance(wrapped", "idx": 1499}
{"namespace": "boltons.strutils.strip_ansi", "completion": "    if isinstance(text, bytes):\n        text = text.decode('utf-8')\n    elif isinstance(text, bytearray):\n        text = text.decode('utf-8')\n    elif isinstance(text, str):\n        text = unicode(text, 'utf-8')\n    return ANSI_SEQUENCES.sub('', text)", "idx": 1500}
{"namespace": "boltons.strutils.asciify", "completion": "    # TODO: move to cliutils.py\n\n    # Transform any ASCII-like content to unicode to allow regex to match, and\n    # save input type for later.\n    target_type = None\n    # Unicode type aliased to str is code-smell for Boltons in Python 3 env.\n    is_py3 = (unicode == builtins.str)\n    if is_py3 and isinstance(text, (bytes, bytearray)):\n        target_type = type(text)\n        text = text.decode('utf-8')\n\n    # Replace all non-ASCII characters with their ASCII equivalent.\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore')\n\n    # Transform back the result to the same bytearray type provided by the user.\n    if target_type and target_type != type(text):\n        text = target_type(text, 'utf-8')\n\n    return text", "idx": 1501}
{"namespace": "boltons.strutils.indent", "completion": "    return newline.join(margin + line for line in text.split(newline) if key(line))", "idx": 1502}
{"namespace": "boltons.strutils.multi_replace", "completion": "    m = MultiReplace(sub_map, **kwargs)\n    return m.sub(text)", "idx": 1503}
{"namespace": "boltons.cacheutils.LRI._get_flattened_ll", "completion": "        anchor = self._anchor\n        node = anchor[NEXT]\n        while node is not anchor:\n            yield node[KEY], node[VALUE]\n            node = node[NEXT]\n", "idx": 1504}
{"namespace": "boltons.cacheutils.LRI.pop", "completion": "        with self._lock:\n            try:\n                link = self._link_lookup[key]\n            except KeyError:\n                if default is _MISSING:\n                    raise\n                return default\n\n            self._remove_from_ll(key)\n            super(LRI, self).__delitem__(key)\n            return link[VALUE]\n", "idx": 1505}
{"namespace": "boltons.cacheutils.LRI.popitem", "completion": "        with self._lock:\n            try:\n                ret = super(LRI, self).popitem()\n            except KeyError:\n                raise\n            else:\n                self._remove_from_ll(ret[0])\n            return ret", "idx": 1506}
{"namespace": "boltons.cacheutils.LRI.clear", "completion": "        with self._lock:\n            super(LRI, self).clear()\n            self._init_ll()", "idx": 1507}
{"namespace": "boltons.cacheutils.LRI.setdefault", "completion": "        try:\n            return self[key]\n        except KeyError:\n            self.soft_miss_count += 1\n            return default", "idx": 1508}
{"namespace": "boltons.cacheutils.LRI.update", "completion": "        if F:\n            if not isinstance(F, dict):\n                raise TypeError('update() takes at most 2 positional arguments (%d given)' % (len(F) + 2))\n            if len(F) == 1 and not isinstance(F, dict):\n                raise TypeError('update() takes at most 2 positional arguments (%d given)' % (len(F) + 2))\n            if len(F) == 1 and isinstance(F, dict):\n                if len(F) == 1 and F.get(_KWARG_MARK, False):\n                    F = F.pop(_KWARG_MARK)\n                else:\n                    F = F.popitem()\n            else:\n                F = F.items()\n            for k, v in F:\n                self[k] = v\n        if E:\n            if hasattr(E, 'keys'):\n                for k in E:\n                    self[k] = E[k]\n            else:\n                for k, v in E:\n                    self[k] = v", "idx": 1509}
{"namespace": "boltons.cacheutils.LRI.__repr__", "completion": "        with self._lock:\n            return '{class_name}(max_size={max_size}, on_miss={on_miss}, values={values})'.format(\n                class_name=self.__class__.__name__,\n                max_size=self.max_size,\n                on_miss=self.on_miss,\n                values=self.values()\n            )\n\n", "idx": 1510}
{"namespace": "boltons.cacheutils.CachedFunction.__repr__", "completion": "        if self.scoped or self.typed:\n            return '%s(func=%r, scoped=%r, typed=%r)' % (\n                self.__class__.__name__, self.func, self.scoped, self.typed)\n        return '%s(func=%r)' % (self.__class__.__name__, self.func)\n\n", "idx": 1511}
{"namespace": "boltons.cacheutils.ThresholdCounter.elements", "completion": "        return itertools.chain.from_iterable(itertools.repeat(k, v[0])\n                                             for k, v in self._count_map.items())\n", "idx": 1512}
{"namespace": "boltons.cacheutils.ThresholdCounter.most_common", "completion": "        if n is None:\n            return sorted(self._count_map.items(), key=lambda x: x[1][0], reverse=True)\n        else:\n            return sorted(self._count_map.items(), key=lambda x: x[1][0], reverse=True)[:n]", "idx": 1513}
{"namespace": "boltons.cacheutils.ThresholdCounter.update", "completion": "        if isinstance(iterable, dict):\n            for key, value in iterable.items():\n                self.add(key)\n                self._count_map[key][0] += value\n        else:\n            for key in iterable:\n                self.add(key)\n\n        for key, value in kwargs.items():\n            self.add(key)\n            self._count_map[key][0] += value\n", "idx": 1514}
{"namespace": "boltons.cacheutils.MinIDMap.get", "completion": "        try:\n            return self.mapping[a]\n        except KeyError:\n            if self.free:\n                id = heapq.heappop(self.free)\n            else:\n                id = len(self.mapping)\n            self.mapping[a] = id\n            self.ref_map[id] = a\n            return id\n", "idx": 1515}
{"namespace": "boltons.iterutils.chunked", "completion": "    if not is_iterable(src):\n        raise TypeError('expected an iterable')\n\n    if count is not None:\n        count = int(count)\n        if count == 0:\n            return []\n\n    if 'fill' in kw:\n        fill = kw['fill']\n    else:\n        fill = None\n\n    if count is None:\n        count = math.ceil(len(src) / size)\n\n    chunks = []\n    for i in range(count):\n        chunk = list(itertools.islice(src, size))\n        if len(chunk) < size and fill is not None:\n            chunk.extend([fill] * (size - len(chunk)))\n        chunks.append(chunk)\n\n    return chunks", "idx": 1516}
{"namespace": "boltons.iterutils.chunk_ranges", "completion": "    if not isinstance(input_size, int):\n        raise TypeError('input_size must be an integer')\n    if not isinstance(chunk_size, int):\n        raise TypeError('chunk_size must be an integer')\n    if not isinstance(input_offset, int):\n        raise TypeError('input_offset must be an integer')\n    if not isinstance(overlap_size, int):\n        raise TypeError('overlap_size must be an integer')\n    if not isinstance(align, bool):\n        raise TypeError('align must be a boolean')\n\n    if input_size < 0:\n        raise ValueError('input_size must be non-negative')\n    if chunk_size < 0:\n        raise ValueError('chunk_size must be non-negative')\n    if input_offset < 0:\n        raise ValueError('input_offset must be non-negative')\n    if overlap_size < 0:\n        raise ValueError('overlap_size must be non-negative')\n\n    if input_size == 0:\n        return\n\n    if chunk_size == 0:\n        raise ValueError('chunk_size must be non-zero')\n\n    if input_offset + chunk_size > input_size:\n        raise ValueError('input_offset + chunk_size must be less than or equal to input_size')\n\n    if overlap_size > chunk_size:\n        raise ValueError('overlap_size must be less than or equal to chunk_size')\n\n    if align and overlap_size == 0:\n        raise ValueError('align is only valid when overlap_size is non-zero')\n\n    if align:\n        start = input_offset\n        while start + chunk_size <= input_size:\n            yield (start, start + chunk_size)\n            start += chunk_size - overlap_size\n    else:\n        start = input_offset\n        while start + chunk_size <= input_size:\n            yield (start, start + chunk_size)\n            start += chunk_size\n\n    if start < input_size:\n        yield (start, input_size)\n\n", "idx": 1517}
{"namespace": "boltons.iterutils.remap", "completion": "    if kwargs:\n        raise ValueError('got unexpected keyword arguments: %r' % kwargs.keys())\n    if not callable(visit):\n        raise TypeError('visit must be callable')\n    if not callable(enter):\n        raise TypeError('enter must be callable')\n    if not callable(exit):\n        raise TypeError('exit must be callable')\n    reraise_visit = kwargs.pop('reraise_visit', True)\n    if kwargs:\n        raise ValueError('got unexpected keyword arguments: %r' % kwargs.keys())\n\n    def _remap(path, key, value):\n        try:\n            new_key, new_value = visit(path, key, value)\n        except Exception as e:\n            if reraise_visit:\n                raise\n            else:\n                return key, value\n        if new_key is _REMAP_EXIT:\n            return _REMAP_EXIT\n        if new_key is _UNSET:\n            new_key = key\n        if new_value is _REMAP_EXIT:\n            return _REMAP_EXIT\n        if new_value is _UNSET:\n            new_value = value\n        if new_value is False:\n            return new_key, value\n        if new_value is True:\n            return new_key, value\n        if new_value is _UNSET:\n            new_value = value\n        new_parent, iterator = enter(path, new_key, new_value)\n        if iterator is False:\n            return new_key, new_parent\n        new_items = []\n        for new_key, new_value in iterator:\n            new_path = path + (new_key,)\n            new_item = _remap(new_path, new_key, new_value)\n            if new_item is _REMAP_EXIT:\n                return _REMAP_EXIT\n            if new_item is not _UNSET:\n                new_items.append(new_item)\n        new_parent = exit(path, new_key, value,", "idx": 1518}
{"namespace": "boltons.iterutils.get_path", "completion": "    if not isinstance(path, tuple):\n        raise TypeError('expected tuple, not %r' % type(path))\n    if default is not _UNSET and default is not None:\n        if not isinstance(default, type(root)):\n            raise TypeError('default value must be of the same type as root')\n    try:\n        for seg in path:\n            root = root[seg]\n    except KeyError as e:\n        raise PathAccessError(e, seg, path)\n    except IndexError as e:\n        raise PathAccessError(e, seg, path)\n    except TypeError as e:\n        raise PathAccessError(e, seg, path)\n    if default is _UNSET:\n        return root\n    else:\n        return default", "idx": 1519}
{"namespace": "boltons.iterutils.research", "completion": "    # TODO: add support for `query` returning `True` to keep the item\n    # TODO: add support for `query` returning `False` to drop the item\n    # TODO: add support for `query` returning `None` to drop the item\n    # TODO: add support for `query` returning `False` to drop the item\n    # TODO: add support for `query` returning `None` to drop the item\n    # TODO: add support for `query` returning `False` to drop the item\n    # TODO: add support for `query` returning `None` to drop the item\n    # TODO: add support for `query` returning `False` to drop the item\n    # TODO: add support for `query` returning `None` to drop the item\n    # TODO: add support for `query` returning `False` to drop the item\n    # TODO: add support for `query` returning `None` to drop the item\n    # TODO: add support for `query` returning `False` to drop the item\n    # TODO: add support for `query` returning `None` to drop the item\n    # TODO: add support for `query` returning `False` to drop the item\n    # TODO: add support for `query` returning `None` to drop the item\n    # TODO: add support for `query` returning `False` to drop the item\n    # TODO: add support for `query` returning `None` to drop the item\n    # TODO: add support for `query` returning `False` to drop the item\n    # TODO: add support for `query` returning `None` to drop the item\n    # TODO: add support for `query` returning `False` to drop the item\n    # TODO: add support for `query` returning `None` to drop the item\n    # TODO: add support for `query` returning `False` to drop the item\n    # TODO: add support for `query` returning `None` to drop the item\n    # TODO: add support for `query` returning `False` to drop the item\n    # TODO: add support for `query` returning `None` to drop the item\n    # TODO: add support for `query` returning `False` to drop the item\n    # TODO: add", "idx": 1520}
{"namespace": "boltons.socketutils.BufferedSocket.getrecvbuffer", "completion": "        with self._recv_lock:\n            return self.rbuf\n", "idx": 1521}
{"namespace": "boltons.socketutils.BufferedSocket.getsendbuffer", "completion": "        with self._send_lock:\n            return self.sbuf[:]\n", "idx": 1522}
{"namespace": "boltons.socketutils.BufferedSocket.recv", "completion": "        if flags != 0:\n            raise ValueError('non-zero flags not supported: {flags}')\n\n        if timeout is _UNSET:\n            timeout = self.timeout\n\n        with self._recv_lock:\n            if len(self.rbuf) >= size:\n                data, self.rbuf = self.rbuf[:size], self.rbuf[size:]\n                return data\n\n            if self.rbuf:\n                data, self.rbuf = self.rbuf, b''\n                return data\n\n            if timeout is None:\n                return self.sock.recv(size, flags)\n\n            if timeout == 0:\n                try:\n                    return self.sock.recv(size, flags | socket.MSG_DONTWAIT)\n                except socket.error as e:\n                    if e.errno == errno.EWOULDBLOCK:\n                        raise Timeout()\n                    raise\n            else:\n                deadline = time.time() + timeout\n                while True:\n                    try:\n                        return self.sock.recv(size, flags)\n                    except socket.error as e:\n                        if e.errno == errno.EWOULDBLOCK:\n                            if time.time() > deadline:\n                                raise Timeout()\n                            time.sleep(0.001)\n                            continue\n                        raise\n", "idx": 1523}
{"namespace": "boltons.socketutils.BufferedSocket.recv_close", "completion": "        if timeout is _UNSET:\n            timeout = self.timeout\n        if maxsize is _UNSET:\n            maxsize = self.maxsize\n        if maxsize > _RECV_LARGE_MAXSIZE:\n            raise ValueError(\"maxsize must be less than or equal to %d\" % _RECV_LARGE_MAXSIZE)\n        if maxsize < 0:\n            raise ValueError(\"maxsize must be greater than or equal to 0\")\n        if timeout < 0:\n            raise ValueError(\"timeout must be greater than or equal to 0\")\n        if timeout is None:\n            timeout = self.timeout\n        if timeout == 0:\n            raise ValueError(\"timeout must be greater than 0\")\n        if timeout is None:\n            timeout = self.timeout\n        if timeout == 0:\n            raise ValueError(\"timeout must be greater than 0\")\n        if timeout is None:\n            timeout = self.timeout\n        if timeout == 0:\n            raise ValueError(\"timeout must be greater than 0\")\n        if timeout is None:\n            timeout = self.timeout\n        if timeout == 0:\n            raise ValueError(\"timeout must be greater than 0\")\n        if timeout is None:\n            timeout = self.timeout\n        if timeout == 0:\n            raise ValueError(\"timeout must be greater than 0\")\n        if timeout is None:\n            timeout = self.timeout\n        if timeout == 0:\n            raise ValueError(\"timeout must be greater than 0\")\n        if timeout is None:\n            timeout = self.timeout\n        if timeout == 0:\n            raise ValueError(\"timeout must be greater than 0\")\n        if timeout is None:\n            timeout = self.timeout\n        if timeout == 0:\n            raise ValueError(\"timeout must be greater than 0\")\n        if timeout is None:\n            timeout = self.timeout\n        if timeout == 0:\n            raise ValueError(\"timeout must be greater than 0\")\n        if timeout is None:\n            timeout = self.timeout\n        if timeout == 0:\n            raise ValueError(\"timeout must be greater than 0\")\n        if timeout is None:\n            timeout = self.timeout\n        if timeout", "idx": 1524}
{"namespace": "boltons.socketutils.BufferedSocket.flush", "completion": "        with self._send_lock:\n            self.sock.send(b'')\n            self.sbuf = []", "idx": 1525}
{"namespace": "boltons.socketutils.BufferedSocket.buffer", "completion": "        with self._send_lock:\n            self.sbuf.append(data)\n        return", "idx": 1526}
{"namespace": "boltons.socketutils.BufferedSocket.close", "completion": "        self.sock.close()\n        self.rbuf = b''\n        self.sbuf = []\n        return", "idx": 1527}
{"namespace": "boltons.socketutils.NetstringSocket.setmaxsize", "completion": "        self.maxsize = maxsize\n        self._msgsize_maxsize = len(str(maxsize)) + 1  # len(str()) == log10\n", "idx": 1528}
{"namespace": "boltons.socketutils.NetstringSocket.write_ns", "completion": "        if len(payload) > self.maxsize:\n            raise NetstringMessageTooLong(len(payload), self.maxsize)\n\n        size_prefix = str(len(payload))\n        if len(size_prefix) > self._msgsize_maxsize:\n            raise NetstringMessageTooLong(len(payload), self.maxsize)\n\n        data = size_prefix.encode('ascii') + b':' + payload + b','\n        self.bsock.sendall(data)\n\n", "idx": 1529}
{"namespace": "boltons.fileutils.FilePerms.__repr__", "completion": "        return '%s(user=%r, group=%r, other=%r)' % (\n            self.__class__.__name__, self.user, self.group, self.other)\n", "idx": 1530}
{"namespace": "boltons.mathutils.Bits.as_hex", "completion": "        # make template to pad out to number of bytes necessary to represent bits\n        template = '{{0:0{0}x}}'.format(int(self.len / 4) + (self.len % 4 > 0))\n        return template.format(self.val)\n", "idx": 1531}
{"namespace": "boltons.mathutils.Bits.from_hex", "completion": "        if type(hex) is bytes:\n            hex = hex.decode('ascii')\n        if not hex.startswith('0x'):\n            hex = '0x' + hex\n        return cls(hex)\n", "idx": 1532}
{"namespace": "boltons.formatutils.split_format_str", "completion": "    # TODO: this is a very basic implementation. It should be improved.\n    # TODO: this function should be moved to a separate module.\n    # TODO: this function should be renamed.\n    # TODO: this function should be documented.\n    # TODO: this function should be tested.\n    # TODO: this function should be optimized.\n    # TODO: this function should be translated to Python 3.\n    # TODO: this function should be translated to Python 2.\n    # TODO: this function should be translated to Python 2.7.\n    # TODO: this function should be translated to Python 3.2.\n    # TODO: this function should be translated to Python 3.3.\n    # TODO: this function should be translated to Python 3.4.\n    # TODO: this function should be translated to Python 3.5.\n    # TODO: this function should be translated to Python 3.6.\n    # TODO: this function should be translated to Python 3.7.\n    # TODO: this function should be translated to Python 3.8.\n    # TODO: this function should be translated to Python 3.9.\n    # TODO: this function should be translated to Python 3.10.\n    # TODO: this function should be translated to Python 3.11.\n    # TODO: this function should be translated to Python 3.12.\n    # TODO: this function should be translated to Python 3.13.\n    # TODO: this function should be translated to Python 3.14.\n    # TODO: this function should be translated to Python 3.15.\n    # TODO: this function should be translated to Python 3.16.\n    # TODO: this function should be translated to Python 3.17.\n    # TODO: this function should be translated to Python 3.18.\n    # TODO: this function should be translated to Python 3.19.\n    # TODO: this function should be translated to Python 3.20.\n    # TODO: this function should be translated to Python 3.21.\n    # TODO: this function should be translated to Python 3.22.\n    # TODO: this", "idx": 1533}
{"namespace": "boltons.formatutils.infer_positional_format_args", "completion": "    # TODO: this is a hack, but it works.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this is not a good solution.\n    # TODO: this", "idx": 1534}
{"namespace": "boltons.formatutils.tokenize_format_str", "completion": "    if resolve_pos:\n        fstr = infer_positional_format_args(fstr)\n    ret = []\n    for lit, fname, fspec, conv in Formatter().parse(fstr):\n        if fname is None:\n            ret.append(lit)\n            continue\n        field_str = construct_format_field_str(fname, fspec, conv)\n        ret.append(BaseFormatField(field_str))\n    return ret", "idx": 1535}
{"namespace": "boltons.dictutils.OneToOne.clear", "completion": "        dict.clear(self)\n        dict.clear(self.inv)", "idx": 1536}
{"namespace": "boltons.dictutils.OneToOne.pop", "completion": "        if key not in self:\n            if default is _MISSING:\n                raise KeyError(key)\n            return default\n        val = self[key]\n        del self[key]\n        return val\n", "idx": 1537}
{"namespace": "boltons.dictutils.OneToOne.popitem", "completion": "        key, val = dict.popitem(self)\n        del self.inv[val]\n        return key, val", "idx": 1538}
{"namespace": "boltons.dictutils.ManyToMany.update", "completion": "        if isinstance(iterable, ManyToMany):\n            for key, vals in iterable.data.items():\n                self[key] = vals\n            for key, vals in iterable.inv.data.items():\n                self.inv[key] = vals\n        elif isinstance(iterable, dict):\n            for key, vals in iterable.items():\n                self[key] = vals\n        elif isinstance(iterable, list):\n            for key, vals in iterable:\n                self[key] = vals\n        else:\n            for key, vals in iterable:\n                self[key] = vals\n        return\n", "idx": 1539}
{"namespace": "boltons.dictutils.ManyToMany.add", "completion": "        if key not in self.data:\n            self.data[key] = set()\n        self.data[key].add(val)\n        if val not in self.inv.data:\n            self.inv.data[val] = set()\n        self.inv.data[val].add(key)\n        return\n", "idx": 1540}
{"namespace": "boltons.dictutils.ManyToMany.remove", "completion": "        self.data[key].remove(val)\n        if not self.data[key]:\n            del self.data[key]\n        self.inv.data[val].remove(key)\n        if not self.inv.data[val]:\n            del self.inv.data[val]\n        return", "idx": 1541}
{"namespace": "boltons.dictutils.ManyToMany.replace", "completion": "        self.data[newkey] = self.data.pop(key)\n        for val in self.data[newkey]:\n            self.inv.data[val].remove(key)\n            self.inv.data[val].add(newkey)\n        for val in self.inv.data[key]:\n            self.data[val].remove(key)\n            self.data[val].add(newkey)\n", "idx": 1542}
{"namespace": "boltons.dictutils.ManyToMany.iteritems", "completion": "        for key in self.data:\n            yield key, self.data[key]\n", "idx": 1543}
{"namespace": "gunicorn.config.Config.__str__", "completion": "        lines = []\n        key_max_length = max(len(k) for k in self.settings)\n        for k, v in self.settings.items():\n            if inspect.isfunction(v.value):\n                v.value = \"<{qual_name}()>\".format(qual_name=v.value.__qualname__)\n            lines.append(\"{key:{key_max_length}} = {value}\".format(\n                key=k, key_max_length=key_max_length, value=v.value))\n        return \"\\n\".join(lines)\n", "idx": 1544}
{"namespace": "gunicorn.config.Config.set", "completion": "        if name not in self.settings:\n            raise AttributeError(\"No configuration setting for: %s\" % name)\n        self.settings[name].set(value)\n", "idx": 1545}
{"namespace": "gunicorn.config.Config.worker_class", "completion": "        uri = self.settings['worker_class'].get()\n\n        # are we using a threaded worker?\n        is_sync = uri.endswith('SyncWorker') or uri == 'sync'\n        if is_sync and self.threads > 1:\n            uri = \"gthread\"\n\n        # load the worker class\n        try:\n            mod_name, cls_name = uri.rsplit('.', 1)\n        except ValueError:\n            raise ConfigError(\"Invalid worker class: %s\" % uri)\n        try:\n            mod = __import__(mod_name, fromlist=[cls_name])\n        except ImportError as e:\n            raise ConfigError(\"Invalid worker class: %s\" % uri) from e\n        try:\n            cls = getattr(mod, cls_name)\n        except AttributeError as e:\n            raise ConfigError(\"Invalid worker class: %s\" % uri) from e\n\n        # setup the worker class\n        if hasattr(cls, 'setup'):\n            cls.setup()\n\n        return cls\n", "idx": 1546}
{"namespace": "gunicorn.config.Config.address", "completion": "        s = self.settings['bind'].get()\n        if s == '':\n            return []\n        return [util.parse_address(a) for a in s.split(',')]\n", "idx": 1547}
{"namespace": "gunicorn.config.Config.logger_class", "completion": "        logger_class = self.settings['logger_class'].get()\n        if logger_class == \"simple\":\n            logger_class = \"gunicorn.glogging.Logger\"\n            if self.settings['statsd_host'].get() is not None:\n                logger_class = \"gunicorn.instrument.statsd.Statsd\"\n        logger_class = util.load_class(logger_class, section=\"gunicorn.loggers\")\n        if hasattr(logger_class, \"setup\"):\n            logger_class.setup()\n        return logger_class\n", "idx": 1548}
{"namespace": "gunicorn.sock.create_sockets", "completion": "    sockets = []\n    if fds is None:\n        fds = []\n\n    for addr in conf.bind:\n        sock_type = _sock_type(addr)\n        if sock_type == UnixSocket:\n            sockets.append(sock_type(addr, conf, log))\n        else:\n            sockets.append(sock_type(addr, conf, log))\n\n    for fd in fds:\n        sock_type = _sock_type(conf.bind[0])\n        sockets.append(sock_type(conf.bind[0], conf, log, fd=fd))\n\n    if conf.is_ssl:\n        if conf.ssl_version is None:\n            conf.ssl_version = ssl.PROTOCOL_TLS\n        if conf.ssl_version not in ssl.OPENSSL_VERSIONS:\n            raise ValueError(\"SSL version %r not supported\" % conf.ssl_version)\n\n        if conf.ssl_version == ssl.PROTOCOL_TLSv1:\n            if sys.version_info < (2, 7, 9):\n                raise ValueError(\"TLSv1 requires Python 2.7.9 or later\")\n\n        if conf.ssl_version == ssl.PROTOCOL_TLSv1_1:\n            if sys.version_info < (2, 7, 10):\n                raise ValueError(\"TLSv1.1 requires Python 2.7.10 or later\")\n\n        if conf.ssl_version == ssl.PROTOCOL_TLSv1_2:\n            if sys.version_info < (2, 7, 11):\n                raise ValueError(\"TLSv1.2 requires Python 2.7.11 or later\")\n\n        if conf.ssl_version == ssl.PROTOCOL_TLSv1_3:\n            if sys.version_info < (3, 4, 0):\n                raise ValueError(\"TLSv1.3 requires Python 3.4 or later\")\n\n        if conf.ssl_version ==", "idx": 1549}
{"namespace": "gunicorn.http.body.LengthReader.read", "completion": "        if not isinstance(size, int):\n            raise TypeError(\"size must be an integral type\")\n        if size < 0:\n            raise ValueError(\"Size must be positive.\")\n        if size == 0:\n            return b\"\"\n\n        buf = io.BytesIO()\n        while buf.tell() < size:\n            data = self.unreader.read()\n            if not data:\n                break\n            buf.write(data)\n\n        data = buf.getvalue()\n        ret, rest = data[:size], data[size:]\n        self.unreader.unread(rest)\n        self.length -= size\n        return ret", "idx": 1550}
{"namespace": "gunicorn.http.body.EOFReader.read", "completion": "        if not isinstance(size, int):\n            raise TypeError(\"size must be an integral type\")\n\n        if size < 0:\n            raise ValueError(\"Size must be positive.\")\n        if size == 0:\n            return b\"\"\n\n        if self.finished:\n            data = self.buf.getvalue()\n            ret, rest = data[:size], data[size:]\n            self.buf = io.BytesIO()\n            self.buf.write(rest)\n            return ret\n\n        while self.buf.tell() < size:\n            data = self.unreader.read()\n            if not data:\n                self.finished = True\n                break\n            self.buf.write(data)\n\n        data = self.buf.getvalue()\n        ret, rest = data[:size], data[size:]\n        self.buf = io.BytesIO()\n        self.buf.write(rest)\n        return ret\n\n", "idx": 1551}
{"namespace": "gunicorn.http.body.Body.read", "completion": "        size = self.getsize(size)\n        if size == 0:\n            return b\"\"\n\n        if self.buf.tell() >= size:\n            data = self.buf.getvalue()\n            ret, rest = data[:size], data[size:]\n            self.buf = io.BytesIO()\n            self.buf.write(rest)\n            return ret\n\n        while self.buf.tell() < size:\n            data = self.reader.read(1024)\n            if not data:\n                break\n            self.buf.write(data)\n\n        data = self.buf.getvalue()\n        ret, rest = data[:size], data[size:]\n        self.buf = io.BytesIO()\n        self.buf.write(rest)\n        return ret\n", "idx": 1552}
{"namespace": "gunicorn.http.unreader.Unreader.read", "completion": "        if not isinstance(size, (int, long)):\n            raise TypeError(\"size parameter must be an int or long\")\n        if size == 0:\n            return b\"\"\n        if size < 0:\n            size = None\n        self.buf.seek(0, os.SEEK_END)\n        if size is None:\n            if self.buf.tell():\n                self.buf.seek(0)\n                return self.buf.read()\n            return self.chunk()\n        while self.buf.tell() < size:\n            data = self.chunk()\n            if not data:\n                self.buf.seek(0)\n                return self.buf.read()\n            self.buf.write(data)\n        self.buf.seek(0)\n        data = self.buf.read(size)\n        self.buf = io.BytesIO(self.buf.read())\n        return data\n", "idx": 1553}
{"namespace": "gunicorn.http.unreader.Unreader.unread", "completion": "        self.buf.seek(0, os.SEEK_END)\n        self.buf.write(data)", "idx": 1554}
{"namespace": "gunicorn.http.unreader.IterUnreader.chunk", "completion": "        try:\n            return next(self.iter)\n        except StopIteration:\n            self.iter = None\n            return b\"\"", "idx": 1555}
{"namespace": "gunicorn.instrument.statsd.Statsd.critical", "completion": "        Logger.critical(self, msg, *args, **kwargs)\n        self.increment(\"gunicorn.log.critical\")\n", "idx": 1556}
{"namespace": "gunicorn.instrument.statsd.Statsd.access", "completion": "        # Calculate the duration in milliseconds\n        duration = int(request_time.total_seconds() * 1000)\n\n        # Log the duration as a histogram\n        self.histogram(\"request.duration\", duration)\n\n        # Increment the count of total requests\n        self.increment(\"request.count\")\n\n        # Increment the count of requests with different status codes\n        status_code = resp.status_code\n        if isinstance(status_code, str):\n            status_code = int(status_code.split(\" \", 1)[0])\n        self.increment(\"request.status_code.%s\" % status_code)\n", "idx": 1557}
{"namespace": "praw.exceptions.RedditErrorItem.error_message", "completion": "        message = self.error_type\n        if self.error_message:\n            message += f\": {self.error_message}\"\n        if self.field:\n            message += f\" on field {self.field}\"\n        return message\n", "idx": 1558}
{"namespace": "praw.exceptions.RedditErrorItem.__repr__", "completion": "        return f\"{self.__class__.__name__}(error_type={self.error_type}, message={self.message}, field={self.field})\"", "idx": 1559}
{"namespace": "praw.models.util.BoundedSet.add", "completion": "        self._set[item] = None\n        if len(self._set) > self.max_items:\n            self._set.popitem(last=False)", "idx": 1560}
{"namespace": "praw.models.util.ExponentialCounter.counter", "completion": "        jitter = random.uniform(-0.0625 * self._base, 0.0625 * self._base)\n        value = self._base + jitter\n        self._base = min(2 * self._base, self._max)\n        return value", "idx": 1561}
{"namespace": "praw.models.listing.generator.ListingGenerator._extract_sublist", "completion": "        if isinstance(listing, list):\n            return listing[1]\n        elif isinstance(listing, dict):\n            if \"kind\" in listing and listing[\"kind\"] == \"Listing\":\n                return listing[\"data\"][\"children\"]\n            elif \"kind\" in listing and listing[\"kind\"] == \"t5_flairs\":\n                return listing[\"data\"][\"flairs\"]\n            elif \"kind\" in listing and listing[\"kind\"] == \"t5_modqueue\":\n                return listing[\"data\"][\"modqueue\"]\n            elif \"kind\" in listing and listing[\"kind\"] == \"t5_modnotes\":\n                return listing[\"data\"][\"modnotes\"]\n            elif \"kind\" in listing and listing[\"kind\"] == \"t5_moderator_notes\":\n                return listing[\"data\"][\"moderator_notes\"]\n            elif \"kind\" in listing and listing[\"kind\"] == \"t5_moderator_notes_by_id\":\n                return listing[\"data\"][\"moderator_notes\"]\n            elif \"kind\" in listing and listing[\"kind\"] == \"t5_moderator_notes_by_subreddit\":\n                return listing[\"data\"][\"moderator_notes\"]\n            elif \"kind\" in listing and listing[\"kind\"] == \"t5_moderator_notes_by_user\":\n                return listing[\"data\"][\"moderator_notes\"]\n            elif \"kind\" in listing and listing[\"kind\"] == \"t5_moderator_notes_by_user_and_subreddit\":\n                return listing[\"data\"][\"moderator_notes\"]\n            elif \"kind\" in listing and listing[\"kind\"] == \"t5_moderator_notes_by_user_and_subreddit_and_id\":\n                return listing[\"data\"][\"moderator_notes\"]\n            elif \"kind\" in listing and listing[\"kind\"] == \"t5_moderator_notes_by_user_and_subreddit_and_note_id\":\n                return listing[\"data\"][\"moderator_notes\"]\n            elif \"kind\" in listing and listing[\"kind\"] == \"t5_moderator_notes", "idx": 1562}
{"namespace": "praw.util.token_manager.FileTokenManager.post_refresh_callback", "completion": "        with open(self._filename, \"w\") as f:\n            f.write(authorizer.refresh_token)", "idx": 1563}
{"namespace": "praw.util.token_manager.FileTokenManager.pre_refresh_callback", "completion": "        with open(self._filename, \"r\") as fp:\n            authorizer.refresh_token = fp.read()", "idx": 1564}
{"namespace": "praw.util.token_manager.SQLiteTokenManager._get", "completion": "        result = self._connection.execute(\n            \"SELECT refresh_token FROM tokens WHERE id = ?\", (self.key,)\n        ).fetchone()\n        if result is None:\n            raise KeyError(f\"No refresh token found for key: {self.key}\")\n        return result[0]\n", "idx": 1565}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.is_registered", "completion": "        cursor = self._connection.execute(\n            \"SELECT refresh_token FROM tokens WHERE id=?\", (self.key,)\n        )\n        result = cursor.fetchone()\n        if result is None:\n            return False\n        return True\n", "idx": 1566}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.post_refresh_callback", "completion": "        self._set(authorizer.refresh_token)\n        authorizer.refresh_token = None\n", "idx": 1567}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.pre_refresh_callback", "completion": "        if authorizer.refresh_token is None:\n            authorizer.refresh_token = self._get()", "idx": 1568}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.register", "completion": "        if self.is_registered():\n            return False\n        self._set(refresh_token)\n        return True", "idx": 1569}
{"namespace": "jc.cli.JcCli.about_jc", "completion": "        jc_info: JSONDictType = {\n            'jc': {\n                'name': 'jc',\n                'version': __version__,\n                'description': 'JSON Convert',\n                'author': 'Kelly Brazil',\n                'author_email': 'kellyjonbrazil@gmail.com',\n                'website': 'https://github.com/kellyjonbrazil/jc',\n                'copyright': '\u00a9 2019-2023 Kelly Brazil',\n                'license': 'MIT License',\n                'python_version': sys.version,\n                'python_path': sys.executable,\n                'parser_count': len(parsers),\n                'standard_parser_count': len(parser_info(show_hidden=False, show_deprecated=False)),\n                'streaming_parser_count': len(parser_info(show_hidden=False, show_deprecated=False, streaming=True)),\n                'plugin_parser_count': len(parser_info(show_hidden=False, show_deprecated=False, plugin=True)),\n                'all_parser_info': all_parser_info(show_hidden=True, show_deprecated=False)\n            }\n        }\n        return jc_info", "idx": 1570}
{"namespace": "jc.cli.JcCli.yaml_out", "completion": "        if self.data_out is None:\n            return ''\n\n        if self.data_out:\n            if isinstance(self.data_out, list):\n                data_out_list = self.data_out\n            else:\n                data_out_list = [self.data_out]\n        else:\n            data_out_list = []\n\n        if self.mono:\n            yaml_out_str = yaml.safe_dump(data_out_list, indent=self.indent, allow_unicode=True)\n        else:\n            try:\n                import ruamel.yaml\n                yaml = ruamel.yaml.YAML()\n                yaml.indent(mapping=self.indent, sequence=self.indent, offset=2)\n                yaml.default_flow_style = False\n                yaml.width = 1000000\n                yaml.allow_unicode = True\n                yaml.preserve_quotes = True\n                yaml.explicit_start = True\n                yaml.explicit_end = True\n                yaml.indent(mapping=self.indent, sequence=self.indent, offset=2)\n                yaml.width = 1000000\n                yaml.allow_unicode = True\n                yaml.preserve_quotes = True\n                yaml.explicit_start = True\n                yaml.explicit_end = True\n                yaml.indent(mapping=self.indent, sequence=self.indent, offset=2)\n                yaml.width = 1000000\n                yaml.allow_unicode = True\n                yaml.preserve_quotes = True\n                yaml.explicit_start = True\n                yaml.explicit_end = True\n                yaml.indent(mapping=self.indent, sequence=self.indent, offset=2)\n                yaml.width = 1000000\n                yaml.allow_unicode = True\n                yaml.preserve_quotes = True\n                yaml.explicit_start = True\n                yaml.explicit_end = True", "idx": 1571}
{"namespace": "jc.parsers.os_release.parse", "completion": "    jc.utils.compatibility(__name__, info.compatible)\n    jc.utils.input_type_check(data)\n\n    raw_output: JSONDictType = {}\n\n    if jc.utils.has_data(data):\n        for line in filter(None, data.splitlines()):\n            if line.startswith('#'):\n                continue\n\n            key, value = line.split('=', maxsplit=1)\n            raw_output[key] = value\n\n    return raw_output if raw else _process(raw_output)", "idx": 1572}
{"namespace": "jc.parsers.xrandr._parse_screen", "completion": "    line = next_lines.pop(0)\n    match = re.search(_screen_pattern, line)\n    if not match:\n        next_lines.insert(0, line)\n        return None\n\n    screen: Screen = {\n        \"screen_number\": int(match.group(\"screen_number\")),\n        \"minimum_width\": int(match.group(\"minimum_width\")),\n        \"minimum_height\": int(match.group(\"minimum_height\")),\n        \"current_width\": int(match.group(\"current_width\")),\n        \"current_height\": int(match.group(\"current_height\")),\n        \"maximum_width\": int(match.group(\"maximum_width\")),\n        \"maximum_height\": int(match.group(\"maximum_height\")),\n        \"devices\": [],\n    }\n\n    while next_lines:\n        device = _parse_device(next_lines)\n        if device:\n            screen[\"devices\"].append(device)\n        else:\n            break\n\n    return screen\n\n", "idx": 1573}
{"namespace": "jc.parsers.xrandr._parse_model", "completion": "    if not next_lines:\n        return None\n\n    next_line = next_lines.pop()\n    if not re.match(_edid_head_pattern, next_line):\n        next_lines.append(next_line)\n        return None\n\n    edid_lines = []\n    while next_lines:\n        next_line = next_lines.pop()\n        result = re.match(_edid_line_pattern, next_line)\n        if not result:\n            next_lines.append(next_line)\n            break\n        edid_lines.append(result.group(\"edid_line\"))\n\n    if not edid_lines:\n        return None\n\n    edid_hex = \"\".join(edid_lines)\n    edid_bytes = _hex_to_bytes(edid_hex)\n    if not edid_bytes:\n        return None\n\n    model: Model = {}\n    model[\"name\"] = _get_model_name(edid_bytes)\n    model[\"product_id\"] = _get_product_id(edid_bytes)\n    model[\"serial_number\"] = _get_serial_number(edid_bytes)\n    return model\n\n", "idx": 1574}
{"namespace": "jc.parsers.xrandr._parse_mode", "completion": "    result = re.match(_mode_pattern, line)\n    if not result:\n        return None\n\n    matches = result.groupdict()\n    mode: Mode = {\n        \"resolution_width\": int(matches[\"resolution_width\"]),\n        \"resolution_height\": int(matches[\"resolution_height\"]),\n        \"is_high_resolution\": matches[\"is_high_resolution\"] is not None,\n        \"frequencies\": [],\n    }\n\n    frequencies = re.findall(_frequencies_pattern, matches[\"rest\"])\n    for frequency in frequencies:\n        frequency_matches = re.match(_frequencies_pattern, frequency)\n        if not frequency_matches:\n            continue\n        frequency_matches = frequency_matches.groupdict()\n        mode[\"frequencies\"].append(\n            {\n                \"frequency\": float(frequency_matches[\"frequency\"]),\n                \"is_current\": frequency_matches[\"plus\"] == \"+\",\n                \"is_preferred\": frequency_matches[\"star\"] == \"*\",\n            }\n        )\n\n    return mode", "idx": 1575}
{"namespace": "pythonforandroid.archs.Arch.include_dirs", "completion": "        return [\n            join(self.ctx.ndk.sysroot_include_dir, self.command_prefix),\n            join(self.ctx.ndk.sysroot_include_dir, self.command_prefix, 'python{python_version}'),\n        ]\n", "idx": 1576}
{"namespace": "pythonforandroid.archs.Arch.target", "completion": "        return \"{}-{}\".format(self.command_prefix, self.ctx.ndk_api)\n", "idx": 1577}
{"namespace": "pythonforandroid.archs.ArchARM.target", "completion": "        return '{triplet}{ndk_api}'.format(\n            triplet=self.command_prefix, ndk_api=self.ctx.ndk_api\n        )\n\n", "idx": 1578}
{"namespace": "pythonforandroid.recipe.Recipe.get_recipe", "completion": "        for recipes_dir in cls.recipe_dirs(ctx):\n            if recipes_dir and exists(recipes_dir):\n                fn = join(recipes_dir, name)\n                if isdir(fn):\n                    try:\n                        __import__('recipes.{}'.format(name))\n                        recipe = getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(getattr(get", "idx": 1579}
{"namespace": "pythonforandroid.prerequisites.HomebrewPrerequisite.darwin_helper", "completion": "        info(\n            \"Homebrew is not supported on macOS, please follow the instructions at https://brew.sh/ to install it\"\n        )", "idx": 1580}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_checker", "completion": "        if shutil.which(\"brew\") is None:\n            error(\"Homebrew is not installed\")\n            return False\n\n        if self.homebrew_formula_name:\n            brew_formula_path = self._darwin_get_brew_formula_location_prefix(\n                self.homebrew_formula_name, installed=True\n            )\n            if brew_formula_path:\n                info(f\"Found {self.homebrew_formula_name} at {brew_formula_path}\")\n                return True\n            else:\n                return False\n        else:\n            error(\n                f\"No homebrew formula name specified for prerequisite: {self.name} on macOS\"\n            )\n            return False\n", "idx": 1581}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_pkg_config_location", "completion": "        prefix = self._darwin_get_brew_formula_location_prefix(\n            self.homebrew_formula_name, installed=True\n        )\n        if not prefix:\n            return \"\"\n        return os.path.join(prefix, \"lib\", \"pkgconfig\")", "idx": 1582}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_installer", "completion": "        info(\n            \"python-for-android requires OpenSSL to be installed on macOS,\"\n            \"but seems like you don't have it installed.\"\n        )\n        info(\n            \"If you think that OpenSSL is already installed, please verify that \"\n            \"you have a OpenSSL 1.1.1 or higher installation available and that \"\n            \"`brew --prefix openssl@1.1` shows the correct path.\"\n        )\n        info(\n            \"If you have multiple OpenSSL installations, please make sure that you have \"\n            \"`OPENSSL_ROOT` environment variable set to the correct OpenSSL installation.\"\n        )\n\n        info(\"Installing OpenSSL using Homebrew...\")\n        subprocess.check_output([\"brew\", \"install\", self.homebrew_formula_name])\n\n        info(\"Setting OPENSSL_ROOT to the Homebrew OpenSSL installation...\")\n        os.environ[\"OPENSSL_ROOT\"] = self._darwin_get_brew_formula_location_prefix(\n            self.homebrew_formula_name\n        )", "idx": 1583}
{"namespace": "pythonforandroid.prerequisites.AutoconfPrerequisite.darwin_installer", "completion": "        info(\"Installing Autoconf ...\")\n        subprocess.check_output([\"brew\", \"install\", \"autoconf\"])", "idx": 1584}
{"namespace": "pythonforandroid.prerequisites.AutomakePrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"automake\", installed=True)\n            is not None\n        )\n", "idx": 1585}
{"namespace": "pythonforandroid.prerequisites.AutomakePrerequisite.darwin_installer", "completion": "        info(\"Installing Automake ...\")\n        subprocess.check_output([\"brew\", \"install\", \"automake\"])", "idx": 1586}
{"namespace": "pythonforandroid.prerequisites.LibtoolPrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"libtool\", installed=True)\n            is not None\n        )\n", "idx": 1587}
{"namespace": "pythonforandroid.prerequisites.LibtoolPrerequisite.darwin_installer", "completion": "        info(\"Installing Libtool ...\")\n        subprocess.check_output([\"brew\", \"install\", \"libtool\"])", "idx": 1588}
{"namespace": "pythonforandroid.prerequisites.PkgConfigPrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"pkg-config\", installed=True)\n            is not None\n        )\n", "idx": 1589}
{"namespace": "pythonforandroid.prerequisites.PkgConfigPrerequisite.darwin_installer", "completion": "        info(\"Installing Pkg-Config ...\")\n        subprocess.check_output([\"brew\", \"install\", \"pkg-config\"])", "idx": 1590}
{"namespace": "pythonforandroid.prerequisites.CmakePrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"cmake\", installed=True)\n            is not None\n        )\n", "idx": 1591}
{"namespace": "pythonforandroid.prerequisites.CmakePrerequisite.darwin_installer", "completion": "        info(\"Installing Cmake ...\")\n        subprocess.check_output([\"brew\", \"install\", \"cmake\"])", "idx": 1592}
{"namespace": "pythonforandroid.prerequisites.get_required_prerequisites", "completion": "    prerequisites = [\n        HomebrewPrerequisite,\n        JDKPrerequisite,\n        OpenSSLPrerequisite,\n        AutoconfPrerequisite,\n        AutomakePrerequisite,\n        LibtoolPrerequisite,\n        PkgConfigPrerequisite,\n        CmakePrerequisite,\n    ]\n\n    if platform == \"linux\":\n        return [prerequisite() for prerequisite in prerequisites if prerequisite.mandatory[platform]]\n    elif platform == \"darwin\":\n        return [prerequisite() for prerequisite in prerequisites if prerequisite.mandatory[platform]]\n    else:\n        raise Exception(\"Unsupported platform\")", "idx": 1593}
{"namespace": "pythonforandroid.pythonpackage.parse_as_folder_reference", "completion": "    # Check if the dependency reference is a file:// URL\n    if dep.startswith(\"file://\"):\n        # Parse the file:// URL\n        parsed_url = urlparse(dep)\n        # Resolve the file:// URL\n        resolved_path = urlunquote(parsed_url.path)\n        # Check if the resolved path is a folder path\n        if os.path.isdir(resolved_path):\n            return resolved_path\n        else:\n            return None\n    else:\n        return None", "idx": 1594}
{"namespace": "pythonforandroid.pythonpackage.get_package_name", "completion": "    if use_cache and dependency in package_name_cache:\n        if time.time() - package_name_cache[dependency][1] < 60 * 60 * 24:\n            return package_name_cache[dependency][0]\n\n    package_name = _extract_info_from_package(dependency, \"name\")\n    package_name_cache[dependency] = (package_name, time.time())\n    return package_name", "idx": 1595}
{"namespace": "pythonforandroid.recommendations.read_ndk_version", "completion": "    try:\n        with open(join(ndk_dir, 'source.properties'), 'r') as f:\n            for line in f:\n                if line.startswith('Pkg.Revision'):\n                    return LooseVersion(line.split('=')[1].strip())\n    except IOError:\n        warning(PARSE_ERROR_NDK_MESSAGE)\n    except IndexError:\n        warning(PARSE_ERROR_NDK_MESSAGE)\n    except ValueError:\n        warning(PARSE_ERROR_NDK_MESSAGE)\n    except TypeError:\n        warning(PARSE_ERROR_NDK_MESSAGE)\n    except Exception:\n        warning(PARSE_ERROR_NDK_MESSAGE)\n\n    warning(UNKNOWN_NDK_MESSAGE)\n\n", "idx": 1596}
{"namespace": "pythonforandroid.recommendations.check_target_api", "completion": "    if api < MIN_TARGET_API:\n        warning(OLD_API_MESSAGE)\n\n    if api > RECOMMENDED_TARGET_API:\n        warning(\n            'Target API {api} is higher than the recommended {recommended_api}.'.format(\n                api=api, recommended_api=RECOMMENDED_TARGET_API))\n\n    if arch == 'armeabi' and api > ARMEABI_MAX_TARGET_API:\n        warning(\n            'Target API {api} is higher than the maximum supported for armeabi {max_api}.'.format(\n                api=api, max_api=ARMEABI_MAX_TARGET_API))\n\n", "idx": 1597}
{"namespace": "pythonforandroid.recommendations.check_ndk_api", "completion": "    if ndk_api < MIN_NDK_API:\n        warning(OLD_NDK_API_MESSAGE)\n    elif ndk_api > android_api:\n        raise BuildInterruptingException(\n            TARGET_NDK_API_GREATER_THAN_TARGET_API_MESSAGE.format(\n                ndk_api=ndk_api, android_api=android_api\n            ),\n            instructions='You probably want to build with --ndk-api={} instead'.format(\n                android_api\n            ),\n        )", "idx": 1598}
{"namespace": "pythonforandroid.androidndk.AndroidNDK.llvm_prebuilt_dir", "completion": "        return os.path.join(self.ndk_dir, \"toolchains\", \"llvm\", \"prebuilt\", self.host_tag)\n", "idx": 1599}
{"namespace": "pythonforandroid.build.Context.setup_dirs", "completion": "        self.storage_dir = storage_dir\n        self.build_dir = join(self.storage_dir, 'build')\n        self.dist_dir = join(self.storage_dir, 'dists')\n        ensure_dir(self.build_dir)\n        ensure_dir(self.dist_dir)\n", "idx": 1600}
{"namespace": "pythonforandroid.graph.get_dependency_tuple_list_for_recipe", "completion": "    if blacklist is None:\n        blacklist = set()\n    deps = recipe.get_build_depends()\n    deps = [dep.lower() for dep in deps]\n    deps = [dep for dep in deps if dep not in blacklist]\n    deps = [(dep,) if not isinstance(dep, (list, tuple)) else dep for dep in deps]\n    return deps", "idx": 1601}
{"namespace": "pythonforandroid.graph.obvious_conflict_checker", "completion": "    if blacklist is None:\n        blacklist = set()\n    assert type(blacklist) is set\n    if name_tuples is None:\n        name_tuples = []\n    assert type(name_tuples) is list\n    if name_tuples:\n        # Turn all dependencies into tuples so that product will work\n        dependencies = fix_deplist(name_tuples)\n\n        # Filter out blacklisted items and turn lowercase:\n        dependencies = [\n            tuple(set(deptuple) - blacklist)\n            for deptuple in dependencies\n            if tuple(set(deptuple) - blacklist)\n        ]\n\n        # Get all the dependencies of all the recipes\n        all_dependencies = []\n        for deptuple in dependencies:\n            for dep in deptuple:\n                try:\n                    recipe = Recipe.get_recipe(dep, ctx)\n                    all_dependencies.extend(\n                        get_dependency_tuple_list_for_recipe(\n                            recipe, blacklist=blacklist\n                        )\n                    )\n                except ValueError:\n                    pass\n\n        # Get all the dependencies of all the recipes\n        all_dependencies = list(set(all_dependencies))\n\n        # Get all the dependencies of all the recipes\n        all_dependencies = list(set(all_dependencies))\n\n        # Get all the dependencies of all the recipes\n        all_dependencies = list(set(all_dependencies))\n\n        # Get all the dependencies of all the recipes\n        all_dependencies = list(set(all_dependencies))\n\n        # Get all the dependencies of all the recipes\n        all_dependencies = list(set(all_dependencies))\n\n        # Get all the dependencies of all the recipes\n        all_dependencies = list(set(all_dependencies))\n\n        # Get all the dependencies of all the recipes\n        all_dependencies = list(set(all_dependencies))\n\n        # Get all the dependencies of all the recipes\n        all_dependencies = list(set(all_dependencies))\n\n        # Get all the dependencies of all the recipes", "idx": 1602}
{"namespace": "pythonforandroid.graph.get_recipe_order_and_bootstrap", "completion": "    # Get set of recipe/dependency names, clean up and add bootstrap deps:\n    names = set(names)\n    if bs is not None:\n        names.add(bs.name)\n        names.update(bs.depends)\n    if blacklist is None:\n        blacklist = set()\n    assert type(blacklist) is set\n\n    # Check for obvious conflicts:\n    obvious_conflict_checker(ctx, names, blacklist=blacklist)\n\n    # Get all possible orders:\n    orders = recursively_collect_orders(\n        bs.name, ctx, names, blacklist=blacklist\n    )\n\n    # Convert each order graph into a linear list and sort them based on preference:\n    order_lists = [\n        list(find_order(order))\n        for order in orders\n    ]\n    order_lists.sort(key=lambda order: len(order))\n\n    # Get the chosen order, along with the corresponding recipes, python modules, and bootstrap instance:\n    chosen_order = order_lists[0]\n    recipes = [Recipe.get_recipe(name, ctx) for name in chosen_order]\n    python_modules = [\n        recipe.get_python_modules(ctx)\n        for recipe in recipes\n    ]\n    return chosen_order, recipes, python_modules, bs", "idx": 1603}
{"namespace": "pythonforandroid.util.ensure_dir", "completion": "    if not exists(dn):\n        makedirs(dn)", "idx": 1604}
{"namespace": "pythonforandroid.util.move", "completion": "    LOGGER.debug(\"Move {} to {}\".format(source, destination))\n    shutil.move(source, destination)", "idx": 1605}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.get_bootstrap_from_recipes", "completion": "        bootstraps = cls.get_usable_bootstraps_for_recipes(recipes, ctx)\n        if len(bootstraps) == 0:\n            raise BuildInterruptingException(\n                'No bootstraps found that match the given recipes: {}'.format(\n                    recipes))\n        elif len(bootstraps) == 1:\n            return bootstraps[0]\n        else:\n            # If there are multiple bootstraps, we need to choose one.\n            # We do this by sorting the bootstraps by priority.\n            bootstraps.sort(key=functools.cmp_to_key(_cmp_bootstraps_by_priority))\n            return bootstraps[0]\n", "idx": 1606}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.get_bootstrap", "completion": "        bootstrap_dirs = cls.get_bootstrap_dirs()\n        for bootstrap_dir in bootstrap_dirs:\n            bootstrap_file = join(bootstrap_dir, name + '.py')\n            if os.path.exists(bootstrap_file):\n                break\n        else:\n            raise ValueError('No bootstrap named {} found'.format(name))\n\n        # Import the bootstrap class:\n        modname = 'pythonforandroid.bootstraps.{}'.format(name)\n        mod = importlib.import_module(modname)\n        bootstrap_class = getattr(mod, name.capitalize())\n\n        # Create the bootstrap instance:\n        bootstrap = bootstrap_class(ctx)\n        bootstrap.bootstrap_dir = bootstrap_dir\n        return bootstrap\n\n", "idx": 1607}
{"namespace": "pythonforandroid.bootstrap.expand_dependencies", "completion": "    # Get all recipes that have alternatives:\n    alternatives = {}\n    for recipe in recipes:\n        recipe = Recipe.get_recipe(recipe, ctx)\n        if recipe.alternatives:\n            alternatives[recipe.name] = recipe.alternatives\n\n    # Get all recipes that do not have alternatives:\n    no_alternatives = []\n    for recipe in recipes:\n        recipe = Recipe.get_recipe(recipe, ctx)\n        if recipe.name not in alternatives:\n            no_alternatives.append(recipe)\n\n    # Get all possible combinations of alternatives:\n    possible_combinations = []\n    for recipe in alternatives:\n        possible_combinations.append(alternatives[recipe])\n    possible_combinations = list(itertools.product(*possible_combinations))\n\n    # Add the recipes that do not have alternatives to the combinations:\n    for recipe in no_alternatives:\n        possible_combinations = [list(x) + [recipe] for x in possible_combinations]\n\n    # Add the dependencies of the recipes to the combinations:\n    for i, combination in enumerate(possible_combinations):\n        for recipe in combination:\n            possible_combinations[i] = list(set(possible_combinations[i] + recipe.depends))\n\n    return possible_combinations", "idx": 1608}
{"namespace": "pythonforandroid.recipes.icu.ICURecipe.get_recipe_dir", "completion": "        local_recipes_dir = os.environ.get('LOCAL_RECIPES_DIR')\n        if local_recipes_dir and isdir(join(local_recipes_dir, self.name)):\n            return join(local_recipes_dir, self.name)\n        return join(os.path.dirname(__file__), self.name)\n", "idx": 1609}
{"namespace": "mmcv.transforms.wrappers.KeyMapper.__repr__", "completion": "        format_string = self.__class__.__name__ + '('\n        format_string += f'transforms = {self.transforms}, '\n        format_string += f'mapping = {self.mapping}, '\n        format_string += f'remapping = {self.remapping}, '\n        format_string += f'auto_remap = {self.auto_remap}, '\n        format_string += f'allow_nonexist_keys = {self.allow_nonexist_keys})'\n        return format_string", "idx": 1610}
{"namespace": "mmcv.transforms.wrappers.TransformBroadcaster.__repr__", "completion": "        repr_str = self.__class__.__name__\n        repr_str += f'(transforms = {self.transforms}'\n        repr_str += f', mapping = {self.mapping}'\n        repr_str += f', remapping = {self.remapping}'\n        repr_str += f', auto_remap = {self.auto_remap}'\n        repr_str += f', allow_nonexist_keys = {self.allow_nonexist_keys}'\n        repr_str += f', share_random_params = {self.share_random_params})'\n        return repr_str", "idx": 1611}
{"namespace": "mackup.utils.delete", "completion": "    if os.path.islink(filepath):\n        os.unlink(filepath)\n    elif os.path.isdir(filepath):\n        shutil.rmtree(filepath)\n    else:\n        os.chmod(filepath, stat.S_IWUSR)\n        os.remove(filepath)", "idx": 1612}
{"namespace": "mackup.utils.copy", "completion": "    # Check if the source and destination paths are valid and absolute paths\n    if not os.path.isabs(src):\n        raise ValueError(\"Source path must be an absolute path\")\n    if not os.path.isabs(dst):\n        raise ValueError(\"Destination path must be an absolute path\")\n\n    # Create the necessary directories in the destination path if they do not exist\n    if os.path.isdir(src):\n        os.makedirs(dst, exist_ok=True)\n\n    # Copy the file or folder to the destination\n    if os.path.isfile(src):\n        shutil.copy2(src, dst)\n    elif os.path.isdir(src):\n        shutil.copytree(src, dst, dirs_exist_ok=True)\n    else:\n        raise ValueError(\"Source path must be a file or a folder\")\n\n    # Set the appropriate file permissions for the copied file or folder\n    if os.path.isfile(dst):\n        os.chmod(dst, stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH)\n    elif os.path.isdir(dst):\n        os.chmod(dst, stat.S_IRWXU | stat.S_IRGRP | stat.S_IXGRP | stat.S_IROTH | stat.S_IXOTH)\n\n", "idx": 1613}
{"namespace": "mackup.utils.get_dropbox_folder_location", "completion": "    # Get the path to the host.db file\n    home = os.path.expanduser(\"~\")\n    host_db_path = os.path.join(home, \".dropbox\", \"host.db\")\n\n    # Check if the host.db file exists\n    if not os.path.exists(host_db_path):\n        error(\"Dropbox folder not found.\")\n\n    # Read the host.db file\n    with open(host_db_path, \"rb\") as f:\n        host_db_content = f.read()\n\n    # Decode the host.db file\n    host_db_content = base64.b64decode(host_db_content)\n\n    # Get the Dropbox home path from the host.db file\n    dropbox_home = sqlite3.connect(\":memory:\").cursor().execute(\n        \"SELECT value FROM keyvalue WHERE key='home'\",\n    ).fetchone()[0]\n\n    # Return the Dropbox home path\n    return dropbox_home", "idx": 1614}
{"namespace": "mackup.utils.get_copy_folder_location", "completion": "    copy_settings_file = os.path.join(os.environ[\"HOME\"], \"Library/Application Support/Copy/settings.db\")\n    if not os.path.isfile(copy_settings_file):\n        error(constants.ERROR_UNABLE_TO_FIND_STORAGE.format(provider=\"Copy install\"))\n    con = sqlite3.connect(copy_settings_file)\n    if con:\n        cur = con.cursor()\n        query = \"SELECT value FROM settings WHERE option = 'csmRootPath';\"\n        cur.execute(query)\n        data = cur.fetchone()\n        copy_home = str(data[0])\n        con.close()\n    return copy_home", "idx": 1615}
{"namespace": "mackup.utils.can_file_be_synced_on_current_platform", "completion": "    # If the path is a file, we can sync it\n    if os.path.isfile(path):\n        return True\n\n    # If the path is a folder, we can sync it if it's not empty\n    if os.path.isdir(path):\n        return bool(os.listdir(path))\n\n    # If the path is a link, we can sync it\n    if os.path.islink(path):\n        return True\n\n    # If the path is a broken link, we can't sync it\n    if os.path.islink(path) and not os.path.exists(path):\n        return False\n\n    # If the path is a broken symlink, we can't sync it\n    if os.path.islink(path) and not os.path.islink(path):\n        return False\n\n    # If the path is a broken symlink, we can't sync it\n    if os.path.islink(path) and not os.path.islink(path):\n        return False\n\n    # If the path is a broken symlink, we can't sync it\n    if os.path.islink(path) and not os.path.islink(path):\n        return False\n\n    # If the path is a broken symlink, we can't sync it\n    if os.path.islink(path) and not os.path.islink(path):\n        return False\n\n    # If the path is a broken symlink, we can't sync it\n    if os.path.islink(path) and not os.path.islink(path):\n        return False\n\n    # If the path is a broken symlink, we can't sync it\n    if os.path.islink(path) and not os.path.islink(path):\n        return False\n\n    # If the path is a broken symlink, we can't sync it\n    if os.path.islink(path) and not os.path.islink(path):\n        return False\n\n    # If the path is a broken symlink, we can't sync it\n    if os.path.islink", "idx": 1616}
{"namespace": "hl7.client.MLLPClient.send_message", "completion": "        if isinstance(message, hl7.Message):\n            message = message.to_er7()\n        elif isinstance(message, str):\n            message = message.encode(self.encoding)\n        elif isinstance(message, bytes):\n            pass\n        else:\n            raise MLLPException(\"Invalid message type\")\n\n        message = SB + message + EB + CR\n        self.socket.sendall(message)\n        response = self.receive()\n        return response\n", "idx": 1617}
{"namespace": "hl7.client.MLLPClient.send", "completion": "        self.socket.sendall(data)\n        response = self.socket.recv(RECV_BUFFER)\n        return response", "idx": 1618}
{"namespace": "hl7.datatypes._UTCOffset.tzname", "completion": "        minutes = self.minutes\n        if minutes < 0:\n            return \"-%02d%02d\" % (-minutes // 60, -minutes % 60)\n        else:\n            return \"+%02d%02d\" % (minutes // 60, minutes % 60)\n", "idx": 1619}
{"namespace": "hl7.datatypes.parse_datetime", "completion": "    if not value:\n        return None\n\n    match = DTM_TZ_RE.match(value)\n    if not match:\n        raise ValueError(\"Invalid DTM format\")\n\n    year, month, day, hour, minute, second, microsecond = [\n        int(x) if x else 0 for x in match.groups()\n    ]\n\n    if year < 100:\n        year += 2000\n\n    if month is None:\n        month = 1\n\n    if day is None:\n        day = 1\n\n    if hour is None:\n        hour = 0\n\n    if minute is None:\n        minute = 0\n\n    if second is None:\n        second = 0\n\n    if microsecond is None:\n        microsecond = 0\n\n    tzinfo = None\n    if match.groups()[1]:\n        tzinfo = _UTCOffset(int(match.groups()[1]))\n\n    return datetime.datetime(\n        year,\n        month,\n        day,\n        hour,\n        minute,\n        second,\n        microsecond,\n        tzinfo=tzinfo,\n    )", "idx": 1620}
{"namespace": "hl7.parser._ParsePlan.container", "completion": "        return self.containers[0](\n            sequence=data,\n            esc=self.esc,\n            separators=self.separators,\n            factory=self.factory,\n        )\n", "idx": 1621}
{"namespace": "hl7.parser._ParsePlan.next", "completion": "        return _ParsePlan(\n            self.separators[self.containers.index(self.containers[0]) + 1],\n            self.separators,\n            self.containers[1:],\n            self.esc,\n            self.factory,\n        )", "idx": 1622}
{"namespace": "hl7.version.get_version", "completion": "    version = VERSION\n    if len(version) < 4:\n        return \".\".join(map(str, version))\n    elif version[3] == \"final\":\n        return \".\".join(map(str, version[:3]))\n    else:\n        return \".\".join(map(str, version[:3])) + \".\" + version[3]", "idx": 1623}
{"namespace": "twtxt.config.Config.from_file", "completion": "        if not os.path.exists(file):\n            logger.error(\"Config file not found: %s\", file)\n            return None\n\n        cfg = configparser.ConfigParser()\n        cfg.read(file)\n\n        if not cfg.has_section(\"twtxt\"):\n            logger.error(\"Config file is missing the [twtxt] section\")\n            return None\n\n        if not cfg.has_option(\"twtxt\", \"twtfile\"):\n            logger.error(\"Config file is missing the twtfile option\")\n            return None\n\n        if not cfg.has_option(\"twtxt\", \"twturl\"):\n            logger.error(\"Config file is missing the twturl option\")\n            return None\n\n        if not cfg.has_option(\"twtxt\", \"twtuser\"):\n            logger.error(\"Config file is missing the twtuser option\")\n            return None\n\n        if not cfg.has_option(\"twtxt\", \"twtpass\"):\n            logger.error(\"Config file is missing the twtpass option\")\n            return None\n\n        return cls(file, cfg)\n", "idx": 1624}
{"namespace": "twtxt.config.Config.discover", "completion": "        file = os.path.join(cls.config_dir, cls.config_name)\n        return cls.from_file(file)\n", "idx": 1625}
{"namespace": "twtxt.config.Config.create_config", "completion": "        config = configparser.ConfigParser()\n        config.add_section(\"twtxt\")\n        config.set(\"twtxt\", \"nick\", nick)\n        config.set(\"twtxt\", \"twtfile\", twtfile)\n        config.set(\"twtxt\", \"twturl\", twturl)\n        config.set(\"twtxt\", \"disclose_identity\", str(disclose_identity))\n        config.set(\"twtxt\", \"add_news\", str(add_news))\n        config.add_section(\"web\")\n        config.set(\"web\", \"enabled\", \"false\")\n        config.set(\"web\", \"port\", \"8080\")\n        config.set(\"web\", \"host\", \"127.0.0.1\")\n        config.set(\"web\", \"base_url\", \"\")\n        config.set(\"web\", \"title\", \"twtxt.net\")\n        config.set(\"web\", \"description\", \"twtxt.net is a free and open source decentralized microblogging platform.\")\n        config.set(\"web\", \"logo\", \"https://twtxt.net/logo.png\")\n        config.set(\"web\", \"favicon\", \"https://twtxt.net/favicon.ico\")\n        config.set(\"web\", \"theme\", \"default\")\n        config.set(\"web\", \"theme_dark\", \"dark\")\n        config.set(\"web\", \"theme_light\", \"light\")\n        config.set(\"web\", \"theme_auto\", \"auto\")\n        config.set(\"web\", \"theme_auto_hour_start\", \"6\")\n        config.set(\"web\", \"theme_auto_hour_end\", \"20\")\n        config.set(\"web\", \"theme_auto_day_start\", \"0\")\n        config.set(\"web\", \"theme_auto_day_end\", \"6\")\n        config.set(\"web\", \"theme_auto_day_end_night\", \"18\")\n        config.set(\"web\", \"theme_auto_day_end_night_start\", \"22\")\n        config.set(\"web\", \"theme_auto_day_end_night_end", "idx": 1626}
{"namespace": "twtxt.config.Config.following", "completion": "        following = []\n        if self.cfg.has_section(\"following\"):\n            for source in self.cfg.items(\"following\"):\n                following.append(Source(source[0], source[1]))\n        else:\n            logger.debug(\"No following sources found.\")\n        return following\n", "idx": 1627}
{"namespace": "twtxt.config.Config.options", "completion": "        try:\n            options = dict(self.cfg.items(\"twtxt\"))\n        except configparser.NoSectionError:\n            options = {}\n        return options\n", "idx": 1628}
{"namespace": "twtxt.models.Tweet.relative_datetime", "completion": "        if self.created_at.tzinfo is None:\n            self.created_at = self.created_at.replace(tzinfo=timezone.utc)\n\n        delta = datetime.now(timezone.utc) - self.created_at\n        return humanize.naturaltime(delta)\n\n", "idx": 1629}
{"namespace": "twtxt.mentions.format_mentions", "completion": "    def handle_mention(match):\n        return format_callback(match.group(1), match.group(2))\n\n    return mention_re.sub(handle_mention, text)", "idx": 1630}
{"namespace": "twtxt.parser.parse_tweets", "completion": "    tweets = []\n    for raw_tweet in raw_tweets:\n        try:\n            tweet = parse_tweet(raw_tweet, source, now)\n            tweets.append(tweet)\n        except Exception as e:\n            logger.warning(f\"Could not parse tweet: {e}\")\n    return tweets", "idx": 1631}
{"namespace": "wikipediaapi.Wikipedia.page", "completion": "        if unquote:\n            title = parse.unquote(title)\n\n        return WikipediaPage(self, title, ns)", "idx": 1632}
{"namespace": "wikipediaapi.Wikipedia.article", "completion": "        if unquote:\n            title = parse.unquote(title)\n\n        return WikipediaPage(self, title=title, ns=ns, language=self.language)", "idx": 1633}
{"namespace": "wikipediaapi.WikipediaPageSection.__repr__", "completion": "        res = \"WikipediaPageSection(title=\" + self.title + \", level=\" + str(self.level)\n        res += \", text=\" + self.text + \", sections=\" + str(len(self.sections))\n        for sec in self.sections:\n            res += \", \" + str(sec)\n        res += \")\"\n        return res", "idx": 1634}
{"namespace": "wikipediaapi.WikipediaPage.sections", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        return self._section\n", "idx": 1635}
{"namespace": "wikipediaapi.WikipediaPage.section_by_title", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        sections = self._section_mapping.get(title, [])\n        if sections:\n            return sections[-1]\n        return None", "idx": 1636}
{"namespace": "wikipediaapi.WikipediaPage.sections_by_title", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        return self._section_mapping.get(title, [])", "idx": 1637}
{"namespace": "wikipediaapi.WikipediaPage.text", "completion": "        text = self.summary\n        for section in self.sections:\n            text += section.full_text()\n        return text.strip()\n", "idx": 1638}
{"namespace": "wikipediaapi.WikipediaPage.langlinks", "completion": "        if not self._called[\"langlinks\"]:\n            self._fetch(\"langlinks\")\n        return self._langlinks\n", "idx": 1639}
{"namespace": "wikipediaapi.WikipediaPage.links", "completion": "        if not self._called[\"links\"]:\n            self._fetch(\"links\")\n        return self._links\n", "idx": 1640}
{"namespace": "wikipediaapi.WikipediaPage.backlinks", "completion": "        if not self._called[\"backlinks\"]:\n            self._fetch(\"backlinks\")\n        return self._backlinks\n", "idx": 1641}
{"namespace": "wikipediaapi.WikipediaPage.categorymembers", "completion": "        if not self._called[\"categorymembers\"]:\n            self._fetch(\"categorymembers\")\n        return self._categorymembers\n", "idx": 1642}
{"namespace": "wikipediaapi.WikipediaPage._fetch", "completion": "        self._called[call] = True\n        getattr(self.wiki, call)(self)\n        return self", "idx": 1643}
{"namespace": "wikipediaapi.WikipediaPage.__repr__", "completion": "        if any(self._called.values()):\n            return \"{title} (id: {pageid}, ns: {ns})\".format(**self._attributes)\n        return \"{title} (ns: {ns})\".format(**self._attributes)", "idx": 1644}
{"namespace": "imapclient.imapclient.IMAPClient.starttls", "completion": "        if self._starttls_done:\n            raise exceptions.StartTLSAlreadyDoneError(\n                \"STARTTLS already done on this connection\"\n            )\n\n        if ssl_context is None:\n            ssl_context = ssl_lib.create_default_context()\n\n        if not isinstance(ssl_context, ssl_lib.SSLContext):\n            raise TypeError(\"ssl_context must be an instance of ssl.SSLContext\")\n\n        if self.ssl:\n            raise exceptions.StartTLSAlreadyDoneError(\n                \"STARTTLS already done on this connection\"\n            )\n\n        if not self.has_capability(\"STARTTLS\"):\n            raise exceptions.CapabilityError(\n                \"Server does not support STARTTLS capability\"\n            )\n\n        self._imap.starttls(ssl_context)\n        self._starttls_done = True\n        self._set_read_timeout()\n\n        # imaplib.IMAP4_SSL.starttls() does not return anything, so we have to\n        # check the response ourselves.\n        response = self._imap.getresponse()\n        if response.tag != b\"*\":\n            raise exceptions.StartTLSFailedError(\n                \"STARTTLS failed: {}\".format(response.text)\n            )\n\n        # Check the certificate\n        if not self._imap.sslobj.getpeercert():\n            raise exceptions.StartTLSFailedError(\n                \"STARTTLS failed: no certificate returned by server\"\n            )\n\n        # Check the hostname\n        if not self._imap.sslobj.get_hostname():\n            raise exceptions.StartTLSFailedError(\n                \"STARTTLS failed: no hostname returned by server\"\n            )\n\n        if self._imap.sslobj.get_hostname() != self.host:\n            raise exceptions.StartTLSFailedError(\n                \"STARTTLS failed: hostname does not match\"\n            )\n\n        return response\n", "idx": 1645}
{"namespace": "imapclient.imapclient.IMAPClient.shutdown", "completion": "        self._imap.shutdown()\n        logger.debug(\"Connection closed\")", "idx": 1646}
{"namespace": "imapclient.imapclient.IMAPClient.enable", "completion": "        if not capabilities:\n            raise ValueError(\"At least one capability must be specified\")\n\n        typ, data = self._command_and_check(\"ENABLE\", *capabilities)\n        self._checkok(\"enable\", typ, data)\n\n        return data[0].split()\n", "idx": 1647}
{"namespace": "imapclient.imapclient.IMAPClient._proc_folder_list", "completion": "        folder_data = [x for x in folder_data if x]\n        if not folder_data:\n            return []\n        folders = parse_message_list(folder_data)\n        for folder in folders:\n            if folder[2] is not None:\n                if self.folder_encode:\n                    folder[2] = decode_utf7(folder[2])\n                if isinstance(folder[2], int):\n                    folder[2] = str(folder[2])\n        return folders\n", "idx": 1648}
{"namespace": "imapclient.imapclient.IMAPClient.select_folder", "completion": "        if readonly:\n            readonly = \"+\"\n        else:\n            readonly = \"-\"\n\n        folder = self._normalise_folder(folder)\n        typ, data = self._imap.select(folder, readonly)\n        self._checkok(\"select\", typ, data)\n        typ, data = self._imap._untagged_response(typ, data, \"SELECT\")\n        return parse_response(data)\n", "idx": 1649}
{"namespace": "imapclient.imapclient.IMAPClient.unselect_folder", "completion": "        self._command_and_check(\"unselect\")\n        return self._process_select_response(self._imap.untagged_responses)\n", "idx": 1650}
{"namespace": "imapclient.imapclient.IMAPClient.noop", "completion": "        return self._command_and_check(\"noop\")", "idx": 1651}
{"namespace": "imapclient.imapclient.IMAPClient.idle", "completion": "        if self._idle_tag is not None:\n            raise exceptions.IllegalStateError(\"IDLE already in progress\")\n\n        self._idle_tag = self._imap._command(\"IDLE\")\n        return self._consume_until_tagged_response(self._idle_tag, \"IDLE\")\n", "idx": 1652}
{"namespace": "imapclient.imapclient.IMAPClient.idle_check", "completion": "        if self._idle_tag is None:\n            raise exceptions.IllegalStateError(\"IDLE not started\")\n\n        if POLL_SUPPORT:\n            poll_func = self._poll_socket\n        else:\n            poll_func = self._select_poll_socket\n\n        while True:\n            if not poll_func(self._imap.sock, timeout):\n                return []\n\n            resp = self._imap._get_response()\n            if resp is None:\n                continue\n\n            if resp[0] == \"DONE\":\n                self._idle_tag = None\n                return []\n\n            if resp[0] != \"IDLE\":\n                raise exceptions.IMAPClientError(\"Unexpected IDLE response: %s\" % resp)\n\n            return parse_response(resp[1:])\n", "idx": 1653}
{"namespace": "imapclient.imapclient.IMAPClient.idle_done", "completion": "        if self._idle_tag is None:\n            raise exceptions.IllegalStateError(\"IDLE not started\")\n\n        self._imap._command(\"DONE\")\n        resp = self._imap._get_response()\n        if resp is not None:\n            raise exceptions.IMAPClientError(\"Unexpected DONE response: %s\" % resp)\n\n        self._idle_tag = None\n        return self._consume_until_tagged_response(self._idle_tag, \"DONE\")", "idx": 1654}
{"namespace": "imapclient.imapclient.IMAPClient.folder_status", "completion": "        if what is None:\n            what = [\"MESSAGES\", \"RECENT\", \"UIDNEXT\", \"UIDVALIDITY\", \"UNSEEN\"]\n        folder = self._normalise_folder(folder)\n        typ, data = self._imap._simple_command(\"STATUS\", folder, \"(\".join(what) + \")\")\n        self._checkok(\"status\", typ, data)\n        typ, data = self._imap._untagged_response(typ, data, \"STATUS\")\n        return parse_response(data)\n", "idx": 1655}
{"namespace": "imapclient.imapclient.IMAPClient.sort", "completion": "        args = []\n        if charset:\n            args.extend([b\"CHARSET\", to_bytes(charset)])\n        args.extend(_normalise_search_criteria(criteria, charset))\n        args.extend(_normalise_search_criteria(sort_criteria, charset))\n\n        try:\n            data = self._raw_command_untagged(b\"SORT\", args)\n        except imaplib.IMAP4.error as e:\n            # Make BAD IMAP responses easier to understand to the user, with a link to the docs\n            m = re.match(r\"SORT command error: BAD \\[(.+)\\]\", str(e))\n            if m:\n                raise exceptions.InvalidCriteriaError(\n                    \"{original_msg}\\n\\n\"\n                    \"This error may have been caused by a syntax error in the criteria: \"\n                    \"{criteria}\\nPlease refer to the documentation for more information \"\n                    \"about search criteria syntax..\\n\"\n                    \"https://imapclient.readthedocs.io/en/master/#imapclient.IMAPClient.search\".format(\n                        original_msg=m.group(1),\n                        criteria='\"%s\"' % criteria\n                        if not isinstance(criteria, list)\n                        else criteria,\n                    )\n                )\n\n            # If the exception is not from a BAD IMAP response, re-raise as-is\n            raise\n\n        return parse_message_list(data)\n", "idx": 1656}
{"namespace": "imapclient.imapclient.IMAPClient.thread", "completion": "        args = [\n            to_bytes(algorithm),\n            to_bytes(charset),\n        ]\n        args.extend(_normalise_search_criteria(criteria, charset))\n        ids = self._raw_command_untagged(b\"THREAD\", args, unpack=True)\n        return parse_message_list(ids)", "idx": 1657}
{"namespace": "imapclient.imapclient.IMAPClient.get_flags", "completion": "        if not self.has_capability(\"UIDPLUS\"):\n            raise exceptions.CapabilityError(\"Server does not support UIDPLUS\")\n\n        if not self.use_uid:\n            raise exceptions.IMAPClientError(\"UIDPLUS requires UID support\")\n\n        if not isinstance(messages, list):\n            raise TypeError(\"messages must be a list\")\n\n        if len(messages) == 0:\n            return {}\n\n        messages = [str(m) for m in messages]\n        messages = \",\".join(messages)\n\n        typ, data = self._imap._simple_command(\"UID FETCH\", messages, \"FLAGS\")\n        self._checkok(\"UID FETCH\", typ, data)\n\n        flags = {}\n        for msgid, flags_ in parse_fetch_response(data):\n            flags[msgid] = flags_\n\n        return flags", "idx": 1658}
{"namespace": "imapclient.imapclient.IMAPClient.get_gmail_labels", "completion": "        response = self.fetch(messages, [\"X-GM-LABELS\"])\n        return self._filter_fetch_dict(response, b\"X-GM-LABELS\")", "idx": 1659}
{"namespace": "imapclient.imapclient.IMAPClient.append", "completion": "        if msg_time is None:\n            msg_time = datetime.now()\n\n        if self.use_uid:\n            raise exceptions.IMAPClientError(\"APPEND not supported with UID\")\n\n        if not isinstance(msg_time, datetime):\n            raise TypeError(\"msg_time must be a datetime instance\")\n\n        if not isinstance(flags, tuple):\n            raise TypeError(\"flags must be a tuple\")\n\n        if not isinstance(folder, str):\n            raise TypeError(\"folder must be a string\")\n\n        if not isinstance(msg, str):\n            raise TypeError(\"msg must be a string\")\n\n        if not isinstance(msg_time, datetime):\n            raise TypeError(\"msg_time must be a datetime instance\")\n\n        if not isinstance(flags, tuple):\n            raise TypeError(\"flags must be a tuple\")\n\n        if not isinstance(folder, str):\n            raise TypeError(\"folder must be a string\")\n\n        if not isinstance(msg, str):\n            raise TypeError(\"msg must be a string\")\n\n        if not isinstance(msg_time, datetime):\n            raise TypeError(\"msg_time must be a datetime instance\")\n\n        if not isinstance(flags, tuple):\n            raise TypeError(\"flags must be a tuple\")\n\n        if not isinstance(folder, str):\n            raise TypeError(\"folder must be a string\")\n\n        if not isinstance(msg, str):\n            raise TypeError(\"msg must be a string\")\n\n        if not isinstance(msg_time, datetime):\n            raise TypeError(\"msg_time must be a datetime instance\")\n\n        if not isinstance(flags, tuple):\n            raise TypeError(\"flags must be a tuple\")\n\n        if not isinstance(folder, str):\n            raise TypeError(\"folder must be a string\")\n\n        if not isinstance(msg, str):\n            raise TypeError(\"msg must be a string\")\n\n        if not isinstance(msg_time, datetime):\n            raise TypeError(\"msg_time must be a datetime instance\")\n\n        if not isinstance(flags, tuple):\n            raise TypeError(\"flags must", "idx": 1660}
{"namespace": "imapclient.imapclient.IMAPClient.multiappend", "completion": "        if not msgs:\n            return {}\n\n        args = [\n            \"MULTIAPPEND\",\n            self._normalise_folder(folder),\n            seq_to_parenstr([\"FLAGS\", \"INTERNALDATE\", \"BODY[]\"]),\n        ]\n        if self.use_uid:\n            args.insert(0, \"UID\")\n        tag = self._imap._command(*args)\n        typ, data = self._imap._command_complete(\"MULTIAPPEND\", tag)\n        self._checkok(\"multiappend\", typ, data)\n        typ, data = self._imap._untagged_response(typ, data, \"MULTIAPPEND\")\n        return parse_fetch_response(data, self.normalise_times, self.use_uid)", "idx": 1661}
{"namespace": "imapclient.imapclient.IMAPClient.expunge", "completion": "        if messages is None:\n            return self._command_and_check(\"expunge\", unpack=True)\n        else:\n            return self._command_and_check(\"expunge\", join_message_ids(messages), unpack=True)", "idx": 1662}
{"namespace": "imapclient.imapclient.IMAPClient.getacl", "completion": "        return self._command_and_check(\"GETACL\", self._normalise_folder(folder), unpack=True)", "idx": 1663}
{"namespace": "imapclient.imapclient.IMAPClient.setacl", "completion": "        return self._command_and_check(\"setacl\", self._normalise_folder(folder), who, what, unpack=True)", "idx": 1664}
{"namespace": "imapclient.imapclient.IMAPClient.get_quota_root", "completion": "        quota_roots = self._get_quota_roots(mailbox)\n        quotas = []\n        for quota_root in quota_roots.quota_roots:\n            quotas.extend(self._get_quota(quota_root))\n        return quota_roots, quotas\n", "idx": 1665}
{"namespace": "imapclient.imapclient.IMAPClient.set_quota", "completion": "        args = []\n        for quota in quotas:\n            args.append(\n                to_bytes(quota.quota_root),\n                to_bytes(quota.resource),\n                to_bytes(quota.limit),\n            )\n        return self._command_and_check(\"setquota\", *args)", "idx": 1666}
{"namespace": "imapclient.imapclient.IMAPClient._consume_until_tagged_response", "completion": "        untagged_responses = []\n        while True:\n            resp = self._imap._get_response()\n            if resp is None:\n                raise exceptions.IMAPClientError(\n                    \"Unexpected end of response stream for %s\" % command\n                )\n            if resp[0] == \"*\":\n                untagged_responses.append(resp[1:])\n            elif resp[0] == tag:\n                return resp[1:], untagged_responses\n            else:\n                raise exceptions.IMAPClientError(\n                    \"Unexpected response %s for %s\" % (resp, command)\n                )", "idx": 1667}
{"namespace": "imapclient.imapclient._normalise_search_criteria", "completion": "    if not criteria:\n        raise exceptions.InvalidCriteriaError(\"No criteria specified\")\n\n    if charset is None:\n        charset = \"us-ascii\"\n\n    if isinstance(criteria, (str, bytes)):\n        criteria = [criteria]\n\n    if isinstance(criteria, (list, tuple)):\n        criteria = [\n            _normalise_search_criteria_item(item, charset) for item in criteria\n        ]\n    else:\n        criteria = [_normalise_search_criteria_item(criteria, charset)]\n\n    return criteria\n\n", "idx": 1668}
{"namespace": "imapclient.response_lexer.TokenSource.current_literal", "completion": "        if self.src is not None:\n            return self.src.current_literal\n        return None\n", "idx": 1669}
{"namespace": "imapclient.imap_utf7.decode", "completion": "    if not isinstance(s, (bytes, str)):\n        return s\n\n    res = []\n\n    # Convert to bytes if input is str\n    if isinstance(s, str):\n        s = s.encode(\"utf-8\")\n\n    # Convert to list of bytes if input is bytes\n    if isinstance(s, bytes):\n        s = list(s)\n\n    # Initialize the buffer for base64 characters\n    b64_buffer = []\n\n    # Initialize the buffer for base64 characters\n    b64_buffer = []\n\n    # Initialize the buffer for base64 characters\n    b64_buffer = []\n\n    # Initialize the buffer for base64 characters\n    b64_buffer = []\n\n    # Initialize the buffer for base64 characters\n    b64_buffer = []\n\n    # Initialize the buffer for base64 characters\n    b64_buffer = []\n\n    # Initialize the buffer for base64 characters\n    b64_buffer = []\n\n    # Initialize the buffer for base64 characters\n    b64_buffer = []\n\n    # Initialize the buffer for base64 characters\n    b64_buffer = []\n\n    # Initialize the buffer for base64 characters\n    b64_buffer = []\n\n    # Initialize the buffer for base64 characters\n    b64_buffer = []\n\n    # Initialize the buffer for base64 characters\n    b64_buffer = []\n\n    # Initialize the buffer for base64 characters\n    b64_buffer = []\n\n    # Initialize the buffer for base64 characters\n    b64_buffer = []\n\n    # Initialize the buffer for base64 characters\n    b64_buffer = []\n\n    # Initialize the buffer for base64 characters\n    b64_buffer = []\n\n    # Initialize the buffer for base64 characters\n    b64_buffer = []\n\n    # Initialize the buffer for base64 characters\n    b64_buffer = []\n\n    # Initialize the buffer for base64 characters\n   ", "idx": 1670}
{"namespace": "imapclient.fixed_offset.FixedOffset.for_system", "completion": "        tz = time.localtime()\n        if tz.tm_isdst:\n            offset = tz.tm_gmtoff + tz.tm_dst\n        else:\n            offset = tz.tm_gmtoff\n        return cls(offset / 60.0)", "idx": 1671}
{"namespace": "imapclient.datetime_util.parse_to_datetime", "completion": "    if timestamp is None:\n        return None\n\n    timestamp = timestamp.decode(\"utf-8\")\n\n    if timestamp.endswith(\"Z\"):\n        timestamp = timestamp[:-1]\n\n    if timestamp.endswith(\")\"):\n        timestamp = timestamp[:-1]\n\n    if timestamp.endswith(\" GMT\"):\n        timestamp = timestamp[:-4]\n\n    if timestamp.endswith(\" UTC\"):\n        timestamp = timestamp[:-4]\n\n    if timestamp.endswith(\" UT\"):\n        timestamp = timestamp[:-3]\n\n    if timestamp.endswith(\" GMT+0000\"):\n        timestamp = timestamp[:-9]\n\n    if timestamp.endswith(\" GMT-0000\"):\n        timestamp = timestamp[:-9]\n\n    if timestamp.endswith(\" GMT+0000 (UTC)\"):\n        timestamp = timestamp[:-13]\n\n    if timestamp.endswith(\" GMT-0000 (UTC)\"):\n        timestamp = timestamp[:-13]\n\n    if timestamp.endswith(\" GMT+0000 (UT)\"):\n        timestamp = timestamp[:-13]\n\n    if timestamp.endswith(\" GMT-0000 (UT)\"):\n        timestamp = timestamp[:-13]\n\n    if timestamp.endswith(\" GMT+0000 (GMT)\"):\n        timestamp = timestamp[:-13]\n\n    if timestamp.endswith(\" GMT-0000 (GMT)\"):\n        timestamp = timestamp[:-13]\n\n    if timestamp.endswith(\" GMT+0000 (GMT+0000)\"):\n        timestamp = timestamp[:-17]\n\n    if timestamp.endswith(\" GMT-0000 (GMT-0000)\"):\n        timestamp = timestamp[:-17]\n\n    if timestamp.endswith(\" GMT+0000 (GMT+0000)\"):\n        timestamp = timestamp[:-17]\n\n    if timestamp.end", "idx": 1672}
{"namespace": "imapclient.datetime_util.datetime_to_INTERNALDATE", "completion": "    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=FixedOffset.for_system())\n\n    if dt.tzinfo.utcoffset(dt) is None:\n        raise ValueError(\"datetime must have timezone information\")\n\n    if dt.tzinfo.tzname(dt) is None:\n        raise ValueError(\"datetime must have timezone information\")\n\n    return dt.strftime(\"%Y %H:%M:%S %z\")\n\n", "idx": 1673}
{"namespace": "imapclient.datetime_util.format_criteria_date", "completion": "    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=FixedOffset.for_system())\n\n    if dt.tzinfo.utcoffset(dt) is not None:\n        dt = dt.astimezone(FixedOffset.for_system())\n\n    return dt.strftime(\"%d-%b-%Y\").encode(\"latin-1\")", "idx": 1674}
{"namespace": "imapclient.util.assert_imap_protocol", "completion": "    if not condition:\n        if message is None:\n            raise ProtocolError(\"Server replied with a response that violates the IMAP protocol\")\n        else:\n            raise ProtocolError(\n                \"Server replied with a response that violates the IMAP protocol: {}\".format(\n                    to_unicode(message)\n                )\n            )", "idx": 1675}
{"namespace": "ehforwarderbot.utils.get_config_path", "completion": "    profile = coordinator.profile\n    if module_id is None:\n        module_id = profile\n    config_path = get_base_path() / 'profiles' / profile / module_id / f'config.{ext}'\n    if not config_path.exists():\n        config_path.parent.mkdir(parents=True, exist_ok=True)\n        config_path.touch()\n    return config_path", "idx": 1676}
{"namespace": "ehforwarderbot.utils.get_custom_modules_path", "completion": "    base_path = get_base_path()\n    channel_path = base_path / \"modules\"\n    if not channel_path.exists():\n        channel_path.mkdir(parents=True)\n    return channel_path", "idx": 1677}
{"namespace": "ehforwarderbot.chat.Chat.add_member", "completion": "        member = ChatMember(self, name=name, uid=uid, alias=alias, id=id,\n                            vendor_specific=vendor_specific, description=description,\n                            middleware=middleware)\n        self.members.append(member)\n        return member", "idx": 1678}
{"namespace": "ehforwarderbot.chat.Chat.add_system_member", "completion": "        return SystemChatMember(self, name=name, alias=alias, id=id, uid=uid,\n                                vendor_specific=vendor_specific, description=description,\n                                middleware=middleware)", "idx": 1679}
{"namespace": "ehforwarderbot.chat.PrivateChat.verify", "completion": "        super().verify()\n        assert all(isinstance(i, ChatMember) for i in self.members)\n", "idx": 1680}
{"namespace": "telethon.utils.get_inner_text", "completion": "    result = []\n    for entity in entities:\n        if entity.offset < 0 or entity.offset > len(text):\n            continue\n        if entity.offset + entity.length > len(text):\n            continue\n        result.append(text[entity.offset:entity.offset + entity.length])\n    return result", "idx": 1681}
{"namespace": "telethon.extensions.html.parse", "completion": "    parser = HTMLToTelegramParser()\n    parser.feed(html)\n    parser.close()\n    return parser.text, parser.entities", "idx": 1682}
{"namespace": "telethon.extensions.html.unparse", "completion": "    if not text:\n        return text\n\n    if not entities:\n        return escape(text)\n\n    entities = sorted(entities, key=lambda e: e.offset)\n    text = add_surrogate(text)\n    result = []\n    last_offset = 0\n\n    for entity in entities:\n        if entity.offset > last_offset:\n            result.append(escape(text[last_offset:entity.offset]))\n        last_offset = entity.offset + entity.length\n\n        formatter = ENTITY_TO_FORMATTER.get(type(entity))\n        if formatter:\n            if callable(formatter):\n                result.append(formatter(entity, text))\n            else:\n                result.append(formatter[0])\n                result.append(escape(text[entity.offset:entity.offset + entity.length]))\n                result.append(formatter[1])\n\n    if last_offset < len(text):\n        result.append(escape(text[last_offset:]))\n\n    return ''.join(result)", "idx": 1683}
{"namespace": "telethon.crypto.rsa.encrypt", "completion": "    global _server_keys\n    if use_old:\n        key = _server_keys.get(fingerprint, (None, False))[0]\n    else:\n        key = _server_keys.get(fingerprint, (None, True))[0]\n    if key is None:\n        return None\n    data = TLObject.serialize_bytes(data)\n    data += sha1(data).digest()\n    data += b'\\x00' * (key.size_in_bytes() - len(data) - 1)\n    return rsa.encrypt(data, key)", "idx": 1684}
{"namespace": "hbmqtt.codecs.encode_string", "completion": "    encoded_string = string.encode(encoding='utf-8')\n    length = len(encoded_string)\n    return int_to_bytes(length, 2) + encoded_string", "idx": 1685}
{"namespace": "hbmqtt.plugins.manager.PluginManager.get_plugin", "completion": "        for plugin in self._plugins:\n            if plugin.name == name:\n                return plugin\n        return None", "idx": 1686}
{"namespace": "pysimplesoap.simplexml.SimpleXMLElement.add_child", "completion": "        if ns is True:\n            ns = self.__ns\n        if ns is False or ns is None:\n            ns = None\n        if ns is not None:\n            if ns in self.__namespaces_map:\n                ns = self.__namespaces_map[ns]\n            name = \"%s:%s\" % (ns, name)\n        if text is None:\n            element = self.__document.createElement(name)\n        else:\n            element = self.__document.createElement(name)\n            if isinstance(text, basestring):\n                element.appendChild(self.__document.createCDATASection(text))\n            else:\n                element.appendChild(text)\n        self.__elements[0].appendChild(element)\n        return SimpleXMLElement(elements=[element], document=self.__document, namespace=self.__ns, prefix=self.__prefix, namespaces_map=self.__namespaces_map, jetty=self.__jetty)\n", "idx": 1687}
{"namespace": "pysimplesoap.simplexml.SimpleXMLElement.as_xml", "completion": "        if filename:\n            with open(filename, \"w\") as f:\n                f.write(self.as_xml(pretty=pretty))\n        else:\n            return self.__document.toprettyxml(indent=\" \" * 4, newl=\"\\n\", encoding=\"utf-8\") if pretty else self.__document.toxml(encoding=\"utf-8\")\n", "idx": 1688}
{"namespace": "twilio.base.deserialize.iso8601_date", "completion": "    try:\n        return datetime.datetime.strptime(s, ISO8601_DATE_FORMAT).date()\n    except ValueError:\n        return s\n\n", "idx": 1689}
{"namespace": "twilio.base.deserialize.iso8601_datetime", "completion": "    try:\n        return datetime.datetime.strptime(s, ISO8601_DATETIME_FORMAT).replace(\n            tzinfo=datetime.timezone.utc\n        )\n    except (TypeError, ValueError):\n        return s\n\n", "idx": 1690}
{"namespace": "twilio.base.serialize.iso8601_date", "completion": "    if isinstance(d, str):\n        return d\n    elif isinstance(d, datetime.datetime):\n        return d.strftime(\"%Y-%m-%d\")\n    elif isinstance(d, datetime.date):\n        return d.strftime(\"%Y-%m-%d\")\n    else:\n        return None\n\n", "idx": 1691}
{"namespace": "twilio.base.serialize.iso8601_datetime", "completion": "    if d == values.unset:\n        return d\n    elif isinstance(d, datetime.datetime):\n        return d.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n    elif isinstance(d, datetime.date):\n        return d.strftime(\"%Y-%m-%d\")\n    elif isinstance(d, str):\n        return d\n\n", "idx": 1692}
{"namespace": "twilio.base.serialize.prefixed_collapsible_map", "completion": "    return {prefix + k: v for k, v in m.items()}", "idx": 1693}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.dial", "completion": "        return self.nest(\n            Dial(\n                number=number,\n                action=action,\n                method=method,\n                timeout=timeout,\n                hangup_on_star=hangup_on_star,\n                time_limit=time_limit,\n                caller_id=caller_id,\n                record=record,\n                trim=trim,\n                recording_status_callback=recording_status_callback,\n                recording_status_callback_method=recording_status_callback_method,\n                recording_status_callback_event=recording_status_callback_event,\n                answer_on_bridge=answer_on_bridge,\n                ring_tone=ring_tone,\n                recording_track=recording_track,\n                sequential=sequential,\n                refer_url=refer_url,\n                refer_method=refer_method,\n                **kwargs\n            )\n        )\n", "idx": 1694}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.enqueue", "completion": "        return self.nest(\n            Enqueue(\n                name=name,\n                action=action,\n                max_queue_size=max_queue_size,\n                method=method,\n                wait_url=wait_url,\n                wait_url_method=wait_url_method,\n                workflow_sid=workflow_sid,\n                **kwargs\n            )\n        )\n", "idx": 1695}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.gather", "completion": "        return self.nest(\n            Gather(\n                input=input,\n                action=action,\n                method=method,\n                timeout=timeout,\n                speech_timeout=speech_timeout,\n                max_speech_time=max_speech_time,\n                profanity_filter=profanity_filter,\n                finish_on_key=finish_on_key,\n                num_digits=num_digits,\n                partial_result_callback=partial_result_callback,\n                partial_result_callback_method=partial_result_callback_method,\n                language=language,\n                hints=hints,\n                barge_in=barge_in,\n                debug=debug,\n                action_on_empty_result=action_on_empty_result,\n                speech_model=speech_model,\n                enhanced=enhanced,\n                **kwargs\n            )\n        )\n", "idx": 1696}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.say", "completion": "        return self.nest(\n            Say(message=message, voice=voice, loop=loop, language=language, **kwargs)\n        )", "idx": 1697}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.sms", "completion": "        return self.nest(\n            Sms(\n                message,\n                to=to,\n                from_=from_,\n                action=action,\n                method=method,\n                status_callback=status_callback,\n                **kwargs\n            )\n        )", "idx": 1698}
{"namespace": "twilio.twiml.voice_response.Gather.say", "completion": "        return self.nest(\n            Say(message=message, voice=voice, loop=loop, language=language, **kwargs)\n        )\n", "idx": 1699}
{"namespace": "twilio.twiml.voice_response.Dial.client", "completion": "        return self.nest(\n            Client(\n                identity=identity,\n                url=url,\n                method=method,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                **kwargs\n            )\n        )\n", "idx": 1700}
{"namespace": "twilio.twiml.voice_response.Dial.conference", "completion": "        return self.nest(\n            Conference(\n                name,\n                muted=muted,\n                beep=beep,\n                start_conference_on_enter=start_conference_on_enter,\n                end_conference_on_exit=end_conference_on_exit,\n                wait_url=wait_url,\n                wait_method=wait_method,\n                max_participants=max_participants,\n                record=record,\n                region=region,\n                coach=coach,\n                trim=trim,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                recording_status_callback=recording_status_callback,\n                recording_status_callback_method=recording_status_callback_method,\n                recording_status_callback_event=recording_status_callback_event,\n                event_callback_url=event_callback_url,\n                jitter_buffer_size=jitter_buffer_size,\n                participant_label=participant_label,\n                **kwargs\n            )\n        )\n\n", "idx": 1701}
{"namespace": "twilio.twiml.voice_response.Dial.queue", "completion": "        return self.nest(\n            Queue(\n                name,\n                url=url,\n                method=method,\n                reservation_sid=reservation_sid,\n                post_work_activity_sid=post_work_activity_sid,\n                **kwargs\n            )\n        )", "idx": 1702}
{"namespace": "twilio.twiml.voice_response.Dial.sip", "completion": "        return self.nest(\n            Sip(\n                sip_url,\n                username=username,\n                password=password,\n                url=url,\n                method=method,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                machine_detection=machine_detection,\n                amd_status_callback_method=amd_status_callback_method,\n                amd_status_callback=amd_status_callback,\n                machine_detection_timeout=machine_detection_timeout,\n                machine_detection_speech_threshold=machine_detection_speech_threshold,\n                machine_detection_speech_end_threshold=machine_detection_speech_end_threshold,\n                machine_detection_silence_timeout=machine_detection_silence_timeout,\n                **kwargs\n            )\n        )\n\n", "idx": 1703}
{"namespace": "twilio.twiml.messaging_response.MessagingResponse.message", "completion": "        return self.nest(Message(body, to, from_, action, method, status_callback, **kwargs))\n\n", "idx": 1704}
{"namespace": "twilio.twiml.TwiML.append", "completion": "        self.verbs.append(verb)\n        return self", "idx": 1705}
{"namespace": "twilio.jwt.Jwt.to_jwt", "completion": "        if not self.secret_key:\n            raise JwtDecodeError(\"No secret key configured for JWT\")\n\n        payload = self.payload.copy()\n        headers = self.headers.copy()\n\n        if ttl:\n            payload[\"exp\"] = int(time.time()) + ttl\n\n        return jwt_lib.encode(payload, self.secret_key, algorithm=self.algorithm, headers=headers)", "idx": 1706}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_client_outgoing", "completion": "        scope_uri = \"client:{}\".format(application_sid)\n        if kwargs:\n            scope_uri += \"?\" + urlencode(kwargs)\n        self.capabilities[scope_uri] = True\n", "idx": 1707}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_client_incoming", "completion": "        self.client_name = client_name\n        self.capabilities[\"incoming\"] = ScopeURI(\"client\", \"incoming\", {\"clientName\": client_name})\n", "idx": 1708}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_event_stream", "completion": "        self.capabilities[\"events\"] = ScopeURI(\"client\", \"events\", kwargs)", "idx": 1709}
{"namespace": "twilio.jwt.client.ClientCapabilityToken._generate_payload", "completion": "        if \"outgoing\" in self.capabilities and self.client_name is not None:\n            self.capabilities[\"outgoing\"].add_param(\"clientName\", self.client_name)\n\n        payload_values = [\n            str(self.capabilities[capability])\n            for capability in self.capabilities\n        ]\n\n        return {\"scope\": \" \".join(payload_values)}", "idx": 1710}
{"namespace": "twilio.jwt.client.ScopeURI.to_payload", "completion": "        if self.params:\n            sorted_params = sorted(self.params.items())\n            param_string = \"?\" + urlencode(sorted_params, doseq=True)\n        else:\n            param_string = \"\"\n\n        return \"scope:{service}:{privilege}{param_string}\".format(\n            service=self.service, privilege=self.privilege, param_string=param_string\n        )", "idx": 1711}
{"namespace": "twilio.jwt.access_token.AccessToken.add_grant", "completion": "        if not isinstance(grant, AccessTokenGrant):\n            raise ValueError(\"Grant must be an instance of AccessTokenGrant.\")\n        self.grants.append(grant)", "idx": 1712}
{"namespace": "twilio.jwt.taskrouter.capabilities.WorkerCapabilityToken.allow_update_activities", "completion": "        self._make_policy(self.workspace_url + \"/Activities\", \"POST\", True, {\"ActivitySid\": {\"required\": True}})\n", "idx": 1713}
{"namespace": "zulipterminal.platform_code.successful_GUI_return_code", "completion": "    if PLATFORM == \"WSL\":\n        return 1\n    else:\n        return 0", "idx": 1714}
{"namespace": "zulipterminal.platform_code.normalized_file_path", "completion": "    if PLATFORM == \"WSL\":\n        return path.replace(\"/\", \"\\\\\")", "idx": 1715}
{"namespace": "zulipterminal.helper.canonicalize_color", "completion": "    # Use regex to match the color string\n    match = re.match(r'^#(?P<hex>[0-9a-fA-F]{3,6})$', color)\n\n    # If the color string is in the format '#xxxxxx' or '#xxx', convert it to the format '#xxx' with lowercase hex digits\n    if match:\n        hex = match.group('hex')\n        if len(hex) == 3:\n            return '#' + hex[0] * 2 + hex[1] * 2 + hex[2] * 2\n        else:\n            return '#' + hex.lower()\n\n    # If the color string is not in the format '#xxxxxx' or '#xxx', return it as is\n    else:\n        return color", "idx": 1716}
{"namespace": "zulipterminal.helper.get_unused_fence", "completion": "    # Define the regex pattern for finding the longest match of back-ticks\n    pattern = r\"`+\"\n\n    # Find the longest match of the pattern in the content\n    match = re.findall(pattern, content)\n\n    # If there is no match, return an empty string\n    if not match:\n        return \"\"\n\n    # Get the length of the longest match\n    max_length = len(max(match, key=len))\n\n    # Return a string of back-ticks with a length equal to the maximum length of the fence\n    return \"`\" * (max_length + 1)", "idx": 1717}
{"namespace": "zulipterminal.helper.open_media", "completion": "    from zulipterminal.config.regexes import REGEX_MEDIA_EXTENSION\n    from zulipterminal.config.regexes import REGEX_MEDIA_EXTENSION_WITH_DOT\n    from zulipterminal.config.regexes import REGEX_MEDIA_EXTENSION_WITH_DOT_AND_SPACE\n    from zulipterminal.config.regexes import REGEX_MEDIA_EXTENSION_WITH_SPACE\n    from zulipterminal.config.regexes import REGEX_MEDIA_EXTENSION_WITH_SPACE_AND_DOT\n    from zulipterminal.config.regexes import REGEX_MEDIA_EXTENSION_WITH_SPACE_AND_DOT_AND_SPACE\n    from zulipterminal.config.regexes import REGEX_MEDIA_EXTENSION_WITH_SPACE_AND_DOT_AND_SPACE_AND_SPACE\n    from zulipterminal.config.regexes import REGEX_MEDIA_EXTENSION_WITH_SPACE_AND_DOT_AND_SPACE_AND_SPACE_AND_SPACE\n    from zulipterminal.config.regexes import REGEX_MEDIA_EXTENSION_WITH_SPACE_AND_DOT_AND_SPACE_AND_SPACE_AND_SPACE_AND_SPACE\n    from zulipterminal.config.regexes import REGEX_MEDIA_EXTENSION_WITH_SPACE_AND_DOT_AND_SPACE_AND_SPACE_AND_SPACE_AND_SPACE_AND_SPACE\n    from zulipterminal.config.regexes import REGEX_MEDIA_EXTENSION_WITH_SPACE_AND_DOT_AND_SPACE_AND_SPACE_AND_SPACE_AND_SPACE_AND_SPACE_AND_SPACE\n    from zulipterminal.config.regexes import REGEX_MEDIA_EXTENSION_WITH_SPACE_AND_DOT_AND_SPACE_AND", "idx": 1718}
{"namespace": "zulipterminal.server_url.encode_stream", "completion": "    stream_name = stream_name.replace(\" \", \"-\")\n    return f\"{stream_id}-{hash_util_encode(stream_name)}\"", "idx": 1719}
{"namespace": "zulipterminal.server_url.near_message_url", "completion": "    if message[\"type\"] == \"stream\":\n        return near_stream_message_url(server_url, message)\n    elif message[\"type\"] == \"private\":\n        return near_pm_message_url(server_url, message)\n    else:\n        raise ValueError(f\"Invalid message type: {message['type']}\")", "idx": 1720}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.update_recipients", "completion": "        # Extract the recipient emails from the input text\n        recipient_emails = self.extract_recipient_emails(write_box.edit_text)\n\n        # Set the recipient user IDs in the WriteBox instance\n        self.recipient_user_ids = [\n            self.model.user_email_id_dict[email]\n            for email in recipient_emails\n            if email in self.model.user_email_id_dict\n        ]\n        self.recipient_emails = recipient_emails\n\n        # Update the typing recipient user IDs\n        self.typing_recipient_user_ids = [\n            user_id\n            for user_id in self.recipient_user_ids\n            if user_id != self.model.user_id\n        ]\n\n        # Update the recipient info in the header\n        recipient_info = \", \".join(\n            [\n                f\"{self.model.user_dict[email]['full_name']} <{email}>\"\n                for email in self.recipient_emails\n            ]\n        )\n        self.to_write_box.edit_text = recipient_info\n", "idx": 1721}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.stream_box_view", "completion": "        self.set_editor_mode()\n        self.compose_box_status = \"open_with_stream\"\n        self.stream_id = stream_id\n        self.recipient_user_ids = self.model.get_other_subscribers_in_stream(\n            stream_id=stream_id\n        )\n        self.msg_write_box = ReadlineEdit(\n            multiline=True, max_char=self.model.max_message_length\n        )\n        self.msg_write_box.enable_autocomplete(\n            func=self.generic_autocomplete,\n            key=primary_key_for_command(\"AUTOCOMPLETE\"),\n            key_reverse=primary_key_for_command(\"AUTOCOMPLETE_REVERSE\"),\n        )\n        self.msg_write_box.set_completer_delims(DELIMS_MESSAGE_COMPOSE)\n\n        self.title_write_box = ReadlineEdit(\n            edit_text=title, max_char=self.model.max_topic_length\n        )\n        self.title_write_box.enable_autocomplete(\n            func=self._topic_box_autocomplete,\n            key=primary_key_for_command(\"AUTOCOMPLETE\"),\n            key_reverse=primary_key_for_command(\"AUTOCOMPLETE_REVERSE\"),\n        )\n        self.title_write_box.set_completer_delims(\"\")\n\n        # NOTE: stream marker should be set during initialization\n        self.header_write_box = urwid.Columns(\n            [\n                (\"pack\", urwid.Text((\"default\", \"?\"))),\n                self.stream_write_box,\n                (\"pack\", urwid.Text(STREAM_TOPIC_SEPARATOR)),\n                self.title_write_box,\n            ],\n            dividechars=1,\n        )\n        header_line_box = urwid.LineBox(\n            self.header_write_box,\n            tlcorner=\"\u2501\",\n           ", "idx": 1722}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.stream_box_edit_view", "completion": "        self.stream_write_box = urwid.Text(caption)\n        self._setup_common_stream_compose(stream_id, caption, title)\n\n        # Add an edit mode button to the header write box\n        self.header_write_box.contents[self.FOCUS_HEADER_BOX_EDIT] = (\n            EditModeButton(\n                self.view.controller.enter_stream_box_edit_mode_with,\n                self.view.controller.enter_stream_box_view_mode_with,\n                self.stream_id,\n                self.title_write_box.edit_text,\n            ),\n            self.header_write_box.contents[self.FOCUS_HEADER_BOX_EDIT][1],\n        )\n\n        # Use and set a callback to set the stream marker\n        self._set_stream_write_box_style(None, caption)\n        urwid.connect_signal(\n            self.stream_write_box, \"change\", self._set_stream_write_box_style\n        )", "idx": 1723}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._set_stream_write_box_style", "completion": "        if new_text:\n            if self.model.is_valid_stream(new_text):\n                stream_info = self.model.stream_dict[new_text]\n                stream_color = stream_info[\"color\"]\n                stream_marker = stream_info[\"stream_marker\"]\n                self.header_write_box.widget_list[0].set_text(stream_marker)\n                self.header_write_box.widget_list[0].set_foreground_color(stream_color)\n            else:\n                self.header_write_box.widget_list[0].set_text(\"?\")\n                self.header_write_box.widget_list[0].set_foreground_color(\"default\")", "idx": 1724}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._to_box_autocomplete", "completion": "        users = self.view.users\n        recipients = [\n            recipient.strip()\n            for recipient in self.to_write_box.edit_text.split(\",\")\n            if recipient.strip()  # This condition avoids whitespace recipients (\",  ,\")\n        ]\n        latest_recipient = recipients[-1]\n        matching_users = [\n            user\n            for user in users\n            if user[\"email\"].startswith(latest_recipient)\n            and user[\"email\"] != latest_recipient\n        ]\n        autocompleted_recipients = [\n            f\"{user['full_name']} <{user['email']}>\"\n            for user in matching_users\n        ]\n        autocompleted_recipients_string = \", \".join(autocompleted_recipients)\n        if autocompleted_recipients_string:\n            autocompleted_recipients_string += \", \"\n        autocompleted_recipients_string += \", \".join(recipients)\n        user_names = [\n            user[\"full_name\"] for user in matching_users if user[\"email\"] != latest_recipient\n        ]\n        return self._process_typeaheads(\n            autocompleted_recipients_string, state, user_names\n        )\n", "idx": 1725}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._topic_box_autocomplete", "completion": "        topics_list = self.model.get_topics_in_stream(self.stream_id)\n        topic_names = [topic[\"name\"] for topic in topics_list]\n        return self._process_typeaheads(topic_names, state)", "idx": 1726}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._stream_box_autocomplete", "completion": "        stream_names = self.view.stream_names\n\n        stream_typeaheads = match_stream(stream_names, text)\n\n        # Typeaheads and suggestions are the same.\n        return self._process_typeaheads(stream_typeaheads, state, stream_typeaheads)", "idx": 1727}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.generic_autocomplete", "completion": "        # FIXME: Refactor when we have ~ Model.is_private_stream\n        if self.compose_box_status == \"open_with_private\":\n            return self._to_box_autocomplete(text, state)\n        elif self.compose_box_status == \"open_with_stream\":\n            if text.startswith(\"@\"):\n                return self._to_box_autocomplete(text, state)\n            elif text.startswith(\"#\"):\n                return self._topic_box_autocomplete(text, state)\n            elif text.startswith(\":\"):\n                return self._stream_box_autocomplete(text, state)\n            else:\n                return None\n        else:\n            return None\n", "idx": 1728}
{"namespace": "zulipterminal.ui_tools.boxes.PanelSearchBox.reset_search_text", "completion": "        self.caption = self.search_text\n        self.set_edit_text(\"\")", "idx": 1729}
{"namespace": "zulipterminal.ui_tools.boxes.PanelSearchBox.valid_char", "completion": "        if self.edit_text:\n            return super().valid_char(ch)\n        else:\n            if ch == \" \":\n                return False\n            return ch.isprintable()", "idx": 1730}
{"namespace": "zulipterminal.ui_tools.utils.is_muted", "completion": "    if msg[\"type\"] == \"private\":\n        return False\n    if model.narrow and model.narrow[0][\"oper\"] == \"stream\":\n        return False\n    if model.narrow and model.narrow[0][\"oper\"] == \"topic\":\n        return False\n    if msg[\"type\"] == \"stream\":\n        return model.index[\"streams\"][msg[\"display_recipient\"]][\"muted\"]\n    if msg[\"type\"] == \"private\":\n        return False\n    if msg[\"type\"] == \"stream\":\n        return model.index[\"streams\"][msg[\"display_recipient\"]][\"muted\"]\n    if msg[\"type\"] == \"topic\":\n        return model.index[\"topics\"][msg[\"display_recipient\"]][\"muted\"]\n    return False", "idx": 1731}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.update_count", "completion": "        if count == 0:\n            text_color = self.original_color\n        elif count > 0:\n            text_color = \"light green\"\n        elif count < 0:\n            text_color = \"light red\"\n\n        if count_style is None:\n            count_style = \"light gray\"\n\n        if count == 0:\n            count_text = \"\"\n        else:\n            count_text = str(count)\n\n        self.count = count\n        self.count_style = count_style\n\n        self.button_suffix.set_text((\"light gray\", count_text))\n        self.button_suffix.set_align_mode(\"right\")\n        self.button_suffix.set_wrap_mode(\"ellipsis\")\n        self.button_suffix.set_text_color(count_style)\n\n        self.button_prefix.set_text((\"light gray\", self.prefix_character))\n        self.button_prefix.set_text_color(text_color)\n\n        self.set_label(self._caption)\n        self.set_label_color(text_color)\n", "idx": 1732}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.update_widget", "completion": "        if count_text[0] is None:\n            prefix = \"\"\n        else:\n            prefix = f\"{count_text[0]} \"\n\n        self.button_prefix.set_text(prefix)\n        self._label.set_text(count_text[1])\n        self._label.set_text_color(text_color)\n        self.button_suffix.set_text(\"\")\n\n        return self._w", "idx": 1733}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.keypress", "completion": "        if key == \"enter\":\n            self.activate(key)\n            return None\n        return super().keypress(size, key)", "idx": 1734}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._parse_narrow_link", "completion": "        from zulipterminal.config.regexes import REGEX_INTERNAL_LINK_NARROW\n        match = re.match(REGEX_INTERNAL_LINK_NARROW, link)\n        if match is None:\n            return ParsedNarrowLink()\n        stream_data = match.group(\"stream_data\")\n        stream = cls._decode_stream_data(stream_data)\n        topic_name = match.group(\"topic_name\")\n        message_id = match.group(\"message_id\")\n        if message_id is not None:\n            message_id = cls._decode_message_id(message_id)\n        return ParsedNarrowLink(\n            narrow=match.group(\"narrow\"),\n            stream=stream,\n            topic_name=topic_name,\n            message_id=message_id,\n        )\n", "idx": 1735}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._validate_and_patch_stream_data", "completion": "        stream_data = parsed_link[\"stream\"]\n        stream_id = stream_data[\"stream_id\"]\n        stream_name = stream_data[\"stream_name\"]\n\n        if stream_id is None and stream_name is None:\n            return \"Invalid stream data\"\n\n        if stream_id is not None:\n            if stream_id not in self.model.stream_dict:\n                return \"Invalid stream ID\"\n            stream_name = self.model.stream_dict[stream_id][\"name\"]\n            stream_data[\"stream_name\"] = stream_name\n\n        if stream_name is not None:\n            if stream_name not in self.model.stream_dict:\n                return \"Invalid stream name\"\n            stream_id = self.model.stream_dict[stream_name][\"id\"]\n            stream_data[\"stream_id\"] = stream_id\n\n        if stream_id not in self.model.subscribed_stream_ids:\n            return \"Stream not subscribed\"\n\n        return \"\"\n", "idx": 1736}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._validate_narrow_link", "completion": "        # Validate the parsed link.\n        if parsed_link[\"narrow\"] == \"stream\":\n            return self._validate_and_patch_stream_data(parsed_link)\n\n        elif parsed_link[\"narrow\"] == \"stream:topic\":\n            stream_id = parsed_link[\"stream\"][\"stream_id\"]\n            topic_name = parsed_link[\"topic_name\"]\n            if not self.model.is_user_subscribed_to_stream(stream_id):\n                return \"The stream seems to be either unknown or unsubscribed\"\n            if not self.model.is_valid_topic(stream_id, topic_name):\n                return \"The topic seems to be unknown\"\n\n        elif parsed_link[\"narrow\"] == \"stream:near\":\n            stream_id = parsed_link[\"stream\"][\"stream_id\"]\n            message_id = parsed_link[\"message_id\"]\n            if not self.model.is_user_subscribed_to_stream(stream_id):\n                return \"The stream seems to be either unknown or unsubscribed\"\n            if not self.model.is_valid_message_id(stream_id, message_id):\n                return \"The message seems to be unknown\"\n\n        elif parsed_link[\"narrow\"] == \"stream:topic:near\":\n            stream_id = parsed_link[\"stream\"][\"stream_id\"]\n            topic_name = parsed_link[\"topic_name\"]\n            message_id = parsed_link[\"message_id\"]\n            if not self.model.is_user_subscribed_to_stream(stream_id):\n                return \"The stream seems to be either unknown or unsubscribed\"\n            if not self.model.is_valid_topic(stream_id, topic_name):\n                return \"The topic seems to be unknown\"\n            if not self.model.is_valid_message_id(stream_id, message_id):\n                return \"The message seems to be unknown\"\n\n        return \"\"", "idx": 1737}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._switch_narrow_to", "completion": "        if parsed_link[\"narrow\"] == \"stream\":\n            stream_id = parsed_link[\"stream\"][\"stream_id\"]\n            stream_name = parsed_link[\"stream\"][\"stream_name\"]\n            self.controller.narrow_to_stream(stream_id=stream_id, stream_name=stream_name)\n        elif parsed_link[\"narrow\"] == \"stream:topic\":\n            stream_id = parsed_link[\"stream\"][\"stream_id\"]\n            stream_name = parsed_link[\"stream\"][\"stream_name\"]\n            topic_name = parsed_link[\"topic_name\"]\n            self.controller.narrow_to_topic(\n                stream_id=stream_id, stream_name=stream_name, topic_name=topic_name\n            )\n        elif parsed_link[\"narrow\"] == \"stream:near\":\n            stream_id = parsed_link[\"stream\"][\"stream_id\"]\n            stream_name = parsed_link[\"stream\"][\"stream_name\"]\n            message_id = parsed_link[\"message_id\"]\n            self.controller.narrow_to_near(\n                stream_id=stream_id, stream_name=stream_name, message_id=message_id\n            )\n        elif parsed_link[\"narrow\"] == \"stream:topic:near\":\n            stream_id = parsed_link[\"stream\"][\"stream_id\"]\n            stream_name = parsed_link[\"stream\"][\"stream_name\"]\n            topic_name = parsed_link[\"topic_name\"]\n            message_id = parsed_link[\"message_id\"]\n            self.controller.narrow_to_near(\n                stream_id=stream_id,\n                stream_name=stream_name,\n                topic_name=topic_name,\n                message_id=message_id,\n            )", "idx": 1738}
{"namespace": "zulipterminal.config.themes.complete_and_incomplete_themes", "completion": "    complete_themes = []\n    incomplete_themes = []\n\n    for theme_name, theme in THEMES.items():\n        if all(\n            theme.get(style_name, None) == style_value\n            for style_name, style_value in REQUIRED_STYLES.items()\n        ):\n            if all(\n                theme[\"pygments\"].get(meta_name, None) == meta_value\n                for meta_name, meta_value in REQUIRED_META[\"pygments\"].items()\n            ):\n                complete_themes.append(theme_name)\n            else:\n                incomplete_themes.append(theme_name)\n        else:\n            incomplete_themes.append(theme_name)\n\n    return sorted(complete_themes), sorted(incomplete_themes)", "idx": 1739}
{"namespace": "zulipterminal.config.themes.validate_colors", "completion": "    theme_styles = THEMES[theme_name].STYLES\n    invalid_colors = []\n    for style in theme_styles:\n        for color in style:\n            if color not in valid_16_color_codes:\n                invalid_colors.append(color)\n    if invalid_colors:\n        raise InvalidThemeColorCode(\n            f\"Invalid color codes in theme {theme_name}: {invalid_colors}\"\n        )", "idx": 1740}
{"namespace": "zulipterminal.config.themes.parse_themefile", "completion": "    theme_spec = []\n    for style_name, (fg, bg) in theme_styles.items():\n        if color_depth == 1:\n            fg = fg.split()[0]\n            bg = bg.split()[0]\n        elif color_depth == 16:\n            fg = fg.split()[0]\n            bg = bg.split()[0]\n        elif color_depth == 256:\n            fg = fg.split()[1]\n            bg = bg.split()[1]\n        elif color_depth == 2**24:\n            fg = fg.split()[2]\n            bg = bg.split()[2]\n        theme_spec.append((style_name, fg, bg))\n    return theme_spec", "idx": 1741}
{"namespace": "zulipterminal.config.themes.add_pygments_style", "completion": "    pygments_styles = theme_meta[\"pygments\"][\"styles\"]\n    pygments_background = theme_meta[\"pygments\"][\"background\"]\n    pygments_overrides = theme_meta[\"pygments\"][\"overrides\"]\n\n    # Add Pygments styles to the Urwid theme\n    for style_name, (fg, bg) in pygments_styles.items():\n        fg_code16, fg_code256, fg_code24, *fg_props = fg.value.split()\n        bg_code16, bg_code256, bg_code24, *bg_props = bg.value.split()\n\n        new_style: StyleSpec\n        if color_depth == 1:\n            new_style = (style_name, \"\", \"\", REQUIRED_STYLES[style_name])\n\n        elif color_depth == 16:\n            fg = \" \".join([fg_code16] + fg_props).replace(\"_\", \" \")\n            bg = \" \".join([bg_code16] + bg_props).replace(\"_\", \" \")\n            new_style = (style_name, fg, bg)\n\n        elif color_depth == 256:\n            fg = \" \".join([fg_code256] + fg_props).lower()\n            bg = \" \".join([bg_code256] + bg_props).lower()\n            new_style = (style_name, \"\", \"\", \"\", fg, bg)\n\n        elif color_depth == 2**24:\n            fg = \" \".join([fg_code24] + fg_props).lower()\n            bg = \" \".join([bg_code24] + bg_props).lower()\n            new_style = (style_name, \"\", \"\", \"\", fg, bg)\n\n        urwid_theme.append(new_style)\n\n    # Add Pygments background color to the Urwid theme\n    if pygments_background:\n        urwid_theme.append((\"\", \"\", \"\", \"\", \"\", pygments_background))", "idx": 1742}
{"namespace": "zulipterminal.config.keys.is_command_key", "completion": "    if command not in KEY_BINDINGS:\n        raise InvalidCommand(f\"Invalid command: {command}\")\n\n    if \"keys\" not in KEY_BINDINGS[command]:\n        return False\n\n    return key in KEY_BINDINGS[command][\"keys\"]\n\n", "idx": 1743}
{"namespace": "zulipterminal.config.keys.keys_for_command", "completion": "    try:\n        return KEY_BINDINGS[command][\"keys\"]\n    except KeyError as exception:\n        raise InvalidCommand(command)", "idx": 1744}
{"namespace": "zulipterminal.config.keys.commands_for_random_tips", "completion": "    return [\n        command\n        for command, key_binding in KEY_BINDINGS.items()\n        if not key_binding.get(\"excluded_from_random_tips\", False)\n    ]", "idx": 1745}
{"namespace": "hypertools.datageometry.DataGeometry.transform", "completion": "        # if no data is passed, return the transformed data\n        if data is None:\n            return self.xform_data\n\n        # if data is a list, transform each element\n        if isinstance(data, list):\n            return list(map(self.transform, data))\n\n        # if data is a pandas dataframe, transform each column\n        if isinstance(data, pd.DataFrame):\n            return data.apply(self.transform)\n\n        # if data is a numpy array, transform it\n        if isinstance(data, np.ndarray):\n            return self.reduce['model'].transform(data)\n\n        # if data is a string, convert it to a numpy array and transform it\n        if isinstance(data, str):\n            return self.reduce['model'].transform(convert_text(data))\n\n        # if data is a dict, transform each value\n        if isinstance(data, dict):\n            return {k: self.transform(v) for k, v in data.items()}\n\n        # if data is a tuple, transform each element\n        if isinstance(data, tuple):\n            return tuple(self.transform(d) for d in data)\n\n        # if data is a set, transform each element\n        if isinstance(data, set):\n            return set(self.transform(d) for d in data)\n\n        # if data is a frozenset, transform each element\n        if isinstance(data, frozenset):\n            return frozenset(self.transform(d) for d in data)\n\n        # if data is a generator, transform each element\n        if isinstance(data, GeneratorType):\n            return (self.transform(d) for d in data)\n\n        # if data is a generator, transform each element\n        if isinstance(data, GeneratorType):\n            return (self.transform(d) for d in data)\n\n        # if data is a generator, transform each element\n        if isinstance(data, GeneratorType):\n            return (self.transform(d) for d in data)\n\n        # if data is a generator, transform each element\n        if", "idx": 1746}
{"namespace": "hypertools.datageometry.DataGeometry.plot", "completion": "        # if no new data passed,\n        if data is None:\n            data = self.xform_data\n\n        # format the data\n        formatted = format_data(\n            data,\n            semantic=self.semantic,\n            vectorizer=self.vectorizer,\n            corpus=self.corpus,\n            ppca=True)\n\n        # normalize the data\n        norm = normalize(formatted, normalize=self.normalize)\n\n        # reduce the data\n        reduction = reduce(\n            norm,\n            reduce=self.reduce,\n            ndims=self.reduce['params']['n_components'])\n\n        # align the data\n        alignment = align(reduction, align=self.align)\n\n        # plot the data\n        return plot(alignment, **kwargs)", "idx": 1747}
{"namespace": "awesome_autodl.autodl_topic2papers", "completion": "    topic2path = autodl_topic2path()\n    topic2papers = OrderedDict()\n    for topic, path in topic2path.items():\n        topic2papers[topic] = AutoDLpaper.load_yaml(path)\n    return topic2papers", "idx": 1748}
{"namespace": "awesome_autodl.get_bib_abbrv_obj", "completion": "    from awesome_autodl.bib_abbrv import BibAbbreviations\n\n    xpath = get_bib_abbrv_file()\n    if not xpath.exists():\n        raise ValueError(f\"Can not find {xpath}.\")\n    return BibAbbreviations(xpath)", "idx": 1749}
{"namespace": "mssqlcli.localized_strings.translation", "completion": "    if languages is None:\n        languages = LANGUAGES\n\n    t = gettext.translation(domain, localedir, languages=languages)\n    t.install()\n    return t", "idx": 1750}
{"namespace": "mssqlcli.mssqlbuffer._is_query_executable", "completion": "    # Remove comments\n    sql = re.sub(r'--.*', '', sql)\n    # Remove open comments\n    sql = re.sub(r'/\\*.*', '', sql)\n    # Remove open quotes\n    sql = re.sub(r\"'(?:[^'\\\\]|\\\\.)*'\", '', sql)\n    # Remove open brackets\n    sql = re.sub(r'\\(', '', sql)\n    # Remove open braces\n    sql = re.sub(r'\\{', '', sql)\n    # Remove open parentheses\n    sql = re.sub(r'\\(', '', sql)\n    # Remove open square brackets\n    sql = re.sub(r'\\[', '', sql)\n    # Remove open angle brackets\n    sql = re.sub(r'\\<', '', sql)\n    # Remove open curly braces\n    sql = re.sub(r'\\{', '', sql)\n    # Remove open parentheses\n    sql = re.sub(r'\\(', '', sql)\n    # Remove open square brackets\n    sql = re.sub(r'\\[', '', sql)\n    # Remove open angle brackets\n    sql = re.sub(r'\\<', '', sql)\n    # Remove open curly braces\n    sql = re.sub(r'\\{', '', sql)\n    # Remove open parentheses\n    sql = re.sub(r'\\(', '', sql)\n    # Remove open square brackets\n    sql = re.sub(r'\\[', '', sql)\n    # Remove open angle brackets\n    sql = re.sub(r'\\<', '', sql)\n    # Remove open curly braces\n    sql = re.sub(r'\\{', '', sql)\n    # Remove open parentheses\n    sql = re.sub(r'\\(', '', sql)\n    # Remove open square brackets\n    sql = re.sub(r'\\[', '', sql)\n    # Remove open angle brackets\n    sql = re.sub(r'\\<', '', sql)\n    # Remove open curly braces\n    sql = re.sub(r'\\{', '', sql)\n    # Remove open parentheses", "idx": 1751}
{"namespace": "mssqlcli.telemetry.conclude", "completion": "    _session.end_time = datetime.now()\n    payload = _session.generate_payload()\n    if payload is None:\n        return\n\n    if separate_process:\n        _upload_in_separate_process(payload, service_endpoint_uri)\n    else:\n        _upload_in_current_process(payload, service_endpoint_uri)\n\n", "idx": 1752}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.start", "completion": "        self.request_thread = threading.Thread(target=self.request_thread_func, name=self.REQUEST_THREAD_NAME)\n        self.request_thread.daemon = True\n        self.request_thread.start()\n\n        self.response_thread = threading.Thread(target=self.response_thread_func, name=self.RESPONSE_THREAD_NAME)\n        self.response_thread.daemon = True\n        self.response_thread.start()\n", "idx": 1753}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.submit_request", "completion": "        if method is None or params is None:\n            raise ValueError(u'Method and params cannot be None.')\n\n        request = {\n            u'jsonrpc': u'2.0',\n            u'method': method,\n            u'params': params,\n            u'id': request_id\n        }\n\n        self.request_queue.put(request)\n", "idx": 1754}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.get_response", "completion": "        if request_id in self.response_map:\n            response = self.response_map[request_id].get()\n            if response:\n                return response\n\n        if owner_uri in self.response_map:\n            response = self.response_map[owner_uri].get()\n            if response:\n                return response\n\n        if self.exception_queue.qsize() > 0:\n            raise self.exception_queue.get()\n\n        return None", "idx": 1755}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.shutdown", "completion": "        self.cancel = True\n        self.request_queue.put(None)\n        self.request_thread.join()\n        self.writer.close()\n        logger.debug('Json Rpc client shutdown.')", "idx": 1756}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcWriter.send_request", "completion": "        content = {u'method': method, u'params': params, u'id': request_id}\n        content_json = json.dumps(content)\n        content_length = len(content_json)\n        header = self.HEADER.format(content_length)\n        self.stream.write(header.encode(self.encoding))\n        self.stream.write(content_json.encode(self.encoding))\n        self.stream.flush()\n", "idx": 1757}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.read_response", "completion": "        while self.needs_more_data:\n            try:\n                self._read_header()\n                self._read_content()\n                self._trim_buffer()\n                return self._parse_content()\n\n            except ValueError as ex:\n                logger.debug(u'Read Response encountered exception %s', ex)\n                raise\n", "idx": 1758}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.read_next_chunk", "completion": "        # If the buffer needs to be resized, resize it.\n        if self.buffer_needs_resize():\n            self.resize_buffer()\n\n        # Read data from the stream into the buffer.\n        try:\n            read_bytes = self.stream.readinto(self.buffer, self.buffer_end_offset)\n        except ValueError as ex:\n            logger.debug(u'JSON RPC Reader on read_next_chunk() encountered exception: %s', ex)\n            raise\n\n        # Update the buffer offset.\n        self.buffer_end_offset += read_bytes\n\n        # If the stream is empty or closed externally, raise an exception.\n        if read_bytes == 0:\n            raise ValueError(u'Stream is empty or closed externally.')\n\n        return True\n", "idx": 1759}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.try_read_headers", "completion": "        # Find the end of the header.\n        header_end_offset = self.buffer.find(b'\\r\\n\\r\\n', self.read_offset)\n        if header_end_offset == -1:\n            # No header found.\n            return False\n\n        # Split the header into lines.\n        header_lines = self.buffer[self.read_offset:header_end_offset].split(b'\\r\\n')\n        # Extract the key-value pairs from the header lines.\n        for line in header_lines:\n            if line:\n                key, value = line.split(b':', 1)\n                self.headers[key.decode(self.encoding)] = value.decode(self.encoding)\n\n        # Check if the 'content-length' header is present.\n        if u'content-length' not in self.headers:\n            raise LookupError(u'No content-length header found.')\n\n        # Store the expected content length.\n        self.expected_content_length = int(self.headers[u'content-length'])\n        # Set the read state to Content.\n        self.read_state = ReadState.Content\n        # Update the read offset.\n        self.read_offset = header_end_offset + 4\n        return True\n", "idx": 1760}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.close", "completion": "        try:\n            self.stream.close()\n        except AttributeError:\n            pass", "idx": 1761}
{"namespace": "mssqlcli.packages.prioritization.PrevalenceCounter.update", "completion": "        # Parse the input text using sqlparse\n        parsed = sqlparse.parse(text)\n\n        # Iterate over the parsed statements\n        for statement in parsed:\n            # Iterate over the tokens in the statement\n            for token in statement.tokens:\n                # Check if the token is a keyword\n                if token.ttype is Name.Keyword:\n                    # Increment the keyword count\n                    self.keyword_counts[token.value] += 1\n                # Check if the token is a name\n                elif token.ttype is Name:\n                    # Increment the name count\n                    self.name_counts[token.value] += 1\n", "idx": 1762}
{"namespace": "mssqlcli.packages.sqlcompletion.suggest_type", "completion": "    if full_text.startswith('\\\\i '):\n        return Path(), None\n\n    try:\n        statement = SqlStatement(full_text, text_before_cursor)\n    except sqlparse.exceptions.ParserError:\n        return None, None\n\n    if statement.is_special():\n        return parse_special_command(statement)\n\n    last_token = statement.last_token\n\n    if last_token.is_keyword:\n        return Keyword(last_token), None\n\n    if last_token.is_identifier:\n        if last_token.get_name() == 'from':\n            return FromClauseItem(), None\n\n        if last_token.get_name() == 'join':\n            return Join(), None\n\n        if last_token.get_name() == 'on':\n            return JoinCondition(), None\n\n        if last_token.get_name() == 'as':\n            return Alias(), None\n\n        if last_token.get_name() == 'select':\n            return Column(), None\n\n        if last_token.get_name() == 'where':\n            return Column(require_last_table=True), None\n\n        if last_token.get_name() == 'order':\n            return Column(require_last_table=True), None\n\n        if last_token.get_name() == 'group':\n            return Column(require_last_table=True), None\n\n        if last_token.get_name() == 'having':\n            return Column(require_last_table=True), None\n\n        if last_token.get_name() == 'values':\n            return Column(require_last_table=True), None\n\n        if last_token.get_name() == 'into':\n            return Column(require_last_table=True), None\n\n        if last_token.get_name() == 'update':\n            return Column(require_last_table=True), None\n\n        if last_token.get_name() == 'set':\n            return Column(require_last_table=True), None\n\n        if last_token.get_name() == 'delete':", "idx": 1763}
{"namespace": "mssqlcli.packages.parseutils.ctes.extract_ctes", "completion": "    # Parse the SQL query using the parser\n    parsed = parse(sql)\n\n    # Check if the first meaningful token is \"WITH\", indicating the presence of CTEs\n    if not parsed[0].is_group or parsed[0].token_first(skip_cm=True).normalized != 'with':\n        return [], sql\n\n    # Extract the CTEs from the query\n    ctes = []\n    for token in parsed[0].tokens:\n        if token.normalized == 'with':\n            # Find the start and stop indices of the CTE\n            start = token.start_pos\n            stop = token.end_pos\n            for t in token.get_sublists():\n                if t.is_group and t.token_first(skip_cm=True).normalized == 'as':\n                    stop = t.end_pos\n                    break\n\n            # Extract the name and columns of the CTE\n            name = token.get_sublists()[0][1].value\n            columns = [c.value for c in token.get_sublists()[0][3].get_sublists()[0]]\n\n            # Create a TableExpression namedtuple and append it to the list of CTEs\n            ctes.append(TableExpression(name, columns, start, stop))\n\n    # Strip the CTEs from the original SQL query\n    sql = sql[:ctes[0].start] + sql[ctes[-1].stop:]\n\n    return ctes, sql", "idx": 1764}
{"namespace": "mssqlcli.packages.parseutils.tables.extract_tables", "completion": "    parsed = sqlparse.parse(sql)[0]\n    return extract_table_identifiers(extract_from_part(parsed))", "idx": 1765}
{"namespace": "googleapiclient.channel.Channel.body", "completion": "        body = {\n            \"id\": self.id,\n            \"token\": self.token,\n            \"type\": self.type,\n            \"address\": self.address,\n        }\n        if self.expiration:\n            body[\"expiration\"] = self.expiration\n        if self.params:\n            body[\"params\"] = self.params\n        if self.resource_id:\n            body[\"resourceId\"] = self.resource_id\n        if self.resource_uri:\n            body[\"resourceUri\"] = self.resource_uri\n        return body\n", "idx": 1766}
{"namespace": "googleapiclient.channel.Channel.update", "completion": "        for k, v in CHANNEL_PARAMS.items():\n            if k in resp:\n                setattr(self, v, resp[k])", "idx": 1767}
{"namespace": "googleapiclient.channel.notification_from_headers", "completion": "    headers = _upper_header_keys(headers)\n    message_number = int(headers[X_GOOG_MESSAGE_NUMBER])\n    state = headers[X_GOOG_RESOURCE_STATE]\n    resource_uri = headers[X_GOOG_RESOURCE_URI]\n    resource_id = headers[X_GOOG_RESOURCE_ID]\n    return Notification(message_number, state, resource_uri, resource_id)", "idx": 1768}
{"namespace": "googleapiclient.channel.new_webhook_channel", "completion": "    from googleapiclient import errors\n    if not url:\n        raise errors.InvalidArgumentError(\"url must be specified\")\n    if expiration is None:\n        expiration = datetime.datetime.utcnow() + datetime.timedelta(days=30)\n    expiration = int((expiration - EPOCH).total_seconds() * 1000)\n    return Channel(\n        \"web_hook\",\n        str(uuid.uuid4()),\n        token,\n        url,\n        expiration=expiration,\n        params=params,\n    )", "idx": 1769}
{"namespace": "googleapiclient.model.BaseModel._build_query", "completion": "        if self.alt_param:\n            params[self.alt_param] = self.alt_param\n        query = []\n        for key, value in params.items():\n            if isinstance(value, list):\n                for v in value:\n                    if callable(v):\n                        v = v()\n                    query.append((key, urllib.quote(v.encode(\"utf-8\"))))\n            else:\n                if callable(value):\n                    value = value()\n                query.append((key, urllib.quote(value.encode(\"utf-8\"))))\n        return urllib.urlencode(query)\n\n", "idx": 1770}
{"namespace": "googleapiclient.model.BaseModel.response", "completion": "        self._log_response(resp, content)\n        if resp.status >= 400:\n            raise HttpError(resp, content)\n        if resp.status == 204:\n            return self.no_content_response\n        return self.deserialize(content)\n\n", "idx": 1771}
{"namespace": "googleapiclient.model.makepatch", "completion": "    patch = {}\n    for key, value in modified.items():\n        if key not in original:\n            patch[key] = value\n        elif isinstance(value, dict):\n            patch[key] = makepatch(original[key], value)\n        elif isinstance(value, list):\n            patch[key] = []\n            for i, item in enumerate(value):\n                if i >= len(original[key]):\n                    patch[key].append(item)\n                elif isinstance(item, dict):\n                    patch[key].append(makepatch(original[key][i], item))\n                else:\n                    if item != original[key][i]:\n                        patch[key].append(item)\n        else:\n            if value != original[key]:\n                patch[key] = value\n    return patch", "idx": 1772}
{"namespace": "googleapiclient._helpers.update_query_params", "completion": "    if not uri:\n        raise ValueError(\"URI cannot be empty.\")\n    if not params:\n        return uri\n    parsed_uri = urllib.parse.urlparse(uri)\n    query_params = urllib.parse.parse_qs(parsed_uri.query)\n    for key, value in params.items():\n        if key in query_params:\n            raise ValueError(\"URI contains repeated key: %s\" % key)\n        query_params[key] = value\n    new_query = urllib.parse.urlencode(query_params, doseq=True)\n    return urllib.parse.urlunparse(\n        (parsed_uri.scheme, parsed_uri.netloc, parsed_uri.path, parsed_uri.params, new_query, parsed_uri.fragment)\n    )", "idx": 1773}
{"namespace": "googleapiclient._helpers._add_query_parameter", "completion": "    if value is None:\n        return url\n    url_parts = list(urllib.parse.urlparse(url))\n    query = dict(urllib.parse.parse_qsl(url_parts[4]))\n    query[name] = value\n    url_parts[4] = urllib.parse.urlencode(query)\n    return urllib.parse.urlunparse(url_parts)", "idx": 1774}
{"namespace": "gif_for_cli.display.display_txt_frames", "completion": "    try:\n        for i in range(num_loops):\n            for frame in txt_frames:\n                stdout.write(frame)\n                stdout.flush()\n                time.sleep(seconds_per_frame)\n    except KeyboardInterrupt:\n        pass\n\n", "idx": 1775}
{"namespace": "pyVmomi.SoapAdapter.SoapResponseDeserializer.Deserialize", "completion": "        self.deser.Deserialize(ParserCreate(namespace_separator=NS_SEP),\n                               resultType,\n                               nsMap=nsMap)\n        # Many existing tests pass in str directly in python2 for testing purpose.\n        # But in python3 the input become unicode and the handling will fall into\n        # ParseFile case.\n        # Adding unicode input support to make it more test friendly.\n        if isinstance(response, six.binary_type) or isinstance(\n                response, six.text_type):\n            self.deser.parser.Parse(response)\n        else:\n            self.deser.parser.ParseFile(response)\n        fault = self.deser.parser.ErrorCode\n        if fault:\n            raise fault\n        return self.deser.GetResult()\n\n", "idx": 1776}
{"namespace": "pyVmomi.VmomiSupport.GetRequestContext", "completion": "    if not hasattr(s, '_context'):\n        s._context = StringDict()\n    return s._context\n\n", "idx": 1777}
{"namespace": "pycoin.bloomfilter.filter_size_required", "completion": "    return max(1, min(36000, int((-1 / (LOG_2 ** 2) * element_count * math.log(false_positive_probability)) / 8)))\n\n", "idx": 1778}
{"namespace": "pycoin.bloomfilter.BloomFilter.add_spendable", "completion": "        if isinstance(spendable, str):\n            the_hash160 = a2b_hashed_base58(spendable)[1:]\n        elif isinstance(spendable, bytes):\n            the_hash160 = spendable\n        else:\n            raise ValueError(\"spendable must be a string or bytes\")\n        self.add_item(the_hash160)\n", "idx": 1779}
{"namespace": "pycoin.bloomfilter.murmur3", "completion": "    c1 = 0xcc9e2d51\n    c2 = 0x1b873593\n    r1 = 15\n    r2 = 13\n    m = 5\n    n = 0xe6546b64\n    h = 0\n    k = 0\n    length = len(data)\n\n    h = seed\n\n    # body\n    for i in range(0, length, 4):\n        k = (data[i] << 24) | (data[i + 1] << 16) | (data[i + 2] << 8) | data[i + 3]\n        k = (k * c1) & 0xffffffff\n        k = (k << r1) | (k >> (32 - r1))\n        k = k * c2 & 0xffffffff\n\n        h = (h ^ k)\n        h = (h << r2) | (h >> (32 - r2))\n        h = h * m + n\n\n    # tail\n    k = 0\n    if length & 3:\n        k = data[length - (length & 3) - 1] << 24\n        if length & 2:\n            k |= data[length - (length & 3) - 2] << 16\n            if length & 1:\n                k |= data[length - (length & 3) - 3] << 8\n\n    k = (k * c1) & 0xffffffff\n    k = (k << r1) | (k >> (32 - r1))\n    k = k * c2 & 0xffffffff\n    h = (h ^ k)\n\n    # finalization\n    h ^= length\n    h ^= h >> 16\n    h = (h * 0x85ebca6b) & 0xffffffff\n    h ^= h >> 13\n    h = (h * 0xc2b2ae35) & 0xffffffff\n    h ^= h >> 16\n\n    return h", "idx": 1780}
{"namespace": "pycoin.networks.registry.network_for_netcode", "completion": "    for prefix in search_prefixes():\n        for _, name, _ in pkgutil.iter_modules([prefix]):\n            try:\n                module = importlib.import_module(prefix + \".\" + name)\n            except ImportError:\n                continue\n            if hasattr(module, \"symbol\"):\n                if module.symbol == symbol:\n                    module.symbol = symbol\n                    return module\n    raise ValueError(\"No network found for netcode %s\" % symbol)", "idx": 1781}
{"namespace": "pycoin.satoshi.IntStreamer.IntStreamer.int_from_script_bytes", "completion": "        if len(s) == 0:\n            return 0\n        s = s[::-1]\n        v = s[0] & 0x7f\n        if require_minimal and v == 0 and len(s) > 1:\n            raise ScriptError(\"non-minimally encoded integer\")\n        if s[0] & 0x80:\n            v = -v\n        for i in range(1, len(s)):\n            v = (v << 8) | s[i]\n        if v & 0x80000000:\n            v = -((v & 0x7fffffff) + 1)\n        return v\n", "idx": 1782}
{"namespace": "pycoin.satoshi.stackops.do_OP_RIPEMD160", "completion": "    v = stack.pop()\n    stack.append(hashlib.new(\"ripemd160\", v).digest())\n\n", "idx": 1783}
{"namespace": "pycoin.satoshi.stackops.do_OP_HASH160", "completion": "    from ..encoding.hash import hash160\n    stack.append(hash160(stack.pop()).digest())\n\n", "idx": 1784}
{"namespace": "pycoin.satoshi.stackops.do_OP_HASH256", "completion": "    stack.append(hashlib.sha256(stack.pop()).digest())", "idx": 1785}
{"namespace": "pycoin.services.providers.providers_for_config_string", "completion": "    providers = []\n    for descriptor in config_string.split():\n        provider = provider_for_descriptor_and_netcode(descriptor, netcode)\n        if provider is None:\n            warnings.warn(\"could not parse descriptor %s\" % descriptor)\n        else:\n            providers.append(provider)\n    return providers", "idx": 1786}
{"namespace": "pycoin.services.providers.get_default_providers_for_netcode", "completion": "    if netcode is None:\n        netcode = get_current_netcode()\n    if not hasattr(THREAD_LOCALS, \"default_providers_for_netcode\"):\n        THREAD_LOCALS.default_providers_for_netcode = {}\n    if netcode not in THREAD_LOCALS.default_providers_for_netcode:\n        THREAD_LOCALS.default_providers_for_netcode[netcode] = providers_for_netcode_from_env(netcode)\n    return THREAD_LOCALS.default_providers_for_netcode[netcode]", "idx": 1787}
{"namespace": "pycoin.services.providers.set_default_providers_for_netcode", "completion": "    if not hasattr(THREAD_LOCALS, \"providers\"):\n        THREAD_LOCALS.providers = {}\n    THREAD_LOCALS.providers[netcode] = provider_list", "idx": 1788}
{"namespace": "pycoin.blockchain.BlockChain.BlockChain.tuple_for_index", "completion": "        if index < 0:\n            index = self.length() + index\n        if index < len(self._locked_chain):\n            return self._locked_chain[index]\n        else:\n            index -= len(self._locked_chain)\n            if index < len(self._longest_local_block_chain()):\n                return self._longest_local_block_chain()[index]\n            else:\n                return self._longest_chain_cache[index]", "idx": 1789}
{"namespace": "pycoin.blockchain.ChainFinder.ChainFinder.find_ancestral_path", "completion": "        if h1 == h2:\n            return [], []\n\n        if h1 in path_cache:\n            path1 = path_cache[h1]\n        else:\n            path1 = self.maximum_path(h1, path_cache)\n\n        if h2 in path_cache:\n            path2 = path_cache[h2]\n        else:\n            path2 = self.maximum_path(h2, path_cache)\n\n        if path1[-1] == path2[-1]:\n            return path1, path2\n\n        i = 0\n        while path1[i] == path2[i]:\n            i += 1\n\n        return path1[i:], path2[i:]", "idx": 1790}
{"namespace": "pycoin.contrib.bech32m.bech32_encode", "completion": "    combined = data + bech32_create_checksum(hrp, data, spec)\n    return hrp + '1' + ''.join([CHARSET[d] for d in combined])\n\n", "idx": 1791}
{"namespace": "pycoin.contrib.bech32m.decode", "completion": "    hrpgot, data, spec = bech32_decode(addr)\n    if hrpgot != hrp:\n        return (None, None)\n    decoded = convertbits(data[1:], 5, 8, False)\n    if decoded is None or len(decoded) < 2 or len(decoded) > 40:\n        return (None, None)\n    if data[0] > 16:\n        return (None, None)\n    if data[0] == 0 and len(decoded) != 20 and len(decoded) != 32:\n        return (None, None)\n    if data[0] == 0x01 and len(decoded) != 20:\n        return (None, None)\n    if data[0] == 0x02 and len(decoded) != 32:\n        return (None, None)\n    return (data[0], decoded)\n\n", "idx": 1792}
{"namespace": "pycoin.crack.bip32.crack_bip32", "completion": "    for i in path:\n        if i == \"m\":\n            continue\n        elif i == \"M\":\n            secret_exponent = ascend_bip32(bip32_pub_node, secret_exponent, 0x80000000)\n        elif i == \"H\":\n            secret_exponent = ascend_bip32(bip32_pub_node, secret_exponent, 0x40000000)\n        elif i == \"L\":\n            secret_exponent = ascend_bip32(bip32_pub_node, secret_exponent, 0x20000000)\n        elif i == \"R\":\n            secret_exponent = ascend_bip32(bip32_pub_node, secret_exponent, 0x10000000)\n        elif i == \"+\":\n            secret_exponent = ascend_bip32(bip32_pub_node, secret_exponent, 0x08000000)\n        elif i == \"-\":\n            secret_exponent = ascend_bip32(bip32_pub_node, secret_exponent, 0x04000000)\n        elif i == \"~\":\n            secret_exponent = ascend_bip32(bip32_pub_node, secret_exponent, 0x02000000)\n        elif i == \"0\":\n            secret_exponent = ascend_bip32(bip32_pub_node, secret_exponent, 0x01000000)\n        elif i == \"1\":\n            secret_exponent = ascend_bip32(bip32_pub_node, secret_exponent, 0x00800000)\n        elif i == \"2\":\n            secret_exponent = ascend_bip32(bip32_pub_node, secret_exponent, 0x00400000)\n        elif", "idx": 1793}
{"namespace": "pycoin.message.PeerAddress.ip_bin_to_ip4_addr", "completion": "    return \".\".join(\"%d\" % v for v in struct.unpack(\">BBBB\", ip_bin))\n\n", "idx": 1794}
{"namespace": "pycoin.message.PeerAddress.PeerAddress.host", "completion": "        if self.ip_bin.startswith(IP4_HEADER):\n            return ip_bin_to_ip4_addr(self.ip_bin)\n        else:\n            return ip_bin_to_ip6_addr(self.ip_bin)\n", "idx": 1795}
{"namespace": "oletools.msodde.field_is_blacklisted", "completion": "    # split into components\n    parts = FIELD_WORD_REGEX.findall(contents)\n    if not parts:\n        return True\n    cmd = parts[0].lower()\n    if cmd not in FIELD_BLACKLIST_CMDS:\n        return True\n    # get the number of arguments and switches\n    n_required_args = FIELD_BLACKLIST[FIELD_BLACKLIST_CMDS.index(cmd)][1]\n    n_optional_args = FIELD_BLACKLIST[FIELD_BLACKLIST_CMDS.index(cmd)][2]\n    switches_with_args = FIELD_BLACKLIST[FIELD_BLACKLIST_CMDS.index(cmd)][3]\n    switches_without_args = FIELD_BLACKLIST[FIELD_BLACKLIST_CMDS.index(cmd)][4]\n    format_switches = FIELD_BLACKLIST[FIELD_BLACKLIST_CMDS.index(cmd)][5]\n    # check the number of arguments\n    if len(parts) - 1 < n_required_args:\n        return True\n    if len(parts) - 1 > n_required_args + n_optional_args:\n        return True\n    # check the switches\n    for switch in parts[1:]:\n        if switch in switches_with_args:\n            if len(parts) - 1 <= n_required_args:\n                return True\n        elif switch in switches_without_args:\n            if len(parts) - 1 > n_required_args:\n                return True\n        elif not FIELD_SWITCH_REGEX.match(switch):\n            return True\n    # check the format switches\n    if format_switches:\n        if len(parts) - 1 < n_required_args:\n            return True\n        for switch in parts[n_required_args + 1:]:\n            if switch not in format_switches:\n                return True\n    return False", "idx": 1796}
{"namespace": "oletools.ppt_record_parser.is_ppt", "completion": "    try:\n        ppt_parser = PptParser(filename)\n        ppt_parser.parse()\n        return True\n    except Exception:\n        return False\n\n", "idx": 1797}
{"namespace": "oletools.rtfobj.is_rtf", "completion": "    if treat_str_as_data:\n        data = arg\n    else:\n        data = open(arg, 'rb').read()\n    if data[:len(RTF_MAGIC)] == RTF_MAGIC:\n        return True\n    return False", "idx": 1798}
{"namespace": "oletools.oleobj.get_sane_embedded_filenames", "completion": "    # Get the filename from the input paths\n    base_filename = os.path.basename(filename)\n    src_filename = os.path.basename(src_path)\n    tmp_filename = os.path.basename(tmp_path)\n\n    # Sanitize the filename\n    sane_filename = sanitize_filename(base_filename, max_len=max_len)\n    sane_src_filename = sanitize_filename(src_filename, max_len=max_len)\n    sane_tmp_filename = sanitize_filename(tmp_filename, max_len=max_len)\n\n    # Preserve the file suffix\n    suffix = os.path.splitext(base_filename)[1]\n    sane_filename_with_suffix = sane_filename + suffix\n    sane_src_filename_with_suffix = sane_src_filename + suffix\n    sane_tmp_filename_with_suffix = sane_tmp_filename + suffix\n\n    # Generate multiple candidates\n    candidates = [sane_filename_with_suffix, sane_filename,\n                  sane_filename_with_suffix + str(noname_index),\n                  sane_filename + str(noname_index)]\n\n    # Add the sanitized filenames to the candidates\n    candidates.append(sane_src_filename_with_suffix)\n    candidates.append(sane_src_filename)\n    candidates.append(sane_src_filename_with_suffix + str(noname_index))\n    candidates.append(sane_src_filename + str(noname_index))\n\n    candidates.append(sane_tmp_filename_with_suffix)\n    candidates.append(sane_tmp_filename)\n    candidates.append(sane_tmp_filename_with_suffix + str(noname_index))\n    candidates.append(sane_tmp_filename + str(noname_index))\n\n    return candidates", "idx": 1799}
{"namespace": "oletools.ooxml.get_type", "completion": "    # Check if the file is a valid OOXML file\n    if not is_zipfile(filename):\n        return DOCTYPE_NONE\n\n    # Open the file as a ZipFile object\n    try:\n        zip_file = ZipFile(filename)\n    except BadZipfile:\n        return DOCTYPE_NONE\n\n    # Check if the file contains the required files\n    if FILE_CONTENT_TYPES not in zip_file.namelist() or FILE_RELATIONSHIPS not in zip_file.namelist():\n        return DOCTYPE_NONE\n\n    # Get the content types XML file\n    content_types_xml = zip_file.read(FILE_CONTENT_TYPES)\n\n    # Parse the content types XML file\n    try:\n        content_types_root = ET.fromstring(content_types_xml)\n    except ET.ParseError:\n        return DOCTYPE_NONE\n\n    # Check if the file is a Word document\n    for content_type in content_types_root.findall('{http://schemas.openxmlformats.org/package/2006/content-types}Override'):\n        if content_type.get('ContentType') in CONTENT_TYPES_WORD:\n            return DOCTYPE_WORD\n\n    # Check if the file is an Excel spreadsheet\n    for content_type in content_types_root.findall('{http://schemas.openxmlformats.org/package/2006/content-types}Override'):\n        if content_type.get('ContentType') in CONTENT_TYPES_EXCEL:\n            return DOCTYPE_EXCEL\n\n    # Check if the file is a PowerPoint presentation\n    for content_type in content_types_root.findall('{http://schemas.openxmlformats.org/package/2006/content-types}Override'):\n        if content_type.get('ContentType') in CONTENT_TYPES_PPT:\n            return DOCTYPE_POWERPOINT\n\n    # If none of the above conditions are met, the file is not a Word", "idx": 1800}
{"namespace": "oletools.ooxml.ZipSubFile.read", "completion": "        if self.closed:\n            raise IOError('file is closed')\n        if self.pos == self.size:\n            return b''\n        if size == -1:\n            size = self.size - self.pos\n        data = self.handle.read(size)\n        self.pos += len(data)\n        return data\n", "idx": 1801}
{"namespace": "oletools.ooxml.ZipSubFile.seek", "completion": "        if self.handle is None:\n            raise IOError('seek on closed handle')\n        if offset == io.SEEK_SET:\n            new_pos = pos\n        elif offset == io.SEEK_CUR:\n            new_pos = self.pos + pos\n        elif offset == io.SEEK_END:\n            new_pos = self.size + pos\n        else:\n            raise ValueError('invalid offset')\n        if new_pos < 0:\n            raise ValueError('negative seek position')\n        if new_pos > self.size:\n            raise ValueError('seek position beyond end of file')\n        self.pos = new_pos\n        # print('ZipSubFile: seek to {}'.format(self.pos))\n        return self.pos\n", "idx": 1802}
{"namespace": "oletools.ooxml.XmlParser.iter_xml", "completion": "        if self.is_single_xml():\n            if subfiles:\n                raise BadOOXML(self.filename, 'xml has no subfiles')\n            # do not use uopen, xml parser determines encoding on its own\n            with open(self.filename, 'rb') as handle:\n                yield None, ET.iterparse(handle), 0\n            self.did_iter_all = True\n        else:\n            zipper = None\n            subfiles = None\n            try:\n                zipper = ZipFile(self.filename)\n                if not subfiles:\n                    subfiles = zipper.namelist()\n                elif isstr(subfiles):\n                    subfiles = [subfiles, ]\n                else:\n                    # make a copy in case original args are modified\n                    # Not sure whether this really is needed...\n                    subfiles = tuple(arg for arg in subfiles)\n\n                for subfile in subfiles:\n                    with zipper.open(subfile, 'r') as handle:\n                        yield subfile, ET.iterparse(handle), 0\n                if not subfiles:\n                    self.did_iter_all = True\n            except KeyError as orig_err:\n                # Note: do not change text of this message without adjusting\n                #       conditions in except handlers\n                raise BadOOXML(self.filename,\n                               'invalid subfile: ' + str(orig_err))\n            except BadZipfile:\n                raise BadOOXML(self.filename, 'not in zip format')\n            finally:\n                if zipper:\n                    zipper.close()\n", "idx": 1803}
{"namespace": "oletools.oleid.OleID.check", "completion": "        if self.ole is None:\n            self.ole = olefile.OleFileIO(self.filename)\n        self.indicators = []\n        self.indicators.append(Indicator('oleid_version', __version__))\n        self.indicators.append(Indicator('oleid_filename', self.filename))\n        self.indicators.append(Indicator('oleid_filesize', len(self.data)))\n        self.indicators.append(Indicator('oleid_isole', self.ole.isOleFile()))\n        self.indicators.append(Indicator('oleid_isole2', self.ole.isOle2File()))\n        self.indicators.append(Indicator('oleid_isole3', self.ole.isOle3File()))\n        self.indicators.append(Indicator('oleid_isole4', self.ole.isOle4File()))\n        self.indicators.append(Indicator('oleid_isole5', self.ole.isOle5File()))\n        self.indicators.append(Indicator('oleid_isole6', self.ole.isOle6File()))\n        self.indicators.append(Indicator('oleid_isole7', self.ole.isOle7File()))\n        self.indicators.append(Indicator('oleid_isole8', self.ole.isOle8File()))\n        self.indicators.append(Indicator('oleid_isole9', self.ole.isOle9File()))\n        self.indicators.append(Indicator('oleid_isole10', self.ole.isOle10File()))\n        self.indicators.append(Indicator('oleid_isole11', self.ole.isOle11File()))\n        self.indicators.append(Indicator('oleid_isole12', self.ole.isOle12File()))\n        self.indicators.append(Indicator('oleid_isole13', self.ole.isOle13File", "idx": 1804}
{"namespace": "tools.cgrep.is_valid_ip", "completion": "  try:\n    ip = nacaddr.IP(arg)\n    return ip\n  except nacaddr.AddrError:\n    raise argparse.ArgumentTypeError(\n        f\"{arg} is not a valid IP address\")\n\n", "idx": 1805}
{"namespace": "tools.cgrep.group_diff", "completion": "  # Get the two objects to compare.\n  first_obj, sec_obj = options.cmp\n  # Get the two objects to compare.\n  first_obj, sec_obj = options.cmp\n  # Get the two objects to compare.\n  first_obj, sec_obj = options.cmp\n  # Get the two objects to compare.\n  first_obj, sec_obj = options.cmp\n  # Get the two objects to compare.\n  first_obj, sec_obj = options.cmp\n  # Get the two objects to compare.\n  first_obj, sec_obj = options.cmp\n  # Get the two objects to compare.\n  first_obj, sec_obj = options.cmp\n  # Get the two objects to compare.\n  first_obj, sec_obj = options.cmp\n  # Get the two objects to compare.\n  first_obj, sec_obj = options.cmp\n  # Get the two objects to compare.\n  first_obj, sec_obj = options.cmp\n  # Get the two objects to compare.\n  first_obj, sec_obj = options.cmp\n  # Get the two objects to compare.\n  first_obj, sec_obj = options.cmp\n  # Get the two objects to compare.\n  first_obj, sec_obj = options.cmp\n  # Get the two objects to compare.\n  first_obj, sec_obj = options.cmp\n  # Get the two objects to compare.\n  first_obj, sec_obj = options.cmp\n  # Get the two objects to compare.\n  first_obj, sec_obj = options.cmp\n  # Get the two objects to compare.\n  first_obj, sec_obj = options.cmp\n  # Get the two objects to compare.\n  first_obj, sec_obj = options.cmp\n  # Get the two objects to compare.\n  first_obj, sec_obj = options.cmp\n  # Get the two objects to compare.\n  first_obj, sec_obj = options.cmp\n  # Get the two objects to compare.\n  first_obj, sec_obj =", "idx": 1806}
{"namespace": "tools.cgrep.compare_tokens", "completion": "  first_obj, sec_obj = options.cmp\n  first_net = db.GetNet(first_obj)\n  sec_net = db.GetNet(sec_obj)\n  first_svc = db.GetSvc(first_obj)\n  sec_svc = db.GetSvc(sec_obj)\n  first_union = first_net + first_svc\n  sec_union = sec_net + sec_svc\n  first_union.sort(key=lambda x: int(x.ip))\n  sec_union.sort(key=lambda x: int(x.ip))\n  meta = (first_obj, sec_obj, first_union)\n  results = []\n  for i in first_union:\n    for j in sec_union:\n      if i.version == j.version:\n        if i.subnet_of(j):\n          results.append(str(i))\n          break\n  return meta, results", "idx": 1807}
{"namespace": "capirca.aclgen.EntryPoint", "completion": "  SetupFlags()\n  app.run(main)", "idx": 1808}
{"namespace": "capirca.lib.nacaddr.IP", "completion": "  if isinstance(ip, ipaddress._BaseNetwork):\n    if ip.version == 4:\n      return IPv4(ip, comment=comment, token=token)\n    elif ip.version == 6:\n      return IPv6(ip, comment=comment, token=token)\n    else:\n      raise ValueError('Invalid IP version: %s' % ip.version)\n  else:\n    ip_obj = ipaddress.ip_network(ip, strict=strict)\n    if ip_obj.version == 4:\n      return IPv4(ip_obj, comment=comment, token=token)\n    elif ip_obj.version == 6:\n      return IPv6(ip_obj, comment=comment, token=token)\n    else:\n      raise ValueError('Invalid IP version: %s' % ip_obj.version)\n\n", "idx": 1809}
{"namespace": "csvkit.cli.CSVKitUtility.run", "completion": "        if 'f' not in self.override_flags:\n            self.input_file = self._open_input_file()\n        try:\n            if 'no_header_row' not in self.override_flags:\n                with warnings.catch_warns():\n                    warnings.simplefilter('ignore', RequiredHeaderError)\n                    self.reader = agate.csv.DictReader(self.input_file, **self.reader_kwargs)\n            else:\n                self.reader = agate.csv.DictReader(self.input_file, **self.reader_kwargs)\n            self.main()\n        finally:\n            if 'f' not in self.override_flags:\n                self.input_file.close()\n", "idx": 1810}
{"namespace": "csvkit.convert.fixed.fixed2csv", "completion": "    # Read the schema file\n    with open(schema, 'r') as f:\n        schema_reader = agate.csv.reader(f)\n        schema_data = list(schema_reader)\n\n    # Create a namedtuple to store the schema information\n    Schema = namedtuple('Schema', ['name', 'start', 'length'])\n\n    # Parse the schema data and create a list of Schema objects\n    schema_list = [Schema(name=row[0], start=int(row[1]), length=int(row[2])) for row in schema_data]\n\n    # Initialize an empty list to store the parsed data\n    parsed_data = []\n\n    # Iterate over the lines in the fixed-width file\n    for line in iterdecode(f, 'utf-8'):\n        # Skip the specified number of lines\n        if skip_lines > 0:\n            skip_lines -= 1\n            continue\n\n        # Parse the line based on the schema\n        parsed_line = [line[schema.start:schema.start + schema.length].strip() for schema in schema_list]\n\n        # Add the parsed line to the list of parsed data\n        parsed_data.append(parsed_line)\n\n    # Create a StringIO object to store the parsed data\n    output_file = StringIO()\n\n    # Write the parsed data to the output file\n    writer = agate.csv.writer(output_file)\n    writer.writerows(parsed_data)\n\n    # If an output file is specified, write the parsed data to the file\n    if output:\n        with open(output, 'w') as f:\n            f.write(output_file.getvalue())\n    else:\n        # Return the parsed data as a string\n        return output_file.getvalue()", "idx": 1811}
{"namespace": "check_dummies.find_backend", "completion": "    match = _re_backend.search(line)\n    if match:\n        return match.group(1)\n    return None", "idx": 1812}
{"namespace": "check_dummies.create_dummy_object", "completion": "    if name.startswith(\"is_\"):\n        return DUMMY_CONSTANT.format(name)\n    elif name.startswith(\"from_\"):\n        return DUMMY_CLASS.format(name, backend_name)\n    else:\n        return DUMMY_FUNCTION.format(name, backend_name)", "idx": 1813}
{"namespace": "pycorrector.en_spell.EnSpell.check_init", "completion": "        if not self.word_freq_dict:\n            self._init()\n", "idx": 1814}
{"namespace": "pycorrector.en_spell.EnSpell.candidates", "completion": "        self.check_init()\n        if word in self.custom_confusion:\n            return self.custom_confusion[word]\n        if len(word) == 0:\n            return set()\n        if word in self.word_freq_dict:\n            return {word}\n        if len(word) <= 2:\n            return self.known(self.edits1(word))\n        return self.known(self.edits2(word)) or {word}\n", "idx": 1815}
{"namespace": "pycorrector.en_spell.EnSpell.correct_word", "completion": "        self.check_init()\n        return max(self.candidates(word), key=self.probability)\n", "idx": 1816}
{"namespace": "pycorrector.en_spell.EnSpell.correct", "completion": "        self.check_init()\n        if not text:\n            return text, []\n        if include_symbol:\n            from pycorrector.utils.tokenizer import whitespace_tokenize\n            words = whitespace_tokenize(text)\n        else:\n            from pycorrector.utils.tokenizer import split_text_to_word\n            words = split_text_to_word(text)\n        details = []\n        for i, word in enumerate(words):\n            if len(word) > 1 and word.isalpha():\n                if word in self.custom_confusion:\n                    correct_word = self.custom_confusion[word]\n                else:\n                    correct_word = self.correct_word(word)\n                if correct_word != word:\n                    begin_index = text.find(word)\n                    end_index = begin_index + len(word)\n                    details.append([word, correct_word, begin_index, end_index])\n                    text = text[:begin_index] + correct_word + text[end_index:]\n        details.sort(key=lambda x: x[2])\n        return text, details\n\n", "idx": 1817}
{"namespace": "whereami.predict.crossval", "completion": "    if X is None or y is None:\n        X, y = get_train_data(path)\n\n    if len(X) < folds:\n        raise ValueError(f\"There are not enough samples ({len(X)}). Need at least {folds}.\")\n\n    if clf is None:\n        clf = get_model(path)\n\n    print(f\"KFold folds={folds}, running {n} times\")\n    for i in range(1, n + 1):\n        scores = cross_val_score(clf, X, y, cv=folds)\n        print(f\"{i}/{n}: {scores.mean()}\")\n    print(\"-------- total --------\")\n    return scores.mean()", "idx": 1818}
{"namespace": "stellar.models.Table.get_table_name", "completion": "        if not self.snapshot:\n            raise Exception('Table name requires snapshot')\n        if not self.snapshot.hash:\n            raise Exception('Snapshot hash is empty.')\n        if old:\n            return 'stellar_{}_{}_{}'.format(\n                self.table_name, self.snapshot.hash, postfix\n            )\n        else:\n            return 'stellar_{}'.format(\n                hashlib.md5(\n                    '{}{}{}'.format(\n                        self.table_name, self.snapshot.hash, postfix\n                    ).encode('utf-8')\n                ).hexdigest()[:16]\n            )", "idx": 1819}
{"namespace": "chatette.utils.Singleton.reset_instance", "completion": "        cls._instance = None\n        return cls.get_or_create(*args, **kwargs)\n\n", "idx": 1820}
{"namespace": "chatette.utils.cast_to_unicode", "completion": "    if sys.version_info[0] == 2:\n        if isinstance(anything, dict):\n            return {cast_to_unicode(key): cast_to_unicode(value) for key, value in anything.iteritems()}\n        elif isinstance(anything, list):\n            return [cast_to_unicode(element) for element in anything]\n        elif isinstance(anything, unicode):\n            return anything\n        else:\n            return unicode(anything, 'utf-8')\n    else:\n        return anything", "idx": 1821}
{"namespace": "chatette.cli.terminal_writer.TerminalWriter.write", "completion": "        if self._file_mode == 'quiet':\n            return\n        if self.redirection_file_path is None:\n            print(text)\n        else:\n            if self.buffered_text is None:\n                self.buffered_text = text\n            else:\n                self.buffered_text += text\n", "idx": 1822}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.find_redirection_file_path", "completion": "        if len(tokens) == 0:\n            return None\n        if len(tokens) == 1:\n            if tokens[0] == REDIRECTION_SYM:\n                return (RedirectionType.overwrite, None)\n            elif tokens[0] == REDIRECTION_APPEND_SYM:\n                return (RedirectionType.append, None)\n            else:\n                return None\n        if tokens[0] == REDIRECTION_SYM:\n            if tokens[1].startswith(\"/\"):\n                return (RedirectionType.overwrite, tokens[1])\n            else:\n                return (RedirectionType.overwrite, None)\n        elif tokens[0] == REDIRECTION_APPEND_SYM:\n            if tokens[1].startswith(\"/\"):\n                return (RedirectionType.append, tokens[1])\n            else:\n                return (RedirectionType.append, None)\n        else:\n            return None\n", "idx": 1823}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.get_unit_type_from_str", "completion": "        if unit_type_str == \"alias\":\n            return AST.UnitType.alias\n        if unit_type_str == \"class\":\n            return AST.UnitType.class_\n        if unit_type_str == \"include\":\n            return AST.UnitType.include\n        if unit_type_str == \"import\":\n            return AST.UnitType.import_\n        if unit_type_str == \"meta\":\n            return AST.UnitType.meta\n        if unit_type_str == \"module\":\n            return AST.UnitType.module\n        if unit_type_str == \"rule\":\n            return AST.UnitType.rule\n        if unit_type_str == \"tag\":\n            return AST.UnitType.tag\n        if unit_type_str == \"variable\":\n            return AST.UnitType.variable\n        if unit_type_str == \"variation\":\n            return AST.UnitType.variation\n        return None\n", "idx": 1824}
{"namespace": "chatette.cli.interactive_commands.unhide_command.UnhideCommand.execute", "completion": "        if len(self.command_tokens) < 3:\n            print(\"Please provide a unit type and a unit name.\")\n            return\n\n        unit_type = self.command_tokens[1]\n        unit_name = self.command_tokens[2]\n\n        if unit_type not in AST.UNIT_TYPES:\n            print(f\"{unit_type} is not a valid unit type.\")\n            return\n\n        if unit_type == \"meta\":\n            print(\"Meta units cannot be hidden.\")\n            return\n\n        if unit_type == \"include\":\n            print(\"Include units cannot be hidden.\")\n            return\n\n        if unit_type == \"comment\":\n            print(\"Comment units cannot be hidden.\")\n            return\n\n        if unit_type == \"tag\":\n            print(\"Tag units cannot be hidden.\")\n            return\n\n        if unit_type == \"variable\":\n            print(\"Variable units cannot be hidden.\")\n            return\n\n        if unit_type == \"macro\":\n            print(\"Macro units cannot be hidden.\")\n            return\n\n        if unit_type == \"rule\":\n            print(\"Rule units cannot be hidden.\")\n            return\n\n        if unit_type == \"pattern\":\n            print(\"Pattern units cannot be hidden.\")\n            return\n\n        if unit_type == \"wildcard\":\n            print(\"Wildcard units cannot be hidden.\")\n            return\n\n        if unit_type == \"wildcard_group\":\n            print(\"Wildcard group units cannot be hidden.\")\n            return\n\n        if unit_type == \"wildcard_group_ref\":\n            print(\"Wildcard group ref units cannot be hidden.\")\n            return\n\n        if unit_type == \"wildcard_group_ref_list\":\n            print(\"Wildcard group ref list units cannot be hidden.\")\n            return\n\n        if unit_type == \"wildcard_group_ref_list_item\":\n            print(\"Wildcard group ref list item units cannot be hidden.\")\n            return\n\n        if unit_type == \"wildcard_group_ref_list_item_list\":\n            print(\"Wildcard group ref list item list units cannot", "idx": 1825}
{"namespace": "chatette.adapters.factory.create_adapter", "completion": "    if adapter_name == 'rasa':\n        from chatette.adapters.rasa_adapter import RasaAdapter\n        return RasaAdapter(base_filepath)\n    elif adapter_name == 'rasa-md':\n        from chatette.adapters.rasa_md_adapter import RasaMdAdapter\n        return RasaMdAdapter(base_filepath)\n    elif adapter_name == 'rasamd':\n        from chatette.adapters.rasa_md_adapter import RasaMdAdapter\n        return RasaMdAdapter(base_filepath, True)\n    elif adapter_name == 'jsonl':\n        from chatette.adapters.jsonl_adapter import JsonlAdapter\n        return JsonlAdapter(base_filepath)\n    else:\n        raise ValueError(f\"Unknown adapter name: {adapter_name}\")", "idx": 1826}
{"namespace": "chatette.parsing.ChoiceBuilder.create_concrete", "completion": "        self._check_information()\n        return Choice(\n            self.leading_space,\n            self._build_modifiers_repr(),\n            self.rules\n        )\n", "idx": 1827}
{"namespace": "chatette.parsing.UnitRefBuilder._build_modifiers_repr", "completion": "        modifiers = super(UnitRefBuilder, self)._build_modifiers_repr()\n        modifiers.arg_value = self.arg_value\n        modifiers.variation = self.variation\n        return modifiers\n", "idx": 1828}
{"namespace": "chatette.parsing.UnitRefBuilder.create_concrete", "completion": "        self._check_information()\n        if self.type == UnitType.UNIT:\n            from chatette.units.modifiable.unit_reference import UnitReference\n            return UnitReference(\n                self.leading_space, self._build_modifiers_repr(),\n                self.identifier, self.variation\n            )\n        elif self.type == UnitType.MACRO:\n            from chatette.units.modifiable.macro_reference import MacroReference\n            return MacroReference(\n                self.leading_space, self._build_modifiers_repr(),\n                self.identifier, self.variation\n            )\n        elif self.type == UnitType.INCLUSION:\n            from chatette.units.modifiable.inclusion_reference import InclusionReference\n            return InclusionReference(\n                self.leading_space, self._build_modifiers_repr(),\n                self.identifier, self.variation\n            )\n        else:\n            raise ValueError(\n                \"Tried to create a concrete unit reference with an unknown type.\"\n            )", "idx": 1829}
{"namespace": "chatette.parsing.UnitDefBuilder._build_modifiers_repr", "completion": "        modifiers = super(UnitDefBuilder, self)._build_modifiers_repr()\n        modifiers.argument_name = self.arg_name\n        return modifiers", "idx": 1830}
{"namespace": "chatette.parsing.AliasDefBuilder.create_concrete", "completion": "        from chatette.units.modifiable.alias_definition import AliasDefinition\n        self._check_information()\n        return AliasDefinition(\n            self.identifier, self.leading_space, self._build_modifiers_repr()\n        )\n", "idx": 1831}
{"namespace": "chatette.parsing.SlotDefBuilder.create_concrete", "completion": "        self._check_information()\n        if self.variation is not None:\n            definitions = AST.get_or_create()[UnitType.slot]\n            if self.identifier in definitions:\n                return definitions[self.identifier]\n        return SlotDefinition(self.identifier, self._build_modifiers_repr())\n", "idx": 1832}
{"namespace": "chatette.parsing.IntentDefBuilder.create_concrete", "completion": "        from chatette.units.modifiable.definitions.intent import IntentDefinition\n        self._check_information()\n        if self.variation is not None:\n            definitions = AST.get_or_create()[UnitType.intent]\n            if self.identifier in definitions:\n                return definitions[self.identifier]\n        return IntentDefinition(\n            self.identifier, self._build_modifiers_repr(),\n            self.nb_training_ex, self.nb_testing_ex\n        )\n", "idx": 1833}
{"namespace": "bentoml._internal.resource.get_resource", "completion": "    if resource_kind in _RESOURCE_REGISTRY:\n        resource_class = _RESOURCE_REGISTRY[resource_kind]\n        if resource_kind in resources:\n            resource_spec = resources[resource_kind]\n            if resource_spec == \"system\":\n                return resource_class.from_system()\n            else:\n                resource = resource_class.from_spec(resource_spec)\n                if validate:\n                    resource.validate()\n                return resource\n        else:\n            return None\n    else:\n        return None\n\n", "idx": 1834}
{"namespace": "bentoml._internal.resource.system_resources", "completion": "    result: dict[str, t.Any] = {}\n    for resource_kind, resource in _RESOURCE_REGISTRY.items():\n        result[resource_kind] = resource.from_system()\n    return result", "idx": 1835}
{"namespace": "bentoml._internal.resource.CpuResource.from_spec", "completion": "        if isinstance(spec, (int, float)):\n            return float(spec)\n        elif isinstance(spec, str):\n            if spec.endswith(\"m\"):\n                return float(spec[:-1]) / 1000\n            elif spec.endswith(\"u\"):\n                return float(spec[:-1]) / 1000000\n            else:\n                return float(spec)\n        else:\n            raise ValueError(f\"Invalid CPU specification: {spec}\")\n", "idx": 1836}
{"namespace": "bentoml._internal.resource.CpuResource.from_system", "completion": "        if os.name == \"nt\":\n            return psutil.cpu_count(logical=False)\n        else:\n            return psutil.cpu_count()\n", "idx": 1837}
{"namespace": "bentoml._internal.resource.CpuResource.validate", "completion": "        if val < 0:\n            raise BentoMLConfigException(\n                f\"CPU resource limit '{val}' is negative. \"\n            )\n\n        if psutil.POSIX:\n            cpu_count = query_cgroup_cpu_count()\n        else:\n            cpu_count = query_os_cpu_count()\n\n        if val > cpu_count:\n            raise BentoMLConfigException(\n                f\"CPU resource limit '{val}' is greater than the system's available CPU resources '{cpu_count}'. \"\n            )\n\n", "idx": 1838}
{"namespace": "bentoml._internal.types.LazyType.get_class", "completion": "        if self._runtime_class is not None:\n            return self._runtime_class\n\n        if import_module:\n            module = importlib.import_module(self.module)\n            return getattr(module, self.qualname)\n\n        raise ImportError(f\"Module {self.module} is not imported\")", "idx": 1839}
{"namespace": "bentoml._internal.models.model.Model.create", "completion": "        if isinstance(name, str):\n            name = Tag(name)\n\n        if labels is not None:\n            normalize_labels_value(labels)\n\n        if metadata is not None:\n            metadata_validator.validate(metadata)\n\n        if options is None:\n            options = ModelOptions()\n\n        if not isinstance(options, ModelOptions):\n            raise BentoMLException(\n                f\"Invalid options type {type(options)} for model {name}. \"\n                \"Model options must be a subclass of ModelOptions.\"\n            )\n\n        if not isinstance(signatures, ModelSignatures):\n            raise BentoMLException(\n                f\"Invalid signatures type {type(signatures)} for model {name}. \"\n                \"Model signatures must be a subclass of ModelSignatures.\"\n            )\n\n        if not isinstance(context, ModelContext):\n            raise BentoMLException(\n                f\"Invalid context type {type(context)} for model {name}. \"\n                \"Model context must be a subclass of ModelContext.\"\n            )\n\n        if not isinstance(name, Tag):\n            raise BentoMLException(\n                f\"Invalid name type {type(name)} for model {name}. \"\n                \"Model name must be a subclass of Tag.\"\n            )\n\n        if not isinstance(module, str):\n            raise BentoMLException(\n                f\"Invalid module type {type(module)} for model {name}. \"\n                \"Model module must be a string.\"\n            )\n\n        if not isinstance(api_version, str):\n            raise BentoMLException(\n                f\"Invalid api_version type {type(api_version)} for model {name}. \"\n                \"Model api_version must be a string.\"\n            )\n\n        if not isinstance(options, ModelOptions):\n            raise BentoMLException(\n                f\"Invalid options type {type(options)} for model {name}. \"\n                \"Model options must be a subclass of ModelOptions.\"\n            )\n\n        if not isinstance(custom_objects, dict) and custom_objects is not None:\n            raise Bent", "idx": 1840}
{"namespace": "bentoml._internal.models.model.Model.from_fs", "completion": "        if not item_fs.isfile(MODEL_YAML_FILENAME):\n            raise NotFound(\n                f\"Model yaml file not found in {item_fs.geturl()}\",\n                MODEL_YAML_FILENAME,\n            )\n\n        with item_fs.open(MODEL_YAML_FILENAME, \"r\") as f:\n            info = bentoml_cattr.load(yaml.safe_load(f), ModelInfo)\n\n        return cls(\n            info.tag,\n            item_fs,\n            info,\n            _internal=True,\n        )\n", "idx": 1841}
{"namespace": "bentoml._internal.utils.metrics.linear_buckets", "completion": "    assert start > 0.0\n    assert start < end\n    assert step > 0.0\n\n    bound = start\n    buckets: list[float] = []\n    while bound < end:\n        buckets.append(bound)\n        bound += step\n\n    if len(buckets) > MAX_BUCKET_COUNT:\n        buckets = buckets[:MAX_BUCKET_COUNT]\n\n    return tuple(buckets) + (end, INF)", "idx": 1842}
{"namespace": "bentoml._internal.utils.validate_metadata", "completion": "    if not isinstance(metadata, dict):\n        raise ValueError(\"metadata must be a dict\")\n\n    for key, val in metadata.items():\n        if not isinstance(key, str):\n            raise ValueError(\"metadata keys must be strings\")\n\n        if not isinstance(val, MetadataType):\n            raise ValueError(\n                f\"metadata values must be one of the following types: \"\n                f\"str, int, float, bool, datetime, date, time, timedelta, list, dict\"\n            )\n\n        if isinstance(val, list):\n            if not all(isinstance(item, MetadataType) for item in val):\n                raise ValueError(\n                    f\"metadata values must be one of the following types: \"\n                    f\"str, int, float, bool, datetime, date, time, timedelta, list, dict\"\n                )\n\n        if isinstance(val, dict):\n            if not all(isinstance(item, MetadataType) for item in val.values()):\n                raise ValueError(\n                    f\"metadata values must be one of the following types: \"\n                    f\"str, int, float, bool, datetime, date, time, timedelta, list, dict\"\n                )\n\n", "idx": 1843}
{"namespace": "bentoml._internal.utils.analytics.usage_stats.get_serve_info", "completion": "    serve_id = secrets.token_urlsafe(16)\n    serve_started_timestamp = datetime.now(timezone.utc)\n    return ServeInfo(serve_id, serve_started_timestamp)\n\n", "idx": 1844}
{"namespace": "bentoml._internal.utils.analytics.usage_stats._track_serve_init", "completion": "    event = ServeInitEvent(\n        serve_id=serve_info.serve_id,\n        serve_started_timestamp=serve_info.serve_started_timestamp,\n        service_name=svc.name,\n        service_version=svc.version,\n        service_module=svc.module,\n        service_path=svc.path,\n        service_bento_path=svc.bento_path,\n        service_bento_name=svc.bento_name,\n        service_bento_version=svc.bento_version,\n        service_bento_metadata=svc.bento_metadata,\n        service_bento_metadata_version=svc.bento_metadata_version,\n        service_bento_metadata_path=svc.bento_metadata_path,\n        service_bento_metadata_name=svc.bento_metadata_name,\n        service_bento_metadata_version=svc.bento_metadata_version,\n        service_bento_metadata_path=svc.bento_metadata_path,\n        service_bento_metadata_name=svc.bento_metadata_name,\n        service_bento_metadata_version=svc.bento_metadata_version,\n        service_bento_metadata_path=svc.bento_metadata_path,\n        service_bento_metadata_name=svc.bento_metadata_name,\n        service_bento_metadata_version=svc.bento_metadata_version,\n        service_bento_metadata_path=svc.bento_metadata_path,\n        service_bento_metadata_name=svc.bento_metadata_name,\n        service_bento_metadata_version=svc.bento_metadata_version,\n        service_bento_metadata_path=svc.bento_metadata_path,\n        service_bento_metadata_name=svc.", "idx": 1845}
{"namespace": "bentoml._internal.service.service.get_valid_service_name", "completion": "    svc_name = user_provided_svc_name.lower()\n    if svc_name != user_provided_svc_name:\n        logger.warning(\n            f\"Service name '{user_provided_svc_name}' is not lowercase, \"\n            f\"converting to '{svc_name}'\"\n        )\n    tag = Tag(svc_name)\n    tag.validate()\n    return svc_name", "idx": 1846}
{"namespace": "bentoml._internal.configuration.helpers.flatten_dict", "completion": "    for k, v in d.items():\n        if any(c in k for c in punctuation):\n            k = f'\"{k}\"'\n        if isinstance(v, dict):\n            yield from flatten_dict(v, parent=f\"{parent}{k}{sep}\", sep=sep)\n        else:\n            yield f\"{parent}{k}\", v", "idx": 1847}
{"namespace": "bentoml._internal.configuration.helpers.load_config_file", "completion": "    if not os.path.exists(path):\n        raise BentoMLConfigException(\n            f\"Configuration file {path} does not exist. Please create it first.\"\n        )\n\n    with open(path, \"r\") as f:\n        config = yaml.safe_load(f)\n\n    return config", "idx": 1848}
{"namespace": "bentoml._internal.configuration.helpers.expand_env_var_in_values", "completion": "    for k, v in d.items():\n        if isinstance(v, t.MutableMapping):\n            expand_env_var_in_values(v)\n        elif isinstance(v, str):\n            d[k] = expand_env_var(v)\n        elif isinstance(v, t.Sequence):\n            d[k] = [expand_env_var(i) for i in v]", "idx": 1849}
{"namespace": "bentoml._internal.runner.strategy.DefaultStrategy.get_worker_count", "completion": "        if resource_request is None:\n            resource_request = {}\n\n        if \"nvidia.com/gpu\" in resource_request:\n            if runnable_class.is_gpu_enabled():\n                gpu_count = get_resource(\"nvidia.com/gpu\", resource_request)\n                return math.ceil(gpu_count * workers_per_resource)\n\n        if \"cpu\" in resource_request:\n            if runnable_class.is_cpu_enabled():\n                cpu_count = get_resource(\"cpu\", resource_request)\n                return math.ceil(cpu_count * workers_per_resource)\n\n        raise ValueError(\n            f\"No known supported resources available for {runnable_class.__name__}\"\n        )\n", "idx": 1850}
{"namespace": "bentoml._internal.runner.strategy.DefaultStrategy.get_worker_env", "completion": "        if resource_request is None:\n            resource_request = system_resources()\n\n        # use nvidia gpu\n        nvidia_gpus = get_resource(resource_request, \"nvidia.com/gpu\")\n        if (\n            nvidia_gpus is not None\n            and len(nvidia_gpus) > 0\n            and \"nvidia.com/gpu\" in runnable_class.SUPPORTED_RESOURCES\n        ):\n            return {\n                \"CUDA_VISIBLE_DEVICES\": nvidia_gpus[worker_index % len(nvidia_gpus)],\n            }\n\n        # use CPU\n        cpus = get_resource(resource_request, \"cpu\")\n        if cpus is not None and cpus > 0:\n            if \"cpu\" not in runnable_class.SUPPORTED_RESOURCES:\n                logger.warning(\n                    \"No known supported resource available for %s, falling back to using CPU.\",\n                    runnable_class,\n                )\n\n            if runnable_class.SUPPORTS_CPU_MULTI_THREADING:\n                if isinstance(workers_per_resource, float):\n                    raise ValueError(\n                        \"Fractional CPU multi threading support is not yet supported.\"\n                    )\n                return {\n                    \"BENTOML_NUM_THREAD\": workers_per_resource,\n                }\n\n            return {\n                \"OMP_NUM_THREADS\": workers_per_resource,\n                \"OPENBLAS_NUM_THREADS\": workers_per_resource,\n                \"MKL_NUM_THREADS\": workers_per_resource,\n                \"VECLIB_MAXIMUM_THREADS\": workers_per_resource,\n                \"NUMEXPR_NUM_THREADS\": workers_per_resource,\n                \"RAYON_RS_NUM_CPUS\": workers_per_resource,\n                \"TF_NUM_INTEROP_THREADS\": workers_per_resource,\n                \"TF_NUM_INTRAOP_THREADS\": workers_per_resource,\n            }\n\n        #", "idx": 1851}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.batches_to_batch", "completion": "        batch = np.concatenate(batches, axis=batch_dim)\n        indices = list(itertools.accumulate(map(len, batches)))\n        return batch, indices\n", "idx": 1852}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.to_payload", "completion": "        if batch.ndim == 0:\n            pickle_bytes_str = pickle.dumps(batch)\n        else:\n            if not batch.flags[\"C_CONTIGUOUS\"]:\n                batch = np.ascontiguousarray(batch)\n            pickle_bytes_str = pep574_dumps(batch)\n        return cls.create_payload(pickle_bytes_str, batch_dim)\n", "idx": 1853}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.from_payload", "completion": "        if payload.meta[\"format\"] == \"pickle5\":\n            bs = base64.b64decode(payload.meta[\"pickle_bytes_str\"])\n            bs = pep574_loads(bs, payload.meta[\"indices\"])\n            return fixed_torch_loads(bs)\n        else:\n            return pickle.loads(payload.data)", "idx": 1854}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.batch_to_payloads", "completion": "        batches = cls.batch_to_batches(batch, indices, batch_dim)\n        return [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n", "idx": 1855}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.from_batch_payloads", "completion": "        batches = [cls.from_payload(payload) for payload in payloads]\n        batch, indices = cls.batches_to_batch(batches, batch_dim)\n        return batch, indices", "idx": 1856}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.to_payload", "completion": "        if batch_dim != 0:\n            raise ValueError(\n                \"PandasDataFrameContainer only supports batch_dim of 0, but got {}\".format(\n                    batch_dim\n                )\n            )\n\n        if isinstance(batch, ext.PdSeries):\n            batch = batch.to_frame()\n\n        meta = {\"format\": \"pickle5\"}\n        bs: bytes\n        concat_buffer_bs: bytes\n        indices: list[int]\n        bs, concat_buffer_bs, indices = pep574_dumps(batch)\n        if indices:\n            meta[\"with_buffer\"] = True\n            meta[\"concat_buffer_bs\"] = base64.b64encode(concat_buffer_bs).decode(\"ascii\")\n            meta[\"indices\"] = indices\n        else:\n            meta[\"with_buffer\"] = False\n            bs = bs\n\n        return cls.create_payload(bs, batch.shape[batch_dim], meta)\n", "idx": 1857}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.from_payload", "completion": "        import pandas as pd\n\n        format = payload.meta.get(\"format\", \"default\")\n        if format == \"pickle5\":\n            bs_str = t.cast(str, payload.meta[\"pickle_bytes_str\"])\n            bs = base64.b64decode(bs_str)\n            indices = t.cast(t.List[int], payload.meta[\"indices\"])\n            if payload.meta[\"with_buffer\"]:\n                return t.cast(\"ext.PdDataFrame\", pep574_loads(bs, payload.data, indices))\n            else:\n                return t.cast(\"ext.PdDataFrame\", fixed_torch_loads(payload.data))\n\n        return pickle.loads(payload.data)\n", "idx": 1858}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batch_to_payloads", "completion": "        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n\n        batches = cls.batch_to_batches(batch, indices, batch_dim)\n\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads\n", "idx": 1859}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.from_batch_payloads", "completion": "        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)\n\n", "idx": 1860}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.to_payload", "completion": "        if isinstance(batch, t.Generator):\n            batch = list(batch)\n        bs: bytes\n        concat_buffer_bs: bytes\n        indices: list[int]\n        bs, concat_buffer_bs, indices = pep574_dumps(batch)\n        bs_str = base64.b64encode(bs).decode(\"ascii\")\n\n        return cls.create_payload(\n            concat_buffer_bs,\n            len(batch),\n            {\n                \"format\": \"pickle5\",\n                \"pickle_bytes_str\": bs_str,\n                \"indices\": indices,\n            },\n        )\n", "idx": 1861}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batch_to_payloads", "completion": "        batches = cls.batch_to_batches(batch, indices, batch_dim)\n\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads\n", "idx": 1862}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.from_batch_payloads", "completion": "        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)\n\n", "idx": 1863}
{"namespace": "sslyze.cli.server_string_parser.CommandLineServerStringParser.parse_server_string", "completion": "        # Extract ip from target\n        if \"{\" in server_str:\n            ip_start = server_str.find(\"{\") + 1\n            ip_end = server_str.find(\"}\")\n            ip = server_str[ip_start:ip_end]\n            server_str = server_str[:ip_start - 1] + server_str[ip_end + 1:]\n        else:\n            ip = None\n\n        # Extract ipv6 from target\n        if \"[\" in server_str:\n            ipv6_start = server_str.find(\"[\") + 1\n            ipv6_end = server_str.find(\"]\")\n            ipv6 = server_str[ipv6_start:ipv6_end]\n            server_str = server_str[:ipv6_start - 1] + server_str[ipv6_end + 1:]\n            ip = ipv6\n\n        # Extract port from target\n        if \":\" in server_str:\n            port_start = server_str.find(\":\") + 1\n            port = int(server_str[port_start:])\n            server_str = server_str[:port_start - 1]\n        else:\n            port = None\n\n        # Extract host from target\n        host = server_str\n\n        return host, ip, port\n", "idx": 1864}
{"namespace": "sslyze.plugins.heartbleed_plugin._HeartbleedCliConnector.result_to_console_output", "completion": "        return [\n            f\"Heartbleed vulnerability: {result.is_vulnerable_to_heartbleed}\",\n        ]\n\n", "idx": 1865}
{"namespace": "sslyze.plugins.http_headers_plugin._HttpHeadersCliConnector.result_to_console_output", "completion": "        output = []\n\n        if result.http_error_trace:\n            output.append(\"SSLyze was unable to connect to the server.\")\n            output.append(\"The server returned the following error:\")\n            output.append(result.http_error_trace.format(chain=False))\n            return output\n\n        output.append(\"SSLyze was able to connect to the server.\")\n        output.append(\"The server returned the following HTTP headers:\")\n\n        if result.strict_transport_security_header:\n            output.append(\"Strict-Transport-Security: max-age={}, preload, includeSubDomains\".format(\n                result.strict_transport_security_header.max_age))\n\n        if result.expect_ct_header:\n            output.append(\"Expect-CT: max-age={}, enforce, report-uri={}\".format(\n                result.expect_ct_header.max_age, result.expect_ct_header.report_uri))\n\n        if result.http_path_redirected_to:\n            output.append(\"The server redirected SSLyze to the following path: {}\".format(\n                result.http_path_redirected_to))\n\n        return output\n\n", "idx": 1866}
{"namespace": "sslyze.plugins.http_headers_plugin._detect_http_redirection", "completion": "    # Check if the response contains a redirection to the same server\n    if http_response.status in [301, 302, 303, 307, 308]:\n        # Get the location header\n        location_header = http_response.getheader(\"Location\")\n        if location_header:\n            # Parse the location header\n            location_url = urlsplit(location_header)\n            # Check if the location header contains a redirection to the same server\n            if location_url.netloc == server_host_name and location_url.port == server_port:\n                # Return the path to the new location\n                return location_url.path\n\n    # No redirection to the same server was found\n    return None", "idx": 1867}
{"namespace": "sslyze.plugins.session_renegotiation_plugin._SessionRenegotiationCliConnector.result_to_console_output", "completion": "        result_txt = []\n        if result.is_vulnerable_to_client_renegotiation_dos:\n            result_txt.append(\"The server is vulnerable to a client-initiated renegotiation DoS attack.\")\n        if result.supports_secure_renegotiation:\n            result_txt.append(\"The server supports secure renegotiation.\")\n        return result_txt\n\n", "idx": 1868}
{"namespace": "sslyze.plugins.certificate_info._cli_connector._CertificateInfoCliConnector.result_to_console_output", "completion": "        result_lines = []\n\n        # Add the hostname sent for SNI\n        result_lines.append(f\"Hostname sent for SNI: {result.hostname}\")\n\n        # Add the number of certificates detected\n        result_lines.append(f\"Number of certificates detected: {len(result.certificate_deployments)}\")\n\n        # Iterate through each certificate deployment and add the formatted information to the result list\n        for cert_deployment in result.certificate_deployments:\n            result_lines.append(f\"\\nCertificate deployment #{cert_deployment.index + 1}:\")\n            result_lines.append(f\"  Certificate chain:\")\n            for cert in cert_deployment.certificate_chain:\n                result_lines.append(f\"    - {cert.subject}\")\n            result_lines.append(f\"  Certificate deployment analysis:\")\n            result_lines.append(f\"    - {cert_deployment.analysis_result.value}\")\n            result_lines.append(f\"  Certificate deployment trust:\")\n            for trust_info in cert_deployment.trust_info:\n                result_lines.append(f\"    - {cls.TRUST_FORMAT.format(store_name=trust_info.store_name, store_version=trust_info.store_version)}\")\n                result_lines.append(f\"      - {trust_info.trust_result.value}\")\n                if trust_info.trust_result == CertificateDeploymentAnalysisResult.CERT_TRUSTED:\n                    result_lines.append(f\"      - {trust_info.trust_reason.value}\")\n                else:\n                    result_lines.append(f\"      - {trust_info.trust_reason.value} ({trust_info.trust_reason.error_message})\")\n            result_lines.append(f\"  Certificate deployment OCSP:\")\n            for ocsp_info in cert_deployment.ocsp_info:\n                result_lines.append(f\"    - {ocsp_info.ocsp_url}\")\n                result_lines.append(", "idx": 1869}
{"namespace": "sslyze.plugins.certificate_info._cli_connector._get_name_as_short_text", "completion": "    # Check if there is a common name (CN) in the name field\n    for field in name_field:\n        if field.oid == x509.NameOID.COMMON_NAME:\n            return field.value\n\n    # If there is no CN, return the entire name field as a string\n    return str(name_field)", "idx": 1870}
{"namespace": "sslyze.plugins.certificate_info._symantec.SymantecDistructTester.get_distrust_timeline", "completion": "        # Check if the chain contains any blacklisted certificates\n        for cert in verified_certificate_chain:\n            if binascii.hexlify(cert.public_key().public_bytes(serialization.Encoding.DER, serialization.PublicFormat.SubjectPublicKeyInfo)).decode() in cls._CA_KEYS_BLACKLIST:\n                return SymantecDistrustTimelineEnum.MARCH_2018\n\n        # Check if the chain contains any whitelisted certificates\n        for cert in verified_certificate_chain:\n            if binascii.hexlify(cert.public_key().public_bytes(serialization.Encoding.DER, serialization.PublicFormat.SubjectPublicKeyInfo)).decode() in cls._CA_KEYS_WHITELIST:\n                return SymantecDistrustTimelineEnum.SEPTEMBER_2018\n\n        # If no blacklisted or whitelisted certificates are found, return None\n        return None", "idx": 1871}
{"namespace": "sslyze.plugins.certificate_info._certificate_utils.parse_subject_alternative_name_extension", "completion": "    san_extension = certificate.extensions.get_extension_for_oid(ExtensionOID.SUBJECT_ALTERNATIVE_NAME)\n    san_extension_value = san_extension.value\n    san_extension_value_bytes = san_extension_value.value\n    san_extension_value_decoded = san_extension_value_bytes.decode(\"utf-8\")\n    san_extension_value_decoded_split = san_extension_value_decoded.split(\",\")\n    san_extension_value_decoded_split_stripped = [s.strip() for s in san_extension_value_decoded_split]\n    san_extension_value_decoded_split_stripped_filtered = [s for s in san_extension_value_decoded_split_stripped if s]\n    san_extension_value_decoded_split_stripped_filtered_dns_names = [\n        s for s in san_extension_value_decoded_split_stripped_filtered if s.startswith(\"DNS:\")\n    ]\n    san_extension_value_decoded_split_stripped_filtered_ip_addresses = [\n        s for s in san_extension_value_decoded_split_stripped_filtered if s.startswith(\"IP:\")\n    ]\n    san_extension_value_decoded_split_stripped_filtered_dns_names_stripped = [\n        s.replace(\"DNS:\", \"\") for s in san_extension_value_decoded_split_stripped_filtered_dns_names\n    ]\n    san_extension_value_decoded_split_stripped_filtered_ip_addresses_stripped = [\n        s.replace(\"IP:\", \"\") for s in san_extension_value_decoded_split_stripped_filtered_ip_addresses\n    ]\n    return SubjectAlternativeNameExtension(\n        dns_names=san_extension_value_decoded_split_stripped_filtered_dns_names_stripped,\n        ip_addresses=san_extension_value_", "idx": 1872}
{"namespace": "sslyze.plugins.certificate_info._cert_chain_analyzer._certificate_matches_hostname", "completion": "    try:\n        # Extract the names from the certificate\n        names = certificate.subject.get_attributes_for_oid(cryptography.x509.NameOID.COMMON_NAME)\n        names.extend(certificate.extensions.get_extension_for_oid(cryptography.x509.ExtensionOID.SUBJECT_ALTERNATIVE_NAME).value.get_values_for_type(cryptography.x509.DNSName))\n\n        # Create a dictionary with the properly formatted names\n        names_dict = {}\n        for name in names:\n            names_dict[name.value] = name.value\n\n        # Check if the server_hostname matches any of the names in the certificate\n        match_hostname(server_hostname, names_dict)\n        return True\n    except CertificateError:\n        return False", "idx": 1873}
