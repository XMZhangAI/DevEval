{"namespace": "benedict.utils.type_util.is_json_serializable", "completion": "    return isinstance(val, (bool, dict, float, int, list, str, tuple))\n\n", "idx": 0}
{"namespace": "feedparser.urls.convert_to_idn", "completion": "    # If the host is already in IDN notation, return the URL as is.\n    if urllib.parse.urlsplit(url).hostname is not None:\n        if urllib.parse.urlsplit(url).hostname.encode('idna').decode('ascii') == urllib.parse.urlsplit(url).hostname:\n            return url\n\n    # If the host is in punycode notation, convert it to IDN notation.\n    if urllib.parse.urlsplit(url).hostname is not None:\n        if urllib.parse.urlsplit(url).hostname.encode('ascii') == urllib.parse.urlsplit(url).hostname.encode('idna'):\n            host = urllib.parse.urlsplit(url).hostname.encode('idna').decode('ascii')\n            return url.replace(urllib.parse.urlsplit(url).hostname, host)\n\n    # If the host is in unicode notation, convert it to punycode notation.\n    if urllib.parse.urlsplit(url).hostname is not None:\n        if urllib.parse.urlsplit(url).hostname.encode('ascii') != urllib.parse.urlsplit(url).hostname.encode('idna'):\n            host = urllib.parse.urlsplit(url).hostname.encode('idna').decode('ascii')\n            return url.replace(urllib.parse.urlsplit(url).hostname, host)\n\n", "idx": 1}
{"namespace": "mistune.toc.add_toc_hook", "completion": "    def toc_treeprocessor(md, toc_tokens):\n        \"\"\" Save TOC into the state. \"\"\"\n        md.toc_tokens = toc_tokens\n\n    md.toc_tokens = []\n    md.toc_tokens.append({'type': 'toc_open'})\n\n    md.toc_tokens.append({'type': 'toc_item', 'level': min_level, 'text': 'Contents'})\n\n    md.toc_tokens.append({'type': 'toc_close'})\n\n    md.treeprocessors.register(toc_treeprocessor, 'toc', 100)\n\n", "idx": 2}
{"namespace": "mistune.plugins.table.table_in_quote", "completion": "    md.block.rules.insert(md.block.rules.index('quote'), 'table')\n    md.block.rules.insert(md.block.rules.index('quote'), 'nptable')\n\n", "idx": 3}
{"namespace": "mistune.plugins.table.table_in_list", "completion": "    md.block.insert_rule('list', 'table', before='paragraph')\n    md.block.insert_rule('list', 'nptable', before='paragraph')", "idx": 4}
{"namespace": "xmnlp.utils.parallel_handler", "completion": "    if not isinstance(texts, list):\n        raise ValueError(\"You should pass a list of texts\")\n\n    with futures.ThreadPoolExecutor(max_workers=n_jobs) as executor:\n        jobs = [executor.submit(partial(callback, text, **kwargs)) for text in texts]\n        for job in futures.as_completed(jobs):\n            yield job.result()", "idx": 5}
{"namespace": "parsel.utils.shorten", "completion": "    if width < 0:\n        raise ValueError(\"width must be equal or greater than 0\")\n\n    if len(text) <= width:\n        return text\n\n    if width < len(suffix):\n        return suffix[:width]\n\n    return text[: width - len(suffix)] + suffix", "idx": 6}
{"namespace": "parsel.xpathfuncs.set_xpathfunc", "completion": "    if func is None:\n        etree.set_xpathfunc(fname, None)\n    else:\n        etree.set_xpathfunc(fname, func)\n\n", "idx": 7}
{"namespace": "dominate.dom_tag._get_thread_context", "completion": "  context = []\n  if hasattr(threading, 'current_thread'):\n    context.append(threading.current_thread().ident)\n  if greenlet is not None:\n    context.append(id(greenlet.getcurrent()))\n  return hash(tuple(context))\n\n", "idx": 8}
{"namespace": "dominate.util.system", "completion": "  import subprocess\n  if data is None:\n    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n  else:\n    p = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    p.stdin.write(data)\n  out, err = p.communicate()\n  return out.decode('utf-8')\n\n", "idx": 9}
{"namespace": "dominate.util.url_unescape", "completion": "  # The list of characters that need to be unescaped.\n  # Note that we are not using the \"unquote\" function from the \"urllib\" module.\n  # This is because \"unquote\" is not Python 2.7 compatible.\n  # The \"unquote\" function in Python 2.7 expects a byte string as input,\n  # while the \"unquote\" function in Python 2.6 expects a Unicode string as input.\n  # The \"unquote\" function in Python 2.6 is not Unicode-aware,\n  # so it will fail if the input string is a Unicode string.\n  # The \"unquote\" function in Python 2.7 is Unicode-aware,\n  # but it will fail if the input string is a byte string.\n  # Therefore, we are using our own list of characters to unescape.\n  _unescape_chars = ''.join([chr(x) for x in range(128)]).translate(None, _reserved)\n\n  # The list of characters that need to be unescaped.\n  # This list is a superset of the list defined above.\n  # This is because the \"unquote\" function in Python 2.7\n  # is more robust and can unescape characters that are not in the list above.\n  _unescape_chars_all = ''.join([chr(x) for x in range(128)]).translate(None, _reserved + '%')\n\n  # The list of characters that need to be escaped.\n  _escape_chars = ''.join([chr(x) for x in range(128)]).translate(None, _reserved + '$')\n\n  # The list of characters that need to be escaped.\n  # This list is a superset of the list defined above.\n  # This is because the \"quote\" function in Python 2.7\n  # is more robust and can escape characters that are not in the list above.\n  _escape_chars_all = ''.join([chr(x) for x in range(128)]).translate(None, _reserved + '$')\n\n  # The list of characters that", "idx": 10}
{"namespace": "rows.fields.DatetimeField.serialize", "completion": "        if value is None:\n            return \"\"\n\n        return six.text_type(value.strftime(\"%Y-%m-%dT%H:%M:%S\"))\n", "idx": 11}
{"namespace": "rows.fields.Field.serialize", "completion": "        return value\n", "idx": 12}
{"namespace": "rows.fields.EmailField.serialize", "completion": "        if value is None:\n            return \"\"\n        else:\n            return as_string(value)\n", "idx": 13}
{"namespace": "rows.fields.as_string", "completion": "    if isinstance(value, six.binary_type):\n        raise ValueError(\"Cannot convert binary to string\")\n    return six.text_type(value)\n\n", "idx": 14}
{"namespace": "rows.fields.get_items", "completion": "    def get_item(obj):\n        result = []\n        for index in indexes:\n            try:\n                result.append(obj[index])\n            except (IndexError, KeyError):\n                result.append(None)\n        return tuple(result)\n\n    return get_item\n\n", "idx": 15}
{"namespace": "pycorrector.proper_corrector.load_dict_file", "completion": "    dictionary = {}\n    if path and os.path.exists(path):\n        with open(path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line:\n                    line_list = line.split()\n                    if len(line_list) == 2:\n                        key, value = line_list\n                        dictionary[key] = value\n    return dictionary\n\n", "idx": 16}
{"namespace": "natasha.span.envelop_spans", "completion": "    for envelope in envelopes:\n        start = envelope.start\n        stop = envelope.stop\n        enveloped_spans = []\n        for span in spans:\n            if span.start >= start and span.stop <= stop:\n                enveloped_spans.append(span)\n        yield enveloped_spans", "idx": 17}
{"namespace": "googleapiclient._helpers.parse_unique_urlencoded", "completion": "    parsed_content = urllib.parse.parse_qs(content, keep_blank_values=True)\n    unique_content = {}\n    for key, value in parsed_content.items():\n        if len(value) != 1:\n            raise ValueError(\"Key %s has non-unique values.\" % key)\n        unique_content[key] = value[0]\n    return unique_content\n\n", "idx": 18}
{"namespace": "jinja2.async_utils.auto_aiter", "completion": "    if hasattr(iterable, \"__aiter__\"):\n        async for item in t.cast(\"t.AsyncIterable[V]\", iterable):\n            yield item\n    else:\n        for item in t.cast(\"t.Iterable[V]\", iterable):\n            yield item", "idx": 19}
{"namespace": "jinja2.utils.consume", "completion": "    for _ in iterable:\n        pass\n\n", "idx": 20}
{"namespace": "pycorrector.utils.tokenizer.segment", "completion": "    if cut_type == 'word':\n        if pos:\n            word_pos_seq = posseg.lcut(sentence)\n            word_seq, pos_seq = [], []\n            for word, pos in word_pos_seq:\n                if word == ' ':\n                    continue\n                word_seq.append(word)\n                pos_seq.append(pos)\n            return list(zip(word_seq, pos_seq))\n        else:\n            return posseg.lcut(sentence)\n\n    elif cut_type == 'char':\n        if pos:\n            word_pos_seq = posseg.cut(sentence)\n            word_seq, pos_seq = [], []\n            for word, pos in word_pos_seq:\n                if word == ' ':\n                    continue\n                for char in word:\n                    word_seq.append(char)\n                    pos_seq.append(pos)\n            return list(zip(word_seq, pos_seq))\n        else:\n            return list(posseg.cut(sentence))\n\n    else:\n        print(\"Wrong cut_type: \" + cut_type)\n        return None\n\n", "idx": 21}
{"namespace": "jinja2.utils.object_type_repr", "completion": "    if obj is None:\n        return \"None\"\n    elif obj is Ellipsis:\n        return \"Ellipsis\"\n    elif obj is NotImplemented:\n        return \"NotImplemented\"\n\n    obj_type = type(obj)\n    if obj_type.__module__ == \"builtins\":\n        return obj_type.__qualname__\n    else:\n        return f\"{obj_type.__module__}.{obj_type.__qualname__}\"\n\n", "idx": 22}
{"namespace": "jinja2.utils.LRUCache.setdefault", "completion": "        try:\n            return self[key]\n        except KeyError:\n            self[key] = default\n            return default\n", "idx": 23}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_word_freq", "completion": "        word_freq = {}\n        for w in list_of_words:\n            if w in word_freq:\n                word_freq[w] += 1\n            else:\n                word_freq[w] = 1\n        return word_freq\n", "idx": 24}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_average_probability_of_words", "completion": "        content_words_count = len(content_words_in_sentence)\n        if content_words_count == 0:\n            return 0\n        else:\n            content_words_freq = 0\n            for w in content_words_in_sentence:\n                content_words_freq += word_freq_in_doc.get(w, 0)\n            return content_words_freq / content_words_count\n", "idx": 25}
{"namespace": "sumy.summarizers.lex_rank.LexRankSummarizer._compute_idf", "completion": "        idf_metrics = {}\n        N = len(sentences)\n\n        for sentence in sentences:\n            for term in sentence:\n                if term not in idf_metrics:\n                    idf_metrics[term] = 0\n                idf_metrics[term] += 1\n\n        for term, freq in idf_metrics.items():\n            idf_metrics[term] = math.log(N / (1 + freq))\n\n        return idf_metrics\n", "idx": 26}
{"namespace": "sumy.summarizers.lex_rank.LexRankSummarizer.cosine_similarity", "completion": "        # Compute the dot product of two sentences\n        dot_product = 0\n        for term1 in sentence1:\n            for term2 in sentence2:\n                if term1 == term2:\n                    dot_product += tf1[term1] * tf2[term2] * idf_metrics[term2]\n\n        # Compute the magnitude of the 1st sentence\n        magnitude_a = 0\n        for term in sentence1:\n            magnitude_a += tf1[term] ** 2\n        magnitude_a = math.sqrt(magnitude_a)\n\n        # Compute the magnitude of the 2nd sentence\n        magnitude_b = 0\n        for term in sentence2:\n            magnitude_b += tf2[term] ** 2\n        magnitude_b = math.sqrt(magnitude_b)\n\n        # Compute the cosine similarity\n        if magnitude_a == 0 or magnitude_b == 0:\n            return 0.0\n        else:\n            return dot_product / (magnitude_a * magnitude_b)\n", "idx": 27}
{"namespace": "sumy.evaluation.rouge._get_ngrams", "completion": "    ngram_set = set()\n    text_length = len(text)\n    max_index_ngram_start = text_length - n\n    for i in range(max_index_ngram_start + 1):\n        ngram_set.add(tuple(text[i:i + n]))\n    return ngram_set\n\n", "idx": 28}
{"namespace": "sumy.evaluation.rouge._split_into_words", "completion": "    if not all(isinstance(sentence, Sentence) for sentence in sentences):\n        raise ValueError(\"Object in collection must be of type Sentence\")\n\n    words = []\n    for sentence in sentences:\n        for word in sentence.words:\n            words.append(word)\n    return words\n\n", "idx": 29}
{"namespace": "falcon.inspect.register_router", "completion": "    def _register_router(inspect_function):\n        if router_class in _supported_routers:\n            raise ValueError(\n                'Router class {} already registered.'.format(router_class)\n            )\n        _supported_routers[router_class] = inspect_function\n        return inspect_function\n\n    return _register_router\n\n", "idx": 30}
{"namespace": "falcon.inspect.inspect_compiled_router", "completion": "    routes = []\n    for route in router._roots:\n        routes.append(\n            RouteInfo(\n                route.method,\n                route.uri_template,\n                route.sink,\n                route.stream,\n                route.resource,\n                route.strict_slashes,\n                route.unquote,\n                route.source,\n            )\n        )\n        for child in route.children:\n            routes.append(\n                RouteInfo(\n                    child.method,\n                    child.uri_template,\n                    child.sink,\n                    child.stream,\n                    child.resource,\n                    child.strict_slashes,\n                    child.unquote,\n                    route.source,\n                )\n            )\n    return routes\n\n", "idx": 31}
{"namespace": "falcon.inspect._is_internal", "completion": "    return obj.__module__.startswith('falcon.')\n\n", "idx": 32}
{"namespace": "falcon.cmd.inspect_app.load_app", "completion": "    try:\n        module_name, app_name = args.app_module.split(':')\n    except ValueError:\n        module_name = args.app_module\n        app_name = 'app'\n\n    try:\n        module = importlib.import_module(module_name)\n    except ImportError:\n        parser.error('Could not import module: {0}'.format(module_name))\n\n    try:\n        app = getattr(module, app_name)\n    except AttributeError:\n        parser.error('Could not get app from module: {0}'.format(app_name))\n\n    if not isinstance(app, falcon.App):\n        try:\n            app = app()\n        except TypeError:\n            parser.error('Could not create app instance from: {0}'.format(app_name))\n\n    if not isinstance(app, falcon.App):\n        parser.error('App is not an instance of falcon.App')\n\n    return app\n\n", "idx": 33}
{"namespace": "falcon.cmd.inspect_app.make_parser", "completion": "    parser = argparse.ArgumentParser(description='Print routes for a Falcon app.')\n    parser.add_argument('-r', '--router', action='store_true',\n                        help='Include the router in the output.')\n    parser.add_argument('-v', '--verbose', action='store_true',\n                        help='Print the handler and kwargs.')\n    parser.add_argument('-i', '--internal', action='store_true',\n                        help='Include the internal routes.')\n    parser.add_argument('app_module',\n                        help='Python path to your app (ex: my.app)')\n    return parser\n\n", "idx": 34}
{"namespace": "falcon.util.uri.unquote_string", "completion": "    if not isinstance(quoted, str):\n        raise TypeError('Input must be a string.')\n\n    if len(quoted) < 2:\n        return quoted\n\n    if quoted[0] != '\"' or quoted[-1] != '\"':\n        return quoted\n\n    unquoted = quoted[1:-1]\n    unquoted = unquoted.replace('\\\\\\\\', '\\\\')\n    unquoted = unquoted.replace('\\\\\"', '\"')\n    unquoted = unquoted.replace('\\\\b', '\\b')\n    unquoted = unquoted.replace('\\\\f', '\\f')\n    unquoted = unquoted.replace('\\\\n', '\\n')\n    unquoted = unquoted.replace('\\\\r', '\\r')\n    unquoted = unquoted.replace('\\\\t', '\\t')\n    unquoted = unquoted.replace('\\\\v', '\\v')\n\n    return unquoted\n\n", "idx": 35}
{"namespace": "falcon.util.misc.get_argnames", "completion": "    # PERF(kgriffs): getargspec is deprecated in Python 3 but is a part of\n    #    the standard library, so it's still the best option for Py2/3\n    #    compatibility. Faster alternatives are only available in the\n    #    standard library in Python 3.4+.\n    arg_names, _, _, _ = inspect.getargspec(func)\n\n    return arg_names\n\n", "idx": 36}
{"namespace": "falcon.testing.client._is_asgi_app", "completion": "    if inspect.isclass(app):\n        app = app()\n\n    if not inspect.isfunction(app):\n        app = app.__call__\n\n    sig = inspect.signature(app)\n    num_args = len(sig.parameters)\n    if num_args == 3:\n        return True\n    elif num_args == 2:\n        return False\n    else:\n        raise TypeError(\n            'app must be a callable that accepts either 2 or 3 arguments, but it accepts '\n            f'{num_args}.'\n        )\n\n", "idx": 37}
{"namespace": "falcon.routing.converters.UUIDConverter.convert", "completion": "        try:\n            return uuid.UUID(value)\n        except ValueError:\n            return None", "idx": 38}
{"namespace": "rest_framework_simplejwt.utils.make_utc", "completion": "    if is_naive(dt):\n        if settings.USE_TZ:\n            return make_aware(dt, timezone.utc)\n        else:\n            return dt.replace(tzinfo=timezone.utc)\n    else:\n        return dt.astimezone(timezone.utc)\n\n", "idx": 39}
{"namespace": "boto.sdb.db.sequence.fib", "completion": "    return cv + lv\n", "idx": 40}
{"namespace": "boto.s3.website.RoutingRules.add_rule", "completion": "        self.append(rule)\n        return self\n", "idx": 41}
{"namespace": "boto.cloudfront.distribution.Distribution._canned_policy", "completion": "        return '{\"Statement\":[{\"Resource\":\"%(resource)s\",\"Condition\":{\"DateLessThan\":{\"AWS:EpochTime\":%(expires)s}}}]}' % {'resource': resource, 'expires': expires}\n", "idx": 42}
{"namespace": "boto.cloudfront.invalidation.InvalidationBatch.escape", "completion": "        if p[0] != '/':\n            p = '/' + p\n        return urllib.quote(p.replace('%', '%25').replace('?', '%3F').replace('=', '%3D').replace('&', '%26').replace(' ', '%20').replace('\"', '%22').replace('~', '%7E').replace('{', '%7B').replace('}', '%7D').replace('\\\\', '%5C').replace('<', '%3C').replace('>', '%3E').replace('|', '%7C').replace('(', '%28').replace(')', '%29').replace('^', '%5E').replace('[', '%5B').replace(']', '%5D').replace(';', '%3B').replace('@', '%40').replace(':', '%3A').replace(',', '%2C').replace('$', '%24').replace('#', '%23').replace('+', '%2B').replace('!', '%21').replace('-', '%2D').replace('.', '%2E').replace(' ','%20'))\n", "idx": 43}
{"namespace": "proxybroker.utils.get_status_code", "completion": "    if resp is None:\n        return 400\n    try:\n        return int(resp[start:stop])\n    except ValueError:\n        return 400\n\n", "idx": 44}
{"namespace": "authlib.oauth2.rfc6749.util.scope_to_list", "completion": "    if isinstance(scope, (tuple, list, set)):\n        return [to_unicode(s) for s in scope]\n    if scope is None:\n        return scope\n    return [to_unicode(s) for s in scope.split(\" \")]\n\n", "idx": 45}
{"namespace": "authlib.common.encoding.to_unicode", "completion": "    if x is None:\n        return None\n    if isinstance(x, str):\n        return x\n    if isinstance(x, bytes):\n        return x.decode(charset, errors)\n    if isinstance(x, (int, float)):\n        return str(x)\n    return str(x)\n\n", "idx": 46}
{"namespace": "authlib.common.encoding.to_bytes", "completion": "    if x is None:\n        return None\n    if isinstance(x, bytes):\n        return x\n    if isinstance(x, str):\n        return x.encode(charset, errors)\n    if isinstance(x, int) or isinstance(x, float):\n        return str(x).encode(charset, errors)\n    return str(x).encode(charset, errors)\n\n", "idx": 47}
{"namespace": "authlib.common.encoding.urlsafe_b64decode", "completion": "    if isinstance(s, str):\n        s = s.encode('ascii')\n    s = s.replace(b'-', b'+')\n    s = s.replace(b'_', b'/')\n    s = s + b'=' * (4 - (len(s) % 4))\n    return base64.b64decode(s)\n\n", "idx": 48}
{"namespace": "csvs_to_sqlite.utils.table_exists", "completion": "    # Check if the table exists in the database\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name=?\", (table,))\n    return len(cursor.fetchall()) > 0\n\n", "idx": 49}
{"namespace": "sqlitedict.SqliteDict.get_tablenames", "completion": "        if not os.path.exists(filename):\n            raise IOError('file {} does not exist'.format(filename))\n\n        # connect to database\n        conn = sqlite3.connect(filename)\n\n        # get cursor\n        cursor = conn.cursor()\n\n        # get table names\n        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n        tablenames = [t[0] for t in cursor.fetchall()]\n\n        # close connection\n        conn.close()\n\n        return tablenames\n", "idx": 50}
{"namespace": "litecli.packages.parseutils.query_starts_with", "completion": "    if not query:\n        return False\n\n    query = query.lower()\n    query = sqlparse.format(query, strip_comments=True)\n    query = query.split()[0]\n\n    return query in prefixes\n\n", "idx": 51}
{"namespace": "rest_framework.negotiation.DefaultContentNegotiation.filter_renderers", "completion": "        filtered_renderers = []\n        for renderer in renderers:\n            if format in renderer.format:\n                filtered_renderers.append(renderer)\n        if not filtered_renderers:\n            raise Http404\n        return filtered_renderers\n", "idx": 52}
{"namespace": "rest_framework.templatetags.rest_framework.as_string", "completion": "    return str(value)\n\n", "idx": 53}
{"namespace": "rest_framework.templatetags.rest_framework.add_nested_class", "completion": "    if isinstance(value, dict):\n        return 'class=nested'\n    elif isinstance(value, list):\n        for item in value:\n            if isinstance(item, dict):\n                return 'class=nested'\n    return ''", "idx": 54}
{"namespace": "pyramid.session.PickleSerializer.loads", "completion": "        return pickle.loads(bstruct)\n", "idx": 55}
{"namespace": "pyramid.testing.DummySession.flash", "completion": "        if not allow_duplicate:\n            if queue in self:\n                if msg in self[queue]:\n                    return\n            self[queue] = self.get(queue, [])\n            self[queue].append(msg)\n", "idx": 56}
{"namespace": "pyramid.testing.DummySession.pop_flash", "completion": "        storage = self.pop('_f_' + queue, [])\n        return storage\n", "idx": 57}
{"namespace": "pyramid.testing.DummySession.peek_flash", "completion": "        storage = self.get('_f_' + queue, [])\n        return storage\n", "idx": 58}
{"namespace": "pyramid.testing.DummySession.new_csrf_token", "completion": "        token = '0123456789012345678901234567890123456789'\n        self['_csrft_'] = token\n        return token\n", "idx": 59}
{"namespace": "pyramid.view.view_defaults", "completion": "    def decorator(wrapped):\n        if not hasattr(wrapped, '__view_defaults__'):\n            wrapped.__view_defaults__ = settings\n        else:\n            wrapped.__view_defaults__.update(settings)\n        return wrapped\n\n    return decorator\n\n", "idx": 60}
{"namespace": "pyramid.util.bytes_", "completion": "    if isinstance(s, str):\n        return s.encode(encoding, errors)\n    return s\n\n", "idx": 61}
{"namespace": "pyramid.scripts.common.parse_vars", "completion": "    # Create an empty dictionary\n    vars = {}\n\n    # Iterate over the list of strings\n    for arg in args:\n\n        # Split the string into a list of two elements\n        key, value = arg.split('=')\n\n        # Add the key and value to the dictionary\n        vars[key] = value\n\n    # Return the dictionary\n    return vars\n\n", "idx": 62}
{"namespace": "pyramid.scripts.pviews.PViewsCommand._find_multi_routes", "completion": "        infos = []\n        for route in mapper.get_routes():\n            match = route.match(request.path_info)\n            if match is not None:\n                infos.append({'match': match, 'route': route})\n        return infos\n", "idx": 63}
{"namespace": "pyramid.scripts.pserve.PServeCommand.guess_server_url", "completion": "        if not server_name:\n            server_name = 'main'\n        server_settings = loader.get_settings('server:' + server_name, global_conf)\n        port = server_settings.get('port')\n        if port:\n            return 'http://127.0.0.1:%s' % port\n", "idx": 64}
{"namespace": "aiohappybase._util.pep8_to_camel_case", "completion": "    chunks = name.split('_')\n    if initial:\n        return ''.join(chunk.title() for chunk in chunks)\n    else:\n        return chunks[0] + ''.join(chunk.title() for chunk in chunks[1:])\n\n", "idx": 65}
{"namespace": "aiohappybase._util.bytes_increment", "completion": "    for i in range(len(b) - 1, -1, -1):\n        if b[i] != 0xff:\n            return b[:i] + bytes([b[i] + 1]) + bytes(len(b) - i - 1)\n    return None\n\n", "idx": 66}
{"namespace": "mssqlcli.config.ensure_dir_exists", "completion": "    directory = dirname(path)\n    if not exists(directory):\n        os.makedirs(directory)\n\n", "idx": 67}
{"namespace": "mssqlcli.telemetry._user_id_file_is_old", "completion": "    if os.path.exists(id_file_path):\n        modified_time = datetime.fromtimestamp(os.path.getmtime(id_file_path))\n        return datetime.now() - modified_time > timedelta(hours=24)\n    return False\n\n", "idx": 68}
{"namespace": "mssqlcli.util.is_command_valid", "completion": "    try:\n        devnull = open(os.devnull, 'w')\n        subprocess.call(command, stdout=devnull, stderr=devnull)\n    except OSError:\n        return False\n    return True\n\n", "idx": 69}
{"namespace": "mssqlcli.packages.parseutils.utils.find_prev_keyword", "completion": "    # Remove comments\n    sql = sqlparse.format(sql, strip_comments=True)\n\n    # Tokenize the SQL statement\n    tokens = sqlparse.parse(sql)[0].tokens\n\n    # Find the last keyword\n    keyword = None\n    for token in tokens:\n        if token.ttype in [Token.Keyword, Token.Keyword.DDL, Token.Keyword.DML]:\n            keyword = token.value\n\n    # If a keyword is found, return the keyword and the text of the query with everything after the keyword stripped\n    if keyword:\n        return (keyword, sql[:sql.find(keyword)].strip())\n\n    # If no keyword is found, return None\n    return (None, None)\n\n", "idx": 70}
{"namespace": "trafilatura.settings.use_config", "completion": "    if config is None:\n        if filename is None:\n            filename = Path(__file__).parent / \"settings.cfg\"\n        config = ConfigParser()\n        config.read(filename)\n    return config\n\n", "idx": 71}
{"namespace": "pyramid.util.text_", "completion": "    if isinstance(s, bytes):\n        return s.decode(encoding, errors)\n    return s\n\n", "idx": 72}
{"namespace": "trafilatura.downloads._parse_config", "completion": "    user_agents = []\n    cookies = []\n    for user_agent in config.get('http', 'user_agents').split(','):\n        user_agents.append(user_agent.strip())\n    for cookie in config.get('http', 'cookies').split(','):\n        cookies.append(cookie.strip())\n    return user_agents, cookies\n\n", "idx": 73}
{"namespace": "trafilatura.downloads.load_download_buffer", "completion": "    # get the list of URLs to download\n    bufferlist = url_store.get_urls()\n\n    # if the bufferlist is empty, sleep\n    if not bufferlist:\n        sleep(sleep_time)\n        return [], url_store\n\n    # if the bufferlist is not empty, return the list of URLs and the url_store object\n    return bufferlist, url_store\n\n", "idx": 74}
{"namespace": "trafilatura.metadata.check_authors", "completion": "    new_authors = []\n    for author in authors.split(';'):\n        if author.lower() not in author_blacklist:\n            new_authors.append(author)\n    if new_authors:\n        return '; '.join(new_authors)\n    return None\n\n", "idx": 75}
{"namespace": "datasette.filters.where_filters", "completion": "    where_clauses = []\n    extra_wheres_for_ui = []\n    if \"_where\" in request.args:\n        if not datasette.permission_allowed(\n            request.actor, \"execute-sql\", database=database.name\n        ):\n            raise DatasetteError(\"Permission denied\", status=403)\n        for where_clause in request.args.getlist(\"_where\"):\n            if not where_clause:\n                continue\n            if \"__\" in where_clause:\n                raise BadRequest(\"__ is not allowed in _where\")\n            where_clause = where_clause.strip()\n            if where_clause.startswith(\"[\") and where_clause.endswith(\"]\"):\n                # It's a JSON array, so pass it straight through\n                pass\n            elif where_clause.startswith(\"(\") and where_clause.endswith(\")\"):\n                # SQL \"where\" clause without the leading \"where\"\n                where_clause = where_clause[1:-1]\n            else:\n                # It's a JSON object\n                where_clause = \"json_extract(json(_row_), '$') @> '{}'\".format(\n                    json.dumps(json.loads(where_clause))\n                )\n            where_clauses.append(where_clause)\n            extra_wheres_for_ui.append(where_clause)\n\n    def inner(template, context):\n        return {\n            \"where_clauses\": where_clauses,\n            \"extra_wheres_for_ui\": extra_wheres_for_ui,\n        }\n\n    return inner\n\n", "idx": 76}
{"namespace": "datasette.utils.path_with_added_args", "completion": "    if path is None:\n        path = request.path\n    args_str = urllib.parse.urlencode(args)\n    if args_str:\n        path_with_args = path + \"?\" + args_str\n    else:\n        path_with_args = path\n    return path_with_args\n\n", "idx": 77}
{"namespace": "datasette.utils.path_with_replaced_args", "completion": "    path = path or request.path\n    if isinstance(args, dict):\n        args = args.items()\n    current = []\n    for key, value in urllib.parse.parse_qsl(request.query_string):\n        if not (key, value) in args:\n            current.append((key, value))\n    current.extend([(key, value) for key, value in args if value is not None])\n    query_string = urllib.parse.urlencode(current)\n    if query_string:\n        query_string = f\"?{query_string}\"\n    return path + query_string\n\n", "idx": 78}
{"namespace": "datasette.utils.format_bytes", "completion": "    bytes = float(bytes)\n    kb = float(1024)\n    mb = float(kb ** 2)  # 1,048,576\n    gb = float(kb ** 3)  # 1,073,741,824\n    tb = float(kb ** 4)  # 1,099,511,627,776\n\n    if bytes < kb:\n        return \"{0} {1}\".format(bytes, \"Bytes\" if 0 == bytes > 1 else \"Byte\")\n    elif kb <= bytes < mb:\n        return \"{0:.2f} KB\".format(bytes / kb)\n    elif mb <= bytes < gb:\n        return \"{0:.2f} MB\".format(bytes / mb)\n    elif gb <= bytes < tb:\n        return \"{0:.2f} GB\".format(bytes / gb)\n    elif tb <= bytes:\n        return \"{0:.2f} TB\".format(bytes / tb)\n\n", "idx": 79}
{"namespace": "datasette.utils.actor_matches_allow", "completion": "    if allow is None:\n        return True\n    if isinstance(allow, str):\n        return actor == allow\n    if isinstance(allow, list):\n        return actor in allow\n    if isinstance(allow, dict):\n        if \"field\" in allow and \"value\" in allow:\n            return actor == allow[\"value\"]\n        if \"field\" in allow and \"match\" in allow:\n            if allow[\"match\"] == \"regex\":\n                return re.match(allow[\"value\"], actor) is not None\n            if allow[\"match\"] == \"ipaddress\":\n                return ipaddress.ip_address(actor) == ipaddress.ip_address(allow[\"value\"])\n            if allow[\"match\"] == \"in\":\n                return actor in allow[\"value\"]\n            if allow[\"match\"] == \"startswith\":\n                return actor.startswith(allow[\"value\"])\n            if allow[\"match\"] == \"endswith\":\n                return actor.endswith(allow[\"value\"])\n            if allow[\"match\"] == \"contains\":\n                return allow[\"value\"] in actor\n            if allow[\"match\"] == \"iequals\":\n                return actor.lower() == allow[\"value\"].lower()\n            if allow[\"match\"] == \"icontains\":\n                return allow[\"value\"].lower() in actor.lower()\n            if allow[\"match\"] == \"istartswith\":\n                return actor.lower().startswith(allow[\"value\"].lower())\n            if allow[\"match\"] == \"iendswith\":\n                return actor.lower().endswith(allow[\"value\"].lower())\n            if allow[\"match\"] == \"isin\":\n                return actor.lower() in [i.lower() for i in allow[\"value\"]]\n            if allow[\"match\"] == \"isnotin\":\n                return actor.lower() not in [i.lower() for i in allow[\"value\"]]\n            if allow[\"match\"] == \"isunique\":\n                return actor not in [i for i in allow[\"value\"]]\n            if allow[\"match\"] == \"isnotunique\":\n                return actor in [i for i in allow[\"value\"]]\n            if allow[\"match\"] == \"equals\":\n                return actor == allow[\"value\"]", "idx": 80}
{"namespace": "datasette.utils.resolve_env_secrets", "completion": "    if isinstance(config, dict):\n        if len(config) == 1:\n            key, value = list(config.items())[0]\n            if key == \"$env\":\n                return environ[value]\n            elif key == \"$file\":\n                with open(value, \"r\") as f:\n                    return f.read()\n            else:\n                return resolve_env_secrets(value, environ)\n        else:\n            return {key: resolve_env_secrets(value, environ) for key, value in config.items()}\n    elif isinstance(config, list):\n        return [resolve_env_secrets(value, environ) for value in config]\n    else:\n        return config\n\n", "idx": 81}
{"namespace": "datasette.utils.display_actor", "completion": "    if \"display_name\" in actor:\n        return actor[\"display_name\"]\n    elif \"name\" in actor:\n        return actor[\"name\"]\n    elif \"username\" in actor:\n        return actor[\"username\"]\n    elif \"login\" in actor:\n        return actor[\"login\"]\n    elif \"id\" in actor:\n        return actor[\"id\"]\n    else:\n        return str(actor)\n\n", "idx": 82}
{"namespace": "datasette.utils.initial_path_for_datasette", "completion": "    dbs = await datasette.get_database_names()\n    if len(dbs) == 1:\n        return \"/{db}\".format(db=dbs[0])\n    tables = await datasette.get_table_names()\n    if len(tables) == 1:\n        return \"/{db}/{table}\".format(db=dbs[0], table=tables[0])\n    return \"/\"\n\n", "idx": 83}
{"namespace": "datasette.utils.tilde_decode", "completion": "    return (\n        s.replace(\"%7E\", \"~\")\n        .encode(\"utf-8\")\n        .decode(\"url\")\n        .replace(\"~\", \"%7E\")\n        .replace(\"%20\", \" \")\n    )\n\n", "idx": 84}
{"namespace": "datasette.utils.resolve_routes", "completion": "    for route in routes:\n        match = re.match(route[0], path)\n        if match:\n            return route[1], match\n    return None\n\n", "idx": 85}
{"namespace": "datasette.utils.truncate_url", "completion": "    if len(url) <= length:\n        return url\n\n    # If the URL ends with a file extension and the extension length is between 1 and 4 characters without a slash, truncate the URL to the specified length and add ellipsis and the extension at the end.\n    if url.endswith(\"/\") == False and url.find(\"/\", len(url) - 5, len(url)) == -1 and url.rfind(\".\", len(url) - 5, len(url)) > len(url) - 5 and url.rfind(\".\", len(url) - 5, len(url)) < len(url):\n        return url[:length - 3] + \"...\" + url[url.rfind(\".\"):]\n\n    # Otherwise, truncate the URL to the specified length and add ellipsis at the end.\n    return url[:length - 3] + \"...\"\n\n", "idx": 86}
{"namespace": "kinto.core.authorization.groupfinder", "completion": "    if not hasattr(request, \"effective_principals\"):\n        request.effective_principals = {}\n\n    if userid not in request.effective_principals:\n        principals = []\n        if request.registry.permission:\n            try:\n                principals = request.registry.permission.get_user_principals(userid)\n            except storage_exceptions.BackendError:\n                logger.exception(\"Failed to fetch principals for user %r\", userid)\n        request.effective_principals[userid] = principals\n    return request.effective_principals[userid]\n\n", "idx": 87}
{"namespace": "kinto.core.utils.json.dumps", "completion": "        kw.setdefault('bytes_mode', rapidjson.BM_NONE)\n        return rapidjson.dumps(v, **kw)\n", "idx": 88}
{"namespace": "kinto.core.utils.json.loads", "completion": "        kw.setdefault(\"number_mode\", rapidjson.NM_NATIVE)\n        return rapidjson.loads(v, **kw)\n\n", "idx": 89}
{"namespace": "kinto.core.utils.hmac_digest", "completion": "    if isinstance(secret, str):\n        secret = secret.encode(encoding)\n    return hmac.new(secret, message.encode(encoding), hashlib.sha256).hexdigest()\n\n", "idx": 90}
{"namespace": "kinto.core.utils.current_service", "completion": "    try:\n        return request.registry.cornice_services[request.matched_route.pattern]\n    except (AttributeError, KeyError):\n        return None\n\n", "idx": 91}
{"namespace": "kinto.core.utils.prefixed_principals", "completion": "    principals = request.effective_principals\n    if Authenticated in principals:\n        prefixed_userid = prefixed_userid(request)\n        if prefixed_userid is not None:\n            principals.remove(Authenticated)\n            principals.insert(0, prefixed_userid)\n    return principals\n\n", "idx": 92}
{"namespace": "kinto.plugins.accounts.views.on_account_created", "completion": "    request = event.request\n    settings = request.registry.settings\n    if settings.get(\"account_validation\", False):\n        email_regexp = settings.get(\"account_validation.email_regexp\", DEFAULT_EMAIL_REGEXP)\n        email_sender = settings.get(\"account_validation.email_sender\", \"no-reply@localhost\")\n        email_subject = settings.get(\"account_validation.email_subject\", \"Account Validation\")\n        email_body = settings.get(\"account_validation.email_body\", \"Please click the link below to activate your account.\")\n        email_body_html = settings.get(\"account_validation.email_body_html\", None)\n        email_body_template = settings.get(\"account_validation.email_body_template\", None)\n        email_sender_name = settings.get(\"account_validation.email_sender_name\", \"Kinto Admin\")\n        email_sender_email = settings.get(\"account_validation.email_sender_email\", email_sender)\n        email_sender_reply = settings.get(\"account_validation.email_sender_reply\", email_sender)\n        email_sender_logo = settings.get(\"account_validation.email_sender_logo\", None)\n        email_sender_color = settings.get(\"account_validation.email_sender_color\", None)\n        email_sender_url = settings.get(\"account_validation.email_sender_url\", None)\n        email_sender_tagline = settings.get(\"account_validation.email_sender_tagline\", None)\n        email_sender_footer_center = settings.get(\"account_validation.email_sender_footer_center\", None)\n        email_sender_footer_left = settings.get(\"account_validation.email_sender_footer_left\", None)\n        email_sender_footer_right = settings.get(\"account_validation.email_sender_footer_right\", None)\n        email_cc = settings.get(\"account_validation.email_cc\", None)\n        email_bcc = settings.get(\"account_validation.email_bcc\", None)\n        email_reply =", "idx": 93}
{"namespace": "kinto.plugins.accounts.utils.hash_password", "completion": "    password_hash = bcrypt.hashpw(password.encode('utf-8'), bcrypt.gensalt())\n    return password_hash.decode('utf-8')\n\n", "idx": 94}
{"namespace": "kinto.views.admin.get_parent_uri", "completion": "    # If the URI is not a valid URL, return an empty string.\n    if not core_utils.is_valid_url(object_uri):\n        return \"\"\n\n    # Split the URI by \"/\".\n    path = object_uri.split(\"/\")\n\n    # If the path length is less than 3, return an empty string.\n    if len(path) < 3:\n        return \"\"\n\n    # Return the first element of the path as the parent URI.\n    return \"/\".join(path[:2])\n\n", "idx": 95}
{"namespace": "alembic.script.write_hooks.register", "completion": "    def inner(func: Callable) -> Callable:\n        _registry[name] = func\n        return func\n\n    return inner\n\n", "idx": 96}
{"namespace": "mongo_connector.namespace_config.match_replace_regex", "completion": "    match = regex.match(src_namespace)\n    if match:\n        return dest_namespace.replace(\"*\", match.group(1))\n    return None\n\n", "idx": 97}
{"namespace": "mongo_connector.namespace_config.namespace_to_regex", "completion": "    # Split the namespace into database and collection names\n    db_name, coll_name = namespace.split(\".\", 1)\n    # Create a regular expression for the database name\n    db_regex = re.escape(db_name).replace(r\"\\*\", \"(.*?)\")\n    # Create a regular expression for the collection name\n    coll_regex = re.escape(coll_name).replace(r\"\\*\", \"(.*?)\")\n    # Compile the regular expression\n    return re.compile(\"^%s\\\\.%s$\" % (db_regex, coll_regex))", "idx": 98}
{"namespace": "mongo_connector.util.long_to_bson_ts", "completion": "    seconds = val >> 32\n    increment = val & 0xffffffff\n    return Timestamp(seconds, increment)\n\n", "idx": 99}
{"namespace": "mongo_connector.doc_managers.formatters.DocumentFlattener.format_document", "completion": "        def _kernel(doc):\n            for key in doc:\n                value = doc[key]\n                for new_k, new_v in self.transform_element(key, value):\n                    yield new_k, new_v\n\n        return dict(_kernel(document))\n\n", "idx": 100}
{"namespace": "bplustree.memory.open_file_in_dir", "completion": "    dir_path = os.path.dirname(path)\n    if platform.system() == \"Windows\":\n        dir_fd = None\n    else:\n        dir_fd = os.open(dir_path, os.O_RDONLY)\n    fd = os.open(path, os.O_RDWR | os.O_CREAT, 0o666)\n    return (os.fdopen(fd, \"rb+\"), dir_fd)\n\n", "idx": 101}
{"namespace": "bplustree.memory.FileMemory.read_transaction", "completion": "        return ReadTransaction(self)\n", "idx": 102}
{"namespace": "bplustree.utils.pairwise", "completion": "    a, b = itertools.tee(iterable)\n    next(b, None)\n    return zip(a, b)", "idx": 103}
{"namespace": "bplustree.utils.iter_slice", "completion": "    for i in range(0, len(iterable), n):\n        yield iterable[i:i + n], i + n >= len(iterable)", "idx": 104}
{"namespace": "bplustree.serializer.StrSerializer.serialize", "completion": "        if len(obj.encode('utf-8')) > key_size:\n            raise ValueError('String is too long.')\n        return obj.encode('utf-8')\n", "idx": 105}
{"namespace": "psd_tools.utils.pack", "completion": "    return struct.pack(fmt, *args)\n\n", "idx": 106}
{"namespace": "psd_tools.utils.unpack", "completion": "    fmt = str(\">\" + fmt)\n    return struct.unpack(fmt, data)\n\n", "idx": 107}
{"namespace": "psd_tools.api.numpy_io.get_pattern", "completion": "    # Extracting the height and width of the pattern\n    height = pattern.data[3]\n    width = pattern.data[4]\n\n    # Creating the pattern array\n    pattern_array = np.zeros((height, width, 4), dtype=np.uint8)\n\n    # Parsing the data from the channels in the pattern's \"data\" attribute\n    for i in range(0, len(pattern.data), 3):\n        pattern_array[pattern.data[i], pattern.data[i + 1]] = pattern.data[i + 2]\n\n    return pattern_array\n\n", "idx": 108}
{"namespace": "sqlite_utils.utils.maximize_csv_field_size_limit", "completion": "    # Set the field size limit to the maximum size of the system.\n    csv.field_size_limit(sys.maxsize)\n\n    # Iteratively try to set the field size limit to the maximum possible value.\n    while True:\n        try:\n            csv.field_size_limit(csv.field_size_limit() * 2)\n        except OverflowError:\n            break\n\n", "idx": 109}
{"namespace": "sqlite_utils.utils.column_affinity", "completion": "    if column_type.upper().startswith(\"INT\"):\n        return \"INT\"\n    elif column_type.upper().startswith(\"CHAR\"):\n        return \"CHAR\"\n    elif column_type.upper().startswith(\"CLOB\"):\n        return \"CLOB\"\n    elif column_type.upper().startswith(\"TEXT\"):\n        return \"TEXT\"\n    elif column_type.upper().startswith(\"BLOB\"):\n        return \"BLOB\"\n    elif column_type.upper().startswith(\"REAL\"):\n        return \"REAL\"\n    elif column_type.upper().startswith(\"FLOA\"):\n        return \"FLOA\"\n    elif column_type.upper().startswith(\"DOUB\"):\n        return \"DOUB\"\n    else:\n        return \"TEXT\"\n\n", "idx": 110}
{"namespace": "sqlite_utils.utils.decode_base64_values", "completion": "    if isinstance(doc, dict):\n        # Decode the 'encoded' value if $base64 is True\n        if \"$base64\" in doc and doc[\"$base64\"] is True:\n            doc[\"decoded\"] = base64.b64decode(doc[\"encoded\"])\n        else:\n            doc[\"decoded\"] = doc[\"encoded\"]\n        # Recurse into each value\n        for key, value in doc.items():\n            if isinstance(value, dict):\n                doc[key] = decode_base64_values(value)\n    return doc\n\n", "idx": 111}
{"namespace": "sqlite_utils.utils.chunks", "completion": "    it = iter(sequence)\n    while True:\n        chunk = tuple(itertools.islice(it, size))\n        if not chunk:\n            return\n        yield chunk\n\n", "idx": 112}
{"namespace": "sqlite_utils.utils.hash_record", "completion": "    if keys is None:\n        keys = record.keys()\n    key_value_pairs = [str(key) + str(record[key]) for key in keys]\n    return hashlib.sha1(\"\".join(key_value_pairs).encode(\"utf-8\")).hexdigest()", "idx": 113}
{"namespace": "arctic.decorators._get_host", "completion": "    if store:\n        if isinstance(store, (list, tuple)):\n            store = store[0]\n        return {'lib': store.library_name, 'nodes': store.nodes, 'host': store.host}\n    else:\n        return None\n\n", "idx": 114}
{"namespace": "arctic.decorators.mongo_retry", "completion": "    @wraps(f)\n    def wrapper(*args, **kwargs):\n        global _in_retry\n        global _retry_count\n        _in_retry = True\n        _retry_count = 0\n        while _retry_count < _MAX_RETRIES:\n            try:\n                return f(*args, **kwargs)\n            except (AutoReconnect, OperationFailure) as e:\n                _retry_count += 1\n                logger.warning(\"Retrying {} {}\".format(f.__name__, _retry_count))\n                sleep(1)\n            except DuplicateKeyError as e:\n                logger.error(\"DuplicateKeyError: {}\".format(e))\n                raise\n            except ServerSelectionTimeoutError as e:\n                logger.error(\"ServerSelectionTimeoutError: {}\".format(e))\n                raise\n            except BulkWriteError as e:\n                logger.error(\"BulkWriteError: {}\".format(e))\n                raise\n            except Exception as e:\n                if 'arctic' in sys.modules and _log_exception:\n                    _log_exception(e)\n                raise\n        _in_retry = False\n        raise Exception(\"Maximum number of retries reached\")\n\n    return wrapper\n\n", "idx": 115}
{"namespace": "arctic._util.are_equals", "completion": "    try:\n        if isinstance(o1, DataFrame) and isinstance(o2, DataFrame):\n            return assert_frame_equal(o1, o2, **kwargs) is None\n        else:\n            return o1 == o2\n    except:\n        return False\n\n", "idx": 116}
{"namespace": "arctic.hooks.register_resolve_mongodb_hook", "completion": "    global _resolve_mongodb_hook\n    _resolve_mongodb_hook = hook\n\n", "idx": 117}
{"namespace": "arctic.hooks.register_log_exception_hook", "completion": "    global _log_exception_hook\n    _log_exception_hook = hook\n\n", "idx": 118}
{"namespace": "arctic.hooks.register_get_auth_hook", "completion": "    global _get_auth_hook\n    _get_auth_hook = hook\n\n", "idx": 119}
{"namespace": "arctic.store._version_store_utils._split_arrs", "completion": "    # Get the number of sub-arrays\n    num_sub_arrs = len(slices)\n\n    # Get the number of rows in the array\n    num_rows = array_2d.shape[0]\n\n    # Get the number of columns in the array\n    num_cols = array_2d.shape[1]\n\n    # Get the number of rows in each sub-array\n    num_rows_per_sub_arr = num_rows // num_sub_arrs\n\n    # Get the number of elements in each sub-array\n    num_elems_per_sub_arr = num_rows_per_sub_arr * num_cols\n\n    # Get the indices where the array is split\n    indices = slices\n\n    # Get the starting indices of each sub-array\n    start_indices = [0] + indices[:-1]\n\n    # Get the starting indices of each sub-array\n    end_indices = indices\n\n    # Get the starting indices of each sub-array\n    sub_arr_start_indices = [start_indices, end_indices]\n\n    # Get the starting indices of each sub-array\n    sub_arr_start_indices = np.array(sub_arr_start_indices).T\n\n    # Get the starting indices of each sub-array\n    sub_arr_start_indices = sub_arr_start_indices.flatten()\n\n    # Get the starting indices of each sub-array\n    sub_arr_end_indices = sub_arr_start_indices + num_elems_per_sub_arr\n\n    # Get the starting indices of each sub-array\n    sub_arr_end_indices = sub_arr_end_indices.tolist()\n\n    # Get the starting indices of each sub-array\n    sub_arr_end_indices.append(num_rows)\n\n    # Get the starting indices of each sub-array\n    sub_arr_end_indices = np.array(sub_arr_end_indices)\n\n    # Get the starting indices of each sub-array\n    sub_arr_start_", "idx": 120}
{"namespace": "arctic.store._version_store_utils.checksum", "completion": "    # Convert the dictionary to a byte string.\n    doc_byte_string = pickle_compat.dumps(doc, pickle.HIGHEST_PROTOCOL)\n\n    # Calculate the checksum of the byte string.\n    checksum_byte_string = hashlib.sha1(symbol.encode('utf-8') + doc_byte_string).digest()\n\n    # Convert the checksum to a Binary object.\n    checksum_binary = Binary(checksum_byte_string)\n\n    return checksum_binary\n\n", "idx": 121}
{"namespace": "arctic.store.versioned_item.VersionedItem.__str__", "completion": "        return \"VersionedItem(symbol={symbol},library={library},data={data},version={version},metadata={metadata},host={host})\".format(\n            symbol=self.symbol,\n            library=self.library,\n            data=self.data,\n            version=self.version,\n            metadata=self.metadata,\n            host=self.host\n        )", "idx": 122}
{"namespace": "arctic.store._ndarray_store.NdarrayStore._dtype", "completion": "        if metadata is None:\n            metadata = {}\n        if string.startswith('[') and string.endswith(']'):\n            string = string[1:-1]\n        if string.startswith('(') and string.endswith(')'):\n            string = string[1:-1]\n        if string.startswith('{'):\n            string = string[1:-1]\n        if string.startswith('('):\n            string = string[1:-1]\n        if string.startswith('{'):\n            string = string[1:-1]\n        if string.startswith('('):\n            string = string[1:-1]\n        if string.startswith('{'):\n            string = string[1:-1]\n        if string.startswith('('):\n            string = string[1:-1]\n        if string.startswith('{'):\n            string = string[1:-1]\n        if string.startswith('('):\n            string = string[1:-1]\n        if string.startswith('{'):\n            string = string[1:-1]\n        if string.startswith('('):\n            string = string[1:-1]\n        if string.startswith('{'):\n            string = string[1:-1]\n        if string.startswith('('):\n            string = string[1:-1]\n        if string.startswith('{'):\n            string = string[1:-1]\n        if string.startswith('('):\n            string = string[1:-1]\n        if string.startswith('{'):\n            string = string[1:-1]\n        if string.startswith('('):\n            string = string[1:-1]\n        if string.startswith('{'):\n            string = string[1:-1]\n        if string.startswith('('):\n            string = string[1:-1]\n        if string.startswith('{'):\n            string = string[1:-1]\n        if string.startswith('('):\n            string = string[1:-1]\n        if string.startswith('{'):\n            string = string[1:-1]\n        if string.startswith('('):", "idx": 123}
{"namespace": "arctic.store._ndarray_store._promote_struct_dtypes", "completion": "    # Check if dtype1 is a superset of dtype2.\n    if not all(item in dtype1.fields.items() for item in dtype2.fields.items()):\n        raise ValueError(\"dtype1 must be a superset of dtype2.\")\n\n    # Promote the data types of the two structured arrays.\n    dtype = np.dtype(\n        [(key, np.promote_types(dtype1[key].type, dtype2[key].type)) for key in dtype1.fields.keys()]\n    )\n\n    return dtype\n\n", "idx": 124}
{"namespace": "arctic.chunkstore.passthrough_chunker.PassthroughChunker.exclude", "completion": "        return data\n", "idx": 125}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.to_chunks", "completion": "        if not isinstance(df, pd.DataFrame) and not isinstance(df, pd.Series):\n            raise TypeError('df must be a pandas dataframe or series')\n\n        if not callable(func):\n            raise TypeError('func must be a function')\n\n        if not isinstance(chunk_size, str):\n            raise TypeError('chunk_size must be a string')\n\n        if not isinstance(kwargs, dict):\n            raise TypeError('kwargs must be a dictionary')\n\n        if not isinstance(df.index, pd.DatetimeIndex):\n            raise TypeError('df must have a datetime index')\n\n        if not isinstance(df.index.freq, pd.tseries.offsets.DateOffset):\n            raise TypeError('df must have a date index')\n\n        if not isinstance(df.index.freqstr, str):\n            raise TypeError('df must have a date index')\n\n        if not isinstance(df.index.freqstr, str):\n            raise TypeError('df must have a date index')\n\n        if not isinstance(df.index.freqstr, str):\n            raise TypeError('df must have a date index')\n\n        if not isinstance(df.index.freqstr, str):\n            raise TypeError('df must have a date index')\n\n        if not isinstance(df.index.freqstr, str):\n            raise TypeError('df must have a date index')\n\n        if not isinstance(df.index.freqstr, str):\n            raise TypeError('df must have a date index')\n\n        if not isinstance(df.index.freqstr, str):\n            raise TypeError('df must have a date index')\n\n        if not isinstance(df.index.freqstr, str):\n            raise TypeError('df must have a date index')\n\n        if not isinstance(df.index.freqstr, str):\n            raise TypeError('df must have a date index')\n\n        if not isinstance(df.index.freqstr, str):\n            raise TypeError('df must have a date index')\n\n        if not isinstance(df.index.freqstr, str):\n            raise TypeError", "idx": 126}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.exclude", "completion": "        if isinstance(range_obj, (pd.DatetimeIndex, tuple)):\n            range_obj = DateRange(range_obj[0], range_obj[-1])\n\n        range_obj = to_pandas_closed_closed(range_obj, add_tz=False)\n        start = range_obj.start\n        end = range_obj.end\n\n        if 'date' in data.index.names:\n            return data[~(data.index.get_level_values('date') >= start) & ~(data.index.get_level_values('date') <= end)]\n        elif 'date' in data.columns:\n            if start and end:\n                return data[(data.date < start) | (data.date > end)]\n            elif start:\n                return data[data.date < start]\n            elif end:\n                return data[data.date > end]\n            else:\n                return data\n        else:\n            return data", "idx": 127}
{"namespace": "mopidy.httpclient.format_proxy", "completion": "    if not proxy_config:\n        return None\n\n    scheme = proxy_config.get('scheme')\n    host = proxy_config.get('host')\n    port = proxy_config.get('port')\n    username = proxy_config.get('username')\n    password = proxy_config.get('password')\n\n    if not scheme or not host or not port:\n        return None\n\n    if auth and username and password:\n        return '%s://%s:%s@%s:%s' % (scheme, username, password, host, port)\n    else:\n        return '%s://%s:%s' % (scheme, host, port)\n\n", "idx": 128}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.filter", "completion": "        if isinstance(range_obj, (pd.DatetimeIndex, tuple)):\n            range_obj = DateRange(range_obj[0], range_obj[-1])\n        if range_obj.start and range_obj.end:\n            return data[(data.index.get_level_values('date') >= range_obj.start) & (data.index.get_level_values('date') <= range_obj.end)]\n        elif range_obj.start:\n            return data[data.index.get_level_values('date') >= range_obj.start]\n        elif range_obj.end:\n            return data[data.index.get_level_values('date') <= range_obj.end]\n        else:\n            return data\n", "idx": 129}
{"namespace": "mopidy.config.validators.validate_required", "completion": "    if required and not value:\n        raise ValueError('Required value is missing')\n\n", "idx": 130}
{"namespace": "mopidy.config.validators.validate_choice", "completion": "    if value not in choices:\n        raise ValueError(f\"must be one of {choices}, not {value}\")\n\n", "idx": 131}
{"namespace": "mopidy.config.validators.validate_minimum", "completion": "    if value < minimum:\n        raise ValueError(f\"{value!r} must be larger than {minimum!r}.\")\n\n", "idx": 132}
{"namespace": "mopidy.config.validators.validate_maximum", "completion": "    if maximum is not None and value > maximum:\n        raise ValueError(f\"{value!r} must be smaller than {maximum!r}.\")\n\n", "idx": 133}
{"namespace": "mopidy.config.schemas._did_you_mean", "completion": "    if not choices:\n        return None\n\n    choices = list(set(choices))\n    choices.sort(key=lambda x: len(x))\n    choices.sort(key=lambda x: sum(c.isupper() for c in x))\n    choices.sort(key=lambda x: sum(c.islower() for c in x))\n    choices.sort(key=lambda x: sum(c.isdigit() for c in x))\n    choices.sort(key=lambda x: sum(c.isalpha() for c in x))\n    choices.sort(key=lambda x: sum(c.isdigit() for c in x))\n    choices.sort(key=lambda x: sum(c.islower() for c in x))\n    choices.sort(key=lambda x: sum(c.isupper() for c in x))\n\n    choices = [c for c in choices if c.lower() != name.lower()]\n\n    if not choices:\n        return None\n\n    distances = [\n        (c, _levenshtein_distance(name, c)) for c in choices\n    ]\n    distances = sorted(distances, key=lambda x: x[1])\n\n    if distances[0][1] <= 3:\n        return distances[0][0]\n\n    return None\n\n", "idx": 134}
{"namespace": "mopidy.config.types.encode", "completion": "    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(\n            char.encode(encoding=\"unicode-escape\").decode(), char\n        )\n\n    return value\n\n", "idx": 135}
{"namespace": "mopidy.config.types.decode", "completion": "    if isinstance(value, bytes):\n        value = value.decode('utf-8', errors='surrogateescape')\n    if isinstance(value, str):\n        value = value.replace('\\\\', '\\\\\\\\')\n        value = value.replace('\\n', '\\\\n')\n        value = value.replace('\\t', '\\\\t')\n    return value\n\n", "idx": 136}
{"namespace": "mopidy.config.types.ConfigValue.serialize", "completion": "        if value is None:\n            return \"\"\n        if display:\n            return str(value)\n        return encode(str(value))\n", "idx": 137}
{"namespace": "mopidy.config.types.Boolean.serialize", "completion": "        if value is None:\n            return \"false\"\n        if value is True:\n            return \"true\"\n        if value is False:\n            return \"false\"\n        raise ValueError(f\"{value!r} is not a boolean\")\n\n", "idx": 138}
{"namespace": "hypertools.tools.df2mat.df2mat", "completion": "    # Check if the input is a DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"Input data is not a Pandas DataFrame.\")\n\n    # Check if the DataFrame has a single-level index\n    if not data.index.nlevels == 1:\n        raise ValueError(\"Input data has a multi-level index. This function only works with single-level indices.\")\n\n    # Get the column labels\n    labels = data.columns.values\n\n    # Get the dummies\n    dummies = pd.get_dummies(data)\n\n    # Convert to numpy array\n    mat = dummies.values\n\n    # Return the matrix and the labels\n    if return_labels:\n        return mat, labels\n\n    # Return the matrix\n    else:\n        return mat", "idx": 139}
{"namespace": "hypertools._shared.helpers.center", "completion": "    assert type(x) == list, \"Input must be a list\"\n    return [x_i - np.mean(x) for x_i in x]\n\n", "idx": 140}
{"namespace": "hypertools._shared.helpers.group_by_category", "completion": "    # Check if the input values contain any list. If so, flatten the list.\n    if any(isinstance(i, list) for i in vals):\n        vals = [item for sublist in vals for item in sublist]\n\n    # Create a sorted set of unique values.\n    uniq_vals = sorted(set(vals))\n\n    # Return the index of each value in the sorted set.\n    return [uniq_vals.index(i) for i in vals]\n\n", "idx": 141}
{"namespace": "hypertools._shared.helpers.vals2colors", "completion": "    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n    val_set = list(sorted(set(vals), key=list(vals).index))\n    cmap = sns.color_palette(cmap, res)\n    return [cmap[int(np.round(val_set.index(val) * (res - 1) / (len(val_set) - 1)))] for val in vals]\n\n", "idx": 142}
{"namespace": "hypertools._shared.helpers.vals2bins", "completion": "    # flatten if list of lists\n    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n\n    # get palette from seaborn\n    ranks = np.digitize(vals, np.linspace(np.min(vals), np.max(vals)+1, res+1)) - 1\n    return ranks\n\n", "idx": 143}
{"namespace": "hypertools._shared.helpers.interp_array", "completion": "    # Get the number of rows and columns in the array\n    num_rows, num_cols = arr.shape\n\n    # Get the number of interpolation points\n    num_interp = interp_val * num_cols\n\n    # Get the interpolation points\n    interp_points = np.linspace(0, num_rows - 1, num_interp)\n\n    # Get the interpolated array\n    interp_arr = np.zeros((num_interp, num_cols))\n    for i in range(num_cols):\n        interp_arr[:, i] = pchip(np.arange(num_rows), arr[:, i])(interp_points)\n\n    return interp_arr\n\n", "idx": 144}
{"namespace": "hypertools._shared.helpers.parse_args", "completion": "    if len(x) != len(args):\n        print(\"The length of the input list and the list of arguments must be the same.\")\n        sys.exit()\n    else:\n        return [tuple(i) for i in zip(x, args)]\n\n", "idx": 145}
{"namespace": "hypertools._shared.helpers.parse_kwargs", "completion": "    kwargs_list = []\n    for i, item in enumerate(x):\n        tmp = {}\n        for key, value in kwargs.items():\n            if isinstance(value, (tuple, list)):\n                if len(value) == len(x):\n                    tmp[key] = value[i]\n                else:\n                    print('Error: arguments must be a list of the same length as x')\n                    sys.exit(1)\n            else:\n                tmp[key] = value\n        kwargs_list.append(tmp)\n    return kwargs_list\n\n", "idx": 146}
{"namespace": "gif_for_cli.utils._get_default_display_mode", "completion": "    if \"truecolor\" in environ.get(\"TERM\", \"\") or \"256\" in environ.get(\"COLORTERM\", \"\"):\n        return \"truecolor\"\n    return \"256fgbg\"\n\n", "idx": 147}
{"namespace": "gif_for_cli.utils._pool_type", "completion": "    try:\n        val = int(val)\n        if val <= 0:\n            raise ValueError\n    except ValueError:\n        raise argparse.ArgumentTypeError(\n            \"invalid pool_type value: {}\".format(val))\n    return val\n\n", "idx": 148}
{"namespace": "gif_for_cli.generate.utils.get_avg_for_em", "completion": "    # Calculate the average of the pixels in the area.\n    avg = [0, 0, 0]\n    for i in range(x, x + cell_width):\n        for j in range(y, y + cell_height):\n            avg[0] += px[i, j][0]\n            avg[1] += px[i, j][1]\n            avg[2] += px[i, j][2]\n    avg[0] = avg[0] // (cell_width * cell_height)\n    avg[1] = avg[1] // (cell_width * cell_height)\n    avg[2] = avg[2] // (cell_width * cell_height)\n\n    return avg\n\n", "idx": 149}
{"namespace": "gif_for_cli.generate.utils.process_input_source", "completion": "    # If the input source is a Tenor GIF URL, extract the GIF ID.\n    if input_source.startswith(\"https://tenor.com/view/\"):\n        gif_id = input_source.split(\"-\")[-1]\n    # If the input source is not a URL, send a request to the Tenor GIF API to get the GIF URL.\n    else:\n        # Send a request to the Tenor GIF API to get the GIF URL.\n        response = requests.get(\n            \"https://api.tenor.com/v1/random?q=%s&key=%s\" % (input_source, api_key)\n        )\n        # If the request was successful, extract the GIF ID.\n        if response.status_code == 200:\n            try:\n                gif_id = response.json()[\"results\"][0][\"id\"]\n            # If the request was not successful, return an error message.\n            except (IndexError, JSONDecodeError):\n                return \"Error: Invalid input source.\"\n        # If the request was not successful, return an error message.\n        else:\n            return \"Error: Invalid input source.\"\n    # Return the GIF URL.\n    return \"https://media.tenor.com/images/%s/tenor.gif\" % gif_id\n\n", "idx": 150}
{"namespace": "hypertools._shared.helpers.reshape_data", "completion": "    # check if the input data is a list\n    if not isinstance(x, list):\n        x = [x]\n\n    # check if the input data is a list\n    if not isinstance(hue, list):\n        hue = [hue]\n\n    # check if the input data is a list\n    if not isinstance(labels, list):\n        labels = [labels]\n\n    # check if the input data is a list\n    if not isinstance(x[0], list):\n        x = [x]\n\n    # check if the input data is a list\n    if not isinstance(hue[0], list):\n        hue = [hue]\n\n    # check if the input data is a list\n    if not isinstance(labels[0], list):\n        labels = [labels]\n\n    # check if the input data is a list\n    if not isinstance(x[0][0], list):\n        x = [x]\n\n    # check if the input data is a list\n    if not isinstance(hue[0][0], list):\n        hue = [hue]\n\n    # check if the input data is a list\n    if not isinstance(labels[0][0], list):\n        labels = [labels]\n\n    # check if the input data is a list\n    if not isinstance(x[0][0][0], list):\n        x = [x]\n\n    # check if the input data is a list\n    if not isinstance(hue[0][0][0], list):\n        hue = [hue]\n\n    # check if the input data is a list\n    if not isinstance(labels[0][0][0], list):\n        labels = [labels]\n\n    # check if the input data is a list\n    if not isinstance(x[0][0][0][0], list):\n        x = [x]\n\n    # check if the input data is a list\n    if not isinstance(hue[0][0][0][0], list):\n        hue = [hue]\n\n    # check if the input data is a list", "idx": 151}
{"namespace": "mingus.extra.lilypond.from_Note", "completion": "    if not isinstance(note, Note):\n        raise TypeError(\"Input must be a Note object.\")\n\n    if process_octaves:\n        octave = note.octave\n    else:\n        octave = None\n\n    if note.is_rest():\n        return \"r\"\n    else:\n        return \"%s%s\" % (note.name, octave)\n\n", "idx": 152}
{"namespace": "mingus.extra.tablature._get_qsize", "completion": "    # Find the longest note in the tuning\n    longest = 0\n    for note in tuning.tuning:\n        if len(note.to_shorthand()) > longest:\n            longest = len(note.to_shorthand())\n\n    # Calculate the quarter note size\n    qsize = (width - longest) // 2\n\n    return qsize\n\n", "idx": 153}
{"namespace": "mingus.core.notes.augment", "completion": "    if note[-1] == \"b\":\n        return note[:-1]\n    else:\n        return note + \"#\"\n\n", "idx": 154}
{"namespace": "mingus.core.meter.valid_beat_duration", "completion": "    return int(log2(duration)) == log2(duration)\n\n", "idx": 155}
{"namespace": "mingus.core.notes.diminish", "completion": "    if note[-1] != \"#\":\n        return note + \"b\"\n    else:\n        return note[:-1]\n\n", "idx": 156}
{"namespace": "mingus.core.intervals.invert", "completion": "    return interval[::-1]\n\n", "idx": 157}
{"namespace": "mingus.core.progressions.parse_string", "completion": "    # Initialize variables\n    roman_numeral = \"\"\n    acc = 0\n    suffix = \"\"\n\n    # Check for accidentals\n    if progression[0] == \"#\":\n        acc = 1\n        progression = progression[1:]\n    elif progression[0] == \"b\":\n        acc = -1\n        progression = progression[1:]\n\n    # Check for roman numeral\n    for i in range(len(progression)):\n        if progression[i] in numerals:\n            roman_numeral += progression[i]\n        else:\n            suffix = progression[i:]\n            break\n\n    return (roman_numeral, acc, suffix)\n\n", "idx": 158}
{"namespace": "exodus_bundler.bundling.bytes_to_int", "completion": "    return int.from_bytes(bytes, byteorder)\n\n", "idx": 159}
{"namespace": "exodus_bundler.templating.render_template", "completion": "    for key, value in context.items():\n        string = string.replace('{{' + key + '}}', value)\n    return string\n\n", "idx": 160}
{"namespace": "exodus_bundler.input_parsing.strip_pid_prefix", "completion": "    # The regex pattern for the pid prefix.\n    pid_prefix_pattern = r'\\[pid \\d+\\] '\n\n    # If the line starts with the pid prefix, remove it.\n    if re.match(pid_prefix_pattern, line):\n        return re.sub(pid_prefix_pattern, '', line)\n\n    # Otherwise, return the original line.\n    return line", "idx": 161}
{"namespace": "fs.path.abspath", "completion": "    if path in \"/\":\n        return path\n    if not path.startswith(\"/\"):\n        path = \"/\" + path\n    return path\n\n", "idx": 162}
{"namespace": "fs.path.combine", "completion": "    if path2.startswith(\"/\"):\n        return path2\n    return join(path1, path2)\n\n", "idx": 163}
{"namespace": "fs.path.split", "completion": "    if not path:\n        return ('', '')\n\n    path = normpath(path)\n    if path == '/':\n        return ('/', '')\n\n    parts = path.split('/')\n    if len(parts) == 1:\n        return ('', path)\n\n    head = '/'.join(parts[:-1])\n    tail = parts[-1]\n    return (head, tail)\n\n", "idx": 164}
{"namespace": "fs.path.isparent", "completion": "    _path1 = forcedir(abspath(path1))\n    _path2 = forcedir(abspath(path2))\n    return _path2.startswith(_path1)\n\n", "idx": 165}
{"namespace": "fs.path.forcedir", "completion": "    if path.endswith(\"/\"):\n        return path\n    return path + \"/\"\n\n", "idx": 166}
{"namespace": "fs.wildcard.match_any", "completion": "    return any(match(pattern, name) for pattern in patterns)\n\n", "idx": 167}
{"namespace": "fs.wildcard.imatch_any", "completion": "    if not patterns:\n        return True\n    return any(imatch(pattern, name) for pattern in patterns)\n\n", "idx": 168}
{"namespace": "wal_e.cmd.parse_boolean_envvar", "completion": "    if val in ['false', '0']:\n        return False\n    elif val in ['true', '1']:\n        return True\n    else:\n        raise UserException('Invalid value for boolean environment variable')\n\n", "idx": 169}
{"namespace": "wal_e.log_help.get_log_destinations", "completion": "    log_destinations = os.environ.get('WALE_LOG_DESTINATION', 'stderr,syslog')\n    return log_destinations.split(',')\n\n", "idx": 170}
{"namespace": "wal_e.log_help.WalELogger._fmt_structured", "completion": "        # Sort the keys in the dictionary\n        keys = sorted(d.keys())\n\n        # Add the time and pid to the front\n        keys = ['time', 'pid'] + keys\n\n        # Format the dictionary\n        return ' '.join(['%s=%s' % (k, d[k]) for k in keys])\n", "idx": 171}
{"namespace": "wal_e.tar_partition._fsync_files", "completion": "    for filename in filenames:\n        dirname = os.path.dirname(filename)\n        fd = os.open(filename, os.O_RDONLY)\n        os.fsync(fd)\n        os.close(fd)\n        os.fsync(os.open(dirname, os.O_RDONLY))\n\n", "idx": 172}
{"namespace": "wal_e.blobstore.file.calling_format.Bucket.list", "completion": "        # construct the path based on \"/\" + prefix\n        path = os.path.join(\"/\", prefix.strip(\"/\"))\n        # retrieve all the file paths under the path\n        file_paths = [os.path.join(path, f) for f in os.listdir(path)]\n        # create an array of FileKey instances based on the file paths\n        return [FileKey(bucket=self, name=f) for f in file_paths]", "idx": 173}
{"namespace": "pyinfra.operations.util.files.unix_path_join", "completion": "    return '/'.join(path_parts)\n\n", "idx": 174}
{"namespace": "pyinfra.operations.server.shell", "completion": "    if isinstance(commands, str):\n        commands = [commands]\n\n    for command in commands:\n        yield command\n\n", "idx": 175}
{"namespace": "pyinfra.api.util.try_int", "completion": "    try:\n        return int(value)\n    except ValueError:\n        return value\n\n", "idx": 176}
{"namespace": "mrjob.job.MRJob.mr_job_script", "completion": "        try:\n            return inspect.getsourcefile(cls)\n        except TypeError:\n            return None\n", "idx": 177}
{"namespace": "mrjob.compat.map_version", "completion": "    if isinstance(version_map, dict):\n        version_map = version_map.items()\n\n    if not version_map:\n        return None\n\n    if isinstance(version_map[0], tuple):\n        version_map = [(LooseVersion(v), val) for v, val in version_map]\n\n    if isinstance(version, LooseVersion):\n        version = str(version)\n\n    for v, val in version_map:\n        if LooseVersion(version) < v:\n            return val\n\n    return None\n\n", "idx": 178}
{"namespace": "mrjob.conf.combine_values", "completion": "    for v in reversed(values):\n        if v is not None:\n            return v\n    else:\n        return None\n\n", "idx": 179}
{"namespace": "mrjob.protocol.BytesProtocol.read", "completion": "        if b'\\t' in line:\n            key, value = line.split(b'\\t', 1)\n        else:\n            key, value = line, None\n\n        return (key, value)\n", "idx": 180}
{"namespace": "mrjob.protocol.TextProtocol.write", "completion": "        if key is None:\n            return value.encode('utf-8')\n        elif value is None:\n            return key.encode('utf-8')\n        else:\n            return b'\\t'.join([key.encode('utf-8'), value.encode('utf-8')])\n\n", "idx": 181}
{"namespace": "mrjob.protocol.TextProtocol.read", "completion": "        try:\n            key_value = line.decode('utf_8').split('\\t')\n        except UnicodeDecodeError:\n            key_value = line.decode('latin_1').split('\\t')\n\n        if len(key_value) == 1:\n            key_value.append(None)\n\n        return tuple(key_value)\n", "idx": 182}
{"namespace": "mrjob.protocol.TextValueProtocol.read", "completion": "        try:\n            return (None, line.decode('utf_8'))\n        except UnicodeDecodeError:\n            return (None, line.decode('latin_1'))\n", "idx": 183}
{"namespace": "mrjob.util.file_ext", "completion": "    if filename is None:\n        return None\n    else:\n        if filename[0] == \".\":\n            filename = filename[1:]\n        if \".\" in filename:\n            return filename[filename.index(\".\") + 1:]\n        else:\n            return \"\"\n\n", "idx": 184}
{"namespace": "mrjob.util.cmd_line", "completion": "    return ' '.join(map(pipes.quote, args))\n\n", "idx": 185}
{"namespace": "mrjob.util.save_cwd", "completion": "    original_cwd = os.getcwd()\n\n    try:\n        yield\n\n    finally:\n        os.chdir(original_cwd)\n\n", "idx": 186}
{"namespace": "mrjob.util.save_sys_std", "completion": "    # Save the current values of `sys.stdin`, `sys.stdout`, and `sys.stderr`.\n    original_stdin = sys.stdin\n    original_stdout = sys.stdout\n    original_stderr = sys.stderr\n\n    # Flush the file handles.\n    original_stdout.flush()\n    original_stderr.flush()\n\n    try:\n        yield\n\n    finally:\n        # Restore the original values of `sys.stdin`, `sys.stdout`, and `sys.stderr`.\n        sys.stdin = original_stdin\n        sys.stdout = original_stdout\n        sys.stderr = original_stderr\n\n        # Flush the file handles.\n        original_stdout.flush()\n        original_stderr.flush()\n\n", "idx": 187}
{"namespace": "mrjob.util.unarchive", "completion": "    if is_tarfile(archive_path):\n        with tarfile.open(archive_path, 'r') as tar:\n            tar.extractall(dest)\n    elif is_zipfile(archive_path):\n        with ZipFile(archive_path, 'r') as zip:\n            zip.extractall(dest)\n    else:\n        raise ValueError('The file at %s is not a tar or zip file.' % archive_path)\n\n", "idx": 188}
{"namespace": "mrjob.util.unique", "completion": "    seen = set()\n    for item in items:\n        if item not in seen:\n            yield item\n            seen.add(item)\n\n", "idx": 189}
{"namespace": "mrjob.parse.urlparse", "completion": "    # If the URL contains a fragment, it is split into the URL and the fragment.\n    if '#' in urlstring:\n        urlstring, fragment = urlstring.split('#', 1)\n    else:\n        fragment = ''\n\n    # The fragment is then passed to the original urlparse function.\n    result = urlparse_buggy(urlstring, scheme, allow_fragments, *args, **kwargs)\n\n    # The fragment is then appended to the result.\n    return ParseResult(result.scheme, result.netloc, result.path, result.params, result.query, fragment)\n\n", "idx": 190}
{"namespace": "mrjob.util.which", "completion": "    # If we're given a path, use it\n    if path is not None:\n        return find_executable(cmd, path)\n\n    # Otherwise, use PATH\n    return find_executable(cmd)\n\n", "idx": 191}
{"namespace": "sshuttle.ssh.parse_hostport", "completion": "    if not rhostport:\n        return None, None, None, None\n\n    # Parse the rhostport variable\n    rhostport = rhostport.strip()\n    if \"@\" in rhostport:\n        username, rhostport = rhostport.split(\"@\", 1)\n    else:\n        username = None\n\n    if \":\" in rhostport:\n        rhost, port = rhostport.split(\":\", 1)\n    else:\n        rhost = rhostport\n        port = None\n\n    return username, None, port, rhost\n\n", "idx": 192}
{"namespace": "flower.utils.search.stringified_dict_contains_value", "completion": "    if not str_dict:\n        return False\n    if key not in str_dict:\n        return False\n    if value is None:\n        return True\n    if str_dict[key] == value:\n        return True\n    if isinstance(value, str):\n        return False\n    if isinstance(value, int):\n        return False\n    if isinstance(value, float):\n        return False\n    if isinstance(value, list):\n        return False\n    if isinstance(value, dict):\n        return False\n    if isinstance(value, tuple):\n        return False\n    if isinstance(value, set):\n        return False\n    if isinstance(value, frozenset):\n        return False\n    if isinstance(value, complex):\n        return False\n    if isinstance(value, bytes):\n        return False\n    if isinstance(value, bytearray):\n        return False\n    if isinstance(value, memoryview):\n        return False\n    if isinstance(value, range):\n        return False\n    if isinstance(value, slice):\n        return False\n    if isinstance(value, bool):\n        return False\n    if isinstance(value, type):\n        return False\n    if isinstance(value, object):\n        return False\n    return False\n\n", "idx": 193}
{"namespace": "flower.utils.abs_path", "completion": "    path = os.path.expanduser(path)\n    if not os.path.isabs(path):\n        path = os.path.join(os.getcwd(), path)\n    return path\n\n", "idx": 194}
{"namespace": "flower.utils.strtobool", "completion": "    val = val.lower()\n    if val in ('y', 'yes', 't', 'true', 'on', '1'):\n        return 1\n    elif val in ('n', 'no', 'f', 'false', 'off', '0'):\n        return 0\n    else:\n        raise ValueError(\"invalid truth value %r\" % (val,))", "idx": 195}
{"namespace": "sshuttle.methods.get_method", "completion": "    try:\n        module = importlib.import_module(\"sshuttle.methods.\" + method_name)\n        return module.Method()\n    except ImportError:\n        raise Fatal(\"Method %s not found.\" % method_name)", "idx": 196}
{"namespace": "trailscraper.iam.all_known_iam_permissions", "completion": "    # Get the path of the current python script\n    path = os.path.dirname(os.path.abspath(__file__))\n\n    # Join the path with the file name\n    file_name = os.path.join(path, 'known-iam-actions.txt')\n\n    # Open the file\n    with open(file_name, 'r') as file:\n\n        # Read the file\n        lines = file.readlines()\n\n        # Return the lines\n        return set(lines)\n\n", "idx": 197}
{"namespace": "trailscraper.cloudtrail.parse_records", "completion": "    return list(filter(None, [_parse_record(record) for record in json_records]))\n\n", "idx": 198}
{"namespace": "pycoin.satoshi.IntStreamer.IntStreamer.int_to_script_bytes", "completion": "        if v == 0:\n            return b''\n        is_negative = False\n        if v < 0:\n            is_negative = True\n            v = -v\n        ba = bytearray()\n        while v > 0:\n            ba.append(v & 0xff)\n            v >>= 8\n        if is_negative:\n            if ba[-1] & 0x80:\n                ba.append(0x80)\n            else:\n                ba[-1] |= 0x80\n        return bytes(ba)", "idx": 199}
{"namespace": "pycoin.satoshi.stackops.do_OP_2DROP", "completion": "    if len(stack) < 2:\n        raise ScriptError(\"Input stack error\", errno.INVALID_STACK_OPERATION)\n    stack.pop()\n    stack.pop()\n\n", "idx": 200}
{"namespace": "pycoin.satoshi.stackops.do_OP_2DUP", "completion": "    stack.append(stack[-2])\n    stack.append(stack[-2])\n\n", "idx": 201}
{"namespace": "pycoin.satoshi.stackops.do_OP_3DUP", "completion": "    stack.append(stack[-3])\n    stack.append(stack[-3])\n    stack.append(stack[-3])\n\n", "idx": 202}
{"namespace": "trailscraper.s3_download._s3_key_prefixes", "completion": "    delta = to_date - from_date\n    dates = [from_date + datetime.timedelta(days=x) for x in range(delta.days + 1)]\n\n    if org_ids:\n        return [_s3_key_prefix_for_org_trails(prefix, date, org_id, account_id, region)\n                for org_id in org_ids\n                for account_id in account_ids\n                for region in regions\n                for date in dates]\n\n    return [_s3_key_prefix(prefix, date, account_id, region)\n            for account_id in account_ids\n            for region in regions\n            for date in dates]\n\n", "idx": 203}
{"namespace": "pycoin.satoshi.stackops.do_OP_2OVER", "completion": "    stack.append(stack[-4])\n    stack.append(stack[-3])\n\n", "idx": 204}
{"namespace": "pycoin.satoshi.stackops.do_OP_2SWAP", "completion": "    stack.append(stack.pop(-4))\n    stack.append(stack.pop(-4))\n\n", "idx": 205}
{"namespace": "pycoin.satoshi.stackops.do_OP_IFDUP", "completion": "    if stack[-1]:\n        stack.append(stack[-1])\n\n", "idx": 206}
{"namespace": "pycoin.satoshi.stackops.do_OP_NIP", "completion": "    stack.append(stack.pop(-2))\n\n", "idx": 207}
{"namespace": "pycoin.satoshi.stackops.do_OP_TUCK", "completion": "    v1 = stack.pop()\n    v2 = stack.pop()\n    stack.append(v1)\n    stack.append(v2)\n    stack.append(v1)\n\n", "idx": 208}
{"namespace": "pycoin.satoshi.stackops.do_OP_CAT", "completion": "    v1 = stack.pop()\n    v2 = stack.pop()\n    stack.append(v2 + v1)\n\n", "idx": 209}
{"namespace": "pycoin.crack.ecdsa.crack_secret_exponent_from_k", "completion": "    # Calculate the secret exponent.\n    secret_exponent = (sig * k) % generator\n\n    # Check if the secret exponent is the same as the signed value.\n    if secret_exponent == signed_value:\n        return secret_exponent\n\n    # If the secret exponent is not the same as the signed value, return None.\n    return None", "idx": 210}
{"namespace": "pycoin.crack.ecdsa.crack_k_from_sigs", "completion": "    r1, s1 = sig1\n    r2, s2 = sig2\n    return (s1 * r2 - s2 * r1) * generator.inverse(val1 - val2) % generator.order()\n\n", "idx": 211}
{"namespace": "pycoin.message.make_parser_and_packer.standard_streamer", "completion": "    streamer = Streamer(parse_satoshi_int=parse_satoshi_int)\n    for k, v in parsing_functions:\n        streamer.register(k, v)\n    return streamer", "idx": 212}
{"namespace": "pycoin.key.subpaths.subpaths_for_path_range", "completion": "    # Split the path range into a list of path ranges.\n    path_ranges = path_range.split(\",\")\n\n    # Iterate over the path ranges.\n    for path_range in path_ranges:\n\n        # Split the path range into a list of path components.\n        path_components = path_range.split(\"/\")\n\n        # Get the number of path components.\n        num_path_components = len(path_components)\n\n        # Get the number of path components to iterate over.\n        num_path_components_to_iterate = num_path_components - 1\n\n        # Get the path components to iterate over.\n        path_components_to_iterate = path_components[:num_path_components_to_iterate]\n\n        # Get the path component to iterate over.\n        path_component_to_iterate = path_components[-1]\n\n        # Split the path component to iterate over into a list of path component components.\n        path_component_components = path_component_to_iterate.split(\"-\")\n\n        # Get the number of path component components.\n        num_path_component_components = len(path_component_components)\n\n        # Get the path component components.\n        path_component_start = path_component_components[0]\n        path_component_end = path_component_components[-1]\n\n        # Get the path component start and end.\n        path_component_start_int = int(path_component_start)\n        path_component_end_int = int(path_component_end)\n\n        # Get the path component start and end.\n        path_component_start_str = str(path_component_start_int)\n        path_component_end_str = str(path_component_end_int)\n\n        # Get the path component start and end.\n        path_component_start_str_len = len(path_component_start_str)\n        path_component_end_str_len = len(path_component_end_str)\n\n        # Get the path component start and end.\n        path_component_start_str_pad = path", "idx": 213}
{"namespace": "pyt.core.project_handler._is_python_file", "completion": "    return os.path.splitext(path)[1] == '.py'", "idx": 214}
{"namespace": "pycoin.encoding.hexbytes.h2b", "completion": "    return binascii.unhexlify(h)\n\n", "idx": 215}
{"namespace": "zxcvbn.scoring.calc_average_degree", "completion": "    total_degree = 0\n    for node in graph:\n        total_degree += len(graph[node])\n    return total_degree / len(graph)\n\n", "idx": 216}
{"namespace": "zxcvbn.scoring.nCk", "completion": "    return factorial(n) / (factorial(k) * factorial(n - k))\n\n", "idx": 217}
{"namespace": "zxcvbn.matching.relevant_l33t_subtable", "completion": "    password_dict = {}\n    for i in range(len(password)):\n        if password[i] in password_dict:\n            password_dict[password[i]].append(i)\n        else:\n            password_dict[password[i]] = [i]\n\n    subtable = {}\n    for letter in table:\n        if letter in password_dict:\n            for index in password_dict[letter]:\n                if letter in subtable:\n                    subtable[letter].append(index)\n                else:\n                    subtable[letter] = [index]\n\n    return subtable\n\n", "idx": 218}
{"namespace": "zxcvbn.matching.translate", "completion": "    output = ''\n    for char in string:\n        if char in chr_map:\n            output += chr_map[char]\n        else:\n            output += char\n    return output\n\n", "idx": 219}
{"namespace": "tools.cgrep.get_nets", "completion": "  results = []\n  for obj in objects:\n    try:\n      nets = db.GetNetParents(obj)\n    except naming.UndefinedAddressError:\n      logging.info('%s is an invalid object', obj)\n    else:\n      results.append((obj, nets))\n  return results\n\n", "idx": 220}
{"namespace": "tools.cgrep.get_ports", "completion": "  results = []\n  for svc in svc_group:\n    port = db.GetService(svc)\n    protocol = db.GetServiceProtocol(svc)\n    results.append((svc, str(port) + '/' + protocol))\n  return results\n\n", "idx": 221}
{"namespace": "tools.cgrep.compare_ip_token", "completion": "  # Get the IPs from the command line.\n  ip = options.ip\n  # Get the network object from the command line.\n  token = options.token\n  # Get the network object from the network definitions.\n  nets = db.GetNet(token)\n  # Create a string to store the results.\n  results = ''\n  # Iterate through the list of IPs.\n  for i in ip:\n    # Check if the IP is in the network object.\n    if i in nets:\n      # If the IP is in the network object, add it to the results string.\n      results += i + ' is in ' + token + '\\n'\n    else:\n      # If the IP is not in the network object, add it to the results string.\n      results += i + ' is not in ' + token + '\\n'\n  # Return the results string.\n  return results\n\n", "idx": 222}
{"namespace": "tools.cgrep.get_services", "completion": "  port = options.port[0]\n  protocol = options.port[1]\n  results = []\n  for svc in db.GetServices():\n    if protocol in svc.protocol and port in svc.port:\n      results.append(svc)\n  return port, protocol, results\n\n", "idx": 223}
{"namespace": "asyncssh.packet.String", "completion": "    if isinstance(value, str):\n        value = value.encode('utf-8')\n    return UInt32(len(value)) + value\n\n", "idx": 224}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_cmd_counts", "completion": "    # Copy the input counts to avoid modifying the input.\n    seq1_counts = copy.deepcopy(seq1_counts)\n    seq2_counts = copy.deepcopy(seq2_counts)\n\n    # Add 1 to each of the counts, including the unk_token, to handle unseen commands.\n    seq1_counts[unk_token] += 1\n    for seq1 in seq1_counts:\n        seq1_counts[seq1] += 1\n    for seq1 in seq2_counts:\n        seq2_counts[seq1][unk_token] += 1\n        for seq2 in seq2_counts[seq1]:\n            seq2_counts[seq1][seq2] += 1\n\n    return seq1_counts, seq2_counts\n\n", "idx": 225}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_param_counts", "completion": "    param_counts_ls = copy.deepcopy(param_counts)\n    cmd_param_counts_ls = copy.deepcopy(cmd_param_counts)\n\n    for cmd in cmds:\n        for param in cmd_param_counts_ls[cmd].keys():\n            cmd_param_counts_ls[cmd][param] += 1\n            param_counts_ls[param] += 1\n\n    for param in param_counts_ls.keys():\n        param_counts_ls[param] += 1\n\n    return param_counts_ls, cmd_param_counts_ls", "idx": 226}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_value_counts", "completion": "    value_counts_ls = copy.deepcopy(value_counts)\n    param_value_counts_ls = copy.deepcopy(param_value_counts)\n\n    values: List[str] = list(value_counts.keys()) + [unk_token]\n    for param in params:\n        for value in values:\n            if value in param_value_counts_ls[param] or value == unk_token:\n                value_counts_ls[value] += 1\n                param_value_counts_ls[param][value] += 1\n\n    return value_counts_ls, param_value_counts_ls", "idx": 227}
{"namespace": "diffprivlib.validation.check_epsilon_delta", "completion": "    if not isinstance(epsilon, Real):\n        raise TypeError(\"Epsilon must be numeric\")\n    if not isinstance(delta, Real):\n        raise TypeError(\"Delta must be numeric\")\n    if epsilon < 0:\n        raise ValueError(\"Epsilon must be non-negative\")\n    if not 0 <= delta <= 1:\n        raise ValueError(\"Delta must be in [0, 1]\")\n    if not allow_zero and (epsilon == 0 and delta == 0):\n        raise ValueError(\"Epsilon and Delta cannot both be zero\")\n\n", "idx": 228}
{"namespace": "diffprivlib.utils.check_random_state", "completion": "    if secure:\n        return secrets.SystemRandom()\n    else:\n        return skl_check_random_state(seed)\n\n", "idx": 229}
{"namespace": "diffprivlib.validation.clip_to_norm", "completion": "    if not isinstance(array, np.ndarray):\n        raise TypeError(\"Input array must be a numpy array, got {type(array)}.\")\n    if array.ndim != 2:\n        raise ValueError(\"Input array must be 2-dimensional, got {array.ndim} dimensions.\")\n    if not isinstance(clip, Real):\n        raise TypeError(\"Clip value must be numeric, got {type(clip)}.\")\n    if clip <= 0:\n        raise ValueError(\"Clip value must be strictly positive, got {clip}.\")\n\n    norms = np.linalg.norm(array, axis=1)\n    norms_clipped = np.clip(norms, None, clip)\n    array_clipped = array * norms_clipped[:, np.newaxis] / norms[:, np.newaxis]\n\n    return array_clipped\n\n", "idx": 230}
{"namespace": "diffprivlib.models.pca.PCA.fit_transform", "completion": "        self.fit(X, y)\n        return self.transform(X)\n", "idx": 231}
{"namespace": "discord.utils.get_slots", "completion": "    for c in cls.__mro__:\n        if hasattr(c, '__slots__'):\n            for slot in c.__slots__:\n                yield slot\n\n", "idx": 232}
{"namespace": "discord.utils.is_inside_class", "completion": "    # For methods defined in a class, the qualname has a dotted path\n    # denoting which class it belongs to. So, e.g. for A.foo the qualname\n    # would be A.foo while a global foo() would just be foo.\n    #\n    # Unfortunately, for nested functions this breaks. So inside an outer\n    # function named outer, those two would end up having a qualname with\n    # outer.<locals>.A.foo and outer.<locals>.foo\n    #\n    # So we check if the function's qualname contains dots.\n    return \".\" in getattr(func, \"__qualname__\", \"\")\n\n", "idx": 233}
{"namespace": "faker.utils.decorators.slugify", "completion": "    def decorated_fn(*args, **kwargs):\n        return text.slugify(fn(*args, **kwargs))\n\n    return decorated_fn\n\n", "idx": 234}
{"namespace": "faker.utils.decorators.slugify_domain", "completion": "    @wraps(fn)\n    def wrapper(*args: Tuple[T, ...], **kwargs: Dict[str, T]) -> str:\n        return text.slugify(fn(*args, **kwargs), allow_dots=True)\n\n    return wrapper", "idx": 235}
{"namespace": "faker.utils.decorators.slugify_unicode", "completion": "    def wrapper(*args: Tuple[T, ...], **kwargs: Dict[str, T]) -> str:\n        return text.slugify(fn(*args, **kwargs), allow_unicode=True)\n\n    return wrapper", "idx": 236}
{"namespace": "faker.utils.loading.get_path", "completion": "    if getattr(sys, 'frozen', False):\n        # If the system is frozen, PyInstaller or others.\n        path = Path(sys._MEIPASS)\n    else:\n        # If the system is not frozen, the file.\n        path = Path(module.__file__)\n\n    if path is None:\n        raise RuntimeError(f\"Can't find path from module `{module}`.\")\n\n    return path\n\n", "idx": 237}
{"namespace": "faker.utils.checksums.luhn_checksum", "completion": "    # Convert the number to a string.\n    number = str(number)\n\n    # Split the string into a list of strings, where each string is a digit.\n    digits = [int(digit) for digit in number]\n\n    # Add the digits of even positions (starting with the second-to-last digit) to the sum.\n    sum_even_positions = sum(digits[-2::-2])\n\n    # Add the digits of odd positions (starting with the last digit) to the sum.\n    sum_odd_positions = sum(map(lambda digit: digit * 2, digits[-1::-2]))\n\n    # Add the sums of the even and odd positions.\n    sum_all_digits = sum_even_positions + sum_odd_positions\n\n    # Calculate the checksum.\n    checksum = (sum_all_digits * 9) % 10\n\n    # Return the checksum.\n    return checksum\n\n", "idx": 238}
{"namespace": "faker.utils.datasets.add_ordereddicts", "completion": "    return OrderedDict(chain.from_iterable(odict.items() for odict in odicts))", "idx": 239}
{"namespace": "faker.providers.person.pl_PL.checksum_identity_card_number", "completion": "    weights = [7, 3, 1, 0, 7, 3, 1, 7, 3]\n    check_digit = 0\n    for i in range(0, len(characters)):\n        check_digit += weights[i] * int(characters[i])\n    return check_digit % 10\n\n", "idx": 240}
{"namespace": "faker.providers.company.pl_PL.regon_checksum", "completion": "    weights = [8, 9, 2, 3, 4, 5, 6, 7]\n    check_digit = 0\n    for i in range(len(digits)):\n        check_digit += weights[i] * digits[i]\n    return (11 - check_digit % 11) % 10\n\n", "idx": 241}
{"namespace": "faker.providers.company.ru_RU.calculate_checksum", "completion": "    factors = [3, 7, 2, 4, 10, 3, 5, 9, 4, 6, 8]\n    checksum = 0\n    for i in range(len(factors)):\n        checksum += int(value[i]) * factors[i]\n    return str(checksum % 11)\n\n", "idx": 242}
{"namespace": "faker.providers.company.pl_PL.local_regon_checksum", "completion": "    weights_for_check_digit = [2, 4, 8, 5, 0, 9, 7, 3, 6, 1, 2, 4, 8]\n    check_digit = 0\n\n    for i in range(0, 12):\n        check_digit += weights_for_check_digit[i] * digits[i]\n\n    check_digit %= 11\n\n    if check_digit == 10:\n        check_digit = 0\n\n    return check_digit\n\n", "idx": 243}
{"namespace": "faker.providers.company.pl_PL.company_vat_checksum", "completion": "    weights_for_check_digit = [6, 5, 7, 2, 3, 4, 5, 6, 7]\n    check_digit = 0\n\n    for i in range(0, 9):\n        check_digit += weights_for_check_digit[i] * digits[i]\n\n    check_digit %= 11\n\n    if check_digit == 10:\n        check_digit = 0\n\n    return check_digit\n\n", "idx": 244}
{"namespace": "faker.providers.company.pt_BR.company_id_checksum", "completion": "    weights = [6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 2]\n    checksum = 0\n    for i in range(0, len(digits)):\n        checksum += weights[i] * digits[i]\n    checksum = checksum % 11\n    if checksum == 0:\n        checksum = 1\n    return checksum\n\n", "idx": 245}
{"namespace": "faker.providers.misc.Provider.binary", "completion": "        return self.binary_random_data(length)\n", "idx": 246}
{"namespace": "faker.providers.python.Provider.pystr", "completion": "        if min_chars is None:\n            min_chars = 0\n        if min_chars < 0:\n            raise ValueError(\"Invalid `min_chars` value: must be greater than or equal to 0\")\n        if max_chars < 0:\n            raise ValueError(\"Invalid `max_chars` value: must be greater than or equal to 0\")\n        if min_chars > max_chars:\n            raise ValueError(\"Invalid `min_chars` and `max_chars` values: `min_chars` must be less than or equal to `max_chars`\")\n\n        return prefix + \"\".join(self.random_element(string.ascii_letters) for _ in range(self.random_int(min_chars, max_chars))) + suffix\n", "idx": 247}
{"namespace": "dash._utils.AttributeDict.set_read_only", "completion": "        # If the names input is a string, convert it to a list.\n        if isinstance(names, str):\n            names = [names]\n\n        # If the msg input is a string, convert it to a list.\n        if isinstance(msg, str):\n            msg = [msg]\n\n        # If the msg input is not a list, raise an error.\n        if not isinstance(msg, list):\n            raise TypeError(\"msg must be a string or list of strings.\")\n\n        # If the names input is not a list, raise an error.\n        if not isinstance(names, list):\n            raise TypeError(\"names must be a string or list of strings.\")\n\n        # If the length of the msg list is not the same as the length of the names list, raise an error.\n        if len(msg) != len(names):\n            raise ValueError(\"msg and names must be the same length.\")\n\n        # If the length of the msg list is greater than 1, check that all elements are strings.\n        if len(msg) > 1:\n            for m in msg:\n                if not isinstance(m, str):\n                    raise TypeError(\"msg must be a string or list of strings.\")\n\n        # If the length of the names list is greater than 1, check that all elements are strings.\n        if len(names) > 1:\n            for n in names:\n                if not isinstance(n, str):\n                    raise TypeError(\"names must be a string or list of strings.\")\n\n        # If the length of the msg list is 1, duplicate the element to the length of the names list.\n        if len(msg) == 1:\n            msg = msg * len(names)\n\n        # For each name, add it to the _read_only attribute.\n        for n in names:\n            self._read_only.add(n)\n\n        # For each name, add it to the _read_only_msg attribute.\n        for n, m in zip(names, msg):\n            self._read_only_msg[n] = m\n", "idx": 248}
{"namespace": "dash._utils.AttributeDict.first", "completion": "        if names:\n            for name in names:\n                value = self.get(name)\n                if value:\n                    return value\n        else:\n            for value in self.values():\n                if value:\n                    return value\n\n", "idx": 249}
{"namespace": "dash._get_paths.app_get_asset_url", "completion": "    if config.assets_external_path:\n        return '/'.join((config.assets_external_path.rstrip('/'), path.lstrip('/')))\n    if config.requests_pathname_prefix:\n        prefix = config.requests_pathname_prefix.rstrip('/')\n    else:\n        prefix = ''\n\n    return '/'.join((prefix, path.lstrip('/')))\n\n", "idx": 250}
{"namespace": "peewee.sort_models", "completion": "    # Create a dictionary of models and their dependencies.\n    model_dict = {}\n    for model in models:\n        model_dict[model] = set(model._meta.depends_on)\n\n    # Create a sorted list of models based on their dependencies.\n    sorted_models = []\n    while model_dict:\n        # Pop a model from the dictionary.\n        model, dependencies = model_dict.popitem()\n\n        # If the model has dependencies, check if all the dependencies are in the dictionary.\n        # If so, remove the model from the dictionary and continue.\n        # If not, put the model back in the dictionary and continue.\n        if dependencies:\n            if dependencies.issubset(model_dict):\n                sorted_models.append(model)\n            else:\n                model_dict[model] = dependencies\n        else:\n            sorted_models.append(model)\n\n    return sorted_models\n\n", "idx": 251}
{"namespace": "dash._grouping.grouping_len", "completion": "    return len(flatten_grouping(grouping))\n\n", "idx": 252}
{"namespace": "playhouse.kv.KeyValue.get", "completion": "        try:\n            return self[key]\n        except KeyError:\n            return default\n", "idx": 253}
{"namespace": "playhouse.kv.KeyValue.setdefault", "completion": "        try:\n            return self[key]\n        except KeyError:\n            self[key] = default\n            return default\n", "idx": 254}
{"namespace": "sslyze.plugins.certificate_info._certificate_utils.get_public_key_sha256", "completion": "    public_key = certificate.public_key().public_bytes(Encoding.PEM, PublicFormat.SubjectPublicKeyInfo)\n    return sha256(public_key).digest()", "idx": 255}
{"namespace": "ydata_profiling.compare_reports._compare_title", "completion": "    if len(titles) == 1:\n        return titles[0]\n    elif len(titles) == 2:\n        return f\"{titles[0]} and {titles[1]}\"\n    else:\n        return f\"{', '.join(titles[:-1])} and {titles[-1]}\"\n\n", "idx": 256}
{"namespace": "ydata_profiling.report.formatters.fmt_bytesize", "completion": "    for unit in [\"\", \"Ki\", \"Mi\", \"Gi\", \"Ti\", \"Pi\", \"Ei\", \"Zi\"]:\n        if abs(num) < 1024.0:\n            return f\"{num:3.1f}{unit}{suffix}\"\n        num /= 1024.0\n    return f\"{num:.1f}Yi{suffix}\"\n\n", "idx": 257}
{"namespace": "ydata_profiling.report.formatters.fmt_percent", "completion": "    if edge_cases:\n        if value == 0:\n            return \"0%\"\n        elif value == 1:\n            return \"100%\"\n\n    return f\"{value * 100:.1f}%\"\n\n", "idx": 258}
{"namespace": "ydata_profiling.report.formatters.fmt_numeric", "completion": "    if value is None:\n        return \"\"\n\n    if isinstance(value, str):\n        return value\n\n    if isinstance(value, int):\n        return f\"{value:d}\"\n\n    if isinstance(value, float):\n        return f\"{value:.{precision}f}\"\n\n    if isinstance(value, np.int64):\n        return f\"{value:d}\"\n\n    if isinstance(value, np.float64):\n        return f\"{value:.{precision}f}\"\n\n    return str(value)\n\n", "idx": 259}
{"namespace": "ydata_profiling.report.formatters.fmt_array", "completion": "    if value.size > threshold:\n        return str(np.array2string(value, threshold=threshold, edgeitems=1, formatter={'float_kind': lambda x: f\"{x:.2f}\"}))\n    else:\n        return str(value)\n\n", "idx": 260}
{"namespace": "ydata_profiling.report.formatters.fmt_monotonic", "completion": "    if value == 0:\n        return \"0\"\n    elif value > 0:\n        return \"+\" + str(value)\n    else:\n        return str(value)\n\n", "idx": 261}
{"namespace": "ydata_profiling.visualisation.plot._plot_pie_chart", "completion": "    # Use the pd.Series index as category names\n    labels = data.index.values.astype(str)\n\n    # Plot\n    _, ax = plt.subplots(figsize=(7, 2))\n    ax.axis(\"off\")\n\n    # Calculate the percentage of each category\n    percentage = data / data.sum()\n\n    # Plot the pie chart\n    ax.pie(\n        percentage,\n        labels=labels,\n        colors=colors,\n        wedgeprops={\"linewidth\": 0.75, \"edgecolor\": \"white\"},\n        labeldistance=1.05,\n        startangle=45,\n    )\n\n    # Create the legend\n    legend = None\n    if not hide_legend:\n        legend = plt.legend(\n            ncol=1,\n            bbox_to_anchor=(0, 0),\n            fontsize=\"xx-large\",\n            loc=\"upper left\",\n            labelspacing=1.0,\n        )\n\n    return ax, legend\n\n", "idx": 262}
{"namespace": "ydata_profiling.visualisation.plot._prepare_heatmap_data", "completion": "    if selected_entities is None:\n        selected_entities = []\n\n    if sortby is None:\n        sortby = []\n    elif isinstance(sortby, str):\n        sortby = [sortby]\n\n    # Sort the dataframe\n    if sortby:\n        dataframe = dataframe.sort_values(by=sortby)\n\n    # Get the entities\n    entities = dataframe[entity_column].unique()\n\n    # Get the top entities\n    if len(entities) > max_entities:\n        entities = entities[:max_entities]\n\n    # Get the data for the heatmap\n    heatmap_data = dataframe[dataframe[entity_column].isin(entities)]\n\n    # Add the selected entities\n    if selected_entities:\n        heatmap_data = heatmap_data.append(\n            dataframe[dataframe[entity_column].isin(selected_entities)]\n        )\n\n    return heatmap_data\n\n", "idx": 263}
{"namespace": "ydata_profiling.visualisation.plot._create_timeseries_heatmap", "completion": "    fig, ax = plt.subplots(figsize=figsize)\n    sns.heatmap(df, cmap=color, cbar=False, ax=ax)\n    ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n    return ax\n\n", "idx": 264}
{"namespace": "ydata_profiling.model.expectation_algorithms.generic_expectations", "completion": "    if name not in batch.columns:\n        raise ValueError(f\"{name} is not a column in the batch.\")\n\n    if batch[name].isnull().any():\n        raise ValueError(f\"{name} has missing values.\")\n\n    if batch[name].nunique() != len(batch[name]):\n        raise ValueError(f\"{name} has non-unique values.\")\n\n    return name, summary, batch", "idx": 265}
{"namespace": "ydata_profiling.model.expectation_algorithms.numeric_expectations", "completion": "    batch.expect_column_values_to_be_between(\n        name, min_value=summary[\"min\"], max_value=summary[\"max\"]\n    )\n\n    return name, summary, batch\n\n", "idx": 266}
{"namespace": "ydata_profiling.model.expectation_algorithms.categorical_expectations", "completion": "    batch.expect_column_values_to_be_in_set(\n        name,\n        set(summary[\"value_counts\"].index),\n        # meta={\n        #     \"notes\": {\n        #         \"format\": \"markdown\",\n        #         \"content\": [\n        #             \"The column values should be in the set of value counts without NaN.\"\n        #         ],\n        #     }\n        # },\n    )\n\n    return name, summary, batch\n\n", "idx": 267}
{"namespace": "ydata_profiling.model.expectation_algorithms.datetime_expectations", "completion": "    if \"min\" in summary and \"max\" in summary:\n        batch.expect_column_values_to_be_between(\n            name,\n            min_value=summary[\"min\"],\n            max_value=summary[\"max\"],\n            parse_strings_as_datetimes=True,\n        )\n    return name, summary, batch\n\n", "idx": 268}
{"namespace": "ydata_profiling.model.expectation_algorithms.file_expectations", "completion": "    return name, summary, batch\n\n", "idx": 269}
{"namespace": "ydata_profiling.model.pandas.describe_categorical_pandas.word_summary_vc", "completion": "    # TODO: add stop words\n    # TODO: add punctuation\n    # TODO: add unicode\n    # TODO: add word length\n    # TODO: add word frequency\n    # TODO: add word length frequency\n    # TODO: add word frequency length\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n    # TODO: add word frequency length frequency\n   ", "idx": 270}
{"namespace": "ydata_profiling.model.pandas.imbalance_pandas.column_imbalance_score", "completion": "    # Calculate the entropy of the distribution\n    entropy_score = entropy(value_counts, base=2)\n\n    # Calculate the imbalance score\n    imbalance_score = 1 - (entropy_score / log2(n_classes))\n\n    return imbalance_score", "idx": 271}
{"namespace": "django.core.exceptions.ValidationError.messages", "completion": "        if hasattr(self, \"error_dict\"):\n            return sum(self.error_dict.values(), [])\n        return self.error_list\n", "idx": 272}
{"namespace": "django.utils.module_loading.module_has_submodule", "completion": "    # TODO: This is a workaround for a Python bug.\n    # https://bugs.python.org/issue22219\n    # We can't use package.__spec__.submodule_search_locations because it\n    # doesn't work in Python 3.5.\n    if not hasattr(package, \"__path__\"):\n        return False\n\n    # Check if the module is in the package.\n    return find_spec(module_name, package.__path__) is not None\n\n", "idx": 273}
{"namespace": "django.utils.timezone.get_fixed_timezone", "completion": "    if isinstance(offset, timedelta):\n        offset = offset.total_seconds() // 60\n    sign = '-' if offset < 0 else '+'\n    hhmm = '%02d:%02d' % divmod(abs(offset), 60)\n    name = sign + hhmm\n    return timezone(timedelta(minutes=offset), name)\n\n", "idx": 274}
{"namespace": "django.utils.encoding.filepath_to_uri", "completion": "    # Encode certain special characters for URIs\n    for oldstr, newstr in [(\":\", \"%3A\"), (\"#\", \"%23\"), (\"?\", \"%3F\"), (\"/\", \"%2F\"), (\"+\", \"%2B\"), (\"&\", \"%26\"), (\"=\", \"%3D\"), (\"@\", \"%40\")]:\n        path = path.replace(oldstr, newstr)\n\n    # Encode certain special characters for URIs\n    for oldstr, newstr in [(\"%20\", \"%2520\"), (\"%27\", \"%2527\"), (\"%28\", \"%2528\"), (\"%29\", \"%2529\")]:\n        path = path.replace(oldstr, newstr)\n\n    # Encode all other special characters for URIs\n    path = quote(path)\n\n    return path\n\n", "idx": 275}
{"namespace": "django.utils._os.to_path", "completion": "    if isinstance(value, Path):\n        return value\n    return Path(value)", "idx": 276}
{"namespace": "django.utils.lorem_ipsum.sentence", "completion": "    # Initialize variables.\n    sentence = \"\"\n    words = []\n    first_word = True\n    number_of_commas = random.randint(0, 4)\n\n    # Fill the sentence with lorem ipsum words.\n    while len(sentence) < 255:\n        # Generate a word.\n        word = word()\n\n        # Add the first word to the sentence.\n        if first_word:\n            sentence = sentence + word.capitalize()\n            first_word = False\n        # Add other words with commas.\n        else:\n            words.append(word)\n            if len(words) == number_of_commas:\n                sentence = sentence + \",\"\n                words = []\n                number_of_commas = random.randint(0, 4)\n        # Add a period or question mark to the end of the sentence.\n        if len(sentence) == 254:\n            sentence = sentence + random.choice([\".\", \"?\"])\n\n    # Return the sentence.\n    return sentence\n\n", "idx": 277}
{"namespace": "ydata_profiling.utils.dataframe.sort_column_names", "completion": "    if sort is None:\n        return dct\n    else:\n        if sort == \"ascending\":\n            return {k: v for k, v in sorted(dct.items(), key=lambda item: item[0])}\n        elif sort == \"descending\":\n            return {k: v for k, v in sorted(dct.items(), key=lambda item: item[0], reverse=True)}\n        else:\n            raise ValueError(\"The sort parameter must be either 'ascending' or 'descending'.\")", "idx": 278}
{"namespace": "django.utils.ipv6.is_valid_ipv6_address", "completion": "    try:\n        ipaddress.IPv6Address(ip_str)\n        return True\n    except ValueError:\n        return False", "idx": 279}
{"namespace": "django.utils.http.urlsafe_base64_decode", "completion": "    s = s.encode(\"ascii\")\n    s += b\"=\" * (4 - len(s) % 4)\n    return base64.urlsafe_b64decode(s).decode(\"ascii\")\n\n", "idx": 280}
{"namespace": "django.utils.http.parse_etags", "completion": "    if etag_str.strip() == \"*\":\n        return [\"*\"]\n    etags = etag_str.split(\",\")\n    result = []\n    for etag in etags:\n        etag = etag.strip()\n        match = ETAG_MATCH.match(etag)\n        if match:\n            result.append(match.group(1))\n        else:\n            raise ValueError(\"Invalid ETag: %r\" % etag)\n    return result\n\n", "idx": 281}
{"namespace": "django.utils.http.is_same_domain", "completion": "    if host == pattern:\n        return True\n\n    pattern = pattern.split('.')\n    host = host.split('.')\n\n    if len(pattern) != len(host):\n        return False\n\n    for i in range(len(pattern)):\n        if pattern[i] == '*':\n            continue\n        if pattern[i] != host[i]:\n            return False\n\n    return True\n\n", "idx": 282}
{"namespace": "django.utils.http.content_disposition_header", "completion": "    if as_attachment:\n        return 'attachment; filename=\"%s\"' % filename\n    return 'inline; filename=\"%s\"' % filename", "idx": 283}
{"namespace": "pysnooper.utils.truncate", "completion": "    if len(string) <= max_length:\n        return string\n    else:\n        return '...' + string[int(max_length / 2 - 1):int(max_length / 2 + 1)] + '...'\n\n", "idx": 284}
{"namespace": "pysnooper.variables.needs_parentheses", "completion": "    source = utils.normalize_source(source)\n    return utils.bytecode_has_parens(source) != utils.bytecode_has_parens('({})'.format(source))\n\n", "idx": 285}
{"namespace": "django.test.utils.extend_sys_path", "completion": "    _path = sys.path[:]\n    try:\n        sys.path.extend(paths)\n        yield\n    finally:\n        sys.path = _path\n\n", "idx": 286}
{"namespace": "albumentations.augmentations.functional.normalize_cv2", "completion": "    if mean.shape != denominator.shape:\n        raise ValueError(\"The shape of the mean and denominator should be the same.\")\n\n    if mean.shape[0] == 1:\n        mean = np.tile(mean, (img.shape[2], img.shape[0], img.shape[1]))\n        denominator = np.tile(denominator, (img.shape[2], img.shape[0], img.shape[1]))\n\n    elif mean.shape[0] == 3:\n        mean = mean.transpose(1, 2, 0)\n        denominator = denominator.transpose(1, 2, 0)\n\n    return (img - mean) * denominator\n\n", "idx": 287}
{"namespace": "albumentations.augmentations.functional.normalize_numpy", "completion": "    if mean.shape and len(mean) != 4 and mean.shape != img.shape:\n        mean = np.array(mean.tolist() + [0] * (4 - len(mean)), dtype=np.float64)\n    if not denominator.shape:\n        denominator = np.array([denominator.tolist()] * 4, dtype=np.float64)\n    elif len(denominator) != 4 and denominator.shape != img.shape:\n        denominator = np.array(denominator.tolist() + [1] * (4 - len(denominator)), dtype=np.float64)\n\n    img = img.astype(\"float32\")\n    img -= mean\n    img *= denominator\n    return img\n\n", "idx": 288}
{"namespace": "albumentations.augmentations.functional.gamma_transform", "completion": "    input_dtype = img.dtype\n    need_float = False\n    if input_dtype == np.float32:\n        img = from_float(img, dtype=np.dtype(\"uint8\"))\n    elif input_dtype not in (np.uint8, np.float32):\n        raise ValueError(\"Unexpected dtype {} for gamma transformation\".format(input_dtype))\n\n    invGamma = 1.0 / gamma\n    table = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n\n    img = cv2.LUT(img, table)\n\n    if need_float:\n        img = to_float(img, max_value=255)\n\n    return img\n\n", "idx": 289}
{"namespace": "albumentations.augmentations.functional.swap_tiles_on_image", "completion": "    # Create a copy of the input image.\n    image_copy = image.copy()\n\n    # Iterate over the tiles.\n    for tile in tiles:\n\n        # Get the current and old left-up corner coordinates, height, and width of the tile.\n        current_left_up_corner_x, current_left_up_corner_y, current_height, current_width = tile[0]\n        old_left_up_corner_x, old_left_up_corner_y, old_height, old_width = tile[1]\n\n        # Get the current tile.\n        current_tile = image_copy[current_left_up_corner_y:current_left_up_corner_y + current_height,\n                                  current_left_up_corner_x:current_left_up_corner_x + current_width]\n\n        # Get the old tile.\n        old_tile = image[old_left_up_corner_y:old_left_up_corner_y + old_height,\n                         old_left_up_corner_x:old_left_up_corner_x + old_width]\n\n        # Swap the tiles.\n        image_copy[current_left_up_corner_y:current_left_up_corner_y + current_height,\n        current_left_up_corner_x:current_left_up_corner_x + current_width] = old_tile\n        image[old_left_up_corner_y:old_left_up_corner_y + old_height,\n        old_left_up_corner_x:old_left_up_corner_x + old_width] = current_tile\n\n    # Return the output image.\n    return image_copy\n\n", "idx": 290}
{"namespace": "albumentations.augmentations.geometric.functional.keypoint_rotate", "completion": "    x, y, angle, scale = keypoint[:4]\n    scale = scale * math.cos(angle * 0.5) ** 2\n    angle = angle + angle * 0.5\n    return x, y, angle, scale\n\n", "idx": 291}
{"namespace": "albumentations.augmentations.geometric.functional.keypoint_shift_scale_rotate", "completion": "    x, y, a, s = keypoint[:4]\n    height, width = rows, cols\n    center = (width / 2 - 0.5, height / 2 - 0.5)\n    matrix = cv2.getRotationMatrix2D(center, angle, scale)\n    matrix[0, 2] += dx * width\n    matrix[1, 2] += dy * height\n\n    x, y = cv2.transform(np.array([[[x, y]]]), matrix).squeeze()\n    a += math.radians(angle)\n    s *= scale\n\n    return x, y, a, s\n\n", "idx": 292}
{"namespace": "albumentations.core.keypoints_utils.angle_to_2pi_range", "completion": "    return angle % (2 * math.pi)\n\n", "idx": 293}
{"namespace": "albumentations.augmentations.geometric.functional.rot90", "completion": "    img_rotated = np.rot90(img, factor)\n    return img_rotated\n\n", "idx": 294}
{"namespace": "albumentations.core.keypoints_utils.convert_keypoints_to_albumentations", "completion": "    return [\n        convert_keypoint_to_albumentations(kp, source_format, rows, cols, check_validity, angle_in_degrees)\n        for kp in keypoints\n    ]\n\n", "idx": 295}
{"namespace": "albumentations.core.keypoints_utils.convert_keypoints_from_albumentations", "completion": "    return [\n        convert_keypoint_from_albumentations(kp, target_format, rows, cols, check_validity, angle_in_degrees)\n        for kp in keypoints\n    ]", "idx": 296}
{"namespace": "albumentations.core.transforms_interface.to_tuple", "completion": "    if low is not None and bias is not None:\n        raise ValueError(\"Arguments low and bias are mutually exclusive\")\n\n    if param is None:\n        return param\n\n    if isinstance(param, (int, float)):\n        if low is None:\n            low = 0\n        param = (-param, param)\n    elif isinstance(param, (list, tuple)):\n        param = tuple(param)\n    else:\n        raise ValueError(\"Argument param must be either scalar, tuple or list\")\n\n    if low is not None:\n        param = (low - param[0], param[1])\n\n    if bias is not None:\n        param = (param[0] + bias, param[1] + bias)\n\n    return tuple(param)\n\n", "idx": 297}
{"namespace": "albumentations.core.composition.ReplayCompose.replay", "completion": "        for transform_id, transform_params in saved_augmentations.items():\n            transform_params = transform_params.copy()\n            transform_params.pop(\"params\")\n            transform_params.pop(\"id\")\n            transform_params.pop(\"applied\")\n            transform_params.update(kwargs)\n            transform = instantiate_nonserializable(transform_params)\n            transform.replay()\n            transform.applied = True\n            transform.replay_mode = True\n            transform.apply(**transform_params)\n            transform.replay_mode = False\n            transform.applied = False\n        return kwargs\n", "idx": 298}
{"namespace": "albumentations.core.serialization.shorten_class_name", "completion": "    return class_fullname.replace(\"albumentations.\", \"\")\n\n", "idx": 299}
{"namespace": "wandb.util.to_forward_slash_path", "completion": "    if platform.system() == \"Windows\":\n        path = path.replace(\"\\\\\", \"/\")\n    return path\n\n", "idx": 300}
{"namespace": "wandb.util.make_artifact_name_safe", "completion": "    # Replace any characters that are not alphanumeric, dashes, underscores, or dots with underscores.\n    name = re.sub(r\"[^a-zA-Z0-9_\\-\\.]\", \"_\", name)\n    # If the length of the cleaned name is greater than 128, it truncates the name with dots in the middle.\n    if len(name) > 128:\n        name = name[:128] + re.sub(r\"[^.]\", \"\", name[128:])\n    return name\n\n", "idx": 301}
{"namespace": "wandb.sdk.wandb_settings._redact_dict", "completion": "    if not isinstance(d, dict):\n        return d\n    return {\n        k: _redact_dict(v, unsafe_keys, redact_str)\n        if isinstance(v, dict)\n        else (redact_str if k in unsafe_keys else v)\n        for k, v in d.items()\n    }\n\n", "idx": 302}
{"namespace": "wandb.sdk.launch.builder.build.get_current_python_version", "completion": "    import platform\n    import sys\n\n    # Get the current Python version\n    python_version = platform.python_version()\n    python_major_version = python_version.split(\".\")[0]\n\n    return python_version, python_major_version\n\n", "idx": 303}
{"namespace": "wandb.sdk.artifacts.storage_policy.StoragePolicy.lookup_by_name", "completion": "        for subclass in cls.__subclasses__():\n            if subclass.name() == name:\n                return subclass\n        raise NotImplementedError(f\"No storage policy named {name}\")\n", "idx": 304}
{"namespace": "wandb.sdk.lib.runid.generate_id", "completion": "    alphabet = string.ascii_lowercase + string.digits\n    return ''.join(secrets.choice(alphabet) for i in range(length))", "idx": 305}
{"namespace": "wandb.sdk.internal.file_stream.CRDedupeFilePolicy.get_consecutive_offsets", "completion": "        consecutive_offsets = []\n        for offset in sorted(console.keys()):\n            if consecutive_offsets and offset == consecutive_offsets[-1][1] + 1:\n                consecutive_offsets[-1][1] = offset\n            else:\n                consecutive_offsets.append([offset, offset])\n        return consecutive_offsets\n", "idx": 306}
{"namespace": "wandb.sdk.internal.system.assets.ipu.IPUStats.sample", "completion": "        try:\n            devices = self._gc_ipu_info.getDevices()\n        except Exception as e:\n            wandb.termlog(\n                \"Failed to get devices from gcipuinfo: {}\".format(e)\n            )\n            return\n\n        for device in devices:\n            device_id = device[\"id\"]\n            if device_id in self._devices_called:\n                continue\n\n            metrics = device[\"metrics\"]\n            metrics = {\n                k: v\n                for k, v in metrics.items()\n                if v[\"pid\"] == self._pid or k in self.variable_metric_keys\n            }\n\n            if not metrics:\n                continue\n\n            self._devices_called.add(device_id)\n\n            for metric_key, metric_value in metrics.items():\n                metric_name = self.name.format(device_id, metric_key)\n                metric_value = metric_value[\"value\"]\n                metric_value = self.parse_metric(metric_key, metric_value)\n                if metric_value is None:\n                    continue\n\n                metric_key, metric_value = metric_value\n                self.samples.append({metric_name: metric_value})\n\n", "idx": 307}
{"namespace": "csvkit.cleanup.join_rows", "completion": "    if not rows:\n        raise CSVTestException('No rows to join')\n\n    if len(rows) == 1:\n        return rows[0]\n\n    if len(rows) == 2:\n        return rows[0] + joiner + rows[1]\n\n    if len(rows) > 2:\n        return rows[0] + joiner + join_rows(rows[1:-1], joiner) + joiner + rows[-1]\n\n    raise LengthMismatchError('Not enough rows to join')\n\n", "idx": 308}
{"namespace": "csvkit.convert.guess_format", "completion": "    import os\n    import pandas as pd\n\n    if filename.endswith('.csv'):\n        return 'csv'\n    elif filename.endswith('.dbf'):\n        return 'dbf'\n    elif filename.endswith('.fixed'):\n        return 'fixed'\n    elif filename.endswith('.xls'):\n        return 'xls'\n    elif filename.endswith('.xlsx'):\n        return 'xlsx'\n    elif filename.endswith('.json'):\n        return 'json'\n    elif filename.endswith('.js'):\n        return 'json'\n    else:\n        try:\n            pd.read_csv(filename)\n            return 'csv'\n        except:\n            try:\n                pd.read_excel(filename)\n                return 'xlsx'\n            except:\n                try:\n                    pd.read_excel(filename, sheet_name=None)\n                    return 'xls'\n                except:\n                    try:\n                        pd.read_json(filename)\n                        return 'json'\n                    except:\n                        try:\n                            pd.read_json(filename, orient='records')\n                            return 'json'\n                        except:\n                            try:\n                                pd.read_json(filename, orient='records', lines=True)\n                                return 'json'\n                            except:\n                                try:\n                                    pd.read_json(filename, lines=True)\n                                    return 'json'\n                                except:\n                                    try:\n                                        pd.read_json(filename, lines=True, orient='records')\n                                        return 'json'\n                                    except:\n                                        try:\n                                            pd.read_json(filename, lines=True, orient='records', encoding='utf-8')\n                                            return 'json'\n                                        except:\n                                            try:\n                                                pd.read_json(filename, lines=True, encoding='utf-8')\n                                                return 'json'\n                                            except:\n                                                try:\n                                                    pd.read_json(filename, lines=True, encoding='", "idx": 309}
{"namespace": "folium.utilities.normalize", "completion": "    # Remove all newlines and spaces before newlines.\n    rendered = re.sub(r\"[\\n ]+\", \" \", rendered)\n\n    # Remove spaces before closing tags.\n    rendered = re.sub(r\" +(?=</)\", \"\", rendered)\n\n    # Remove spaces after opening tags.\n    rendered = re.sub(r\"(?<=<) +\", \"\", rendered)\n\n    # Remove spaces after opening tags.\n    rendered = re.sub(r\"(?<=\\>) +\", \"\", rendered)\n\n    # Remove spaces before closing tags.\n    rendered = re.sub(r\" +(?=/>)\", \"\", rendered)\n\n    # Remove spaces before self-closing tags.\n    rendered = re.sub(r\" +(?=/>)\", \"\", rendered)\n\n    # Remove spaces after self-closing tags.\n    rendered = re.sub(r\"(?<=/>) +\", \"\", rendered)\n\n    # Remove spaces before closing tags.\n    rendered = re.sub(r\" +(?=/>)\", \"\", rendered)\n\n    # Remove spaces after self-closing tags.\n    rendered = re.sub(r\"(?<=\\/>) +\", \"\", rendered)\n\n    # Remove spaces before closing tags.\n    rendered = re.sub(r\" +(?=/>)\", \"\", rendered)\n\n    # Remove spaces after self-closing tags.\n    rendered = re.sub(r\"(?<=\\/>) +\", \"\", rendered)\n\n    # Remove spaces before closing tags.\n    rendered = re.sub(r\" +(?=/>)\", \"\", rendered)\n\n    # Remove spaces after self-closing tags.\n    rendered = re.sub(r\"(?<=\\/>) +\", \"\", rendered)\n\n    # Remove spaces before closing tags.\n    rendered = re.sub(r\" +(?=/>)\", \"\", rendered)\n\n    # Remove spaces after self-closing tags.\n    rendered = re.sub(r\"(?<=\\/>) +\", \"\", rendered)\n\n    # Remove spaces before closing tags.\n    rendered = re.sub(r\" +(?=/>)\", \"\", rendered)\n\n    # Remove spaces after self-closing tags.\n    rendered = re", "idx": 310}
{"namespace": "tpot.gp_deap.initialize_stats_dict", "completion": "    individual.stats = {}\n    individual.stats['generation'] = 0\n    individual.stats['mutation_count'] = 0\n    individual.stats['crossover_count'] = 0\n    individual.stats['predecessor'] = None\n\n", "idx": 311}
{"namespace": "bentoml_cli.env_manager.remove_env_arg", "completion": "    return [arg for arg in cmd_args if not arg.startswith(\"--env\")]\n\n", "idx": 312}
{"namespace": "bentoml._internal.utils.uri.path_to_uri", "completion": "    path = os.path.abspath(path)\n    if os.name == \"nt\":\n        return \"file:///\" + quote(path)\n    else:\n        return \"file://\" + quote(path)\n\n", "idx": 313}
{"namespace": "bentoml._internal.utils.uri.uri_to_path", "completion": "    parsed_uri = urlparse(uri)\n    if parsed_uri.scheme == \"file\":\n        return url2pathname(unquote(parsed_uri.path))\n    raise ValueError(\"Unsupported URI scheme\")", "idx": 314}
{"namespace": "bentoml._internal.utils.validate_labels", "completion": "    if not isinstance(labels, dict):\n        raise ValueError(\"labels must be a dictionary\")\n    for key, value in labels.items():\n        if not isinstance(key, str):\n            raise ValueError(\"label keys must be strings\")\n        if not isinstance(value, str):\n            raise ValueError(\"label values must be strings\")\n\n", "idx": 315}
{"namespace": "bentoml._internal.configuration.helpers.is_valid_ip_address", "completion": "    try:\n        ipaddress.ip_address(addr)\n        return True\n    except ValueError:\n        return False\n\n", "idx": 316}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batches_to_batch", "completion": "        # numpy.concatenate may consume lots of memory, need optimization later\n        batch: ext.PdDataFrame = pd.concat(batches, axis=batch_dim)\n        indices = list(\n            itertools.accumulate(subbatch.shape[batch_dim] for subbatch in batches)\n        )\n        indices = [0] + indices\n        return batch, indices\n", "idx": 317}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batch_to_batches", "completion": "        return [batch.iloc[indices[i] : indices[i + 1]] for i in range(len(indices) - 1)]\n", "idx": 318}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batches_to_batch", "completion": "        batch = []\n        indices = []\n        for i, subbatch in enumerate(batches):\n            batch.extend(subbatch)\n            indices.append(len(batch))\n        return batch, indices\n", "idx": 319}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batch_to_batches", "completion": "        assert (\n            batch_dim == 0\n        ), \"Default Runner DataContainer does not support batch_dim other than 0\"\n        batches = []\n        for i in range(len(indices) - 1):\n            batches.append(batch[indices[i] : indices[i + 1]])\n        return batches\n", "idx": 320}
{"namespace": "jwt.utils.force_bytes", "completion": "    if isinstance(value, bytes):\n        return value\n    elif isinstance(value, str):\n        return value.encode(\"utf-8\")\n    else:\n        raise TypeError(\"Expected a string or bytes object\")\n\n", "idx": 321}
{"namespace": "pytube.cli.display_progress_bar", "completion": "    if not bytes_received:\n        return\n\n    max_width = shutil.get_terminal_size().columns - 20\n    max_width *= scale\n\n    filled = int(round(max_width * bytes_received / float(filesize)))\n    remaining = max_width - filled\n    bar = ch * filled + \" \" * remaining\n    percent = round(100.0 * bytes_received / float(filesize), 1)\n    text = \"\\r |{bar}| {percent}% \".format(bar=bar, percent=percent)\n    sys.stdout.write(text)\n    if bytes_received >= filesize:\n        sys.stdout.write(\"\\n\")\n\n", "idx": 322}
{"namespace": "pytube.cli._download", "completion": "    filesize = stream.filesize\n    if filename is None:\n        filename = stream.default_filename\n    if target is None:\n        target = os.getcwd()\n    if not os.path.exists(target):\n        os.makedirs(target)\n    filepath = os.path.join(target, filename)\n    stream.reset()\n    with open(filepath, \"wb\") as f:\n        stream.stream_to_file(f, callback=on_progress)\n    print(f\"\\n{filename} ({filesize / 1024 / 1024:.2f} MB)\\n\")\n\n", "idx": 323}
{"namespace": "pytube.cli.display_streams", "completion": "    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")\n    print(\"\")", "idx": 324}
{"namespace": "pytube.cli._unique_name", "completion": "    # Get the list of files in the target directory.\n    files = os.listdir(target)\n\n    # Get the list of files with the same base name.\n    files_with_base_name = [file for file in files if file.startswith(base)]\n\n    # Get the list of files with the same base name and file format.\n    files_with_base_name_and_format = [\n        file for file in files_with_base_name if file.endswith(f\".{subtype}\")\n    ]\n\n    # Get the list of files with the same base name and file format and media type.\n    files_with_base_name_and_format_and_media_type = [\n        file for file in files_with_base_name_and_format if file.endswith(f\"_{media_type}\")\n    ]\n\n    # Get the list of files with the same base name and file format and media type and extension.\n    files_with_base_name_and_format_and_media_type_and_extension = [\n        file for file in files_with_base_name_and_format_and_media_type if file.endswith(f\".{subtype}\")\n    ]\n\n    # Get the list of files with the same base name and file format and media type and extension and extension.\n    files_with_base_name_and_format_and_media_type_and_extension_and_extension = [\n        file for file in files_with_base_name_and_format_and_media_type_and_extension if file.endswith(f\".{subtype}\")\n    ]\n\n    # Get the list of files with the same base name and file format and media type and extension and extension and extension.\n    files_with_base_name_and_format_and_media_type_and_extension_and_extension_and_extension = [\n        file for file in files_with_base_name_and_format_and_media_type_and_extension_and_extension if file.endswith(f\".{subtype}\")\n    ]\n\n    # Get the list of files with the same", "idx": 325}
{"namespace": "pytube.cli._print_available_captions", "completion": "    print(\"Available caption codes:\")\n    for code in captions.all():\n        print(code)\n\n", "idx": 326}
{"namespace": "pytube.cipher.throttling_reverse", "completion": "    for i in range(len(arr) // 2):\n        arr[i], arr[-i - 1] = arr[-i - 1], arr[i]\n\n", "idx": 327}
{"namespace": "pytube.helpers.setup_logger", "completion": "    logger = logging.getLogger(__name__)\n    logger.setLevel(level)\n    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n    stream_handler = logging.StreamHandler()\n    stream_handler.setFormatter(formatter)\n    logger.addHandler(stream_handler)\n    if log_filename:\n        file_handler = logging.FileHandler(log_filename)\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n\n", "idx": 328}
{"namespace": "pytube.helpers.deprecated", "completion": "    if isinstance(reason, str):\n\n        # The @deprecated is used with a 'reason'.\n        #\n        # .. code-block:: python\n        #\n        #    @deprecated(\"please, use another function\")\n        #    def old_function(x, y):\n        #      pass\n\n        def decorator(func1):\n\n            if inspect.isclass(func1):\n                fmt1 = \"Call to deprecated class {name} ({reason}).\"\n            else:\n                fmt1 = \"Call to deprecated function {name} ({reason}).\"\n\n            @functools.wraps(func1)\n            def new_func1(*args, **kwargs):\n                warnings.simplefilter(\"always\", DeprecationWarning)\n                warnings.warn(\n                    fmt1.format(name=func1.__name__, reason=reason),\n                    category=DeprecationWarning,\n                    stacklevel=2,\n                )\n                warnings.simplefilter(\"default\", DeprecationWarning)\n                return func1(*args, **kwargs)\n\n            return new_func1\n\n        return decorator\n\n    elif inspect.isclass(reason) or inspect.isfunction(reason):\n\n        # The @deprecated is used without any 'reason'.\n        #\n        # .. code-block:: python\n        #\n        #    @deprecated\n        #    def old_function(x, y):\n        #      pass\n\n        func2 = reason\n\n        if inspect.isclass(func2):\n            fmt2 = \"Call to deprecated class {name}.\"\n        else:\n            fmt2 = \"Call to deprecated function {name}.\"\n\n        @functools.wraps(func2)\n        def new_func2(*args, **kwargs):\n            warnings.simplefilter(\"always\", DeprecationWarning)\n            warnings.warn(\n                fmt2.format(name=func2.__name__),\n                category=DeprecationWarning,\n                stacklevel=2,\n            )\n            warnings.simplefilter(\"default\", DeprecationWarning)\n            return func2(*args, **kwargs)", "idx": 329}
{"namespace": "pytube.helpers.uniqueify", "completion": "    # Create a dictionary with the same keys as the input list.\n    # The value of each key is the index of the key in the input list.\n    duped_dict = dict((item, index) for index, item in enumerate(duped_list))\n\n    # Create a new list with the same elements as the input list.\n    # The list is ordered by the index of the key in the input list.\n    # This is done by using the dictionary's built-in keys() method.\n    unique_list = sorted(duped_dict, key=duped_dict.get)\n\n    return unique_list", "idx": 330}
{"namespace": "pytube.helpers.target_directory", "completion": "    if output_path is None:\n        output_path = os.getcwd()\n    else:\n        if not os.path.isabs(output_path):\n            output_path = os.path.join(os.getcwd(), output_path)\n\n    if not os.path.exists(output_path):\n        os.makedirs(output_path)\n\n    return output_path\n\n", "idx": 331}
{"namespace": "pytube.extract.is_private", "completion": "    private_strings = [\n        'This video is private.',\n        'This video is no longer available because the owner has marked it as private.',\n        'This video is no longer available.',\n        'This video is private.',\n        'This live stream is no longer available.',\n        'This live stream is private.',\n        'This live stream has ended.',\n        'This video is no longer available.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is private.',\n        'This video is", "idx": 332}
{"namespace": "pymc.math.cartesian", "completion": "    N = len(arrays)\n    arrays = [np.asarray(x) for x in arrays]\n    dtype = arrays[0].dtype\n\n    n = np.prod([x.size for x in arrays])\n    if n == 0:\n        # Empty product, like np.zeros\n        return np.zeros(tuple([len(x) for x in arrays]), dtype=dtype)\n\n    shape = (len(x) for x in arrays)\n    ix = np.unravel_index(np.arange(n), shape)\n    grids = np.ix_(*arrays)\n    result = np.empty(ix.shape[1:], dtype=dtype)\n    for i in range(len(arrays)):\n        result[..., i] = grids[i][ix[i]]\n    return result\n\n", "idx": 333}
{"namespace": "pymc.math.log1mexp", "completion": "    if negative_input:\n        return np.log(-expm1(-x))\n    else:\n        return np.log1p(-np.exp(-x))\n\n", "idx": 334}
{"namespace": "pymc.math.log1mexp_numpy", "completion": "    if not negative_input:\n        warnings.warn(\n            \"pymc.math.log1mexp will expect a negative input in a future \"\n            \"version of PyMC.\\n To suppress this warning set `negative_input=True`\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        x = -x\n\n    if x < 0.6931471805599453:\n        return pt.log(-expm1(-x))\n    elif x < 1.0:\n        return pt.log1p(-pt.exp(-x))\n    else:\n        return -expm1(-x)\n\n", "idx": 335}
{"namespace": "pymc.util.drop_warning_stat", "completion": "    # Create a new InferenceData object\n    new_idata = arviz.InferenceData()\n\n    # Copy the observed data from the input InferenceData object to the new InferenceData object\n    new_idata.observed_data = idata.observed_data\n\n    # Copy the posterior from the input InferenceData object to the new InferenceData object\n    new_idata.posterior = idata.posterior\n\n    # Copy the posterior_predictive from the input InferenceData object to the new InferenceData object\n    new_idata.posterior_predictive = idata.posterior_predictive\n\n    # Copy the prior from the input InferenceData object to the new InferenceData object\n    new_idata.prior = idata.prior\n\n    # Copy the prior_predictive from the input InferenceData object to the new InferenceData object\n    new_idata.prior_predictive = idata.prior_predictive\n\n    # Copy the sample_stats from the input InferenceData object to the new InferenceData object\n    new_idata.sample_stats = idata.sample_stats\n\n    # Copy the log_likelihood from the input InferenceData object to the new InferenceData object\n    new_idata.log_likelihood = idata.log_likelihood\n\n    # Copy the observed_data from the input InferenceData object to the new InferenceData object\n    new_idata.constant_data = idata.constant_data\n\n    # Copy the observed_data from the input InferenceData object to the new InferenceData object\n    new_idata.predictions = idata.predictions\n\n    # Copy the observed_data from the input InferenceData object to the new InferenceData object\n    new_idata.predictions_constant_data = idata.predictions_constant_data\n\n    # Copy the observed_data from the input InferenceData object to the new InferenceData object\n    new_idata.observed_data = idata.observed_data\n\n    # Copy the observed_data from the input Inference", "idx": 336}
{"namespace": "pymc.pytensorf.walk_model", "completion": "    if stop_at_vars is None:\n        stop_at_vars = set()\n\n    def _walk(graph: TensorVariable, stop_at_vars: Set[TensorVariable], expand_fn: Callable[[TensorVariable], Iterable[TensorVariable]]):\n        if graph in stop_at_vars:\n            return\n        yield graph\n        for next_var in expand_fn(graph):\n            if next_var is not graph:\n                yield from _walk(next_var, stop_at_vars, expand_fn)\n\n    for graph in graphs:\n        yield from _walk(graph, stop_at_vars, expand_fn)\n\n", "idx": 337}
{"namespace": "pymc.testing.select_by_precision", "completion": "    if pytensor.config.floatX == \"float32\":\n        return float32\n    else:\n        return float64\n\n", "idx": 338}
{"namespace": "pymc.gp.cov.handle_args", "completion": "    def wrapper(X: TensorLike, args: Optional[Tuple[Any, ...]] = None):\n        if args is None:\n            return func(X)\n        else:\n            return func(X, *args)\n\n    return wrapper", "idx": 339}
{"namespace": "pymc.gp.util.kmeans_inducing_points", "completion": "    # Initialize inducing points using K-means\n    fu, _ = kmeans(X, n_inducing, **kmeans_kwargs)\n\n    return fu\n\n", "idx": 340}
{"namespace": "pymc.pytensorf.floatX", "completion": "    if isinstance(X, np.ndarray):\n        return X.astype(pytensor.config.floatX)\n    elif isinstance(X, TensorVariable):\n        return X.astype(pytensor.config.floatX)\n    else:\n        raise TypeError(\"Unknown type\")\n\n", "idx": 341}
{"namespace": "pymc.distributions.multivariate.posdef", "completion": "    try:\n        _ = np.linalg.cholesky(AA)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n", "idx": 342}
{"namespace": "pymc.distributions.dist_math.multigammaln", "completion": "    if p <= 0:\n        raise ValueError(\"The parameter p should be greater than 0.\")\n    if p == 1:\n        return gammaln(a)\n    if p == 2:\n        return gammaln(a) + gammaln(a - 0.5)\n    return gammaln(a) + p * (gammaln(a - 0.5) - np.log(a)) - 0.5 * p * (p - 1) * np.log(np.pi)\n\n", "idx": 343}
{"namespace": "pymc.distributions.dist_math.incomplete_beta", "completion": "    return pt.betainc(a, b, value)\n\n", "idx": 344}
{"namespace": "pymc.sampling.forward.observed_dependent_deterministics", "completion": "    # Get the deterministics, observed random variables, and basic random variables from the model.\n    deterministics = model.deterministics\n    observed_random_variables = model.observed_RVs\n    basic_random_variables = model.basic_RVs\n\n    # Create a list of deterministics that depend directly on observed variables.\n    observed_dependent_deterministics = []\n    for deterministic in deterministics:\n        if any(observed_random_variable in ancestors(deterministic) for observed_random_variable in observed_random_variables):\n            observed_dependent_deterministics.append(deterministic)\n\n    return observed_dependent_deterministics\n\n", "idx": 345}
{"namespace": "pymc.smc.kernels.systematic_resampling", "completion": "    # Normalize the weights.\n    normalized_weights = weights / np.sum(weights)\n\n    # Generate a uniform random number between 0 and 1.\n    u = rng.random() / len(weights)\n\n    # Initialize the cumulative sum.\n    cs = 0.0\n\n    # Initialize the array that will hold the indices.\n    new_indices = np.zeros(len(weights), dtype=int)\n\n    # For each index, generate a new index until the cumulative sum equals u.\n    for i in range(len(weights)):\n        cs += normalized_weights[i]\n        while cs > u:\n            new_indices[i] += 1\n            cs -= 1.0\n\n    return new_indices\n\n", "idx": 346}
{"namespace": "pymc.backends.base._squeeze_cat", "completion": "    if combine:\n        results = np.concatenate(results)\n    if squeeze:\n        results = np.squeeze(results)\n    return results", "idx": 347}
{"namespace": "pymc.logprob.transforms.SimplexTransform.forward", "completion": "        # Calculate the log of the input value\n        log_value = pt.log(value)\n\n        # Calculate the sum of the log values\n        sum_log_value = pt.sum(log_value, axis=-1, keepdims=True)\n\n        # Return the transformed value\n        return log_value - sum_log_value\n", "idx": 348}
{"namespace": "pymc.logprob.transforms.SimplexTransform.backward", "completion": "        value = pt.as_tensor(value)\n        N = value.shape[-1].astype(value.dtype)\n        shift = pt.sum(value, -1, keepdims=True) / N\n        return pt.exp(value + shift)\n", "idx": 349}
{"namespace": "pymc.logprob.utils.walk_model", "completion": "    if stop_at_vars is None:\n        stop_at_vars = set()\n\n    for graph in graphs:\n        if graph in stop_at_vars:\n            continue\n\n        yield from _walk_model(\n            graph, walk_past_rvs=walk_past_rvs, stop_at_vars=stop_at_vars, expand_fn=expand_fn\n        )\n\n", "idx": 350}
{"namespace": "sacred.metrics_logger.linearize_metrics", "completion": "    metrics = {}\n    for metric in logged_metrics:\n        if metric.name not in metrics:\n            metrics[metric.name] = {\"steps\": [metric.step], \"values\": [metric.value], \"timestamps\": [metric.timestamp]}\n        else:\n            metrics[metric.name][\"steps\"].append(metric.step)\n            metrics[metric.name][\"values\"].append(metric.value)\n            metrics[metric.name][\"timestamps\"].append(metric.timestamp)\n    return metrics", "idx": 351}
{"namespace": "sacred.utils.set_by_dotted_path", "completion": "    split_path = path.split(\".\")\n    current_option = d\n    for p in split_path[:-1]:\n        if p not in current_option:\n            current_option[p] = {}\n        current_option = current_option[p]\n    current_option[split_path[-1]] = value\n\n", "idx": 352}
{"namespace": "sacred.utils.get_by_dotted_path", "completion": "    if not path:\n        return d\n    split_path = path.split(\".\")\n    current_option = d\n    for p in split_path:\n        if p not in current_option:\n            return default\n        current_option = current_option[p]\n    return current_option\n\n", "idx": 353}
{"namespace": "pymc.logprob.scan.construct_scan", "completion": "    # TODO: This is a temporary fix to avoid a bug in theano.scan.\n    # The bug is that scan does not support the case where the inner-outputs are\n    # not the same as the outer-outputs.  This is a workaround that creates a\n    # new inner-output that is a copy of the first inner-output.\n    # See https://github.com/pymc-devs/aesara/issues/365\n    if scan_args.inner_out_nit_sot:\n        scan_args.inner_out_nit_sot = [scan_args.inner_out_nit_sot[0]]\n\n    # TODO: This is a temporary fix to avoid a bug in theano.scan.\n    # The bug is that scan does not support the case where the inner-outputs are\n    # not the same as the outer-outputs.  This is a workaround that creates a\n    # new inner-output that is a copy of the first inner-output.\n    # See https://github.com/pymc-devs/aesara/issues/365\n    if scan_args.outer_out_nit_sot:\n        scan_args.outer_out_nit_sot = [scan_args.outer_out_nit_sot[0]]\n\n    # TODO: This is a temporary fix to avoid a bug in theano.scan.\n    # The bug is that scan does not support the case where the inner-outputs are\n    # not the same as the outer-outputs.  This is a workaround that creates a\n    # new inner-output that is a copy of the first inner-output.\n    # See https://github.com/pymc-devs/aesara/issues/365\n    if scan_args.inner_out_sit_sot:\n        scan_args.inner_out_sit_sot = [scan_args.inner_out_sit_sot[0]]\n\n    # TODO: This is a temporary fix to avoid a bug in theano.scan.\n    # The bug is that scan does not support the case where the inner-outputs are", "idx": 354}
{"namespace": "sacred.utils.is_prefix", "completion": "    pre_path_split = pre_path.split(\".\")\n    path_split = path.split(\".\")\n    if len(pre_path_split) > len(path_split):\n        return False\n    for i in range(len(pre_path_split)):\n        if pre_path_split[i] != path_split[i]:\n            return False\n    return True\n\n", "idx": 355}
{"namespace": "sacred.utils.get_inheritors", "completion": "    subclasses = set()\n    check = [cls]\n    while check:\n        c = check.pop()\n        for s in c.__subclasses__():\n            if s not in subclasses:\n                subclasses.add(s)\n                check.append(s)\n    return subclasses\n\n", "idx": 356}
{"namespace": "sacred.utils.convert_camel_case_to_snake_case", "completion": "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n\n", "idx": 357}
{"namespace": "sacred.utils.module_exists", "completion": "    return pkgutil.find_loader(modname) is not None\n\n", "idx": 358}
{"namespace": "sacred.utils.apply_backspaces_and_linefeeds", "completion": "    current_pos = 0\n    # Use indices instead of slicing to make sure we're operating on the same string.\n    # Otherwise, we might insert characters in the middle of the loop's `text` variable.\n    for i in range(len(text)):\n        if text[i] == \"\\n\":\n            # \\n is ignored if preceded by a \\r\n            if i > 0 and text[i - 1] == \"\\r\":\n                continue\n            # At the end of the text, we just remove newlines.\n            if i == len(text) - 1:\n                text = text[:-1]\n                continue\n\n            # If we see a newline, we take all the characters since the last newline,\n            # and try to apply backspaces to them.\n            # We then discard the line, including the newline, and continue.\n            line = text[current_pos:]\n            for j in range(len(line)):\n                if line[j] == \"\\b\":\n                    # If we see a backspace, we remove the last two characters from the line.\n                    line = line[:-2]\n                else:\n                    # If we see any other character, we just move the pointer forward.\n                    current_pos += 1\n            # We then insert the modified line, and continue.\n            text = text[:current_pos] + line\n        elif text[i] == \"\\b\":\n            # If we see a backspace, we remove the last character, and continue.\n            text = text[:-1]\n        else:\n            # If we see any other character, we just move the pointer forward.\n            current_pos += 1\n    return text\n\n", "idx": 359}
{"namespace": "sacred.commands.help_for_command", "completion": "    # Get the help text for the command.\n    help_text = pydoc.text.document(command)\n\n    # Remove any backspaces from the help text.\n    help_text = help_text.replace('\\b', '')\n\n    return help_text\n\n", "idx": 360}
{"namespace": "sacred.optional.optional_import", "completion": "    for package_name in package_names:\n        try:\n            package = importlib.import_module(package_name)\n            return True, package\n        except ImportError:\n            pass\n    return False, None\n\n", "idx": 361}
{"namespace": "sacred.dependencies.get_py_file_if_possible", "completion": "    if pyc_name.endswith(\".py\") or pyc_name.endswith(\".so\") or pyc_name.endswith(\".pyd\") or pyc_name.endswith(\".ipynb\"):\n        return pyc_name\n    else:\n        if os.path.isfile(pyc_name[:-1]):\n            return pyc_name[:-1]\n        else:\n            return pyc_name\n\n", "idx": 362}
{"namespace": "sacred.config.custom_containers.DogmaticDict.update", "completion": "        if iterable is not None:\n            try:\n                for k, v in iterable.items():\n                    self[k] = v\n            except AttributeError:\n                for k, v in iterable:\n                    self[k] = v\n\n        for k, v in kwargs.items():\n            self[k] = v\n", "idx": 363}
{"namespace": "sacred.config.config_scope.is_empty_or_comment", "completion": "    line = line.strip()\n    if not line:\n        return True\n    if line.startswith(\"#\"):\n        return True\n    return False\n\n", "idx": 364}
{"namespace": "boltons.funcutils.copy_function", "completion": "    func = FunctionType(orig.__code__,\n                        orig.__globals__,\n                        name=orig.__name__,\n                        argdefs=getattr(orig, \"__defaults__\", None),\n                        closure=getattr(orig, \"__closure__\", None))\n    func.__dict__.update(orig.__dict__)\n    if copy_dict:\n        func.__dict__ = func.__dict__.copy()\n    return func\n\n", "idx": 365}
{"namespace": "sacred.config.config_scope.dedent_line", "completion": "    if line.startswith(indent):\n        line = line[len(indent) :]\n    return line\n\n", "idx": 366}
{"namespace": "boltons.funcutils.format_invocation", "completion": "    if kwargs is None:\n        kwargs = {}\n\n    # Format the positional arguments.\n    if args:\n        args_str = ', '.join([repr(arg) for arg in args])\n    else:\n        args_str = ''\n\n    # Format the keyword arguments.\n    if kwargs:\n        kwargs_str = ', '.join([\n            '{}={}'.format(k, repr(v)) for k, v in kwargs.items()\n        ])\n    else:\n        kwargs_str = ''\n\n    # Join the arguments together.\n    if args_str and kwargs_str:\n        args_kwargs_str = ', '.join([args_str, kwargs_str])\n    else:\n        args_kwargs_str = args_str or kwargs_str\n\n    # Join the name and arguments together.\n    if name:\n        name_args_kwargs_str = ', '.join([name, args_kwargs_str])\n    else:\n        name_args_kwargs_str = args_kwargs_str\n\n    # Format the invocation.\n    return '{}('.format(name_args_kwargs_str) + args_kwargs_str + ')'\n\n", "idx": 367}
{"namespace": "boltons.listutils.SplayList.shift", "completion": "        item = self.pop(item_index)\n        self.insert(dest_index, item)\n", "idx": 368}
{"namespace": "boltons.strutils.gzip_bytes", "completion": "    if not 1 <= level <= 9:\n        raise ValueError('Invalid compression level: %s' % level)\n    buf = StringIO()\n    f = GzipFile(fileobj=buf, mode='wb', compresslevel=level)\n    f.write(bytestring)\n    f.close()\n    return buf.getvalue()\n\n", "idx": 369}
{"namespace": "boltons.strutils.is_uuid", "completion": "    if not obj:\n        return False\n\n    if isinstance(obj, uuid.UUID):\n        return version == 0 or obj.version == version\n\n    try:\n        uuid_obj = uuid.UUID(obj, version=version)\n        return uuid_obj.version == version\n    except ValueError:\n        return False\n\n", "idx": 370}
{"namespace": "boltons.strutils.parse_int_list", "completion": "    range_list = []\n    range_split_list = range_string.split(delim)\n\n    for token in range_split_list:\n        try:\n            # Check if the token is just an integer\n            range_list.append(int(token))\n        except ValueError:\n            # Otherwise, the token is a range with a contiguous range of integers\n            try:\n                range_start, range_end = token.split(range_delim)\n                range_list.extend(range(int(range_start), int(range_end) + 1))\n            except ValueError:\n                # If the token is not an integer or a range of integers, then raise an error\n                raise ValueError(\"Invalid range parameters in token %s\" % token)\n\n    return sorted(range_list)\n\n", "idx": 371}
{"namespace": "boltons.cacheutils.ThresholdCounter.get", "completion": "        try:\n            return self._count_map[key][0]\n        except KeyError:\n            return default\n", "idx": 372}
{"namespace": "boltons.iterutils.backoff_iter", "completion": "    if not start > 0:\n        raise ValueError('start must be > 0')\n    if not stop > 0:\n        raise ValueError('stop must be > 0')\n    if not factor > 0:\n        raise ValueError('factor must be > 0')\n    if not (jitter is False or jitter is None or jitter is True):\n        raise ValueError('jitter must be False, None, or True')\n    if jitter is True:\n        jitter = 1.0\n    if jitter is not None and not -1.0 <= jitter <= 1.0:\n        raise ValueError('jitter must be between -1.0 and 1.0')\n    if count is None:\n        count = int(math.ceil(math.log(stop / start, factor)))\n    elif count == 'repeat':\n        count = None\n    else:\n        count = int(count)\n        if count <= 0:\n            raise ValueError('count must be > 0')\n    if count == 0:\n        return\n    if jitter:\n        jitter = abs(jitter)\n        jitter = jitter * start * (factor - 1)\n    if factor == 1:\n        if jitter:\n            for i in xrange(count):\n                yield start + random.uniform(-jitter, jitter)\n        else:\n            for i in xrange(count):\n                yield start\n        return\n    if jitter:\n        jitter = jitter * start * (factor - 1)\n    if count == 1:\n        yield start\n        return\n    if jitter:\n        yield start + random.uniform(-jitter, jitter)\n    else:\n        yield start\n    for i in xrange(1, count):\n        yield start * factor\n        start = start * factor\n        if jitter:\n            yield start + random.uniform(-jitter, jitter)\n        else:\n            yield start\n\n", "idx": 373}
{"namespace": "boltons.cacheutils.cached", "completion": "    def decorator(func):\n        return CachedFunction(func, cache, scoped=scoped, typed=typed, key=key)\n    return decorator\n\n", "idx": 374}
{"namespace": "boltons.timeutils.total_seconds", "completion": "    return (td.microseconds + (td.seconds + td.days * 24 * 3600) * 10**6) / 10**6\n\n", "idx": 375}
{"namespace": "boltons.gcutils.get_all", "completion": "    # Get all objects of the given type.\n    all_objects = [obj for obj in gc.get_objects() if isinstance(obj, type_obj)]\n\n    # If we don't need to include subtypes, return the list.\n    if not include_subtypes:\n        return all_objects\n\n    # Otherwise, we need to get all subtypes of the given type.\n    subtypes = type_obj.__subclasses__()\n\n    # If there are no subtypes, return the list.\n    if not subtypes:\n        return all_objects\n\n    # Otherwise, we need to get all subtypes of the subtypes.\n    subsubtypes = [subtype for subtype in subtypes if subtype.__subclasses__()]\n\n    # If there are no subsubtypes, we can just return the list of all subtypes.\n    if not subsubtypes:\n        return all_objects + [obj for obj in gc.get_objects() if isinstance(obj, subtypes)]\n\n    # Otherwise, we need to get all subsubtypes of the subsubtypes.\n    subsubsubtypes = [subsubtype for subsubtype in subsubtypes if subsubtype.__subclasses__()]\n\n    # If there are no subsubsubtypes, we can just return the list of all subsubtypes.\n    if not subsubsubtypes:\n        return all_objects + [obj for obj in gc.get_objects() if isinstance(obj, subsubtypes)]\n\n    # Otherwise, we need to get all subsubsubtypes of the subsubsubtypes.\n    subsubsubsubtypes = [subsubsubtype for subsubsubtype in subsubsubtypes if subsubsubtype.__subclasses__()]\n\n    # If there are no subsubsubsubtypes, we can just return the list of all subsubsubtypes.\n    if not subsubsubsubtypes:\n        return all_objects + [obj for obj in gc.get_objects() if isinstance(obj, subsubsubtypes)]\n\n    # Otherwise, we need to get all subsubsubsubtypes of the subsubsubsubtypes.\n    subsubsubsubsub", "idx": 376}
{"namespace": "boltons.timeutils.daterange", "completion": "    if isinstance(step, timedelta):\n        step = (step.days, step.seconds, step.microseconds)\n    elif isinstance(step, int):\n        step = (step, 0, 0)\n    elif isinstance(step, tuple):\n        pass\n    else:\n        raise TypeError(\"step must be an int, timedelta, or tuple\")\n\n    if step[0]:\n        if not isinstance(start, date):\n            raise TypeError(\"start must be a date\")\n        if not isinstance(stop, date):\n            raise TypeError(\"stop must be a date\")\n\n        if step[0] > 0:\n            if stop is not None and start > stop:\n                raise ValueError(\"start must be less than stop\")\n        elif step[0] < 0:\n            if stop is not None and start < stop:\n                raise ValueError(\"start must be greater than stop\")\n\n    if step[0] < 0:\n        if stop is None:\n            raise ValueError(\"stop must be provided when step is negative\")\n        if inclusive:\n            while start >= stop:\n                yield start\n                start += timedelta(step[0], step[1], step[2])\n        else:\n            while start > stop:\n                yield start\n                start += timedelta(step[0], step[1], step[2])\n    else:\n        if stop is None:\n            while True:\n                yield start\n                start += timedelta(step[0], step[1], step[2])\n        elif inclusive:\n            while start <= stop:\n                yield start\n                start += timedelta(step[0], step[1], step[2])\n        else:\n            while start < stop:\n                yield start\n                start += timedelta(step[0], step[1], step[2])\n\n", "idx": 377}
{"namespace": "boltons.mathutils.clamp", "completion": "    return max(lower, min(x, upper))\n\n", "idx": 378}
{"namespace": "boltons.mathutils.ceil", "completion": "    if options is None:\n        return _ceil(x)\n    else:\n        options = sorted(options)\n        if x <= options[0]:\n            return options[0]\n        elif x >= options[-1]:\n            return options[-1]\n        else:\n            index = bisect.bisect_left(options, x)\n            return options[index]\n\n", "idx": 379}
{"namespace": "boltons.formatutils.get_format_args", "completion": "    # TODO: memoize\n    pos_args, named_args = [], []\n    for lit, fname in split_format_str(fstr):\n        if fname is None:\n            continue\n        fname = fname[1:]\n        if fname[0] in _INTCHARS:\n            pos_args.append((fname, _TYPE_MAP[fname[0]]))\n        else:\n            named_args.append((fname, _TYPE_MAP[fname[1]]))\n    return pos_args, named_args\n\n", "idx": 380}
{"namespace": "boltons.mathutils.floor", "completion": "    if options is None:\n        return _floor(x)\n    options = sorted(options)\n    i = bisect.bisect_right(options, x)\n    if i == 0:\n        raise ValueError(\"no floor options less than or equal to: %r\" % x)\n    return options[i - 1]\n\n", "idx": 381}
{"namespace": "boltons.dictutils.OneToOne.setdefault", "completion": "        if key in self:\n            return self[key]\n        self[key] = default\n        return default\n", "idx": 382}
{"namespace": "boltons.dictutils.OneToOne.update", "completion": "        if isinstance(dict_or_iterable, dict):\n            for key, val in dict_or_iterable.items():\n                self[key] = val\n        else:\n            for key, val in dict_or_iterable:\n                self[key] = val\n        for key, val in kw.items():\n            self[key] = val\n", "idx": 383}
{"namespace": "boltons.dictutils.ManyToMany.get", "completion": "        return self.data.get(key, default)\n", "idx": 384}
{"namespace": "boltons.dictutils.FrozenDict.updated", "completion": "        d = self.copy()\n        d.update(*a, **kw)\n        return d\n", "idx": 385}
{"namespace": "boltons.dictutils.subdict", "completion": "    if keep is None:\n        keep = d.keys()\n    if drop is None:\n        drop = []\n    return dict([(k, d[k]) for k in keep if k not in drop])\n\n", "idx": 386}
{"namespace": "boltons.dictutils.FrozenDict.__repr__", "completion": "        return '%s(%r)' % (self.__class__.__name__, dict.__repr__(self))\n", "idx": 387}
{"namespace": "gunicorn.config.validate_callable", "completion": "    def validator(val):\n        if isinstance(val, str):\n            val = util.load_class(val)\n\n        if not callable(val):\n            raise TypeError(\"Value is not a callable object: %s\" % val)\n\n        if arity > 0 and not hasattr(val, '__code__'):\n            raise TypeError(\"Value is not a callable object: %s\" % val)\n\n        if arity > 0 and val.__code__.co_argcount != arity:\n            raise TypeError(\"Value has an invalid arity: %s\" % val)\n\n        if arity == 0 and hasattr(val, '__code__') and val.__code__.co_argcount > 0:\n            raise TypeError(\"Value has an invalid arity: %s\" % val)\n\n        return val\n\n    return validator\n\n", "idx": 388}
{"namespace": "gunicorn.config.get_default_config_file", "completion": "    # Create the path to the default configuration file.\n    default_config_file_path = os.path.join(util.getcwd(), 'gunicorn.conf.py')\n\n    # Check if the file exists.\n    if os.path.exists(default_config_file_path):\n        return default_config_file_path\n\n    return None\n\n", "idx": 389}
{"namespace": "gunicorn.util.is_ipv6", "completion": "    try:\n        socket.inet_pton(socket.AF_INET6, addr)\n        return True\n    except socket.error:\n        return False\n\n", "idx": 390}
{"namespace": "gunicorn.systemd.listen_fds", "completion": "    listen_pid = os.environ.get('LISTEN_PID')\n    if not listen_pid or int(listen_pid) != os.getpid():\n        return 0\n\n    listen_fds = os.environ.get('LISTEN_FDS')\n    if not listen_fds:\n        return 0\n\n    try:\n        listen_fds = int(listen_fds)\n    except ValueError:\n        return 0\n\n    if unset_environment:\n        for k in list(os.environ.keys()):\n            if k.startswith('LISTEN_'):\n                os.unsetenv(k)\n\n    return listen_fds\n\n", "idx": 391}
{"namespace": "gunicorn.util.http_date", "completion": "    if timestamp is None:\n        timestamp = time.time()\n    year, month, day, hh, mm, ss, wd, y, z = time.gmtime(timestamp)\n    s = \"%s, %02d %3s %4d %02d:%02d:%02d GMT\" % (\n        (\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")[wd],\n        day,\n        (\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")[month - 1],\n        year,\n        hh,\n        mm,\n        ss,\n    )\n    return s\n\n", "idx": 392}
{"namespace": "gunicorn.util.parse_address", "completion": "    if netloc.startswith('unix://'):\n        return netloc.split('://')[1]\n    elif netloc.startswith('fd://'):\n        return int(netloc.split('://')[1])\n    else:\n        host, port = netloc.split(':')\n        port = int(port)\n        if port == 0:\n            port = default_port\n        return host, port\n\n", "idx": 393}
{"namespace": "gunicorn.util.to_bytestring", "completion": "    if isinstance(value, bytes):\n        return value\n    elif isinstance(value, str):\n        return value.encode(encoding)\n    else:\n        raise TypeError(\"Expected str or bytes, received %s\" % type(value))\n\n", "idx": 394}
{"namespace": "gunicorn.util.warn", "completion": "    sys.stderr.write(\"WARNING: %s\\n\" % msg)\n\n", "idx": 395}
{"namespace": "gunicorn.util.split_request_uri", "completion": "    if uri.startswith(\"//\"):\n        uri = \".\" + uri\n\n    scheme, netloc, path, params, query, fragment = urllib.parse.urlparse(uri)\n    path = urllib.parse.unquote(path)\n    query = urllib.parse.unquote(query)\n    fragment = urllib.parse.unquote(fragment)\n\n    return scheme, netloc, path, params, query, fragment\n\n", "idx": 396}
{"namespace": "praw.models.listing.listing.ModNoteListing.after", "completion": "        if not getattr(self, \"has_next_page\", False):\n            return None\n        return getattr(self, \"end_cursor\")\n\n", "idx": 397}
{"namespace": "praw.models.util.permissions_string", "completion": "    if permissions is None:\n        return \"+all\"\n\n    # Remove all instances of whitespace\n    permissions = [permission.strip() for permission in permissions]\n\n    # Remove all instances of empty strings\n    permissions = [permission for permission in permissions if permission]\n\n    # Remove all instances of empty strings\n    permissions = [permission for permission in permissions if permission]\n\n    # Remove all instances of duplicate strings\n    permissions = list(OrderedDict.fromkeys(permissions))\n\n    # Remove all instances of duplicate strings\n    permissions = [permission for permission in permissions if permission]\n\n    # Remove all instances of duplicate strings\n    permissions = [permission for permission in permissions if permission]\n\n    # Remove all instances of duplicate strings\n    permissions = [permission for permission in permissions if permission]\n\n    # Remove all instances of duplicate strings\n    permissions = [permission for permission in permissions if permission]\n\n    # Remove all instances of duplicate strings\n    permissions = [permission for permission in permissions if permission]\n\n    # Remove all instances of duplicate strings\n    permissions = [permission for permission in permissions if permission]\n\n    # Remove all instances of duplicate strings\n    permissions = [permission for permission in permissions if permission]\n\n    # Remove all instances of duplicate strings\n    permissions = [permission for permission in permissions if permission]\n\n    # Remove all instances of duplicate strings\n    permissions = [permission for permission in permissions if permission]\n\n    # Remove all instances of duplicate strings\n    permissions = [permission for permission in permissions if permission]\n\n    # Remove all instances of duplicate strings\n    permissions = [permission for permission in permissions if permission]\n\n    # Remove all instances of duplicate strings\n    permissions = [permission for permission in permissions if permission]\n\n    # Remove all instances of duplicate strings\n    permissions = [permission for permission in permissions if permission]\n\n    # Remove all instances of duplicate strings\n    permissions = [permission for permission in permissions if permission]\n\n    # Remove all instances of duplicate strings\n    permissions = [permission for permission in permissions if permission]\n\n    # Remove all instances of duplicate strings\n    permissions = [permission for permission in permissions if permission]\n\n    # Remove all instances of duplicate strings\n    permissions = [permission for permission in permissions if permission]", "idx": 398}
{"namespace": "jc.cli.JcCli.json_out", "completion": "        if self.pretty:\n            return json.dumps(self.data_out, indent=self.indent, separators=self.json_separators)\n\n        if self.mono:\n            return json.dumps(self.data_out, separators=self.json_separators)\n\n        class JcStyle(Style):\n            styles: CustomColorType = self.custom_colors\n\n        return str(highlight(json.dumps(self.data_out, separators=self.json_separators), JsonLexer(), Terminal256Formatter(style=JcStyle))[0:-1])\n", "idx": 399}
{"namespace": "pythonforandroid.pythonpackage.transform_dep_for_pip", "completion": "    if \"://\" in dependency:\n        dependency = dependency.split(\"://\")[1]\n        if \"@\" in dependency:\n            dependency = dependency.split(\"@\")[1]\n        return dependency\n    else:\n        return dependency\n\n", "idx": 400}
{"namespace": "pythonforandroid.graph.fix_deplist", "completion": "    for i, dep in enumerate(deps):\n        if isinstance(dep, str):\n            deps[i] = (dep,)\n    return [tuple(dep.lower() for dep in deps_tuple) for deps_tuple in deps]\n\n", "idx": 401}
{"namespace": "pythonforandroid.util.walk_valid_filens", "completion": "    for dir_path, dir_names, file_names in walk(base_dir):\n        # Remove invalid directory names.\n        for invalid_dir_name in invalid_dir_names:\n            if invalid_dir_name in dir_names:\n                dir_names.remove(invalid_dir_name)\n        # Check if the file path matches the glob patterns.\n        for file_name in file_names:\n            full_path = join(dir_path, file_name)\n            if any(fnmatch(full_path, pattern) for pattern in invalid_file_patterns):\n                continue\n            yield full_path\n\n", "idx": 402}
{"namespace": "pythonforandroid.bootstrap._cmp_bootstraps_by_priority", "completion": "    a_priority = default_recipe_priorities.index(a.name)\n    b_priority = default_recipe_priorities.index(b.name)\n    if a_priority != b_priority:\n        return a_priority - b_priority\n    else:\n        return (a.name > b.name) - (a.name < b.name)\n\n", "idx": 403}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.all_bootstraps", "completion": "        # Get the path to the bootstraps directory:\n        bootstrap_dir = join(dirname(__file__), 'bootstraps')\n\n        # Get the names of all the directories in the bootstraps directory:\n        bootstrap_subdirs = [dir for dir in listdir(bootstrap_dir) if isdir(join(bootstrap_dir, dir))]\n\n        # Get the names of all the python files in the bootstraps directory:\n        bootstrap_modules = [module.replace('.py', '') for module in listdir(bootstrap_dir) if module.endswith('.py')]\n\n        # Get the names of all the bootstrap classes in the bootstraps directory:\n        bootstrap_classes = [cls.__name__ for cls in itermodules(bootstrap_dir)]\n\n        # Get the set of all the available bootstraps:\n        available_bootstraps = set(bootstrap_subdirs + bootstrap_modules + bootstrap_classes)\n\n        # Return the set of available bootstraps:\n        return available_bootstraps\n", "idx": 404}
{"namespace": "mmcv.image.colorspace._convert_input_type_range", "completion": "    img_type = img.dtype\n    if img_type != np.float32:\n        img = img.astype(np.float32)\n    # add eps to prevent division by zero\n    eps = np.finfo(np.float32).eps\n    img = img / 255.0\n    return img\n\n", "idx": 405}
{"namespace": "mackup.utils.error", "completion": "    print(\"ERROR: {}\".format(message))\n    sys.exit(1)\n\n", "idx": 406}
{"namespace": "mmcv.image.colorspace._convert_output_type_range", "completion": "    if dst_type not in (np.uint8, np.float32):\n        raise TypeError('The dst_type should be np.float32 or np.uint8, but got '\n                        f'{dst_type}')\n    if dst_type == np.uint8:\n        img = img.round()\n    else:\n        img /= 255.\n    return img.astype(dst_type)\n\n", "idx": 407}
{"namespace": "mackup.utils.is_process_running", "completion": "    try:\n        subprocess.check_output([\"pgrep\", \"-f\", process_name])\n        return True\n    except subprocess.CalledProcessError:\n        return False\n\n", "idx": 408}
{"namespace": "stellar.operations._get_pid_column", "completion": "    server_version = raw_conn.execute('SELECT version();').fetchone()[0]\n    server_version = re.sub(r'[^\\d.]', '', server_version)\n    version_number = server_version.split('.')[0:2]\n    version_number = int(''.join(version_number))\n    if version_number >= 100000:\n        return 'pid'\n    else:\n        return 'procpid'\n\n", "idx": 409}
{"namespace": "imapclient.imap_utf7.encode", "completion": "    if isinstance(s, str):\n        s = s.encode('utf-8')\n\n    res = b''\n    for c in s:\n        if c in b'&+,:;\\\\()':\n            res += b'&' + binascii.hexlify(c) + b'&'\n        elif c == b' ':\n            res += b'&-&'\n        else:\n            res += c\n\n    return res\n\n", "idx": 410}
{"namespace": "imapclient.version._imapclient_version_string", "completion": "    major, minor, micro, releaselevel = vinfo\n    version_string = \"%d.%d.%d\" % (major, minor, micro)\n    if releaselevel != \"final\":\n        version_string += \"-%s\" % releaselevel\n    return version_string\n\n", "idx": 411}
{"namespace": "telethon.helpers.generate_key_data_from_nonce", "completion": "    server_nonce = int.to_bytes(server_nonce, 16, 'little', signed=True)\n    new_nonce = int.to_bytes(new_nonce, 16, 'little', signed=True)\n\n    hash1 = sha1(new_nonce + server_nonce).digest()\n    hash2 = sha1(server_nonce + new_nonce).digest()\n    hash3 = sha1(new_nonce + new_nonce).digest()\n\n    key = hash1 + hash2[:12]\n    iv = hash2[12:] + hash3 + new_nonce[:4]\n\n    return key, iv\n\n", "idx": 412}
{"namespace": "hbmqtt.codecs.bytes_to_int", "completion": "    return int.from_bytes(data, byteorder='big')\n\n", "idx": 413}
{"namespace": "zulipterminal.helper.display_error_if_present", "completion": "    if \"result\" in response and response[\"result\"] == \"error\" and hasattr(controller, \"view\"):\n        controller.report_error(response[\"msg\"])\n\n", "idx": 414}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._decode_message_id", "completion": "        try:\n            return int(message_id)\n        except ValueError:\n            return None\n", "idx": 415}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton.handle_narrow_link", "completion": "        parsed_link = self._parse_narrow_link(self.link)\n        error = self._validate_narrow_link(parsed_link)\n\n        if error:\n            self.controller.report_error(error)\n        else:\n            self._switch_narrow_to(parsed_link)\n", "idx": 416}
{"namespace": "zulipterminal.config.color.color_properties", "completion": "    class Color(Enum):\n        pass\n\n    for color in colors:\n        setattr(Color, color.name, color.value)\n\n    for prop in prop:\n        setattr(Color, f\"{prop}_\", lambda self: self.replace(color=getattr(Color, self.color.name + prop)))\n\n    return Color\n\n", "idx": 417}
{"namespace": "twilio.base.deserialize.decimal", "completion": "    if d is None:\n        return d\n    if d == \"\":\n        return d\n    return Decimal(d, BasicContext)", "idx": 418}
{"namespace": "twilio.base.deserialize.integer", "completion": "    try:\n        return int(i)\n    except ValueError:\n        return i", "idx": 419}
{"namespace": "twilio.base.serialize.object", "completion": "    try:\n        return json.dumps(obj)\n    except:\n        return obj", "idx": 420}
{"namespace": "twilio.base.serialize.map", "completion": "    if lst is None:\n        return None\n    return [serialize_func(elem) for elem in lst]", "idx": 421}
{"namespace": "twilio.base.obsolete.deprecated_method", "completion": "    def deprecated_method_wrapper(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            warnings.warn(\n                \"{} is a deprecated method. Use {} instead.\".format(func.__name__, new_func.__name__),\n                category=DeprecationWarning,\n                stacklevel=2,\n            )\n            return func(*args, **kwargs)\n\n        return wrapper\n\n    return deprecated_method_wrapper", "idx": 422}
{"namespace": "chatette.utils.sample_indulgent", "completion": "    if nb_items > len(array):\n        return deepcopy(array)\n    else:\n        return sample(array, nb_items)\n\n", "idx": 423}
{"namespace": "chatette.utils.rchop", "completion": "    if string.endswith(ending):\n        return string[:-len(ending)]\n    return string\n\n", "idx": 424}
{"namespace": "chatette.utils.str_to_bool", "completion": "    if text.lower() == \"true\":\n        return True\n    elif text.lower() == \"false\":\n        return False\n    else:\n        raise ValueError(\"Cannot convert '\" + text + \"' to a boolean.\")\n\n", "idx": 425}
{"namespace": "chatette.utils.min_if_exist", "completion": "    if n1 is None and n2 is None:\n        return None\n    if n1 is None:\n        return n2\n    if n2 is None:\n        return n1\n    return min(n1, n2)\n\n", "idx": 426}
{"namespace": "chatette.utils.append_to_list_in_dict", "completion": "    if key in dict_of_lists:\n        dict_of_lists[key].append(value)\n    else:\n        dict_of_lists[key] = [value]\n\n", "idx": 427}
{"namespace": "chatette.utils.extend_list_in_dict", "completion": "    if key not in dict_of_lists:\n        dict_of_lists[key] = values\n    else:\n        dict_of_lists[key].extend(values)\n\n", "idx": 428}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy._is_end_regex", "completion": "        if word.endswith('/'):\n            return True\n        if word.endswith('$'):\n            return False\n        if word.endswith('\\\\'):\n            return False\n        return False\n", "idx": 429}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.execute", "completion": "        raise NotImplementedError()", "idx": 430}
{"namespace": "aioxmpp.network.group_and_order_srv_records", "completion": "    if rng is None:\n        rng = random\n\n    # Sort the SRV records by priority and then by weight.\n    all_records.sort(key=lambda x: (x[0], x[1]))\n\n    # Group the SRV records by priority.\n    grouped_records = itertools.groupby(all_records, key=lambda x: x[0])\n\n    # Order the SRV records by weight.\n    for _, g in grouped_records:\n        g = list(g)\n        g.sort(key=lambda x: x[1])\n\n        # Order the SRV records by random.\n        for _, _, _, hostname, port in g:\n            yield hostname, port\n\n", "idx": 431}
{"namespace": "aioxmpp.nonza.StreamFeatures.get_feature", "completion": "        try:\n            return self[feature_cls]\n        except KeyError:\n            return default\n\n", "idx": 432}
{"namespace": "aioxmpp.connector.XMPPOverTLSConnector._context_factory_factory", "completion": "        def context_factory():\n            \"\"\"\n            This function creates the context factory for the XMPPOverTLSConnector.\n            Input-Output Arguments\n            :param self: XMPPOverTLSConnector. An instance of the XMPPOverTLSConnector class.\n            :return: The ssl context.\n            \"\"\"\n            ssl_context = metadata.ssl_context_factory()\n            verifier.setup_context(ssl_context, None)\n            if hasattr(ssl_context, \"set_alpn_protocols\"):\n                ssl_context.set_alpn_protocols([\"xmpp-client\"])\n            return ssl_context\n\n        return context_factory\n", "idx": 433}
{"namespace": "aioxmpp.xmltestutils.element_path", "completion": "    path = []\n    while el is not upto:\n        if el.getparent() is not None:\n            for i, child in enumerate(el.getparent()):\n                if child is el:\n                    path.append(el.tag + \"[\" + str(i + 1) + \"]\")\n                    break\n            el = el.getparent()\n        else:\n            break\n    return \"/\".join(reversed(path))\n\n", "idx": 434}
{"namespace": "aioxmpp.structs.JID.fromstr", "completion": "        if not s:\n            raise ValueError(\"JID must not be empty\")\n\n        if not s.startswith(\"@\"):\n            localpart, s = s.split(\"@\", 1)\n            if localpart:\n                localpart = nodeprep(\n                    localpart,\n                    allow_unassigned=not strict\n                )\n        else:\n            localpart = None\n\n        if not s:\n            raise ValueError(\"JID must not be empty\")\n\n        if \"/\" in s:\n            domain, resource = s.split(\"/\", 1)\n        else:\n            domain = s\n            resource = None\n\n        if domain:\n            domain = nameprep(\n                domain,\n                allow_unassigned=not strict\n            )\n\n        if not domain:\n            raise ValueError(\"domain must not be empty\")\n\n        if len(domain.encode(\"utf-8\")) > 1023:\n            raise ValueError(\"domain too long\")\n\n        if resource:\n            resource = resourceprep(\n                resource,\n                allow_unassigned=not strict\n            )\n\n        if len(resource.encode(\"utf-8\")) > 1023:\n            raise ValueError(\"resource too long\")\n\n        return cls(localpart, domain, resource, strict=strict)\n\n", "idx": 435}
{"namespace": "aioxmpp.security_layer.extract_python_dict_from_x509", "completion": "    result = {}\n    subject = x509.get_subject()\n    subject_components = subject.get_components()\n    for component in subject_components:\n        result[component[0]] = component[1]\n    subject_alt_name = x509.get_extension_at(1).get_data()\n    subject_alt_name_components = subject_alt_name.get_components()\n    for component in subject_alt_name_components:\n        result[component[0]] = component[1]\n    return result\n\n", "idx": 436}
{"namespace": "aioxmpp.security_layer.extract_blob", "completion": "    der = pyasn1.codec.der.encoder.encode(x509)\n    return der\n\n", "idx": 437}
{"namespace": "aioxmpp.security_layer.blob_to_pyasn1", "completion": "    return pyasn1.codec.der.decoder.decode(blob, asn1Spec=pyasn1_modules.rfc2459.Certificate())[0]\n\n", "idx": 438}
{"namespace": "aioxmpp.security_layer.extract_pk_blob_from_pyasn1", "completion": "    return pyasn1.codec.der.encoder.encode(\n        pyasn1_struct[0][2][1]\n    )\n\n", "idx": 439}
{"namespace": "aioxmpp.callbacks.AdHocSignal.ASYNC_WITH_LOOP", "completion": "        if loop is None:\n            loop = asyncio.get_event_loop()\n\n        def wrapper(f):\n            if not hasattr(f, \"__call__\"):\n                raise TypeError(\"must be callable, got {!r}\".format(f))\n            return functools.partial(cls._async_with_loop_wrapper, f, loop)\n\n        return wrapper\n", "idx": 440}
{"namespace": "aioxmpp.callbacks.AdHocSignal.SPAWN_WITH_LOOP", "completion": "        if loop is None:\n            loop = asyncio.get_event_loop()\n\n        def spawn(f):\n            if not asyncio.iscoroutinefunction(f):\n                raise TypeError(\"must be a coroutine function\")\n\n            @functools.wraps(f)\n            def wrapper(*args, **kwargs):\n                task = loop.create_task(f(*args, **kwargs))\n                task.add_done_callback(log_spawned, logger=logger)\n                return task\n\n            return wrapper\n\n        return spawn\n", "idx": 441}
{"namespace": "aioxmpp.callbacks.first_signal", "completion": "    async def _first_signal_awaitable(signals):\n        signals = list(signals)\n        if len(signals) == 0:\n            raise ValueError(\"At least one signal must be provided\")\n        for signal in signals:\n            if not isinstance(signal, AdHocSignal):\n                raise TypeError(\"Signals must be of type AdHocSignal\")\n        first_signal = signals[0]\n        for signal in signals[1:]:\n            first_signal.connect(signal.fire)\n        await first_signal.future()\n        return first_signal\n\n    return _first_signal_awaitable(signals)", "idx": 442}
{"namespace": "aioxmpp.tasks.TaskPool.spawn", "completion": "        if not isinstance(__groups, set):\n            raise TypeError(\"groups must be a set\")\n        if not all(isinstance(group, type(None)) or isinstance(group, type(())) for group in __groups):\n            raise TypeError(\"groups must be a set of None or tuple\")\n        if not callable(__coro_fun):\n            raise TypeError(\"coro_fun must be a coroutine function\")\n        if not isinstance(args, tuple):\n            raise TypeError(\"args must be a tuple\")\n        if not isinstance(kwargs, dict):\n            raise TypeError(\"kwargs must be a dict\")\n        if not all(isinstance(key, str) for key in kwargs.keys()):\n            raise TypeError(\"kwargs must be a dict of str: object\")\n\n        # Check if the groups have free slots available for the coroutine to be spawned\n        for group in __groups:\n            limit = self.get_limit(group)\n            if limit is not None and self.get_task_count(group) >= limit:\n                raise RuntimeError(\"group {} is at capacity\".format(group))\n        # Check if the total limit is exhausted\n        if self.default_limit is not None:\n            total_limit = self.default_limit\n            for group in __groups:\n                limit = self.get_limit(group)\n                if limit is not None:\n                    total_limit -= limit\n            if total_limit < 0:\n                raise RuntimeError(\"total limit is exhausted\")\n\n        # Spawn the coroutine\n        task = asyncio.ensure_future(__coro_fun(*args, **kwargs))\n        for group in __groups:\n            self._group_tasks.setdefault(group, set()).add(task)\n        return task\n", "idx": 443}
{"namespace": "aioxmpp.protocol.send_and_wait_for", "completion": "    if not isinstance(send, nonza.Stanza):\n        raise TypeError(\"send must be a Stanza\")\n    if not isinstance(wait_for, nonza.Stanza):\n        raise TypeError(\"wait_for must be a Stanza\")\n\n    # Create a future to wait for the response\n    fut = asyncio.Future()\n\n    # Create a deferred to wait for the response\n    def deferred(stanza):\n        if stanza.handled:\n            return\n        if stanza.xml.tag == wait_for.xml.tag:\n            stanza.handled = True\n            fut.set_result(stanza)\n\n    # Add the deferred to the callbacks\n    xmlstream.register_handler(\n        callback.Callback(None, deferred, run_once=True)\n    )\n\n    # Send the message\n    xmlstream.send(send)\n\n    # Wait for the response\n    try:\n        response = await asyncio.wait_for(fut, timeout)\n    except asyncio.TimeoutError:\n        raise errors.TimeoutError(\"Timed out waiting for response\")\n\n    # Remove the deferred\n    xmlstream.remove_handler(deferred)\n\n    # Call the callback\n    if cb:\n        cb(response)\n\n    return response\n\n", "idx": 444}
{"namespace": "aioxmpp.testutils.run_coroutine_with_peer", "completion": "    if not loop:\n        loop = asyncio.get_event_loop()\n    local_future = asyncio.ensure_future(coroutine, loop=loop)\n    peer_future = asyncio.ensure_future(peer_coroutine, loop=loop)\n    try:\n        loop.run_until_complete(\n            asyncio.wait(\n                [local_future, peer_future],\n                timeout=timeout,\n                loop=loop))\n    except asyncio.TimeoutError:\n        raise TimeoutError(\"Timeout reached\")\n    return local_future.result()\n\n", "idx": 445}
{"namespace": "aioxmpp.testutils.make_listener", "completion": "    listener = unittest.mock.Mock()\n    for name, signal in vars(instance).items():\n        if isinstance(signal, callbacks.Signal):\n            setattr(listener, name, unittest.mock.Mock())\n            signal.connect(getattr(listener, name))\n    return listener\n\n", "idx": 446}
{"namespace": "aioxmpp.vcard.service.VCardService.set_vcard", "completion": "        iq = aioxmpp.IQ(\n            type_=aioxmpp.IQType.SET,\n            to=jid,\n            payload=vcard,\n        )\n\n        await self.client.send(iq)", "idx": 447}
{"namespace": "aioxmpp.rsm.xso.ResultSetMetadata.limit", "completion": "        result = copy.copy(self)\n        result.max_ = max_\n        return result\n", "idx": 448}
{"namespace": "aioxmpp.muc.service.Room.features", "completion": "        features = set()\n        if self.muc_active:\n            features.add(\"http://jabber.org/protocol/muc#user\")\n            features.add(\"http://jabber.org/protocol/muc#admin\")\n            features.add(\"http://jabber.org/protocol/muc#owner\")\n            features.add(\"jabber:x:muc#affiliation\")\n            features.add(\"jabber:x:muc#history\")\n            features.add(\"jabber:x:muc#passwordprotected\")\n            features.add(\"jabber:x:muc#persistent\")\n            features.add(\"jabber:x:muc#rooms\")\n            features.add(\"jabber:x:muc#unsecured\")\n            features.add(\"jabber:x:muc#unique\")\n            features.add(\"jabber:x:muc#unmoderated\")\n            features.add(\"jabber:x:muc#register\")\n            features.add(\"jabber:x:muc#multipleowners\")\n            features.add(\"jabber:x:muc#ignore_change_config\")\n            features.add(\"jabber:x:muc#ignore_set\")\n            features.add(\"jabber:x:muc#ignore_kick\")\n            features.add(\"jabber:x:muc#ignore_prune\")\n            features.add(\"jabber:x:muc#ignore_delete\")\n            features.add(\"jabber:x:muc#ignore_banned\")\n            features.add(\"jabber:x:muc#ignore_nonanonymous\")\n            features.add(\"jabber:x:muc#ignore_semi_anon\")\n            features.add(\"jabber:x:muc#ignore_public\")\n            features.add(\"jabber:x:muc#ignore_log\")\n            features.add(\"jabber:x:muc#ignore_visitor_only\")\n            features.add(\"jabber:x:muc#ignore_notices", "idx": 449}
{"namespace": "aioxmpp.xso.query.EvaluationContext.eval_bool", "completion": "        return bool(self.eval(expr))\n\n", "idx": 450}
{"namespace": "aioxmpp.xso.query._BoolOpMixin.eval", "completion": "        for value in self.expr.eval(ec):\n            yield value\n\n", "idx": 451}
{"namespace": "aioxmpp.xso.model.drop_handler", "completion": "    depth = 0\n    while True:\n        ev_type, *ev_args = yield\n        if ev_type == \"start\":\n            depth += 1\n        elif ev_type == \"end\":\n            depth -= 1\n        if depth == 0:\n            break\n\n", "idx": 452}
{"namespace": "aioxmpp.xso.model.guard", "completion": "    depth = 1\n    while depth:\n        try:\n            ev = yield from dest\n            if ev[0] == \"start\":\n                depth += 1\n            elif ev[0] == \"end\":\n                depth -= 1\n        except Exception as e:\n            yield (\"error\", e)\n            return\n    yield (\"end\", ev_args)\n\n", "idx": 453}
{"namespace": "aioxmpp.xso.model.capture_events", "completion": "    try:\n        while True:\n            ev = yield\n            dest.append(ev)\n            receiver.send(ev)\n    except StopIteration as exc:\n        return exc.value\n    except:\n        dest.clear()\n        raise\n    finally:\n        dest.clear()", "idx": 454}
{"namespace": "aioxmpp.xso.model.events_to_sax", "completion": "    for event in events:\n        if event[0] == \"start\":\n            dest.startElementNS(event[1], None, event[2])\n        elif event[0] == \"text\":\n            dest.characters(event[1])\n        elif event[0] == \"end\":\n            dest.endElementNS(event[1], None)\n\n", "idx": 455}
{"namespace": "aioxmpp.adhoc.service.AdHocClient.get_command_info", "completion": "        disco = self.dependencies[aioxmpp.disco.DiscoClient]\n        response = await disco.query_info(\n            peer_jid,\n            node=command_name,\n        )\n        return response\n", "idx": 456}
{"namespace": "aioxmpp.entitycaps.caps115.build_identities_string", "completion": "    identities_string = b''\n    identities_set = set()\n    for identity in identities:\n        identity_string = identity.to_string()\n        identities_set.add(identity_string)\n\n    identities_list = list(identities_set)\n    identities_list.sort()\n    for identity in identities_list:\n        identities_string += identity + b'<'\n\n    return identities_string[:-1]\n\n", "idx": 457}
{"namespace": "aioxmpp.entitycaps.caps115.build_features_string", "completion": "    features = [\n        escape(str(feature)).encode(\"utf-8\")\n        for feature in features\n    ]\n\n    if len(set(features)) != len(features):\n        raise ValueError(\"duplicate feature\")\n\n    features.sort()\n    features.append(b\"\")\n    return b\"<\".join(features)\n\n", "idx": 458}
{"namespace": "aioxmpp.entitycaps.caps115.build_forms_string", "completion": "    forms = [\n        b\"/\".join([\n            escape(form.category).encode(\"utf-8\"),\n            escape(form.type_).encode(\"utf-8\"),\n            escape(str(form.lang or \"\")).encode(\"utf-8\"),\n            escape(form.var or \"\").encode(\"utf-8\"),\n            escape(form.label or \"\").encode(\"utf-8\"),\n            escape(form.instructions or \"\").encode(\"utf-8\"),\n            escape(form.value or \"\").encode(\"utf-8\"),\n            escape(form.options or \"\").encode(\"utf-8\"),\n            escape(form.extras or \"\").encode(\"utf-8\"),\n        ])\n        for form in forms\n    ]\n\n    if len(set(forms)) != len(forms):\n        raise ValueError(\"duplicate form\")\n\n    forms.sort()\n    forms.append(b\"\")\n    return b\"<\".join(forms)\n\n", "idx": 459}
{"namespace": "aioxmpp.entitycaps.caps115.Key.path", "completion": "        return pathlib.Path(\n            \"hashes\",\n            self.algo,\n            urllib.parse.quote(self.node, safe=\"\")\n        )\n\n", "idx": 460}
{"namespace": "aioxmpp.entitycaps.caps390._process_features", "completion": "    features_string = b\"\"\n    for feature in features:\n        feature_bytes = feature.encode(\"utf-8\")\n        feature_length = len(feature_bytes)\n        feature_length_bytes = feature_length.to_bytes(2, byteorder=\"big\")\n        features_string += feature_length_bytes + feature_bytes\n    return features_string\n\n", "idx": 461}
{"namespace": "aioxmpp.entitycaps.caps390._process_identities", "completion": "    parts = [\n        _process_identity(identity)\n        for identity in identities\n    ]\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1c\"\n\n", "idx": 462}
{"namespace": "aioxmpp.entitycaps.caps390._process_extensions", "completion": "    parts = [\n        _process_form(form)\n        for form in exts\n    ]\n\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1c\"\n\n", "idx": 463}
{"namespace": "aioxmpp.entitycaps.caps390._calculate_hash", "completion": "    if algo == \"sha-1\":\n        return aioxmpp.hashes.sha1(hash_input)\n    elif algo == \"sha-256\":\n        return aioxmpp.hashes.sha256(hash_input)\n    elif algo == \"sha-512\":\n        return aioxmpp.hashes.sha512(hash_input)\n    elif algo == \"sha3-256\":\n        return aioxmpp.hashes.sha3_256(hash_input)\n    elif algo == \"sha3-512\":\n        return aioxmpp.hashes.sha3_512(hash_input)\n    else:\n        raise ValueError(\"Unsupported hash algorithm: %s\" % algo)\n\n", "idx": 464}
{"namespace": "aioxmpp.entitycaps.caps390.Key.node", "completion": "        return base64.b64encode(self.digest).decode(\"utf-8\")\n", "idx": 465}
{"namespace": "aioxmpp.entitycaps.caps390.Key.path", "completion": "        return pathlib.Path(\n            \"caps\",\n            self.algo,\n            base64.b64encode(self.digest).decode(\"ascii\")+\".key\"\n        )\n", "idx": 466}
{"namespace": "aioxmpp.entitycaps.caps390.Implementation.extract_keys", "completion": "        if presence.xep0390_caps is not None:\n            return tuple(Key(algo, digest)\n                         for algo, digest in presence.xep0390_caps.keys)\n        else:\n            return ()\n", "idx": 467}
{"namespace": "aioxmpp.roster.service.RosterClient.approve", "completion": "        self.client.send(\n            stanza.Presence(\n                to_jid=peer_jid,\n                type_=structs.PresenceType.SUBSCRIBED\n            )\n        )\n", "idx": 468}
{"namespace": "aioxmpp.roster.service.RosterClient.subscribe", "completion": "        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.SUBSCRIBE,\n                            to=peer_jid)\n        )\n", "idx": 469}
{"namespace": "aioxmpp.roster.service.RosterClient.unsubscribe", "completion": "        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.UNSUBSCRIBE,\n                            to=peer_jid)\n        )\n", "idx": 470}
{"namespace": "aioxmpp.forms.fields.BoundSingleValueField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass\n", "idx": 471}
{"namespace": "aioxmpp.forms.fields.BoundMultiValueField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass\n", "idx": 472}
{"namespace": "aioxmpp.forms.fields.BoundOptionsField.options", "completion": "        try:\n            del self._options\n        except AttributeError:\n            pass\n", "idx": 473}
{"namespace": "aioxmpp.forms.fields.BoundSelectField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass\n", "idx": 474}
{"namespace": "aioxmpp.forms.fields.BoundMultiSelectField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass\n", "idx": 475}
{"namespace": "cupy.random._generator.reset_states", "completion": "    global _random_states\n    _random_states = {}\n\n", "idx": 476}
{"namespace": "cupy.random._generator._check_and_get_dtype", "completion": "    if dtype is None:\n        return numpy.float64\n\n    if isinstance(dtype, cupy.dtype):\n        dtype = dtype.type\n\n    if dtype not in [numpy.float32, numpy.float64]:\n        raise ValueError('Only float32 and float64 are supported')\n\n    return dtype\n\n", "idx": 477}
{"namespace": "cupy_builder._command.filter_files_by_extension", "completion": "    filtered_sources = []\n    other_sources = []\n\n    for source in sources:\n        if source.endswith(extension):\n            filtered_sources.append(source)\n        else:\n            other_sources.append(source)\n\n    return filtered_sources, other_sources\n\n", "idx": 478}
{"namespace": "datasets.table._in_memory_arrow_table_from_file", "completion": "    in_memory_stream = pa.input_stream(filename)\n    arrow_table = pa.read_table(in_memory_stream)\n    return arrow_table\n\n", "idx": 479}
{"namespace": "datasets.table._in_memory_arrow_table_from_buffer", "completion": "    in_memory_stream = pa.input_stream(buffer)\n    opened_stream = pa.ipc.open_stream(in_memory_stream)\n    pa_table = opened_stream.read_all()\n    return pa_table\n\n", "idx": 480}
{"namespace": "datasets.table._interpolation_search", "completion": "    i, j = 0, len(arr) - 1\n    while i < j and arr[i] <= x < arr[j]:\n        k = i + ((j - i) * (x - arr[i]) // (arr[j] - arr[i]))\n        if arr[k] <= x < arr[k + 1]:\n            return k\n        elif arr[k] < x:\n            i = k + 1\n        else:\n            j = k\n    raise IndexError(f\"Can't find an appropriate position for {x} in {arr}\")\n\n", "idx": 481}
{"namespace": "datasets.data_files._is_inside_unrequested_special_dir", "completion": "    # If the pattern is a relative path, it is ignored\n    if not is_relative_path(pattern):\n        # If the pattern is a relative path, it is ignored\n        if is_relative_path(pattern):\n            return False\n        # If the pattern is an absolute path, we check if it is inside a special directory\n        else:\n            # If the pattern is an absolute path, we check if it is inside a special directory\n            pattern_dir = Path(pattern).parent\n            # If the pattern is a directory, we check if the path is inside it\n            if pattern_dir.is_dir():\n                # If the pattern is a directory, we check if the path is inside it\n                if Path(matched_rel_path).parent == pattern_dir:\n                    return True\n            # If the pattern is a file, we check if the path is inside the parent directory of the file\n            elif pattern_dir.is_file():\n                # If the pattern is a file, we check if the path is inside the parent directory of the file\n                if Path(matched_rel_path).parent == pattern_dir.parent:\n                    return True\n    return False\n\n", "idx": 482}
{"namespace": "datasets.data_files._is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir", "completion": "    # We just need to check if every special directories from the path is present explicly in the pattern.\n    # Since we assume that the path matches the pattern, it's equivalent to counting that both\n    # the parent path and the parent pattern have the same number of special directories.\n    data_dirs_to_ignore_in_path = [part for part in PurePath(matched_rel_path).parent.parts if part.startswith(\".\")]\n    data_dirs_to_ignore_in_pattern = [part for part in PurePath(pattern).parent.parts if part.startswith(\".\")]\n    return len(data_dirs_to_ignore_in_path) != len(data_dirs_to_ignore_in_pattern)\n\n", "idx": 483}
{"namespace": "datasets.iterable_dataset._batch_to_examples", "completion": "    # we order the columns by order of appearance\n    # to do so, we use a dict as an ordered set\n    cols = {col: None for example in batch for col in example}\n    # when an example is missing a column, we set the value to None with .get()\n    examples = [\n        {col: batch.get(col)[idx] for col in cols} for idx in range(len(batch.get(list(cols)[0])))\n    ]\n    return examples\n\n", "idx": 484}
{"namespace": "datasets.iterable_dataset._examples_to_batch", "completion": "    columns = {}\n    for example in examples:\n        for key, value in example.items():\n            if key not in columns:\n                columns[key] = []\n            columns[key].append(value)\n    return {key: list(value) for key, value in columns.items()}\n\n", "idx": 485}
{"namespace": "datasets.iterable_dataset.RandomlyCyclingMultiSourcesExamplesIterable._iter_random_indices", "completion": "        if p is None:\n            p = [1 / num_sources] * num_sources\n        else:\n            if len(p) != num_sources:\n                raise ValueError(\n                    f\"The number of probabilities {len(p)} doesn't match the number of sources {num_sources}.\"\n                )\n            if not np.isclose(sum(p), 1):\n                raise ValueError(f\"The probabilities {p} must sum to 1.\")\n\n        while True:\n            yield from rng.choice(num_sources, random_batch_size, p=p, replace=True)\n", "idx": 486}
{"namespace": "datasets.iterable_dataset.BufferShuffledExamplesIterable._iter_random_indices", "completion": "        while True:\n            yield from rng.integers(0, buffer_size, size=random_batch_size)\n", "idx": 487}
{"namespace": "datasets.iterable_dataset.IterableDataset.remove_columns", "completion": "        if isinstance(column_names, str):\n            column_names = [column_names]\n        return self.map(partial(_remove_columns_fn, column_names=column_names), remove_columns=column_names)\n", "idx": 488}
{"namespace": "datasets.dataset_dict.DatasetDict.with_format", "completion": "        self._check_values_type()\n        return DatasetDict(\n            {\n                k: dataset.with_format(\n                    type=type, columns=columns, output_all_columns=output_all_columns, **format_kwargs\n                )\n                for k, dataset in self.items()\n            }\n        )\n", "idx": 489}
{"namespace": "datasets.dataset_dict.DatasetDict.with_transform", "completion": "        dataset = copy.deepcopy(self)\n        dataset.set_transform(transform=transform, columns=columns, output_all_columns=output_all_columns)\n        return dataset\n", "idx": 490}
{"namespace": "datasets.dataset_dict.DatasetDict.align_labels_with_mapping", "completion": "        self._check_values_type()\n        return DatasetDict({k: dataset.align_labels_with_mapping(label2id=label2id, label_column=label_column) for k, dataset in self.items()})\n", "idx": 491}
{"namespace": "datasets.dataset_dict.IterableDatasetDict.map", "completion": "        self._check_format()\n        if not batched:\n            if with_indices:\n                function = self._add_with_indices(function)\n            if fn_kwargs:\n                function = self._add_fn_kwargs(function, fn_kwargs)\n            if remove_columns:\n                function = self._add_remove_columns(function, remove_columns)\n            if input_columns:\n                function = self._add_input_columns(function, input_columns)\n            return IterableDatasetDict(\n                {k: dataset.map(function, with_indices=with_indices, input_columns=input_columns, fn_kwargs=fn_kwargs) for k, dataset in self.items()}\n            )\n        else:\n            if with_indices:\n                function = self._add_with_indices(function)\n            if fn_kwargs:\n                function = self._add_fn_kwargs(function, fn_kwargs)\n            if remove_columns:\n                function = self._add_remove_columns(function, remove_columns)\n            if input_columns:\n                function = self._add_input_columns(function, input_columns)\n            return IterableDatasetDict(\n                {k: dataset.map(function, with_indices=with_indices, input_columns=input_columns, batched=batched, batch_size=batch_size, drop_last_batch=drop_last_batch, fn_kwargs=fn_kwargs) for k, dataset in self.items()}\n            )\n", "idx": 492}
{"namespace": "datasets.dataset_dict.IterableDatasetDict.filter", "completion": "        return IterableDatasetDict(\n            {\n                k: dataset.filter(\n                    function=function,\n                    with_indices=with_indices,\n                    input_columns=input_columns,\n                    batched=batched,\n                    batch_size=batch_size,\n                    fn_kwargs=fn_kwargs,\n                )\n                for k, dataset in self.items()\n            }\n        )\n", "idx": 493}
{"namespace": "datasets.arrow_dataset.Dataset.num_rows", "completion": "        if self._indices is not None:\n            return len(self._indices)\n        return len(self._data)\n", "idx": 494}
{"namespace": "datasets.filesystems.extract_path_from_uri", "completion": "    if dataset_path.startswith(\"s3://\"):\n        dataset_path = dataset_path.replace(\"s3://\", \"\")\n\n    return dataset_path\n\n", "idx": 495}
{"namespace": "datasets.filesystems.is_remote_filesystem", "completion": "    return fs.protocol != \"file\"\n\n", "idx": 496}
{"namespace": "datasets.utils.file_utils.hash_url_to_filename", "completion": "    # If the etag is not None, then append it to the end of the URL's hash.\n    if etag is not None:\n        etag = etag.replace('\"', \"\")  # Remove the quotes around the etag that Requests may add to the etag parameter\n        base_filename = hashlib.md5((\"{url}.{etag}\".format(url=url, etag=etag)).encode()).hexdigest()\n    else:\n        base_filename = hashlib.md5(url.encode()).hexdigest()\n\n    # If the filename is .h5 (Keras HDF5 weights), prepend 'h5.' to the filename\n    if url.endswith(\".h5\"):\n        return \"h5.\" + base_filename + \".h5\"\n    else:\n        return base_filename\n\n", "idx": 497}
{"namespace": "datasets.utils.hub.hf_hub_url", "completion": "    # Check the version of the Hugging Face Hub\n    hf_hub_version = version.parse(hfh.__version__)\n\n    # If the version is older than 0.11.0, encode the file path\n    if hf_hub_version < version.parse(\"0.11.0\"):\n        path = quote(path)\n\n    # Return the URL of the file in the Hugging Face Hub\n    return hfh.hf_hub_url(repo_id, path, revision)", "idx": 498}
{"namespace": "datasets.utils.sharding._number_of_shards_in_gen_kwargs", "completion": "    # get the number of shards\n    num_shards = 1\n    for key, value in gen_kwargs.items():\n        if isinstance(value, list):\n            if len(value) > 1:\n                num_shards *= len(value)\n            else:\n                raise ValueError(\"The length of the list in the gen_kwargs must be greater than 1.\")\n        else:\n            raise ValueError(\"The gen_kwargs must contain only lists.\")\n\n    return num_shards\n\n", "idx": 499}
{"namespace": "datasets.utils.sharding._distribute_shards", "completion": "    # If there are more shards than jobs, then we need to distribute the shards among the jobs.\n    if num_shards > max_num_jobs:\n        # The number of shards per job is the ceiling of the number of shards divided by the number of jobs.\n        num_shards_per_job = int(np.ceil(num_shards / max_num_jobs))\n        # The number of jobs is the ceiling of the number of shards divided by the number of shards per job.\n        num_jobs = int(np.ceil(num_shards / num_shards_per_job))\n        # The number of shards per job is the ceiling of the number of shards divided by the number of jobs.\n        num_shards_per_job = int(np.ceil(num_shards / num_jobs))\n        # The number of shards per job is the ceiling of the number of shards divided by the number of jobs.\n        num_jobs = int(np.ceil(num_shards / num_shards_per_job))\n        # The number of shards per job is the ceiling of the number of shards divided by the number of jobs.\n        num_shards_per_job = int(np.ceil(num_shards / num_jobs))\n        # The number of shards per job is the ceiling of the number of shards divided by the number of jobs.\n        num_jobs = int(np.ceil(num_shards / num_shards_per_job))\n        # The number of shards per job is the ceiling of the number of shards divided by the number of jobs.\n        num_shards_per_job = int(np.ceil(num_shards / num_jobs))\n        # The number of shards per job is the ceiling of the number of shards divided by the number of jobs.\n        num_jobs = int(np.ceil(num_shards / num_shards_per_job))\n        # The number of shards per job is the ceiling of the number of shards divided by the number of", "idx": 500}
{"namespace": "datasets.utils.py_utils.temporary_assignment", "completion": "    assert hasattr(obj, attr)\n    original_value = getattr(obj, attr)\n    setattr(obj, attr, value)\n    try:\n        yield\n    finally:\n        setattr(obj, attr, original_value)\n\n", "idx": 501}
{"namespace": "datasets.utils.extract.TarExtractor.extract", "completion": "        # Create the output directory if it does not exist\n        if not os.path.exists(output_path):\n            os.makedirs(output_path)\n\n        # Open the tar file\n        with tarfile.open(input_path) as tar:\n            # Extract all the contents of the tar file to the output path\n            tar.extractall(output_path, members=TarExtractor.safemembers(tar, output_path))\n\n        # Close the tar file\n        tar.close()\n\n", "idx": 502}
{"namespace": "datasets.utils.extract.Extractor.infer_extractor_format", "completion": "        magic_number_max_length = cls._get_magic_number_max_length()\n        magic_number = cls._read_magic_number(path, magic_number_max_length)\n        for extractor_format, extractor in cls.extractors.items():\n            if issubclass(extractor, MagicNumberBaseExtractor) and extractor.is_extractable(path, magic_number):\n                return extractor_format\n        return \"\"\n", "idx": 503}
{"namespace": "datasets.utils.py_utils.asdict", "completion": "    if is_dataclass(obj):\n        return asdict(obj)\n    elif isinstance(obj, (list, tuple)):\n        return [asdict(item) for item in obj]\n    elif isinstance(obj, dict):\n        return {key: asdict(value) for key, value in obj.items()}\n    elif isinstance(obj, types.FunctionType):\n        return obj.__name__\n    elif isinstance(obj, np.ndarray):\n        return obj.tolist()\n    elif isinstance(obj, np.integer):\n        return int(obj)\n    elif isinstance(obj, np.floating):\n        return float(obj)\n    elif isinstance(obj, np.bool_):\n        return bool(obj)\n    elif isinstance(obj, np.void):\n        return None\n    elif isinstance(obj, np.void):\n        return None\n    elif isinstance(obj, np.str_):\n        return str(obj)\n    elif isinstance(obj, np.bytes_):\n        return bytes(obj)\n    elif isinstance(obj, np.complexfloating):\n        return complex(obj)\n    elif isinstance(obj, np.object_):\n        return str(obj)\n    elif isinstance(obj, np.unicode_):\n        return str(obj)\n    elif isinstance(obj, np.character):\n        return str(obj)\n    elif isinstance(obj, np.flexible):\n        return str(obj)\n    elif isinstance(obj, np.void):\n        return None\n    elif isinstance(obj, np.recarray):\n        return asdict(obj.tolist())\n    elif isinstance(obj, np.memmap):\n        return asdict(obj.tolist())\n    elif isinstance(obj, np.dtype):\n        return str(obj)\n    elif isinstance(obj, np.ufunc):\n        return str(obj)\n    elif isinstance(obj, np.lib.ufunc_identity_dict):\n        return str(obj)\n    elif isinstance(obj, np.lib.NumpyVersion):\n        return", "idx": 504}
{"namespace": "datasets.utils.metadata.MetadataConfigs.from_dataset_card_data", "completion": "        metadata_configs = dataset_card_data.get(cls.FIELD_NAME)\n        if metadata_configs is not None:\n            metadata_configs = cls(metadata_configs)\n            for metadata_config in metadata_configs.values():\n                cls._raise_if_data_files_field_not_valid(metadata_config)\n        return metadata_configs\n", "idx": 505}
{"namespace": "pymorphy2.analyzer.lang_dict_path", "completion": "    paths = _lang_dict_paths()\n    if lang in paths:\n        return paths[lang]\n    else:\n        raise ValueError(\"No dictionary found for language: \" + lang)\n\n", "idx": 506}
{"namespace": "pymorphy2.dawg.assert_can_create", "completion": "    if not EXTENSION_AVAILABLE:\n        raise NotImplementedError(\"The extension is not available.\")\n\n", "idx": 507}
{"namespace": "pymorphy2.opencorpora_dict.compile._to_paradigm", "completion": "    # Extract prefixes from each word form.\n    prefixes = []\n    for word_form, tag in lexeme:\n        prefix = word_form.split('-')[0]\n        prefixes.append(prefix)\n\n    # Check if any prefix is not in the paradigm prefixes.\n    if not all(prefix in paradigm_prefixes for prefix in prefixes):\n        # If any prefix is not in the paradigm prefixes, set the stem to an empty string and assign empty prefixes to all word forms.\n        stem = ''\n        prefixes = [''] * len(lexeme)\n    else:\n        # If all prefixes are in the paradigm prefixes, extract the stem from the first word form.\n        stem = lexeme[0][0].split('-')[0]\n\n    # Extract suffixes from each word form.\n    suffixes = []\n    for word_form, tag in lexeme:\n        suffix = word_form.split('-')[1:]\n        suffixes.append(suffix)\n\n    # Create a tuple of suffixes, tags, and prefixes.\n    paradigm = []\n    for suffix, tag, prefix in zip(suffixes, [tag for word_form, tag in lexeme], prefixes):\n        paradigm.append((suffix, tag, prefix))\n\n    return stem, paradigm\n\n", "idx": 508}
{"namespace": "pymorphy2.units.by_analogy.KnownPrefixAnalyzer.tag", "completion": "        result = []\n        for prefix, unprefixed_word in self.possible_splits(word_lower):\n            tag = self.morph.tag(unprefixed_word)\n            if tag.is_productive():\n                add_tag_if_not_seen(tag, result, seen_tags)\n\n        return result\n", "idx": 509}
{"namespace": "pymorphy2.units.by_analogy.UnknownPrefixAnalyzer.tag", "completion": "        result = []\n        for prefix, unprefixed_word in word_splits(word_lower):\n            tags = self.dict_analyzer.tag(unprefixed_word, unprefixed_word, seen_tags)\n            for tag in tags:\n                if tag.is_productive():\n                    add_tag_if_not_seen(tag, result, seen_tags)\n        return result", "idx": 510}
{"namespace": "benedict.dicts.keylist.keylist_util.get_item", "completion": "    if not type_util.is_list_or_tuple(keys):\n        keys = [keys]\n    for key in keys:\n        if key in d:\n            d = d[key]\n        else:\n            return (None,)*len(keys)\n    return d\n\n", "idx": 511}
{"namespace": "benedict.dicts.keylist.keylist_util.set_item", "completion": "    item = d\n    for key in keys[:-1]:\n        try:\n            item = item[key]\n        except KeyError:\n            item[key] = {}\n            item = item[key]\n    item[keys[-1]] = value\n\n", "idx": 512}
{"namespace": "benedict.dicts.keypath.keypath_util._split_key_indexes", "completion": "    if re.search(KEY_INDEX_RE, key):\n        return re.findall(KEY_INDEX_RE, key)\n    return [key]\n\n", "idx": 513}
{"namespace": "feedparser.urls.make_safe_absolute_uri", "completion": "    if not ACCEPTABLE_URI_SCHEMES:\n        return ''\n    if not base:\n        return rel or ''\n    if not rel:\n        return base\n    base = convert_to_idn(base)\n    rel = convert_to_idn(rel)\n    uri = _urljoin(base, rel)\n    if uri.split(':', 1)[0] not in ACCEPTABLE_URI_SCHEMES:\n        return ''\n    return uri\n\n", "idx": 514}
{"namespace": "feedparser.api._open_resource", "completion": "    if isinstance(url_file_stream_or_string, str):\n        if url_file_stream_or_string.startswith('http:') or url_file_stream_or_string.startswith('https:'):\n            if not handlers:\n                handlers = []\n            if not agent:\n                agent = 'feedparser/' + __version__\n            if not referrer:\n                referrer = url_file_stream_or_string\n            if not request_headers:\n                request_headers = {}\n            if etag:\n                request_headers['If-None-Match'] = etag\n            if modified:\n                if isinstance(modified, str):\n                    request_headers['If-Modified-Since'] = modified\n                else:\n                    request_headers['If-Modified-Since'] = rfc822.formatdate(modified)\n            request_headers['User-Agent'] = agent\n            request_headers['Referer'] = referrer\n            opener = urllib.request.build_opener(*handlers)\n            for key, val in request_headers.items():\n                request_headers[key.capitalize()] = val\n            opener.addheaders = list(request_headers.items())\n            try:\n                result = opener.open(url_file_stream_or_string)\n            except urllib.error.HTTPError as e:\n                if e.code == 304:\n                    result = e\n                else:\n                    raise\n        else:\n            if url_file_stream_or_string.startswith('file:'):\n                url_file_stream_or_string = url_file_stream_or_string[5:]\n            if url_file_stream_or_string:\n                if hasattr(url_file_stream_or_string, 'read'):\n                    result = url_file_stream_or_string\n                else:\n                    try:\n                        result = open(url_file_stream_or_string, 'rb')\n                    except IOError as e:\n                        raise FeedParserException(e)\n    if hasattr", "idx": 515}
{"namespace": "feedparser.http._build_urllib2_request", "completion": "    # Create a request object\n    request = urllib.request.Request(url)\n\n    # Add headers to the request\n    if agent:\n        request.add_header('User-Agent', agent)\n    if accept_header:\n        request.add_header('Accept', accept_header)\n    if etag:\n        request.add_header('If-None-Match', etag)\n    if modified:\n        if isinstance(modified, datetime.datetime):\n            modified = modified.utctimetuple()\n        if isinstance(modified, tuple):\n            modified = rfc822.formatdate(rfc822.mktime_tz(modified))\n        request.add_header('If-Modified-Since', modified)\n    if referrer:\n        request.add_header('Referer', referrer)\n    if auth:\n        request.add_header('Authorization', auth)\n    if request_headers:\n        for header in request_headers:\n            request.add_header(header, request_headers[header])\n\n    return request\n\n", "idx": 516}
{"namespace": "pylatex.utils.dumps_list", "completion": "    if escape:\n        l = [escape_latex(str(item)) for item in l]\n    else:\n        l = [str(item) for item in l]\n\n    if mapper is not None:\n        if not _is_iterable(mapper):\n            mapper = [mapper]\n\n        for func in mapper:\n            l = [func(item) for item in l]\n\n    if as_content:\n        l = [item.dumps_as_content() for item in l]\n    else:\n        l = [item.dumps() for item in l]\n\n    return NoEscape(token.join(l))\n\n", "idx": 517}
{"namespace": "pylatex.utils._latex_item_to_string", "completion": "    if isinstance(item, pylatex.base_classes.LatexObject):\n        if as_content:\n            return item.dumps_as_content()\n        else:\n            return item.dumps()\n    else:\n        if escape:\n            return escape_latex(str(item))\n        else:\n            return str(item)\n\n", "idx": 518}
{"namespace": "mistune.markdown.Markdown.read", "completion": "        if state is None:\n            state = self.block.state_cls()\n\n        with open(filepath, 'r', encoding=encoding) as f:\n            content = f.read()\n\n        return self.parse(content, state)\n", "idx": 519}
{"namespace": "mistune.create_markdown", "completion": "    if renderer == 'html':\n        renderer = HTMLRenderer(escape=escape, hard_wrap=hard_wrap)\n    elif isinstance(renderer, str):\n        renderer = import_plugin(renderer)\n\n    md = Markdown(renderer)\n\n    if plugins:\n        for plugin in plugins:\n            if isinstance(plugin, str):\n                plugin = import_plugin(plugin)\n            plugin(md)\n\n    return md\n\n", "idx": 520}
{"namespace": "parsel.utils.extract_regex", "completion": "    if isinstance(regex, str):\n        regex = re.compile(regex)\n\n    if \"extract\" in regex.groupindex:\n        group = regex.groupindex[\"extract\"]\n    else:\n        group = 0\n\n    strings = []\n    for match in regex.finditer(text):\n        if group == 0:\n            value = match.group(0)\n        else:\n            value = match.group(group)\n\n        if replace_entities:\n            value = w3lib_replace_entities(value)\n\n        strings.append(value)\n\n    return strings\n\n", "idx": 521}
{"namespace": "dominate.dom_tag.dom_tag.render", "completion": "    # If the tag is a single tag, return the tag name\n    if self.is_single:\n      return self.name\n\n    # If the tag is a comment, return the comment\n    if self.name == '!--':\n      return self.children[0]\n\n    # If the tag is a doctype, return the doctype\n    if self.name == '!DOCTYPE':\n      return '<!DOCTYPE %s>' % self.children[0]\n\n    # If the tag is a CDATA, return the CDATA\n    if self.name == '!CDATA':\n      return '<![CDATA[%s]]>' % self.children[0]\n\n    # If the tag is a processing instruction, return the instruction\n    if self.name == '?':\n      return '<?%s?>' % self.children[0]\n\n    # If the tag is a declaration, return the declaration\n    if self.name == '!':\n      return '<!%s>' % self.children[0]\n\n    # If the tag is a declaration, return the declaration\n    if self.name == '::':\n      return '::%s::' % self.children[0]\n\n    # If the tag is a declaration, return the declaration\n    if self.name == ':::':\n      return ':::%s:::' % self.children[0]\n\n    # If the tag is a declaration, return the declaration\n    if self.name == '::::':\n      return '::::%s::::' % self.children[0]\n\n    # If the tag is a declaration, return the declaration\n    if self.name == ':::::':\n      return ':::::%s:::::' % self.children[0]\n\n    # If the tag is a declaration, return the declaration\n    if self.name == '::::::':\n      return '::::::%s::::::' % self.children[0]\n\n    # If the tag is a declaration, return the declaration\n    if self.name == ':::::::':\n      return ':::::::%s:::::::' % self.children[0]\n\n   ", "idx": 522}
{"namespace": "dominate.util.include", "completion": "  with open(f, 'r') as fp:\n    return fp.read()\n\n", "idx": 523}
{"namespace": "dominate.util.unescape", "completion": "  def unescape_match(match):\n    \"\"\"\n    This function is used to unescape HTML entities in the given data.\n    :param match: The match object of the regular expression.\n    :return: The unescaped string.\n    \"\"\"\n    if match.group(1) is not None:\n      return unichr(_unescape[match.group(1)])\n    else:\n      return unichr(_unescape[match.group(2)])\n\n  return re.sub(r'&#?(\\w+);', unescape_match, data)\n\n", "idx": 524}
{"namespace": "onlinejudge_command.pretty_printers._tokenize_line", "completion": "    tokens = []\n    l = 0\n    while l < len(line):\n        r = l + 1\n        while r < len(line) and (line[l] in ' \\t') == (line[r] in ' \\t'):\n            r += 1\n        if line[l] in ' \\t':\n            typ = _PrettyTokenType.WHITESPACE\n        elif line[l] in '\\r\\n':\n            typ = _PrettyTokenType.NEWLINE\n        else:\n            typ = _PrettyTokenType.BODY\n        tokens.append(_PrettyToken(typ, line[l:r]))\n        l = r\n    return tokens\n\n", "idx": 525}
{"namespace": "onlinejudge_command.pretty_printers._render_tokens", "completion": "    if font_bold is None:\n        font_bold = lambda s: s\n    if font_dim is None:\n        font_dim = lambda s: s\n    if font_red is None:\n        font_red = lambda s: s\n    if font_blue is None:\n        font_blue = lambda s: s\n    if font_normal is None:\n        font_normal = lambda s: s\n\n    rendered = []\n    for token in tokens:\n        if token.type == _PrettyTokenType.BODY:\n            rendered.append(font_normal(token.value))\n        elif token.type == _PrettyTokenType.BODY_HIGHLIGHT_LEFT:\n            rendered.append(font_red(token.value))\n        elif token.type == _PrettyTokenType.BODY_HIGHLIGHT_RIGHT:\n            rendered.append(font_blue(token.value))\n        elif token.type == _PrettyTokenType.WHITESPACE:\n            rendered.append(font_dim(_replace_whitespace(token.value)))\n        elif token.type == _PrettyTokenType.NEWLINE:\n            rendered.append(token.value)\n        elif token.type == _PrettyTokenType.HINT:\n            rendered.append(font_dim(token.value))\n        elif token.type == _PrettyTokenType.LINENO:\n            rendered.append(font_dim('{:4} '.format(token.value)))\n        elif token.type == _PrettyTokenType.OTHERS:\n            rendered.append(font_dim(token.value))\n        else:\n            raise AssertionError\n    return ''.join(rendered)\n\n", "idx": 526}
{"namespace": "onlinejudge_command.pretty_printers._tokenize_file_content_without_snipping", "completion": "    tokens = []\n    try:\n        text = content.decode()\n    except UnicodeDecodeError as e:\n        tokens.append(_PrettyToken(_PrettyTokenType.HINT, str(e)))\n        text = content.decode(errors='replace')\n    for line in text.splitlines(keepends=True):\n        tokens += _tokenize_line(line)\n    tokens = _warn_if_empty(tokens)\n    return tokens\n\n", "idx": 527}
{"namespace": "jinja2.environment.Environment.get_template", "completion": "        if isinstance(name, Template):\n            return name\n\n        if parent is not None:\n            name = self.join_path(name, parent)\n\n        if globals is None:\n            globals = {}\n\n        # if the template is already in the cache we return it instead of\n        # loading it from the file system again.\n        return self._load_template(name, globals)\n", "idx": 528}
{"namespace": "jinja2.environment.Environment.from_string", "completion": "        if template_class is None:\n            template_class = self.template_class\n\n        if isinstance(source, str):\n            source = nodes.Template(self.parse(source), filename=\"<string>\")\n\n        return template_class(self, source, globals)\n", "idx": 529}
{"namespace": "jinja2.environment.Template.render", "completion": "        if self.environment.is_async:\n            return self.root_render_func(\n                *args, **kwargs\n            )\n        else:\n            return self.root_render_func(\n                *args, **kwargs\n            )\n", "idx": 530}
{"namespace": "jinja2.utils.generate_lorem_ipsum", "completion": "    from random import randint\n\n    # Create a list of words\n    words = [\n        \"lorem\",\n        \"ipsum\",\n        \"dolor\",\n        \"sit\",\n        \"amet\",\n        \"consectetur\",\n        \"adipiscing\",\n        \"elit\",\n        \"sed\",\n        \"do\",\n        \"eiusmod\",\n        \"tempor\",\n        \"incididunt\",\n        \"ut\",\n        \"labore\",\n        \"et\",\n        \"dolore\",\n        \"magna\",\n        \"aliqua\",\n    ]\n\n    # Create a list of endings for words\n    endings = [\".\", \".\", \"?\", \"!\", \"?\", \"!\", \".\", \".\", \"?\", \"!\", \"?\", \"!\", \".\", \"?\", \"!\"]\n\n    # Create a list of commas\n    commas = [\"\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",", "idx": 531}
{"namespace": "jinja2.utils.LRUCache.clear", "completion": "        self._wlock.acquire()\n        self._mapping.clear()\n        self._queue.clear()\n        self._wlock.release()\n", "idx": 532}
{"namespace": "jinja2.utils.LRUCache.items", "completion": "        with self._wlock:\n            return [(key, self._mapping[key]) for key in reversed(self._queue)]\n", "idx": 533}
{"namespace": "jinja2.idtracking.symbols_for_node", "completion": "    sym = Symbols(parent=parent_symbols)\n    visitor = FrameSymbolVisitor(sym)\n    visitor.visit(node)\n    return sym\n\n", "idx": 534}
{"namespace": "jinja2.idtracking.Symbols.find_ref", "completion": "        if name in self.refs:\n            return self.refs[name]\n\n        if self.parent is not None:\n            return self.parent.find_ref(name)\n\n        return None\n", "idx": 535}
{"namespace": "jinja2.idtracking.Symbols.dump_stores", "completion": "        stores = {}\n        for sym in self.stores:\n            stores[sym] = sym\n        if self.parent is not None:\n            stores.update(self.parent.dump_stores())\n        return stores\n\n", "idx": 536}
{"namespace": "jinja2.meta.find_undeclared_variables", "completion": "    code = TrackingCodeGenerator(ast.environment)\n    code.visit(ast)\n    return code.undeclared_identifiers", "idx": 537}
{"namespace": "jinja2.loaders.split_template_path", "completion": "    if (\n        (os.path.sep in template or os.path.altsep in template)\n        and (os.path.sep == \"\\\\\" or os.path.altsep == \"\\\\\")\n        and (os.path.sep in template or os.path.altsep in template)\n    ):\n        raise TemplateNotFound(template)\n    return template.split(os.path.sep)\n\n", "idx": 538}
{"namespace": "jinja2.bccache.MemcachedBytecodeCache.load_bytecode", "completion": "        key = self.prefix + bucket.key\n        try:\n            data = self.client.get(key)\n        except Exception as e:\n            if not self.ignore_memcache_errors:\n                raise e\n            return\n        if data is None:\n            return\n        bucket.bytecode_from_string(data)\n", "idx": 539}
{"namespace": "jinja2.bccache.MemcachedBytecodeCache.dump_bytecode", "completion": "        try:\n            self.client.set(\n                self.prefix + bucket.key, bucket.bytecode_to_string(), self.timeout\n            )\n        except Exception:\n            if not self.ignore_memcache_errors:\n                raise", "idx": 540}
{"namespace": "sumy.utils.get_stop_words", "completion": "    language = normalize_language(language)\n    data_path = expand_resource_path(\"stopwords/%s.txt\" % language)\n    if not exists(data_path):\n        raise LookupError(\"Stop words for language '%s' are not available.\" % language)\n\n    with open(data_path, \"r\") as stop_words_stream:\n        stop_words = frozenset(\n            to_unicode(word.strip()) for word in stop_words_stream\n        )\n    return stop_words\n\n", "idx": 541}
{"namespace": "sumy._compat.to_bytes", "completion": "    if isinstance(object, bytes):\n        return object\n    if isinstance(object, unicode):\n        return object.encode(\"utf-8\")\n    return to_bytes(object.__str__())\n\n", "idx": 542}
{"namespace": "sumy._compat.to_unicode", "completion": "    if isinstance(object, unicode):\n        return object\n    elif isinstance(object, bytes):\n        return object.decode(\"utf-8\")\n    else:\n        # try decode instance to unicode\n        return instance_to_unicode(object)\n\n", "idx": 543}
{"namespace": "sumy.summarizers.lsa.LsaSummarizer._create_dictionary", "completion": "        words = self._infer_tokens(document.words)\n        words = self._tokens_to_lower_case(words)\n        words = self._filter_out_stop_words(words)\n        words = self._filter_out_short_words(words)\n        return dict(zip(words, range(len(words))))\n", "idx": 544}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._get_content_words_in_sentence", "completion": "        # Get the words in the sentence.\n        words = sentence.words\n\n        # Normalize the words.\n        words = [self.normalize_word(w) for w in words]\n\n        # Filter out stop words.\n        words = [w for w in words if w not in self.stop_words]\n\n        # Stem the words.\n        words = [self.stem_word(w) for w in words]\n\n        return words\n", "idx": 545}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._get_all_content_words_in_doc", "completion": "        # Get all words in the sentences\n        all_words_in_doc = self._get_all_words_in_doc(sentences)\n\n        # Filter out the stop words\n        content_words_in_doc = self._filter_out_stop_words(all_words_in_doc)\n\n        # Normalize the content words\n        normalized_content_words_in_doc = self._normalize_words(content_words_in_doc)\n\n        return normalized_content_words_in_doc\n", "idx": 546}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_tf", "completion": "        content_words = self._get_all_content_words_in_doc(sentences)\n        content_word_freq = self._compute_word_freq(content_words)\n        return self._normalize_scores(content_word_freq)\n", "idx": 547}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_ratings", "completion": "        word_freq = self._compute_tf(sentences)\n        sentences_as_words = [self._get_content_words_in_sentence(s) for s in sentences]\n        ratings = {}\n        for i in range(len(sentences)):\n            best_sentence_index = self._find_index_of_best_sentence(word_freq, sentences_as_words)\n            best_sentence = sentences[best_sentence_index]\n            ratings[best_sentence] = -i\n            word_freq = self._update_tf(word_freq, sentences_as_words[best_sentence_index])\n            del sentences[best_sentence_index]\n            del sentences_as_words[best_sentence_index]\n        return ratings", "idx": 548}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.cue_method", "completion": "        self.bonus_words = self.bonus_words | set(document.bonus_words)\n        self.stigma_words = self.stigma_words | set(document.stigma_words)\n        self._cue_weight = 1.0\n        self._key_weight = 0.0\n        self._title_weight = 0.0\n        self._location_weight = 0.0\n        return self(document, sentences_count)\n", "idx": 549}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.key_method", "completion": "        summarization_method = self._build_key_method_instance()\n        return summarization_method(document, sentences_count, weight)\n", "idx": 550}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.title_method", "completion": "        summarization_method = self._build_title_method_instance()\n        return summarization_method(document, sentences_count)\n", "idx": 551}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.location_method", "completion": "        summarization_method = self._build_location_method_instance()\n        return summarization_method(document, sentences_count, w_h, w_p1, w_p2, w_s1, w_s2)\n", "idx": 552}
{"namespace": "sumy.summarizers.reduction.ReductionSummarizer.rate_sentences", "completion": "        ratings = defaultdict(float)\n        sentences = document.sentences\n        sentence_count = len(sentences)\n\n        for i, first in enumerate(sentences):\n            for j, second in enumerate(sentences[i+1:], i+1):\n                # Don't compare a sentence with itself\n                if first == second:\n                    continue\n\n                # Calculate the similarity between two sentences\n                similarity = self.similarity(first, second)\n\n                # Assign the rating to the first sentence\n                ratings[first] += similarity\n\n                # Assign the rating to the second sentence\n                ratings[second] += similarity\n\n        # Normalize the ratings\n        max_rating = max(ratings.values())\n        min_rating = min(ratings.values())\n        for sentence in sentences:\n            ratings[sentence] = (ratings[sentence] - min_rating) / (max_rating - min_rating)\n\n        return ratings\n", "idx": 553}
{"namespace": "sumy.summarizers.reduction.ReductionSummarizer._to_words_set", "completion": "        words = map(self.normalize_word, sentence.words)\n        return {w for w in words if w not in self.stop_words}\n", "idx": 554}
{"namespace": "sumy.summarizers.text_rank.TextRankSummarizer._to_words_set", "completion": "        words = map(self.normalize_word, sentence.words)\n        words = [word for word in words if word not in self._stop_words]\n        return words\n", "idx": 555}
{"namespace": "sumy.summarizers.kl.KLSummarizer.compute_tf", "completion": "        content_words = self._get_all_content_words_in_doc(sentences)\n        content_word_freq = self._compute_word_freq(content_words)\n        total_content_words = len(content_words)\n\n        for word in content_word_freq:\n            content_word_freq[word] = content_word_freq[word] / total_content_words\n\n        return content_word_freq\n", "idx": 556}
{"namespace": "sumy.evaluation.rouge._get_word_ngrams", "completion": "    assert (n > 0)\n    assert (sentences)\n    assert (isinstance(n, int))\n    assert (isinstance(sentences, list))\n    assert (isinstance(sentences[0], Sentence))\n\n    words = _split_into_words(sentences)\n    return _get_ngrams(n, words)\n\n", "idx": 557}
{"namespace": "sumy.evaluation.rouge._len_lcs", "completion": "    m = len(x)\n    n = len(y)\n\n    table = dict()\n    for i in range(m + 1):\n        for j in range(n + 1):\n            if i == 0 or j == 0:\n                table[i, j] = 0\n            elif x[i - 1] == y[j - 1]:\n                table[i, j] = table[i - 1, j - 1] + 1\n            else:\n                table[i, j] = max(table[i - 1, j], table[i, j - 1])\n\n    return table[m, n]\n\n", "idx": 558}
{"namespace": "sumy.evaluation.rouge._recon_lcs", "completion": "    i, j = _get_index_of_lcs(x, y)\n    return _recon_lcs_helper(x, y, i, j)\n\n", "idx": 559}
{"namespace": "sumy.evaluation.rouge._union_lcs", "completion": "    combined_lcs_score = 0\n    reference_words = _split_into_words([reference_sentence])\n    for eval_s in evaluated_sentences:\n        evaluated_words = _split_into_words([eval_s])\n        lcs = set(_recon_lcs(reference_words, evaluated_words))\n        combined_lcs_score += len(lcs)\n\n    return combined_lcs_score / (len(evaluated_sentences) + len(reference_words))\n\n", "idx": 560}
{"namespace": "sumy.parsers.html.HtmlParser.from_file", "completion": "        with open(file_path, \"r\") as f:\n            return cls(f.read(), tokenizer, url)\n", "idx": 561}
{"namespace": "sumy.parsers.plaintext.PlaintextParser.document", "completion": "        # Create a document model object\n        document = ObjectDocumentModel()\n\n        # Initialize the current paragraph\n        paragraph = Paragraph()\n\n        # Initialize the current sentence\n        sentence = Sentence()\n\n        # Initialize the current heading\n        heading = None\n\n        # Initialize the current paragraph's heading\n        paragraph_heading = None\n\n        # Initialize the current paragraph's sentences\n        paragraph_sentences = []\n\n        # Initialize the current paragraph's headings\n        paragraph_headings = []\n\n        # Initialize the current paragraph's paragraphs\n        paragraph_paragraphs = []\n\n        # Initialize the current paragraph's paragraphs\n        paragraph_paragraphs = []\n\n        # Initialize the current paragraph's paragraphs\n        paragraph_paragraphs = []\n\n        # Initialize the current paragraph's paragraphs\n        paragraph_paragraphs = []\n\n        # Initialize the current paragraph's paragraphs\n        paragraph_paragraphs = []\n\n        # Initialize the current paragraph's paragraphs\n        paragraph_paragraphs = []\n\n        # Initialize the current paragraph's paragraphs\n        paragraph_paragraphs = []\n\n        # Initialize the current paragraph's paragraphs\n        paragraph_paragraphs = []\n\n        # Initialize the current paragraph's paragraphs\n        paragraph_paragraphs = []\n\n        # Initialize the current paragraph's paragraphs\n        paragraph_paragraphs = []\n\n        # Initialize the current paragraph's paragraphs\n        paragraph_paragraphs = []\n\n        # Initialize the current paragraph's paragraphs\n        paragraph_paragraphs = []\n\n        # Initialize the current paragraph's paragraphs\n        paragraph_paragraphs = []\n\n        # Initialize the current paragraph's paragraphs\n        paragraph_paragraphs = []\n\n        # Initialize the current paragraph's paragraphs\n        paragraph_paragraphs = []\n\n        # Initialize the current paragraph's paragraphs\n        paragraph_paragraphs = []\n\n        # Initialize the current paragraph's paragraphs\n        paragraph_paragraphs = []\n\n        # Initialize the current paragraph's paragraphs\n        paragraph_paragraphs = []\n\n        # Initialize the current paragraph's paragraphs\n        paragraph_paragraphs = []", "idx": 562}
{"namespace": "sumy.nlp.tokenizers.Tokenizer.to_sentences", "completion": "        # update abbreviations\n        if self._language in self.LANGUAGE_EXTRA_ABREVS:\n            abbreviations = self._word_tokenizer._WORD_PATTERN.abbrev_dict\n            for abbr in self.LANGUAGE_EXTRA_ABREVS[self._language]:\n                abbreviations[abbr] = 1\n\n        # tokenize\n        sentences = self._sentence_tokenizer.tokenize(paragraph)\n        return sentences\n", "idx": 563}
{"namespace": "sumy.nlp.stemmers.null_stemmer", "completion": "    return str(object).lower()\n\n", "idx": 564}
{"namespace": "rows.fields.BinaryField.serialize", "completion": "        if value is None:\n            return \"\"\n        elif isinstance(value, six.binary_type):\n            try:\n                return b64encode(value).decode(\"ascii\")\n            except:\n                return value\n        else:\n            value_error(value, cls)\n", "idx": 565}
{"namespace": "rows.fields.BoolField.deserialize", "completion": "        value = super(BoolField, cls).deserialize(value, *args, **kwargs)\n        if value is None or isinstance(value, bool):\n            return value\n        elif isinstance(value, six.string_types):\n            value = value.lower()\n            if value in cls.TRUE_VALUES:\n                return True\n            elif value in cls.FALSE_VALUES:\n                return False\n            else:\n                raise ValueError(\"Value is not boolean\")\n        else:\n            value_error(value, cls)\n\n", "idx": 566}
{"namespace": "rows.fields.DateField.serialize", "completion": "        if value is None:\n            return \"\"\n        elif isinstance(value, datetime.date):\n            return value.strftime(cls.OUTPUT_FORMAT)\n        else:\n            value_error(value, cls)\n", "idx": 567}
{"namespace": "rows.fields.DateField.deserialize", "completion": "        value = super(DateField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n        try:\n            return datetime.datetime.strptime(value, cls.INPUT_FORMAT).date()\n        except ValueError:\n            value_error(value, cls)\n\n", "idx": 568}
{"namespace": "rows.fields.TextField.deserialize", "completion": "        value = super(TextField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            return six.text_type(value)\n\n", "idx": 569}
{"namespace": "rows.fields.EmailField.deserialize", "completion": "        value = super(EmailField, cls).deserialize(value)\n        if value is None or value == \"\":\n            return None\n\n        if cls.EMAIL_REGEXP.match(value):\n            return value\n        else:\n            raise ValueError(\"Value is not a valid email\")\n\n", "idx": 570}
{"namespace": "rows.fields.JSONField.deserialize", "completion": "        value = super(JSONField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            return json.loads(value)\n\n", "idx": 571}
{"namespace": "falcon.http_error.HTTPError.to_json", "completion": "        if handler is None:\n            handler = JSONHandler()\n\n        return handler.serialize(self.to_dict())\n", "idx": 572}
{"namespace": "falcon.inspect.inspect_app", "completion": "    return AppInfo(app)\n\n", "idx": 573}
{"namespace": "falcon.inspect.inspect_routes", "completion": "    routes = []\n    for route in app._router._roots:\n        routes.extend(inspect_route(route))\n    return routes\n\n", "idx": 574}
{"namespace": "falcon.inspect.inspect_static_routes", "completion": "    static_routes = []\n    for route in app._static_routes:\n        static_routes.append(\n            StaticRouteInfo(\n                route.uri_template,\n                route.directory,\n                route.fallback_filename,\n                route.name,\n            )\n        )\n    return static_routes\n\n", "idx": 575}
{"namespace": "falcon.inspect.inspect_sinks", "completion": "    sinks = []\n    for sink in app._sinks:\n        info = SinkInfo(sink[0], sink[1])\n        sinks.append(info)\n    return sinks\n\n", "idx": 576}
{"namespace": "falcon.inspect.inspect_error_handlers", "completion": "    error_handlers = []\n    for exception, handler in app._error_handlers:\n        source_info, name = _get_source_info_and_name(handler)\n        info = ErrorHandlerInfo(exception, name, source_info)\n        error_handlers.append(info)\n    return error_handlers\n\n", "idx": 577}
{"namespace": "falcon.inspect.inspect_middleware", "completion": "    middleware_info = MiddlewareInfo()\n    middleware_info.prepare_middleware(app)\n    return middleware_info\n\n", "idx": 578}
{"namespace": "falcon.inspect.InspectVisitor.process", "completion": "        visit_name = instance.__visit_name__\n        visit_method = getattr(self, 'visit_' + visit_name, None)\n        if visit_method is None:\n            raise RuntimeError('No visit method for {}'.format(visit_name))\n        return visit_method(instance)\n\n", "idx": 579}
{"namespace": "falcon.request.Request.forwarded", "completion": "        if self._cached_forwarded is None:\n            self._cached_forwarded = Forwarded.parse(self.env.get('HTTP_FORWARDED'))\n        return self._cached_forwarded\n", "idx": 580}
{"namespace": "falcon.request.Request.client_accepts_msgpack", "completion": "        return self.client_accepts('application/x-msgpack') or self.client_accepts('application/msgpack')\n", "idx": 581}
{"namespace": "falcon.request.Request.content_length", "completion": "        try:\n            content_length = self.env['CONTENT_LENGTH']\n            if content_length is not None:\n                content_length = int(content_length)\n                if content_length < 0:\n                    raise ValueError\n            return content_length\n        except (KeyError, ValueError):\n            raise falcon.HTTPInvalidHeader('Content-Length', content_length)\n", "idx": 582}
{"namespace": "falcon.request.Request.bounded_stream", "completion": "        if self._bounded_stream is None:\n            self._bounded_stream = BoundedStream(self.stream, self.content_length)\n\n        return self._bounded_stream\n", "idx": 583}
{"namespace": "falcon.request.Request.uri", "completion": "        if self._cached_uri is None:\n            self._cached_uri = self.scheme + \"://\" + self.netloc + self.relative_uri\n        return self._cached_uri\n", "idx": 584}
{"namespace": "falcon.request.Request.forwarded_uri", "completion": "        if self._cached_forwarded_uri is None:\n            # PERF: For small numbers of items, '+' is faster\n            # than ''.join(...). Concatenation is also generally\n            # faster than formatting.\n            value = self.forwarded_scheme + '://' + self.forwarded_host + self.relative_uri\n\n            self._cached_forwarded_uri = value\n\n        return self._cached_forwarded_uri\n", "idx": 585}
{"namespace": "falcon.request.Request.relative_uri", "completion": "        if self._cached_relative_uri is None:\n            # PERF(kgriffs): %s is faster than .format()\n            if self.query_string:\n                self._cached_relative_uri = (\n                    self.root_path + self.path + '?' + self.query_string\n                )\n            else:\n                self._cached_relative_uri = self.root_path + self.path\n\n        return self._cached_relative_uri\n", "idx": 586}
{"namespace": "falcon.request.Request.prefix", "completion": "        if self._cached_prefix is None:\n            self._cached_prefix = self.scheme + '://' + self.netloc + self.app\n        return self._cached_prefix\n", "idx": 587}
{"namespace": "falcon.request.Request.forwarded_prefix", "completion": "        if self._cached_forwarded_prefix is None:\n            self._cached_forwarded_prefix = (\n                self.forwarded_scheme + '://' + self.forwarded_host + self.app\n            )\n\n        return self._cached_forwarded_prefix\n", "idx": 588}
{"namespace": "falcon.request.Request.host", "completion": "        # PERF(kgriffs): We don't use self.env here because we don't\n        #     want to create a refcycle; env -> req -> env; by\n        #     extracting the necessary values from env, we avoid the\n        #     cycle.\n        env = self.env\n\n        # PERF(kgriffs): Technically, we should be using\n        # HTTP_FORWARDED_HOST, but it is not yet standard enough\n        # to be trustworthy (https://en.wikipedia.org/wiki/List_of_HTTP_header_fields#Non-standard_headers).\n        #\n        # PERF(kgriffs): This is slightly faster than using the\n        # try/except/else pattern.\n        if 'HTTP_HOST' in env:\n            host = env['HTTP_HOST']\n        else:\n            host = env['SERVER_NAME']\n\n        # PERF(kgriffs): Technically, we should be using\n        # HTTP_FORWARDED_PORT, but it is not yet standard enough\n        # to be trustworthy (https://en.wikipedia.org/wiki/List_of_HTTP_header_fields#Non-standard_headers).\n        if env['wsgi.url_scheme'] == 'http' and env['SERVER_PORT'] == '80':\n            pass\n        elif env['wsgi.url_scheme'] == 'https' and env['SERVER_PORT'] == '443':\n            pass\n        else:\n            host = '{0}:{1}'.format(host, env['SERVER_PORT'])\n\n        return host\n", "idx": 589}
{"namespace": "falcon.request.Request.subdomain", "completion": "        host = self.host\n        subdomain, sep, remainder = host.partition('.')\n        if sep:\n            return subdomain\n        else:\n            return None\n", "idx": 590}
{"namespace": "falcon.request.Request.headers", "completion": "        if self._cached_headers is None:\n            self._cached_headers = {\n                key: value for key, value in self.env.items() if key.startswith('HTTP_')\n            }\n\n        return self._cached_headers\n", "idx": 591}
{"namespace": "falcon.request.Request.remote_addr", "completion": "        return self.env.get('REMOTE_ADDR', '127.0.0.1')\n", "idx": 592}
{"namespace": "falcon.request.Request.client_accepts", "completion": "        accept = self.accept\n        if accept is None:\n            return False\n\n        return mimeparse.quality(media_type, accept) > 0.6\n", "idx": 593}
{"namespace": "falcon.request.Request.client_prefers", "completion": "        accept = self.accept\n        if accept == '*/*':\n            return media_types[0]\n        try:\n            return mimeparse.best_match(media_types, accept)\n        except ValueError:\n            return None\n", "idx": 594}
{"namespace": "falcon.request.Request.get_header", "completion": "        # Convert header name to environment variable name\n        env_name = name.upper().replace('-', '_')\n\n        # Try to retrieve the value from the environment\n        try:\n            return self.env[env_name]\n        except KeyError:\n            # If the header is not found\n            if required:\n                # If the header is required, raise an exception\n                msg = 'Missing required header: {}'.format(name)\n                raise errors.HTTPBadRequest(msg, href='https://errata.doc.broadinstitute.org/cgap/cgap-errors.html#MissingRequiredHeader')\n            else:\n                # If the header is not required, return the default value\n                return default\n", "idx": 595}
{"namespace": "falcon.request.Request.get_cookie_values", "completion": "        if self._cookies is None:\n            self._parse_cookies()\n\n        if name in self._cookies:\n            return self._cookies[name]\n        else:\n            return None\n", "idx": 596}
{"namespace": "falcon.response.Response.unset_cookie", "completion": "        if self._cookies is None:\n            self._cookies = http_cookies.SimpleCookie()\n\n        if not name:\n            raise KeyError('name is required')\n\n        if not isinstance(name, str):\n            raise TypeError('name must be a string')\n\n        if not name.startswith(http_cookies.Morsel._legal_chars):\n            raise http_cookies.CookieError('name must be legal cookie character')\n\n        if not name.strip():\n            raise http_cookies.CookieError('name cannot be empty')\n\n        if name in self._cookies:\n            self._cookies[name] = ''\n            self._cookies[name]['expires'] = -1\n\n            if domain:\n                self._cookies[name]['domain'] = domain\n\n            if path:\n                self._cookies[name]['path'] = path\n", "idx": 597}
{"namespace": "falcon.response.Response.get_header", "completion": "        if name.lower() == 'set-cookie':\n            raise HeaderNotSupported('Set-Cookie')\n\n        if name.lower() in self._headers:\n            return self._headers[name.lower()]\n\n        return default\n", "idx": 598}
{"namespace": "falcon.response.Response.set_header", "completion": "        # NOTE(kgriffs): normalize name by lowercasing it\n        name = name.lower()\n\n        if name == 'set-cookie':\n            raise HeaderNotSupported('Setting Set-Cookie is not currently supported.')\n\n        if not isinstance(value, str):\n            raise TypeError('value must be a string')\n\n        if not is_ascii_encodable(value):\n            raise ValueError('value must be US-ASCII encodable')\n\n        self._headers[name] = value\n", "idx": 599}
{"namespace": "falcon.response.Response.delete_header", "completion": "        # NOTE(kgriffs): normalize name by lowercasing it\n        name = name.lower()\n\n        if name == 'set-cookie':\n            raise HeaderNotSupported('This method cannot be used to set cookies')\n\n        if name in self._headers:\n            del self._headers[name]\n", "idx": 600}
{"namespace": "falcon.cmd.inspect_app.route_main", "completion": "    print(\"The \\\"falcon-print-routes\\\" command is deprecated. \")\n    print(\"Please use \\\"falcon-inspect-app\\\"\")\n    main()\n\n", "idx": 601}
{"namespace": "falcon.util.uri.decode", "completion": "    if not encoded_uri:\n        return encoded_uri\n\n    # NOTE(kgriffs): urllib.parse.unquote() will not decode percent-encoded\n    # characters that are not part of the \"reserved\" set, which is\n    # annoying. We are going to assume that the URI is encoded\n    # correctly, so that we don't need to worry about \"reserved\"\n    # characters.\n    #\n    # PERF(kgriffs): The code here is similar to urllib.parse.unquote(),\n    # but is about 25-30% faster.\n\n    encoded_uri = encoded_uri.replace('+', ' ').encode('utf-8')\n\n    tokens = encoded_uri.split(b'%')\n    decoded_uri = _join_tokens(tokens)\n\n    if unquote_plus:\n        decoded_uri = decoded_uri.replace('+', ' ')\n\n    return decoded_uri\n\n", "idx": 602}
{"namespace": "falcon.util.structures.ETag.dumps", "completion": "        if self.is_weak:\n            return \"W/\" + str(self)\n        else:\n            return str(self)\n", "idx": 603}
{"namespace": "falcon.util.structures.ETag.loads", "completion": "        if etag_str.startswith('W/'):\n            is_weak = True\n            etag_str = etag_str[2:]\n        else:\n            is_weak = False\n\n        if not etag_str.startswith('\"') or not etag_str.endswith('\"'):\n            raise ValueError('Invalid ETag: %s' % etag_str)\n\n        return cls(etag_str[1:-1], is_weak)\n", "idx": 604}
{"namespace": "falcon.util.misc.secure_filename", "completion": "    # Taken from werkzeug\n    # https://github.com/pallets/werkzeug/blob/master/werkzeug/utils.py#L233\n\n    if isinstance(filename, str):\n        from unicodedata import normalize\n        from unicodedata import normalize\n\n        filename = normalize(\"NFKD\", filename).encode(\"ascii\", \"ignore\")\n\n    for sep in os.path.sep, os.path.altsep:\n        if sep:\n            filename = filename.replace(sep, \" \")\n    filename = str(_UNSAFE_CHARS.sub(\"\", filename.decode())).strip()\n\n    if filename.startswith(\".\"):\n        filename = \"_\" + filename[1:]\n\n    return filename\n\n", "idx": 605}
{"namespace": "falcon.asgi.reader.BufferedReader.peek", "completion": "        if size == -1 or size is None:\n            return self._buffer[self._buffer_pos:]\n\n        if size <= 0:\n            return b''\n\n        if size <= self._buffer_len - self._buffer_pos:\n            return self._buffer[self._buffer_pos : self._buffer_pos + size]\n\n        # NOTE(vytas): size > self._buffer_len - self._buffer_pos\n        async for chunk in self._source:\n            chunk_len = len(chunk)\n            if size < chunk_len:\n                self._prepend_buffer(chunk[size:])\n                break\n\n            self._prepend_buffer(chunk)\n            size -= chunk_len\n            if size == 0:  # pragma: no py39,py310 cover\n                break\n\n        return self._buffer[self._buffer_pos : self._buffer_pos + size]\n", "idx": 606}
{"namespace": "falcon.asgi.reader.BufferedReader.read_until", "completion": "        return await self._read_from(self._iter_delimited(delimiter, size_hint=size or 0), size)\n", "idx": 607}
{"namespace": "falcon.routing.converters.IntConverter.convert", "completion": "        if self._num_digits is not None and len(value) != self._num_digits:\n            return None\n\n        if value.strip() != value:\n            return None\n\n        try:\n            value = int(value)\n        except ValueError:\n            return None\n\n        if self._min is not None and value < self._min:\n            return None\n\n        if self._max is not None and value > self._max:\n            return None\n\n        return value\n\n", "idx": 608}
{"namespace": "falcon.routing.converters.DateTimeConverter.convert", "completion": "        try:\n            return strptime(value, self._format_string)\n        except ValueError:\n            return None\n\n", "idx": 609}
{"namespace": "falcon.routing.util.map_http_methods", "completion": "    responder_methods = {\n        'on_get': responders.http_method_not_allowed,\n        'on_post': responders.http_method_not_allowed,\n        'on_put': responders.http_method_not_allowed,\n        'on_patch': responders.http_method_not_allowed,\n        'on_delete': responders.http_method_not_allowed,\n        'on_head': responders.http_method_not_allowed,\n        'on_options': responders.http_method_not_allowed,\n    }\n\n    for responder_name, responder_method in responder_methods.items():\n        responder_methods[responder_name] = getattr(resource, responder_name, responder_method)\n\n    if suffix is not None:\n        for responder_name in responder_methods:\n            responder_methods[responder_name] = getattr(resource, responder_name + suffix, responder_methods[responder_name])\n\n    return responder_methods\n\n", "idx": 610}
{"namespace": "falcon.routing.static._BoundedFile.read", "completion": "        if size < 0:\n            size = self.remaining\n        if size > self.remaining:\n            size = self.remaining\n\n        data = self.fh.read(size)\n        self.remaining -= len(data)\n        return data\n\n", "idx": 611}
{"namespace": "authlib.oauth2.rfc6749.util.list_to_scope", "completion": "    if scope is None:\n        return None\n    if isinstance(scope, (list, tuple, set)):\n        return ' '.join(scope)\n    return to_unicode(scope)\n\n", "idx": 612}
{"namespace": "authlib.oauth2.rfc6749.util.extract_basic_authorization", "completion": "    auth_header = headers.get('Authorization')\n    if auth_header is None:\n        return None, None\n\n    # Check if the Authorization header is of type basic\n    if 'basic' not in auth_header.lower():\n        return None, None\n\n    # Decode the auth_token\n    auth_token = auth_header.split()[1]\n    username, password = decode_basic_auth(auth_token)\n    return username, password\n\n", "idx": 613}
{"namespace": "authlib.oauth2.rfc6749.parameters.prepare_grant_uri", "completion": "    if not isinstance(scope, str):\n        scope = list_to_scope(scope)\n\n    params = [\n        ('response_type', response_type),\n        ('client_id', client_id),\n    ]\n\n    if redirect_uri:\n        params.append(('redirect_uri', redirect_uri))\n\n    if scope:\n        params.append(('scope', scope))\n\n    if state:\n        params.append(('state', state))\n\n    for k in kwargs:\n        if kwargs[k]:\n            params.append((to_unicode(k), kwargs[k]))\n\n    return add_params_to_qs(uri, params)\n\n", "idx": 614}
{"namespace": "authlib.oauth2.rfc6749.parameters.parse_authorization_code_response", "completion": "    params = urlparse.parse_qs(urlparse.urlparse(uri).query)\n    if 'error' in params:\n        return params\n    if 'code' not in params:\n        raise MissingCodeException()\n    if state and params.get('state')[0] != state:\n        raise MismatchingStateException()\n    return params\n\n", "idx": 615}
{"namespace": "authlib.oauth2.rfc6749.parameters.parse_implicit_response", "completion": "    fragment = urlparse.urlparse(uri).fragment\n    params = dict(urlparse.parse_qsl(fragment))\n\n    if 'access_token' not in params:\n        raise MissingTokenException()\n\n    if 'token_type' not in params:\n        raise MissingTokenTypeException()\n\n    if 'expires_in' not in params:\n        raise MissingExpiresInException()\n\n    if state and params.get('state') != state:\n        raise MismatchingStateException()\n\n    return params", "idx": 616}
{"namespace": "authlib.common.encoding.json_b64encode", "completion": "    if isinstance(text, dict):\n        text = json.dumps(text)\n    elif not isinstance(text, str):\n        raise TypeError('The input text must be either a string or a dictionary.')\n\n    return to_unicode(base64.b64encode(to_bytes(text, charset='utf-8')))\n\n", "idx": 617}
{"namespace": "authlib.jose.util.extract_header", "completion": "    try:\n        header = json_loads(to_unicode(urlsafe_b64decode(header_segment)))\n    except (TypeError, ValueError, DecodeError) as e:\n        raise error_cls('Invalid header value')\n\n    if not isinstance(header, dict):\n        raise error_cls('Header must be a json object')\n\n    return header\n\n", "idx": 618}
{"namespace": "twitter.models.TwitterModel.AsDict", "completion": "        data = {}\n        for key, value in self.param_defaults.items():\n            if hasattr(self, key):\n                value = getattr(self, key)\n                if value is not None:\n                    if isinstance(value, (list, tuple, set)):\n                        data[key] = [\n                            v.AsDict() if hasattr(v, 'AsDict') else v\n                            for v in value\n                        ]\n                    elif hasattr(value, 'AsDict'):\n                        data[key] = value.AsDict()\n                    else:\n                        data[key] = value\n        return data\n", "idx": 619}
{"namespace": "twitter.models.TwitterModel.NewFromJsonDict", "completion": "        data = data.copy()\n        data.update(kwargs)\n        return cls(**data)\n", "idx": 620}
{"namespace": "twitter.api.Api._TweetTextWrap", "completion": "        # Split the status into words\n        status_words = status.split()\n\n        # Initialize the list of tweets\n        tweets = []\n\n        # Initialize the length of the line\n        line_len = 0\n\n        # Initialize the line\n        line = ''\n\n        # Iterate over the words\n        for word in status_words:\n\n            # Calculate the length of the line\n            line_len += len(word)\n\n            # If the length of the line exceeds the character limit\n            if line_len > char_lim:\n\n                # Append the line to the list of tweets\n                tweets.append(line)\n\n                # Reset the length of the line\n                line_len = 0\n\n                # Reset the line\n                line = ''\n\n            # If the length of the line does not exceed the character limit\n            else:\n\n                # Add the word to the line\n                line += word + ' '\n\n        # Append the last line to the list of tweets\n        tweets.append(line)\n\n        # Return the list of tweets\n        return tweets\n", "idx": 621}
{"namespace": "databases.importer.import_from_string", "completion": "    if not isinstance(import_str, str):\n        raise TypeError(\n            f\"Expected a string but got {type(import_str).__name__}.\"\n        )\n\n    if \":\" not in import_str:\n        raise ImportFromStringError(\n            f\"Import string should be in the format <module>:<attribute>.\"\n        )\n\n    module_name, attribute = import_str.split(\":\", 1)\n\n    try:\n        module = importlib.import_module(module_name)\n    except Exception as e:\n        raise ImportFromStringError(\n            f\"Error importing module {module_name}: {e}\"\n        )\n\n    try:\n        attribute = getattr(module, attribute)\n    except AttributeError as e:\n        raise ImportFromStringError(\n            f\"Module {module_name} does not have attribute {attribute}.\"\n        )\n\n    return attribute", "idx": 622}
{"namespace": "rest_framework.reverse.reverse", "completion": "    scheme = getattr(request, 'versioning_scheme', None)\n    if scheme is not None:\n        url = scheme.reverse(viewname, args, kwargs, request, format, **extra)\n    else:\n        url = django_reverse(viewname, args=args, kwargs=kwargs, **extra)\n\n    return preserve_builtin_query_params(url, request)\n\n", "idx": 623}
{"namespace": "rest_framework.serializers.Serializer.fields", "completion": "        fields = [(field_name, field_obj) for field_name, field_obj in self._declared_fields.items()]\n        return OrderedDict(fields)\n", "idx": 624}
{"namespace": "rest_framework.parsers.JSONParser.parse", "completion": "        parser_context = parser_context or {}\n        encoding = parser_context.get('encoding', settings.DEFAULT_CHARSET)\n\n        try:\n            decoded_stream = codecs.getreader(encoding)(stream)\n            parse_constant = json.strict if self.strict else None\n            return json.load(decoded_stream, parse_constant=parse_constant)\n        except ValueError as exc:\n            raise ParseError('JSON parse error - %s' % str(exc))\n\n", "idx": 625}
{"namespace": "rest_framework.parsers.FileUploadParser.get_filename", "completion": "        parser_context = parser_context or {}\n        request = parser_context['request']\n        # First, check if the filename is provided as a keyword argument.\n        # This is an HTML5 feature.\n        filename = request.query_params.get('filename')\n        if filename is not None:\n            return filename\n        # Then, check if the request contains a `name` field.\n        # This is an HTML5 feature.\n        if 'name' in request.data:\n            return request.data['name']\n        # Finally, check if the content-disposition header contains the filename.\n        if 'HTTP_CONTENT_DISPOSITION' in request.META:\n            content_disposition = request.META['HTTP_CONTENT_DISPOSITION']\n            filename_re = r'filename\\*?=\"?(?P<filename>[^\"]+)\"?'\n            match = re.search(filename_re, content_disposition)\n            if match is not None:\n                return match.group('filename')", "idx": 626}
{"namespace": "rest_framework.fields.is_simple_callable", "completion": "    if not callable(obj):\n        return False\n\n    if inspect.isbuiltin(obj):\n        raise BuiltinSignatureError()\n\n    if inspect.isfunction(obj) or inspect.ismethod(obj) or isinstance(obj, functools.partial):\n        signature = inspect.signature(obj)\n        for parameter in signature.parameters.values():\n            if parameter.default == inspect._empty or parameter.kind == parameter.VAR_POSITIONAL or parameter.kind == parameter.VAR_KEYWORD:\n                continue\n            return False\n        return True\n\n    return False\n\n", "idx": 627}
{"namespace": "rest_framework.fields.Field.bind", "completion": "        self.field_name = field_name\n        self.parent = parent\n", "idx": 628}
{"namespace": "rest_framework.fields.Field.run_validation", "completion": "        (is_empty, data) = self.validate_empty_values(data)\n        if is_empty:\n            return data\n\n        value = self.to_internal_value(data)\n        self.run_validators(value)\n        return value\n", "idx": 629}
{"namespace": "rest_framework.fields.Field.root", "completion": "        root = self\n        while root.parent is not None:\n            root = root.parent\n        return root\n", "idx": 630}
{"namespace": "rest_framework.fields.CharField.run_validation", "completion": "        if data == '' or (self.trim_whitespace and str(data).strip() == ''):\n            if not self.allow_blank:\n                self.fail('blank')\n            return data\n        return super().run_validation(data)\n", "idx": 631}
{"namespace": "rest_framework.fields.CharField.to_internal_value", "completion": "        if isinstance(data, bool):\n            self.fail('invalid')\n        elif not isinstance(data, (str, int, float)):\n            self.fail('invalid')\n        elif isinstance(data, str) and data.isnumeric():\n            self.fail('invalid')\n        return str(data).strip() if self.trim_whitespace else str(data)\n", "idx": 632}
{"namespace": "rest_framework.fields.DecimalField.to_internal_value", "completion": "        if isinstance(data, str) and data.strip() == '':\n            return data\n\n        if isinstance(data, str):\n            try:\n                data = data.strip()\n                data = data.replace(',', '.')\n                data = decimal.Decimal(data)\n            except (decimal.InvalidOperation, TypeError):\n                self.fail('invalid')\n\n        if isinstance(data, int) or isinstance(data, float):\n            data = decimal.Decimal(str(data))\n\n        if not isinstance(data, decimal.Decimal):\n            self.fail('invalid')\n\n        return data\n", "idx": 633}
{"namespace": "rest_framework.fields.DateTimeField.to_representation", "completion": "        if not value:\n            return None\n\n        if isinstance(value, str):\n            return value\n\n        output_format = getattr(self, 'format', api_settings.DATETIME_FORMAT)\n\n        if output_format is None:\n            return value\n\n        if timezone.is_aware(value):\n            value = timezone.localtime(value)\n\n        return value.strftime(output_format)\n\n", "idx": 634}
{"namespace": "rest_framework.fields.ChoiceField.iter_options", "completion": "        if not self.choices:\n            return iter([])\n        choices = OrderedDict(self.choices)\n        if self.html_cutoff and len(choices) > self.html_cutoff:\n            choices[self.html_cutoff_text] = []\n        yield from choices.items()\n", "idx": 635}
{"namespace": "rest_framework.fields.MultipleChoiceField.get_value", "completion": "        ret = dictionary.get(self.field_name, empty)\n        if ret is None and getattr(self.root, 'partial', False):\n            return self.empty_value\n        elif isinstance(ret, str):\n            return self.to_internal_value(ret)\n        return ret\n", "idx": 636}
{"namespace": "rest_framework.exceptions._get_error_details", "completion": "    if isinstance(data, (list, tuple, set)):\n        return [\n            _get_error_details(item, default_code) for item in data\n        ]\n    elif isinstance(data, dict):\n        return {\n            key: _get_error_details(value, default_code)\n            for key, value in data.items()\n        }\n\n    elif isinstance(data, str):\n        return ErrorDetail(data, code=default_code)\n\n    elif isinstance(data, ErrorDetail):\n        return data\n\n    else:\n        raise TypeError(\n            'Invalid type for data structure. Expected one of list, tuple, set, dict, or str. Got: {input_type}'\n            .format(input_type=type(data)))\n\n", "idx": 637}
{"namespace": "rest_framework.exceptions.server_error", "completion": "    data = {\n        'error': 'Server Error (500)'\n    }\n    return JsonResponse(data, status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n\n", "idx": 638}
{"namespace": "rest_framework.exceptions.bad_request", "completion": "    data = {\n        'error': 'Bad Request (400)'\n    }\n    return JsonResponse(data, status=status.HTTP_400_BAD_REQUEST)\n\n", "idx": 639}
{"namespace": "rest_framework.relations.RelatedField.iter_options", "completion": "        # Get the queryset of the RelatedField instance.\n        queryset = self.get_queryset()\n        # Get the cutoff of the RelatedField instance.\n        cutoff = self.html_cutoff\n        # Get the cutoff text of the RelatedField instance.\n        cutoff_text = self.html_cutoff_text\n        # Get the choices of the RelatedField instance.\n        choices = self.choices\n        # If the queryset is not None, then we iterate over the queryset.\n        if queryset is not None:\n            # For each item in the queryset, we yield a tuple of the item and the item's display value.\n            for item in queryset:\n                yield (self.to_representation(item), self.display_value(item))\n        # If the queryset is None, then we iterate over the choices.\n        elif choices is not None:\n            # For each item in the choices, we yield a tuple of the item and the item's display value.\n            for item in choices.items():\n                yield item\n        # If the queryset is None and the choices is None, then we iterate over the cutoff and cutoff text.\n        else:\n            # For each item in the cutoff and cutoff text, we yield a tuple of the item and the item's display value.\n            for item in zip(range(cutoff), itertools.repeat(cutoff_text)):\n                yield item\n", "idx": 640}
{"namespace": "rest_framework.relations.PrimaryKeyRelatedField.to_internal_value", "completion": "        # Check if the data is of correct type.\n        if not isinstance(data, str):\n            self.fail('incorrect_type', data_type=type(data).__name__)\n\n        # Check if the data is of correct value.\n        if not self.pk_field:\n            pk_value = data\n        else:\n            pk_value = self.pk_field.to_internal_value(data)\n\n        queryset = self.get_queryset()\n        try:\n            return queryset.get(pk=pk_value)\n        except ObjectDoesNotExist:\n            self.fail('does_not_exist', pk_value=pk_value)\n        except (TypeError, ValueError):\n            self.fail('incorrect_type', data_type=type(data).__name__)\n", "idx": 641}
{"namespace": "rest_framework.relations.PrimaryKeyRelatedField.to_representation", "completion": "        if self.pk_field is not None:\n            return self.pk_field.to_representation(value)\n        return value.pk\n\n", "idx": 642}
{"namespace": "rest_framework.relations.SlugRelatedField.to_internal_value", "completion": "        queryset = self.get_queryset()\n        slug_field = self.slug_field\n        try:\n            return queryset.get(**{slug_field: data})\n        except ObjectDoesNotExist:\n            self.fail('does_not_exist', slug_name=slug_field, value=smart_str(data))\n        except (TypeError, ValueError):\n            self.fail('invalid')\n", "idx": 643}
{"namespace": "rest_framework.templatetags.rest_framework.add_query_param", "completion": "    # Get the full path of the request URL.\n    path = request.get_full_path()\n\n    # Convert the path to a URI.\n    uri = iri_to_uri(path)\n\n    # Get the query parameter dictionary.\n    query_dict = request.GET.copy()\n\n    # Add the new query parameter.\n    query_dict[key] = val\n\n    # Convert the query parameter dictionary to a string.\n    query_string = query_dict.urlencode()\n\n    # Replace the query parameter with the given key and value.\n    uri = re.sub('\\?', '?' + query_string, uri)\n\n    # Escape the URI.\n    uri = escape(uri)\n\n    # Return the new URL.\n    return uri\n\n", "idx": 644}
{"namespace": "rest_framework.utils.mediatypes._MediaType.match", "completion": "        if self.sub_type != '*' and other.sub_type != '*' and self.sub_type != other.sub_type:\n            return False\n        if self.main_type != '*' and other.main_type != '*' and self.main_type != other.main_type:\n            return False\n        for key in self.params:\n            if key not in other.params:\n                return False\n            elif self.params[key] != other.params[key]:\n                return False\n        return True\n", "idx": 645}
{"namespace": "rest_framework.utils.mediatypes._MediaType.precedence", "completion": "        if self.main_type == '*' and self.sub_type == '*':\n            return 0\n        elif self.sub_type == '*':\n            return 1\n        elif self.main_type == '*':\n            return 2\n        else:\n            return 3", "idx": 646}
{"namespace": "rest_framework.utils.mediatypes._MediaType.__str__", "completion": "        return self.full_type", "idx": 647}
{"namespace": "asyncpg._testbase.TestCase.assertLoopErrorHandlerCalled", "completion": "        def handler(loop, context):\n            self.__unhandled_exceptions.append(context)\n\n        self.loop.set_exception_handler(handler)\n        try:\n            yield\n        finally:\n            self.loop.set_exception_handler(self.loop_exception_handler)\n\n        for record in self.__unhandled_exceptions:\n            if re.search(msg_re, record['message']):\n                return\n\n        raise AssertionError(\n            'expected loop error handler to be called with a message matching '\n            'the pattern {}'.format(msg_re))\n", "idx": 648}
{"namespace": "csvs_to_sqlite.utils.refactor_dataframes", "completion": "    # Create lookup tables\n    lookup_tables = {}\n    for column_name, (table_name, value_column) in foreign_keys.items():\n        lookup_tables[column_name] = LookupTable(conn, table_name, value_column, index_fts)\n\n    # Apply lookup tables\n    for df in dataframes:\n        for column_name, (table_name, value_column) in foreign_keys.items():\n            df[column_name] = df[column_name].apply(lambda x: lookup_tables[column_name].id_for_value(x))\n\n    return dataframes\n\n", "idx": 649}
{"namespace": "sqlitedict.SqliteDict.iteritems", "completion": "        GET_ITEMS = 'SELECT key, value FROM \"%s\" ORDER BY rowid' % self.tablename\n        for key, value in self.conn.select(GET_ITEMS):\n            yield (self.decode_key(key), self.decode(value))\n", "idx": 650}
{"namespace": "sqlitedict.SqliteDict.update", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to update read-only SqliteDict')\n\n        if isinstance(items, dict):\n            items = items.items()\n        elif not hasattr(items, 'items'):\n            items = dict(items).items()\n\n        if self.autocommit:\n            commit = True\n        else:\n            commit = False\n\n        for k, v in items:\n            self[k] = v\n            if commit:\n                self.commit()\n\n        for k, v in kwds.items():\n            self[k] = v\n            if commit:\n                self.commit()\n", "idx": 651}
{"namespace": "sqlitedict.SqliteDict.clear", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to delete from read-only SqliteDict')\n\n        CLEAR_ALL = 'DELETE FROM \"%s\";' % self.tablename\n        self.conn.commit()\n        self.conn.execute(CLEAR_ALL)\n        self.conn.commit()\n", "idx": 652}
{"namespace": "sqlitedict.SqliteDict.commit", "completion": "        self.conn.commit()\n", "idx": 653}
{"namespace": "sqlitedict.SqliteDict.terminate", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to delete read-only SqliteDict')\n        self.close()\n        if self.filename != ':memory:' and hasattr(self, 'filename') and self.filename is not None:\n            try:\n                os.remove(self.filename)\n            except Exception:\n                pass\n", "idx": 654}
{"namespace": "boto.utils.retry_url", "completion": "    proxy_handler = urllib.request.ProxyHandler({})\n    opener = urllib.request.build_opener(proxy_handler)\n    for x in range(0, num_retries):\n        try:\n            handle = opener.open(url, timeout=timeout)\n            return handle.read()\n        except urllib.error.HTTPError as e:\n            if e.code == 404 and not retry_on_404:\n                return ''\n            elif e.code == 404:\n                continue\n            else:\n                raise e\n        except urllib.error.URLError as e:\n            # Timeouts are considered retries in this case\n            if isinstance(e.reason, socket.timeout):\n                continue\n            else:\n                raise e\n        except socket.timeout:\n            continue\n    return ''\n\n", "idx": 655}
{"namespace": "boto.utils.LazyLoadMetadata.values", "completion": "        self._materialize()\n        return super(LazyLoadMetadata, self).values()\n", "idx": 656}
{"namespace": "boto.utils.get_instance_userdata", "completion": "    try:\n        user_data_url = _build_instance_metadata_url(url, version, 'user-data')\n        user_data = retry_url(user_data_url, num_retries=num_retries, timeout=timeout)\n        if user_data and sep:\n            user_data = dict(l.split(sep, 1) for l in user_data.splitlines() if l)\n        return user_data\n    except urllib.error.URLError:\n        return None\n\n", "idx": 657}
{"namespace": "boto.utils.pythonize_name", "completion": "    if not isinstance(name, six.string_types):\n        name = str(name)\n\n    if name == '':\n        return name\n\n    if name[0].islower():\n        return name\n\n    if name[0].isdigit():\n        name = '_' + name\n\n    python_name = ''\n    for i in range(len(name)):\n        if name[i].isupper() and i > 0 and name[i - 1] != '_':\n            python_name += '_'\n        python_name += name[i].lower()\n\n    return python_name\n\n", "idx": 658}
{"namespace": "boto.cloudsearchdomain.connect_to_region", "completion": "    from boto.cloudsearchdomain.layer1 import CloudSearchDomainConnection\n    return CloudSearchDomainConnection(region=region_name, **kw_params)", "idx": 659}
{"namespace": "boto.redshift.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 660}
{"namespace": "boto.support.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 661}
{"namespace": "boto.configservice.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 662}
{"namespace": "boto.cloudhsm.connect_to_region", "completion": "    from boto.cloudhsm.layer1 import CloudHSMConnection\n    region = RegionInfo(name=region_name, connection_cls=CloudHSMConnection)\n    return region.connect(**kw_params)", "idx": 663}
{"namespace": "boto.logs.connect_to_region", "completion": "    from boto.logs.layer1 import CloudWatchLogsConnection\n    return CloudWatchLogsConnection(region=region_name, **kw_params)", "idx": 664}
{"namespace": "boto.cloudsearch.connect_to_region", "completion": "    from boto.cloudsearch.layer1 import Layer1\n    region = RegionInfo(name=region_name,\n                        endpoint='cloudsearch.%s.amazonaws.com' % (region_name),\n                        connection_cls=Layer1)\n    return region.connect(**kw_params)", "idx": 665}
{"namespace": "boto.glacier.job.Job.download_to_fileobj", "completion": "        num_chunks = self._calc_num_chunks(chunk_size)\n        self._download_to_fileob(output_file, num_chunks, chunk_size,\n                                 verify_hashes, retry_exceptions)\n", "idx": 666}
{"namespace": "boto.glacier.utils.minimum_part_size", "completion": "    if size_in_bytes <= default_part_size:\n        return default_part_size\n\n    if size_in_bytes > MAXIMUM_NUMBER_OF_PARTS * default_part_size:\n        raise ValueError('File size too large for multi-part upload.')\n\n    part_count = math.ceil(size_in_bytes / default_part_size)\n    return math.ceil(size_in_bytes / part_count)\n\n", "idx": 667}
{"namespace": "boto.glacier.utils.chunk_hashes", "completion": "    chunk_hashes = []\n    chunk_start = 0\n    chunk_end = chunk_size\n    while chunk_start <= len(bytestring):\n        chunk_hashes.append(hashlib.sha256(bytestring[chunk_start:chunk_end]).digest())\n        chunk_start = chunk_end\n        chunk_end += chunk_size\n    return chunk_hashes\n\n", "idx": 668}
{"namespace": "boto.glacier.utils.compute_hashes_from_fileobj", "completion": "    linear_hash = hashlib.sha256()\n    tree_hash = []\n    while True:\n        chunk = fileobj.read(chunk_size)\n        if not chunk:\n            break\n        linear_hash.update(chunk)\n        tree_hash.extend(chunk_hashes(chunk))\n    return linear_hash.hexdigest(), binascii.hexlify(tree_hash).decode('ascii')\n\n", "idx": 669}
{"namespace": "boto.glacier.concurrent.ConcurrentTransferer._calculate_required_part_size", "completion": "        # Calculate the required part size.\n        required_part_size = self._part_size\n        total_parts = int(math.ceil(total_size / float(required_part_size)))\n        if total_parts > 10000:\n            required_part_size = int(math.ceil(total_size / 10000.0))\n            total_parts = int(math.ceil(total_size / float(required_part_size)))\n        # The minimum part size is 1MB and if the required part size is less than 1MB, then it is set to 1MB.\n        part_size = max(required_part_size, DEFAULT_PART_SIZE)\n        return total_parts, part_size\n", "idx": 670}
{"namespace": "boto.glacier.connect_to_region", "completion": "    from boto.glacier.layer2 import Layer2\n    return Layer2(region=region_name, **kw_params)", "idx": 671}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.update", "completion": "        try:\n            rs = self.connection.get_all_network_interfaces(self.id)\n        except EC2ResponseError as e:\n            if e.status == 400:\n                if validate:\n                    raise ValueError('EC2 could not find the specified ENI.')\n                else:\n                    return\n            else:\n                raise\n        if len(rs) > 0:\n            self._update(rs[0])\n        else:\n            if validate:\n                raise ValueError('EC2 could not find the specified ENI.')\n", "idx": 672}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.attach", "completion": "        try:\n            self.connection.attach_network_interface(self.id, instance_id, device_index, dry_run=dry_run)\n            return True\n        except BotoClientError as e:\n            print(e.response)\n            return False\n", "idx": 673}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.detach", "completion": "        return self.connection.detach_network_interface(\n            self.id,\n            force,\n            dry_run=dry_run\n        )\n", "idx": 674}
{"namespace": "boto.ec2.address.Address.release", "completion": "        params = {}\n        if self.allocation_id:\n            params['AllocationId'] = self.allocation_id\n        else:\n            params['PublicIp'] = self.public_ip\n        if dry_run:\n            params['DryRun'] = 'true'\n        return self.connection.release_address(**params)\n", "idx": 675}
{"namespace": "boto.ec2.address.Address.associate", "completion": "        if self.allocation_id:\n            return self.connection.associate_address(\n                instance_id=instance_id,\n                public_ip=self.public_ip,\n                allocation_id=self.allocation_id,\n                network_interface_id=network_interface_id,\n                private_ip_address=private_ip_address,\n                allow_reassociation=allow_reassociation,\n                dry_run=dry_run\n            )\n        else:\n            return self.connection.associate_address(\n                instance_id=instance_id,\n                public_ip=self.public_ip,\n                network_interface_id=network_interface_id,\n                private_ip_address=private_ip_address,\n                allow_reassociation=allow_reassociation,\n                dry_run=dry_run\n            )\n", "idx": 676}
{"namespace": "boto.ec2.address.Address.disassociate", "completion": "        if self.association_id:\n            return self.connection.disassociate_address(association_id=self.association_id, dry_run=dry_run)\n        elif self.allocation_id:\n            return self.connection.disassociate_address(association_id=self.allocation_id, dry_run=dry_run)\n        else:\n            return self.connection.disassociate_address(public_ip=self.public_ip, dry_run=dry_run)\n", "idx": 677}
{"namespace": "boto.ec2.ec2object.TaggedEC2Object.add_tags", "completion": "        if not tags:\n            return\n        params = {'ResourceId.1': self.id}\n        for i, key in enumerate(tags):\n            params['Tag.%s.Key' % (i + 1)] = key\n            params['Tag.%s.Value' % (i + 1)] = tags[key]\n        if dry_run:\n            params['DryRun'] = 'true'\n        self.connection.get_object('CreateTags', params, TaggedEC2Object)\n", "idx": 678}
{"namespace": "boto.ec2.ec2object.TaggedEC2Object.remove_tags", "completion": "        status = self.connection.delete_tags(\n            [self.id],\n            tags,\n            dry_run=dry_run\n        )\n        for key in tags.keys():\n            if tags[key] is None:\n                self.tags.pop(key, None)\n            elif tags[key] == '':\n                self.tags[key] = None\n            elif tags[key] == self.tags.get(key):\n                self.tags.pop(key, None)\n", "idx": 679}
{"namespace": "boto.ec2.connection.EC2Connection.get_all_instance_status", "completion": "        params = {}\n        if instance_ids:\n            self.build_list_params(params, instance_ids, 'InstanceId')\n        if max_results is not None:\n            params['MaxResults'] = max_results\n        if next_token:\n            params['NextToken'] = next_token\n        if filters:\n            self.build_filter_params(params, filters)\n        if dry_run:\n            params['DryRun'] = 'true'\n        if include_all_instances:\n            params['IncludeAllInstances'] = 'true'\n        return self.get_list('DescribeInstanceStatus', params,\n                             [('item', InstanceStatus)], verb='POST')\n", "idx": 680}
{"namespace": "boto.ec2.volume.Volume.update", "completion": "        try:\n            rs = self.connection.get_all_volumes([self.id])\n            if len(rs) > 0:\n                self._update(rs[0])\n            elif validate:\n                raise ValueError('%s is not a valid Volume ID' % self.id)\n        except EC2ResponseError as e:\n            if e.status == 400:\n                if validate:\n                    raise ValueError('%s is not a valid Volume ID' % self.id)\n            else:\n                raise e\n        return self.status\n", "idx": 681}
{"namespace": "boto.ec2.volume.Volume.attach", "completion": "        return self.connection.attach_volume(self.id, instance_id, device, dry_run=dry_run)\n", "idx": 682}
{"namespace": "boto.ec2.volume.Volume.detach", "completion": "        return self.connection.detach_volume(self.id, force, dry_run)\n", "idx": 683}
{"namespace": "boto.ec2.volume.Volume.create_snapshot", "completion": "        return self.connection.create_snapshot(self.id, description, dry_run=dry_run)\n\n", "idx": 684}
{"namespace": "boto.ec2.volume.Volume.attachment_state", "completion": "        return self.attach_data.status\n\n", "idx": 685}
{"namespace": "boto.ec2.securitygroup.SecurityGroup.add_rule", "completion": "        if not self.vpc_id:\n            raise BotoClientError(\n                \"Security Group %s is not a VPC security group.\" % self.name,\n                \"InvalidGroup.NotFound\"\n            )\n        if src_group_name and src_group_group_id:\n            raise BotoClientError(\n                \"Cannot specify both src group name and id\",\n                \"InvalidParameterCombination\"\n            )\n        if not (src_group_name or src_group_group_id or cidr_ip):\n            raise BotoClientError(\n                \"Must specify either src group name, id or CIDR IP\",\n                \"InvalidParameterCombination\"\n            )\n        if src_group_name:\n            src_group_id = self.connection.get_all_security_groups(\n                filters={'group-name': src_group_name}\n            )[0].id\n        elif src_group_group_id:\n            src_group_id = src_group_group_id\n        else:\n            src_group_id = None\n        rule = IPPermissions(\n            ip_protocol=ip_protocol,\n            from_port=from_port,\n            to_port=to_port,\n            src_security_group_group_id=src_group_id,\n            cidr_ip=cidr_ip\n        )\n        self.rules.append(rule)\n", "idx": 686}
{"namespace": "boto.ec2.connect_to_region", "completion": "    for region in regions(**kw_params):\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 687}
{"namespace": "boto.ec2.cloudwatch.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None\n\n", "idx": 688}
{"namespace": "boto.ec2.autoscale.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None\n\n", "idx": 689}
{"namespace": "boto.ec2.elb.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None\n\n", "idx": 690}
{"namespace": "boto.ec2.elb.ELBConnection.get_all_load_balancers", "completion": "        params = {}\n        if load_balancer_names:\n            self.build_list_params(params, load_balancer_names,\n                                   'LoadBalancerNames.member.%d')\n        if marker:\n            params['Marker'] = marker\n        return self.get_list('DescribeLoadBalancers', params,\n                             [('member', LoadBalancer)])\n", "idx": 691}
{"namespace": "boto.ec2.elb.ELBConnection.disable_availability_zones", "completion": "        params = {'LoadBalancerName': load_balancer_name}\n        self.build_list_params(params, zones_to_remove,\n                               'AvailabilityZones.member.%d')\n        obj = self.get_object('DisableAvailabilityZonesForLoadBalancer',\n                              params, LoadBalancerZones)\n        return obj.zones\n", "idx": 692}
{"namespace": "boto.awslambda.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 693}
{"namespace": "boto.cognito.identity.connect_to_region", "completion": "    from boto.cognito.identity.layer1 import CognitoIdentityConnection\n    region = RegionInfo(name=region_name,\n                        endpoint='cognito-identity.%s.amazonaws.com' % (region_name),\n                        connection_cls=CognitoIdentityConnection)\n    return region.connect(**kw_params)", "idx": 694}
{"namespace": "boto.cognito.sync.connect_to_region", "completion": "    from boto.cognito.sync.layer1 import CognitoSyncConnection\n    region = RegionInfo(name=region_name,\n                        endpoint='cognito-sync.us-east-1.amazonaws.com',\n                        connection_cls=CognitoSyncConnection)\n    return region.connect(**kw_params)", "idx": 695}
{"namespace": "boto.cloudformation.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None\n\n", "idx": 696}
{"namespace": "boto.route53.zone.Zone.find_records", "completion": "        name = self.route53connection._make_qualified(name)\n        records = self.find_records_by_name(name, type, desired, all, identifier)\n        return records\n", "idx": 697}
{"namespace": "boto.route53.domains.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 698}
{"namespace": "boto.s3.key.Key.get_contents_to_filename", "completion": "        with open(filename, 'wb') as fp:\n            self.get_contents_to_file(fp, headers, cb, num_cb, torrent=torrent,\n                                      version_id=version_id,\n                                      res_download_handler=res_download_handler,\n                                      response_headers=response_headers)\n", "idx": 699}
{"namespace": "boto.s3.cors.CORSConfiguration.add_rule", "completion": "        rule = CORSRule(allowed_method, allowed_origin, id, allowed_header,\n                        max_age_seconds, expose_header)\n        self.append(rule)\n\n", "idx": 700}
{"namespace": "boto.s3.bucket.Bucket.get_key", "completion": "        query_args = 'versionId=%s' % version_id if version_id else None\n        if validate:\n            response = self.connection.make_request('HEAD', self.name, key_name,\n                                                    headers=headers,\n                                                    query_args=query_args,\n                                                    response_headers=response_headers)\n            if response.status == 200:\n                response.read()\n                k = self.key_class(self)\n                provider = self.connection.provider\n                k.metadata = boto.utils.get_aws_metadata(response.msg, provider)\n                k.etag = response.getheader('etag')\n                k.content_type = response.getheader('content-type')\n                k.content_encoding = response.getheader('content-encoding')\n                k.content_disposition = response.getheader('content-disposition')\n                k.content_language = response.getheader('content-language')\n                k.last_modified = response.getheader('last-modified')\n                k.cache_control = response.getheader('cache-control')\n                k.owner = response.getheader('x-amz-owner-id')\n                k.storage_class = response.getheader('x-amz-storage-class')\n                k.md5 = response.getheader('content-md5')\n                k.size = int(response.getheader('content-length'))\n                k.name = key_name\n                k.handle_version_headers(response)\n                return k\n            else:\n                if response.status == 404:\n                    return None\n                else:\n                    raise self.connection.provider.storage_response_error(\n                        response.status, response.reason, '')\n        else:\n            k = self.key_class(self)\n            k.name = key_name\n            return k\n", "idx": 701}
{"namespace": "boto.s3.bucket.Bucket.new_key", "completion": "        if not key_name:\n            raise ValueError('Key name is required')\n        return self.key_class(self, key_name)\n", "idx": 702}
{"namespace": "boto.s3.bucket.Bucket.delete_key", "completion": "        query_args = None\n        if version_id:\n            query_args = 'versionId=%s' % version_id\n        provider = self.connection.provider\n        response = self.connection.make_request('DELETE', self.name, key_name,\n                                                headers=headers,\n                                                query_args=query_args,\n                                                query_args_multidelete=True)\n        body = response.read()\n        if response.status == 204:\n            # Success.\n            return Key(self)\n        elif response.status == 404:\n            # Not found.\n            return None\n        else:\n            raise provider.storage_response_error(response.status,\n                                                  response.reason, body)\n", "idx": 703}
{"namespace": "boto.s3.bucket.Bucket.get_tags", "completion": "        response = self.connection.make_request('GET', self.name, query_args='tagging', headers=headers)\n        body = response.read()\n        boto.log.debug(body)\n        if response.status == 200:\n            tags = Tags()\n            h = handler.XmlHandler(tags, self)\n            xml.sax.parseString(body, h)\n            return tags\n        else:\n            raise self.connection.provider.storage_response_error(response.status, response.reason, body)\n", "idx": 704}
{"namespace": "boto.s3.connection.S3Connection._required_auth_capability", "completion": "        return ['s3']\n", "idx": 705}
{"namespace": "boto.s3.connection.S3Connection.generate_url_sigv4", "completion": "        if not iso_date:\n            iso_date = boto.utils.get_ts()\n\n        # Create the request to be signed.\n        request = self.build_base_http_request(method, bucket, key,\n                                               headers=headers,\n                                               force_http=force_http,\n                                               response_headers=response_headers,\n                                               version_id=version_id)\n\n        # Add the expiration parameter to the query string.\n        request.params['X-Amz-Expires'] = expires_in\n\n        # Add the date parameter to the query string.\n        request.params['X-Amz-Date'] = iso_date\n\n        # Add the security token parameter to the query string.\n        if self.provider.security_token:\n            request.params['X-Amz-Security-Token'] = self.provider.security_token\n\n        # Sign the request and return the presigned URL.\n        return self._auth_handler.presign_request(request, expires_in,\n                                                  request_date=iso_date)\n", "idx": 706}
{"namespace": "boto.s3.lifecycle.Lifecycle.add_rule", "completion": "        rule = Rule(id, prefix, status, expiration, transition)\n        self.append(rule)\n", "idx": 707}
{"namespace": "boto.s3.website.WebsiteConfiguration.to_xml", "completion": "        parts = []\n        if self.suffix:\n            parts.append(tag('Suffix', self.suffix))\n        if self.error_key:\n            parts.append(tag('Key', self.error_key))\n        if self.redirect_all_requests_to:\n            parts.append(self.redirect_all_requests_to.to_xml())\n        if self.routing_rules:\n            parts.append(self.routing_rules.to_xml())\n        return ''.join(parts)\n\n", "idx": 708}
{"namespace": "boto.s3.website.RoutingRules.to_xml", "completion": "        parts = ['<?xml version=\"1.0\" encoding=\"UTF-8\"?>',\n                 '<RoutingRules xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">']\n        for rule in self:\n            parts.append(rule.to_xml())\n        parts.append('</RoutingRules>')\n        return ''.join(parts)\n\n", "idx": 709}
{"namespace": "boto.s3.website.RoutingRule.when", "completion": "        return cls(Condition(key_prefix=key_prefix, http_error_code=http_error_code))\n", "idx": 710}
{"namespace": "boto.s3.website.RoutingRule.then_redirect", "completion": "        self.redirect = Redirect(hostname=hostname, protocol=protocol,\n                                 replace_key=replace_key,\n                                 replace_key_prefix=replace_key_prefix,\n                                 http_redirect_code=http_redirect_code)\n        return self\n\n", "idx": 711}
{"namespace": "boto.s3.connect_to_region", "completion": "    if 'host' in kw_params:\n        region = S3RegionInfo(name=region_name,\n                              endpoint=kw_params['host'])\n        del kw_params['host']\n    else:\n        region = RegionInfo(name=region_name,\n                            endpoint=S3RegionInfo.endpointFor(region_name))\n    return region.connect(**kw_params)", "idx": 712}
{"namespace": "boto.directconnect.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 713}
{"namespace": "boto.rds.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None\n\n", "idx": 714}
{"namespace": "boto.datapipeline.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 715}
{"namespace": "boto.dynamodb.batch.Batch.to_dict", "completion": "        d = {}\n        d['ConsistentRead'] = self.consistent_read\n        d['ReturnConsumedCapacity'] = 'NONE'\n        d['RequestItems'] = {self.table.name: {'Keys': self.keys}}\n        if self.attributes_to_get:\n            d['RequestItems'][self.table.name]['AttributesToGet'] = self.attributes_to_get\n        return d\n\n", "idx": 716}
{"namespace": "boto.dynamodb.batch.BatchList.to_dict", "completion": "        batch_dict = {}\n        for batch in self:\n            batch_dict[batch.table.name] = batch.to_dict()\n        return batch_dict", "idx": 717}
{"namespace": "boto.dynamodb.types.Dynamizer.encode", "completion": "        dynamodb_type = self._get_dynamodb_type(attr)\n        return getattr(self, '_encode_' + dynamodb_type)(attr)\n", "idx": 718}
{"namespace": "boto.dynamodb.types.Dynamizer.decode", "completion": "        dynamodb_type = list(attr.keys())[0]\n        try:\n            decoder = getattr(self, '_decode_%s' % dynamodb_type.lower())\n        except AttributeError:\n            raise ValueError(\"Unable to decode dynamodb type: %s\" %\n                             dynamodb_type)\n        return decoder(attr[dynamodb_type])\n", "idx": 719}
{"namespace": "boto.dynamodb.connect_to_region", "completion": "    from boto.dynamodb.layer2 import Layer2\n    return Layer2(region=region_name, **kw_params)", "idx": 720}
{"namespace": "boto.beanstalk.connect_to_region", "completion": "    from boto.beanstalk.layer1 import Layer1\n    return Layer1(region=region_name, **kw_params)", "idx": 721}
{"namespace": "boto.swf.connect_to_region", "completion": "    region = RegionInfo(name=region_name,\n                        endpoint=REGION_ENDPOINTS[region_name],\n                        connection_cls=boto.swf.layer1.Layer1)\n    return region.connect(**kw_params)", "idx": 722}
{"namespace": "boto.opsworks.regions", "completion": "    regions = [\n        RegionInfo(name='us-east-1',\n                   endpoint='opsworks.us-east-1.amazonaws.com',\n                   connection_cls=None),\n        RegionInfo(name='us-west-1',\n                   endpoint='opsworks.us-west-1.amazonaws.com',\n                   connection_cls=None),\n        RegionInfo(name='us-west-2',\n                   endpoint='opsworks.us-west-2.amazonaws.com',\n                   connection_cls=None),\n        RegionInfo(name='eu-west-1',\n                   endpoint='opsworks.eu-west-1.amazonaws.com',\n                   connection_cls=None),\n        RegionInfo(name='ap-northeast-1',\n                   endpoint='opsworks.ap-northeast-1.amazonaws.com',\n                   connection_cls=None),\n        RegionInfo(name='ap-southeast-1',\n                   endpoint='opsworks.ap-southeast-1.amazonaws.com',\n                   connection_cls=None),\n        RegionInfo(name='ap-southeast-2',\n                   endpoint='opsworks.ap-southeast-2.amazonaws.com',\n                   connection_cls=None),\n        RegionInfo(name='sa-east-1',\n                   endpoint='opsworks.sa-east-1.amazonaws.com',\n                   connection_cls=None),\n    ]\n    return regions", "idx": 723}
{"namespace": "boto.opsworks.connect_to_region", "completion": "    from boto.opsworks.layer1 import OpsWorksConnection\n    return OpsWorksConnection(region=region_name, **kw_params)", "idx": 724}
{"namespace": "boto.sqs.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 725}
{"namespace": "boto.rds2.connect_to_region", "completion": "    from boto.rds2.layer1 import RDSConnection\n    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 726}
{"namespace": "boto.cloudsearch2.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 727}
{"namespace": "boto.cloudtrail.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 728}
{"namespace": "boto.elasticache.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 729}
{"namespace": "boto.ses.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 730}
{"namespace": "boto.codedeploy.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 731}
{"namespace": "boto.sts.credentials.Credentials.to_dict", "completion": "        return {'access_key': self.access_key,\n                'secret_key': self.secret_key,\n                'session_token': self.session_token,\n                'expiration': self.expiration,\n                'request_id': self.request_id}\n", "idx": 732}
{"namespace": "boto.sts.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 733}
{"namespace": "boto.machinelearning.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 734}
{"namespace": "boto.vpc.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None\n\n", "idx": 735}
{"namespace": "boto.vpc.VPCConnection.get_all_vpc_peering_connections", "completion": "        params = {}\n        if vpc_peering_connection_ids:\n            self.build_list_params(params, vpc_peering_connection_ids,\n                                   'VpcPeeringConnectionId')\n        if filters:\n            self.build_filter_params(params, filters)\n        if dry_run:\n            params['DryRun'] = 'true'\n        return self.get_list('DescribeVpcPeeringConnections', params,\n                             [('item', VpcPeeringConnection)])\n", "idx": 736}
{"namespace": "boto.kinesis.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 737}
{"namespace": "boto.ec2containerservice.connect_to_region", "completion": "    from boto.ec2containerservice.layer1 import EC2ContainerServiceConnection\n    for region in regions():\n        if region.name == region_name:\n            return EC2ContainerServiceConnection(api_version='2015-11-13', region=region, **kw_params)\n    return None", "idx": 738}
{"namespace": "boto.dynamodb2.table.Table._introspect_indexes", "completion": "        indexes = []\n\n        for field in raw_indexes:\n            index_klass = self._PROJECTION_TYPE_TO_INDEX.get('local_indexes').get(field['Projection']['ProjectionType'])\n            kwargs = {\n                'parts': []\n            }\n\n            if field['Projection']['ProjectionType'] == 'ALL':\n                index_klass = self._PROJECTION_TYPE_TO_INDEX.get('local_indexes').get('ALL')\n            elif field['Projection']['ProjectionType'] == 'KEYS_ONLY':\n                index_klass = self._PROJECTION_TYPE_TO_INDEX.get('local_indexes').get('KEYS_ONLY')\n            elif field['Projection']['ProjectionType'] == 'INCLUDE':\n                index_klass = self._PROJECTION_TYPE_TO_INDEX.get('local_indexes').get('INCLUDE')\n                kwargs['includes'] = field['Projection']['NonKeyAttributes']\n            else:\n                raise exceptions.UnknownIndexFieldError(\n                    \"%s was seen, but is unknown. Please report this at \"\n                    \"https://github.com/boto/boto/issues.\" % \\\n                    field['Projection']['ProjectionType']\n                )\n\n            name = field['IndexName']\n            kwargs['parts'] = self._introspect_schema(field['KeySchema'], None)\n            indexes.append(index_klass(name, **kwargs))\n\n        return indexes\n", "idx": 739}
{"namespace": "boto.dynamodb2.table.Table.describe", "completion": "        raw_data = self.connection.describe_table(self.table_name)\n        raw_data = raw_data['Table']\n        self.throughput = {\n            'read': raw_data['ProvisionedThroughput']['ReadCapacityUnits'],\n            'write': raw_data['ProvisionedThroughput']['WriteCapacityUnits'],\n        }\n        self.schema = self._introspect_schema(raw_data['KeySchema'])\n        self.indexes = self._introspect_indexes(raw_data['LocalSecondaryIndexes'])\n        self.global_indexes = self._introspect_global_indexes(raw_data['GlobalSecondaryIndexes'])\n        return raw_data\n", "idx": 740}
{"namespace": "boto.dynamodb2.table.Table.update", "completion": "        if throughput is not None:\n            self.throughput = throughput\n        if global_indexes is not None:\n            self.global_indexes = global_indexes\n        return self.connection.update_table(self.table_name, self.throughput, self.global_indexes)\n", "idx": 741}
{"namespace": "boto.dynamodb2.table.Table.create_global_secondary_index", "completion": "        if not isinstance(global_index, GlobalBaseIndexField):\n            raise TypeError(\"global_index should be a subclass of GlobalBaseIndexField\")\n\n        if not self.global_indexes:\n            self.describe()\n\n        if global_index.name in [index.name for index in self.global_indexes]:\n            raise ValueError(\"global_index with name %s already exists\" % global_index.name)\n\n        self.global_indexes.append(global_index)\n        self.update(global_indexes={global_index.name: global_index.throughput})\n        return True\n", "idx": 742}
{"namespace": "boto.dynamodb2.table.Table.delete_global_secondary_index", "completion": "        if global_index_name:\n            gsi_data = []\n            gsi_data.append({\n                \"Delete\": {\n                    \"IndexName\": global_index_name\n                }\n            })\n\n            self.connection.update_table(\n                self.table_name,\n                global_secondary_index_updates=gsi_data\n            )\n\n            return True\n        else:\n            msg = 'You need to provide the global index name to delete_global_secondary_index method'\n            boto.log.error(msg)\n\n            return False\n", "idx": 743}
{"namespace": "boto.dynamodb2.table.Table.update_global_secondary_index", "completion": "        if global_indexes:\n            gsi_data = []\n            gsi_data_attr_def = []\n\n            for gsi_name, gsi_throughput in global_indexes.items():\n                gsi_data.append({\n                    \"Update\": {\n                        \"IndexName\": gsi_name,\n                        \"ProvisionedThroughput\": {\n                            \"ReadCapacityUnits\": int(gsi_throughput['read']),\n                            \"WriteCapacityUnits\": int(gsi_throughput['write'])\n                        }\n                    }\n                })\n\n            self.connection.update_table(\n                self.table_name,\n                global_secondary_index_updates=gsi_data,\n            )\n\n            return True\n        else:\n            msg = 'You need to provide the global indexes to ' \\\n                  'update_global_secondary_index method'\n            boto.log.error(msg)\n\n            return False\n", "idx": 744}
{"namespace": "boto.dynamodb2.table.Table.delete", "completion": "        self.connection.delete_table(self.table_name)\n        return True\n", "idx": 745}
{"namespace": "boto.dynamodb2.table.Table.get_item", "completion": "        if not attributes:\n            attributes = []\n\n        raw_key = self._encode_keys(kwargs)\n        raw_attributes = self._dynamizer.encode_keys(attributes)\n\n        try:\n            raw_item = self.connection.get_item(self.table_name, raw_key,\n                                                consistent=consistent,\n                                                attributes_to_get=raw_attributes)\n        except exceptions.DynamoDBKeyNotFoundError:\n            raise exceptions.ItemNotFound()\n\n        return Item(self, raw_item)\n", "idx": 746}
{"namespace": "boto.dynamodb2.table.Table.has_item", "completion": "        try:\n            self.get_item(**kwargs)\n            return True\n        except exceptions.ItemNotFound:\n            return False\n", "idx": 747}
{"namespace": "boto.dynamodb2.table.Table._put_item", "completion": "        if not self.schema:\n            self.describe()\n        if expects is None:\n            expects = []\n        if isinstance(item_data, dict):\n            item_data = [item_data]\n        if isinstance(item_data, list):\n            for item in item_data:\n                if not isinstance(item, dict):\n                    raise TypeError(\"Item must be a dictionary\")\n                if not all(key in item for key in self.schema):\n                    raise ValueError(\"Item must contain all schema keys\")\n        else:\n            raise TypeError(\"Item must be a dictionary\")\n        if not isinstance(expects, list):\n            raise TypeError(\"Expects must be a list\")\n        if len(expects) > 0:\n            for expect in expects:\n                if not isinstance(expect, Expected):\n                    raise TypeError(\"Expects must be a list of Expected instances\")\n        raw_item = []\n        for item in item_data:\n            raw_item.append(self._encode_keys(item))\n        return self.connection.batch_write_item({self.table_name: [{'PutRequest': {'Item': raw_item}}]})\n", "idx": 748}
{"namespace": "boto.dynamodb2.table.Table.delete_item", "completion": "        raw_key = self._encode_keys(kwargs)\n        kwargs = {}\n        if expected is not None:\n            kwargs['expected'] = expected\n        if conditional_operator is not None:\n            kwargs['conditional_operator'] = conditional_operator\n        self.connection.delete_item(self.table_name, raw_key, **kwargs)\n        return True\n", "idx": 749}
{"namespace": "boto.dynamodb2.table.Table.get_key_fields", "completion": "        if not self.schema:\n            self.describe()\n        key_fields = []\n        for field in self.schema:\n            if field.is_hash_key or field.is_range_key:\n                key_fields.append(field.name)\n        return key_fields\n", "idx": 750}
{"namespace": "boto.dynamodb2.table.Table._build_filters", "completion": "        filters = {}\n        for key, value in filter_kwargs.items():\n            key_parts = key.rsplit('__', 1)\n            if len(key_parts) == 1:\n                key_parts.append(None)\n            field, operator = key_parts\n            if field not in self.schema:\n                raise exceptions.UnknownSchemaFieldError(\n                    \"%s was not found in the schema for %s.\" % (field, self.table_name)\n                )\n            if operator not in using:\n                raise exceptions.UnknownFilterOperatorError(\n                    \"%s is not a valid filter operator.\" % operator\n                )\n            filters[field] = {\n                'AttributeValueList': [self._dynamizer.encode(value)],\n                'ComparisonOperator': using[operator],\n            }\n        return filters\n", "idx": 751}
{"namespace": "boto.dynamodb2.table.Table.batch_get", "completion": "        results = ResultSet()\n        results.to_call(self._batch_get, keys, consistent, attributes)\n        return results\n", "idx": 752}
{"namespace": "boto.dynamodb2.table.Table.count", "completion": "        return self.query_count()\n", "idx": 753}
{"namespace": "boto.dynamodb2.table.BatchTable.put_item", "completion": "        if overwrite:\n            self._to_put.append({\n                'PutRequest': {\n                    'Item': data,\n                }\n            })\n        else:\n            self._to_put.append({\n                'PutRequest': {\n                    'Item': data,\n                }\n            })\n", "idx": 754}
{"namespace": "boto.dynamodb2.table.BatchTable.delete_item", "completion": "        self._to_delete.append(kwargs)\n\n        if self.should_flush():\n            self.flush()\n", "idx": 755}
{"namespace": "boto.dynamodb2.table.BatchTable.flush", "completion": "        # Prepare the data to be inserted or deleted.\n        put_data = []\n        for item in self._to_put:\n            put_data.append(self.table._encode_keys(item))\n\n        delete_data = []\n        for item in self._to_delete:\n            delete_data.append(self.table._encode_keys(item))\n\n        # Flush the data.\n        result = self.table.connection.batch_write_item(\n            request_items={\n                self.table.table_name: {\n                    'PutRequest': [{'Item': item} for item in put_data],\n                    'DeleteRequest': [{'Key': item} for item in delete_data],\n                }\n            }\n        )\n\n        # Handle any unprocessed items.\n        unprocessed_data = result.get('UnprocessedItems', {}).get(self.table.table_name, {})\n        if unprocessed_data:\n            self._unprocessed.extend(unprocessed_data)\n\n        # Clear the batch data.\n        self._to_put = []\n        self._to_delete = []\n\n        return True\n", "idx": 756}
{"namespace": "boto.dynamodb2.table.BatchTable.resend_unprocessed", "completion": "        while len(self._unprocessed) > 0:\n            # Get the unprocessed items\n            unprocessed = self._unprocessed\n            # Clear the unprocessed items\n            self._unprocessed = []\n            # Send the unprocessed items\n            self.flush()\n            # If the unprocessed items are still in the unprocessed items list, add them to the list of unprocessed items\n            if len(unprocessed) > 0:\n                self._unprocessed.extend(unprocessed)\n\n", "idx": 757}
{"namespace": "boto.dynamodb2.fields.BaseSchemaField.definition", "completion": "        return {\n            'AttributeName': self.name,\n            'AttributeType': self.data_type\n        }\n", "idx": 758}
{"namespace": "boto.dynamodb2.fields.BaseIndexField.definition", "completion": "        return [\n            {\n                'AttributeName': part.name,\n                'AttributeType': part.data_type\n            }\n            for part in self.parts\n        ]\n", "idx": 759}
{"namespace": "boto.dynamodb2.fields.BaseIndexField.schema", "completion": "        schema = []\n\n        for part in self.parts:\n            schema.append(part.schema())\n\n        return {\n            'IndexName': self.name,\n            'KeySchema': schema,\n            'Projection': {\n                'ProjectionType': self.projection_type,\n            }\n        }\n\n", "idx": 760}
{"namespace": "boto.dynamodb2.fields.GlobalBaseIndexField.schema", "completion": "        schema_data = super(GlobalBaseIndexField, self).schema()\n        schema_data['ProvisionedThroughput'] = {\n            'ReadCapacityUnits': self.throughput['read'],\n            'WriteCapacityUnits': self.throughput['write'],\n        }\n        return schema_data\n\n", "idx": 761}
{"namespace": "boto.dynamodb2.fields.GlobalIncludeIndex.schema", "completion": "        schema_data = super(GlobalIncludeIndex, self).schema()\n        schema_data['Projection']['NonKeyAttributes'] = self.includes_fields\n        return schema_data", "idx": 762}
{"namespace": "boto.dynamodb2.items.Item.get_keys", "completion": "        keys = {}\n        for key in self.table.key_fields:\n            keys[key] = self[key]\n        return keys\n", "idx": 763}
{"namespace": "boto.dynamodb2.items.Item.get_raw_keys", "completion": "        key_fields = self.table.get_key_fields()\n        key_data = {}\n\n        for key in key_fields:\n            key_data[key] = self._dynamizer.encode(self[key])\n\n        return key_data\n", "idx": 764}
{"namespace": "boto.dynamodb2.items.Item.build_expects", "completion": "        expects = {}\n        if fields is None:\n            fields = self.keys()\n        for field in fields:\n            if field in self._data:\n                if field in self._orig_data:\n                    if self._data[field] == self._orig_data[field]:\n                        # Unchanged\n                        expects[field] = {'Exists': True}\n                    else:\n                        # Modified\n                        expects[field] = {'Exists': True, 'Value': self._dynamizer.encode(self._data[field])}\n                else:\n                    # New\n                    expects[field] = {'Exists': True, 'Value': self._dynamizer.encode(self._data[field])}\n            else:\n                # Deleted\n                expects[field] = {'Exists': False}\n        return expects\n", "idx": 765}
{"namespace": "boto.dynamodb2.items.Item.prepare_full", "completion": "        data = {}\n        for key, value in self._data.items():\n            if self._is_storable(value):\n                data[key] = self._dynamizer.encode(value)\n        return data\n", "idx": 766}
{"namespace": "boto.dynamodb2.items.Item.prepare_partial", "completion": "        # This doesn't save on its own. Rather, we prepare the datastructure\n        # and hand-off to the table to handle creation/update.\n        final_data = {}\n        altered_fields = set()\n        for key, value in self._data.items():\n            if not self._is_storable(value):\n                continue\n            if key in self._orig_data:\n                if self._data[key] != self._orig_data[key]:\n                    final_data[key] = self._dynamizer.encode(self._data[key])\n                    altered_fields.add(key)\n            else:\n                final_data[key] = self._dynamizer.encode(self._data[key])\n                altered_fields.add(key)\n        return final_data, altered_fields\n", "idx": 767}
{"namespace": "boto.dynamodb2.items.Item.partial_save", "completion": "        if not self.needs_save():\n            return False\n\n        key_data = self.get_raw_keys()\n        update_data, fields = self.prepare_partial()\n\n        try:\n            self.table.connection.update_item(\n                self.table.table_name,\n                key_data,\n                update_data,\n                self.table.connection.aws_dynamodb_conditions('AND', fields),\n            )\n        except Exception as e:\n            return False\n\n        return True\n", "idx": 768}
{"namespace": "boto.dynamodb2.items.Item.save", "completion": "        key = self.get_keys()\n        # Build a new dict of only the data we're changing.\n        final_data = self.prepare_full()\n\n        # Build expectations of only the fields we're planning to update.\n        expects = self.build_expects()\n        returned = self.table._update_item(key, final_data, expects=expects)\n        # Mark the object as clean.\n        self.mark_clean()\n        return returned\n", "idx": 769}
{"namespace": "boto.dynamodb2.items.Item.delete", "completion": "        key = self.get_keys()\n        returned = self.table._delete_item(key)\n        return returned\n\n", "idx": 770}
{"namespace": "boto.dynamodb2.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 771}
{"namespace": "litecli.packages.parseutils.extract_tables", "completion": "    parsed = sqlparse.parse(sql)\n    if not parsed:\n        return []\n\n    # INSERT statements must stop looking for tables at the sign of first (\"(\")\n    insert_stmt = (parsed[0].tokens[0].ttype == Keyword.DML and parsed[0].tokens[0].value == \"INSERT\")\n\n    extracted_tables = []\n    for item in extract_from_part(parsed[0], stop_at_punctuation=insert_stmt):\n        if isinstance(item, IdentifierList):\n            # first we assume a JOIN, then we check for alias\n            if is_subselect(item.tokens[0]):\n                # it's a subselect, omit this part\n                continue\n            tables_or_cols = [\n                t.value\n                for t in item.tokens\n                if t.ttype is not Punctuation and not t.is_group\n            ]\n            if len(tables_or_cols) > 1:\n                # join\n                extracted_tables += [\n                    tuple(\n                        s.strip('\"') for s in table.split(\".\")\n                    )\n                    for table in tables_or_cols\n                ]\n            else:\n                # compare\n                tables_or_cols = tables_or_cols[0]\n                if \" as \" in tables_or_cols:\n                    tables_or_cols = tables_or_cols.split(\" as \")[1]\n                extracted_tables.append(\n                    tuple(\n                        s.strip('\"') for s in tables_or_cols.split(\".\")\n                    )\n                )\n        elif isinstance(item, Identifier):\n            # compare\n            if \" as \" in item.value:\n                tables_or_cols = item.value.split(\" as \")[1]\n            else:\n                tables_or_cols = item.value\n            extracted_tables.append(\n                tuple(\n                    s.strip('\"') for s in tables_or_cols.split(\".\")\n                )\n            )\n        elif isinstance(item, Function):\n            from_", "idx": 772}
{"namespace": "litecli.packages.parseutils.queries_start_with", "completion": "    for query in queries:\n        if query_starts_with(query, prefixes):\n            return True\n    return False\n\n", "idx": 773}
{"namespace": "litecli.packages.parseutils.is_destructive", "completion": "    destructive_keywords = [\"alter\", \"create\", \"drop\", \"truncate\"]\n    return queries_start_with(queries, destructive_keywords)\n\n", "idx": 774}
{"namespace": "litecli.packages.completion_engine.suggest_type", "completion": "    # The first token is the current cursor position\n    tokens = sqlparse.parse(text_before_cursor)[0].tokens\n    token_before_cursor = tokens[-1]\n\n    # If the query is an SQL special command, attempt to parse the scope\n    if isinstance(token_before_cursor, Where):\n        return suggest_special(text_before_cursor)\n\n    # If the query is a CREATE TABLE AS, suggest columns based on the scope\n    elif (\n        isinstance(token_before_cursor, Comparison)\n        and token_before_cursor.left.value.lower() == \"as\"\n    ):\n        return suggest_create_table_as(full_text, text_before_cursor)\n\n    # If the query is an INSERT statement, suggest columns based on the table scope\n    elif (\n        isinstance(token_before_cursor, Comparison)\n        and token_before_cursor.left.value.lower() == \"insert\"\n    ):\n        return suggest_insert(full_text, text_before_cursor)\n\n    # If the query is an UPDATE statement, suggest columns based on the table scope\n    elif (\n        isinstance(token_before_cursor, Comparison)\n        and token_before_cursor.left.value.lower() == \"update\"\n    ):\n        return suggest_update(full_text, text_before_cursor)\n\n    # If the query is a CREATE TABLE statement, suggest columns based on the table scope\n    elif (\n        isinstance(token_before_cursor, Comparison)\n        and token_before_cursor.left.value.lower() == \"table\"\n    ):\n        return suggest_create_table(full_text, text_before_cursor)\n\n    # If the query is a CREATE VIEW statement, suggest keywords\n    elif (\n        isinstance(token_before_cursor, Comparison)\n        and token_before_cursor.left.value.lower() == \"view\"\n    ):\n        return suggest_create_view(full_text, text_before_cursor)\n\n    # If the query is a CREATE DATABASE statement, suggest keywords\n    elif (\n        isinstance(", "idx": 775}
{"namespace": "datasette.plugins.get_plugins", "completion": "    plugins = []\n    for plugin in pm.get_plugins():\n        plugin_name = plugin.__name__.split(\".\")[-1]\n        plugin_info = {\n            \"name\": plugin_name,\n            \"static_path\": pkg_resources.resource_filename(plugin_name, \"static\"),\n            \"templates_path\": pkg_resources.resource_filename(\n                plugin_name, \"templates\"\n            ),\n            \"hooks\": {},\n        }\n        for hook_name, methods in pm.list_plugin_hooks():\n            if plugin_name in methods:\n                plugin_info[\"hooks\"][hook_name] = methods[plugin_name]\n        if hasattr(plugin, \"__version__\"):\n            plugin_info[\"version\"] = plugin.__version__\n        if hasattr(plugin, \"__project__\"):\n            plugin_info[\"project\"] = plugin.__project__\n        plugins.append(plugin_info)\n    return plugins", "idx": 776}
{"namespace": "datasette.facets.ColumnFacet.suggest", "completion": "        facet_results = []\n        row_count = await self.get_row_count()\n        columns = await self.get_columns(self.sql, self.params)\n        facet_size = self.get_facet_size()\n        for column in columns:\n            if column == \"rowid\" or column.startswith(\"rowid,\"):\n                continue\n            distinct_query = f\"select count({column}) as total, {column} as value from ({self.sql}) group by {column} order by total desc limit {facet_size + 1}\"\n            try:\n                results = await self.ds.execute(\n                    self.database, distinct_query, self.params\n                )\n            except sqlite3.OperationalError as e:\n                if e.args == (\"no such column: %s\" % column,):\n                    continue\n                else:\n                    raise\n            num_distinct = len(results.rows)\n            if (\n                1 <= num_distinct <= row_count\n                and num_distinct <= facet_size\n                and any(cell[0] > 1 for cell in results.rows)\n            ):\n                facet_results.append(\n                    {\n                        \"name\": column,\n                        \"toggle_url\": self.ds.absolute_url(\n                            self.request,\n                            path_with_added_args(self.request, {\"_facet\": column}),\n                        ),\n                    }\n                )\n        return facet_results\n", "idx": 777}
{"namespace": "datasette.facets.ColumnFacet.facet_results", "completion": "        facet_results = []\n        facets_timed_out = []\n        facet_size = self.get_facet_size()\n        facet_configs = self.get_configs()\n        facet_sql = self.sql\n        facet_params = self.params\n        facet_name = None\n        facet_type = None\n        facet_toggle_url = None\n        facet_results = []\n        facets_timed_out = []\n        facet_size = self.get_facet_size()\n        facet_configs = self.get_configs()\n        facet_sql = self.sql\n        facet_params = self.params\n        facet_name = None\n        facet_type = None\n        facet_toggle_url = None\n        facet_results = []\n        facets_timed_out = []\n        facet_size = self.get_facet_size()\n        facet_configs = self.get_configs()\n        facet_sql = self.sql\n        facet_params = self.params\n        facet_name = None\n        facet_type = None\n        facet_toggle_url = None\n        facet_results = []\n        facets_timed_out = []\n        facet_size = self.get_facet_size()\n        facet_configs = self.get_configs()\n        facet_sql = self.sql\n        facet_params = self.params\n        facet_name = None\n        facet_type = None\n        facet_toggle_url = None\n        facet_results = []\n        facets_timed_out = []\n        facet_size = self.get_facet_size()\n        facet_configs = self.get_configs()\n        facet_sql = self.sql\n        facet_params = self.params\n        facet_name = None\n        facet_type = None\n        facet_toggle_url = None\n        facet_results = []\n        facets_timed_out = []", "idx": 778}
{"namespace": "datasette.facets.ArrayFacet.suggest", "completion": "        row_count = await self.get_row_count()\n        columns = await self.get_columns(self.sql, self.params)\n        facet_size = self.get_facet_size()\n        suggested_facets = []\n        already_enabled = [c[\"config\"][\"simple\"] for c in self.get_configs()]\n        for column in columns:\n            if column in already_enabled:\n                continue\n            facet_sql = \"\"\"\n                select {col} as value, count(*) as count from (\n                    {sql}\n                )\n                where {col} is not null\n                group by {col} order by count desc, value limit {limit}\n            \"\"\".format(\n                col=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            try:\n                facet_rows_results = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_suggest_time_limit_ms\"),\n                )\n                facet_rows = facet_rows_results.rows[:facet_size]\n                if self.table:\n                    # Attempt to expand foreign keys into labels\n                    values = [row[\"value\"] for row in facet_rows]\n                    expanded = await self.ds.expand_foreign_keys(\n                        self.database, self.table, column, values\n                    )\n                else:\n                    expanded = {}\n                for row in facet_rows:\n                    if row[\"value\"] is None:\n                        continue\n                    if not self._is_json_array_of_strings(row[\"value\"]):\n                        continue\n                    column_qs = column\n                    if column.startswith(\"_\"):\n                        column_qs = \"{}__exact\".format(column)\n                    suggested_facets.append(\n                        {\n                            \"name\": column,\n                            \"type\": self.type,", "idx": 779}
{"namespace": "datasette.facets.ArrayFacet.facet_results", "completion": "        facet_results = []\n        facets_timed_out = []\n        facet_size = self.get_facet_size()\n        for source_and_config in self.get_configs():\n            config = source_and_config[\"config\"]\n            source = source_and_config[\"source\"]\n            column = config.get(\"column\") or config[\"simple\"]\n            facet_sql = \"\"\"\n                select value, count(*) as n\n                from (\n                    select json_each_text({column}) as x\n                    from (\n                        {sql}\n                    )\n                )\n                where x.value is not null\n                group by x.value\n                order by n desc\n                limit {limit}\n            \"\"\".format(\n                column=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            try:\n                facet_rows_results = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n                facet_rows = facet_rows_results.rows[:facet_size]\n                facet_results.append(\n                    {\n                        \"name\": column,\n                        \"type\": self.type,\n                        \"results\": [\n                            {\n                                \"value\": r[\"value\"],\n                                \"label\": r[\"value\"],\n                                \"count\": r[\"n\"],\n                                \"toggle_url\": self.ds.absolute_url(\n                                    self.request,\n                                    self.ds.urls.path(\n                                        path_with_removed_args(\n                                            self.request, {\"_facet\": column}\n                                        )\n                                    )\n                                    + self.ds.render_json(\n                                        {\"_facet_array\": column, column: r[\"value\"]}\n                                    ),\n                                ),\n                                \"selected\": False,", "idx": 780}
{"namespace": "datasette.facets.DateFacet.facet_results", "completion": "        facet_results = []\n        facets_timed_out = []\n\n        facet_size = self.get_facet_size()\n        for source_and_config in self.get_configs():\n            config = source_and_config[\"config\"]\n            source = source_and_config[\"source\"]\n            column = config.get(\"column\") or config[\"simple\"]\n            facet_sql = \"\"\"\n                select date({column}) as value, count(*) as count\n                from (\n                    {sql}\n                )\n                where {column} glob \"????-??-*\"\n                group by value\n                order by value desc\n                limit {limit}\n            \"\"\".format(\n                column=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            try:\n                facet_rows_results = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n                facet_results_values = []\n                facet_results.append(\n                    {\n                        \"name\": column,\n                        \"type\": self.type,\n                        \"results\": facet_results_values,\n                        \"hideable\": source != \"metadata\",\n                        \"toggle_url\": self.ds.urls.path(\n                            path_with_removed_args(self.request, {\"_facet_date\": column})\n                        ),\n                        \"truncated\": len(facet_rows_results) > facet_size,\n                    }\n                )\n                facet_rows = facet_rows_results.rows[:facet_size]\n                pairs = self.get_querystring_pairs()\n                for row in facet_rows:\n                    value = str(row[\"value\"])\n                    selected = (f\"{column}__date\", value) in pairs\n                    if selected:\n                        toggle_path = path_with", "idx": 781}
{"namespace": "datasette.app.Datasette.invoke_startup", "completion": "        # Execute the startup process\n        await self.refresh_schemas()\n", "idx": 782}
{"namespace": "datasette.app.Datasette.get_database", "completion": "        if route:\n            return self.databases[route]\n        else:\n            for database in self.databases.values():\n                if database.name != \"_internal\":\n                    return database\n", "idx": 783}
{"namespace": "datasette.app.Datasette.add_database", "completion": "        # Create a copy of the existing databases\n        databases = self.databases.copy()\n\n        # Assign a unique name to the new database if no name is provided\n        if name is None:\n            name = \"new_db\"\n\n        # Check if the name already exists\n        if name in databases:\n\n            # If the name already exists, append a number to make it unique\n            name += str(len(databases))\n\n        # Assign the name and route to the new database\n        db.name = name\n        db.route = route\n\n        # Add the new database to the copied databases dictionary\n        databases[name] = db\n\n        # Assign the copied databases dictionary back to the instance\n        self.databases = databases\n\n        return db\n", "idx": 784}
{"namespace": "datasette.app.Datasette.ensure_permissions", "completion": "        for permission in permissions:\n            if isinstance(permission, str):\n                action = permission\n                resource = None\n            else:\n                action = permission[0]\n                resource = permission[1]\n            if not await self.permission_allowed(actor, action, resource):\n                raise Forbidden(f\"{action} not allowed on {resource}\")\n", "idx": 785}
{"namespace": "datasette.app.Datasette.check_visibility", "completion": "        if permissions is None:\n            permissions = []\n        if resource is not None and isinstance(resource, tuple):\n            resource = resource[0]\n        if resource is not None and resource.startswith(\"-\"):\n            resource = resource[1:]\n        if resource is not None and resource.startswith(\"_\"):\n            resource = None\n        if resource is not None and resource.startswith(\"config:\"):\n            resource = None\n        if resource is not None and resource.startswith(\"instance:\"):\n            resource = None\n        if resource is not None and resource.startswith(\"staff:\"):\n            resource = None\n        if resource is not None and resource.startswith(\"staff_config:\"):\n            resource = None\n        if resource is not None and resource.startswith(\"user:\"):\n            resource = None\n        if resource is not None and resource.startswith(\"user_config:\"):\n            resource = None\n        if resource is not None and resource.startswith(\"table:\"):\n            resource = None\n        if resource is not None and resource.startswith(\"database:\"):\n            resource = None\n        if resource is not None and resource.startswith(\"hidden:\"):\n            resource = None\n        if resource is not None and resource.startswith(\"hidden_config:\"):\n            resource = None\n        if resource is not None and resource.startswith(\"plugin:\"):\n            resource = None\n        if resource is not None and resource.startswith(\"plugin_config:\"):\n            resource = None\n        if resource is not None and resource.startswith(\"allow_sql:\"):\n            resource = None\n        if resource is not None and resource.startswith(\"allow_sql_config:\"):\n            resource = None\n        if resource is not None and resource.startswith(\"allow_csv:\"):\n            resource = None\n        if resource is not None and resource.startswith(\"allow_csv_config:\"):\n            resource = None\n        if resource is not None and resource.startswith(\"allow_search:\"):\n            resource = None\n        if resource is not None and resource.startswith(\"allow_search_config:\"):\n            resource = None\n        if resource is not None and resource.startswith(\"allow_", "idx": 786}
{"namespace": "datasette.app.Datasette.render_template", "completion": "        if not self._startup_called:\n            raise Exception(\"Datasette.render_template called before startup()\")\n\n        if context is None:\n            context = {}\n\n        if not isinstance(templates, (list, tuple)):\n            templates = [templates]\n\n        # Add some useful things to the context\n        context.update(\n            {\n                \"app_css_hash\": self.app_css_hash(),\n                \"app_title\": self.setting(\"title\"),\n                \"base_url\": self.config(\"base_url\"),\n                \"datasette_version\": __version__,\n                \"default_page_size\": self.default_page_size(),\n                \"files\": self.files(),\n                \"get_database\": lambda database: self.databases[database],\n                \"get_view_name\": lambda: view_name or \"\",\n                \"metadata\": self._metadata,\n                \"metadata_tables\": self.metadata_tables(),\n                \"plugins\": self._plugins(request),\n                \"show_messages\": lambda: bool(request.scope.get(\"messages\")),\n                \"show_menu_links\": self.setting(\"show_menu_links\"),\n                \"show_hide_table_options\": self.setting(\"show_hide_table_options\"),\n                \"show_hide_facet_options\": self.setting(\"show_hide_facet_options\"),\n                \"show_facet_extremes\": self.setting(\"show_facet_extremes\"),\n                \"show_json_facet_values\": self.setting(\"show_json_facet_values\"),\n                \"show_query_log\": self.setting(\"show_query_log\"),\n                \"show_sql_query_button\": self.setting(\"show_sql_query_button\"),\n                \"show_table_options\": self.setting(\"show_table_options\"),\n                \"support_level\": self.setting(\"support_level\"),\n                \"table_columns_cache_time\": self.setting(\"table_columns_cache_time\"),\n                \"template_debug\": self.template_debug,\n                \"urls\": self.urls,\n                \"version_note\": self.version", "idx": 787}
{"namespace": "datasette.app.DatasetteClient.get", "completion": "        path = self._fix(path)\n        return await self.request(\"get\", path, **kwargs)\n", "idx": 788}
{"namespace": "datasette.utils.asgi.Request.full_path", "completion": "        path = self.path\n        if self.query_string:\n            return \"{}?{}\".format(path, self.query_string)\n        return path\n", "idx": 789}
{"namespace": "datasette.utils.asgi.Request.post_body", "completion": "        body = b''\n        more_body = True\n        while more_body:\n            message = await self.receive()\n            if message['type'] == 'http.request':\n                body += message.get('body', b'')\n                if not message.get('more_body'):\n                    more_body = False\n        return body\n", "idx": 790}
{"namespace": "datasette.utils.asgi.Request.fake", "completion": "        path, _, query_string = path_with_query_string.partition(\"?\")\n        query_string = query_string or \"\"\n        scope = {\n            \"http_version\": \"1.1\",\n            \"method\": method,\n            \"path\": path,\n            \"raw_path\": path.encode(\"latin-1\"),\n            \"query_string\": query_string.encode(\"latin-1\"),\n            \"root_path\": \"\",\n            \"scheme\": scheme,\n            \"type\": \"http\",\n        }\n        if url_vars:\n            scope[\"url_route\"] = {\"kwargs\": url_vars}\n        return cls(scope, None)\n\n", "idx": 791}
{"namespace": "datasette.utils.asgi.Response.asgi_send", "completion": "        # Prepare the headers\n        headers = self.headers.copy()\n        headers[\"content-type\"] = self.content_type\n        if self._set_cookie_headers:\n            headers[\"set-cookie\"] = self._set_cookie_headers\n\n        # Prepare the body\n        if isinstance(self.body, str):\n            body = self.body.encode(\"utf-8\")\n        else:\n            body = self.body\n\n        # Send the response\n        await asgi_send(send, body, self.status, headers)\n\n", "idx": 792}
{"namespace": "datasette.utils.asgi.Response.set_cookie", "completion": "        cookie = SimpleCookie()\n        cookie[key] = value\n        for var_name, var_value in [\n            (\"max-age\", max_age),\n            (\"expires\", expires),\n            (\"path\", path),\n            (\"domain\", domain),\n            (\"secure\", secure),\n            (\"httponly\", httponly),\n            (\"samesite\", samesite),\n        ]:\n            if var_value is not None:\n                cookie[key][var_name] = var_value\n        self._set_cookie_headers.append(cookie.output(header=\"\").strip())\n\n", "idx": 793}
{"namespace": "datasette.utils.asgi.Response.html", "completion": "        return cls(body, status, headers, \"text/html; charset=utf-8\")\n", "idx": 794}
{"namespace": "datasette.utils.asgi.Response.text", "completion": "        return cls(\n            body,\n            status=status,\n            headers=headers,\n            content_type=\"text/plain; charset=utf-8\",\n        )\n", "idx": 795}
{"namespace": "datasette.utils.asgi.Response.json", "completion": "        return cls(\n            json.dumps(body, default=default),\n            status=status,\n            headers=headers,\n            content_type=\"application/json; charset=utf-8\",\n        )\n", "idx": 796}
{"namespace": "datasette.utils.asgi.Response.redirect", "completion": "        headers = headers or {}\n        headers[\"Location\"] = path\n        return cls(\"\", status=status, headers=headers)\n\n", "idx": 797}
{"namespace": "datasette.utils.validate_sql_select", "completion": "    # Remove any comments from the SQL statement\n    sql = remove_comments(sql)\n\n    # Convert the SQL statement to lowercase\n    sql = sql.lower()\n\n    # Check if the SQL statement matches any of the allowed SQL patterns\n    for allowed_sql_re in allowed_sql_res:\n        if allowed_sql_re.match(sql):\n            return\n\n    # Check if the SQL statement matches any of the disallowed SQL patterns\n    for disallowed_sql_re, error_message in disallawed_sql_res:\n        if disallowed_sql_re.search(sql):\n            raise InvalidSql(error_message)\n\n    # If the SQL statement does not match any of the allowed or disallowed SQL patterns, raise an invalid SQL exception\n    raise InvalidSql(\"Statement must be a SELECT\")\n\n", "idx": 798}
{"namespace": "datasette.utils.escape_css_string", "completion": "    s = s.replace(\"\\r\\n\", \"\\n\")\n    s = _css_re.sub(lambda m: f\"\\\\{ord(m.group(0))}\", s)\n    return s\n\n", "idx": 799}
{"namespace": "datasette.utils.detect_fts", "completion": "    # Get the tables in the database\n    tables = [r[0] for r in conn.execute('select name from sqlite_master where type=\"table\"').fetchall()]\n\n    # Get the FTS virtual tables in the database by checking for the presence of the \"fts\" and \"content\" entries in the\n    # \"sql\" entry of the \"sqlite_master\" table\n    fts_tables = [\n        r[1] for r in conn.execute(\n            'select name, sql from sqlite_master where type=\"table\" and sql like \"%fts%\" and sql like \"%content%\"'\n        ).fetchall()\n    ]\n\n    # Get the table entries in the \"sqlite_master\" table\n    table_entries = conn.execute(f'select * from sqlite_master where type=\"table\" and name=\"{table}\"').fetchall()\n\n    # Get the FTS virtual table associated with the table\n    fts_table = next((t for t in fts_tables if any(f'\"{table}\"' in r[1] for r in table_entries)), None)\n\n    return fts_table\n\n", "idx": 800}
{"namespace": "datasette.utils.is_url", "completion": "    return isinstance(value, str) and value.startswith(\"http://\") or value.startswith(\"https://\") and not bool(whitespace_re.search(value))\n\n", "idx": 801}
{"namespace": "datasette.utils.to_css_class", "completion": "    if css_class_re.match(s):\n        return s\n    else:\n        return css_invalid_chars_re.sub(\"\", s) + \"-\" + hashlib.md5(s.encode(\"utf-8\")).hexdigest()[:6]\n\n", "idx": 802}
{"namespace": "datasette.utils.escape_fts", "completion": "    if not query:\n        return query\n    query = _escape_fts_re.findall(query)\n    query = [term for term in query if term and term != '\"\"']\n    return ''.join(f'\"{term}\"' for term in query)\n\n", "idx": 803}
{"namespace": "datasette.utils.check_connection", "completion": "    cursor = conn.cursor()\n    cursor.execute(\"SELECT * FROM sqlite_master WHERE type='table'\")\n    tables = [x[1] for x in cursor.fetchall()]\n    for table in tables:\n        cursor.execute(f\"PRAGMA table_info({table})\")\n        cursor.fetchall()\n    conn.close()\n\n", "idx": 804}
{"namespace": "datasette.utils.parse_metadata", "completion": "    try:\n        metadata = json.loads(content)\n    except json.JSONDecodeError:\n        try:\n            metadata = yaml.safe_load(content)\n        except yaml.YAMLError:\n            raise BadMetadataError(\n                \"Unable to parse metadata. Please ensure that the metadata is in JSON or YAML format.\"\n            )\n    return metadata\n\n", "idx": 805}
{"namespace": "datasette.utils.call_with_supported_arguments", "completion": "    parameters = inspect.signature(fn).parameters.keys()\n    call_with = {}\n    for parameter in parameters:\n        if parameter in kwargs:\n            call_with[parameter] = kwargs[parameter]\n    return fn(**call_with)\n\n", "idx": 806}
{"namespace": "datasette.utils.derive_named_parameters", "completion": "    # Strip any trailing semicolon from the SQL query\n    sql = sql.strip().rstrip(\";\")\n\n    # Find all possible named parameters in the SQL query\n    named_parameters = _re_named_parameter.findall(sql)\n\n    # Construct the \"explain\" statement\n    explain_sql = f\"explain {sql}\"\n\n    # Execute the \"explain\" statement with a dictionary of named parameters, where the values are set to None\n    try:\n        explain_results = await db.execute(\n            explain_sql,\n            {parameter: None for parameter in named_parameters},\n        )\n    except sqlite3.OperationalError as e:\n        # Return the list of possible named parameters found in the input SQL query\n        return named_parameters\n\n    # Return the list of named parameters identified as variables in the \"explain\" results, after removing the leading \":\" character\n    return [\n        parameter[1:]\n        for parameter in named_parameters\n        if any(\n            [\n                parameter in row[0]\n                for row in explain_results.rows\n                if row[1] == \"VARIABLE\"\n            ]\n        )\n    ]\n\n", "idx": 807}
{"namespace": "pyramid.path.Resolver.get_package_name", "completion": "        if self.package == CALLER_PACKAGE:\n            return package_name(caller_module(2))\n        else:\n            return package_name(self.package)\n", "idx": 808}
{"namespace": "pyramid.path.Resolver.get_package", "completion": "        if self.package is CALLER_PACKAGE:\n            return caller_package()\n        else:\n            return self.package\n", "idx": 809}
{"namespace": "pyramid.path.DottedNameResolver.resolve", "completion": "        if ':' in dotted:\n            return self._resolve_pkg_resources(dotted)\n        else:\n            return self._resolve_zope(dotted)\n", "idx": 810}
{"namespace": "pyramid.path.DottedNameResolver.maybe_resolve", "completion": "        if isinstance(dotted, str):\n            return self.resolve(dotted)\n        else:\n            return dotted\n\n", "idx": 811}
{"namespace": "pyramid.path.PkgResourcesAssetDescriptor.abspath", "completion": "        return self.pkg_resources.resource_filename(self.pkg_name, self.path)\n", "idx": 812}
{"namespace": "pyramid.renderers.render_to_response", "completion": "    if response is None:\n        response = request.response\n    response.text = render(\n        renderer_name, value, request=request, package=package\n    )\n    return response\n\n", "idx": 813}
{"namespace": "pyramid.renderers.JSON.add_adapter", "completion": "        self.components.registerAdapter(adapter, (type_or_iface,), IJSONAdapter)\n", "idx": 814}
{"namespace": "pyramid.renderers.RendererHelper.settings", "completion": "        return self.registry.settings\n", "idx": 815}
{"namespace": "pyramid.renderers.RendererHelper.render_view", "completion": "        # Create a dictionary called system to pass to the renderer\n        system = {\n            'view': view,\n            'renderer_name': self.name,\n            'renderer_info': self,\n            'context': context,\n            'request': request,\n            'response': response,\n            'csrf_token': get_csrf_token(request),\n        }\n\n        # Render the view using the renderer\n        result = self.renderer(system)\n\n        # Set the response body to the result of the renderer\n        response.text = result\n", "idx": 816}
{"namespace": "pyramid.renderers.RendererHelper.render", "completion": "        system = self.get_system_values(system_values, request)\n        return self.render_to_response(value, system, request=request)\n", "idx": 817}
{"namespace": "pyramid.renderers.RendererHelper.render_to_response", "completion": "        from pyramid.events import BeforeRender\n        renderer = self.renderer\n        if system_values is None:\n            system_values = {\n                'view': None,\n                'renderer_name': self.name,  # b/c\n                'renderer_info': self,\n                'context': getattr(request, 'context', None),\n                'request': request,\n                'req': request,\n                'get_csrf_token': partial(get_csrf_token, request),\n            }\n\n        system_values = BeforeRender(system_values, value)\n\n        registry = self.registry\n        registry.notify(system_values)\n        result = renderer(value, system_values)\n        response = request.response\n        response.text = result\n        return response\n", "idx": 818}
{"namespace": "pyramid.renderers.RendererHelper.clone", "completion": "        if name is None:\n            name = self.name\n        if package is None:\n            package = self.package\n        if registry is None:\n            registry = self.registry\n        return RendererHelper(name, package, registry)\n\n", "idx": 819}
{"namespace": "pyramid.urldispatch.RoutesMapper.get_routes", "completion": "        if include_static:\n            return self.routelist + self.static_routes\n        return self.routelist\n", "idx": 820}
{"namespace": "pyramid.urldispatch.RoutesMapper.connect", "completion": "        route = Route(name, pattern, factory, predicates, pregenerator)\n        self.routes[name] = route\n        if static:\n            self.static_routes.append(route)\n        else:\n            self.routelist.append(route)\n        return route\n", "idx": 821}
{"namespace": "pyramid.testing.DummyTemplateRenderer.assert_", "completion": "        for k, v in kw.items():\n            if self._received.get(k, _marker) != v:\n                raise AssertionError(\n                    'Expected %s=%r but got %r' % (k, v, self._received.get(k))\n                )\n        return True\n\n", "idx": 822}
{"namespace": "pyramid.testing.DummyResource.__getitem__", "completion": "        return self.subs[name]\n\n", "idx": 823}
{"namespace": "pyramid.testing.DummyResource.clone", "completion": "        if __name__ is _marker:\n            __name__ = self.__name__\n        if __parent__ is _marker:\n            __parent__ = self.__parent__\n        return self.__class__(\n            __name__=__name__, __parent__=__parent__, **self.kw\n        ).__of__(self.__parent__)\n\n", "idx": 824}
{"namespace": "pyramid.testing.DummySession.get_csrf_token", "completion": "        token = self.get('_csrft_', None)\n        if token is None:\n            token = self.new_csrf_token()\n        return token\n\n", "idx": 825}
{"namespace": "pyramid.testing.DummyRequest.response", "completion": "        from pyramid.response import Response\n        from pyramid.registry import Registry\n        from pyramid.interfaces import IResponseFactory\n        registry = Registry()\n        registry.registerUtility(Response, IResponseFactory)\n        return registry.queryUtility(IResponseFactory)(self)\n", "idx": 826}
{"namespace": "pyramid.testing.DummyRendererFactory.add", "completion": "        self.renderers[spec] = renderer\n", "idx": 827}
{"namespace": "pyramid.authorization.ACLAuthorizationPolicy.principals_allowed_by_permission", "completion": "        return self.helper.principals_allowed_by_permission(context, permission)\n", "idx": 828}
{"namespace": "pyramid.url.URLMethodsMixin.route_url", "completion": "        app_url, qs, frag = parse_url_overrides(self, kw)\n        return self.application_url + self.route_path(route_name, *elements, **kw) + qs + frag\n", "idx": 829}
{"namespace": "pyramid.predicates.CustomPredicate.text", "completion": "        if hasattr(self.func, '__text__'):\n            return self.func.__text__()\n        else:\n            return 'custom predicate: %s' % (self.func.__doc__ or '')\n", "idx": 830}
{"namespace": "pyramid.threadlocal.ThreadLocalManager.pop", "completion": "        return self.stack.pop()\n", "idx": 831}
{"namespace": "pyramid.threadlocal.ThreadLocalManager.get", "completion": "        if self.stack:\n            return self.stack[-1]\n        else:\n            return self.default()\n\n", "idx": 832}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.authenticated_userid", "completion": "        debug = self.debug\n        identity = self._get_identity(request)\n        if identity is None:\n            debug and self._log(\n                'no repoze.who identity in request; returning None',\n                'authenticated_userid',\n                request,\n            )\n            return None\n\n        userid = identity.get('repoze.who.userid')\n        if userid is None:\n            debug and self._log(\n                'no userid in repoze.who identity; returning None',\n                'authenticated_userid',\n                request,\n            )\n            return None\n\n        if self._clean_principal(userid) is None:\n            debug and self._log(\n                (\n                    'userid %r is disallowed by any built-in Pyramid '\n                    'security policy, returning None' % (userid,)\n                ),\n                'authenticated_userid',\n                request,\n            )\n            return None\n\n        if self.callback is None:\n            debug and self._log(\n                'no groupfinder callback; returning userid %r' % (userid,),\n                'authenticated_userid',\n                request,\n            )\n            return userid\n\n        callback_ok = self.callback(userid, request)\n        if callback_ok is not None:  # is not None!\n            debug and self._log(\n                'groupfinder callback returned %r as groups' % (callback_ok,),\n                'authenticated_userid',\n                request,\n            )\n            return userid\n\n        debug and self._log(\n            'groupfinder callback returned None; returning None',\n            'authenticated_userid',\n            request,\n        )\n", "idx": 833}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.unauthenticated_userid", "completion": "        identity = self._get_identity(request)\n        if identity is None:\n            self.debug and self._log(\n                'repoze.who identity is None, returning None',\n                'unauthenticated_userid',\n                request,\n            )\n            return None\n\n        userid = identity['repoze.who.userid']\n\n        if userid is None:\n            self.debug and self._log(\n                'repoze.who.userid is None, returning None' % userid,\n                'unauthenticated_userid',\n                request,\n            )\n            return None\n\n        if self._clean_principal(userid) is None:\n            self.debug and self._log(\n                (\n                    'use of userid %r is disallowed by any built-in Pyramid '\n                    'security policy, returning None' % userid\n                ),\n                'unauthenticated_userid',\n                request,\n            )\n            return None\n\n        return userid\n", "idx": 834}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.forget", "completion": "        identifier = self._get_identifier(request)\n        if identifier is None:\n            return []\n        environ = request.environ\n        return identifier.forget(environ)\n\n", "idx": 835}
{"namespace": "pyramid.authentication.AuthTktAuthenticationPolicy.unauthenticated_userid", "completion": "        cookie = self.cookie\n        cookie_name = cookie.cookie_name\n        cookie_value = request.cookies.get(cookie_name)\n        if cookie_value is None:\n            return None\n        try:\n            userid, userdata, timestamp, cookie_data = cookie.parse_ticket(\n                cookie_value, request.remote_addr\n            )\n        except AuthTktError:\n            return None\n        if timestamp < time_mod.time() - cookie.timeout:\n            return None\n        return userid\n", "idx": 836}
{"namespace": "pyramid.authentication.SessionAuthenticationHelper.remember", "completion": "        request.session[self.userid_key] = userid\n        return []\n", "idx": 837}
{"namespace": "pyramid.authentication.SessionAuthenticationHelper.forget", "completion": "        request.session.pop(self.userid_key, None)\n        return []\n", "idx": 838}
{"namespace": "pyramid.authentication.BasicAuthAuthenticationPolicy.unauthenticated_userid", "completion": "        authorization = request.headers.get('Authorization')\n        if not authorization:\n            return None\n        try:\n            authmeth, auth = authorization.split(' ', 1)\n        except ValueError:  # not enough values to unpack\n            return None\n        if authmeth.lower() != 'basic':\n            return None\n        try:\n            auth = b64decode(auth.strip().encode('ascii')).decode('ascii')\n        except (binascii.Error, UnicodeDecodeError):  # can't decode\n            return None\n        try:\n            username, password = auth.split(':', 1)\n        except ValueError:  # not enough values to unpack\n            return None\n        return username\n", "idx": 839}
{"namespace": "pyramid.request.CallbackMethodsMixin._process_response_callbacks", "completion": "        for callback in self.response_callbacks:\n            callback(self, response)\n\n", "idx": 840}
{"namespace": "pyramid.request.CallbackMethodsMixin._process_finished_callbacks", "completion": "        callbacks = self.finished_callbacks\n        while callbacks:\n            callback = callbacks.popleft()\n            callback(self)\n\n", "idx": 841}
{"namespace": "pyramid.request.Request.session", "completion": "        # This is a reified property, so it will only be called once.\n        # We can't use a reified method, because we need to pass in the\n        # request object.\n\n        # This is a reified property, so it will only be called once.\n        # We can't use a reified method, because we need to pass in the\n        # request object.\n\n        # This is a reified property, so it will only be called once.\n        # We can't use a reified method, because we need to pass in the\n        # request object.\n\n        # This is a reified property, so it will only be called once.\n        # We can't use a reified method, because we need to pass in the\n        # request object.\n\n        # This is a reified property, so it will only be called once.\n        # We can't use a reified method, because we need to pass in the\n        # request object.\n\n        # This is a reified property, so it will only be called once.\n        # We can't use a reified method, because we need to pass in the\n        # request object.\n\n        # This is a reified property, so it will only be called once.\n        # We can't use a reified method, because we need to pass in the\n        # request object.\n\n        # This is a reified property, so it will only be called once.\n        # We can't use a reified method, because we need to pass in the\n        # request object.\n\n        # This is a reified property, so it will only be called once.\n        # We can't use a reified method, because we need to pass in the\n        # request object.\n\n        # This is a reified property, so it will only be called once.\n        # We can't use a reified method, because we need to pass in the\n        # request object.\n\n        # This is a reified property, so it will only be called once.\n        # We can't use a reified method, because we need to pass in the\n        # request object.\n\n        # This", "idx": 842}
{"namespace": "pyramid.request.RequestLocalCache.get_or_create", "completion": "        if creator is None:\n            creator = self._creator\n        if creator is None:\n            raise ValueError('creator function is required')\n        try:\n            return self._store[request]\n        except KeyError:\n            value = self._store[request] = creator(request)\n            return value\n", "idx": 843}
{"namespace": "pyramid.request.RequestLocalCache.set", "completion": "        if request not in self._store:\n            request.add_finished_callback(self._remove_request)\n        self._store[request] = value\n", "idx": 844}
{"namespace": "pyramid.i18n.Localizer.pluralize", "completion": "        if self.pluralizer is None:\n            self.pluralizer = Pluralizer(self.translations)\n        return self.pluralizer(singular, plural, n, domain=domain, mapping=mapping)\n", "idx": 845}
{"namespace": "pyramid.i18n.Translations.load", "completion": "        if locales is None:\n            locales = []\n        if not isinstance(locales, (list, tuple)):\n            locales = [locales]\n        if not locales:\n            locales = [None]\n        if dirname is None:\n            dirname = os.path.join(os.path.dirname(__file__), 'i18n')\n        filenames = []\n        for locale in locales:\n            if locale is None:\n                locale = ''\n            elif isinstance(locale, Locale):\n                locale = str(locale)\n            filenames.append(os.path.join(dirname, locale, 'LC_MESSAGES',\n                                          '%s.mo' % domain))\n        for filename in filenames:\n            try:\n                return cls(open(filename, 'rb'))\n            except IOError:\n                continue\n        return gettext.NullTranslations()\n", "idx": 846}
{"namespace": "pyramid.i18n.Translations.add", "completion": "        if translations.domain != self.domain:\n            self._domains[translations.domain] = translations\n        elif merge:\n            self.merge(translations)\n        return self\n", "idx": 847}
{"namespace": "pyramid.i18n.Translations.merge", "completion": "        self._catalog.update(translations._catalog)\n        self.files.extend(translations.files)\n        return self\n", "idx": 848}
{"namespace": "pyramid.i18n.LocalizerRequestMixin.locale_name", "completion": "        return negotiate_locale_name(self)\n\n", "idx": 849}
{"namespace": "pyramid.csrf.LegacySessionCSRFStoragePolicy.check_csrf_token", "completion": "        expected_token = self.get_csrf_token(request)\n        if expected_token is None:\n            return False\n        return bytes_(expected_token) == bytes_(supplied_token)\n\n", "idx": 850}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.new_csrf_token", "completion": "        token = self._token_factory()\n        request.session[self.key] = token\n        return token\n", "idx": 851}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.get_csrf_token", "completion": "        token = request.session.get(self.key)\n        if token is None:\n            token = self.new_csrf_token(request)\n        return token\n", "idx": 852}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.check_csrf_token", "completion": "        expected_token = self.get_csrf_token(request)\n        return not strings_differ(\n            bytes_(expected_token), bytes_(supplied_token)\n        )\n\n", "idx": 853}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.new_csrf_token", "completion": "        token = self._token_factory()\n        request.add_response_callback(self.set_csrf_token, token)\n        return token\n", "idx": 854}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.get_csrf_token", "completion": "        token = request.cookies.get(self.cookie_name, None)\n        if not token:\n            token = self.new_csrf_token(request)\n        return token\n", "idx": 855}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.check_csrf_token", "completion": "        expected_token = self.get_csrf_token(request)\n        return not strings_differ(\n            bytes_(expected_token), bytes_(supplied_token)\n        )\n\n", "idx": 856}
{"namespace": "pyramid.security.PermitsResult.__repr__", "completion": "        return '<{class name} instance at {instance id} with msg {message}>'.format(\n            class name=self.__class__.__name__,\n            instance id=id(self),\n            message=self.msg\n        )\n\n", "idx": 857}
{"namespace": "pyramid.util.InstancePropertyHelper.make_property", "completion": "        if name is None:\n            name = callable.__name__\n\n        if reify:\n            callable = SettableProperty(callable)\n\n        return name, property(callable)\n", "idx": 858}
{"namespace": "pyramid.util.InstancePropertyHelper.set_property", "completion": "        name, fn = cls.make_property(callable, name, reify)\n        cls.apply_properties(target, [(name, fn)])\n", "idx": 859}
{"namespace": "pyramid.util.InstancePropertyHelper.add_property", "completion": "        prop = self.make_property(callable, name=name, reify=reify)\n        self.properties[prop[0]] = prop[1]\n", "idx": 860}
{"namespace": "pyramid.util.InstancePropertyHelper.apply", "completion": "        self.apply_properties(target, self.properties.items())\n\n", "idx": 861}
{"namespace": "pyramid.util.InstancePropertyMixin.set_property", "completion": "        InstancePropertyHelper.set_property(self, callable, name=name, reify=reify)\n", "idx": 862}
{"namespace": "pyramid.util.TopologicalSorter.remove", "completion": "        if name not in self.name2val:\n            raise ValueError('No node named %s' % name)\n        self.names.remove(name)\n        del self.name2val[name]\n        self.order = []\n        for name in self.names:\n            self.order.append((name, self.name2after.get(name), self.name2before.get(name)))\n", "idx": 863}
{"namespace": "pyramid.util.TopologicalSorter.add", "completion": "        if name in self.names:\n            raise ValueError('Duplicate name: %s' % name)\n        self.names.append(name)\n        self.name2val[name] = val\n        if after is None:\n            after = self.default_after\n        if before is None:\n            before = self.default_before\n        if after is self.first:\n            after = []\n        if before is self.last:\n            before = []\n        if after is not None:\n            self.req_after.add(name)\n            if isinstance(after, str):\n                after = [after]\n            for a in after:\n                self.order.append((a, name))\n                if a in self.name2before:\n                    self.name2before[a].append(name)\n                else:\n                    self.name2before[a] = [name]\n        if before is not None:\n            self.req_before.add(name)\n            if isinstance(before, str):\n                before = [before]\n            for b in before:\n                self.order.append((name, b))\n                if b in self.name2after:\n                    self.name2after[b].append(name)\n                else:\n                    self.name2after[b] = [name]\n", "idx": 864}
{"namespace": "pyramid.traversal.find_resource", "completion": "    if isinstance(path, str):\n        if path.startswith(\"/\"):\n            path = path[1:]\n            absolute = True\n        else:\n            absolute = False\n        path = path.split(\"/\")\n    else:\n        absolute = path[0] == \"\"\n        path = path[1:]\n\n    if absolute:\n        resource = find_root(resource)\n\n    for segment in path:\n        if segment == \"\":\n            continue\n        try:\n            resource = resource[segment]\n        except KeyError:\n            raise KeyError(\n                \"No resource found at path %r (%r was not found in %r)\"\n                % (path, segment, resource)\n            )\n\n    return resource\n\n", "idx": 865}
{"namespace": "pyramid.static.ManifestCacheBuster.manifest", "completion": "        if self.reload:\n            mtime = self.getmtime(self.manifest_path)\n            if mtime != self._mtime:\n                self._manifest = self.get_manifest()\n                self._mtime = mtime\n        return self._manifest\n", "idx": 866}
{"namespace": "pyramid.registry.Registry.registerSubscriptionAdapter", "completion": "        self.has_listeners = True\n        return super(Registry, self).registerSubscriptionAdapter(*arg, **kw)\n", "idx": 867}
{"namespace": "pyramid.registry.Registry.registerHandler", "completion": "        result = Components.registerHandler(self, *arg, **kw)\n        self.has_listeners = True\n        return result\n", "idx": 868}
{"namespace": "pyramid.registry.Registry.notify", "completion": "        if self.has_listeners:\n            Components.notify(self, *events)\n", "idx": 869}
{"namespace": "pyramid.registry.Introspector.add", "completion": "        category = intr.category\n        discriminator = intr.discriminator\n        if category not in self._categories:\n            self._categories[category] = {}\n        if discriminator not in self._categories[category]:\n            self._categories[category][discriminator] = []\n        self._counter += 1\n        self._categories[category][discriminator].append((self._counter, intr))\n        self._refs[intr.ref] = intr\n", "idx": 870}
{"namespace": "pyramid.registry.Introspector.get", "completion": "        category = self._categories.get(category_name)\n        if category is None:\n            return default\n        return category.get(discriminator, default)\n", "idx": 871}
{"namespace": "pyramid.registry.Introspector.get_category", "completion": "        category = self._categories.get(category_name, default)\n        if category is None:\n            return default\n        if sort_key is None:\n            sort_key = operator.attrgetter('order')\n        return [\n            {\n                'introspectable': intr,\n                'related': intr.related\n            } for intr in sorted(category.values(), key=sort_key)\n        ]\n", "idx": 872}
{"namespace": "pyramid.registry.Introspector.categorized", "completion": "        if sort_key is None:\n            sort_key = operator.attrgetter('order')\n        categories = self._categories.values()\n        categories = sorted(set(categories), key=sort_key)\n        return [\n            (\n                category_name,\n                [\n                    {'introspectable': intr, 'related': self.related(intr)}\n                    for intr in category\n                ],\n            )\n            for category_name, category in categories\n        ]\n", "idx": 873}
{"namespace": "pyramid.registry.Introspector.remove", "completion": "        category = self._categories.get(category_name)\n        if category is None:\n            return\n        intr = category.get(discriminator)\n        if intr is None:\n            return\n        for key, value in list(category.items()):\n            if value is intr:\n                del category[key]\n        del self._refs[intr.ref_id]\n", "idx": 874}
{"namespace": "pyramid.registry.Introspector.relate", "completion": "        intrs = self._get_intrs_by_pairs(pairs)\n        for intr in intrs:\n            self._refs.setdefault(intr, []).extend(intrs)\n", "idx": 875}
{"namespace": "pyramid.registry.Introspector.related", "completion": "        category_name = intr.category_name\n        discriminator = intr.discriminator\n        intr = self._categories.get(category_name, {}).get(discriminator)\n        if intr is None:\n            raise KeyError((category_name, discriminator))\n        return self._refs.get(intr, [])\n\n", "idx": 876}
{"namespace": "pyramid.registry.Introspectable.discriminator_hash", "completion": "        self._assert_resolved()\n        return hash(self.discriminator)\n", "idx": 877}
{"namespace": "pyramid.registry.Introspectable.__repr__", "completion": "        return '<%s category %r, discriminator %r>' % (\n            self.type_name,\n            self.category_name,\n            self.discriminator,\n        )\n\n", "idx": 878}
{"namespace": "pyramid.scripts.proutes.PRoutesCommand._get_mapper", "completion": "        mapper = registry.queryUtility(IRoutesMapper)\n        if mapper is None:\n            mapper = registry.queryUtility(IRequestExtensions).mapper\n        return mapper\n", "idx": 879}
{"namespace": "pyramid.scripts.pshell.PShellCommand.make_shell", "completion": "        shells = self.find_all_shells()\n        shell_names = list(shells.keys())\n        shell_names.sort(key=lambda x: x.lower())\n\n        if self.args.python_shell:\n            if self.args.python_shell in shell_names:\n                return self.default_runner\n            else:\n                raise ValueError(\n                    'could not find a shell named \"%s\"' % self.args.python_shell\n                )\n\n        if self.preferred_shells:\n            for shell_name in self.preferred_shells:\n                if shell_name in shell_names:\n                    return shells[shell_name]\n\n        if shell_names:\n            return shells[shell_names[0]]\n\n        return self.default_runner\n\n", "idx": 880}
{"namespace": "pyramid.config.assets.PackageOverrides.insert", "completion": "        if not path:\n            raise ValueError(\"path must be a non-empty string\")\n        if path.endswith('/'):\n            override = DirectoryOverride(path, source)\n        else:\n            override = FileOverride(path, source)\n        self.overrides.insert(0, override)\n        return override\n", "idx": 881}
{"namespace": "pyramid.config.assets.PackageOverrides.filtered_sources", "completion": "        for override in self.overrides:\n            if override.match(resource_name):\n                for source in override.filtered_sources(resource_name):\n                    yield source\n", "idx": 882}
{"namespace": "pyramid.config.assets.PackageOverrides.real_loader", "completion": "        if self._real_loader is None:\n            raise NotImplementedError(\n                \"The real loader of the PackageOverrides instance is not set.\"\n            )\n        return self._real_loader\n\n", "idx": 883}
{"namespace": "pyramid.config.views.MultiView.add", "completion": "        if phash is None:\n            phash = DEFAULT_PHASH\n        if accept is None:\n            self.views.append((order, phash, view))\n            self.views.sort(key=operator.itemgetter(0, 1))\n        else:\n            if accept_order is None:\n                accept_order = MAX_ORDER\n            self.media_views.setdefault(accept, []).append(\n                (order, accept_order, phash, view)\n            )\n            self.media_views[accept].sort(key=operator.itemgetter(0, 1, 2))\n        self.accepts = sorted(self.media_views.keys(), key=Accept)\n", "idx": 884}
{"namespace": "pyramid.config.views.MultiView.get_views", "completion": "        if hasattr(request, 'accept') and self.accepts:\n            views = []\n            for offer in request.accept.offers(self.accepts):\n                subset = self.media_views.get(offer)\n                if subset:\n                    views.extend(subset)\n            views.extend(self.views)\n            return views\n        return self.views\n", "idx": 885}
{"namespace": "pyramid.config.views.MultiView.match", "completion": "        for order, view, phash in self.get_views(request):\n            if not hasattr(view, '__predicated__'):\n                return view\n            if view.__predicated__(context, request):\n                return view\n        raise PredicateMismatch\n", "idx": 886}
{"namespace": "pyramid.config.views.MultiView.__permitted__", "completion": "        try:\n            view = self.match(context, request)\n        except PredicateMismatch:\n            return False\n        if not hasattr(view, '__permitted__'):\n            return True\n        return view.__permitted__(context, request)\n", "idx": 887}
{"namespace": "pyramid.config.views.MultiView.__call_permissive__", "completion": "        view = self.match(context, request)\n        if hasattr(view, '__call_permissive__'):\n            return view.__call_permissive__(context, request)\n        return view(context, request)\n", "idx": 888}
{"namespace": "pyramid.config.actions.ActionState.processSpec", "completion": "        if spec in self._seen_files:\n            return False\n        self._seen_files.add(spec)\n        return True\n", "idx": 889}
{"namespace": "pyramid.config.actions.ActionState.action", "completion": "        action = extra\n        action.update(\n            dict(\n                discriminator=discriminator,\n                callable=callable,\n                args=args,\n                kw=kw,\n                order=order,\n                includepath=includepath,\n                info=info,\n                introspectables=introspectables,\n            )\n        )\n        self.actions.append(action)\n", "idx": 890}
{"namespace": "pyramid.config.actions.ActionInfo.__str__", "completion": "        return 'Line {} of file {}\\n{}'.format(self.line, self.file, indent(self.src))\n\n", "idx": 891}
{"namespace": "pyramid.config.Configurator.__getattr__", "completion": "        directives = getattr(self.registry, '_directives', None)\n        if directives is not None:\n            directive = directives.get(name)\n            if directive is not None:\n                c, action_wrap = directive\n                if action_wrap:\n\n                    def directive_wrapper(*args, **kw):\n                        return c(self, *args, **kw)\n\n                    return directive_wrapper\n                return c\n        raise AttributeError(name)\n", "idx": 892}
{"namespace": "pyramid.config.Configurator.with_package", "completion": "        return self.__class__(\n            registry=self.registry,\n            package=package,\n            root_package=self.root_package,\n            autocommit=self.autocommit,\n            route_prefix=self.route_prefix,\n        )\n", "idx": 893}
{"namespace": "pyramid.config.Configurator.absolute_asset_spec", "completion": "        if not isinstance(relative_spec, str):\n            return relative_spec\n        if ':' in relative_spec:\n            return relative_spec\n        return '%s:%s' % (self.package_name, relative_spec)\n", "idx": 894}
{"namespace": "pyramid.config.Configurator.begin", "completion": "        if request is _marker:\n            request = self.registry.request_iface(self.registry)\n        self.manager.push({'registry': self.registry, 'request': request})\n", "idx": 895}
{"namespace": "pyramid.config.Configurator.scan", "completion": "        if package is None:\n            package = caller_package()\n        if ignore is None:\n            ignore = []\n        ignore.append(self.registry)\n        ignore.append(self)\n        ignore.append(self.__class__)\n        ignore.append(self.__class__.__bases__[0])\n        ignore.append(self.__class__.__bases__[0].__class__)\n        ignore.append(self.__class__.__bases__[0].__class__.__bases__[0])\n        ignore.append(self.__class__.__bases__[0].__class__.__bases__[0].__class__)\n        ignore.append(self.__class__.__bases__[0].__class__.__bases__[0].__class__.__bases__[0])\n        ignore.append(self.__class__.__bases__[0].__class__.__bases__[0].__class__.__bases__[0].__class__)\n        ignore.append(self.__class__.__bases__[0].__class__.__bases__[0].__class__.__bases__[0].__class__.__bases__[0])\n        ignore.append(self.__class__.__bases__[0].__class__.__bases__[0].__class__.__bases__[0].__class__.__bases__[0].__class__)\n        ignore.append(self.__class__.__bases__[0].__class__.__bases__[0].__class__.__bases__[0].__class__.__bases__[0].__class__.__bases__[0])\n        ignore.append(self.__class__.__bases__[0].__class__.__bases__[0].__class__.__bases__[0].__class__.__bases__[0].__class__.__bases__[0].__class__)\n        ignore.append(self.__class__.__bases__[0].__class__.__bases__[0].__class__.__bases__[0].__class__.__bases__[0].__class__.__bases", "idx": 896}
{"namespace": "pyramid.config.Configurator.make_wsgi_app", "completion": "        self.commit()\n        self.registry.notify(ApplicationCreated(self.registry))\n        self.registry.global_registries.append(self.registry)\n        return self.registry.create_wsgi_app()\n", "idx": 897}
{"namespace": "aiohappybase._util.camel_case_to_pep8", "completion": "    return CAPITALS.sub(r'_\\1', name).lower().strip('_')\n\n", "idx": 898}
{"namespace": "kinto.authorization._relative_object_uri", "completion": "    obj_parts = object_uri.split(\"/\")\n    if len(obj_parts) <= 2:\n        raise ValueError(\"The object URI is not valid.\")\n\n    # /buckets/bid -> buckets\n    parent_resource_name = obj_parts[-2]\n    # buckets -> bucket\n    parent_resource_name = parent_resource_name.rstrip(\"s\")\n\n    if parent_resource_name == resource_name:\n        return \"/\".join(obj_parts[:-2])\n    else:\n        raise ValueError(\"The resource name does not match the parent resource name.\")\n\n", "idx": 899}
{"namespace": "kinto.core.openapi.OpenAPI.expose_authentication_method", "completion": "        cls.security_definitions[method_name] = definition\n\n        for scope in definition['scopes']:\n            cls.security_roles[scope] = method_name\n", "idx": 900}
{"namespace": "kinto.core.openapi.OpenAPI.generate", "completion": "        # Create a base specification dictionary\n        base_spec = {\n            \"host\": self.request.host,\n            \"schemes\": [self.request.scheme],\n            \"securityDefinitions\": self.security_definitions,\n        }\n\n        # Call the parent class's generate method\n        return super().generate(base_spec)", "idx": 901}
{"namespace": "kinto.core.testing.get_user_headers", "completion": "    return {\"Authorization\": \"Basic \" + b64encode(f\"{user}:{password}\".encode()).decode()}\n\n", "idx": 902}
{"namespace": "kinto.core.authorization.RouteFactory.fetch_shared_objects", "completion": "        # If the permission is not a write permission, we can return None.\n        if perm != \"write\":\n            return None\n\n        # If the permission is a write permission, we can fetch the shared objects.\n        # We need to get the object ID match for the permission.\n        object_id_match = self.get_permission_object_id(self._request, perm)\n\n        # If the object ID match is not set, we can return None.\n        if object_id_match is None:\n            return None\n\n        # If the object ID match is set, we can fetch the shared objects.\n        # We need to get the bound permissions for the object ID match.\n        bound_perms = get_bound_permissions(object_id_match, perm)\n\n        # If the bound permissions are not set, we can return None.\n        if bound_perms is None:\n            return None\n\n        # If the bound permissions are set, we can fetch the shared objects.\n        # We need to get the readable or writable objects for the bound permissions.\n        shared_ids = self._get_accessible_objects(principals, bound_perms)\n\n        # If the readable or writable objects are not set, we can return None.\n        if shared_ids is None:\n            return None\n\n        # If the readable or writable objects are set, we can return the shared objects.\n        return shared_ids\n", "idx": 903}
{"namespace": "kinto.core.authorization.RouteFactory.get_permission_object_id", "completion": "        if object_id is None:\n            object_id = request.matchdict.get(\"id\", \"\")\n        if object_id == \"\":\n            object_id = \"*\"\n        if self.on_plural_endpoint:\n            if object_id == \"*\":\n                object_id = request.path.split(\"/\")[-1]\n            service = utils.current_service(request)\n            object_service = service.sibling(\"object\")\n            object_id = f\"{object_service.name}:{object_id}\"\n        return object_id\n", "idx": 904}
{"namespace": "kinto.core.utils.recursive_update_dict", "completion": "    if changes is None:\n        changes = root\n    for key, value in changes.items():\n        if key in ignores:\n            root.pop(key, None)\n        elif isinstance(value, dict):\n            recursive_update_dict(root.setdefault(key, {}), value, ignores)\n        else:\n            root[key] = value\n\n", "idx": 905}
{"namespace": "kinto.core.utils.native_value", "completion": "    if isinstance(value, str):\n        try:\n            value = json.loads(value)\n        except ValueError:\n            pass\n    return value\n\n", "idx": 906}
{"namespace": "kinto.core.utils.dict_subset", "completion": "    new_dict = {}\n    for key in keys:\n        if \".\" in key:\n            key_list = key.split(\".\")\n            if key_list[0] in d:\n                new_dict[key_list[0]] = dict_subset(d[key_list[0]], key_list[1:])\n        else:\n            if key in d:\n                new_dict[key] = d[key]\n    return new_dict\n\n", "idx": 907}
{"namespace": "kinto.core.utils.dict_merge", "completion": "    result = a.copy()\n    result.update({key: b[key] for key in b if key not in result})\n\n    for key in result:\n        if key in a and key in b and isinstance(a[key], dict) and isinstance(b[key], dict):\n            result[key] = dict_merge(a[key], b[key])\n\n    return result\n\n", "idx": 908}
{"namespace": "kinto.core.utils.find_nested_value", "completion": "    if path in d:\n        return d[path]\n\n    path_parts = path.split(\".\")\n    root_keys = [key for key in d.keys() if key in path_parts]\n\n    if not root_keys:\n        return default\n\n    root_key = max(root_keys, key=len)\n\n    if not isinstance(d[root_key], dict):\n        return default\n\n    subpath = path.replace(root_key, \"\").strip(\".\")\n\n    if subpath:\n        return find_nested_value(d[root_key], subpath, default)\n\n    return d[root_key]\n\n", "idx": 909}
{"namespace": "kinto.core.utils.instance_uri_registry", "completion": "    api_prefix = f\"/{registry.route_prefix}\"\n    path = api_prefix + instance_uri(Request.blank(\"/\"), resource_name, **params)\n    return path\n\n", "idx": 910}
{"namespace": "kinto.core.statsd.load_from_config", "completion": "    if statsd_module is None:\n        raise ConfigurationError(\n            \"statsd_url setting specified, but \"\n            \"statsd library not installed.\"\n        )\n\n    statsd_url = config.get_settings().get(\"statsd_url\")\n    parsed_url = urlparse(statsd_url)\n    hostname = parsed_url.hostname\n    port = parsed_url.port\n    prefix = parsed_url.path\n    return Client(hostname, port, prefix)", "idx": 911}
{"namespace": "kinto.core.errors.http_error", "completion": "    if not code:\n        code = httpexception.code\n    if not error:\n        error = httpexception.title\n    if not errno:\n        errno = ERRORS.UNDEFINED.value\n\n    body = ErrorSchema().deserialize(\n        {\n            \"code\": code,\n            \"errno\": errno,\n            \"error\": error,\n            \"message\": message,\n            \"info\": info,\n            \"details\": details,\n        }\n    )\n\n    httpexception.body = body\n    httpexception.content_type = \"application/json\"\n    return httpexception\n\n", "idx": 912}
{"namespace": "kinto.core.resource.schema.ResourceReponses.get_and_bind", "completion": "        # Get the default response schemas\n        default_schemas = self.default_schemas\n        default_object_schemas = self.default_object_schemas\n        default_plural_schemas = self.default_plural_schemas\n        default_get_schemas = self.default_get_schemas\n        default_post_schemas = self.default_post_schemas\n        default_put_schemas = self.default_put_schemas\n        default_patch_schemas = self.default_patch_schemas\n        default_delete_schemas = self.default_delete_schemas\n\n        # Get the endpoint-specific response schemas\n        object_get_schemas = self.object_get_schemas\n        object_patch_schemas = self.object_patch_schemas\n        object_delete_schemas = self.object_delete_schemas\n\n        # Get the method-specific response schemas\n        object_get_schemas = self.object_get_schemas\n        object_patch_schemas = self.object_patch_schemas\n        object_delete_schemas = self.object_delete_schemas\n\n        # Get the endpoint-specific response schemas\n        endpoint_schemas = getattr(self, endpoint_type + \"_schemas\", {})\n        object_get_schemas = endpoint_schemas.get(\"object_get_schemas\", {})\n        object_patch_schemas = endpoint_schemas.get(\"object_patch_schemas\", {})\n        object_delete_schemas = endpoint_schemas.get(\"object_delete_schemas\", {})\n\n        # Get the method-specific response schemas\n        method_schemas = getattr(self, method + \"_schemas\", {})\n        object_get_schemas = method_schemas.get(\"object_get_schemas\", {})\n        object_patch_schemas = method_schemas.get(\"object_patch_schemas\", {})\n        object_delete_schemas = method_schemas.get(\"object_delete_schemas\", {})\n\n        # Get the response schemas\n        schemas = {}\n        schemas.update(default_schemas)\n        schemas.update(default_object_schemas)\n        schemas.update(default_plural_schemas)\n        schemas.update(default_get_", "idx": 913}
{"namespace": "kinto.core.resource.Resource.timestamp", "completion": "        try:\n            timestamp = self.model.timestamp\n        except storage_exceptions.ReadOnlyError as e:\n            http_error = http_error_from_exception(e)\n            http_error.body = json.dumps(http_error.body)\n            raise http_error\n        return timestamp\n", "idx": 914}
{"namespace": "kinto.core.resource.Resource.plural_post", "completion": "        self._add_timestamp_header(self.request.response)\n        self._add_cache_header(self.request.response)\n        self._raise_304_if_not_modified()\n        self._raise_412_if_modified(obj={})\n\n        # Check if the object id is specified in the request body.\n        if self.object_id:\n            # If the object id is specified, look up the object.\n            obj = self.model.get_object(self.object_id)\n            # If the object exists, return it with a status code of 200.\n            if obj:\n                return self.postprocess(obj)\n            # If the object does not exist, return a 404 error.\n            else:\n                raise HTTPNotFound()\n        # If the object id is not specified, create a new object.\n        else:\n            # Check if the \"If-Match\" header is provided.\n            if self.request.if_match:\n                # If the \"If-Match\" header is provided, check if the object has been modified in the meantime.\n                self._raise_412_if_modified(obj={})\n            # Create a new object.\n            obj = self.model.create_object(self.request.validated)\n            # Return the new object with a status code of 201.\n            return self.postprocess(obj, 201)\n", "idx": 915}
{"namespace": "kinto.core.resource.Resource.get", "completion": "        # Get the object\n        obj = self._get_object_or_404(self.object_id)\n\n        # Check if the object has been modified\n        self._raise_412_if_modified(obj)\n\n        # Check if any partial fields need to be extracted\n        partial_fields = self._extract_partial_fields()\n\n        # Add a timestamp header\n        self._add_timestamp_header(self.request.response, timestamp=obj[self.model.modified_field])\n\n        # Add a cache header\n        self._add_cache_header(self.request.response)\n\n        # If there are partial fields, extract them from the object\n        if partial_fields:\n            return self.postprocess(dict_subset(obj, partial_fields))\n        else:\n            return self.postprocess(obj)\n", "idx": 916}
{"namespace": "kinto.core.resource.Resource.delete", "completion": "        self._raise_400_if_invalid_id(self.object_id)\n        existing = self._get_object_or_404(self.object_id)\n        self._raise_412_if_modified(existing)\n        self.model.delete_object(self.object_id)\n        timestamp = existing[self.model.modified_field]\n        self._add_timestamp_header(self.request.response, timestamp=timestamp)\n        return self.postprocess(existing, action=ACTIONS.DELETE)\n", "idx": 917}
{"namespace": "kinto.core.permission.memory.Permission.add_principal_to_ace", "completion": "        # Get the set of principals associated with the object and permission.\n        ace_key = f\"{object_id}:{permission}\"\n        principals = self._store.get(ace_key, set())\n        # Add the new principal to the set.\n        principals.add(principal)\n        # Update the store with the modified set.\n        self._store[ace_key] = principals\n", "idx": 918}
{"namespace": "kinto.core.permission.memory.Permission.get_object_permission_principals", "completion": "        permission_key = f\"permission:{object_id}:{permission}\"\n        return self._store.get(permission_key, set())\n", "idx": 919}
{"namespace": "kinto.core.storage.generators.Generator.match", "completion": "        if not self._regexp:\n            self._regexp = re.compile(self.regexp)\n\n        return bool(self._regexp.match(object_id))\n", "idx": 920}
{"namespace": "kinto.core.storage.postgresql.migrator.MigratorMixin.create_or_migrate_schema", "completion": "        # Get the current version of the schema\n        current_version = self.get_installed_version()\n\n        # If the current version matches the desired version, log that the schema is up-to-date\n        if current_version == self.schema_version:\n            logger.info(f\"{self.name} schema is up-to-date\")\n\n        # Otherwise, migrate the schema to the desired version\n        else:\n            logger.info(f\"{self.name} schema is out of date\")\n            self.migrate_schema(current_version, dry_run)\n", "idx": 921}
{"namespace": "kinto.core.views.batch.BatchPayloadSchema.deserialize", "completion": "        # Deserialize the data.\n        deserialized = super().deserialize(cstruct)\n\n        # If defaults are provided, merge them with the requests.\n        if \"defaults\" in deserialized:\n            defaults = deserialized[\"defaults\"]\n            requests = deserialized[\"requests\"]\n            for request in requests:\n                for key, value in defaults.items():\n                    if key not in request:\n                        request[key] = value\n\n        return deserialized\n\n", "idx": 922}
{"namespace": "kinto.plugins.accounts.utils.get_cached_reset_password", "completion": "    settings = registry.settings\n    hmac_secret = settings[\"userid_hmac_secret\"]\n    cache_key = utils.hmac_digest(hmac_secret, ACCOUNT_RESET_PASSWORD_CACHE_KEY.format(username))\n    cache = registry.cache\n    return cache.get(cache_key)\n\n", "idx": 923}
{"namespace": "kinto.plugins.accounts.utils.get_cached_validation_key", "completion": "    hmac_secret = registry.settings[\"userid_hmac_secret\"]\n    cache_key = utils.hmac_digest(hmac_secret, ACCOUNT_VALIDATION_CACHE_KEY.format(username))\n\n    cache = registry.cache\n    cache_result = cache.get(cache_key)\n    return cache_result\n\n", "idx": 924}
{"namespace": "kinto.plugins.accounts.views.validation.on_account_activated", "completion": "    settings = event.request.registry.settings\n    if not settings.get(\"account_validation.enabled\", False):\n        return\n\n    for impacted_object in event.impacted_objects:\n        old_account = impacted_object.get(\"old\", {})\n        new_account = impacted_object.get(\"new\", {})\n\n        if old_account.get(\"validated\", False) or not new_account.get(\"validated\", False):\n            continue\n\n        Emailer(event.request, new_account).send_confirmation_email()", "idx": 925}
{"namespace": "kinto.plugins.openid.OpenIDConnectPolicy._verify_token", "completion": "        # Fetch user info\n        userinfo_endpoint = self.oid_config.get(\"userinfo_endpoint\")\n        headers = {\"Authorization\": f\"Bearer {access_token}\"}\n        try:\n            response = requests.get(userinfo_endpoint, headers=headers)\n            if response.status_code == 200:\n                return response.json()\n            else:\n                return None\n        except Exception as e:\n            self.logger.debug(f\"Error verifying token: {e}\")\n            return None\n\n", "idx": 926}
{"namespace": "kinto.plugins.quotas.scripts.rebuild_quotas", "completion": "    for bucket in paginated(storage, storage.get_all, parent_id=None, with_deleted=False):\n        bucket_id = bucket[\"id\"]\n        bucket_uri = bucket[\"uri\"]\n        bucket_size = 0\n        bucket_count = 0\n        bucket_record_count = 0\n        bucket_collection_count = 0\n        logger.info(\"Rebuilding quotas for bucket %s\", bucket_id)\n        for collection in paginated(storage, storage.get_all, parent_id=bucket_id, with_deleted=False):\n            collection_id = collection[\"id\"]\n            collection_uri = collection[\"uri\"]\n            collection_size = 0\n            collection_count = 0\n            collection_record_count = 0\n            logger.info(\"Rebuilding quotas for collection %s\", collection_id)\n            for record in paginated(storage, storage.get_all, parent_id=collection_id, with_deleted=False):\n                record_size_ = record_size(record)\n                collection_size += record_size_\n                collection_count += 1\n                collection_record_count += 1\n                bucket_size += record_size_\n                bucket_count += 1\n                bucket_record_count += 1\n            if not dry_run:\n                storage.update(\n                    parent_id=bucket_id,\n                    object_id=COLLECTION_QUOTA_OBJECT_ID,\n                    data={\n                        \"collection_count\": collection_count,\n                        \"collection_record_count\": collection_record_count,\n                        \"collection_size\": collection_size,\n                    },\n                )\n                storage.update(\n                    parent_id=collection_id,\n                    object_id=COLLECTION_QUOTA_OBJECT_ID,\n                    data={\n                        \"collection_count\": collection_count,\n                        \"collection_record_count\": collection_record_count,\n                        \"collection_size\": collection_size,\n                    },\n                )\n            logger.info", "idx": 927}
{"namespace": "kinto.config.render_template", "completion": "    with codecs.open(template, 'r', 'utf-8') as f:\n        template_content = f.read()\n\n    for key, value in kwargs.items():\n        template_content = template_content.replace('{{%s}}' % key, value)\n\n    with codecs.open(destination, 'w', 'utf-8') as f:\n        f.write(template_content)\n\n", "idx": 928}
{"namespace": "trafilatura.sitemaps.SitemapObject.extract_sitemap_langlinks", "completion": "        # Extract the hreflang attributes from the content of the SitemapObject\n        hreflang_attributes = HREFLANG_REGEX.findall(self.content)\n\n        # Iterate through the hreflang attributes\n        for hreflang_attribute in hreflang_attributes:\n\n            # Check if the hreflang attribute matches the target language\n            if hreflang_attribute == self.target_lang:\n\n                # Extract the link from the hreflang attribute\n                link = hreflang_attribute.split('=')[1]\n\n                # Handle the link\n                self.handle_link(link)\n\n        # Log a debug message\n        LOGGER.debug('%s sitemaps and %s links with hreflang found for %s', len(self.sitemap_urls), len(self.urls), self.target_lang)", "idx": 929}
{"namespace": "trafilatura.sitemaps.SitemapObject.extract_sitemap_links", "completion": "        # extract\n        for attrs in (m[0] for m in islice(LINK_REGEX.finditer(self.content), MAX_LINKS)):\n            self.handle_link(attrs)\n        LOGGER.debug('%s sitemaps and %s links found for %s', len(self.sitemap_urls), len(self.urls), self.sitemap_url)\n\n", "idx": 930}
{"namespace": "trafilatura.sitemaps.SitemapObject.process", "completion": "        # check if sitemap is plausible\n        if not SITEMAP_FORMAT.search(self.content):\n            return\n\n        # extract links from a TXT file\n        if self.content.startswith('http'):\n            self.urls = filter_urls(self.content)\n            if self.urls:\n                return\n\n        # extract links from an XML file\n        self.extract_sitemap_links()\n\n        # extract language links from an XML file\n        if self.target_lang and self.sitemap_urls:\n            self.extract_sitemap_langlinks()\n\n        # extract links from an XML file\n        if self.sitemap_urls or self.urls:\n            return\n\n        # extract links from an XML file\n        self.extract_sitemap_links()\n\n", "idx": 931}
{"namespace": "trafilatura.sitemaps.is_plausible_sitemap", "completion": "    # Check if the URL is in the expected format.\n    if not SITEMAP_FORMAT.match(url):\n        return False\n\n    # Check if the URL is in the expected format.\n    if not SITEMAP_FORMAT.match(url):\n        return False\n\n    # Check if the URL is in the expected format.\n    if not SITEMAP_FORMAT.match(url):\n        return False\n\n    # Check if the URL is in the expected format.\n    if not SITEMAP_FORMAT.match(url):\n        return False\n\n    # Check if the URL is in the expected format.\n    if not SITEMAP_FORMAT.match(url):\n        return False\n\n    # Check if the URL is in the expected format.\n    if not SITEMAP_FORMAT.match(url):\n        return False\n\n    # Check if the URL is in the expected format.\n    if not SITEMAP_FORMAT.match(url):\n        return False\n\n    # Check if the URL is in the expected format.\n    if not SITEMAP_FORMAT.match(url):\n        return False\n\n    # Check if the URL is in the expected format.\n    if not SITEMAP_FORMAT.match(url):\n        return False\n\n    # Check if the URL is in the expected format.\n    if not SITEMAP_FORMAT.match(url):\n        return False\n\n    # Check if the URL is in the expected format.\n    if not SITEMAP_FORMAT.match(url):\n        return False\n\n    # Check if the URL is in the expected format.\n    if not SITEMAP_FORMAT.match(url):\n        return False\n\n    # Check if the URL is in the expected format.\n    if not SITEMAP_FORMAT.match(url):\n        return False\n\n    # Check if the URL is in the expected format.\n    if not SITEMAP_FORMAT.match(url):\n        return False\n\n    # Check if the URL is in the expected format.\n    if not SITEMAP_FORMAT.match(url):\n        return False\n\n    # Check", "idx": 932}
{"namespace": "trafilatura.sitemaps.extract_robots_sitemaps", "completion": "    sitemaps = []\n    for line in robotstxt.splitlines():\n        line = line.strip()\n        if line.startswith('#'):\n            continue\n        if line.startswith('Sitemap:'):\n            sitemap = line[9:].strip()\n            sitemap = fix_relative_urls(baseurl, sitemap)\n            sitemaps.append(sitemap)\n    return sitemaps", "idx": 933}
{"namespace": "trafilatura.feeds.handle_link_list", "completion": "    links = []\n    for link in linklist:\n        if not BLACKLIST.search(link):\n            if check_url(link, domainname):\n                if validate_url(link):\n                    if fix_relative_urls(link, baseurl):\n                        if target_lang:\n                            if check_language(link, target_lang):\n                                links.append(link)\n                        else:\n                            links.append(link)\n    return links\n\n", "idx": 934}
{"namespace": "trafilatura.feeds.find_feed_urls", "completion": "    # extract domain name and base URL\n    domainname, baseurl = get_hostinfo(url)\n    # fetch the webpage content\n    htmlstring = load_html(url)\n    # check if it is a feed\n    feed_urls = extract_links(htmlstring, baseurl, url, target_lang=target_lang)\n    # if it is a web page, determine the feed and fetch the feed content\n    if not feed_urls:\n        feed_urls = determine_feed(htmlstring, baseurl, url)\n    # filter the URLs based on the target language\n    if target_lang:\n        feed_urls = filter_urls(feed_urls, target_lang)\n    # return the extracted feed links as a sorted list of unique links\n    return sorted(set(feed_urls))", "idx": 935}
{"namespace": "trafilatura.hashing.generate_hash_filename", "completion": "    # remove any potential XML tags\n    content = CLEAN_XML.sub(\"\", content)\n    # generate a bag-of-word hashing of length 12\n    hash_bytes = generate_bow_hash(content, length=12)\n    # encode the hash using urlsafe_b64encode\n    hash_encoded = urlsafe_b64encode(hash_bytes)\n    # return the decoded string\n    return hash_encoded.decode()\n\n", "idx": 936}
{"namespace": "trafilatura.cli_utils.download_queue_processing", "completion": "    # load the download buffer\n    download_buffer = load_download_buffer(args.download_buffer)\n\n    # process the downloads\n    for url in url_store:\n        # check if the URL is in the download buffer\n        if url in download_buffer:\n            # process the URL\n            try:\n                # get the HTML content\n                htmlstring = download_buffer[url]\n                # process the result\n                counter = process_result(htmlstring, args, url, counter, config)\n            except Exception as e:\n                # handle the error\n                LOGGER.error('Error processing URL: %s', url)\n                LOGGER.error(traceback.format_exc())\n                # add the URL to the error list\n                url_store.add_error(url, e)\n\n    # return the error list and the updated counter\n    return url_store.errors, counter\n\n", "idx": 937}
{"namespace": "trafilatura.utils.decode_response", "completion": "    if isinstance(response, HTTPResponse):\n        # urllib3 response\n        response = response.data\n    elif isinstance(response, bytes):\n        # bytes\n        response = handle_compressed_file(response)\n    else:\n        raise TypeError('response must be urllib3 response or bytes')\n    # detect encoding\n    encodings = detect_encoding(response)\n    # try to decode\n    for encoding in encodings:\n        try:\n            return response.decode(encoding)\n        except UnicodeDecodeError:\n            pass\n    # fallback to utf-8\n    return response.decode('utf-8', errors='ignore')\n\n", "idx": 938}
{"namespace": "trafilatura.utils.txttocsv", "completion": "    # Create a list of the metadata fields\n    metadata_fields = ['URL', 'fingerprint', 'hostname', 'title', 'image', 'date', 'license', 'pagetype', 'ID']\n\n    # Create a list of the metadata values\n    metadata_values = [docmeta['URL'], docmeta['fingerprint'], docmeta['hostname'], docmeta['title'], docmeta['image'], docmeta['date'], docmeta['license'], docmeta['pagetype'], docmeta['ID']]\n\n    # Create a list of the text and comment fields\n    text_fields = ['text', 'comments']\n\n    # Create a list of the text and comment values\n    text_values = [text, comments]\n\n    # Create a list of the metadata fields and text fields\n    fields = metadata_fields + text_fields\n\n    # Create a list of the metadata values and text values\n    values = metadata_values + text_values\n\n    # Create a string of the metadata fields\n    metadata_fields_string = '\\t'.join(metadata_fields)\n\n    # Create a string of the text fields\n    text_fields_string = '\\t'.join(text_fields)\n\n    # Create a string of the metadata values and text values\n    values_string = '\\t'.join(values)\n\n    # Create a string of the metadata fields and text fields\n    fields_string = metadata_fields_string + '\\t' + text_fields_string\n\n    # Create a string of the metadata fields and text fields and metadata values and text values\n    result = fields_string + '\\n' + values_string\n\n    # Return the result\n    return result\n\n", "idx": 939}
{"namespace": "trafilatura.utils.is_similar_domain", "completion": "    # Remove the file extension from both strings\n    reference = STRIP_EXTENSION.sub('', reference)\n    new_string = STRIP_EXTENSION.sub('', new_string)\n\n    # Calculate the similarity ratio\n    similarity_ratio = SequenceMatcher(None, reference, new_string).ratio()\n\n    # If the similarity ratio is above the threshold, return True\n    if similarity_ratio >= threshold:\n        return True\n\n    # Otherwise, return False\n    return False\n\n", "idx": 940}
{"namespace": "trafilatura.xml.remove_empty_elements", "completion": "    for elem in tree.iter():\n        if elem.text is None and elem.tail is None and len(elem) == 0:\n            elem.getparent().remove(elem)\n\n    return tree\n\n", "idx": 941}
{"namespace": "trafilatura.xml.strip_double_tags", "completion": "    for element in tree.iter('*'):\n        if element.tag in NESTING_WHITELIST:\n            parent = element.getparent()\n            if parent is not None:\n                if parent.tag in NESTING_WHITELIST:\n                    parent.text = (parent.text or '') + (element.text or '')\n                    parent.extend(element)\n                    parent.tail = (parent.tail or '') + (element.tail or '')\n                    parent.attrib.update(element.attrib)\n                    parent.remove(element)\n\n    return tree\n\n", "idx": 942}
{"namespace": "trafilatura.xml.check_tei", "completion": "    # check if the document is valid\n    if not validate_tei(xmldoc):\n        LOGGER.warning('TEI validation failed for %s', url)\n\n    # scrub the document\n    xmldoc = scrub_tei(xmldoc)\n\n    # check if the document is valid\n    if not validate_tei(xmldoc):\n        LOGGER.warning('TEI validation failed for %s', url)\n\n    return xmldoc\n\n", "idx": 943}
{"namespace": "trafilatura.xml.validate_tei", "completion": "    global TEI_RELAXNG\n    if TEI_RELAXNG is None:\n        with lzma.open(TEI_SCHEMA, 'rb') as f:\n            TEI_RELAXNG = load_pickle(f)\n    return TEI_RELAXNG.validate(xmldoc)\n\n", "idx": 944}
{"namespace": "trafilatura.xml.merge_with_parent", "completion": "    parent = element.getparent()\n    if parent is not None:\n        parent.text = replace_element_text(parent, include_formatting)\n        parent.append(element)\n        parent.text = parent.text.strip()\n        parent.remove(element)\n\n", "idx": 945}
{"namespace": "trafilatura.downloads._determine_headers", "completion": "    myagents, mycookie = _parse_config(config)\n    if myagents is not None:\n        headers['User-Agent'] = random.choice(myagents)\n    if mycookie is not None:\n        headers['Cookie'] = mycookie\n    return headers\n\n", "idx": 946}
{"namespace": "trafilatura.meta.reset_caches", "completion": "    reset_caches_htmldate()\n    reset_caches_courlan()\n    gc.collect()", "idx": 947}
{"namespace": "trafilatura.core.handle_table", "completion": "    # define new table\n    newtable = Element('table')\n    newrow = Element('row')\n    # iterate through sub-elements\n    for child in table_elem.iter():\n        # check for table elements\n        if child.tag in potential_tags:\n            # check for row elements\n            if child.tag == 'tr':\n                # check for previously processed row\n                if len(newrow) > 0:\n                    # append row to newtable\n                    newtable.append(newrow)\n                    # start new row\n                    newrow = Element('row')\n                # process row\n                newrow = process_node(child, options)\n            # check for cell elements\n            elif child.tag in TABLE_ELEMS:\n                # define new cell\n                newcell = define_cell_type(child)\n                # process cell\n                newcell = process_node(child, options)\n                # check for nested table\n                if newcell.find('table') is not None:\n                    # remove nested table\n                    newcell.remove(newcell.find('table'))\n                # append cell to newrow\n                newrow.append(newcell)\n            # check for other elements\n            else:\n                # process element\n                newchild = process_node(child, options)\n                # check for nested table\n                if newchild.find('table') is not None:\n                    # remove nested table\n                    newchild.remove(newchild.find('table'))\n                # append element to newrow\n                newrow.append(newchild)\n        # check for row end\n        if child.tag == 'tr':\n            # check for previously processed row\n            if len(newrow) > 0:\n                # append row to newtable\n                newtable.append(newrow)\n                # start new row\n                newrow = Element('row')\n        # check for table end\n        if child.tag == 'table':\n            # check for previously processed row\n            if len(newrow) > 0:\n                # append row to newtable\n                newtable.append(newrow)\n            # return newtable\n            return newtable\n   ", "idx": 948}
{"namespace": "trafilatura.filters.language_filter", "completion": "    # If the target language is specified, check if the language of the text is the same as the target language.\n    if target_language:\n        # If the language of the text is not the same as the target language, log a warning and return True along with the updated docmeta.\n        if language_classifier(temp_text, temp_comments) != target_language:\n            LOGGER.warning('Language of text is not %s', target_language)\n            docmeta['language'] = language_classifier(temp_text, temp_comments)\n            return True, docmeta\n        # Otherwise, return False along with the original docmeta.\n        return False, docmeta\n    # Otherwise, return False along with the original docmeta.\n    return False, docmeta\n\n", "idx": 949}
{"namespace": "trafilatura.filters.textfilter", "completion": "    # Check if the element is a paragraph.\n    if element.tag == 'p':\n        # Check if the text of the paragraph contains any of the words in the blacklist.\n        if any(word in element.text for word in COMMENTS_BLACKLIST):\n            return True\n        # Check if the text of the paragraph contains any of the words in the blacklist.\n        if any(word in element.text for word in COMMENTS_BLACKLIST):\n            return True\n        # Check if the text of the paragraph contains any of the words in the blacklist.\n        if any(word in element.text for word in COMMENTS_BLACKLIST):\n            return True\n        # Check if the text of the paragraph contains any of the words in the blacklist.\n        if any(word in element.text for word in COMMENTS_BLACKLIST):\n            return True\n        # Check if the text of the paragraph contains any of the words in the blacklist.\n        if any(word in element.text for word in COMMENTS_BLACKLIST):\n            return True\n        # Check if the text of the paragraph contains any of the words in the blacklist.\n        if any(word in element.text for word in COMMENTS_BLACKLIST):\n            return True\n        # Check if the text of the paragraph contains any of the words in the blacklist.\n        if any(word in element.text for word in COMMENTS_BLACKLIST):\n            return True\n        # Check if the text of the paragraph contains any of the words in the blacklist.\n        if any(word in element.text for word in COMMENTS_BLACKLIST):\n            return True\n        # Check if the text of the paragraph contains any of the words in the blacklist.\n        if any(word in element.text for word in COMMENTS_BLACKLIST):\n            return True\n        # Check if the text of the paragraph contains any of the words in the blacklist.\n        if any(word in element.text for word in COMMENTS_BLACKLIST):\n            return True\n        # Check if the text of the paragraph contains any of the words in the black", "idx": 950}
{"namespace": "trafilatura.metadata.extract_meta_json", "completion": "    for element in tree.iter():\n        if element.tag == 'script':\n            if element.get('type') in ['application/ld+json', 'application/settings+json']:\n                try:\n                    json_text = json.loads(element.text)\n                    metadata = extract_meta_json_text(json_text, metadata)\n                except json.decoder.JSONDecodeError:\n                    metadata = extract_meta_json_text(element.text, metadata)\n    return metadata\n\n", "idx": 951}
{"namespace": "trafilatura.external.try_justext", "completion": "    # init body element\n    body = Element('body')\n\n    # determine language\n    if target_language is None:\n        target_language = 'en'\n    else:\n        target_language = target_language.lower()\n\n    # extract paragraphs\n    try:\n        if target_language in JUSTEXT_LANGUAGES:\n            stoplist = JUSTEXT_LANGUAGES[target_language]\n        else:\n            stoplist = JT_STOPLIST\n        paragraphs = custom_justext(tree, stoplist)\n    except Exception as err:\n        LOGGER.warning('justext failed: %s', err)\n        return None\n\n    # populate body with paragraphs\n    for paragraph in paragraphs:\n        if paragraph.class_type == 'good':\n            p = Element('p')\n            p.text = paragraph.text\n            body.append(p)\n\n    return body\n\n", "idx": 952}
{"namespace": "mongoengine.base.datastructures.BaseDict.get", "completion": "        try:\n            return self[key]\n        except KeyError:\n            return default\n", "idx": 953}
{"namespace": "sqlite_utils.utils.suggest_column_types", "completion": "    column_types = {}\n    for record in records:\n        for key, value in record.items():\n            column_types.setdefault(key, set()).add(type(value))\n\n    return suggest_column_types_for_column_types(column_types)\n\n", "idx": 954}
{"namespace": "sqlite_utils.plugins.get_plugins", "completion": "    plugins = []\n    for plugin in pm.get_plugins():\n        hook_names = [name for name, _ in pm.list_name_plugin() if plugin.name == name[0]]\n        plugin_dict = {\"name\": plugin.name, \"hooks\": hook_names}\n        try:\n            version = plugin.version\n            plugin_dict[\"version\"] = version\n        except AttributeError:\n            pass\n        try:\n            project_name = plugin.project_name\n            plugin_dict[\"project_name\"] = project_name\n        except AttributeError:\n            pass\n        plugins.append(plugin_dict)\n    return plugins\n\n", "idx": 955}
{"namespace": "alembic.config.Config.print_stdout", "completion": "        if self.stdout:\n            if arg:\n                text = text % arg\n            self.stdout.write(text)\n            self.stdout.write(\"\\n\")\n", "idx": 956}
{"namespace": "alembic.config.Config.set_section_option", "completion": "        if not self.file_config.has_section(section):\n            self.file_config.add_section(section)\n        self.file_config.set(section, name, value)\n", "idx": 957}
{"namespace": "alembic.command.merge", "completion": "    script_directory = ScriptDirectory.from_config(config)\n\n    command_args = dict(\n        message=message,\n        autogenerate=True,\n        sql=False,\n        head=\"head\",\n        splice=False,\n        branch_label=branch_label,\n        version_path=None,\n        rev_id=rev_id,\n        depends_on=None,\n    )\n    revision_context = autogen.RevisionContext(\n        config,\n        script_directory,\n        command_args,\n    )\n\n    def retrieve_migrations(rev, context):\n        revision_context.run_autogenerate(rev, context)\n        return []\n\n    with EnvironmentContext(\n        config,\n        script_directory,\n        fn=retrieve_migrations,\n        as_sql=False,\n        template_args=revision_context.template_args,\n        revision_context=revision_context,\n    ):\n        script_directory.run_env()\n\n    # the revision_context now has MigrationScript structure(s) present.\n\n    migration_script = revision_context.generated_revisions[-1]\n    migration_script.doc = message\n    migration_script.upgrade_ops.reverse()\n    migration_script.downgrade_ops.reverse()\n\n    for migration_script in revision_context.generated_revisions:\n        migration_script.merge_branch_points(revisions)\n\n    return migration_script", "idx": 958}
{"namespace": "alembic.command.upgrade", "completion": "    script = ScriptDirectory.from_config(config)\n    environment = util.asbool(config.get_main_option(\"revision_environment\"))\n\n    if environment:\n        def nothing(rev, context):\n            return []\n\n        with EnvironmentContext(\n            config,\n            script,\n            fn=nothing,\n            as_sql=sql,\n            tag=tag,\n        ):\n            script.run_env()\n\n    script.upgrade_revision(revision, sql=sql)\n\n", "idx": 959}
{"namespace": "alembic.command.downgrade", "completion": "    script = ScriptDirectory.from_config(config)\n\n    starting_rev = None\n    if \":\" in revision:\n        if not sql:\n            raise util.CommandError(\"Range revision not allowed\")\n        starting_rev, revision = revision.split(\":\", 2)\n\n    def downgrade(rev, context):\n        return script._downgrade_revs(revision, rev)\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=downgrade,\n        as_sql=sql,\n        starting_rev=starting_rev,\n        destination_rev=revision,\n        tag=tag,\n    ):\n        script.run_env()\n\n", "idx": 960}
{"namespace": "alembic.command.history", "completion": "    script = ScriptDirectory.from_config(config)\n\n    def _display_history(config, script, start, end, verbose, indicate_current):\n        for sc in script.walk_revisions(start, end):\n            if sc.is_head:\n                continue\n            if indicate_current and sc.is_current:\n                config.print_stdout(\n                    \"Current Revision: [%s] %s\" % (sc.revision, sc.log_entry)\n                )\n            else:\n                config.print_stdout(\n                    \"Revision: [%s] %s\" % (sc.revision, sc.log_entry)\n                )\n            if verbose:\n                config.print_stdout(\"Parent Revisions: %s\" % (sc.down_revision or \"BASE\"))\n\n    if not rev_range:\n        start_rev = script.get_base()\n        end_rev = script.get_current_head()\n    else:\n        start_rev, end_rev = script._resolve_revision_range(rev_range)\n\n    with EnvironmentContext(config, script, fn=_display_history):\n        script.run_env()\n\n", "idx": 961}
{"namespace": "alembic.command.stamp", "completion": "    script = ScriptDirectory.from_config(config)\n\n    def do_stamp(rev, context):\n        if purge:\n            context.stamp_all()\n        if rev == \"heads\":\n            context.stamp_revs(script.get_revisions(\"heads\"), True)\n        elif rev == \"current\":\n            context.stamp_revs(script.get_revisions(\"heads\"), False)\n        else:\n            context.stamp_revs(script.get_revisions(rev), True)\n        return []\n\n    with EnvironmentContext(\n        config, script, fn=do_stamp, as_sql=sql, tag=tag\n    ):\n        script.run_env()", "idx": 962}
{"namespace": "alembic.command.ensure_version", "completion": "    script = ScriptDirectory.from_config(config)\n\n    def do_ensure_version(rev, context):\n        return script._ensure_version_table_exists(rev)\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=do_ensure_version,\n        as_sql=sql,\n        starting_rev=None,\n        destination_rev=None,\n        tag=None,\n    ):\n        script.run_env()", "idx": 963}
{"namespace": "alembic.autogenerate.compare._compare_server_default", "completion": "    if conn_col.server_default is not None:\n        alter_column_op.existing_server_default = conn_col.server_default\n\n    if metadata_col.server_default is not None:\n        alter_column_op.modify_server_default = metadata_col.server_default\n\n", "idx": 964}
{"namespace": "alembic.autogenerate.render._render_server_default", "completion": "    rendered = _user_defined_render(\"server_default\", default, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    if sqla_compat._server_default_is_computed(default):\n        return \"%r\" % default.arg\n    elif sqla_compat._server_default_is_identity(default):\n        return \"None\"\n    elif isinstance(default, sqla_compat.DefaultClause):\n        if isinstance(default.arg, str):\n            return \"%r\" % default.arg\n        else:\n            return _render_potential_expr(default.arg, autogen_context)\n    elif isinstance(default, sqla_compat.TextClause):\n        return _render_potential_expr(default, autogen_context)\n    elif isinstance(default, sqla_compat.ColumnElement):\n        return _render_potential_expr(default, autogen_context)\n    elif isinstance(default, str) and repr_:\n        return default.strip(\"'\").strip('\"')\n    else:\n        return repr(default)\n\n", "idx": 965}
{"namespace": "alembic.autogenerate.render._render_constraint", "completion": "    rendered = _user_defined_render(\"constraint\", constraint, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    try:\n        return _constraint_renderers.dispatch_on_obj(constraint)(\n            constraint, autogen_context, namespace_metadata\n        )\n    except NotImplementedError:\n        return \"UNKNOWN\"\n\n", "idx": 966}
{"namespace": "alembic.autogenerate.render._render_unique_constraint", "completion": "    rendered = _user_defined_render(\"unique_constraint\", constraint, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    opts = []\n    if constraint.name:\n        opts.append(\n            (\"name\", repr(_render_gen_name(autogen_context, constraint.name)))\n        )\n\n    if constraint.deferrable:\n        opts.append((\"deferrable\", repr(constraint.deferrable)))\n    if constraint.initially:\n        opts.append((\"initially\", repr(constraint.initially)))\n\n    return (\n        \"%(prefix)sUniqueConstraint(%(columns)s, %(args)s)\"\n        % {\n            \"prefix\": _sqlalchemy_autogenerate_prefix(autogen_context),\n            \"columns\": \", \".join(\n                [\n                    _ident(cast(\"Column\", col).name)\n                    for col in constraint.columns\n                ]\n            ),\n            \"args\": \", \".join(\n                [\"%s=%s\" % (kwname, val) for kwname, val in opts]\n            ),\n        }\n    )\n\n", "idx": 967}
{"namespace": "alembic.autogenerate.render._render_check_constraint", "completion": "    rendered = _user_defined_render(\"check\", constraint, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    if constraint.table is None:\n        return None\n\n    if constraint.table.name in autogen_context.metadata_to_save:\n        return None\n\n    opts = []\n    if constraint.name:\n        opts.append(\n            (\"name\", repr(_render_gen_name(autogen_context, constraint.name)))\n        )\n\n    return (\n        \"%(prefix)sCheckConstraint(%(sqltext)r, %(args)s)\"\n        % {\n            \"prefix\": _sqlalchemy_autogenerate_prefix(autogen_context),\n            \"sqltext\": constraint.sqltext.text,\n            \"args\": \", \".join([\"%s=%r\" % (kwname, val) for kwname, val in opts]),\n        }\n    )\n\n", "idx": 968}
{"namespace": "alembic.autogenerate.api.compare_metadata", "completion": "    dialect = context.dialect\n    connection = context.bind\n\n    # get the set of metadata from the database\n    # and the set of metadata from the current model\n    # (or at least the model that we're working with)\n    #\n    # the model can be a superset of the database,\n    # so new tables and columns can be present in the\n    # model but not in the database.\n\n    database_schema = schema_from_metadata(\n        metadata,\n        connection,\n        context.opts.get(\"include_object\", None),\n        context.opts.get(\"include_schemas\", False),\n        context.opts.get(\"include_tables\", None),\n        context.opts.get(\"include_columns\", None),\n        context.opts.get(\"exclude_object\", None),\n        context.opts.get(\"exclude_schemas\", False),\n        context.opts.get(\"exclude_tables\", None),\n        context.opts.get(\"exclude_columns\", None),\n    )\n\n    model_schema = schema_from_metadata(\n        metadata,\n        None,\n        context.opts.get(\"include_object\", None),\n        context.opts.get(\"include_schemas\", False),\n        context.opts.get(\"include_tables\", None),\n        context.opts.get(\"include_columns\", None),\n        context.opts.get(\"exclude_object\", None),\n        context.opts.get(\"exclude_schemas\", False),\n        context.opts.get(\"exclude_tables\", None),\n        context.opts.get(\"exclude_columns\", None),\n    )\n\n    # the model is the \"set that knows\",\n    # the database is the \"subset\".\n    return compare.compare_metadata(\n        database_schema,\n        model_schema,\n        context.opts.get(\"compare_server_default\", False),\n        context.opts.get(\"compare_server_default\", False),\n        context.opts.get(\"compare_server_default\", False),\n    )\n\n", "idx": 969}
{"namespace": "alembic.autogenerate.api.AutogenContext._within_batch", "completion": "        self._has_batch = True\n        yield\n        self._has_batch = False\n", "idx": 970}
{"namespace": "alembic.util.sqla_compat._connectable_has_table", "completion": "    if sqla_14:\n        return connectable.dialect.has_table(connectable, tablename, schema=schemaname)\n    else:\n        return connectable.bind.dialect.has_table(\n            connectable.bind, tablename, schema=schemaname\n        )\n\n", "idx": 971}
{"namespace": "alembic.util.sqla_compat._get_constraint_final_name", "completion": "    if not constraint.name:\n        return None\n\n    if sqla_14:\n        return constraint.name\n\n    if dialect is None:\n        dialect = sqla_compat._get_dialect(constraint)\n\n    compiler = dialect.statement_compiler(dialect, None)\n    return compiler.preparer.format_constraint(constraint)\n\n", "idx": 972}
{"namespace": "alembic.testing.env.env_file_fixture", "completion": "    dir_ = os.path.join(_get_staging_directory(), \"scripts\")\n    path = os.path.join(dir_, \"env.py\")\n    with open(path, \"w\") as f:\n        f.write(txt)\n\n", "idx": 973}
{"namespace": "alembic.testing.env._no_sql_testing_config", "completion": "    dir_ = os.path.join(_get_staging_directory(), \"scripts\")\n    return _write_config_file(\n        \"\"\"", "idx": 974}
{"namespace": "alembic.testing.env._write_config_file", "completion": "    config = _testing_config()\n    with open(config.config_file_name, \"w\") as f:\n        f.write(text)\n    return config\n\n", "idx": 975}
{"namespace": "alembic.testing.env.three_rev_fixture", "completion": "    a, b, c = \"a\", \"b\", \"c\"\n\n    script = ScriptDirectory.from_config(cfg)\n\n    a_script = script.generate_revision(a, \"revision a\", refresh=True)\n    a_script.upgrade_ops.ops = [\n        \"CREATE TABLE a (id INTEGER)\"]\n    a_script.downgrade_ops.ops = [\n        \"DROP TABLE a\"]\n\n    b_script = script.generate_revision(b, \"revision b\", refresh=True)\n    b_script.upgrade_ops.ops = [\n        \"CREATE TABLE b (id INTEGER)\"]\n    b_script.downgrade_ops.ops = [\n        \"DROP TABLE b\"]\n\n    c_script = script.generate_revision(c, \"revision c\", refresh=True)\n    c_script.upgrade_ops.ops = [\n        \"CREATE TABLE c (id INTEGER)\"]\n    c_script.downgrade_ops.ops = [\n        \"DROP TABLE c\"]\n\n    script.generate_revision(a, None, refresh=True)\n    script.generate_revision(b, None, refresh=True)\n    script.generate_revision(c, None, refresh=True)\n\n    return a, b, c", "idx": 976}
{"namespace": "alembic.testing.env.multi_heads_fixture", "completion": "    script = ScriptDirectory.from_config(cfg)\n    d = util.rev_id()\n    e = util.rev_id()\n    f = util.rev_id()\n\n    script.generate_revision(d, \"revision d\", refresh=True, head=a)\n    write_script(\n        script,\n        d,\n        \"\"\"\\", "idx": 977}
{"namespace": "alembic.testing.fixtures.capture_db", "completion": "    buf = io.StringIO()\n    eng = create_engine(dialect, echo=True, paramstyle=\"qmark\", logging_name=\"capture\")\n    eng.dialect.dbapi.paramstyle = \"qmark\"\n    eng.connect().connection.set_trace_callback(buf.write)\n    return eng, buf\n\n", "idx": 978}
{"namespace": "alembic.testing.fixtures.capture_engine_context_buffer", "completion": "    if kw.pop(\"bytes_io\", False):\n        buf = io.BytesIO()\n    else:\n        buf = io.StringIO()\n\n    kw.update({\"dialect_name\": \"sqlite\", \"output_buffer\": buf})\n    conf = EnvironmentContext.configure\n\n    def configure(*arg, **opt):\n        opt.update(**kw)\n        return conf(*arg, **opt)\n\n    with mock.patch.object(EnvironmentContext, \"configure\", configure):\n        eng = config.db\n        eng, buf = _engs[eng] = capture_db(dialect=eng.url)\n        yield buf\n\n", "idx": 979}
{"namespace": "alembic.operations.schemaobj.SchemaObjects.unique_constraint", "completion": "        m = self.metadata()\n        t = sa_schema.Table(\n            source,\n            m,\n            *[sa_schema.Column(n, NULLTYPE) for n in local_cols],\n            schema=schema,\n        )\n        uq = sa_schema.UniqueConstraint(*[t.c[n] for n in local_cols], name=name, **kw)\n        t.append_constraint(uq)\n        return uq\n", "idx": 980}
{"namespace": "alembic.operations.schemaobj.SchemaObjects.index", "completion": "        m = self.metadata()\n        t = sa_schema.Table(tablename, m, schema=schema)\n        return sa_schema.Index(name, t, columns, **kw)\n", "idx": 981}
{"namespace": "alembic.operations.ops.DropConstraintOp.from_constraint", "completion": "        if isinstance(constraint, ForeignKeyConstraint):\n            return cls(\n                constraint.name,\n                constraint.parent.name,\n                \"foreignkey\",\n                schema=constraint.parent.schema,\n                _reverse=AddConstraintOp.from_constraint(constraint),\n            )\n        elif isinstance(constraint, CheckConstraint):\n            return cls(\n                constraint.name,\n                constraint.table.name,\n                \"check\",\n                schema=constraint.table.schema,\n                _reverse=AddConstraintOp.from_constraint(constraint),\n            )\n        elif isinstance(constraint, UniqueConstraint):\n            return cls(\n                constraint.name,\n                constraint.table.name,\n                \"unique\",\n                schema=constraint.table.schema,\n                _reverse=AddConstraintOp.from_constraint(constraint),\n            )\n        elif isinstance(constraint, PrimaryKeyConstraint):\n            return cls(\n                constraint.name,\n                constraint.table.name,\n                \"primary\",\n                schema=constraint.table.schema,\n                _reverse=AddConstraintOp.from_constraint(constraint),\n            )\n        elif isinstance(constraint, Identity):\n            return cls(\n                constraint.name,\n                constraint.table.name,\n                \"identity\",\n                schema=constraint.table.schema,\n                _reverse=AddConstraintOp.from_constraint(constraint),\n            )\n        elif isinstance(constraint, Computed):\n            return cls(\n                constraint.name,\n                constraint.table.name,\n                \"computed\",\n                schema=constraint.table.schema,\n                _reverse=AddConstraintOp.from_constraint(constraint),\n            )\n        else:\n            raise ValueError(\n                \"Constraint object is not supported: %r\" % constraint\n            )\n", "idx": 982}
{"namespace": "alembic.operations.ops.DropConstraintOp.to_constraint", "completion": "        if self._reverse:\n            constraint = self._reverse.to_constraint()\n            constraint.name = self.constraint_name\n            constraint.table = sqla_compat._table_for_constraint(constraint)\n            constraint.table.name = self.table_name\n            constraint.table.schema = self.schema\n            return constraint\n        else:\n            raise ValueError(\n                \"Cannot convert a DropConstraintOp object to a Constraint object \"\n                \"if the reverse operation is not present.\"\n            )\n\n", "idx": 983}
{"namespace": "alembic.operations.ops.CreatePrimaryKeyOp.to_constraint", "completion": "        schema = migration_context.impl.dialect.default_schema_name\n        schema_obj = schemaobj.SchemaObjects(migration_context, schema)\n        return schema_obj.primary_key_constraint(\n            self.constraint_name, self.table_name, self.columns, **self.kw\n        )\n", "idx": 984}
{"namespace": "alembic.operations.ops.CreateIndexOp.from_index", "completion": "        kw: dict = {}\n        if index.dialect_options:\n            kw[\"dialect_options\"] = index.dialect_options\n        if index.comment:\n            kw[\"comment\"] = index.comment\n        if index.connection:\n            kw[\"connection\"] = index.connection\n        if index.name:\n            kw[\"name\"] = index.name\n        if index.tablespace:\n            kw[\"tablespace\"] = index.tablespace\n        if index.postgresql_using:\n            kw[\"postgresql_using\"] = index.postgresql_using\n        if index.postgresql_where:\n            kw[\"postgresql_where\"] = index.postgresql_where\n        if index.mysql_length:\n            kw[\"mysql_length\"] = index.mysql_length\n        if index.mysql_prefix:\n            kw[\"mysql_prefix\"] = index.mysql_prefix\n        if index.oracle_compress:\n            kw[\"oracle_compress\"] = index.oracle_compress\n        if index.quoted_name:\n            kw[\"quoted_name\"] = index.quoted_name\n        if index.dialect_options:\n            kw[\"dialect_options\"] = index.dialect_options\n        if index.postgresql_where:\n            kw[\"postgresql_where\"] = index.postgresql_where\n        if index.postgresql_ops:\n            kw[\"postgresql_ops\"] = index.postgresql_ops\n        if index.postgresql_where:\n            kw[\"postgresql_where\"] = index.postgresql_where\n        if index.postgresql_where:\n            kw[\"postgresql_where\"] = index.postgresql_where\n        if index.postgresql_where:\n            kw[\"postgresql_where\"] = index.postgresql_where\n        if index.postgresql_where:\n            kw[\"postgresql_where\"] = index.postgresql_where\n        if index.postgresql_where:\n            kw[\"postgresql_where\"] = index.postgresql_where\n        if index.postgresql_where:\n            kw[\"postgresql_where\"] = index.postgresql_where\n        if index.postgresql_where:\n            kw[\"postgresql_where\"] = index.postgresql_where\n        if index.postgresql_where:\n            kw[\"postgresql_where\"] =", "idx": 985}
{"namespace": "alembic.operations.ops.DropIndexOp.from_index", "completion": "        assert index.table is not None\n        return cls(\n            index.name,\n            index.table.name,\n            schema=index.table.schema,\n            if_exists=True,\n            _reverse=CreateIndexOp.from_index(index),\n        )\n", "idx": 986}
{"namespace": "alembic.operations.ops.DropIndexOp.to_index", "completion": "        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.index(\n            self.index_name,\n            self.table_name,\n            schema=self.schema,\n            **self.kw,\n        )\n", "idx": 987}
{"namespace": "alembic.operations.ops.CreateTableOp.from_table", "completion": "        if _namespace_metadata is None:\n            _namespace_metadata = table.metadata\n\n        return cls(\n            table.name,\n            table.columns.values(),\n            schema=table.schema,\n            _namespace_metadata=_namespace_metadata,\n            _constraints_included=True,\n            info=table.info,\n            comment=table.comment,\n            prefixes=table.prefixes,\n            **table.kwargs,\n        )\n", "idx": 988}
{"namespace": "alembic.operations.ops.DropTableOp.from_table", "completion": "        if _namespace_metadata is None:\n            _namespace_metadata = table.metadata\n\n        return cls(\n            table.name,\n            schema=table.schema,\n            table_kw=dict(\n                comment=table.comment,\n                info=dict(table.info),\n                prefixes=list(table._prefixes),\n            ),\n            _reverse=CreateTableOp.from_table(table, _namespace_metadata=_namespace_metadata),\n        )\n", "idx": 989}
{"namespace": "alembic.operations.ops.DropTableOp.to_table", "completion": "        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.table(\n            self.table_name,\n            schema=self.schema,\n            prefixes=list(self.prefixes) if self.prefixes else [],\n            comment=self.comment,\n            info=self.info.copy() if self.info else {},\n            **self.table_kw,\n        )\n", "idx": 990}
{"namespace": "alembic.operations.ops.AlterColumnOp.to_diff_tuple", "completion": "        diff_tuple = ()\n        if self.modify_type is not None:\n            diff_tuple += (\n                \"modify_type\",\n                {\n                    \"existing_type\": self.existing_type,\n                    \"modify_type\": self.modify_type,\n                },\n            )\n        if self.modify_nullable is not None:\n            diff_tuple += (\n                \"modify_nullable\",\n                {\n                    \"existing_nullable\": self.existing_nullable,\n                    \"modify_nullable\": self.modify_nullable,\n                },\n            )\n        if self.modify_server_default is not False:\n            diff_tuple += (\n                \"modify_server_default\",\n                {\n                    \"existing_server_default\": self.existing_server_default,\n                    \"modify_server_default\": self.modify_server_default,\n                },\n            )\n        if self.modify_comment is not False:\n            diff_tuple += (\n                \"modify_comment\",\n                {\n                    \"existing_comment\": self.existing_comment,\n                    \"modify_comment\": self.modify_comment,\n                },\n            )\n        if self.modify_name is not None:\n            diff_tuple += (\n                \"modify_name\",\n                {\n                    \"existing_name\": self.column_name,\n                    \"modify_name\": self.modify_name,\n                },\n            )\n        return diff_tuple\n", "idx": 991}
{"namespace": "alembic.operations.ops.AddColumnOp.reverse", "completion": "        return DropColumnOp.from_column(self.column, schema=self.schema)\n", "idx": 992}
{"namespace": "alembic.operations.ops.DropColumnOp.reverse", "completion": "        if self._reverse is None:\n            raise ValueError(\n                \"This operation cannot be reversed.\"\n            )\n        return self._reverse\n", "idx": 993}
{"namespace": "alembic.operations.ops.DropColumnOp.from_column_and_tablename", "completion": "        return cls(tname, col.name, schema=schema, _reverse=AddColumnOp.from_column(col))\n", "idx": 994}
{"namespace": "alembic.operations.ops.DropColumnOp.to_column", "completion": "        if self._reverse:\n            return self._reverse.to_column(migration_context)\n\n        if migration_context is None:\n            raise ValueError(\n                \"migration_context is required to create a column object\"\n            )\n\n        schema = schemaobj.SchemaObjects(migration_context)\n        return schema.column(self.column_name, NULLTYPE)\n", "idx": 995}
{"namespace": "alembic.script.revision.RevisionMap.heads", "completion": "        self._map_revisions()\n        return self._heads\n", "idx": 996}
{"namespace": "alembic.script.revision.RevisionMap.add_revision", "completion": "        if revision.revision in self._revision_map:\n            if _replace:\n                self._revision_map[revision.revision] = revision\n            else:\n                raise RevisionError(\n                    \"Revision %s already exists in the map\" % revision.revision\n                )\n        else:\n            self._revision_map[revision.revision] = revision\n\n        if revision.branch_labels:\n            self._map_branch_labels([revision], self._revision_map)\n\n        if revision.dependencies:\n            self._add_depends_on([revision], self._revision_map)\n\n        if revision.down_revision:\n            downrev = self._revision_map[revision.down_revision]\n            downrev.add_nextrev(revision)\n\n        if revision.is_branch_point:\n            self._add_branches([revision], self._revision_map)\n\n        if revision.is_merge_point:\n            self._add_merge_points([revision], self._revision_map)\n\n        if revision.is_ancestor:\n            self._add_ancestors([revision], self._revision_map)\n\n        if revision.is_or_has_direct_dependencies:\n            self._add_referenced_revisions([revision], self._revision_map)\n\n        if revision.is_or_has_descendant_dependencies:\n            self._add_referenced_revisions(\n                self._get_descendant_nodes([revision], self._revision_map),\n                self._revision_map,\n            )\n\n        if revision.is_or_has_next_revision:\n            self._add_referenced_revisions(\n                self._get_next_nodes([revision], self._revision_map),\n                self._revision_map,\n            )\n\n        if revision.is_or_has_prev_revision:\n            self._add_referenced_revisions(\n                self._get_prev_nodes([revision], self._", "idx": 997}
{"namespace": "alembic.script.revision.RevisionMap.get_revisions", "completion": "        if id_ is None:\n            return ()\n        elif isinstance(id_, str):\n            id_ = [id_]\n        elif not isinstance(id_, collections.abc.Sequence):\n            raise TypeError(\n                \"get_revisions() argument must be a string or sequence of strings\"\n            )\n\n        # convert to a set of string\n        id_ = set(id_)\n\n        # if we have a single identifier, we want to expand it to a list of\n        # identifiers\n        if len(id_) == 1:\n            id_ = self._expand_heads(id_.pop(), False)\n\n        # convert to a set of string\n        id_ = set(id_)\n\n        # now we want to remove all identifiers that are not in the revision map\n        id_ = id_.intersection(self._revision_map.keys())\n\n        # now we want to remove all identifiers that are not in the revision map\n        id_ = id_.intersection(self._revision_map.keys())\n\n        # now we want to remove all identifiers that are not in the revision map\n        id_ = id_.intersection(self._revision_map.keys())\n\n        # now we want to remove all identifiers that are not in the revision map\n        id_ = id_.intersection(self._revision_map.keys())\n\n        # now we want to remove all identifiers that are not in the revision map\n        id_ = id_.intersection(self._revision_map.keys())\n\n        # now we want to remove all identifiers that are not in the revision map\n        id_ = id_.intersection(self._revision_map.keys())\n\n        # now we want to remove all identifiers that are not in the revision map\n        id_ = id_.intersection(self._revision_map.keys())\n\n        # now we want to remove all identifiers that are not in the revision map\n        id_ = id_.intersection(self._revision_map.keys())\n\n        # now we want to remove all identifiers that are not in the revision map\n        id_ = id_.intersection(self._", "idx": 998}
{"namespace": "alembic.script.revision.RevisionMap.get_revision", "completion": "        if id_ is None:\n            return None\n\n        if id_ == \"base\":\n            return self.get_base()\n\n        if id_ == \"head\":\n            return self.get_current_head()\n\n        if id_ == \"heads\":\n            return self.get_heads()\n\n        if id_ == \"current\":\n            return self.get_revision(self.get_current_head())\n\n        if id_ == \"current_heads\":\n            return self.get_revision(self.get_heads())\n\n        if id_ == \"pending\":\n            return self.get_revision(self.get_heads() + self.get_base())\n\n        if id_ == \"all\":\n            return self.get_revisions(self.get_heads() + self.get_base())\n\n        if id_ == \"all_heads\":\n            return self.get_revisions(self.get_heads())\n\n        if id_ == \"all_base\":\n            return self.get_revisions(self.get_base())\n\n        if id_ == \"all_current\":\n            return self.get_revisions(self.get_current_head())\n\n        if id_ == \"all_pending\":\n            return self.get_revisions(self.get_heads() + self.get_base())\n\n        if id_ == \"all_current_heads\":\n            return self.get_revisions(self.get_heads())\n\n        if id_ == \"all_head\":\n            return self.get_revisions(self.get_heads())\n\n        if id_ == \"all_base\":\n            return self.get_revisions(self.get_base())\n\n        if id_ == \"all_current\":\n            return self.get_revisions(self.get_current_head())\n\n        if id_ == \"all_pending\":\n            return self.get_revisions(self.get_heads() + self.get_base())\n\n        if id_ == \"", "idx": 999}
{"namespace": "alembic.script.revision.RevisionMap.filter_for_lineage", "completion": "        targets = set(targets)\n\n        for rev in list(targets):\n            assert rev\n            if targets.intersection(\n                self._get_descendant_nodes(\n                    [rev], include_dependencies=include_dependencies\n                )\n            ).difference([rev]):\n                targets.discard(rev)\n        return tuple(targets)\n", "idx": 1000}
{"namespace": "alembic.script.revision.RevisionMap.iterate_revisions", "completion": "        # ensure map is loaded\n        self._revision_map\n\n        # get the upper and lower revision objects\n        upper_rev = self._revision_for_ident(upper)\n        lower_rev = self._revision_for_ident(lower)\n\n        # get the list of revisions between the upper and lower\n        # revisions\n        revs = self._get_revisions(\n            upper_rev,\n            lower_rev,\n            inclusive=inclusive,\n            implicit_base=implicit_base,\n            select_for_downgrade=select_for_downgrade,\n        )\n\n        # assert that the number of revisions between the upper and lower\n        # is the same as the number of revisions returned\n        if assert_relative_length:\n            assert len(revs) == len(\n                self._get_revisions(\n                    upper_rev,\n                    lower_rev,\n                    inclusive=inclusive,\n                    implicit_base=implicit_base,\n                    select_for_downgrade=select_for_downgrade,\n                    assert_relative_length=False,\n                )\n            )\n\n        return iter(revs)\n", "idx": 1001}
{"namespace": "alembic.script.revision.RevisionMap._topological_sort", "completion": "        # Create a dictionary of the revisions and their dependencies.\n        # The keys of the dictionary are the revision ids and the values are the dependencies of the revision.\n        # The dependencies are represented as a set of revision ids.\n        # The dictionary is of the form:\n        # {\n        #     \"revision_id\": {\"dependencies_revision_id1\", \"dependencies_revision_id2\", ...},\n        #     ...\n        # }\n        # The dictionary is created in the following way:\n        # 1. For each revision in the revisions collection, add it to the dictionary with an empty set as its value.\n        # 2. For each revision in the revisions collection, add its dependencies to the value of the revision in the dictionary.\n        # 3. For each revision in the revisions collection, add the revision to the value of the dependencies of the revision in the dictionary.\n        # 4. For each revision in the revisions collection, add the revision to the value of the dependencies of the dependencies of the revision in the dictionary.\n        # 5. Repeat step 4 until there are no more revisions in the revisions collection.\n        # 6. For each revision in the revisions collection, add its dependencies to the value of the revision in the dictionary.\n        # 7. For each revision in the revisions collection, add the revision to the value of the dependencies of the revision in the dictionary.\n        # 8. For each revision in the revisions collection, add the revision to the value of the dependencies of the dependencies of the revision in the dictionary.\n        # 9. Repeat step 8 until there are no more revisions in the revisions collection.\n        # 10. For each revision in the revisions collection, add its dependencies to the value of the revision in the dictionary.\n        # 11. For each revision in the revisions collection, add the revision to the value of the dependencies of the revision in the dictionary.\n        # 12. For each revision in the revisions collection, add the revision to the value of the dependencies of the dependencies of the revision in the dictionary.\n        # 13. Repeat step 12 until there are no more revisions in the revisions collection.", "idx": 1002}
{"namespace": "alembic.script.revision.Revision._all_down_revisions", "completion": "        return tuple(\n            set(\n                util.to_tuple(self.down_revision, default=())\n                + util.to_tuple(self._resolved_dependencies)\n            )\n        )\n", "idx": 1003}
{"namespace": "alembic.script.revision.Revision._normalized_down_revisions", "completion": "        return util.dedupe_tuple(\n            util.to_tuple(self.down_revision, default=())\n            + self._normalized_resolved_dependencies\n        )\n", "idx": 1004}
{"namespace": "alembic.script.write_hooks._invoke", "completion": "    formatter = _registry.get(name)\n    if formatter is None:\n        raise util.CommandError(\n            \"No formatter with name '%s' registered\" % name\n        )\n    else:\n        return formatter(revision, **options)\n\n", "idx": 1005}
{"namespace": "bplustree.memory.FileMemory.get_node", "completion": "        with self._lock.gen_rlock():\n            if page in self._cache:\n                return self._cache[page]\n            else:\n                data = self._read_page(page)\n                node = Node.from_bytes(data, self._tree_conf)\n                self._cache[page] = node\n                return node\n", "idx": 1006}
{"namespace": "bplustree.memory.FileMemory.next_available_page", "completion": "        self._lock.reader_lock.acquire()\n        self.last_page += 1\n        self._lock.reader_lock.release()\n        return self.last_page\n", "idx": 1007}
{"namespace": "bplustree.memory.FileMemory.get_metadata", "completion": "        # Get the first page of the file\n        data = self._read_page(0)\n\n        # Extract the root node page\n        root_node_page = int.from_bytes(data[:PAGE_REFERENCE_BYTES], ENDIAN)\n\n        # Extract the page size\n        page_size = int.from_bytes(data[PAGE_REFERENCE_BYTES:PAGE_REFERENCE_BYTES + OTHERS_BYTES], ENDIAN)\n\n        # Extract the order\n        order = int.from_bytes(data[PAGE_REFERENCE_BYTES + OTHERS_BYTES:PAGE_REFERENCE_BYTES + OTHERS_BYTES + OTHERS_BYTES], ENDIAN)\n\n        # Extract the key size\n        key_size = int.from_bytes(data[PAGE_REFERENCE_BYTES + OTHERS_BYTES + OTHERS_BYTES:PAGE_REFERENCE_BYTES + OTHERS_BYTES + OTHERS_BYTES + OTHERS_BYTES + OTHERS_BYTES], ENDIAN)\n\n        # Extract the value size\n        value_size = int.from_bytes(data[PAGE_REFERENCE_BYTES + OTHERS_BYTES + OTHERS_BYTES + OTHERS_BYTES + OTHERS_BYTES + OTHERS_BYTES:], ENDIAN)\n\n        # Create a TreeConf object with the extracted values\n        tree_conf = TreeConf(page_size, order, key_size, value_size)\n\n        # Return the root node page and the TreeConf object\n        return root_node_page, tree_conf\n", "idx": 1008}
{"namespace": "bplustree.memory.FileMemory.set_metadata", "completion": "        self._tree_conf = tree_conf\n        self._write_page(0, root_node_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN) +\n                         tree_conf.page_size.to_bytes(OTHERS_BYTES, ENDIAN) +\n                         tree_conf.order.to_bytes(OTHERS_BYTES, ENDIAN) +\n                         tree_conf.key_size.to_bytes(OTHERS_BYTES, ENDIAN) +\n                         tree_conf.value_size.to_bytes(OTHERS_BYTES, ENDIAN))\n", "idx": 1009}
{"namespace": "bplustree.memory.WAL.checkpoint", "completion": "        if self._not_committed_pages:\n            logger.warning('There are uncommitted pages, '\n                           'performing a checkpoint will discard them')\n\n        self._fd.flush()\n        fsync_file_and_dir(self._fd.fileno(), self._dir_fd)\n\n        self._fd.seek(self.FRAME_HEADER_LENGTH)\n        while True:\n            frame_type = self._fd.read(FRAME_TYPE_BYTES)\n            if not frame_type:\n                break\n            frame_type = FrameType(int.from_bytes(frame_type, ENDIAN))\n            if frame_type == FrameType.PAGE:\n                page = int.from_bytes(self._fd.read(PAGE_REFERENCE_BYTES),\n                                      ENDIAN)\n                data = self._fd.read(self._page_size)\n                yield page, data\n\n        self._fd.close()\n        if self._dir_fd is not None:\n            os.close(self._dir_fd)\n\n        os.remove(self.filename)\n        if self._dir_fd is not None:\n            fsync_file_and_dir(self._dir_fd)\n", "idx": 1010}
{"namespace": "bplustree.memory.WAL.commit", "completion": "        if self._not_committed_pages:\n            self._add_frame(FrameType.COMMIT)\n", "idx": 1011}
{"namespace": "bplustree.memory.WAL.rollback", "completion": "        if self._not_committed_pages:\n            self._add_frame(FrameType.ROLLBACK)\n", "idx": 1012}
{"namespace": "bplustree.entry.Record.dump", "completion": "        used_key_length = len(self.key)\n        used_value_length = len(self.value)\n        overflow_page = self.overflow_page\n\n        if overflow_page:\n            used_value_length = 0\n            value = b''\n        else:\n            value = self.value\n\n        used_key_length_bytes = used_key_length.to_bytes(\n            USED_KEY_LENGTH_BYTES, ENDIAN\n        )\n        used_value_length_bytes = used_value_length.to_bytes(\n            USED_VALUE_LENGTH_BYTES, ENDIAN\n        )\n        overflow_page_bytes = overflow_page.to_bytes(\n            PAGE_REFERENCE_BYTES, ENDIAN\n        )\n\n        return (\n            used_key_length_bytes +\n            self._tree_conf.serializer.serialize(self.key) +\n            used_value_length_bytes +\n            value +\n            overflow_page_bytes\n        )\n", "idx": 1013}
{"namespace": "bplustree.entry.Reference.__repr__", "completion": "        return '<Reference: key={} before={} after={}>'.format(self.key, self.before, self.after)", "idx": 1014}
{"namespace": "bplustree.node.Node.dump", "completion": "        data = bytearray()\n        data.extend(self._node_type_int.to_bytes(NODE_TYPE_BYTES, ENDIAN))\n        data.extend(len(self.entries).to_bytes(USED_PAGE_LENGTH_BYTES, ENDIAN))\n        data.extend(self.next_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN))\n        for entry in self.entries:\n            data.extend(entry.dump())\n        data.extend(b'\\x00' * (self._tree_conf.page_size - len(data)))\n        return data\n", "idx": 1015}
{"namespace": "bplustree.node.Node._find_entry_index", "completion": "        entry = self._entry_class(self._tree_conf, key=key)\n        return bisect.bisect_left(self.entries, entry)\n", "idx": 1016}
{"namespace": "bplustree.node.Node.from_page_data", "completion": "        node_type = data[NODE_TYPE_BYTES]\n        if node_type == cls._node_type_int:\n            return cls(tree_conf, data=data, page=page)\n        elif node_type == 1:\n            from .leaf import Leaf\n            return Leaf(tree_conf, data=data, page=page)\n        elif node_type == 2:\n            from .index import Index\n            return Index(tree_conf, data=data, page=page)\n        else:\n            raise ValueError('Unknown node type {}'.format(node_type))\n\n", "idx": 1017}
{"namespace": "bplustree.tree.BPlusTree._root_node", "completion": "        return self._mem.get_node(self._root_node_page)\n", "idx": 1018}
{"namespace": "bplustree.tree.BPlusTree._left_record_node", "completion": "        node = self._root_node\n        while not isinstance(node, (LonelyRootNode, LeafNode)):\n            node = self._mem.get_node(node.next_page)\n        return node\n", "idx": 1019}
{"namespace": "mopidy.ext.Extension.get_cache_dir", "completion": "        cache_dir = path.expand_path(config[cls.ext_name][\"cache_dir\"])\n        if not cache_dir.exists():\n            cache_dir.mkdir(parents=True)\n        return cache_dir\n", "idx": 1020}
{"namespace": "mopidy.ext.Extension.get_config_dir", "completion": "        if cls.ext_name is None:\n            raise AssertionError\n        config_dir_path = (\n            path.expand_path(config[\"core\"][\"config_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(config_dir_path)\n        return config_dir_path\n", "idx": 1021}
{"namespace": "mopidy.ext.Extension.get_data_dir", "completion": "        if cls.ext_name is None:\n            raise AssertionError\n        data_dir_path = (\n            path.expand_path(config[\"core\"][\"data_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(data_dir_path)\n        return data_dir_path\n", "idx": 1022}
{"namespace": "mopidy.ext.load_extensions", "completion": "    extensions = []\n    for entry_point in pkg_resources.iter_entry_points(group='mopidy.ext'):\n        logger.debug(f\"Loading extension: {entry_point.name}\")\n        try:\n            ext_class = entry_point.load()\n        except Exception as e:\n            logger.error(f\"Failed to load extension {entry_point.name}: {e}\")\n            continue\n\n        if not issubclass(ext_class, Extension):\n            logger.error(\n                f\"{entry_point.name} does not implement the Extension class\"\n            )\n            continue\n\n        ext = ext_class()\n        ext.entry_point = entry_point\n        ext.ext_name = entry_point.name\n        ext.version = pkg_resources.get_distribution(ext.dist_name).version\n\n        extensions.append(\n            ExtensionData(\n                extension=ext,\n                entry_point=entry_point,\n                config_schema=ext.get_config_schema(),\n                config_defaults=ext.get_default_config(),\n                command=ext.get_command(),\n            )\n        )\n\n    return extensions", "idx": 1023}
{"namespace": "mopidy.ext.validate_extension_data", "completion": "    # Check if the extension's entry point name matches its extension name\n    if data.extension.ext_name != data.entry_point.name:\n        logger.error(\n            \"Entry point name does not match extension name: %s\",\n            data.entry_point.name,\n        )\n        return False\n\n    # Check if the required dependencies are installed\n    try:\n        data.entry_point.require()\n    except pkg_resources.DistributionNotFound as e:\n        logger.error(\n            \"Extension %s requires distribution %s which is not installed\",\n            data.entry_point.name,\n            e.args[0],\n        )\n        return False\n    except pkg_resources.VersionConflict as e:\n        logger.error(\n            \"Extension %s requires distribution %s version %s but version %s is installed\",\n            data.entry_point.name,\n            e.args[0],\n            e.args[1],\n            e.args[2],\n        )\n        return False\n\n    # Check if the environment is valid\n    try:\n        data.extension.validate_environment()\n    except Exception as e:\n        logger.error(\n            \"Extension %s has an invalid environment: %s\",\n            data.entry_point.name,\n            e,\n        )\n        return False\n\n    # Check if the extension has a valid config schema\n    if not isinstance(data.config_schema, config_lib.ConfigSchema):\n        logger.error(\n            \"Extension %s does not have a valid config schema\",\n            data.entry_point.name,\n        )\n        return False\n\n    # Check if the extension has a valid default config\n    if not isinstance(data.config_defaults, dict):\n        logger.error(\n            \"Extension %s does not have a valid default config\",\n            data.entry_point.name,\n        )\n        return False\n\n    return True", "idx": 1024}
{"namespace": "mopidy.httpclient.format_user_agent", "completion": "    if name:\n        return f\"{name} mopidy/{mopidy_version} {platform.python_implementation()}/{platform.python_version()}\"\n    else:\n        return f\"mopidy/{mopidy_version} {platform.python_implementation()}/{platform.python_version()}\"", "idx": 1025}
{"namespace": "mopidy.models.immutable.ValidatedImmutableObject.replace", "completion": "        other = copy.copy(self)\n        for key, value in kwargs.items():\n            if not self._is_valid_field(key):\n                raise TypeError(\n                    f\"replace() got an unexpected keyword argument {key!r}\"\n                )\n            other._set_field(key, value)\n        return other\n", "idx": 1026}
{"namespace": "mopidy.http.Extension.get_default_config", "completion": "        conf_file = os.path.join(os.path.dirname(__file__), \"ext.conf\")\n        return config_lib.read(conf_file)\n", "idx": 1027}
{"namespace": "mopidy.http.Extension.get_config_schema", "completion": "        schema = super().get_config_schema()\n        schema[\"hostname\"] = config_lib.Hostname()\n        schema[\"port\"] = config_lib.Port()\n        schema[\"static_dir\"] = config_lib.Deprecated()\n        schema[\"zeroconf\"] = config_lib.String()\n        schema[\"zeroconf_name\"] = config_lib.String()\n        schema[\"zeroconf_port\"] = config_lib.Port()\n        return schema\n", "idx": 1028}
{"namespace": "mopidy.internal.network.try_ipv6_socket", "completion": "    try:\n        s = socket.socket(socket.AF_INET6, socket.SOCK_STREAM)\n        s.close()\n        return True\n    except OSError:\n        logger.debug(\"IPv6 is not supported on this system\")\n        return False\n\n", "idx": 1029}
{"namespace": "mopidy.internal.network.format_hostname", "completion": "    if re.match(r\"^[0-9a-fA-F]{1,4}:[0-9a-fA-F]{1,4}:[0-9a-fA-F]{1,4}:[0-9a-fA-F]{1,4}:[0-9a-fA-F]{1,4}:[0-9a-fA-F]{1,4}:[0-9a-fA-F]{1,4}:[0-9a-fA-F]{1,4}$\", hostname):\n        hostname = \"::ffff:\" + \".\".join(hostname.split(\":\")[6:])\n    return hostname", "idx": 1030}
{"namespace": "mopidy.internal.xdg.get_dirs", "completion": "    # Initialize the dictionary\n    xdg_dirs = {\n        \"XDG_CACHE_DIR\": pathlib.Path(\"~/.cache\").expanduser(),\n        \"XDG_CONFIG_DIR\": pathlib.Path(\"~/.config\").expanduser(),\n        \"XDG_DATA_DIR\": pathlib.Path(\"~/.local/share\").expanduser(),\n        \"XDG_DESKTOP_DIR\": pathlib.Path(\"~/Desktop\").expanduser(),\n        \"XDG_DOCUMENTS_DIR\": pathlib.Path(\"~/Documents\").expanduser(),\n        \"XDG_DOWNLOAD_DIR\": pathlib.Path(\"~/Downloads\").expanduser(),\n        \"XDG_ETC_DIR\": pathlib.Path(\"/etc\").expanduser(),\n        \"XDG_MUSIC_DIR\": pathlib.Path(\"~/Music\").expanduser(),\n        \"XDG_PICTURES_DIR\": pathlib.Path(\"~/Pictures\").expanduser(),\n        \"XDG_PUBLICSHARE_DIR\": pathlib.Path(\"/usr/share\").expanduser(),\n        \"XDG_TEMPLATES_DIR\": pathlib.Path(\"~/Templates\").expanduser(),\n        \"XDG_VIDEOS_DIR\": pathlib.Path(\"~/Videos\").expanduser(),\n    }\n\n    # Update the dictionary with the values of the environment variables\n    for key in xdg_dirs.keys():\n        if key in os.environ.keys():\n            xdg_dirs[key] = pathlib.Path(os.environ[key]).expanduser()\n\n    # Update the dictionary with the values of the user-dirs.dirs file\n    user_dirs_file = pathlib.Path(\"~/.config/user-dirs.dirs\").expanduser()\n    if user_dirs_file.exists():\n        with open(user_dirs_file, \"r\") as f:\n            user_dirs_config = configparser.ConfigParser()\n            user_dirs_config.read_file(f)\n            for key in user_dirs_", "idx": 1031}
{"namespace": "mopidy.internal.log.get_verbosity_level", "completion": "    verbosity_level = base_verbosity_level\n\n    if args_verbosity_level is not None:\n        verbosity_level += args_verbosity_level\n\n    verbosity_level += logging_config[\"verbosity\"]\n\n    if verbosity_level < -3:\n        verbosity_level = -3\n    elif verbosity_level > 4:\n        verbosity_level = 4\n\n    return verbosity_level\n\n", "idx": 1032}
{"namespace": "mopidy.internal.validation.check_instance", "completion": "    if not isinstance(arg, cls):\n        raise exceptions.ValidationError(msg.format(arg=arg, name=cls.__name__))\n\n", "idx": 1033}
{"namespace": "mopidy.internal.validation.check_instances", "completion": "    if not isinstance(arg, Iterable):\n        raise exceptions.ValidationError(msg.format(arg=arg, name=cls.__name__))\n    for a in arg:\n        if not isinstance(a, cls):\n            raise exceptions.ValidationError(msg.format(arg=arg, name=cls.__name__))\n\n", "idx": 1034}
{"namespace": "mopidy.internal.validation.check_uri", "completion": "    if not isinstance(arg, str):\n        raise exceptions.ValidationError(msg.format(arg=arg))\n\n    if not urllib.parse.urlparse(arg).scheme:\n        raise exceptions.ValidationError(msg.format(arg=arg))\n\n", "idx": 1035}
{"namespace": "mopidy.internal.validation.check_uris", "completion": "    _check_iterable(arg, msg)\n    for uri in arg:\n        check_uri(uri)\n\n", "idx": 1036}
{"namespace": "mopidy.internal.playlists.parse", "completion": "    handlers = {\n        _is_m3u: _parse_m3u,\n        _is_pls: _parse_pls,\n        _is_asx: _parse_asx,\n        _is_xspf: _parse_xspf,\n        _is_mpd: _parse_mpd,\n        _is_vlc: _parse_vlc,\n        _is_native_m3u: _parse_native_m3u,\n        _is_native_pls: _parse_native_pls,\n    }\n\n    for handler in handlers:\n        if handler(data):\n            return handlers[handler](data)\n\n    return _parse_uris(data)\n\n", "idx": 1037}
{"namespace": "mopidy.config.schemas.ConfigSchema.deserialize", "completion": "        result = {}\n        errors = {}\n        for key, value in values.items():\n            if key in self:\n                try:\n                    result[key] = self[key].deserialize(value)\n                except Exception as e:\n                    errors[key] = str(e)\n            else:\n                errors[key] = \"Unknown config key.\"\n\n        for key in list(result.keys()):\n            if self[key].deprecated:\n                result.pop(key)\n\n        return result, errors\n", "idx": 1038}
{"namespace": "mopidy.config.types.String.deserialize", "completion": "        if not isinstance(value, str):\n            raise TypeError(f\"Expected string, got {type(value)}\")\n\n        value = decode(value).strip()\n\n        if self._required and not value:\n            raise ValueError(\"Missing value\")\n\n        if self._transformer:\n            value = self._transformer(value)\n\n        if self._choices and value not in self._choices:\n            raise ValueError(f\"Invalid value, must be one of {self._choices}\")\n\n        return value\n", "idx": 1039}
{"namespace": "mopidy.config.types.String.serialize", "completion": "        if value is None:\n            return \"\"\n        if isinstance(value, _TransformedValue):\n            value = value.original\n        return encode(value)\n\n", "idx": 1040}
{"namespace": "mopidy.config.types.Secret.serialize", "completion": "        if value is None:\n            return \"\"\n        if display:\n            return \"********\"\n        return super().serialize(value, display)\n\n", "idx": 1041}
{"namespace": "mopidy.config.types.Integer.deserialize", "completion": "        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        value = int(value)\n        validators.validate_choice(value, self._choices)\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        return value\n", "idx": 1042}
{"namespace": "mopidy.config.types.Float.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = float(value)\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        return value\n\n", "idx": 1043}
{"namespace": "mopidy.config.types.Boolean.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        if value.lower() in self.true_values:\n            return True\n        if value.lower() in self.false_values:\n            return False\n        raise ValueError(f\"{value} is not a valid boolean value\")\n", "idx": 1044}
{"namespace": "mopidy.config.types.Pair.deserialize", "completion": "        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        if self._separator in value:\n            parts = value.split(self._separator)\n            if self._optional_pair:\n                return (self._subtypes[0].deserialize(parts[0]), self._subtypes[1].deserialize(parts[0]))\n            else:\n                return (self._subtypes[0].deserialize(parts[0]), self._subtypes[1].deserialize(parts[1]))\n        else:\n            raise ValueError(f\"{value!r} is not a pair\")\n", "idx": 1045}
{"namespace": "mopidy.config.types.Pair.serialize", "completion": "        if value is None:\n            return \"\"\n        if self._optional_pair and value[0] == value[1]:\n            return self._subtypes[0].serialize(value[0], display)\n        else:\n            return self._separator.join(\n                [\n                    self._subtypes[0].serialize(value[0], display),\n                    self._subtypes[1].serialize(value[1], display),\n                ]\n            )\n\n", "idx": 1046}
{"namespace": "mopidy.config.types.List.serialize", "completion": "        if value is None:\n            return \"\"\n        if not isinstance(value, list):\n            raise ValueError(f\"{value!r} is not a list\")\n\n        # This is necessary for backwards-compatibility, in case subclasses\n        # aren't calling their parent constructor.\n        subtype = getattr(self, \"_subtype\", String())\n\n        serialized_values = [subtype.serialize(v, display=display) for v in value]\n        return \"\\n\".join(serialized_values)\n\n", "idx": 1047}
{"namespace": "mopidy.config.types.LogColor.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        valid_colors = [\"black\", \"red\", \"green\", \"yellow\", \"blue\", \"magenta\", \"cyan\", \"white\"]\n        if value.lower() not in valid_colors:\n            raise ValueError(f\"{value} is not a valid color\")\n        return value.lower()\n\n", "idx": 1048}
{"namespace": "mopidy.config.types.LogColor.serialize", "completion": "        if value in log.COLORS:\n            if display:\n                return value\n            else:\n                return log.COLORS[value]\n        else:\n            return \"\"\n\n", "idx": 1049}
{"namespace": "mopidy.config.types.LogLevel.deserialize", "completion": "        value = decode(value)\n        validators.validate_choice(value.lower(), self.levels)\n        return self.levels[value.lower()]\n", "idx": 1050}
{"namespace": "mopidy.config.types.LogLevel.serialize", "completion": "        for key, val in self.levels.items():\n            if val == value:\n                return encode(key)\n        return \"\"\n\n", "idx": 1051}
{"namespace": "mopidy.config.types.Hostname.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        if value.startswith(\"/\"):\n            return str(value)\n        try:\n            socket.gethostbyname(value)\n            return value\n        except socket.gaierror:\n            pass\n        try:\n            socket.gethostname(value)\n            return value\n        except socket.gaierror:\n            raise ValueError(f\"{value!r} is not a valid hostname or IP address\")\n", "idx": 1052}
{"namespace": "mopidy.config.load", "completion": "    # Determine the configuration directory.\n    config_dir = pathlib.Path(files[0]).parent\n\n    # Read the default configuration file.\n    defaults = []\n    for file_ in ext_defaults:\n        defaults.append(read(file_))\n\n    # Extend the list of configuration files.\n    configs = []\n    for file_ in files:\n        configs.append(read(file_))\n\n    # Load the configuration files.\n    raw_config = []\n    for config_ in configs:\n        raw_config.append(configparser.ConfigParser())\n        raw_config[-1].read_string(config_)\n\n    # Combine the default configurations with the configuration files.\n    for default_ in defaults:\n        raw_config.append(configparser.ConfigParser())\n        raw_config[-1].read_string(default_)\n\n    # Combine the configuration files with the overrides.\n    for override in overrides:\n        for section, key, value in override:\n            raw_config[-1].set(section, key, value)\n\n    # Append the external schemas to the list of schemas.\n    schemas = _schemas\n    for schema in ext_schemas:\n        schemas.append(ConfigSchema.from_path(schema))\n\n    # Validate the configuration.\n    return validate(raw_config, schemas, config_dir)\n\n", "idx": 1053}
{"namespace": "mopidy.config.format_initial", "completion": "    from mopidy.config import keyring\n    config_dir = pathlib.Path(__file__).parent\n    defaults = [read(config_dir / \"default.conf\")]\n    defaults.extend(ext_defaults)\n    raw_config = _load(files, defaults, keyring.fetch() + (overrides or []))\n\n    schemas = _schemas[:]\n    schemas.extend(ext_schemas)\n    return _format(config, comments or {}, schemas, display, False)\n\n", "idx": 1054}
{"namespace": "mopidy.config._load", "completion": "    # Create a RawConfigParser instance and set the comment prefixes.\n    config = configparser.RawConfigParser(\n        allow_no_value=True, comment_prefixes=(\";\", \"#\")\n    )\n\n    # Load the configuration from the builtin defaults.\n    config.read_string(\"\".join(defaults))\n\n    # Iterate over the files list and load the configuration from each file.\n    for file in files:\n        if os.path.isdir(file):\n            # If the file is a directory, iterate over the files in the directory and load the configuration from each file with the \".conf\" suffix.\n            for filename in os.listdir(file):\n                if filename.endswith(\".conf\"):\n                    config.read(os.path.join(file, filename))\n        else:\n            # Otherwise, load the configuration from the file.\n            config.read(file)\n\n    # Create a dictionary `raw_config` where each section is a key and the corresponding value is a dictionary of key-value pairs for that section.\n    raw_config = {}\n    for section in config.sections():\n        raw_config[section] = {}\n        for option in config.options(section):\n            raw_config[section][option] = config.get(section, option)\n\n    # Update the `raw_config` dictionary with any command line overrides specified in the `overrides` list.\n    for override in overrides:\n        if override[0] not in raw_config:\n            raw_config[override[0]] = {}\n        raw_config[override[0]][override[1]] = override[2]\n\n    return raw_config\n\n", "idx": 1055}
{"namespace": "mopidy.config._validate", "completion": "    config = {}\n    errors = {}\n    for schema in schemas:\n        section = schema.name\n        if section in raw_config:\n            try:\n                config[section] = schema.deserialize(raw_config[section])\n            except ValueError as e:\n                errors[section] = e.args[0]\n        else:\n            logger.warning(f\"Config section {section} not found\")\n    return config, errors\n\n", "idx": 1056}
{"namespace": "mingus.extra.tunings.get_tunings", "completion": "    searchi = str.upper(instrument)\n    searchd = str.upper(description)\n    keys = list(_known.keys())\n    for x in keys:\n        if (\n            searchi not in keys\n            and x.find(searchi) == 0\n            or searchi in keys\n            and x == searchi\n        ):\n            for (desc, tun) in six.iteritems(_known[x][1]):\n                if desc.find(searchd) == 0:\n                    if nr_of_strings is None and nr_of_courses is None:\n                        return tun\n                    elif nr_of_strings is not None and nr_of_courses is None:\n                        if tun.count_strings() == nr_of_strings:\n                            return tun\n                    elif nr_of_strings is None and nr_of_courses is not None:\n                        if tun.count_courses() == nr_of_courses:\n                            return tun\n                    else:\n                        if (\n                            tun.count_courses() == nr_of_courses\n                            and tun.count_strings() == nr_of_strings\n                        ):\n                            return tun\n\n", "idx": 1057}
{"namespace": "mingus.containers.instrument.Instrument.note_in_range", "completion": "        if isinstance(note, six.string_types):\n            note = Note(note)\n        if not hasattr(note, \"name\"):\n            raise UnexpectedObjectError(\n                \"Unexpected object '%s'. \"\n                \"Expecting a mingus.containers.Note object\" % note\n            )\n        if note < self.range[0] or note > self.range[1]:\n            return False\n        return True\n", "idx": 1058}
{"namespace": "mingus.containers.instrument.Guitar.can_play_notes", "completion": "        if len(notes) > 6:\n            return False\n        else:\n            return Instrument.can_play_notes(self, notes)\n\n", "idx": 1059}
{"namespace": "mingus.containers.bar.Bar.get_range", "completion": "        # Initialize variables\n        highest = 0\n        lowest = 128\n\n        # Iterate through the notes in the bar\n        for note in self.bar:\n            # If the note is higher than the current highest, update the highest\n            if note[2].higher_note() > highest:\n                highest = note[2].higher_note()\n            # If the note is lower than the current lowest, update the lowest\n            if note[2].lower_note() < lowest:\n                lowest = note[2].lower_note()\n\n        # Return the highest and lowest notes\n        return (highest, lowest)\n", "idx": 1060}
{"namespace": "mingus.containers.bar.Bar.transpose", "completion": "        for cont in self.bar:\n            cont[2].transpose(interval, up)\n", "idx": 1061}
{"namespace": "mingus.containers.bar.Bar.determine_chords", "completion": "        chords = []\n        for i in range(len(self.bar)):\n            chords.append([i, self.bar[i][2].determine_chords(shorthand)])\n        return chords\n", "idx": 1062}
{"namespace": "mingus.containers.note.Note.transpose", "completion": "        if up:\n            self.name = notes.transpose(self.name, interval)\n        else:\n            self.name = notes.transpose(self.name, interval, up=False)\n", "idx": 1063}
{"namespace": "mingus.containers.note.Note.from_int", "completion": "        if not isinstance(integer, int):\n            raise TypeError(\"integer must be an integer\")\n        if integer < 0:\n            raise ValueError(\"integer must be a positive integer\")\n        if integer > 127:\n            raise ValueError(\"integer must be less than 128\")\n        self.name = notes.int_to_note(integer)\n        self.octave = int(log(integer, 2))\n        return self\n", "idx": 1064}
{"namespace": "mingus.containers.note.Note.to_hertz", "completion": "        return (2 ** ((self.octave + 1) / 12)) * standard_pitch * (2 ** (self.name / 12))\n", "idx": 1065}
{"namespace": "mingus.containers.note.Note.from_hertz", "completion": "        # 12 * log2(f/440) + 9\n        diff = log(hertz / standard_pitch, 2) * 12 + 9\n        self.name = notes.int_to_note(int(diff))\n        self.octave = int(diff) // 12\n        return self\n", "idx": 1066}
{"namespace": "mingus.containers.note.Note.to_shorthand", "completion": "        if self.octave == 0:\n            return self.name\n        elif self.octave < 0:\n            return self.name + str(self.octave)\n        else:\n            return self.name + \"+\" + str(self.octave)\n", "idx": 1067}
{"namespace": "mingus.containers.note_container.NoteContainer.from_chord_shorthand", "completion": "        self.empty()\n        for x in chords.from_shorthand(shorthand):\n            self.add_note(x)\n        return self\n", "idx": 1068}
{"namespace": "mingus.containers.note_container.NoteContainer.from_interval_shorthand", "completion": "        self.empty()\n        if isinstance(startnote, six.string_types):\n            startnote = Note(startnote)\n        self.add_note(intervals.transpose(startnote, shorthand, up))\n        return self\n", "idx": 1069}
{"namespace": "mingus.containers.note_container.NoteContainer.from_progression_shorthand", "completion": "        self.empty()\n        for x in shorthand.split():\n            if x[0] == \"(\":\n                self.add_note(x[1])\n            else:\n                self.add_note(intervals.from_shorthand(x, key))\n        return self\n", "idx": 1070}
{"namespace": "mingus.containers.note_container.NoteContainer.transpose", "completion": "        for n in self.notes:\n            n.transpose(interval, up)\n        return self\n", "idx": 1071}
{"namespace": "mingus.containers.note_container.NoteContainer.get_note_names", "completion": "        note_names = []\n        for note in self.notes:\n            note_names.append(note.name)\n        note_names = list(set(note_names))\n        return note_names\n", "idx": 1072}
{"namespace": "mingus.core.notes.int_to_note", "completion": "    if note_int < 0 or note_int > 11:\n        raise NoteFormatError(\"Note must be in the range 0-11\")\n\n    note_name = fifths[note_int % 7]\n    if note_int % 7 == 0:\n        return note_name + accidentals[0]\n    else:\n        return note_name + accidentals[1]\n\n", "idx": 1073}
{"namespace": "mingus.core.notes.is_valid_note", "completion": "    try:\n        note_to_int(note)\n        return True\n    except NoteFormatError:\n        return False\n\n", "idx": 1074}
{"namespace": "mingus.core.notes.reduce_accidentals", "completion": "    if is_valid_note(note):\n        val = _note_dict[note[0]]\n    else:\n        raise NoteFormatError(\"Unknown note format '%s'\" % note)\n\n    # Check for '#' and 'b' postfixes\n    for post in note[1:]:\n        if post == \"b\":\n            val -= 1\n        elif post == \"#\":\n            val += 1\n    return int_to_note(val % 12)\n\n", "idx": 1075}
{"namespace": "mingus.core.notes.remove_redundant_accidentals", "completion": "    if is_valid_note(note):\n        val = note_to_int(note[0])\n        for token in note[1:]:\n            if token == \"b\":\n                val -= 1\n            elif token == \"#\":\n                val += 1\n            else:\n                raise NoteFormatError(\"Unknown note format '%s'\" % note)\n        if val >= note_to_int(note[0]):\n            return int_to_note(val % 12)\n        else:\n            return int_to_note(val % 12, \"b\")\n    else:\n        raise NoteFormatError(\"Unknown note format '%s'\" % note)\n\n", "idx": 1076}
{"namespace": "mingus.core.intervals.minor_second", "completion": "    return notes.diminish(note)\n\n", "idx": 1077}
{"namespace": "mingus.core.intervals.major_second", "completion": "    sec = second(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sec, 2)\n\n", "idx": 1078}
{"namespace": "mingus.core.intervals.minor_third", "completion": "    third = third(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, third, 1)\n\n", "idx": 1079}
{"namespace": "mingus.core.intervals.minor_fourth", "completion": "    frt = fourth(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, frt, 5)\n\n", "idx": 1080}
{"namespace": "mingus.core.intervals.minor_seventh", "completion": "    sth = sixth(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sth, 10)\n\n", "idx": 1081}
{"namespace": "mingus.core.intervals.major_seventh", "completion": "    sth = seventh(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sth, 11)\n\n", "idx": 1082}
{"namespace": "mingus.core.intervals.measure", "completion": "    # Check if the input notes are valid\n    if not notes.is_valid_note(note1):\n        raise ValueError(\"The first note is invalid.\")\n    if not notes.is_valid_note(note2):\n        raise ValueError(\"The second note is invalid.\")\n\n    # Get the index of the notes in the list of notes\n    index1 = notes.note_to_int(note1)\n    index2 = notes.note_to_int(note2)\n\n    # Calculate the number of half-note steps between the notes\n    steps = (index2 - index1) % 12\n\n    return steps\n\n", "idx": 1083}
{"namespace": "mingus.core.intervals.determine", "completion": "    # Check if the notes are valid\n    if not notes.is_valid_note(note1):\n        raise ValueError(\"The first note is not valid.\")\n    if not notes.is_valid_note(note2):\n        raise ValueError(\"The second note is not valid.\")\n\n    # Check if the notes are the same\n    if note1 == note2:\n        raise ValueError(\"The notes are the same.\")\n\n    # Check if the notes are in the same octave\n    if note1[0] != note2[0]:\n        raise ValueError(\"The notes are not in the same octave.\")\n\n    # Get the notes in the key of C\n    note1 = notes.convert_note_to_key_of_c(note1)\n    note2 = notes.convert_note_to_key_of_c(note2)\n\n    # Get the interval between the notes\n    interval = measure(note1, note2)\n\n    # Get the name of the interval\n    if shorthand:\n        return interval_shorthand[interval]\n    else:\n        return interval_full_name[interval]\n\n", "idx": 1084}
{"namespace": "mingus.core.intervals.from_shorthand", "completion": "    # Check if the input is valid\n    if not isinstance(note, str):\n        return False\n    if not isinstance(interval, str):\n        return False\n    if not isinstance(up, bool):\n        return False\n\n    # Check if the note is valid\n    if not notes.is_valid_note(note):\n        return False\n\n    # Check if the interval is valid\n    if not interval.isdigit():\n        return False\n    if not 1 <= int(interval) <= 7:\n        return False\n\n    # Check if the note is valid\n    if not notes.is_valid_note(note):\n        return False\n\n    # Check if the interval is valid\n    if not interval.isdigit():\n        return False\n    if not 1 <= int(interval) <= 7:\n        return False\n\n    # Check if the note is valid\n    if not notes.is_valid_note(note):\n        return False\n\n    # Check if the interval is valid\n    if not interval.isdigit():\n        return False\n    if not 1 <= int(interval) <= 7:\n        return False\n\n    # Check if the note is valid\n    if not notes.is_valid_note(note):\n        return False\n\n    # Check if the interval is valid\n    if not interval.isdigit():\n        return False\n    if not 1 <= int(interval) <= 7:\n        return False\n\n    # Check if the note is valid\n    if not notes.is_valid_note(note):\n        return False\n\n    # Check if the interval is valid\n    if not interval.isdigit():\n        return False\n    if not 1 <= int(interval) <= 7:\n        return False\n\n    # Check if the note is valid\n    if not notes.is_valid_note(note):\n        return False\n\n    # Check if the interval is valid\n    if not interval.isdigit():\n        return False\n    if not 1 <= int(interval) <= 7:\n        return False\n\n    # Check if the note is valid\n    if not notes.is_valid_note(note):", "idx": 1085}
{"namespace": "mingus.core.intervals.is_consonant", "completion": "    # Check if the notes are valid\n    if not notes.is_valid_note(note1):\n        raise ValueError(\"The first note is invalid.\")\n    if not notes.is_valid_note(note2):\n        raise ValueError(\"The second note is invalid.\")\n\n    # Get the interval between the two notes\n    interval = measure(note1, note2)\n\n    # Check if the interval is consonant\n    if interval in [1, 2, 4, 5, 8]:\n        return True\n    elif interval == 3 and include_fourths:\n        return True\n    else:\n        return False\n\n", "idx": 1086}
{"namespace": "mingus.core.intervals.is_perfect_consonant", "completion": "    if not notes.is_valid_note(note1) or not notes.is_valid_note(note2):\n        return False\n\n    if note1 == note2:\n        return True\n\n    if measure(note1, note2) == 0:\n        return True\n\n    if measure(note1, note2) == 7:\n        return True\n\n    if include_fourths and measure(note1, note2) == 5:\n        return True\n\n    return False\n\n", "idx": 1087}
{"namespace": "mingus.core.keys.get_key", "completion": "    if accidentals in _key_cache:\n        return _key_cache[accidentals]\n\n    if accidentals < 0:\n        raise NoteFormatError(\"Negative accidentals not allowed\")\n\n    if accidentals > 7:\n        raise NoteFormatError(\"Too many accidentals\")\n\n    major_key = base_scale[accidentals]\n    minor_key = base_scale[accidentals + 3]\n\n    _key_cache[accidentals] = (major_key, minor_key)\n\n    return (major_key, minor_key)\n\n", "idx": 1088}
{"namespace": "mingus.core.keys.get_key_signature", "completion": "    if key not in major_keys and key not in minor_keys:\n        raise NoteFormatError(\"Invalid key.\")\n    if key in major_keys:\n        return 0\n    else:\n        return minor_keys.index(key)\n\n", "idx": 1089}
{"namespace": "mingus.core.keys.get_key_signature_accidentals", "completion": "    if not is_valid_key(key):\n        raise NoteFormatError(\"unrecognized format for key '%s'\" % key)\n\n    key_signature_accidentals = []\n    key_signature_accidentals_count = get_key_signature(key)\n\n    if key_signature_accidentals_count < 0:\n        for i in range(abs(key_signature_accidentals_count)):\n            key_signature_accidentals.append(\"b\")\n    elif key_signature_accidentals_count > 0:\n        for i in range(key_signature_accidentals_count):\n            key_signature_accidentals.append(\"#\")\n\n    return key_signature_accidentals\n\n", "idx": 1090}
{"namespace": "mingus.core.keys.get_notes", "completion": "    if not is_valid_key(key):\n        raise NoteFormatError(\"unrecognized format for key '%s'\" % key)\n\n    for couple in keys:\n        if key in couple:\n            accidentals = keys.index(couple) - 7\n            return list(islice(cycle(base_scale), accidentals, accidentals + 7))\n\n", "idx": 1091}
{"namespace": "mingus.core.keys.relative_major", "completion": "    if not is_valid_key(key):\n        raise NoteFormatError(\"unrecognized format for key '%s'\" % key)\n\n    if key in minor_keys:\n        return major_keys[minor_keys.index(key)]\n    else:\n        raise NoteFormatError(\"unrecognized format for key '%s'\" % key)\n\n", "idx": 1092}
{"namespace": "mingus.core.chords.augmented_triad", "completion": "    return [note, intervals.major_third(note), intervals.augmented_fifth(note)]\n\n", "idx": 1093}
{"namespace": "mingus.core.chords.determine", "completion": "    # Check if the chord is a triad\n    if len(chord) == 3:\n        # Check if the chord is a major triad\n        if chord[1] == \"C#\" or chord[1] == \"Db\":\n            if shorthand:\n                return \"M\"\n            else:\n                return \"Major triad\"\n        # Check if the chord is a minor triad\n        elif chord[1] == \"D#\" or chord[1] == \"Eb\":\n            if shorthand:\n                return \"m\"\n            else:\n                return \"Minor triad\"\n        # Check if the chord is a diminished triad\n        elif chord[1] == \"F#\" or chord[1] == \"Gb\":\n            if shorthand:\n                return \"dim\"\n            else:\n                return \"Diminished triad\"\n        # Check if the chord is an augmented triad\n        elif chord[1] == \"G#\" or chord[1] == \"Ab\":\n            if shorthand:\n                return \"aug\"\n            else:\n                return \"Augmented triad\"\n        # Check if the chord is a suspended triad\n        elif chord[1] == \"A#\" or chord[1] == \"Bb\":\n            if shorthand:\n                return \"sus4\"\n            else:\n                return \"Suspended triad\"\n        # Check if the chord is a suspended second triad\n        elif chord[1] == \"B#\" or chord[1] == \"Cb\":\n            if shorthand:\n                return \"sus2\"\n            else:\n                return \"Suspended second triad\"\n        # Check if the chord is a suspended fourth triad\n        elif chord[1] == \"D\" or chord[1] == \"Eb\":\n            if shorthand:\n                return \"sus47\"\n            else:\n                return \"Suspended fourth triad\"\n        # Check if the chord is a susp", "idx": 1094}
{"namespace": "mingus.core.value.septuplet", "completion": "    if in_fourths:\n        return tuplet(value, 7, 4)\n    else:\n        return tuplet(value, 7, 8)\n\n", "idx": 1095}
{"namespace": "mingus.core.value.determine", "completion": "    # Check if the value is a base value\n    for i in range(len(base_values)):\n        if value == base_values[i]:\n            return (base_values[i], 0, 1)\n\n    # Check if the value is a triplet\n    for i in range(len(base_triplets)):\n        if value == base_triplets[i]:\n            return (base_triplets[i], 0, 3)\n\n    # Check if the value is a quintuplet\n    for i in range(len(base_quintuplets)):\n        if value == base_quintuplets[i]:\n            return (base_quintuplets[i], 0, 5)\n\n    # Check if the value is a septuplet\n    for i in range(len(base_septuplets)):\n        if value == base_septuplets[i]:\n            return (base_septuplets[i], 0, 7)\n\n    # Check if the value is a dotted value\n    for i in range(len(base_values)):\n        for j in range(1, 5):\n            if value == dots(base_values[i], j):\n                return (base_values[i], j, 1)\n\n    # Check if the value is a triplet dotted value\n    for i in range(len(base_triplets)):\n        for j in range(1, 5):\n            if value == dots(base_triplets[i], j):\n                return (base_triplets[i], j, 3)\n\n    # Check if the value is a quintuplet dotted value\n    for i in range(len(base_quintuplets)):\n        for j in range(1, 5):\n            if value == dots(base_quintuplets[i], j):\n                return (base_quintuplets[i], j, 5)\n\n    # Check if the value is a septuplet dotted value\n    for i in range(len", "idx": 1096}
{"namespace": "mingus.core.progressions.substitute_major_for_minor", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Major to minor substitution\n    if (\n        suff == \"M\"\n        or suff == \"M7\"\n        or suff == \"\"\n        and roman in [\"I\", \"IV\", \"V\"]\n        or ignore_suffix\n    ):\n        n = skip(roman, 3)\n        a = interval_diff(roman, n, 2) + acc\n        if suff == \"M\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"m\")))\n        elif suff == \"M7\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"m7\")))\n        elif suff == \"\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"\")))\n    return res\n\n", "idx": 1097}
{"namespace": "mingus.core.progressions.substitute_diminished_for_diminished", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Diminished to diminished substitution\n    if (\n        suff == \"dim7\"\n        or suff == \"dim\"\n        or suff == \"\"\n        and roman == \"VII\"\n        or ignore_suffix\n    ):\n        n = skip(roman, 10)\n        a = interval_diff(roman, n, 11) + acc\n        if suff == \"dim7\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"dim7\")))\n        elif suff == \"dim\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"dim\")))\n        elif suff == \"\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"\")))\n\n    return res\n\n", "idx": 1098}
{"namespace": "mingus.core.progressions.substitute_diminished_for_dominant", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Diminished progressions\n    if (\n        suff == \"dim7\"\n        or suff == \"dim\"\n        or suff == \"\"\n        and roman in [\"VII\"]\n        or ignore_suffix\n    ):\n        if suff == \"\":\n            suff = \"dim\"\n\n        # Add diminished chord\n        last = roman\n        for x in range(4):\n            next = skip(last, 2)\n            acc += interval_diff(last, next, 3)\n            res.append(tuple_to_string((next, acc, suff)))\n            last = next\n    return res\n\n", "idx": 1099}
{"namespace": "mingus.core.progressions.substitute", "completion": "    res = []\n    res.extend(substitute_harmonic(progression, substitute_index))\n    res.extend(substitute_minor_for_major(progression, substitute_index))\n    res.extend(substitute_major_for_minor(progression, substitute_index))\n    res.extend(substitute_diminished_for_diminished(progression, substitute_index))\n    res.extend(substitute_diminished_for_dominant(progression, substitute_index))\n    if depth > 0:\n        for i in range(len(res)):\n            res.extend(substitute(res[i], 0, depth - 1))\n    return res\n\n", "idx": 1100}
{"namespace": "mingus.core.progressions.skip", "completion": "    # Check if the input is a valid roman numeral\n    if roman_numeral not in numerals:\n        raise ValueError(\"Invalid roman numeral\")\n\n    # Check if the skip count is a valid integer\n    if not isinstance(skip_count, int):\n        raise TypeError(\"Skip count must be an integer\")\n\n    # Check if the skip count is a positive integer\n    if skip_count < 0:\n        raise ValueError(\"Skip count must be a positive integer\")\n\n    # Calculate the index of the roman numeral\n    index = numerals.index(roman_numeral)\n\n    # Calculate the index of the roman numeral that is a certain number of places behind in the sequence\n    index = (index + skip_count) % 7\n\n    # Return the roman numeral at the calculated index\n    return numerals[index]", "idx": 1101}
{"namespace": "exodus_bundler.cli.configure_logging", "completion": "    level = logging.WARNING\n    if verbose:\n        level = logging.INFO\n    if quiet:\n        level = logging.ERROR\n    logging.basicConfig(level=level)\n\n    # Add a stderr handler to log warning and error messages.\n    stderr_handler = logging.StreamHandler(sys.stderr)\n    stderr_handler.setLevel(level)\n    stderr_handler.setFormatter(logging.Formatter('%(levelname)s: %(message)s'))\n    logging.getLogger().addHandler(stderr_handler)\n\n    # Add a stdout handler to log debug and info messages.\n    if not suppress_stdout:\n        stdout_handler = logging.StreamHandler(sys.stdout)\n        stdout_handler.setLevel(logging.DEBUG)\n        stdout_handler.setFormatter(logging.Formatter('%(message)s'))\n        logging.getLogger().addHandler(stdout_handler)\n\n", "idx": 1102}
{"namespace": "exodus_bundler.bundling.create_unpackaged_bundle", "completion": "    # Create a temporary directory to store the bundle.\n    root_directory = tempfile.mkdtemp()\n\n    # Create the bundle directory structure.\n    bundle_directory = os.path.join(root_directory, 'exodus')\n    os.makedirs(bundle_directory)\n\n    # Create the bin directory.\n    bin_directory = os.path.join(bundle_directory, 'bin')\n    os.makedirs(bin_directory)\n\n    # Create the lib directory.\n    lib_directory = os.path.join(bundle_directory, 'lib')\n    os.makedirs(lib_directory)\n\n    # Create the share directory.\n    share_directory = os.path.join(bundle_directory, 'share')\n    os.makedirs(share_directory)\n\n    # Create the bin/launchers directory.\n    launchers_directory = os.path.join(bin_directory, 'launchers')\n    os.makedirs(launchers_directory)\n\n    # Create the bin/launchers/shell directory.\n    shell_launchers_directory = os.path.join(launchers_directory, 'shell')\n    os.makedirs(shell_launchers_directory)\n\n    # Create the bin/launchers/binary directory.\n    binary_launchers_directory = os.path.join(launchers_directory, 'binary')\n    os.makedirs(binary_launchers_directory)\n\n    # Create the bin/launchers/compiler directory.\n    compiler_launchers_directory = os.path.join(launchers_directory, 'compiler')\n    os.makedirs(compiler_launchers_directory)\n\n    # Create the bin/launchers/interpreter directory.\n    interpreter_launchers_directory = os.path.join(launchers_directory, 'interpreter')\n    os.makedirs(interpreter_launchers_directory)\n\n    # Create the bin/launchers/script directory.\n    script_launchers_directory = os.path.join(launchers_directory, 'script')\n    os.makedirs(script_launchers_directory", "idx": 1103}
{"namespace": "exodus_bundler.bundling.detect_elf_binary", "completion": "    with open(filename, 'rb') as f:\n        signature = f.read(4)\n        if signature == b'\\x7fELF':\n            return True\n        else:\n            return False\n\n", "idx": 1104}
{"namespace": "exodus_bundler.bundling.resolve_binary", "completion": "    if os.path.exists(binary):\n        return os.path.abspath(binary)\n\n    for path in os.environ['PATH'].split(os.pathsep):\n        path = path.strip('\"')\n        exe_file = os.path.join(path, binary)\n        if os.path.isfile(exe_file) and os.access(exe_file, os.X_OK):\n            return os.path.abspath(exe_file)\n\n    raise MissingFileError('The \"%s\" binary could not be found in $PATH.' % binary)\n\n", "idx": 1105}
{"namespace": "exodus_bundler.bundling.resolve_file_path", "completion": "    # If the path is already absolute, just return it.\n    if os.path.isabs(path):\n        return os.path.normpath(path)\n\n    # If the path is already relative, we'll need to resolve it.\n    if not search_environment_path:\n        # If the path is relative, we'll need to resolve it.\n        for directory in os.getcwd(), os.path.dirname(os.path.abspath(__file__)):\n            absolute_path = os.path.normpath(os.path.join(directory, path))\n            if os.path.exists(absolute_path):\n                return absolute_path\n        raise MissingFileError('The \"%s\" file could not be found in the current working directory or the directory of the exodus-bundler script.' % path)\n\n    # If the path is relative, we'll need to resolve it.\n    for directory in os.getenv('PATH', '/bin/:/usr/bin/').split(os.pathsep):\n        absolute_path = os.path.normpath(os.path.join(directory, path))\n        if os.path.exists(absolute_path):\n            return absolute_path\n    raise MissingFileError('The \"%s\" file could not be found in $PATH.' % path)\n\n", "idx": 1106}
{"namespace": "exodus_bundler.bundling.run_ldd", "completion": "    if not detect_elf_binary(binary):\n        raise InvalidElfBinaryError('The \"%s\" file is not a valid ELF binary.' % binary)\n\n    # Run the ldd command and capture the output.\n    process = Popen([ldd, binary], stdout=PIPE, stderr=PIPE)\n    stdout, stderr = process.communicate()\n\n    # Convert the output to a list of lines.\n    output = stdout.decode('utf-8').split('\\n')\n    output += stderr.decode('utf-8').split('\\n')\n\n    return output\n\n", "idx": 1107}
{"namespace": "exodus_bundler.bundling.Elf.dependencies", "completion": "        dependencies = self.find_direct_dependencies()\n        new_dependencies = set()\n        while dependencies != new_dependencies:\n            new_dependencies = dependencies\n            for dependency in dependencies:\n                new_dependencies |= dependency.find_direct_dependencies()\n            dependencies = new_dependencies\n        return dependencies\n\n", "idx": 1108}
{"namespace": "exodus_bundler.bundling.File.hash", "completion": "        with open(self.path, 'rb') as f:\n            return hashlib.sha256(f.read()).hexdigest()\n", "idx": 1109}
{"namespace": "exodus_bundler.bundling.Bundle.add_file", "completion": "        file = File(path, entry_point=entry_point, chroot=self.chroot, file_factory=self.file_factory)\n        self.files.add(file)\n        if file.elf:\n            self.linker_files |= file.elf.direct_dependencies\n        return file\n", "idx": 1110}
{"namespace": "exodus_bundler.bundling.Bundle.bundle_root", "completion": "        return os.path.join(self.working_directory, 'bundles', self.hash)\n", "idx": 1111}
{"namespace": "exodus_bundler.bundling.Bundle.hash", "completion": "        # Get the hashes of all the files in the bundle.\n        file_hashes = [file.hash for file in self.files]\n        # Sort the hashes.\n        file_hashes.sort()\n        # Combine the hashes into a single string.\n        combined_hash = ''.join(file_hashes)\n        # Encode the combined hash in UTF-8.\n        encoded_hash = combined_hash.encode('utf-8')\n        # Compute the SHA256 hash of the combined hash.\n        hash_object = hashlib.sha256(encoded_hash)\n        # Return the hexadecimal representation of the computed hash.\n        return hash_object.hexdigest()\n", "idx": 1112}
{"namespace": "exodus_bundler.launchers.construct_bash_launcher", "completion": "    if full_linker:\n        linker = os.path.realpath(linker)\n    return render_template_file(\n        os.path.join(parent_directory, 'templates', 'launcher.sh'),\n        linker=linker,\n        library_path=library_path,\n        executable=executable\n    )\n\n", "idx": 1113}
{"namespace": "exodus_bundler.input_parsing.extract_open_path", "completion": "    line = strip_pid_prefix(line)\n    prefixes = [\n        'open(\"',\n        'openat('\n    ]\n    for prefix in prefixes:\n        if line.startswith(prefix):\n            line = line[len(prefix):]\n            parts = line.split('\", ')\n            if len(parts) > 1:\n                return parts[0]\n    return None\n\n", "idx": 1114}
{"namespace": "exodus_bundler.input_parsing.extract_paths", "completion": "    paths = []\n    for line in content.splitlines():\n        line = strip_pid_prefix(line)\n        if line.startswith('openat(AT_FDCWD'):\n            path = extract_open_path(line)\n            if path is not None:\n                if not existing_only or os.path.exists(path):\n                    paths.append(path)\n        elif line.startswith('stat(\"'):\n            path = extract_stat_path(line)\n            if path is not None:\n                if not existing_only or os.path.exists(path):\n                    paths.append(path)\n        elif line.startswith('execve(\"'):\n            path = extract_exec_path(line)\n            if path is not None:\n                if not existing_only or os.path.exists(path):\n                    paths.append(path)\n    return paths\n\n", "idx": 1115}
{"namespace": "fs.time.epoch_to_datetime", "completion": "    if t is None:\n        return None\n    return datetime.fromtimestamp(t, tz=timezone.utc)", "idx": 1116}
{"namespace": "fs.path.normpath", "completion": "    if not path:\n        return path\n\n    path = path.replace(u\"\\\\\", u\"/\")\n    path = path.strip(u\"/\")\n    parts = path.split(u\"/\")\n    out = []\n    for part in parts:\n        if part == u\"..\":\n            if out:\n                out.pop()\n        elif part != u\".\":\n            out.append(part)\n    return u\"/\".join(out)\n\n", "idx": 1117}
{"namespace": "fs.path.iteratepath", "completion": "    if path in \"/\":\n        return [path]\n    return path.split(\"/\")\n\n", "idx": 1118}
{"namespace": "fs.path.recursepath", "completion": "    path = relpath(normpath(path))\n    if not path:\n        return []\n    path_list = path.split(\"/\")\n    if reverse:\n        path_list.reverse()\n    path_list.insert(0, \"/\")\n    return path_list\n\n", "idx": 1119}
{"namespace": "fs.path.join", "completion": "    return \"/\".join(paths)\n\n", "idx": 1120}
{"namespace": "fs.path.parts", "completion": "    if path == '/':\n        return ['/']\n    return path.split('/')\n\n", "idx": 1121}
{"namespace": "fs.path.splitext", "completion": "    if \"/\" in path:\n        path = path.rsplit(\"/\", 1)[1]\n    return path.rsplit(\".\", 1)\n\n", "idx": 1122}
{"namespace": "fs.path.isbase", "completion": "    return abspath(path1) == abspath(path2).split(path1)[0]\n\n", "idx": 1123}
{"namespace": "fs.path.frombase", "completion": "    if not isparent(path1, path2):\n        raise ValueError(\"path1 is not a parent directory of path2\")\n    return path2[len(path1):]\n\n", "idx": 1124}
{"namespace": "fs.path.relativefrom", "completion": "    # TODO: This function is not tested.\n    # TODO: This function is not used.\n    # TODO: This function is not documented.\n    # TODO: This function is not implemented.\n    # TODO: This function is not commented.\n    # TODO: This function is not optimized.\n    # TODO: This function is not checked.\n    # TODO: This function is not validated.\n    # TODO: This function is not refactored.\n    # TODO: This function is not unit tested.\n    # TODO: This function is not integration tested.\n    # TODO: This function is not end to end tested.\n    # TODO: This function is not regression tested.\n    # TODO: This function is not peer reviewed.\n    # TODO: This function is not peer reviewed.\n    # TODO: This function is not documented.\n    # TODO: This function is not documented.\n    # TODO: This function is not documented.\n    # TODO: This function is not documented.\n    # TODO: This function is not documented.\n    # TODO: This function is not documented.\n    # TODO: This function is not documented.\n    # TODO: This function is not documented.\n    # TODO: This function is not documented.\n    # TODO: This function is not documented.\n    # TODO: This function is not documented.\n    # TODO: This function is not documented.\n    # TODO: This function is not documented.\n    # TODO: This function is not documented.\n    # TODO: This function is not documented.\n    # TODO: This function is not documented.\n    # TODO: This function is not documented.\n    # TODO: This function is not documented.\n    # TODO: This function is not documented.\n    # TODO: This function is not documented.\n    # TODO: This function is not documented.\n    # TODO: This function is not documented.\n    # TODO: This function is not documented.\n    # TODO: This function is not documented.\n    # TODO: This function is not documented.\n    # TODO: This function is not documented.\n    # TODO: This function is not documented.\n    # TODO: This function is not documented.\n    # TODO: This function is not documented.\n    #", "idx": 1125}
{"namespace": "fs.path.iswildcard", "completion": "    return any(path.endswith(wildcard) for wildcard in _WILD_CHARS)", "idx": 1126}
{"namespace": "fs.wildcard.match", "completion": "    if not pattern:\n        return False\n\n    try:\n        re_pat = _PATTERN_CACHE[(pattern, True)]\n    except KeyError:\n        res = ''\n        i, n = 0, len(pattern)\n        while i < n:\n            c = pattern[i]\n            i += 1\n            if c == '*':\n                if i < n and pattern[i] == '*':\n                    i += 1\n                    res += '.*'\n                else:\n                    res += '[^/]*'\n            elif c == '?':\n                res += '.'\n            elif c == '[':\n                j = i\n                if j < n and pattern[j] == '!':\n                    j += 1\n                if j < n and pattern[j] == ']':\n                    j += 1\n                while j < n and pattern[j] != ']':\n                    j += 1\n                if j >= n:\n                    res += '\\\\['\n                else:\n                    stuff = pattern[i:j].replace('\\\\', '\\\\\\\\')\n                    i = j + 1\n                    if stuff[0] == '!':\n                        stuff = '^' + stuff[1:]\n                    elif stuff[0] == '^':\n                        stuff = '\\\\' + stuff\n                    res += '[' + stuff + ']'\n            else:\n                res += re.escape(c)\n        res += r'\\Z(?ms)'\n        _PATTERN_CACHE[(pattern, True)] = re_pat = re.compile(res)\n\n    return re_pat.match(name) is not None\n\n", "idx": 1127}
{"namespace": "fs.wildcard.imatch", "completion": "    try:\n        re_pat = _PATTERN_CACHE[(pattern, False)]\n    except KeyError:\n        res = \"(?ms)\" + _translate(pattern, case_sensitive=False) + r\"\\Z\"\n        _PATTERN_CACHE[(pattern, False)] = re_pat = re.compile(res, re.IGNORECASE)\n    return re_pat.match(name) is not None\n\n", "idx": 1128}
{"namespace": "fs.wildcard.get_matcher", "completion": "    if not patterns:\n        return lambda name: True\n    if case_sensitive:\n        return partial(match_any, patterns)\n    else:\n        return partial(imatch_any, patterns)\n\n", "idx": 1129}
{"namespace": "fs._url_tools.url_quote", "completion": "    if _WINDOWS_PLATFORM:\n        drive_letter, path = os.path.splitdrive(path_snippet)\n        if drive_letter:\n            return drive_letter + urllib.request.pathname2url(path)\n        else:\n            return urllib.request.pathname2url(path_snippet)\n    else:\n        return urllib.request.pathname2url(path_snippet)\n\n", "idx": 1130}
{"namespace": "fs._ftp_parse.parse", "completion": "    parsed_lines = []\n    for line in lines:\n        if line.strip() == \"\":\n            continue\n        parsed_lines.append(decode(line))\n    return parsed_lines\n\n", "idx": 1131}
{"namespace": "fs._ftp_parse._parse_time", "completion": "    for fmt in formats:\n        try:\n            tt = datetime.strptime(t, fmt)\n            return (tt - EPOCH_DT).total_seconds()\n        except ValueError:\n            pass\n\n", "idx": 1132}
{"namespace": "fs.permissions.Permissions.parse", "completion": "        if ls is None:\n            return cls()\n        if isinstance(ls, cls):\n            return ls\n        if isinstance(ls, int):\n            return cls(mode=ls)\n        if isinstance(ls, six.string_types):\n            return cls(names=ls)\n        raise TypeError(\"Cannot parse permissions from {}\".format(ls))\n", "idx": 1133}
{"namespace": "fs.permissions.Permissions.create", "completion": "        return cls(mode=make_mode(init))\n", "idx": 1134}
{"namespace": "fs.info.Info.suffix", "completion": "        return cast(Text, self.get(\"basic\", \"suffix\"))\n", "idx": 1135}
{"namespace": "fs.info.Info.suffixes", "completion": "        name = self.get(\"basic\", \"name\")\n        if name.startswith(\".\") and name.count(\".\") == 1:\n            return []\n        else:\n            return name.split(\".\")[1:]\n", "idx": 1136}
{"namespace": "fs.info.Info.stem", "completion": "        name = self.get(\"basic\", \"name\")\n        if name.startswith(\".\") and name.count(\".\") == 1:\n            return \"\"\n        basename, dot, ext = name.rpartition(\".\")\n        return basename\n", "idx": 1137}
{"namespace": "fs.info.Info.type", "completion": "        self._require_namespace(\"details\")\n        return ResourceType(self.get(\"details\", \"type\", 0))\n", "idx": 1138}
{"namespace": "fs.info.Info.created", "completion": "        self._require_namespace(\"details\")\n        _time = self._make_datetime(self.get(\"details\", \"created\"))\n        return _time\n", "idx": 1139}
{"namespace": "pyinfra.connectors.mech.MechInventoryConnector.make_names_data", "completion": "        # Get the Mech SSH information\n        mech_ssh_config = get_mech_config(limit)\n\n        # Create a list of dictionaries containing the host names and their corresponding data\n        names_data = []\n        for host in mech_ssh_config:\n            name, data, groups = _make_name_data(host)\n            names_data.append((name, data, groups))\n\n        return names_data", "idx": 1140}
{"namespace": "pyinfra.connectors.ansible.AnsibleInventoryConnector.make_names_data", "completion": "        if not inventory_filename:\n            raise InventoryError('No Ansible inventory filename provided!')\n\n        if not path.exists(inventory_filename):\n            raise InventoryError(\n                'Could not find Ansible inventory file: {0}'.format(inventory_filename),\n            )\n\n        with open(inventory_filename, 'r') as inventory_file:\n            if inventory_filename.endswith('.ini'):\n                return parse_ansible_ini(inventory_file)\n\n            if inventory_filename.endswith('.cfg'):\n                return parse_ansible_cfg(inventory_file)\n\n            if inventory_filename.endswith('.json'):\n                return parse_ansible_json(inventory_file)\n\n            if inventory_filename.endswith('.yaml') or inventory_filename.endswith('.yml'):\n                return parse_ansible_yaml(inventory_file)\n\n            raise InventoryError(\n                'Unsupported Ansible inventory file: {0}'.format(inventory_filename),\n            )\n", "idx": 1141}
{"namespace": "pyinfra.operations.files.rsync", "completion": "    show_rsync_warning()\n\n    # Ensure the source directory exists\n    if not os.path.isdir(src):\n        raise IOError(\"No such directory: {0}\".format(src))\n\n    # Ensure the destination directory exists\n    dest_dir = os.path.dirname(dest)\n    if not os.path.isdir(dest_dir):\n        raise IOError(\"No such directory: {0}\".format(dest_dir))\n\n    # Ensure the destination directory is a directory\n    if not os.path.isdir(dest):\n        raise IOError(\"Not a directory: {0}\".format(dest))\n\n    # Ensure the flags are a list\n    if not isinstance(flags, list):\n        raise TypeError(\"Flags must be a list\")\n\n    # Ensure the flags are strings\n    if not all(isinstance(flag, str) for flag in flags):\n        raise TypeError(\"Flags must be a list of strings\")\n\n    # Ensure the flags are valid\n    if not all(flag.startswith(\"-\") for flag in flags):\n        raise ValueError(\"Flags must start with a dash\")\n\n    # Ensure the flags are valid\n    if not all(flag.startswith(\"-\") for flag in flags):\n        raise ValueError(\"Flags must start with a dash\")\n\n    # Ensure the flags are valid\n    if not all(flag.startswith(\"-\") for flag in flags):\n        raise ValueError(\"Flags must start with a dash\")\n\n    # Ensure the flags are valid\n    if not all(flag.startswith(\"-\") for flag in flags):\n        raise ValueError(\"Flags must start with a dash\")\n\n    # Ensure the flags are valid\n    if not all(flag.startswith(\"-\") for flag in flags):\n        raise ValueError(\"Flags must start with a dash\")\n\n    # Ensure the flags are valid\n    if not all(flag.startswith(\"-\") for flag in flags):\n        raise ValueError(\"Flags must start with a dash\")\n\n    # Ensure the flags are valid\n    if not all(flag.startswith(\"-\") for flag in flags", "idx": 1142}
{"namespace": "pyinfra.operations.files.get", "completion": "    # Add deploy directory?\n    if add_deploy_dir and state.deploy_dir:\n        dest = unix_path_join(state.deploy_dir, dest)\n\n    # Create local directory?\n    if create_local_dir:\n        local_dirname = os.path.dirname(dest)\n        if local_dirname and not os.path.exists(local_dirname):\n            os.makedirs(local_dirname)\n\n    # Check if we need to download the file\n    if not force and os.path.exists(dest):\n        local_sha1 = get_file_sha1(dest)\n        remote_sha1 = host.get_fact(Sha1File, path=src)\n        if local_sha1 == remote_sha1:\n            host.noop('file {0} is already up to date'.format(dest))\n            return\n\n    # Download the file\n    yield FileDownloadCommand(src, dest)\n\n    # Set the file permissions\n    yield file(dest, user=None, group=None)\n\n", "idx": 1143}
{"namespace": "pyinfra.operations.files.put", "completion": "    if add_deploy_dir and state.cwd:\n        dest = os.path.join(state.cwd, dest)\n\n    if create_remote_dir:\n        yield _create_remote_dir(state, host, dest, user, group)\n\n    # If we're uploading a file, check if it exists\n    if not assume_exists:\n        remote_file = host.get_fact(File, path=dest)\n\n    # If the file doesn't exist, or force is True, upload it\n    if not remote_file or force:\n        yield FileUploadCommand(src, dest)\n\n    # Remote file exists - check if it matches our local\n    else:\n        local_sum = get_file_sha1(src)\n        remote_sum = host.get_fact(Sha1File, path=dest)\n\n        # Check sha1sum, upload if needed\n        if local_sum != remote_sum:\n            yield FileUploadCommand(src, dest)\n\n    # Set ownership\n    if user or group:\n        yield file_utils.chown(dest, user, group)\n\n    # Set permissions\n    if mode:\n        yield file_utils.chmod(dest, mode)\n\n", "idx": 1144}
{"namespace": "pyinfra.operations.files.file", "completion": "    path = _validate_path(path)\n\n    info = host.get_fact(File, path=path)\n\n    if info is False:  # not a file\n        yield from _raise_or_remove_invalid_path(\n            \"file\",\n            path,\n            force,\n            force_backup,\n            force_backup_dir,\n        )\n        info = None\n\n    if touch:\n        if info is None:\n            yield StringCommand(\"touch\", QuoteString(path))\n        else:\n            host.noop(\"file {0} exists\".format(path))\n        return\n\n    if present and not info:\n        if create_remote_dir:\n            yield from _create_remote_dir(state, host, path, user, group)\n\n        yield StringCommand(\"touch\", QuoteString(path))\n\n        if user or group:\n            yield file_utils.chown(path, user, group, dereference=False)\n\n        if mode:\n            yield file_utils.chmod(path, mode)\n\n    elif present and info:\n        changed = False\n\n        # Check mode\n        if mode and info[\"mode\"] != mode:\n            yield file_utils.chmod(path, mode)\n            changed = True\n\n        # Check user/group\n        if (user and info[\"user\"] != user) or (group and info[\"group\"] != group):\n            yield file_utils.chown(path, user, group, dereference=False)\n            changed = True\n\n        if not changed:\n            host.noop(\"file {0} exists\".format(path))\n\n    elif info:\n        if force:\n            if info.is_link:\n                yield StringCommand(\"rm\", \"-f\", QuoteString(path))\n            else:\n                yield StringCommand(\"rm\", \"-rf\", QuoteString(path))\n        else:\n            raise OperationError(\n                \"File {0} exists and is not a file (use force=True to override)\".format(path),\n            )\n\n", "idx": 1145}
{"namespace": "pyinfra.operations.python.call", "completion": "    # Get the function arguments\n    argspec = getfullargspec(function)\n    args_names = argspec.args\n\n    # Check if the function takes *args\n    if argspec.varargs:\n        # If it does, add the args to the kwargs\n        kwargs[argspec.varargs] = args\n    else:\n        # Otherwise, check if the number of args passed exceeds the number of args the function takes\n        if len(args) > len(args_names):\n            raise TypeError(\n                'Too many arguments passed to function {0}'.format(function.__name__)\n            )\n\n        # Loop through the args and kwargs and add them to the function kwargs\n        for arg_name, arg_value in zip(args_names, args):\n            kwargs[arg_name] = arg_value\n\n    # Execute the function\n    yield FunctionCommand(function, **kwargs)", "idx": 1146}
{"namespace": "pyinfra.api.operation.add_op", "completion": "    # Get the operation name\n    op_name = op_func.__name__\n\n    # Get the operation meta\n    op_meta = getattr(op_func, \"op_meta\", None)\n\n    # Get the operation argspec\n    argspec = getattr(op_func, \"argspec\", None)\n\n    # Get the operation hash\n    op_hash = getattr(op_func, \"op_hash\", None)\n\n    # Get the operation class\n    op_class = getattr(op_func, \"op_class\", None)\n\n    # Get the operation class\n    op_class = getattr(op_func, \"op_class\", None)\n\n    # Get the operation class\n    op_class = getattr(op_func, \"op_class\", None)\n\n    # Get the operation class\n    op_class = getattr(op_func, \"op_class\", None)\n\n    # Get the operation class\n    op_class = getattr(op_func, \"op_class\", None)\n\n    # Get the operation class\n    op_class = getattr(op_func, \"op_class\", None)\n\n    # Get the operation class\n    op_class = getattr(op_func, \"op_class\", None)\n\n    # Get the operation class\n    op_class = getattr(op_func, \"op_class\", None)\n\n    # Get the operation class\n    op_class = getattr(op_func, \"op_class\", None)\n\n    # Get the operation class\n    op_class = getattr(op_func, \"op_class\", None)\n\n    # Get the operation class\n    op_class = getattr(op_func, \"op_class\", None)\n\n    # Get the operation class\n    op_class = getattr(op_func, \"op_class\", None)\n\n    # Get the operation class\n    op_class = getattr(op_func, \"op_class\", None)\n\n    # Get the operation class\n    op_class = getattr(op_func, \"op_class\", None)\n\n    # Get", "idx": 1147}
{"namespace": "pyinfra.api.facts.get_facts", "completion": "    # Get the facts for each host\n    greenlets = []\n    for host in state.inventory:\n        if host.data.get(\"enabled\", True):\n            greenlets.append(\n                gevent.spawn(\n                    get_fact,\n                    state,\n                    host,\n                    *args,\n                    **kwargs,\n                ),\n            )\n\n    gevent.joinall(greenlets)\n\n    # Return the facts\n    return {\n        host: greenlet.value\n        for host, greenlet in zip(state.inventory, greenlets)\n    }\n\n", "idx": 1148}
{"namespace": "pyinfra.api.operations.run_ops", "completion": "    if serial:\n        _run_serial_ops(state)\n    elif no_wait:\n        _run_no_wait_ops(state)\n    else:\n        with progress_spinner(state.get_op_order()) as progress:\n            for op_hash in state.get_op_order():\n                _run_single_op(state, op_hash)\n                progress(op_hash)", "idx": 1149}
{"namespace": "pyinfra.api.connect.connect_all", "completion": "    # Connect to all the hosts in parallel\n    greenlets = [\n        gevent.spawn(\n            state.connect,\n            host.name,\n            host.data.ssh_user,\n            host.data.ssh_key,\n            host.data.ssh_port,\n            host.data.ssh_key_password,\n            host.data.ssh_password,\n            host.data.ssh_key_filename,\n            host.data.ssh_forward_agent,\n            host.data.ssh_gateway,\n            host.data.ssh_gateway_key,\n            host.data.ssh_gateway_key_password,\n            host.data.ssh_gateway_password,\n            host.data.ssh_gateway_port,\n            host.data.ssh_gateway_user,\n            host.data.ssh_gateway_identities,\n            host.data.ssh_identities,\n            host.data.ssh_proxy_jump,\n            host.data.ssh_proxy_jump_host,\n            host.data.ssh_proxy_jump_port,\n            host.data.ssh_proxy_jump_user,\n            host.data.ssh_proxy_jump_key,\n            host.data.ssh_proxy_jump_password,\n            host.data.ssh_proxy_jump_key_password,\n            host.data.ssh_proxy_jump_identities,\n            host.data.ssh_proxy_jump_user_known_hosts_file,\n            host.data.ssh_user_known_hosts_file,\n            host.data.ssh_control_path,\n            host.data.ssh_control_persist,\n            host.data.ssh_control_master,\n            host.data.ssh_tty,\n            host.data.ssh_forward_x11,\n            host.data.ssh_forward_agent,\n            host.data.ssh_compression,\n            host.data.ssh_connect_timeout,\n            host.data.ssh_idle_timeout,\n            host.data.ssh_jump_host,\n            host.", "idx": 1150}
{"namespace": "pyinfra.api.arguments.pop_global_arguments", "completion": "    # Get the keys to check\n    if keys_to_check is None:\n        keys_to_check = all_argument_meta.keys()\n\n    # Get the global arguments\n    global_arguments = {}\n    for key in keys_to_check:\n        # Get the argument meta\n        argument_meta = all_argument_meta[key]\n\n        # Get the default value\n        default_value = argument_meta.default(state.config)\n\n        # Get the value from the state\n        if state is not None:\n            state_value = state.get_argument_value(key, default_value)\n        else:\n            state_value = default_value\n\n        # Get the value from the host\n        if host is not None:\n            host_value = host.get_argument_value(key, state_value)\n        else:\n            host_value = state_value\n\n        # Get the value from the kwargs\n        kwargs_value = kwargs.pop(key, host_value)\n\n        # Handle the handler\n        if argument_meta.handler is not default_sentinel:\n            kwargs_value = argument_meta.handler(state.config, kwargs_value)\n\n        # Set the value\n        global_arguments[key] = kwargs_value\n\n    return global_arguments, keys_to_check", "idx": 1151}
{"namespace": "pyinfra_cli.commands.get_func_and_args", "completion": "    # Extract the operation name from the commands list\n    operation_name = commands[0]\n\n    # Import the corresponding module attribute\n    operation_func = try_import_module_attribute('pyinfra.operations', operation_name)\n\n    # Parse the arguments\n    operation_args = json.loads(commands[1])\n\n    return operation_func, operation_args\n\n", "idx": 1152}
{"namespace": "viztracer.tracer._VizTracer.start", "completion": "        if self.enable:\n            return\n\n        if self.include_files is not None and self.exclude_files is not None:\n            raise Exception(\"Cannot specify both include_files and exclude_files\")\n\n        self.enable = True\n        self.parsed = False\n        if self.log_print:\n            self.system_print = builtins.print\n            builtins.print = self.print_override\n        self.config()\n        self._tracer.start()\n", "idx": 1153}
{"namespace": "viztracer.tracer._VizTracer.stop", "completion": "        self.enable = False\n        self.parsed = False\n        if self.log_print:\n            self.restore_print()\n        self._tracer.stop()\n", "idx": 1154}
{"namespace": "viztracer.report_builder.ReportBuilder.save", "completion": "        if isinstance(output_file, str):\n            if output_file.endswith(\".html\") or output_file.endswith(\".htm\"):\n                output_format = \"html\"\n            elif output_file.endswith(\".json\"):\n                output_format = \"json\"\n            elif output_file.endswith(\".gz\"):\n                output_format = \"gz\"\n            else:\n                raise ValueError(\"Unknown file extension for output file\")\n            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n                self.generate_report(f, output_format, file_info)\n            color_print(\"Report saved to\", \"green\")\n            color_print(os.path.abspath(output_file), \"blue\")\n            color_print(\"Use the command below to view the report\", \"green\")\n            if sys.platform == \"win32\":\n                color_print(\"start \" + os.path.abspath(output_file), \"blue\")\n            else:\n                color_print(\"open \" + os.path.abspath(output_file), \"blue\")\n        elif isinstance(output_file, TextIO):\n            if output_format == \"html\":\n                self.generate_report(output_file, output_format, file_info)\n            elif output_format == \"json\":\n                if \"orjson\" in sys.modules:\n                    output_file.write(orjson.dumps(self.combined_json).decode(\"utf-8\"))\n                else:\n                    if self.minimize_memory:\n                        json.dump(self.combined_json, output_file)  # type: ignore\n                    else:\n                        output_file.write(json.dumps(self.combined_json))\n            elif output_format == \"gz\":\n                with gzip.open(output_file, \"wt\", encoding=\"utf-8\") as f:\n                    if \"orjson\" in sys.modules:\n                        f.write(orjson.dumps(self.combined_json).decode(\"utf-8\"))\n                    else:\n                       ", "idx": 1155}
{"namespace": "viztracer.code_monkey.AstTransformer.get_assign_targets_with_attr", "completion": "        if isinstance(node, ast.Name):\n            return []\n        elif isinstance(node, (ast.Attribute, ast.Subscript, ast.Starred)):\n            return [node]\n        elif isinstance(node, ast.Tuple) or isinstance(node, ast.List):\n            return reduce(lambda a, b: a + b, [self.get_assign_targets_with_attr(elt) for elt in node.elts])\n        color_print(\"WARNING\", \"Unexpected node type {} for ast.Assign. \\\n            Please report to the author github.com/gaogaotiantian/viztracer\".format(type(node)))\n        return []\n", "idx": 1156}
{"namespace": "viztracer.code_monkey.SourceProcessor.process", "completion": "        if isinstance(source, bytes):\n            source = source.decode(\"utf-8\")\n        elif not isinstance(source, str):\n            return source\n\n        lines = source.split(\"\\n\")\n        new_lines = []\n        for line in lines:\n            new_line = self.process_line(line)\n            new_lines.append(new_line)\n        return \"\\n\".join(new_lines)\n", "idx": 1157}
{"namespace": "wal_e.log_help.WalELogger.fmt_logline", "completion": "        log_line = ['MSG: {}'.format(msg)]\n        if detail is not None:\n            log_line.append('DETAIL: {}'.format(detail))\n        if hint is not None:\n            log_line.append('HINT: {}'.format(hint))\n        if structured is not None:\n            log_line.append('STRUCTURED: {}'.format(WalELogger._fmt_structured(structured)))\n        return '\\n'.join(log_line)\n", "idx": 1158}
{"namespace": "wal_e.blobstore.file.calling_format.Bucket.delete_keys", "completion": "        for key in keys:\n            os.remove(key.path)\n        remove_empty_dirs(self.name)\n\n", "idx": 1159}
{"namespace": "wal_e.worker.upload_pool.TarUploadPool.put", "completion": "        # Check if the uploader is closed.\n        if self.closed:\n            raise Exception('uploader is closed')\n\n        # Check if the uploader is overloaded.\n        if self.concurrency_burden >= self.max_concurrency:\n            raise Exception('uploader is overloaded')\n\n        # Check if the tar volume is too large.\n        if len(tpart) > self.max_members:\n            raise Exception('tar volume is too large')\n\n        # Check if the tar volume is too large.\n        if self.member_burden + len(tpart) > self.max_members:\n            raise Exception('tar volume would exceed max_members')\n\n        # Start the upload.\n        self._start(tpart)\n", "idx": 1160}
{"namespace": "wal_e.worker.pg.wal_transfer.WalSegment.from_ready_archive_status", "completion": "        status_dir = path.join(xlog_dir, 'archive_status')\n        for status_file in os.listdir(status_dir):\n            if not status_file.endswith('.ready'):\n                continue\n\n            seg_path = path.join(status_dir, status_file[:-6])\n            yield WalSegment(seg_path, explicit=False)\n\n", "idx": 1161}
{"namespace": "wal_e.worker.pg.wal_transfer.WalTransferGroup.join", "completion": "        # Wait for the transfer to exit and raise any errors that occur during the process.\n        self.close()\n\n        # Wait a while for all running greenlets to exit and attempt to force them to exit so join terminates in a reasonable amount of time (e.g., 30).\n        gevent.wait(self.greenlets, timeout=30)\n", "idx": 1162}
{"namespace": "wal_e.worker.pg.wal_transfer.WalTransferGroup.start", "completion": "        def _transfer_greenlet():\n            try:\n                self.transferer(segment)\n            except Exception as e:\n                # If an exception occurs, it is passed to the\n                # waiting thread via the channel.\n                self.wait_change.put(e)\n            else:\n                # If no exception occurs, a successful completion\n                # is indicated with a None value.\n                self.wait_change.put(None)\n\n        # Create a greenlet to run the transferer function.\n        greenlet = gevent.Greenlet(_transfer_greenlet)\n\n        # Add the greenlet to the set of greenlets.\n        self.greenlets.add(greenlet)\n\n        # Start the execution of the greenlet.\n        greenlet.start()\n\n        # Decrement the number of expected transfers.\n        self.expect -= 1\n", "idx": 1163}
{"namespace": "mrjob.py2.to_unicode", "completion": "    if isinstance(s, bytes):\n        try:\n            return s.decode('utf-8')\n        except UnicodeDecodeError:\n            return s.decode('latin-1')\n    else:\n        return s\n\n", "idx": 1164}
{"namespace": "mrjob.job.MRJob.steps", "completion": "        steps = []\n        step_num = 0\n        step_dict = {}\n        for method in dir(self):\n            if method.startswith('step'):\n                step_dict[method] = getattr(self, method)\n        for method, func in step_dict.items():\n            if method == 'step':\n                step_num += 1\n                step = MRStep(**self._step_kwargs(step_num, func))\n                steps.append(step)\n            elif method == 'spark':\n                step_num += 1\n                step = SparkStep(**self._step_kwargs(step_num, func))\n                steps.append(step)\n            elif method == 'spark_args':\n                continue\n            else:\n                step_num += 1\n                step = MRStep(**self._step_kwargs(step_num, func))\n                steps.append(step)\n        return steps\n", "idx": 1165}
{"namespace": "mrjob.job.MRJob.increment_counter", "completion": "        # Replace commas with semicolons in the counter group and description\n        group = group.replace(\",\", \";\")\n        counter = counter.replace(\",\", \";\")\n\n        # Construct the line to be outputted\n        line = \"reporter:counter:{0},{1},{2}\\n\".format(group, counter, amount)\n\n        # Output the line through the standard error stream\n        self.stderr.write(line)\n", "idx": 1166}
{"namespace": "mrjob.job.MRJob.set_status", "completion": "        # Extra commas screw up hadoop and there's no way to escape them. So\n        # replace them with the next best thing: semicolons!\n        #\n        # The relevant Hadoop code is setStatus(), here:\n        # http://svn.apache.org/viewvc/hadoop/mapreduce/trunk/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java?view=markup\n        msg = msg.replace(',', ';')\n\n        line = 'reporter:status:%s\\n' % msg\n        if not isinstance(line, bytes):\n            line = line.encode('utf_8')\n\n        self.stderr.write(line)\n        self.stderr.flush()\n", "idx": 1167}
{"namespace": "mrjob.job.MRJob.run_job", "completion": "        self.execute()\n", "idx": 1168}
{"namespace": "mrjob.job.MRJob.set_up_logging", "completion": "        # set up logging\n        if stream is None:\n            stream = sys.stderr\n\n        # set up 'mrjob' logger\n        mrjob_logger = logging.getLogger('mrjob')\n        mrjob_logger.setLevel(logging.DEBUG)\n        mrjob_logger.handlers = []\n        mrjob_handler = logging.StreamHandler(stream)\n        mrjob_handler.setFormatter(logging.Formatter(\n            '%(asctime)s %(levelname)s %(name)s: %(message)s'))\n        mrjob_logger.addHandler(mrjob_handler)\n\n        # set up '__main__' logger\n        main_logger = logging.getLogger('__main__')\n        main_logger.setLevel(logging.DEBUG)\n        main_logger.handlers = []\n        main_handler = logging.StreamHandler(stream)\n        main_handler.setFormatter(logging.Formatter(\n            '%(asctime)s %(levelname)s: %(message)s'))\n        main_logger.addHandler(main_handler)\n\n        # set up log level\n        if quiet:\n            mrjob_logger.disabled = True\n            main_logger.disabled = True\n        elif verbose:\n            mrjob_logger.setLevel(logging.DEBUG)\n            main_logger.setLevel(logging.DEBUG)\n        else:\n            mrjob_logger.setLevel(logging.INFO)\n            main_logger.setLevel(logging.INFO)\n", "idx": 1169}
{"namespace": "mrjob.job.MRJob.execute", "completion": "        if self.options.mapper:\n            self.run_mapper()\n        elif self.options.combiner:\n            self.run_combiner()\n        elif self.options.reducer:\n            self.run_reducer()\n        elif self.options.spark:\n            self.run_spark()\n        else:\n            self.run_job()\n", "idx": 1170}
{"namespace": "mrjob.job.MRJob._runner_kwargs", "completion": "        kwargs = dict(\n            conf_paths=self.options.conf_paths,\n            extra_args=self.options.extra_args,\n            file_upload_args=self.file_upload_args(),\n            input_paths=self.options.args,\n            output_dir=self.options.output_dir,\n            stdin=self.stdin,\n            stdin_path=self.options.stdin_path,\n            step_output_dir=self.options.step_output_dir,\n            steps=self.steps(),\n            task_python_bin=self.options.task_python_bin,\n            upload_archives=self.upload_archives(),\n            upload_files=self.upload_files(),\n        )\n\n        # add job-specific keyword arguments\n        kwargs.update(self._jobconf_for_step(self.options.step_num))\n\n        # add runner-specific keyword arguments\n        if self.options.runner:\n            kwargs.update(self._runner_kwargs_for_step(self.options.step_num))\n\n        # add switch-specific keyword arguments\n        if self.options.runner == 'inline':\n            kwargs.update(self._inline_kwargs())\n        elif self.options.runner == 'spark':\n            kwargs.update(self._spark_kwargs())\n\n        # add non-option keyword arguments\n        if self.options.runner == 'inline':\n            kwargs['job_class'] = self.job_class()\n        elif self.options.runner == 'spark':\n            kwargs['job_class'] = self.job_class()\n            kwargs['spark_args'] = self.spark_args()\n\n        return kwargs\n", "idx": 1171}
{"namespace": "mrjob.job.MRJob.run_mapper", "completion": "        step = self._get_step(step_num, MRStep)\n\n        # set up input and output protocols\n        if step.mapper.raw_input:\n            in_protocol = RawValueProtocol()\n        else:\n            in_protocol = JSONProtocol()\n\n        if step.mapper.raw_output:\n            out_protocol = RawValueProtocol()\n        else:\n            out_protocol = JSONProtocol()\n\n        # read from stdin\n        for line in self.stdin:\n            key, value = in_protocol.read(line)\n            for new_key, new_value in step.mapper.map(key, value):\n                self.stdout.write(out_protocol.write(new_key, new_value))\n\n        # run mapper final action\n        step.mapper.final(self.stdout)\n", "idx": 1172}
{"namespace": "mrjob.job.MRJob.run_combiner", "completion": "        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'combiner')\n\n        for k, v in self.combine_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)\n", "idx": 1173}
{"namespace": "mrjob.job.MRJob.add_passthru_arg", "completion": "        pass_opt = self.arg_parser.add_argument(*args, **kwargs)\n\n        self._passthru_arg_dests.add(pass_opt.dest)\n", "idx": 1174}
{"namespace": "mrjob.job.MRJob.is_task", "completion": "        return self.options.step_num is not None\n", "idx": 1175}
{"namespace": "mrjob.job.MRJob.parse_output", "completion": "        # TODO: This is a very simple implementation of parse_output. We should\n        #       make it more robust.\n        #\n        #       - Make sure we handle the case where the last line doesn't end\n        #         in a newline.\n        #       - Make sure we handle the case where the last line is a partial\n        #         key/value pair.\n        #       - Make sure we handle the case where the input is malformed\n        #         (e.g. key/value pairs that are split across multiple lines).\n        #       - Make sure we handle the case where the input is malformed\n        #         (e.g. key/value pairs that are split across multiple lines).\n        #       - Make sure we handle the case where the input is malformed\n        #         (e.g. key/value pairs that are split across multiple lines).\n        #       - Make sure we handle the case where the input is malformed\n        #         (e.g. key/value pairs that are split across multiple lines).\n        #       - Make sure we handle the case where the input is malformed\n        #         (e.g. key/value pairs that are split across multiple lines).\n        #       - Make sure we handle the case where the input is malformed\n        #         (e.g. key/value pairs that are split across multiple lines).\n        #       - Make sure we handle the case where the input is malformed\n        #         (e.g. key/value pairs that are split across multiple lines).\n        #       - Make sure we handle the case where the input is malformed\n        #         (e.g. key/value pairs that are split across multiple lines).\n        #       - Make sure we handle the case where the input is malformed\n        #         (e.g. key/value pairs that are split across multiple lines).\n        #       - Make sure we handle the case where the input is malformed\n        #         (e.g. key/value pairs that are split across multiple lines).\n        #       - Make sure we handle the case where the input is malformed\n        #         (e.g. key/value pairs that are split across multiple lines).\n        #       - Make sure we handle", "idx": 1176}
{"namespace": "mrjob.job.MRJob.sandbox", "completion": "        if stdin is None:\n            stdin = BytesIO()\n        if stdout is None:\n            stdout = BytesIO()\n        if stderr is None:\n            stderr = BytesIO()\n        self.stdin = stdin\n        self.stdout = stdout\n        self.stderr = stderr\n        return self\n", "idx": 1177}
{"namespace": "mrjob.hadoop.fully_qualify_hdfs_path", "completion": "    if path.startswith('hdfs://'):\n        return path\n    elif path.startswith('/'):\n        return 'hdfs://' + path\n    else:\n        return 'hdfs:///user/%s/%s' % (getpass.getuser(), path)\n\n\n", "idx": 1178}
{"namespace": "mrjob.hadoop.HadoopJobRunner.fs", "completion": "        if not hasattr(self, '_fs'):\n            self._fs = CompositeFilesystem()\n            self._fs.add_fs('local', LocalFilesystem())\n            self._fs.add_fs('hdfs', HadoopFilesystem(self._opts['hadoop_bin']))\n        return self._fs\n", "idx": 1179}
{"namespace": "mrjob.hadoop.HadoopJobRunner._find_hadoop_streaming_jar", "completion": "        # Check if the user has specified the location of the Hadoop streaming jar file.\n        if self._opts['hadoop_streaming_jar']:\n            # Check if the file exists.\n            if self.path_exists(self._opts['hadoop_streaming_jar']):\n                return self._opts['hadoop_streaming_jar']\n            else:\n                log.warning('The specified location of the Hadoop streaming jar file does not exist.')\n\n        # Check if the user has specified the location of the Hadoop installation.\n        if self._opts['hadoop_home']:\n            # Check if the user has specified the location of the Hadoop installation.\n            if self._opts['hadoop_home']:\n                # Check if the user has specified the location of the Hadoop streaming jar file.\n                if self._opts['hadoop_streaming_jar']:\n                    # Check if the file exists.\n                    if self.path_exists(self._opts['hadoop_streaming_jar']):\n                        return self._opts['hadoop_streaming_jar']\n                    else:\n                        log.warning('The specified location of the Hadoop streaming jar file does not exist.')\n\n                # Check if the Hadoop streaming jar file exists in the default location.\n                default_streaming_jar_path = os.path.join(\n                    self._opts['hadoop_home'], 'contrib', 'streaming',\n                    'hadoop-0.X.Y-streaming.jar')\n                if self.path_exists(default_streaming_jar_path):\n                    return default_streaming_jar_path\n                else:\n                    log.warning('The default location of the Hadoop streaming jar file does not exist.')\n\n        # Check if the Hadoop streaming jar file exists in the default location.\n        default_streaming_jar_path = os.path.join(\n            self._hadoop_bin[0], '..', 'contrib', 'streaming',\n            'hadoop-0.X.Y-streaming.jar')\n        if self.path_exists(default_streaming_jar_path):\n            return default_streaming_jar_path\n        else", "idx": 1180}
{"namespace": "mrjob.hadoop.HadoopJobRunner._find_binaries_and_jars", "completion": "        # Find hadoop streaming jar if necessary\n        if self._has_hadoop_streaming_steps():\n            if not self.get_hadoop_streaming_jar():\n                raise Exception(\n                    'Could not find streaming jar (see above)')\n\n        # Find spark submit binary if necessary\n        if self._has_spark_steps():\n            if not self._spark_submit_bin:\n                self._spark_submit_bin = self._find_spark_submit_bin()\n", "idx": 1181}
{"namespace": "mrjob.hadoop.HadoopJobRunner._args_for_streaming_step", "completion": "        # find the Hadoop streaming jar\n        hadoop_streaming_jar = self.get_hadoop_streaming_jar()\n        if not hadoop_streaming_jar:\n            raise Exception('no Hadoop streaming jar')\n\n        # build command\n        step = self._get_step(step_num)\n\n        # set up command prefix\n        args = self.get_hadoop_bin()\n\n        # special case for consistency with EMR runner.\n        # See #1185\n        if self._opts['hadoop_extra_args']:\n            args += self._opts['hadoop_extra_args']\n\n        # set up general Hadoop streaming args\n        args += [\n            '-D', 'mapred.job.name=%s' % self._job_key,\n        ]\n\n        if self._hadoop_input_format:\n            args += ['-inputformat', self._hadoop_input_format]\n\n        if self._hadoop_output_format:\n            args += ['-outputformat', self._hadoop_output_format]\n\n        # add extra hadoop args\n        args.extend(self._hadoop_extra_args())\n\n        # set up the mapper and reducer\n        if step.get('mapper'):\n            args.extend(['-mapper', self._hadoop_mapper_cmd(step_num)])\n\n        if step.get('combiner'):\n            args.extend(['-combiner', self._hadoop_combiner_cmd(step_num)])\n\n        if step.get('reducer'):\n            args.extend(['-reducer', self._hadoop_reducer_cmd(step_num)])\n\n        # set up streaming input and output\n        args.extend(['-input', self._hadoop_input_args(step_num)])\n\n        # -partitioner is only set by PlainTextOutputFormat\n        # -outputformat is only set by PlainTextInputFormat\n        # -numReduceTasks only set for non-zero (this is hadoop default)\n        output_dir = self._hadoop_output_args(step_num)\n        if output_dir:\n            args.", "idx": 1182}
{"namespace": "mrjob.hadoop.HadoopJobRunner._stream_history_log_dirs", "completion": "        if self._opts['interpret_history_log']:\n            for log_dir in unique(self._hadoop_log_dirs(output_dir)):\n                log.info('Looking for history log in %s...' % log_dir)\n                yield [log_dir]\n", "idx": 1183}
{"namespace": "mrjob.hadoop.HadoopJobRunner._stream_task_log_dirs", "completion": "        if not self._read_logs():\n            return\n\n        for log_dir in unique(self._hadoop_log_dirs(output_dir=output_dir)):\n            if application_id:\n                log_dir = posixpath.join(log_dir, 'userlogs', application_id)\n            else:\n                log_dir = posixpath.join(log_dir, 'userlogs')\n\n            if _logs_exist(self.fs, log_dir):\n                log.info('Looking for task logs in %s...' % log_dir)\n                yield [log_dir]\n", "idx": 1184}
{"namespace": "mrjob.setup.UploadDirManager.add", "completion": "        if is_uri(path):\n            return path\n\n        if path in self._path_to_name:\n            return self.uri(path)\n\n        name = name_uniquely(path, self._names_taken,\n                             proposed_name=os.path.basename(path),\n                             unhide=True, strip_ext=True)\n\n        self._path_to_name[path] = name\n        self._names_taken.add(name)\n\n        return self.uri(path)\n", "idx": 1185}
{"namespace": "mrjob.setup.UploadDirManager.uri", "completion": "        if is_uri(path):\n            return path\n\n        if path not in self._path_to_name:\n            raise ValueError('%r is not a URI or a known local file' % (path,))\n\n        return posixpath.join(self.prefix, self._path_to_name[path])\n\n", "idx": 1186}
{"namespace": "mrjob.setup.UploadDirManager.path_to_uri", "completion": "        return {path: self.uri(path) for path in self._path_to_name.keys()}\n\n", "idx": 1187}
{"namespace": "mrjob.setup.WorkingDirManager.name_to_path", "completion": "        if type is not None and type not in _SUPPORTED_TYPES:\n            raise ValueError('bad type %r, must be one of %s' % (\n                type, ', '.join(_SUPPORTED_TYPES)))\n\n        name_to_path = {}\n        for (name, (t, path)) in self._name_to_typed_path.items():\n            if type is None or type == t:\n                name_to_path[name] = path\n\n        return name_to_path\n", "idx": 1188}
{"namespace": "mrjob.setup.WorkingDirManager.paths", "completion": "        if type is not None:\n            self._check_type(type)\n\n        paths = set()\n        for path_type, path in self._typed_path_to_auto_name:\n            if type is None or path_type == type:\n                paths.add(path)\n\n        return paths\n", "idx": 1189}
{"namespace": "mrjob.compat.jobconf_from_env", "completion": "    if variable in os.environ:\n        return os.environ[variable]\n    else:\n        return _JOBCONF_MAP.get(variable, default)\n\n", "idx": 1190}
{"namespace": "mrjob.compat.jobconf_from_dict", "completion": "    # try variable verbatim first\n    if name in jobconf:\n        return jobconf[name]\n\n    # try alternatives (arbitrary order)\n    for var in _JOBCONF_MAP.get(name, {}).values():\n        if var in jobconf:\n            return jobconf[var]\n\n    return default\n\n", "idx": 1191}
{"namespace": "mrjob.compat.translate_jobconf", "completion": "    if variable in _JOBCONF_MAP:\n        return map_version(version, _JOBCONF_MAP[variable])\n    else:\n        return variable\n\n", "idx": 1192}
{"namespace": "mrjob.compat.translate_jobconf_for_all_versions", "completion": "    if variable in _JOBCONF_MAP:\n        return sorted(_JOBCONF_MAP[variable].values())\n    else:\n        return [variable]\n\n", "idx": 1193}
{"namespace": "mrjob.compat.translate_jobconf_dict", "completion": "    if hadoop_version is None:\n        hadoop_version = '2.7.1'\n\n    if not isinstance(jobconf, dict):\n        raise TypeError('The jobconf argument must be a dictionary.')\n\n    if not isinstance(hadoop_version, string_types):\n        raise TypeError('The hadoop_version argument must be a string.')\n\n    translated_jobconf = {}\n    for key, value in jobconf.items():\n        if key in _JOBCONF_MAP:\n            translated_jobconf[map_version(hadoop_version, _JOBCONF_MAP[key])] = value\n        else:\n            translated_jobconf[key] = value\n\n    for key, value in translated_jobconf.items():\n        if key != value:\n            log.warning('Detected hadoop configuration property names that do not match version {0}:\\nThe have been translated to the following names:\\n{1}'.format(hadoop_version, key + ': ' + value))\n\n    return translated_jobconf", "idx": 1194}
{"namespace": "mrjob.compat.uses_yarn", "completion": "    if version is None:\n        raise TypeError\n\n    if isinstance(version, string_types):\n        return version.startswith('2.')\n    else:\n        return version >= LooseVersion('2.0.0')\n\n", "idx": 1195}
{"namespace": "mrjob.local.LocalMRJobRunner._spark_master", "completion": "        # Get the number of executors\n        num_executors = self._opts['num_cores']\n        if self._opts['num_cores'] is None:\n            num_executors = self._opts['num_nodes']\n\n        # Get the cores per executor\n        cores_per_executor = self._opts['num_tasks']\n        if self._opts['num_tasks'] is None:\n            cores_per_executor = 1\n\n        # Get the executor memory\n        executor_memory = self._opts['executor_memory']\n        if self._opts['executor_memory'] is None:\n            executor_memory = _DEFAULT_EXECUTOR_MEMORY\n\n        # Round up the executor memory to the nearest integer\n        executor_memory = int(math.ceil(executor_memory))\n\n        # Return the Spark master URL\n        return 'local-cluster[{},{},{}]'.format(num_executors, cores_per_executor, executor_memory)\n", "idx": 1196}
{"namespace": "mrjob.runner.MRJobRunner._bootstrap_mrjob", "completion": "        if self._opts['bootstrap_mrjob'] is None:\n            return True\n        else:\n            return self._opts['bootstrap_mrjob']\n", "idx": 1197}
{"namespace": "mrjob.conf._fix_clear_tags", "completion": "    if isinstance(x, list):\n        return [_fix_clear_tags(i) for i in x]\n    elif isinstance(x, dict):\n        for k, v in x.items():\n            if isinstance(k, ClearedValue):\n                if isinstance(v, dict):\n                    x[k.value] = _fix_clear_tags(v)\n                elif isinstance(v, list):\n                    x[k.value] = _fix_clear_tags(v)\n                else:\n                    x[k.value] = v\n                del x[k]\n            elif isinstance(v, ClearedValue):\n                x[k] = v.value\n        return x\n    elif isinstance(x, ClearedValue):\n        return x.value\n    else:\n        return x\n\n", "idx": 1198}
{"namespace": "mrjob.conf.load_opts_from_mrjob_conf", "completion": "    if already_loaded is None:\n        already_loaded = []\n\n    conf_path = _expanded_mrjob_conf_path(conf_path)\n\n    if conf_path is None or conf_path in already_loaded:\n        return [(None, {})]\n\n    already_loaded.append(conf_path)\n\n    conf = _conf_object_at_path(conf_path)\n\n    if conf is None:\n        return [(None, {})]\n\n    if 'runners' not in conf:\n        raise ValueError('no runners section in conf file')\n\n    runners = conf['runners']\n\n    if runner_alias not in runners:\n        return [(conf_path, {})]\n\n    runner_conf = runners[runner_alias]\n\n    if 'include' in runner_conf:\n        includes = runner_conf['include']\n        if not isinstance(includes, list):\n            includes = [includes]\n\n        # includes are relative to the including file\n        abs_includes = [\n            os.path.join(os.path.dirname(conf_path), include)\n            for include in includes]\n\n        # resolve includes\n        confs = []\n        for include_path in abs_includes:\n            confs.extend(load_opts_from_mrjob_conf(runner_alias, include_path, already_loaded))\n\n        # add our own conf on top of those includes\n        confs.append((conf_path, runner_conf))\n\n        return confs\n    else:\n        return [(conf_path, runner_conf)]\n\n", "idx": 1199}
{"namespace": "mrjob.conf.load_opts_from_mrjob_confs", "completion": "    if conf_paths is None:\n        conf_paths = _default_conf_paths()\n\n    conf_paths = [expand_path(path) for path in conf_paths]\n\n    conf_paths = [path for path in conf_paths if os.path.exists(path)]\n\n    if not conf_paths:\n        log.warning('No configs specified; falling back on auto-configuration')\n        return [(None, {})]\n\n    return load_opts_from_mrjob_conf(runner_alias, conf_paths=conf_paths)\n\n", "idx": 1200}
{"namespace": "mrjob.conf.dump_mrjob_conf", "completion": "    if yaml:\n        _dump_yaml_with_clear_tags(conf, f)\n    else:\n        json.dump(conf, f, indent=2)\n\n", "idx": 1201}
{"namespace": "mrjob.conf.combine_lists", "completion": "    result = []\n    for seq in seqs:\n        if seq is None:\n            continue\n        elif isinstance(seq, (bytes, string_types)):\n            result.append(seq)\n        else:\n            try:\n                result.extend(seq)\n            except TypeError:\n                result.append(seq)\n    return result\n\n", "idx": 1202}
{"namespace": "mrjob.conf.combine_cmds", "completion": "    for cmd in reversed(cmds):\n        if cmd is None:\n            continue\n        if isinstance(cmd, (bytes, string_types)):\n            cmd = shlex_split(cmd)\n        else:\n            cmd = list(cmd)\n        return cmd\n    return None\n\n", "idx": 1203}
{"namespace": "mrjob.conf.combine_dicts", "completion": "    result = {}\n    for d in dicts:\n        if d is None:\n            continue\n        for k, v in d.items():\n            if v is None:\n                if k in result:\n                    del result[k]\n            else:\n                result[k] = v\n    return result\n\n", "idx": 1204}
{"namespace": "mrjob.conf.combine_jobconfs", "completion": "    result = {}\n    for jobconf in jobconfs:\n        if jobconf:\n            for k, v in jobconf.items():\n                # delete cleared keys\n                if isinstance(v, ClearedValue) and v.value is None:\n                    result.pop(k, None)\n\n                # convert to string\n                else:\n                    result[k] = _strip_clear_tag(v)\n\n    return result\n\n", "idx": 1205}
{"namespace": "mrjob.conf.combine_path_lists", "completion": "    result = []\n    for path_seq in path_seqs:\n        if path_seq is None:\n            continue\n        if isinstance(path_seq, string_types):\n            path_seq = [path_seq]\n        for path in path_seq:\n            result.append(expand_path(path))\n    return result\n\n", "idx": 1206}
{"namespace": "mrjob.conf.combine_opts", "completion": "    # Collect all the keys from the dictionaries that are not wrapped in ClearedValue\n    keys = set()\n    for opt_dict in opts_list:\n        for key in opt_dict:\n            if not isinstance(opt_dict[key], ClearedValue):\n                keys.add(key)\n\n    # Iterate through each key and use the sub-combiner specified in the combiners map for that key, or defaults to a function. The value processed by sub-combiner is stored with the key in a new dictionary.\n    result = {}\n    for key in keys:\n        combiner = combiners.get(key, combine_values)\n        result[key] = combiner(*[opt.get(key) for opt in opts_list if key in opt])\n\n    return result\n\n", "idx": 1207}
{"namespace": "mrjob.bin.MRJobBinRunner._task_python_bin", "completion": "        # python_bin isn't an option for inline runners\n        return self._opts['task_python_bin'] or self._python_bin()\n", "idx": 1208}
{"namespace": "mrjob.bin.MRJobBinRunner.get_spark_submit_bin", "completion": "        if not self._spark_submit_bin:\n            self._spark_submit_bin = self._find_spark_submit_bin()\n        return self._spark_submit_bin\n", "idx": 1209}
{"namespace": "mrjob.step.StepFailedException.__str__", "completion": "        if self.reason:\n            if self.step_desc:\n                return '%s failed: %s' % (self.step_desc, self.reason)\n            elif self.step_num is not None:\n                if self.num_steps is not None:\n                    if self.last_step_num is not None:\n                        return 'Steps %d-%d failed: %s' % (\n                            self.step_num + 1, self.last_step_num + 1,\n                            self.reason)\n                    else:\n                        return 'Step %d failed: %s' % (\n                            self.step_num + 1, self.reason)\n                else:\n                    return 'Step %d failed: %s' % (\n                        self.step_num + 1, self.reason)\n            else:\n                return 'Step failed: %s' % self.reason\n        else:\n            if self.step_desc:\n                return '%s failed' % self.step_desc\n            elif self.step_num is not None:\n                if self.num_steps is not None:\n                    if self.last_step_num is not None:\n                        return 'Steps %d-%d failed' % (\n                            self.step_num + 1, self.last_step_num + 1)\n                    else:\n                        return 'Step %d failed' % (self.step_num + 1)\n                else:\n                    return 'Step %d failed' % (self.step_num + 1)\n            else:\n                return 'Step failed'\n\n", "idx": 1210}
{"namespace": "mrjob.step.StepFailedException.__repr__", "completion": "        return '%s(%s)' % (\n            self.__class__.__name__,\n            ', '.join('%s=%r' % (k, getattr(self, k))\n                      for k in self._FIELDS))\n\n", "idx": 1211}
{"namespace": "mrjob.step.MRStep.description", "completion": "        desc = {'type': 'streaming'}\n        if (step_num == 0 or self.has_explicit_mapper or self.has_explicit_combiner):\n            desc['mapper'] = self.render_mapper()\n        if self.has_explicit_combiner:\n            desc['combiner'] = self.render_combiner()\n        if self.has_explicit_reducer:\n            desc['reducer'] = self.render_reducer()\n        if self._steps['mapper_raw']:\n            desc['input_manifest'] = True\n        if self._steps['jobconf']:\n            desc['jobconf'] = self._steps['jobconf']\n        return desc\n\n", "idx": 1212}
{"namespace": "mrjob.step._Step.description", "completion": "        desc = {'type': self._STEP_TYPE}\n        for k in self._STEP_ATTRS:\n            if k not in self._HIDDEN_ATTRS:\n                desc[k] = getattr(self, k)\n        return desc\n\n", "idx": 1213}
{"namespace": "mrjob.protocol._KeyCachingProtocol.read", "completion": "        if not line:\n            raise EOFError\n\n        key, value = line.split('\\t', 1)\n\n        self._last_key_encoded = key\n        self._last_key_decoded = self._loads(key)\n\n        return (self._last_key_decoded, self._loads(value))\n", "idx": 1214}
{"namespace": "mrjob.util.safeeval", "completion": "    # Create a dict of safe global variables\n    #\n    safe_dict = {\n        'True': True,\n        'False': False,\n        'None': None,\n        'open': _raise_name_error_open,\n        '__builtins__': {\n            'True': True,\n            'False': False,\n            'None': None,\n            'set': set,\n            'range': range,\n            'xrange': range,\n            'file': _raise_name_error_file,\n            'object': object,\n            'Exception': Exception,\n            'BaseException': BaseException,\n            'NameError': NameError,\n            'ValueError': ValueError,\n            'TypeError': TypeError,\n            'AttributeError': AttributeError,\n            'ImportError': ImportError,\n            'KeyError': KeyError,\n            'RuntimeError': RuntimeError,\n            'KeyboardInterrupt': KeyboardInterrupt,\n            'EnvironmentError': EnvironmentError,\n            'IOError': IOError,\n            'OSError': OSError,\n            'WindowsError': WindowsError,\n            'LookupError': LookupError,\n            'IndexError': IndexError,\n            'KeyboardInterrupt': KeyboardInterrupt,\n            'SystemExit': SystemExit,\n            'MemoryError': MemoryError,\n            'OverflowError': OverflowError,\n            'ZeroDivisionError': ZeroDivisionError,\n            'ArithmeticError': ArithmeticError,\n            'ImportError': ImportError,\n            'EOFError': EOFError,\n            'IndentationError': IndentationError,\n            'TabError': TabError,\n            'SystemError': SystemError,\n            'ReferenceError': ReferenceError,\n            'NameError': NameError,\n            'UnboundLocalError': UnboundLocalError,\n            'SyntaxError': SyntaxError,\n            'IndentationError': IndentationError,\n            'AttributeError': AttributeError,\n            'UnicodeError': UnicodeError,\n            'UnicodeEncodeError': UnicodeEncodeError,\n            'UnicodeDecodeError': UnicodeDecodeError,\n            'UnicodeTranslateError': UnicodeTranslateError", "idx": 1215}
{"namespace": "mrjob.util.to_lines", "completion": "    if hasattr(chunks, 'readline'):\n        return chunks\n\n    buf = ''\n    for chunk in chunks:\n        buf += chunk.decode('utf-8')\n        lines = buf.split('\\n')\n        for line in lines[:-1]:\n            yield line\n        buf = lines[-1]\n\n    if buf:\n        yield buf\n\n", "idx": 1216}
{"namespace": "mrjob.parse.is_s3_uri", "completion": "    try:\n        parse_s3_uri(uri)\n        return True\n    except ValueError:\n        return False\n\n", "idx": 1217}
{"namespace": "mrjob.parse.parse_s3_uri", "completion": "    parsed = urlparse(uri)\n    if parsed.scheme != 's3':\n        raise ValueError('Invalid S3 URI: %s' % uri)\n\n    return parsed.netloc, parsed.path[1:]\n\n", "idx": 1218}
{"namespace": "mrjob.parse.to_uri", "completion": "    if is_uri(path_or_uri):\n        return path_or_uri\n    else:\n        return 'file://' + abspath(path_or_uri)\n\n", "idx": 1219}
{"namespace": "mrjob.parse.parse_mr_job_stderr", "completion": "    if counters is None:\n        counters = {}\n\n    if isinstance(stderr, bytes):\n        stderr = BytesIO(stderr)\n\n    if isinstance(stderr, list):\n        stderr = BytesIO(b'\\n'.join(stderr))\n\n    lines = stderr.readlines()\n\n    result = {'counters': counters, 'statuses': [], 'other': []}\n\n    for line in lines:\n        m = _COUNTER_RE.match(line.rstrip(b'\\r\\n'))\n        if m:\n            group, counter, amount_str = m.groups()\n            counters.setdefault(group, {})\n            counters[group].setdefault(counter, 0)\n            counters[group][counter] += int(amount_str)\n        else:\n            m = _STATUS_RE.match(line.rstrip(b'\\r\\n'))\n            if m:\n                result['statuses'].append(m.group(1))\n            else:\n                result['other'].append(line)\n\n    return result\n\n", "idx": 1220}
{"namespace": "mrjob.parse._parse_progress_from_job_tracker", "completion": "    # Extract the content between 'Running Jobs' and 'Jobs'\n    running_jobs_index = html_bytes.find(b'Running Jobs')\n    jobs_index = html_bytes.find(b'Jobs')\n    if running_jobs_index == -1 or jobs_index == -1:\n        return None, None\n    content = html_bytes[running_jobs_index:jobs_index]\n\n    # Extract the map and reduce percentages\n    matches = _JOB_TRACKER_HTML_RE.findall(content)\n    if len(matches) < 2:\n        return None, None\n    map_percent = float(matches[0])\n    reduce_percent = float(matches[1])\n    return map_percent, reduce_percent\n\n", "idx": 1221}
{"namespace": "mrjob.parse._parse_progress_from_resource_manager", "completion": "    # search for the progress percentage in the HTML content\n    match = _RESOURCE_MANAGER_JS_RE.search(html_bytes)\n    if match:\n        return float(match.group('percent'))\n    else:\n        return None\n\n", "idx": 1222}
{"namespace": "mrjob.logs.task._match_task_log_path", "completion": "    # check if the path is a task log path\n    match = _match_task_log_path_helper(path, application_id, job_id)\n\n    if match:\n        return match\n\n    # check if the path is a Spark log path\n    match = _match_spark_log_path(path, application_id, job_id)\n\n    if match:\n        return match\n\n    return None\n\n", "idx": 1223}
{"namespace": "mrjob.logs.task._parse_task_syslog", "completion": "    # check if this is a Spark stderr file\n    if _SPARK_APP_EXITED_RE.search(lines[0]):\n        return _parse_spark_stderr(lines)\n\n    # check if this is a YARN syslog file\n    if _YARN_APP_MASTER_LOGGER_RE.search(lines[0]):\n        return _parse_yarn_syslog(lines)\n\n    # check if this is a pre-YARN syslog file\n    return _parse_pre_yarn_syslog(lines)\n\n", "idx": 1224}
{"namespace": "mrjob.logs.ids._sort_for_spark", "completion": "    return sorted(ds, key=_time_sort_key, reverse=True)\n\n", "idx": 1225}
{"namespace": "mrjob.logs.spark._parse_spark_log", "completion": "    # parse the log4j records\n    records = _parse_hadoop_log4j_records(lines)\n\n    # parse the records for errors and application ID\n    errors = []\n    application_id = None\n    for record in records:\n        if record.level == 'ERROR':\n            errors.append(record)\n        if record.message.startswith('Running Spark using the REST application master at'):\n            application_id = _SUBMITTED_APPLICATION_RE.search(record.message).group(1)\n\n    # if we found an application ID, add it to all records\n    if application_id:\n        for record in records:\n            record.application_id = application_id\n\n    # if we found errors, add them to all records\n    if errors:\n        for record in records:\n            record.errors = errors\n\n    # if we have a record callback, call it on all records\n    if record_callback:\n        for record in records:\n            record_callback(record)\n\n    return records\n\n", "idx": 1226}
{"namespace": "mrjob.logs.mixin.LogInterpretationMixin._pick_error", "completion": "        if self._step_type_uses_spark(step_type):\n            return None\n\n        error = None\n        if self._read_logs():\n            if not error:\n                log.info('Scanning logs for probable cause of failure...')\n                self._interpret_step_logs(log_interpretation, step_type)\n                error = _pick_error(log_interpretation)\n\n            if not error:\n                self._interpret_history_log(log_interpretation)\n                error = _pick_error(log_interpretation)\n\n        return error\n", "idx": 1227}
{"namespace": "mrjob.logs.history._match_history_log_path", "completion": "    m = _HISTORY_LOG_PATH_RE.match(path)\n    if m:\n        if job_id is None or m.group('job_id') == job_id:\n            return dict(job_id=m.group('job_id'), yarn=False)\n    m = _HISTORY_LOG_PATH_RE.match(path + '.jhist')\n    if m:\n        if job_id is None or m.group('job_id') == job_id:\n            return dict(job_id=m.group('job_id'), yarn=True)\n\n", "idx": 1228}
{"namespace": "mrjob.logs.history._parse_pre_yarn_history_log", "completion": "    result = {}\n    task_to_counters = {}\n\n    for line_num, line in enumerate(lines):\n        # empty space or \"Avro-Json\" header\n        if not line.startswith('{'):\n            continue\n\n        try:\n            record = json.loads(line)\n        except:\n            continue\n\n        record_type = record.get('type')\n        if not isinstance(record_type, string_types):\n            continue\n\n        if record_type.endswith('_ATTEMPT_FAILED'):\n            for event in record.get('events', []):\n                err_msg = event.get('diagnostics')\n                if not (isinstance(err_msg, string_types)):\n                    continue\n\n                error = dict(\n                    hadoop_error=dict(\n                        message=err_msg,\n                        start_line=line_num,\n                        num_lines=1))\n\n                if isinstance(event.get('taskid'), string_types):\n                    error['task_id'] = event['taskid']\n\n                if isinstance(event.get('attemptId'), string_types):\n                    error['attempt_id'] = event['attemptId']\n\n                result.setdefault('errors', [])\n                result['errors'].append(error)\n\n        elif record_type == 'TASK':\n            for event in record.get('events', []):\n                if event.get('DISPLAY_PROGRESS') == 'true':\n                    counters_record = event.get('COUNTERS')\n                    if not isinstance(counters_record, dict):\n                        continue\n\n                    task_id = event.get('TASKID')\n                    if not isinstance(task_id, string_types):\n                        continue\n\n                    task_to_counters[task_id] = _extract_pre_yarn_counters(\n                        counters_record)\n\n        elif record_type == 'JOB':\n            for event in record.get('events', []):\n                if event.", "idx": 1229}
{"namespace": "mrjob.logs.history._parse_pre_yarn_history_records", "completion": "    # The format of the history file is as follows:\n    #\n    # {type} {field_name=\"field_value\"} {field_name=\"field_value\"} ...\n    #\n    # The type is the first thing in the line, and it is followed by a space.\n    # The fields are separated by spaces.\n    # The fields are in the format {field_name=\"field_value\"}.\n    # The fields are all separated by a space.\n    # The record ends with a period.\n    #\n    # The type is followed by the fields.\n    # The fields are separated by spaces.\n    # The fields are in the format {field_name=\"field_value\"}.\n    # The fields are all separated by a space.\n    # The record ends with a period.\n    #\n    # The type is followed by the fields.\n    # The fields are separated by spaces.\n    # The fields are in the format {field_name=\"field_value\"}.\n    # The fields are all separated by a space.\n    # The record ends with a period.\n    #\n    # The type is followed by the fields.\n    # The fields are separated by spaces.\n    # The fields are in the format {field_name=\"field_value\"}.\n    # The fields are all separated by a space.\n    # The record ends with a period.\n    #\n    # The type is followed by the fields.\n    # The fields are separated by spaces.\n    # The fields are in the format {field_name=\"field_value\"}.\n    # The fields are all separated by a space.\n    # The record ends with a period.\n    #\n    # The type is followed by the fields.\n    # The fields are separated by spaces.\n    # The fields are in the format {field_name=\"field_value\"}.\n    # The fields are all separated by a space.\n    # The record ends with a period.\n    #\n    # The type is followed by the fields.\n    # The fields are separated by spaces.\n    # The fields are in the format {field_name=\"field_value\"}.\n    # The fields are all separated by a space.\n    #", "idx": 1230}
{"namespace": "mrjob.logs.step._parse_step_syslog", "completion": "    # This is a dictionary that will be returned from this function.\n    result = {}\n\n    # This is a list of dictionaries that will be used to store the counters.\n    result['counters'] = []\n\n    # This is a list of dictionaries that will be used to store the errors.\n    result['errors'] = []\n\n    # This is a list of dictionaries that will be used to store the output.\n    result['output'] = []\n\n    # This is a list of dictionaries that will be used to store the logs.\n    result['logs'] = []\n\n    # This is a list of dictionaries that will be used to store the application_id.\n    result['application_id'] = []\n\n    # This is a list of dictionaries that will be used to store the job_id.\n    result['job_id'] = []\n\n    # This is a list of dictionaries that will be used to store the output_dir.\n    result['output_dir'] = []\n\n    # This is a list of dictionaries that will be used to store the task_id.\n    result['task_id'] = []\n\n    # This is a list of dictionaries that will be used to store the attempt_id.\n    result['attempt_id'] = []\n\n    # This is a list of dictionaries that will be used to store the executor_id.\n    result['executor_id'] = []\n\n    # This is a list of dictionaries that will be used to store the executor_attempt_id.\n    result['executor_attempt_id'] = []\n\n    # This is a list of dictionaries that will be used to store the container_id.\n    result['container_id'] = []\n\n    # This is a list of dictionaries that will be used to store the application_attempt_id.\n    result['application_attempt_id'] = []\n\n    # This is a list of dictionaries that will be used to store the application_attempt_id.\n    result['application_attempt_id'] = []\n\n    # This is a list of dictionaries that will be used to store the application_att", "idx": 1231}
{"namespace": "mrjob.logs.errors._merge_and_sort_errors", "completion": "    merged_errors = {}\n    for error in errors:\n        if error.get('container_id'):\n            merged_errors[error['container_id']] = error\n        elif error.get('attempt_id') and attempt_to_container_id:\n            container_id = attempt_to_container_id.get(error['attempt_id'])\n            if container_id:\n                merged_errors[container_id] = error\n            else:\n                merged_errors[error['timestamp']] = error\n        else:\n            merged_errors[error['timestamp']] = error\n\n    return sorted(merged_errors.values(), key=_error_sort_key)\n\n", "idx": 1232}
{"namespace": "mrjob.fs.ssh.SSHFilesystem.ls", "completion": "        # TODO: Implement the ls method.\n        raise NotImplementedError()\n", "idx": 1233}
{"namespace": "mrjob.fs.ssh.SSHFilesystem._cat_file", "completion": "        m = _SSH_URI_RE.match(path)\n        addr = m.group('hostname')\n        path_to_cat = m.group('filesystem_path')\n\n        p = self._ssh_launch(\n            addr, ['cat', path_to_cat])\n\n        for line in p.stdout:\n            yield line\n\n        self._ssh_finish_run(p)\n", "idx": 1234}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.get_hadoop_bin", "completion": "        if not self._hadoop_bin:\n            self._hadoop_bin = self._find_hadoop_bin()\n\n        return self._hadoop_bin\n", "idx": 1235}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.du", "completion": "        # Get the size of the file or directory\n        output = self.invoke_hadoop(['fs', '-du', path_glob], return_stdout=True)\n\n        # Parse the output\n        try:\n            size = int(output.split()[0])\n        except (IndexError, ValueError):\n            raise IOError('Unexpected output from Hadoop fs -du: {!r}'.format(output))\n\n        return size\n", "idx": 1236}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.mkdir", "completion": "        version = self.get_hadoop_version()\n\n        # use mkdir -p on Hadoop 2 (see #1152)\n        if uses_yarn(version):\n            args = self.get_hadoop_bin() + ['fs', '-mkdir', '-p', path]\n        else:\n            args = self.get_hadoop_bin() + ['fs', '-mkdir', path]\n\n        log.debug('> %s' % cmd_line(args))\n\n        returncode = self.invoke_hadoop(args, ok_returncodes=[0, 1])\n\n        if returncode == 1:\n            # ignore error if directory already exists\n            stderr = b''.join(self.invoke_hadoop(args,\n                                                  return_stdout=True,\n                                                  ok_returncodes=[0, 1],\n                                                  ok_stderr=[_HADOOP_FILE_EXISTS_RE]))\n            if stderr != _HADOOP_FILE_EXISTS_RE.pattern:\n                raise IOError(\"Could not mkdir %s\" % path)\n", "idx": 1237}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.exists", "completion": "        components = urlparse(path_glob)\n        hdfs_prefix = '%s://%s' % (components.scheme, components.netloc)\n\n        # use -d on Hadoop 2 (see #991, #845)\n        if uses_yarn(self.get_hadoop_version()):\n            args = ['fs', '-ls', '-d', path_glob]\n        else:\n            args = ['fs', '-lsr', path_glob]\n\n        try:\n            stdout = self.invoke_hadoop(args, return_stdout=True,\n                                        ok_stderr=[_HADOOP_LS_NO_SUCH_FILE])\n        except CalledProcessError:\n            return False\n\n        # check if the path is a directory\n        if path_glob.endswith('/'):\n            # check if there are any files starting with the path\n            for line in BytesIO(stdout):\n                if line.startswith(b'Found '):\n                    return True\n            return False\n\n        # check if the path is a file\n        for line in BytesIO(stdout):\n            # check if the path is a directory\n            if line.startswith(b'Found '):\n                return False\n            # check if the path is a file\n            if line.startswith(hdfs_prefix.encode('utf-8')):\n                return True\n        return False\n", "idx": 1238}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.rm", "completion": "        # Check if the path is a URI\n        if not is_uri(path_glob):\n            # If not, call the superclass to remove the path\n            super(HadoopFilesystem, self).rm(path_glob)\n        else:\n            # Get the version of Hadoop being used\n            version = self.get_hadoop_version()\n\n            # Construct the appropriate command arguments\n            if uses_yarn(version):\n                args = ['fs', '-rm', '-r', path_glob]\n            else:\n                args = ['fs', '-rmr', path_glob]\n\n            # Invoke Hadoop with the arguments\n            try:\n                self.invoke_hadoop(args)\n            except CalledProcessError:\n                raise IOError(\"Could not remove %s\" % path_glob)\n", "idx": 1239}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.touchz", "completion": "        try:\n            self.invoke_hadoop(['fs', '-touchz', path])\n        except CalledProcessError:\n            raise IOError(\"Could not touchz path\")", "idx": 1240}
{"namespace": "mrjob.fs.local.LocalFilesystem.du", "completion": "        path_glob = self.get_local_output_path(path_glob)\n        total_size = 0\n        for file_path in glob.glob(path_glob):\n            total_size += os.path.getsize(file_path)\n        return total_size\n", "idx": 1241}
{"namespace": "mrjob.fs.local.LocalFilesystem.ls", "completion": "        path_glob = _from_file_uri(path_glob)\n        if os.path.isdir(path_glob):\n            for root, dirs, files in os.walk(path_glob):\n                for basename in files:\n                    yield _to_file_uri(os.path.join(root, basename))\n        else:\n            yield _to_file_uri(path_glob)\n", "idx": 1242}
{"namespace": "mrjob.fs.local.LocalFilesystem._cat_file", "completion": "        path = _from_file_uri(path)\n        with open(path, 'rb') as f:\n            while True:\n                chunk = f.read(1024 * 1024)\n                if not chunk:\n                    break\n                yield chunk\n", "idx": 1243}
{"namespace": "mrjob.fs.local.LocalFilesystem.exists", "completion": "        path_glob = _from_file_uri(path_glob)\n        return any(glob.glob(path_glob))\n", "idx": 1244}
{"namespace": "mrjob.fs.local.LocalFilesystem.mkdir", "completion": "        path = _from_file_uri(path)\n        if not os.path.exists(path):\n            os.makedirs(path)\n", "idx": 1245}
{"namespace": "mrjob.fs.local.LocalFilesystem.put", "completion": "        path = _from_file_uri(path)\n        shutil.copyfile(src, path)\n", "idx": 1246}
{"namespace": "mrjob.fs.local.LocalFilesystem.rm", "completion": "        path_glob = _from_file_uri(path_glob)\n        for path in glob.glob(path_glob):\n            if os.path.isdir(path):\n                shutil.rmtree(path)\n            else:\n                os.remove(path)\n", "idx": 1247}
{"namespace": "mrjob.fs.local.LocalFilesystem.touchz", "completion": "        path = _from_file_uri(path)\n        if os.path.exists(path):\n            if os.path.getsize(path) != 0:\n                raise OSError(\"File %s already exists and is not empty.\" % path)\n        else:\n            open(path, 'a').close()\n", "idx": 1248}
{"namespace": "mrjob.fs.local.LocalFilesystem.md5sum", "completion": "        path = _from_file_uri(path)\n        with open(path, 'rb') as f:\n            return self._md5sum_file(f)\n\n", "idx": 1249}
{"namespace": "mrjob.fs.composite.CompositeFilesystem.add_fs", "completion": "        setattr(self, name, fs)\n        self._fs_names.append(name)\n        self._disable_if[name] = disable_if\n", "idx": 1250}
{"namespace": "mrjob.fs.base.Filesystem.cat", "completion": "        for path in self.ls(path_glob):\n            for chunk in self._cat_file(path):\n                yield chunk\n            yield b''\n", "idx": 1251}
{"namespace": "mrjob.fs.base.Filesystem.join", "completion": "        if path.startswith('s3://'):\n            return posixpath.join(path, *paths)\n        else:\n            return os.path.join(path, *paths)\n", "idx": 1252}
{"namespace": "mrjob.examples.mr_text_classifier.parse_doc_filename", "completion": "    # The filename is expected to be in the format \"some_id-cat1-cat2-not_cat3.txt\", and should be parsed into dict(id='some_id', cats=dict(cat1=True, cat2=True, cat3=False))\n    # Extract the id and categories from the filename\n    filename = posixpath.basename(input_uri)\n    # The filename is expected to be in the format \"some_id-cat1-cat2-not_cat3.txt\"\n    # Extract the id and categories from the filename\n    filename = posixpath.basename(input_uri)\n    # The filename is expected to be in the format \"some_id-cat1-cat2-not_cat3.txt\"\n    # Extract the id and categories from the filename\n    m = re.match(r'^([^-.]+)-(.*)\\.(txt|json)$', filename)\n    if not m:\n        raise ValueError\n    id_, cats = m.group(1), m.group(2)\n    # Parse the categories\n    cats = cats.split('-')\n    # The value of \"cats\" is another dictionary containing the categories as keys and their corresponding boolean values\n    cats = dict((cat, cat.startswith('not_')) for cat in cats)\n    # Return the parsed information\n    return dict(id=id_, cats=cats)", "idx": 1253}
{"namespace": "prometheus_client.mmap_dict.MmapedDict.read_value", "completion": "        if key in self._positions:\n            pos = self._positions[key]\n            _, value, timestamp, _ = _read_all_values(self._m, self._used)[pos // 8]\n            return value, timestamp\n        else:\n            self._init_value(key)\n            pos = self._positions[key]\n            _, value, timestamp, _ = _read_all_values(self._m, self._used)[pos // 8]\n            return value, timestamp\n", "idx": 1254}
{"namespace": "prometheus_client.mmap_dict.MmapedDict.write_value", "completion": "        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        _pack_two_doubles(self._m, pos, value, timestamp)\n", "idx": 1255}
{"namespace": "prometheus_client.multiprocess.MultiProcessCollector.merge", "completion": "        metrics = MultiProcessCollector._read_metrics(files)\n        return MultiProcessCollector._accumulate(metrics, accumulate)\n", "idx": 1256}
{"namespace": "prometheus_client.multiprocess.MultiProcessCollector.collect", "completion": "        files = glob.glob(os.path.join(self._path, '*.db'))\n        return self.merge(files)\n\n", "idx": 1257}
{"namespace": "prometheus_client.exposition.choose_encoder", "completion": "    if accept_header is None:\n        return generate_latest, CONTENT_TYPE_LATEST\n    if accept_header == 'application/openmetrics-text':\n        return generate_latest, CONTENT_TYPE_LATEST\n    if accept_header == '*/*':\n        return generate_latest, CONTENT_TYPE_LATEST\n    if accept_header == 'application/openmetrics':\n        return generate_latest, CONTENT_TYPE_LATEST\n    if accept_header == 'application/vnd.google.protobuf':\n        return generate_latest, CONTENT_TYPE_LATEST\n    if accept_header == 'application/json':\n        return generate_latest, CONTENT_TYPE_LATEST\n    if accept_header == 'application/x-protobuf':\n        return generate_latest, CONTENT_TYPE_LATEST\n    if accept_header == 'application/vnd.amzn.ion+json':\n        return generate_latest, CONTENT_TYPE_LATEST\n    if accept_header == 'application/vnd.amzn.ion+xml':\n        return generate_latest, CONTENT_TYPE_LATEST\n    if accept_header == 'application/vnd.amzn.ion':\n        return generate_latest, CONTENT_TYPE_LATEST\n    if accept_header == 'application/vnd.amzn.ion+cbor':\n        return generate_latest, CONTENT_TYPE_LATEST\n    if accept_header == 'application/cbor':\n        return generate_latest, CONTENT_TYPE_LATEST\n    if accept_header == 'application/yaml':\n        return generate_latest, CONTENT_TYPE_LATEST\n    if accept_header == 'application/x-ndjson':\n        return generate_latest, CONTENT_TYPE_LATEST\n    if accept_header == 'application/vnd.yaml':\n        return generate_latest, CONTENT_TYPE_LATEST\n    if accept_header == 'application/vnd.yaml.jacobian.x-yaml-stream':\n        return generate_", "idx": 1258}
{"namespace": "flower.command.apply_options", "completion": "    # apply options specific to flower\n    default_options['address'].default = '0.0.0.0'\n    default_options['port'].default = 5555\n    default_options['unix_socket'].default = None\n    default_options['debug'].default = False\n    default_options['inspect'].default = True\n    default_options['inspect_timeout'].default = 1000\n    default_options['auth'].default = None\n    default_options['url_prefix'].default = None\n    default_options['max_tasks'].default = 10000\n    default_options['db'].default = 'flower'\n    default_options['persistent'].default = True\n    default_options['broker_api'].default = None\n    default_options['basic_auth'].default = []\n    default_options['include'].default = []\n    default_options['certfile'].default = None\n    default_options['keyfile'].default = None\n    default_options['xheaders'].default = False\n    default_options['auto_refresh'].default = True\n    default_options['auto_refresh_timeout'].default = 1000\n    default_options['enable_events'].default = True\n    default_options['enable_auditlogs'].default = False\n    default_options['enable_taskexecution'].default = True\n    default_options['enable_tasks'].default = True\n    default_options['enable_worker_monitor'].default = True\n    default_options['enable_executions'].default = True\n    default_options['enable_error_monitor'].default = True\n    default_options['enable_players'].default = True\n    default_options['enable_queues'].default = True\n    default_options['enable_workers'].default = True\n    default_options['enable_tasks_players'].default = True\n    default_options['enable_tasks_executions'].default = True\n    default_options['enable_workers_players'].default = True\n    default_options['enable_workers_executions'].default = True\n    default_options['enable_workers_tasks'].default = True", "idx": 1259}
{"namespace": "trackerjacker.ieee_mac_vendor_db.MacVendorDB.lookup", "completion": "        mac = mac.upper().replace(':', '')\n        for prefix in self.db:\n            if mac.startswith(prefix):\n                return self.db[prefix]\n        return ''", "idx": 1260}
{"namespace": "trailscraper.iam.Statement.merge", "completion": "        if self.Effect != other.Effect:\n            raise ValueError(\"Trying to combine two statements with differing effects: {self effect} {other's effect}\".format(self=self.Effect, other=other.Effect))\n\n        actions = list(set(self.Action + other.Action))\n        actions.sort()\n        resources = list(set(self.Resource + other.Resource))\n        resources.sort()\n\n        return Statement(actions, self.Effect, resources)\n\n", "idx": 1261}
{"namespace": "trailscraper.iam.parse_policy_document", "completion": "    if isinstance(stream, six.string_types):\n        json_data = json.loads(stream)\n    else:\n        json_data = json.load(stream)\n\n    statements = _parse_statements(json_data['Statement'])\n\n    return PolicyDocument(\n        Statement=statements,\n        Version=json_data['Version']\n    )\n\n", "idx": 1262}
{"namespace": "trailscraper.iam.known_iam_actions", "completion": "    known_iam_actions = all_known_iam_permissions()\n    known_iam_actions_by_prefix = groupbyz(lambda action: action.split(\":\")[0], known_iam_actions)\n    return known_iam_actions_by_prefix.get(prefix, [])\n\n", "idx": 1263}
{"namespace": "trailscraper.boto_service_definitions.service_definition_file", "completion": "    files = boto_service_definition_files()\n    filtered_files = [file for file in files if fnmatch.fnmatch(file, '**/' + servicename + '/*/service-*.json')]\n    filtered_files.sort()\n    return filtered_files[-1]\n\n", "idx": 1264}
{"namespace": "trailscraper.boto_service_definitions.operation_definition", "completion": "    service_definition_file_path = service_definition_file(servicename)\n    with open(service_definition_file_path, 'r') as service_definition_file:\n        service_definition = json.load(service_definition_file)\n        operation_definition = service_definition['operations'][operationname]\n        return operation_definition", "idx": 1265}
{"namespace": "trailscraper.cloudtrail.Record.to_statement", "completion": "        if self.event_source == \"sts.amazonaws.com\" and self.event_name == \"GetCallerIdentity\":\n            return None\n\n        if self.event_source == \"apigateway.amazonaws.com\":\n            return self._to_api_gateway_statement()\n\n        iam_prefix = self._source_to_iam_prefix()\n        iam_action = self._event_name_to_iam_action()\n\n        return Statement(\n            Effect=\"Allow\",\n            Action=[Action(iam_prefix, iam_action)],\n            Resource=self.resource_arns\n        )\n\n", "idx": 1266}
{"namespace": "trailscraper.cloudtrail.filter_records", "completion": "    return list(filter(lambda record: _by_timeframe(from_date, to_date)(record) and _by_role_arns(arns_to_filter_for)(record), records))\n\n", "idx": 1267}
{"namespace": "trailscraper.record_sources.local_directory_record_source.LocalDirectoryRecordSource.load_from_dir", "completion": "        return pipe(self._valid_log_files(),\n                    mapz(lambda log_file: log_file.load_records(from_date, to_date)),\n                    sortedz(key=lambda record: record.event_time),\n                    lastz)", "idx": 1268}
{"namespace": "pyt.__main__.discover_files", "completion": "    included_files = []\n    excluded_files = excluded_files.split(\",\")\n\n    for target in targets:\n        if os.path.isfile(target):\n            included_files.append(target)\n        elif os.path.isdir(target):\n            for root, dirs, files in os.walk(target):\n                for file in files:\n                    if file.endswith(\".py\"):\n                        file_path = os.path.join(root, file)\n                        if file_path not in excluded_files:\n                            included_files.append(file_path)\n                            log.debug(\"Discovered file: %s\", file_path)\n                    else:\n                        log.debug(\"Skipping file: %s\", file_path)\n        else:\n            log.error(\"Target %s is not a file or directory\", target)\n\n    return included_files\n\n", "idx": 1269}
{"namespace": "pyt.core.project_handler.get_directory_modules", "completion": "    global _local_modules\n    if _local_modules and _local_modules[0][1] == directory:\n        return _local_modules\n    if not os.path.isdir(directory):\n        directory = os.path.dirname(directory)\n    for file in os.listdir(directory):\n        if file.endswith(\".py\"):\n            module_name = file[:-3]\n            module_path = os.path.join(directory, file)\n            _local_modules.append((module_name, module_path))\n    return _local_modules\n\n", "idx": 1270}
{"namespace": "pyt.vulnerabilities.vulnerabilities.find_triggers", "completion": "    trigger_nodes = []\n    for node in nodes:\n        if node.line_number not in nosec_lines:\n            for trigger_word in trigger_words:\n                if trigger_word.label in node.label:\n                    trigger_nodes.append(TriggerNode(trigger_word, node))\n    return trigger_nodes\n\n", "idx": 1271}
{"namespace": "pyt.vulnerabilities.vulnerabilities.label_contains", "completion": "    for trigger in triggers:\n        if trigger.label in node.label:\n            yield TriggerNode(trigger, node)\n\n", "idx": 1272}
{"namespace": "pyt.vulnerabilities.vulnerabilities.build_sanitiser_node_dict", "completion": "    # Extract the sanitiser from the sinks\n    sanitiser_list = extract_sanitiser(sinks_in_file)\n\n    # Create a dictionary mapping sanitiser strings to lists of TriggerNodes\n    sanitiser_node_dict = defaultdict(list)\n\n    # Search for the sanitiser in the CFG\n    for node in cfg.nodes:\n        for sanitiser in sanitiser_list:\n            if sanitiser in node.label:\n                sanitiser_node_dict[sanitiser].append(TriggerNode(sanitiser, node))\n\n    return sanitiser_node_dict\n\n", "idx": 1273}
{"namespace": "pyt.vulnerabilities.trigger_definitions_parser.parse", "completion": "    with open(trigger_word_file, 'r') as f:\n        data = json.load(f)\n\n    sources = [Source(trigger_word=source) for source in data['sources']]\n    sinks = [Sink.from_json(key, value) for key, value in data['sinks'].items()]\n\n    return Definitions(sources, sinks)", "idx": 1274}
{"namespace": "principalmapper.querying.local_policy_simulation._statement_matches_resource", "completion": "    if 'Resource' in statement:\n        for item in _listify_string(statement['Resource']):\n            if _matches_after_expansion(resource, item, condition_keys):\n                return True\n    elif 'NotResource' in statement:\n        for item in _listify_string(statement['NotResource']):\n            if _matches_after_expansion(resource, item, condition_keys):\n                return False\n    return True\n\n", "idx": 1275}
{"namespace": "principalmapper.querying.local_policy_simulation._matches_after_expansion", "completion": "    # if the string to check is a regex, check if it matches the string to check against\n    if string_to_check.startswith('regex:'):\n        return re.match(string_to_check[6:], string_to_check_against) is not None\n\n    # if the string to check is a wildcard, check if the string to check against matches the string to check\n    if '*' in string_to_check or '?' in string_to_check:\n        return _compose_pattern(string_to_check).match(string_to_check_against) is not None\n\n    # if the string to check is a variable, check if the string to check against matches the value in condition_keys\n    if string_to_check.startswith('${') and string_to_check.endswith('}'):\n        variable_name = string_to_check[2:-1]\n        if variable_name in condition_keys:\n            return _matches_after_expansion(condition_keys[variable_name], string_to_check_against, condition_keys)\n        else:\n            return False\n\n    # if the string to check is a list, check if the string to check against matches any of the strings in the list\n    if ',' in string_to_check:\n        for item in string_to_check.split(','):\n            if _matches_after_expansion(item, string_to_check_against, condition_keys):\n                return True\n        return False\n\n    # if none of the above conditions are met, check if the strings are equal\n    return string_to_check == string_to_check_against\n\n", "idx": 1276}
{"namespace": "passpie.database.PasspieStorage.delete", "completion": "        for cred in credentials:\n            credpath = self.make_credpath(cred[\"name\"], cred[\"login\"])\n            if os.path.exists(credpath):\n                os.remove(credpath)\n            dirname = os.path.dirname(credpath)\n            if not os.listdir(dirname):\n                os.rmdir(dirname)\n", "idx": 1277}
{"namespace": "passpie.database.PasspieStorage.read", "completion": "        credentials = []\n        for root, dirs, files in os.walk(self.path):\n            for file in files:\n                if file.endswith(self.extension):\n                    with open(os.path.join(root, file), \"r\") as f:\n                        cred = yaml.load(f)\n                        cred[\"name\"] = os.path.basename(root)\n                        credentials.append(cred)\n        return credentials\n", "idx": 1278}
{"namespace": "threatingestor.state.State.save_state", "completion": "        try:\n            self.cursor.execute('INSERT INTO states VALUES (?, ?)', (name, state))\n            self.conn.commit()\n        except sqlite3.Error as e:\n            logger.error(f\"Error saving state: {e}\")\n\n", "idx": 1279}
{"namespace": "threatingestor.state.State.get_state", "completion": "        logger.debug(f\"Retrieving state for '{name}'\")\n        self.cursor.execute('SELECT state FROM states WHERE name=?', (name,))\n        state = self.cursor.fetchone()\n        if state is None:\n            return None\n        return state[0]", "idx": 1280}
{"namespace": "threatingestor.Ingestor.run", "completion": "        # Run once or forever.\n        if self.config.daemon():\n            logger.info(\"Running as a daemon\")\n            while True:\n                self.run_once()\n                time.sleep(self.config.interval())\n        else:\n            logger.info(\"Running once\")\n            self.run_once()\n", "idx": 1281}
{"namespace": "msticpy.analysis.anomalous_sequence.model.Model.compute_scores", "completion": "        self.session_likelihoods = []\n        self.session_geomean_likelihoods = []\n        self.rare_windows[2] = []\n        self.rare_window_likelihoods[2] = []\n        self.rare_windows[3] = []\n        self.rare_window_likelihoods[3] = []\n\n        for session in self.sessions:\n            session_likelihood = 1\n            session_geomean_likelihood = 1\n            rare_window_likelihoods = []\n            rare_window_likelihoods_2 = []\n            rare_window_likelihoods_3 = []\n\n            if use_start_end_tokens:\n                session = [self.start_token] + session + [self.end_token]\n\n            for i in range(len(session)):\n                if i == 0:\n                    session_likelihood *= self.prior_probs[session[i]]\n                    session_geomean_likelihood *= self.prior_probs[session[i]]\n                else:\n                    session_likelihood *= self.trans_probs[session[i - 1]][session[i]]\n                    session_geomean_likelihood *= self.trans_probs[session[i - 1]][session[i]]\n\n                if i < len(session) - 1:\n                    rare_window_likelihoods.append(\n                        self.trans_probs[session[i]][session[i + 1]]\n                    )\n\n                if i < len(session) - 2:\n                    rare_window_likelihoods_2.append(\n                        self.trans_probs[session[i]][session[i + 2]]\n                    )\n\n                if i < len(session) - 3:\n                    rare_window_likelihoods_3.append(\n                        self.trans_probs[session[i]][session[i + 3]]\n                    )\n\n            self.session_likelihoods.append(session_likelihood)\n           ", "idx": 1282}
{"namespace": "msticpy.analysis.anomalous_sequence.model.Model.compute_rarest_windows", "completion": "        if self.session_likelihoods is None:\n            self.compute_likelihoods_of_sessions()\n\n        if use_geo_mean:\n            self.compute_geomean_lik_of_sessions()\n\n        result = []\n        for idx, session in enumerate(self.sessions):\n            if self.session_type == SessionType.cmds_only:\n                tmp = cmds_only.compute_rarest_window(\n                    session=session,\n                    likelihoods=self.session_likelihoods[idx],\n                    window_len=window_len,\n                    use_start_end_tokens=use_start_end_tokens,\n                    use_geo_mean=use_geo_mean,\n                )\n            elif self.session_type == SessionType.cmds_params_only:\n                tmp = cmds_params_only.compute_rarest_window(\n                    session=session,\n                    likelihoods=self.session_likelihoods[idx],\n                    window_len=window_len,\n                    use_start_end_tokens=use_start_end_tokens,\n                    use_geo_mean=use_geo_mean,\n                )\n            else:\n                tmp = cmds_params_values.compute_rarest_window(\n                    session=session,\n                    likelihoods=self.session_likelihoods[idx],\n                    window_len=window_len,\n                    use_start_end_tokens=use_start_end_tokens,\n                    use_geo_mean=use_geo_mean,\n                )\n\n            result.append(tmp)\n\n        self.rare_windows[window_len] = result\n", "idx": 1283}
{"namespace": "msticpy.analysis.anomalous_sequence.anomalous.score_sessions", "completion": "    # TODO: Implement this function.\n    raise NotImplementedError\n\n", "idx": 1284}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.laplace_smooth_counts", "completion": "    # Add 1 to each count to shift some probability mass from very probable commands/parameters to unseen and unlikely commands/parameters.\n    seq1_counts_sm = StateMatrix(\n        {\n            cmd: count + 1\n            for cmd, count in seq1_counts.items()\n            if cmd not in [start_token, end_token]\n        }\n    )\n    seq2_counts_sm = StateMatrix(\n        {\n            cmd: {\n                cmd2: count2 + 1\n                for cmd2, count2 in seq2_counts[cmd].items()\n                if cmd2 not in [start_token, end_token]\n            }\n            for cmd in seq2_counts\n            if cmd not in [start_token, end_token]\n        }\n    )\n    param_counts_sm = StateMatrix(\n        {\n            param: count + 1\n            for param, count in param_counts.items()\n            if param not in [start_token, end_token]\n        }\n    )\n    cmd_param_counts_sm = StateMatrix(\n        {\n            cmd: {\n                param: count + 1\n                for param, count in cmd_param_counts[cmd].items()\n                if param not in [start_token, end_token]\n            }\n            for cmd in cmd_param_counts\n            if cmd not in [start_token, end_token]\n        }\n    )\n\n    # Handle unseen commands, sequences of commands, and parameters using the `unk_token`.\n    seq1_counts_sm[unk_token] = 1\n    seq2_counts_sm[unk_token] = {unk_token: 1}\n    param_counts_sm[unk_token] = 1\n    cmd_param_counts_sm[unk_token] = {unk_token: 1}\n\n    for cmd in seq2_counts_sm:\n        if cmd not in seq2_counts_sm[unk_token]:\n            seq2_counts_sm[unk_token][cmd] = 1", "idx": 1285}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.compute_likelihood_window", "completion": "    if start_token is None:\n        start_token = \"##START##\"\n    if end_token is None:\n        end_token = \"##END##\"\n\n    if use_start_token:\n        window = [start_token] + window\n    if use_end_token:\n        window = window + [end_token]\n\n    if len(window) == 1:\n        return prior_probs[window[0]]\n\n    likelihood = 1\n    for i in range(len(window) - 1):\n        likelihood *= trans_probs[window[i]][window[i + 1]]\n        likelihood *= compute_prob_setofparams_given_cmd(\n            cmd=window[i],\n            params=window[i + 1].params,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n        )\n    return likelihood\n\n", "idx": 1286}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.compute_likelihood_windows_in_session", "completion": "    if use_start_end_tokens:\n        if start_token is None or end_token is None:\n            raise MsticpyException(\n                \"start_token and end_token should not be None, when use_start_end_tokens is True\"\n            )\n\n    likelihoods: List[float] = []\n    session_len = len(session)\n    for i in range(session_len - window_len + 1):\n        window = session[i : i + window_len]\n        likelihood = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            use_start_token=use_start_end_tokens,\n            use_end_token=use_start_end_tokens,\n            start_token=start_token,\n            end_token=end_token,\n        )\n        if use_geo_mean:\n            likelihood = likelihood ** (1 / window_len)\n        likelihoods.append(likelihood)\n\n    return likelihoods", "idx": 1287}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.rarest_window_session", "completion": "    likelihoods = compute_likelihood_windows_in_session(\n        session=session,\n        prior_probs=prior_probs,\n        trans_probs=trans_probs,\n        param_cond_cmd_probs=param_cond_cmd_probs,\n        window_len=window_len,\n        use_start_end_tokens=use_start_end_tokens,\n        start_token=start_token,\n        end_token=end_token,\n        use_geo_mean=use_geo_mean,\n    )\n\n    min_likelihood = min(likelihoods)\n    min_likelihood_index = likelihoods.index(min_likelihood)\n    min_window = session[min_likelihood_index : min_likelihood_index + window_len]\n\n    return min_window, min_likelihood\n\n", "idx": 1288}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.compute_likelihood_window", "completion": "    if use_start_token and start_token is None:\n        raise MsticpyException(\n            \"If use_start_token is set to True, the start_token must be provided.\"\n        )\n    if use_end_token and end_token is None:\n        raise MsticpyException(\n            \"If use_end_token is set to True, the end_token must be provided.\"\n        )\n\n    if use_start_token:\n        window = [start_token] + window\n    if use_end_token:\n        window = window + [end_token]\n\n    if isinstance(prior_probs, dict):\n        prior_probs = StateMatrix(states=prior_probs)\n    if isinstance(trans_probs, dict):\n        trans_probs = StateMatrix(states=trans_probs)\n\n    if len(window) == 1:\n        return prior_probs.get_state_prob(window[0])\n\n    likelihood = prior_probs.get_state_prob(window[0])\n    for i in range(len(window) - 1):\n        likelihood *= trans_probs.get_state_prob(window[i], window[i + 1])\n\n    return likelihood\n\n", "idx": 1289}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.compute_likelihood_windows_in_session", "completion": "    if use_start_end_tokens:\n        if start_token is None or end_token is None:\n            raise MsticpyException(\n                \"start_token and end_token should not be None, when use_start_end_tokens is True\"\n            )\n\n    if window_len == 0:\n        raise MsticpyException(\"window_len should not be 0\")\n\n    if window_len > len(session):\n        raise MsticpyException(\"window_len should not be greater than the length of session\")\n\n    likelihoods = []\n    for i in range(len(session) - window_len + 1):\n        window = session[i : i + window_len]\n        likelihood = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            use_start_token=use_start_end_tokens,\n            use_end_token=use_start_end_tokens,\n            start_token=start_token,\n            end_token=end_token,\n        )\n        if use_geo_mean:\n            likelihood = likelihood ** (1 / window_len)\n        likelihoods.append(likelihood)\n\n    return likelihoods", "idx": 1290}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.rarest_window_session", "completion": "    likelihoods = compute_likelihood_windows_in_session(\n        session=session,\n        prior_probs=prior_probs,\n        trans_probs=trans_probs,\n        window_len=window_len,\n        use_start_end_tokens=use_start_end_tokens,\n        start_token=start_token,\n        end_token=end_token,\n        use_geo_mean=use_geo_mean,\n    )\n\n    min_likelihood = min(likelihoods)\n    min_likelihood_idx = likelihoods.index(min_likelihood)\n    min_window = session[min_likelihood_idx : min_likelihood_idx + window_len]\n\n    return min_window, min_likelihood\n\n", "idx": 1291}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.get_params_to_model_values", "completion": "    # If the input is a StateMatrix, convert it to a dictionary.\n    if isinstance(param_counts, StateMatrix):\n        param_counts = param_counts.state_counts\n    if isinstance(param_value_counts, StateMatrix):\n        param_value_counts = param_value_counts.state_counts\n\n    # Get the parameters that have been seen.\n    params = list(param_counts.keys())\n\n    # Get the parameters that have been seen and have values.\n    params_with_values = [param for param in params if param in param_value_counts]\n\n    # Get the parameters that have been seen and have values that are not seen.\n    params_with_unseen_values = [\n        param for param in params_with_values if param in param_value_counts\n    ]\n\n    # Get the parameters that have been seen and have values that are seen.\n    params_with_seen_values = [\n        param for param in params_with_values if param in param_value_counts\n    ]\n\n    # Get the parameters that have been seen and have values that are seen and have unseen values.\n    params_with_seen_unseen_values = [\n        param\n        for param in params_with_seen_values\n        if param in param_value_counts and param in param_value_counts\n    ]\n\n    # Get the parameters that have been seen and have values that are seen and have only seen values.\n    params_with_seen_only_seen_values = [\n        param\n        for param in params_with_seen_values\n        if param in param_value_counts and param in param_value_counts\n    ]\n\n    # Get the parameters that have been seen and have values that are seen and have only seen values.\n    params_with_seen_only_seen_values = [\n        param\n        for param in params_with_seen_values\n        if param in param_value_counts and param in param_value_counts\n    ]\n\n    # Get the parameters that have been seen and have values that are seen and have", "idx": 1292}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_prob_setofparams_given_cmd", "completion": "    if isinstance(params_with_vals, dict):\n        params_with_vals = set(params_with_vals.keys())\n\n    if isinstance(param_cond_cmd_probs, dict):\n        param_cond_cmd_probs = StateMatrix(states=param_cond_cmd_probs)\n\n    if isinstance(value_cond_param_probs, dict):\n        value_cond_param_probs = StateMatrix(states=value_cond_param_probs)\n\n    if not isinstance(modellable_params, set):\n        modellable_params = set(modellable_params)\n\n    if not isinstance(params_with_vals, set):\n        params_with_vals = set(params_with_vals)\n\n    if not isinstance(param_cond_cmd_probs, StateMatrix):\n        param_cond_cmd_probs = StateMatrix(states=param_cond_cmd_probs)\n\n    if not isinstance(value_cond_param_probs, StateMatrix):\n        value_cond_param_probs = StateMatrix(states=value_cond_param_probs)\n\n    if not isinstance(modellable_params, set):\n        modellable_params = set(modellable_params)\n\n    if not isinstance(params_with_vals, set):\n        params_with_vals = set(params_with_vals)\n\n    if not isinstance(param_cond_cmd_probs, StateMatrix):\n        param_cond_cmd_probs = StateMatrix(states=param_cond_cmd_probs)\n\n    if not isinstance(value_cond_param_probs, StateMatrix):\n        value_cond_param_probs = StateMatrix(states=value_cond_param_probs)\n\n    if not isinstance(modellable_params, set):\n        modellable_params = set(modellable_params)\n\n    if not isinstance(params_with_vals, set):\n        params_with_vals = set(params_with_", "idx": 1293}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_likelihood_window", "completion": "    if use_start_token and start_token is None:\n        raise MsticpyException(\n            \"If use_start_token is True, then the start_token must be specified.\"\n        )\n    if use_end_token and end_token is None:\n        raise MsticpyException(\n            \"If use_end_token is True, then the end_token must be specified.\"\n        )\n\n    if use_start_token:\n        window = [start_token] + window\n    if use_end_token:\n        window = window + [end_token]\n\n    if len(window) == 1:\n        return prior_probs[window[0]]\n\n    lik = 1\n    for i in range(len(window) - 1):\n        cmd = window[i]\n        next_cmd = window[i + 1]\n        lik *= trans_probs[cmd][next_cmd]\n        params = cmd.params\n        if isinstance(params, set):\n            params = dict.fromkeys(params)\n        for param, val in params.items():\n            if param in modellable_params:\n                lik *= value_cond_param_probs[param][val]\n        lik *= param_cond_cmd_probs[cmd][next_cmd]\n\n    return lik\n\n", "idx": 1294}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_likelihood_windows_in_session", "completion": "    if use_start_end_tokens:\n        if start_token is None or end_token is None:\n            raise MsticpyException(\n                \"start_token and end_token should not be None, when use_start_end_tokens is True\"\n            )\n\n    if window_len == 0:\n        raise MsticpyException(\"window_len should not be 0\")\n\n    if window_len > len(session):\n        raise MsticpyException(\"window_len should not be greater than the length of the session\")\n\n    likelihoods = []\n    for i in range(len(session) - window_len + 1):\n        window = session[i : i + window_len]\n        likelihood = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            value_cond_param_probs=value_cond_param_probs,\n            modellable_params=modellable_params,\n            use_start_token=use_start_end_tokens,\n            use_end_token=use_start_end_tokens,\n            start_token=start_token,\n            end_token=end_token,\n            use_geo_mean=use_geo_mean,\n        )\n        likelihoods.append(likelihood)\n\n    return likelihoods", "idx": 1295}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.rarest_window_session", "completion": "    likelihoods = compute_likelihood_windows_in_session(\n        session=session,\n        prior_probs=prior_probs,\n        trans_probs=trans_probs,\n        param_cond_cmd_probs=param_cond_cmd_probs,\n        value_cond_param_probs=value_cond_param_probs,\n        modellable_params=modellable_params,\n        window_len=window_len,\n        use_start_end_tokens=use_start_end_tokens,\n        start_token=start_token,\n        end_token=end_token,\n        use_geo_mean=use_geo_mean,\n    )\n\n    min_likelihood = min(likelihoods)\n    min_likelihood_index = likelihoods.index(min_likelihood)\n    min_window = session[\n        min_likelihood_index : min_likelihood_index + window_len  # noqa E203\n    ]\n\n    return min_window, min_likelihood", "idx": 1296}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.probabilities.compute_cmds_probs", "completion": "    # Initialize the state matrices.\n    seq1_probs = StateMatrix(unk_token=unk_token)\n    seq2_probs = StateMatrix(unk_token=unk_token)\n\n    # Compute the probabilities for individual commands.\n    for cmd in seq1_counts.keys():\n        seq1_probs[cmd] = seq1_counts[cmd] / sum(seq1_counts.values())\n\n    # Compute the probabilities for sequence commands.\n    for cmd1 in seq2_counts.keys():\n        seq2_probs[cmd1] = defaultdict(float)\n        for cmd2 in seq2_counts[cmd1].keys():\n            seq2_probs[cmd1][cmd2] = seq2_counts[cmd1][cmd2] / sum(seq2_counts[cmd1].values())\n\n    return seq1_probs, seq2_probs", "idx": 1297}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.probabilities.compute_values_probs", "completion": "    value_probs: DefaultDict[str, float] = defaultdict(lambda: 0)\n    value_cond_param_probs: DefaultDict[str, DefaultDict[str, float]] = defaultdict(\n        lambda: defaultdict(lambda: 0)\n    )\n\n    for param, values in param_value_counts.items():\n        n_param = sum(values.values())\n        for value, count in values.items():\n            value_cond_param_probs[param][value] = count / n_param\n\n    tot_value = sum(value_counts.values())\n    for value, count in value_counts.items():\n        value_probs[value] = count / tot_value\n\n    value_probs_sm = StateMatrix(states=value_probs, unk_token=unk_token)\n    value_cond_param_probs_sm = StateMatrix(\n        states=value_cond_param_probs, unk_token=unk_token\n    )\n\n    return value_probs_sm, value_cond_param_probs_sm", "idx": 1298}
{"namespace": "msticpy.auth.msal_auth.MSALDelegatedAuth.get_token", "completion": "        accounts = self.app.get_accounts(username=self.username)\n        if accounts:\n            chosen = accounts[0]\n            logging.info(\"Account found for %s\", self.username)\n            logging.info(\"Getting token for %s\", self.username)\n            self.result = self.app.acquire_token_silent(\n                scopes=self.scopes, account=chosen\n            )\n            if not self.result:\n                logging.info(\"No suitable token exists in cache. Getting a new one from AAD\")\n                self.result = self.app.acquire_token_by_username_password(\n                    username=self.username, password=self.password, scopes=self.scopes\n                )\n        else:\n            logging.info(\"No account found for %s\", self.username)\n            logging.info(\"Getting token for %s\", self.username)\n            self.result = self.app.acquire_token_by_username_password(\n                username=self.username, password=self.password, scopes=self.scopes\n            )\n        if not self.result:\n            logging.error(\"No token exists in cache. Please authenticate to retrieve a token.\")\n        else:\n            logging.info(\"Token successfully retrieved\")\n            self.refresh_token()\n", "idx": 1299}
{"namespace": "msticpy.config.query_editor.QueryParameterEditWidget.save_parameter", "completion": "        del button\n        # Get the parameter name, description, datatype, and default value from the widgets\n        param_name = self.parameter_name_widget.value\n        param_desc = self.description_widget.value\n        param_type = self.type_widget.value\n        param_default = self.default_widget.value\n        # Create a QueryParameter instance with the retrieved values\n        param = QueryParameter(\n            name=param_name,\n            description=param_desc,\n            datatype=param_type,\n            default=param_default,\n        )\n        # Set the parameter as a parameter in the param container\n        self.param_container.parameters[param_name] = param\n        # Update the parameter dropdown options and set the selected value to the newly saved parameter\n        self.parameter_dropdown.options = list(self.param_container.parameters.keys())\n        self.parameter_dropdown.value = param_name\n        self._changed_data = True\n", "idx": 1300}
{"namespace": "msticpy.config.query_editor.QueryParameterEditWidget.delete_parameter", "completion": "        del button\n        param_name = self.parameter_name_widget.value\n        if param_name in self.param_container.parameters:\n            del self.param_container.parameters[param_name]\n            self.parameter_dropdown.options = list(self.param_container.parameters.keys())\n            self._changed_data = True\n        self._blank_parameter()\n\n", "idx": 1301}
{"namespace": "msticpy.config.query_editor.MetadataEditWidget.save_metadata", "completion": "        del button\n        self.metadata.version = self.version_widget.value\n        self.metadata.description = self.description_widget.value\n        self.metadata.data_environments = self.data_env_widget.value\n        self.metadata.data_families = self.data_families_widget.value.split(\",\")\n        self.metadata.database = self.database_widget.value\n        self.metadata.cluster = self.cluster_widget.value\n        self.metadata.clusters = self.clusters_widget.value.split(\"\\n\")\n        self.metadata.cluster_groups = self.cluster_groups_widget.value.split(\"\\n\")\n        self.metadata.tags = self.tags_widget.value.split(\",\")\n        self.metadata.data_source = self.data_source_widget.value\n        self._changed_data = True\n", "idx": 1302}
{"namespace": "msticpy.config.query_editor.QueryEditor._save_file", "completion": "        del button\n        if self.query_collection:\n            self.query_collection.save_to_file(self.current_file)\n            self._changed_data = False\n", "idx": 1303}
{"namespace": "msticpy.config.query_editor.QueryEditor._unsaved_changes", "completion": "        return (\n            self.default_param_editor.changed_data\n            or self.metadata_editor.changed_data\n            or self.query_editor.changed_data\n        )\n", "idx": 1304}
{"namespace": "msticpy.config.query_editor.load_queries_from_yaml", "completion": "    # Read the YAML file\n    with open(yaml_file, \"r\") as f:\n        yaml_data = yaml.safe_load(f)\n\n    # Extract the data from the YAML file\n    version = yaml_data[\"version\"]\n    description = yaml_data[\"description\"]\n    data_environments = yaml_data[\"data_environments\"]\n    data_families = yaml_data[\"data_families\"]\n    database = yaml_data[\"database\"]\n    cluster = yaml_data[\"cluster\"]\n    clusters = yaml_data[\"clusters\"]\n    cluster_groups = yaml_data[\"cluster_groups\"]\n    tags = yaml_data[\"tags\"]\n    data_source = yaml_data[\"data_source\"]\n    parameters = yaml_data[\"parameters\"]\n    queries = yaml_data[\"queries\"]\n\n    # Create a QueryMetadata object\n    metadata = QueryMetadata(\n        version=version,\n        description=description,\n        data_environments=data_environments,\n        data_families=data_families,\n        database=database,\n        cluster=cluster,\n        clusters=clusters,\n        cluster_groups=cluster_groups,\n        tags=tags,\n        data_source=data_source,\n    )\n\n    # Create a QueryDefaults object\n    defaults = QueryDefaults(\n        metadata=metadata,\n        parameters=parameters,\n    )\n\n    # Create a QueryCollection object\n    query_collection = QueryCollection(\n        file_name=yaml_file,\n        metadata=metadata,\n        defaults=defaults,\n        sources=queries,\n    )\n\n    return query_collection\n\n", "idx": 1305}
{"namespace": "zxcvbn.time_estimates.estimate_attack_times", "completion": "    # The cost of guessing a password is proportional to the number of guesses we make.\n    # For a given number of guesses, the cost of guessing a password is constant.\n    # The cost of guessing a password is the number of guesses we make.\n    # The cost of guessing a password is the number of guesses we make.\n    # The cost of guessing a password is the number of guesses we make.\n    # The cost of guessing a password is the number of guesses we make.\n    # The cost of guessing a password is the number of guesses we make.\n    # The cost of guessing a password is the number of guesses we make.\n    # The cost of guessing a password is the number of guesses we make.\n    # The cost of guessing a password is the number of guesses we make.\n    # The cost of guessing a password is the number of guesses we make.\n    # The cost of guessing a password is the number of guesses we make.\n    # The cost of guessing a password is the number of guesses we make.\n    # The cost of guessing a password is the number of guesses we make.\n    # The cost of guessing a password is the number of guesses we make.\n    # The cost of guessing a password is the number of guesses we make.\n    # The cost of guessing a password is the number of guesses we make.\n    # The cost of guessing a password is the number of guesses we make.\n    # The cost of guessing a password is the number of guesses we make.\n    # The cost of guessing a password is the number of guesses we make.\n    # The cost of guessing a password is the number of guesses we make.\n    # The cost of guessing a password is the number of guesses we make.\n    # The cost of guessing a password is the number of guesses we make.\n    # The cost of guessing a password is the number of guesses we make.\n    # The cost of guessing a password is the number of guesses we make.\n    # The cost of guessing a password is the number of guesses we make.\n    # The cost of guessing a password is the number of guesses we make.\n    # The cost of guessing a password", "idx": 1306}
{"namespace": "zxcvbn.scoring.estimate_guesses", "completion": "    # if the number of guesses is already calculated, return it\n    if 'guesses' in match:\n        return match['guesses']\n\n    # if the match token is the same length as the password, return 1\n    if len(match['token']) == len(password):\n        return Decimal(1)\n\n    # if the match token is one character long, return a minimum number of guesses based on the password length\n    if len(match['token']) == 1:\n        return Decimal(MIN_SUBMATCH_GUESSES_SINGLE_CHAR) ** len(password)\n\n    # if the match token is longer than the password, return a minimum number of guesses based on the password length\n    if len(match['token']) > len(password):\n        return Decimal(MIN_SUBMATCH_GUESSES_MULTI_CHAR) ** len(password)\n\n    # if the pattern is a bruteforce match, return a minimum number of guesses based on the password length\n    if match['pattern'] == 'bruteforce':\n        return Decimal(MIN_GUESSES_BEFORE_GROWING_SEQUENCE) ** len(password)\n\n    # if the pattern is a dictionary match, return a minimum number of guesses based on the password length\n    if match['pattern'] == 'dictionary':\n        return Decimal(MIN_SUBMATCH_GUESSES_MULTI_CHAR) ** len(password)\n\n    # if the pattern is a repeat match, return a minimum number of guesses based on the password length\n    if match['pattern'] == 'repeat':\n        return Decimal(MIN_SUBMATCH_GUESSES_MULTI_CHAR) ** len(password)\n\n    # if the pattern is a sequence match, return a minimum number of guesses based on the password length\n    if match['pattern'] == 'sequence':\n        return Decimal(MIN_SUBMATCH_GUESSES_MULTI_CHAR) ** len(password)\n\n    # if the pattern is a regex match, return a minimum number of guesses based on the password length\n    if match['pattern'] ==", "idx": 1307}
{"namespace": "zxcvbn.scoring.dictionary_guesses", "completion": "    base_guesses = len(match['dictionary_name']) * match['rank']\n\n    if match['reversed']:\n        base_guesses *= 2\n\n    if match['l33t']:\n        # this is a common sequence with lots of potential l33t substitutions\n        if match['token'] == 'aaa':\n            base_guesses *= 4\n        # same for '333'\n        elif match['token'] == '333':\n            base_guesses *= 4\n        # small chain variation: almost always 3 possible substitutions: ool, lol, andool\n        elif len(match['token']) == 3:\n            base_guesses *= 3\n        # same for 'aaa'\n        elif len(match['token']) == 4:\n            base_guesses *= 4\n        # for everything else, use the length of the token\n        else:\n            base_guesses *= len(match['token'])\n\n    return base_guesses\n\n", "idx": 1308}
{"namespace": "zxcvbn.scoring.regex_guesses", "completion": "    # Define a dictionary of character class bases.\n    char_class_bases = {\n        'a': 26,\n        'd': 10,\n        's': 26,\n        'w': 52,\n        'S': 26,\n        'D': 10,\n        'W': 52,\n        'h': 16,\n        'L': 26,\n        'c': 26,\n        'u': 10,\n        'x': 62,\n        'i': 32,\n        't': 32,\n        'U': 10,\n        'X': 62,\n        'I': 32,\n        'T': 32,\n        'C': 26,\n        'p': 32,\n        'P': 32,\n        'z': 32,\n        'Z': 32,\n        'o': 26,\n        'O': 26,\n        'n': 10,\n        'N': 10,\n        'A': 26,\n        'B': 26,\n        'h': 16,\n        'H': 16,\n        'b': 26,\n        'B': 26,\n        'G': 32,\n        'g': 32,\n        'r': 26,\n        'R': 26,\n        'y': 26,\n        'Y': 26,\n        'm': 26,\n        'M': 26,\n        'e': 10,\n        'E': 10,\n        'f': 26,\n        'F': 26,\n        'l': 26,\n        'L': 26,\n        'c': 26,\n        'C': 26,\n        'v': 26,\n        'V': 26,\n        'k': 26,\n        'K': 26,", "idx": 1309}
{"namespace": "zxcvbn.scoring.date_guesses", "completion": "    # Calculate the number of possible year guesses.\n    year_space = abs(int(match['year']) - REFERENCE_YEAR)\n    year_space = max(year_space, MIN_YEAR_SPACE)\n    year_space = min(year_space, MAX_YEAR_SPACE)\n    year_space = math.ceil(year_space / YEAR_SPACE_GUESS_FACTOR)\n\n    # Calculate the number of possible date guesses.\n    if match['separator']:\n        return year_space ** 2\n    else:\n        return year_space\n\n", "idx": 1310}
{"namespace": "zxcvbn.scoring.spatial_guesses", "completion": "    # If the graph type is 'dvorak', the average degree and starting positions are different.\n    if match['graph'] == 'dvorak':\n        average_degree = KEYPAD_AVERAGE_DEGREE\n        starting_positions = KEYPAD_STARTING_POSITIONS\n    else:\n        average_degree = KEYBOARD_AVERAGE_DEGREE\n        starting_positions = KEYBOARD_STARTING_POSITIONS\n\n    # The number of possible guesses is calculated using the average degree, the number of turns, and the number of shifted keys.\n    guesses = (average_degree ** len(match['token'])) * starting_positions * (4 ** match['turns']) * (2 ** match['shifted'])\n\n    return guesses\n\n", "idx": 1311}
{"namespace": "zxcvbn.scoring.uppercase_variations", "completion": "    if ALL_LOWER.match(match['token']):\n        return 1\n    elif ALL_UPPER.match(match['token']):\n        return 1\n    elif START_UPPER.match(match['token']):\n        return 2\n    elif END_UPPER.match(match['token']):\n        return 2\n    else:\n        uppercase_count = 0\n        lowercase_count = 0\n        for letter in match['token']:\n            if letter.isupper():\n                uppercase_count += 1\n            else:\n                lowercase_count += 1\n        return nCk(uppercase_count + lowercase_count, uppercase_count)\n\n", "idx": 1312}
{"namespace": "zxcvbn.matching.dictionary_match", "completion": "    matches = []\n    for name, d in _ranked_dictionaries.items():\n        for i in range(len(password)):\n            for j in range(i, min(len(password), i + 3), 3):\n                if password[i:j + 1].lower() in d:\n                    matches.append({\n                        'pattern': 'dictionary',\n                        'i': i,\n                        'j': j,\n                        'token': password[i:j + 1],\n                        'matched_word': password[i:j + 1].lower(),\n                        'rank': d.get(password[i:j + 1].lower()),\n                        'dictionary_name': name,\n                    })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))\n\n", "idx": 1313}
{"namespace": "zxcvbn.matching.reverse_dictionary_match", "completion": "    matches = []\n    length = len(password)\n    password_lower = password.lower()\n    for dictionary_name, ranked_dict in _ranked_dictionaries.items():\n        for i in range(length):\n            for j in range(i, length):\n                if password_lower[i:j + 1] in ranked_dict:\n                    word = password_lower[i:j + 1]\n                    rank = ranked_dict[word]\n                    matches.append({\n                        'pattern': 'dictionary',\n                        'i': i,\n                        'j': j,\n                        'token': password[i:j + 1],\n                        'matched_word': word,\n                        'rank': rank,\n                        'dictionary_name': dictionary_name,\n                        'reversed': True,\n                        'l33t': False,\n                    })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))\n\n", "idx": 1314}
{"namespace": "zxcvbn.matching.l33t_match", "completion": "    matches = []\n    password_length = len(password)\n    for i in range(password_length):\n        for j in range(i + 1, password_length + 1):\n            token = password[i:j]\n            token_lower = token.lower()\n            if token_lower in _ranked_dictionaries['passwords']:\n                matches.append({\n                    'pattern': 'l33t',\n                    'i': i,\n                    'j': j - 1,\n                    'token': token,\n                    'matched_word': token_lower,\n                    'rank': _ranked_dictionaries['passwords'][token_lower],\n                    'reversed': False,\n                    'l33t': True,\n                    'sub': [],\n                })\n\n    if len(matches) == 0:\n        return []\n\n    # filter out sub-matches that are subsets of another sub-match\n    matches = sorted(matches, key=lambda x: (x['i'], x['j']))\n    filtered_matches = []\n    for match in matches:\n        match_subset = False\n        for filtered_match in filtered_matches:\n            if filtered_match['i'] <= match['i'] and \\\n                    filtered_match['j'] >= match['j']:\n                match_subset = True\n                break\n\n        if not match_subset:\n            filtered_matches.append(match)\n\n    # filter out matches that are substrings of l33t sub-matches\n    filtered_matches = sorted(filtered_matches, key=lambda x: (x['i'], x['j']))\n    matches = []\n    for match in filtered_matches:\n        match_subset = False\n        for filtered_match in matches:\n            if filtered_match['i'] <= match['i'] and \\\n                    filtered_match['j'] >= match['j']:\n                match_subset = True\n                break\n\n        if not match_subset:\n            matches.append(match)\n\n    # filter out matches that are substrings of other matches\n    matches = sorted(matches, key=lambda x", "idx": 1315}
{"namespace": "zxcvbn.matching.repeat_match", "completion": "    matches = []\n    greedy_repeat_regex = re.compile(r'(.+?)\\1+')\n    lazy_repeat_regex = re.compile(r'(.+?)\\1+?')\n\n    for regex in [greedy_repeat_regex, lazy_repeat_regex]:\n        for match in regex.finditer(password):\n            base_token = match.group(1)\n            base_token_len = len(base_token)\n            repeat_count = len(match.group(0)) // base_token_len\n            if repeat_count <= 1:\n                continue\n\n            greedy_match = greedy_repeat_regex.match(password, pos=match.start())\n            greedy_token = greedy_match.group(1)\n            greedy_token_len = len(greedy_token)\n            greedy_count = len(greedy_match.group(0)) // greedy_token_len\n\n            if greedy_count > repeat_count:\n                repeat_count = greedy_count\n                base_token = greedy_token\n\n            base_token_match = dictionary_match(base_token, _ranked_dictionaries)\n            if base_token_match:\n                base_token_match['repeat_count'] = repeat_count\n                base_token_match['base_token'] = base_token\n                base_token_match['i'] = match.start()\n                base_token_match['j'] = match.start() + len(base_token_match['token']) - 1\n                base_token_match['repeated_seq'] = [\n                    {\n                        'i': match.start(),\n                        'j': match.start() + len(base_token_match['token']) - 1,\n                    },\n                ]\n                matches.append(base_token_match)\n\n    return matches\n\n", "idx": 1316}
{"namespace": "zxcvbn.matching.spatial_match", "completion": "    matches = []\n    for graph_name, graph in _graphs.items():\n        matches.extend(spatial_match_helper(password, graph, graph_name, _ranked_dictionaries))\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))\n\n", "idx": 1317}
{"namespace": "zxcvbn.matching.sequence_match", "completion": "    matches = []\n    length = len(password)\n    if length < 3:\n        return matches\n\n    # for each possible sequence type, find sequences in password\n    for sequence_name, sequence_space in [\n        ('lower', string.ascii_lowercase),\n        ('upper', string.ascii_uppercase),\n        ('digits', string.digits),\n        ('symbols', string.punctuation),\n    ]:\n        # for each possible length of sequence, find sequences in password\n        for seq_len in range(3, 13):\n            # for each possible starting point, search for sequences\n            for i in range(0, length - seq_len + 1):\n                # find the sequence\n                seq = password[i:i + seq_len]\n                # check if the sequence is in the dictionary\n                if seq in sequence_space:\n                    # find the index of the sequence in the dictionary\n                    seq_idx = sequence_space.index(seq)\n                    # find the index of the next item in the sequence\n                    j = i + seq_len\n                    while j < length and sequence_space.index(password[j:j + seq_len]) == seq_idx + 1:\n                        j += seq_len\n                    # check if the sequence is ascending or descending\n                    if j < length:\n                        delta = sequence_space.index(password[j:j + seq_len]) - seq_idx\n                    else:\n                        delta = 0\n                    if delta >= MAX_DELTA:\n                        ascending = True\n                    elif delta <= -MAX_DELTA:\n                        ascending = False\n                    else:\n                        continue\n                    # add the sequence to the list of matches\n                    matches.append({\n                        'pattern': 'sequence',\n                        'i': i,\n                        'j': j - 1,\n                        'token': password[i:j],\n                        'sequence_name': sequence_name,\n                        'sequence_space': sequence_space,\n                        'ascending': ascending,\n                    })", "idx": 1318}
{"namespace": "zxcvbn.matching.regex_match", "completion": "    matches = []\n    for regex_name, regex in _regexen.items():\n        match = regex.search(password)\n        if match:\n            i, j = match.span()\n            token = password[i:j]\n            matches.append({\n                'pattern': 'regex',\n                'token': token,\n                'i': i,\n                'j': j,\n                'regex_name': regex_name,\n                'regex': regex,\n                'match': match,\n            })\n\n    def sort_key(match):\n        return (match['i'], match['j'])\n\n    return sorted(matches, key=sort_key)\n\n", "idx": 1319}
{"namespace": "OpenSSL.rand.add", "completion": "    _lib.RAND_add(buffer, len(buffer), entropy)\n\n", "idx": 1320}
{"namespace": "asyncssh.kex.register_kex_alg", "completion": "    if alg in _kex_algs:\n        raise ValueError('Key exchange algorithm already registered')\n\n    _kex_algs.append(alg)\n    if default:\n        _default_kex_algs.append(alg)\n\n    _kex_handlers[alg] = (handler, hash_alg, args)\n\n", "idx": 1321}
{"namespace": "asyncssh.auth.get_supported_server_auth_methods", "completion": "    supported_methods = []\n    for method in _server_auth_handlers:\n        if _server_auth_handlers[method].supported(conn):\n            supported_methods.append(method)\n    return supported_methods\n\n", "idx": 1322}
{"namespace": "asyncssh.mac.get_mac", "completion": "    if mac_alg == b'none':\n        return _NullMAC(key, 0, None)\n\n    handler, hash_size, args = _mac_handler[mac_alg]\n    return handler(key, hash_size, *args)\n\n", "idx": 1323}
{"namespace": "asyncssh.auth_keys.SSHAuthorizedKeys.validate", "completion": "        if ca:\n            entries = self._ca_entries\n        elif key.is_certificate:\n            entries = self._x509_entries\n        else:\n            entries = self._user_entries\n\n        for entry in entries:\n            if entry.match_options(client_host, client_addr, cert_principals):\n                return entry.options\n\n        return None\n", "idx": 1324}
{"namespace": "asyncssh.saslprep.saslprep", "completion": "    return _stringprep(s, True, _map_saslprep, 'NFKC',\n                       [stringprep.in_table_c12,\n                        stringprep.in_table_c21,\n                        stringprep.in_table_c22,\n                        stringprep.in_table_c3,\n                        stringprep.in_table_c4,\n                        stringprep.in_table_c5,\n                        stringprep.in_table_c6,\n                        stringprep.in_table_c7,\n                        stringprep.in_table_c8,\n                        stringprep.in_table_c9], True)", "idx": 1325}
{"namespace": "asyncssh.asn1.der_decode", "completion": "    value, end = der_decode_partial(data)\n    if end < len(data):\n        raise ASN1DecodeError('Data contains unexpected bytes at end')\n\n    return value", "idx": 1326}
{"namespace": "asyncssh.packet.SSHPacket.check_end", "completion": "        if self._idx != self._len:\n            raise PacketDecodeError('Packet not fully consumed')\n", "idx": 1327}
{"namespace": "asyncssh.public_key.SSHKey.verify", "completion": "        if not sig:\n            return False\n\n        try:\n            sig_algorithm, sig_data = der_decode(sig,\n                                                 asn1Spec=Signature)\n        except (ASN1DecodeError, ValueError):\n            return False\n\n        if sig_algorithm not in self.all_sig_algorithms:\n            return False\n\n        return self.verify_ssh(data, sig_algorithm, sig_data)\n", "idx": 1328}
{"namespace": "asyncssh.public_key.SSHKey.convert_to_public", "completion": "        # Decrypt the private key\n        decrypted_key = self.decrypt()\n\n        # Create a new SSHKey object that contains only the public key\n        public_key = SSHKey(decrypted_key)\n\n        # Assign a comment and filename to the public key\n        public_key.set_comment(self.get_comment())\n        public_key.set_filename(self.get_filename())\n\n        return public_key\n", "idx": 1329}
{"namespace": "asyncssh.public_key.SSHKey.generate_x509_user_certificate", "completion": "        if not _x509_available: # pragma: no cover\n            raise KeyGenerationError('X.509 certificate generation '\n                                     'requires PyOpenSSL')\n\n        if not self.x509_algorithms:\n            raise KeyGenerationError('X.509 certificate generation not '\n                                     'supported for ' + self.get_algorithm() +\n                                     ' keys')\n\n        if not self.x509_algorithms:\n            raise KeyGenerationError('X.509 certificate generation not '\n                                     'supported for ' + self.get_algorithm() +\n                                     ' keys')\n\n        if not isinstance(subject, str):\n            raise TypeError('Subject must be a string')\n\n        if issuer is not None and not isinstance(issuer, str):\n            raise TypeError('Issuer must be a string')\n\n        if serial is not None and not isinstance(serial, int):\n            raise TypeError('Serial must be an integer')\n\n        if not isinstance(principals, (list, tuple)):\n            raise TypeError('Principals must be a list or tuple')\n\n        if not isinstance(hash_alg, (list, tuple)):\n            raise TypeError('Hash algorithm must be a list or tuple')\n\n        if not isinstance(comment, (list, tuple)):\n            raise TypeError('Comment must be a list or tuple')\n\n        if not hash_alg:\n            hash_alg = [self.default_x509_hash]\n\n        if not comment:\n            comment = [user_key.get_comment_bytes()]\n\n        if not purposes:\n            purposes = ['secureShellClient']\n\n        if not isinstance(purposes, (list, tuple)):\n            raise TypeError('Purposes must be a list or tuple')\n\n        if not principals:\n            principals = ['*']\n\n        if not issuer:\n            issuer = subject\n\n        if not serial:\n            serial = int(os.urandom(8).hex(), 16)\n\n        valid_after = _parse_time(valid_", "idx": 1330}
{"namespace": "asyncssh.misc.write_file", "completion": "    with open_file(filename, mode) as f:\n        return f.write(data)\n\n", "idx": 1331}
{"namespace": "diffprivlib.accountant.BudgetAccountant.__repr__", "completion": "        attributes = []\n        if self.epsilon != float(\"inf\"):\n            attributes.append(\"epsilon={}\".format(self.epsilon))\n        if self.delta != 1.0:\n            attributes.append(\"delta={}\".format(self.delta))\n        if self.slack != 0.0:\n            attributes.append(\"slack={}\".format(self.slack))\n        if len(self.spent_budget) > 0:\n            attributes.append(\"spent_budget={}\".format(self.spent_budget[:n_budget_max]))\n            if len(self.spent_budget) > n_budget_max:\n                attributes.append(\"...\")\n        return \"BudgetAccountant({})\".format(\", \".join(attributes))\n", "idx": 1332}
{"namespace": "diffprivlib.accountant.BudgetAccountant.check", "completion": "        if epsilon < 0 or delta < 0:\n            raise ValueError(\"Epsilon and delta must be non-negative.\")\n\n        if epsilon == 0 and delta == 0:\n            raise ValueError(\"Epsilon and delta must be non-zero.\")\n\n        if epsilon == float(\"inf\") and delta == float(\"inf\"):\n            raise ValueError(\"Epsilon and delta must be finite.\")\n\n        if epsilon == 0:\n            return True\n\n        if delta == 0:\n            return True\n\n        if epsilon == float(\"inf\"):\n            return True\n\n        if delta == float(\"inf\"):\n            return True\n\n        if epsilon > self.epsilon:\n            raise BudgetError(f\"Epsilon budget spend of {epsilon} not permissible; will exceed remaining epsilon budget of {self.epsilon}.\")\n\n        if delta > self.delta:\n            raise BudgetError(f\"Delta budget spend of {delta} not permissible; will exceed remaining delta budget of {self.delta}.\")\n\n        return True\n", "idx": 1333}
{"namespace": "diffprivlib.accountant.BudgetAccountant.spend", "completion": "        check_epsilon_delta(epsilon, delta)\n        if self.epsilon == float(\"inf\") and self.delta == 1:\n            return self\n\n        if 0 < epsilon < self.__min_epsilon:\n            raise ValueError(f\"Epsilon must be at least {self.__min_epsilon} if non-zero, got {epsilon}.\")\n\n        spent_budget = self.spent_budget + [(epsilon, delta)]\n\n        if Budget(self.epsilon, self.delta) >= self.total(spent_budget=spent_budget):\n            self.__spent_budget = spent_budget\n            return self\n\n        raise BudgetError(f\"Privacy spend of ({epsilon},{delta}) not permissible; will exceed remaining privacy budget.\"\n                          f\" Use {self.__class__.__name__}.{self.remaining.__name__}() to check remaining budget.\")\n", "idx": 1334}
{"namespace": "diffprivlib.accountant.BudgetAccountant.load_default", "completion": "        if accountant is None:\n            if BudgetAccountant._default is None:\n                BudgetAccountant._default = BudgetAccountant()\n            return BudgetAccountant._default\n        elif isinstance(accountant, BudgetAccountant):\n            return accountant\n        else:\n            raise TypeError(\"The supplied accountant is not an instance of the BudgetAccountant class.\")\n", "idx": 1335}
{"namespace": "diffprivlib.accountant.BudgetAccountant.set_default", "completion": "        BudgetAccountant._default = self\n        return self\n", "idx": 1336}
{"namespace": "diffprivlib.accountant.BudgetAccountant.pop_default", "completion": "        if BudgetAccountant._default is None:\n            return None\n\n        default = BudgetAccountant._default\n        BudgetAccountant._default = None\n        return default\n\n", "idx": 1337}
{"namespace": "diffprivlib.validation.clip_to_bounds", "completion": "    if not isinstance(array, np.ndarray):\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\n    if array.ndim != 2:\n        raise ValueError(f\"input array must be 2-dimensional, got {array.ndim} dimensions.\")\n    if not isinstance(bounds, tuple):\n        raise TypeError(f\"Bounds must be specified as a tuple of (min, max), got {type(bounds)}.\")\n    if len(bounds) != 2:\n        raise ValueError(f\"Bounds must be specified as a tuple of (min, max), got {len(bounds)} elements.\")\n\n    lower, upper = bounds\n\n    if not isinstance(lower, Real) or not isinstance(upper, Real):\n        raise TypeError(f\"Each bound must be numeric, got {type(lower)} and {type(upper)}.\")\n    if lower > upper:\n        raise ValueError(f\"For each bound, lower bound must be smaller than upper bound, got {lower} and {upper}.\")\n\n    lower = np.asarray(lower)\n    upper = np.asarray(upper)\n\n    if lower.shape != upper.shape:\n        raise ValueError(f\"lower and upper bounds must be the same shape array, got {lower.shape} and {upper.shape}.\")\n    if lower.ndim > 1:\n        raise ValueError(f\"lower and upper bounds must be scalar or a 1-dimensional array, got {lower.ndim} dimensions.\")\n\n    if lower.size == 1:\n        lower = np.ones(array.shape[1], dtype=lower.dtype) * lower.item()\n        upper = np.ones(array.shape[1], dtype=upper.dtype) * upper.item()\n\n    return np.clip(array, lower, upper)\n\n", "idx": 1338}
{"namespace": "diffprivlib.models.naive_bayes.GaussianNB._update_mean_variance", "completion": "        if sample_weight is not None:\n            warn_unused_args(\"sample_weight\")\n\n        if n_noisy is None:\n            n_noisy = n_past\n\n        n_total = n_past + n_noisy\n\n        # Compute the new mean\n        total_mu = (n_past * mu + n_noisy * X.mean(axis=0)) / n_total\n\n        # Compute the new variance\n        total_var = (n_past * var + n_noisy * np.var(X, axis=0) + n_past * n_noisy * np.sum((X - mu) * (X - total_mu), axis=0)) / n_total\n\n        return total_mu, total_var\n", "idx": 1339}
{"namespace": "diffprivlib.models.naive_bayes.GaussianNB._noisy_class_counts", "completion": "        # Get the unique class labels\n        unique_classes = np.unique(y)\n\n        # Get the number of unique class labels\n        num_unique_classes = len(unique_classes)\n\n        # Get the number of samples in the dataset\n        num_samples = len(y)\n\n        # Get the number of noisy samples to add to each class label\n        num_noisy_samples = int(self.epsilon * num_samples)\n\n        # Get the number of bits of noise to add to each class label\n        num_noisy_bits = int(self.epsilon * num_unique_classes)\n\n        # Get the number of noisy bits to add to each class label\n        num_noisy_bits_per_class = num_noisy_bits // num_unique_classes\n\n        # Get the number of noisy bits to add to each class label\n        num_noisy_bits_remainder = num_noisy_bits % num_unique_classes\n\n        # Get the number of noisy samples to add to each class label\n        num_noisy_samples_per_class = num_noisy_samples // num_unique_classes\n\n        # Get the number of noisy samples to add to each class label\n        num_noisy_samples_remainder = num_noisy_samples % num_unique_classes\n\n        # Get the number of noisy samples to add to each class label\n        num_noisy_samples_per_class_remainder = num_noisy_samples_remainder // num_unique_classes\n\n        # Get the number of noisy samples to add to each class label\n        num_noisy_samples_per_class_remainder_mod = num_noisy_samples_per_class_remainder % num_unique_classes\n\n        # Get the number of noisy samples to add to each class label\n        num_noisy_samples_per_class_remainder_div = num_noisy_samples_per_class_remainder // num_unique_classes\n\n        # Get the number of noisy samples to add to each class label\n        num_noisy_samples_per_", "idx": 1340}
{"namespace": "diffprivlib.models.standard_scaler._incremental_mean_and_var", "completion": "    random_state = check_random_state(random_state)\n    n_samples = X.shape[0]\n    if X.dtype.kind == 'c':\n        X = np.asarray(X, dtype=np.float64)\n    if last_mean is None:\n        last_mean = np.zeros(n_samples)\n    if last_variance is None:\n        last_variance = np.zeros(n_samples)\n    if last_sample_count is None:\n        last_sample_count = np.zeros(n_samples)\n    # Check for the edge case that the dataset is empty\n    if n_samples == 0:\n        return last_mean, last_variance, last_sample_count\n    # Check for the edge case that the dataset has a single sample\n    if n_samples == 1:\n        return np.array(X, copy=False, ndmin=1), np.array(0, copy=False), np.array(1, copy=False)\n    # Check for the edge case that the dataset has two samples\n    if n_samples == 2:\n        difference = X - last_mean\n        return (last_mean + X) / 2, (last_variance + difference * difference) / 2, np.array(2, copy=False)\n    # Check for the edge case that the dataset has three samples\n    if n_samples == 3:\n        difference1 = X[0] - last_mean\n        difference2 = X[1] - last_mean\n        return (last_mean + X[0] + X[1]) / 3, (last_variance + difference1 * difference1 + difference2 * difference2) / 3, np.array(3, copy=False)\n    # Check for the edge case that the dataset has four samples\n    if n_samples == 4:\n        difference1 = X[0] - last_mean\n        difference2 = X[1] - last_mean\n        difference3 = X[2] - last_mean\n        return (last_mean + X[0] + X[1] + X[2", "idx": 1341}
{"namespace": "diffprivlib.models.linear_regression.LinearRegression.fit", "completion": "        self.accountant.check(self.epsilon, 0)\n\n        X, y, X_offset, y_offset, X_scale = _preprocess_data(X, y, self.fit_intercept, self.bounds_X, self.bounds_y,\n                                                             self.copy_X, self.check_input, self.random_state)\n\n        if sample_weight is not None:\n            warnings.warn(\"Sample weights are not supported in diffprivlib.  Continuing without setting.\")\n\n        if self.fit_intercept:\n            X = np.hstack([X, np.ones((X.shape[0], 1))])\n\n        self.n_features_in_ = X.shape[1]\n\n        if self.n_targets_ > 1:\n            self.coef_ = []\n            self.intercept_ = []\n            for k in range(self.n_targets_):\n                self.coef_.append(np.zeros(X.shape[1]))\n                self.intercept_.append(0.)\n        else:\n            self.coef_ = np.zeros(X.shape[1])\n            self.intercept_ = 0.\n\n        self.n_iter_ = 0\n\n        if self.n_targets_ > 1:\n            for k in range(self.n_targets_):\n                self.coef_[k], self.intercept_[k] = self._fit(X, y[:, k:k + 1], self.epsilon, self.bounds_X,\n                                                              self.bounds_y, self.fit_intercept, self.copy_X,\n                                                              self.check_input, self.random_state)\n        else:\n            self.coef_, self.intercept_ = self._fit(X, y, self.epsilon, self.bounds_X, self.bounds_y, self.fit_intercept,\n                                                    self.copy_X, self.check_input, self.random_state)\n\n        self.n_iter_ += 1", "idx": 1342}
{"namespace": "diffprivlib.models.k_means.KMeans.fit", "completion": "        self.accountant.check(self.epsilon, 0)\n\n        if self.bounds is None:\n            warnings.warn(\"Bounds have not been specified and will be calculated on the data provided. This will \"\n                          \"result in additional privacy leakage. It is recommended to provide bounds as input to the \"\n                          \"mechanism.\", PrivacyLeakWarning)\n\n        if self.bounds is not None:\n            self.bounds_processed = self._process_bounds(self.bounds)\n\n        self._n_threads = self._process_n_threads(self._n_threads)\n\n        self.cluster_centers_, self.labels_, self.inertia_, self.n_iter_ = self._k_means(X, self.n_clusters,\n                                                                                         self.epsilon,\n                                                                                         self.bounds_processed,\n                                                                                         self.random_state,\n                                                                                         self._n_threads)\n\n        self.accountant.spend(self.epsilon, 0)\n\n        return self\n", "idx": 1343}
{"namespace": "diffprivlib.models.forest._FittingTree.__getstate__", "completion": "        return dict(\n            max_depth=self.max_depth,\n            node_count=self.node_count,\n            nodes=self.nodes,\n            values=self.values,\n        )\n", "idx": 1344}
{"namespace": "diffprivlib.models.forest._FittingTree.fit", "completion": "        # Check if the tree has been built\n        if not hasattr(self, \"nodes\"):\n            raise ValueError(\"Tree has not been built. Please call build() first.\")\n\n        # Check if the input data is valid\n        X, y = self._validate_data(X, y, multi_output=False, accept_sparse=\"csc\", dtype=None,\n                                   y_numeric=True, ensure_2d=True, allow_nd=False,\n                                   ensure_min_samples=1, ensure_min_features=1, y_order=\"c\")\n\n        # Initialize the values for each leaf\n        self.values_ = np.empty(shape=(self.node_count, self.n_classes_), dtype=np.float64)\n\n        # Populate the values for the real leaves\n        self._fit_leaves(X, y)\n\n        # Populate the values for the empty leaves\n        self._fit_empty_leaves()\n\n        # Assign the calculated values to the tree\n        self._set_tree()\n\n        return self\n", "idx": 1345}
{"namespace": "diffprivlib.tools.histograms.histogram", "completion": "    warn_unused_args(unused_args)\n\n    if accountant is None:\n        accountant = BudgetAccountant()\n\n    # Check if epsilon is negative or NaN.\n    if epsilon < 0:\n        raise ValueError(\"epsilon must be non-negative\")\n    accountant.spend(epsilon, 0)\n\n    # Check if bins is negative or NaN.\n    if bins is None:\n        raise ValueError(\"bins must be non-negative\")\n    if bins < 0:\n        raise ValueError(\"bins must be non-negative\")\n\n    # Check if range is negative or NaN.\n    if range is None:\n        raise ValueError(\"range must be non-negative\")\n    if range < 0:\n        raise ValueError(\"range must be non-negative\")\n\n    # Check if weights is negative or NaN.\n    if weights is None:\n        raise ValueError(\"weights must be non-negative\")\n    if weights < 0:\n        raise ValueError(\"weights must be non-negative\")\n\n    # Check if density is negative or NaN.\n    if density is None:\n        raise ValueError(\"density must be non-negative\")\n    if density < 0:\n        raise ValueError(\"density must be non-negative\")\n\n    # Check if random_state is negative or NaN.\n    if random_state is None:\n        raise ValueError(\"random_state must be non-negative\")\n    if random_state < 0:\n        raise ValueError(\"random_state must be non-negative\")\n\n    # Check if sample is negative or NaN.\n    if sample is None:\n        raise ValueError(\"sample must be non-negative\")\n    if sample < 0:\n        raise ValueError(\"sample must be non-negative\")\n\n    # Check if epsilon is negative or NaN.\n    if epsilon is None:\n        raise ValueError(\"epsilon must be non-negative\")\n    if epsilon < 0:\n        raise ValueError(\"epsilon must be non-negative\")\n\n    # Check if bins is negative or NaN.\n    if bins is None:\n        raise Value", "idx": 1346}
{"namespace": "diffprivlib.tools.histograms.histogram2d", "completion": "    warn_unused_args(unused_args)\n\n    random_state = check_random_state(random_state)\n\n    accountant = BudgetAccountant.load_default(accountant)\n    accountant.check(epsilon, 0)\n\n    # Range only required if bin edges not specified\n    if range is None:\n        warnings.warn(\"Range parameter has not been specified (or has missing elements). Falling back to taking \"\n                      \"range from the data.\\n \"\n                      \"To ensure differential privacy, and no additional privacy leakage, the range must be \"\n                      \"specified for each dimension independently of the data (i.e., using domain knowledge).\",\n                      PrivacyLeakWarning)\n\n    hist, xedges, yedges = np.histogram2d(array_x, array_y, bins=bins, range=range, weights=weights, density=None)\n\n    dp_mech = GeometricTruncated(epsilon=epsilon, sensitivity=1, lower=0, upper=maxsize, random_state=random_state)\n\n    dp_hist = np.zeros_like(hist)\n    iterator = np.nditer(hist, flags=['multi_index'])\n\n    while not iterator.finished:\n        dp_hist[iterator.multi_index] = dp_mech.randomise(int(iterator[0]))\n        iterator.iternext()\n\n    dp_hist = dp_hist.astype(float, casting='safe')\n\n    if density:\n        # calculate the probability density function\n        dims = len(dp_hist.shape)\n        dp_hist_sum = dp_hist.sum()\n        for i in np.arange(dims):\n            shape = np.ones(dims, int)\n            shape[i] = dp_hist.shape[i]\n            # noinspection PyUnresolvedReferences\n            dp_hist = dp_hist / np.diff(xedges[i]).reshape(shape) / np", "idx": 1347}
{"namespace": "diffprivlib.tools.utils.nanmean", "completion": "    warn_unused_args(unused_args)\n\n    return _mean(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                 random_state=random_state, accountant=accountant, nan=True)\n\n", "idx": 1348}
{"namespace": "diffprivlib.tools.utils.var", "completion": "    warn_unused_args(unused_args)\n\n    return _var(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                 random_state=random_state, accountant=accountant, nan=False)\n\n", "idx": 1349}
{"namespace": "diffprivlib.tools.utils.nanvar", "completion": "    warn_unused_args(unused_args)\n\n    return _var(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)\n\n", "idx": 1350}
{"namespace": "diffprivlib.tools.utils.std", "completion": "    warn_unused_args(unused_args)\n\n    return _std(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=False)\n\n", "idx": 1351}
{"namespace": "diffprivlib.tools.utils.nanstd", "completion": "    warn_unused_args(unused_args)\n\n    return _std(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)\n\n", "idx": 1352}
{"namespace": "diffprivlib.tools.utils.sum", "completion": "    warn_unused_args(unused_args)\n\n    return _sum(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=False)\n\n", "idx": 1353}
{"namespace": "diffprivlib.tools.utils.nansum", "completion": "    warn_unused_args(unused_args)\n\n    return _sum(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)\n\n", "idx": 1354}
{"namespace": "diffprivlib.tools.quantiles.quantile", "completion": "    warn_unused_args(unused_args)\n\n    if accountant is None:\n        from diffprivlib.accountant import BudgetAccountant\n        accountant = BudgetAccountant()\n\n    if random_state is None:\n        random_state = np.random.RandomState()\n    elif isinstance(random_state, int):\n        random_state = np.random.RandomState(random_state)\n\n    if bounds is None:\n        bounds = (np.min(array), np.max(array))\n\n    if np.isscalar(quant):\n        quant = [quant]\n\n    quantiles = np.ravel(quant)\n\n    # Check if quantiles are in the unit interval\n    if not np.all((0 <= quantiles) & (quantiles <= 1)):\n        raise ValueError(\"Quantiles must be in the unit interval [0, 1].\")\n\n    # Check if quantiles are in the unit interval\n    if not np.all((0 <= quantiles) & (quantiles <= 1)):\n        raise ValueError(\"Quantiles must be in the unit interval [0, 1].\")\n\n    # Check if array is a single-dimensional array\n    if not np.isscalar(quantiles):\n        if not np.ndim(array) == 1:\n            raise ValueError(\"Array must be a single-dimensional array.\")\n\n    # Check if array is a single-dimensional array\n    if not np.isscalar(quantiles):\n        if not np.ndim(array) == 1:\n            raise ValueError(\"Array must be a single-dimensional array.\")\n\n    # Check if array is a single-dimensional array\n    if not np.isscalar(quantiles):\n        if not np.ndim(array) == 1:\n            raise ValueError(\"Array must be a single-dimensional array.\")\n\n    # Check if array is a single-dimensional array\n    if not np.isscalar(quantiles):\n        if not np.ndim(array) == 1:\n            raise ValueError(\"Array must be a single-dimensional array.\")\n\n    # Check if array is a", "idx": 1355}
{"namespace": "diffprivlib.tools.quantiles.percentile", "completion": "    from diffprivlib.utils import check_random_state\n    from diffprivlib.utils import PrivacyLeakWarning\n    from diffprivlib.accountant import BudgetAccountant\n    from diffprivlib.validation import clip_to_bounds\n    from diffprivlib.validation import check_bounds\n    warn_unused_args(unused_args)\n\n    random_state = check_random_state(random_state)\n\n    if bounds is None:\n        warnings.warn(\"Bounds have not been specified and will be calculated on the data provided. This will \"\n                      \"result in additional privacy leakage. To ensure differential privacy and no additional \"\n                      \"privacy leakage, specify bounds for each dimension.\", PrivacyLeakWarning)\n        bounds = (np.min(array), np.max(array))\n\n    percent = np.ravel(percent)\n\n    if np.any(percent < 0) or np.any(percent > 100):\n        raise ValueError(\"Percentiles must be in the range [0, 100].\")\n\n    if len(percent) > 1:\n        return np.array([percentile(array, p_i, epsilon=epsilon / len(percent), bounds=bounds, axis=axis,\n                                    keepdims=keepdims, random_state=random_state, accountant=accountant) for p_i in percent])\n\n    # Dealing with a single percentile from now on\n    percent = percent.item()\n\n    if axis is not None or keepdims:\n        return _wrap_axis(quantile, array, quant=percent / 100, epsilon=epsilon, bounds=bounds, axis=axis,\n                          keepdims=keepdims, random_state=random_state, accountant=accountant)\n\n    # Dealing with a scalar output from now on\n    bounds = check_bounds(bounds, shape=0, min_separation=1e-5)\n\n    accountant = BudgetAccountant.load_default(accountant)\n    accountant.check(epsilon, 0)\n\n    # Let's ravel array to", "idx": 1356}
{"namespace": "diffprivlib.mechanisms.base.bernoulli_neg_exp", "completion": "    if gamma < 0:\n        raise ValueError(\"gamma must be non-negative\")\n\n    random_state = check_random_state(random_state)\n    return int(random_state.binomial(1, 1 - np.exp(-gamma)))\n\n", "idx": 1357}
{"namespace": "discord.utils.snowflake_time", "completion": "    timestamp = ((id >> 22) + DISCORD_EPOCH) / 1000\n    return datetime.datetime.fromtimestamp(timestamp, tz=datetime.timezone.utc)\n\n", "idx": 1358}
{"namespace": "discord.utils.time_snowflake", "completion": "    dt = dt.astimezone(datetime.timezone.utc)\n    discord_timestamp = int(dt.timestamp() * 1000 - DISCORD_EPOCH)\n    return (discord_timestamp << 22) + (2 ** 22 - 1 if high else 0)\n\n", "idx": 1359}
{"namespace": "discord.utils.resolve_invite", "completion": "    if isinstance(invite, Invite):\n        return ResolvedInvite(invite.code, invite.guild.id if invite.guild else None)\n\n    if isinstance(invite, str):\n        if invite.startswith('https://discord.gg/'):\n            return ResolvedInvite(invite[22:], None)\n        return ResolvedInvite(invite, None)\n\n    raise TypeError(f'invite must be an Invite or a string, not {type(invite).__name__}')\n\n", "idx": 1360}
{"namespace": "discord.utils.resolve_annotation", "completion": "    if annotation is None:\n        return type(None)\n\n    if isinstance(annotation, str):\n        annotation = ForwardRef(annotation)\n\n    if localns is None:\n        localns = globalns\n\n    if cache is None:\n        cache = {}\n\n    return evaluate_annotation(annotation, globalns, localns, cache)\n\n", "idx": 1361}
{"namespace": "discord.ext.tasks.loop", "completion": "    def decorator(func: LF) -> Loop[LF]:\n        return Loop(\n            func,\n            seconds=seconds,\n            minutes=minutes,\n            hours=hours,\n            time=time,\n            count=count,\n            reconnect=reconnect,\n        )\n\n    return decorator", "idx": 1362}
{"namespace": "barf.analysis.gadgets.classifier.GadgetClassifier.classify", "completion": "        # Classify gadget.\n        try:\n            classified_gadgets = self._classify(gadget)\n\n        # If an error occurs during classification, print the error message and traceback.\n        except Exception as e:\n            print(\"[*] Error classifying gadget:\")\n            print(\"    - Gadget: %s\" % gadget)\n            print(\"    - Error: %s\" % e)\n            print(\"    - Traceback: %s\" % e.__traceback__)\n\n            classified_gadgets = []\n\n        # Sort gadgets by their string representation.\n        classified_gadgets.sort(key=lambda g: str(g))\n\n        return classified_gadgets\n", "idx": 1363}
{"namespace": "barf.analysis.gadgets.finder.GadgetFinder.find", "completion": "        self._max_bytes = byte_depth\n        self._instrs_depth = instrs_depth\n\n        if self._architecture == ARCH_X86:\n            return self._find_x86(start_address, end_address)\n        elif self._architecture == ARCH_ARM:\n            return self._find_arm(start_address, end_address)\n", "idx": 1364}
{"namespace": "barf.core.reil.parser.ReilParser.parse", "completion": "        parsed_instrs = []\n\n        for instr in instrs:\n            instr = instr.lower()\n\n            if instr in self._cache:\n                parsed_instrs.append(copy.deepcopy(self._cache[instr]))\n            else:\n                try:\n                    parsed_instr = instruction.parseString(instr)\n\n                    self._cache[instr] = parsed_instr[0]\n\n                    parsed_instrs.append(copy.deepcopy(parsed_instr[0]))\n                except Exception as e:\n                    logger.error(\"Error parsing instruction: %s\", instr)\n                    logger.error(e)\n\n        return parsed_instrs", "idx": 1365}
{"namespace": "barf.core.smt.smtfunction.zero_extend", "completion": "    if isinstance(s, BitVec):\n        if s.size == size:\n            return s\n        else:\n            return BitVec(s.name, size)\n    elif isinstance(s, Constant):\n        if s.size == size:\n            return s\n        else:\n            return BitVec(s.name, size)\n    else:\n        raise Exception(\"Invalid type for zero-extension: %s\" % type(s))\n\n", "idx": 1366}
{"namespace": "barf.core.smt.smtfunction.extract", "completion": "    assert type(s) in (Constant, BitVec) and offset >= 0 and size >= 0 and offset + size <= s.size\n\n    if offset == 0 and size == s.size:\n        return s\n\n    return BitVec(size, \"(_ extract {})\".format(offset + size - 1, offset), s)\n\n", "idx": 1367}
{"namespace": "barf.core.smt.smtfunction.ite", "completion": "    assert type(cond) is bool\n    assert type(true) in (Constant, BitVec)\n    assert type(false) in (Constant, BitVec)\n\n    if true.size != size or false.size != size:\n        true = zero_extend(true, size)\n        false = zero_extend(false, size)\n\n    return BitVec(size, \"(_ bv{0} {1})\".format(cond, size), true, false)", "idx": 1368}
{"namespace": "barf.core.smt.smtfunction.concat", "completion": "    assert type(size) is int\n    assert all(type(arg) is BitVec for arg in args)\n\n    if len(args) == 1:\n        return args[0]\n\n    return BitVec(size, \"concat\", *args)", "idx": 1369}
{"namespace": "barf.core.smt.smtsymbol.BitVecArray.declaration", "completion": "        return \"(declare-fun {} () (Array (_ BitVec {}) (_ BitVec {})))\".format(self.name, self.key_size, self.value_size)\n", "idx": 1370}
{"namespace": "barf.arch.translator.InstructionTranslator.translate", "completion": "        # TODO: Implement this function.\n        #\n        # 1. Create a ReilBuilder instance.\n        # 2. Call the appropriate method of the ReilBuilder instance to translate the instruction.\n        # 3. Return the ReilBuilder instance.\n        #\n        # Hint: Use the following methods of the ReilBuilder class:\n        #\n        # 1. __init__(self, arch_name)\n        # 2. add()\n        # 3. nop()\n        # 4. mov()\n        # 5. add()\n        # 6. sub()\n        # 7. mul()\n        # 8. div()\n        # 9. and_()\n        # 10. or_()\n        # 11. xor()\n        # 12. not_()\n        # 13. sll()\n        # 14. srl()\n        # 15. sra()\n        # 16. eq()\n        # 17. neq()\n        # 18. lt()\n        # 19. gt()\n        # 20. lte()\n        # 21. gte()\n        # 22. jcc()\n        # 23. j()\n        # 24. label()\n        # 25. comment()\n        # 26. end()\n        #\n        # Hint: Use the following methods of the ReilBuilder class to access the following methods:\n        #\n        # 1. __init__(self, arch_name)\n        # 2. add()\n        # 3. nop()\n        # 4. mov()\n        # 5. add()\n        # 6. sub()\n        # 7. mul()\n        # 8. div()\n        # 9. and_()\n        # 10. or_()\n        # 11. xor()\n        # 12. not_()\n        # 13. sll()\n        # 14. srl()\n        # 15. sra()", "idx": 1371}
{"namespace": "barf.arch.emulator.Emulator.load_binary", "completion": "        # Read the first 4 bytes of the file.\n        signature = binary[0:4]\n\n        # Check if it is an ELF file.\n        if signature == b'\\x7fELF':\n            self._load_binary_elf(binary)\n        # Check if it is a PE file.\n        elif signature == b'MZ':\n            self._load_binary_pe(binary)\n        else:\n            raise Exception(\"Unknown file format.\")\n", "idx": 1372}
{"namespace": "barf.arch.arm.parser.ArmParser.parse", "completion": "        instr = instr.lower()\n\n        if instr in self._cache:\n            return copy.deepcopy(self._cache[instr])\n\n        try:\n            parsed_instr = instruction.parseString(instr)[0]\n        except Exception as e:\n            logger.error(\"Failed to parse instruction: %s\" % instr)\n            logger.error(\"Exception: %s\" % e)\n            return None\n\n        self._cache[instr] = parsed_instr\n\n        return copy.deepcopy(parsed_instr)", "idx": 1373}
{"namespace": "barf.arch.x86.parser.X86Parser.parse", "completion": "        instr = instr.lower()\n\n        if instr in self._cache:\n            return copy.deepcopy(self._cache[instr])\n\n        try:\n            instr_parsed = instruction.parseString(instr)\n\n            instr_parsed = instr_parsed[0]\n\n            self._cache[instr] = instr_parsed\n\n            return copy.deepcopy(instr_parsed)\n\n        except Exception as e:\n            logger.error(\"Failed to parse instruction: %s\" % instr)\n            logger.error(e)\n\n            return None", "idx": 1374}
{"namespace": "faker.utils.text.slugify", "completion": "    if allow_dots:\n        value = _re_pattern_allow_dots.sub(\"\", value)\n    else:\n        value = _re_pattern.sub(\"\", value)\n\n    if not allow_unicode:\n        value = unicodedata.normalize(\"NFKD\", value).encode(\"ascii\", \"ignore\").decode(\"utf-8\")\n\n    value = _re_spaces.sub(\"-\", value).strip(\"-_\")\n\n    return value", "idx": 1375}
{"namespace": "faker.utils.checksums.calculate_luhn", "completion": "    checksum = luhn_checksum(partial_number * 10)\n    return checksum if checksum == 0 else 10 - checksum\n\n", "idx": 1376}
{"namespace": "faker.utils.distribution.choices_distribution_unique", "completion": "    if random is None:\n        random = mod_random\n\n    if length < 0:\n        raise ValueError(\"length must be non-negative\")\n\n    if length == 0:\n        return []\n\n    if length == 1:\n        return [random.choices(a, p)[0]]\n\n    if p is None:\n        p = [1 / len(a)] * len(a)\n\n    if len(a) != len(p):\n        raise ValueError(\"a and p must have the same length\")\n\n    if sum(p) != 1.0:\n        raise ValueError(\"sum of p must be 1.0\")\n\n    if length > len(a):\n        raise ValueError(\"length must be less than or equal to len(a)\")\n\n    # We need to generate a cumulative distribution function from the probabilities\n    # to use with random.random().\n    cdf = list(cumsum(p))\n\n    # We need to make sure that the last element of the cdf is 1.0\n    cdf[-1] = 1.0\n\n    # We need to make sure that the cdf is strictly increasing\n    if any(x2 <= x1 for x1, x2 in zip(cdf, cdf[1:])):\n        raise ValueError(\"p must be a valid probability distribution\")\n\n    # We need to make sure that the cdf is a valid probability distribution\n    if any(x < 0 for x in cdf):\n        raise ValueError(\"p must be a valid probability distribution\")\n\n    # We need to make sure that the cdf is a valid probability distribution\n    if any(x > 1 for x in cdf):\n        raise ValueError(\"p must be a valid probability distribution\")\n\n    # We need to make sure that the cdf is a valid probability distribution\n    if any(x != 0 for x in cdf[:-1]) and any(x == 0 for x in cdf[1:]):\n        raise ValueError(\"p must be a valid probability distribution\")\n\n    # We need to make sure that the cdf is a valid probability distribution\n    if any(x", "idx": 1377}
{"namespace": "faker.utils.loading.find_available_locales", "completion": "    available_locales = []\n\n    for provider in providers:\n        provider_module = import_module(provider)\n        if hasattr(provider_module, \"is_localized\"):\n            if provider_module.is_localized():\n                if hasattr(provider_module, \"get_languages\"):\n                    available_locales.extend(provider_module.get_languages())\n\n    return sorted(available_locales)", "idx": 1378}
{"namespace": "faker.utils.loading.find_available_providers", "completion": "    available_providers = set()\n\n    for module in modules:\n        path = get_path(module)\n        if getattr(sys, \"_MEIPASS\", False):\n            # PyInstaller\n            path = Path(path)\n            providers = [\n                f\"{module.__package__}.{file.parent.name}\"\n                for file in path.glob(\"*/__init__.py\")\n            ]\n        else:\n            providers = [\n                f\"{module.__package__}.{name}\"\n                for _, name, is_pkg in pkgutil.iter_modules([str(path)])\n                if not is_pkg and name != \"__pycache__\"\n            ]\n        available_providers.update(providers)\n    return sorted(available_providers)", "idx": 1379}
{"namespace": "faker.providers.credit_card.Provider._generate_number", "completion": "        # Initialize the number with the prefix\n        number = prefix\n\n        # Generate random digits to fill the remaining length\n        number += self.numerify(\"\".join([\"#\" for i in range(length - len(prefix))]))\n\n        # Calculate the check digit using the Luhn algorithm\n        check_digit = self._calculate_check_digit(number)\n\n        # Append the check digit to the number\n        number += check_digit\n\n        return number\n", "idx": 1380}
{"namespace": "faker.decode.unidecode", "completion": "    # Iterate over each character in the input string\n    for i in range(len(txt)):\n        # Check if the character is in the codes dictionary\n        if txt[i] in codes:\n            # Replace the character with its closest ASCII equivalent\n            txt = txt.replace(txt[i], codes[txt[i]])\n\n    return txt", "idx": 1381}
{"namespace": "dash.fingerprint.build_fingerprint", "completion": "    file_path, file_name = path.rsplit(\"/\", 1)\n    file_name, file_ext = file_name.rsplit(\".\", 1)\n    file_name = version_clean.sub(\"_\", str(version))\n    file_name = file_name.replace(\"__\", \"_\")\n    file_name = file_name.strip(\"_\")\n    file_name = file_name.replace(\" \", \"_\")\n    file_name = file_name.replace(\"-\", \"_\")\n    file_name = file_name.replace(\"__\", \"_\")\n    file_name = file_name.lower()\n    fingerprint = f\"{file_path}/{file_name}.v{hash_value}.{file_ext}\"\n    return fingerprint\n\n", "idx": 1382}
{"namespace": "dash.fingerprint.check_fingerprint", "completion": "    if cache_regex.match(path):\n        path_parts = path.split(\"/\")\n        filename, extension = path_parts[-1].split(\".\", 1)\n        file_path = \"/\".join(path_parts[:-1] + [filename])\n        return file_path, True\n    else:\n        return path, False", "idx": 1383}
{"namespace": "dash._configs.pages_folder_config", "completion": "    if use_pages:\n        if not os.path.isdir(pages_folder):\n            raise exceptions.InvalidConfig(\n                f\"{name} pages folder `{pages_folder}` does not exist.\"\n            )\n        return pages_folder\n    return None\n\n", "idx": 1384}
{"namespace": "dash._grouping.flatten_grouping", "completion": "    # If schema is not provided, use the input grouping as the schema.\n    if schema is None:\n        schema = grouping\n\n    # If the schema is a scalar value, return the input grouping as a list.\n    if not isinstance(schema, (list, tuple, dict)):\n        return [grouping]\n\n    # If the schema is a list or tuple, return the input grouping as a list.\n    if isinstance(schema, (list, tuple)):\n        return list(grouping)\n\n    # If the schema is a dict, return a list of the values in the input grouping.\n    if isinstance(schema, dict):\n        return list(grouping.values())\n\n", "idx": 1385}
{"namespace": "dash._grouping.make_grouping_by_index", "completion": "    if isinstance(schema, (tuple, list)):\n        return [\n            make_grouping_by_index(group_schema, group_values)\n            for group_schema, group_values in zip(schema, flat_values)\n        ]\n\n    if isinstance(schema, dict):\n        return {\n            k: make_grouping_by_index(group_schema, group_values)\n            for k, (group_schema, group_values) in zip(schema, flat_values)\n        }\n\n    return flat_values[0]\n\n", "idx": 1386}
{"namespace": "dash._grouping.map_grouping", "completion": "    if isinstance(grouping, (tuple, list)):\n        return [map_grouping(fn, el) for el in grouping]\n\n    if isinstance(grouping, dict):\n        return {k: map_grouping(fn, v) for k, v in grouping.items()}\n\n    return fn(grouping)\n\n", "idx": 1387}
{"namespace": "dash._grouping.validate_grouping", "completion": "    if full_schema is None:\n        full_schema = schema\n\n    if isinstance(schema, (tuple, list)):\n        SchemaTypeValidationError.check(grouping, full_schema, path, (tuple, list))\n        SchemaLengthValidationError.check(grouping, full_schema, path, len(schema))\n        for i, (group_el, schema_el) in enumerate(zip(grouping, schema)):\n            validate_grouping(group_el, schema_el, full_schema, path + (i,))\n\n    elif isinstance(schema, dict):\n        SchemaTypeValidationError.check(grouping, full_schema, path, dict)\n        SchemaKeysValidationError.check(grouping, full_schema, path, set(schema.keys()))\n        for k, (group_el, schema_el) in zip(schema, grouping.items()):\n            validate_grouping(group_el, schema_el, full_schema, path + (k,))\n\n    else:\n        SchemaTypeValidationError.check(grouping, full_schema, path, schema)\n\n", "idx": 1388}
{"namespace": "dash._get_paths.app_get_relative_path", "completion": "    if requests_pathname == \"/\" and not path:\n        return \"/\"\n    if not requests_pathname != \"/\" and not path:\n        return requests_pathname\n    if not path.startswith(\"/\"):\n        raise exceptions.UnsupportedRelativePath(\n            \"The given path is not a relative path: {}\".format(path)\n        )\n    return \"/\".join([requests_pathname.rstrip(\"/\"), path.lstrip(\"/\")])\n\n", "idx": 1389}
{"namespace": "dash._get_paths.app_strip_relative_path", "completion": "    if requests_pathname != \"/\" and path.startswith(requests_pathname.rstrip(\"/\") + \"/\"):\n        return path[len(requests_pathname.rstrip(\"/\")) :]\n    if requests_pathname.endswith(\"/\") and path.startswith(requests_pathname.rstrip(\"/\")):\n        return path[len(requests_pathname.rstrip(\"/\")) :]\n    return path", "idx": 1390}
{"namespace": "dash.development._py_components_generation.js_to_py_type", "completion": "    if is_flow_type:\n        js_to_py_map = map_js_to_py_types_flow_types(type_object)\n    else:\n        js_to_py_map = map_js_to_py_types_prop_types(type_object, indent_num)\n\n    if \"name\" in type_object:\n        if type_object[\"name\"] in js_to_py_map:\n            return js_to_py_map[type_object[\"name\"]]()\n\n    if \"computed\" in type_object and type_object[\"computed\"]:\n        return \"dict\"\n\n    if \"value\" in type_object:\n        if type_object[\"value\"] in js_to_py_map:\n            return js_to_py_map[type_object[\"value\"]]()\n\n    if \"elements\" in type_object:\n        if type_object[\"elements\"][0] in js_to_py_map:\n            return js_to_py_map[type_object[\"elements\"][0]]()\n\n    if \"signature\" in type_object:\n        if \"properties\" in type_object[\"signature\"]:\n            return js_to_py_map[\"signature\"](indent_num)\n\n    return \"\"", "idx": 1391}
{"namespace": "dash.development.component_loader.load_components", "completion": "    # Start processing\n    data = _get_metadata(metadata_path)\n\n    # Register the component library\n    dash.register_component(namespace, data)\n\n    # Iterate over each component in the metadata\n    components = []\n    for component in data:\n        # Extract the component name\n        name = component[\"displayName\"]\n\n        # Generate the class\n        component_class = generate_class(component)\n\n        # Add the class to the list\n        components.append(component_class)\n\n    return components\n\n", "idx": 1392}
{"namespace": "dash.development.component_loader.generate_classes", "completion": "    # Start processing\n    components = load_components(metadata_path, namespace)\n\n    # Generate classes\n    for component in components:\n        component.generate_class()\n\n    # Generate imports file\n    with open(os.path.join(namespace, \"imports.py\"), \"w\") as imports_file:\n        imports_file.write(\"__all__ = \" + str(list(map(lambda x: x.name, components))))", "idx": 1393}
{"namespace": "dash.development.base_component.Component.to_plotly_json", "completion": "        # pylint: disable=no-member\n        # pylint: disable=too-many-branches\n        # pylint: disable=too-many-nested-blocks\n        # pylint: disable=too-many-statements\n        # pylint: disable=too-many-locals\n\n        # pylint: disable=import-outside-toplevel\n        from ._utils import convert_to_camel_case\n\n        # pylint: disable=import-outside-toplevel\n        from ._utils import patch_collections_abc\n\n        # pylint: disable=import-outside-toplevel\n        from ._utils import patch_pandas_dataframe_column_name_typing\n\n        patch_pandas_dataframe_column_name_typing()\n\n        # pylint: disable=import-outside-toplevel\n        from ._utils import patch_pandas_series_name_typing\n\n        patch_pandas_series_name_typing()\n\n        patch_collections_abc()\n\n        # pylint: disable=import-outside-toplevel\n        from ._utils import patch_dash_deprecation_warnings\n\n        patch_dash_deprecation_warnings()\n\n        # pylint: disable=import-outside-toplevel\n        from ._utils import patch_dash_enums\n\n        patch_dash_enums()\n\n        # pylint: disable=import-outside-toplevel\n        from ._utils import patch_typing_docs\n\n        patch_typing_docs()\n\n        # pylint: disable=import-outside-toplevel\n        from ._utils import patch_typing_docs_end\n\n        patch_typing_docs_end()\n\n        # pylint: disable=import-outside-toplevel\n        from ._utils import patch_typing_forwardref_docs\n\n        patch_typing_forwardref_docs()\n\n        # pylint: disable=import-outside-toplevel\n        from .", "idx": 1394}
{"namespace": "dash.development.base_component.Component._traverse", "completion": "        # A component's children can be undefined, a string, another component,\n        # or a list of components.\n        if isinstance(self.children, Component):\n            yield from self.children._traverse()\n        elif isinstance(self.children, (tuple, MutableSequence)):\n            for child in self.children:\n                if isinstance(child, Component):\n                    yield from child._traverse()\n", "idx": 1395}
{"namespace": "dash.development._r_components_generation.make_namespace_exports", "completion": "    # Create a list of components that are not dash components\n    non_components = []\n    for component in components:\n        if component not in dash_components:\n            non_components.append(component)\n\n    # Create a list of components that are dash components\n    components = list(set(components) - set(non_components))\n\n    # Create a list of components that are dash components with a dash\n    dash_components_with_dash = []\n    for component in components:\n        if \"-\" in component:\n            dash_components_with_dash.append(component)\n\n    # Create a list of components that are dash components without a dash\n    dash_components_without_dash = list(set(components) - set(dash_components_with_dash))\n\n    # Create a list of components that are dash components with a dash\n    dash_components_with_dash = list(set(dash_components_with_dash) - set(dash_components_without_dash))\n\n    # Create a list of components that are dash components without a dash\n    dash_components_without_dash = list(set(dash_components_without_dash) - set(dash_components_with_dash))\n\n    # Create a list of components that are dash components with a dash\n    dash_components_with_dash = list(set(dash_components_with_dash) - set(dash_components_without_dash))\n\n    # Create a list of components that are dash components without a dash\n    dash_components_without_dash = list(set(dash_components_without_dash) - set(dash_components_with_dash))\n\n    # Create a list of components that are dash components with a dash\n    dash_components_with_dash = list(set(dash_components_with_dash) - set(dash_components_without_dash))\n\n    # Create a list of components that are dash components without a dash\n    dash_components_without_dash = list(set(dash_components_without_dash) - set(dash_components_with_dash))\n\n    # Create a list of components that are dash components with a dash\n    dash_components_with_dash = list(set", "idx": 1396}
{"namespace": "dash.development._collect_nodes.collect_nodes", "completion": "    if nodes is None:\n        nodes = []\n    for key, value in metadata.items():\n        if is_node(key):\n            nodes.append(base)\n        elif is_shape(key):\n            nodes = collect_nodes(value, base, nodes)\n        elif key == \"union\":\n            nodes = collect_union(value, base, nodes)\n        elif key == \"arrayOf\":\n            nodes = collect_array(value, base, nodes)\n        elif key == \"objectOf\":\n            nodes = collect_object(value, base, nodes)\n    return nodes", "idx": 1397}
{"namespace": "peewee.Index.where", "completion": "        if self._where is not None:\n            expressions = (self._where,) + expressions\n        self._where = reduce(operator.and_, expressions)\n", "idx": 1398}
{"namespace": "playhouse.dataset.DataSet.tables", "completion": "        tables = self._database.get_tables()\n        if self._include_views:\n            views = self._database.get_views()\n            tables.extend(views)\n        return tables\n", "idx": 1399}
{"namespace": "playhouse.dataset.DataSet.update_cache", "completion": "        if table is None:\n            tables = self.tables\n        else:\n            tables = [table]\n\n        for table in tables:\n            self._models[table] = self._introspector.generate_model(\n                table,\n                self._base_model,\n                include_views=self._include_views)\n", "idx": 1400}
{"namespace": "playhouse.dataset.DataSet.freeze", "completion": "        self._check_arguments(filename, file_obj, format, self._export_formats)\n        exporter = self._export_formats[format](\n            file_obj=file_obj, filename=filename, encoding=encoding)\n        exporter.export(query, **kwargs)\n        if not file_obj and filename:\n            exporter.file.close()\n", "idx": 1401}
{"namespace": "playhouse.db_url.parse", "completion": "    parsed = urlparse(url)\n    return parseresult_to_dict(parsed, unquote_password)\n", "idx": 1402}
{"namespace": "playhouse.db_url.connect", "completion": "    connect_kwargs = parse(url, unquote_password)\n    connect_kwargs.update(connect_params)\n    db_class = schemes[connect_kwargs.pop('scheme')]\n    return db_class.connect(**connect_kwargs)", "idx": 1403}
{"namespace": "playhouse.sqlite_changelog.ChangeLog.install", "completion": "        if create_table:\n            self.model.create_table()\n\n        if drop:\n            for action in self._actions:\n                self.db.execute_sql(self.drop_trigger_sql(model, action))\n\n        if insert:\n            self.db.execute_sql(self.trigger_sql(model, 'INSERT', skip_fields))\n        if update:\n            self.db.execute_sql(self.trigger_sql(model, 'UPDATE', skip_fields))\n        if delete:\n            self.db.execute_sql(self.trigger_sql(model, 'DELETE', skip_fields))\n", "idx": 1404}
{"namespace": "playhouse.kv.KeyValue.pop", "completion": "        with self._database.atomic():\n            try:\n                value = self[key]\n            except KeyError:\n                if default is Sentinel:\n                    raise\n                return default\n            del self[key]\n            return value\n", "idx": 1405}
{"namespace": "playhouse.signals.Signal.connect", "completion": "        if not callable(receiver):\n            raise ValueError('Signal receivers must be callable.')\n\n        if name is None:\n            name = receiver.__name__\n\n        if (name, sender) in self._receivers:\n            raise ValueError('A receiver named {0} already exists.'.format(name))\n\n        self._receivers.add((name, sender))\n        self._receiver_list.append((name, receiver, sender))\n", "idx": 1406}
{"namespace": "playhouse.signals.Signal.disconnect", "completion": "        if receiver is not None:\n            self._receivers.discard((name, sender))\n            self._receiver_list = [\n                (n, r, s) for n, r, s in self._receiver_list\n                if r != receiver and n != name\n            ]\n        elif name is not None:\n            self._receivers.discard((name, sender))\n            self._receiver_list = [\n                (n, r, s) for n, r, s in self._receiver_list\n                if n != name\n            ]\n        elif sender is not None:\n            self._receivers.discard((name, sender))\n            self._receiver_list = [\n                (n, r, s) for n, r, s in self._receiver_list\n                if s != sender\n            ]\n        else:\n            self._receivers = set()\n            self._receiver_list = []\n", "idx": 1407}
{"namespace": "backtrader.trade.Trade.update", "completion": "        # Update the commissions\n        self.commission += commission\n\n        # Update the size\n        self.size += size\n\n        # Check if the trade was opened\n        if not self.isopen:\n            self.isopen = True\n            self.justopened = True\n            self.baropen = order.baropen\n            self.dtopen = order.dtopen\n\n        # Update the trade length\n        if self.size != 0:\n            self.barlen += 1\n\n        # Check if the trade was closed\n        if self.size == 0:\n            self.isclosed = True\n            self.barclose = order.barclose\n            self.dtclose = order.dtclose\n\n        # Update the average price\n        if abs(self.size) > abs(self.size):\n            self.price = price\n\n        # Update the trade status\n        if self.size != 0:\n            self.status = self.Open\n        else:\n            self.status = self.Closed\n\n        # Update the history\n        if self.historyon:\n            self.history.append(\n                TradeHistory(\n                    self.status,\n                    order.dtopen,\n                    self.barlen,\n                    self.size,\n                    self.price,\n                    self.value,\n                    self.pnl,\n                    self.pnlcomm,\n                    self.data.get_tz()\n                )\n            )\n", "idx": 1408}
{"namespace": "ydata_profiling.profile_report.ProfileReport.typeset", "completion": "        if self._typeset is None:\n            self._typeset = VisionsTypeset(\n                Config.get_typeset(self.config.vars.typeset.config),\n                self._type_schema,\n            )\n        return self._typeset\n", "idx": 1409}
{"namespace": "ydata_profiling.report.presentation.flavours.html.frequency_table.HTMLFrequencyTable.render", "completion": "        if isinstance(self.content, list):\n            return self.render_table(self.content)\n        else:\n            return self.render_row(self.content)\n", "idx": 1410}
{"namespace": "ydata_profiling.report.presentation.flavours.html.image.HTMLImage.render", "completion": "        return self.template.render(\n            image=self.image,\n            image_name=self.image_name,\n            image_type=self.image_type,\n            image_format=self.image_format,\n            image_width=self.image_width,\n            image_height=self.image_height,\n            image_size=self.image_size,\n            image_url=self.image_url,\n            image_description=self.image_description,\n            image_copyright=self.image_copyright,\n            image_license=self.image_license,\n            image_creator=self.image_creator,\n            image_creator_url=self.image_creator_url,\n            image_source=self.image_source,\n            image_source_url=self.image_source_url,\n            image_metadata=self.image_metadata,\n            image_tags=self.image_tags,\n            image_keywords=self.image_keywords,\n            image_date=self.image_date,\n            image_location=self.image_location,\n            image_location_name=self.image_location_name,\n            image_location_country=self.image_location_country,\n            image_location_state_province=self.image_location_state_province,\n            image_location_city=self.image_location_city,\n            image_location_address=self.image_location_address,\n            image_location_zip_postcode=self.image_location_zip_postcode,\n            image_location_coordinates=self.image_location_coordinates,\n            image_location_timezone=self.image_location_timezone,\n            image_location_utc_offset=self.image_location_utc_offset,\n            image_location_description=self.image_location_description,\n            image_location_units=self.image_location_units,\n            image_location_accuracy=self.image_location_accuracy,\n            image_location_type=self.image_location_type,", "idx": 1411}
{"namespace": "ydata_profiling.model.summary_algorithms.histogram_compute", "completion": "    # Determine the number of bins\n    n_bins = config.histogram_bins\n    if n_bins is None:\n        n_bins = min(20, n_unique)\n    else:\n        n_bins = min(n_bins, n_unique)\n\n    # Reduce the number of bins if the number of bins exceeds the maximum allowed bins\n    if n_bins > config.histogram_max_bins:\n        n_bins = config.histogram_max_bins\n\n    # Compute the histogram\n    hist, bin_edges = np.histogram(finite_values, bins=n_bins, weights=weights)\n\n    # Compute the bin centers\n    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n\n    # Compute the bin widths\n    bin_widths = bin_edges[1:] - bin_edges[:-1]\n\n    # Compute the bin counts\n    bin_counts = hist\n\n    # Compute the bin probabilities\n    bin_probabilities = bin_counts / np.sum(bin_counts)\n\n    # Compute the bin cumulative probabilities\n    bin_cum_probabilities = np.cumsum(bin_probabilities)\n\n    # Compute the bin cumulative counts\n    bin_cum_counts = np.cumsum(bin_counts)\n\n    # Compute the bin cumulative probabilities\n    bin_cum_count_probabilities = bin_cum_counts / np.sum(bin_counts)\n\n    # Compute the bin cumulative probabilities\n    bin_cum_count_probabilities = bin_cum_counts / np.sum(bin_counts)\n\n    # Compute the bin cumulative probabilities\n    bin_cum_count_probabilities = bin_cum_counts / np.sum(bin_counts)\n\n    # Compute the bin cumulative probabilities\n    bin_c", "idx": 1412}
{"namespace": "ydata_profiling.model.summarizer.BaseSummarizer.summarize", "completion": "        raise NotImplementedError\n\n", "idx": 1413}
{"namespace": "ydata_profiling.model.pandas.discretize_pandas.Discretizer.discretize_dataframe", "completion": "        # Create a copy of the input DataFrame\n        dataframe_copy = dataframe.copy()\n\n        # Get the numerical columns of the DataFrame\n        numerical_columns = dataframe_copy.select_dtypes(include=[\"int64\", \"float64\"]).columns\n\n        # Discretize the numerical columns\n        for column in numerical_columns:\n            dataframe_copy[column] = self.discretize_column(dataframe_copy[column])\n\n        # Reset the index of the DataFrame\n        if self.reset_index:\n            dataframe_copy.reset_index(inplace=True, drop=True)\n\n        return dataframe_copy\n", "idx": 1414}
{"namespace": "ydata_profiling.model.pandas.correlations_pandas.pandas_cramers_compute", "completion": "    # get categorical variables\n    categorical_variables = [\n        var\n        for var in summary.keys()\n        if summary[var][\"type\"] == \"categorical\" and summary[var][\"n_unique\"] <= config.cramers_threshold\n    ]\n\n    # check if there are at least 2 categorical variables\n    if len(categorical_variables) <= 1:\n        return None\n\n    # create empty correlation matrix\n    correlation_matrix = pd.DataFrame(\n        index=categorical_variables, columns=categorical_variables\n    )\n\n    # calculate Cramer's V correlation coefficient for each pair of categorical variables\n    for col_1, col_2 in itertools.combinations(categorical_variables, 2):\n        correlation_matrix.loc[col_1, col_2] = _cramers_corrected_stat(\n            pd.crosstab(df[col_1], df[col_2]), correction=True\n        )\n\n    return correlation_matrix\n\n", "idx": 1415}
{"namespace": "ydata_profiling.model.pandas.correlations_pandas.pandas_auto_compute", "completion": "    # Identify numerical and categorical columns\n    numerical_cols = {\n        key\n        for key, value in summary.items()\n        if value[\"type\"] == \"Numeric\" and 1 < value[\"n_distinct\"]\n    }\n    categorical_cols = {\n        key\n        for key, value in summary.items()\n        if value[\"type\"] in {\"Categorical\", \"Boolean\"}\n        and 1 < value[\"n_distinct\"] <= config.categorical_maximum_correlation_distinct\n    }\n\n    # If there are no numerical or categorical columns, return None\n    if len(numerical_cols) == 0 or len(categorical_cols) == 0:\n        return None\n\n    # Discretize the DataFrame\n    df_disc = pd.DataFrame(\n        df.apply(\n            lambda x: pd.qcut(x, q=config.categorical_maximum_correlation_distinct)\n            if x.name in categorical_cols\n            else x,\n            axis=0,\n        )\n    )\n\n    # Calculate the correlation scores between each pair of columns\n    correlation_scores = {}\n    for col1, col2 in itertools.product(numerical_cols, numerical_cols):\n        correlation_scores[(col1, col2)] = _pairwise_spearman(df_disc[col1], df_disc[col2])\n\n    for col1, col2 in itertools.product(categorical_cols, categorical_cols):\n        correlation_scores[(col1, col2)] = _pairwise_cramers(df_disc[col1], df_disc[col2])\n\n    # Return the correlation matrix\n    return pd.DataFrame(\n        correlation_scores.values(),\n        index=correlation_scores.keys(),\n        columns=correlation_scores.keys(),\n    )", "idx": 1416}
{"namespace": "ydata_profiling.controller.console.main", "completion": "    args = parse_args(args)\n\n    # Read the data\n    df = ProfileReport.read_data(args.input_file)\n\n    # Create the report\n    report = ProfileReport(\n        df,\n        minimal=args.minimal,\n        explorative=args.explorative,\n        title=args.title,\n        config_file=args.config_file,\n        pool_size=args.pool_size,\n        infer_dtypes=args.infer_dtypes,\n    )\n\n    # Save the report\n    report.to_file(args.output_file)\n\n    # Open the report\n    if not args.silent:\n        report.to_notebook_iframe()\n\n", "idx": 1417}
{"namespace": "ydata_profiling.utils.cache.cache_file", "completion": "    data_path = get_data_path()\n    file_path = data_path / file_name\n\n    if not file_path.exists():\n        print(f\"Downloading {file_name} from {url}\")\n        request.urlretrieve(url, file_path)\n\n    return file_path\n\n", "idx": 1418}
{"namespace": "ydata_profiling.utils.dataframe.expand_mixed", "completion": "    if types is None:\n        types = [list, dict, tuple]\n\n    expanded_df = pd.DataFrame()\n    for col in df.columns:\n        for i in df[col].index:\n            if isinstance(df[col][i], types):\n                expanded_df = expanded_df.append(pd.DataFrame(df[col][i]))\n\n    expanded_df = expanded_df.reset_index(drop=True)\n    expanded_df.columns = [\n        f\"{col}_{i}\" for col in expanded_df.columns\n    ]  # add prefix to column names\n    return expanded_df\n\n", "idx": 1419}
{"namespace": "pysnooper.utils.ensure_tuple", "completion": "    if isinstance(x, tuple):\n        return x\n    elif isinstance(x, collections_abc.Iterable) and not isinstance(x, str):\n        return tuple(x)\n    else:\n        return (x,)\n\n", "idx": 1420}
{"namespace": "rq.serializers.resolve_serializer", "completion": "    if serializer is None:\n        return DefaultSerializer\n\n    if isinstance(serializer, str):\n        serializer = import_serializer(serializer)\n\n    if not hasattr(serializer, 'dumps') or not callable(serializer.dumps):\n        raise NotImplementedError(\"Serializer must implement 'dumps' method.\")\n\n    if not hasattr(serializer, 'loads') or not callable(serializer.loads):\n        raise NotImplementedError(\"Serializer must implement 'loads' method.\")\n\n    return serializer\n\n", "idx": 1421}
{"namespace": "lux.vis.Vis.Vis.get_attr_by_channel", "completion": "        attr_list = []\n        for clause in self._inferred_intent:\n            if clause.channel == channel:\n                attr_list.append(clause)\n        return attr_list\n", "idx": 1422}
{"namespace": "lux.action.default.register_default_actions", "completion": "    import lux.action.CustomActions\n    import lux.action.Distribution\n    import lux.action.Correlation\n    import lux.action.Enhance\n    import lux.action.Filter\n    import lux.action.Generalize\n    import lux.action.Temporal\n    import lux.action.Uniqueness\n    import lux.action.Symmetry\n    import lux.action.Sampling\n    import lux.action.Cast\n    import lux.action.Census\n    import lux.action.Extract\n    import lux.action.General\n\n    from lux.action.CustomActions import register_custom_actions\n\n    # Registering default actions\n    register_custom_actions()\n\n    lux.action.Distribution.register_actions()\n    lux.action.Correlation.register_actions()\n    lux.action.Enhance.register_actions()\n    lux.action.Filter.register_actions()\n    lux.action.Generalize.register_actions()\n    lux.action.Temporal.register_actions()\n    lux.action.Uniqueness.register_actions()\n    lux.action.Symmetry.register_actions()\n    lux.action.Sampling.register_actions()\n    lux.action.Cast.register_actions()\n    lux.action.Census.register_actions()\n    lux.action.Extract.register_actions()\n    lux.action.General.register_actions()\n\n    # Registering default display conditions\n    lux.action.Distribution.register_display_condition()\n    lux.action.Correlation.register_display_condition()\n    lux.action.Enhance.register_display_condition()\n    lux.action.Filter.register_display_condition()\n    lux.action.Generalize.register_display_condition()\n    lux.action.Temporal.register_display_condition()\n    lux.action.Uniqueness.register_display_condition()\n    lux.action.Symmetry.register_display_condition()\n    lux.action.Sampling.register_display_condition()\n    lux.action.Cast.register_display_condition()\n    lux.action.Census", "idx": 1423}
{"namespace": "folium.utilities.get_bounds", "completion": "    bounds = [[None, None], [None, None]]\n    for point in iter_coords(locations):\n        if lonlat:\n            lat, lon = point\n        else:\n            lon, lat = point\n        bounds = [\n            [\n                none_min(bounds[0][0], lat),\n                none_min(bounds[0][1], lon),\n            ],\n            [\n                none_max(bounds[1][0], lat),\n                none_max(bounds[1][1], lon),\n            ],\n        ]\n    return bounds\n\n", "idx": 1424}
{"namespace": "folium.features.VegaLite.vegalite_major_version", "completion": "        return int(self.data[\"$schema\"].split(\"/\")[-1].split(\"-\")[0].split(\".\")[0])\n", "idx": 1425}
{"namespace": "music_dl.utils.colorize", "completion": "    if platform.system() == \"Windows\" or color not in colors:\n        return string\n    return colors[color] + string + \"\\033[0m\"\n\n", "idx": 1426}
{"namespace": "music_dl.source.MusicSource.search", "completion": "        # Create a list of threads to search for the keyword in each source concurrently\n        threads = []\n        for source in sources_list:\n            thread = threading.Thread(target=self.search_source, args=(source, keyword))\n            threads.append(thread)\n            thread.start()\n\n        # Wait for all threads to finish\n        for thread in threads:\n            thread.join()\n\n        # Sort and remove duplicates from the search results\n        self.sort_results()\n        self.remove_duplicates()\n\n        return self.results\n", "idx": 1427}
{"namespace": "jwt.utils.base64url_decode", "completion": "    if isinstance(input, str):\n        input = input.encode(\"utf-8\")\n    elif not isinstance(input, bytes):\n        raise TypeError(\"Expected a string value\")\n\n    input = re.sub(rb\"[^a-zA-Z0-9_=-]\", b\"\", input)  # Remove any characters that are not in the base64 alphabet\n    return base64.urlsafe_b64decode(input)\n\n", "idx": 1428}
{"namespace": "jwt.utils.to_base64url_uint", "completion": "    if not isinstance(val, int):\n        raise TypeError(\"The input value must be an integer.\")\n\n    if val < 0:\n        raise ValueError(\"The input value must be a positive integer.\")\n\n    val_bytes = val.to_bytes((val.bit_length() + 7) // 8, \"big\")\n\n    if len(val_bytes) == 0:\n        val_bytes = b\"\\x00\"\n\n    return base64url_encode(val_bytes)\n\n", "idx": 1429}
{"namespace": "jwt.algorithms.HMACAlgorithm.prepare_key", "completion": "        if isinstance(key, str):\n            key = key.encode(\"utf-8\")\n\n        if key.startswith(b\"-----BEGIN CERTIFICATE-----\\r\\n\") or key.startswith(b\"ssh-rsa\"):\n            raise InvalidKeyError(\"The specified key is an asymmetric key or x509 certificate and should not be used as an HMAC secret.\")\n\n        return key\n", "idx": 1430}
{"namespace": "jwt.algorithms.HMACAlgorithm.to_jwk", "completion": "        import jwt\n\n        key_obj = force_bytes(key_obj)\n        key_dict = {\"kty\": \"oct\", \"k\": base64url_encode(key_obj)}\n        if as_dict:\n            return key_dict\n        return json.dumps(key_dict)\n", "idx": 1431}
{"namespace": "jwt.algorithms.HMACAlgorithm.from_jwk", "completion": "        if isinstance(jwk, str):\n            jwk = json.loads(jwk)\n\n        if jwk.get(\"kty\") != \"oct\":\n            raise InvalidKeyError(\"Not an HMAC key\")\n\n        return base64url_decode(jwk.get(\"k\"))\n", "idx": 1432}
{"namespace": "sacred.arg_parser._convert_value", "completion": "    try:\n        return ast.literal_eval(value)\n    except (ValueError, SyntaxError):\n        return value", "idx": 1433}
{"namespace": "sacred.utils.recursive_update", "completion": "    for k, v in u.items():\n        if isinstance(v, collections.Mapping):\n            r = recursive_update(d.get(k, {}), v)\n            d[k] = r\n        else:\n            d[k] = u[k]\n    return d\n\n", "idx": 1434}
{"namespace": "sacred.utils.iterate_flattened_separately", "completion": "    # Initialize the list of manually sorted keys.\n    if manually_sorted_keys is None:\n        manually_sorted_keys = []\n\n    # Initialize the list of manually sorted keys.\n    if manually_sorted_keys is None:\n        manually_sorted_keys = []\n\n    # Initialize the list of manually sorted keys.\n    if manually_sorted_keys is None:\n        manually_sorted_keys = []\n\n    # Initialize the list of manually sorted keys.\n    if manually_sorted_keys is None:\n        manually_sorted_keys = []\n\n    # Initialize the list of manually sorted keys.\n    if manually_sorted_keys is None:\n        manually_sorted_keys = []\n\n    # Initialize the list of manually sorted keys.\n    if manually_sorted_keys is None:\n        manually_sorted_keys = []\n\n    # Initialize the list of manually sorted keys.\n    if manually_sorted_keys is None:\n        manually_sorted_keys = []\n\n    # Initialize the list of manually sorted keys.\n    if manually_sorted_keys is None:\n        manually_sorted_keys = []\n\n    # Initialize the list of manually sorted keys.\n    if manually_sorted_keys is None:\n        manually_sorted_keys = []\n\n    # Initialize the list of manually sorted keys.\n    if manually_sorted_keys is None:\n        manually_sorted_keys = []\n\n    # Initialize the list of manually sorted keys.\n    if manually_sorted_keys is None:\n        manually_sorted_keys = []\n\n    # Initialize the list of manually sorted keys.\n    if manually_sorted_keys is None:\n        manually_sorted_keys = []\n\n    # Initialize the list of manually sorted keys.\n    if manually_sorted_keys is None:\n        manually_sorted_keys = []\n\n    # Initialize the list of manually sorted keys.\n    if manually_sorted_keys is None:\n        manually_sorted_keys = []\n\n    # Initialize the list of manually sorted keys.\n    if manually_sorted_keys is None:\n        manually_sorted_keys = []\n\n    # Initialize the list of manually sorted keys.\n    if manually_sorted_keys", "idx": 1435}
{"namespace": "sacred.utils.iterate_flattened", "completion": "    for key, value in d.items():\n        if isinstance(value, dict) and value:\n            for k, v in iterate_flattened(value):\n                yield join_paths(key, k), v\n        else:\n            yield key, value\n\n", "idx": 1436}
{"namespace": "sacred.utils.iter_prefixes", "completion": "    split_path = path.split(\".\")\n    for i in range(1, len(split_path) + 1):\n        yield \".\".join(split_path[:i])\n\n", "idx": 1437}
{"namespace": "sacred.utils.rel_path", "completion": "    assert is_prefix(base, path), f\"{base} not a prefix of {path}\"\n    return path[len(base):]\n\n", "idx": 1438}
{"namespace": "sacred.utils.convert_to_nested_dict", "completion": "    nested_dict = {}\n    for key, value in dotted_dict.items():\n        set_by_dotted_path(nested_dict, key, value)\n    return nested_dict\n\n", "idx": 1439}
{"namespace": "sacred.utils.format_sacred_error", "completion": "    lines = []\n    if e.short_usage:\n        lines.append(e.short_usage)\n    if e.filter_traceback == \"never\":\n        lines.append(\"{}: {}\".format(e.__class__.__name__, e))\n    else:\n        lines.append(format_filtered_stacktrace(e.filter_traceback))\n    return \"\\n\".join(lines)\n\n", "idx": 1440}
{"namespace": "sacred.utils.get_package_version", "completion": "    if name not in sys.modules:\n        __import__(name)\n    version_obj = getattr(sys.modules[name], \"__version__\", None)\n    if version_obj is None:\n        raise ImportError(\"Package `{}` has no attribute `__version__`.\".format(name))\n    if not isinstance(version_obj, str):\n        raise ImportError(\"Package `{}` has an invalid `__version__` attribute.\".format(name))\n    return parse_version(version_obj)\n\n", "idx": 1441}
{"namespace": "sacred.experiment.Experiment.main", "completion": "        self.default_command = self.command(function)\n        return self.default_command\n", "idx": 1442}
{"namespace": "sacred.experiment.Experiment.run", "completion": "        if command_name is None:\n            command_name = self.default_command\n\n        if command_name is None:\n            raise RuntimeError(\n                \"No command found! Use @ex.command to add commands to \"\n                \"your experiment or specify a command on the command line.\"\n            )\n\n        # check if command is valid\n        if command_name not in self.gather_commands():\n            raise RuntimeError(\n                'Command \"{}\" not found in this experiment!'.format(command_name)\n            )\n\n        # create run\n        config_updates = config_updates or {}\n        options = options or {}\n        run = create_run(\n            self,\n            command_name,\n            config_updates,\n            named_configs,\n            info,\n            meta_info,\n            options,\n        )\n\n        # run the command\n        self.current_run = run\n        try:\n            self.call(command_name)\n        except Exception as e:\n            # handle exception\n            self.current_run.status = \"FAILED\"\n            if SETTINGS.COMMAND_LINE_LOGGING:\n                print_filtered_stacktrace(e)\n            else:\n                print(format_sacred_error(e))\n            self.current_run.log_scalar(\"_exception\", e)\n        else:\n            self.current_run.status = \"COMPLETED\"\n        finally:\n            self.current_run = None\n\n        return run\n", "idx": 1443}
{"namespace": "sacred.host_info.host_info_getter", "completion": "    if name is None:\n        name = func.__name__\n    host_info_gatherers[name] = func\n    return func\n\n", "idx": 1444}
{"namespace": "sacred.ingredient.Ingredient.command", "completion": "        if function in self.commands:\n            return function\n        captured_function = create_captured_function(function, prefix=prefix)\n        captured_function.unobserved = unobserved\n        self.commands[function.__name__] = captured_function\n        return captured_function\n", "idx": 1445}
{"namespace": "sacred.ingredient.Ingredient.config", "completion": "        self.configurations.append(ConfigScope(function))\n        return self.configurations[-1]\n", "idx": 1446}
{"namespace": "sacred.ingredient.Ingredient.named_config", "completion": "        self.named_configs[func.__name__] = ConfigScope(func)\n        return self.named_configs[func.__name__]\n", "idx": 1447}
{"namespace": "sacred.ingredient.Ingredient.gather_commands", "completion": "        for cmd_name, cmd in self.commands.items():\n            yield self.post_process_name(cmd_name, self), cmd\n        for ingred in self.ingredients:\n            for cmd_name, cmd in ingred.gather_commands():\n                yield self.post_process_name(cmd_name, self), cmd\n", "idx": 1448}
{"namespace": "sacred.ingredient.Ingredient.gather_named_configs", "completion": "        for ingredient, _ in self.traverse_ingredients():\n            for name, config in ingredient.named_configs.items():\n                yield join_paths(ingredient.path, name), config\n", "idx": 1449}
{"namespace": "sacred.dependencies.Source.create", "completion": "        if not os.path.isfile(filename):\n            raise ValueError(\"invalid filename or file not found {}\".format(filename))\n\n        digest = get_digest(filename)\n        repo, commit, isdirty = get_commit_if_possible(filename, save_git_info)\n        return Source(filename, digest, repo, commit, isdirty)\n", "idx": 1450}
{"namespace": "sacred.dependencies.Source.to_json", "completion": "        if base_dir is not None:\n            return os.path.relpath(self.filename, base_dir), self.digest\n        else:\n            return self.filename, self.digest\n", "idx": 1451}
{"namespace": "sacred.dependencies.PackageDependency.create", "completion": "        if mod.__name__ in cls.modname_to_dist:\n            return cls.modname_to_dist[mod.__name__]\n\n        dist = pkg_resources.working_set.by_key.get(mod.__name__)\n        if dist is None:\n            dist = pkg_resources.working_set.by_key.get(mod.__package__)\n\n        if dist is None:\n            return None\n\n        dist_name = dist.project_name\n        dist_version = dist.version\n        dist = PackageDependency(dist_name, dist_version)\n        cls.modname_to_dist[mod.__name__] = dist\n        return dist\n\n", "idx": 1452}
{"namespace": "sacred.dependencies.is_local_source", "completion": "    if not experiment_path:\n        return False\n    if not filename:\n        return False\n    if not modname:\n        return False\n    if not os.path.isabs(filename):\n        return False\n    if not os.path.isabs(experiment_path):\n        return False\n    if not os.path.isdir(experiment_path):\n        return False\n    if not os.path.isdir(os.path.dirname(filename)):\n        return False\n    if not os.path.isfile(filename):\n        return False\n    if not os.path.isfile(os.path.join(experiment_path, \"main.py\")):\n        return False\n    if not os.path.isfile(os.path.join(experiment_path, \"config.py\")):\n        return False\n    if not os.path.isfile(os.path.join(experiment_path, \"host_info.json\")):\n        return False\n    if not os.path.isfile(os.path.join(experiment_path, \"dependencies.json\")):\n        return False\n    if not os.path.isfile(os.path.join(experiment_path, \"requirements.txt\")):\n        return False\n    if not os.path.isfile(os.path.join(experiment_path, \"main.py\")):\n        return False\n    if not os.path.isfile(os.path.join(experiment_path, \"config.py\")):\n        return False\n    if not os.path.isfile(os.path.join(experiment_path, \"host_info.json\")):\n        return False\n    if not os.path.isfile(os.path.join(experiment_path, \"dependencies.json\")):\n        return False\n    if not os.path.isfile(os.path.join(experiment_path, \"requirements.txt\")):\n        return False\n    if not os.path.isfile(os.path.join(experiment_path, \"main.py\")):\n        return False\n    if not os.path.isfile(os", "idx": 1453}
{"namespace": "sacred.dependencies.gather_sources_and_dependencies", "completion": "    base_dir = base_dir or os.path.abspath(os.path.curdir)\n    experiment_path, main = get_main_file(globs, save_git_info)\n    sources = source_discovery_strategies[SETTINGS[\"SOURCE_DISCOVERY\"]](\n        globs, experiment_path, save_git_info\n    )\n    dependencies = dependency_discovery_strategies[SETTINGS[\"DEPENDENCY_DISCOVERY\"]](\n        globs, experiment_path\n    )\n    if main is not None:\n        sources.add(main)\n    if \"numpy\" in sys.modules:\n        dependencies.add(PackageDependency(\"numpy\", sys.modules[\"numpy\"].__version__))\n    return main, sources, dependencies\n\n", "idx": 1454}
{"namespace": "sacred.observers.file_storage.FileStorageObserver.resource_event", "completion": "        self.run_entry[\"resources\"].append(filename)\n        self.save_json(self.run_entry, \"run.json\")\n", "idx": 1455}
{"namespace": "sacred.config.signature.Signature.get_free_parameters", "completion": "        # If the signature is bound to an instance, the first argument is the instance.\n        if bound:\n            args = args[1:]\n\n        # Get the list of arguments that are passed to the function.\n        passed_args = args + list(kwargs.keys())\n\n        # Get the list of arguments that are required by the function.\n        required_args = self.positional_args + list(self.kwargs.keys())\n\n        # Get the list of free parameters.\n        free_parameters = [arg for arg in required_args if arg not in passed_args]\n\n        return free_parameters\n", "idx": 1456}
{"namespace": "sacred.config.signature.Signature.construct_arguments", "completion": "        expected_args = self._get_expected_args(bound)\n        if len(args) > len(expected_args):\n            raise SignatureError(\n                \"Too many positional arguments passed to \"\n                + self.name\n                + \" (expected \"\n                + str(len(expected_args))\n                + \")\"\n            )\n        if len(args) < len(expected_args):\n            for i in range(len(expected_args) - len(args)):\n                if expected_args[len(args) + i] in kwargs:\n                    raise SignatureError(\n                        \"Parameter \"\n                        + expected_args[len(args) + i]\n                        + \" given by name and as positional argument\"\n                    )\n        for k in kwargs:\n            if k not in expected_args:\n                raise SignatureError(\n                    \"Unexpected argument \" + k + \" for \" + self.name\n                )\n        for i in range(len(args)):\n            if expected_args[i] in kwargs:\n                raise SignatureError(\n                    \"Parameter \"\n                    + expected_args[i]\n                    + \" given by name and as positional argument\"\n                )\n        for k in kwargs:\n            if k not in expected_args:\n                raise SignatureError(\n                    \"Unexpected argument \" + k + \" for \" + self.name\n                )\n        for k in kwargs:\n            if k in self.kwargs:\n                if kwargs[k] != self.kwargs[k]:\n                    raise SignatureError(\n                        \"Parameter \"\n                        + k\n                        + \" has conflicting default value and value given by user\"\n                    )\n        for k in self.kwargs:\n            if k not in kwargs:\n                kwargs[k] = self.kwargs[k]\n        for k in options:\n            if k in kwargs:\n                if kwargs[k] != options[k]:\n                    raise SignatureError(\n                        \"Parameter \"\n                        + k\n                        + \" has conflicting default value and value given by", "idx": 1457}
{"namespace": "sacred.config.config_files.load_config_file", "completion": "    handler = get_handler(filename)\n    with open(filename, \"r\" + handler.mode) as f:\n        return handler.load(f)\n\n", "idx": 1458}
{"namespace": "sacred.config.custom_containers.DogmaticDict.get", "completion": "        if dict.__contains__(self, k):\n            return dict.__getitem__(self, k)\n        elif k in self.fallback:\n            if k in self.fixed:\n                return self.fixed[k]\n            else:\n                return self.fallback[k]\n        else:\n            return d\n", "idx": 1459}
{"namespace": "sacred.config.custom_containers.DogmaticDict.revelation", "completion": "        missing_keys = set()\n        for key in self.fixed:\n            if key not in self:\n                missing_keys.add(key)\n            elif isinstance(self[key], DogmaticDict):\n                missing_keys |= self[key].revelation()\n        for key in missing_keys:\n            self[key] = self.fixed[key]\n        return missing_keys\n\n", "idx": 1460}
{"namespace": "sacred.config.custom_containers.make_read_only", "completion": "    if isinstance(o, dict):\n        return ReadOnlyDict({k: make_read_only(v) for k, v in o.items()})\n    elif isinstance(o, list):\n        return ReadOnlyList([make_read_only(v) for v in o])\n    elif isinstance(o, tuple):\n        return tuple(make_read_only(v) for v in o)\n    else:\n        return o\n\n", "idx": 1461}
{"namespace": "sacred.config.config_scope.dedent_function_body", "completion": "    lines = body.splitlines()\n    non_empty_lines = [line for line in lines if not is_empty_or_comment(line)]\n    if not non_empty_lines:\n        return body\n    indent = non_empty_lines[0]\n    for line in non_empty_lines[1:]:\n        if not is_empty_or_comment(line):\n            indent = min(indent, line, key=len)\n    if not indent:\n        return body\n    return \"\\n\".join(dedent_line(line, indent) for line in lines)\n\n", "idx": 1462}
{"namespace": "boltons.funcutils.FunctionBuilder.get_sig_str", "completion": "            if with_annotations:\n                return inspect.formatargspec(self.args, self.varargs, self.varkw, self.defaults, self.kwonlyargs,\n                                             self.kwonlydefaults, self.annotations)\n            else:\n                return inspect.formatargspec(self.args, self.varargs, self.varkw, self.defaults, self.kwonlyargs,\n                                             self.kwonlydefaults)\n", "idx": 1463}
{"namespace": "boltons.funcutils.FunctionBuilder.get_invocation_str", "completion": "            args = self.args\n            kwonlyargs = self.kwonlyargs\n            if kwonlyargs:\n                args += kwonlyargs\n            return inspect.formatargspec(args,\n                                         self.varargs,\n                                         self.varkw,\n                                         [],\n                                         self.kwonlyargs,\n                                         self.kwonlydefaults)[1:-1]\n", "idx": 1464}
{"namespace": "boltons.funcutils.FunctionBuilder.from_func", "completion": "        if isinstance(func, partial):\n            func = func.func\n\n        name = func.__name__\n        fb = cls(name)\n        fb.body = 'return ' + fb._get_func_body(func)\n        fb.module = _get_func_module(func)\n        fb.doc = func.__doc__\n        fb.dict = dict(func.__dict__)\n\n        try:\n            fb.dict['__name__'] = name\n        except TypeError:\n            pass\n\n        fb.args, fb.varargs, fb.varkw, fb.defaults = inspect.getargspec(func)\n        if _IS_PY2:\n            fb.args.extend(getattr(func, 'func_defaults', ()))\n        else:\n            fb.args.extend(getattr(func, '__defaults__', ()))\n\n        try:\n            fb.kwonlyargs, fb.kwonlydefaults = getattr(func, '__kwdefaults__', ({}, {}))\n        except TypeError:\n            pass\n\n        return fb\n", "idx": 1465}
{"namespace": "boltons.funcutils.FunctionBuilder.get_defaults_dict", "completion": "        defaults_dict = {}\n        for i in range(len(self.args)):\n            if self.defaults[i] is not None:\n                defaults_dict[self.args[i]] = self.defaults[i]\n        return defaults_dict\n", "idx": 1466}
{"namespace": "boltons.funcutils.FunctionBuilder.get_arg_names", "completion": "        arg_names = self.args\n        if only_required:\n            defaults_dict = self.get_defaults_dict()\n            arg_names = tuple(arg for arg in arg_names if arg not in defaults_dict)\n        return arg_names\n", "idx": 1467}
{"namespace": "boltons.ioutils.SpooledIOBase.writelines", "completion": "        for line in lines:\n            self.write(line)\n", "idx": 1468}
{"namespace": "boltons.ioutils.SpooledBytesIO.write", "completion": "        self._checkClosed()\n        if not isinstance(s, binary_type):\n            raise TypeError(\"input must be a byte string: %r\" % s)\n        if len(s) > self._max_size:\n            self.rollover()\n        self.buffer.write(s)\n", "idx": 1469}
{"namespace": "boltons.ioutils.SpooledBytesIO.seek", "completion": "        self._checkClosed()\n        return self.buffer.seek(pos, mode)\n", "idx": 1470}
{"namespace": "boltons.ioutils.SpooledBytesIO.len", "completion": "        self._checkClosed()\n        pos = self.tell()\n        if self._rolled:\n            self.seek(0)\n            ret = self.buffer.seek(0, os.SEEK_END)\n            self.seek(pos)\n            return ret\n        else:\n            return self.buffer.seek(0, os.SEEK_END)\n\n", "idx": 1471}
{"namespace": "boltons.ioutils.SpooledStringIO.read", "completion": "        self._checkClosed()\n        if n == 0:\n            return \"\"\n        if n < 0:\n            new_pos = self._tell + n\n            if new_pos < 0:\n                raise IOError(\"can't read from beyond EOF\")\n            return self.buffer.getvalue()[self._tell:]\n        else:\n            new_pos = self._tell + n\n            if new_pos > self._max_size:\n                raise IOError(\"can't read from beyond EOF\")\n            return self.buffer.getvalue()[self._tell:new_pos]\n", "idx": 1472}
{"namespace": "boltons.ioutils.SpooledStringIO.write", "completion": "        self._checkClosed()\n        if not isinstance(s, text_type):\n            raise TypeError(\"str expected, got {0}\".format(type(s)))\n        if self.tell() + len(s) >= self._max_size:\n            self.rollover()\n        self.buffer.write(s.encode(\"utf-8\"))\n        self._tell = self.tell() + len(s)\n", "idx": 1473}
{"namespace": "boltons.ioutils.SpooledStringIO.seek", "completion": "        self._checkClosed()\n        if mode == os.SEEK_SET:\n            self._tell = pos\n        elif mode == os.SEEK_CUR:\n            self._tell += pos\n        elif mode == os.SEEK_END:\n            self._tell = self.len + pos\n        else:\n            raise ValueError('Invalid whence ({}, should be 0, 1, or 2)'.format(mode))\n\n        return self._tell\n", "idx": 1474}
{"namespace": "boltons.ioutils.SpooledStringIO.tell", "completion": "        self._checkClosed()\n        return self._tell\n", "idx": 1475}
{"namespace": "boltons.ioutils.SpooledStringIO.len", "completion": "        pos = self.tell()\n        self.seek(0)\n        codepoints = 0\n        while True:\n            ret = self.read(READ_CHUNK_SIZE)\n            if not ret:\n                break\n            codepoints += len(ret)\n        self.seek(pos)\n        return codepoints\n\n", "idx": 1476}
{"namespace": "boltons.ioutils.MultiFileReader.read", "completion": "        if amt is None:\n            return self._joiner.join(f.read() for f in self._fileobjs)\n        else:\n            return self._joiner.join(f.read(amt) for f in self._fileobjs)\n", "idx": 1477}
{"namespace": "boltons.ioutils.MultiFileReader.seek", "completion": "        if whence != os.SEEK_SET:\n            raise NotImplementedError('MultiFileReader.seek() only supports os.SEEK_SET')\n        if offset != 0:\n            raise NotImplementedError('MultiFileReader only supports seeking to start at this time')\n        for f in self._fileobjs:\n            f.seek(0, os.SEEK_SET)\n\n", "idx": 1478}
{"namespace": "boltons.listutils.BarrelList.insert", "completion": "        if index < 0:\n            index += len(self)\n        if index > len(self):\n            self.append(item)\n            return\n        list_idx, rel_idx = self._translate_index(index)\n        if list_idx is None:\n            self.append(item)\n            return\n        cur_list = self.lists[list_idx]\n        cur_list.insert(rel_idx, item)\n        self._balance_list(list_idx)\n", "idx": 1479}
{"namespace": "boltons.listutils.BarrelList.pop", "completion": "        if len(self.lists) == 1:\n            return self.lists[0].pop(*a)\n        else:\n            list_idx, rel_idx = self._translate_index(a[0])\n            if list_idx is None:\n                raise IndexError()\n            return self.lists[list_idx].pop(rel_idx)\n", "idx": 1480}
{"namespace": "boltons.listutils.BarrelList.sort", "completion": "        if len(self.lists) == 1:\n            self.lists[0].sort()\n        else:\n            tmp = list(self)\n            tmp.sort()\n            self.lists[:] = [tmp]\n        self._balance_list(0)\n", "idx": 1481}
{"namespace": "boltons.urlutils.URL.path", "completion": "        self.path_parts = tuple([unquote(p) if '%' in p else p\n                                 for p in path_text.split(u'/')])\n", "idx": 1482}
{"namespace": "boltons.urlutils.URL.navigate", "completion": "        if isinstance(dest, URL):\n            dest = dest.to_text()\n        elif isinstance(dest, bytes):\n            dest = dest.decode(DEFAULT_ENCODING)\n        elif not isinstance(dest, unicode):\n            raise TypeError(\"dest must be a string or URL object\")\n\n        # Parse the destination URL\n        dest_url = parse_url(dest)\n\n        # If the destination URL is a relative URL, then we need to\n        # parse the current URL and use it as the base URL.\n        if not dest_url['scheme']:\n            dest_url['scheme'] = self.scheme\n            dest_url['host'] = self.host\n            dest_url['port'] = self.port\n            dest_url['path'] = self.path\n            dest_url['query'] = self.query_params.to_text()\n\n        # Create a new URL object from the destination URL\n        new_url = URL.from_parts(**dest_url)\n\n        # Normalize the URL\n        new_url.normalize()\n\n        return new_url\n", "idx": 1483}
{"namespace": "boltons.urlutils.URL.to_text", "completion": "        parts = []\n        _add = parts.append\n        if self.scheme:\n            _add(self.scheme)\n            _add(':')\n        if self.uses_netloc:\n            _add(self.get_authority(full_quote=full_quote))\n            _add(self.uses_netloc._netloc_sep)\n        if self.path:\n            _add(quote_path_part(self.path, full_quote=full_quote))\n        if self.query_params:\n            _add('?')\n            _add(self.query_params.to_text(full_quote=full_quote))\n        if self.fragment:\n            _add('#')\n            _add(quote_fragment_part(self.fragment, full_quote=full_quote))\n        return u''.join(parts)\n", "idx": 1484}
{"namespace": "boltons.urlutils.QueryParamDict.to_text", "completion": "        pairs = []\n        for k, v in self.iteritems(multi=True):\n            k = quote(k, safe='')\n            if full_quote:\n                k = quote_plus(k, safe='')\n            if isinstance(v, (list, tuple)):\n                for vv in v:\n                    vv = quote(vv, safe='')\n                    if full_quote:\n                        vv = quote_plus(vv, safe='')\n                    pairs.append((k, vv))\n            else:\n                v = quote(v, safe='')\n                if full_quote:\n                    v = quote_plus(v, safe='')\n                pairs.append((k, v))\n        return urlencode(pairs, doseq=True)\n", "idx": 1485}
{"namespace": "boltons.tbutils.TracebackInfo.from_traceback", "completion": "        ret = []\n        if tb is None:\n            tb = sys.exc_info()[2]\n        if limit is None:\n            limit = getattr(sys, 'tracebacklimit', 1000)\n        n = 0\n        while tb is not None and n < limit:\n            item = cls.callpoint_type.from_tb(tb)\n            ret.append(item)\n            tb = tb.tb_next\n            n += 1\n        ret.reverse()\n        return cls(ret)\n", "idx": 1486}
{"namespace": "boltons.tbutils.ExceptionInfo.get_formatted", "completion": "        return 'Traceback (most recent call last):\\n' + self.tb_info.get_formatted() + self.exc_type + ': ' + self.exc_msg\n", "idx": 1487}
{"namespace": "boltons.tbutils.print_exception", "completion": "    if file is None:\n        file = sys.stderr\n    if tb:\n        for line in traceback.format_list(traceback.extract_tb(tb, limit=limit)):\n            file.write(line + '\\n')\n    lines = format_exception_only(etype, value)\n    for line in lines:\n        file.write(line)\n\n", "idx": 1488}
{"namespace": "boltons.tbutils.ParsedException.to_string", "completion": "        ret = []\n        ret.append('Traceback (most recent call last):')\n        for frame in self.frames:\n            ret.append('  File \"%s\", line %s, in %s' % (frame['filepath'], frame['lineno'], frame['funcname']))\n            ret.append('    %s' % frame['line'].strip())\n        ret.append('%s: %s' % (self.exc_type, self.exc_msg))\n        return '\\n'.join(ret)\n", "idx": 1489}
{"namespace": "boltons.tbutils.ParsedException.from_string", "completion": "        # TODO: handle SyntaxError\n        # TODO: handle non-ascii\n        # TODO: handle unicode\n        # TODO: handle non-utf8\n        # TODO: handle non-ascii\n        # TODO: handle non-unicode\n        # TODO: handle non-utf8\n        # TODO: handle non-ascii\n        # TODO: handle non-unicode\n        # TODO: handle non-utf8\n        # TODO: handle non-ascii\n        # TODO: handle non-unicode\n        # TODO: handle non-utf8\n        # TODO: handle non-ascii\n        # TODO: handle non-unicode\n        # TODO: handle non-utf8\n        # TODO: handle non-ascii\n        # TODO: handle non-unicode\n        # TODO: handle non-utf8\n        # TODO: handle non-ascii\n        # TODO: handle non-unicode\n        # TODO: handle non-utf8\n        # TODO: handle non-ascii\n        # TODO: handle non-unicode\n        # TODO: handle non-utf8\n        # TODO: handle non-ascii\n        # TODO: handle non-unicode\n        # TODO: handle non-utf8\n        # TODO: handle non-ascii\n        # TODO: handle non-unicode\n        # TODO: handle non-utf8\n        # TODO: handle non-ascii\n        # TODO: handle non-unicode\n        # TODO: handle non-utf8\n        # TODO: handle non-ascii\n        # TODO: handle non-unicode\n        # TODO: handle non-utf8\n        # TODO: handle non-ascii\n        # TODO: handle non-unicode\n        # TODO: handle non-utf8\n        # TODO: handle non-ascii\n        # TODO: handle non-unicode\n        # TODO: handle non-utf8\n        # TODO: handle non-ascii\n        # TODO: handle non-unicode\n        # TODO: handle non-utf8\n        # TODO: handle non-ascii\n        # TODO: handle non-unicode\n        # TODO: handle non-utf8\n        # TODO: handle non-ascii\n        # TODO", "idx": 1490}
{"namespace": "boltons.tableutils.Table.extend", "completion": "        if not data:\n            return\n\n        # TODO: this is a bit of a hack\n        if isinstance(data, Table):\n            data = data._data\n\n        # TODO: this is a bit of a hack\n        if isinstance(data, list):\n            data = data[0]\n\n        # TODO: this is a bit of a hack\n        if isinstance(data, dict):\n            data = data.values()\n\n        # TODO: this is a bit of a hack\n        if isinstance(data, tuple):\n            data = list(data)\n\n        # TODO: this is a bit of a hack\n        if isinstance(data, object):\n            data = [data]\n\n        # TODO: this is a bit of a hack\n        if isinstance(data, list):\n            data = data[0]\n\n        # TODO: this is a bit of a hack\n        if isinstance(data, dict):\n            data = data.values()\n\n        # TODO: this is a bit of a hack\n        if isinstance(data, tuple):\n            data = list(data)\n\n        # TODO: this is a bit of a hack\n        if isinstance(data, object):\n            data = [data]\n\n        # TODO: this is a bit of a hack\n        if isinstance(data, list):\n            data = data[0]\n\n        # TODO: this is a bit of a hack\n        if isinstance(data, dict):\n            data = data.values()\n\n        # TODO: this is a bit of a hack\n        if isinstance(data, tuple):\n            data = list(data)\n\n        # TODO: this is a bit of a hack\n        if isinstance(data, object):\n            data = [data]\n\n        # TODO: this is a bit of a hack\n        if isinstance(data, list):\n            data = data[0]\n\n        # TODO: this is a bit of a hack\n        if isinstance(data, dict):\n            data = data.values()\n\n        # TODO: this is a bit of a hack\n        if isinstance(data, tuple):\n            data = list(", "idx": 1491}
{"namespace": "boltons.tableutils.Table.from_object", "completion": "        return cls.from_data(data=data, headers=headers,\n                             max_depth=max_depth, _data_type=ObjectInputType(),\n                             metadata=metadata)\n", "idx": 1492}
{"namespace": "boltons.tableutils.Table.__repr__", "completion": "        return '{type name}(headers={headers!r}, data={data!r})'.format(type name=type(self), headers=self.headers, data=self._data)\n", "idx": 1493}
{"namespace": "boltons.tableutils.Table.to_text", "completion": "        if with_headers:\n            if self.headers:\n                header_row = ' | '.join(self.headers)\n            else:\n                header_row = ''\n            header_row = ' | '.join(self.headers)\n            header_row = ' | '.join(self.headers)\n            header_row = ' | '.join(self.headers)\n            header_row = ' | '.join(self.headers)\n            header_row = ' | '.join(self.headers)\n            header_row = ' | '.join(self.headers)\n            header_row = ' | '.join(self.headers)\n            header_row = ' | '.join(self.headers)\n            header_row = ' | '.join(self.headers)\n            header_row = ' | '.join(self.headers)\n            header_row = ' | '.join(self.headers)\n            header_row = ' | '.join(self.headers)\n            header_row = ' | '.join(self.headers)\n            header_row = ' | '.join(self.headers)\n            header_row = ' | '.join(self.headers)\n            header_row = ' | '.join(self.headers)\n            header_row = ' | '.join(self.headers)\n            header_row = ' | '.join(self.headers)\n            header_row = ' | '.join(self.headers)\n            header_row = ' | '.join(self.headers)\n            header_row = ' | '.join(self.headers)\n            header_row = ' | '.join(self.headers)\n            header_row = ' | '.join(self.headers)\n            header_row = ' | '.join(self.headers)\n            header_row = ' | '.join(self.headers)\n            header_row = ' | '.join(self.headers)\n            header_row = ' | '.join(self.headers)\n            header_row = ' | '.join(self.headers)\n            header_row = ' | '.join(self.headers)\n            header_row = ' | '.join(self.headers)\n            header_row = ' | '.join", "idx": 1494}
{"namespace": "boltons.statsutils.Stats.get_histogram_counts", "completion": "        if bins is None:\n            bins = self._get_bin_bounds(**kw)\n        elif isinstance(bins, int):\n            bins = self._get_bin_bounds(bins, **kw)\n        elif isinstance(bins, list):\n            bins = [float(b) for b in bins]\n\n        counts = [0] * (len(bins) - 1)\n        for d in self.data:\n            for i, b in enumerate(bins[:-1]):\n                if d >= b and d < bins[i + 1]:\n                    counts[i] += 1\n                    break\n        return list(zip(bins, counts))\n", "idx": 1495}
{"namespace": "boltons.setutils.IndexedSet.add", "completion": "        if item not in self.item_index_map:\n            self.item_index_map[item] = len(self.item_list)\n            self.item_list.append(item)\n", "idx": 1496}
{"namespace": "boltons.setutils.IndexedSet.pop", "completion": "        if index is None:\n            index = -1\n        elif index < 0:\n            index += len(self)\n        real_index = self._get_real_index(index)\n        item = self.item_list[real_index]\n        del self.item_index_map[item]\n        self.item_list[real_index] = _MISSING\n        self._add_dead(real_index)\n        self._cull()\n        return item\n", "idx": 1497}
{"namespace": "boltons.setutils.IndexedSet.index", "completion": "        if val not in self.item_index_map:\n            raise ValueError(f\"{val!r} is not in {type(self).__name__}\")\n        return self.item_index_map[val]\n", "idx": 1498}
{"namespace": "boltons.setutils.complement", "completion": "    class _ComplementSet(set):\n\n        def __init__(self, wrapped):\n            self.wrapped = wrapped\n\n        def __contains__(self, item):\n            return item not in self.wrapped\n\n        def __iter__(self):\n            for item in self.wrapped:\n                yield item\n\n        def __len__(self):\n            return len(self.wrapped)\n\n        def __repr__(self):\n            return 'complement(%r)' % self.wrapped\n\n        def __str__(self):\n            return 'complement(%r)' % self.wrapped\n\n        def add(self, item):\n            if item in self.wrapped:\n                raise ValueError('%r already in wrapped set' % item)\n            self.wrapped.add(item)\n\n        def clear(self):\n            self.wrapped.clear()\n\n        def difference(self, other):\n            return self.wrapped.difference(other)\n\n        def difference_update(self, other):\n            self.wrapped.difference_update(other)\n\n        def discard(self, item):\n            if item in self.wrapped:\n                self.wrapped.discard(item)\n\n        def intersection(self, other):\n            return self.wrapped.intersection(other)\n\n        def intersection_update(self, other):\n            self.wrapped.intersection_update(other)\n\n        def isdisjoint(self, other):\n            return self.wrapped.isdisjoint(other)\n\n        def issubset(self, other):\n            return self.wrapped.issubset(other)\n\n        def issuperset(self, other):\n            return self.wrapped.issuperset(other)\n\n        def pop(self):\n            return self.wrapped.pop()\n\n        def remove(self, item):\n            if item not in self.wrapped:\n                raise KeyError(item)\n            self.wrapped.remove(item)\n\n        def symmetric_difference(", "idx": 1499}
{"namespace": "boltons.strutils.strip_ansi", "completion": "    if isinstance(text, str):\n        return ANSI_SEQUENCES.sub('', text)\n    elif isinstance(text, (bytes, bytearray)):\n        return ANSI_SEQUENCES.sub(b'', text)\n    else:\n        raise TypeError('Invalid input type.')\n\n", "idx": 1500}
{"namespace": "boltons.strutils.asciify", "completion": "    # TODO: move to cliutils.py\n\n    # If the input is a unicode string, encode it as ASCII.\n    if isinstance(text, unicode):\n        text = text.encode('ascii', 'ignore')\n\n    # Create a translation table for ASCII conversion.\n    asc_conv = dict((ord(char), None) for char in text)\n\n    # Perform basic deaccenting for European characters.\n    text = text.translate({\n        0xe0: b'a', 0xc0: b'A', 0xf2: b'o', 0xd2: b'O', 0xf9: b'u', 0xd9: b'U', 0xc7: b'C', 0xfc: b'u', 0xec: b'c',\n        0xe7: b'c', 0xc6: b'A', 0xd0: b'D', 0xda: b'U', 0xf0: b'o', 0xd0: b'O', 0xfe: b'u', 0xde: b'U', 0xdf: b'ss',\n        0xf4: b'o', 0xb0: b'_', 0xe4: b'a', 0xc4: b'A', 0xf6: b'o', 0xd6: b'O', 0xfa: b'u', 0xda: b'U', 0xfc: b'u',\n        0xec: b'c', 0xc6: b'A', 0xd0: b'D', 0xde: b'U', 0xd4: b'O', 0xd9: b'U', 0xd6: b'O', 0xf1: b'n', 0xd1: b'N',\n        0xf3: b'o', 0xd3: b'O', 0xfa: b'u', 0xba: b'\\x84', 0xe3: b'a', 0xc3: b'", "idx": 1501}
{"namespace": "boltons.strutils.indent", "completion": "    indented_list = [\n        margin + line if key(line) else line\n        for line in text.splitlines()\n    ]\n    return newline.join(indented_list)\n\n", "idx": 1502}
{"namespace": "boltons.strutils.multi_replace", "completion": "    m = MultiReplace(sub_map, **kwargs)\n    return m.sub(text)\n\n", "idx": 1503}
{"namespace": "boltons.cacheutils.LRI._get_flattened_ll", "completion": "        flattened_ll = []\n        anchor = self._anchor\n        while True:\n            if anchor[VALUE] is _MISSING:\n                break\n            flattened_ll.append((anchor[KEY], anchor[VALUE]))\n            anchor = anchor[NEXT]\n        return flattened_ll\n", "idx": 1504}
{"namespace": "boltons.cacheutils.LRI.pop", "completion": "        with self._lock:\n            try:\n                value = super(LRI, self).pop(key)\n            except KeyError:\n                if default is _MISSING:\n                    raise\n                return default\n            self._remove_from_ll(key)\n            return value\n", "idx": 1505}
{"namespace": "boltons.cacheutils.LRI.popitem", "completion": "        with self._lock:\n            try:\n                key, value = super(LRI, self).popitem()\n            except KeyError:\n                raise KeyError('popitem(): cache is empty')\n            else:\n                self._remove_from_ll(key)\n            return key, value\n", "idx": 1506}
{"namespace": "boltons.cacheutils.LRI.clear", "completion": "        with self._lock:\n            super(LRI, self).clear()\n            self._init_ll()\n", "idx": 1507}
{"namespace": "boltons.cacheutils.LRI.setdefault", "completion": "        with self._lock:\n            try:\n                return self[key]\n            except KeyError:\n                self.soft_miss_count += 1\n                self[key] = default\n                return default\n", "idx": 1508}
{"namespace": "boltons.cacheutils.LRI.update", "completion": "        if hasattr(E, 'keys'):\n            for k in E.keys():\n                self[k] = E[k]\n        else:\n            for k, v in E:\n                self[k] = v\n        for k in F:\n            self[k] = F[k]\n", "idx": 1509}
{"namespace": "boltons.cacheutils.LRI.__repr__", "completion": "        return '{class name}(max_size={max size}, on_miss={on miss}, values={values})'.format(class name=self.__class__.__name__, max size=self.max_size, on miss=self.on_miss, values=dict(self))\n\n", "idx": 1510}
{"namespace": "boltons.cacheutils.CachedFunction.__repr__", "completion": "        if self.typed or not self.scoped:\n            return '{}(func={!r}, scoped={!r}, typed={!r})'.format(self.__class__.__name__, self.func, self.scoped, self.typed)\n        else:\n            return '{}(func={!r})'.format(self.__class__.__name__, self.func)\n\n", "idx": 1511}
{"namespace": "boltons.cacheutils.ThresholdCounter.elements", "completion": "        for key, value in self._count_map.items():\n            for i in range(value[0]):\n                yield key\n", "idx": 1512}
{"namespace": "boltons.cacheutils.ThresholdCounter.most_common", "completion": "        if n is None:\n            n = len(self)\n        return heapq.nlargest(n, enumerate(self._count_map.values()), key=lambda x: x[1][0])\n", "idx": 1513}
{"namespace": "boltons.cacheutils.ThresholdCounter.update", "completion": "        if iterable is not None:\n            if isinstance(iterable, Mapping):\n                for key, value in iterable.items():\n                    self.add(key)\n                    self.add(key)\n            else:\n                for key in iterable:\n                    self.add(key)\n                    self.add(key)\n        if kwargs:\n            for key, value in kwargs.items():\n                self.add(key)\n                self.add(key)\n", "idx": 1514}
{"namespace": "boltons.cacheutils.MinIDMap.get", "completion": "        if a in self.mapping:\n            return self.mapping[a]\n        else:\n            if self.free:\n                id = self.free.pop()\n            else:\n                id = len(self.ref_map)\n            self.mapping[a] = id\n            self.ref_map[id] = a\n            return id\n", "idx": 1515}
{"namespace": "boltons.iterutils.chunked", "completion": "    if count is not None:\n        count = int(count)\n        if count <= 0:\n            raise ValueError(\"count must be greater than 0\")\n\n    if size <= 0:\n        raise ValueError(\"size must be greater than 0\")\n\n    if count is not None and count * size > len(src):\n        raise ValueError(\"count * size must be less than or equal to the length of the input iterable\")\n\n    if \"fill\" in kw:\n        fill = kw[\"fill\"]\n    else:\n        fill = None\n\n    if fill is not None:\n        if not is_scalar(fill):\n            raise ValueError(\"fill must be a scalar\")\n\n    if count is None:\n        count = math.ceil(len(src) / size)\n\n    chunks = []\n    for i in range(count):\n        chunk = []\n        for j in range(size):\n            try:\n                chunk.append(src[i * size + j])\n            except IndexError:\n                if fill is not None:\n                    chunk.append(fill)\n                else:\n                    break\n        chunks.append(chunk)\n\n    return chunks\n\n", "idx": 1516}
{"namespace": "boltons.iterutils.chunk_ranges", "completion": "    if input_size < 0:\n        raise ValueError('input_size must be non-negative')\n    if chunk_size < 0:\n        raise ValueError('chunk_size must be non-negative')\n    if overlap_size < 0:\n        raise ValueError('overlap_size must be non-negative')\n    if input_offset < 0:\n        raise ValueError('input_offset must be non-negative')\n\n    if align:\n        input_offset = (chunk_size - overlap_size) * math.floor(input_offset / (chunk_size - overlap_size))\n\n    if input_size == 0:\n        return\n\n    if overlap_size == 0:\n        for i in range(input_offset, input_size + input_offset, chunk_size):\n            yield (i, i + chunk_size)\n    else:\n        for i in range(input_offset, input_size + input_offset, chunk_size - overlap_size):\n            yield (i, i + chunk_size)\n\n", "idx": 1517}
{"namespace": "boltons.iterutils.remap", "completion": "    if not callable(visit):\n        raise TypeError('visit must be callable')\n    if not callable(enter):\n        raise TypeError('enter must be callable')\n    if not callable(exit):\n        raise TypeError('exit must be callable')\n\n    reraise_visit = kwargs.pop('reraise_visit', True)\n    if kwargs:\n        raise TypeError('unexpected keyword arguments: %r' % kwargs.keys())\n\n    # The path is a tuple of keys/indices.\n    path = ()\n    # The parent is the current object.\n    parent = root\n    # The items are the items to visit.\n    items = enter(path, None, root)\n    if items is False:\n        return root\n    elif items is not True:\n        try:\n            items = list(items)\n        except TypeError:\n            raise TypeError('expected iterator, not %r' % items)\n\n    # The new parent is the blank object returned from \"enter\".\n    new_parent = items[0][0]\n    # The new items are the remapped items.\n    new_items = []\n    # The old items are the items from the parent.\n    old_items = items\n\n    # The index is the index of the current item.\n    index = 0\n    # The key is the key of the current item.\n    key = None\n    # The value is the value of the current item.\n    value = None\n\n    # The old parent is the parent of the current item.\n    old_parent = None\n    # The old key is the key of the current item.\n    old_key = None\n    # The old value is the value of the current item.\n    old_value = None\n\n    # The new key is the key of the current item.\n    new_key = None\n    # The new value is the value of the current item.\n    new_value = None\n\n    # The new path is the path of the current item.\n    new_path = None\n\n    # The new parent is the parent of the current item.\n    new_parent = None\n\n    # The new", "idx": 1518}
{"namespace": "boltons.iterutils.get_path", "completion": "    if not isinstance(path, tuple):\n        raise TypeError('path must be a tuple, not: %r' % path)\n    if default is not _UNSET:\n        try:\n            return remap(root, visit=lambda p, k, v: (k, v))\n        except Exception:\n            return default\n    return remap(root, visit=lambda p, k, v: (k, v))\n\n", "idx": 1519}
{"namespace": "boltons.iterutils.research", "completion": "    if not callable(query):\n        raise TypeError('query must be callable, not %r' % type(query))\n\n    path, registry, stack = (), {}, [(None, root)]\n    results = []\n    while stack:\n        key, value = stack.pop()\n        id_value = id(value)\n        if key is _REMAP_EXIT:\n            key, new_parent, old_parent = value\n            id_value = id(old_parent)\n            path, results = research(old_parent, query, reraise)\n            if results:\n                continue\n            path, new_items = new_parent\n            if new_items is not False:\n                stack.extend(reversed(list(new_items)))\n            continue\n        try:\n            if query(path, key, value):\n                results.append((path, value))\n        except Exception as exc:\n            if reraise:\n                raise\n            continue\n        try:\n            new_items = iter(value)\n        except TypeError:\n            continue\n        except Exception as exc:\n            if reraise:\n                raise\n            continue\n        registry[id_value] = new_items\n        stack.extend(reversed(list(new_items)))\n    return results\n\n", "idx": 1520}
{"namespace": "boltons.socketutils.BufferedSocket.getrecvbuffer", "completion": "        return self.rbuf\n", "idx": 1521}
{"namespace": "boltons.socketutils.BufferedSocket.getsendbuffer", "completion": "        with self._send_lock:\n            return self.sbuf\n", "idx": 1522}
{"namespace": "boltons.socketutils.BufferedSocket.recv", "completion": "        if flags != 0:\n            raise ValueError('non-zero flags not supported: {flags!r}'.format(flags=flags))\n\n        if timeout is _UNSET:\n            timeout = self.timeout\n\n        with self._recv_lock:\n            if len(self.rbuf) >= size:\n                data = self.rbuf[:size]\n                self.rbuf = self.rbuf[size:]\n                return data\n\n            if len(self.rbuf) > 0:\n                data = self.rbuf\n                self.rbuf = b''\n                return data\n\n            if timeout is None:\n                timeout = 0.0\n\n            self.sock.settimeout(timeout)\n            data = self.sock.recv(self._recvsize)\n            if len(data) == 0:\n                raise ConnectionClosed()\n\n            if len(data) > size:\n                self.rbuf = data[size:]\n                data = data[:size]\n\n            return data\n", "idx": 1523}
{"namespace": "boltons.socketutils.BufferedSocket.recv_close", "completion": "        with self._recv_lock:\n            if timeout is _UNSET:\n                timeout = self.timeout\n            if maxsize is _UNSET:\n                maxsize = self.maxsize\n            if len(self.rbuf) >= maxsize:\n                return self.rbuf[:maxsize]\n            data = self.recv_size(maxsize, timeout=timeout)\n            self.rbuf = data + self.rbuf\n            return data\n", "idx": 1524}
{"namespace": "boltons.socketutils.BufferedSocket.flush", "completion": "        with self._send_lock:\n            self.sock.send(b'')\n", "idx": 1525}
{"namespace": "boltons.socketutils.BufferedSocket.buffer", "completion": "        with self._send_lock:\n            self.sbuf.append(data)\n", "idx": 1526}
{"namespace": "boltons.socketutils.BufferedSocket.close", "completion": "        with self._send_lock:\n            self.sock.close()\n        with self._recv_lock:\n            self.rbuf = b''\n            self.sbuf = []\n", "idx": 1527}
{"namespace": "boltons.socketutils.NetstringSocket.setmaxsize", "completion": "        self.maxsize = maxsize\n        self._msgsize_maxsize = len(str(maxsize)) + 1  # len(str()) == log10\n", "idx": 1528}
{"namespace": "boltons.socketutils.NetstringSocket.write_ns", "completion": "        if len(payload) > self.maxsize:\n            raise NetstringMessageTooLong(len(payload), self.maxsize)\n\n        size_prefix = str(len(payload)).encode('ascii')\n        size_prefix = b':' + size_prefix\n        data = size_prefix + payload + b','\n        self.bsock.send(data)\n", "idx": 1529}
{"namespace": "boltons.fileutils.FilePerms.__repr__", "completion": "        return '%s(user=%r, group=%r, other=%r)' % (self.__class__.__name__,\n                                                    self._user, self._group,\n                                                    self._other)\n", "idx": 1530}
{"namespace": "boltons.mathutils.Bits.as_hex", "completion": "        template = '{{0:0{0}x}}'.format(int(self.len / 4))\n        return template.format(self.val)\n", "idx": 1531}
{"namespace": "boltons.mathutils.Bits.from_hex", "completion": "        if type(hex) is bytes:\n            hex = hex.decode('ascii')\n        if not hex.startswith('0x'):\n            hex = '0x' + hex\n        return cls(hex)\n", "idx": 1532}
{"namespace": "boltons.formatutils.split_format_str", "completion": "    # TODO: add tests\n    # TODO: add support for named fields\n    # TODO: add support for positional fields\n    # TODO: add support for conversion characters\n    # TODO: add support for format specs\n    # TODO: add support for nested format fields\n    # TODO: add support for non-string types\n    # TODO: add support for non-ascii characters\n    # TODO: add support for unicode\n    # TODO: add support for f-strings\n    # TODO: add support for format strings with no fields\n    # TODO: add support for format strings with only fields\n    # TODO: add support for format strings with only literals\n    # TODO: add support for format strings with only conversion characters\n    # TODO: add support for format strings with only format specs\n    # TODO: add support for format strings with only nested format fields\n    # TODO: add support for format strings with only non-string types\n    # TODO: add support for format strings with only non-ascii characters\n    # TODO: add support for format strings with only unicode\n    # TODO: add support for format strings with only f-strings\n    # TODO: add support for format strings with only positional fields\n    # TODO: add support for format strings with only named fields\n    # TODO: add support for format strings with only conversion characters\n    # TODO: add support for format strings with only format specs\n    # TODO: add support for format strings with only nested format fields\n    # TODO: add support for format strings with only non-string types\n    # TODO: add support for format strings with only non-ascii characters\n    # TODO: add support for format strings with only unicode\n    # TODO: add support for format strings with only f-strings\n    # TODO: add support for format strings with only positional fields\n    # TODO: add support for format strings with only named fields\n    # TODO: add support for format strings with only conversion characters\n    # TODO: add support for format strings with only format specs\n    # TODO: add support for format strings with only nested format fields\n    # TODO: add support for format strings with only non-string types\n    # TODO: add support for format strings with only non-ascii characters\n    # TODO: add support for format strings", "idx": 1533}
{"namespace": "boltons.formatutils.infer_positional_format_args", "completion": "    # Get the number of anonymous positional arguments in the format string.\n    num_anon_args = len(re.findall('{}', fstr))\n\n    # If there are no anonymous positional arguments, return the format string.\n    if num_anon_args == 0:\n        return fstr\n\n    # Get the number of named arguments in the format string.\n    num_named_args = len(re.findall('{.*[a-zA-Z_]+.*}', fstr))\n\n    # If there are no named arguments, set the starting number for the positional arguments to 1.\n    if num_named_args == 0:\n        num_start = 1\n    # If there are named arguments, set the starting number for the positional arguments to the number of named arguments plus 1.\n    else:\n        num_start = num_named_args + 1\n\n    # Create a dictionary with the starting number for the positional arguments as the key and the anonymous positional argument as the value.\n    anon_dict = dict(zip(range(num_start, num_start + num_anon_args), re.findall('{}', fstr)))\n\n    # Replace the anonymous positional arguments with their numbered counterparts.\n    for key, value in anon_dict.items():\n        fstr = fstr.replace(value, '{' + str(key) + '}', 1)\n\n    # Return the modified format string.\n    return fstr\n\n", "idx": 1534}
{"namespace": "boltons.formatutils.tokenize_format_str", "completion": "    # TODO: memoize\n    formatter = Formatter()\n    ret = []\n    for lit, fname, fspec, conv in formatter.parse(fstr):\n        if lit is not None:\n            ret.append(lit)\n        if fname is not None:\n            if resolve_pos:\n                fstr = infer_positional_format_args(fstr)\n            fname_list = re.split('[.[]', fname)\n            if len(fname_list) > 1:\n                raise ValueError('encountered compound format arg: %r' % fname)\n            try:\n                base_fname = fname_list[0]\n                assert base_fname\n            except (IndexError, AssertionError):\n                raise ValueError('encountered anonymous positional argument')\n            ret.append(BaseFormatField(base_fname, fspec, conv))\n    return ret\n\n", "idx": 1535}
{"namespace": "boltons.dictutils.OneToOne.clear", "completion": "        dict.clear(self)\n        dict.clear(self.inv)\n", "idx": 1536}
{"namespace": "boltons.dictutils.OneToOne.pop", "completion": "        if key in self:\n            val = self[key]\n            del self[key]\n            return val\n        if default is _MISSING:\n            raise KeyError(key)\n        return default\n", "idx": 1537}
{"namespace": "boltons.dictutils.OneToOne.popitem", "completion": "        try:\n            key = next(iter(self))\n        except StopIteration:\n            raise KeyError('dictionary is empty')\n        val = self[key]\n        del self[key]\n        return key, val\n", "idx": 1538}
{"namespace": "boltons.dictutils.ManyToMany.update", "completion": "        if isinstance(iterable, self.__class__):\n            for key in iterable.data:\n                self.data.setdefault(key, set()).update(iterable.data[key])\n            for key in iterable.inv.data:\n                self.inv.data.setdefault(key, set()).update(iterable.inv.data[key])\n        elif isinstance(iterable, dict):\n            for key in iterable:\n                self.data.setdefault(key, set()).update(iterable[key])\n        elif isinstance(iterable, list):\n            for key, val in iterable:\n                self.data.setdefault(key, set()).add(val)\n                self.inv.data.setdefault(val, set()).add(key)\n        else:\n            raise TypeError(\"unsupported operand type(s) for +: '%s' and '%s'\" % (type(self), type(iterable)))\n", "idx": 1539}
{"namespace": "boltons.dictutils.ManyToMany.add", "completion": "        if key not in self.data:\n            self.data[key] = set()\n        self.data[key].add(val)\n        if val not in self.inv.data:\n            self.inv.data[val] = set()\n        self.inv.data[val].add(key)\n", "idx": 1540}
{"namespace": "boltons.dictutils.ManyToMany.remove", "completion": "        self.data[key].remove(val)\n        if not self.data[key]:\n            del self.data[key]\n        self.inv.data[val].remove(key)\n        if not self.inv.data[val]:\n            del self.inv.data[val]\n", "idx": 1541}
{"namespace": "boltons.dictutils.ManyToMany.replace", "completion": "        # Check if the key is in the data dictionary.\n        if key in self.data:\n            # If the key is in the data dictionary, replace it with the new key.\n            self.data[newkey] = self.data.pop(key)\n            # Iterate through the values in the data dictionary.\n            for val in self.data[newkey]:\n                # Check if the value is in the inverse dictionary.\n                if val in self.inv.data:\n                    # If the value is in the inverse dictionary, replace the key with the new key.\n                    self.inv.data[val].remove(key)\n                    self.inv.data[val].add(newkey)\n        return\n", "idx": 1542}
{"namespace": "boltons.dictutils.ManyToMany.iteritems", "completion": "        for key in self.data:\n            for val in self.data[key]:\n                yield key, val\n", "idx": 1543}
{"namespace": "gunicorn.config.Config.__str__", "completion": "        settings = {}\n        for key, value in self.settings.items():\n            if callable(value):\n                value = \"<%s()>\" % value.__qualname__\n            settings[key] = value\n\n        max_length = max([len(key) for key in settings.keys()])\n        lines = []\n        for key, value in sorted(settings.items()):\n            line = \"%s:%s %s\" % (key, \" \" * (max_length - len(key)), value)\n            lines.append(line)\n        return \"\\n\".join(lines)\n", "idx": 1544}
{"namespace": "gunicorn.config.Config.set", "completion": "        if name not in self.settings:\n            raise AttributeError(\"No configuration setting for: %s\" % name)\n        self.settings[name].set(value)\n", "idx": 1545}
{"namespace": "gunicorn.config.Config.worker_class", "completion": "        uri = self.settings['worker_class'].get()\n\n        # are we using a threaded worker?\n        is_sync = uri.endswith('SyncWorker') or uri == 'sync'\n        if is_sync and self.threads > 1:\n            uri = \"gthread\"\n\n        # load the worker class\n        worker_class = util.load_class(uri)\n\n        # setup the worker class\n        if hasattr(worker_class, \"setup\"):\n            worker_class.setup()\n\n        return worker_class\n", "idx": 1546}
{"namespace": "gunicorn.config.Config.address", "completion": "        bind = self.settings['bind'].get()\n        return [x.strip() for x in bind.split(\",\") if x.strip()]\n", "idx": 1547}
{"namespace": "gunicorn.config.Config.logger_class", "completion": "        uri = self.settings['logger_class'].get()\n        if uri == \"simple\":\n            uri = \"gunicorn.glogging.Logger\"\n        elif uri == \"simple_with_statsd\":\n            uri = \"gunicorn.instrument.statsd.Statsd\"\n\n        try:\n            logger_class = util.load_class(uri, section=\"gunicorn.loggers\")\n            if hasattr(logger_class, \"install\"):\n                logger_class.install()\n        except:\n            raise ConfigError(\"Invalid logger class: %s\" % uri)\n\n        return logger_class\n", "idx": 1548}
{"namespace": "gunicorn.sock.create_sockets", "completion": "    listeners = []\n    # if we have a fd inherit, create the socket from it\n    if fds is not None:\n        for fd in fds:\n            listeners.append(_sock_type(\"inherit\")(conf, log, fd=fd))\n        return listeners\n\n    # otherwise create the sockets from the addresses\n    for addr in conf.addresses:\n        listeners.append(_sock_type(addr)(conf, log))\n\n    return listeners\n\n", "idx": 1549}
{"namespace": "gunicorn.http.body.LengthReader.read", "completion": "        if not isinstance(size, int):\n            raise TypeError(\"size must be an integral type\")\n        if size < 0:\n            raise ValueError(\"Size must be positive.\")\n        if size == 0:\n            return b\"\"\n\n        buf = io.BytesIO()\n        while buf.tell() < size:\n            data = self.unreader.read()\n            if not data:\n                raise NoMoreData()\n            buf.write(data)\n\n        data = buf.getvalue()\n        ret, rest = data[:size], data[size:]\n\n        self.unreader.unread(rest)\n        self.length -= size\n        return ret\n\n", "idx": 1550}
{"namespace": "gunicorn.http.body.EOFReader.read", "completion": "        if not isinstance(size, int):\n            raise TypeError(\"size must be an integral type\")\n\n        if size < 0:\n            raise ValueError(\"Size must be positive.\")\n\n        if size == 0:\n            return b\"\"\n\n        if self.finished:\n            buf = self.buf.getvalue()\n            ret, rest = buf[:size], buf[size:]\n            self.buf = io.BytesIO()\n            self.buf.write(rest)\n            return ret\n\n        data = self.unreader.read()\n        while data:\n            self.buf.write(data)\n            if self.buf.tell() >= size:\n                break\n            data = self.unreader.read()\n\n        if not data:\n            self.finished = True\n\n        buf = self.buf.getvalue()\n        ret, rest = buf[:size], buf[size:]\n        self.buf = io.BytesIO()\n        self.buf.write(rest)\n        return ret\n\n", "idx": 1551}
{"namespace": "gunicorn.http.body.Body.read", "completion": "        size = self.getsize(size)\n        if size == 0:\n            return b\"\"\n\n        if size < self.buf.tell():\n            self.buf.seek(0)\n            ret = self.buf.read(size)\n            rest = self.buf.read()\n            self.buf = io.BytesIO()\n            self.buf.write(rest)\n            return ret\n\n        while self.buf.tell() < size:\n            data = self.reader.read(1024)\n            if not data:\n                break\n            self.buf.write(data)\n\n        self.buf.seek(0)\n        ret = self.buf.read(size)\n        rest = self.buf.read()\n        self.buf = io.BytesIO()\n        self.buf.write(rest)\n        return ret\n", "idx": 1552}
{"namespace": "gunicorn.http.unreader.Unreader.read", "completion": "        if not isinstance(size, (int, long)):\n            raise TypeError(\"size parameter must be an int or long.\")\n        if size == 0:\n            return b''\n        if size < 0:\n            size = None\n\n        self.buf.seek(0, os.SEEK_END)\n        if size is None and self.buf.tell() > 0:\n            data = self.buf.read()\n            self.buf.seek(0)\n            self.buf.truncate()\n            return data\n\n        data = b''\n        while size is None or self.buf.tell() < size:\n            chunk = self.chunk()\n            if not chunk:\n                data += self.buf.read()\n                self.buf.seek(0)\n                self.buf.truncate()\n                return data\n            data += chunk\n\n        data += self.buf.read(size - self.buf.tell())\n        self.buf.seek(0)\n        self.buf.truncate()\n        return data\n", "idx": 1553}
{"namespace": "gunicorn.http.unreader.Unreader.unread", "completion": "        self.buf.write(data)\n\n", "idx": 1554}
{"namespace": "gunicorn.http.unreader.IterUnreader.chunk", "completion": "        try:\n            return next(self.iter)\n        except StopIteration:\n            self.iter = None\n            return b''\n\n", "idx": 1555}
{"namespace": "gunicorn.instrument.statsd.Statsd.critical", "completion": "        Logger.critical(self, msg, *args, **kwargs)\n        self.log_metric(METRIC_VAR, \"gunicorn.log.critical\", 1, COUNTER_TYPE)\n", "idx": 1556}
{"namespace": "gunicorn.instrument.statsd.Statsd.access", "completion": "        try:\n            # Calculate the duration of the request in milliseconds\n            duration = request_time.total_seconds() * 1000\n            # Log the duration as a histogram\n            self.histogram(\"gunicorn.request.duration\", duration)\n            # Increment the count of total requests\n            self.increment(\"gunicorn.request.count\")\n            # Increment the count of requests with the status code of the response\n            self.increment(\"gunicorn.request.status.%s\" % resp.status.split()[0])\n        except Exception:\n            Logger.warning(self, \"Failed to log to statsd\", exc_info=True)\n", "idx": 1557}
{"namespace": "praw.exceptions.RedditErrorItem.error_message", "completion": "        error_message = self.error_type\n        if self.message:\n            error_message += f\": {self.message}\"\n        if self.field:\n            error_message += f\" on field {self.field}\"\n        return error_message\n", "idx": 1558}
{"namespace": "praw.exceptions.RedditErrorItem.__repr__", "completion": "        return f\"{self.__class__.__name__}(error_type={self.error_type!r}, message={self.message!r}, field={self.field!r})\"\n\n", "idx": 1559}
{"namespace": "praw.models.util.BoundedSet.add", "completion": "        self._access(item)\n        if item not in self._set:\n            self._set[item] = None\n            if len(self._set) > self.max_items:\n                self._set.popitem(last=False)\n", "idx": 1560}
{"namespace": "praw.models.util.ExponentialCounter.counter", "completion": "        jitter = random.uniform(-self._base / 16, self._base / 16)\n        value = self._base + jitter\n        if self._base < self._max / 2:\n            self._base *= 2\n        else:\n            self._base = self._max\n        return value\n", "idx": 1561}
{"namespace": "praw.models.listing.generator.ListingGenerator._extract_sublist", "completion": "        if isinstance(listing, list):\n            return listing[1]\n        elif isinstance(listing, dict):\n            if \"flair\" in listing:\n                return listing[\"flair\"]\n            elif \"modnote\" in listing:\n                return listing[\"modnote\"]\n            else:\n                raise ValueError(\"The generator returned a dictionary PRAW didn't recognize. File a bug report at PRAW.\")\n        else:\n            raise ValueError(\"The generator returned a type PRAW didn't recognize. File a bug report at PRAW.\")\n", "idx": 1562}
{"namespace": "praw.util.token_manager.FileTokenManager.post_refresh_callback", "completion": "        with open(self._filename, 'w') as fp:\n            fp.write(authorizer.refresh_token)\n", "idx": 1563}
{"namespace": "praw.util.token_manager.FileTokenManager.pre_refresh_callback", "completion": "        with open(self._filename, \"r\") as fp:\n            authorizer.refresh_token = fp.read()\n\n", "idx": 1564}
{"namespace": "praw.util.token_manager.SQLiteTokenManager._get", "completion": "        cursor = self._connection.execute(\n            \"SELECT refresh_token FROM tokens WHERE id=?\", (self.key,)\n        )\n        row = cursor.fetchone()\n        if row is None:\n            raise KeyError(self.key)\n        return row[0]\n", "idx": 1565}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.is_registered", "completion": "        cursor = self._connection.execute(\n            \"SELECT refresh_token FROM tokens WHERE id=?\", (self.key,)\n        )\n        result = cursor.fetchone()\n        if result is None:\n            return False\n        else:\n            return True\n", "idx": 1566}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.post_refresh_callback", "completion": "        self._set(authorizer.refresh_token)\n        authorizer.refresh_token = None\n", "idx": 1567}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.pre_refresh_callback", "completion": "        if authorizer.refresh_token is None:\n            authorizer.refresh_token = self._get()\n", "idx": 1568}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.register", "completion": "        cursor = self._connection.execute(\n            \"SELECT refresh_token FROM tokens WHERE id=?\", (self.key,)\n        )\n        result = cursor.fetchone()\n        if result is None:\n            self._set(refresh_token)\n            return True\n        return False", "idx": 1569}
{"namespace": "jc.cli.JcCli.about_jc", "completion": "        import jc.parsers\n        import platform\n        import json\n        import sys\n        import os\n        import importlib\n\n        # get the path to the parsers directory\n        parsers_path = os.path.dirname(jc.parsers.__file__)\n\n        # get the path to the plugins directory\n        plugins_path = os.path.join(parsers_path, 'plugins')\n\n        # get the path to the parsers.info file\n        parsers_info_path = os.path.join(parsers_path, 'parsers.info')\n\n        # get the path to the plugins.info file\n        plugins_info_path = os.path.join(plugins_path, 'plugins.info')\n\n        # get the path to the parsers.json file\n        parsers_json_path = os.path.join(parsers_path, 'parsers.json')\n\n        # get the path to the plugins.json file\n        plugins_json_path = os.path.join(plugins_path, 'plugins.json')\n\n        # get the path to the parsers.json file\n        parsers_json_path = os.path.join(parsers_path, 'parsers.json')\n\n        # get the path to the plugins.json file\n        plugins_json_path = os.path.join(plugins_path, 'plugins.json')\n\n        # get the path to the parsers.json file\n        parsers_json_path = os.path.join(parsers_path, 'parsers.json')\n\n        # get the path to the plugins.json file\n        plugins_json_path = os.path.join(plugins_path, 'plugins.json')\n\n        # get the path to the parsers.json file\n        parsers_json_path = os.path.join(parsers_path, 'parsers.json')\n\n        # get the path to the plugins.json file\n        plugins_json_path = os.path.join(plugins_path, 'plugins.json')\n\n        # get the path to the parsers.json file\n        parsers_json_path = os.", "idx": 1570}
{"namespace": "jc.cli.JcCli.yaml_out", "completion": "        try:\n            import ruamel.yaml\n            from ruamel.yaml.compat import StringIO\n            from ruamel.yaml.representer import SafeRepresenter\n            from ruamel.yaml.dumper import Dumper\n            from ruamel.yaml.tokens import (\n                CommentToken,\n                StreamStartToken,\n                StreamEndToken,\n                BlockEndToken,\n                BlockEntryToken,\n                PlainToken,\n                KeyToken,\n                ValueToken,\n                AliasToken,\n                ScalarToken,\n                FlowMappingStartToken,\n                FlowMappingEndToken,\n                FlowSequenceStartToken,\n                FlowSequenceEndToken,\n                FlowEntryToken,\n                BlockSequenceStartToken,\n                BlockSequenceEndToken,\n                BlockMappingStartToken,\n                BlockMappingEndToken,\n                DirectiveToken,\n                DocumentStartToken,\n                DocumentEndToken,\n                StreamEndToken,\n                CommentToken,\n                DocumentStartToken,\n                DocumentEndToken,\n                AliasToken,\n                AnchorToken,\n                TagToken,\n                ScalarToken,\n                BlockEndToken,\n                BlockEntryToken,\n                FlowEntryToken,\n                FlowEndToken,\n                FlowMappingEndToken,\n                FlowSequenceEndToken,\n                BlockSequenceEndToken,\n                BlockEndToken,\n                BlockEntryToken,\n                FlowEntryToken,\n                FlowEndToken,\n                FlowMappingEndToken,\n                FlowSequenceEndToken,\n                BlockEndToken,\n                BlockEntryToken,\n                FlowEntryToken,\n                FlowEndToken,\n                FlowMappingEndToken,\n                FlowSequenceEndToken,\n                BlockEndToken,\n                BlockEntryToken,\n                FlowEntryToken,\n                FlowEndToken,\n                FlowMappingEndToken,\n                FlowSequenceEndToken,\n                BlockEndToken,\n                BlockEntryToken,\n                FlowEntryToken,\n                FlowEndToken,\n                FlowMappingEndToken,\n                FlowSequenceEndToken,\n                BlockEndToken,\n                BlockEntryToken,\n                FlowEntryToken,\n                FlowEndToken,\n                FlowMappingEndToken,\n                FlowSequenceEndToken,\n                BlockEndToken,\n               ", "idx": 1571}
{"namespace": "jc.parsers.os_release.parse", "completion": "    return jc.parsers.kv.parse(data, raw, quiet, _process)", "idx": 1572}
{"namespace": "jc.parsers.xrandr._parse_screen", "completion": "    screen_match = re.match(_screen_pattern, next_lines.pop(0))\n    if screen_match is None:\n        next_lines.insert(0, next_lines.pop(0))\n        return None\n\n    screen_dict = screen_match.groupdict()\n    screen_dict[\"devices\"] = []\n\n    while len(next_lines) > 0:\n        device_dict = _parse_device(next_lines)\n        if device_dict is not None:\n            screen_dict[\"devices\"].append(device_dict)\n        else:\n            break\n\n    return screen_dict\n\n", "idx": 1573}
{"namespace": "jc.parsers.xrandr._parse_model", "completion": "    if not next_lines:\n        return None\n\n    next_line = next_lines.pop()\n    result = re.match(_edid_head_pattern, next_line)\n    if not result:\n        next_lines.append(next_line)\n        return None\n\n    edid_lines = []\n    while next_lines:\n        next_line = next_lines.pop()\n        result = re.match(_edid_line_pattern, next_line)\n        if result:\n            edid_lines.append(result.group(\"edid_line\"))\n        else:\n            if not quiet:\n                jc.utils.warning_message(\n                    [f\"{next_line} : does not match EDID line pattern\"]\n                )\n            next_lines.append(next_line)\n            break\n\n    if not edid_lines:\n        return None\n\n    edid_hex = \"\".join(edid_lines)\n    edid_bytes = jc.utils.hex_convert(edid_hex)\n\n    return _parse_edid(edid_bytes)\n\n", "idx": 1574}
{"namespace": "jc.parsers.xrandr._parse_mode", "completion": "    result = re.match(_mode_pattern, line)\n    if not result:\n        return None\n\n    matches = result.groupdict()\n    mode: Mode = {\n        \"resolution_width\": int(matches[\"resolution_width\"]),\n        \"resolution_height\": int(matches[\"resolution_height\"]),\n        \"is_high_resolution\": matches[\"is_high_resolution\"] is not None,\n        \"frequencies\": [],\n    }\n\n    frequencies_result = re.findall(_frequencies_pattern, matches[\"rest\"])\n    for frequency_match in frequencies_result:\n        frequency_matches = frequency_match[0]\n        frequency = float(frequency_matches[\"frequency\"])\n        is_current = frequency_matches[\"star\"] == \"*\"\n        is_preferred = frequency_matches[\"plus\"] == \"+\"\n        mode[\"frequencies\"].append(\n            {\n                \"frequency\": frequency,\n                \"is_current\": is_current,\n                \"is_preferred\": is_preferred,\n            }\n        )\n\n    return mode\n\n", "idx": 1575}
{"namespace": "pythonforandroid.archs.Arch.include_dirs", "completion": "        return [\n            join(self.ctx.ndk.sysroot_usr_include, 'jni', 'include'),\n            join(self.ctx.ndk.sysroot_usr_include, 'linux', 'include'),\n            join(self.ctx.ndk.sysroot_usr_include, 'libc', 'include'),\n            join(self.ctx.ndk.sysroot_usr_include, 'libc', 'usr', 'include'),\n            join(self.ctx.ndk.sysroot_usr_include, 'libc', 'kernel', 'common'),\n            join(self.ctx.ndk.sysroot_usr_include, 'libc', 'kernel', 'public'),\n            join(self.ctx.ndk.sysroot_usr_include, 'libc', 'kernel', 'private'),\n            join(self.ctx.ndk.sysroot_usr_include, 'libc', 'kernel', 'uapi'),\n            join(self.ctx.ndk.sysroot_usr_include, 'libc', 'kernel', 'uapi', 'asm-{arch}'),\n            join(self.ctx.ndk.sysroot_usr_include, 'libc', 'kernel', 'uapi', 'asm-generic'),\n            join(self.ctx.ndk.sysroot_usr_include, 'libc', 'kernel', 'uapi', 'asm-generic', 'asm'),\n            join(self.ctx.ndk.sysroot_usr_include, 'libc', 'kernel', 'uapi', 'asm-generic', 'mman'),\n            join(self.ctx.ndk.sysroot_usr_include, 'libc', 'kernel', 'uapi', 'asm-generic', 'sysinfo'),\n            join(self.ctx.ndk.sysroot_usr_include, 'libc', 'kernel', 'uapi', 'asm-generic', 'unistd'),\n            join(self.ctx.ndk.sysroot_usr_include, 'libc', 'kernel', 'uapi', 'asm-generic', 'utsname'),\n            join(self.ctx.ndk.sysroot_usr_include, 'lib", "idx": 1576}
{"namespace": "pythonforandroid.archs.Arch.target", "completion": "        return \"{}-{}\".format(self.command_prefix, self.ctx.ndk_api)\n", "idx": 1577}
{"namespace": "pythonforandroid.archs.ArchARM.target", "completion": "        return '{triplet}{ndk_api}'.format(\n            triplet=self.command_prefix,\n            ndk_api=self.ctx.ndk_api\n        )\n", "idx": 1578}
{"namespace": "pythonforandroid.recipe.Recipe.get_recipe", "completion": "        if name in cls.recipes:\n            return cls.recipes[name]\n\n        for recipes_dir in cls.recipe_dirs(ctx):\n            if not exists(recipes_dir):\n                continue\n            modules = glob.glob(join(recipes_dir, name, '*.py'))\n            if not modules:\n                continue\n            if len(modules) > 1:\n                raise ValueError(\n                    'multiple modules in one recipe directory are not allowed')\n            module = basename(modules[0])\n            if not module.endswith('.py'):\n                continue\n            module = module[:-3]\n\n            modlib = 'p4a.recipes.{name}.{mod}'.format(name=name, mod=module)\n            if modlib in sys.modules:\n                del sys.modules[modlib]\n\n            try:\n                mod = importlib.import_module(modlib)\n            except ImportError:\n                raise\n\n            for mod_cls_name in dir(mod):\n                mod_cls = getattr(mod, mod_cls_name)\n                if (\n                    not inspect.isclass(mod_cls) or\n                    not issubclass(mod_cls, Recipe)\n                ):\n                    continue\n                cls.recipes[name] = mod_cls\n                return mod_cls\n\n        raise ValueError('No recipe named {} found'.format(name))\n", "idx": 1579}
{"namespace": "pythonforandroid.prerequisites.HomebrewPrerequisite.darwin_helper", "completion": "        info(\n            \"Homebrew is not supported on macOS. Please install Homebrew using the instructions at https://brew.sh/\"\n        )\n", "idx": 1580}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_checker", "completion": "        return shutil.which(\"openssl\") is not None\n", "idx": 1581}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_pkg_config_location", "completion": "        prefix = self._darwin_get_brew_formula_location_prefix(\n            self.homebrew_formula_name, installed=True\n        )\n        return os.path.join(prefix, \"lib\", \"pkgconfig\")\n", "idx": 1582}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_installer", "completion": "        # Check if Homebrew is installed\n        if not shutil.which(\"brew\"):\n            info(\"Homebrew is not installed, installing it ...\")\n            subprocess.check_output([\"/bin/bash\", \"-c\", \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"])\n\n        # Check if OpenSSL is installed\n        if not self.darwin_checker():\n            info(\"OpenSSL is not installed, installing it ...\")\n            subprocess.check_output([\"brew\", \"install\", self.homebrew_formula_name])\n\n        # Check if OpenSSL is installed\n        if not self.darwin_checker():\n            error(\"Failed to install OpenSSL\")\n\n", "idx": 1583}
{"namespace": "pythonforandroid.prerequisites.AutoconfPrerequisite.darwin_installer", "completion": "        info(\"Installing Autoconf ...\")\n        subprocess.check_output([\"brew\", \"install\", \"autoconf\"])\n\n", "idx": 1584}
{"namespace": "pythonforandroid.prerequisites.AutomakePrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"automake\", installed=True)\n            is not None\n        )\n", "idx": 1585}
{"namespace": "pythonforandroid.prerequisites.AutomakePrerequisite.darwin_installer", "completion": "        info(\"Installing Automake ...\")\n        subprocess.check_output([\"brew\", \"install\", \"automake\"])\n\n", "idx": 1586}
{"namespace": "pythonforandroid.prerequisites.LibtoolPrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"libtool\", installed=True)\n            is not None\n        )\n", "idx": 1587}
{"namespace": "pythonforandroid.prerequisites.LibtoolPrerequisite.darwin_installer", "completion": "        info(\"Installing Libtool ...\")\n        subprocess.check_output([\"brew\", \"install\", \"libtool\"])\n\n", "idx": 1588}
{"namespace": "pythonforandroid.prerequisites.PkgConfigPrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"pkg-config\", installed=True)\n            is not None\n        )\n", "idx": 1589}
{"namespace": "pythonforandroid.prerequisites.PkgConfigPrerequisite.darwin_installer", "completion": "        info(\"Installing Pkg-Config ...\")\n        subprocess.check_output([\"brew\", \"install\", \"pkg-config\"])\n\n", "idx": 1590}
{"namespace": "pythonforandroid.prerequisites.CmakePrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"cmake\", installed=True)\n            is not None\n        )\n", "idx": 1591}
{"namespace": "pythonforandroid.prerequisites.CmakePrerequisite.darwin_installer", "completion": "        info(\"Installing Cmake ...\")\n        subprocess.check_output([\"brew\", \"install\", \"cmake\"])\n\n", "idx": 1592}
{"namespace": "pythonforandroid.prerequisites.get_required_prerequisites", "completion": "    prerequisites = [\n        HomebrewPrerequisite(),\n        JDKPrerequisite(),\n        OpenSSLPrerequisite(),\n        AutoconfPrerequisite(),\n        AutomakePrerequisite(),\n        LibtoolPrerequisite(),\n        PkgConfigPrerequisite(),\n        CmakePrerequisite(),\n    ]\n\n    return [\n        prerequisite\n        for prerequisite in prerequisites\n        if prerequisite.mandatory[platform]\n    ]\n\n", "idx": 1593}
{"namespace": "pythonforandroid.pythonpackage.parse_as_folder_reference", "completion": "    # Check if the dependency is a local folder path:\n    if dep.startswith(\"/\") or dep.startswith(\"./\") or dep.startswith(\"../\"):\n        # This is a local folder path.\n        # Resolve it:\n        return os.path.abspath(os.path.expanduser(dep))\n\n    # Check if the dependency is a file:// URL:\n    if dep.startswith(\"file://\"):\n        # This is a file:// URL.\n        # Resolve it:\n        return urlunquote(urlparse(dep).path)\n\n    # This is not a local folder path.\n    return None\n\n", "idx": 1594}
{"namespace": "pythonforandroid.pythonpackage.get_package_name", "completion": "    if use_cache and dependency in package_name_cache:\n        if time.time() - package_name_cache[dependency][1] < 86400:\n            return package_name_cache[dependency][0]\n    package_name = _extract_info_from_package(dependency, \"name\")\n    package_name_cache[dependency] = (package_name, time.time())\n    return package_name\n\n", "idx": 1595}
{"namespace": "pythonforandroid.recommendations.read_ndk_version", "completion": "    try:\n        with open(join(ndk_dir, 'source.properties')) as f:\n            for line in f:\n                if line.startswith('Pkg.Revision'):\n                    return LooseVersion(line.strip().split('=')[1])\n    except IOError:\n        warning(PARSE_ERROR_NDK_MESSAGE)\n        return None\n\n", "idx": 1596}
{"namespace": "pythonforandroid.recommendations.check_target_api", "completion": "    if api < MIN_TARGET_API:\n        warning(OLD_API_MESSAGE)\n\n    if api < ARMEABI_MAX_TARGET_API and arch == \"armeabi\":\n        warning(UNSUPPORTED_NDK_API_FOR_ARMEABI_MESSAGE.format(\n            req_ndk_api=MIN_TARGET_API, max_ndk_api=ARMEABI_MAX_TARGET_API))\n\n", "idx": 1597}
{"namespace": "pythonforandroid.recommendations.check_ndk_api", "completion": "    if ndk_api < MIN_NDK_API:\n        warning(OLD_NDK_API_MESSAGE)\n\n    if ndk_api > android_api:\n        raise BuildInterruptingException(\n            TARGET_NDK_API_GREATER_THAN_TARGET_API_MESSAGE.format(\n                ndk_api=ndk_api, android_api=android_api\n            ),\n            instructions='Decrease the NDK API version or increase the target Android API version',\n        )\n\n", "idx": 1598}
{"namespace": "pythonforandroid.androidndk.AndroidNDK.llvm_prebuilt_dir", "completion": "        return os.path.join(self.ndk_dir, \"toolchains\", \"llvm\", \"prebuilt\", self.host_tag)\n", "idx": 1599}
{"namespace": "pythonforandroid.build.Context.setup_dirs", "completion": "        self.storage_dir = expanduser(storage_dir)\n        self.build_dir = join(self.storage_dir, 'build')\n        self.dist_dir = join(self.storage_dir, 'dists')\n", "idx": 1600}
{"namespace": "pythonforandroid.graph.get_dependency_tuple_list_for_recipe", "completion": "    if blacklist is None:\n        blacklist = set()\n\n    recipe_dependencies = recipe.depends\n    recipe_dependencies = fix_deplist(recipe_dependencies)\n    recipe_dependencies = [\n        dep for dep in recipe_dependencies if dep not in blacklist\n    ]\n    recipe_dependencies = [\n        dep if isinstance(dep, tuple) else (dep,) for dep in recipe_dependencies\n    ]\n    return recipe_dependencies\n\n", "idx": 1601}
{"namespace": "pythonforandroid.graph.obvious_conflict_checker", "completion": "    if blacklist is None:\n        blacklist = set()\n\n    # Get all recipes\n    all_recipes = set(Recipe.list_recipes(ctx))\n    # Get all recipes to be added\n    recipes_to_add = set(name_tuples)\n    # Get all recipes to be added before\n    recipes_to_add_before = set(name_tuples)\n    # Get all recipes to be added after\n    recipes_to_add_after = set(name_tuples)\n    # Get all recipes to be added after\n    recipes_to_add_after = set(name_tuples)\n    # Get all recipes to be added after\n    recipes_to_add_after = set(name_tuples)\n    # Get all recipes to be added after\n    recipes_to_add_after = set(name_tuples)\n    # Get all recipes to be added after\n    recipes_to_add_after = set(name_tuples)\n    # Get all recipes to be added after\n    recipes_to_add_after = set(name_tuples)\n    # Get all recipes to be added after\n    recipes_to_add_after = set(name_tuples)\n    # Get all recipes to be added after\n    recipes_to_add_after = set(name_tuples)\n    # Get all recipes to be added after\n    recipes_to_add_after = set(name_tuples)\n    # Get all recipes to be added after\n    recipes_to_add_after = set(name_tuples)\n    # Get all recipes to be added after\n    recipes_to_add_after = set(name_tuples)\n    # Get all recipes to be added after\n    recipes_to_add_after = set(name_tuples)\n    # Get all recipes to be added after\n    recipes_to_add_after = set(name_tuples)\n    # Get all recipes to be added after\n    recipes_to_add_after = set(name", "idx": 1602}
{"namespace": "pythonforandroid.graph.get_recipe_order_and_bootstrap", "completion": "    # Clean up input names:\n    names = [name.lower() for name in names]\n    if bs is None:\n        bs = ctx.bootstrap\n    if blacklist is None:\n        blacklist = set()\n\n    # Add bootstrap dependencies:\n    if bs:\n        names.extend(bs.distribution_name)\n        names.extend(bs.extra_distribute)\n\n    # Check for conflicts:\n    obvious_conflict_checker(ctx, names, blacklist=blacklist)\n\n    # Generate all possible order graphs:\n    all_orders = []\n    for name in names:\n        all_orders.extend(\n            recursively_collect_orders(\n                name, ctx, names, orders=[RecipeOrder(ctx)], blacklist=blacklist\n            )\n        )\n\n    # Convert each order graph into a linear list:\n    all_orders = [list(find_order(order)) for order in all_orders]\n\n    # Sort the order graphs based on preference:\n    all_orders.sort(key=lambda order: ctx.preferred_host_to_target(order))\n\n    # Choose the first order:\n    chosen_order = all_orders[0]\n\n    # Get the corresponding recipes:\n    recipes = [Recipe.get_recipe(name, ctx) for name in chosen_order]\n\n    # Get the corresponding python modules:\n    modules = [recipe.get_recipe_dir() for recipe in recipes]\n\n    return chosen_order, recipes, modules, bs", "idx": 1603}
{"namespace": "pythonforandroid.util.ensure_dir", "completion": "    if not exists(dn):\n        makedirs(dn)\n\n", "idx": 1604}
{"namespace": "pythonforandroid.util.move", "completion": "    LOGGER.debug(\"Moving from {} to {}\".format(source, destination))\n    shutil.move(source, destination)\n\n", "idx": 1605}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.get_bootstrap_from_recipes", "completion": "        # Get all the possible bootstraps:\n        possible_bootstraps = cls.get_usable_bootstraps_for_recipes(recipes, ctx)\n\n        # If there are no possible bootstraps, return None:\n        if len(possible_bootstraps) == 0:\n            return None\n\n        # If there is only one possible bootstrap, return it:\n        if len(possible_bootstraps) == 1:\n            return possible_bootstraps[0]\n\n        # If there are more than one possible bootstrap, we need to determine which one to return:\n        # 1. Check if there is a bootstrap with a recipe name in the recipes list:\n        for bs in possible_bootstraps:\n            if bs.name in recipes:\n                return bs\n\n        # 2. Check if there is a bootstrap with a recipe name in the recipes list:\n        for bs in possible_bootstraps:\n            for recipe in recipes:\n                if recipe in bs.recipe_depends:\n                    return bs\n\n        # 3. Check if there is a bootstrap with a recipe name in the recipes list:\n        for bs in possible_bootstraps:\n            for recipe in recipes:\n                recipe = Recipe.get_recipe(recipe, ctx)\n                if recipe.name in bs.recipe_depends:\n                    return bs\n\n        # 4. If there is a webview bootstrap, return it:\n        for bs in possible_bootstraps:\n            if \"webview\" in bs.recipe_depends:\n                return bs\n\n        # 5. If there is an SDL2 bootstrap, return it:\n        for bs in possible_bootstraps:\n            if \"sdl2\" in bs.recipe_depends:\n                return bs\n\n        # 6. If there is a bootstrap with a recipe name in the recipes list:\n        for bs in possible_bootstraps:\n            for recipe in recipes:\n                recipe = Recipe.get_recipe(recipe, ctx)\n                if recipe.name", "idx": 1606}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.get_bootstrap", "completion": "        if name is None:\n            return None\n        if not hasattr(cls, 'all_bootstraps'):\n            cls.all_bootstraps = cls.all_bootstraps()\n        if name not in cls.all_bootstraps:\n            raise ValueError('Unknown bootstrap: \"{}\"'.format(name))\n        filen = join(ctx.root_dir, 'bootstraps', name + '.py')\n        modul = importlib.machinery.SourceFileLoader(name, filen).load_module()\n        return modul.bootstrap\n", "idx": 1607}
{"namespace": "pythonforandroid.bootstrap.expand_dependencies", "completion": "    recipes_with_alternatives = []\n    recipes_without_alternatives = []\n    for recipe in recipes:\n        if recipe.alternative_recipes:\n            recipes_with_alternatives.append(recipe)\n        else:\n            recipes_without_alternatives.append(recipe)\n\n    recipes_with_alternatives_expanded = []\n    for recipe in recipes_with_alternatives:\n        recipes_with_alternatives_expanded.append(recipe.alternative_recipes)\n\n    recipes_without_alternatives_expanded = []\n    for recipe in recipes_without_alternatives:\n        recipes_without_alternatives_expanded.append([recipe])\n\n    recipes_with_alternatives_expanded = list(itertools.product(\n        *recipes_with_alternatives_expanded))\n\n    recipes_without_alternatives_expanded = list(itertools.product(\n        *recipes_without_alternatives_expanded))\n\n    recipes_expanded = list(itertools.chain(\n        recipes_with_alternatives_expanded, recipes_without_alternatives_expanded))\n\n    recipes_expanded_with_dependencies = []\n    for recipe_combination in recipes_expanded:\n        recipes_expanded_with_dependencies.append(\n            [recipe for recipe in recipe_combination])\n\n    return recipes_expanded_with_dependencies", "idx": 1608}
{"namespace": "pythonforandroid.recipes.icu.ICURecipe.get_recipe_dir", "completion": "        if self.ctx.local_recipes is None:\n            return join(self.ctx.root_dir, 'recipes', 'icu4c')\n        else:\n            if isdir(join(self.ctx.local_recipes, 'icu4c')):\n                return join(self.ctx.local_recipes, 'icu4c')\n            else:\n                return join(self.ctx.root_dir, 'recipes', 'icu4c')\n", "idx": 1609}
{"namespace": "mmcv.transforms.wrappers.KeyMapper.__repr__", "completion": "        return self.__class__.__name__ + \\\n            f'(transforms={self.transforms}, mapping={self.mapping}, remapping={self.remapping}, auto_remap={self.auto_remap}, allow_nonexist_keys={self.allow_nonexist_keys})'\n\n", "idx": 1610}
{"namespace": "mmcv.transforms.wrappers.TransformBroadcaster.__repr__", "completion": "        repr_str = self.__class__.__name__\n        repr_str += f'(transforms = {self.transforms}'\n        repr_str += f', mapping = {self.mapping}'\n        repr_str += f', remapping = {self.remapping}'\n        repr_str += f', auto_remap = {self.auto_remap}'\n        repr_str += f', allow_nonexist_keys = {self.allow_nonexist_keys}'\n        repr_str += f', share_random_params = {self.share_random_params})'\n        return repr_str\n\n", "idx": 1611}
{"namespace": "mackup.utils.delete", "completion": "    if not os.path.exists(filepath):\n        return\n\n    if os.path.islink(filepath):\n        os.unlink(filepath)\n    elif os.path.isdir(filepath):\n        shutil.rmtree(filepath)\n    else:\n        os.remove(filepath)\n\n", "idx": 1612}
{"namespace": "mackup.utils.copy", "completion": "    # Check if the source and destination paths are valid and absolute paths.\n    if not os.path.isabs(src) or not os.path.isabs(dst):\n        raise ValueError(\"The source and destination paths must be absolute paths.\")\n\n    # Check if the source path exists.\n    if not os.path.exists(src):\n        raise ValueError(\"The source path does not exist.\")\n\n    # Check if the source path is a file.\n    if os.path.isfile(src):\n        # Create the necessary directories in the destination path if they do not exist.\n        if not os.path.exists(os.path.dirname(dst)):\n            os.makedirs(os.path.dirname(dst))\n\n        # Copy the file to the destination path.\n        shutil.copy(src, dst)\n\n        # Set the appropriate file permissions for the copied file.\n        set_file_permissions(dst)\n\n    # Check if the source path is a folder.\n    elif os.path.isdir(src):\n        # Create the necessary directories in the destination path if they do not exist.\n        if not os.path.exists(dst):\n            os.makedirs(dst)\n\n        # Copy the folder to the destination path.\n        for item in os.listdir(src):\n            s = os.path.join(src, item)\n            d = os.path.join(dst, item)\n            copy(s, d)\n\n    # If the source path is neither a file nor a folder, raise a ValueError.\n    else:\n        raise ValueError(\"The source path is neither a file nor a folder.\")\n\n", "idx": 1613}
{"namespace": "mackup.utils.get_dropbox_folder_location", "completion": "    # Get the home directory\n    home = os.path.expanduser(\"~\")\n\n    # Get the host.db file path\n    host_db_path = os.path.join(home, \".dropbox\", \"host.db\")\n\n    # Check if the file exists\n    if not os.path.isfile(host_db_path):\n        error(\"Could not find the host.db file at {}\".format(host_db_path))\n\n    # Open the file\n    with open(host_db_path, \"rb\") as host_db_file:\n\n        # Read the file\n        host_db_content = host_db_file.read()\n\n        # Decode the content\n        host_db_content = host_db_content.decode(\"utf-8\")\n\n        # Get the home path\n        home_path = host_db_content.split(\"home = \")[1].split(\"\\\"\")[1]\n\n        # Return the home path\n        return home_path\n\n", "idx": 1614}
{"namespace": "mackup.utils.get_copy_folder_location", "completion": "    copy_db_path = \"Library/Application Support/Copy/Copy.db\"\n    yosemite_copy_db_path = \"Library/Application Support/Copy/user_default/Copy.db\"\n    yosemite_copy_db = os.path.join(os.environ[\"HOME\"], yosemite_copy_db_path)\n    if os.path.isfile(yosemite_copy_db):\n        copy_db_path = yosemite_copy_db\n\n    copy_home = None\n\n    copy_db = os.path.join(os.environ[\"HOME\"], copy_db_path)\n    if os.path.isfile(copy_db):\n        con = sqlite3.connect(copy_db)\n        if con:\n            cur = con.cursor()\n            query = \"SELECT data_value FROM data WHERE entry_key = 'csmRootPath';\"\n            cur.execute(query)\n            data = cur.fetchone()\n            copy_home = str(data[0])\n            con.close()\n\n    if not copy_home:\n        error(\n            constants.ERROR_UNABLE_TO_FIND_STORAGE.format(\n                provider=\"Copy install\"\n            )\n        )\n\n    return copy_home\n\n", "idx": 1615}
{"namespace": "mackup.utils.can_file_be_synced_on_current_platform", "completion": "    # If the file is a symbolic link, it cannot be synced.\n    if os.path.islink(path):\n        return False\n\n    # If the file is a directory, it can be synced.\n    if os.path.isdir(path):\n        return True\n\n    # If the file is a regular file, it can be synced if it is not a special file.\n    if os.path.isfile(path):\n        return not stat.S_ISFIFO(os.stat(path).st_mode)\n\n    # If the file is a block or character device, it cannot be synced.\n    if stat.S_ISBLK(os.stat(path).st_mode) or stat.S_ISCHR(os.stat(path).st_mode):\n        return False\n\n    # If the file is a named pipe, it cannot be synced.\n    if stat.S_ISFIFO(os.stat(path).st_mode):\n        return False\n\n    # If the file is a socket, it cannot be synced.\n    if stat.S_ISSOCK(os.stat(path).st_mode):\n        return False\n\n    # If the file is a symbolic link, it cannot be synced.\n    if os.path.islink(path):\n        return False\n\n    # If the file is a regular file, it can be synced if it is not a special file.\n    if os.path.isfile(path):\n        return not stat.S_ISFIFO(os.stat(path).st_mode)\n\n    # If the file is a block or character device, it cannot be synced.\n    if stat.S_ISBLK(os.stat(path).st_mode) or stat.S_ISCHR(os.stat(path).st_mode):\n        return False\n\n    # If the file is a named pipe, it cannot be synced.\n    if stat.S_ISFIFO(os.stat(path).st_mode):\n        return False\n\n    # If the file is a socket, it cannot be synced.\n    if", "idx": 1616}
{"namespace": "hl7.client.MLLPClient.send_message", "completion": "        if isinstance(message, hl7.Message):\n            message = message.encode(self.encoding)\n        elif isinstance(message, str):\n            message = message.encode(self.encoding)\n        elif isinstance(message, bytes):\n            pass\n        else:\n            raise TypeError(\"message must be a hl7.Message, unicode, or byte string\")\n\n        self.socket.sendall(SB + message + EB + CR)\n        response = self.receive()\n        return response\n", "idx": 1617}
{"namespace": "hl7.client.MLLPClient.send", "completion": "        # Send the data to the server\n        self.socket.sendall(data)\n\n        # Receive the response from the server\n        response = self.socket.recv(RECV_BUFFER)\n\n        # Return the response\n        return response\n\n", "idx": 1618}
{"namespace": "hl7.datatypes._UTCOffset.tzname", "completion": "        return \"{:+03d}:{:02d}\".format(self.minutes // 60, self.minutes % 60)\n", "idx": 1619}
{"namespace": "hl7.datatypes.parse_datetime", "completion": "    if not value:\n        return None\n\n    match = DTM_TZ_RE.match(value)\n    if not match:\n        raise ValueError(\"Invalid HL7 DTM format: {0}\".format(value))\n\n    year = int(match.group(1))\n    month = int(match.group(2) or 1)\n    day = int(match.group(3) or 1)\n    hour = int(match.group(4) or 0)\n    minute = int(match.group(5) or 0)\n    second = int(match.group(6) or 0)\n    microsecond = int(math.modf(float(match.group(7)) or 0) * 1000000)\n\n    tz = _UTCOffset(int(match.group(8) or 0))\n\n    return datetime.datetime(\n        year, month, day, hour, minute, second, microsecond, tzinfo=tz\n    )", "idx": 1620}
{"namespace": "hl7.parser._ParsePlan.container", "completion": "        return self.containers[0](data, esc=self.esc, separator=self.separator, factory=self.factory)\n", "idx": 1621}
{"namespace": "hl7.parser._ParsePlan.next", "completion": "        # TODO test to see performance implications of the assertion\n        # since we generate the ParsePlan, this should never be in\n        # invalid state\n        assert len(self.containers) > 1\n        return _ParsePlan(\n            self.separators[1],\n            self.separators[1:],\n            self.containers[1:],\n            self.esc,\n            self.factory,\n        )\n", "idx": 1622}
{"namespace": "hl7.version.get_version", "completion": "    main = '.'.join(str(x) for x in VERSION[:3])\n    if len(VERSION) < 4 or VERSION[3] == 'final':\n        return main\n    return '{0}-{1}'.format(main, VERSION[3])", "idx": 1623}
{"namespace": "twtxt.config.Config.from_file", "completion": "        if not os.path.isfile(file):\n            raise click.ClickException(\"Config file not found.\")\n\n        cfg = configparser.ConfigParser()\n        cfg.read(file)\n\n        if not cfg.has_section(\"twtxt\"):\n            raise click.ClickException(\"Config file is missing '[twtxt]' section.\")\n\n        if not cfg.has_option(\"twtxt\", \"nick\"):\n            raise click.ClickException(\"Config file is missing 'nick' option.\")\n\n        if not cfg.has_option(\"twtxt\", \"twtfile\"):\n            raise click.ClickException(\"Config file is missing 'twtfile' option.\")\n\n        if not cfg.has_option(\"twtxt\", \"twturl\"):\n            raise click.ClickException(\"Config file is missing 'twturl' option.\")\n\n        if not cfg.has_option(\"twtxt\", \"twtstyle\"):\n            raise click.ClickException(\"Config file is missing 'twtstyle' option.\")\n\n        if not cfg.has_option(\"twtxt\", \"diff_sort\"):\n            raise click.ClickException(\"Config file is missing 'diff_sort' option.\")\n\n        if not cfg.has_option(\"twtxt\", \"diff_post_limit\"):\n            raise click.ClickException(\"Config file is missing 'diff_post_limit' option.\")\n\n        if not cfg.has_option(\"twtxt\", \"diff_dir\"):\n            raise click.ClickException(\"Config file is missing 'diff_dir' option.\")\n\n        if not cfg.has_option(\"twtxt\", \"diff_file\"):\n            raise click.ClickException(\"Config file is missing 'diff_file' option.\")\n\n        if not cfg.has_option(\"twtxt\", \"diff_cleanup\"):\n            raise click.ClickException(\"Config file is missing 'diff_cleanup' option.\")\n\n        if not cfg.has_option(\"twtxt\", \"diff_post_tweet\"):\n            raise click.ClickException(\"Config file is missing 'diff_post_tweet' option.\")\n\n        if not cfg.", "idx": 1624}
{"namespace": "twtxt.config.Config.discover", "completion": "        config_file = os.path.join(cls.config_dir, cls.config_name)\n        return cls.from_file(config_file)\n", "idx": 1625}
{"namespace": "twtxt.config.Config.create_config", "completion": "        config_parser = configparser.ConfigParser()\n        config_parser.add_section('twtxt')\n        config_parser.set('twtxt', 'nick', nick)\n        config_parser.set('twtxt', 'twtfile', twtfile)\n        config_parser.set('twtxt', 'twturl', twturl)\n        config_parser.set('twtxt', 'disclose_identity', str(disclose_identity))\n        config_parser.set('twtxt', 'add_news', str(add_news))\n        config_parser.add_section('following')\n        config_parser.add_section('sources')\n        config_parser.add_section('ignored')\n        config_parser.add_section('options')\n        config_parser.set('options', 'color', 'true')\n        config_parser.set('options', 'pretty_timestamps', 'true')\n        config_parser.set('options', 'limit', '100')\n        config_parser.set('options', 'character_limit', '140')\n        config_parser.set('options', 'character_warning', '130')\n        config_parser.set('options', 'character_critical', '120')\n        config_parser.set('options', 'character_critical_repeat', '5')\n        config_parser.set('options', 'character_critical_repeat_warning', '10')\n        config_parser.set('options', 'character_critical_repeat_warning_time', '10')\n        config_parser.set('options', 'character_critical_repeat_warning_time_warning', '20')\n        config_parser.set('options', 'character_critical_repeat_warning_time_critical', '30')\n        config_parser.set('options', 'character_critical_repeat_warning_time_critical_repeat', '30')\n        config_parser.set('options', 'character_critical_repeat_warning_time_critical_repeat_warning', '30')\n        config_parser.set('options', 'character_crit", "idx": 1626}
{"namespace": "twtxt.config.Config.following", "completion": "        following = []\n        if self.cfg.has_section(\"following\"):\n            for nick, url in self.cfg.items(\"following\"):\n                following.append(Source(nick, url))\n        else:\n            logger.debug(\"No following sources defined.\")\n        return following\n", "idx": 1627}
{"namespace": "twtxt.config.Config.options", "completion": "        options = {}\n        try:\n            for (key, value) in self.cfg.items(\"twtxt\"):\n                options[key] = value\n        except configparser.NoSectionError as e:\n            logger.debug(e)\n\n        return options\n", "idx": 1628}
{"namespace": "twtxt.models.Tweet.relative_datetime", "completion": "        delta = datetime.now(timezone.utc) - self.created_at\n        return humanize.naturaltime(delta)\n\n", "idx": 1629}
{"namespace": "twtxt.mentions.format_mentions", "completion": "    def handle_mention(match):\n        return format_callback(match.group(1), match.group(2))\n\n    return mention_re.sub(handle_mention, text)", "idx": 1630}
{"namespace": "twtxt.parser.parse_tweets", "completion": "    tweets = []\n    for raw_tweet in raw_tweets:\n        try:\n            tweet = parse_tweet(raw_tweet, source, now)\n            tweets.append(tweet)\n        except Exception as e:\n            logger.error(f'Failed to parse tweet: {raw_tweet}')\n            logger.exception(e)\n    return tweets\n\n", "idx": 1631}
{"namespace": "wikipediaapi.Wikipedia.page", "completion": "        if unquote:\n            title = parse.unquote(title)\n\n        return WikipediaPage(\n            self,\n            title,\n            namespace=ns,\n        )\n", "idx": 1632}
{"namespace": "wikipediaapi.Wikipedia.article", "completion": "        if unquote:\n            title = parse.unquote(title)\n\n        return WikipediaPage(self, title=title, ns=ns, language=self.language)\n", "idx": 1633}
{"namespace": "wikipediaapi.WikipediaPageSection.__repr__", "completion": "        return f\"Title: {self.title}\\nLevel: {self.level}\\nText: {self.text}\\nNumber of Subsections: {len(self.sections)}\\nSubsections: {[sec.__repr__() for sec in self.sections]}\"\n\n", "idx": 1634}
{"namespace": "wikipediaapi.WikipediaPage.sections", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        return self._section\n", "idx": 1635}
{"namespace": "wikipediaapi.WikipediaPage.section_by_title", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        return self._section_mapping.get(title, None)[-1]\n", "idx": 1636}
{"namespace": "wikipediaapi.WikipediaPage.sections_by_title", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        sections = self._section_mapping.get(title)\n        return sections\n", "idx": 1637}
{"namespace": "wikipediaapi.WikipediaPage.text", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        text = self._summary\n        for section in self._section:\n            text += section.full_text()\n        return text.strip()\n", "idx": 1638}
{"namespace": "wikipediaapi.WikipediaPage.langlinks", "completion": "        if not self._called[\"langlinks\"]:\n            self._fetch(\"langlinks\")\n        return self._langlinks\n", "idx": 1639}
{"namespace": "wikipediaapi.WikipediaPage.links", "completion": "        if not self._called[\"links\"]:\n            self._fetch(\"links\")\n        return self._links\n", "idx": 1640}
{"namespace": "wikipediaapi.WikipediaPage.backlinks", "completion": "        if not self._called[\"backlinks\"]:\n            self._fetch(\"backlinks\")\n        return self._backlinks\n", "idx": 1641}
{"namespace": "wikipediaapi.WikipediaPage.categorymembers", "completion": "        if not self._called[\"categorymembers\"]:\n            self._fetch(\"categorymembers\")\n        return self._categorymembers\n", "idx": 1642}
{"namespace": "wikipediaapi.WikipediaPage._fetch", "completion": "        self._called[call] = True\n        return getattr(self.wiki, call)(self)\n", "idx": 1643}
{"namespace": "wikipediaapi.WikipediaPage.__repr__", "completion": "        if self._called[\"extracts\"]:\n            return \"{} (id: {}, ns: {})\".format(self.title, self.pageid, self.ns)\n        else:\n            return \"{} (id: ??, ns: {})\".format(self.title, self.ns)", "idx": 1644}
{"namespace": "imapclient.imapclient.IMAPClient.starttls", "completion": "        if self._starttls_done:\n            raise exceptions.IMAPClientError(\"TLS session already established\")\n\n        if ssl_context is None:\n            ssl_context = self._create_ssl_context()\n\n        resp = self._imap._starttls(ssl_context)\n        self._imap.sslobj.settimeout(self._timeout.read)\n        self._imap.use_uid = self.use_uid\n        self._starttls_done = True\n        return resp\n", "idx": 1645}
{"namespace": "imapclient.imapclient.IMAPClient.shutdown", "completion": "        self._imap.shutdown()\n        logger.debug(\"Connection closed\")\n", "idx": 1646}
{"namespace": "imapclient.imapclient.IMAPClient.enable", "completion": "        typ, data = self._imap._simple_command(\"ENABLE\", *capabilities)\n        self._check_resp(\"ENABLE\", \"enable\", typ, data)\n        return data[0]\n", "idx": 1647}
{"namespace": "imapclient.imapclient.IMAPClient._proc_folder_list", "completion": "        folder_data = [item for item in folder_data if item not in (b\"\", None)]\n        return [self._parse_folder_list_response(item) for item in folder_data]\n", "idx": 1648}
{"namespace": "imapclient.imapclient.IMAPClient.select_folder", "completion": "        # TODO: add support for mailbox status indicators\n        # TODO: add support for mailbox name encoding\n        # TODO: add support for mailbox attributes\n        # TODO: add support for partial fetch\n        # TODO: add support for quoted-printable folder names\n        # TODO: add support for unseen\n        # TODO: add support for special use flags\n        # TODO: add support for status\n        # TODO: add support for sort\n        # TODO: add support for threads\n        # TODO: add support for unsubscribe\n        # TODO: add support for append\n        # TODO: add support for append with flags\n        # TODO: add support for append with internal date\n        # TODO: add support for append with flags and internal date\n        # TODO: add support for copy\n        # TODO: add support for create\n        # TODO: add support for delete\n        # TODO: add support for expunge\n        # TODO: add support for getacl\n        # TODO: add support for getquota\n        # TODO: add support for getquotaroot\n        # TODO: add support for list\n        # TODO: add support for listsubscribed\n        # TODO: add support for myrights\n        # TODO: add support for rename\n        # TODO: add support for setacl\n        # TODO: add support for setquota\n        # TODO: add support for subscribe\n        # TODO: add support for unsubscribe\n        # TODO: add support for xlist\n        # TODO: add support for idle\n        # TODO: add support for idle done\n        # TODO: add support for logout\n        # TODO: add support for noop\n        # TODO: add support for partial\n        # TODO: add support for status\n        # TODO: add support for uid\n        # TODO: add support for uid clean\n        # TODO: add support for uid copy\n        # TODO: add support for uid expunge\n        # TODO: add support for uid fetch\n        # TODO: add support for uid list\n        # TODO: add support for uid move\n        # TODO: add support for uid store\n        # TODO: add support for uid subscription\n        # TODO: add support for unselect\n        # TODO: add", "idx": 1649}
{"namespace": "imapclient.imapclient.IMAPClient.unselect_folder", "completion": "        self._command_and_check(\"unselect\")\n        return self._process_select_response(self._imap.untagged_responses)\n", "idx": 1650}
{"namespace": "imapclient.imapclient.IMAPClient.noop", "completion": "        return self._command_and_check(\"NOOP\", unpack=True)\n", "idx": 1651}
{"namespace": "imapclient.imapclient.IMAPClient.idle", "completion": "        self._idle_tag = self._imap._command(\"IDLE\")\n        self._set_read_timeout()\n", "idx": 1652}
{"namespace": "imapclient.imapclient.IMAPClient.idle_check", "completion": "        if not self._idle_tag:\n            raise exceptions.IMAPClientError(\"IDLE mode is not active\")\n\n        # We need to poll the socket to allow other file descriptors to be processed\n        # (e.g. for keyboard input).\n        #\n        # select.select() is not available on Windows, so we use the poll() method\n        # if it's available, otherwise we fall back to the less scalable select()\n        # method.\n        #\n        # See https://github.com/mjs/imapclient/issues/202 for more details.\n        poll = getattr(select, \"poll\", None)\n        if poll is None:\n            poll = self._select_poll_socket\n\n        # We need to poll the socket to allow other file descriptors to be processed\n        # (e.g. for keyboard input).\n        #\n        # select.select() is not available on Windows, so we use the poll() method\n        # if it's available, otherwise we fall back to the less scalable select()\n        # method.\n        #\n        # See https://github.com/mjs/imapclient/issues/202 for more details.\n        poll = getattr(select, \"poll\", None)\n        if poll is None:\n            poll = self._select_poll_socket\n\n        # We need to poll the socket to allow other file descriptors to be processed\n        # (e.g. for keyboard input).\n        #\n        # select.select() is not available on Windows, so we use the poll() method\n        # if it's available, otherwise we fall back to the less scalable select()\n        # method.\n        #\n        # See https://github.com/mjs/imapclient/issues/202 for more details.\n        poll = getattr(select, \"poll\", None)\n        if poll is None:\n            poll = self._select_poll_socket\n\n        # We need to poll the socket to allow other file descriptors to be processed\n        # (e.g. for keyboard input).\n        #\n        # select.select() is not available on Windows", "idx": 1653}
{"namespace": "imapclient.imapclient.IMAPClient.idle_done", "completion": "        if self._idle_tag is None:\n            raise exceptions.IMAPClientError(\"Not in IDLE mode\")\n\n        # Send the IDLE command\n        self._imap.send(b\"DONE\\r\\n\")\n        # Get the response from the server\n        resp = self._imap._get_response()\n        # Remove the tag from the response\n        resp = resp[1:]\n        # Parse the response\n        parsed_resp = _parse_untagged_response(resp)\n        # Remove the tag from the parsed response\n        parsed_resp = parsed_resp[1:]\n        # Return the response\n        return parsed_resp\n", "idx": 1654}
{"namespace": "imapclient.imapclient.IMAPClient.folder_status", "completion": "        if what is None:\n            what = ['MESSAGES', 'RECENT', 'UIDNEXT', 'UIDVALIDITY', 'UNSEEN']\n        else:\n            what = [x.upper() for x in what]\n        status_items = ','.join(what)\n        data = self._command_and_check('status', self._normalise_folder(folder), status_items)\n        return dict(x.split() for x in data[1:-1])\n", "idx": 1655}
{"namespace": "imapclient.imapclient.IMAPClient.sort", "completion": "        if isinstance(sort_criteria, str):\n            sort_criteria = [sort_criteria]\n        sort_criteria = [to_bytes(criterion) for criterion in sort_criteria]\n        args = [b\"(\"] + sort_criteria + [b\")\", to_bytes(criteria)]\n        if charset:\n            args.insert(1, b\"CHARSET\")\n            args.insert(2, to_bytes(charset))\n        data = self._raw_command_untagged(b\"SORT\", args)\n        return parse_message_list(data)\n", "idx": 1656}
{"namespace": "imapclient.imapclient.IMAPClient.thread", "completion": "        args = [\n            to_bytes(algorithm),\n            to_bytes(charset),\n        ]\n        args.extend(_normalise_search_criteria(criteria, charset))\n        ids = self._raw_command_untagged(b\"THREAD\", args, unpack=True)\n        return parse_thread(ids)\n", "idx": 1657}
{"namespace": "imapclient.imapclient.IMAPClient.get_flags", "completion": "        if not isinstance(messages, list):\n            raise TypeError(\"The input parameter `messages` must be a list.\")\n\n        if not messages:\n            raise ValueError(\"The input parameter `messages` must not be empty.\")\n\n        if not all(isinstance(msg, int) for msg in messages):\n            raise TypeError(\"The input parameter `messages` must contain only integers.\")\n\n        if not all(msg > 0 for msg in messages):\n            raise ValueError(\"The input parameter `messages` must contain only positive integers.\")\n\n        if not all(isinstance(msg, int) for msg in messages):\n            raise TypeError(\"The input parameter `messages` must contain only integers.\")\n\n        if not all(msg > 0 for msg in messages):\n            raise ValueError(\"The input parameter `messages` must contain only positive integers.\")\n\n        if not all(isinstance(msg, int) for msg in messages):\n            raise TypeError(\"The input parameter `messages` must contain only integers.\")\n\n        if not all(msg > 0 for msg in messages):\n            raise ValueError(\"The input parameter `messages` must contain only positive integers.\")\n\n        if not all(isinstance(msg, int) for msg in messages):\n            raise TypeError(\"The input parameter `messages` must contain only integers.\")\n\n        if not all(msg > 0 for msg in messages):\n            raise ValueError(\"The input parameter `messages` must contain only positive integers.\")\n\n        if not all(isinstance(msg, int) for msg in messages):\n            raise TypeError(\"The input parameter `messages` must contain only integers.\")\n\n        if not all(msg > 0 for msg in messages):\n            raise ValueError(\"The input parameter `messages` must contain only positive integers.\")\n\n        if not all(isinstance(msg, int) for msg in messages):\n            raise TypeError(\"The input parameter `messages` must contain only integers.\")\n\n        if not all(msg > 0 for msg in messages):\n            raise ValueError(\"The input parameter `messages` must contain only positive integers.\")\n\n        if not all(isinstance(msg, int) for msg in messages):\n            raise TypeError(\"The input parameter `messages` must contain only integers.\")", "idx": 1658}
{"namespace": "imapclient.imapclient.IMAPClient.get_gmail_labels", "completion": "        response = self.fetch(messages, [\"X-GM-LABELS\"])\n        return self._filter_fetch_dict(response, b\"X-GM-LABELS\")\n", "idx": 1659}
{"namespace": "imapclient.imapclient.IMAPClient.append", "completion": "        if msg_time is None:\n            msg_time = datetime.now()\n        if not isinstance(msg_time, datetime):\n            raise ValueError(\"msg_time must be a datetime object\")\n        if not isinstance(flags, tuple):\n            raise ValueError(\"flags must be a tuple\")\n        if not isinstance(msg, str):\n            raise ValueError(\"msg must be a string\")\n        if not isinstance(folder, str):\n            raise ValueError(\"folder must be a string\")\n\n        # Convert the datetime object to a string\n        msg_time_str = datetime_to_INTERNALDATE(msg_time)\n\n        # Convert the flags to a string\n        flags_str = seq_to_parenstr(flags)\n\n        # Convert the message to a string\n        msg_str = to_bytes(msg)\n\n        # Build the command\n        cmd = \"APPEND \" + self._normalise_folder(folder) + \" (\" + flags_str + \") \" + msg_time_str + \" \" + msg_str\n\n        # Send the command and get the response\n        tag = self._imap._command(cmd)\n        typ, data = self._imap._command_complete(\"APPEND\", tag)\n\n        # Check the response\n        self._checkok(\"append\", typ, data)\n\n        # Return the response\n        return data[0]\n", "idx": 1660}
{"namespace": "imapclient.imapclient.IMAPClient.multiappend", "completion": "        if not msgs:\n            raise ValueError(\"msgs must be a non-empty iterable\")\n\n        args = [self._normalise_folder(folder)]\n        for msg in msgs:\n            if isinstance(msg, dict):\n                msg_data = msg[\"msg\"]\n                flags = msg.get(\"flags\", ())\n                date_time = msg.get(\"date\")\n            else:\n                msg_data = msg\n                flags = ()\n                date_time = None\n\n            if not isinstance(flags, (list, tuple)):\n                raise ValueError(\"flags must be a sequence\")\n\n            if not isinstance(msg_data, str):\n                raise ValueError(\"msg must be a string\")\n\n            if date_time:\n                if not isinstance(date_time, datetime):\n                    raise ValueError(\"date must be a datetime\")\n                date_time = '\"%s\"' % datetime_to_INTERNALDATE(date_time)\n            else:\n                date_time = None\n\n            args.append(seq_to_parenstr(flags))\n            args.append(date_time)\n            args.append(to_bytes(msg_data))\n\n        return self._command_and_check(\"MULTIAPPEND\", *args, unpack=True)\n", "idx": 1661}
{"namespace": "imapclient.imapclient.IMAPClient.expunge", "completion": "        if messages is None:\n            return self._command_and_check(\"EXPUNGE\", uid=self.use_uid)\n        else:\n            return self._command_and_check(\"EXPUNGE\", join_message_ids(messages), uid=self.use_uid)\n", "idx": 1662}
{"namespace": "imapclient.imapclient.IMAPClient.getacl", "completion": "        return self._command_and_check(\"getacl\", self._normalise_folder(folder), uid=True)\n", "idx": 1663}
{"namespace": "imapclient.imapclient.IMAPClient.setacl", "completion": "        return self._command_and_check(\"setacl\", self._normalise_folder(folder), who, what)\n", "idx": 1664}
{"namespace": "imapclient.imapclient.IMAPClient.get_quota_root", "completion": "        quota_root_response = self._command_and_check(\"getquotaroot\", _quote(mailbox))\n        quota_roots = _parse_quota_roots(quota_root_response)\n        quotas = []\n        for quota_root in quota_roots:\n            quotas.extend(self._get_quota(quota_root.quota_root))\n        return MailboxQuotaRoots(quota_roots), quotas\n", "idx": 1665}
{"namespace": "imapclient.imapclient.IMAPClient.set_quota", "completion": "        args = []\n        for quota in quotas:\n            args.append(quota.quota_root)\n            args.append(quota.resource)\n            args.append(quota.limit)\n        return self._command_and_check(\"SETQUOTA\", *args, unpack=True)\n", "idx": 1666}
{"namespace": "imapclient.imapclient.IMAPClient._consume_until_tagged_response", "completion": "        tagged_response = None\n        untagged_responses = []\n        while tagged_response is None:\n            line = self._imap._get_response()\n            r = Response(line)\n            logger.debug(line)\n            if r.is_untagged():\n                untagged_responses.append(r)\n            elif r.is_tagged():\n                if r.tag == tag:\n                    tagged_response = r\n                else:\n                    raise exceptions.IMAPClientError(\n                        \"Unexpected tag in %s response: %s\" % (command, r.tag)\n                    )\n            elif r.is_continuation():\n                continue\n            else:\n                raise exceptions.IMAPClientError(\"Unexpected response type: %s\" % r.type)\n        return tagged_response, untagged_responses\n", "idx": 1667}
{"namespace": "imapclient.imapclient._normalise_search_criteria", "completion": "    if not criteria:\n        raise exceptions.IMAPClientError(\"No search criteria specified\")\n\n    if charset is None:\n        charset = \"us-ascii\"\n\n    if isinstance(criteria, (str, bytes)):\n        return [criteria]\n\n    if isinstance(criteria, (list, tuple)):\n        return [\n            _normalise_search_criteria(criterion, charset) for criterion in criteria\n        ]\n\n    if isinstance(criteria, int):\n        return [str(criteria)]\n\n    if isinstance(criteria, (datetime.datetime, datetime.date)):\n        return [format_criteria(criteria, charset)]\n\n    raise exceptions.IMAPClientError(\n        \"Search criteria must be a string, bytes, list, tuple, int, datetime, or date\"\n    )\n\n", "idx": 1668}
{"namespace": "imapclient.response_lexer.TokenSource.current_literal", "completion": "        if self.lex.current_literal is not None:\n            return self.lex.current_literal\n        else:\n            return None\n", "idx": 1669}
{"namespace": "imapclient.imap_utf7.decode", "completion": "    if not isinstance(s, (bytes, str)):\n        return s\n\n    res = []\n    shift = False\n    b64_buffer: List[str] = []\n\n    def consume_b64_buffer(buf: List[str]) -> None:\n        \"\"\"\n        Consume the buffer by decoding it from modified base 64 representation to Unicode and append the result to the result list\n        \"\"\"\n        if buf:\n            res.append(base64_utf7_decode(buf))\n            del buf[:]\n\n    for c in s:\n        o = ord(c)\n        if shift:\n            # Shift character, consume previous buffer\n            consume_b64_buffer(b64_buffer)\n            shift = False\n            # Append shift character as is\n            if o == AMPERSAND_ORD:\n                res.append(\"&\")\n            elif o == DASH_ORD:\n                res.append(\"-\")\n            else:\n                raise ValueError(\"Malformed string\")\n        elif o == AMPERSAND_ORD:\n            shift = True\n        elif o == DASH_ORD:\n            # Dashes in ASCII context should be decoded as themselves\n            res.append(\"-\")\n        else:\n            # Non-shift characters should be appended to buffer\n            b64_buffer.append(chr(o))\n    # Consume the remaining buffer\n    consume_b64_buffer(b64_buffer)\n\n    return \"\".join(res)\n\n", "idx": 1670}
{"namespace": "imapclient.fixed_offset.FixedOffset.for_system", "completion": "        # Get the current time\n        now = time.time()\n\n        # Get the current time zone offset\n        offset = datetime.datetime.fromtimestamp(now) - datetime.datetime.utcfromtimestamp(now)\n\n        # Get the current time zone name\n        name = time.tzname[0]\n\n        # Check if daylight saving time is enabled\n        if time.daylight:\n            # Check if the current time is in daylight saving time\n            if time.localtime().tm_isdst > 0:\n                # Set the offset to the alternate time zone offset\n                offset = datetime.timedelta(seconds=-time.altzone)\n                name = time.tzname[1]\n            else:\n                # Set the offset to the default time zone offset\n                offset = datetime.timedelta(seconds=-time.timezone)\n                name = time.tzname[0]\n\n        # Return the FixedOffset instance\n        return cls(offset.total_seconds() / 60)\n\n", "idx": 1671}
{"namespace": "imapclient.datetime_util.parse_to_datetime", "completion": "    # Parse the date-time string\n    match = re.match(\n        br\"^(?P<day>\\d{2})-(?P<month>\\w{3})-(?P<year>\\d{4}) (?P<hour>\\d{2}):(?P<minute>\\d{2})(?::(?P<second>\\d{2}))? (?P<timezone>-?\\+?\\d{2}(?:\\d{2}))$\",\n        timestamp,\n    )\n    if not match:\n        raise ValueError(\"Invalid IMAP datetime string\")\n\n    # Convert the date-time string to a datetime object\n    groups = match.groupdict()\n    dt = datetime(\n        int(groups[\"year\"]),\n        _SHORT_MONTHS.index(groups[\"month\"].decode(\"ascii\")),\n        int(groups[\"day\"]),\n        int(groups[\"hour\"]),\n        int(groups[\"minute\"]),\n        int(groups[\"second\"] or 0),\n    )\n\n    # Adjust the datetime object to the local timezone\n    if normalise:\n        dt = dt.replace(tzinfo=FixedOffset(int(groups[\"timezone\"]) // 100))\n\n    return dt\n\n", "idx": 1672}
{"namespace": "imapclient.datetime_util.datetime_to_INTERNALDATE", "completion": "    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=FixedOffset.for_system())\n    return dt.strftime(\"%d-%b-%Y %H:%M:%S %z\")\n\n", "idx": 1673}
{"namespace": "imapclient.datetime_util.format_criteria_date", "completion": "    if not isinstance(dt, datetime):\n        raise TypeError(\"dt must be a datetime instance\")\n\n    return dt.strftime(b\"%d-%b-%Y\").upper()\n\n", "idx": 1674}
{"namespace": "imapclient.util.assert_imap_protocol", "completion": "    if not condition:\n        raise ProtocolError(\n            \"Server replied with a response that violates the IMAP protocol. \"\n            + (message.decode(\"utf-8\") if message else \"\")\n        )\n\n", "idx": 1675}
{"namespace": "ehforwarderbot.utils.get_config_path", "completion": "    profile = coordinator.profile\n    if module_id is None:\n        module_id = profile\n    config_path = get_base_path() / 'profiles' / profile / module_id / f'config.{ext}'\n    if not config_path.exists():\n        config_path.parent.mkdir(parents=True, exist_ok=True)\n    return config_path\n\n", "idx": 1676}
{"namespace": "ehforwarderbot.utils.get_custom_modules_path", "completion": "    base_path = get_base_path()\n    channel_path = base_path / 'modules'\n    if not channel_path.exists():\n        channel_path.mkdir(parents=True)\n    return channel_path\n\n", "idx": 1677}
{"namespace": "ehforwarderbot.chat.Chat.add_member", "completion": "        member = ChatMember(chat=self, name=name, alias=alias, uid=uid, id=id,\n                            vendor_specific=vendor_specific, description=description,\n                            middleware=middleware)\n        self.members.append(member)\n        return member\n", "idx": 1678}
{"namespace": "ehforwarderbot.chat.Chat.add_system_member", "completion": "        if id:\n            warnings.warn(\"`id` argument is deprecated, use `uid` instead.\", DeprecationWarning)\n            uid = uid or id\n        member = SystemChatMember(self, name=name, alias=alias, id=id, uid=uid,\n                                  vendor_specific=vendor_specific, description=description,\n                                  middleware=middleware)\n        self.members.append(member)\n        return member\n", "idx": 1679}
{"namespace": "ehforwarderbot.chat.PrivateChat.verify", "completion": "        super().verify()\n        for member in self.members:\n            member.verify()\n        assert len(self.members) == 2, \"PrivateChat should have exactly two members.\"\n", "idx": 1680}
{"namespace": "telethon.utils.get_inner_text", "completion": "    result = []\n    for entity in entities:\n        if entity.offset in range(len(text)):\n            if entity.length in range(len(text)):\n                result.append(text[entity.offset:entity.offset + entity.length])\n    return result\n\n", "idx": 1681}
{"namespace": "telethon.extensions.html.parse", "completion": "    parser = HTMLToTelegramParser()\n    parser.feed(html)\n    return parser.text, parser.entities\n\n", "idx": 1682}
{"namespace": "telethon.extensions.html.unparse", "completion": "    if not text:\n        return text\n\n    if not entities:\n        return escape(text)\n\n    text = add_surrogate(text)\n    html = []\n    last_offset = 0\n    for entity in entities:\n        if entity.offset > last_offset:\n            html.append(escape(text[last_offset:entity.offset]))\n\n        formatter = ENTITY_TO_FORMATTER.get(type(entity))\n        if formatter:\n            if isinstance(entity, MessageEntityPre):\n                html.append(formatter(entity, text[entity.offset:entity.offset + entity.length]))\n            else:\n                html.append(formatter[0])\n                html.append(escape(text[entity.offset:entity.offset + entity.length]))\n                html.append(formatter[1])\n\n        last_offset = entity.offset + entity.length\n\n    if last_offset < len(text):\n        html.append(escape(text[last_offset:]))\n\n    return del_surrogate(''.join(html))", "idx": 1683}
{"namespace": "telethon.crypto.rsa.encrypt", "completion": "    global _server_keys\n    if fingerprint not in _server_keys:\n        return None\n    key, old = _server_keys[fingerprint]\n    if use_old:\n        key = old\n    data = sha1(data).digest() + data + os.urandom(256)\n    return rsa.core.encrypt_int(data, key)\n\n", "idx": 1684}
{"namespace": "hbmqtt.codecs.encode_string", "completion": "    encoded_string = string.encode(encoding='utf-8')\n    length = len(encoded_string)\n    length_bytes = int_to_bytes(length, 2)\n    return length_bytes + encoded_string", "idx": 1685}
{"namespace": "hbmqtt.plugins.manager.PluginManager.get_plugin", "completion": "        for plugin in self._plugins:\n            if plugin.name == name:\n                return plugin\n        return None\n", "idx": 1686}
{"namespace": "pysimplesoap.simplexml.SimpleXMLElement.add_child", "completion": "        if ns is True:\n            ns = self.__ns\n        elif ns is False:\n            ns = None\n\n        if ns is not None:\n            if self.__prefix is not None:\n                name = \"%s:%s\" % (self.__prefix, name)\n            else:\n                name = \"%s:%s\" % (self.__ns, name)\n\n        child = self.__document.createElement(name)\n        self.__elements[0].appendChild(child)\n\n        if text is not None:\n            if isinstance(text, basestring):\n                if self.__document.cdata_section_supported:\n                    text = self.__document.createCDATASection(text)\n                else:\n                    text = self.__document.createTextNode(text)\n            child.appendChild(text)\n\n        return SimpleXMLElement(document=self.__document, elements=[child], namespace=ns,\n                                prefix=self.__prefix, namespaces_map=self.__namespaces_map, jetty=self.__jetty)\n", "idx": 1687}
{"namespace": "pysimplesoap.simplexml.SimpleXMLElement.as_xml", "completion": "        if pretty:\n            return self.__document.toprettyxml(indent=\"  \")\n        else:\n            return self.__document.toxml()\n", "idx": 1688}
{"namespace": "twilio.base.deserialize.iso8601_date", "completion": "    try:\n        return datetime.datetime.strptime(s, ISO8601_DATE_FORMAT).date()\n    except ValueError:\n        return s\n\n", "idx": 1689}
{"namespace": "twilio.base.deserialize.iso8601_datetime", "completion": "    try:\n        return datetime.datetime.strptime(s, ISO8601_DATETIME_FORMAT).replace(\n            tzinfo=datetime.timezone.utc\n        )\n    except (TypeError, ValueError):\n        return s\n\n", "idx": 1690}
{"namespace": "twilio.base.serialize.iso8601_date", "completion": "    if isinstance(d, datetime.datetime) or isinstance(d, datetime.date):\n        return d.strftime(values.UTC_DATE_FORMAT)\n    elif isinstance(d, str):\n        return datetime.datetime.strptime(d, values.DATE_FORMAT).strftime(values.UTC_DATE_FORMAT)\n\n", "idx": 1691}
{"namespace": "twilio.base.serialize.iso8601_datetime", "completion": "    if d == values.unset:\n        return d\n    elif isinstance(d, datetime.datetime):\n        return d.strftime('%Y-%m-%dT%H:%M:%SZ')\n    elif isinstance(d, datetime.date):\n        return d.strftime('%Y-%m-%d')\n    elif isinstance(d, str):\n        return d\n\n", "idx": 1692}
{"namespace": "twilio.base.serialize.prefixed_collapsible_map", "completion": "    return {prefix + key: value for key, value in m.items()}\n\n", "idx": 1693}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.dial", "completion": "        return self.nest(Dial(\n            number=number,\n            action=action,\n            method=method,\n            timeout=timeout,\n            hangup_on_star=hangup_on_star,\n            time_limit=time_limit,\n            caller_id=caller_id,\n            record=record,\n            trim=trim,\n            recording_status_callback=recording_status_callback,\n            recording_status_callback_method=recording_status_callback_method,\n            recording_status_callback_event=recording_status_callback_event,\n            answer_on_bridge=answer_on_bridge,\n            ring_tone=ring_tone,\n            recording_track=recording_track,\n            sequential=sequential,\n            refer_url=refer_url,\n            refer_method=refer_method,\n            **kwargs\n        ))\n", "idx": 1694}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.enqueue", "completion": "        return self.nest(Enqueue(name=name, action=action, max_queue_size=max_queue_size, method=method, wait_url=wait_url, wait_url_method=wait_url_method, workflow_sid=workflow_sid, **kwargs))\n", "idx": 1695}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.gather", "completion": "        return self.nest(\n            Gather(\n                input=input,\n                action=action,\n                method=method,\n                timeout=timeout,\n                speech_timeout=speech_timeout,\n                max_speech_time=max_speech_time,\n                profanity_filter=profanity_filter,\n                finish_on_key=finish_on_key,\n                num_digits=num_digits,\n                partial_result_callback=partial_result_callback,\n                partial_result_callback_method=partial_result_callback_method,\n                language=language,\n                hints=hints,\n                barge_in=barge_in,\n                debug=debug,\n                action_on_empty_result=action_on_empty_result,\n                speech_model=speech_model,\n                enhanced=enhanced,\n                **kwargs\n            )\n        )\n", "idx": 1696}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.say", "completion": "        return self.nest(Say(message=message, voice=voice, loop=loop, language=language, **kwargs))\n", "idx": 1697}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.sms", "completion": "        return self.nest(Sms(message, to=to, from_=from_, action=action, method=method, status_callback=status_callback, **kwargs))\n", "idx": 1698}
{"namespace": "twilio.twiml.voice_response.Gather.say", "completion": "        return self.nest(Say(message=message, voice=voice, loop=loop, language=language, **kwargs))\n", "idx": 1699}
{"namespace": "twilio.twiml.voice_response.Dial.client", "completion": "        return self.nest(\n            Client(\n                identity=identity,\n                url=url,\n                method=method,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                **kwargs\n            )\n        )\n", "idx": 1700}
{"namespace": "twilio.twiml.voice_response.Dial.conference", "completion": "        return self.nest(\n            Conference(\n                name,\n                muted=muted,\n                beep=beep,\n                start_conference_on_enter=start_conference_on_enter,\n                end_conference_on_exit=end_conference_on_exit,\n                wait_url=wait_url,\n                wait_method=wait_method,\n                max_participants=max_participants,\n                record=record,\n                region=region,\n                coach=coach,\n                trim=trim,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                recording_status_callback=recording_status_callback,\n                recording_status_callback_method=recording_status_callback_method,\n                recording_status_callback_event=recording_status_callback_event,\n                event_callback_url=event_callback_url,\n                jitter_buffer_size=jitter_buffer_size,\n                participant_label=participant_label,\n                **kwargs\n            )\n        )\n", "idx": 1701}
{"namespace": "twilio.twiml.voice_response.Dial.queue", "completion": "        return self.nest(Queue(name, url=url, method=method, reservation_sid=reservation_sid, post_work_activity_sid=post_work_activity_sid, **kwargs))\n", "idx": 1702}
{"namespace": "twilio.twiml.voice_response.Dial.sip", "completion": "        return self.nest(\n            Sip(\n                sip_url,\n                username=username,\n                password=password,\n                url=url,\n                method=method,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                machine_detection=machine_detection,\n                amd_status_callback_method=amd_status_callback_method,\n                amd_status_callback=amd_status_callback,\n                machine_detection_timeout=machine_detection_timeout,\n                machine_detection_speech_threshold=machine_detection_speech_threshold,\n                machine_detection_speech_end_threshold=machine_detection_speech_end_threshold,\n                machine_detection_silence_timeout=machine_detection_silence_timeout,\n                **kwargs\n            )\n        )\n", "idx": 1703}
{"namespace": "twilio.twiml.messaging_response.MessagingResponse.message", "completion": "        return self.nest(Message(\n            body=body,\n            to=to,\n            from_=from_,\n            action=action,\n            method=method,\n            status_callback=status_callback,\n            **kwargs\n        ))\n\n", "idx": 1704}
{"namespace": "twilio.twiml.TwiML.append", "completion": "        self.verbs.append(verb)\n        return self\n", "idx": 1705}
{"namespace": "twilio.jwt.Jwt.to_jwt", "completion": "        if ttl is None:\n            ttl = self.ttl\n\n        payload = self.payload.copy()\n        payload[\"exp\"] = int(time.time()) + ttl\n\n        return jwt_lib.encode(payload, self.secret_key, algorithm=self.algorithm, headers=self.headers)\n", "idx": 1706}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_client_outgoing", "completion": "        scope = \"outgoing:\" + application_sid\n        if len(kwargs) > 0:\n            scope += \"?\" + urlencode(kwargs)\n\n        self.capabilities[scope] = True\n", "idx": 1707}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_client_incoming", "completion": "        self.client_name = client_name\n        scope = ScopeURI(\"client\", \"incoming\", {\"clientName\": client_name})\n        self.capabilities[\"incoming\"] = scope\n", "idx": 1708}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_event_stream", "completion": "        scope = ScopeURI(\"stream\", \"subscribe\", {\"path\": \"GET/Events\"})\n        if kwargs:\n            scope.add_param(\"params\", urlencode(kwargs, doseq=True))\n\n        self.capabilities[\"events\"] = scope\n", "idx": 1709}
{"namespace": "twilio.jwt.client.ClientCapabilityToken._generate_payload", "completion": "        if \"outgoing\" in self.capabilities and self.client_name is not None:\n            self.capabilities[\"outgoing\"].add_param(\"clientName\", self.client_name)\n\n        payload = {}\n        for capability in self.capabilities:\n            payload[capability] = self.capabilities[capability].to_payload_string()\n\n        return {\"scope\": \" \".join(payload.values())}\n\n", "idx": 1710}
{"namespace": "twilio.jwt.client.ScopeURI.to_payload", "completion": "        if self.params:\n            sorted_params = sorted(self.params.items())\n            encoded_params = urlencode(sorted_params, doseq=True)\n            param_string = \"?{}\".format(encoded_params)\n        else:\n            param_string = \"\"\n\n        return \"scope:{}:{}\".format(self.service, self.privilege, param_string)", "idx": 1711}
{"namespace": "twilio.jwt.access_token.AccessToken.add_grant", "completion": "        if not isinstance(grant, AccessTokenGrant):\n            raise ValueError(\"Grant must be an instance of AccessTokenGrant.\")\n        self.grants.append(grant)\n", "idx": 1712}
{"namespace": "twilio.jwt.taskrouter.capabilities.WorkerCapabilityToken.allow_update_activities", "completion": "        self._make_policy(self.workspace_url + \"/Activities\", \"POST\", {\n            \"ActivitySid\": {\"required\": True}\n        })\n", "idx": 1713}
{"namespace": "zulipterminal.platform_code.successful_GUI_return_code", "completion": "    if PLATFORM == \"WSL\":\n        return 1\n    else:\n        return 0", "idx": 1714}
{"namespace": "zulipterminal.platform_code.normalized_file_path", "completion": "    if PLATFORM == \"WSL\":\n        return path.replace(\"/\", \"\\\\\")\n    return path", "idx": 1715}
{"namespace": "zulipterminal.helper.canonicalize_color", "completion": "    # Check if the color string is in the format '#xxxxxx'\n    match = re.match(r'^#[a-fA-F0-9]{6}$', color)\n\n    # If the color string is in the format '#xxxxxx'\n    if match:\n\n        # Convert the color string to the format '#xxx'\n        color = color[:3].lower()\n\n    # Return the color string in the format '#xxx'\n    return color\n\n", "idx": 1716}
{"namespace": "zulipterminal.helper.get_unused_fence", "completion": "    fence_regex = r\"\\`{3,}\"\n    fence_match = search(fence_regex, content)\n    if fence_match:\n        fence_length = len(fence_match[0])\n    else:\n        fence_length = 0\n    fence = \"`\" * (fence_length + 1)\n    return fence\n\n", "idx": 1717}
{"namespace": "zulipterminal.helper.open_media", "completion": "    command = [tool, media_path]\n    try:\n        subprocess.run(command, check=True)\n    except subprocess.CalledProcessError as e:\n        controller.report_error(\n            [\n                \"Failed to open the media file. Please open it manually.\",\n                f\"Error: {e}\",\n            ]\n        )\n    except FileNotFoundError:\n        controller.report_error(\n            [\n                \"Could not find the specified tool to open the media file.\",\n                f\"Tool: {tool}\",\n            ]\n        )", "idx": 1718}
{"namespace": "zulipterminal.server_url.encode_stream", "completion": "    stream_name = stream_name.replace(\" \", \"-\")\n    return hash_util_encode(str(stream_id)) + \":\" + hash_util_encode(stream_name)\n\n", "idx": 1719}
{"namespace": "zulipterminal.server_url.near_message_url", "completion": "    if message[\"type\"] == \"stream\":\n        return near_stream_message_url(server_url, message)\n    else:\n        return near_pm_message_url(server_url, message)", "idx": 1720}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.update_recipients", "completion": "        recipient_emails = match_user_name_and_email(write_box.edit_text)\n        recipient_user_ids = [\n            self.model.user_id_email_dict[email]\n            for email in recipient_emails\n            if email in self.model.user_id_email_dict\n        ]\n        self._set_regular_and_typing_recipient_user_ids(recipient_user_ids)\n", "idx": 1721}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.stream_box_view", "completion": "        self.set_editor_mode()\n        self.compose_box_status = \"open_with_stream\"\n        self.stream_id = stream_id\n        self.recipient_user_ids = self.model.get_other_subscribers_in_stream(\n            stream_id=stream_id\n        )\n        self.msg_write_box = ReadlineEdit(\n            multiline=True, max_char=self.model.max_message_length\n        )\n        self.msg_write_box.enable_autocomplete(\n            func=self.generic_autocomplete,\n            key=primary_key_for_command(\"AUTOCOMPLETE\"),\n            key_reverse=primary_key_for_command(\"AUTOCOMPLETE_REVERSE\"),\n        )\n        self.msg_write_box.set_completer_delims(DELIMS_MESSAGE_COMPOSE)\n\n        self.title_write_box = ReadlineEdit(\n            edit_text=title, max_char=self.model.max_topic_length\n        )\n        self.title_write_box.enable_autocomplete(\n            func=self._topic_box_autocomplete,\n            key=primary_key_for_command(\"AUTOCOMPLETE\"),\n            key_reverse=primary_key_for_command(\"AUTOCOMPLETE_REVERSE\"),\n        )\n        self.title_write_box.set_completer_delims(\"\")\n\n        # NOTE: stream marker should be set during initialization\n        self.header_write_box = urwid.Columns(\n            [\n                (\"pack\", urwid.Text((\"default\", \"@\"))),\n                self.stream_write_box,\n                (\"pack\", urwid.Text(STREAM_TOPIC_SEPARATOR)),\n                self.title_write_box,\n            ],\n            dividechars=1,\n        )\n        header_line_box = urwid.LineBox(\n            self.header_write_box,\n            tlcorner=\"\u2501\",\n            tline=\"\u2501\",\n            trcorner=\"\u2501\",", "idx": 1722}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.stream_box_edit_view", "completion": "        self.stream_write_box = ReadlineEdit(\n            edit_text=caption, max_char=self.model.max_stream_name_length\n        )\n        self.stream_write_box.enable_autocomplete(\n            func=self._stream_box_autocomplete,\n            key=primary_key_for_command(\"AUTOCOMPLETE\"),\n            key_reverse=primary_key_for_command(\"AUTOCOMPLETE_REVERSE\"),\n        )\n        self.stream_write_box.set_completer_delims(\"\")\n        self._setup_common_stream_compose(stream_id, caption, title)\n\n        # Add an edit mode button to the header write box\n        self.edit_mode_button = EditModeButton(\n            self.view.controller,\n            self.view.write_box,\n            self.view.header,\n            self.view.footer,\n        )\n        self.header_write_box.contents.append((self.edit_mode_button, self.options()))\n\n        # Use and set a callback to set the stream marker\n        self._set_stream_write_box_style(None, caption)\n        urwid.connect_signal(\n            self.stream_write_box, \"change\", self._set_stream_write_box_style\n        )\n", "idx": 1723}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._set_stream_write_box_style", "completion": "        stream_marker = STREAM_MARKER_PRIVATE if self.model.is_valid_stream(new_text) else STREAM_MARKER_PUBLIC\n        self.header_write_box.focus_position = 0\n        self.header_write_box.contents[0] = (\n            urwid.AttrMap(\n                urwid.Text(stream_marker),\n                \"bar\",\n                \"focused\",\n            ),\n            self.header_write_box.options(),\n        )\n", "idx": 1724}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._to_box_autocomplete", "completion": "        # Get the list of users from the view.\n        user_list = self.view.users\n\n        # Split the text by comma and get the most recent recipient for autocomplete.\n        recipients, recent_recipient = self.recipient_emails\n\n        # Find the users that match the latest text.\n        match_user = [\n            user\n            for user in user_list\n            if match_user_name_and_email(recent_recipient, user)\n        ]\n\n        # Append the autocompleted recipients to the string containing the previous recipients.\n        if match_user:\n            recipients.append(match_user[state])\n\n        # Get the full names of the matching users.\n        user_names = [\n            get_display_recipient(user) for user in recipients if user != \"\"\n        ]\n\n        # Process the typeaheads using the updated recipients, state, and user names.\n        return self.process_typeaheads(text, state, user_names)\n", "idx": 1725}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._topic_box_autocomplete", "completion": "        # Get the list of topic names from the model.\n        topics_list = self.model.topics_in_stream(self.stream_id)\n\n        # Match the topic names with the input text to generate typeaheads.\n        typeaheads = [\n            topic for topic in topics_list if match_topics(topic, text)\n        ]\n\n        # Append the potential autocompleted topic names to the input text.\n        updated_recipients = [\n            f\"{text}{typeahead}\" for typeahead in typeaheads\n        ]\n\n        # Process the typeaheads and return them as suggestions.\n        return self._process_typeaheads(updated_recipients, state, typeaheads)\n", "idx": 1726}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._stream_box_autocomplete", "completion": "        stream_names = self.view.pinned_streams + self.view.unpinned_streams\n\n        stream_typeaheads = match_stream(stream_names, text)\n\n        # Typeaheads and suggestions are the same.\n        return self._process_typeaheads(stream_typeaheads, state, stream_typeaheads)\n", "idx": 1727}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.generic_autocomplete", "completion": "        # Check for specific prefixes in the text and call the corresponding autocomplete function.\n        if text.startswith(\"@\"):\n            return self.user_box_autocomplete(text, state)\n        elif text.startswith(\"#\"):\n            return self.stream_box_autocomplete(text, state)\n        else:\n            return None\n", "idx": 1728}
{"namespace": "zulipterminal.ui_tools.boxes.PanelSearchBox.reset_search_text", "completion": "        self.set_caption(self.search_text)\n        self.set_edit_text(\"\")\n", "idx": 1729}
{"namespace": "zulipterminal.ui_tools.boxes.PanelSearchBox.valid_char", "completion": "        if self.edit_text == \"\":\n            return unicodedata.category(ch)[0] != \"C\" and ch != \" \"\n        else:\n            return super().valid_char(ch)\n", "idx": 1730}
{"namespace": "zulipterminal.ui_tools.utils.is_muted", "completion": "    if msg[\"type\"] == \"private\":\n        return False\n    if model.narrow and model.narrow[0][1] == \"topic\":\n        return False\n    return (\n        msg[\"display_recipient\"] in model.is_muted_topic\n        and msg[\"subject\"] in model.is_muted_topic[msg[\"display_recipient\"]]\n    ) or msg[\"sender_id\"] in model.is_muted\n\n", "idx": 1731}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.update_count", "completion": "        if text_color is None:\n            text_color = self.original_color\n\n        self.count = count\n        self.text_color = text_color\n        self.count_text = str(count) if count != 0 else \"\"\n        self.update_widget()\n", "idx": 1732}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.update_widget", "completion": "        self.button_prefix.set_text(count_text[0])\n        self._label.set_text(count_text[1])\n        self.button_suffix.set_text(\"\")\n        self._w.set_attr_map({None: text_color})\n", "idx": 1733}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.keypress", "completion": "        if key == \"enter\":\n            self.activate(key)\n        else:\n            super().keypress(size, key)\n\n", "idx": 1734}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._parse_narrow_link", "completion": "        # The expected format of the fragment is one of the following types:\n        # a. narrow/stream/[{stream_id}-]{stream-name}\n        # b. narrow/stream/[{stream_id}-]{stream-name}/near/{message_id}\n        # c. narrow/stream/[{stream_id}-]{stream-name}/topic/{encoded.20topic.20name}\n        # d. narrow/stream/[{stream_id}-]{stream-name}/topic/{encoded.20topic.20name}/near/{message_id}\n        #\n        # The following table describes the expected types of each part of the fragment.\n        #\n        # +-------------------------------------------+-----------------------------------------------+----------------------------------------------------------+\n        # | Part                                      | Type                                         | Example                                                  |\n        # +===========================================+===============================================+==========================================================+\n        # | Part                                      | Type                                         | Example                                                  |\n        # +-------------------------------------------+-----------------------------------------------+----------------------------------------------------------+\n        # | narrow/                                   | \"narrow\"                                     | narrow/                                                  |\n        # |                                           |                                               |                                                          |\n        # |                                           |                                               |                                                          |\n        # +-------------------------------------------+-----------------------------------------------+----------------------------------------------------------+\n        # | {stream_id}-                              | \"stream_id\"                                   | 103-                                                     |\n        # |                                           |                                               |                                                          |\n        # |                                           |                                               |                                                          |\n        # +-------------------------------------------+-----------------------------------------------+----------------------------------------------------------+\n        # | {stream-name}                             | \"stream_name\"                                 | design                                                   |\n        # |                                           |                                               |                                                          |\n        # |                                           |                                               |                                                          |\n        # +-------------------------------------------+-----------------------------------------------+----------------------------------------------------------+\n        # | /near/                                    | \"near\"                                        |", "idx": 1735}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._validate_and_patch_stream_data", "completion": "        stream_data = parsed_link[\"stream\"]\n        stream_id = stream_data[\"stream_id\"]\n        stream_name = stream_data[\"stream_name\"]\n\n        if stream_id is not None:\n            if not self.model.is_user_subscribed_to_stream(stream_id):\n                return \"The stream is either unknown or unsubscribed\"\n        elif stream_name is not None:\n            if not self.model.is_valid_stream(stream_name):\n                return \"The stream is either unknown or unsubscribed\"\n            stream_id = self.model.stream_id_from_name(stream_name)\n            if stream_id is None:\n                return \"The stream is either unknown or unsubscribed\"\n            stream_data[\"stream_id\"] = stream_id\n        else:\n            return \"The stream is either unknown or unsubscribed\"\n\n        return \"\"\n", "idx": 1736}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._validate_narrow_link", "completion": "        # Check if the narrow link is a valid narrow link.\n        if parsed_link[\"narrow\"] not in [\"stream\", \"stream:near\", \"stream:topic\", \"stream:topic:near\"]:\n            return \"Invalid narrow link\"\n\n        # Check if the stream ID and name are valid.\n        validation_error_message = self._validate_and_patch_stream_data(parsed_link)\n        if validation_error_message:\n            return validation_error_message\n\n        # Check if the topic name is valid.\n        if parsed_link[\"narrow\"] in [\"stream:topic\", \"stream:topic:near\"]:\n            stream_id = parsed_link[\"stream\"][\"stream_id\"]\n            stream_name = parsed_link[\"stream\"][\"stream_name\"]\n            topic_name = parsed_link[\"topic_name\"]\n            if not self.model.is_valid_stream_name(stream_name) or not self.model.is_valid_stream_topic(\n                stream_id, topic_name\n            ):\n                return \"Invalid topic name\"\n\n        # Check if the message ID is valid.\n        if parsed_link[\"narrow\"] in [\"stream:near\", \"stream:topic:near\"]:\n            message_id = parsed_link[\"message_id\"]\n            if not self.model.is_valid_message(message_id):\n                return \"Invalid message ID\"\n\n        return \"\"\n", "idx": 1737}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._switch_narrow_to", "completion": "        if parsed_link[\"narrow\"] == \"stream\":\n            self.controller.narrow_to_stream(\n                stream_name=parsed_link[\"stream\"][\"stream_name\"]\n            )\n        elif parsed_link[\"narrow\"] == \"topic\":\n            self.controller.narrow_to_topic(\n                stream_name=parsed_link[\"stream\"][\"stream_name\"],\n                topic_name=parsed_link[\"topic_name\"],\n            )\n        elif parsed_link[\"narrow\"] == \"stream:near\":\n            self.controller.narrow_to_stream(\n                stream_name=parsed_link[\"stream\"][\"stream_name\"],\n                contextual_message_id=parsed_link[\"message_id\"],\n            )\n        elif parsed_link[\"narrow\"] == \"topic:near\":\n            self.controller.narrow_to_topic(\n                stream_name=parsed_link[\"stream\"][\"stream_name\"],\n                topic_name=parsed_link[\"topic_name\"],\n                contextual_message_id=parsed_link[\"message_id\"],\n            )\n", "idx": 1738}
{"namespace": "zulipterminal.config.themes.complete_and_incomplete_themes", "completion": "    complete_themes = []\n    incomplete_themes = []\n    for theme_name, theme_styles in THEMES.items():\n        if theme_styles.get(\"meta\") == REQUIRED_META:\n            complete_themes.append(theme_name)\n        else:\n            incomplete_themes.append(theme_name)\n    complete_themes.sort()\n    incomplete_themes.sort()\n    return complete_themes, incomplete_themes\n\n", "idx": 1739}
{"namespace": "zulipterminal.config.themes.validate_colors", "completion": "    if color_depth != 16:\n        return\n\n    theme_styles = THEMES[theme_name].STYLES\n    for style in theme_styles:\n        if style[0] is not None:\n            if style[0] not in valid_16_color_codes:\n                raise InvalidThemeColorCode(\n                    f\"Invalid color code {style[0]} in theme {theme_name}\"\n                )\n\n", "idx": 1740}
{"namespace": "zulipterminal.config.themes.parse_themefile", "completion": "    urwid_theme = []\n    for style, (fg, bg) in theme_styles.items():\n        if style is None:\n            style = \"\"\n        if fg is not None:\n            fg = convert_color(fg, color_depth)\n        if bg is not None:\n            bg = convert_color(bg, color_depth)\n        urwid_theme.append((style, fg, bg))\n    return urwid_theme\n\n", "idx": 1741}
{"namespace": "zulipterminal.config.themes.add_pygments_style", "completion": "    pygments_styles = theme_meta[\"pygments\"][\"styles\"]\n    pygments_background = theme_meta[\"pygments\"][\"background\"]\n    pygments_overrides = theme_meta[\"pygments\"][\"overrides\"]\n\n    # Add pygments styles to urwid theme\n    for style in pygments_styles:\n        urwid_theme.append(style)\n\n    # Add pygments background to urwid theme\n    urwid_theme.append((\"default\", \"\", \"\", \"\", \"\", pygments_background))\n\n    # Add pygments overrides to urwid theme\n    for override in pygments_overrides:\n        urwid_theme.append(override)\n\n    # Add pygments styles to pygments styles\n    for token, style in pygments_styles:\n        if style in REQUIRED_STYLES:\n            continue\n        if style == \"pygments\":\n            continue\n        if style == \"pygments_keyword\":\n            style = \"keyword\"\n        if style == \"pygments_literal\":\n            style = \"literal\"\n        if style == \"pygments_name\":\n            style = \"name\"\n        if style == \"pygments_string\":\n            style = \"string\"\n        if style == \"pygments_comment\":\n            style = \"comment\"\n        if style == \"pygments_operator\":\n            style = \"operator\"\n        if style == \"pygments_punctuation\":\n            style = \"punctuation\"\n        if style == \"pygments_generic\":\n            style = \"generic\"\n        if style == \"pygments_generic_embedded\":\n            style = \"generic.embedded\"\n        if style == \"pygments_generic_heading\":\n            style = \"generic.heading\"\n        if style == \"pygments_generic_prompt\":\n            style = \"generic.prompt\"\n        if style == \"pygments_generic_strong\":\n            style = \"generic.strong\"\n        if style == \"pygments_generic_subheading\":\n            style = \"generic.subheading\"\n        if", "idx": 1742}
{"namespace": "zulipterminal.config.keys.is_command_key", "completion": "    if command in KEY_BINDINGS:\n        for key_combo in KEY_BINDINGS[command][\"keys\"]:\n            if key == key_combo:\n                return True\n    return False\n\n", "idx": 1743}
{"namespace": "zulipterminal.config.keys.keys_for_command", "completion": "    try:\n        return KEY_BINDINGS[command][\"keys\"]\n    except KeyError as exception:\n        raise InvalidCommand(command)\n\n", "idx": 1744}
{"namespace": "zulipterminal.config.keys.commands_for_random_tips", "completion": "    return [\n        key_binding\n        for key_binding in KEY_BINDINGS.values()\n        if not key_binding.get(\"excluded_from_random_tips\", False)\n    ]\n\n", "idx": 1745}
{"namespace": "hypertools.datageometry.DataGeometry.transform", "completion": "        # if no data is passed, return the xform_data\n        if data is None:\n            return self.xform_data\n\n        # convert to numpy array if text\n        if isinstance(data, list):\n            data = list(map(convert_text, data))\n\n        # if the data is a list of dataframes, convert to list of arrays\n        if isinstance(data, list) and isinstance(data[0], pd.DataFrame):\n            data = list(map(lambda x: x.values, data))\n\n        # if the data is a dataframe, convert to array\n        if isinstance(data, pd.DataFrame):\n            data = data.values\n\n        # if the data is a list of arrays, convert to list of arrays\n        if isinstance(data, list) and isinstance(data[0], np.ndarray):\n            data = data\n\n        # if the data is an array, convert to list of arrays\n        if isinstance(data, np.ndarray):\n            data = [data]\n\n        # if the data is a list of lists, convert to list of arrays\n        if isinstance(data, list) and isinstance(data[0], list):\n            data = list(map(np.array, data))\n\n        # if the data is a list of strings, convert to list of arrays\n        if isinstance(data, list) and isinstance(data[0], str):\n            data = list(map(convert_text, data))\n\n        # if the data is a list of numbers, convert to list of arrays\n        if isinstance(data, list) and isinstance(data[0], (int, float)):\n            data = list(map(np.array, data))\n\n        # if the data is a list of lists, convert to list of arrays\n        if isinstance(data, list) and isinstance(data[0], (int, float)):\n            data = list(map(np.array, data))\n\n        # if the data is a list of lists, convert to list of arrays\n        if isinstance(data, list) and isinstance(data[0], (int, float)):\n            data = list(", "idx": 1746}
{"namespace": "hypertools.datageometry.DataGeometry.plot", "completion": "        # if no new data passed,\n        if data is None:\n            data = self.xform_data\n        else:\n            data = format_data(data)\n\n        # if no kwargs passed,\n        if self.kwargs is None:\n            self.kwargs = {}\n\n        # update kwargs with any passed kwargs\n        self.kwargs.update(kwargs)\n\n        # if the data is a list of dataframes,\n        if isinstance(data, list):\n            # if the data is a list of dataframes,\n            if isinstance(data[0], pd.DataFrame):\n                # if the data is a list of dataframes,\n                if isinstance(data[0].index, pd.MultiIndex):\n                    # if the data is a list of dataframes,\n                    if isinstance(data[0].columns, pd.MultiIndex):\n                        # if the data is a list of dataframes,\n                        if isinstance(data[0].index.levels[0], pd.DatetimeIndex):\n                            # if the data is a list of dataframes,\n                            if isinstance(data[0].columns.levels[0], pd.DatetimeIndex):\n                                # if the data is a list of dataframes,\n                                if isinstance(data[0].index.levels[0].freq, pd.tseries.offsets.DateOffset):\n                                    # if the data is a list of dataframes,\n                                    if isinstance(data[0].columns.levels[0].freq, pd.tseries.offsets.DateOffset):\n                                        # if the data is a list of dataframes,\n                                        if isinstance(data[0].index.levels[0].freq.n, int):\n                                            # if the data is a list of dataframes,\n                                            if isinstance(data[0].columns.levels[0].freq.n, int):\n                                                # if the data is a list of dataframes,\n                                                if data[0].index.levels[0].freq.n == data[0].columns.levels[0", "idx": 1747}
{"namespace": "awesome_autodl.autodl_topic2papers", "completion": "    import yaml\n    import os\n    import sys\n    import numpy as np\n    from collections import OrderedDict\n    from autodl.datasets.paper import AutoDLDataset\n    from autodl.utils.logging import logger\n\n    topic2path = autodl_topic2path()\n    topic2papers = OrderedDict()\n    for topic, path in topic2path.items():\n        with open(path, \"r\") as f:\n            papers = yaml.load(f, Loader=yaml.FullLoader)\n            topic2papers[topic] = []\n            for paper in papers:\n                topic2papers[topic].append(AutoDLDataset(**paper))\n    return topic2papers\n\n", "idx": 1748}
{"namespace": "awesome_autodl.get_bib_abbrv_obj", "completion": "    from awesome_autodl.utils import BibAbbreviations\n\n    bib_abbrv_file = get_bib_abbrv_file()\n    bib_abbrv_obj = BibAbbreviations(bib_abbrv_file)\n    return bib_abbrv_obj\n\n", "idx": 1749}
{"namespace": "mssqlcli.localized_strings.translation", "completion": "    global LANGUAGES\n    if languages is None:\n        languages = LANGUAGES\n    return gettext.translation(domain, localedir, languages=languages, fallback=True)\n\n", "idx": 1750}
{"namespace": "mssqlcli.mssqlbuffer._is_query_executable", "completion": "    sql = sql.strip()\n    if len(sql) == 0:\n        return False\n\n    # Check for open comments\n    if sql.find(\"/*\") >= 0:\n        return False\n\n    # Check for open quotes\n    if sql.find(\"'\") >= 0 and sql.find(\"'\") % 2 == 0:\n        return False\n\n    # Check for open quotes\n    if sql.find('\"') >= 0 and sql.find('\"') % 2 == 0:\n        return False\n\n    # Check for open brackets\n    if sql.find(\"(\") >= 0 and sql.find(\")\") >= 0:\n        return False\n\n    # Check for open brackets\n    if sql.find(\"[\") >= 0 and sql.find(\"]\") >= 0:\n        return False\n\n    # Check for open brackets\n    if sql.find(\"{\") >= 0 and sql.find(\"}\") >= 0:\n        return False\n\n    # Check for open brackets\n    if sql.find(\"'\") >= 0 and sql.find(\"'\") % 2 == 0:\n        return False\n\n    # Check for open brackets\n    if sql.find('\"') >= 0 and sql.find('\"') % 2 == 0:\n        return False\n\n    # Check for open brackets\n    if sql.find(\"(\") >= 0 and sql.find(\")\") >= 0:\n        return False\n\n    # Check for open brackets\n    if sql.find(\"[\") >= 0 and sql.find(\"]\") >= 0:\n        return False\n\n    # Check for open brackets\n    if sql.find(\"{\") >= 0 and sql.find(\"}\") >= 0:\n        return False\n\n    # Check for open brackets\n    if sql.find(\"'\") >= 0 and sql.find(\"'\") % 2 == 0:\n        return False\n\n    # Check for open brackets\n    if sql.find('\"') >= 0 and sql.find('\"') % 2 == 0:\n        return False\n\n    # Check for open brackets\n    if sql.find(\"(\") >=", "idx": 1751}
{"namespace": "mssqlcli.telemetry.conclude", "completion": "    _session.end_time = datetime.now()\n    payload = _session.generate_payload()\n    if payload:\n        _output_to_file(payload)\n        if separate_process:\n            return _upload_payload_in_separate_process(service_endpoint_uri, payload)\n        else:\n            return _upload_payload(service_endpoint_uri, payload)\n\n", "idx": 1752}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.start", "completion": "        self.request_thread = threading.Thread(\n            target=self.read_requests,\n            name=self.REQUEST_THREAD_NAME)\n        self.request_thread.daemon = True\n        self.request_thread.start()\n\n        self.response_thread = threading.Thread(\n            target=self.read_responses,\n            name=self.RESPONSE_THREAD_NAME)\n        self.response_thread.daemon = True\n        self.response_thread.start()\n", "idx": 1753}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.submit_request", "completion": "        if method is None or params is None:\n            raise ValueError(u'Method and params must be specified.')\n\n        request = {\n            u'jsonrpc': u'2.0',\n            u'method': method,\n            u'params': params,\n            u'id': request_id\n        }\n\n        self.request_queue.put(request)\n", "idx": 1754}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.get_response", "completion": "        if request_id in self.response_map:\n            return self.response_map[request_id].get()\n        elif owner_uri in self.response_map:\n            return self.response_map[owner_uri].get()\n        elif self.exception_queue.qsize() > 0:\n            raise self.exception_queue.get()\n        else:\n            return None\n", "idx": 1755}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.shutdown", "completion": "        self.cancel = True\n        self.request_queue.put(None)\n        self.request_thread.join()\n        self.writer.close()\n        logger.debug('Json Rpc client shutdown.')\n\n", "idx": 1756}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcWriter.send_request", "completion": "        content = {u'jsonrpc': u'2.0', u'method': method, u'params': params}\n        if request_id is not None:\n            content[u'id'] = request_id\n\n        content_body = json.dumps(content, ensure_ascii=False)\n        content_length = len(content_body)\n        header = self.HEADER.format(content_length)\n\n        try:\n            self.stream.write(header.encode(self.encoding))\n            self.stream.write(content_body.encode(self.encoding))\n            self.stream.flush()\n        except ValueError as error:\n            logger.debug(u'Stream was closed.')\n            raise error\n", "idx": 1757}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.read_response", "completion": "        while self.needs_more_data:\n            self._read_data()\n            if self.read_state == ReadState.Header:\n                self._read_header()\n            elif self.read_state == ReadState.Content:\n                self._read_content()\n\n        # Parse the content as JSON.\n        try:\n            content = self.buffer[:self.expected_content_length].decode(self.encoding)\n            return json.loads(content)\n        except json.JSONDecodeError as ex:\n            logger.debug(u'Read Response encountered exception %s', ex)\n            raise ValueError(u'Invalid JSON content.')\n", "idx": 1758}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.read_next_chunk", "completion": "        # If we need to resize the buffer, do so.\n        if self.needs_to_resize_buffer():\n            self.resize_buffer()\n\n        # Read a chunk from the stream.\n        chunk_size = self.stream.readinto(self.buffer)\n\n        # If the stream is empty or closed, raise an exception.\n        if chunk_size == 0:\n            raise EOFError(u'End of stream reached.')\n\n        # Update the buffer offset.\n        self.read_offset += chunk_size\n\n        return True\n", "idx": 1759}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.try_read_headers", "completion": "        # Check if we have enough data to read a header.\n        if self.buffer_end_offset - self.read_offset < 4:\n            return False\n\n        # Check if we have a header delimiter.\n        if self.buffer[self.read_offset] != self.CR or self.buffer[\n                self.read_offset + 1] != self.LF or self.buffer[\n                    self.read_offset + 2] != self.CR or self.buffer[\n                        self.read_offset + 3] != self.LF:\n            return False\n\n        # We have a header delimiter.\n        # Read the headers.\n        header_end_offset = self.read_offset\n        while header_end_offset < self.buffer_end_offset:\n            # Check if we have a header delimiter.\n            if self.buffer[header_end_offset] == self.CR and self.buffer[\n                    header_end_offset + 1] == self.LF and self.buffer[\n                        header_end_offset + 2] == self.CR and self.buffer[\n                            header_end_offset + 3] == self.LF:\n                break\n            header_end_offset += 1\n\n        # Check if we have read a full header.\n        if header_end_offset == self.read_offset:\n            return False\n\n        # We have a full header.\n        # Read the header.\n        header = self.buffer[self.read_offset:header_end_offset].decode(\n            self.encoding)\n        # Split the header by new line.\n        header_lines = header.splitlines()\n        # Iterate over the header lines.\n        for header_line in header_lines:\n            # Split the header line by ':'.\n            header_line_split = header_line.split(u':')\n            # Check if the header line is valid.\n            if len(header_line_split) != 2:\n                return False\n            # Store the header.\n            self.headers[header_line_split[0].lower()] = header_line_split[1]", "idx": 1760}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.close", "completion": "        try:\n            self.stream.close()\n        except AttributeError:\n            raise AttributeError(\"Failed to close the stream associated with the JsonRpcReader instance.\")", "idx": 1761}
{"namespace": "mssqlcli.packages.prioritization.PrevalenceCounter.update", "completion": "        # Split the text into tokens\n        tokens = sqlparse.parse(text)\n\n        # Iterate through the tokens\n        for token in tokens[0].flatten():\n            # If the token is a keyword, update the keyword counts\n            if token.ttype in [Name, Name.Builtin]:\n                for keyword in keywords:\n                    if keyword_regexs[keyword].search(token.value):\n                        self.keyword_counts[keyword] += 1\n                        break\n            # If the token is a name, update the name counts\n            elif token.ttype == Name.Variable:\n                self.name_counts[token.value] += 1\n", "idx": 1762}
{"namespace": "mssqlcli.packages.sqlcompletion.suggest_type", "completion": "    # Check if the input text starts with \\\\i\n    if text_before_cursor.startswith('\\\\i ') or text_before_cursor.startswith('\\\\.\\\\i '):\n        return Path(), text_before_cursor\n\n    # Create a SqlStatement instance\n    statement = SqlStatement(full_text, text_before_cursor)\n\n    # Check for special commands\n    if statement.parsed.token_first().value.lower() == '\\\\':\n        return suggest_special(statement.text_before_cursor)\n\n    # Check for special commands\n    if statement.parsed.token_first().value.lower() == '\\\\':\n        return suggest_special(statement.text_before_cursor)\n\n    # Check for special commands\n    if statement.parsed.token_first().value.lower() == '\\\\':\n        return suggest_special(statement.text_before_cursor)\n\n    # Check for special commands\n    if statement.parsed.token_first().value.lower() == '\\\\':\n        return suggest_special(statement.text_before_cursor)\n\n    # Check for special commands\n    if statement.parsed.token_first().value.lower() == '\\\\':\n        return suggest_special(statement.text_before_cursor)\n\n    # Check for special commands\n    if statement.parsed.token_first().value.lower() == '\\\\':\n        return suggest_special(statement.text_before_cursor)\n\n    # Check for special commands\n    if statement.parsed.token_first().value.lower() == '\\\\':\n        return suggest_special(statement.text_before_cursor)\n\n    # Check for special commands\n    if statement.parsed.token_first().value.lower() == '\\\\':\n        return suggest_special(statement.text_before_cursor)\n\n    # Check for special commands\n    if statement.parsed.token_first().value.lower() == '\\\\':\n        return suggest_special(statement.text_before_cursor)\n\n    # Check for special commands\n    if statement.parsed.token_first().value.lower() == '\\\\':\n        return suggest_", "idx": 1763}
{"namespace": "mssqlcli.packages.parseutils.ctes.extract_ctes", "completion": "    # Parse the query using the sqlparse library\n    parsed = parse(sql)\n\n    # Check if the query contains CTEs\n    if not parsed.token_first().match(Keyword, 'WITH'):\n        return [], sql\n\n    # Extract the CTEs from the query\n    ctes = []\n    for cte in parsed.token_first().tokens[1:]:\n        # Extract the CTE name\n        name = cte.tokens[0].tokens[0]\n        # Extract the CTE columns\n        columns = [column.tokens[0] for column in cte.tokens[2].tokens[1:]]\n        # Extract the CTE body\n        body = cte.tokens[-1]\n        # Extract the CTE start and stop positions\n        start = cte.tokens[0].start\n        stop = cte.tokens[-1].stop\n        # Create a TableExpression namedtuple\n        ctes.append(TableExpression(name, columns, start, stop))\n\n    # Remove the CTEs from the query\n    sql = sql[:ctes[0].start] + sql[ctes[-1].stop:]\n\n    return ctes, sql\n\n", "idx": 1764}
{"namespace": "mssqlcli.packages.parseutils.tables.extract_tables", "completion": "    parsed = sqlparse.parse(sql)\n    if not parsed:\n        return ()\n\n    # INSERT statements must stop looking for tables at the sign of first (\"(\")\n    # because table name might be inside parenthesis.\n    stop_at_punctuation = (parsed[0].get_type() == 'INSERT')\n    token_stream = extract_from_part(parsed[0], stop_at_punctuation)\n    return tuple(extract_table_identifiers(token_stream))\n\n", "idx": 1765}
{"namespace": "googleapiclient.channel.Channel.body", "completion": "        body = {\n            \"id\": self.id,\n            \"token\": self.token,\n            \"type\": self.type,\n            \"address\": self.address,\n        }\n        if self.params:\n            body[\"params\"] = self.params\n        if self.resource_id:\n            body[\"resourceId\"] = self.resource_id\n        if self.resource_uri:\n            body[\"resourceUri\"] = self.resource_uri\n        if self.expiration:\n            body[\"expiration\"] = self.expiration\n        return body\n", "idx": 1766}
{"namespace": "googleapiclient.channel.Channel.update", "completion": "        for key, value in CHANNEL_PARAMS.items():\n            if key in resp:\n                setattr(self, value, resp[key])\n", "idx": 1767}
{"namespace": "googleapiclient.channel.notification_from_headers", "completion": "    # Convert the headers to a case-insensitive dictionary.\n    headers = _upper_header_keys(headers)\n\n    # Check that the channel id matches.\n    if channel.id != headers[X_GOOG_CHANNEL_ID]:\n        raise InvalidNotificationError(\n            \"Channel id mismatch: %s != %s\"\n            % (channel.id, headers[X_GOOG_CHANNEL_ID])\n        )\n\n    # Check that the message number is an integer.\n    try:\n        message_number = int(headers[X_GOOG_MESSAGE_NUMBER])\n    except ValueError:\n        raise InvalidNotificationError(\n            \"Message number is not an integer: %s\" % headers[X_GOOG_MESSAGE_NUMBER]\n        )\n\n    # Check that the resource state is valid.\n    if headers[X_GOOG_RESOURCE_STATE] not in (\"exists\", \"not_exists\", \"sync\"):\n        raise InvalidNotificationError(\n            \"Invalid resource state: %s\" % headers[X_GOOG_RESOURCE_STATE]\n        )\n\n    # Check that the resource uri matches.\n    if channel.resource_uri != headers[X_GOOG_RESOURCE_URI]:\n        raise InvalidNotificationError(\n            \"Resource uri mismatch: %s != %s\"\n            % (channel.resource_uri, headers[X_GOOG_RESOURCE_URI])\n        )\n\n    # Check that the resource id matches.\n    if channel.resource_id != headers[X_GOOG_RESOURCE_ID]:\n        raise InvalidNotificationError(\n            \"Resource id mismatch: %s != %s\"\n            % (channel.resource_id, headers[X_GOOG_RESOURCE_ID])\n        )\n\n    # Return a new notification.\n    return Notification(\n        message_number,\n        headers[X_GOOG_RESOURCE_STATE],\n        headers[X_GOOG_RESOURCE_URI],\n        headers[X_GOOG_RESOURCE_ID],\n   ", "idx": 1768}
{"namespace": "googleapiclient.channel.new_webhook_channel", "completion": "    if expiration is None:\n        expiration = datetime.datetime.utcnow() + datetime.timedelta(days=1)\n    expiration = int((expiration - EPOCH).total_seconds() * 1000)\n    return Channel(\n        \"web_hook\",\n        str(uuid.uuid4()),\n        token,\n        url,\n        expiration=expiration,\n        params=params,\n    )", "idx": 1769}
{"namespace": "googleapiclient.model.BaseModel._build_query", "completion": "        if self.alt_param is not None:\n            params[self.alt_param] = \"json\"\n        query = \"\"\n        for key, value in params.items():\n            if isinstance(value, list):\n                for v in value:\n                    if callable(v):\n                        v = v()\n                    query += \"&%s=%s\" % (urllib.parse.quote(key, \"\"), urllib.parse.quote(v, \"\"))\n            else:\n                if callable(value):\n                    value = value()\n                query += \"&%s=%s\" % (urllib.parse.quote(key, \"\"), urllib.parse.quote(value, \"\"))\n        return query[1:]\n", "idx": 1770}
{"namespace": "googleapiclient.model.BaseModel.response", "completion": "        self._log_response(resp, content)\n        if resp.status < 300:\n            if resp.status == 204:\n                return self.no_content_response\n            return self.deserialize(content)\n        else:\n            raise HttpError(resp, self.deserialize(content))\n", "idx": 1771}
{"namespace": "googleapiclient.model.makepatch", "completion": "    patch = {}\n    for key in modified:\n        if key in original:\n            if isinstance(original[key], dict):\n                if original[key] != modified[key]:\n                    patch[key] = makepatch(original[key], modified[key])\n            elif isinstance(original[key], list):\n                if original[key] != modified[key]:\n                    patch[key] = modified[key]\n            else:\n                if original[key] != modified[key]:\n                    patch[key] = modified[key]\n        else:\n            patch[key] = modified[key]\n    return patch", "idx": 1772}
{"namespace": "googleapiclient._helpers.update_query_params", "completion": "    # Parse the URI and extract the query parameters.\n    parsed_uri = urllib.parse.urlparse(uri)\n    query_params = parse_unique_urlencoded(parsed_uri.query)\n\n    # Update the query parameters with the new ones.\n    query_params.update(params)\n\n    # Encode the new query parameters.\n    encoded_query_params = urllib.parse.urlencode(query_params)\n\n    # Build the new URI.\n    new_uri = urllib.parse.urlunparse(\n        (\n            parsed_uri.scheme,\n            parsed_uri.netloc,\n            parsed_uri.path,\n            parsed_uri.params,\n            encoded_query_params,\n            parsed_uri.fragment,\n        )\n    )\n\n    return new_uri", "idx": 1773}
{"namespace": "googleapiclient._helpers._add_query_parameter", "completion": "    if value is None:\n        return url\n    else:\n        scheme, netloc, path, query, fragment = urllib.parse.urlsplit(url)\n        query_dict = urllib.parse.parse_qs(query)\n        query_dict[name] = [value]\n        new_query = urllib.parse.urlencode(query_dict, doseq=True)\n        return urllib.parse.urlunsplit((scheme, netloc, path, new_query, fragment))\n\n", "idx": 1774}
{"namespace": "gif_for_cli.display.display_txt_frames", "completion": "    try:\n        # Iterate through the text frames and print them on a new line.\n        for i in range(num_loops):\n            for txt_frame in txt_frames:\n                stdout.write(txt_frame + '\\n')\n                time.sleep(seconds_per_frame)\n\n    except KeyboardInterrupt:\n        raise KeyboardInterrupt\n\n    except Exception as e:\n        raise e\n\n", "idx": 1775}
{"namespace": "pyVmomi.SoapAdapter.SoapResponseDeserializer.Deserialize", "completion": "        self.result = None\n        self.fault = None\n        self.parser = ParserCreate(namespace_separator=NS_SEP)\n        self.origHandlers = GetHandlers(self.parser)\n        SetHandlers(self.parser, GetHandlers(self))\n        self.parser.ParseFile(response)\n        result = self.result\n        fault = self.fault\n        SetHandlers(self.parser, self.origHandlers)\n        del self.parser, self.origHandlers, self.result, self.fault\n        if fault:\n            if isinstance(fault, MethodFault):\n                raise fault\n            else:\n                raise TypeError(fault)\n        if not IsChildVersion(self.version, resultType):\n            result = self.deser.ChangeVersion(result, resultType, self.version)\n        return result\n", "idx": 1776}
{"namespace": "pyVmomi.VmomiSupport.GetRequestContext", "completion": "    if not hasattr(_threadLocalContext, \"context\"):\n        _threadLocalContext.context = StringDict()\n    return _threadLocalContext.context\n\n", "idx": 1777}
{"namespace": "pycoin.bloomfilter.filter_size_required", "completion": "    m = int(-1 / (math.pow(LOG_2, 2) * element_count * math.log(false_positive_probability)))\n    # We use 8 bits per byte.\n    return int(math.ceil(m / 8.0))\n\n", "idx": 1778}
{"namespace": "pycoin.bloomfilter.BloomFilter.add_spendable", "completion": "        self.add_item(spendable.to_bytes())\n", "idx": 1779}
{"namespace": "pycoin.bloomfilter.murmur3", "completion": "    c1 = 0xcc9e2d51\n    c2 = 0x1b873593\n\n    length = len(data)\n    h1 = seed\n    roundedEnd = (length & 0xfffffffc)  # round down to 4 byte block\n    for i in range(0, roundedEnd, 4):\n        # little endian load order\n        k1 = (data[i] & 0xff) | ((data[i + 1] & 0xff) << 8) | \\\n             ((data[i + 2] & 0xff) << 16) | (data[i + 3] << 24)\n        k1 *= c1\n        k1 = (k1 << 15) | ((k1 & 0xffffffff) >> 17)  # ROTL32(k1,15)\n        k1 *= c2\n\n        h1 ^= k1\n        h1 = (h1 << 13) | ((h1 & 0xffffffff) >> 19)  # ROTL32(h1,13)\n        h1 = h1 * 5 + 0xe6546b64\n\n    # tail\n    k1 = 0\n\n    val = length & 0x03\n    if val == 3:\n        k1 = (data[roundedEnd + 2] & 0xff) << 16\n    # fallthrough\n    if val in [2, 3]:\n        k1 |= (data[roundedEnd + 1] & 0xff) << 8\n    # fallthrough\n    if val >= 2:\n        k1 |= data[roundedEnd] & 0xff\n    k1 *= c1\n    k1 = (k1 << 15) | ((k1 & 0xffffffff) >> 17)  # ROTL32(k1,15)\n    k1 *= c2\n    h1 ^= k1\n\n    # finalization", "idx": 1780}
{"namespace": "pycoin.networks.registry.network_for_netcode", "completion": "    for prefix in search_prefixes():\n        try:\n            network_module = importlib.import_module(prefix + \".\" + symbol)\n            if network_module.symbol == symbol:\n                return network_module\n        except ImportError:\n            pass\n    raise ValueError(\"no network found for symbol %s\" % symbol)\n\n", "idx": 1781}
{"namespace": "pycoin.satoshi.IntStreamer.IntStreamer.int_from_script_bytes", "completion": "        if len(s) == 0:\n            return 0\n\n        x = s[-1]\n        if x & 0x80:\n            return -((x & 0x7f) << 8) + (s[-2] if len(s) > 1 else 0)\n        else:\n            return (x << 8) + (s[-2] if len(s) > 1 else 0)", "idx": 1782}
{"namespace": "pycoin.satoshi.stackops.do_OP_RIPEMD160", "completion": "    v = stack.pop()\n    stack.append(hashlib.new('ripemd160', v).digest())\n\n", "idx": 1783}
{"namespace": "pycoin.satoshi.stackops.do_OP_HASH160", "completion": "    stack.append(hashlib.sha256(stack.pop()).digest())\n\n", "idx": 1784}
{"namespace": "pycoin.satoshi.stackops.do_OP_HASH256", "completion": "    stack.append(hashlib.sha256(stack.pop()).digest())\n\n", "idx": 1785}
{"namespace": "pycoin.services.providers.providers_for_config_string", "completion": "    providers = []\n    descriptors = config_string.split(\",\")\n    for descriptor in descriptors:\n        provider = provider_for_descriptor_and_netcode(descriptor, netcode)\n        if provider is not None:\n            providers.append(provider)\n        else:\n            warnings.warn(\"Could not parse provider for descriptor %s\" % descriptor)\n    return providers\n\n", "idx": 1786}
{"namespace": "pycoin.services.providers.get_default_providers_for_netcode", "completion": "    if netcode is None:\n        netcode = get_current_netcode()\n    if not hasattr(THREAD_LOCALS, \"providers\"):\n        THREAD_LOCALS.providers = {}\n    if netcode not in THREAD_LOCALS.providers:\n        THREAD_LOCALS.providers[netcode] = providers_for_netcode_from_env(netcode)\n    return THREAD_LOCALS.providers[netcode]", "idx": 1787}
{"namespace": "pycoin.services.providers.set_default_providers_for_netcode", "completion": "    if not hasattr(THREAD_LOCALS, \"providers\"):\n        THREAD_LOCALS.providers = {}\n    THREAD_LOCALS.providers[netcode] = provider_list", "idx": 1788}
{"namespace": "pycoin.blockchain.BlockChain.BlockChain.tuple_for_index", "completion": "        if index < 0:\n            index += self.length()\n        if index < len(self._locked_chain):\n            return self._locked_chain[index]\n        if index < len(self._longest_local_block_chain()):\n            return self._longest_local_block_chain()[index]\n        if index < len(self._longest_chain_cache()):\n            return self._longest_chain_cache()[index]\n        return None\n", "idx": 1789}
{"namespace": "pycoin.blockchain.ChainFinder.ChainFinder.find_ancestral_path", "completion": "        # If the two nodes are the same, return an empty list\n        if h1 == h2:\n            return [], []\n\n        # If the first node is None, return the second node's ancestral path\n        if h1 is None:\n            return [], self.maximum_path(h2, path_cache)\n\n        # If the second node is None, return the first node's ancestral path\n        if h2 is None:\n            return self.maximum_path(h1, path_cache), []\n\n        # If the first node is the common ancestor, return the second node's ancestral path\n        if h1 == h2:\n            return [], self.maximum_path(h2, path_cache)\n\n        # If the first node is the second node's ancestor, return the first node's ancestral path\n        if h1 in self.descendents_by_top.get(h2, []):\n            return self.maximum_path(h1, path_cache), []\n\n        # If the second node is the first node's ancestor, return the second node's ancestral path\n        if h2 in self.descendents_by_top.get(h1, []):\n            return [], self.maximum_path(h2, path_cache)\n\n        # If the first node is not the second node's ancestor, but the second node is the first node's ancestor, return the first node's ancestral path\n        if h1 in self.descendents_by_top.get(h2, []):\n            return self.maximum_path(h1, path_cache), []\n\n        # If the second node is not the first node's ancestor, but the first node is the second node's ancestor, return the second node's ancestral path\n        if h2 in self.descendents_by_top.get(h1, []):\n            return [], self.maximum_path(h2, path_cache)\n\n        # If the first node is not the second node's ancestor, and the second node is not the first node's ancestor,", "idx": 1790}
{"namespace": "pycoin.contrib.bech32m.bech32_encode", "completion": "    if spec == Encoding.BECH32:\n        return hrp + \"1\" + \"\".join(CHARSET[x] for x in data)\n    return hrp + \"m1\" + \"\".join(CHARSET[x] for x in data)\n\n", "idx": 1791}
{"namespace": "pycoin.contrib.bech32m.decode", "completion": "    if addr is None:\n        return (None, None)\n    if len(addr) < 2 or len(addr) > 90:\n        return (None, None)\n    if addr[0:2] != hrp:\n        return (None, None)\n    if addr[0:2] == hrp and addr[2] == '1':\n        data = convertbits(list(addr[3:]), 5, 8, False)\n        if data is None or len(data) % 8 != 0:\n            return (None, None)\n        return (addr[2], data)\n    if addr[0:2] == hrp and addr[2] == '0':\n        data = convertbits(list(addr[3:]), 5, 5, False)\n        if data is None:\n            return (None, None)\n        return (addr[2], data)\n    return (None, None)\n\n", "idx": 1792}
{"namespace": "pycoin.crack.bip32.crack_bip32", "completion": "    for child in path:\n        secret_exponent = ascend_bip32(bip32_pub_node, secret_exponent, child)\n        bip32_pub_node = bip32_pub_node.child_safe(child)\n    return bip32_pub_node", "idx": 1793}
{"namespace": "pycoin.message.PeerAddress.ip_bin_to_ip4_addr", "completion": "    return \".\".join(str(v) for v in struct.unpack(\"BBBB\", ip_bin))\n\n", "idx": 1794}
{"namespace": "pycoin.message.PeerAddress.PeerAddress.host", "completion": "        if self.ip_bin.startswith(IP4_HEADER):\n            return ip_bin_to_ip4_addr(self.ip_bin[-4:])\n        else:\n            return ip_bin_to_ip6_addr(self.ip_bin)\n", "idx": 1795}
{"namespace": "oletools.msodde.field_is_blacklisted", "completion": "    # check if the field is empty\n    if contents == \"\":\n        return False\n\n    # check if the field is a DDE field\n    if FIELD_DDE_REGEX.match(contents):\n        return False\n\n    # check if the field is a switch\n    if FIELD_SWITCH_REGEX.match(contents):\n        return False\n\n    # check if the field is a blacklisted command\n    if contents.lower() in FIELD_BLACKLIST_CMDS:\n        return True\n\n    # check if the field is a blacklisted command with arguments\n    if contents.lower().startswith(FIELD_BLACKLIST_CMDS):\n        # split the field into words\n        words = FIELD_WORD_REGEX.findall(contents)\n        # check if the number of arguments match\n        if len(words) - 1 != FIELD_BLACKLIST[FIELD_BLACKLIST_CMDS.index(contents.lower())][1]:\n            return True\n        # check if the number of switches match\n        if len(FIELD_BLACKLIST[FIELD_BLACKLIST_CMDS.index(contents.lower())][2]) != \\\n                len([switch for switch in FIELD_BLACKLIST[FIELD_BLACKLIST_CMDS.index(contents.lower())][2] if\n                     switch in words]):\n            return True\n\n    return False\n\n", "idx": 1796}
{"namespace": "oletools.ppt_record_parser.is_ppt", "completion": "    # check if the file is a ppt file\n    if not isinstance(filename, io.IOBase):\n        try:\n            f = open(filename, 'rb')\n        except Exception as e:\n            logging.error(e)\n            return False\n    else:\n        f = filename\n\n    # check if the file is a ppt file\n    try:\n        # check if the file is a ppt file\n        if not isinstance(filename, io.IOBase):\n            try:\n                f = open(filename, 'rb')\n            except Exception as e:\n                logging.error(e)\n                return False\n        else:\n            f = filename\n\n        # check if the file is a ppt file\n        if not isinstance(filename, io.IOBase):\n            try:\n                f = open(filename, 'rb')\n            except Exception as e:\n                logging.error(e)\n                return False\n        else:\n            f = filename\n\n        # check if the file is a ppt file\n        if not isinstance(filename, io.IOBase):\n            try:\n                f = open(filename, 'rb')\n            except Exception as e:\n                logging.error(e)\n                return False\n        else:\n            f = filename\n\n        # check if the file is a ppt file\n        if not isinstance(filename, io.IOBase):\n            try:\n                f = open(filename, 'rb')\n            except Exception as e:\n                logging.error(e)\n                return False\n        else:\n            f = filename\n\n        # check if the file is a ppt file\n        if not isinstance(filename, io.IOBase):\n            try:\n                f = open(filename, 'rb')\n            except Exception as e:\n                logging.error(e)\n                return False\n        else:\n            f = filename\n\n        # check if the file is a ppt file\n        if not isinstance(filename, io.IOBase):\n            try:\n                f = open(filename, 'rb')\n            except Exception as e:\n                logging.error(e)\n                return False\n        else", "idx": 1797}
{"namespace": "oletools.rtfobj.is_rtf", "completion": "    if treat_str_as_data:\n        data = arg\n    else:\n        data = open(arg, 'rb').read()\n\n    if data[:4] == b'{\\\\rt':\n        return True\n    else:\n        return False\n\n", "idx": 1798}
{"namespace": "oletools.oleobj.get_sane_embedded_filenames", "completion": "    # Get the filename from the source path\n    src_filename = os.path.basename(src_path)\n    # Get the filename from the temporary path\n    tmp_filename = os.path.basename(tmp_path)\n\n    # Sanitize the filenames\n    src_filename = sanitize_filename(src_filename)\n    tmp_filename = sanitize_filename(tmp_filename)\n\n    # Create a list of sane filenames\n    sane_filenames = []\n\n    # Add the filename with the suffix\n    sane_filenames.append(src_filename)\n    sane_filenames.append(tmp_filename)\n\n    # Add the filename without the suffix\n    sane_filenames.append(os.path.splitext(src_filename)[0])\n    sane_filenames.append(os.path.splitext(tmp_filename)[0])\n\n    # Add the filename with a random suffix\n    sane_filenames.append(src_filename + str(random.randint(10000, 99999)))\n    sane_filenames.append(tmp_filename + str(random.randint(10000, 99999)))\n\n    # Add the filename with the maximum length\n    sane_filenames.append(shorten_filename(filename, max_len))\n\n    # Add the filename with the maximum length and a random suffix\n    sane_filenames.append(shorten_filename(filename, max_len) + str(random.randint(10000, 99999)))\n\n    # Add the filename with the maximum length and the noname index\n    sane_filenames.append(shorten_filename(filename, max_len) + str(noname_index))\n\n    return sane_filenames\n\n", "idx": 1799}
{"namespace": "oletools.ooxml.get_type", "completion": "    # TODO: add support for docx, xlsx, pptx\n    # TODO: add support for docm, xlsm, pptm\n    # TODO: add support for docb, xlsb, pptb\n    # TODO: add support for doc, xls, ppt\n    # TODO: add support for dot, dotx, dotm, doct, docm, dotx, docb, xlsx, xlsm, xlsb, xltx, xltm, xls, xla, xlam, xll, xlt, xld, xldm, xldb\n    # TODO: add support for ppt, pps, pot, potx, potm, ppsx, ppsm, ppsb, pptx, pptm, pptm, pptb, ppsx, ppsm, ppsb\n    # TODO: add support for xlam, xla, xll, xld, xldm, xldb\n    # TODO: add support for xlw, xlm, xld, xla, xll, xld, xlm, xls, xlt, xld, xlm, xls, xlt, xld, xlm, xls, xlt, xld, xlm, xls, xlt, xld, xlm, xls, xlt, xld, xlm, xls, xlt, xld, xlm, xls, xlt, xld, xlm, xls, xlt, xld, xlm, xls, xlt, xld, xlm, xls, xlt, xld, xlm, xls, xlt, xld, xlm, xls, xlt, xld, xlm, xls, xlt, xld, xlm, xls, xlt, xld, xlm, xls, xlt, xld, xlm, xls, xlt, xld, xlm, xls, xlt, xld, xlm, xls, xlt, xld, xlm, xls, xlt, xld, xlm, xls, xlt, xld, xlm", "idx": 1800}
{"namespace": "oletools.ooxml.ZipSubFile.read", "completion": "        if self.handle is None:\n            raise IOError('read on closed stream')\n        if self.pos >= self.size:\n            return b''\n        if size == -1:\n            size = self.size - self.pos\n        data = self.handle.read(size)\n        self.pos += len(data)\n        return data\n", "idx": 1801}
{"namespace": "oletools.ooxml.ZipSubFile.seek", "completion": "        if self.handle is None:\n            raise IOError('seek on closed handle')\n        if offset == io.SEEK_SET:\n            new_pos = pos\n        elif offset == io.SEEK_CUR:\n            new_pos = self.pos + pos\n        elif offset == io.SEEK_END:\n            new_pos = self.size + pos\n        else:\n            raise ValueError('invalid offset')\n        if new_pos < 0:\n            raise ValueError('invalid position')\n        if new_pos > self.size:\n            new_pos = self.size\n        self.pos = new_pos\n        # print('ZipSubFile: seek to {}'.format(self.pos))\n        self.handle.seek(self.pos)\n", "idx": 1802}
{"namespace": "oletools.ooxml.XmlParser.iter_xml", "completion": "        if self.is_single_xml():\n            if subfiles:\n                raise BadOOXML(self.filename, 'xml has no subfiles')\n            # do not use uopen, xml parser determines encoding on its own\n            with open(self.filename, 'rb') as handle:\n                for elem in ET.iterparse(handle, events=('start', 'end')):\n                    yield None, elem, 0\n        else:\n            for subfile, handle in self.iter_files(subfiles):\n                for elem in ET.iterparse(handle, events=('start', 'end')):\n                    if elem[0] == 'start':\n                        yield subfile, elem[1], 0\n                    else:\n                        if need_children:\n                            yield subfile, elem[1], 0\n                        else:\n                            yield subfile, elem[1].getparent(), elem[1].getparent().index(elem[1]) + 1\n", "idx": 1803}
{"namespace": "oletools.oleid.OleID.check", "completion": "        # TODO: add logging\n        # TODO: add logging level\n        # TODO: add logging to file\n        # TODO: add logging to console\n        # TODO: add logging to file\n        # TODO: add logging to console\n        # TODO: add logging to file\n        # TODO: add logging to console\n        # TODO: add logging to file\n        # TODO: add logging to console\n        # TODO: add logging to file\n        # TODO: add logging to console\n        # TODO: add logging to file\n        # TODO: add logging to console\n        # TODO: add logging to file\n        # TODO: add logging to console\n        # TODO: add logging to file\n        # TODO: add logging to console\n        # TODO: add logging to file\n        # TODO: add logging to console\n        # TODO: add logging to file\n        # TODO: add logging to console\n        # TODO: add logging to file\n        # TODO: add logging to console\n        # TODO: add logging to file\n        # TODO: add logging to console\n        # TODO: add logging to file\n        # TODO: add logging to console\n        # TODO: add logging to file\n        # TODO: add logging to console\n        # TODO: add logging to file\n        # TODO: add logging to console\n        # TODO: add logging to file\n        # TODO: add logging to console\n        # TODO: add logging to file\n        # TODO: add logging to console\n        # TODO: add logging to file\n        # TODO: add logging to console\n        # TODO: add logging to file\n        # TODO: add logging to console\n        # TODO: add logging to file\n        # TODO: add logging to console\n        # TODO: add logging to file\n        # TODO: add logging to console\n        # TODO: add logging to file\n        # TODO: add logging to console\n        # TODO: add logging to file\n        # TODO: add logging to console\n        # TODO: add logging to file\n        # TODO: add logging to console\n        # TODO: add logging to file\n        # TODO: add logging to console\n        # TODO: add logging to file\n        # TODO: add logging to console\n        # TODO: add logging to file\n        # TODO: add logging to", "idx": 1804}
{"namespace": "tools.cgrep.is_valid_ip", "completion": "  try:\n    ip = nacaddr.IP(arg)\n    return ip\n  except (nacaddr.AddrFormatError, ValueError):\n    raise argparse.ArgumentTypeError(f'{arg} is not a valid IP address')\n\n", "idx": 1805}
{"namespace": "tools.cgrep.group_diff", "completion": "  # Get the first group\n  first_group = get_nets([options.gmp[0]], db)\n  # Get the second group\n  second_group = get_nets([options.gmp[1]], db)\n  # Get the common lines\n  common = list(set(first_group[0][1]) & set(second_group[0][1]))\n  # Get the lines in the first group but not in the second group\n  diff1 = list(set(first_group[0][1]) - set(second_group[0][1]))\n  # Get the lines in the second group but not in the first group\n  diff2 = list(set(second_group[0][1]) - set(first_group[0][1]))\n  return common, diff1, diff2\n\n", "idx": 1806}
{"namespace": "tools.cgrep.compare_tokens", "completion": "  # Get the network objects from the database based on the options provided.\n  first_obj = db.GetNet(options.cmp[0])\n  second_obj = db.GetNet(options.cmp[1])\n  # Get the service objects from the database based on the options provided.\n  first_svc = db.GetService(options.cmp[0])\n  second_svc = db.GetService(options.cmp[1])\n  # Get the union of the two network objects.\n  union = first_obj + second_obj\n  # Get the difference between the two network objects.\n  diff = first_obj - second_obj\n  # Get the difference between the two service objects.\n  diff_svc = first_svc - second_svc\n  # Get the intersection of the two network objects.\n  intersection = first_obj & second_obj\n  # Get the symmetric difference of the two network objects.\n  sym_diff = first_obj ^ second_obj\n  # Get the meta information for the comparison.\n  meta = (options.cmp[0], options.cmp[1], union)\n  # Return the meta information and the differences between the two network objects.\n  return meta, (diff, intersection, sym_diff, diff_svc)\n\n", "idx": 1807}
{"namespace": "capirca.aclgen.EntryPoint", "completion": "  SetupFlags()\n  app.run(main)\n\n", "idx": 1808}
{"namespace": "capirca.lib.nacaddr.IP", "completion": "  if isinstance(ip, ipaddress._BaseNetwork):\n    return ip\n  try:\n    ip = ipaddress.ip_network(ip, strict=strict)\n  except ValueError:\n    raise ValueError('Invalid IP address: %s' % ip)\n  if ip.version == 4:\n    return IPv4(ip.network_address.compressed, comment, token)\n  elif ip.version == 6:\n    return IPv6(ip.network_address.compressed, comment, token)\n  else:\n    raise ValueError('Invalid IP address: %s' % ip)\n\n", "idx": 1809}
{"namespace": "csvkit.cli.CSVKitUtility.run", "completion": "        if 'f' not in self.override_flags:\n            self.input_file = self._open_input_file()\n        self._main()\n        if 'f' not in self.override_flags:\n            self.input_file.close()\n", "idx": 1810}
{"namespace": "csvkit.convert.fixed.fixed2csv", "completion": "    # Read the schema file\n    with open(schema, 'r') as schema_file:\n        schema_data = schema_file.read()\n\n    # Parse the schema file\n    schema_reader = agate.csv.reader(StringIO(schema_data))\n\n    # Create a list of column names\n    column_names = []\n\n    # Create a list of column start positions\n    column_starts = []\n\n    # Create a list of column lengths\n    column_lengths = []\n\n    # Create a list of column types\n    column_types = []\n\n    # Create a list of column decimals\n    column_decimals = []\n\n    # Create a list of column null values\n    column_nulls = []\n\n    # Create a list of column date formats\n    column_dates = []\n\n    # Create a list of column boolean values\n    column_booleans = []\n\n    # Create a list of column number values\n    column_numbers = []\n\n    # Create a list of column date values\n    column_dates = []\n\n    # Create a list of column time values\n    column_times = []\n\n    # Create a list of column datetime values\n    column_datetimes = []\n\n    # Create a list of column timedeltas\n    column_timedeltas = []\n\n    # Create a list of column intervals\n    column_intervals = []\n\n    # Create a list of column durations\n    column_durations = []\n\n    # Create a list of column geospatial values\n    column_geospatials = []\n\n    # Create a list of column geohash values\n    column_geohashs = []\n\n    # Create a list of column geojson values\n    column_geojsons = []\n\n    # Create a list of column geobuffers\n    column_geobuffers = []\n\n    # Create a list of column raster values\n    column_rasters = []\n\n    # Create a list of column rasters\n    column_raster_projections = []\n\n    # Create a list of column rasters\n    column_raster_", "idx": 1811}
{"namespace": "check_dummies.find_backend", "completion": "    backends = _re_backend.findall(line)\n    if backends:\n        return \"_and_\".join(backends)\n\n", "idx": 1812}
{"namespace": "check_dummies.create_dummy_object", "completion": "    if name.startswith(\"_\"):\n        return DUMMY_CONSTANT.format(name)\n    elif name.endswith(\"()\"):\n        return DUMMY_FUNCTION.format(name[:-2], backend_name)\n    else:\n        return DUMMY_CLASS.format(name, backend_name)\n\n", "idx": 1813}
{"namespace": "pycorrector.en_spell.EnSpell.check_init", "completion": "        if not self.word_freq_dict:\n            self._init()\n", "idx": 1814}
{"namespace": "pycorrector.en_spell.EnSpell.candidates", "completion": "        self.check_init()\n        return self.known([word]) or self.known(self.edits1(word)) or self.known(self.edits2(word)) or {word}\n", "idx": 1815}
{"namespace": "pycorrector.en_spell.EnSpell.correct_word", "completion": "        self.check_init()\n        candidates = self.candidates(word)\n        candidates_prob = [(p, w) for w, p in ((self.probability(w), w) for w in candidates)]\n        candidates_prob.sort(reverse=True)\n        return candidates_prob[0][1]\n", "idx": 1816}
{"namespace": "pycorrector.en_spell.EnSpell.correct", "completion": "        self.check_init()\n        # Initialize the details list\n        details = []\n        # Split the text into blocks of words\n        blocks = whitespace_tokenize(text)\n        # Iterate over each block of words and their corresponding indices\n        for i, block in enumerate(blocks):\n            # If the block is more than one character long and consists of alphabetical characters\n            if len(block) > 1 and block.isalpha():\n                # Check if the block is a confusion\n                if block in self.custom_confusion:\n                    # Retrieve the corrected item\n                    corrected_item = self.custom_confusion[block]\n                else:\n                    # Parse the block to obtain the corrected item\n                    corrected_item = self.correct_word(block)\n                # If the corrected item is different from the original block\n                if block != corrected_item:\n                    # Calculate the beginning and ending indices of the block\n                    start_idx = text.index(block)\n                    end_idx = start_idx + len(block)\n                    # Create a detail tuple containing the original block, the corrected item, and the beginning and ending indices\n                    detail = [block, corrected_item, start_idx, end_idx]\n                    # Save the detail tuple in the details list\n                    details.append(detail)\n                    # Replace the original block with the corrected item\n                    text = text.replace(block, corrected_item)\n        # Sort the details list based on the beginning indices of the words\n        details = sorted(details, key=lambda detail: detail[2])\n        # Return the corrected text and the details list\n        return text, details\n\n", "idx": 1817}
{"namespace": "whereami.predict.crossval", "completion": "    if X is None or y is None:\n        X, y = get_train_data(path)\n\n    if len(X) < folds:\n        raise ValueError(\"There are not enough samples ({}). Need at least {}.\".format(len(X), folds))\n\n    if clf is None:\n        clf = get_model(path)\n\n    print(\"KFold folds={}, running {} times\".format(folds, n))\n    for i in range(n):\n        scores = cross_val_score(clf, X, y, cv=folds)\n        print(\"{}/{}: {}\".format(i + 1, n, scores.mean()))\n    print(\"-------- total --------\")\n    print(scores.mean())\n    return scores.mean()\n\n", "idx": 1818}
{"namespace": "stellar.models.Table.get_table_name", "completion": "        if self.snapshot is None:\n            raise Exception('Table name requires snapshot')\n        if self.snapshot.hash == '':\n            raise Exception('Snapshot hash is empty.')\n        if old:\n            return 'stellar_{}{}{}'.format(\n                self.table_name,\n                self.snapshot.hash,\n                postfix\n            )\n        else:\n            return hashlib.md5(\n                ('{}{}{}'.format(\n                    self.table_name,\n                    self.snapshot.hash,\n                    postfix\n                )).encode('utf-8')\n            ).hexdigest()[:16]\n", "idx": 1819}
{"namespace": "chatette.utils.Singleton.reset_instance", "completion": "        cls._instance = None\n        return cls.get_or_create(*args, **kwargs)\n\n", "idx": 1820}
{"namespace": "chatette.utils.cast_to_unicode", "completion": "    if sys.version_info[0] < 3:\n        if isinstance(anything, str):\n            return unicode(anything, \"utf-8\")\n        elif isinstance(anything, list):\n            return [unicode(item, \"utf-8\") for item in anything]\n        elif isinstance(anything, dict):\n            return {unicode(key, \"utf-8\"): unicode(value, \"utf-8\") for key, value in anything.items()}\n        else:\n            return anything\n    else:\n        return anything\n\n", "idx": 1821}
{"namespace": "chatette.cli.terminal_writer.TerminalWriter.write", "completion": "        if self._file_mode == 'quiet':\n            return\n        if self.redirection_file_path is None:\n            print(text, end='')\n        else:\n            if self.buffered_text is None:\n                self.buffered_text = text\n            else:\n                self.buffered_text += text\n", "idx": 1822}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.find_redirection_file_path", "completion": "        if len(tokens) == 0:\n            return None\n        if tokens[-1].startswith(REDIRECTION_SYM):\n            return (RedirectionType.overwrite, tokens[-1][1:])\n        elif tokens[-1].startswith(REDIRECTION_APPEND_SYM):\n            return (RedirectionType.append, tokens[-1][2:])\n        elif tokens[-1].startswith(REGEX_SYM):\n            return (RedirectionType.regex, tokens[-1][1:])\n        return None\n", "idx": 1823}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.get_unit_type_from_str", "completion": "        if unit_type_str == \"alias\":\n            return AST.UnitType.alias\n        elif unit_type_str == \"intent\":\n            return AST.UnitType.intent\n        elif unit_type_str == \"slot\":\n            return AST.UnitType.slot\n        elif unit_type_str == \"synonym\":\n            return AST.UnitType.synonym\n        elif unit_type_str == \"all\":\n            return AST.UnitType.all\n        else:\n            return None\n", "idx": 1824}
{"namespace": "chatette.cli.interactive_commands.unhide_command.UnhideCommand.execute", "completion": "        if len(self.command_tokens) < 3:\n            self.print_wrapper.error_log(\n                \"Too few arguments\\nUsage: \" + \\\n                \"unhide <unit-type> <unit-name> [<regexp-group-name>]\"\n            )\n            return\n\n        unit_type = self.command_tokens[1]\n        if unit_type not in \\\n                [\"alias\", \"slot\", \"intent\", \"synonym\", \"all\"]:\n            self.print_wrapper.error_log(\n                \"Unknown unit type: '\" + str(unit_type) + \"'\"\n            )\n            return\n\n        if unit_type == \"all\":\n            self.print_wrapper.write(\"Unhiding all units...\")\n            AST.revert_hidden_units()\n            self.print_wrapper.write(\"Done.\")\n            return\n\n        unit_name = self.command_tokens[2]\n        if not AST.has_unit_decl(unit_type, unit_name):\n            self.print_wrapper.error_log(\n                \"No \" + unit_type.lower() + \" named '\" + unit_name + \"'.\"\n            )\n            return\n\n        if len(self.command_tokens) > 3:\n            regexp_group_name = self.command_tokens[3]\n            if not AST.get_or_create_unit(unit_type, unit_name).\\\n                    add_regexp_group(regexp_group_name):\n                self.print_wrapper.error_log(\n                    \"No group named '\" + regexp_group_name + \"' in \" + \\\n                    unit_type.lower() + \" '\" + unit_name + \"'.\"\n                )\n                return\n\n        self.print_wrapper.write(\n            \"Unhiding \" + unit_type.lower() + \" '\" + unit_name + \"'...\"\n        )\n        AST.get_or_create_unit(unit_type, unit_name).unhide()\n        self.print_wrapper.write(\"Done.\")\n\n\n   ", "idx": 1825}
{"namespace": "chatette.adapters.factory.create_adapter", "completion": "    if adapter_name == \"rasa\":\n        from chatette.adapters.rasa import RasaAdapter\n        return RasaAdapter(base_filepath)\n    elif adapter_name == \"rasa-md\":\n        from chatette.adapters.rasa_md import RasaMdAdapter\n        return RasaMdAdapter(base_filepath)\n    elif adapter_name == \"rasamd\":\n        from chatette.adapters.rasamd import RasaMdAdapter\n        return RasaMdAdapter(base_filepath)\n    elif adapter_name == \"jsonl\":\n        from chatette.adapters.jsonl import JsonListAdapter\n        return JsonListAdapter(base_filepath)\n    else:\n        raise ValueError(\n            \"Unknown adapter name: '\" + adapter_name + \"'. Expected one of \" +\n            \"['rasa','rasa-md','jsonl'].\"\n        )", "idx": 1826}
{"namespace": "chatette.parsing.ChoiceBuilder.create_concrete", "completion": "        self._check_information()\n        return Choice(\n            self.leading_space,\n            self._build_modifiers_repr(),\n            self.rules\n        )\n", "idx": 1827}
{"namespace": "chatette.parsing.UnitRefBuilder._build_modifiers_repr", "completion": "        modifiers = super(UnitRefBuilder, self)._build_modifiers_repr()\n        if self.arg_value is not None:\n            modifiers.arg_value = self.arg_value\n        if self.variation is not None:\n            modifiers.variation_name = self.variation\n        return modifiers\n", "idx": 1828}
{"namespace": "chatette.parsing.UnitRefBuilder.create_concrete", "completion": "        self._check_information()\n        from chatette.units.modifiable.unit_reference import UnitReference\n        return UnitReference(\n            self.type, self.identifier, self.leading_space,\n            self._build_modifiers_repr(), self.variation, self.arg_value\n        )\n", "idx": 1829}
{"namespace": "chatette.parsing.UnitDefBuilder._build_modifiers_repr", "completion": "        modifiers = super(UnitDefBuilder, self)._build_modifiers_repr()\n        modifiers.argument_name = self.arg_name\n        return modifiers\n", "idx": 1830}
{"namespace": "chatette.parsing.AliasDefBuilder.create_concrete", "completion": "        from chatette.units.modifiable.alias import AliasDefinition\n        self._check_information()\n        if self.variation is not None:\n            if self.identifier in AST.get_or_create().aliases:\n                return AST.get_or_create().aliases[self.identifier].get_variation(self.variation)\n            else:\n                raise KeyError(\n                    \"Tried to create an alias definition '\" + str(self.identifier) + \\\n                    \"' with a variation that was not defined.\"\n                )\n        else:\n            return AliasDefinition(\n                self.identifier, self._build_modifiers_repr()\n            )\n", "idx": 1831}
{"namespace": "chatette.parsing.SlotDefBuilder.create_concrete", "completion": "        from chatette.units.modifiable.definitions.slot import SlotDefinition\n        self._check_information()\n        if self.variation is not None:\n            definitions = AST.get_or_create()[UnitType.slot]\n            if self.identifier in definitions:\n                return definitions[self.identifier]\n        return SlotDefinition(self.identifier, self._build_modifiers_repr())\n", "idx": 1832}
{"namespace": "chatette.parsing.IntentDefBuilder.create_concrete", "completion": "        from chatette.units.modifiable.definitions.intent import IntentDefinition\n        self._check_information()\n        if self.variation is not None:\n            definitions = AST.get_or_create()[UnitType.intent]\n            if self.identifier in definitions:\n                return definitions[self.identifier]\n        return IntentDefinition(\n            self.identifier, self._build_modifiers_repr(),\n            self.nb_training_ex, self.nb_testing_ex\n        )\n", "idx": 1833}
{"namespace": "bentoml._internal.resource.get_resource", "completion": "    if resource_kind not in _RESOURCE_REGISTRY:\n        raise BentoMLConfigException(\n            f\"Resource kind '{resource_kind}' is not registered. Registered resource kinds are {list(_RESOURCE_REGISTRY.keys())}.\"\n        )\n\n    resource_class = _RESOURCE_REGISTRY[resource_kind]\n\n    if resource_kind not in resources:\n        return None\n\n    resource_spec = resources[resource_kind]\n\n    if resource_spec == \"system\":\n        return resource_class.from_system()\n\n    return resource_class.from_spec(resource_spec, validate=validate)\n\n", "idx": 1834}
{"namespace": "bentoml._internal.resource.system_resources", "completion": "    resources: dict[str, t.Any] = {}\n    for resource_kind in _RESOURCE_REGISTRY:\n        resources[resource_kind] = _RESOURCE_REGISTRY[resource_kind].from_system()\n    return resources\n\n", "idx": 1835}
{"namespace": "bentoml._internal.resource.CpuResource.from_spec", "completion": "        if isinstance(spec, float):\n            return spec\n        elif isinstance(spec, int):\n            return float(spec)\n        elif isinstance(spec, str):\n            try:\n                return float(spec)\n            except ValueError:\n                pass\n\n            if spec.endswith(\"m\"):\n                try:\n                    return float(spec[:-1]) / 1000\n                except ValueError:\n                    pass\n\n            raise BentoMLConfigException(\n                f\"Invalid CPU specification: '{spec}'. \"\n                f\"Expecting a float, int, or string with a unit postfix 'm'.\"\n            )\n        else:\n            raise BentoMLConfigException(\n                f\"Invalid CPU specification: '{spec}'. \"\n                f\"Expecting a float, int, or string with a unit postfix 'm'.\"\n            )\n", "idx": 1836}
{"namespace": "bentoml._internal.resource.CpuResource.from_system", "completion": "        system = platform.system()\n        if system == \"Darwin\":\n            return float(os.popen(\"sysctl -n hw.ncpu\").readlines()[0])\n        elif system == \"Linux\":\n            return float(os.popen(\"nproc\").readlines()[0])\n        else:\n            raise BentoMLConfigException(\n                f\"Unsupported operating system '{system}' for CPU resource.\"\n            )\n", "idx": 1837}
{"namespace": "bentoml._internal.resource.CpuResource.validate", "completion": "        if val < 0:\n            raise BentoMLConfigException(f\"CPU resource limit '{val}' is invalid. \")\n\n        if psutil.POSIX:\n            system_cpu_count = query_cgroup_cpu_count()\n        else:\n            system_cpu_count = query_os_cpu_count()\n\n        if val > system_cpu_count:\n            raise BentoMLConfigException(\n                f\"CPU resource limit '{val}' is greater than the system's available CPU resources '{system_cpu_count}'. \"\n            )\n\n", "idx": 1838}
{"namespace": "bentoml._internal.types.LazyType.get_class", "completion": "        if self._runtime_class is None:\n            if import_module:\n                module = __import__(self.module, fromlist=[self.qualname])\n                self._runtime_class = getattr(module, self.qualname)\n            else:\n                raise ModuleNotFoundError(\n                    f\"Module {self.module} is not imported. Set import_module=True to import the module.\"\n                )\n        return self._runtime_class\n", "idx": 1839}
{"namespace": "bentoml._internal.models.model.Model.create", "completion": "        if isinstance(name, str):\n            name = Tag.from_taglike(name)\n\n        if not isinstance(name, Tag):\n            raise BentoMLException(\"'name' must be either a string or a Tag\")\n\n        if not isinstance(module, str):\n            raise BentoMLException(\"'module' must be a string\")\n\n        if not isinstance(api_version, str):\n            raise BentoMLException(\"'api_version' must be a string\")\n\n        if not isinstance(signatures, dict):\n            raise BentoMLException(\"'signatures' must be a dict\")\n\n        if labels is not None and not isinstance(labels, dict):\n            raise BentoMLException(\"'labels' must be a dict\")\n\n        if options is not None and not isinstance(options, ModelOptions):\n            raise BentoMLException(\"'options' must be a ModelOptions\")\n\n        if custom_objects is not None and not isinstance(custom_objects, dict):\n            raise BentoMLException(\"'custom_objects' must be a dict\")\n\n        if metadata is not None and not isinstance(metadata, dict):\n            raise BentoMLException(\"'metadata' must be a dict\")\n\n        if not isinstance(context, ModelContext):\n            raise BentoMLException(\"'context' must be a ModelContext\")\n\n        if not isinstance(name, Tag):\n            raise BentoMLException(\"'name' must be a Tag\")\n\n        if not name.version:\n            name.version = \"local_model\"\n\n        if not name.tag_version_str.endswith(cls._export_ext()):\n            name.version += f\".{cls._export_ext()}\"\n\n        if not name.tag_str.endswith(cls._export_ext()):\n            name.tag_str += f\".{cls._export_ext()}\"\n\n        if not name.tag_version_str.endswith(cls._export_ext()):\n            name.tag_version_str += f\".{cls._export_ext()}\"\n\n        if not name.tag_version_str.startswith", "idx": 1840}
{"namespace": "bentoml._internal.models.model.Model.from_fs", "completion": "        with item_fs.open(MODEL_YAML_FILENAME, \"r\", encoding=\"utf-8\") as yaml_file:\n            model_info = ModelInfo.from_yaml(yaml_file)\n\n        return cls(\n            model_info.tag,\n            item_fs,\n            model_info,\n            _internal=True,\n        )\n", "idx": 1841}
{"namespace": "bentoml._internal.utils.metrics.linear_buckets", "completion": "    assert start < end\n    assert step > 0.0\n\n    bound = start\n    buckets: list[float] = []\n    while bound < end:\n        buckets.append(bound)\n        bound += step\n\n    if len(buckets) > MAX_BUCKET_COUNT:\n        buckets = buckets[:MAX_BUCKET_COUNT]\n\n    return tuple(buckets) + (end, INF)", "idx": 1842}
{"namespace": "bentoml._internal.utils.validate_metadata", "completion": "    if not isinstance(metadata, dict):\n        raise ValueError(\"metadata must be a dict!\")\n\n    for key, val in metadata.items():\n        if not isinstance(key, str):\n            raise ValueError(\"metadata keys must be strings\")\n\n        if not isinstance(val, str):\n            raise ValueError(\"metadata values must be strings\")\n\n", "idx": 1843}
{"namespace": "bentoml._internal.utils.analytics.usage_stats.get_serve_info", "completion": "    # Generate a safe token\n    serve_id = secrets.token_urlsafe(16)\n    # Get the current timestamp\n    serve_started_timestamp = datetime.now(timezone.utc)\n    return ServeInfo(serve_id, serve_started_timestamp)\n\n", "idx": 1844}
{"namespace": "bentoml._internal.utils.analytics.usage_stats._track_serve_init", "completion": "    track(\n        ServeInitEvent(\n            serve_id=serve_info.serve_id,\n            production=production,\n            serve_kind=serve_kind,\n            from_server_api=from_server_api,\n            bento_creation_timestamp=svc.bento_creation_timestamp,\n            num_models=len(svc.models),\n            num_runners=len(svc.runners),\n            num_apis=len(svc.apis),\n            model_types=list(svc.model_map.keys()),\n            runner_types=list(svc.runner_map.keys()),\n            api_input_types=[api.input.__name__ for api in svc.apis],\n            api_output_types=[api.output.__name__ for api in svc.apis],\n        )\n    )\n\n", "idx": 1845}
{"namespace": "bentoml._internal.service.service.get_valid_service_name", "completion": "    # Convert the service name to lowercase if it is not already lowercase.\n    if user_provided_svc_name != user_provided_svc_name.lower():\n        logger.warning(\n            f\"The service name '{user_provided_svc_name}' is not lowercase. It will be converted to lowercase.\"\n        )\n        user_provided_svc_name = user_provided_svc_name.lower()\n\n    # Create a dummy tag using the lowercase service name to validate it.\n    try:\n        dummy_tag = Tag.from_str(user_provided_svc_name)\n    except Exception as e:\n        raise BentoMLException(\n            f\"The service name '{user_provided_svc_name}' is not valid. It must be a valid BentoML tag.\"\n        ) from e\n\n    return user_provided_svc_name\n\n", "idx": 1846}
{"namespace": "bentoml._internal.configuration.helpers.flatten_dict", "completion": "    for k, v in d.items():\n        key = f\"{parent}{sep}{k}\" if parent else k\n        if isinstance(v, dict):\n            yield from flatten_dict(v, key, sep)\n        else:\n            yield key, v\n\n", "idx": 1847}
{"namespace": "bentoml._internal.configuration.helpers.load_config_file", "completion": "    if not os.path.isfile(path):\n        raise BentoMLConfigException(f\"Config file {path} not found\")\n\n    with open(path, \"rb\") as f:\n        return yaml.safe_load(f)\n\n", "idx": 1848}
{"namespace": "bentoml._internal.configuration.helpers.expand_env_var_in_values", "completion": "    for k, v in d.items():\n        if isinstance(v, t.MutableMapping):\n            expand_env_var_in_values(v)\n        elif isinstance(v, str):\n            d[k] = expand_env_var(v)\n        elif isinstance(v, t.Sequence):\n            if isinstance(v, str):\n                d[k] = expand_env_var(v)\n            else:\n                for i, _ in enumerate(v):\n                    v[i] = expand_env_var(v[i])", "idx": 1849}
{"namespace": "bentoml._internal.runner.strategy.DefaultStrategy.get_worker_count", "completion": "        if resource_request is None:\n            return 1\n\n        if \"GPU\" in resource_request:\n            gpu_count = system_resources.get_gpu_count()\n            if gpu_count == 0:\n                raise ValueError(\n                    f\"The runnable class {runnable_class.__name__} requires {resource_request['GPU']} GPUs, but no GPUs are available.\"\n                )\n            return math.ceil(gpu_count / workers_per_resource)\n\n        if \"CPU\" in resource_request:\n            cpu_count = system_resources.get_cpu_count()\n            if cpu_count == 0:\n                raise ValueError(\n                    f\"The runnable class {runnable_class.__name__} requires {resource_request['CPU']} CPUs, but no CPUs are available.\"\n                )\n            return math.ceil(cpu_count / workers_per_resource)\n\n        raise ValueError(\n            f\"The runnable class {runnable_class.__name__} requires {resource_request} resources, but no known resources are available.\"\n        )\n", "idx": 1850}
{"namespace": "bentoml._internal.runner.strategy.DefaultStrategy.get_worker_env", "completion": "        env = {}\n        if resource_request is None:\n            resource_request = system_resources()\n\n        # use nvidia gpu\n        nvidia_gpus = get_resource(resource_request, \"nvidia.com/gpu\")\n        if (\n            nvidia_gpus is not None\n            and len(nvidia_gpus) > 0\n            and \"nvidia.com/gpu\" in runnable_class.SUPPORTED_RESOURCES\n        ):\n            env[\"CUDA_VISIBLE_DEVICES\"] = nvidia_gpus[worker_index % len(nvidia_gpus)]\n            env[\"NVIDIA_VISIBLE_DEVICES\"] = nvidia_gpus[\n                worker_index % len(nvidia_gpus)\n            ]\n            env[\"NVIDIA_DRIVER_CAPABILITIES\"] = \"compute,utility\"\n            env[\"NVIDIA_REQUIRE_CUDA\"] = \"cuda>=10.0\"\n            env[\"NVIDIA_REQUIRE_DRIVER\"] = \"driver>=384,driver<385\"\n            env[\"NVIDIA_REQUIRE_DTD_CORE\"] = \"dtd-core>=1.4.0\"\n            env[\"NVIDIA_REQUIRE_RAPIDS\"] = \"rapids>=0.16.0\"\n            env[\"NVIDIA_REQUIRE_CUDNN\"] = \"cudnn>=7.6.0\"\n            env[\"NVIDIA_REQUIRE_CUFFT\"] = \"cufft>=10.0\"\n            env[\"NVIDIA_REQUIRE_CURAND\"] = \"curand>=10.0\"\n            env[\"NVIDIA_REQUIRE_CUB\"] = \"cub>=1.9.9\"\n            env[\"NVIDIA_REQUIRE_CUDACXX\"] = \"cudacxx>=10.0\"\n            env[\"NVIDIA_REQUIRE_CUDNN_DETERMIN", "idx": 1851}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.batches_to_batch", "completion": "        batch = np.concatenate(batches, axis=batch_dim)\n        indices = [batch_dim * len(batches) + i for i in range(len(batches))]\n        return batch, indices\n", "idx": 1852}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.to_payload", "completion": "        if batch.ndim == 0:\n            return cls.create_payload(\n                pickle.dumps(batch, protocol=5),\n                batch_size=1,\n                meta={\"name\": \"0-dimensional\", \"dtype\": str(batch.dtype)},\n            )\n        else:\n            if not batch.flags[\"C_CONTIGUOUS\"] and not batch.flags[\"F_CONTIGUOUS\"]:\n                batch = np.ascontiguousarray(batch)\n            return cls.create_payload(\n                pep574_dumps(batch, protocol=5),\n                batch_size=batch.shape[batch_dim],\n                meta={\"name\": batch.dtype.name, \"dtype\": str(batch.dtype)},\n            )\n", "idx": 1853}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.from_payload", "completion": "        if payload.meta[\"format\"] == \"pickle5\":\n            bs = base64.b64decode(payload.meta[\"pickle_bytes_str\"])\n            indices = payload.meta[\"indices\"]\n            return pep574_loads(bs, indices)\n        else:\n            return pickle.loads(payload.data)\n", "idx": 1854}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.batch_to_payloads", "completion": "        payloads = []\n        for subbatch in cls.batch_to_batches(batch, indices, batch_dim):\n            payloads.append(cls.to_payload(subbatch, batch_dim))\n        return payloads\n", "idx": 1855}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.from_batch_payloads", "completion": "        batches = [cls.from_payload(payload) for payload in payloads]\n        batch, indices = cls.batches_to_batch(batches, batch_dim)\n        return batch, indices\n\n", "idx": 1856}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.to_payload", "completion": "        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n\n        if isinstance(batch, ext.PdSeries):\n            batch = batch.to_frame()\n\n        bs: bytes\n        concat_buffer_bs: bytes\n        indices: list[int]\n        bs, concat_buffer_bs, indices = pep574_dumps(batch)\n        bs_str = base64.b64encode(bs).decode(\"ascii\")\n\n        if indices:\n            meta = {\n                \"format\": \"pickle5\",\n                \"pickle_bytes_str\": bs_str,\n                \"indices\": indices,\n                \"with_buffer\": True,\n            }\n        else:\n            meta = {\"format\": \"pickle5\", \"pickle_bytes_str\": bs_str, \"with_buffer\": False}\n\n        return cls.create_payload(\n            concat_buffer_bs,\n            batch.shape[batch_dim],\n            meta,\n        )\n", "idx": 1857}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.from_payload", "completion": "        format = payload.meta.get(\"format\", \"default\")\n        if format == \"pickle5\":\n            bs_str = t.cast(str, payload.meta[\"pickle_bytes_str\"])\n            bs = base64.b64decode(bs_str)\n            indices = t.cast(t.List[int], payload.meta[\"indices\"])\n            return t.cast(ext.PdDataFrame, pep574_loads(bs, payload.data, indices))\n\n        return pickle.loads(payload.data)\n", "idx": 1858}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batch_to_payloads", "completion": "        batches = cls.batch_to_batches(batch, indices, batch_dim)\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads\n", "idx": 1859}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.from_batch_payloads", "completion": "        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)\n\n", "idx": 1860}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.to_payload", "completion": "        if isinstance(batch, t.Iterator):\n            batch = list(batch)\n\n        batch_size = len(batch)\n\n        if batch_size == 0:\n            raise ValueError(\"Batch size must be greater than 0\")\n\n        if batch_dim != 0:\n            raise ValueError(\"Default Runner DataContainer does not support batch_dim other than 0\")\n\n        return cls.create_payload(pickle.dumps(batch), batch_size)\n", "idx": 1861}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batch_to_payloads", "completion": "        batches = cls.batch_to_batches(batch, indices, batch_dim)\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads\n", "idx": 1862}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.from_batch_payloads", "completion": "        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)\n\n", "idx": 1863}
{"namespace": "sslyze.cli.server_string_parser.CommandLineServerStringParser.parse_server_string", "completion": "        # Extract ip from target\n        ip = None\n        if '{' in server_str:\n            ip, server_str = cls._extract_ip_from_server_string(server_str)\n        if '[' in server_str:\n            ip, server_str = cls._extract_ipv6_from_server_string(server_str)\n        elif ip and '[' in ip:\n            ip = cls._extract_ipv6_from_ip_string(ip)\n        host, port = cls._extract_host_and_port_from_server_string(server_str)\n        return host, ip, port\n", "idx": 1864}
{"namespace": "sslyze.plugins.heartbleed_plugin._HeartbleedCliConnector.result_to_console_output", "completion": "        output = []\n        output.append(cls._get_title_with_target(result))\n        output.append(\"\")\n        output.append(\"  Vulnerable to Heartbleed: %s\" % result.is_vulnerable_to_heartbleed)\n        output.append(\"\")\n        return output\n\n", "idx": 1865}
{"namespace": "sslyze.plugins.http_headers_plugin._HttpHeadersCliConnector.result_to_console_output", "completion": "        output = []\n        if result.http_error_trace:\n            output.append(\"Error:\")\n            for line in result.http_error_trace.format(chain=False):\n                output.append(line)\n        else:\n            output.append(\"HTTP request sent:\")\n            output.append(result.http_request_sent)\n            if result.http_path_redirected_to:\n                output.append(\"HTTP path redirected to:\")\n                output.append(result.http_path_redirected_to)\n            if result.strict_transport_security_header:\n                output.append(\"Strict-Transport-Security header:\")\n                output.append(str(result.strict_transport_security_header))\n            if result.expect_ct_header:\n                output.append(\"Expect-CT header:\")\n                output.append(str(result.expect_ct_header))\n\n        return output\n\n", "idx": 1866}
{"namespace": "sslyze.plugins.http_headers_plugin._detect_http_redirection", "completion": "    # If the response is a redirection, extract the new location\n    if http_response.status == 301 or http_response.status == 302:\n        # Extract the new location\n        new_location = http_response.getheader(\"Location\")\n        if not new_location:\n            raise NotAValidHttpResponseError(\"HTTP response is a redirection but no Location header is present\")\n\n        # Extract the path from the new location\n        parsed_new_location = urlsplit(new_location)\n        if not parsed_new_location.path:\n            raise NotAValidHttpResponseError(\"HTTP response is a redirection but the Location header does not contain a path\")\n\n        # If the new location is not the same server, return None\n        if parsed_new_location.hostname != server_host_name or parsed_new_location.port != server_port:\n            return None\n\n        return parsed_new_location.path\n\n    return None\n\n", "idx": 1867}
{"namespace": "sslyze.plugins.session_renegotiation_plugin._SessionRenegotiationCliConnector.result_to_console_output", "completion": "        result_txt = []\n        if result.is_vulnerable_to_client_renegotiation_dos:\n            result_txt.append('The server is vulnerable to a TLS renegotiation DoS attack.')\n        else:\n            result_txt.append('The server is not vulnerable to a TLS renegotiation DoS attack.')\n        if result.supports_secure_renegotiation:\n            result_txt.append('The server supports secure renegotiation.')\n        else:\n            result_txt.append('The server does not support secure renegotiation.')\n        return result_txt\n\n", "idx": 1868}
{"namespace": "sslyze.plugins.certificate_info._cli_connector._CertificateInfoCliConnector.result_to_console_output", "completion": "        output_lines = []\n\n        # Add the hostname used for SNI\n        output_lines.append(\"Hostname: {}\".format(result.hostname))\n\n        # Add the number of certificates detected\n        output_lines.append(\"Number of certificates detected: {}\".format(len(result.certificate_deployments)))\n\n        # Add the certificate deployment information\n        for deployment in result.certificate_deployments:\n            output_lines.append(\"\")\n            output_lines.append(cls.certificate_deployment_to_console_output(deployment))\n\n        return output_lines\n", "idx": 1869}
{"namespace": "sslyze.plugins.certificate_info._cli_connector._get_name_as_short_text", "completion": "    # Check if there is a common name (CN) in the name field\n    for name in name_field:\n        if name.oid._name == \"commonName\":\n            return name.value\n\n    # If there is no common name (CN), return the entire name field as a string\n    return str(name_field)", "idx": 1870}
{"namespace": "sslyze.plugins.certificate_info._symantec.SymantecDistructTester.get_distrust_timeline", "completion": "        # Check if the chain contains any Symantec root certificate.\n        for cert in verified_certificate_chain:\n            if cert.fingerprint_sha256 in cls._CA_KEYS_BLACKLIST:\n                return SymantecDistrustTimelineEnum.MARCH_2018\n            elif cert.fingerprint_sha256 in cls._CA_KEYS_WHITELIST:\n                return SymantecDistrustTimelineEnum.SEPTEMBER_2018\n\n        return None\n\n", "idx": 1871}
{"namespace": "sslyze.plugins.certificate_info._certificate_utils.parse_subject_alternative_name_extension", "completion": "    try:\n        sans = cast(SubjectAlternativeName, certificate.extensions.get_extension_for_oid(ExtensionOID.SUBJECT_ALTERNATIVE_NAME))\n    except ExtensionNotFound:\n        return SubjectAlternativeNameExtension(dns_names=[], ip_addresses=[])\n\n    dns_names = []\n    ip_addresses = []\n\n    for name in sans.value:\n        if isinstance(name, DNSName):\n            dns_names.append(name.value)\n        elif isinstance(name, IPAddress):\n            ip_addresses.append(name.value)\n        else:\n            raise ValueError(f\"Unknown SubjectAlternativeName type: {name}\")\n\n    return SubjectAlternativeNameExtension(dns_names=dns_names, ip_addresses=ip_addresses)\n\n", "idx": 1872}
{"namespace": "sslyze.plugins.certificate_info._cert_chain_analyzer._certificate_matches_hostname", "completion": "    try:\n        # Extract the names from the certificate\n        cert_names = _get_certificate_names(certificate)\n\n        # Check if the server_hostname matches any of the names in the certificate\n        return any(\n            _match_hostname(server_hostname, cert_name) for cert_name in cert_names\n        )\n    except CertificateError:\n        return False\n\n", "idx": 1873}
