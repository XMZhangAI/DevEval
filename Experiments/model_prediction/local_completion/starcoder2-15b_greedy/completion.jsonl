{"namespace": "benedict.utils.type_util.is_json_serializable", "completion": "    return isinstance(val, (bool, dict, float, int, list, str, tuple))\n\n", "idx": 0}
{"namespace": "feedparser.urls.convert_to_idn", "completion": "    try:\n        url = url.encode('ascii')\n    except UnicodeEncodeError:\n        url = urllib.parse.urlparse(url)\n        url = url._replace(netloc=url.netloc.encode('idna').decode('ascii'))\n        url = url.geturl()\n    return url\n\n", "idx": 1}
{"namespace": "mistune.toc.add_toc_hook", "completion": "    def toc_hook(md, buf, code):\n        toc_items = []\n        for token in buf:\n            if token.type == 'heading':\n                if token.level >= min_level and token.level <= max_level:\n                    toc_items.append({\n                        'level': token.level,\n                        'text': striptags(token.get_text()),\n                        'id': heading_id(token.get_text()) if heading_id else None\n                    })\n        md.state.env['toc'] = toc_items\n\n    md.registerExtension('toc', toc_hook)", "idx": 2}
{"namespace": "mistune.plugins.table.table_in_quote", "completion": "    md.block.rules.insert(md.block.rules.index('paragraph'), 'table')\n    md.block.rules.insert(md.block.rules.index('paragraph'), 'nptable')\n\n", "idx": 3}
{"namespace": "mistune.plugins.table.table_in_list", "completion": "    md.block.insert_rule(md.block.list_rules, 'table', before='paragraph')\n    md.block.insert_rule(md.block.list_rules, 'nptable', before='paragraph')", "idx": 4}
{"namespace": "xmnlp.utils.parallel_handler", "completion": "    if not isinstance(texts, list):\n        raise ValueError(\"You should pass a list of texts\")\n\n    with futures.ThreadPoolExecutor(max_workers=n_jobs) as executor:\n        for result in executor.map(partial(callback, **kwargs), texts):\n            yield result", "idx": 5}
{"namespace": "parsel.utils.shorten", "completion": "    if len(text) <= width:\n        return text\n    if width < len(suffix):\n        return suffix[:width]\n    return text[: width - len(suffix)] + suffix\n\n", "idx": 6}
{"namespace": "parsel.xpathfuncs.set_xpathfunc", "completion": "    etree.set_xpath_func(fname, func)\n\n", "idx": 7}
{"namespace": "dominate.dom_tag._get_thread_context", "completion": "  context = [threading.current_thread()]\n  if greenlet is not None:\n    context.append(greenlet.getcurrent())\n  return hash(tuple(context))\n\n", "idx": 8}
{"namespace": "dominate.util.system", "completion": "  import subprocess\n  import shlex\n  import sys\n  import os\n\n  if sys.version_info[0] == 2:\n    import subprocess32 as subprocess\n\n  if sys.version_info[0] == 3:\n    import subprocess\n\n  if data is None:\n    p = subprocess.Popen(shlex.split(cmd), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    out, err = p.communicate()\n    return out.decode('utf-8')\n  else:\n    p = subprocess.Popen(shlex.split(cmd), stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE)\n    out, err = p.communicate(data)\n    return out.decode('utf-8')\n\n", "idx": 9}
{"namespace": "dominate.util.url_unescape", "completion": "  cc = re.compile(r'%([0-9a-fA-F]{2})')\n\n  result = []\n  m = cc.search(data)\n  while m:\n    result.append(data[0:m.start()])\n    d = m.group(1)\n    if d:\n      d = int(d, 16)\n      result.append(unichr(d))\n\n    data = data[m.end():]\n    m = cc.search(data)\n\n  result.append(data)\n  return ''.join(result)\n\n", "idx": 10}
{"namespace": "rows.fields.DatetimeField.serialize", "completion": "        if value is None:\n            return \"\"\n\n        return six.text_type(value.isoformat())\n", "idx": 11}
{"namespace": "rows.fields.Field.serialize", "completion": "        if value is None:\n            return \"\"\n        if not isinstance(value, cls.TYPE):\n            value_error(value, cls)\n        return six.text_type(value)\n", "idx": 12}
{"namespace": "rows.fields.EmailField.serialize", "completion": "        if value is None:\n            return \"\"\n        else:\n            return six.text_type(value)\n", "idx": 13}
{"namespace": "rows.fields.as_string", "completion": "    if isinstance(value, six.text_type):\n        return value\n    elif isinstance(value, six.binary_type):\n        raise ValueError(\"Can't convert binary to string\")\n    else:\n        return six.text_type(value)\n\n", "idx": 14}
{"namespace": "rows.fields.get_items", "completion": "    def getter(obj):\n        return tuple(obj[i] if i in range(len(obj)) else None for i in indexes)\n\n    return getter\n\n", "idx": 15}
{"namespace": "pycorrector.proper_corrector.load_dict_file", "completion": "    words = {}\n    if path and os.path.exists(path):\n        with open(path, 'r', encoding='utf-8') as f:\n            for w in f:\n                w = w.strip()\n                if w.startswith('#'):\n                    continue\n                if w:\n                    words[w] = 1\n    return words\n\n", "idx": 16}
{"namespace": "natasha.span.envelop_spans", "completion": "    for envelope in envelopes:\n        for span in spans:\n            if span.start >= envelope.start and span.stop <= envelope.stop:\n                yield span\n\n", "idx": 17}
{"namespace": "googleapiclient._helpers.parse_unique_urlencoded", "completion": "    parsed_content = urllib.parse.parse_qs(content)\n    for key, value in parsed_content.items():\n        if len(value) > 1:\n            raise ValueError(\"Repeated key: {}\".format(key))\n    return parsed_content\n\n", "idx": 18}
{"namespace": "jinja2.async_utils.auto_aiter", "completion": "    if hasattr(iterable, \"__aiter__\"):\n        async for item in iterable:\n            yield item\n    else:\n        for item in iterable:\n            yield item", "idx": 19}
{"namespace": "jinja2.utils.consume", "completion": "    for _ in iterable:\n        pass\n\n", "idx": 20}
{"namespace": "pycorrector.utils.tokenizer.segment", "completion": "    if cut_type == 'word':\n        if pos:\n            return posseg.lcut(sentence)\n        else:\n            return jieba.lcut(sentence)\n    elif cut_type == 'char':\n        if pos:\n            return posseg.lcut(sentence, HMM=False)\n        else:\n            return list(sentence)\n    else:\n        raise ValueError('cut_type must be either \"word\" or \"char\"')\n\n", "idx": 21}
{"namespace": "jinja2.utils.object_type_repr", "completion": "    if obj is None:\n        return \"None\"\n    elif obj is Ellipsis:\n        return \"Ellipsis\"\n    elif obj is True:\n        return \"True\"\n    elif obj is False:\n        return \"False\"\n    elif obj is missing:\n        return \"missing\"\n    elif obj is internal_code:\n        return \"internal_code\"\n    elif obj is pass_context:\n        return \"pass_context\"\n    elif obj is pass_eval_context:\n        return \"pass_eval_context\"\n    elif obj is pass_environment:\n        return \"pass_environment\"\n    elif obj is import_string:\n        return \"import_string\"\n    elif obj is open_if_exists:\n        return \"open_if_exists\"\n    elif obj is object_type_repr:\n        return \"object_type_repr\"\n    elif obj is is_undefined:\n        return \"is_undefined\"\n    elif obj is consume:\n        return \"consume\"\n    elif obj is clear_caches:\n        return \"clear_caches\"\n    elif obj is internalcode:\n        return \"internalcode\"\n    elif obj is _PassArg:\n        return \"_PassArg\"\n    elif obj is _PassArg.from_obj:\n        return \"_PassArg.from_obj\"\n    elif obj is _PassArg.context:\n        return \"_PassArg.context\"\n    elif obj is _PassArg.eval_context:\n        return \"_PassArg.eval_context\"\n    elif obj is _PassArg.environment:\n        return \"_PassArg.environment\"\n    elif obj is _PassArg.from_obj:\n        return \"_PassArg.from_obj\"\n    elif obj is _PassArg.context:\n        return \"_PassArg.context\"\n    elif obj is _PassArg.eval_context:\n        return \"_PassArg.eval_context\"\n    elif obj is _PassArg.environment:\n        return \"_PassArg.environment\"\n    elif obj is _PassArg.from_obj:\n        return \"_PassArg.from_obj\"\n    elif obj is _PassArg.context:\n        return \"_PassArg.context\"\n    elif obj is _PassArg.eval_context:\n        return \"_PassArg.eval_context\"\n    elif obj is _PassArg.environment:\n        return \"_PassArg.environment\"\n    elif obj is _PassArg.from_obj:\n        return \"_Pass", "idx": 22}
{"namespace": "jinja2.utils.LRUCache.setdefault", "completion": "        try:\n            return self[key]\n        except KeyError:\n            self[key] = default\n            return default\n", "idx": 23}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_word_freq", "completion": "        word_freq = {}\n        for word in list_of_words:\n            if word in word_freq:\n                word_freq[word] += 1\n            else:\n                word_freq[word] = 1\n        return word_freq\n", "idx": 24}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_average_probability_of_words", "completion": "        if len(content_words_in_sentence) == 0:\n            return 0\n        else:\n            return sum([word_freq_in_doc.get(w, 0) for w in content_words_in_sentence]) / len(content_words_in_sentence)\n", "idx": 25}
{"namespace": "sumy.summarizers.lex_rank.LexRankSummarizer._compute_idf", "completion": "        idf_metrics = {}\n        sentences_count = len(sentences)\n\n        for sentence in sentences:\n            for term in sentence:\n                if term not in idf_metrics:\n                    n_j = sum(term in sentence_words for sentence_words in sentences)\n                    idf_metrics[term] = math.log(sentences_count / (1 + n_j))\n\n        return idf_metrics\n", "idx": 26}
{"namespace": "sumy.summarizers.lex_rank.LexRankSummarizer.cosine_similarity", "completion": "        common_words = set(sentence1).intersection(set(sentence2))\n        dot_product = 0.0\n        magnitude_1 = 0.0\n        magnitude_2 = 0.0\n\n        for term in common_words:\n            dot_product += tf1[term] * tf2[term] * idf_metrics[term] * idf_metrics[term]\n            magnitude_1 += tf1[term] * tf1[term] * idf_metrics[term] * idf_metrics[term]\n            magnitude_2 += tf2[term] * tf2[term] * idf_metrics[term] * idf_metrics[term]\n\n        magnitude = math.sqrt(magnitude_1) * math.sqrt(magnitude_2)\n\n        if not magnitude:\n            return 0.0\n        else:\n            return dot_product / magnitude\n", "idx": 27}
{"namespace": "sumy.evaluation.rouge._get_ngrams", "completion": "    ngrams = set()\n    text = text.split()\n    text_length = len(text)\n    max_index_ngram_start = text_length - n\n    for i in range(max_index_ngram_start + 1):\n        ngram = ' '.join(text[i:i + n])\n        ngrams.add(ngram)\n    return ngrams\n\n", "idx": 28}
{"namespace": "sumy.evaluation.rouge._split_into_words", "completion": "    if not isinstance(sentences, list):\n        raise ValueError(\"Input sentences must be of type list\")\n\n    words = []\n    for sentence in sentences:\n        if not isinstance(sentence, Sentence):\n            raise ValueError(\"Object in collection must be of type Sentence\")\n        words.extend(sentence.words)\n\n    return words\n\n", "idx": 29}
{"namespace": "falcon.inspect.register_router", "completion": "    if router_class in _supported_routers:\n        raise ValueError(\n            'Router class {} is already registered'.format(router_class)\n        )\n\n    def decorator(func):\n        _supported_routers[router_class] = func\n        return func\n\n    return decorator\n\n", "idx": 30}
{"namespace": "falcon.inspect.inspect_compiled_router", "completion": "    routes = []\n    for route in router.routes:\n        route_info = RouteInfo(\n            route.method,\n            route.resource.uri_template,\n            _get_source_info(route.resource.func),\n            route.resource.options,\n        )\n        routes.append(route_info)\n    return routes\n\n", "idx": 31}
{"namespace": "falcon.inspect._is_internal", "completion": "    try:\n        # NOTE(CaselIT): if the object is a class, check the module of the class\n        # if the object is an instance, check the module of the instance\n        module = inspect.getmodule(obj)\n        if module is None:\n            return False\n        return module.__name__.startswith('falcon')\n    except Exception:\n        return False\n\n", "idx": 32}
{"namespace": "falcon.cmd.inspect_app.load_app", "completion": "    try:\n        module_name, app_name = args.app_module.split(':')\n    except ValueError:\n        parser.error('Invalid app module: %s' % args.app_module)\n\n    try:\n        module = importlib.import_module(module_name)\n    except ImportError:\n        parser.error('Unable to import module: %s' % module_name)\n\n    try:\n        app = getattr(module, app_name)\n    except AttributeError:\n        parser.error('Unable to find app: %s' % app_name)\n\n    if not isinstance(app, falcon.App):\n        try:\n            app = app()\n        except TypeError:\n            parser.error('Unable to instantiate app: %s' % app_name)\n\n        if not isinstance(app, falcon.App):\n            parser.error('App is not an instance of falcon.App: %s' % app_name)\n\n    return app\n\n", "idx": 33}
{"namespace": "falcon.cmd.inspect_app.make_parser", "completion": "    parser = argparse.ArgumentParser(description='Print out the routes of an App instance.')\n    parser.add_argument('-r', '--router', action='store_true', help='Print out the routes of the router.')\n    parser.add_argument('-v', '--verbose', action='store_true', help='Print out the routes of the router and the routes of the app.')\n    parser.add_argument('-i', '--internal', action='store_true', help='Print out the internal routes of the app.')\n    parser.add_argument('app_module', help='The module name of the app.')\n\n    return parser\n\n", "idx": 34}
{"namespace": "falcon.util.uri.unquote_string", "completion": "    if not isinstance(quoted, str):\n        raise TypeError('The input is not a string.')\n\n    return decode(quoted)\n\n", "idx": 35}
{"namespace": "falcon.util.misc.get_argnames", "completion": "    if not callable(func):\n        raise TypeError('func must be callable')\n\n    if inspect.ismethod(func):\n        func = func.__func__\n\n    if inspect.isfunction(func):\n        return inspect.getargspec(func).args\n    elif inspect.isclass(func):\n        return inspect.getargspec(func.__init__).args\n    else:\n        raise TypeError('func must be a function or class')\n\n", "idx": 36}
{"namespace": "falcon.testing.client._is_asgi_app", "completion": "    return inspect.signature(app).parameters == (inspect.Parameter('self'),)\n\n", "idx": 37}
{"namespace": "falcon.routing.converters.UUIDConverter.convert", "completion": "        try:\n            return uuid.UUID(value)\n        except ValueError:\n            return None", "idx": 38}
{"namespace": "rest_framework_simplejwt.utils.make_utc", "completion": "    if is_naive(dt):\n        return make_aware(dt, timezone.utc)\n    return dt\n\n", "idx": 39}
{"namespace": "boto.sdb.db.sequence.fib", "completion": "    return cv + lv\n", "idx": 40}
{"namespace": "boto.s3.website.RoutingRules.add_rule", "completion": "        self.append(rule)\n        return self\n", "idx": 41}
{"namespace": "boto.cloudfront.distribution.Distribution._canned_policy", "completion": "        policy = '{\"Statement\":[{\"Resource\":\"%(resource)s\",\"Condition\":{\"DateLessThan\":{\"AWS:EpochTime\":%(expires)s}}}]}' % {'resource': resource, 'expires': expires}\n        return policy\n", "idx": 42}
{"namespace": "boto.cloudfront.invalidation.InvalidationBatch.escape", "completion": "        if p[0] != '/':\n            p = '/' + p\n        return urllib.quote(p, safe='/*')\n", "idx": 43}
{"namespace": "proxybroker.utils.get_status_code", "completion": "    try:\n        return int(resp[start:stop])\n    except ValueError:\n        return 400\n\n", "idx": 44}
{"namespace": "authlib.oauth2.rfc6749.util.scope_to_list", "completion": "    if isinstance(scope, (set, tuple, list)):\n        return [to_unicode(s) for s in scope]\n    if scope is None:\n        return scope\n    return [to.unicode(s) for s in scope.split()]\n\n", "idx": 45}
{"namespace": "authlib.common.encoding.to_unicode", "completion": "    if x is None:\n        return None\n    if isinstance(x, str):\n        return x\n    if isinstance(x, bytes):\n        return x.decode(charset, errors)\n    return str(x)\n\n", "idx": 46}
{"namespace": "authlib.common.encoding.to_bytes", "completion": "    if x is None:\n        return None\n    elif isinstance(x, bytes):\n        return x\n    elif isinstance(x, str):\n        return x.encode(charset, errors)\n    elif isinstance(x, int):\n        return struct.pack('>I', x)\n    elif isinstance(x, float):\n        return struct.pack('>d', x)\n    else:\n        raise TypeError('Unsupported type: {}'.format(type(x)))\n\n", "idx": 47}
{"namespace": "authlib.common.encoding.urlsafe_b64decode", "completion": "    s = to_bytes(s)\n    s = s.rstrip(b'=')\n    return base64.urlsafe_b64decode(s)\n\n", "idx": 48}
{"namespace": "csvs_to_sqlite.utils.table_exists", "completion": "    sql = \"\"\"\n        SELECT name\n        FROM sqlite_master\n        WHERE type='table'\n        AND name=?\n    \"\"\"\n    result = conn.execute(sql, (table,)).fetchall()\n    return len(result) > 0\n\n", "idx": 49}
{"namespace": "sqlitedict.SqliteDict.get_tablenames", "completion": "        if not os.path.isfile(filename):\n            raise IOError('file {} does not exist'.format(filename))\n\n        conn = sqlite3.connect(filename)\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n        tables = cursor.fetchall()\n        conn.close()\n        return [table[0] for table in tables]\n", "idx": 50}
{"namespace": "litecli.packages.parseutils.query_starts_with", "completion": "    prefixes = [prefix.lower() for prefix in prefixes]\n    query = query.lower()\n    query = sqlparse.format(query, strip_comments=True)\n    if query and query.split()[0] in prefixes:\n        return True\n    return False\n\n", "idx": 51}
{"namespace": "rest_framework.negotiation.DefaultContentNegotiation.filter_renderers", "completion": "        filtered_renderers = []\n        for renderer in renderers:\n            if renderer.format == format:\n                filtered_renderers.append(renderer)\n\n        if not filtered_renderers:\n            raise Http404(\"No renderer found for format '{}'\".format(format))\n\n        return filtered_renderers\n", "idx": 52}
{"namespace": "rest_framework.templatetags.rest_framework.as_string", "completion": "    if value is None:\n        return ''\n    return str(value)\n\n", "idx": 53}
{"namespace": "rest_framework.templatetags.rest_framework.add_nested_class", "completion": "    if isinstance(value, dict):\n        return 'class=nested'\n    elif isinstance(value, list):\n        for item in value:\n            if isinstance(item, dict) or isinstance(item, list):\n                return 'class=nested'\n    return ''\n\n", "idx": 54}
{"namespace": "pyramid.session.PickleSerializer.loads", "completion": "        try:\n            return pickle.loads(bstruct)\n        except Exception as e:\n            raise ValueError(e)\n", "idx": 55}
{"namespace": "pyramid.testing.DummySession.flash", "completion": "        if not allow_duplicate:\n            for message in self.get(queue, []):\n                if message == msg:\n                    return\n        self.setdefault(queue, []).append(msg)\n", "idx": 56}
{"namespace": "pyramid.testing.DummySession.pop_flash", "completion": "        return self.pop('_f_' + queue, [])\n\n", "idx": 57}
{"namespace": "pyramid.testing.DummySession.peek_flash", "completion": "        storage = self.get('_f_' + queue, [])\n        return storage\n\n", "idx": 58}
{"namespace": "pyramid.testing.DummySession.new_csrf_token", "completion": "        token = '0123456789012345678901234567890123456789'\n        self['csrf_token'] = token\n        return token\n", "idx": 59}
{"namespace": "pyramid.view.view_defaults", "completion": "    def decorator(wrapped):\n        if not hasattr(wrapped, '__view_defaults__'):\n            wrapped.__view_defaults__ = {}\n        wrapped.__view_defaults__.update(settings)\n        return wrapped\n\n    return decorator\n\n", "idx": 60}
{"namespace": "pyramid.util.bytes_", "completion": "    if isinstance(s, str):\n        return s.encode(encoding, errors)\n    return s\n\n", "idx": 61}
{"namespace": "pyramid.scripts.common.parse_vars", "completion": "    vars = {}\n    for arg in args:\n        key, value = arg.split('=')\n        vars[key] = value\n    return vars\n\n", "idx": 62}
{"namespace": "pyramid.scripts.pviews.PViewsCommand._find_multi_routes", "completion": "        infos = []\n        for route in mapper.get_routes():\n            match = route.match(request.path)\n            if match is not None:\n                infos.append({'match': match, 'route': route})\n        return infos\n", "idx": 63}
{"namespace": "pyramid.scripts.pserve.PServeCommand.guess_server_url", "completion": "        if not server_name:\n            server_name = 'main'\n        settings = loader.get_settings('server:' + server_name, global_conf)\n        port = settings.get('port')\n        if port:\n            return 'http://127.0.0.1:%s' % port\n", "idx": 64}
{"namespace": "aiohappybase._util.pep8_to_camel_case", "completion": "    chunks = name.split('_')\n    if initial:\n        return ''.join(chunks)\n    else:\n        return chunks[0] + ''.join(x.title() for x in chunks[1:])\n\n", "idx": 65}
{"namespace": "aiohappybase._util.bytes_increment", "completion": "    if not b:\n        return None\n    if len(b) == 1:\n        if b[0] == 0xFF:\n            return None\n        else:\n            return bytes([b[0] + 1])\n    if b[-1] == 0xFF:\n        return bytes_increment(b[:-1]) + b\"\\x00\"\n    else:\n        return b[:-1] + bytes([b[-1] + 1])\n\n", "idx": 66}
{"namespace": "mssqlcli.config.ensure_dir_exists", "completion": "    if not os.path.exists(os.path.dirname(path)):\n        os.makedirs(os.path.dirname(path))\n\n", "idx": 67}
{"namespace": "mssqlcli.telemetry._user_id_file_is_old", "completion": "    if not os.path.exists(id_file_path):\n        return True\n    modified_time = os.path.getmtime(id_file_path)\n    current_time = time.time()\n    time_difference = current_time - modified_time\n    return time_difference > 86400\n\n", "idx": 68}
{"namespace": "mssqlcli.util.is_command_valid", "completion": "    if command == '':\n        return False\n    try:\n        subprocess.check_output(command, stderr=subprocess.STDOUT, shell=True)\n    except OSError:\n        return False\n    return True\n", "idx": 69}
{"namespace": "mssqlcli.packages.parseutils.utils.find_prev_keyword", "completion": "    # Parse the SQL statement\n    parsed = sqlparse.parse(sql)[0]\n\n    # Find the last keyword in the SQL statement\n    last_keyword = None\n    for token in reversed(parsed.tokens):\n        if token.ttype is Token.Keyword:\n            last_keyword = token.value\n            break\n\n    # If no keyword is found, return None\n    if last_keyword is None:\n        return None\n\n    # Find the index of the last keyword in the SQL statement\n    last_keyword_index = parsed.token_index(last_keyword)\n\n    # If the index is not found, return None\n    if last_keyword_index is None:\n        return None\n\n    # Find the index of the token after the last keyword\n    next_token_index = last_keyword_index + 1 + n_skip\n\n    # If the index is out of range, return None\n    if next_token_index >= len(parsed.tokens):\n        return None\n\n    # Find the token after the last keyword\n    next_token = parsed.tokens[next_token_index]\n\n    # If the token is not a keyword, return None\n    if next_token.ttype is not Token.Keyword:\n        return None\n\n    # Find the index of the token after the next token\n    next_next_token_index = next_token_index + 1\n\n    # If the index is out of range, return None\n    if next_next_token_index >= len(parsed.tokens):\n        return None\n\n    # Find the token after the next token\n    next_next_token = parsed.tokens[next_next_token_index]\n\n    # If the token is not a keyword, return None\n    if next_next_token.ttype is not Token.Keyword:\n        return None\n\n    # Find the index of the token after the next next token\n    next_next_next_token_index = next_next_token_index + 1\n\n    # If the index is out of range, return None\n    if next_next_next_token_index >= len(parsed.tokens):\n        return None\n\n    # Find the token after the next next token\n    next_next_next_token = parsed.tokens[next_next_next_token_index]\n\n    # If the token is not a keyword, return None\n    if next_next_next_token.ttype is not Token.", "idx": 70}
{"namespace": "trafilatura.settings.use_config", "completion": "    if config is None:\n        config = ConfigParser()\n        if filename is None:\n            filename = Path(__file__).parent / \"settings.cfg\"\n        config.read(filename)\n    return config\n\n", "idx": 71}
{"namespace": "pyramid.util.text_", "completion": "    if isinstance(s, bytes):\n        return s.decode(encoding, errors)\n    return s\n\n", "idx": 72}
{"namespace": "trafilatura.downloads._parse_config", "completion": "    user_agents = config.get('HTTP', 'user_agents').split(',')\n    cookies = config.get('HTTP', 'cookies')\n    return user_agents, cookies\n\n", "idx": 73}
{"namespace": "trafilatura.downloads.load_download_buffer", "completion": "    # Determine the threading strategy\n    if url_store.compressed:\n        # If the url_store is compressed, we use the threading strategy\n        # that is based on the domain-based back-off rules.\n        bufferlist = url_store.get_urls_with_backoff()\n    else:\n        # If the url_store is not compressed, we use the threading strategy\n        # that is based on the domain-based back-off rules.\n        bufferlist = url_store.get_urls_with_backoff()\n\n    # If the bufferlist is empty, we sleep for the specified time.\n    if not bufferlist:\n        sleep(sleep_time)\n\n    return bufferlist, url_store\n\n", "idx": 74}
{"namespace": "trafilatura.metadata.check_authors", "completion": "    if not authors:\n        return None\n\n    new_authors = []\n    for author in authors.split('; '):\n        if author.lower() not in author_blacklist:\n            new_authors.append(author)\n\n    if new_authors:\n        return '; '.join(new_authors)\n    else:\n        return None\n\n", "idx": 75}
{"namespace": "datasette.filters.where_filters", "completion": "    if \"_where\" not in request.args:\n        return None\n    if not datasette.permission_allowed(request.actor, \"execute-sql\"):\n        raise DatasetteError(403, \"Permission denied\")\n    where_clauses = []\n    extra_wheres_for_ui = []\n    for where in request.args.getlist(\"_where\"):\n        if not where:\n            continue\n        if \"=\" not in where:\n            raise BadRequest(\"Invalid _where parameter\")\n        key, value = where.split(\"=\", 1)\n        if value == \"\":\n            continue\n        if key == \"id\":\n            key = \"rowid\"\n        if key == \"pk\":\n            key = \"rowid\"\n        if key == \"rowid\":\n            key = \"rowid\"\n        if key == \"rowid\":\n            key = \"rowid\"\n        if key == \"rowid\":\n            key = \"rowid\"\n        if key == \"rowid\":\n            key = \"rowid\"\n        if key == \"rowid\":\n            key = \"rowid\"\n        if key == \"rowid\":\n            key = \"rowid\"\n        if key == \"rowid\":\n            key = \"rowid\"\n        if key == \"rowid\":\n            key = \"rowid\"\n        if key == \"rowid\":\n            key = \"rowid\"\n        if key == \"rowid\":\n            key = \"rowid\"\n        if key == \"rowid\":\n            key = \"rowid\"\n        if key == \"rowid\":\n            key = \"rowid\"\n        if key == \"rowid\":\n            key = \"rowid\"\n        if key == \"rowid\":\n            key = \"rowid\"\n        if key == \"rowid\":\n            key = \"rowid\"\n        if key == \"rowid\":\n            key = \"rowid\"\n        if key == \"rowid\":\n            key = \"rowid\"\n        if key == \"rowid\":\n            key = \"rowid\"\n        if key == \"rowid\":\n            key = \"rowid\"\n        if key == \"rowid\":\n            key = \"rowid\"\n        if key == \"rowid\":\n            key = \"rowid\"\n        if key == \"rowid\":\n            key = \"rowid\"\n        if key == \"rowid\":\n            key = \"rowid\"\n        if key == \"rowid", "idx": 76}
{"namespace": "datasette.utils.path_with_added_args", "completion": "    if path is None:\n        path = request.path\n    if args:\n        path = append_querystring(path, urllib.parse.urlencode(args))\n    return path\n\n", "idx": 77}
{"namespace": "datasette.utils.path_with_replaced_args", "completion": "    if path is None:\n        path = request.path\n    else:\n        if \"?\" in path:\n            bits = path.split(\"?\", 1)\n            path, query_string = bits\n    # args can be a dict or a set\n    current = []\n    if isinstance(args, dict):\n        args = args.items()\n    for key, value in urllib.parse.parse_qsl(request.query_string):\n        if key not in args:\n            current.append((key, value))\n    current.extend([(key, value) for key, value in args if value is not None])\n    query_string = urllib.parse.urlencode(current)\n    if query_string:\n        query_string = f\"?{query_string}\"\n    return path + query_string\n\n", "idx": 78}
{"namespace": "datasette.utils.format_bytes", "completion": "    if bytes < 1024:\n        return f\"{bytes} bytes\"\n    elif bytes < 1024 * 1024:\n        return f\"{bytes / 1024} KB\"\n    elif bytes < 1024 * 1024 * 1024:\n        return f\"{bytes / 1024 / 1024} MB\"\n    elif bytes < 1024 * 1024 * 1024 * 1024:\n        return f\"{bytes / 1024 / 1024 / 1024} GB\"\n    else:\n        return f\"{bytes / 1024 / 1024 / 1024 / 1024} TB\"\n\n", "idx": 79}
{"namespace": "datasette.utils.actor_matches_allow", "completion": "    if isinstance(actor, str):\n        actor = actor.lower()\n    if isinstance(allow, str):\n        allow = allow.lower()\n    if isinstance(actor, (list, tuple)):\n        actor = tuple(actor)\n    if isinstance(allow, (list, tuple)):\n        allow = tuple(allow)\n    if isinstance(actor, (list, tuple)):\n        if isinstance(allow, (list, tuple)):\n            return any(a in allow for a in actor)\n        else:\n            return allow in actor\n    else:\n        return actor == allow\n\n", "idx": 80}
{"namespace": "datasette.utils.resolve_env_secrets", "completion": "    if isinstance(config, dict):\n        new_config = {}\n        for key, value in config.items():\n            if isinstance(value, dict):\n                new_config[key] = resolve_env_secrets(value, environ)\n            elif isinstance(value, list):\n                new_config[key] = resolve_env_secrets(value, environ)\n            elif isinstance(value, str):\n                if value.startswith(\"$env:\"):\n                    name = value.split(\":\", 1)[1]\n                    new_config[key] = environ.get(name)\n                elif value.startswith(\"$file:\"):\n                    name = value.split(\":\", 1)[1]\n                    with open(name, \"r\") as file:\n                        new_config[key] = file.read()\n                else:\n                    new_config[key] = value\n            else:\n                new_config[key] = value\n        return new_config\n    elif isinstance(config, list):\n        return [resolve_env_secrets(item, environ) for item in config]\n    else:\n        return config\n\n", "idx": 81}
{"namespace": "datasette.utils.display_actor", "completion": "    if actor is None:\n        return \"None\"\n    if \"display_name\" in actor:\n        return actor[\"display_name\"]\n    if \"name\" in actor:\n        return actor[\"name\"]\n    if \"username\" in actor:\n        return actor[\"username\"]\n    if \"login\" in actor:\n        return actor[\"login\"]\n    if \"id\" in actor:\n        return actor[\"id\"]\n    return str(actor)\n\n", "idx": 82}
{"namespace": "datasette.utils.initial_path_for_datasette", "completion": "    # If there is only one database, return the path to that database\n    if len(datasette.databases) == 1:\n        return datasette.databases[0].path\n\n    # If there is only one table, return the path to that table\n    if len(datasette.tables) == 1:\n        return datasette.tables[0].path\n\n    # Return the path to the instance\n    return datasette.path\n\n", "idx": 83}
{"namespace": "datasette.utils.tilde_decode", "completion": "    s = s.replace(\"%\", \"~%\")\n    return urllib.parse.unquote(s)\n\n", "idx": 84}
{"namespace": "datasette.utils.resolve_routes", "completion": "    for regex, view in routes:\n        if re.match(regex, path):\n            return regex, view\n    return None\n\n", "idx": 85}
{"namespace": "datasette.utils.truncate_url", "completion": "    if length is None:\n        return url\n    if \".\" in url:\n        extension = url.split(\".\")[-1]\n        if len(extension) > 1:\n            url = url[: -(1 + len(extension))]\n    if len(url) > length:\n        url = url[:length] + \"...\"\n    return url\n\n", "idx": 86}
{"namespace": "kinto.core.authorization.groupfinder", "completion": "    if not request.registry.settings.get(\"permission.backend\"):\n        return []\n\n    if not hasattr(request, \"principals\"):\n        request.principals = request.permission.get_principals(userid)\n\n    return request.principals\n\n", "idx": 87}
{"namespace": "kinto.core.utils.json.dumps", "completion": "        kw.setdefault(\"bytes_mode\", rapidjson.BM_NONE)\n        return rapidjson.dumps(v, **kw)\n", "idx": 88}
{"namespace": "kinto.core.utils.json.loads", "completion": "        kw.setdefault(\"number_mode\", rapidjson.NM_NATIVE)\n        return rapidjson.loads(v, **kw)\n\n", "idx": 89}
{"namespace": "kinto.core.utils.hmac_digest", "completion": "    if isinstance(secret, str):\n        secret = secret.encode(encoding)\n    if isinstance(message, str):\n        message = message.encode(encoding)\n    return hmac.new(secret, message, digestmod=\"sha256\").hexdigest()\n\n", "idx": 90}
{"namespace": "kinto.core.utils.current_service", "completion": "    # Import service here because kinto.core import utils\n    from kinto.core import Service\n\n    # Get the routes mapper from the request registry\n    mapper = request.registry.queryUtility(IRoutesMapper)\n    if mapper is None:\n        return None\n\n    # Get the route match from the request\n    route_match = mapper.match(request)\n    if route_match is None:\n        return None\n\n    # Get the service name from the route match\n    service_name = route_match.get(\"service_name\")\n    if service_name is None:\n        return None\n\n    # Get the service from the request registry\n    service = request.registry.queryUtility(Service, name=service_name)\n    if service is None:\n        return None\n\n    return service\n\n", "idx": 91}
{"namespace": "kinto.core.utils.prefixed_principals", "completion": "    principals = request.effective_principals\n    if \"Authenticated\" not in principals:\n        return principals\n\n    principals.remove(\"Authenticated\")\n    principals.insert(0, prefixed_userid(request))\n    return principals\n\n", "idx": 92}
{"namespace": "kinto.plugins.accounts.views.on_account_created", "completion": "    request = event.request\n    settings = request.registry.settings\n\n    # Check if account validation is enabled\n    if not settings.get(\"account_validation\", False):\n        return\n\n    for obj in event.impacted_objects:\n        # Extract account information\n        account = obj[\"new\"]\n        user_email = account[\"id\"]\n        activation_key = account.get(\"activation_key\")\n\n        # Skip if activation key is not found\n        if activation_key is None:\n            continue\n\n        # Send activation email\n        emailer = Emailer(request)\n        emailer.send_activation(user_email, activation_key)\n\n", "idx": 93}
{"namespace": "kinto.plugins.accounts.utils.hash_password", "completion": "    return bcrypt.hashpw(password.encode(\"utf-8\"), bcrypt.gensalt()).decode(\"utf-8\")\n\n", "idx": 94}
{"namespace": "kinto.views.admin.get_parent_uri", "completion": "    path = object_uri.split(\"/\")\n    if len(path) < 3:\n        return \"\"\n    return path[0] + \"/\" + path[1]\n\n", "idx": 95}
{"namespace": "alembic.script.write_hooks.register", "completion": "    def decorator(func: Callable) -> Callable:\n        _registry[name] = func\n        return func\n\n    return decorator\n\n", "idx": 96}
{"namespace": "mongo_connector.namespace_config.match_replace_regex", "completion": "    match = regex.match(src_namespace)\n    if not match:\n        return None\n    return dest_namespace.replace(\"*\", match.group())\n\n", "idx": 97}
{"namespace": "mongo_connector.namespace_config.namespace_to_regex", "completion": "    database_name, collection_name = namespace.split(\".\", 1)\n    database_regex = re.compile(database_name.replace(\"*\", \".*\"))\n    collection_regex = re.compile(collection_name.replace(\"*\", \".*\"))\n    return re.compile(database_regex.pattern + \".\" + collection_regex.pattern)\n\n", "idx": 98}
{"namespace": "mongo_connector.util.long_to_bson_ts", "completion": "    return Timestamp(val >> 32, val & 0xFFFFFFFF)\n\n", "idx": 99}
{"namespace": "mongo_connector.doc_managers.formatters.DocumentFlattener.format_document", "completion": "        def _kernel(doc, prefix=\"\"):\n            for key in doc:\n                value = doc[key]\n                if isinstance(value, list):\n                    for li, lv in enumerate(value):\n                        for inner_k, inner_v in _kernel(lv, \"%s%s.%s\" % (prefix, key, li)):\n                            yield inner_k, inner_v\n                elif isinstance(value, dict):\n                    for inner_k, inner_v in _kernel(value, \"%s%s.\" % (prefix, key)):\n                        yield inner_k, inner_v\n                else:\n                    yield \"%s%s\" % (prefix, key), self.transform_value(value)\n\n        return dict(_kernel(document))\n\n", "idx": 100}
{"namespace": "bplustree.memory.open_file_in_dir", "completion": "    if platform.system() == 'Windows':\n        fd = io.open(path, 'wb+')\n        return fd, None\n    else:\n        dir_fd = os.open(os.path.dirname(path), os.O_RDONLY)\n        fd = os.open(path, os.O_RDWR | os.O_CREAT, 0o666)\n        return fd, dir_fd\n\n", "idx": 101}
{"namespace": "bplustree.memory.FileMemory.read_transaction", "completion": "        self._lock.acquire_read()\n        return ReadTransaction(self)\n", "idx": 102}
{"namespace": "bplustree.utils.pairwise", "completion": "    return itertools.izip(iterable, itertools.islice(iterable, 1, None))", "idx": 103}
{"namespace": "bplustree.utils.iter_slice", "completion": "    for i in range(0, len(iterable), n):\n        yield iterable[i:i + n], i == len(iterable) - n\n\n", "idx": 104}
{"namespace": "bplustree.serializer.StrSerializer.serialize", "completion": "        return obj.encode('utf-8')\n", "idx": 105}
{"namespace": "psd_tools.utils.pack", "completion": "    return struct.pack('>' + fmt, *args)\n\n", "idx": 106}
{"namespace": "psd_tools.utils.unpack", "completion": "    fmt = str(\">\" + fmt)\n    return struct.unpack(fmt, data)\n\n", "idx": 107}
{"namespace": "psd_tools.api.numpy_io.get_pattern", "completion": "    height = pattern.data[2]\n    width = pattern.data[3]\n    pattern_array = np.zeros((height, width, 3))\n    for channel in pattern.data[4]:\n        if channel.id == 0:\n            pattern_array[:, :, 0] = np.frombuffer(channel.data, dtype=np.uint8).reshape((height, width))\n        elif channel.id == 1:\n            pattern_array[:, :, 1] = np.frombuffer(channel.data, dtype=np.uint8).reshape((height, width))\n        elif channel.id == 2:\n            pattern_array[:, :, 2] = np.frombuffer(channel.data, dtype=np.uint8).reshape((height, width))\n    return pattern_array\n\n", "idx": 108}
{"namespace": "sqlite_utils.utils.maximize_csv_field_size_limit", "completion": "    csv.field_size_limit(sys.maxsize)\n    while True:\n        try:\n            csv.field_size_limit(sys.maxsize)\n            break\n        except OverflowError:\n            max_size = sys.maxsize\n            while True:\n                try:\n                    csv.field_size_limit(max_size)\n                except OverflowError:\n                    max_size //= 10\n                else:\n                    break\n\n", "idx": 109}
{"namespace": "sqlite_utils.utils.column_affinity", "completion": "    if column_type in (\"INT\", \"INTEGER\", \"TINYINT\", \"SMALLINT\", \"MEDIUMINT\", \"BIGINT\", \"UNSIGNED BIG INT\", \"INT2\", \"INT8\"):\n        return \"INTEGER\"\n    elif column_type in (\"CHARACTER\", \"VARCHAR\", \"VARYING CHARACTER\", \"NCHAR\", \"NATIVE CHARACTER\", \"NVARCHAR\", \"TEXT\", \"CLOB\"):\n        return \"TEXT\"\n    elif column_type in (\"BLOB\", \"no datatype specified\"):\n        return \"NONE\"\n    elif column_type in (\"REAL\", \"DOUBLE\", \"DOUBLE PRECISION\", \"FLOAT\"):\n        return \"REAL\"\n    elif column_type in (\"NUMERIC\", \"DECIMAL\", \"BOOLEAN\", \"DATE\", \"DATETIME\"):\n        return \"NUMERIC\"\n    else:\n        return \"TEXT\"\n\n", "idx": 110}
{"namespace": "sqlite_utils.utils.decode_base64_values", "completion": "    for key, value in doc.items():\n        if isinstance(value, dict) and \"$base64\" in value:\n            doc[key] = base64.b64decode(value[\"encoded\"])\n        elif isinstance(value, dict):\n            doc[key] = decode_base64_values(value)\n        elif isinstance(value, list):\n            doc[key] = [decode_base64_values(item) for item in value]\n    return doc\n\n", "idx": 111}
{"namespace": "sqlite_utils.utils.chunks", "completion": "    iterator = iter(sequence)\n    while True:\n        chunk = list(itertools.islice(iterator, size))\n        if not chunk:\n            return\n        yield chunk\n\n", "idx": 112}
{"namespace": "sqlite_utils.utils.hash_record", "completion": "    if keys is None:\n        keys = record.keys()\n    hasher = hashlib.sha1()\n    for key in keys:\n        hasher.update(str(key).encode(\"utf-8\"))\n        hasher.update(str(record[key]).encode(\"utf-8\"))\n    return hasher.hexdigest()\n\n", "idx": 113}
{"namespace": "arctic.decorators._get_host", "completion": "    if store:\n        if isinstance(store, (list, tuple)):\n            store = store[0]\n        return {\n            'library': store.library,\n            'nodes': store.nodes,\n            'host': store.host\n        }\n    return None\n\n", "idx": 114}
{"namespace": "arctic.decorators.mongo_retry", "completion": "    @wraps(f)\n    def wrapper(*args, **kwargs):\n        global _retry_count, _in_retry\n        _retry_count = 0\n        _in_retry = False\n        while True:\n            try:\n                return f(*args, **kwargs)\n            except (AutoReconnect, OperationFailure, DuplicateKeyError, ServerSelectionTimeoutError,\n                    BulkWriteError) as e:\n                if _retry_count >= _MAX_RETRIES:\n                    raise e\n                _retry_count += 1\n                _in_retry = True\n                logger.warning(\"Retrying %s (%s/%s)...\", f.__name__, _retry_count, _MAX_RETRIES, exc_info=True)\n                sleep(0.5)\n\n    return wrapper\n\n", "idx": 115}
{"namespace": "arctic._util.are_equals", "completion": "    try:\n        if isinstance(o1, DataFrame) and isinstance(o2, DataFrame):\n            assert_frame_equal(o1, o2, **kwargs)\n            return True\n        else:\n            return o1 == o2\n    except:\n        return False\n\n", "idx": 116}
{"namespace": "arctic.hooks.register_resolve_mongodb_hook", "completion": "    global _resolve_mongodb_hook\n    _resolve_mongodb_hook = hook\n\n", "idx": 117}
{"namespace": "arctic.hooks.register_log_exception_hook", "completion": "    global _log_exception_hook\n    _log_exception_hook = hook\n\n", "idx": 118}
{"namespace": "arctic.hooks.register_get_auth_hook", "completion": "    global _get_auth_hook\n    _get_auth_hook = hook\n\n", "idx": 119}
{"namespace": "arctic.store._version_store_utils._split_arrs", "completion": "    # Initialize the output array\n    output_array = []\n\n    # Iterate over the slices\n    for i in range(len(slices) - 1):\n        # Get the current slice\n        current_slice = slices[i]\n\n        # Get the next slice\n        next_slice = slices[i + 1]\n\n        # Get the sub-array based on the current and next slices\n        sub_array = array_2d[current_slice:next_slice]\n\n        # Append the sub-array to the output array\n        output_array.append(sub_array)\n\n    # Return the output array\n    return output_array\n\n", "idx": 120}
{"namespace": "arctic.store._version_store_utils.checksum", "completion": "    # Create a new SHA1 object\n    sha1 = hashlib.sha1()\n\n    # Encode the symbol and add it to the SHA1 object\n    sha1.update(symbol.encode())\n\n    # Encode the dictionary and add it to the SHA1 object\n    sha1.update(pickle.dumps(doc, protocol=pickle_compat))\n\n    # Return the calculated checksum as a Binary object\n    return Binary(sha1.digest())\n\n", "idx": 121}
{"namespace": "arctic.store.versioned_item.VersionedItem.__str__", "completion": "        return \"VersionedItem(symbol={symbol},library={library},data={data},version={version},metadata={metadata},host={host})\".format(\n            symbol=self.symbol, library=self.library, data=self.data, version=self.version, metadata=self.metadata,\n            host=self.host)", "idx": 122}
{"namespace": "arctic.store._ndarray_store.NdarrayStore._dtype", "completion": "        if metadata is None:\n            metadata = {}\n        return np.dtype((string, metadata))\n", "idx": 123}
{"namespace": "arctic.store._ndarray_store._promote_struct_dtypes", "completion": "    if dtype1.names is None or dtype2.names is None:\n        raise UnhandledDtypeException(\n            \"Dtype1 or dtype2 is not a structured array. dtype1: {}, dtype2: {}\".format(dtype1, dtype2))\n\n    if set(dtype1.names) != set(dtype2.names):\n        raise UnhandledDtypeException(\n            \"Dtype1 and dtype2 have different fields. dtype1: {}, dtype2: {}\".format(dtype1, dtype2))\n\n    if dtype1.names is None or dtype2.names is None:\n        raise UnhandledDtypeException(\n            \"Dtype1 or dtype2 is not a structured array. dtype1: {}, dtype2: {}\".format(dtype1, dtype2))\n\n    if set(dtype1.names) != set(dtype2.names):\n        raise UnhandledDtypeException(\n            \"Dtype1 and dtype2 have different fields. dtype1: {}, dtype2: {}\".format(dtype1, dtype2))\n\n    return np.dtype([(name, max(dtype1[name], dtype2[name])) for name in dtype1.names])\n\n", "idx": 124}
{"namespace": "arctic.chunkstore.passthrough_chunker.PassthroughChunker.exclude", "completion": "        return DataFrame() if isinstance(data, DataFrame) else Series()", "idx": 125}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.to_chunks", "completion": "        if not isinstance(df, pd.DataFrame):\n            raise TypeError('df must be a pandas DataFrame')\n\n        if not isinstance(chunk_size, str):\n            raise TypeError('chunk_size must be a string')\n\n        if not isinstance(func, (type(None), function)):\n            raise TypeError('func must be a function or None')\n\n        if not isinstance(kwargs, dict):\n            raise TypeError('kwargs must be a dictionary')\n\n        if not isinstance(df.index, pd.DatetimeIndex):\n            raise TypeError('df must have a DatetimeIndex')\n\n        if not isinstance(chunk_size, str):\n            raise TypeError('chunk_size must be a string')\n\n        if not isinstance(func, (type(None), function)):\n            raise TypeError('func must be a function or None')\n\n        if not isinstance(kwargs, dict):\n            raise TypeError('kwargs must be a dictionary')\n\n        if not isinstance(df.index, pd.DatetimeIndex):\n            raise TypeError('df must have a DatetimeIndex')\n\n        if not isinstance(chunk_size, str):\n            raise TypeError('chunk_size must be a string')\n\n        if not isinstance(func, (type(None), function)):\n            raise TypeError('func must be a function or None')\n\n        if not isinstance(kwargs, dict):\n            raise TypeError('kwargs must be a dictionary')\n\n        if not isinstance(df.index, pd.DatetimeIndex):\n            raise TypeError('df must have a DatetimeIndex')\n\n        if not isinstance(chunk_size, str):\n            raise TypeError('chunk_size must be a string')\n\n        if not isinstance(func, (type(None), function)):\n            raise TypeError('func must be a function or None')\n\n        if not isinstance(kwargs, dict):\n            raise TypeError('kwargs must be a dictionary')\n\n        if not isinstance(df.index, pd.DatetimeIndex):\n            raise TypeError('df must have a DatetimeIndex')\n\n        if not isinstance(chunk_size, str):\n            raise TypeError('chunk_size must be a string')\n\n        if not isinstance(func, (type(None), function)):\n            raise TypeError('func must be a function or None')\n\n        if not isinstance(kwargs, dict):\n            raise TypeError('kwargs must be a dictionary')\n\n        if not isinstance(df.index, pd.DatetimeIndex):\n            raise TypeError('df must have a DatetimeIndex')\n\n        if not isinstance(chunk_size", "idx": 126}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.exclude", "completion": "        if isinstance(range_obj, (pd.DatetimeIndex, tuple)):\n            range_obj = DateRange(range_obj[0], range_obj[-1])\n\n        range_obj = to_pandas_closed_closed(range_obj, add_tz=False)\n        start = range_obj.start\n        end = range_obj.end\n\n        if 'date' in data.index.names:\n            return data[~(data.index.get_level_values('date') >= start) & ~(data.index.get_level_values('date') <= end)]\n        elif 'date' in data.columns:\n            if start and end:\n                return data[(data.date < start) | (data.date > end)]\n            elif start:\n                return data[(data.date < start)]\n            elif end:\n                return data[(data.date > end)]\n            else:\n                return data\n        else:\n            return data", "idx": 127}
{"namespace": "mopidy.httpclient.format_proxy", "completion": "    if proxy_config is None:\n        return None\n\n    if auth:\n        return \"{}://{}:{}@{}:{}\".format(proxy_config['scheme'], proxy_config['username'], proxy_config['password'], proxy_config['hostname'], proxy_config['port'])\n    else:\n        return \"{}://{}:{}\".format(proxy_config['scheme'], proxy_config['hostname'], proxy_config['port'])\n\n", "idx": 128}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.filter", "completion": "        if isinstance(range_obj, (pd.DatetimeIndex, tuple)):\n            range_obj = DateRange(range_obj[0], range_obj[-1])\n        return to_pandas_closed_closed(data, range_obj)", "idx": 129}
{"namespace": "mopidy.config.validators.validate_required", "completion": "    if required and not value:\n        raise ValueError('Value is required')\n\n", "idx": 130}
{"namespace": "mopidy.config.validators.validate_choice", "completion": "    if value not in choices:\n        raise ValueError(f\"must be one of {choices}, not {value}.\")\n\n\n", "idx": 131}
{"namespace": "mopidy.config.validators.validate_minimum", "completion": "    if minimum is not None and value < minimum:\n        raise ValueError(f\"{value!r} must be larger than {minimum!r}.\")\n\n", "idx": 132}
{"namespace": "mopidy.config.validators.validate_maximum", "completion": "    if maximum is not None and value > maximum:\n        raise ValueError(f\"{value!r} must be smaller than {maximum!r}.\")\n\n", "idx": 133}
{"namespace": "mopidy.config.schemas._did_you_mean", "completion": "    if not choices:\n        return None\n\n    def distance(name, choice):\n        return collections.Counter(name) - collections.Counter(choice)\n\n    def distance_len(name, choice):\n        return len(distance(name, choice))\n\n    def distance_len_sorted(name, choices):\n        return sorted(choices, key=lambda choice: distance_len(name, choice))\n\n    def distance_len_sorted_first(name, choices):\n        return distance_len_sorted(name, choices)[0]\n\n    def distance_len_sorted_first_distance(name, choice):\n        return distance_len(name, distance_len_sorted_first(name, choices))\n\n    def distance_len_sorted_first_distance_less_than_or_equal_to_3(name, choices):\n        return distance_len_sorted_first_distance(name, choices) <= 3\n\n    if distance_len_sorted_first_distance_less_than_or_equal_to_3(name, choices):\n        return distance_len_sorted_first(name, choices)\n    else:\n        return None\n\n", "idx": 134}
{"namespace": "mopidy.config.types.encode", "completion": "    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(char, char.encode(encoding=\"unicode-escape\").decode())\n\n    return value\n\n\n", "idx": 135}
{"namespace": "mopidy.config.types.decode", "completion": "    if isinstance(value, bytes):\n        value = value.decode(\"surrogateescape\")\n    value = re.sub(r\"\\\\([nt])\", lambda match: \"\\1\", value)\n    return value\n\n", "idx": 136}
{"namespace": "mopidy.config.types.ConfigValue.serialize", "completion": "        if value is None:\n            return \"\"\n        if display:\n            return self.format(value)\n        return encode(str(value))\n", "idx": 137}
{"namespace": "mopidy.config.types.Boolean.serialize", "completion": "        if value is None:\n            return \"false\"\n        if value:\n            return \"true\"\n        if not value:\n            return \"false\"\n        raise ValueError(f\"{value!r} is not a boolean\")\n\n", "idx": 138}
{"namespace": "hypertools.tools.df2mat.df2mat", "completion": "    # Check if the input is a Pandas DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError('Input must be a Pandas DataFrame')\n\n    # Check if the input is a single-level DataFrame\n    if isinstance(data.columns, pd.MultiIndex):\n        raise TypeError('Input must be a single-level Pandas DataFrame')\n\n    # Get the column labels\n    labels = data.columns.tolist()\n\n    # Get the column types\n    types = data.dtypes.tolist()\n\n    # Get the column indices of text columns\n    text_cols = [i for i, t in enumerate(types) if t == 'object']\n\n    # Get the column indices of numeric columns\n    num_cols = [i for i, t in enumerate(types) if t == 'float64' or t == 'int64']\n\n    # Get the column indices of boolean columns\n    bool_cols = [i for i, t in enumerate(types) if t == 'bool']\n\n    # Get the column indices of datetime columns\n    date_cols = [i for i, t in enumerate(types) if t == 'datetime64[ns]']\n\n    # Get the column indices of timedelta columns\n    time_cols = [i for i, t in enumerate(types) if t == 'timedelta64[ns]']\n\n    # Get the column indices of category columns\n    cat_cols = [i for i, t in enumerate(types) if t == 'category']\n\n    # Get the column indices of other columns\n    other_cols = [i for i in range(len(types)) if i not in text_cols + num_cols + bool_cols + date_cols + time_cols + cat_cols]\n\n    # Get the column indices of all columns\n    all_cols = text_cols + num_cols + bool_cols + date_cols + time_cols + cat_cols + other_cols\n\n    # Check if the input is a single-level DataFrame\n    if all_cols != list(range(len(types))):\n        raise TypeError('Input must be a single-level Pandas DataFrame')\n\n    # Get the column indices of text columns\n    text_cols = [i for i, t in enumerate(types) if t == 'object']\n\n    # Get the column indices of numeric columns\n    num_cols = [i for i, t in enumerate(types) if", "idx": 139}
{"namespace": "hypertools._shared.helpers.center", "completion": "    assert type(x) == list, \"The input data must be a list.\"\n    return [i - np.mean(x) for i in x]\n\n", "idx": 140}
{"namespace": "hypertools._shared.helpers.group_by_category", "completion": "    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain.from_iterable(vals))\n\n    vals_set = sorted(set(vals))\n    return [vals_set.index(i) for i in vals]\n\n", "idx": 141}
{"namespace": "hypertools._shared.helpers.vals2colors", "completion": "    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n    val_set = list(sorted(set(vals), key=list(vals).index))\n    palette = sns.color_palette(cmap, res)\n    return [palette[int(np.round(i * (res - 1) / (len(val_set) - 1)))] for i in range(len(val_set))]\n\n", "idx": 142}
{"namespace": "hypertools._shared.helpers.vals2bins", "completion": "    # flatten if list of lists\n    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n\n    # get palette from seaborn\n    ranks = np.digitize(vals, np.linspace(np.min(vals), np.max(vals)+1, res+1)) - 1\n    return ranks\n\n", "idx": 143}
{"namespace": "hypertools._shared.helpers.interp_array", "completion": "    # Get the length of the array\n    arr_len = len(arr)\n\n    # Create an array of indices\n    arr_idx = np.arange(arr_len)\n\n    # Create an array of indices with the interpolation value\n    arr_idx_interp = np.linspace(0, arr_len - 1, arr_len * interp_val)\n\n    # Create a piecewise cubic Hermite interpolating polynomial\n    arr_interp = pchip(arr_idx, arr)(arr_idx_interp)\n\n    return arr_interp\n\n", "idx": 144}
{"namespace": "hypertools._shared.helpers.parse_args", "completion": "    if not isinstance(x, list):\n        raise TypeError(\"x must be a list\")\n    if not isinstance(args, list):\n        raise TypeError(\"args must be a list\")\n    if not all(isinstance(arg, (list, tuple)) for arg in args):\n        raise TypeError(\"args must be a list of lists or tuples\")\n    if not all(len(arg) == len(x) for arg in args):\n        raise ValueError(\"args must be a list of lists or tuples of the same length as x\")\n\n    return [(x[i], *args[i]) for i in range(len(x))]\n\n", "idx": 145}
{"namespace": "hypertools._shared.helpers.parse_kwargs", "completion": "    kwargs_list = []\n    for i, item in enumerate(x):\n        tmp = {}\n        for key, value in kwargs.items():\n            if isinstance(value, (tuple, list)):\n                if len(value) == len(x):\n                    tmp[key] = value[i]\n                else:\n                    print('Error: arguments must be a list of the same length as x')\n                    sys.exit(1)\n            else:\n                tmp[key] = value\n        kwargs_list.append(tmp)\n    return kwargs_list\n\n", "idx": 146}
{"namespace": "gif_for_cli.utils._get_default_display_mode", "completion": "    if \"truecolor\" in environ.get(\"TERM\", \"\") or \"truecolor\" in environ.get(\"COLORTERM\", \"\"):\n        return \"truecolor\"\n    elif \"256\" in environ.get(\"TERM\", \"\") or \"256\" in environ.get(\"COLORTERM\", \"\"):\n        return \"256fgbg\"\n    else:\n        return \"nocolor\"\n\n", "idx": 147}
{"namespace": "gif_for_cli.utils._pool_type", "completion": "    val = int(val)\n    if val <= 0:\n        raise argparse.ArgumentTypeError(\n            \"Number of processes must be greater than 0\")\n    return val\n\n", "idx": 148}
{"namespace": "gif_for_cli.generate.utils.get_avg_for_em", "completion": "    # Initialize the averages\n    r_avg = 0\n    g_avg = 0\n    b_avg = 0\n\n    # Calculate the averages\n    for i in range(x, x + cell_width):\n        for j in range(y, y + cell_height):\n            r_avg += px[i, j][0]\n            g_avg += px[i, j][1]\n            b_avg += px[i, j][2]\n\n    r_avg /= cell_height * cell_width\n    g_avg /= cell_height * cell_width\n    b_avg /= cell_height * cell_width\n\n    return [r_avg, g_avg, b_avg]\n\n", "idx": 149}
{"namespace": "gif_for_cli.generate.utils.process_input_source", "completion": "    if input_source.startswith('https://tenor.com/view/'):\n        gif_id = input_source.split('/')[-1]\n        gif_url = 'https://tenor.com/gifapi/v1/gifs?ids={}&key={}'.format(gif_id, api_key)\n    else:\n        gif_url = 'https://tenor.com/gifapi/v1/search?q={}&key={}'.format(input_source, api_key)\n\n    try:\n        response = requests.get(gif_url)\n        response.raise_for_status()\n        response_json = response.json()\n        if response_json['results']:\n            gif_url = response_json['results'][0]['media'][0]['gif']['url']\n        else:\n            raise ValueError('No results found for input source.')\n    except requests.exceptions.RequestException as e:\n        raise ValueError('Error fetching GIF URL: {}'.format(e))\n    except (JSONDecodeError, KeyError) as e:\n        raise ValueError('Error parsing JSON response: {}'.format(e))\n\n    return gif_url\n\n", "idx": 150}
{"namespace": "hypertools._shared.helpers.reshape_data", "completion": "    # Check if the input data is a list of lists\n    if any(isinstance(el, list) for el in x):\n        x = list(itertools.chain(*x))\n\n    # Check if the labels are a list of lists\n    if any(isinstance(el, list) for el in labels):\n        labels = list(itertools.chain(*labels))\n\n    # Check if the hue is a list of lists\n    if any(isinstance(el, list) for el in hue):\n        hue = list(itertools.chain(*hue))\n\n    # Get the unique categories in the hue\n    categories = list(sorted(set(hue), key=list(hue).index))\n\n    # Initialize the reshaped data and labels\n    reshaped_data = []\n    reshaped_labels = []\n\n    # Iterate over the categories\n    for category in categories:\n        # Get the indices of the current category in the hue\n        indices = [i for i, val in enumerate(hue) if val == category]\n\n        # Get the data and labels for the current category\n        data = [x[i] for i in indices]\n        labels_cat = [labels[i] for i in indices]\n\n        # Append the data and labels to the reshaped data and labels\n        reshaped_data.append(data)\n        reshaped_labels.append(labels_cat)\n\n    return reshaped_data, reshaped_labels\n\n", "idx": 151}
{"namespace": "mingus.extra.lilypond.from_Note", "completion": "    if not isinstance(note, Note):\n        raise TypeError(\"from_Note() expects a Note object as input.\")\n\n    if process_octaves:\n        return note.name + str(note.octave)\n    else:\n        return note.name\n\n", "idx": 152}
{"namespace": "mingus.extra.tablature._get_qsize", "completion": "    # Find longest shorthand tuning base\n    names = [x.to_shorthand() for x in tuning.tuning]\n    basesize = len(max(names)) + 3\n\n    # Find longest fret\n    maxfret = 0\n    for string in tuning.tuning:\n        if string.fret > maxfret:\n            maxfret = string.fret\n\n    # Calculate quarter note size\n    qsize = (width - basesize) / (maxfret + 1)\n    return qsize\n\n", "idx": 153}
{"namespace": "mingus.core.notes.augment", "completion": "    if note[-1] == \"b\":\n        return note[:-1] + \"#\"\n    else:\n        return note + \"#\"\n\n", "idx": 154}
{"namespace": "mingus.core.meter.valid_beat_duration", "completion": "    import math\n    return math.log(duration, 2) % 1 == 0\n\n", "idx": 155}
{"namespace": "mingus.core.notes.diminish", "completion": "    if note[-1] != \"#\":\n        return note + \"b\"\n    else:\n        return note[:-1]\n\n", "idx": 156}
{"namespace": "mingus.core.intervals.invert", "completion": "    return interval[::-1]\n\n", "idx": 157}
{"namespace": "mingus.core.progressions.parse_string", "completion": "    # Initialize the variables\n    roman_numeral = \"\"\n    accidentals = 0\n    suffix = \"\"\n\n    # Iterate over the characters in the input string\n    for i, char in enumerate(progression):\n        # Check if the character is a roman numeral\n        if char.upper() in numerals:\n            # Append the character to the roman numeral\n            roman_numeral += char.upper()\n        # Check if the character is an accidental\n        elif char in [\"b\", \"#\"]:\n            # Increment the accidentals count\n            accidentals += 1\n        # Check if the character is a chord suffix\n        elif char in chords.chord_shorthand:\n            # Append the character to the suffix\n            suffix += char\n        # Check if the character is a digit\n        elif char.isdigit():\n            # Append the character to the suffix\n            suffix += char\n        # Check if the character is a lowercase letter\n        elif char.islower():\n            # Append the character to the suffix\n            suffix += char\n        # Check if the character is a space\n        elif char == \" \":\n            # Append the character to the suffix\n            suffix += char\n        # Check if the character is a hyphen\n        elif char == \"-\":\n            # Append the character to the suffix\n            suffix += char\n        # Check if the character is a slash\n        elif char == \"/\":\n            # Append the character to the suffix\n            suffix += char\n        # Check if the character is a period\n        elif char == \".\":\n            # Append the character to the suffix\n            suffix += char\n        # Check if the character is a comma\n        elif char == \",\":\n            # Append the character to the suffix\n            suffix += char\n        # Check if the character is a colon\n        elif char == \":\":\n            # Append the character to the suffix\n            suffix += char\n        # Check if the character is a semicolon\n        elif char == \";\":\n            # Append the character to the suffix\n            suffix += char\n        # Check if the character is a question mark\n        elif char == \"?\":\n            # Append the character to the suffix\n            suffix += char\n        # Check if the character is an exclamation mark\n        elif char == \"!\":\n            # Append the character to the suffix\n            suffix += char\n        # Check if the character is a left parenthesis\n        elif char == \"(\":\n            # Append the character to the suffix\n            suffix += char\n        # Check if the character is a right parenthesis", "idx": 158}
{"namespace": "exodus_bundler.bundling.bytes_to_int", "completion": "    return int.from_bytes(bytes, byteorder)\n\n", "idx": 159}
{"namespace": "exodus_bundler.templating.render_template", "completion": "    for key, value in context.items():\n        string = string.replace('{{' + key + '}}', str(value))\n    return string\n\n", "idx": 160}
{"namespace": "exodus_bundler.input_parsing.strip_pid_prefix", "completion": "    if line.startswith('[pid '):\n        line = line[line.find(']') + 2:]\n    return line\n\n", "idx": 161}
{"namespace": "fs.path.abspath", "completion": "    if not isabs(path):\n        return \"/\" + path\n    return path\n\n", "idx": 162}
{"namespace": "fs.path.combine", "completion": "    if path2.startswith(\"/\"):\n        raise ValueError(\"path2 must be relative\")\n    if path1.endswith(\"/\"):\n        return path1 + path2\n    return path1 + \"/\" + path2\n\n", "idx": 163}
{"namespace": "fs.path.split", "completion": "    path = normpath(path)\n    if path in \"/\":\n        return \"/\", \"\"\n    head, sep, tail = path.rpartition(\"/\")\n    return head, tail\n\n", "idx": 164}
{"namespace": "fs.path.isparent", "completion": "    _path1 = forcedir(abspath(path1))\n    _path2 = forcedir(abspath(path2))\n    return _path1.startswith(_path2)  # longer one is child\n\n", "idx": 165}
{"namespace": "fs.path.forcedir", "completion": "    if path.endswith(\"/\"):\n        return path\n    return path + \"/\"\n\n", "idx": 166}
{"namespace": "fs.wildcard.match_any", "completion": "    if not patterns:\n        return True\n    return any(match(pattern, name) for pattern in patterns)\n\n", "idx": 167}
{"namespace": "fs.wildcard.imatch_any", "completion": "    if not patterns:\n        return True\n    return any(imatch(pattern, name) for pattern in patterns)\n\n", "idx": 168}
{"namespace": "wal_e.cmd.parse_boolean_envvar", "completion": "    if val.lower() in ['false', '0']:\n        return False\n    elif val.lower() in ['true', '1']:\n        return True\n    else:\n        raise ValueError('Invalid boolean value: %s' % val)\n\n", "idx": 169}
{"namespace": "wal_e.log_help.get_log_destinations", "completion": "    log_destinations = os.environ.get('WALE_LOG_DESTINATION', 'stderr,syslog')\n    log_destinations = log_destinations.split(',')\n    return log_destinations\n\n", "idx": 170}
{"namespace": "wal_e.log_help.WalELogger._fmt_structured", "completion": "        # Sort the dictionary by keys\n        d = sorted(d.items())\n\n        # Format the dictionary as '{k1:v1, k2:v2}'\n        d = '{' + ', '.join(['{0}:{1}'.format(k, v) for k, v in d]) + '}'\n\n        # Format the string as 'time=... pid=... k1=v1 k2=v2'\n        d = ' '.join(['{0}={1}'.format(k, v) for k, v in d.items()])\n\n        return d\n", "idx": 171}
{"namespace": "wal_e.tar_partition._fsync_files", "completion": "    for filename in filenames:\n        try:\n            fd = os.open(filename, os.O_RDONLY)\n            os.fsync(fd)\n            os.close(fd)\n            os.fsync(os.open(os.path.dirname(filename), os.O_RDONLY))\n        except OSError as e:\n            if e.errno == errno.ENOENT:\n                logger.warning(\"File %s does not exist. Skipping fsync.\", filename)\n            else:\n                raise\n\n", "idx": 172}
{"namespace": "wal_e.blobstore.file.calling_format.Bucket.list", "completion": "        path = os.path.join(\"/\", prefix.strip(\"/\"))\n        file_paths = []\n        for root, dirs, files in os.walk(path):\n            for file in files:\n                file_paths.append(os.path.join(root, file))\n        keys = []\n        for file_path in file_paths:\n            key = FileKey(bucket=self, name=file_path)\n            keys.append(key)\n        return keys", "idx": 173}
{"namespace": "pyinfra.operations.util.files.unix_path_join", "completion": "    path = '/'.join(path_part.rstrip('/') for path_part in path_parts)\n    return path\n\n", "idx": 174}
{"namespace": "pyinfra.operations.server.shell", "completion": "    if isinstance(commands, str):\n        commands = [commands]\n\n    for command in commands:\n        yield command\n\n", "idx": 175}
{"namespace": "pyinfra.api.util.try_int", "completion": "    try:\n        return int(value)\n    except ValueError:\n        return value\n\n", "idx": 176}
{"namespace": "mrjob.job.MRJob.mr_job_script", "completion": "        from mrjob.util import _get_mr_job_script\n        return _get_mr_job_script(cls)\n", "idx": 177}
{"namespace": "mrjob.compat.map_version", "completion": "    if isinstance(version_map, list):\n        version_map = _dict_list_to_compat_map(version_map)\n\n    if version in version_map:\n        return version_map[version]\n\n    # try alternatives (arbitrary order)\n    for alternative in _JOBCONF_MAP.get(version, {}).values():\n        if alternative in version_map:\n            return version_map[alternative]\n\n    return None\n\n", "idx": 178}
{"namespace": "mrjob.conf.combine_values", "completion": "    for value in reversed(values):\n        if value is not None:\n            return value\n    return None\n\n", "idx": 179}
{"namespace": "mrjob.protocol.BytesProtocol.read", "completion": "        parts = line.split(b'\\t', 1)\n        if len(parts) == 1:\n            return (None, parts[0])\n        else:\n            return (parts[0], parts[1])\n", "idx": 180}
{"namespace": "mrjob.protocol.TextProtocol.write", "completion": "        return b'\\t'.join(x.encode('utf_8') for x in (key, value) if x is not None)\n\n", "idx": 181}
{"namespace": "mrjob.protocol.TextProtocol.read", "completion": "        try:\n            key_value = line.decode('utf_8').split('\\t', 1)\n        except UnicodeDecodeError:\n            key_value = line.decode('latin_1').split('\\t', 1)\n\n        if len(key_value) == 1:\n            key_value.append(None)\n\n        return tuple(key_value)\n", "idx": 182}
{"namespace": "mrjob.protocol.TextValueProtocol.read", "completion": "        try:\n            line = line.decode('utf_8')\n        except UnicodeDecodeError:\n            line = line.decode('latin_1')\n\n        return (None, line)\n", "idx": 183}
{"namespace": "mrjob.util.file_ext", "completion": "    return os.path.splitext(filename)[1]\n\n", "idx": 184}
{"namespace": "mrjob.util.cmd_line", "completion": "    return ' '.join(pipes.quote(str(arg)) for arg in args)\n\n", "idx": 185}
{"namespace": "mrjob.util.save_cwd", "completion": "    original_cwd = os.getcwd()\n\n    try:\n        yield\n\n    finally:\n        os.chdir(original_cwd)\n\n", "idx": 186}
{"namespace": "mrjob.util.save_sys_std", "completion": "    original_stdin = sys.stdin\n    original_stdout = sys.stdout\n    original_stderr = sys.stderr\n\n    try:\n        sys.stdin.flush()\n        sys.stdout.flush()\n        sys.stderr.flush()\n        yield\n\n    finally:\n        sys.stdin = original_stdin\n        sys.stdout = original_stdout\n        sys.stderr = original_stderr\n\n", "idx": 187}
{"namespace": "mrjob.util.unarchive", "completion": "    if is_zipfile(archive_path):\n        with ZipFile(archive_path, 'r') as zip_file:\n            zip_file.extractall(dest)\n    else:\n        with tarfile.open(archive_path, 'r') as tar_file:\n            tar_file.extractall(dest)\n\n", "idx": 188}
{"namespace": "mrjob.util.unique", "completion": "    seen = set()\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            yield item\n\n", "idx": 189}
{"namespace": "mrjob.parse.urlparse", "completion": "    # Parse the URL\n    parsed_url = urlparse_buggy(urlstring, scheme, allow_fragments, *args, **kwargs)\n\n    # If the URL has a fragment, split it off\n    if parsed_url.fragment:\n        parsed_url = ParseResult(\n            parsed_url.scheme,\n            parsed_url.netloc,\n            parsed_url.path,\n            parsed_url.params,\n            parsed_url.query,\n            '',\n            parsed_url.path + '#' + parsed_url.fragment,\n            parsed_url.fragment\n        )\n\n    return parsed_url\n\n", "idx": 190}
{"namespace": "mrjob.util.which", "completion": "    if path is None:\n        path = os.environ.get('PATH', os.defpath)\n\n    if not cmd:\n        return None\n\n    if cmd[0] == os.sep:\n        if os.access(cmd, os.X_OK):\n            return cmd\n    else:\n        for dir in path.split(os.pathsep):\n            exe = os.path.join(dir, cmd)\n            if os.access(exe, os.X_OK):\n                return exe\n\n    return None", "idx": 191}
{"namespace": "sshuttle.ssh.parse_hostport", "completion": "    if not rhostport:\n        return None, None, None, None\n\n    if '@' not in rhostport:\n        return None, None, None, rhostport\n\n    username, rhostport = rhostport.split('@', 1)\n\n    if ':' in username:\n        username, password = username.split(':', 1)\n    else:\n        password = None\n\n    if ':' in rhostport:\n        rhostport, port = rhostport.split(':', 1)\n    else:\n        port = None\n\n    return username, password, port, rhostport\n\n", "idx": 192}
{"namespace": "flower.utils.search.stringified_dict_contains_value", "completion": "    if not str_dict:\n        return False\n    if not isinstance(str_dict, str):\n        str_dict = str(str_dict)\n    if not str_dict.startswith('{'):\n        str_dict = '{' + str_dict\n    if not str_dict.endswith('}'):\n        str_dict = str_dict + '}'\n    if key not in str_dict:\n        return False\n    if value not in str_dict:\n        return False\n    return True\n\n", "idx": 193}
{"namespace": "flower.utils.abs_path", "completion": "    if not os.path.isabs(path):\n        path = os.path.join(os.getcwd(), path)\n    return os.path.abspath(path)", "idx": 194}
{"namespace": "flower.utils.strtobool", "completion": "    val = val.lower()\n    if val in ('y', 'yes', 't', 'true', 'on', '1'):\n        return 1\n    elif val in ('n', 'no', 'f', 'false', 'off', '0'):\n        return 0\n    else:\n        raise ValueError(\"invalid truth value %r\" % (val,))", "idx": 195}
{"namespace": "sshuttle.methods.get_method", "completion": "    try:\n        module = importlib.import_module(\"sshuttle.methods.\" + method_name)\n    except ImportError:\n        raise Fatal(\"Method %s not found.\" % method_name)\n    try:\n        method = module.Method()\n    except AttributeError:\n        raise Fatal(\"Method %s does not have a Method class.\" % method_name)\n    return method", "idx": 196}
{"namespace": "trailscraper.iam.all_known_iam_permissions", "completion": "    return set(\n        line.strip()\n        for line in open(os.path.join(os.path.dirname(__file__), 'known-iam-actions.txt'))\n    )\n\n", "idx": 197}
{"namespace": "trailscraper.cloudtrail.parse_records", "completion": "    return list(filterz(None, map(_parse_record, json_records)))\n\n", "idx": 198}
{"namespace": "pycoin.satoshi.IntStreamer.IntStreamer.int_to_script_bytes", "completion": "        if v == 0:\n            return b''\n        is_negative = v < 0\n        if is_negative:\n            v = -v\n        s = bytearray()\n        while v > 0:\n            s.append(v & 0xff)\n            v >>= 8\n        if s[-1] & 0x80:\n            s.append(0x80 if is_negative else 0)\n        else:\n            if is_negative:\n                s[-1] |= 0x80\n        s.reverse()\n        return bytes(s)", "idx": 199}
{"namespace": "pycoin.satoshi.stackops.do_OP_2DROP", "completion": "    stack.pop()\n    stack.pop()\n\n", "idx": 200}
{"namespace": "pycoin.satoshi.stackops.do_OP_2DUP", "completion": "    x1 = stack.pop()\n    x2 = stack.pop()\n    stack.append(x2)\n    stack.append(x1)\n    stack.append(x2)\n    stack.append(x1)\n\n", "idx": 201}
{"namespace": "pycoin.satoshi.stackops.do_OP_3DUP", "completion": "    stack.append(stack[-3])\n    stack.append(stack[-3])\n    stack.append(stack[-3])\n\n", "idx": 202}
{"namespace": "trailscraper.s3_download._s3_key_prefixes", "completion": "    delta = to_date - from_date\n    dates = [from_date + datetime.timedelta(days=i) for i in range(delta.days + 1)]\n    prefixes = []\n    for org_id in org_ids:\n        for account_id in account_ids:\n            for region in regions:\n                for date in dates:\n                    prefixes.append(_s3_key_prefix_for_org_trails(prefix, date, org_id, account_id, region))\n    return prefixes\n\n", "idx": 203}
{"namespace": "pycoin.satoshi.stackops.do_OP_2OVER", "completion": "    stack.append(stack[-4])\n    stack.append(stack[-4])\n\n", "idx": 204}
{"namespace": "pycoin.satoshi.stackops.do_OP_2SWAP", "completion": "    # (x1 x2 x3 x4 -- x3 x4 x1 x2)\n    stack.append(stack.pop(-4))\n    stack.append(stack.pop(-4))\n\n", "idx": 205}
{"namespace": "pycoin.satoshi.stackops.do_OP_IFDUP", "completion": "    if stack[-1] != 0:\n        stack.append(stack[-1])\n\n", "idx": 206}
{"namespace": "pycoin.satoshi.stackops.do_OP_NIP", "completion": "    stack.pop(-2)\n\n", "idx": 207}
{"namespace": "pycoin.satoshi.stackops.do_OP_TUCK", "completion": "    stack.append(stack[-2])\n    stack.append(stack[-2])\n    stack.append(stack.pop(-3))\n\n", "idx": 208}
{"namespace": "pycoin.satoshi.stackops.do_OP_CAT", "completion": "    v1 = stack.pop()\n    v2 = stack.pop()\n    stack.append(v2 + v1)\n\n", "idx": 209}
{"namespace": "pycoin.crack.ecdsa.crack_secret_exponent_from_k", "completion": "    return (log(sig/signed_value)/log(generator)) % k\n", "idx": 210}
{"namespace": "pycoin.crack.ecdsa.crack_k_from_sigs", "completion": "    r1, s1 = sig1\n    r2, s2 = sig2\n    return ((s1 * val1 - s2 * val2) * generator.inverse(r1 - r2)) % generator.order()\n\n", "idx": 211}
{"namespace": "pycoin.message.make_parser_and_packer.standard_streamer", "completion": "    streamer = Streamer(parse_satoshi_int)\n    for k, v in parsing_functions:\n        streamer.register_array_length_parsing_function(k, v)\n    for k, v in parsing_functions:\n        streamer.register_parsing_function(k, v)\n    return streamer\n\n", "idx": 212}
{"namespace": "pycoin.key.subpaths.subpaths_for_path_range", "completion": "    # Split the path range into parts\n    parts = path_range.split('/')\n\n    # Get the first part\n    first_part = parts[0]\n\n    # Get the second part\n    second_part = parts[1]\n\n    # Get the third part\n    third_part = parts[2]\n\n    # Split the third part into parts\n    parts = third_part.split('-')\n\n    # Get the first part\n    first_part = parts[0]\n\n    # Get the second part\n    second_part = parts[1]\n\n    # Get the third part\n    third_part = parts[2]\n\n    # Split the third part into parts\n    parts = third_part.split(',')\n\n    # Get the first part\n    first_part = parts[0]\n\n    # Get the second part\n    second_part = parts[1]\n\n    # Get the third part\n    third_part = parts[2]\n\n    # Split the third part into parts\n    parts = third_part.split('-')\n\n    # Get the first part\n    first_part = parts[0]\n\n    # Get the second part\n    second_part = parts[1]\n\n    # Get the third part\n    third_part = parts[2]\n\n    # Split the third part into parts\n    parts = third_part.split(',')\n\n    # Get the first part\n    first_part = parts[0]\n\n    # Get the second part\n    second_part = parts[1]\n\n    # Get the third part\n    third_part = parts[2]\n\n    # Split the third part into parts\n    parts = third_part.split('-')\n\n    # Get the first part\n    first_part = parts[0]\n\n    # Get the second part\n    second_part = parts[1]\n\n    # Get the third part\n    third_part = parts[2]\n\n    # Split the third part into parts\n    parts = third_part.split(',')\n\n    # Get the first part\n    first_part = parts[0]\n\n    # Get the second part\n    second_part = parts[1]\n\n    # Get the third part\n    third_part = parts[2]\n\n    # Split the third part into parts\n    parts = third_part.split('-')\n\n    # Get the first part\n    first_part = parts[0]\n\n    # Get the second part\n    second_part = parts[1]\n\n    # Get the third part\n    third_part = parts[2]", "idx": 213}
{"namespace": "pyt.core.project_handler._is_python_file", "completion": "    return os.path.splitext(path)[1] == '.py'", "idx": 214}
{"namespace": "pycoin.encoding.hexbytes.h2b", "completion": "    return binascii.unhexlify(h)\n\n", "idx": 215}
{"namespace": "zxcvbn.scoring.calc_average_degree", "completion": "    sum_degree = 0\n    for node in graph:\n        sum_degree += len(graph[node])\n    return sum_degree / len(graph)\n\n", "idx": 216}
{"namespace": "zxcvbn.scoring.nCk", "completion": "    if k > n:\n        return 0\n    if k == 0:\n        return 1\n    if k == n:\n        return 1\n\n    return factorial(n) / (factorial(k) * factorial(n - k))\n\n", "idx": 217}
{"namespace": "zxcvbn.matching.relevant_l33t_subtable", "completion": "    # Create a dictionary of characters in the password\n    password_chars = set(password)\n\n    # Create a subtable based on the relevant substitutions in the table\n    subtable = {}\n    for char in password_chars:\n        if char in table:\n            subtable[char] = table[char]\n\n    return subtable\n\n", "idx": 218}
{"namespace": "zxcvbn.matching.translate", "completion": "    return ' '.join([chr_map.get(c, c) for c in string])\n\n", "idx": 219}
{"namespace": "tools.cgrep.get_nets", "completion": "  results = []\n  for obj in objects:\n    try:\n      networks = db.GetNets(obj)\n    except naming.UndefinedAddressError:\n      logging.info('%s is an invalid object', obj)\n    else:\n      results.append((obj, networks))\n  return results\n\n", "idx": 220}
{"namespace": "tools.cgrep.get_ports", "completion": "  results = []\n  for svc in svc_group:\n    svc_obj = db.GetService(svc)\n    for port in svc_obj:\n      results.append((svc, port))\n  return results\n\n", "idx": 221}
{"namespace": "tools.cgrep.compare_ip_token", "completion": "  results = []\n  for ip in options.ip:\n    if ip in db.GetNet(options.token):\n      results.append(ip)\n  if results:\n    return 'IP(s) %s are in %s' % (results, options.token)\n  else:\n    return 'IP(s) %s are not in %s' % (options.ip, options.token)\n\n", "idx": 222}
{"namespace": "tools.cgrep.get_services", "completion": "  port, protocol = options.port\n  results = []\n  for svc in db.GetServiceNames():\n    for port_proto in db.GetService(svc):\n      if port_proto.port == port and port_proto.protocol == protocol:\n        results.append(svc)\n  return port, protocol, results\n\n", "idx": 223}
{"namespace": "asyncssh.packet.String", "completion": "    if isinstance(value, str):\n        value = value.encode('utf-8')\n    return UInt32(len(value)) + value\n\n", "idx": 224}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_cmd_counts", "completion": "    # Add 1 to each of the counts, including the unk_token, to handle unseen commands.\n    for cmd in seq1_counts:\n        seq1_counts[cmd] += 1\n    for cmd1 in seq2_counts:\n        for cmd2 in seq2_counts[cmd1]:\n            seq2_counts[cmd1][cmd2] += 1\n\n    # Add the unk_token to the counts.\n    seq1_counts[unk_token] = 1\n    seq2_counts[unk_token] = {unk_token: 1}\n\n    # Add the start_token and end_token to the counts.\n    seq1_counts[start_token] = 1\n    seq1_counts[end_token] = 1\n    seq2_counts[start_token] = {start_token: 1}\n    seq2_counts[end_token] = {end_token: 1}\n\n    return seq1_counts, seq2_counts\n\n", "idx": 225}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_param_counts", "completion": "    param_counts_ls = copy.deepcopy(param_counts)\n    cmd_param_counts_ls = copy.deepcopy(cmd_param_counts)\n\n    for cmd in cmds:\n        for param in param_counts_ls.keys():\n            param_counts_ls[param] += 1\n            cmd_param_counts_ls[cmd][param] += 1\n\n    return param_counts_ls, cmd_param_counts_ls", "idx": 226}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_value_counts", "completion": "    value_counts_ls = copy.deepcopy(value_counts)\n    param_value_counts_ls = copy.deepcopy(param_value_counts)\n\n    values: List[str] = list(value_counts.keys()) + [unk_token]\n    for param in params:\n        for value in values:\n            if value in param_value_counts_ls[param] or value == unk_token:\n                value_counts_ls[value] += 1\n                param_value_counts_ls[param][value] += 1\n\n    return value_counts_ls, param_value_counts_ls", "idx": 227}
{"namespace": "diffprivlib.validation.check_epsilon_delta", "completion": "    if not isinstance(epsilon, Real):\n        raise TypeError(\"Epsilon must be numeric\")\n    if not isinstance(delta, Real):\n        raise TypeError(\"Delta must be numeric\")\n    if epsilon < 0:\n        raise ValueError(\"Epsilon must be non-negative\")\n    if delta < 0 or delta > 1:\n        raise ValueError(\"Delta must be in [0, 1]\")\n    if not allow_zero and epsilon == 0 and delta == 0:\n        raise ValueError(\"Epsilon and Delta cannot both be zero\")\n\n", "idx": 228}
{"namespace": "diffprivlib.utils.check_random_state", "completion": "    if seed is None:\n        if secure:\n            return secrets.SystemRandom()\n        else:\n            return np.random.mtrand._rand\n    if isinstance(seed, (int, np.integer)):\n        return np.random.RandomState(seed)\n    if isinstance(seed, (np.random.RandomState, secrets.SystemRandom)):\n        return seed\n    raise ValueError('%r cannot be used to seed a numpy.random.RandomState or secrets.SystemRandom instance' % seed)\n\n", "idx": 229}
{"namespace": "diffprivlib.validation.clip_to_norm", "completion": "    if not isinstance(array, np.ndarray):\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\n    if array.ndim != 2:\n        raise ValueError(f\"Input array must be 2-dimensional, got {array.ndim} dimensions.\")\n    if not isinstance(clip, Real):\n        raise TypeError(f\"Clip value must be numeric, got {type(clip)}.\")\n    if clip <= 0:\n        raise ValueError(f\"Clip value must be strictly positive, got {clip}.\")\n\n    norms = np.linalg.norm(array, axis=1)\n    norms[norms > clip] = clip\n    array = array / norms[:, None]\n\n    return array\n\n", "idx": 230}
{"namespace": "diffprivlib.models.pca.PCA.fit_transform", "completion": "        self.fit(X)\n        return self.transform(X)\n", "idx": 231}
{"namespace": "discord.utils.get_slots", "completion": "    for base in cls.__mro__[1:]:\n        for slot in base.__slots__:\n            yield slot\n\n", "idx": 232}
{"namespace": "discord.utils.is_inside_class", "completion": "    return '.' in getattr(func, '__qualname__', '')\n\n", "idx": 233}
{"namespace": "faker.utils.decorators.slugify", "completion": "    def wrapper(*args, **kwargs):\n        result = fn(*args, **kwargs)\n        return text.slugify(result)\n\n    return wrapper\n\n", "idx": 234}
{"namespace": "faker.utils.decorators.slugify_domain", "completion": "    def wrapper(*args: Tuple[T, ...], **kwargs: Dict[str, T]) -> str:\n        return text.slugify(fn(*args, **kwargs), allow_dots=True)\n\n    return wrapper", "idx": 235}
{"namespace": "faker.utils.decorators.slugify_unicode", "completion": "    def wrapper(*args: Tuple[T, ...], **kwargs: Dict[str, T]) -> str:\n        return text.slugify(fn(*args, **kwargs), allow_unicode=True)\n\n    return wrapper", "idx": 236}
{"namespace": "faker.utils.loading.get_path", "completion": "    if hasattr(sys, 'frozen'):\n\n        if hasattr(sys, '_MEIPASS'):\n\n            return sys._MEIPASE\n\n        return sys.executable\n\n    return module.__file__\n\n", "idx": 237}
{"namespace": "faker.utils.checksums.luhn_checksum", "completion": "    digits = [int(digit) for digit in str(number)]\n    odd_digits = digits[-1::-2]\n    even_digits = digits[-2::-2]\n    checksum = sum(odd_digits) + sum(even_digit * 2 for even_digit in even_digits) % 10\n    return checksum\n\n", "idx": 238}
{"namespace": "faker.utils.datasets.add_ordereddicts", "completion": "    return OrderedDict(chain.from_iterable(odict.items() for odict in odict))", "idx": 239}
{"namespace": "faker.providers.person.pl_PL.checksum_identity_card_number", "completion": "    if len(characters) != 9:\n        raise ValueError(\"The length of the characters must be 9.\")\n\n    weights = [7, 3, 1, 0, 7, 3, 1, 7, 3]\n    sum_of_products = sum(int(c) * w for c, w in zip(characters, weights))\n    remainder = sum_of_products % 10\n    return 10 - remainder\n\n", "idx": 240}
{"namespace": "faker.providers.company.pl_PL.regon_checksum", "completion": "    weights = [8, 9, 2, 3, 4, 5, 6, 7]\n    sum_ = 0\n    for i, digit in enumerate(digits):\n        sum_ += digit * weights[i]\n    return sum_ % 11\n\n", "idx": 241}
{"namespace": "faker.providers.company.ru_RU.calculate_checksum", "completion": "    checksum = 0\n    for i in range(len(value)):\n        checksum += int(value[i]) * [3, 7, 2, 4, 10, 3, 5, 9, 4, 6, 8][i]\n    return str(checksum % 11)\n\n", "idx": 242}
{"namespace": "faker.providers.company.pl_PL.local_regon_checksum", "completion": "    weights_for_check_digit = [2, 4, 8, 5, 0, 9, 7, 3, 6, 1, 2, 4, 8]\n    check_digit = 0\n\n    for i in range(0, 13):\n        check_digit += weights_for_check_digit[i] * digits[i]\n\n    check_digit %= 11\n\n    if check_digit == 10:\n        check_digit = 0\n\n    return check_digit\n\n", "idx": 243}
{"namespace": "faker.providers.company.pl_PL.company_vat_checksum", "completion": "    weights_for_check_digit = [6, 5, 7, 2, 3, 4, 5, 6, 7]\n    check_digit = 0\n\n    for i in range(0, 9):\n        check_digit += weights_for_check_digit[i] * digits[i]\n\n    check_digit %= 11\n\n    if check_digit == 10:\n        check_digit = 0\n\n    return check_digit\n\n", "idx": 244}
{"namespace": "faker.providers.company.pt_BR.company_id_checksum", "completion": "    weights = [6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 2]\n    checksum = sum(d * w for d, w in zip(digits, weights)) % 11\n    return [checksum]\n\n", "idx": 245}
{"namespace": "faker.providers.misc.Provider.binary", "completion": "        if self.generator.random.getstate()[1] is None:\n            return self.generator.random.getrandbits(length * 8).to_bytes(length, \"big\")\n        else:\n            return self.generator.random.getrandbits(length * 8).to_bytes(length, \"big\")\n", "idx": 246}
{"namespace": "faker.providers.python.Provider.pystr", "completion": "        if min_chars is None:\n            min_chars = 0\n        if min_chars < 0:\n            raise ValueError(\"min_chars must be greater than or equal to 0\")\n        if max_chars < 0:\n            raise ValueError(\"max_chars must be greater than or equal to 0\")\n        if min_chars > max_chars:\n            raise ValueError(\"min_chars must be less than or equal to max_chars\")\n\n        length = self.random_int(min_chars, max_chars)\n        return prefix + self.random_letters(length) + suffix\n", "idx": 247}
{"namespace": "dash._utils.AttributeDict.set_read_only", "completion": "        if not hasattr(self, \"_read_only\"):\n            self._read_only = {}\n        for name in names:\n            self._read_only[name] = msg\n", "idx": 248}
{"namespace": "dash._utils.AttributeDict.first", "completion": "        if not names:\n            names = self.keys()\n        for name in names:\n            if name in self:\n                return self[name]\n\n", "idx": 249}
{"namespace": "dash._get_paths.app_get_asset_url", "completion": "    if config.assets_external_path:\n        return config.assets_external_path + path\n    else:\n        return config.requests_pathname_prefix + path\n\n", "idx": 250}
{"namespace": "peewee.sort_models", "completion": "    # First collect the list of model classes.\n    model_classes = [type(model) for model in models]\n\n    # Now collect the list of model instances for each model class.\n    model_instances = [[] for _ in model_classes]\n    for model in models:\n        model_instances[model_classes.index(type(model))].append(model)\n\n    # Now collect the list of model classes for each model class.\n    model_class_graph = [[] for _ in model_classes]\n    for model_class in model_classes:\n        model_class_graph[model_classes.index(model_class)].append(\n            model_class._meta.model_graph())\n\n    # Now collect the list of model classes for each model class.\n    model_class_graph = [[] for _ in model_classes]\n    for model_class in model_classes:\n        model_class_graph[model_classes.index(model_class)].append(\n            model_class._meta.model_graph())\n\n    # Now collect the list of model classes for each model class.\n    model_class_graph = [[] for _ in model_classes]\n    for model_class in model_classes:\n        model_class_graph[model_classes.index(model_class)].append(\n            model_class._meta.model_graph())\n\n    # Now collect the list of model classes for each model class.\n    model_class_graph = [[] for _ in model_classes]\n    for model_class in model_classes:\n        model_class_graph[model_classes.index(model_class)].append(\n            model_class._meta.model_graph())\n\n    # Now collect the list of model classes for each model class.\n    model_class_graph = [[] for _ in model_classes]\n    for model_class in model_classes:\n        model_class_graph[model_classes.index(model_class)].append(\n            model_class._meta.model_graph())\n\n    # Now collect the list of model classes for each model class.\n    model_class_graph = [[] for _ in model_classes]\n    for model_class in model_classes:\n        model_class_graph[model_classes.index(model_class)].append(\n            model_class._meta.model_graph())\n\n   ", "idx": 251}
{"namespace": "dash._grouping.grouping_len", "completion": "    return len(flatten_grouping(grouping))\n\n", "idx": 252}
{"namespace": "playhouse.kv.KeyValue.get", "completion": "        try:\n            return self[key]\n        except KeyError:\n            return default\n", "idx": 253}
{"namespace": "playhouse.kv.KeyValue.setdefault", "completion": "        try:\n            return self[key]\n        except KeyError:\n            self[key] = default\n            return default\n", "idx": 254}
{"namespace": "sslyze.plugins.certificate_info._certificate_utils.get_public_key_sha256", "completion": "    public_key = certificate.public_key()\n    public_key_bytes = public_key.public_bytes(Encoding.DER, PublicFormat.SubjectPublicKeyInfo)\n    return sha256(public_key_bytes).digest()", "idx": 255}
{"namespace": "ydata_profiling.compare_reports._compare_title", "completion": "    if len(titles) == 0:\n        return \"\"\n    elif len(titles) == 1:\n        return titles[0]\n    else:\n        if all(titles[0] == title for title in titles):\n            return titles[0]\n        else:\n            return \"Comparison of \" + \", \".join(titles)\n\n", "idx": 256}
{"namespace": "ydata_profiling.report.formatters.fmt_bytesize", "completion": "    for unit in [\"\", \"Ki\", \"Mi\", \"Gi\", \"Ti\", \"Pi\", \"Ei\", \"Zi\"]:\n        if abs(num) < 1024.0:\n            return f\"{num:3.1f}{unit}{suffix}\"\n        num /= 1024.0\n    return f\"{num:.1f}Yi{suffix}\"\n\n", "idx": 257}
{"namespace": "ydata_profiling.report.formatters.fmt_percent", "completion": "    if edge_cases:\n        if value == 0:\n            return \"0%\"\n        elif value == 1:\n            return \"100%\"\n        elif value > 1:\n            return \">100%\"\n        elif value < 0:\n            return \"<0%\"\n\n    return f\"{value * 100:.1f}%\"\n\n", "idx": 258}
{"namespace": "ydata_profiling.report.formatters.fmt_numeric", "completion": "    if isinstance(value, np.ndarray):\n        value = value.item()\n    if isinstance(value, np.generic):\n        value = np.asscalar(value)\n    if isinstance(value, pd.Series):\n        value = value.item()\n    if isinstance(value, pd.DataFrame):\n        value = value.iloc[0, 0]\n    if isinstance(value, pd.Timestamp):\n        value = value.to_pydatetime()\n    if isinstance(value, pd.Timedelta):\n        value = value.total_seconds()\n    if isinstance(value, pd.Interval):\n        value = value.mid\n    if isinstance(value, pd.core.arrays.categorical.Categorical):\n        value = value.item()\n    if isinstance(value, pd.core.arrays.integer.IntegerArray):\n        value = value.item()\n    if isinstance(value, pd.core.arrays.floating.FloatingArray):\n        value = value.item()\n    if isinstance(value, pd.core.arrays.boolean.BooleanArray):\n        value = value.item()\n    if isinstance(value, pd.core.arrays.datetimelike.DatetimeArray):\n        value = value.item()\n    if isinstance(value, pd.core.arrays.datetimelike.TimedeltaArray):\n        value = value.item()\n    if isinstance(value, pd.core.arrays.period.PeriodArray):\n        value = value.item()\n    if isinstance(value, pd.core.arrays.string_.StringArray):\n        value = value.item()\n    if isinstance(value, pd.core.arrays.masked.BaseMaskedArray):\n        value = value.item()\n    if isinstance(value, pd.core.arrays.sparse.BaseSparseArray):\n        value = value.item()\n    if isinstance(value, pd.core.arrays.numpy_.BaseMaskedArray):\n        value = value.item()\n    if isinstance(value, pd.core.arrays.numpy_.BaseExtensionArray):\n        value = value.item()\n    if isinstance(value, pd.core.arrays.integer.IntegerArray):\n        value = value.item()\n    if isinstance(value, pd.core.arrays.floating.FloatingArray):\n        value = value.item()\n    if isinstance(value, pd.core.arrays.boolean.BooleanArray):\n        value = value.", "idx": 259}
{"namespace": "ydata_profiling.report.formatters.fmt_array", "completion": "    if threshold is np.nan:\n        return str(value)\n    else:\n        if value.size > threshold:\n            return str(value[:threshold]) + \"...\"\n        else:\n            return str(value)\n\n", "idx": 260}
{"namespace": "ydata_profiling.report.formatters.fmt_monotonic", "completion": "    if value == 0:\n        return \"No\"\n    elif value == 1:\n        return \"Yes\"\n    else:\n        return \"Unknown\"\n\n", "idx": 261}
{"namespace": "ydata_profiling.visualisation.plot._plot_pie_chart", "completion": "    # Use the pd.Series indices as category names\n    labels = data.index.values.astype(str)\n\n    # Plot\n    _, ax = plt.subplots(figsize=(7, 2))\n    ax.axis(\"off\")\n\n    # Add a pie chart\n    wedges, _, autotexts = ax.pie(\n        data,\n        colors=colors,\n        autopct=lambda pct: f\"{pct:.1f}%\\n({pct * sum(data) / 100:.0f})\",\n        textprops={\"fontsize\": \"x-large\", \"fontweight\": \"bold\"},\n    )\n\n    # Label color depends on the darkness of the wedge\n    for wedge, autotext in zip(wedges, autotexts):\n        r, g, b, _ = wedge.get_facecolor()\n        text_color = \"white\" if r * g * b < 0.5 else \"darkgrey\"\n        autotext.set_color(text_color)\n\n    legend = None\n    if not hide_legend:\n        legend = ax.legend(\n            labels,\n            ncol=1,\n            bbox_to_anchor=(0, 0),\n            fontsize=\"xx-large\",\n            loc=\"upper left\",\n        )\n\n    return ax, legend\n\n", "idx": 262}
{"namespace": "ydata_profiling.visualisation.plot._prepare_heatmap_data", "completion": "    if sortby is not None:\n        dataframe = dataframe.sort_values(by=sortby)\n\n    if selected_entities is not None:\n        dataframe = dataframe[dataframe[entity_column].isin(selected_entities)]\n\n    if len(dataframe) > max_entities:\n        dataframe = dataframe.head(max_entities)\n\n    return dataframe\n\n", "idx": 263}
{"namespace": "ydata_profiling.visualisation.plot._create_timeseries_heatmap", "completion": "    fig = plt.figure(figsize=figsize)\n    ax = fig.add_subplot(111)\n    ax.set_title(\"Heatmap\")\n    ax.set_xlabel(\"Entities\")\n    ax.set_ylabel(\"Bins\")\n    ax.set_xticks(range(len(df.columns)))\n    ax.set_yticks(range(len(df.index)))\n    ax.set_xticklabels(df.columns)\n    ax.set_yticklabels(df.index)\n    ax.imshow(df, cmap=\"Blues\")\n    return ax\n\n", "idx": 264}
{"namespace": "ydata_profiling.model.expectation_algorithms.generic_expectations", "completion": "    if name not in summary:\n        raise ValueError(f\"Column {name} not found in summary\")\n\n    if summary[name][\"unique\"] == 0:\n        raise ValueError(f\"Column {name} has no unique values\")\n\n    if summary[name][\"missing\"] > 0:\n        raise ValueError(f\"Column {name} has missing values\")\n\n    return name, summary, batch", "idx": 265}
{"namespace": "ydata_profiling.model.expectation_algorithms.numeric_expectations", "completion": "    batch.expect_column_to_be_numeric(name)\n\n    if summary[\"n_missing\"] == 0:\n        batch.expect_column_values_to_be_in_set(name, [0, 1])\n\n    if summary[\"p_unique\"] == 1.0:\n        batch.expect_column_values_to_be_unique(name)\n\n    return name, summary, batch\n\n", "idx": 266}
{"namespace": "ydata_profiling.model.expectation_algorithms.categorical_expectations", "completion": "    if summary[\"n_distinct\"] < 100:\n        batch.expect_column_values_to_be_in_set(\n            name,\n            list(summary[\"value_counts\"].keys()),\n            result_format=\"COMPLETE\",\n            meta={\n                \"notes\": {\n                    \"format\": \"markdown\",\n                    \"content\": [\n                        \"The column values should be in the set of value counts.\"\n                    ],\n                }\n            },\n        )\n\n    if summary[\"p_distinct\"] < 0.9:\n        batch.expect_column_values_to_be_in_set(\n            name,\n            list(summary[\"value_counts\"].keys()),\n            result_format=\"COMPLETE\",\n            meta={\n                \"notes\": {\n                    \"format\": \"markdown\",\n                    \"content\": [\n                        \"The column values should be in the set of value counts.\"\n                    ],\n                }\n            },\n        )\n\n    return name, summary, batch\n\n", "idx": 267}
{"namespace": "ydata_profiling.model.expectation_algorithms.datetime_expectations", "completion": "    if \"min\" in summary and \"max\" in summary:\n        batch.expect_column_values_to_be_between(\n            name, min_value=summary[\"min\"], max_value=summary[\"max\"]\n        )\n\n    return name, summary, batch\n\n", "idx": 268}
{"namespace": "ydata_profiling.model.expectation_algorithms.file_expectations", "completion": "    return name, summary, batch\n\n", "idx": 269}
{"namespace": "ydata_profiling.model.pandas.describe_categorical_pandas.word_summary_vc", "completion": "    # Word Summaries\n    words = vc.index.str.split(expand=True).stack()\n    words = words[words != \"\"]\n    words = words.str.lower()\n    words = words[~words.isin(stop_words)]\n    words = words.groupby(level=0, sort=False).apply(lambda x: \" \".join(x))\n    words = words.value_counts()\n    words = words.sort_values(ascending=False)\n    words = words.rename(\"count\")\n\n    return {\"words\": words}\n\n", "idx": 270}
{"namespace": "ydata_profiling.model.pandas.imbalance_pandas.column_imbalance_score", "completion": "    if n_classes == 2:\n        return 1 - abs(value_counts[0] / value_counts[1] - 1)\n    else:\n        return entropy(value_counts) / log2(n_classes)", "idx": 271}
{"namespace": "django.core.exceptions.ValidationError.messages", "completion": "        if hasattr(self, \"error_dict\"):\n            return sum(self.error_dict.values(), [])\n        return self.error_list\n", "idx": 272}
{"namespace": "django.utils.module_loading.module_has_submodule", "completion": "    if not (\n        importlib_find(package)\n        and importlib_find(package)\n        .parent\n        .joinpath(module_name)\n        .is_file()\n    ):\n        return False\n    try:\n        import_module(\"{}.{}\".format(package, module_name))\n    except ImportError:\n        return False\n    return True\n\n", "idx": 273}
{"namespace": "django.utils.timezone.get_fixed_timezone", "completion": "    if isinstance(offset, timedelta):\n        offset = offset.total_seconds() / 60\n\n    return timezone(timedelta(minutes=offset))\n\n", "idx": 274}
{"namespace": "django.utils.encoding.filepath_to_uri", "completion": "    return iri_to_uri(path)\n\n", "idx": 275}
{"namespace": "django.utils._os.to_path", "completion": "    if isinstance(value, Path):\n        return value\n    return Path(value)\n\n", "idx": 276}
{"namespace": "django.utils.lorem_ipsum.sentence", "completion": "    # Generate a random number of words\n    num_words = random.randint(1, 10)\n\n    # Generate a random number of commas\n    num_commas = random.randint(0, num_words - 1)\n\n    # Generate a random number of spaces\n    num_spaces = num_words + num_commas\n\n    # Generate a random number of periods\n    num_periods = random.randint(0, 1)\n\n    # Generate a random number of question marks\n    num_question_marks = random.randint(0, 1)\n\n    # Generate a random number of exclamation points\n    num_exclamation_points = random.randint(0, 1)\n\n    # Generate a random number of question marks\n    num_question_marks = random.randint(0, 1)\n\n    # Generate a random number of exclamation points\n    num_exclamation_points = random.randint(0, 1)\n\n    # Generate a random number of question marks\n    num_question_marks = random.randint(0, 1)\n\n    # Generate a random number of exclamation points\n    num_exclamation_points = random.randint(0, 1)\n\n    # Generate a random number of question marks\n    num_question_marks = random.randint(0, 1)\n\n    # Generate a random number of exclamation points\n    num_exclamation_points = random.randint(0, 1)\n\n    # Generate a random number of question marks\n    num_question_marks = random.randint(0, 1)\n\n    # Generate a random number of exclamation points\n    num_exclamation_points = random.randint(0, 1)\n\n    # Generate a random number of question marks\n    num_question_marks = random.randint(0, 1)\n\n    # Generate a random number of exclamation points\n    num_exclamation_points = random.randint(0, 1)\n\n    # Generate a random number of question marks\n    num_question_marks = random.randint(0, 1)\n\n    # Generate a random number of exclamation points\n    num_exclamation_points = random.randint(0, 1)\n\n    # Generate a random number of question marks\n    num_question_marks = random.randint(0, 1)\n\n    # Generate a random number of exclamation points\n    num_exclamation_points =", "idx": 277}
{"namespace": "ydata_profiling.utils.dataframe.sort_column_names", "completion": "    if sort is None:\n        return dct\n    elif sort == \"ascending\":\n        return {k: v for k, v in sorted(dct.items(), key=lambda item: item[0])}\n    elif sort == \"descending\":\n        return {k: v for k, v in sorted(dct.items(), key=lambda item: item[0], reverse=True)}\n    else:\n        raise ValueError(\n            \"The sort parameter must be either 'ascending', 'descending' or None.\"\n        )", "idx": 278}
{"namespace": "django.utils.ipv6.is_valid_ipv6_address", "completion": "    try:\n        ipaddress.IPv6Address(ip_str)\n    except ValueError:\n        return False\n    return True", "idx": 279}
{"namespace": "django.utils.http.urlsafe_base64_decode", "completion": "    s = s.encode(\"ascii\")\n    s += b\"=\" * (4 - len(s) % 4)\n    return base64.urlsafe_b64decode(s)\n\n", "idx": 280}
{"namespace": "django.utils.http.parse_etags", "completion": "    if not etag_str:\n        return []\n\n    etags = []\n    for etag in etag_str.split(\",\"):\n        etag = etag.strip()\n        if etag == \"*\":\n            return [\"*\"]\n        match = ETAG_MATCH.match(etag)\n        if match:\n            etags.append(match.group(1))\n\n    return etags\n\n", "idx": 281}
{"namespace": "django.utils.http.is_same_domain", "completion": "    if pattern == \"*\":\n        return True\n    if pattern.startswith(\"*.\"):\n        return host.endswith(pattern[1:])\n    return host == pattern\n\n", "idx": 282}
{"namespace": "django.utils.http.content_disposition_header", "completion": "    if as_attachment:\n        header_value = \"attachment\"\n    else:\n        header_value = \"inline\"\n    if filename:\n        header_value += \"; filename=\" + quote(filename)\n    return header_value\n\n", "idx": 283}
{"namespace": "pysnooper.utils.truncate", "completion": "    if len(string) <= max_length:\n        return string\n    else:\n        return '...' + string[len(string) - max_length + 3:] + '...'\n\n", "idx": 284}
{"namespace": "pysnooper.variables.needs_parentheses", "completion": "    return utils.get_bytecode(source) != utils.get_bytecode(f'({source})')\n\n", "idx": 285}
{"namespace": "django.test.utils.extend_sys_path", "completion": "    old_path = sys.path\n    sys.path = list(old_path)\n    sys.path.extend(paths)\n    try:\n        yield\n    finally:\n        sys.path = old_path\n\n", "idx": 286}
{"namespace": "albumentations.augmentations.functional.normalize_cv2", "completion": "    if mean.shape == (3,):\n        mean = np.reshape(mean, (1, 1, 3))\n    if denominator.shape == (3,):\n        denominator = np.reshape(denominator, (1, 1, 3))\n    return (img - mean) / denominator\n\n", "idx": 287}
{"namespace": "albumentations.augmentations.functional.normalize_numpy", "completion": "    if mean.shape and len(mean) != 4 and mean.shape != img.shape:\n        mean = np.array(mean.tolist() + [0] * (4 - len(mean)), dtype=np.float64)\n    if not denominator.shape:\n        denominator = np.array([denominator.tolist()] * 4, dtype=np.float64)\n    elif len(denominator) != 4 and denominator.shape != img.shape:\n        denominator = np.array(denominator.tolist() + [1] * (4 - len(denominator)), dtype=np.float64)\n\n    img = np.ascontiguousarray(img.astype(\"float32\"))\n    img -= mean.astype(np.float64)\n    img *= denominator.astype(np.float64)\n    return img\n\n", "idx": 288}
{"namespace": "albumentations.augmentations.functional.gamma_transform", "completion": "    non_rgb_warning(img)\n\n    input_dtype = img.dtype\n    needs_float = False\n\n    if input_dtype == np.float32:\n        img = from_float(img, dtype=np.dtype(\"uint8\"))\n        needs_float = True\n    elif input_dtype not in (np.uint8, np.float32):\n        raise ValueError(\"Unexpected dtype {} for GammaTransform augmentation\".format(input_dtype))\n\n    image_hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n    image_hls[:, :, 1] = image_hls[:, :, 1] ** gamma\n    image_rgb = cv2.cvtColor(image_hls, cv2.COLOR_HLS2RGB)\n\n    if needs_float:\n        image_rgb = to_float(image_rgb, max_value=255)\n\n    return image_rgb\n\n", "idx": 289}
{"namespace": "albumentations.augmentations.functional.swap_tiles_on_image", "completion": "    non_rgb_warning(image)\n\n    input_dtype = image.dtype\n    needs_cast = False\n\n    if input_dtype == np.float32:\n        image = from_float(image, dtype=np.dtype(\"uint8\"))\n        needs_cast = True\n    elif input_dtype not in (np.uint8, np.float32):\n        raise ValueError(\"Unexpected dtype {} for SwapTilesOnImage augmentation\".format(input_dtype))\n\n    output = image.copy()\n\n    for (x0, y0, h0, w0), (x1, y1, h1, w1) in tiles:\n        output[x0:x0 + w0, y0:y0 + h0] = image[x1:x1 + w1, y1:y1 + h1]\n\n    if needs_cast:\n        output = to_float(output, max_value=255)\n\n    return output\n\n", "idx": 290}
{"namespace": "albumentations.augmentations.geometric.functional.keypoint_rotate", "completion": "    x, y, angle, scale = keypoint[:4]\n\n    # Calculate the center of the image\n    center_x = cols / 2\n    center_y = rows / 2\n\n    # Calculate the new position of the keypoint after rotation\n    new_x = (x - center_x) * np.cos(angle) - (y - center_y) * np.sin(angle) + center_x\n    new_y = (x - center_x) * np.sin(angle) + (y - center_y) * np.cos(angle) + center_y\n\n    # Update the keypoint with the new position\n    keypoint = (new_x, new_y, angle, scale)\n\n    return keypoint\n\n", "idx": 291}
{"namespace": "albumentations.augmentations.geometric.functional.keypoint_shift_scale_rotate", "completion": "    x, y, a, s = keypoint[:4]\n    center = (cols - 1) * 0.5, (rows - 1) * 0.5\n    matrix = cv2.getRotationMatrix2D(center, angle, scale)\n    x, y = cv2.transform(np.array([[[x, y]]]), matrix).squeeze()\n    x += dx * cols\n    y += dy * rows\n    return x, y, a + math.radians(angle), s * scale\n\n", "idx": 292}
{"namespace": "albumentations.core.keypoints_utils.angle_to_2pi_range", "completion": "    return angle % (2 * math.pi)\n\n", "idx": 293}
{"namespace": "albumentations.augmentations.geometric.functional.rot90", "completion": "    if factor == 0:\n        return img\n    elif factor > 0:\n        return np.ascontiguousarray(np.rot90(img, factor))\n    else:\n        raise ValueError(f\"Expected positive factor, got {factor}.\")\n\n", "idx": 294}
{"namespace": "albumentations.core.keypoints_utils.convert_keypoints_to_albumentations", "completion": "    return [\n        convert_keypoint_to_albumentations(\n            keypoint, source_format, rows, cols, check_validity=check_validity, angle_in_degrees=angle_in_degrees\n        )\n        for keypoint in keypoints\n    ]\n\n", "idx": 295}
{"namespace": "albumentations.core.keypoints_utils.convert_keypoints_from_albumentations", "completion": "    return [\n        convert_keypoint_from_albumentations(kp, target_format, rows, cols, check_validity, angle_in_degrees)\n        for kp in keypoints\n    ]", "idx": 296}
{"namespace": "albumentations.core.transforms_interface.to_tuple", "completion": "    if isinstance(param, (list, tuple)):\n        if len(param) == 1:\n            param = (param[0], param[0])\n        elif len(param) == 2:\n            param = (param[0], param[1])\n        else:\n            raise ValueError(\"Input argument should be a scalar or a tuple of 2 elements.\")\n    elif isinstance(param, (int, float)):\n        param = (param, param)\n    else:\n        raise ValueError(\"Input argument should be a scalar or a tuple of 2 elements.\")\n\n    if bias is not None:\n        param = (param[0] + bias, param[1] + bias)\n\n    if low is not None:\n        param = (low, param[1])\n\n    return param\n\n", "idx": 297}
{"namespace": "albumentations.core.composition.ReplayCompose.replay", "completion": "        if saved_augmentations is None:\n            raise ValueError(\"You must pass saved_augmentations to replay the saved augmentations.\")\n        if not isinstance(saved_augmentations, dict):\n            raise ValueError(\"saved_augmentations must be a dictionary.\")\n        if \"transforms\" not in saved_augmentations:\n            raise ValueError(\"saved_augmentations must contain 'transforms' key.\")\n        if \"params\" not in saved_augmentations:\n            raise ValueError(\"saved_augmentations must contain 'params' key.\")\n        if \"id\" not in saved_augmentations:\n            raise ValueError(\"saved_augmentations must contain 'id' key.\")\n        if \"additional_targets\" not in saved_augmentations:\n            raise ValueError(\"saved_augmentations must contain 'additional_targets' key.\")\n        if \"bbox_params\" not in saved_augmentations:\n            raise ValueError(\"saved_augmentations must contain 'bbox_params' key.\")\n        if \"keypoint_params\" not in saved_augmentations:\n            raise ValueError(\"saved_augmentations must contain 'keypoint_params' key.\")\n        if \"is_check_shapes\" not in saved_augmentations:\n            raise ValueError(\"saved_augmentations must contain 'is_check_shapes' key.\")\n        if \"p\" not in saved_augmentations:\n            raise ValueError(\"saved_augmentations must contain 'p' key.\")\n\n        transforms = saved_augmentations[\"transforms\"]\n        params = saved_augmentations[\"params\"]\n        id = saved_augmentations[\"id\"]\n        additional_targets = saved_augmentations[\"additional_targets\"]\n        bbox_params = saved_augmentations[\"bbox_params\"]\n        keypoint_params = saved_augmentations[\"keypoint_params\"]\n        is_check_shapes = saved_augmentations[\"is_check_shapes\"]\n        p = saved_augmentations[\"p\"]\n\n        replay_transforms = []\n        for t in transforms:\n            if t[\"id\"] == id:\n                replay_transforms.append(t)\n\n        replay_params = []\n        for p in params:\n            if p[\"id\"] == id:\n                replay_params.append(p)\n\n        replay_additional_targets = []\n        for t in additional_targets:\n            if t[\"id\"] == id:\n                replay_additional_targets.append(t)\n\n        replay_bbox_params = []\n        for p in", "idx": 298}
{"namespace": "albumentations.core.serialization.shorten_class_name", "completion": "    if class_fullname.startswith(\"albumentations.\"):\n        return class_fullname[len(\"albumentations.\") :]\n    return class_fullname\n\n", "idx": 299}
{"namespace": "wandb.util.to_forward_slash_path", "completion": "    if platform.system() == \"Windows\":\n        return path.replace(\"\\\\\", \"/\")\n    return path\n\n", "idx": 300}
{"namespace": "wandb.util.make_artifact_name_safe", "completion": "    return re.sub(\n        r\"[^0-9A-Za-z\\-_\\.]\", \"_\", name[:128]\n    )  # type: ignore[no-any-return]\n\n", "idx": 301}
{"namespace": "wandb.sdk.wandb_settings._redact_dict", "completion": "    return {k: redact_str if k in unsafe_keys else v for k, v in d.items()}\n\n", "idx": 302}
{"namespace": "wandb.sdk.launch.builder.build.get_current_python_version", "completion": "    return sys.version_info[0], sys.version_info[0] + \".\" + sys.version_info[1]\n\n", "idx": 303}
{"namespace": "wandb.sdk.artifacts.storage_policy.StoragePolicy.lookup_by_name", "completion": "        for subclass in cls.__subclasses__():\n            if subclass.name == name:\n                return subclass\n        raise NotImplementedError(f\"No storage policy named {name}\")\n", "idx": 304}
{"namespace": "wandb.sdk.lib.runid.generate_id", "completion": "    return ''.join(secrets.choice(string.ascii_lowercase + string.digits) for _ in range(length))", "idx": 305}
{"namespace": "wandb.sdk.internal.file_stream.CRDedupeFilePolicy.get_consecutive_offsets", "completion": "        # Sort the offsets in ascending order\n        sorted_offsets = sorted(console.keys())\n\n        # Initialize an empty list to store the intervals\n        intervals = []\n\n        # Initialize a variable to store the start of the current interval\n        current_start = None\n\n        # Iterate over the sorted offsets\n        for offset in sorted_offsets:\n            # If this is the first offset or the previous offset is not consecutive, start a new interval\n            if current_start is None or offset != sorted_offsets[offset - 1] + 1:\n                current_start = offset\n\n            # If this is the last offset or the next offset is not consecutive, end the current interval\n            if offset == sorted_offsets[-1] or offset != sorted_offsets[offset + 1] - 1:\n                intervals.append((current_start, offset))\n\n        return intervals\n", "idx": 306}
{"namespace": "wandb.sdk.internal.system.assets.ipu.IPUStats.sample", "completion": "        devices = self._gc_ipu_info.getDevices()\n        for device in devices:\n            device_metrics = device.getMetrics()\n            device_metrics = {\n                key: value\n                for key, value in device_metrics.items()\n                if self.parse_metric(key, value)\n            }\n\n            device_metrics = {\n                key: value\n                for key, value in device_metrics.items()\n                if self._pid in value[\"pid\"]\n            }\n\n            if not device_metrics:\n                continue\n\n            device_name = device.getName()\n            if device_name not in self._devices_called:\n                self._devices_called.add(device_name)\n                for key, value in device_metrics.items():\n                    key, value = self.parse_metric(key, value)\n                    if key is None:\n                        continue\n                    wandb.log({self.name.format(device_name, key): value})\n\n            if not any(\n                key in self.variable_metric_keys for key in device_metrics.keys()\n            ):\n                continue\n\n            self.samples.append(device_metrics)\n", "idx": 307}
{"namespace": "csvkit.cleanup.join_rows", "completion": "    if len(rows) == 0:\n        return []\n\n    if len(rows) == 1:\n        return rows[0]\n\n    if len(rows) == 2:\n        return [rows[0][0]] + [joiner] + [rows[1][-1]]\n\n    if len(rows) > 2:\n        return [rows[0][0]] + [joiner] + join_rows(rows[1:], joiner)\n\n", "idx": 308}
{"namespace": "csvkit.convert.guess_format", "completion": "    import os\n\n    # Get the file extension\n    file_extension = os.path.splitext(filename)[1][1:]\n\n    # Check the file extension\n    if file_extension in ['csv', 'dbf', 'fixed', 'xls', 'xlsx', 'json']:\n        return file_extension\n    elif file_extension == 'js':\n        return 'json'\n    else:\n        return None\n\n", "idx": 309}
{"namespace": "folium.utilities.normalize", "completion": "    # Remove all spaces and newlines from the input string\n    rendered = rendered.replace(\" \", \"\")\n    rendered = rendered.replace(\"\\n\", \"\")\n\n    # Return the normalized string\n    return rendered\n\n", "idx": 310}
{"namespace": "tpot.gp_deap.initialize_stats_dict", "completion": "    individual.stats = defaultdict(list)\n    individual.stats['generation'] = []\n    individual.stats['mutation_count'] = []\n    individual.stats['crossover_count'] = []\n    individual.stats['predecessor'] = []\n\n", "idx": 311}
{"namespace": "bentoml_cli.env_manager.remove_env_arg", "completion": "    env_arg_pattern = r\"^--env(=)?\\s*(\\w+)?$\"\n    env_arg_regex = re.compile(env_arg_pattern)\n\n    updated_cmd_args = []\n    for arg in cmd_args:\n        if not env_arg_regex.match(arg):\n            updated_cmd_args.append(arg)\n\n    return updated_cmd_args\n\n", "idx": 312}
{"namespace": "bentoml._internal.utils.uri.path_to_uri", "completion": "    return quote(os.path.abspath(path))\n\n", "idx": 313}
{"namespace": "bentoml._internal.utils.uri.uri_to_path", "completion": "    parsed = urlparse(uri)\n    if parsed.scheme == \"file\":\n        return url2pathname(parsed.path)\n    raise ValueError(\"Unsupported URI scheme\")", "idx": 314}
{"namespace": "bentoml._internal.utils.validate_labels", "completion": "    if not isinstance(labels, dict):\n        raise ValueError(\"labels must be a dictionary\")\n    if not all(isinstance(k, str) for k in labels.keys()):\n        raise ValueError(\"labels keys must be strings\")\n    if not all(isinstance(v, str) for v in labels.values()):\n        raise ValueError(\"labels values must be strings\")\n\n", "idx": 315}
{"namespace": "bentoml._internal.configuration.helpers.is_valid_ip_address", "completion": "    try:\n        ipaddress.ip_address(addr)\n        return True\n    except ValueError:\n        return False\n\n", "idx": 316}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batches_to_batch", "completion": "        batch: ext.PdDataFrame = pd.concat(batches, axis=batch_dim)\n        indices = list(\n            itertools.accumulate(subbatch.shape[batch_dim] for subbatch in batches)\n        )\n        indices = [0] + indices\n        return batch, indices\n", "idx": 317}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batch_to_batches", "completion": "        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n        return [batch.iloc[indices[i] : indices[i + 1]] for i in range(len(indices) - 1)]\n", "idx": 318}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batches_to_batch", "completion": "        batch: list[t.Any] = []\n        indices: list[int] = [0]\n        for subbatch in batches:\n            batch.extend(subbatch)\n            indices.append(indices[-1] + len(subbatch))\n        return batch, indices\n", "idx": 319}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batch_to_batches", "completion": "        assert (\n            batch_dim == 0\n        ), \"Default Runner DataContainer does not support batch_dim other than 0\"\n        return [batch[indices[i] : indices[i + 1]] for i in range(len(indices) - 1)]\n", "idx": 320}
{"namespace": "jwt.utils.force_bytes", "completion": "    if isinstance(value, bytes):\n        return value\n    elif isinstance(value, str):\n        return value.encode(\"utf-8\")\n    else:\n        raise TypeError(f\"Unsupported type: {type(value)}\")\n\n", "idx": 321}
{"namespace": "pytube.cli.display_progress_bar", "completion": "    # Calculate the progress percentage\n    percent = bytes_received / filesize * 100\n    # Calculate the number of progress segments to display\n    num_segments = int(percent * scale)\n    # Calculate the number of empty segments to display\n    num_empty = int(100 * scale) - num_segments\n    # Create the progress bar string\n    progress_bar = f\"[{ch * num_segments}{' ' * num_empty}] {percent:.2f}%\"\n    # Print the progress bar to the terminal\n    print(progress_bar, end=\"\\r\")\n\n", "idx": 322}
{"namespace": "pytube.cli._download", "completion": "    if not target:\n        target = os.getcwd()\n    if not filename:\n        filename = stream.default_filename\n    filesize = stream.filesize\n    print(f\"Downloading {filename} ({round(filesize / 1024 / 1024, 2)} MB)\")\n    stream.download(\n        output_path=target,\n        filename=filename,\n        on_progress=on_progress,\n    )\n\n", "idx": 323}
{"namespace": "pytube.cli.display_streams", "completion": "    print(\"Available streams:\")\n    for stream in youtube.streams.all():\n        print(stream)\n\n", "idx": 324}
{"namespace": "pytube.cli._unique_name", "completion": "    if not target:\n        target = os.getcwd()\n    if not os.path.exists(target):\n        os.makedirs(target)\n    filename = f\"{base}.{subtype}\"\n    file_path = os.path.join(target, filename)\n    if os.path.exists(file_path):\n        i = 1\n        while True:\n            filename = f\"{base} ({i}).{subtype}\"\n            file_path = os.path.join(target, filename)\n            if not os.path.exists(file_path):\n                break\n            i += 1\n    return filename\n\n", "idx": 325}
{"namespace": "pytube.cli._print_available_captions", "completion": "    print(\"Available caption codes:\")\n    for caption in captions.all():\n        print(f\"{caption.code} - {caption.name}\")\n\n", "idx": 326}
{"namespace": "pytube.cipher.throttling_reverse", "completion": "    arr.reverse()\n    return arr\n\n", "idx": 327}
{"namespace": "pytube.helpers.setup_logger", "completion": "    logger = logging.getLogger(__name__)\n    logger.setLevel(level)\n    stream_handler = logging.StreamHandler()\n    stream_handler.setLevel(level)\n    logger.addHandler(stream_handler)\n    if log_filename:\n        file_handler = logging.FileHandler(log_filename)\n        file_handler.setLevel(level)\n        logger.addHandler(file_handler)\n\n", "idx": 328}
{"namespace": "pytube.helpers.deprecated", "completion": "    def decorator(func: Callable) -> Callable:\n        @functools.wraps(func)\n        def wrapper(*args: Any, **kwargs: Any) -> Any:\n            warnings.warn(\n                f\"{func.__name__} is deprecated. {reason}\",\n                category=DeprecationWarning,\n                stacklevel=2,\n            )\n            return func(*args, **kwargs)\n\n        return wrapper\n\n    return decorator\n\n", "idx": 329}
{"namespace": "pytube.helpers.uniqueify", "completion": "    seen = set()\n    seen_add = seen.add\n    return [x for x in duped_list if not (x in seen or seen_add(x))]\n\n", "idx": 330}
{"namespace": "pytube.helpers.target_directory", "completion": "    if output_path is None:\n        output_path = os.getcwd()\n    else:\n        output_path = os.path.abspath(output_path)\n\n    if not os.path.exists(output_path):\n        os.makedirs(output_path)\n\n    return output_path\n\n", "idx": 331}
{"namespace": "pytube.extract.is_private", "completion": "    unavailable_strings = [\n        'This video is unavailable.',\n        'This live stream is unavailable.',\n        'This live stream is no longer available.',\n        'This live stream is not available.',\n        'This live stream recording is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',\n        'This live stream is not available.',", "idx": 332}
{"namespace": "pymc.math.cartesian", "completion": "    arrays = [np.asarray(x) for x in arrays]\n    shape = (len(x) for x in arrays)\n    ix = np.indices(shape)\n    ix = ix.reshape(len(arrays), -1).T\n    for n, arr in enumerate(arrays):\n        ix[:, n] = arrays[n][ix[:, n]]\n    return ix\n\n", "idx": 333}
{"namespace": "pymc.math.log1mexp", "completion": "    if negative_input:\n        return pt.log1p(-pt.exp(x))\n    else:\n        return pt.log1p(-pt.exp(-x))\n\n", "idx": 334}
{"namespace": "pymc.math.log1mexp_numpy", "completion": "    if not negative_input:\n        warnings.warn(\n            \"pymc.math.log1mexp will expect a negative input in a future \"\n            \"version of PyMC.\\n To suppress this warning set `negative_input=True`\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        x = -x\n\n    return np.log1mexp(x)\n\n", "idx": 335}
{"namespace": "pymc.util.drop_warning_stat", "completion": "    # Check if the input InferenceData object is valid\n    if not isinstance(idata, arviz.InferenceData):\n        raise TypeError(\"The input idata must be an arviz.InferenceData object.\")\n\n    # Create a new InferenceData object to store the modified data\n    new_idata = arviz.InferenceData()\n\n    # Iterate over the groups in the input InferenceData object\n    for group in idata.groups():\n        # Check if the group is a sample stats group\n        if group.startswith(\"sample_stats\"):\n            # Get the group data\n            group_data = getattr(idata, group)\n            # Check if the \"warning\" stat is present in the group\n            if \"warning\" in group_data.coords:\n                # Remove the \"warning\" stat from the group data\n                group_data = group_data.drop_sel(warning=slice(None))\n            # Add the modified group data to the new InferenceData object\n            setattr(new_idata, group, group_data)\n        else:\n            # Add the group data to the new InferenceData object\n            setattr(new_idata, group, getattr(idata, group))\n\n    # Return the new InferenceData object\n    return new_idata\n\n", "idx": 336}
{"namespace": "pymc.pytensorf.walk_model", "completion": "    if stop_at_vars is None:\n        stop_at_vars = set()\n\n    for graph in graphs:\n        for var in walk(graph, stop_at_vars, expand_fn):\n            yield var\n\n", "idx": 337}
{"namespace": "pymc.testing.select_by_precision", "completion": "    if pytensor.config.floatX == \"float64\":\n        return float64\n    else:\n        return float32\n\n", "idx": 338}
{"namespace": "pymc.gp.cov.handle_args", "completion": "    @wraps(func)\n    def new_func(X: TensorLike, args: Optional[IntSequence] = None) -> TensorVariable:\n        if args is None:\n            return func(X)\n        else:\n            return func(X, *args)\n\n    return new_func", "idx": 339}
{"namespace": "pymc.gp.util.kmeans_inducing_points", "completion": "    # Initialize the inducing points using K-means\n    inducing_points = kmeans(X, n_inducing, **kmeans_kwargs)[0]\n\n    return inducing_points\n\n", "idx": 340}
{"namespace": "pymc.pytensorf.floatX", "completion": "    if isinstance(X, np.ndarray):\n        return X.astype(pytensor.config.floatX)\n    elif isinstance(X, Variable):\n        return X.astype(pytensor.config.floatX)\n    else:\n        raise TypeError(\"Expected numpy array or PyTensor tensor, got %s\" % type(X))\n\n", "idx": 341}
{"namespace": "pymc.distributions.multivariate.posdef", "completion": "    try:\n        np.linalg.cholesky(AA)\n        return True\n    except:\n        return False\n\n", "idx": 342}
{"namespace": "pymc.distributions.dist_math.multigammaln", "completion": "    if p < 1:\n        raise ValueError(\"The degrees of freedom should be greater than 0.\")\n    if p == 1:\n        return pt.log(a)\n    else:\n        return (p / 2.0) * pt.log(np.pi) + (p - 1) * pt.log(a) + multigammaln(a + (1 - p) / 2.0, p - 1)\n\n", "idx": 343}
{"namespace": "pymc.distributions.dist_math.incomplete_beta", "completion": "    return pt.betainc(a, b, value)\n\n", "idx": 344}
{"namespace": "pymc.sampling.forward.observed_dependent_deterministics", "completion": "    # Get the deterministics, observed random variables, and basic random variables from the model\n    deterministics = model.deterministics\n    observed_rvs = model.observed_RVs\n    basic_rvs = model.basic_RVs\n\n    # Find the deterministics that depend directly on observed variables\n    observed_dependent_deterministics = []\n    for deterministic in deterministics:\n        if any(\n            ancestors(deterministic) & set(observed_rvs + basic_rvs)\n        ):  # Check if any of the ancestors of the deterministic are observed random variables or basic random variables\n            observed_dependent_deterministics.append(deterministic)\n\n    return observed_dependent_deterministics\n\n", "idx": 345}
{"namespace": "pymc.smc.kernels.systematic_resampling", "completion": "    normalized_weights = weights / weights.sum()\n    new_indices = np.zeros(len(weights), dtype=int)\n    r = rng.random()\n    i = 0\n    while r > 0:\n        r -= normalized_weights[i]\n        i += 1\n    new_indices[0] = i - 1\n    for j in range(1, len(weights)):\n        r = rng.random()\n        while r > 0:\n            r -= normalized_weights[i]\n            i += 1\n        new_indices[j] = i - 1\n    return new_indices\n\n", "idx": 346}
{"namespace": "pymc.backends.base._squeeze_cat", "completion": "    if combine:\n        results = np.concatenate(results)\n    if squeeze and len(results) == 1:\n        results = results[0]\n    return results\n\n", "idx": 347}
{"namespace": "pymc.logprob.transforms.SimplexTransform.forward", "completion": "        return pt.log(value)\n", "idx": 348}
{"namespace": "pymc.logprob.transforms.SimplexTransform.backward", "completion": "        value = pt.as_tensor(value)\n        N = value.shape[-1].astype(value.dtype)\n        shift = pt.sum(value, -1, keepdims=True) / N\n        return pt.exp(value[..., :-1] + shift)\n", "idx": 349}
{"namespace": "pymc.logprob.utils.walk_model", "completion": "    if stop_at_vars is None:\n        stop_at_vars = set()\n\n    for graph in graphs:\n        if graph in stop_at_vars:\n            continue\n        yield graph\n        if isinstance(graph, MeasurableVariable):\n            if walk_past_rvs:\n                continue\n            else:\n                break\n        for var in expand_fn(graph):\n            yield from walk_model(\n                [var],\n                walk_past_rvs=walk_past_rvs,\n                stop_at_vars=stop_at_vars,\n                expand_fn=expand_fn,\n            )\n\n", "idx": 350}
{"namespace": "sacred.metrics_logger.linearize_metrics", "completion": "    metrics = {}\n    for metric in logged_metrics:\n        if metric.name not in metrics:\n            metrics[metric.name] = {\"steps\": [], \"values\": [], \"timestamps\": []}\n        metrics[metric.name][\"steps\"].append(metric.step)\n        metrics[metric.name][\"values\"].append(metric.value)\n        metrics[metric.name][\"timestamps\"].append(metric.timestamp)\n    return metrics", "idx": 351}
{"namespace": "sacred.utils.set_by_dotted_path", "completion": "    if \".\" in path:\n        key, rest = path.split(\".\", 1)\n    else:\n        key, rest = path, \"\"\n    if rest:\n        if key not in d:\n            d[key] = {}\n        set_by_dotted_path(d[key], rest, value)\n    else:\n        d[key] = value\n\n", "idx": 352}
{"namespace": "sacred.utils.get_by_dotted_path", "completion": "    split_path = path.split(\".\")\n    current_option = d\n    for p in split_path:\n        if p not in current_option:\n            return default\n        current_option = current_option[p]\n    return current_option\n\n", "idx": 353}
{"namespace": "pymc.logprob.scan.construct_scan", "completion": "    # Construct the scan operation\n    scan_op = MeasurableScan(\n        n_steps=scan_args.n_steps,\n        sequences=scan_args.inner_in_seqs,\n        outputs_info=scan_args.inner_out_mit_sot + scan_args.inner_out_sit_sot,\n        non_sequences=scan_args.inner_in_mit_sot + scan_args.inner_in_sit_sot,\n        mode=scan_args.mode,\n        profile=scan_args.profile,\n        name=scan_args.name,\n        allow_gc=scan_args.allow_gc,\n        strict=scan_args.strict,\n        go_backwards=scan_args.go_backwards,\n        taps=scan_args.mit_sot_in_slices,\n    )\n\n    # Construct the node based on the input and output variables\n    node = scan_op.make_node(\n        inputs=scan_args.outer_in_seqs,\n        outputs=scan_args.outer_outputs,\n        updates=scan_args.updates,\n        strict=scan_args.strict,\n        name=scan_args.name,\n    )\n\n    # Return the node outputs and updates\n    return node.outputs, node.updates\n\n", "idx": 354}
{"namespace": "sacred.utils.is_prefix", "completion": "    if pre_path == path:\n        return True\n    if pre_path == \"\":\n        return False\n    if path == \"\":\n        return False\n    if pre_path == \".\":\n        return False\n    if path == \".\":\n        return False\n    if pre_path == \"..\":\n        return False\n    if path == \"..\":\n        return False\n    if pre_path == \"...\":\n        return False\n    if path == \"...\":\n        return False\n    if pre_path == \"....\":\n        return False\n    if path == \"....\":\n        return False\n    if pre_path == \".....\":\n        return False\n    if path == \".....\":\n        return False\n    if pre_path == \"......\":\n        return False\n    if path == \"......\":\n        return False\n    if pre_path == \".......\":\n        return False\n    if path == \".......\":\n        return False\n    if pre_path == \"........\":\n        return False\n    if path == \"........\":\n        return False\n    if pre_path == \".........\":\n        return False\n    if path == \".........\":\n        return False\n    if pre_path == \"..........\":\n        return False\n    if path == \"..........\":\n        return False\n    if pre_path == \"...........\":\n        return False\n    if path == \"...........\":\n        return False\n    if pre_path == \"............\":\n        return False\n    if path == \"............\":\n        return False\n    if pre_path == \".............\":\n        return False\n    if path == \".............\":\n        return False\n    if pre_path == \"..............\":\n        return False\n    if path == \"..............\":\n        return False\n    if pre_path == \"...............\":\n        return False\n    if path == \"...............\":\n        return False\n    if pre_path == \"................\":\n        return False\n    if path == \"................\":\n        return False\n    if pre_path == \".................\":\n        return False\n    if path == \".................\":\n        return False\n    if pre_path == \"..................\":\n        return False\n    if path == \"..................\":\n        return False\n    if pre_path == \"...................\":\n        return False\n    if path == \"...................\":\n        return False\n    if pre_path == \"....................\":\n        return False\n    if path == \"....................\":\n        return False\n    if pre_path == \".....................\":\n        return False", "idx": 355}
{"namespace": "sacred.utils.get_inheritors", "completion": "    subclasses = set()\n\n    for subclass in cls.__subclasses__():\n        subclasses.add(subclass)\n        subclasses.update(get_inheritors(subclass))\n\n    return subclasses\n\n", "idx": 356}
{"namespace": "sacred.utils.convert_camel_case_to_snake_case", "completion": "    s1 = re.sub(\"(?!^)([A-Z]+)\", r\"_\\g<1>\", name)\n    return s1.lower()\n\n", "idx": 357}
{"namespace": "sacred.utils.module_exists", "completion": "    try:\n        pkgutil.find_loader(modname)\n        return True\n    except ImportError:\n        return False\n\n", "idx": 358}
{"namespace": "sacred.utils.apply_backspaces_and_linefeeds", "completion": "    lines = text.split(\"\\n\")\n    for i in range(len(lines)):\n        lines[i] = lines[i].replace(\"\\b\", \"\")\n    return \"\\n\".join(lines)\n\n", "idx": 359}
{"namespace": "sacred.commands.help_for_command", "completion": "    help_text = pydoc.getdoc(command)\n    help_text = help_text.replace(\"\\b\", \"\")\n    return help_text\n\n", "idx": 360}
{"namespace": "sacred.optional.optional_import", "completion": "    for package_name in package_names:\n        try:\n            package = importlib.import_module(package_name)\n            return True, package\n        except ImportError:\n            pass\n    return False, None\n\n", "idx": 361}
{"namespace": "sacred.dependencies.get_py_file_if_possible", "completion": "    if pyc_name.endswith((\".py\", \".so\", \".pyd\", \".ipynb\")):\n        return pyc_name\n    py_name = pyc_name[:-1]\n    if os.path.exists(py_name):\n        return py_name\n    return pyc_name\n\n", "idx": 362}
{"namespace": "sacred.config.custom_containers.DogmaticDict.update", "completion": "        if iterable is not None:\n            if hasattr(iterable, 'keys'):\n                for key in iterable.keys():\n                    self[key] = iterable[key]\n            else:\n                for key, value in iterable:\n                    self[key] = value\n\n        for key, value in kwargs.items():\n            self[key] = value\n", "idx": 363}
{"namespace": "sacred.config.config_scope.is_empty_or_comment", "completion": "    return line.strip().startswith(SETTINGS.COMMENT_PREFIX) or line.strip() == \"\"\n\n", "idx": 364}
{"namespace": "boltons.funcutils.copy_function", "completion": "    if not callable(orig):\n        raise TypeError(\"copy_function() can only copy functions, not %s\" % type(orig).__name__)\n\n    if not isinstance(orig, FunctionType):\n        raise TypeError(\"copy_function() can only copy functions, not %s\" % type(orig).__name__)\n\n    if _IS_PY2:\n        copy = FunctionType(orig.func_code, orig.func_globals, orig.func_name, orig.func_defaults, orig.func_closure)\n    else:\n        copy = FunctionType(orig.__code__, orig.__globals__, orig.__name__, orig.__defaults__, orig.__closure__)\n\n    if copy_dict:\n        copy.__dict__.update(orig.__dict__)\n\n    return copy\n\n", "idx": 365}
{"namespace": "sacred.config.config_scope.dedent_line", "completion": "    if line.startswith(indent):\n        return line[len(indent) :]\n    else:\n        return line\n\n", "idx": 366}
{"namespace": "boltons.funcutils.format_invocation", "completion": "    if kwargs is None:\n        kwargs = {}\n    if isinstance(kwargs, dict):\n        kwargs = kwargs.items()\n    return '%s(%s)' % (name, ', '.join(\n        itertools.chain(\n            (str(arg) for arg in args),\n            ('%s=%s' % (k, v) for k, v in kwargs),\n            ('%s=%s' % (k, v) for k, v in kw.items())\n        )\n    ))\n\n", "idx": 367}
{"namespace": "boltons.listutils.SplayList.shift", "completion": "        item = self.pop(item_index)\n        self.insert(dest_index, item)\n", "idx": 368}
{"namespace": "boltons.strutils.gzip_bytes", "completion": "    return zlib.compress(bytestring, level)\n\n", "idx": 369}
{"namespace": "boltons.strutils.is_uuid", "completion": "    try:\n        uuid.UUID(obj)\n        if version:\n            return uuid.UUID(obj).version == version\n        else:\n            return True\n    except ValueError:\n        return False\n\n", "idx": 370}
{"namespace": "boltons.strutils.parse_int_list", "completion": "    # Parse the input range string and return a list of integers.\n    #\n    # The input range string is a comma-separated string of positive integers and/or ranges of positive integers.\n    #\n    # The output list of integers is a sorted list of positive integers based on the input range string.\n    #\n    # The input range string is parsed into a list of integers and ranges of integers.\n    #\n    # The input range string is split into a list of integers and ranges of integers.\n    #\n    # The input range string is parsed into a list of integers and ranges of integers.\n    #\n    # The input range string is parsed into a list of integers and ranges of integers.\n    #\n    # The input range string is parsed into a list of integers and ranges of integers.\n    #\n    # The input range string is parsed into a list of integers and ranges of integers.\n    #\n    # The input range string is parsed into a list of integers and ranges of integers.\n    #\n    # The input range string is parsed into a list of integers and ranges of integers.\n    #\n    # The input range string is parsed into a list of integers and ranges of integers.\n    #\n    # The input range string is parsed into a list of integers and ranges of integers.\n    #\n    # The input range string is parsed into a list of integers and ranges of integers.\n    #\n    # The input range string is parsed into a list of integers and ranges of integers.\n    #\n    # The input range string is parsed into a list of integers and ranges of integers.\n    #\n    # The input range string is parsed into a list of integers and ranges of integers.\n    #\n    # The input range string is parsed into a list of integers and ranges of integers.\n    #\n    # The input range string is parsed into a list of integers and ranges of integers.\n    #\n    # The input range string is parsed into a list of integers and ranges of integers.\n    #\n    # The input range string is parsed into a list of integers and ranges of integers.\n    #\n    # The input range string is parsed into a list of integers and ranges of integers.\n    #\n    # The input range string is parsed into a list of integers and ranges of integers.\n    #\n    # The input range string is parsed into a list of integers and ranges of integers.\n    #\n    # The input range string is parsed into a list of integers and ranges of", "idx": 371}
{"namespace": "boltons.cacheutils.ThresholdCounter.get", "completion": "        try:\n            return self._count_map[key][0]\n        except KeyError:\n            return default\n", "idx": 372}
{"namespace": "boltons.iterutils.backoff_iter", "completion": "    if not factor:\n        raise ValueError('factor must be non-zero')\n    if not start:\n        raise ValueError('start must be non-zero')\n    if not stop:\n        raise ValueError('stop must be non-zero')\n    if start > stop:\n        raise ValueError('start must be less than stop')\n    if count == 'repeat':\n        count = None\n    if count is not None:\n        count = int(count)\n        if count < 1:\n            raise ValueError('count must be positive')\n    if jitter:\n        if jitter < -1.0 or jitter > 1.0:\n            raise ValueError('jitter must be between -1.0 and 1.0')\n        if jitter == 1.0:\n            jitter = 0.0\n        jitter = abs(jitter)\n    cur = start\n    while True:\n        yield cur\n        if count is not None:\n            count -= 1\n            if not count:\n                break\n        cur *= factor\n        if jitter:\n            cur *= (1.0 - jitter) + (random.random() * jitter)\n    return\n\n", "idx": 373}
{"namespace": "boltons.cacheutils.cached", "completion": "    def decorator(func):\n        if not callable(func):\n            raise TypeError('expected func to be callable, not %r' % func)\n        if not isinstance(cache, basestring):\n            if not (callable(getattr(cache, '__getitem__', None))\n                    and callable(getattr(cache, '__setitem__', None))):\n                raise TypeError('expected cache to be a dict-like object,'\n                                ' or callable returning a dict-like object, not %r'\n                                % cache)\n        return CachedFunction(func, cache, scoped, typed, key)\n    return decorator\n\n", "idx": 374}
{"namespace": "boltons.timeutils.total_seconds", "completion": "    return (td.microseconds + (td.seconds + td.days * 24 * 3600) * 10**6) / 10**6\n\n", "idx": 375}
{"namespace": "boltons.gcutils.get_all", "completion": "    # TODO: type survey\n    # TODO: docstring\n\n    if include_subtypes:\n        return [obj for obj in gc.get_objects() if isinstance(obj, type_obj)]\n    else:\n        return [obj for obj in gc.get_objects() if type(obj) is type_obj]\n\n", "idx": 376}
{"namespace": "boltons.timeutils.daterange", "completion": "    if isinstance(step, int):\n        step = timedelta(days=step)\n    elif isinstance(step, tuple):\n        step = timedelta(days=365 * step[0] + 30 * step[1] + step[2])\n    if stop is None:\n        while True:\n            yield start\n            start += step\n    else:\n        while start < stop:\n            yield start\n            start += step\n        if inclusive and start == stop:\n            yield start\n\n", "idx": 377}
{"namespace": "boltons.mathutils.clamp", "completion": "    return max(lower, min(x, upper))\n\n", "idx": 378}
{"namespace": "boltons.mathutils.ceil", "completion": "    if options is None:\n        return _ceil(x)\n    else:\n        return min(options, key=lambda y: y >= x)\n\n", "idx": 379}
{"namespace": "boltons.formatutils.get_format_args", "completion": "    # TODO: memoize\n    pos_args, named_args = [], []\n    # look for {: or {! or {. or {[ or {}\n    start, end, prev_end = 0, 0, 0\n    for match in _pos_farg_re.finditer(fstr):\n        start, end, group = match.start(), match.end(), match.group()\n        if prev_end < start:\n            ret += fstr[prev_end:start]\n        prev_end = end\n        if group == '{{' or group == '}}':\n            ret += group\n            continue\n        ret += '{%s%s' % (max_anon, group[1:])\n        max_anon += 1\n    ret += fstr[prev_end:]\n    return ret\n\n", "idx": 380}
{"namespace": "boltons.mathutils.floor", "completion": "    if options is None:\n        return _floor(x)\n    options = sorted(options)\n    i = bisect.bisect_right(options, x)\n    if i == 0:\n        raise ValueError(\"no floor options less than or equal to: %r\" % x)\n    return options[i-1]\n\n", "idx": 381}
{"namespace": "boltons.dictutils.OneToOne.setdefault", "completion": "        if key in self:\n            return self[key]\n        if default is not None:\n            self[key] = default\n        return default\n", "idx": 382}
{"namespace": "boltons.dictutils.OneToOne.update", "completion": "        if isinstance(dict_or_iterable, dict):\n            for k, v in dict_or_iterable.items():\n                self[k] = v\n        else:\n            for k, v in dict_or_iterable:\n                self[k] = v\n        for k, v in kw.items():\n            self[k] = v\n", "idx": 383}
{"namespace": "boltons.dictutils.ManyToMany.get", "completion": "        if key in self:\n            return self.data[key]\n        return default\n", "idx": 384}
{"namespace": "boltons.dictutils.FrozenDict.updated", "completion": "        if not a and not kw:\n            return self\n\n        if type(self) is type(a[0]) and a:\n            a = a[0]\n            if type(a) is type(self):\n                a = a.copy()\n            else:\n                a = type(self)(a)\n            a.update(kw)\n            return a\n\n        if type(self) is type(kw):\n            kw = kw.copy()\n        else:\n            kw = type(self)(kw)\n\n        if type(self) is type(a):\n            a = a.copy()\n        else:\n            a = type(self)(a)\n\n        a.update(kw)\n        return a\n", "idx": 385}
{"namespace": "boltons.dictutils.subdict", "completion": "    if keep is None:\n        keep = d.keys()\n    if drop is None:\n        drop = []\n    return dict([(k, d[k]) for k in keep if k not in drop])\n\n", "idx": 386}
{"namespace": "boltons.dictutils.FrozenDict.__repr__", "completion": "        cn = self.__class__.__name__\n        dict_repr = dict.__repr__(self)\n        return '%s(%s)' % (cn, dict_repr)\n", "idx": 387}
{"namespace": "gunicorn.config.validate_callable", "completion": "    def validate(val):\n        if not callable(val):\n            raise TypeError(\"Not a callable: %s\" % val)\n        if arity != -1 and arity != len(inspect.getargspec(val).args):\n            raise TypeError(\"Invalid arity for callable: %s\" % val)\n        return val\n\n    return validate\n\n", "idx": 388}
{"namespace": "gunicorn.config.get_default_config_file", "completion": "    # Get the current working directory\n    cwd = os.getcwd()\n\n    # Join the current working directory with the file name 'gunicorn.conf.py'\n    default_config_file = os.path.join(cwd, 'gunicorn.conf.py')\n\n    # Check if the file exists\n    if os.path.exists(default_config_file):\n        return default_config_file\n    else:\n        return None\n\n", "idx": 389}
{"namespace": "gunicorn.util.is_ipv6", "completion": "    try:\n        socket.inet_pton(socket.AF_INET6, addr)\n    except socket.error:\n        return False\n    return True\n\n", "idx": 390}
{"namespace": "gunicorn.systemd.listen_fds", "completion": "    if os.environ.get('LISTEN_PID') != str(os.getpid()):\n        return 0\n\n    fds = int(os.environ.get('LISTEN_FDS', 0))\n\n    if unset_environment:\n        for key in ('LISTEN_FDS', 'LISTEN_PID', 'LISTEN_FDNAMES'):\n            if key in os.environ:\n                del os.environ[key]\n\n    return fds\n\n", "idx": 391}
{"namespace": "gunicorn.util.http_date", "completion": "    if timestamp is None:\n        timestamp = time.time()\n    return email.utils.formatdate(timestamp, usegmt=True)\n\n", "idx": 392}
{"namespace": "gunicorn.util.parse_address", "completion": "    if netloc.startswith('unix:'):\n        return netloc[5:], None\n    elif netloc.startswith('fd:'):\n        return int(netloc[3:]), None\n    elif ':' in netloc:\n        host, port = netloc.split(':', 1)\n        if not port.isdigit():\n            raise ValueError(\"Port is not a number.\")\n        return host, int(port)\n    else:\n        return netloc, int(default_port)\n\n", "idx": 393}
{"namespace": "gunicorn.util.to_bytestring", "completion": "    if isinstance(value, str):\n        return value.encode(encoding)\n    return value\n\n", "idx": 394}
{"namespace": "gunicorn.util.warn", "completion": "    sys.stderr.write(\"WARNING: %s\\n\" % msg)\n\n", "idx": 395}
{"namespace": "gunicorn.util.split_request_uri", "completion": "    if uri.startswith(\"//\"):\n        uri = uri[2:]\n\n    host, port = uri, \"8000\"\n\n    if '[' in uri and ']' in uri:\n        host = uri.split(']')[0][1:]\n        port = (uri.split(']:') + [\"8000\"])[1]\n    elif ':' in uri:\n        host, port = (uri.split(':') + [\"8000\"])[:2]\n    elif uri == \"\":\n        host, port = \"0.0.0.0\", \"8000\"\n\n    return host.lower(), port\n\n", "idx": 396}
{"namespace": "praw.models.listing.listing.ModNoteListing.after", "completion": "        if not getattr(self, \"has_next_page\", False):\n            return None\n        return getattr(self, \"end_cursor\", None)\n\n", "idx": 397}
{"namespace": "praw.models.util.permissions_string", "completion": "    if permissions is None:\n        return \"+all\"\n    if not permissions:\n        return \"\"\n    if not known_permissions:\n        return \",\".join(permissions)\n    if not isinstance(permissions, list):\n        raise TypeError(\n            \"permissions must be a list of strings or None, not {}\".format(\n                type(permissions).__name__\n            )\n        )\n    if not all(isinstance(x, str) for x in permissions):\n        raise TypeError(\n            \"permissions must be a list of strings or None, not {}\".format(\n                type(permissions).__name__\n            )\n        )\n    if not all(x.startswith(\"+\") or x.startswith(\"-\") for x in permissions):\n        permissions = [\"+\" + x for x in permissions]\n    if not all(x in known_permissions for x in permissions):\n        raise ValueError(\n            \"permissions must be a subset of known_permissions, not {}\".format(\n                permissions\n            )\n        )\n    return \",\".join(permissions)\n\n", "idx": 398}
{"namespace": "jc.cli.JcCli.json_out", "completion": "        if self.data_out is None:\n            return ''\n\n        if self.raw:\n            return self.data_out\n\n        if self.yaml_output:\n            return self.yaml_out()\n\n        if self.pretty:\n            return utils.pretty_json(self.data_out, indent=self.json_indent, separators=self.json_separators)\n\n        if self.mono:\n            return utils.json_string(self.data_out, indent=self.json_indent, separators=self.json_separators)\n\n        class JcStyle(Style):\n            styles: CustomColorType = self.custom_colors\n\n        return str(highlight(utils.json_string(self.data_out, indent=self.json_indent, separators=self.json_separators), JsonLexer(), Terminal256Formatter(style=JcStyle)))\n", "idx": 399}
{"namespace": "pythonforandroid.pythonpackage.transform_dep_for_pip", "completion": "    if \"@\" in dependency and \"://\" in dependency:\n        return dependency\n    else:\n        return \"https://pypi.org/project/\" + dependency + \"/#files\"\n\n", "idx": 400}
{"namespace": "pythonforandroid.graph.fix_deplist", "completion": "    for i in range(len(deps)):\n        if isinstance(deps[i], str):\n            deps[i] = (deps[i],)\n        deps[i] = tuple(map(str.lower, deps[i]))\n\n    return deps\n\n", "idx": 401}
{"namespace": "pythonforandroid.util.walk_valid_filens", "completion": "    for root, dirs, files in walk(base_dir):\n        dirs[:] = [d for d in dirs if d not in invalid_dir_names]\n        for file in files:\n            full_path = join(root, file)\n            if not any(fnmatch(full_path, pattern) for pattern in invalid_file_patterns):\n                yield full_path\n\n", "idx": 402}
{"namespace": "pythonforandroid.bootstrap._cmp_bootstraps_by_priority", "completion": "    # Get the priority of the two bootstraps\n    a_priority = a.priority\n    b_priority = b.priority\n\n    # Compare the priorities\n    if a_priority > b_priority:\n        return -1\n    elif a_priority < b_priority:\n        return 1\n    else:\n        # If the priorities are equal, compare the names\n        return cmp(a.name, b.name)\n\n", "idx": 403}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.all_bootstraps", "completion": "        bootstraps_dir = join(cls.ctx.root_dir, 'bootstraps')\n        bootstraps = set()\n        for bootstrap_dir in listdir(bootstraps_dir):\n            if isdir(join(bootstraps_dir, bootstrap_dir)):\n                bootstraps.add(bootstrap_dir)\n        return bootstraps\n", "idx": 404}
{"namespace": "mmcv.image.colorspace._convert_input_type_range", "completion": "    if img.dtype == np.uint8:\n        img = img.astype(np.float32) / 255.\n    elif img.dtype == np.float32:\n        img = img.copy()\n    else:\n        raise TypeError(\n            f'Unsupported input type {img.dtype}, supported types are '\n            f'np.uint8 and np.float32.')\n    return img\n\n", "idx": 405}
{"namespace": "mackup.utils.error", "completion": "    sys.stderr.write(\"Error: {}\\n\".format(message))\n    sys.exit(1)\n\n", "idx": 406}
{"namespace": "mmcv.image.colorspace._convert_output_type_range", "completion": "    img_type = img.dtype\n    if img_type == np.float32:\n        if dst_type == np.uint8:\n            img *= 255.\n            img = img.astype(np.uint8)\n        elif dst_type == np.float32:\n            pass\n        else:\n            raise TypeError('The dst_type should be np.float32 or np.uint8, '\n                            f'but got {dst_type}')\n    elif img_type == np.uint8:\n        if dst_type == np.uint8:\n            pass\n        elif dst_type == np.float32:\n            img = img.astype(np.float32) / 255.\n        else:\n            raise TypeError('The dst_type should be np.float32 or np.uint8, '\n                            f'but got {dst_type}')\n    else:\n        raise TypeError('The img type should be np.float32 or np.uint8, '\n                        f'but got {img_type}')\n    return img\n\n", "idx": 407}
{"namespace": "mackup.utils.is_process_running", "completion": "    try:\n        subprocess.check_output([\"pgrep\", process_name])\n        return True\n    except subprocess.CalledProcessError:\n        return False\n\n", "idx": 408}
{"namespace": "stellar.operations._get_pid_column", "completion": "    server_version = raw_conn.engine.execute('select version()').scalar()\n    version_number = re.search(r'\\d+\\.\\d+\\.\\d+', server_version).group()\n    if sqlalchemy_utils.version.parse(version_number) >= sqlalchemy_utils.version.parse('10.0'):\n        return 'pid'\n    else:\n        return 'procpid'\n\n", "idx": 409}
{"namespace": "imapclient.imap_utf7.encode", "completion": "    if isinstance(s, str):\n        s = s.encode('utf-8')\n    if not isinstance(s, bytes):\n        return s\n    result = []\n    for c in s:\n        if c < 0x80:\n            result.append(c)\n        else:\n            result.append(0x26)\n            result.extend(binascii.b2a_base64(bytes([c])).rstrip(b'='))\n            result.append(0x26)\n    return bytes(result)\n\n", "idx": 410}
{"namespace": "imapclient.version._imapclient_version_string", "completion": "    major, minor, micro, releaselevel = vinfo[:4]\n    assert releaselevel in (\"alpha\", \"beta\", \"rc\", \"final\")\n\n    if releaselevel == \"final\":\n        version_string = f\"{major}.{minor}.{micro}\"\n    else:\n        version_string = f\"{major}.{minor}.{micro}{releaselevel[0]}{vinfo[4]}\"\n\n    return version_string\n\n", "idx": 411}
{"namespace": "telethon.helpers.generate_key_data_from_nonce", "completion": "    server_nonce = server_nonce.to_bytes(16, 'big')\n    new_nonce = new_nonce.to_bytes(32, 'big')\n\n    hash1 = sha1(server_nonce + new_nonce).digest()\n    hash2 = sha1(new_nonce + server_nonce + new_nonce).digest()\n    hash3 = sha1(server_nonce + new_nonce + server_nonce + new_nonce).digest()\n\n    key = hash1[:16] + hash2[:12]\n    iv = hash2[12:] + hash3[:4]\n\n    return key, iv\n\n", "idx": 412}
{"namespace": "hbmqtt.codecs.bytes_to_int", "completion": "    return int.from_bytes(data, byteorder='big')\n\n", "idx": 413}
{"namespace": "zulipterminal.helper.display_error_if_present", "completion": "    if \"msg\" in response.keys() and hasattr(controller, \"view\"):\n        controller.view.display_error(response[\"msg\"])\n\n", "idx": 414}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._decode_message_id", "completion": "        try:\n            return int(message_id)\n        except ValueError:\n            return None\n", "idx": 415}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton.handle_narrow_link", "completion": "        parsed_link = self._parse_narrow_link(self.link)\n        error = self._validate_narrow_link(parsed_link)\n        if error:\n            self.view.footer.update_footer(error)\n        else:\n            self._switch_narrow_to(parsed_link)\n", "idx": 416}
{"namespace": "zulipterminal.config.color.color_properties", "completion": "    class Color(Enum):\n        pass\n\n    for color in colors:\n        name = color.name\n        value = color.value\n        for p in prop:\n            name = p + \"_\" + name\n            value = p + \" \" + value\n        Color.__setitem__(name, value)\n\n    return Color\n\n", "idx": 417}
{"namespace": "twilio.base.deserialize.decimal", "completion": "    if d is None or d == \"\":\n        return d\n    try:\n        return Decimal(d, context=BasicContext)\n    except (TypeError, ValueError):\n        return d", "idx": 418}
{"namespace": "twilio.base.deserialize.integer", "completion": "    try:\n        return int(i)\n    except ValueError:\n        return i", "idx": 419}
{"namespace": "twilio.base.serialize.object", "completion": "    try:\n        return json.dumps(obj)\n    except:\n        return obj", "idx": 420}
{"namespace": "twilio.base.serialize.map", "completion": "    return [serialize_func(x) for x in lst]", "idx": 421}
{"namespace": "twilio.base.obsolete.deprecated_method", "completion": "    def deprecated_method_wrapper(func):\n        @funfunctools.wraps(func)\n        def new_func(*args, **kwargs):\n            warnings.warn(\n                \"{} has been deprecated. Please refer to current documentation for guidance.\".format(\n                    func.__name__\n                ),\n                DeprecationWarning,\n            )\n            return func(*args, **kwargs)\n\n        return new_func\n\n    return deprecated_method_wrapper(new_func) if new_func else deprecated_method_wrapper", "idx": 422}
{"namespace": "chatette.utils.sample_indulgent", "completion": "    if nb_items > len(array):\n        return deepcopy(array)\n    else:\n        return sample(array, nb_items)\n\n", "idx": 423}
{"namespace": "chatette.utils.rchop", "completion": "    if string.endswith(ending):\n        return string[:-len(ending)]\n    return string\n\n", "idx": 424}
{"namespace": "chatette.utils.str_to_bool", "completion": "    if text == 'True':\n        return True\n    elif text == 'False':\n        return False\n    else:\n        raise ValueError(\"The input string is neither 'True' nor 'False'.\")\n\n", "idx": 425}
{"namespace": "chatette.utils.min_if_exist", "completion": "    if n1 is None and n2 is None:\n        return None\n    if n1 is None:\n        return n2\n    if n2 is None:\n        return n1\n    return min(n1, n2)\n\n", "idx": 426}
{"namespace": "chatette.utils.append_to_list_in_dict", "completion": "    if key in dict_of_lists:\n        dict_of_lists[key].append(value)\n    else:\n        dict_of_lists[key] = [value]\n\n", "idx": 427}
{"namespace": "chatette.utils.extend_list_in_dict", "completion": "    if key not in dict_of_lists:\n        dict_of_lists[key] = values\n    else:\n        dict_of_lists[key].extend(values)", "idx": 428}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy._is_end_regex", "completion": "        return re.match(r'\\/(g?i?|i?g?)$', word) is not None\n", "idx": 429}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.execute", "completion": "        pass", "idx": 430}
{"namespace": "aioxmpp.network.group_and_order_srv_records", "completion": "    if rng is None:\n        rng = random.Random()\n\n    # Group by priority\n    grouped = itertools.groupby(\n        sorted(all_records, key=lambda x: x[0]),\n        key=lambda x: x[0]\n    )\n\n    # Order by weight\n    for priority, group in grouped:\n        group = list(group)\n        rng.shuffle(group)\n        yield from group\n\n", "idx": 431}
{"namespace": "aioxmpp.nonza.StreamFeatures.get_feature", "completion": "        try:\n            return self.features[feature_cls.TAG][0]\n        except IndexError:\n            return default\n\n", "idx": 432}
{"namespace": "aioxmpp.connector.XMPPOverTLSConnector._context_factory_factory", "completion": "        def context_factory():\n            ssl_context = metadata.ssl_context_factory()\n            if hasattr(ssl_context, \"set_alpn_protos\"):\n                ssl_context.set_alpn_protos(b\"xmpp-client\")\n            verifier.setup_context(ssl_context)\n            return ssl_context\n\n        return context_factory\n", "idx": 433}
{"namespace": "aioxmpp.xmltestutils.element_path", "completion": "    path = \"\"\n    if el is None:\n        return path\n    if upto is None:\n        upto = el.getroottree().getroot()\n    if el == upto:\n        return path\n    path = element_path(el.getparent(), upto)\n    path += \"/\" + el.tag\n    if el.getparent() is not None:\n        path += \"[\" + str(el.getparent().index(el) + 1) + \"]\"\n    return path\n\n", "idx": 434}
{"namespace": "aioxmpp.structs.JID.fromstr", "completion": "        if not s:\n            raise ValueError(\"s must not be empty or None\")\n\n        if len(s.encode(\"utf-8\")) > 1023:\n            raise ValueError(\"s too long\")\n\n        if \"@\" in s:\n            localpart, domain = s.split(\"@\", 1)\n        else:\n            localpart = None\n            domain = s\n\n        if localpart:\n            if not localpart:\n                raise ValueError(\"localpart must not be empty\")\n            if len(localpart.encode(\"utf-8\")) > 1023:\n                raise ValueError(\"localpart too long\")\n            localpart = nodeprep(\n                localpart,\n                allow_unassigned=not strict\n            )\n\n        if not domain:\n            raise ValueError(\"domain must not be empty or None\")\n        if len(domain.encode(\"utf-8\")) > 1023:\n            raise ValueError(\"domain too long\")\n        domain = nameprep(\n            domain,\n            allow_unassigned=not strict\n        )\n\n        if \"/\" in s:\n            resource = s.split(\"/\", 1)[1]\n        else:\n            resource = None\n\n        if resource:\n            if not resource:\n                raise ValueError(\"resource must not be empty\")\n            if len(resource.encode(\"utf-8\")) > 1023:\n                raise ValueError(\"resource too long\")\n            resource = resourceprep(\n                resource,\n                allow_unassigned=not strict\n            )\n\n        return cls(localpart, domain, resource)\n\n", "idx": 435}
{"namespace": "aioxmpp.security_layer.extract_python_dict_from_x509", "completion": "    result = {}\n    result['subject'] = x509.get_subject()\n    result['subjectAltName'] = x509.get_ext_by_NID(OpenSSL.crypto.NID_subject_alt_name)[0]\n    return result\n\n", "idx": 436}
{"namespace": "aioxmpp.security_layer.extract_blob", "completion": "    # Extract the ASN.1 blob from the X509 certificate.\n    blob = x509.get_extension(x509.get_extension_count() - 1).get_data()\n\n    # Return the ASN.1 blob.\n    return blob\n\n", "idx": 437}
{"namespace": "aioxmpp.security_layer.blob_to_pyasn1", "completion": "    return pyasn1.codec.der.decoder.decode(\n        blob,\n        asn1Spec=pyasn1_modules.rfc2459.Certificate())[0]\n\n", "idx": 438}
{"namespace": "aioxmpp.security_layer.extract_pk_blob_from_pyasn1", "completion": "    return pyasn1.codec.der.encoder.encode(\n        pyasn1_struct.getComponentByName(\"tbsCertificate\").getComponentByName(\"subjectPublicKeyInfo\")\n    )\n\n", "idx": 439}
{"namespace": "aioxmpp.callbacks.AdHocSignal.ASYNC_WITH_LOOP", "completion": "        if loop is None:\n            loop = asyncio.get_event_loop()\n        return functools.partial(cls._async_wrapper, loop)\n", "idx": 440}
{"namespace": "aioxmpp.callbacks.AdHocSignal.SPAWN_WITH_LOOP", "completion": "        if loop is None:\n            loop = asyncio.get_event_loop()\n\n        def create_wrapper(f):\n            if not hasattr(f, \"__call__\"):\n                raise TypeError(\"must be callable, got {!r}\".format(f))\n            if not asyncio.iscoroutinefunction(f):\n                raise TypeError(\"must be a coroutine function, got {!r}\".format(f))\n            return functools.partial(cls._spawn_wrapper,\n                                     f,\n                                     loop)\n\n        return create_wrapper\n", "idx": 441}
{"namespace": "aioxmpp.callbacks.first_signal", "completion": "    def _first_signal(signals):\n        if len(signals) == 0:\n            raise ValueError(\"no signals\")\n        if len(signals) == 1:\n            return signals[0]\n        first = signals[0]\n        second = signals[1]\n        first_token = first.connect(second)\n        second_token = second.connect(first)\n        try:\n            return first\n        finally:\n            first.disconnect(first_token)\n            second.disconnect(second_token)\n\n    return _first_signal(signals)\n\n", "idx": 442}
{"namespace": "aioxmpp.tasks.TaskPool.spawn", "completion": "        if not isinstance(__groups, set):\n            raise TypeError(\"groups must be a set\")\n        if not callable(__coro_fun):\n            raise TypeError(\"coro_fun must be callable\")\n\n        # Check if the groups have free slots available for the coroutine to be spawned\n        for group in __groups:\n            if self.get_limit(group) is not None and self.get_task_count(group) >= self.get_limit(group):\n                raise RuntimeError(\"Group limit reached\")\n\n        # Check if the total limit is exhausted\n        if self.get_limit(()) is not None and self.get_task_count(()) >= self.get_limit(()):\n            raise RuntimeError(\"Total limit reached\")\n\n        # Spawn the coroutine\n        coro = __coro_fun(*args, **kwargs)\n        task = asyncio.ensure_future(coro)\n\n        # Add the coroutine to the pool\n        for group in __groups:\n            self._group_tasks[group].add(task)\n        self._group_tasks[()].add(task)\n\n        # Remove the coroutine from the pool when it completes\n        def done_callback(task):\n            for group in __groups:\n                self._group_tasks[group].remove(task)\n            self._group_tasks[()].remove(task)\n\n        task.add_done_callback(done_callback)\n\n        return task", "idx": 443}
{"namespace": "aioxmpp.protocol.send_and_wait_for", "completion": "    if timeout is None:\n        await xmlstream.send_xso(send)\n        return await xmlstream.wait_for(wait_for)\n    else:\n        fut = xmlstream.wait_for(wait_for)\n        try:\n            await asyncio.wait_for(fut, timeout)\n        except asyncio.TimeoutError:\n            raise TimeoutError(\"timeout while waiting for response\")\n        else:\n            if cb is not None:\n                cb(fut.result())\n            return fut.result()", "idx": 444}
{"namespace": "aioxmpp.testutils.run_coroutine_with_peer", "completion": "    if not loop:\n        loop = asyncio.get_event_loop()\n\n    future = asyncio.Future()\n    peer_future = asyncio.Future()\n\n    def done_callback(f):\n        if not future.done():\n            future.set_result(f.result())\n\n    def peer_done_callback(f):\n        if not peer_future.done():\n            peer_future.set_result(f.result())\n\n    asyncio.ensure_future(coroutine, loop=loop).add_done_callback(done_callback)\n    asyncio.ensure_future(peer_coroutine, loop=loop).add_done_callback(peer_done_callback)\n\n    loop.run_until_complete(asyncio.wait_for(future, timeout=timeout))\n    loop.run_until_complete(asyncio.wait_for(peer_future, timeout=timeout))\n\n    return future.result()\n\n", "idx": 445}
{"namespace": "aioxmpp.testutils.make_listener", "completion": "    listener = unittest.mock.Mock()\n    for signal in instance.signals:\n        listener.add_child(signal.name)\n        signal.connect(listener.children[signal.name])\n    return listener\n\n", "idx": 446}
{"namespace": "aioxmpp.vcard.service.VCardService.set_vcard", "completion": "        iq = aioxmpp.IQ(\n            type_=aioxmpp.IQType.SET,\n            to=jid,\n            payload=vcard,\n        )\n\n        await self.client.send(iq)", "idx": 447}
{"namespace": "aioxmpp.rsm.xso.ResultSetMetadata.limit", "completion": "        result = copy.copy(self)\n        result.max_ = max_\n        return result\n", "idx": 448}
{"namespace": "aioxmpp.muc.service.Room.features", "completion": "        return {\n            \"muc.self.ping.logic\": \"0.11\",\n            \"muc.self.ping.logic.self\": \"0.11\",\n            \"muc.self.ping.logic.self.ping\": \"0.11\",\n            \"muc.self.ping.logic.self.ping.self\": \"0.11\",\n            \"muc.self.ping.logic.self.ping.self.ping\": \"0.11\",\n            \"muc.self.ping.logic.self.ping.self.ping.self\": \"0.11\",\n            \"muc.self.ping.logic.self.ping.self.ping.self.ping\": \"0.11\",\n            \"muc.self.ping.logic.self.ping.self.ping.self.ping.self.ping\": \"0.11\",\n            \"muc.self.ping.logic.self.ping.self.ping.self.ping.self.ping.self.ping\": \"0.11\",\n            \"muc.self.ping.logic.self.ping.self.ping.self.ping.self.ping.self.ping.self.ping\": \"0.11\",\n            \"muc.self.ping.logic.self.ping.self.ping.self.ping.self.ping.self.ping.self.ping.self.ping\": \"0.11\",\n            \"muc.self.ping.logic.self.ping.self.ping.self.ping.self.ping.self.ping.self.ping.self.ping.self.ping\": \"0.11\",\n            \"muc.self.ping.logic.self.ping.self.ping.self.ping.self.ping.self.ping.self.ping.self.ping.self.ping.self.ping\": \"0.11\",\n            \"muc.self.ping.logic.self.ping.self.ping.self.ping.self.ping.self.ping.self.ping.self.ping.self.ping.self.ping.self.ping\": \"0.11\",\n            \"muc.self.ping.logic.self.ping.self.ping.self.", "idx": 449}
{"namespace": "aioxmpp.xso.query.EvaluationContext.eval_bool", "completion": "        return bool(self.eval(expr))\n\n", "idx": 450}
{"namespace": "aioxmpp.xso.query._BoolOpMixin.eval", "completion": "        for value in self.expr.eval(ec):\n            sub_ec = copy.copy(ec)\n            sub_ec.set_toplevel_object(value)\n            if sub_ec.eval_bool(self.leaf):\n                yield True\n\n", "idx": 451}
{"namespace": "aioxmpp.xso.model.drop_handler", "completion": "    while True:\n        ev_type, *ev_args = yield\n        if ev_type == \"end\":\n            ev_args[0] -= 1\n        yield ev_type, *ev_args\n\n", "idx": 452}
{"namespace": "aioxmpp.xso.model.guard", "completion": "    depth = 0\n    while True:\n        ev = ev_args[0]\n        if ev == \"start\":\n            depth += 1\n        elif ev == \"end\":\n            depth -= 1\n        if depth:\n            ev_args = yield from dest.send(ev_args)\n        else:\n            return (yield from dest.send(ev_args))\n\n", "idx": 453}
{"namespace": "aioxmpp.xso.model.capture_events", "completion": "    while True:\n        ev = yield\n        try:\n            receiver.send(ev)\n        except StopIteration as exc:\n            return exc.value\n        except:  # NOQA\n            raise\n        try:\n            dest.send(ev)\n        except StopIteration as exc:\n            return exc.value\n        except:  # NOQA\n            raise\n\n", "idx": 454}
{"namespace": "aioxmpp.xso.model.events_to_sax", "completion": "    for ev_type, *ev_args in events:\n        if ev_type == \"start\":\n            dest.startElementNS(ev_args, None, {})\n        elif ev_type == \"text\":\n            dest.characters(ev_args[0])\n        elif ev_type == \"end\":\n            dest.endElementNS(ev_args, None)\n\n", "idx": 455}
{"namespace": "aioxmpp.adhoc.service.AdHocClient.get_command_info", "completion": "        disco = self.dependencies[aioxmpp.disco.DiscoClient]\n        response = await disco.query_info(\n            peer_jid,\n            node=namespaces.xep0050_commands + \"/\" + command_name,\n        )\n        return response\n", "idx": 456}
{"namespace": "aioxmpp.entitycaps.caps115.build_identities_string", "completion": "    identities_string = b''\n    identities_set = set()\n    for identity in identities:\n        identity_string = identity.encode()\n        if identity_string not in identities_set:\n            identities_string += identity_string + b'<'\n            identities_set.add(identity_string)\n    identities_string = identities_string[:-1]\n    identities_string = identities_string.split(b'<')\n    identities_string.sort()\n    identities_string = b'<'.join(identities_string)\n    return identities_string\n\n", "idx": 457}
{"namespace": "aioxmpp.entitycaps.caps115.build_features_string", "completion": "    features = [\n        escape(feature).encode(\"utf-8\")\n        for feature in features\n    ]\n\n    if len(set(features)) != len(features):\n        raise ValueError(\"duplicate feature\")\n\n    features.sort()\n    features.append(b\"\")\n    return b\"<\".join(features)\n\n", "idx": 458}
{"namespace": "aioxmpp.entitycaps.caps115.build_forms_string", "completion": "    forms = [\n        b\"/\".join([\n            escape(form.namespace).encode(\"utf-8\"),\n            escape(form.lang or \"\").encode(\"utf-8\"),\n            escape(form.name or \"\").encode(\"utf-8\"),\n        ])\n        for form in forms\n    ]\n\n    if len(set(forms)) != len(forms):\n        raise ValueError(\"duplicate form\")\n\n    forms.sort()\n    forms.append(b\"\")\n    return b\"<\".join(forms)\n\n", "idx": 459}
{"namespace": "aioxmpp.entitycaps.caps115.Key.path", "completion": "        return pathlib.Path(\n            urllib.parse.quote(self.node, safe=\"\"),\n            self.algo,\n            \"hashes\"\n        )\n\n", "idx": 460}
{"namespace": "aioxmpp.entitycaps.caps390._process_features", "completion": "    return b\"\".join(feature.encode(\"utf-8\") for feature in features)\n\n", "idx": 461}
{"namespace": "aioxmpp.entitycaps.caps390._process_identities", "completion": "    parts = [\n        _process_identity(identity)\n        for identity in identities\n    ]\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1d\"\n\n", "idx": 462}
{"namespace": "aioxmpp.entitycaps.caps390._process_extensions", "completion": "    parts = [\n        _process_form(ext)\n        for ext in exts\n    ]\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1c\"\n\n", "idx": 463}
{"namespace": "aioxmpp.entitycaps.caps390._calculate_hash", "completion": "    if algo == \"sha-1\":\n        return aioxmpp.hashes.sha1(hash_input).digest()\n    elif algo == \"sha-256\":\n        return aioxmpp.hashes.sha256(hash_input).digest()\n    elif algo == \"sha-512\":\n        return aioxmpp.hashes.sha512(hash_input).digest()\n    else:\n        raise ValueError(\"Unknown hash algorithm: {}\".format(algo))\n\n", "idx": 464}
{"namespace": "aioxmpp.entitycaps.caps390.Key.node", "completion": "        return self.algo + \":\" + base64.b64encode(self.digest).decode(\"utf-8\")\n", "idx": 465}
{"namespace": "aioxmpp.entitycaps.caps390.Key.path", "completion": "        return pathlib.Path(\n            \"caps\",\n            self.algo,\n            base64.b64encode(self.digest).decode(\"ascii\")\n        )\n", "idx": 466}
{"namespace": "aioxmpp.entitycaps.caps390.Implementation.extract_keys", "completion": "        if presence.xep0390_caps is None:\n            return ()\n        return (\n            Key(algo=algo, digest=digest)\n            for algo, digest in presence.xep0390_caps.hashes.items()\n            if algo in self.__algorithms\n        )\n", "idx": 467}
{"namespace": "aioxmpp.roster.service.RosterClient.approve", "completion": "        self.client.send(\n            stanza.Presence(\n                to_=peer_jid,\n                type_=structs.PresenceType.SUBSCRIBED\n            )\n        )\n", "idx": 468}
{"namespace": "aioxmpp.roster.service.RosterClient.subscribe", "completion": "        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.SUBSCRIBE,\n                            to=peer_jid)\n        )\n", "idx": 469}
{"namespace": "aioxmpp.roster.service.RosterClient.unsubscribe", "completion": "        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.UNSUBSCRIBE,\n                            to=peer_jid)\n        )", "idx": 470}
{"namespace": "aioxmpp.forms.fields.BoundSingleValueField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass\n", "idx": 471}
{"namespace": "aioxmpp.forms.fields.BoundMultiValueField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass\n", "idx": 472}
{"namespace": "aioxmpp.forms.fields.BoundOptionsField.options", "completion": "        try:\n            del self._options\n        except AttributeError:\n            pass\n", "idx": 473}
{"namespace": "aioxmpp.forms.fields.BoundSelectField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass\n", "idx": 474}
{"namespace": "aioxmpp.forms.fields.BoundMultiSelectField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass\n", "idx": 475}
{"namespace": "cupy.random._generator.reset_states", "completion": "    global _random_states\n    _random_states = {}\n\n", "idx": 476}
{"namespace": "cupy.random._generator._check_and_get_dtype", "completion": "    if dtype is None:\n        dtype = numpy.float32\n    else:\n        dtype = numpy.dtype(dtype)\n\n    if dtype.kind not in ('f', 'i'):\n        raise ValueError('dtype must be a float or an integer')\n\n    return dtype\n\n", "idx": 477}
{"namespace": "cupy_builder._command.filter_files_by_extension", "completion": "    filtered_sources = []\n    other_sources = []\n    for source in sources:\n        if source.endswith(extension):\n            filtered_sources.append(source)\n        else:\n            other_sources.append(source)\n    return filtered_sources, other_sources\n\n", "idx": 478}
{"namespace": "datasets.table._in_memory_arrow_table_from_file", "completion": "    return pa.ipc.open_file(filename).read_all()\n\n", "idx": 479}
{"namespace": "datasets.table._in_memory_arrow_table_from_buffer", "completion": "    buffer_reader = pa.BufferReader(buffer)\n    opened_stream = pa.ipc.open_stream(buffer_reader)\n    pa_table = opened_stream.read_all()\n    return pa_table\n\n", "idx": 480}
{"namespace": "datasets.table._interpolation_search", "completion": "    if len(arr) == 0:\n        raise IndexError(\"The array is empty.\")\n    if x < arr[0]:\n        raise IndexError(\"The query is smaller than the smallest element of the array.\")\n    if x >= arr[-1]:\n        raise IndexError(\"The query is larger than the largest element of the array.\")\n\n    # Find the index of the smallest value larger than x\n    i = np.searchsorted(arr, x, side=\"right\")\n    if arr[i] != x:\n        i -= 1\n    return i\n\n", "idx": 481}
{"namespace": "datasets.data_files._is_inside_unrequested_special_dir", "completion": "    # Check if the path is inside a special directory that is ignored by default.\n    if matched_rel_path.startswith(\"._\"):\n        return True\n    if matched_rel_path.startswith(\".DS_Store\"):\n        return True\n    if matched_rel_path.startswith(\".git\"):\n        return True\n    if matched_rel_path.startswith(\".github\"):\n        return True\n    if matched_rel_path.startswith(\".mypy_cache\"):\n        return True\n    if matched_rel_path.startswith(\".pytest_cache\"):\n        return True\n    if matched_rel_path.startswith(\".vscode\"):\n        return True\n    if matched_rel_path.startswith(\"__pycache__\"):\n        return True\n    if matched_rel_path.startswith(\"build\"):\n        return True\n    if matched_rel_path.startswith(\"dist\"):\n        return True\n    if matched_rel_path.startswith(\"docs\"):\n        return True\n    if matched_rel_path.startswith(\"env\"):\n        return True\n    if matched_rel_path.startswith(\"venv\"):\n        return True\n    if matched_rel_path.startswith(\"site\"):\n        return True\n    if matched_rel_path.startswith(\"target\"):\n        return True\n    if matched_rel_path.startswith(\"tests\"):\n        return True\n    if matched_rel_path.startswith(\"test\"):\n        return True\n    if matched_rel_path.startswith(\"tmp\"):\n        return True\n    if matched_rel_path.startswith(\"temp\"):\n        return True\n    if matched_rel_path.startswith(\"node_modules\"):\n        return True\n    if matched_rel_path.startswith(\"data\"):\n        return True\n    if matched_rel_path.startswith(\"datasets\"):\n        return True\n    if matched_rel_path.startswith(\"examples\"):\n        return True\n    if matched_rel_path.startswith(\"notebooks\"):\n        return True\n    if matched_rel_path.startswith(\"scripts\"):\n        return True\n    if matched_rel_path.startswith(\"src\"):\n        return True\n    if matched_rel_path.startswith(\"tools\"):\n        return True\n    if matched_rel_path.startswith(\"utils\"):\n        return True\n    if matched_rel_path.startswith(\"website\"):\n        return True\n    if matched_rel_path.startswith(\"assets\"):\n        return True\n    if matched_rel_path.startswith(\"bin\"):\n        return True\n    if matched_rel_path", "idx": 482}
{"namespace": "datasets.data_files._is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir", "completion": "    # We just need to check if every special directories from the path is present explicly in the pattern.\n    # Since we assume that the path matches the pattern, it's equivalent to counting that both\n    # the parent path and the parent pattern have the same number of special directories.\n    data_dirs_to_ignore_in_path = [part for part in PurePath(matched_rel_path).parent.parts if part.startswith(\".\")]\n    data_dirs_to_ignore_in_pattern = [part for part in PurePath(pattern).parent.parts if part.startswith(\".\")]\n    return len(data_dirs_to_ignore_in_path) != len(data_dirs_to_ignore_in_pattern)\n\n", "idx": 483}
{"namespace": "datasets.iterable_dataset._batch_to_examples", "completion": "    # we order the columns by order of appearance\n    # to do so, we use a dict as an ordered set\n    cols = {col: None for col in batch}\n    # when an example is missing a column, we set the value to None with .get()\n    arrays = [[example.get(col) for example in batch[col]] for col in cols]\n    return [dict(zip(cols, arrays[i])) for i in range(len(arrays[0]))]\n\n", "idx": 484}
{"namespace": "datasets.iterable_dataset._examples_to_batch", "completion": "    columns = {}\n    for example in examples:\n        for column in example:\n            if column not in columns:\n                columns[column] = []\n    for example in examples:\n        for column in columns:\n            if column in example:\n                columns[column].append(example[column])\n            else:\n                columns[column].append(None)\n    return {column: array for column, array in columns.items()}\n\n", "idx": 485}
{"namespace": "datasets.iterable_dataset.RandomlyCyclingMultiSourcesExamplesIterable._iter_random_indices", "completion": "        # if the probabilities are not provided, we assume that they are uniform\n        if p is None:\n            p = [1 / num_sources] * num_sources\n\n        # we check that the probabilities sum to 1\n        if not np.isclose(sum(p), 1):\n            raise ValueError(f\"The probabilities {p} don't sum to 1.\")\n\n        # we check that the probabilities are positive\n        if not all(p > 0):\n            raise ValueError(f\"The probabilities {p} are not positive.\")\n\n        # we check that the probabilities are normalized\n        if not np.isclose(sum(p), 1):\n            raise ValueError(f\"The probabilities {p} are not normalized.\")\n\n        # we check that the probabilities are sorted\n        if not all(p[i] <= p[i + 1] for i in range(len(p) - 1)):\n            raise ValueError(f\"The probabilities {p} are not sorted.\")\n\n        # we check that the probabilities are not duplicated\n        if not all(p[i] != p[i + 1] for i in range(len(p) - 1)):\n            raise ValueError(f\"The probabilities {p} are duplicated.\")\n\n        # we check that the probabilities are not too small\n        if not all(p[i] > 1e-10 for i in range(len(p))):\n            raise ValueError(f\"The probabilities {p} are too small.\")\n\n        # we check that the probabilities are not too big\n        if not all(p[i] < 1 - 1e-10 for i in range(len(p))):\n            raise ValueError(f\"The probabilities {p} are too big.\")\n\n        # we check that the probabilities are not too big\n        if not all(p[i] < 1 - 1e-10 for i in range(len(p))):\n            raise ValueError(f\"The probabilities {p} are too big.\")\n\n        # we check that the probabilities are not too small\n        if not all(p[i] > 1e-10 for i in range(len(p))):\n            raise ValueError(f\"The probabilities {p} are too small.\")\n\n        # we check that the probabilities are not too big\n        if not all(p[i] < 1 - 1", "idx": 486}
{"namespace": "datasets.iterable_dataset.BufferShuffledExamplesIterable._iter_random_indices", "completion": "        while True:\n            yield from (int(i) for i in rng.integers(0, buffer_size, size=random_batch_size))\n", "idx": 487}
{"namespace": "datasets.iterable_dataset.IterableDataset.remove_columns", "completion": "        if isinstance(column_names, str):\n            column_names = [column_names]\n        return self.map(partial(_remove_columns_fn, column_names=column_names), remove_columns=column_names)\n", "idx": 488}
{"namespace": "datasets.dataset_dict.DatasetDict.with_format", "completion": "        self._check_values_type()\n        return DatasetDict(\n            {k: dataset.with_format(type=type, columns=columns, output_all_columns=output_all_columns, **format_kwargs)\n             for k, dataset in self.items()}\n        )\n", "idx": 489}
{"namespace": "datasets.dataset_dict.DatasetDict.with_transform", "completion": "        dataset = copy.deepcopy(self)\n        dataset.set_format(\"custom\", columns=columns, output_all_columns=output_all_columns, transform=transform)\n        return dataset\n", "idx": 490}
{"namespace": "datasets.dataset_dict.DatasetDict.align_labels_with_mapping", "completion": "        self._check_values_type()\n        return DatasetDict({k: dataset.align_labels_with_mapping(label2id=label2id, label_column=label_column) for k, dataset in self.items()})\n", "idx": 491}
{"namespace": "datasets.dataset_dict.IterableDatasetDict.map", "completion": "        return IterableDatasetDict({k: dataset.map(function=function, with_indices=with_indices, input_columns=input_columns, batched=batched, batch_size=batch_size, drop_last_batch=drop_last_batch, remove_columns=remove_columns, fn_kwargs=fn_kwargs) for k, dataset in self.items()})\n", "idx": 492}
{"namespace": "datasets.dataset_dict.IterableDatasetDict.filter", "completion": "        return IterableDatasetDict(\n            {\n                k: dataset.filter(\n                    function=function,\n                    with_indices=with_indices,\n                    input_columns=input_columns,\n                    batched=batched,\n                    batch_size=batch_size,\n                    fn_kwargs=fn_kwargs,\n                )\n                for k, dataset in self.items()\n            }\n        )\n", "idx": 493}
{"namespace": "datasets.arrow_dataset.Dataset.num_rows", "completion": "        if self._indices is not None:\n            return self._indices.num_rows\n        return self._data.num_rows\n", "idx": 494}
{"namespace": "datasets.filesystems.extract_path_from_uri", "completion": "    if dataset_path.startswith(\"s3://\"):\n        dataset_path = dataset_path[5:]\n    return dataset_path\n\n", "idx": 495}
{"namespace": "datasets.filesystems.is_remote_filesystem", "completion": "    return fs.protocol != \"file\"\n\n", "idx": 496}
{"namespace": "datasets.utils.file_utils.hash_url_to_filename", "completion": "    url_bytes = url.encode(\"utf-8\")\n    url_hash = sha256(url_bytes)\n    filename = url_hash.hexdigest()\n\n    if etag:\n        etag_bytes = etag.encode(\"utf-8\")\n        etag_hash = sha256(etag_bytes)\n        filename += \".\" + etag_hash.hexdigest()\n\n    if url.endswith(\".h5\"):\n        filename += \".h5\"\n\n    return filename\n\n", "idx": 497}
{"namespace": "datasets.utils.hub.hf_hub_url", "completion": "    if version.parse(hfh.__version__) < version.parse(\"0.11.0\"):\n        path = quote(path)\n\n    if revision is None:\n        url = f\"https://huggingface.co/{repo_id}/resolve/{path}\"\n    else:\n        url = f\"https://huggingface.co/{repo_id}/resolve/{revision}/{path}\"\n\n    return url", "idx": 498}
{"namespace": "datasets.utils.sharding._number_of_shards_in_gen_kwargs", "completion": "    # Check if the lengths of the lists in the input dictionary are the same\n    if len(set([len(gen_kwargs[key]) for key in gen_kwargs])) != 1:\n        raise ValueError(\"The lengths of the lists in the input dictionary are not the same.\")\n\n    # Return the number of possible shards\n    return len(gen_kwargs[list(gen_kwargs.keys())[0]])\n\n", "idx": 499}
{"namespace": "datasets.utils.sharding._distribute_shards", "completion": "    if num_shards <= max_num_jobs:\n        return [range(i, i + 1) for i in range(num_shards)]\n    else:\n        # distribute the shards among the jobs\n        num_jobs = min(num_shards, max_num_jobs)\n        num_shards_per_job = int(np.ceil(num_shards / num_jobs))\n        return [range(i * num_shards_per_job, min((i + 1) * num_shards_per_job, num_shards)) for i in range(num_jobs)]\n\n", "idx": 500}
{"namespace": "datasets.utils.py_utils.temporary_assignment", "completion": "    original_value = getattr(obj, attr)\n    setattr(obj, attr, value)\n    try:\n        yield\n    finally:\n        setattr(obj, attr, original_value)\n\n", "idx": 501}
{"namespace": "datasets.utils.extract.TarExtractor.extract", "completion": "        if not os.path.exists(output_path):\n            os.makedirs(output_path)\n\n        with tarfile.open(input_path, \"r\") as tar:\n            tar.extractall(path=output_path, members=TarExtractor.safemembers(tar, output_path))\n\n", "idx": 502}
{"namespace": "datasets.utils.extract.Extractor.infer_extractor_format", "completion": "        magic_number_length = cls._get_magic_number_max_length()\n        magic_number = cls._read_magic_number(path, magic_number_length)\n        for extractor_format, extractor in cls.extractors.items():\n            if extractor.is_extractable(path, magic_number=magic_number):\n                return extractor_format\n        return \"\"\n", "idx": 503}
{"namespace": "datasets.utils.py_utils.asdict", "completion": "    if is_dataclass(obj):\n        return {f.name: asdict(getattr(obj, f.name)) for f in fields(obj)}\n    elif isinstance(obj, tuple) and hasattr(obj, \"_fields\"):  # namedtuple\n        return {k: asdict(v) for k, v in zip(obj._fields, obj)}\n    elif isinstance(obj, (list, tuple)):\n        return [asdict(item) for item in obj]\n    elif isinstance(obj, dict):\n        return {k: asdict(v) for k, v in obj.items()}\n    else:\n        return obj\n\n", "idx": 504}
{"namespace": "datasets.utils.metadata.MetadataConfigs.from_dataset_card_data", "completion": "        if cls.FIELD_NAME not in dataset_card_data:\n            return cls()\n\n        metadata_configs = dataset_card_data[cls.FIELD_NAME]\n        if not isinstance(metadata_configs, dict):\n            raise ValueError(f\"Expected {cls.FIELD_NAME} to be a dict, but got {metadata_configs}\")\n\n        for config_name, metadata_config in metadata_configs.items():\n            if not isinstance(metadata_config, dict):\n                raise ValueError(f\"Expected {config_name} to be a dict, but got {metadata_config}\")\n            cls._raise_if_data_files_field_not_valid(metadata_config)\n\n        return cls(metadata_configs)\n", "idx": 505}
{"namespace": "pymorphy2.analyzer.lang_dict_path", "completion": "    lang_dict_paths = _lang_dict_paths()\n    if lang not in lang_dict_paths:\n        raise ValueError('Language \"%s\" is not supported. '\n                         'Supported languages are: %s' % (lang, ', '.join(lang_dict_paths.keys())))\n    return lang_dict_paths[lang]\n\n", "idx": 506}
{"namespace": "pymorphy2.dawg.assert_can_create", "completion": "    if not EXTENSION_AVAILABLE:\n        raise NotImplementedError(\"DAWG extension is not available.\")\n\n", "idx": 507}
{"namespace": "pymorphy2.opencorpora_dict.compile._to_paradigm", "completion": "    stem = ''\n    paradigm = []\n\n    for form, tag in lexeme:\n        if stem == '':\n            stem = form\n        else:\n            stem = _common_prefix(stem, form)\n\n        prefixes = []\n        for prefix in paradigm_prefixes:\n            if form.startswith(prefix):\n                prefixes.append(prefix)\n                form = form[len(prefix):]\n                break\n\n        if not prefixes:\n            prefixes = ['']\n\n        paradigm.append((form, tag, prefixes[0]))\n\n    return stem, paradigm\n\n", "idx": 508}
{"namespace": "pymorphy2.units.by_analogy.KnownPrefixAnalyzer.tag", "completion": "        result = []\n        for prefix, unprefixed_word in self.possible_splits(word_lower):\n            method = (self, prefix)\n\n            tags = self.morph.tag(unprefixed_word)\n            for tag in tags:\n\n                if not tag.is_productive():\n                    continue\n\n                parse = (\n                    prefix + tag.word,\n                    tag,\n                    prefix + tag.normal_form,\n                    tag.score * self.score_multiplier,\n                    method\n                )\n\n                add_tag_if_not_seen(parse, result, seen_tags)\n\n        return result\n", "idx": 509}
{"namespace": "pymorphy2.units.by_analogy.UnknownPrefixAnalyzer.tag", "completion": "        result = []\n        for prefix, unprefixed_word in word_splits(word_lower):\n            for tag in self.dict_analyzer.tag(unprefixed_word, unprefixed_word, seen_tags):\n                if not tag.is_productive():\n                    continue\n                add_tag_if_not_seen(tag, result, seen_tags)\n        return result\n\n", "idx": 510}
{"namespace": "benedict.dicts.keylist.keylist_util.get_item", "completion": "    if not type_util.is_dict(d):\n        raise TypeError(f\"Invalid type: {type(d)!r}\")\n    if not type_util.is_list(keys):\n        raise TypeError(f\"Invalid type: {type(keys)!r}\")\n    if not keys:\n        return None, None\n    item = d\n    for key in keys[:-1]:\n        item = _get_or_new_item_value(item, key, keys[-1])\n    return _get_item_key_and_value(item, keys[-1])\n\n", "idx": 511}
{"namespace": "benedict.dicts.keylist.keylist_util.set_item", "completion": "    if not keys:\n        return\n    item = d\n    for key in keys[:-1]:\n        item = _get_or_new_item_value(item, key, keys[-1])\n    _set_item_value(item, keys[-1], value)\n\n", "idx": 512}
{"namespace": "benedict.dicts.keypath.keypath_util._split_key_indexes", "completion": "    if not re.search(KEY_INDEX_RE, key):\n        return [key]\n\n    indexes = []\n    for match in re.finditer(KEY_INDEX_RE, key):\n        indexes.append(match.group(1))\n    return indexes\n\n", "idx": 513}
{"namespace": "feedparser.urls.make_safe_absolute_uri", "completion": "    if not ACCEPTABLE_URI_SCHEMES:\n        return ''\n    if not base:\n        return rel or ''\n    if not rel:\n        return base\n    if not isinstance(base, str):\n        base = base.decode('utf-8')\n    if not isinstance(rel, str):\n        rel = rel.decode('utf-8')\n    base = convert_to_idn(base)\n    rel = convert_to_idn(rel)\n    uri = _urljoin(base, rel)\n    if not uri:\n        return ''\n    scheme = urllib.parse.urlparse(uri)[0]\n    if scheme not in ACCEPTABLE_URI_SCHEMES:\n        return ''\n    return uri\n\n", "idx": 514}
{"namespace": "feedparser.api._open_resource", "completion": "    # If the input is a string, wrap it in a BytesIO object.\n    if isinstance(url_file_stream_or_string, str):\n        url_file_stream_or_string = io.BytesIO(url_file_stream_or_string.encode(\"utf-8\"))\n\n    # If the input is a file-like object, return it.\n    if hasattr(url_file_stream_or_string, \"read\"):\n        return url_file_stream_or_string\n\n    # If the input is a URL, open it using urllib2.\n    if url_file_stream_or_string.startswith(\"http://\") or url_file_stream_or_string.startswith(\"https://\"):\n        # Build the request headers.\n        headers = {}\n        if etag:\n            headers[\"If-None-Match\"] = etag\n        if modified:\n            headers[\"If-Modified-Since\"] = modified\n        if agent:\n            headers[\"User-Agent\"] = agent\n        if referrer:\n            headers[\"Referer\"] = referrer\n        if request_headers:\n            headers.update(request_headers)\n\n        # Build the request.\n        request = urllib.request.Request(url_file_stream_or_string, headers=headers)\n\n        # Build the opener.\n        if handlers:\n            opener = urllib.request.build_opener(*handlers)\n        else:\n            opener = urllib.request.build_opener()\n\n        # Open the URL.\n        try:\n            stream = opener.open(request)\n        except urllib.error.HTTPError as e:\n            if e.code == 304:\n                # Not Modified\n                result[\"status\"] = \"304 Not Modified\"\n                result[\"etag\"] = etag\n                result[\"modified\"] = modified\n                return None\n            else:\n                raise\n\n        # Get the response headers.\n        result[\"status\"] = stream.getcode()\n        result[\"etag\"] = stream.info().get(\"ETag\")\n        result[\"modified\"] = stream.info().get(\"Last-Modified\")\n\n        # Return the stream.\n        return stream\n\n    # If the input is a pathname, open it using the built-in open() function.\n    if url_file_stream_or_string.startswith(\"file://\"):\n        url_file_stream_or_string = url_", "idx": 515}
{"namespace": "feedparser.http._build_urllib2_request", "completion": "    # Create the request object\n    request = urllib.request.Request(url)\n\n    # Add the user agent header\n    request.add_header('User-Agent', agent)\n\n    # Add the accept header\n    if accept_header:\n        request.add_header('Accept', accept_header)\n\n    # Add the etag header\n    if etag:\n        request.add_header('If-None-Match', etag)\n\n    # Add the modified header\n    if modified:\n        if isinstance(modified, datetime.datetime):\n            modified = modified.strftime('%a, %d %b %Y %H:%M:%S GMT')\n        request.add_header('If-Modified-Since', modified)\n\n    # Add the referrer header\n    if referrer:\n        request.add_header('Referer', referrer)\n\n    # Add the authorization header\n    if auth:\n        request.add_header('Authorization', auth)\n\n    # Add the additional headers\n    if request_headers:\n        for header, value in request_headers.items():\n            request.add_header(header, value)\n\n    return request\n\n", "idx": 516}
{"namespace": "pylatex.utils.dumps_list", "completion": "    if mapper is None:\n        mapper = []\n\n    if not isinstance(mapper, list):\n        mapper = [mapper]\n\n    if not isinstance(token, list):\n        token = [token]\n\n    if len(mapper) < len(token):\n        mapper = mapper * len(token)\n\n    if len(token) < len(mapper):\n        token = token * len(mapper)\n\n    if escape:\n        mapper.append(escape_latex)\n\n    if as_content:\n        mapper.append(pylatex.base_classes.LatexObject.dumps_as_content)\n\n    return NoEscape(\n        token[0].join(\n            mapper[0](item) for item in l\n        )\n    )\n\n", "idx": 517}
{"namespace": "pylatex.utils._latex_item_to_string", "completion": "    if isinstance(item, pylatex.base_classes.LatexObject):\n        if as_content:\n            return item.dumps_as_content(escape=escape)\n        else:\n            return item.dumps(escape=escape)\n    else:\n        return escape_latex(item) if escape else str(item)\n\n", "idx": 518}
{"namespace": "mistune.markdown.Markdown.read", "completion": "        self.set_state(state)\n        with open(filepath, 'rb') as f:\n            content = f.read().decode(encoding)\n        return self.parse(content)\n", "idx": 519}
{"namespace": "mistune.create_markdown", "completion": "    if isinstance(renderer, str):\n        renderer = HTMLRenderer(escape, hard_wrap)\n\n    if isinstance(plugins, str):\n        plugins = [plugins]\n\n    if isinstance(plugins, list):\n        for plugin in plugins:\n            import_plugin(plugin)\n\n    return Markdown(renderer, plugins)", "idx": 520}
{"namespace": "parsel.utils.extract_regex", "completion": "    if isinstance(regex, str):\n        regex = re.compile(regex)\n\n    if replace_entities:\n        text = w3lib_replace_entities(text)\n\n    matches = regex.finditer(text)\n    results = []\n    for match in matches:\n        if match.groupdict():\n            results.append(match.groupdict()[\"extract\"])\n        elif match.groups():\n            results.extend(match.groups())\n        else:\n            results.append(match.group())\n    return results\n\n", "idx": 521}
{"namespace": "dominate.dom_tag.dom_tag.render", "completion": "    # If the tag is not pretty, we don't need to do anything special\n    if not pretty:\n      return self.render_pretty(indent=indent, xhtml=xhtml)\n\n    # If the tag is pretty, we need to do some extra work\n    else:\n      # If the tag is pretty, we need to do some extra work\n      if self.is_pretty:\n        return self.render_pretty(indent=indent, xhtml=xhtml)\n\n      # If the tag is not pretty, we don't need to do anything special\n      else:\n        return self.render_pretty(indent=indent, xhtml=xhtml)\n\n", "idx": 522}
{"namespace": "dominate.util.include", "completion": "  with open(f, 'r') as f:\n    return f.read()\n\n", "idx": 523}
{"namespace": "dominate.util.unescape", "completion": "  def unescape_match(match):\n    entity = match.group(1)\n    if entity.startswith('#'):\n      return chr(int(entity[1:]))\n    else:\n      return chr(_unescape[entity])\n\n  return re.sub(r'&(#?x?)(\\w+);', unescape_match, data)\n\n", "idx": 524}
{"namespace": "onlinejudge_command.pretty_printers._tokenize_line", "completion": "    tokens = []\n    l = 0\n    while l < len(line):\n        r = l + 1\n        while r < len(line) and (line[l] in ' \\t') == (line[r] in ' \\t'):\n            r += 1\n        if line[l] in ' \\t':\n            typ = _PrettyTokenType.WHITESPACE\n        else:\n            typ = _PrettyTokenType.BODY\n        tokens.append(_PrettyToken(typ, line[l:r]))\n        l = r\n    return tokens\n\n", "idx": 525}
{"namespace": "onlinejudge_command.pretty_printers._render_tokens", "completion": "    def _format_token(token: _PrettyToken) -> str:\n        if token.type == _PrettyTokenType.BODY:\n            return font_normal(token.value)\n        elif token.type == _PrettyTokenType.BODY_HIGHLIGHT_LEFT:\n            return font_blue(token.value)\n        elif token.type == _PrettyTokenType.BODY_HIGHLIGHT_RIGHT:\n            return font_red(token.value)\n        elif token.type == _PrettyTokenType.WHITESPACE:\n            return font_dim(token.value)\n        elif token.type == _PrettyTokenType.NEWLINE:\n            return font_bold(token.value)\n        elif token.type == _PrettyTokenType.HINT:\n            return font_dim(token.value)\n        elif token.type == _PrettyTokenType.LINENO:\n            return font_dim(token.value)\n        elif token.type == _PrettyTokenType.OTHERS:\n            return font_dim(token.value)\n        else:\n            raise ValueError(f'unknown token type: {token.type}')\n\n    return ''.join(_format_token(token) for token in tokens)\n\n", "idx": 526}
{"namespace": "onlinejudge_command.pretty_printers._tokenize_file_content_without_snipping", "completion": "    tokens = []\n    try:\n        text = content.decode()\n    except UnicodeDecodeError as e:\n        tokens.append(_PrettyToken(_PrettyTokenType.HINT, str(e)))\n        text = content.decode(errors='replace')\n    for line in text.splitlines(keepends=True):\n        tokens += _tokenize_line(line)\n    tokens = _warn_if_empty(tokens)\n    return tokens\n\n", "idx": 527}
{"namespace": "jinja2.environment.Environment.get_template", "completion": "        if isinstance(name, Template):\n            template = name\n        else:\n            template = self._load_template(name, globals)\n\n        if parent is not None:\n            template.parent = self.get_template(parent)\n\n        return template\n", "idx": 528}
{"namespace": "jinja2.environment.Environment.from_string", "completion": "        if isinstance(source, str):\n            source = self._parse(source)\n        if template_class is None:\n            template_class = self.template_class\n        return template_class(source, self, self.make_globals(globals))\n", "idx": 529}
{"namespace": "jinja2.environment.Template.render", "completion": "        if self._uptodate is not None and not self._uptodate():\n            raise TemplatesNotUpToDate(self.name)\n\n        context = Context(self, self.globals, args, kwargs)\n        return \"\".join(self.root_render_func(context))\n", "idx": 530}
{"namespace": "jinja2.utils.generate_lorem_ipsum", "completion": "    import random\n\n    words = [\n        \"lorem\",\n        \"ipsum\",\n        \"dolor\",\n        \"sit\",\n        \"amet\",\n        \"consectetur\",\n        \"adipiscing\",\n        \"elit\",\n        \"sed\",\n        \"do\",\n        \"eiusmod\",\n        \"tempor\",\n        \"incididunt\",\n        \"ut\",\n        \"labore\",\n        \"et\",\n        \"dolore\",\n        \"magna\",\n        \"aliqua\",\n        \"ut\",\n        \"enim\",\n        \"ad\",\n        \"minim\",\n        \"veniam\",\n        \"quis\",\n        \"nostrud\",\n        \"exercitation\",\n        \"ullamco\",\n        \"laboris\",\n        \"nisi\",\n        \"ut\",\n        \"aliquip\",\n        \"ex\",\n        \"ea\",\n        \"commodo\",\n        \"consequat\",\n        \"duis\",\n        \"aute\",\n        \"irure\",\n        \"dolor\",\n        \"in\",\n        \"reprehenderit\",\n        \"in\",\n        \"voluptate\",\n        \"velit\",\n        \"esse\",\n        \"cillum\",\n        \"dolore\",\n        \"eu\",\n        \"fugiat\",\n        \"nulla\",\n        \"pariatur\",\n        \"excepteur\",\n        \"sint\",\n        \"occaecat\",\n        \"cupidatat\",\n        \"non\",\n        \"proident\",\n        \"sunt\",\n        \"in\",\n        \"culpa\",\n        \"qui\",\n        \"officia\",\n        \"deserunt\",\n        \"mollit\",\n        \"anim\",\n        \"id\",\n        \"est\",\n        \"laborum\",\n    ]\n\n    def generate_paragraph(min: int, max: int) -> str:\n        \"\"\"\n        This function generates a single paragraph of Lorem Ipsum text based on the given parameters. It creates a random number of words within a given range, and adds a comma after every 3 to 8 words, and a period after every 10 to 20 words. The first word of the paragraph is capitalized.\n        Input-Output Arguments\n        :param min: int. The minimum number of words in the paragraph.\n        :param max: int. The maximum number of words in the paragraph.\n        :return: str. The generated paragraph of Lorem Ipsum text.\n        \"\"\"\n        num_words = random.randint(min, max)\n        paragraph = []\n        for i in range", "idx": 531}
{"namespace": "jinja2.utils.LRUCache.clear", "completion": "        self._mapping.clear()\n        self._queue.clear()\n", "idx": 532}
{"namespace": "jinja2.utils.LRUCache.items", "completion": "        return self._mapping.items()\n", "idx": 533}
{"namespace": "jinja2.idtracking.symbols_for_node", "completion": "    sym = Symbols(parent=parent_symbols)\n    visitor = FrameSymbolVisitor(sym)\n    visitor.visit(node)\n    return sym\n\n", "idx": 534}
{"namespace": "jinja2.idtracking.Symbols.find_ref", "completion": "        if name in self.refs:\n            return self.refs[name]\n\n        if self.parent is not None:\n            return self.parent.find_ref(name)\n\n        return None\n", "idx": 535}
{"namespace": "jinja2.idtracking.Symbols.dump_stores", "completion": "        if self.parent is not None:\n            return {**self.parent.dump_stores(), **self.stores}\n        return self.stores\n\n", "idx": 536}
{"namespace": "jinja2.meta.find_undeclared_variables", "completion": "    generator = TrackingCodeGenerator(ast.environment)\n    generator.visit(ast)\n    return generator.undeclared_identifiers", "idx": 537}
{"namespace": "jinja2.loaders.split_template_path", "completion": "    segments = template.split('/')\n    if any(seg.startswith('.') for seg in segments):\n        raise TemplateNotFound(template)\n    return segments\n\n", "idx": 538}
{"namespace": "jinja2.bccache.MemcachedBytecodeCache.load_bytecode", "completion": "        key = self.prefix + bucket.key\n        try:\n            bucket.bytecode_from_string(self.client.get(key))\n        except Exception as e:\n            if not self.ignore_memcache_errors:\n                raise e\n", "idx": 539}
{"namespace": "jinja2.bccache.MemcachedBytecodeCache.dump_bytecode", "completion": "        try:\n            self.client.set(\n                self.prefix + bucket.key,\n                bucket.bytecode_to_string(),\n                self.timeout,\n            )\n        except Exception:\n            if not self.ignore_memcache_errors:\n                raise\n", "idx": 540}
{"namespace": "sumy.utils.get_stop_words", "completion": "    language = normalize_language(language)\n    path = expand_resource_path(\"stopwords/%s.txt\" % language)\n\n    if not exists(path):\n        raise LookupError(\"Language '%s' is not supported.\" % language)\n\n    with open(path, \"r\") as f:\n        return frozenset(to_unicode(line.strip()) for line in f)\n\n", "idx": 541}
{"namespace": "sumy._compat.to_bytes", "completion": "    if isinstance(object, bytes):\n        return object\n    elif isinstance(object, unicode):\n        return object.encode(\"utf-8\")\n    else:\n        return str(object)\n\n", "idx": 542}
{"namespace": "sumy._compat.to_unicode", "completion": "    if isinstance(object, unicode):\n        return object\n    elif isinstance(object, bytes):\n        return object.decode(\"utf-8\")\n    else:\n        # try decode instance to unicode\n        return instance_to_unicode(object)\n\n", "idx": 543}
{"namespace": "sumy.summarizers.lsa.LsaSummarizer._create_dictionary", "completion": "        dictionary = {}\n        for sentence in document.sentences:\n            for word in sentence.words:\n                word = self.normalize_word(word)\n                if word not in self.stop_words and word not in dictionary:\n                    dictionary[word] = len(dictionary)\n        return dictionary\n", "idx": 544}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._get_content_words_in_sentence", "completion": "        return [w for w in sentence.words if w not in self.stop_words]\n", "idx": 545}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._get_all_content_words_in_doc", "completion": "        all_words = self._get_all_words_in_doc(sentences)\n        content_words = self._filter_out_stop_words(all_words)\n        normalized_content_words = self._normalize_words(content_words)\n        return normalized_content_words\n", "idx": 546}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_tf", "completion": "        content_words = self._get_all_content_words_in_doc(sentences)\n        word_freq = self._compute_word_freq(content_words)\n        total_count = sum(word_freq.values())\n        tf = {w: word_freq[w] / total_count for w in word_freq}\n        return tf\n", "idx": 547}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_ratings", "completion": "        sentences_as_words = [self._get_content_words_in_sentence(s) for s in sentences]\n        word_freq = self._compute_tf(sentences)\n        ratings = {}\n        for i in range(len(sentences)):\n            best_sentence_index = self._find_index_of_best_sentence(word_freq, sentences_as_words)\n            best_sentence = sentences[best_sentence_index]\n            ratings[best_sentence] = -i\n            word_freq = self._update_tf(word_freq, sentences_as_words[best_sentence_index])\n            sentences_as_words.pop(best_sentence_index)\n        return ratings", "idx": 548}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.cue_method", "completion": "        method = self._build_cue_method_instance(bonus_word_value, stigma_word_value)\n        ratings = method.rate_sentences(document)\n        return self._get_best_sentences(document.sentences, sentences_count, ratings)\n", "idx": 549}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.key_method", "completion": "        summarization_method = self._build_key_method_instance()\n        return summarization_method(document, sentences_count, weight)\n", "idx": 550}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.title_method", "completion": "        summarization_method = self._build_title_method_instance()\n        return summarization_method(document, sentences_count)\n", "idx": 551}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.location_method", "completion": "        summarization_method = self._build_location_method_instance()\n        return summarization_method(document, sentences_count, w_h, w_p1, w_p2, w_s1, w_s2)\n", "idx": 552}
{"namespace": "sumy.summarizers.reduction.ReductionSummarizer.rate_sentences", "completion": "        # Create a dictionary to store the ratings for each sentence\n        ratings = defaultdict(int)\n\n        # Iterate over all pairs of sentences in the document\n        for sentence1, sentence2 in combinations(document.sentences, 2):\n\n            # Calculate the similarity between the two sentences\n            similarity = self._similarity(sentence1, sentence2)\n\n            # Add the similarity to the ratings for both sentences\n            ratings[sentence1] += similarity\n            ratings[sentence2] += similarity\n\n        # Return the ratings dictionary\n        return ratings\n", "idx": 553}
{"namespace": "sumy.summarizers.reduction.ReductionSummarizer._to_words_set", "completion": "        return {self.normalize_word(word) for word in sentence.words if word not in self.stop_words}\n", "idx": 554}
{"namespace": "sumy.summarizers.text_rank.TextRankSummarizer._to_words_set", "completion": "        return {self.normalize_word(word) for word in sentence.words\n                if self.normalize_word(word) not in self.stop_words}\n", "idx": 555}
{"namespace": "sumy.summarizers.kl.KLSummarizer.compute_tf", "completion": "        content_words = [self._get_content_words_in_sentence(s) for s in sentences]\n        content_words = [w for words in content_words for w in words]\n        content_words = self._normalize_words(content_words)\n        word_freq = self._compute_word_freq(content_words)\n        total_words = len(content_words)\n        tf = {w: word_freq[w] / total_words for w in word_freq}\n        return tf\n", "idx": 556}
{"namespace": "sumy.evaluation.rouge._get_word_ngrams", "completion": "    assert len(sentences) > 0\n    assert n > 0\n\n    words = _split_into_words(sentences)\n    return _get_ngrams(n, words)\n\n", "idx": 557}
{"namespace": "sumy.evaluation.rouge._len_lcs", "completion": "    table = _lcs(x, y)\n    return table[_len_lcs(x, y)]\n\n", "idx": 558}
{"namespace": "sumy.evaluation.rouge._recon_lcs", "completion": "    def _recon_lcs_helper(i, j):\n        \"\"\"\n        This function returns the Longest Common Subsequence (LCS) between two sequences of words. It designs a helper function to recursively reconstruct the LCS based on a table that saves the length of LCS at any position.\n        Input-Output Arguments\n        :param i: Integer. The index of the first sequence of words.\n        :param j: Integer. The index of the second sequence of words.\n        :return: List of words. The LCS of x and y.\n        \"\"\"\n        if i == 0 or j == 0:\n            return []\n        elif x[i - 1] == y[j - 1]:\n            return _recon_lcs_helper(i - 1, j - 1) + [x[i - 1]]\n        elif table[i - 1, j] > table[i, j - 1]:\n            return _recon_lcs_helper(i - 1, j)\n        else:\n            return _recon_lcs_helper(i, j - 1)\n\n    n, m = _get_index_of_lcs(x, y)\n    table = _lcs(x, y)\n    recon_lcs = _recon_lcs_helper(n, m)\n    return recon_lcs\n\n", "idx": 559}
{"namespace": "sumy.evaluation.rouge._union_lcs", "completion": "    lcs_union = set()\n    for s_i in evaluated_sentences:\n        lcs = set(_recon_lcs(_split_into_words([s_i]), _split_into_words([reference_sentence])))\n        lcs_union = lcs_union.union(lcs)\n    r = len(_split_into_words([reference_sentence]))\n    return len(lcs_union) / r\n\n", "idx": 560}
{"namespace": "sumy.parsers.html.HtmlParser.from_file", "completion": "        with open(file_path, \"r\") as f:\n            return cls(f.read(), tokenizer, url)\n", "idx": 561}
{"namespace": "sumy.parsers.plaintext.PlaintextParser.document", "completion": "        # Create a new document model object\n        document = ObjectDocumentModel()\n\n        # Split the text into lines\n        lines = self._text.splitlines()\n\n        # Initialize variables to track the current paragraph and sentence\n        current_paragraph = None\n        current_sentence = None\n\n        # Iterate through each line of the input text\n        for line in lines:\n            # Remove leading and trailing whitespace\n            line = line.strip()\n\n            # Check if the line is empty\n            if not line:\n                # If the line is empty, check if the current paragraph is not None\n                if current_paragraph is not None:\n                    # If the current paragraph is not None, add it to the document model\n                    document.add_paragraph(current_paragraph)\n                    # Reset the current paragraph to None\n                    current_paragraph = None\n                # If the line is empty and the current paragraph is None, continue to the next line\n                continue\n\n            # Check if the line is all uppercase\n            if line.isupper():\n                # If the line is all uppercase, create a new paragraph object\n                current_paragraph = Paragraph(line)\n                # If the current sentence is not None, add it to the current paragraph\n                if current_sentence is not None:\n                    current_paragraph.add_sentence(current_sentence)\n                    # Reset the current sentence to None\n                    current_sentence = None\n            else:\n                # If the line is not all uppercase, create a new sentence object\n                current_sentence = Sentence(line)\n                # If the current paragraph is not None, add the sentence to the current paragraph\n                if current_paragraph is not None:\n                    current_paragraph.add_sentence(current_sentence)\n                # If the current paragraph is None, add the sentence to the document model\n                else:\n                    document.add_sentence(current_sentence)\n\n        # Check if the current paragraph is not None\n        if current_paragraph is not None:\n            # If the current paragraph is not None, add it to the document model\n            document.add_paragraph(current_paragraph)\n\n        # Return the created document model object\n        return document", "idx": 562}
{"namespace": "sumy.nlp.tokenizers.Tokenizer.to_sentences", "completion": "        # update abbreviations used by the tokenizer based on the language\n        if self.language in self.LANGUAGE_EXTRA_ABREVS:\n            self._sentence_tokenizer._params.abbrev_types.update(self.LANGUAGE_EXTRA_ABREVS[self.language])\n\n        # tokenize the paragraph into sentences\n        sentences = self._sentence_tokenizer.tokenize(paragraph)\n\n        return sentences\n", "idx": 563}
{"namespace": "sumy.nlp.stemmers.null_stemmer", "completion": "    return object.lower()\n\n", "idx": 564}
{"namespace": "rows.fields.BinaryField.serialize", "completion": "        if value is not None:\n            if isinstance(value, six.binary_type):\n                try:\n                    return b64encode(value).decode(\"ascii\")\n                except Exception:\n                    return value\n            else:\n                value_error(value, cls)\n        else:\n            return \"\"\n", "idx": 565}
{"namespace": "rows.fields.BoolField.deserialize", "completion": "        value = super(BoolField, cls).deserialize(value, *args, **kwargs)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            value = value.lower()\n            if value in cls.TRUE_VALUES:\n                return True\n            elif value in cls.FALSE_VALUES:\n                return False\n            else:\n                raise ValueError(\"Value is not boolean\")\n\n", "idx": 566}
{"namespace": "rows.fields.DateField.serialize", "completion": "        if value is None:\n            return \"\"\n        return value.strftime(cls.OUTPUT_FORMAT)\n", "idx": 567}
{"namespace": "rows.fields.DateField.deserialize", "completion": "        value = super(DateField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n        try:\n            return datetime.datetime.strptime(value, cls.INPUT_FORMAT).date()\n        except ValueError:\n            value_error(value, cls)\n\n", "idx": 568}
{"namespace": "rows.fields.TextField.deserialize", "completion": "        if isinstance(value, cls.TYPE):\n            return value\n        elif is_null(value):\n            return None\n        else:\n            return as_string(value)\n\n", "idx": 569}
{"namespace": "rows.fields.EmailField.deserialize", "completion": "        value = super(EmailField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        elif not value:\n            return None\n        elif cls.EMAIL_REGEXP.match(value):\n            return value\n        else:\n            raise ValueError(\"Value is not an email\")\n\n", "idx": 570}
{"namespace": "rows.fields.JSONField.deserialize", "completion": "        value = super(JSONField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        elif isinstance(value, six.text_type):\n            return json.loads(value)\n        else:\n            value_error(value, cls)\n\n", "idx": 571}
{"namespace": "falcon.http_error.HTTPError.to_json", "completion": "        if handler is None:\n            import json\n\n            handler = json.dumps\n\n        return handler(self.to_dict())\n", "idx": 572}
{"namespace": "falcon.inspect.inspect_app", "completion": "    routes = inspect_routes(app)\n    static_routes = inspect_static_routes(app)\n    sinks = inspect_sinks(app)\n    error_handlers = inspect_error_handlers(app)\n    middleware = inspect_middleware(app)\n\n    return AppInfo(\n        routes=routes,\n        static_routes=static_routes,\n        sinks=sinks,\n        error_handlers=error_handlers,\n        middleware=middleware,\n    )\n\n", "idx": 573}
{"namespace": "falcon.inspect.inspect_routes", "completion": "    router = app._router\n    if isinstance(router, CompiledRouter):\n        routes = router.map._routes\n    else:\n        routes = router.routes\n    return [RouteInfo(route) for route in routes]\n\n", "idx": 574}
{"namespace": "falcon.inspect.inspect_static_routes", "completion": "    static_routes = app._static_routes\n    static_routes_info = []\n    for route in static_routes:\n        static_routes_info.append(StaticRouteInfo(route.path, route.resource))\n    return static_routes_info\n\n", "idx": 575}
{"namespace": "falcon.inspect.inspect_sinks", "completion": "    sinks = []\n    for sink in app._sinks:\n        info = SinkInfo(sink._route, sink._handler)\n        sinks.append(info)\n    return sinks\n\n", "idx": 576}
{"namespace": "falcon.inspect.inspect_error_handlers", "completion": "    error_handlers = []\n    for error_code, handler in app._error_handlers.items():\n        source_info, name = _get_source_info_and_name(handler)\n        info = ErrorHandlerInfo(error_code, name, source_info)\n        error_handlers.append(info)\n    return error_handlers\n\n", "idx": 577}
{"namespace": "falcon.inspect.inspect_middleware", "completion": "    middleware_info = MiddlewareInfo()\n    middleware_info.middleware_tree = app._middleware_tree\n    middleware_info.middleware_classes = app._middleware_classes\n    return middleware_info\n\n", "idx": 578}
{"namespace": "falcon.inspect.InspectVisitor.process", "completion": "        visit_name = instance.__visit_name__\n        visit_method = getattr(self, 'visit_{}'.format(visit_name), None)\n        if visit_method is None:\n            raise RuntimeError(\n                'No visit method found for {}. '\n                'Please implement a visit method for this class'.format(visit_name)\n            )\n        return visit_method(instance)\n\n", "idx": 579}
{"namespace": "falcon.request.Request.forwarded", "completion": "        if self._cached_forwarded is None:\n            self._cached_forwarded = helpers.parse_forwarded(\n                self.env.get('HTTP_FORWARDED')\n            )\n\n        return self._cached_forwarded\n", "idx": 580}
{"namespace": "falcon.request.Request.client_accepts_msgpack", "completion": "        return self.client_accepts('application/x-msgpack') or self.client_accepts(\n            'application/msgpack'\n        )\n", "idx": 581}
{"namespace": "falcon.request.Request.content_length", "completion": "        try:\n            return int(self.env['CONTENT_LENGTH'])\n        except (KeyError, ValueError):\n            return None\n", "idx": 582}
{"namespace": "falcon.request.Request.bounded_stream", "completion": "        if self._bounded_stream is None:\n            self._bounded_stream = BoundedStream(self.stream, self.content_length)\n\n        return self._bounded_stream\n", "idx": 583}
{"namespace": "falcon.request.Request.uri", "completion": "        if self._cached_uri is None:\n            self._cached_uri = '%s://%s%s' % (self.scheme, self.netloc, self.relative_uri)\n\n        return self._cached_uri\n", "idx": 584}
{"namespace": "falcon.request.Request.forwarded_uri", "completion": "        if self._cached_forwarded_uri is None:\n            # PERF(kgriffs): Since the Forwarded header is still relatively\n            # new, we expect X-Forwarded-Proto to be more common, so\n            # try to avoid calling self.forwarded if we can, since it uses a\n            # try...catch that will usually result in a relatively expensive\n            # raised exception.\n            if 'HTTP_FORWARDED' in self.env:\n                forwarded = self.forwarded\n                if forwarded:\n                    # Use first hop, fall back on own scheme\n                    scheme = forwarded[0].scheme or self.scheme\n                    host = forwarded[0].host or self.host\n                else:\n                    scheme = self.scheme\n                    host = self.netloc\n            else:\n                # PERF(kgriffs): This call should normally succeed, so\n                # just go for it without wasting time checking it\n                # first. Note also that the indexing operator is\n                # slightly faster than using get().\n                try:\n                    scheme = self.env['HTTP_X_FORWARDED_PROTO'].lower()\n                except KeyError:\n                    scheme = self.env['wsgi.url_scheme']\n\n                try:\n                    host = self.env['HTTP_X_FORWARDED_HOST']\n                except KeyError:\n                    host = self.netloc\n\n            value = scheme + '://' + host + self.relative_uri\n\n            self._cached_forwarded_uri = value\n\n        return self._cached_forwarded_uri\n", "idx": 585}
{"namespace": "falcon.request.Request.relative_uri", "completion": "        if self._cached_relative_uri is None:\n            # PERF(kgriffs): Since the relative URI is not cached, we\n            # need to generate it.\n            # PERF(kgriffs): Since the relative URI is not cached, we\n            # need to generate it.\n            # PERF(kgriffs): Since the relative URI is not cached, we\n            # need to generate it.\n            # PERF(kgriffs): Since the relative URI is not cached, we\n            # need to generate it.\n            # PERF(kgriffs): Since the relative URI is not cached, we\n            # need to generate it.\n            # PERF(kgriffs): Since the relative URI is not cached, we\n            # need to generate it.\n            # PERF(kgriffs): Since the relative URI is not cached, we\n            # need to generate it.\n            # PERF(kgriffs): Since the relative URI is not cached, we\n            # need to generate it.\n            # PERF(kgriffs): Since the relative URI is not cached, we\n            # need to generate it.\n            # PERF(kgriffs): Since the relative URI is not cached, we\n            # need to generate it.\n            # PERF(kgriffs): Since the relative URI is not cached, we\n            # need to generate it.\n            # PERF(kgriffs): Since the relative URI is not cached, we\n            # need to generate it.\n            # PERF(kgriffs): Since the relative URI is not cached, we\n            # need to generate it.\n            # PERF(kgriffs): Since the relative URI is not cached, we\n            # need to generate it.\n            # PERF(kgriffs): Since the relative URI is not cached, we\n            # need to generate it.\n            # PERF(kgriffs): Since the relative URI is not cached, we\n            # need to generate it.\n            # PERF(kgriffs): Since the relative URI is not cached, we\n            # need to generate it.\n            # PERF(kgriffs): Since the relative URI is not cached, we\n            # need to generate it.\n            # PERF(kgriffs): Since the relative URI is not cached, we\n            # need to generate it.\n            # PERF(kgriffs): Since the relative URI", "idx": 586}
{"namespace": "falcon.request.Request.prefix", "completion": "        if self._cached_prefix is None:\n            # PERF(kgriffs): Since the Forwarded header is still relatively\n            # new, we expect X-Forwarded-Proto to be more common, so\n            # try to avoid calling self.forwarded if we can, since it uses a\n            # try...catch that will usually result in a relatively expensive\n            # raised exception.\n            if 'HTTP_FORWARDED' in self.env:\n                forwarded = self.forwarded\n                if forwarded:\n                    # Use first hop, fall back on own scheme\n                    scheme = forwarded[0].scheme or self.scheme\n                else:\n                    scheme = self.scheme\n            else:\n                # PERF(kgriffs): This call should normally succeed, so\n                # just go for it without wasting time checking it\n                # first. Note also that the indexing operator is\n                # slightly faster than using get().\n                try:\n                    scheme = self.env['HTTP_X_FORWARDED_PROTO'].lower()\n                except KeyError:\n                    scheme = self.env['wsgi.url_scheme']\n\n            # PERF: For small numbers of items, '+' is faster\n            # than ''.join(...). Concatenation is also generally\n            # faster than formatting.\n            value = scheme + '://' + self.netloc + self.app\n\n            self._cached_prefix = value\n\n        return self._cached_prefix\n", "idx": 587}
{"namespace": "falcon.request.Request.forwarded_prefix", "completion": "        if self._cached_forwarded_prefix is None:\n            self._cached_forwarded_prefix = (\n                self.forwarded_scheme + '://' + self.forwarded_host + self.app\n            )\n\n        return self._cached_forwarded_prefix\n", "idx": 588}
{"namespace": "falcon.request.Request.host", "completion": "        try:\n            return self.env['HTTP_HOST']\n        except KeyError:\n            return self.env['SERVER_NAME']\n", "idx": 589}
{"namespace": "falcon.request.Request.subdomain", "completion": "        host = self.host\n        if '.' in host:\n            subdomain, sep, remainder = host.partition('.')\n            return subdomain\n        else:\n            return None\n", "idx": 590}
{"namespace": "falcon.request.Request.headers", "completion": "        if self._cached_headers is None:\n            self._cached_headers = self.env.copy()\n\n        return self._cached_headers\n", "idx": 591}
{"namespace": "falcon.request.Request.remote_addr", "completion": "        try:\n            return self.env['REMOTE_ADDR']\n        except KeyError:\n            return '127.0.0.1'\n", "idx": 592}
{"namespace": "falcon.request.Request.client_accepts", "completion": "        # NOTE(kgriffs): According to PEP-3333, the Accept header is supposed to be case-insensitive.\n        #\n        # PERF(kgriffs): Since the Accept header is supposed to be case-insensitive, we can just\n        #   use the lower() method to normalize the value.\n        #\n        # PERF(kgriffs): Since the Accept header is supposed to be case-insensitive, we can just\n        #   use the lower() method to normalize the value.\n        #\n        # PERF(kgriffs): Since the Accept header is supposed to be case-insensitive, we can just\n        #   use the lower() method to normalize the value.\n        #\n        # PERF(kgriffs): Since the Accept header is supposed to be case-insensitive, we can just\n        #   use the lower() method to normalize the value.\n        #\n        # PERF(kgriffs): Since the Accept header is supposed to be case-insensitive, we can just\n        #   use the lower() method to normalize the value.\n        #\n        # PERF(kgriffs): Since the Accept header is supposed to be case-insensitive, we can just\n        #   use the lower() method to normalize the value.\n        #\n        # PERF(kgriffs): Since the Accept header is supposed to be case-insensitive, we can just\n        #   use the lower() method to normalize the value.\n        #\n        # PERF(kgriffs): Since the Accept header is supposed to be case-insensitive, we can just\n        #   use the lower() method to normalize the value.\n        #\n        # PERF(kgriffs): Since the Accept header is supposed to be case-insensitive, we can just\n        #   use the lower() method to normalize the value.\n        #\n        # PERF(kgriffs): Since the Accept header is supposed to be case-insensitive, we can just\n        #   use the lower() method to normalize the value.\n        #\n        # PERF(kgriffs): Since the Accept header is supposed to be case-insensitive, we can just\n        #   use the lower() method to normalize the value.\n        #\n        # PERF(kgriffs): Since the Accept header is supposed to be case-insensitive, we can just\n        #   use", "idx": 593}
{"namespace": "falcon.request.Request.client_prefers", "completion": "        accept = self.accept\n\n        # PERF(kgriffs): Usually the following will be true, so\n        # try it first.\n        if accept == '*/*':\n            return media_types[0]\n\n        # Fall back to full-blown parsing\n        try:\n            return mimeparse.best_match(media_types, accept)\n        except ValueError:\n            return None\n", "idx": 594}
{"namespace": "falcon.request.Request.get_header", "completion": "        try:\n            return self.env['HTTP_' + name.replace('-', '_').upper()]\n        except KeyError:\n            if required:\n                raise errors.HTTPBadRequest(\n                    'The header was not found in the request, but it was required.'\n                )\n            return default\n", "idx": 595}
{"namespace": "falcon.request.Request.get_cookie_values", "completion": "        if self._cookies is None:\n            header_value = self.get_header('Cookie')\n            if header_value:\n                self._cookies = helpers.parse_cookie_header(header_value)\n            else:\n                self._cookies = {}\n\n        return self._cookies.get(name)\n", "idx": 596}
{"namespace": "falcon.response.Response.unset_cookie", "completion": "        if self._cookies is None:\n            self._cookies = http_cookies.SimpleCookie()\n\n        try:\n            self._cookies[name] = ''\n        except http_cookies.CookieError as e:  # pragma: no cover\n            # NOTE(tbug): we raise a KeyError here, to avoid leaking\n            # the CookieError to the user. SimpleCookie (well, BaseCookie)\n            # only throws CookieError on issues with the cookie key\n            raise KeyError(str(e))\n\n        if domain:\n            self._cookies[name]['domain'] = domain\n\n        if path:\n            self._cookies[name]['path'] = path\n", "idx": 597}
{"namespace": "falcon.response.Response.get_header", "completion": "        if name.lower() == 'set-cookie':\n            raise ValueError(\n                'This method does not support the \"Set-Cookie\" header format.'\n            )\n\n        return self._headers.get(name, default)\n", "idx": 598}
{"namespace": "falcon.response.Response.set_header", "completion": "        # NOTE(kgriffs): normalize name by lowercasing it\n        name = name.lower()\n\n        if name == 'set-cookie':\n            raise HeaderNotSupported('Setting Set-Cookie is not currently supported.')\n\n        self._headers[name] = value\n", "idx": 599}
{"namespace": "falcon.response.Response.delete_header", "completion": "        # NOTE(kgriffs): normalize name by lowercasing it\n        name = name.lower()\n\n        if name in self._headers:\n            del self._headers[name]\n", "idx": 600}
{"namespace": "falcon.cmd.inspect_app.route_main", "completion": "    print('The \"falcon-print-routes\" command is deprecated. ')\n    print('Please use \"falcon-inspect-app\"')\n    main()\n\n", "idx": 601}
{"namespace": "falcon.util.uri.decode", "completion": "    if not encoded_uri:\n        return ''\n\n    if unquote_plus:\n        encoded_uri = encoded_uri.replace('+', ' ')\n\n    # PERF(kgriffs): This is faster than using a regex to split on %\n    tokens = encoded_uri.split('%')\n\n    if len(tokens) == 1:\n        return encoded_uri\n\n    return _join_tokens(tokens)\n\n", "idx": 602}
{"namespace": "falcon.util.structures.ETag.dumps", "completion": "        if self.is_weak:\n            return \"W/\" + self\n        else:\n            return self\n", "idx": 603}
{"namespace": "falcon.util.structures.ETag.loads", "completion": "        if etag_str.startswith('W/'):\n            is_weak = True\n            etag_str = etag_str[2:]\n        else:\n            is_weak = False\n\n        if etag_str.startswith('\"') and etag_str.endswith('\"'):\n            etag_str = etag_str[1:-1]\n\n        return cls(etag_str, is_weak=is_weak)\n", "idx": 604}
{"namespace": "falcon.util.misc.secure_filename", "completion": "    if isinstance(filename, str):\n        filename = filename.encode('utf-8')\n\n    filename = unicodedata.normalize('NFKD', filename.decode('utf-8'))\n    filename = _UNSAFE_CHARS.sub('_', filename)\n\n    if filename[0] == '.':\n        filename = '_' + filename[1:]\n\n    return filename\n\n", "idx": 605}
{"namespace": "falcon.asgi.reader.BufferedReader.peek", "completion": "        if size == -1 or size is None:\n            size = self._buffer_len - self._buffer_pos\n\n        if size <= 0:\n            return b''\n\n        if size <= self._buffer_len - self._buffer_pos:\n            return self._buffer[self._buffer_pos : self._buffer_pos + size]\n\n        if self._buffer_pos > 0:\n            self._trim_buffer()\n\n        return await self._read_from(self._source, size)\n", "idx": 606}
{"namespace": "falcon.asgi.reader.BufferedReader.read_until", "completion": "        return await self._read_from(self._iter_delimited(delimiter, size_hint=size), size)\n", "idx": 607}
{"namespace": "falcon.routing.converters.IntConverter.convert", "completion": "        if self._num_digits is not None and len(value) != self._num_digits:\n            return None\n\n        if value.strip() != value:\n            return None\n\n        try:\n            value = int(value)\n        except ValueError:\n            return None\n\n        if self._min is not None and value < self._min:\n            return None\n\n        if self._max is not None and value > self._max:\n            return None\n\n        return value\n\n", "idx": 608}
{"namespace": "falcon.routing.converters.DateTimeConverter.convert", "completion": "        try:\n            return strptime(value, self._format_string)\n        except ValueError:\n            return None\n\n", "idx": 609}
{"namespace": "falcon.routing.util.map_http_methods", "completion": "    if suffix is not None:\n        if not isinstance(suffix, str):\n            raise TypeError('suffix is not a string')\n\n        if not suffix:\n            raise ValueError('suffix is empty')\n\n    methods = {}\n\n    for method in constants.HTTP_METHODS:\n        responder_name = 'on_' + method.lower()\n\n        if suffix is not None:\n            responder_name += suffix\n\n        if hasattr(resource, responder_name):\n            methods[method] = getattr(resource, responder_name)\n\n    return methods\n\n", "idx": 610}
{"namespace": "falcon.routing.static._BoundedFile.read", "completion": "        if size < 0:\n            size = self.remaining\n        else:\n            size = min(size, self.remaining)\n        data = self.fh.read(size)\n        self.remaining -= len(data)\n        return data\n\n", "idx": 611}
{"namespace": "authlib.oauth2.rfc6749.util.list_to_scope", "completion": "    if isinstance(scope, (set, tuple, list)):\n        return ' '.join(scope)\n    elif scope is None:\n        return None\n    else:\n        return to_unicode(scope)\n\n", "idx": 612}
{"namespace": "authlib.oauth2.rfc6749.util.extract_basic_authorization", "completion": "    if 'Authorization' not in headers:\n        return None, None\n    auth = headers['Authorization']\n    if not auth or not auth.startswith('Basic '):\n        return None, None\n    auth_token = auth[6:]\n    try:\n        auth_token = base64.b64decode(auth_token).decode('utf-8')\n    except (TypeError, binascii.Error):\n        return None, None\n    parts = auth_token.split(':')\n    if len(parts) != 2:\n        return auth_token, None\n    return parts[0], parts[1]", "idx": 613}
{"namespace": "authlib.oauth2.rfc6749.parameters.prepare_grant_uri", "completion": "    if not uri:\n        raise ValueError('Missing authorization endpoint URI')\n\n    if not client_id:\n        raise ValueError('Missing client identifier')\n\n    if not response_type:\n        raise ValueError('Missing response type')\n\n    if not isinstance(response_type, str):\n        raise ValueError('Invalid response type')\n\n    if not isinstance(client_id, str):\n        raise ValueError('Invalid client identifier')\n\n    if not isinstance(response_type, str):\n        raise ValueError('Invalid response type')\n\n    if redirect_uri and not isinstance(redirect_uri, str):\n        raise ValueError('Invalid redirect URI')\n\n    if scope and not isinstance(scope, (str, list)):\n        raise ValueError('Invalid scope')\n\n    if state and not isinstance(state, str):\n        raise ValueError('Invalid state')\n\n    if kwargs and not isinstance(kwargs, dict):\n        raise ValueError('Invalid extra arguments')\n\n    if scope:\n        scope = list_to_scope(scope)\n\n    params = {\n        'client_id': client_id,\n        'response_type': response_type,\n        'redirect_uri': redirect_uri,\n        'scope': scope,\n        'state': state,\n    }\n\n    params.update(kwargs)\n\n    return add_params_to_qs(uri, params)\n\n", "idx": 614}
{"namespace": "authlib.oauth2.rfc6749.parameters.parse_authorization_code_response", "completion": "    from authlib.common.urls import parse_query_string\n    params = parse_query_string(uriparse(uri).query)\n    code = params.get('code')\n    if not code:\n        raise MissingCodeException()\n    if state and state != params.get('state'):\n        raise MismatchingStateException()\n    return {'code': code, 'state': params.get('state')}\n\n", "idx": 615}
{"namespace": "authlib.oauth2.rfc6749.parameters.parse_implicit_response", "completion": "    fragment = urlparse.urlparse(uri).fragment\n    params = dict(urlparse.parse_qsl(fragment))\n\n    if 'access_token' not in params:\n        raise MissingCodeException()\n\n    params_state = params.get('state')\n    if state and params_state != state:\n        raise MismatchingStateException()\n\n    return params\n\n", "idx": 616}
{"namespace": "authlib.common.encoding.json_b64encode", "completion": "    if isinstance(text, dict):\n        text = json_dumps(text)\n    return urlsafe_b64encode(to_bytes(text))\n\n", "idx": 617}
{"namespace": "authlib.jose.util.extract_header", "completion": "    try:\n        header_data = urlsafe_b64decode(header_segment)\n        header = json_loads(header_data)\n        if not isinstance(header, dict):\n            raise error_cls('Invalid header')\n        return header\n    except Exception as e:\n        raise error_cls('Invalid header') from e\n\n", "idx": 618}
{"namespace": "twitter.models.TwitterModel.AsDict", "completion": "        # Create a dictionary to store the attributes and their values\n        result = {}\n\n        # Iterate through all attributes of the object\n        for key in dir(self):\n            # Skip private attributes and methods\n            if key.startswith('_'):\n                continue\n\n            # Get the value of the attribute\n            value = getattr(self, key)\n\n            # Check if the value is a list, tuple, or set\n            if isinstance(value, (list, tuple, set)):\n                # Create a list to store the dictionaries of the elements\n                list_of_dicts = []\n                # Iterate through the elements of the list\n                for element in value:\n                    # Check if the element supports the dict format\n                    if hasattr(element, 'AsDict'):\n                        # Add the dictionary representation of the element to the list\n                        list_of_dicts.append(element.AsDict())\n                    else:\n                        # Add the element directly to the list\n                        list_of_dicts.append(element)\n                # Assign the list of dictionaries to the result dictionary\n                result[key] = list_of_dicts\n            # Check if the value is a subclass of TwitterModel\n            elif isinstance(value, TwitterModel):\n                # Assign the dictionary representation of the value to the result dictionary\n                result[key] = value.AsDict()\n            # Check if the value supports the dict format\n            elif hasattr(value, 'AsDict'):\n                # Assign the dictionary representation of the value to the result dictionary\n                result[key] = value.AsDict()\n            # Assign the value directly to the result dictionary\n            else:\n                result[key] = value\n\n        # Return the result dictionary\n        return result\n", "idx": 619}
{"namespace": "twitter.models.TwitterModel.NewFromJsonDict", "completion": "        if data is None:\n            return None\n\n        data = data.copy()\n        for k, v in data.items():\n            if isinstance(v, list):\n                data[k] = [cls.NewFromJsonDict(x, **kwargs) for x in v]\n            elif isinstance(v, dict):\n                data[k] = cls.NewFromJsonDict(v, **kwargs)\n\n        return cls(**dict(list(kwargs.items()) + list(data.items())))\n", "idx": 620}
{"namespace": "twitter.api.Api._TweetTextWrap", "completion": "        if not isinstance(status, str):\n            raise TwitterError({'message': 'Status must be a string instance.'})\n\n        if not isinstance(char_lim, (int, long)):\n            raise TwitterError({'message': 'Character limit must be an integer.'})\n\n        if char_lim < 1:\n            raise TwitterError({'message': 'Character limit must be greater than 0.'})\n\n        if char_lim > 140:\n            raise TwitterError({'message': 'Character limit must be less than or equal to 140.'})\n\n        if len(status) > char_lim:\n            raise TwitterError({'message': 'Status must be less than or equal to character limit.'})\n\n        words = status.split()\n\n        if len(words) == 1:\n            if len(words[0]) > char_lim:\n                raise TwitterError({'message': 'Status must be less than or equal to character limit.'})\n            return [status]\n\n        line = []\n        line_len = 0\n        for word in words:\n            if len(word) > char_lim:\n                raise TwitterError({'message': 'Status must be less than or equal to character limit.'})\n            if line_len + len(word) > char_lim:\n                tweets.append(' '.join(line))\n                line = []\n                line_len = 0\n            line.append(word)\n            line_len += len(word)\n        tweets.append(' '.join(line))\n\n        return tweets\n", "idx": 621}
{"namespace": "databases.importer.import_from_string", "completion": "    try:\n        module_name, attr_name = import_str.split(':')\n        module = importlib.import_module(module_name)\n        attr = getattr(module, attr_name)\n        return attr\n    except (ImportError, AttributeError):\n        raise ImportFromStringError(f'Could not import \"{import_str}\"')", "idx": 622}
{"namespace": "rest_framework.reverse.reverse", "completion": "    url = django_reverse(viewname, args=args, kwargs=kwargs, **extra)\n    url = preserve_builtin_query_params(url, request)\n    return url\n\n", "idx": 623}
{"namespace": "rest_framework.serializers.Serializer.fields", "completion": "        # This is a bit of a hack, but it works.\n        # We want to avoid running the import for `ModelSerializer` if we don't have to.\n        # We know that we need to run it if a field is a `ModelSerializer` instance.\n        # We also know that we need to run it if a field is *any* class that subclasses `Serializer`.\n        # So we check for both conditions.\n        # If we don't need to run the import, we just return an empty dictionary.\n        if not any(\n            isinstance(field, Serializer)\n            for field in self._declared_fields.values()\n        ):\n            return {}\n\n        from rest_framework.serializers import ModelSerializer\n\n        # We need to import `ModelSerializer` in order to evaluate the fields.\n        # We do this here rather than at the top of the file because we want to avoid\n        # importing `ModelSerializer` if we don't have to.\n        # We also want to avoid importing it if we're running the tests, because\n        # it imports `django.db.models`, which can cause problems for some test suites.\n        # We can't just check `settings.DEBUG` because Django sets `settings.DEBUG` to\n        # `False` in some test suites.\n        # Instead, we check if `ModelSerializer` is already imported.\n        # If it is, then we know that we don't need to import it.\n        if not inspect.isclass(ModelSerializer):\n            from rest_framework.serializers import ModelSerializer\n\n        # We need to import `ModelSerializer` in order to evaluate the fields.\n        # We do this here rather than at the top of the file because we want to avoid\n        # importing `ModelSerializer` if we don't have to.\n        # We also want to avoid importing it if we're running the tests, because\n        # it imports `django.db.models`, which can cause problems for some test suites.\n        # We can't just check `settings.DEBUG` because Django sets `settings.DEBUG` to\n        # `False` in some test suites.\n        # Instead, we check if `ModelSerializer` is already imported.\n        # If it is, then we know that we don't need to import it.\n        if not inspect.isclass(ModelSerializer):\n            from rest_framework.serializers import ModelSerializer\n\n        # We need to", "idx": 624}
{"namespace": "rest_framework.parsers.JSONParser.parse", "completion": "        parser_context = parser_context or {}\n        encoding = parser_context.get('encoding', settings.DEFAULT_CHARSET)\n        try:\n            data = stream.read()\n            if isinstance(data, bytes):\n                data = data.decode(encoding)\n            return renderers.JSONRenderer().render(data)\n        except ValueError as exc:\n            raise ParseError('JSON parse error - %s' % str(exc))\n\n", "idx": 625}
{"namespace": "rest_framework.parsers.FileUploadParser.get_filename", "completion": "        parser_context = parser_context or {}\n        request = parser_context['request']\n        filename = request.GET.get('filename')\n        if filename:\n            return filename\n\n        # Try to extract the filename from the Content-Disposition header.\n        meta = request.META\n        content_disposition = meta.get('HTTP_CONTENT_DISPOSITION',\n                                       meta.get('CONTENT_DISPOSITION'))\n        if content_disposition:\n            try:\n                content_type, options = content_disposition.split(';')\n                if content_type.strip().lower() == 'form-data':\n                    for option in options.split(';'):\n                        option = option.strip()\n                        if option.startswith('filename'):\n                            filename = option.split('=')[1].strip()\n                            filename = filename.strip('\"')\n                            return filename\n            except ValueError:\n                pass\n\n        return None", "idx": 626}
{"namespace": "rest_framework.fields.is_simple_callable", "completion": "    if not callable(obj):\n        return False\n\n    if inspect.isbuiltin(obj):\n        raise BuiltinSignatureError(\n            'Built-in function signatures are not inspectable.'\n        )\n\n    if inspect.isfunction(obj) or inspect.ismethod(obj) or isinstance(obj, functools.partial):\n        signature = inspect.signature(obj)\n        for parameter in signature.parameters.values():\n            if parameter.default is parameter.empty:\n                return False\n    return True\n\n", "idx": 627}
{"namespace": "rest_framework.fields.Field.bind", "completion": "        self.field_name = field_name\n        self.parent = parent\n", "idx": 628}
{"namespace": "rest_framework.fields.Field.run_validation", "completion": "        (is_empty_value, validated_data) = self.validate_empty_values(data)\n        if is_empty_value:\n            return validated_data\n        value = self.to_internal_value(validated_data)\n        self.run_validators(value)\n        return value\n", "idx": 629}
{"namespace": "rest_framework.fields.Field.root", "completion": "        root = self\n        while root.parent is not None:\n            root = root.parent\n        return root\n", "idx": 630}
{"namespace": "rest_framework.fields.CharField.run_validation", "completion": "        if data == '' and not self.allow_blank:\n            self.fail('blank')\n        return super().run_validation(data)\n", "idx": 631}
{"namespace": "rest_framework.fields.CharField.to_internal_value", "completion": "        if isinstance(data, bool):\n            self.fail('invalid', input=data)\n        if not isinstance(data, str):\n            try:\n                data = str(data)\n            except (ValueError, TypeError):\n                self.fail('invalid', input=data)\n        if self.trim_whitespace:\n            data = data.strip()\n        return data\n", "idx": 632}
{"namespace": "rest_framework.fields.DecimalField.to_internal_value", "completion": "        if isinstance(data, str) and len(data) > self.MAX_STRING_LENGTH:\n            self.fail('max_string_length')\n\n        try:\n            if self.max_digits is not None and self.decimal_places is not None:\n                data = Decimal(data, self.max_digits, self.decimal_places, self.rounding)\n            else:\n                data = Decimal(data)\n        except (TypeError, ValueError):\n            self.fail('invalid')\n\n        return data\n", "idx": 633}
{"namespace": "rest_framework.fields.DateTimeField.to_representation", "completion": "        if value is None:\n            return None\n\n        if self.format is None or isinstance(value, str):\n            return value\n\n        if timezone.is_aware(value):\n            try:\n                value = value.astimezone(self.default_timezone())\n            except OverflowError:\n                self.fail('overflow')\n\n        return value.strftime(self.format)\n\n", "idx": 634}
{"namespace": "rest_framework.fields.ChoiceField.iter_options", "completion": "        if self.html_cutoff is None:\n            return self.choices\n\n        cutoff = self.html_cutoff\n        cutoff_text = self.html_cutoff_text\n\n        for index, choice in enumerate(self.choices):\n            if index == cutoff:\n                yield (choice, cutoff_text)\n            else:\n                yield (choice, None)\n\n", "idx": 635}
{"namespace": "rest_framework.fields.MultipleChoiceField.get_value", "completion": "        if self.name in dictionary:\n            return dictionary[self.name]\n        elif self.form.is_bound:\n            return self.default_empty_html\n        elif self.name in self.data:\n            return self.data[self.name]\n        else:\n            return self.to_internal_value(self.data[self.name])\n", "idx": 636}
{"namespace": "rest_framework.exceptions._get_error_details", "completion": "    if isinstance(data, dict):\n        return {\n            key: _get_error_details(value, default_code)\n            for key, value in data.items()\n        }\n    elif isinstance(data, list):\n        return [_get_error_details(item, default_code) for item in data]\n    elif isinstance(data, str):\n        return ErrorDetail(data, default_code)\n    else:\n        return data\n\n", "idx": 637}
{"namespace": "rest_framework.exceptions.server_error", "completion": "    return JsonResponse({'error': 'Internal Server Error'}, status=500)\n\n", "idx": 638}
{"namespace": "rest_framework.exceptions.bad_request", "completion": "    data = {\n        'error': 'Bad Request (400)'\n    }\n    return JsonResponse(data, status=status.HTTP_400_BAD_REQUEST)\n\n", "idx": 639}
{"namespace": "rest_framework.relations.RelatedField.iter_options", "completion": "        return iter_options(self)\n", "idx": 640}
{"namespace": "rest_framework.relations.PrimaryKeyRelatedField.to_internal_value", "completion": "        try:\n            f = self.pk_field or self.Meta.model._meta.pk\n        except AttributeError:\n            self.fail('incorrect_type', data_type=type(data).__name__)\n\n        if isinstance(data, str):\n            try:\n                data = f.to_python(data)\n            except (ValueError, TypeError):\n                self.fail('incorrect_type', data_type=type(data).__name__)\n        else:\n            self.fail('incorrect_type', data_type=type(data).__name__)\n\n        try:\n            obj = self.get_queryset().get(**{f.name: data})\n        except ObjectDoesNotExist:\n            self.fail('does_not_exist', pk_value=data)\n        except (TypeError, ValueError):\n            raise\n        except AttributeError:\n            msg = (\n                'Invalid value for `pk_field` on field {field_name}. '\n                'Related field has no `pk_field` attribute.'\n            )\n            raise ImproperlyConfigured(msg.format(field_name=self.field_name))\n        return obj\n", "idx": 641}
{"namespace": "rest_framework.relations.PrimaryKeyRelatedField.to_representation", "completion": "        if self.pk_field is not None:\n            return self.pk_field.to_representation(value.pk)\n        return value.pk\n\n", "idx": 642}
{"namespace": "rest_framework.relations.SlugRelatedField.to_internal_value", "completion": "        try:\n            if isinstance(data, bool):\n                raise TypeError\n            return queryset.get(slug=data)\n        except ValueError:\n            exc = ObjectValueError(str(sys.exc_info()[1]))\n            raise exc.with_traceback(sys.exc_info()[2])\n        except TypeError:\n            exc = ObjectTypeError(str(sys.exc_info()[1]))\n            raise exc.with_traceback(sys.exc_info()[2])\n", "idx": 643}
{"namespace": "rest_framework.templatetags.rest_framework.add_query_param", "completion": "    uri = iri_to_uri(request.get_full_path())\n    uri = uri.replace(key + '=' + request.GET.get(key, ''), '')\n    uri = uri.replace('?', '?{}={}&'.format(key, val))\n    uri = uri.replace('&', '?{}={}&'.format(key, val))\n    return smart_urlquote(uri)\n\n", "idx": 644}
{"namespace": "rest_framework.utils.mediatypes._MediaType.match", "completion": "        if self.main_type == '*' and self.sub_type == '*':\n            return True\n        if self.main_type == '*' and self.sub_type == other.sub_type:\n            return True\n        if self.main_type == other.main_type and self.sub_type == '*':\n            return True\n        if self.main_type == other.main_type and self.sub_type == other.sub_type:\n            return True\n        return False\n", "idx": 645}
{"namespace": "rest_framework.utils.mediatypes._MediaType.precedence", "completion": "        if self.main_type == '*' and self.sub_type == '*':\n            return 0\n        elif self.sub_type == '*':\n            return 1\n        elif self.params:\n            return 2\n        else:\n            return 3", "idx": 646}
{"namespace": "rest_framework.utils.mediatypes._MediaType.__str__", "completion": "        ret = self.main_type + '/' + self.sub_type\n        for key, value in self.params.items():\n            ret += '; ' + key + '=' + value\n        return ret\n", "idx": 647}
{"namespace": "asyncpg._testbase.TestCase.assertLoopErrorHandlerCalled", "completion": "        def handler(loop, context):\n            if msg_re.search(context['message']):\n                raise AssertionError(\n                    'loop error handler called with message: {}'.format(\n                        context['message']))\n\n        old_handler = self.loop.get_exception_handler()\n        self.loop.set_exception_handler(handler)\n        try:\n            yield\n        finally:\n            self.loop.set_exception_handler(old_handler)\n", "idx": 648}
{"namespace": "csvs_to_sqlite.utils.refactor_dataframes", "completion": "    for df in dataframes:\n        for column, (table_name, value_column) in foreign_keys.items():\n            if column in df.columns:\n                lookup_table = LookupTable(\n                    conn, table_name, value_column, index_fts\n                )\n                df[column] = df[column].apply(lookup_table.id_for_value)\n\n", "idx": 649}
{"namespace": "sqlitedict.SqliteDict.iteritems", "completion": "        GET_ITEMS = 'SELECT key, value FROM \"%s\" ORDER BY rowid' % self.tablename\n        for key, value in self.conn.select(GET_ITEMS):\n            yield self.decode_key(key), self.decode(value)\n", "idx": 650}
{"namespace": "sqlitedict.SqliteDict.update", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to write to read-only SqliteDict')\n\n        if items:\n            if hasattr(items, 'keys'):\n                items = items.items()\n            for key, value in items:\n                self[key] = value\n        for key, value in kwds.items():\n            self[key] = value\n        if self.autocommit:\n            self.commit()\n", "idx": 651}
{"namespace": "sqlitedict.SqliteDict.clear", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to clear read-only SqliteDict')\n\n        CLEAR_TABLE = 'DELETE FROM \"%s\"' % self.tablename\n        self.conn.execute(CLEAR_TABLE)\n        if self.autocommit:\n            self.commit()\n", "idx": 652}
{"namespace": "sqlitedict.SqliteDict.commit", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to commit read-only SqliteDict')\n\n        if blocking:\n            self.conn.commit()\n        else:\n            self.conn.commit_async()\n", "idx": 653}
{"namespace": "sqlitedict.SqliteDict.terminate", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to delete read-only SqliteDict')\n\n        self.close()\n        if not self.in_temp:\n            try:\n                os.remove(self.filename)\n            except Exception:\n                pass\n\n", "idx": 654}
{"namespace": "boto.utils.retry_url", "completion": "    # Create a proxy handler with the default proxy settings\n    proxy_handler = urllib.request.ProxyHandler()\n\n    # Create an opener with the proxy handler\n    opener = urllib.request.build_opener(proxy_handler)\n\n    # Install the opener as the default opener\n    urllib.request.install_opener(opener)\n\n    # Set the default timeout value if not provided\n    if timeout is None:\n        timeout = 1.0\n\n    # Retry the request for the specified number of times\n    for i in range(num_retries):\n        try:\n            # Open the URL and read the result\n            response = urllib.request.urlopen(url, timeout=timeout)\n            result = response.read()\n            return result\n        except urllib.error.HTTPError as e:\n            if e.code == 404 and retry_on_404:\n                continue\n            else:\n                raise\n        except urllib.error.URLError as e:\n            if i < num_retries - 1:\n                continue\n            else:\n                raise\n\n    # If the request fails after the specified number of retries, raise an exception\n    raise Exception(\"Failed to open URL after %d retries\" % num_retries)\n\n", "idx": 655}
{"namespace": "boto.utils.LazyLoadMetadata.values", "completion": "        self._materialize()\n        return super(LazyLoadMetadata, self).values()\n", "idx": 656}
{"namespace": "boto.utils.get_instance_userdata", "completion": "    try:\n        userdata_url = _build_instance_metadata_url(url, version, 'user-data')\n        userdata = retry_url(userdata_url, num_retries=num_retries, timeout=timeout)\n        if sep:\n            userdata = dict(item.split(sep) for item in userdata.split('\\n'))\n        return userdata\n    except urllib.error.URLError:\n        return None\n\n", "idx": 657}
{"namespace": "boto.utils.pythonize_name", "completion": "    if not isinstance(name, six.string_types):\n        name = six.text_type(name)\n\n    if not name:\n        return name\n\n    if name.isupper():\n        name = name.lower()\n\n    if name.islower():\n        name = name.title()\n\n    if not name.islower():\n        name = name.lower()\n\n    if not name.islower():\n        name = name.lower()\n\n    if not name.islower():\n        name = name.lower()\n\n    if not name.islower():\n        name = name.lower()\n\n    if not name.islower():\n        name = name.lower()\n\n    if not name.islower():\n        name = name.lower()\n\n    if not name.islower():\n        name = name.lower()\n\n    if not name.islower():\n        name = name.lower()\n\n    if not name.islower():\n        name = name.lower()\n\n    if not name.islower():\n        name = name.lower()\n\n    if not name.islower():\n        name = name.lower()\n\n    if not name.islower():\n        name = name.lower()\n\n    if not name.islower():\n        name = name.lower()\n\n    if not name.islower():\n        name = name.lower()\n\n    if not name.islower():\n        name = name.lower()\n\n    if not name.islower():\n        name = name.lower()\n\n    if not name.islower():\n        name = name.lower()\n\n    if not name.islower():\n        name = name.lower()\n\n    if not name.islower():\n        name = name.lower()\n\n    if not name.islower():\n        name = name.lower()\n\n    if not name.islower():\n        name = name.lower()\n\n    if not name.islower():\n        name = name.lower()\n\n    if not name.islower():\n        name = name.lower()\n\n    if not name.islower():\n        name = name.lower()\n\n    if not name.islower():\n        name = name.lower()\n\n    if not name.islower():\n        name = name.lower()\n\n    if not name.islower():\n        name = name.lower()\n\n    if not name.islower():\n        name = name.lower()\n\n    if not name.islower():\n        name = name.lower()\n\n    if not name.", "idx": 658}
{"namespace": "boto.cloudsearchdomain.connect_to_region", "completion": "    from boto.cloudsearchdomain.layer1 import CloudSearchDomainConnection\n    return CloudSearchDomainConnection(region=RegionInfo(None, region_name, 'cloudsearchdomain'), **kw_params)", "idx": 659}
{"namespace": "boto.redshift.connect_to_region", "completion": "    from boto.redshift.layer1 import RedshiftConnection\n    return RedshiftConnection(region=RegionInfo(None, region_name, 'redshift'), **kw_params)", "idx": 660}
{"namespace": "boto.support.connect_to_region", "completion": "    from boto.support.layer1 import SupportConnection\n    return SupportConnection(region_name, **kw_params)", "idx": 661}
{"namespace": "boto.configservice.connect_to_region", "completion": "    from boto.configservice.layer1 import ConfigServiceConnection\n    return ConfigServiceConnection(region_name, **kw_params)", "idx": 662}
{"namespace": "boto.cloudhsm.connect_to_region", "completion": "    from boto.cloudhsm.layer1 import CloudHSMConnection\n    return CloudHSMConnection(region=RegionInfo(None, region_name, 'cloudhsm'), **kw_params)", "idx": 663}
{"namespace": "boto.logs.connect_to_region", "completion": "    from boto.logs.layer1 import CloudWatchLogsConnection\n    return CloudWatchLogsConnection(region_name, **kw_params)", "idx": 664}
{"namespace": "boto.cloudsearch.connect_to_region", "completion": "    from boto.cloudsearch.layer1 import Layer1\n    return Layer1(region=get_region(region_name), **kw_params)\n\n", "idx": 665}
{"namespace": "boto.glacier.job.Job.download_to_fileobj", "completion": "        num_chunks = self._calc_num_chunks(chunk_size)\n        self._download_to_fileob(output_file, num_chunks, chunk_size,\n                                 verify_hashes, retry_exceptions)\n", "idx": 666}
{"namespace": "boto.glacier.utils.minimum_part_size", "completion": "    if size_in_bytes > MAXIMUM_NUMBER_OF_PARTS * default_part_size:\n        raise ValueError(\"File size exceeds the maximum allowed archive size.\")\n\n    if size_in_bytes <= default_part_size:\n        return default_part_size\n\n    number_of_parts = int(math.ceil(float(size_in_bytes) / default_part_size))\n    return int(math.ceil(float(size_in_bytes) / number_of_parts))\n\n", "idx": 667}
{"namespace": "boto.glacier.utils.chunk_hashes", "completion": "    if not bytestring:\n        return [hashlib.sha256(b'').digest()]\n\n    hashes = []\n    for i in range(0, len(bytestring), chunk_size):\n        hashes.append(hashlib.sha256(bytestring[i:i + chunk_size]).digest())\n\n    return hashes\n\n", "idx": 668}
{"namespace": "boto.glacier.utils.compute_hashes_from_fileobj", "completion": "    # Initialize the linear hash and tree hash\n    linear_hash = hashlib.sha256()\n    tree_hash = hashlib.sha256()\n\n    # Read the file in chunks\n    while True:\n        chunk = fileobj.read(chunk_size)\n        if not chunk:\n            break\n\n        # Update the linear hash\n        linear_hash.update(chunk)\n\n        # Update the tree hash\n        tree_hash.update(chunk)\n\n    # Return the linear and tree hashes in hexadecimal format\n    return linear_hash.hexdigest(), tree_hash.hexdigest()\n\n", "idx": 669}
{"namespace": "boto.glacier.concurrent.ConcurrentTransferer._calculate_required_part_size", "completion": "        # Calculate the number of parts required for the specified part size\n        num_parts = int(math.ceil(float(total_size) / self._part_size))\n\n        # Calculate the minimum required part size\n        min_part_size = int(math.ceil(float(total_size) / 10000))\n\n        # If the specified part size is less than the minimum required part size, then use the minimum required part size\n        if self._part_size < min_part_size:\n            self._part_size = min_part_size\n\n        # Calculate the final part size to be used for concurrent transfer\n        final_part_size = int(math.ceil(float(total_size) / num_parts))\n\n        # Return the total number of parts and the final part size to be used for concurrent transfer\n        return num_parts, final_part_size\n", "idx": 670}
{"namespace": "boto.glacier.connect_to_region", "completion": "    from boto.glacier.layer2 import Layer2\n    return Layer2(region=RegionInfo(name=region_name, endpoint=region_name), **kw_params)", "idx": 671}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.update", "completion": "        if self.id is None:\n            raise ValueError('Cannot update a NetworkInterface without an ID')\n\n        if dry_run:\n            return 'dry-run'\n\n        try:\n            updated = self.connection.get_all_network_interfaces(\n                [self.id],\n                dry_run=dry_run\n            )[0]\n        except BotoClientError as e:\n            if validate and e.status == 400 and e.reason == 'InvalidNetworkInterfaceID.NotFound':\n                raise ValueError('ENI %s does not exist' % self.id)\n            else:\n                raise e\n\n        self._update(updated)\n        return self.status\n", "idx": 672}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.attach", "completion": "        try:\n            self.connection.attach_network_interface(\n                self.id,\n                instance_id,\n                device_index,\n                dry_run=dry_run\n            )\n            return True\n        except BotoClientError as e:\n            if e.error_code == 'InvalidNetworkInterfaceID.NotFound':\n                raise ValueError('%s is not a valid ENI ID' % self.id)\n            elif e.error_code == 'InvalidInstanceID.NotFound':\n                raise ValueError('%s is not a valid instance ID' % instance_id)\n            else:\n                raise e\n", "idx": 673}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.detach", "completion": "        return self.connection.detach_network_interface(\n            self.id,\n            force=force,\n            dry_run=dry_run\n        )\n", "idx": 674}
{"namespace": "boto.ec2.address.Address.release", "completion": "        return self.connection.release_address(self.public_ip, self.allocation_id, dry_run)\n", "idx": 675}
{"namespace": "boto.ec2.address.Address.associate", "completion": "        if self.allocation_id:\n            return self.connection.associate_address(\n                instance_id=instance_id,\n                network_interface_id=network_interface_id,\n                private_ip_address=private_ip_address,\n                allow_reassociation=allow_reassociation,\n                allocation_id=self.allocation_id,\n                dry_run=dry_run)\n        else:\n            return self.connection.associate_address(\n                instance_id=instance_id,\n                network_interface_id=network_interface_id,\n                private_ip_address=private_ip_address,\n                allow_reassociation=allow_reassociation,\n                public_ip=self.public_ip,\n                dry_run=dry_run)\n", "idx": 676}
{"namespace": "boto.ec2.address.Address.disassociate", "completion": "        if self.allocation_id:\n            return self.connection.disassociate_address(\n                allocation_id=self.allocation_id,\n                dry_run=dry_run\n            )\n        return self.connection.disassociate_address(\n            public_ip=self.public_ip,\n            dry_run=dry_run\n        )", "idx": 677}
{"namespace": "boto.ec2.ec2object.TaggedEC2Object.add_tags", "completion": "        if not isinstance(tags, dict):\n            raise TypeError('tags must be a dict')\n        if not self.id:\n            raise ValueError('Cannot add tags to an object that has no id')\n        if not self.connection:\n            raise ValueError('Cannot add tags to an object that has no connection')\n        self.connection.create_tags(self.id, tags, dry_run)\n", "idx": 678}
{"namespace": "boto.ec2.ec2object.TaggedEC2Object.remove_tags", "completion": "        status = self.connection.delete_tags(\n            [self.id],\n            tags,\n            dry_run=dry_run\n        )\n        if self.tags is not None:\n            self.tags.remove(tags)\n", "idx": 679}
{"namespace": "boto.ec2.connection.EC2Connection.get_all_instance_status", "completion": "        params = {}\n        if instance_ids:\n            self.build_list_params(params, instance_ids, 'InstanceId')\n        if filters:\n            if 'group-id' in filters:\n                gid = filters.get('group-id')\n                if not gid.startswith('sg-') or len(gid) != 11:\n                    warnings.warn(\n                        \"The group-id filter now requires a security group \"\n                        \"identifier (sg-*) instead of a group name. To filter \"\n                        \"by group name use the 'group-name' filter instead.\",\n                        UserWarning)\n            self.build_filter_params(params, filters)\n        if dry_run:\n            params['DryRun'] = 'true'\n        if max_results is not None:\n            params['MaxResults'] = max_results\n        if next_token:\n            params['NextToken'] = next_token\n        if include_all_instances:\n            params['IncludeAllInstances'] = 'true'\n        return self.get_list('DescribeInstanceStatus', params,\n                             [('item', InstanceStatus)], verb='POST')\n", "idx": 680}
{"namespace": "boto.ec2.volume.Volume.update", "completion": "        if self.id is None:\n            raise ValueError('Volume ID is not set')\n        if self.connection is None:\n            raise ValueError('Connection is not set')\n\n        params = {'VolumeId': self.id}\n        if dry_run:\n            params['DryRun'] = 'true'\n        response = self.connection.get_object('DescribeVolumes', params,\n                                               Volume, verb='POST')\n        if response.status == 'error':\n            if validate:\n                raise ValueError(response.errors[0].message)\n            else:\n                return None\n        else:\n            self._update(response)\n            return self.status\n", "idx": 681}
{"namespace": "boto.ec2.volume.Volume.attach", "completion": "        return self.connection.attach_volume(self.id, instance_id, device, dry_run=dry_run)\n", "idx": 682}
{"namespace": "boto.ec2.volume.Volume.detach", "completion": "        return self.connection.detach_volume(\n            self.id,\n            force=force,\n            dry_run=dry_run\n        )\n", "idx": 683}
{"namespace": "boto.ec2.volume.Volume.create_snapshot", "completion": "        return self.connection.create_snapshot(self.id, description, dry_run)\n", "idx": 684}
{"namespace": "boto.ec2.volume.Volume.attachment_state", "completion": "        if self.attach_data:\n            return self.attach_data.status\n        else:\n            return None\n", "idx": 685}
{"namespace": "boto.ec2.securitygroup.SecurityGroup.add_rule", "completion": "        if self.vpc_id:\n            if src_group_group_id:\n                self.rules.append(\n                    IPPermission(\n                        ip_protocol,\n                        from_port,\n                        to_port,\n                        src_group_group_id\n                    )\n                )\n            else:\n                self.rules.append(\n                    IPPermission(\n                        ip_protocol,\n                        from_port,\n                        to_port,\n                        cidr_ip\n                    )\n                )\n        else:\n            if src_group_name:\n                self.rules.append(\n                    IPPermission(\n                        ip_protocol,\n                        from_port,\n                        to_port,\n                        src_group_name,\n                        src_group_owner_id\n                    )\n                )\n            else:\n                self.rules.append(\n                    IPPermission(\n                        ip_protocol,\n                        from_port,\n                        to_port,\n                        cidr_ip\n                    )\n                )\n", "idx": 686}
{"namespace": "boto.ec2.connect_to_region", "completion": "    regions = get_regions('ec2', connection_cls=EC2Connection)\n    for region in regions:\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 687}
{"namespace": "boto.ec2.cloudwatch.connect_to_region", "completion": "    return boto.connect_to_region(region_name, **kw_params)\n\n", "idx": 688}
{"namespace": "boto.ec2.autoscale.connect_to_region", "completion": "    return AutoScaleConnection(region_name, **kw_params)\n\n", "idx": 689}
{"namespace": "boto.ec2.elb.connect_to_region", "completion": "    return ELBConnection(region=RegionInfo(None, region_name, RegionData), **kw_params)\n\n", "idx": 690}
{"namespace": "boto.ec2.elb.ELBConnection.get_all_load_balancers", "completion": "        params = {}\n        if load_balancer_names:\n            self.build_list_params(params, load_balancer_names,\n                                   'LoadBalancerNames.member.%d')\n        if marker:\n            params['Marker'] = marker\n        return self.get_list('DescribeLoadBalancers', params,\n                             [('member', LoadBalancer)], verb='POST')\n", "idx": 691}
{"namespace": "boto.ec2.elb.ELBConnection.disable_availability_zones", "completion": "        params = {'LoadBalancerName': load_balancer_name}\n        self.build_list_params(params, zones_to_remove,\n                               'AvailabilityZones.member.%d')\n        obj = self.get_object('DisableAvailabilityZonesForLoadBalancer',\n                              params, LoadBalancerZones)\n        return obj.zones\n", "idx": 692}
{"namespace": "boto.awslambda.connect_to_region", "completion": "    from boto.awslambda.layer1 import AWSLambdaConnection\n    return AWSLambdaConnection(region=RegionInfo(name=region_name, endpoint=region_name), **kw_params)", "idx": 693}
{"namespace": "boto.cognito.identity.connect_to_region", "completion": "    from boto.cognito.identity.layer1 import CognitoIdentityConnection\n    return CognitoIdentityConnection(region_name, **kw_params)", "idx": 694}
{"namespace": "boto.cognito.sync.connect_to_region", "completion": "    from boto.cognito.sync.layer1 import CognitoSyncConnection\n    return CognitoSyncConnection(region=RegionInfo(None, region_name, 'cognito-sync'), **kw_params)", "idx": 695}
{"namespace": "boto.cloudformation.connect_to_region", "completion": "    region = RegionInfo(\n        name=region_name,\n        endpoint=RegionData.get(region_name, {}).get('endpoint'),\n        connection_cls=CloudFormationConnection\n    )\n    return region.connect(**kw_params)", "idx": 696}
{"namespace": "boto.route53.zone.Zone.find_records", "completion": "        name = self.route53connection._make_qualified(name)\n        if identifier is not None:\n            try:\n                int(identifier[1])\n                weight = identifier[1]\n                identifier = identifier[0]\n            except:\n                region = identifier[1]\n                identifier = identifier[0]\n        else:\n            weight = None\n            region = None\n        return self.route53connection.get_all_rrsets(self.id, name, type, desired, all, identifier, weight, region)\n", "idx": 697}
{"namespace": "boto.route53.domains.connect_to_region", "completion": "    from boto.route53.domains.layer1 import Route53DomainsConnection\n    region = RegionInfo(name=region_name, endpoint='route53domains.amazonaws.com')\n    return Route53DomainsConnection(region=region, **kw_params)", "idx": 698}
{"namespace": "boto.s3.key.Key.get_contents_to_filename", "completion": "        with open(filename, 'wb') as fp:\n            self.get_contents_to_file(fp, headers, cb, num_cb, torrent,\n                                      version_id, res_download_handler,\n                                      response_headers)\n", "idx": 699}
{"namespace": "boto.s3.cors.CORSConfiguration.add_rule", "completion": "        rule = CORSRule(allowed_method, allowed_origin, id, allowed_header, max_age_seconds, expose_header)\n        self.append(rule)", "idx": 700}
{"namespace": "boto.s3.bucket.Bucket.get_key", "completion": "        if validate:\n            try:\n                self.connection.provider.get_object_head(self.name, key_name, headers=headers, version_id=version_id, response_headers=response_headers)\n            except boto.exception.S3ResponseError as e:\n                if e.status == 404:\n                    return None\n                else:\n                    raise e\n        return self.key_class(self, key_name, version_id=version_id)\n", "idx": 701}
{"namespace": "boto.s3.bucket.Bucket.new_key", "completion": "        if key_name is None:\n            key_name = ''\n        k = self.key_class(self)\n        k.name = key_name\n        return k\n", "idx": 702}
{"namespace": "boto.s3.bucket.Bucket.delete_key", "completion": "        if not key_name:\n            raise ValueError('Empty key names are not allowed')\n        if version_id:\n            key = DeleteMarker(self, key_name, version_id)\n        else:\n            key = Key(self, key_name)\n        provider = self.connection.provider\n        query_args = 'delete'\n        data = u\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\"\"\"\n        data += u\"<Delete>\"\n        data += u\"<Object><Key>%s</Key>\" % xml.sax.saxutils.escape(key_name)\n        if version_id:\n            data += u\"<VersionId>%s</VersionId>\" % version_id\n        data += u\"</Object></Delete>\"\n        data = data.encode('utf-8')\n        fp = BytesIO(data)\n        md5 = boto.utils.compute_md5(fp)\n        headers = headers or {}\n        headers['Content-MD5'] = md5[1]\n        headers['Content-Type'] = 'text/xml'\n        if mfa_token:\n            headers[provider.mfa_header] = ' '.join(mfa_token)\n        response = self.connection.make_request('POST', self.name,\n                                                headers=headers,\n                                                query_args=query_args,\n                                                data=data)\n        body = response.read()\n        if response.status == 200:\n            h = handler.XmlHandler(key, self)\n            if not isinstance(body, bytes):\n                body = body.encode('utf-8')\n            xml.sax.parseString(body, h)\n            return key\n        else:\n            raise provider.storage_response_error(response.status,\n                                                  response.reason,\n                                                  body)\n", "idx": 703}
{"namespace": "boto.s3.bucket.Bucket.get_tags", "completion": "        response = self.connection.make_request('GET', self.name, query_args='tagging', headers=headers)\n        body = response.read()\n        boto.log.debug(body)\n        if response.status == 200:\n            tags = Tags()\n            h = handler.XmlHandler(tags, self)\n            if not isinstance(body, bytes):\n                body = body.encode('utf-8')\n            xml.sax.parseString(body, h)\n            return tags\n        else:\n            raise self.connection.provider.storage_response_error(\n                response.status, response.reason, body)\n", "idx": 704}
{"namespace": "boto.s3.connection.S3Connection._required_auth_capability", "completion": "        # If the user has specified a host, then we can assume that they are using the\n        # default calling format.\n        if self.host != self.DefaultHost:\n            return ['hmac-v2']\n        else:\n            return ['hmac-v2', 'hmac-v4-s3']\n", "idx": 705}
{"namespace": "boto.s3.connection.S3Connection.generate_url_sigv4", "completion": "        if headers is None:\n            headers = {}\n        if response_headers is None:\n            response_headers = {}\n        if iso_date is None:\n            iso_date = boto.utils.get_iso_8601_datetime()\n        if version_id is not None:\n            key = boto.utils.get_utf8_value(key)\n            key = '%s?versionId=%s' % (key, version_id)\n        if force_http:\n            protocol = 'http'\n        else:\n            protocol = 'https'\n        url_base = self.calling_format.build_url_base(self, protocol,\n                                                      self.server_name(),\n                                                      bucket, key)\n        if response_headers:\n            headers['response-content-type'] = response_headers.get('Content-Type')\n            headers['response-content-language'] = response_headers.get('Content-Language')\n            headers['response-expires'] = response_headers.get('Expires')\n            headers['response-cache-control'] = response_headers.get('Cache-Control')\n            headers['response-content-disposition'] = response_headers.get('Content-Disposition')\n            headers['response-content-encoding'] = response_headers.get('Content-Encoding')\n        request = self._auth_handler.generate_request(method, url_base, headers, iso_date)\n        return self._auth_handler.generate_presigned_url(request, expires_in)\n", "idx": 706}
{"namespace": "boto.s3.lifecycle.Lifecycle.add_rule", "completion": "        rule = Rule(id, prefix, status, expiration, transition)\n        self.append(rule)\n", "idx": 707}
{"namespace": "boto.s3.website.WebsiteConfiguration.to_xml", "completion": "        xml = []\n        if self.suffix is not None:\n            xml.append(tag('IndexDocument',\n                            tag('Suffix', self.suffix)))\n        if self.error_key is not None:\n            xml.append(tag('ErrorDocument',\n                            tag('Key', self.error_key)))\n        if self.redirect_all_requests_to is not None:\n            xml.append(tag('RedirectAllRequestsTo',\n                            self.redirect_all_requests_to.to_xml()))\n        if self.routing_rules is not None:\n            xml.append(self.routing_rules.to_xml())\n        return ''.join(xml)\n\n", "idx": 708}
{"namespace": "boto.s3.website.RoutingRules.to_xml", "completion": "        parts = []\n        for rule in self:\n            parts.append(rule.to_xml())\n        return tag('RoutingRules', ''.join(parts))\n\n", "idx": 709}
{"namespace": "boto.s3.website.RoutingRule.when", "completion": "        condition = Condition(key_prefix=key_prefix, http_error_code=http_error_code)\n        return cls(condition=condition)\n", "idx": 710}
{"namespace": "boto.s3.website.RoutingRule.then_redirect", "completion": "        self.redirect = Redirect(hostname=hostname, protocol=protocol,\n                                 replace_key=replace_key,\n                                 replace_key_prefix=replace_key_prefix,\n                                 http_redirect_code=http_redirect_code)\n        return self\n\n", "idx": 711}
{"namespace": "boto.s3.connect_to_region", "completion": "    if 'host' in kw_params:\n        region = S3RegionInfo(name=region_name, endpoint=kw_params['host'])\n        return region.connect(**kw_params)\n    else:\n        return S3Connection(region=region_name, **kw_params)", "idx": 712}
{"namespace": "boto.directconnect.connect_to_region", "completion": "    from boto.directconnect.layer1 import DirectConnectConnection\n    return DirectConnectConnection(region=RegionInfo(None, region_name, 'directconnect'), **kw_params)", "idx": 713}
{"namespace": "boto.rds.connect_to_region", "completion": "    return regions()[region_name].connect(**kw_params)\n\n", "idx": 714}
{"namespace": "boto.datapipeline.connect_to_region", "completion": "    from boto.datapipeline.layer1 import DataPipelineConnection\n    return DataPipelineConnection(region_name, **kw_params)", "idx": 715}
{"namespace": "boto.dynamodb.batch.Batch.to_dict", "completion": "        if self.attributes_to_get is None:\n            self.attributes_to_get = []\n\n        if self.keys is None:\n            self.keys = []\n\n        if self.table is None:\n            self.table = []\n\n        if self.consistent_read is None:\n            self.consistent_read = []\n\n        return {\n            'RequestItems': {\n                self.table.name: {\n                    'Keys': self._convert_keys_to_dict(self.keys),\n                    'AttributesToGet': self.attributes_to_get,\n                    'ConsistentRead': self.consistent_read\n                }\n            }\n        }\n", "idx": 716}
{"namespace": "boto.dynamodb.batch.BatchList.to_dict", "completion": "        batch_dict = {}\n        for batch in self:\n            batch_dict[batch.table.name] = batch.to_dict()\n        return batch_dict\n\n", "idx": 717}
{"namespace": "boto.dynamodb.types.Dynamizer.encode", "completion": "        dynamodb_type = self._get_dynamodb_type(attr)\n        if dynamodb_type == 'N':\n            return self._encode_n(attr)\n        elif dynamodb_type == 'S':\n            return self._encode_s(attr)\n        elif dynamodb_type == 'NS':\n            return self._encode_ns(attr)\n        elif dynamodb_type == 'SS':\n            return self._encode_ss(attr)\n        elif dynamodb_type == 'B':\n            return self._encode_b(attr)\n        elif dynamodb_type == 'BS':\n            return self._encode_bs(attr)\n        elif dynamodb_type == 'BOOL':\n            return self._encode_bool(attr)\n        elif dynamodb_type == 'M':\n            return self._encode_m(attr)\n        elif dynamodb_type == 'L':\n            return self._encode_l(attr)\n        elif dynamodb_type == 'NULL':\n            return self._encode_null(attr)\n        else:\n            raise TypeError('Unsupported type \"%s\" for value \"%s\"' % (type(attr), attr))\n", "idx": 718}
{"namespace": "boto.dynamodb.types.Dynamizer.decode", "completion": "        if len(attr.keys()) > 1:\n            return attr\n        dynamodb_type = list(attr.keys())[0]\n        try:\n            decoder = getattr(self, '_decode_%s' % dynamodb_type.lower())\n        except AttributeError:\n            raise ValueError(\"Unable to decode dynamodb type: %s\" %\n                             dynamodb_type)\n        return decoder(attr)\n", "idx": 719}
{"namespace": "boto.dynamodb.connect_to_region", "completion": "    from boto.dynamodb.layer2 import Layer2\n    return Layer2(region_name, **kw_params)", "idx": 720}
{"namespace": "boto.beanstalk.connect_to_region", "completion": "    from boto.beanstalk.layer1 import Layer1\n    return Layer1(region=RegionInfo(None, region_name), **kw_params)", "idx": 721}
{"namespace": "boto.swf.connect_to_region", "completion": "    region = RegionInfo(name=region_name, endpoint=REGION_ENDPOINTS[region_name])\n    return boto.swf.layer1.Layer1(region=region, **kw_params)", "idx": 722}
{"namespace": "boto.opsworks.regions", "completion": "    return [RegionInfo(name='us-east-1', endpoint='opsworks.us-east-1.amazonaws.com'), RegionInfo(name='us-west-2', endpoint='opsworks.us-west-2.amazonaws.com'), RegionInfo(name='us-west-1', endpoint='opsworks.us-west-1.amazonaws.com'), RegionInfo(name='eu-west-1', endpoint='opsworks.eu-west-1.amazonaws.com'), RegionInfo(name='ap-northeast-1', endpoint='opsworks.ap-northeast-1.amazonaws.com'), RegionInfo(name='ap-southeast-1', endpoint='opsworks.ap-southeast-1.amazonaws.com'), RegionInfo(name='ap-southeast-2', endpoint='opsworks.ap-southeast-2.amazonaws.com'), RegionInfo(name='sa-east-1', endpoint='opsworks.sa-east-1.amazonaws.com')]", "idx": 723}
{"namespace": "boto.opsworks.connect_to_region", "completion": "    from boto.opsworks.layer1 import OpsWorksConnection\n    return OpsWorksConnection(region=RegionInfo(None, region_name, 'opsworks'), **kw_params)", "idx": 724}
{"namespace": "boto.sqs.connect_to_region", "completion": "    from boto.sqs.connection import SQSConnection\n    return SQSConnection.connect_to_region(region_name, **kw_params)", "idx": 725}
{"namespace": "boto.rds2.connect_to_region", "completion": "    from boto.rds2.layer1 import RDSConnection\n    return RDSConnection(region=get_regions('rds', connection_cls=RDSConnection)[region_name], **kw_params)", "idx": 726}
{"namespace": "boto.cloudsearch2.connect_to_region", "completion": "    from boto.cloudsearch2.layer1 import CloudSearchConnection\n    return CloudSearchConnection(region_name, **kw_params)", "idx": 727}
{"namespace": "boto.cloudtrail.connect_to_region", "completion": "    from boto.cloudtrail.layer1 import CloudTrailConnection\n    return CloudTrailConnection(region=RegionInfo(None, region_name, 'cloudtrail'), **kw_params)", "idx": 728}
{"namespace": "boto.elasticache.connect_to_region", "completion": "    from boto.elasticache.layer1 import ElastiCacheConnection\n    return ElastiCacheConnection(region=RegionInfo(None, region_name, 'elasticache'), **kw_params)", "idx": 729}
{"namespace": "boto.ses.connect_to_region", "completion": "    region = get_regions('ses', connection_cls=SESConnection)[region_name]\n    return region.connect(**kw_params)", "idx": 730}
{"namespace": "boto.codedeploy.connect_to_region", "completion": "    from boto.codedeploy.layer1 import CodeDeployConnection\n    return CodeDeployConnection(region=RegionInfo(None, region_name, 'codedeploy'), **kw_params)", "idx": 731}
{"namespace": "boto.sts.credentials.Credentials.to_dict", "completion": "        return {\n            'access_key': self.access_key,\n            'secret_key': self.secret_key,\n            'session_token': self.session_token,\n            'expiration': self.expiration,\n            'request_id': self.request_id\n        }\n", "idx": 732}
{"namespace": "boto.sts.connect_to_region", "completion": "    region = None\n    for reg in regions():\n        if reg.name == region_name:\n            region = reg\n            break\n    if region is None:\n        return None\n    return STSConnection(region=region, **kw_params)", "idx": 733}
{"namespace": "boto.machinelearning.connect_to_region", "completion": "    from boto.machinelearning.layer1 import MachineLearningConnection\n    return MachineLearningConnection(region_name, **kw_params)", "idx": 734}
{"namespace": "boto.vpc.connect_to_region", "completion": "    region = RegionInfo(name=region_name, endpoint='ec2.amazonaws.com')\n    return VPCConnection(region=region, **kw_params)\n\n", "idx": 735}
{"namespace": "boto.vpc.VPCConnection.get_all_vpc_peering_connections", "completion": "        params = {}\n        if vpc_peering_connection_ids:\n            self.build_list_params(params, vpc_peering_connection_ids, 'VpcPeeringConnectionId')\n        if filters:\n            self.build_filter_params(params, filters)\n        if dry_run:\n            params['DryRun'] = 'true'\n        return self.get_list('DescribeVpcPeeringConnections', params, [('item', VPC)])\n", "idx": 736}
{"namespace": "boto.kinesis.connect_to_region", "completion": "    from boto.kinesis.layer1 import KinesisConnection\n    return KinesisConnection(region=RegionInfo(None, region_name, 'kinesis'), **kw_params)", "idx": 737}
{"namespace": "boto.ec2containerservice.connect_to_region", "completion": "    from boto.ec2containerservice.layer1 import EC2ContainerServiceConnection\n    return EC2ContainerServiceConnection(region_name, **kw_params)", "idx": 738}
{"namespace": "boto.dynamodb2.table.Table._introspect_indexes", "completion": "        return self._introspect_all_indexes(raw_indexes, self._PROJECTION_TYPE_TO_INDEX['local_indexes'])\n", "idx": 739}
{"namespace": "boto.dynamodb2.table.Table.describe", "completion": "        raw_table = self.connection.describe_table(self.table_name)\n        raw_schema = raw_table['Table']['KeySchema']\n        raw_attributes = raw_table['Table']['AttributeDefinitions']\n        raw_throughput = raw_table['Table']['ProvisionedThroughput']\n        raw_indexes = raw_table['Table'].get('LocalSecondaryIndexes', [])\n        raw_global_indexes = raw_table['Table'].get('GlobalSecondaryIndexes', [])\n\n        self.schema = self._introspect_schema(raw_schema, raw_attributes)\n        self.throughput = {\n            'read': raw_throughput['ReadCapacityUnits'],\n            'write': raw_throughput['WriteCapacityUnits'],\n        }\n        self.indexes = self._introspect_indexes(raw_indexes)\n        self.global_indexes = self._introspect_global_indexes(raw_global_indexes)\n\n        return raw_table\n", "idx": 740}
{"namespace": "boto.dynamodb2.table.Table.update", "completion": "        if throughput is not None:\n            self.throughput = throughput\n\n        if global_indexes is not None:\n            self.global_indexes = global_indexes\n\n        raw_throughput = {\n            'ReadCapacityUnits': int(self.throughput['read']),\n            'WriteCapacityUnits': int(self.throughput['write']),\n        }\n\n        kwargs = {}\n\n        kwarg_map = {\n            'global_indexes': 'global_secondary_indexes',\n        }\n        for index_attr in ('global_indexes'):\n            table_indexes = getattr(self, index_attr)\n            if table_indexes:\n                raw_indexes = []\n                for index_field in table_indexes:\n                    raw_indexes.append(index_field.schema())\n\n                kwargs[kwarg_map[index_attr]] = raw_indexes\n\n        self.connection.update_table(\n            table_name=self.table_name,\n            provisioned_throughput=raw_throughput,\n            **kwargs\n        )\n        return True\n", "idx": 741}
{"namespace": "boto.dynamodb2.table.Table.create_global_secondary_index", "completion": "        if not self.global_indexes:\n            self.describe()\n\n        if global_index.name in [gsi.name for gsi in self.global_indexes]:\n            boto.log.error('Global index with name %s already exists' % global_index.name)\n            return False\n\n        self.global_indexes.append(global_index)\n\n        self.connection.update_table(\n            self.table_name,\n            global_secondary_index_updates=[\n                {\n                    \"Create\": {\n                        \"IndexName\": global_index.name,\n                        \"KeySchema\": global_index.schema(),\n                        \"Projection\": global_index.projection(),\n                        \"ProvisionedThroughput\": {\n                            \"ReadCapacityUnits\": int(global_index.throughput['read']),\n                            \"WriteCapacityUnits\": int(global_index.throughput['write']),\n                        },\n                    },\n                },\n            ],\n        )\n\n        self.describe()\n\n        return True\n", "idx": 742}
{"namespace": "boto.dynamodb2.table.Table.delete_global_secondary_index", "completion": "        if global_index_name:\n            gsi_data = []\n\n            gsi_data.append({\n                \"Delete\": {\n                    \"IndexName\": global_index_name,\n                },\n            })\n\n            self.connection.update_table(\n                self.table_name,\n                global_secondary_index_updates=gsi_data,\n            )\n\n            return True\n        else:\n            msg = 'You need to provide the global index name to ' \\\n                  'delete_global_secondary_index method'\n            boto.log.error(msg)\n\n            return False\n", "idx": 743}
{"namespace": "boto.dynamodb2.table.Table.update_global_secondary_index", "completion": "        if global_indexes:\n            gsi_data = []\n\n            for gsi_name, gsi_throughput in global_indexes.items():\n                gsi_data.append({\n                    \"Update\": {\n                        \"IndexName\": gsi_name,\n                        \"ProvisionedThroughput\": {\n                            \"ReadCapacityUnits\": int(gsi_throughput['read']),\n                            \"WriteCapacityUnits\": int(gsi_throughput['write']),\n                        },\n                    },\n                })\n\n            self.connection.update_table(\n                self.table_name,\n                global_secondary_index_updates=gsi_data,\n            )\n\n            return True\n        else:\n            msg = 'You need to provide the global indexes to ' \\\n                  'update_global_secondary_index method'\n            boto.log.error(msg)\n\n            return False\n", "idx": 744}
{"namespace": "boto.dynamodb2.table.Table.delete", "completion": "        self.connection.delete_table(self.table_name)\n        return True\n", "idx": 745}
{"namespace": "boto.dynamodb2.table.Table.get_item", "completion": "        raw_key = self._encode_keys(kwargs)\n        raw_attributes = None\n\n        if attributes:\n            raw_attributes = self._dynamizer.encode_attributes(attributes)\n\n        result = self.connection.get_item(\n            self.table_name,\n            raw_key,\n            consistent_read=consistent,\n            attributes_to_get=raw_attributes,\n        )\n\n        if 'Item' in result:\n            return Item(self, result['Item'])\n        else:\n            raise exceptions.ItemNotFound(\n                \"The item was not found in the table.\"\n            )\n", "idx": 746}
{"namespace": "boto.dynamodb2.table.Table.has_item", "completion": "        try:\n            self.get_item(**kwargs)\n            return True\n        except exceptions.ItemNotFound:\n            return False\n", "idx": 747}
{"namespace": "boto.dynamodb2.table.Table._put_item", "completion": "        if expects:\n            expects = self._encode_keys(expects)\n        item_data = self._encode_keys(item_data)\n        self.connection.put_item(self.table_name, item_data, expects)\n        return True\n", "idx": 748}
{"namespace": "boto.dynamodb2.table.Table.delete_item", "completion": "        if not self.schema:\n            self.describe()\n        raw_key = self._encode_keys(kwargs)\n        kwargs = {}\n        if expected is not None:\n            kwargs['expected'] = expected\n        if conditional_operator is not None:\n            kwargs['conditional_operator'] = conditional_operator\n        self.connection.delete_item(self.table_name, raw_key, **kwargs)\n        return True\n", "idx": 749}
{"namespace": "boto.dynamodb2.table.Table.get_key_fields", "completion": "        if not self.schema:\n            self.describe()\n\n        fields = []\n\n        for field in self.schema:\n            fields.append(field.name)\n\n        return fields\n", "idx": 750}
{"namespace": "boto.dynamodb2.table.Table._build_filters", "completion": "        if not filter_kwargs:\n            return None\n\n        filters = {}\n\n        for key, value in filter_kwargs.items():\n            if '__' not in key:\n                raise exceptions.InvalidFilterExpression(\n                    'Invalid filter expression: %s' % key)\n            field, operator = key.split('__')\n            if operator not in using:\n                raise exceptions.InvalidFilterExpression(\n                    'Invalid filter expression: %s' % key)\n            filters[field] = {operator: value}\n\n        return filters\n", "idx": 751}
{"namespace": "boto.dynamodb2.table.Table.batch_get", "completion": "        results = ResultSet()\n        kwargs = {\n            'consistent_read': consistent,\n            'attributes_to_get': attributes,\n        }\n        results.to_call(self._batch_get, keys, **kwargs)\n        return results\n", "idx": 752}
{"namespace": "boto.dynamodb2.table.Table.count", "completion": "        return self.connection.count(self.table_name)\n", "idx": 753}
{"namespace": "boto.dynamodb2.table.BatchTable.put_item", "completion": "        if overwrite:\n            self._to_put.append(data)\n        else:\n            for item in self._to_put:\n                if item.get('Key') == data.get('Key'):\n                    return\n\n            self._to_put.append(data)\n", "idx": 754}
{"namespace": "boto.dynamodb2.table.BatchTable.delete_item", "completion": "        self._to_delete.append(kwargs)\n\n        if self.should_flush():\n            self.flush()\n", "idx": 755}
{"namespace": "boto.dynamodb2.table.BatchTable.flush", "completion": "        if self._to_put:\n            self._to_put = self.table.batch_put_item(self._to_put)\n\n        if self._to_delete:\n            self._to_delete = self.table.batch_delete_item(self._to_delete)\n\n        if self._to_put or self._to_delete:\n            self.resend_unprocessed()\n", "idx": 756}
{"namespace": "boto.dynamodb2.table.BatchTable.resend_unprocessed", "completion": "        while self._unprocessed:\n            unprocessed = self._unprocessed[:25]\n            self._unprocessed = self._unprocessed[25:]\n            self.flush()\n\n", "idx": 757}
{"namespace": "boto.dynamodb2.fields.BaseSchemaField.definition", "completion": "        return {\n            'AttributeName': self.name,\n            'AttributeType': self.data_type\n        }\n", "idx": 758}
{"namespace": "boto.dynamodb2.fields.BaseIndexField.definition", "completion": "        return [\n            {\n                'AttributeName': part.name,\n                'AttributeType': part.data_type,\n            }\n            for part in self.parts\n        ]\n", "idx": 759}
{"namespace": "boto.dynamodb2.fields.BaseIndexField.schema", "completion": "        schema = {\n            'IndexName': self.name,\n            'KeySchema': [],\n            'Projection': {\n                'ProjectionType': self.projection_type,\n            },\n        }\n\n        for part in self.parts:\n            schema['KeySchema'].append(part.schema())\n\n        return schema\n\n", "idx": 760}
{"namespace": "boto.dynamodb2.fields.GlobalBaseIndexField.schema", "completion": "        schema_data = super(GlobalBaseIndexField, self).schema()\n        schema_data['ProvisionedThroughput'] = self.throughput\n        return schema_data\n\n", "idx": 761}
{"namespace": "boto.dynamodb2.fields.GlobalIncludeIndex.schema", "completion": "        schema_data = IncludeIndex.schema(self)\n        schema_data.update(GlobalBaseIndexField.schema(self))\n        return schema_data", "idx": 762}
{"namespace": "boto.dynamodb2.items.Item.get_keys", "completion": "        keys = {}\n        for key in self.table.schema.keys:\n            keys[key.name] = self[key.name]\n        return keys\n", "idx": 763}
{"namespace": "boto.dynamodb2.items.Item.get_raw_keys", "completion": "        key_fields = self.table.get_key_fields()\n        key_data = {}\n\n        for key in key_fields:\n            key_data[key] = self._dynamizer.encode(self[key])\n\n        return key_data\n", "idx": 764}
{"namespace": "boto.dynamodb2.items.Item.build_expects", "completion": "        expects = {}\n        if fields is None:\n            fields = self.keys()\n        for field in fields:\n            if field in self._orig_data:\n                if field in self._data:\n                    if self._data[field] != self._orig_data[field]:\n                        expects[field] = self._dynamizer.encode(self._orig_data[field])\n                else:\n                    expects[field] = self._dynamizer.encode(self._orig_data[field])\n            else:\n                if field in self._data:\n                    expects[field] = self._dynamizer.encode(self._data[field])\n        return expects\n", "idx": 765}
{"namespace": "boto.dynamodb2.items.Item.prepare_full", "completion": "        encoded_data = {}\n        for field_name, field_value in self._data.items():\n            encoded_data[field_name] = self._dynamizer.encode(field_value)\n        return encoded_data\n", "idx": 766}
{"namespace": "boto.dynamodb2.items.Item.prepare_partial", "completion": "        # This doesn't save on its own. Rather, we prepare the datastructure\n        # and hand-off to the table to handle creation/update.\n        final_data = {}\n        altered_fields = set()\n\n        alterations = self._determine_alterations()\n\n        for kind in ['adds', 'changes', 'deletes']:\n            for key, value in alterations[kind].items():\n                if not self._is_storable(value):\n                    continue\n\n                final_data[key] = {\n                    'Action': kind.title(),\n                    'Value': self._dynamizer.encode(value),\n                }\n                altered_fields.add(key)\n\n        return final_data, altered_fields\n", "idx": 767}
{"namespace": "boto.dynamodb2.items.Item.partial_save", "completion": "        alterations, fields = self.prepare_partial()\n        expects = self.build_expects(fields=fields)\n\n        if not self.needs_save(data=alterations):\n            return False\n\n        return self.table.update_item(self.get_raw_keys(), alterations, expects)\n", "idx": 768}
{"namespace": "boto.dynamodb2.items.Item.save", "completion": "        if not self.needs_save():\n            return False\n\n        if overwrite:\n            final_data = self.prepare_full()\n            returned = self.table._put_item(final_data)\n        else:\n            returned = self.partial_save()\n\n        return returned", "idx": 769}
{"namespace": "boto.dynamodb2.items.Item.delete", "completion": "        key = self.get_keys()\n        returned = self.table._delete_item(key)\n        return returned", "idx": 770}
{"namespace": "boto.dynamodb2.connect_to_region", "completion": "    from boto.dynamodb2.layer1 import DynamoDBConnection\n    return DynamoDBConnection(region_name, **kw_params)", "idx": 771}
{"namespace": "litecli.packages.parseutils.extract_tables", "completion": "    parsed = sqlparse.parse(sql)[0]\n    tables = []\n    for item in extract_from_part(parsed):\n        tables.extend(extract_table_identifiers(item))\n    return tables\n\n", "idx": 772}
{"namespace": "litecli.packages.parseutils.queries_start_with", "completion": "    prefixes = [prefix.lower() for prefix in prefixes]\n    for query in queries:\n        formatted_sql = sqlparse.format(query.lower(), strip_comments=True)\n        if formatted_sql.split()[0] in prefixes:\n            return True\n    return False", "idx": 773}
{"namespace": "litecli.packages.parseutils.is_destructive", "completion": "    destructive_keywords = [\n        \"alter\",\n        \"create\",\n        \"delete\",\n        \"drop\",\n        \"insert\",\n        \"replace\",\n        \"truncate\",\n        \"update\",\n    ]\n\n    return queries_start_with(queries, destructive_keywords)", "idx": 774}
{"namespace": "litecli.packages.completion_engine.suggest_type", "completion": "    # If the text before the cursor is empty, then we are at the start of the line.\n    if not text_before_cursor:\n        return [{'type': 'keyword', 'scope': None}]\n\n    # If the text before the cursor is a space, then we are at the start of a new token.\n    if text_before_cursor.isspace():\n        return [{'type': 'keyword', 'scope': None}]\n\n    # If the text before the cursor is a semicolon, then we are at the start of a new statement.\n    if text_before_cursor == ';':\n        return [{'type': 'keyword', 'scope': None}]\n\n    # If the text before the cursor is a comma, then we are at the start of a new column.\n    if text_before_cursor == ',':\n        return [{'type': 'column', 'scope': None}]\n\n    # If the text before the cursor is a dot, then we are at the start of a new table.\n    if text_before_cursor == '.':\n        return [{'type': 'table', 'scope': None}]\n\n    # If the text before the cursor is a space, then we are at the start of a new token.\n    if text_before_cursor.isspace():\n        return [{'type': 'keyword', 'scope': None}]\n\n    # If the text before the cursor is a space, then we are at the start of a new token.\n    if text_before_cursor.isspace():\n        return [{'type': 'keyword', 'scope': None}]\n\n    # If the text before the cursor is a space, then we are at the start of a new token.\n    if text_before_cursor.isspace():\n        return [{'type': 'keyword', 'scope': None}]\n\n    # If the text before the cursor is a space, then we are at the start of a new token.\n    if text_before_cursor.isspace():\n        return [{'type': 'keyword', 'scope': None}]\n\n    # If the text before the cursor is a space, then we are at the start of a new token.\n    if text_before_cursor.isspace():\n        return [{'type': 'keyword', 'scope': None}]\n\n    # If the text before the cursor is a space, then we are at the start of a new token.\n    if text_before_cursor.isspace():\n        return", "idx": 775}
{"namespace": "datasette.plugins.get_plugins", "completion": "    plugins = []\n    for entry in pkg_resources.iter_entry_points(\"datasette\"):\n        plugin = {}\n        plugin[\"name\"] = entry.name\n        plugin[\"static_path\"] = getattr(entry.module_name, \"static_path\", None)\n        plugin[\"templates_path\"] = getattr(entry.module_name, \"templates_path\", None)\n        plugin[\"hooks\"] = pm.get_hookimpls(entry.name)\n        plugin[\"version\"] = getattr(entry.module_name, \"__version__\", None)\n        plugin[\"project_name\"] = getattr(entry.module_name, \"__project_name__\", None)\n        plugins.append(plugin)\n    return plugins", "idx": 776}
{"namespace": "datasette.facets.ColumnFacet.suggest", "completion": "        row_count = await self.get_row_count()\n        columns = await self.get_columns(self.sql, self.params)\n        facet_size = self.get_facet_size()\n        suggestions = []\n        for column in columns:\n            sql = f\"select distinct [{column}], count(*) from ({self.sql}) group by [{column}]\"\n            rows = await self.ds.execute(self.database, sql, self.params)\n            if len(rows.rows) > 1 and len(rows.rows) <= row_count:\n                if len(rows.rows) <= facet_size:\n                    if any(row[1] > 1 for row in rows.rows):\n                        suggestions.append(\n                            {\n                                \"name\": column,\n                                \"toggle_url\": path_with_added_args(\n                                    self.request.path,\n                                    {\"_facet\": column},\n                                    remove_args=[\"_facet_size\"],\n                                ),\n                            }\n                        )\n        return suggestions\n", "idx": 777}
{"namespace": "datasette.facets.ColumnFacet.facet_results", "completion": "        facet_size = self.get_facet_size()\n        row_count = await self.get_row_count()\n        columns = await self.get_columns(self.sql, self.params)\n        configs = self.get_configs()\n        facets_timed_out = []\n        results = []\n        for config in configs:\n            column = config[\"config\"][\"simple\"]\n            if column not in columns:\n                continue\n            facet_sql = \"\"\"\n                select {column} as value, count(*) as n from (\n                    {sql}\n                ) where value is not null\n                group by value\n                order by n desc\n                limit {limit}\n            \"\"\".format(\n                column=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            distinct_values = None\n            try:\n                distinct_values = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n            except QueryInterrupted:\n                facets_timed_out.append(column)\n                continue\n            num_distinct_values = len(distinct_values)\n            truncated = False\n            if num_distinct_values > facet_size:\n                distinct_values = distinct_values[:facet_size]\n                truncated = True\n            for row in distinct_values:\n                value = row[\"value\"]\n                label = value\n                if isinstance(value, str):\n                    label = value.replace(\"\\n\", \" \")\n                results.append(\n                    {\n                        \"value\": value,\n                        \"label\": label,\n                        \"count\": row[\"n\"],\n                        \"toggle_url\": self.ds.absolute_url(\n                            self.request,\n                            self.ds.urls.path(\n                                path_with_removed_args(\n                                    self.request, {\"_facet\": column}\n                                )\n                            ),\n                        ),\n                        \"selected\": True,\n                    }\n                )\n            if truncated:\n                results.append(\n                    {\n                        \"value\": None,\n                        \"label\": \"More...\",\n                        \"count\": row_count - sum(r[\"count\"] for r in distinct_values),\n                        \"toggle_url\": self.ds.absolute_url(\n                            self.request,\n                            self.ds.urls.path(\n                                path", "idx": 778}
{"namespace": "datasette.facets.ArrayFacet.suggest", "completion": "        row_count = await self.get_row_count()\n        columns = await self.get_columns(self.sql, self.params)\n        facet_size = self.get_facet_size()\n        suggested_facets = []\n        already_enabled = [c[\"config\"][\"simple\"] for c in self.get_configs()]\n        for column in columns:\n            if column in already_enabled:\n                continue\n            suggested_facet_sql = \"\"\"\n                select {column} as value, count(*) as n from (\n                    {sql}\n                ) where value is not null\n                group by value\n                limit {limit}\n            \"\"\".format(\n                column=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            distinct_values = None\n            try:\n                distinct_values = await self.ds.execute(\n                    self.database,\n                    suggested_facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_suggest_time_limit_ms\"),\n                )\n                num_distinct_values = len(distinct_values)\n                if (\n                    1 < num_distinct_values < row_count\n                    and num_distinct_values <= facet_size\n                    # And at least one has n > 1\n                    and any(r[\"n\"] > 1 for r in distinct_values)\n                ):\n                    suggested_facets.append(\n                        {\n                            \"name\": column,\n                            \"toggle_url\": self.ds.absolute_url(\n                                self.request,\n                                self.ds.urls.path(\n                                    path_with_added_args(\n                                        self.request, {\"_facet\": column}\n                                    )\n                                ),\n                            ),\n                        }\n                    )\n            except QueryInterrupted:\n                continue\n        return suggested_facets\n", "idx": 779}
{"namespace": "datasette.facets.ArrayFacet.facet_results", "completion": "        facet_results = []\n        facets_timed_out = []\n\n        qs_pairs = self.get_querystring_pairs()\n\n        facet_size = self.get_facet_size()\n        for source_and_config in self.get_configs():\n            config = source_and_config[\"config\"]\n            source = source_and_config[\"source\"]\n            column = config.get(\"column\") or config[\"simple\"]\n            facet_sql = \"\"\"\n                select json_each({col}) as value, count(*) as count from (\n                    {sql}\n                )\n                where {col} is not null\n                group by value order by count desc, value limit {limit}\n            \"\"\".format(\n                col=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            try:\n                facet_rows_results = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n                facet_results_values = []\n                facet_results.append(\n                    {\n                        \"name\": column,\n                        \"type\": self.type,\n                        \"hideable\": source != \"metadata\",\n                        \"toggle_url\": self.ds.urls.path(\n                            path_with_removed_args(self.request, {\"_facet_array\": column})\n                        ),\n                        \"results\": facet_results_values,\n                        \"truncated\": len(facet_rows_results) > facet_size,\n                    }\n                )\n                facet_rows = facet_rows_results.rows[:facet_size]\n                if self.table:\n                    # Attempt to expand foreign keys into labels\n                    values = [row[\"value\"] for row in facet_rows]\n                    expanded = await self.ds.expand_foreign_keys(\n                        self.database, self.table, column, values\n                    )\n                else:\n                    expanded = {}\n                for row in facet_rows:\n                    column_qs = column\n                    if column.startswith(\"_\"):\n                        column_qs = \"{}__exact\".format(column)\n                    selected = (column_qs, str(row[\"value\"])) in qs_pairs\n                    if selected:\n                        toggle_path = path_with_removed_args(\n                            self.request, {", "idx": 780}
{"namespace": "datasette.facets.DateFacet.facet_results", "completion": "        facet_results = []\n        facets_timed_out = []\n\n        facet_size = self.get_facet_size()\n        for source_and_config in self.get_configs():\n            config = source_and_config[\"config\"]\n            source = source_and_config[\"source\"]\n            column = config.get(\"column\") or config[\"simple\"]\n            facet_sql = \"\"\"\n                select\n                    strftime('%Y-%m', {column}) as year_month,\n                    count(*) as count\n                from (\n                    {sql}\n                )\n                where {column} is not null\n                group by year_month\n                order by year_month\n            \"\"\".format(\n                column=escape_sqlite(column), sql=self.sql\n            )\n            try:\n                facet_rows_results = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n                facet_results_values = []\n                facet_results.append(\n                    {\n                        \"name\": column,\n                        \"type\": self.type,\n                        \"results\": facet_results_values,\n                        \"hideable\": source != \"metadata\",\n                        \"toggle_url\": self.ds.urls.path(\n                            path_with_removed_args(self.request, {\"_facet_date\": column})\n                        ),\n                        \"truncated\": len(facet_rows_results) > facet_size,\n                    }\n                )\n                facet_rows = facet_rows_results.rows[:facet_size]\n                pairs = self.get_querystring_pairs()\n                for row in facet_rows:\n                    value = row[\"year_month\"]\n                    selected = (f\"{column}__date\", value) in pairs\n                    if selected:\n                        toggle_path = path_with_removed_args(\n                            self.request, {f\"{column}__date\": value}\n                        )\n                    else:\n                        toggle_path = path_with_added_args(\n                            self.request, {f\"{column}__date\": value}\n                        )\n                    facet_results_values.append(\n                        {\n                            \"value\": value,\n                            \"label\": value,\n                            \"count\": row[\"count\"],\n                            \"toggle_url\": self.ds.absolute_url(", "idx": 781}
{"namespace": "datasette.app.Datasette.invoke_startup", "completion": "        if self._startup_invoked:\n            return\n        self._startup_invoked = True\n        await self.refresh_schemas()\n        await self.client.refresh_metadata()\n        await self.client.refresh_permissions()\n        await self.client.refresh_facets()\n        await self.client.refresh_plugins()\n        await self.client.refresh_templates()\n        await self.client.refresh_static_files()\n        await self.client.refresh_routes()\n        await self.client.refresh_views()\n        await self.client.refresh_plugins_metadata()\n        await self.client.refresh_plugins_templates()\n        await self.client.refresh_plugins_static_files()\n        await self.client.refresh_plugins_routes()\n        await self.client.refresh_plugins_views()\n        await self.client.refresh_plugins_permissions()\n        await self.client.refresh_plugins_facets()\n        await self.client.refresh_plugins_config()\n        await self.client.refresh_plugins_settings()\n        await self.client.refresh_plugins_databases()\n        await self.client.refresh_plugins_inspect_data()\n        await self.client.refresh_plugins_metadata()\n        await self.client.refresh_plugins_metadata()\n        await self.client.refresh_plugins_metadata()\n        await self.client.refresh_plugins_metadata()\n        await self.client.refresh_plugins_metadata()\n        await self.client.refresh_plugins_metadata()\n        await self.client.refresh_plugins_metadata()\n        await self.client.refresh_plugins_metadata()\n        await self.client.refresh_plugins_metadata()\n        await self.client.refresh_plugins_metadata()\n        await self.client.refresh_plugins_metadata()\n        await self.client.refresh_plugins_metadata()\n        await self.client.refresh_plugins_metadata()\n        await self.client.refresh_plugins_metadata()\n        await self.client.refresh_plugins_metadata()\n        await self.client.refresh_plugins_metadata()\n        await self.client.refresh_plugins_metadata()\n        await self.client.refresh_plugins_metadata()\n        await self.client.refresh_plugins_metadata()\n        await self.client.refresh_plugins_metadata()\n        await self.client.refresh_plugins_metadata()", "idx": 782}
{"namespace": "datasette.app.Datasette.get_database", "completion": "        if route:\n            for db in self.databases.values():\n                if db.route == route:\n                    return db\n        if name:\n            return self.databases[name]\n        for db in self.databases.values():\n            if db.name != \"_internal\":\n                return db\n        return None\n", "idx": 783}
{"namespace": "datasette.app.Datasette.add_database", "completion": "        if name is None:\n            name = \"db{}\".format(len(self.databases))\n        if name in self.databases:\n            i = 1\n            while \"{}{}\".format(name, i) in self.databases:\n                i += 1\n            name = \"{}{}\".format(name, i)\n        if route is None:\n            route = name\n        db.name = name\n        db.route = route\n        self.databases = dict(self.databases, **{name: db})\n        return db\n", "idx": 784}
{"namespace": "datasette.app.Datasette.ensure_permissions", "completion": "        for permission in permissions:\n            if isinstance(permission, tuple):\n                action, resource = permission\n            else:\n                action = permission\n                resource = None\n            if not await self.permission_allowed(actor=actor, action=action, resource=resource):\n                raise Forbidden(\n                    \"Permission denied: {}{}\".format(\n                        action, \" on {}\".format(resource) if resource else \"\"\n                    )\n                )\n", "idx": 785}
{"namespace": "datasette.app.Datasette.check_visibility", "completion": "        if permissions is None:\n            permissions = []\n        if resource is not None:\n            permissions.append((action, resource))\n        if not await self.permission_allowed(actor, action, resource, default=True):\n            return False, False\n        for permission in permissions:\n            if isinstance(permission, str):\n                action = permission\n                resource = None\n            elif isinstance(permission, (tuple, list)) and len(permission) == 2:\n                action, resource = permission\n            else:\n                assert (\n                    False\n                ), \"permission should be string or tuple of two items: {}\".format(\n                    repr(permission)\n                )\n            if not await self.permission_allowed(actor, action, resource, default=True):\n                return False, False\n        return True, False\n", "idx": 786}
{"namespace": "datasette.app.Datasette.render_template", "completion": "        if not hasattr(self, \"_startup_invoked\"):\n            raise Exception(\"Datasette startup has not been invoked\")\n        if context is None:\n            context = {}\n        if request is None:\n            request = {}\n        if view_name is None:\n            view_name = \"\"\n        # Prepare the context\n        context.update(\n            {\n                \"instance\": self,\n                \"request\": request,\n                \"actor\": request.actor,\n                \"databases\": self._connected_databases(),\n                \"versions\": self._versions(),\n                \"plugins\": self._plugins(request),\n                \"threads\": self._threads(),\n                \"actor\": self._actor(request),\n                \"view_name\": view_name,\n                \"view_url\": self.absolute_url(request, view_name),\n                \"view_path\": view_name,\n                \"view_hash\": hashlib.sha1(view_name.encode(\"utf-8\")).hexdigest(),\n                \"view_id\": hashlib.sha1(view_name.encode(\"utf-8\")).hexdigest(),\n                \"view_crumbs\": self._crumb_items(request, view_name),\n                \"view_messages\": self._show_messages(request),\n                \"view_message\": lambda message, type=INFO: self.add_message(\n                    request, message, type\n                ),\n                \"view_write_messages_to_response\": lambda response: self._write_messages_to_response(\n                    request, response\n                ),\n                \"view_show_messages\": lambda: self._show_messages(request),\n                \"view_ensure_permissions\": lambda permissions: self.ensure_permissions(\n                    request.actor, permissions\n                ),\n                \"view_permission_allowed\": lambda action, resource=None: self.permission_allowed(\n                    request.actor, action, resource\n                ),\n                \"view_check_visibility\": lambda action, resource=None: self.check_visibility(\n                    request.actor, action, resource\n                ),\n                \"view_expand_foreign_keys\": lambda database, table, column, values: self.expand_foreign_keys(\n                    database, table, column, values\n                ),\n                \"view_absolute_url\": lambda path: self.absolute_url(request, path),\n                \"view_absolute_path\": lambda path: self.absolute_url(request, path),\n                \"", "idx": 787}
{"namespace": "datasette.app.DatasetteClient.get", "completion": "        path = self._fix(path)\n        return await httpx.get(path, **kwargs)\n", "idx": 788}
{"namespace": "datasette.utils.asgi.Request.full_path", "completion": "        if self.query_string:\n            return self.path + \"?\" + self.query_string\n        else:\n            return self.path\n", "idx": 789}
{"namespace": "datasette.utils.asgi.Request.post_body", "completion": "        body = b\"\"\n        more_body = True\n        while more_body:\n            message = await self.receive()\n            if message[\"type\"] == \"http.request\":\n                body += message.get(\"body\", b\"\")\n                more_body = message.get(\"more_body\", False)\n        return body\n", "idx": 790}
{"namespace": "datasette.utils.asgi.Request.fake", "completion": "        path, query_string = path_with_query_string.split(\"?\")\n        scope = {\n            \"type\": \"http\",\n            \"method\": method,\n            \"scheme\": scheme,\n            \"path\": path,\n            \"query_string\": query_string.encode(\"latin-1\"),\n            \"headers\": [],\n        }\n        if url_vars:\n            scope[\"url_route\"] = {\"kwargs\": url_vars}\n        return cls(scope, None)\n\n", "idx": 791}
{"namespace": "datasette.utils.asgi.Response.asgi_send", "completion": "        # Remove any existing content-type header\n        headers = {k: v for k, v in self.headers.items() if k.lower() != \"content-type\"}\n        headers[\"content-type\"] = self.content_type\n        await send(\n            {\n                \"type\": \"http.response.start\",\n                \"status\": self.status,\n                \"headers\": [\n                    [key.encode(\"latin1\"), value.encode(\"latin1\")]\n                    for key, value in headers.items()\n                ],\n            }\n        )\n        if self.body:\n            await send({\"type\": \"http.response.body\", \"body\": self.body})\n\n", "idx": 792}
{"namespace": "datasette.utils.asgi.Response.set_cookie", "completion": "        if samesite not in SAMESITE_VALUES:\n            raise ValueError(\n                \"Invalid samesite value. Must be one of {}\".format(SAMESITE_VALUES)\n            )\n\n        cookie = Morsel()\n        cookie.set(key, value, \"utf-8\")\n        if max_age is not None:\n            cookie[\"max-age\"] = str(max_age)\n        if expires is not None:\n            cookie[\"expires\"] = expires\n        cookie[\"path\"] = path\n        if domain is not None:\n            cookie[\"domain\"] = domain\n        if secure:\n            cookie[\"secure\"] = \"secure\"\n        if httponly:\n            cookie[\"httponly\"] = \"httponly\"\n        if samesite is not None:\n            cookie[\"samesite\"] = samesite\n        self._set_cookie_headers.append(cookie.OutputString())\n\n", "idx": 793}
{"namespace": "datasette.utils.asgi.Response.html", "completion": "        return cls(body, status, headers, content_type=\"text/html; charset=utf-8\")\n", "idx": 794}
{"namespace": "datasette.utils.asgi.Response.text", "completion": "        return cls(\n            body,\n            status=status,\n            headers=headers,\n            content_type=\"text/plain; charset=utf-8\",\n        )\n", "idx": 795}
{"namespace": "datasette.utils.asgi.Response.json", "completion": "        return cls(\n            json.dumps(body, default=default),\n            status=status,\n            headers=headers,\n            content_type=\"application/json; charset=utf-8\",\n        )\n", "idx": 796}
{"namespace": "datasette.utils.asgi.Response.redirect", "completion": "        return cls(\n            \"\",\n            status=status,\n            headers=headers or {},\n            content_type=\"text/html; charset=utf-8\",\n        )\n\n", "idx": 797}
{"namespace": "datasette.utils.validate_sql_select", "completion": "    sql = re.sub(comment_re, \"\", sql, flags=re.MULTILINE)\n    sql = sql.lower()\n    if not any(res.match(sql) for res in allowed_sql_res):\n        raise InvalidSql(\"SQL statement must start with SELECT, WITH, or EXPLAIN\")\n    for res, error in disallawed_sql_res:\n        if res.match(sql):\n            raise InvalidSql(error)\n\n", "idx": 798}
{"namespace": "datasette.utils.escape_css_string", "completion": "    def replace(match):\n        return \"\\\\%04x\" % ord(match.group(0))\n\n    return _css_re.sub(replace, s).replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n\n", "idx": 799}
{"namespace": "datasette.utils.detect_fts", "completion": "    rows = conn.execute(\n        f\"PRAGMA table_info([{table}])\"\n    ).fetchall()\n    for row in rows:\n        if row is not None:\n            id, name, type, notnull, dflt_value, pk = row\n            if name == \"fts\":\n                return f\"[{table}]_fts\"\n    return None\n\n", "idx": 800}
{"namespace": "datasette.utils.is_url", "completion": "    return (\n        isinstance(value, str)\n        and value.startswith(\"http://\")\n        and not whitespace_re.search(value)\n    )\n\n", "idx": 801}
{"namespace": "datasette.utils.to_css_class", "completion": "    if css_class_re.match(s):\n        return s\n    else:\n        return f\"{s}-{hashlib.md5(s.encode()).hexdigest()[:6]}\"\n\n", "idx": 802}
{"namespace": "datasette.utils.escape_fts", "completion": "    if not query:\n        return \"\"\n    terms = _escape_fts_re.split(query)\n    terms = [term for term in terms if term]\n    terms = [term for term in terms if term not in terms]\n    return '\"{}\"'.format('\", \"'.join(terms))\n\n", "idx": 803}
{"namespace": "datasette.utils.check_connection", "completion": "    try:\n        conn.execute(\"SELECT name FROM sqlite_master WHERE type='table'\").fetchall()\n    except Exception as e:\n        raise ConnectionProblem(str(e))\n    for table in conn.execute(\"SELECT name FROM sqlite_master WHERE type='table'\").fetchall():\n        try:\n            conn.execute(f\"PRAGMA table_info({escape_sqlite(table[0])})\").fetchall()\n        except Exception as e:\n            raise ConnectionProblem(str(e))\n\n", "idx": 804}
{"namespace": "datasette.utils.parse_metadata", "completion": "    try:\n        return json.loads(content)\n    except json.JSONDecodeError:\n        try:\n            return yaml.safe_load(content)\n        except yaml.YAMLError as e:\n            raise BadMetadataError(e)\n\n", "idx": 805}
{"namespace": "datasette.utils.call_with_supported_arguments", "completion": "    return fn(*_gather_arguments(fn, kwargs))\n\n", "idx": 806}
{"namespace": "datasette.utils.derive_named_parameters", "completion": "    # Strip any trailing semicolon from the input SQL query\n    sql = sql.rstrip(\";\")\n    # Construct the \"explain\" statement by prepending \"explain\" to the input SQL query\n    explain = f\"explain {sql}\"\n    # Find all possible named parameters in the input SQL query using a regular expression\n    named_parameters = _re_named_parameter.findall(sql)\n    # Execute the \"explain\" statement on the database with a dictionary of named parameters, where the values are set to None\n    try:\n        results = await db.execute(explain, {key: None for key in named_parameters})\n    except sqlite3.OperationalError:\n        return [key[1:] for key in named_parameters]\n    # Return a list of named parameters identified as variables in the \"explain\" results, after removing the leading \":\" character\n    return [key[1:] for key in results if key[0] == \":\"]\n\n", "idx": 807}
{"namespace": "pyramid.path.Resolver.get_package_name", "completion": "        if self.package == CALLER_PACKAGE:\n            return package_name(caller_package())\n        return self.package.__name__\n", "idx": 808}
{"namespace": "pyramid.path.Resolver.get_package", "completion": "        if self.package is CALLER_PACKAGE:\n            package = caller_package()\n        else:\n            package = self.package\n        return package\n", "idx": 809}
{"namespace": "pyramid.path.DottedNameResolver.resolve", "completion": "        if ':' in dotted:\n            package_name, dotted = dotted.split(':', 1)\n        else:\n            if self.package is CALLER_PACKAGE:\n                package_name = caller_package().__name__\n            else:\n                package_name = getattr(self.package, '__name__', None)\n            if package_name is None:\n                raise ValueError(\n                    'relative dotted name %r irresolveable without package'\n                    % (dotted,)\n                )\n        return pkg_resources.load_entry_point(package_name, 'pyramid.dotted', dotted)\n\n", "idx": 810}
{"namespace": "pyramid.path.DottedNameResolver.maybe_resolve", "completion": "        if not isinstance(dotted, str):\n            return dotted\n        package = self.get_package()\n        if dotted.startswith('.'):\n            dotted = package.__name__ + dotted\n        try:\n            return __import__(dotted, fromlist=[''])\n        except ImportError:\n            raise ValueError(\n                'The dotted name %r cannot be imported' % (dotted,)\n            )\n\n", "idx": 811}
{"namespace": "pyramid.path.PkgResourcesAssetDescriptor.abspath", "completion": "        return self.pkg_resources.resource_filename(self.pkg_name, self.path)\n", "idx": 812}
{"namespace": "pyramid.renderers.render_to_response", "completion": "    try:\n        registry = request.registry\n    except AttributeError:\n        registry = None\n    if package is None:\n        package = caller_package()\n    helper = RendererHelper(\n        name=renderer_name, package=package, registry=registry\n    )\n\n    with hide_attrs(request, 'response'):\n        result = helper.render(value, response, request=request)\n\n    return result\n\n", "idx": 813}
{"namespace": "pyramid.renderers.JSON.add_adapter", "completion": "        self.components.registerAdapter(adapter, (type_or_iface,), IJSONAdapter)\n", "idx": 814}
{"namespace": "pyramid.renderers.RendererHelper.settings", "completion": "        return self.registry.settings.get('pyramid.renderers', {})\n", "idx": 815}
{"namespace": "pyramid.renderers.RendererHelper.render_view", "completion": "        system = {\n            'view': view,\n            'renderer_name': self.name,\n            'renderer_info': self,\n            'context': context,\n            'request': request,\n            'csrf_token': get_csrf_token(request),\n        }\n        return self.renderer(system)(response, system)\n", "idx": 816}
{"namespace": "pyramid.renderers.RendererHelper.render", "completion": "        if system_values is None:\n            system_values = {}\n        system_values['view'] = system_values.get('view', None)\n        system_values['renderer_name'] = self.name\n        system_values['renderer_info'] = self\n        system_values['context'] = system_values.get('context', None)\n        system_values['request'] = request\n        system_values['req'] = request\n        system_values['get_csrf_token'] = partial(get_csrf_token, request)\n\n        self.registry.notify(system_values)\n        return self.renderer(system_values)(value, system_values)\n", "idx": 817}
{"namespace": "pyramid.renderers.RendererHelper.render_to_response", "completion": "        from pyramid.events import BeforeRender\n        renderer = self.renderer\n        if system_values is None:\n            system_values = {\n                'view': None,\n                'renderer_name': self.name,  # b/c\n                'renderer_info': self,\n                'context': getattr(request, 'context', None),\n                'request': request,\n                'req': request,\n                'get_csrf_token': partial(get_csrf_token, request),\n            }\n\n        system_values = BeforeRender(system_values, value)\n\n        registry = self.registry\n        registry.notify(system_values)\n        result = renderer(value, system_values)\n        response = request.response\n        response.text = result\n        return response", "idx": 818}
{"namespace": "pyramid.renderers.RendererHelper.clone", "completion": "        if name is None:\n            name = self.name\n        if package is None:\n            package = self.package\n        if registry is None:\n            registry = self.registry\n        return RendererHelper(name, package, registry)", "idx": 819}
{"namespace": "pyramid.urldispatch.RoutesMapper.get_routes", "completion": "        if include_static:\n            return self.routelist + self.static_routes\n        else:\n            return self.routelist\n", "idx": 820}
{"namespace": "pyramid.urldispatch.RoutesMapper.connect", "completion": "        route = Route(name, pattern, factory, predicates, pregenerator)\n        self.routes[name] = route\n        if static is True:\n            self.static_routes.append(route)\n        else:\n            self.routelist.append(route)\n        return route\n\n", "idx": 821}
{"namespace": "pyramid.testing.DummyTemplateRenderer.assert_", "completion": "        for k, v in kw.items():\n            if k not in self._received:\n                raise AssertionError(\n                    \"Key %s not found in renderer's received data\" % k\n                )\n            if self._received[k] != v:\n                raise AssertionError(\n                    \"Key %s has value %s, not %s\" % (k, self._received[k], v)\n                )\n        return True\n\n", "idx": 822}
{"namespace": "pyramid.testing.DummyResource.__getitem__", "completion": "        return self.subs[name]\n", "idx": 823}
{"namespace": "pyramid.testing.DummyResource.clone", "completion": "        if __name__ is _marker:\n            __name__ = self.__name__\n        if __parent__ is _marker:\n            __parent__ = self.__parent__\n        return DummyResource(__name__, __parent__, **kw)\n\n", "idx": 824}
{"namespace": "pyramid.testing.DummySession.get_csrf_token", "completion": "        return self.get('_csrft_', self.new_csrf_token())\n\n", "idx": 825}
{"namespace": "pyramid.testing.DummyRequest.response", "completion": "        from pyramid.response import Response\n\n        return Response(request=self)\n", "idx": 826}
{"namespace": "pyramid.testing.DummyRendererFactory.add", "completion": "        self.renderers[spec] = renderer\n", "idx": 827}
{"namespace": "pyramid.authorization.ACLAuthorizationPolicy.principals_allowed_by_permission", "completion": "        return self.helper.principals_allowed_by_permission(context, permission)\n\n", "idx": 828}
{"namespace": "pyramid.url.URLMethodsMixin.route_url", "completion": "        return self.route_path(route_name, *elements, **kw)\n", "idx": 829}
{"namespace": "pyramid.predicates.CustomPredicate.text", "completion": "        if hasattr(self.func, '__text__'):\n            return self.func.__text__\n        return 'custom predicate %s' % (self.func,)\n", "idx": 830}
{"namespace": "pyramid.threadlocal.ThreadLocalManager.pop", "completion": "        return self.stack.pop()\n", "idx": 831}
{"namespace": "pyramid.threadlocal.ThreadLocalManager.get", "completion": "        try:\n            return self.stack[-1]\n        except IndexError:\n            return self.default()\n", "idx": 832}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.authenticated_userid", "completion": "        identity = self._get_identity(request)\n        if identity is None:\n            return None\n        userid = identity.get('repoze.who.userid')\n        if userid is None:\n            return None\n        if self._clean_principal(userid) is None:\n            return None\n        if self.callback is None:\n            return userid\n        callback_ok = self.callback(userid, request)\n        if callback_ok is not None:  # is not None!\n            return userid\n        return None\n", "idx": 833}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.unauthenticated_userid", "completion": "        identity = self._get_identity(request)\n        if identity is None:\n            self.debug and self._log(\n                'repoze.who identity is None, returning None',\n                'unauthenticated_userid',\n                request,\n            )\n            return None\n        userid = identity['repoze.who.userid']\n        if userid is None:\n            self.debug and self._log(\n                'repoze.who.userid is None, returning None' % userid,\n                'unauthenticated_userid',\n                request,\n            )\n            return None\n        if self._clean_principal(userid) is None:\n            self.debug and self._log(\n                (\n                    'use of userid %r is disallowed by any built-in Pyramid '\n                    'security policy, returning None' % userid\n                ),\n                'unauthenticated_userid',\n                request,\n            )\n            return None\n        return userid\n", "idx": 834}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.forget", "completion": "        identifier = self._get_identifier(request)\n        if identifier is None:\n            return []\n        environ = request.environ\n        return identifier.forget(environ)\n\n", "idx": 835}
{"namespace": "pyramid.authentication.AuthTktAuthenticationPolicy.unauthenticated_userid", "completion": "        return self.cookie.get_user(request)\n", "idx": 836}
{"namespace": "pyramid.authentication.SessionAuthenticationHelper.remember", "completion": "        environ = request.environ\n        environ[self.userid_key] = userid\n        return []\n", "idx": 837}
{"namespace": "pyramid.authentication.SessionAuthenticationHelper.forget", "completion": "        request.session.pop(self.userid_key, None)\n        return []\n", "idx": 838}
{"namespace": "pyramid.authentication.BasicAuthAuthenticationPolicy.unauthenticated_userid", "completion": "        authorization = request.headers.get('Authorization')\n        if authorization is None:\n            return None\n\n        try:\n            authorization = authorization.split(' ')\n        except ValueError:\n            return None\n\n        if len(authorization) != 2:\n            return None\n\n        if authorization[0] != 'Basic':\n            return None\n\n        try:\n            authorization = b64decode(authorization[1])\n        except UnicodeEncodeError:\n            return None\n\n        try:\n            authorization = authorization.split(':')\n        except ValueError:\n            return None\n\n        if len(authorization) != 2:\n            return None\n\n        username = authorization[0]\n        password = authorization[1]\n\n        principals = self.check(username, password, request)\n\n        if principals is None:\n            return None\n\n        environ = request.environ\n        environ['REMOTE_USER_TOKENS'] = principals\n        environ['REMOTE_USER_DATA'] = ''\n        environ['AUTH_TYPE'] = 'basic'\n\n        return username\n\n", "idx": 839}
{"namespace": "pyramid.request.CallbackMethodsMixin._process_response_callbacks", "completion": "        for callback in self.response_callbacks:\n            callback(self, response)\n\n", "idx": 840}
{"namespace": "pyramid.request.CallbackMethodsMixin._process_finished_callbacks", "completion": "        callbacks = self.finished_callbacks\n        while callbacks:\n            callback = callbacks.popleft()\n            callback(self)\n\n", "idx": 841}
{"namespace": "pyramid.request.Request.session", "completion": "        from pyramid.session import get_session\n\n        return get_session(self)\n", "idx": 842}
{"namespace": "pyramid.request.RequestLocalCache.get_or_create", "completion": "        if creator is None:\n            creator = self._creator\n\n        if creator is None:\n            raise ValueError(\n                'No creator function provided and no creator function bound to the cache'\n            )\n\n        store = self._store\n        cache = store.get(request)\n        if cache is None:\n            cache = {}\n            store[request] = cache\n\n        value = cache.get(creator, self.NO_VALUE)\n        if value is self.NO_VALUE:\n            value = creator(request)\n            cache[creator] = value\n\n        return value\n", "idx": 843}
{"namespace": "pyramid.request.RequestLocalCache.set", "completion": "        if request not in self._store:\n            self._store[request] = value\n            request.add_finished_callback(self._remove_request)\n", "idx": 844}
{"namespace": "pyramid.i18n.Localizer.pluralize", "completion": "        if self.pluralizer is None:\n            self.pluralizer = Pluralizer(self.translations)\n        return self.pluralizer(singular, plural, n, domain=domain, mapping=mapping)\n", "idx": 845}
{"namespace": "pyramid.i18n.Translations.load", "completion": "        if not locales:\n            locales = []\n        if not isinstance(locales, list):\n            locales = [locales]\n        if not dirname:\n            dirname = os.path.join(os.path.dirname(__file__), 'locale')\n        if not os.path.isdir(dirname):\n            return gettext.NullTranslations()\n        translations = cls()\n        for locale in locales:\n            if isinstance(locale, str):\n                locale = Locale.parse(locale)\n            if not locale:\n                continue\n            filename = os.path.join(dirname, locale.language, 'LC_MESSAGES',\n                                    '%s.mo' % domain)\n            if os.path.isfile(filename):\n                with open(filename, 'rb') as fp:\n                    translations.add_fallback(cls(fp, domain))\n        return translations\n", "idx": 846}
{"namespace": "pyramid.i18n.Translations.add", "completion": "        if translations.domain != self.domain:\n            if translations.domain not in self._domains:\n                self._domains[translations.domain] = Translations()\n            self._domains[translations.domain].add(translations, merge)\n            return self\n\n        if merge:\n            self._catalog.update(translations._catalog)\n            self._plural_forms.update(translations._plural_forms)\n            self._plural_func = translations._plural_func\n            self._charset = translations._charset\n            self._info.update(translations._info)\n            self.plural = translations.plural\n            self.files.extend(translations.files)\n        else:\n            self._catalog = translations._catalog\n            self._plural_forms = translations._plural_forms\n            self._plural_func = translations._plural_func\n            self._charset = translations._charset\n            self._info = translations._info\n            self.plural = translations.plural\n            self.files = translations.files\n\n        return self\n", "idx": 847}
{"namespace": "pyramid.i18n.Translations.merge", "completion": "        # Merge the catalogs\n        for key, value in translations._catalog.items():\n            if key in self._catalog:\n                self._catalog[key].update(value)\n            else:\n                self._catalog[key] = value\n\n        # Merge the files\n        self.files.extend(translations.files)\n\n        return self\n", "idx": 848}
{"namespace": "pyramid.i18n.LocalizerRequestMixin.locale_name", "completion": "        return negotiate_locale_name(self)", "idx": 849}
{"namespace": "pyramid.csrf.LegacySessionCSRFStoragePolicy.check_csrf_token", "completion": "        expected_token = self.get_csrf_token(request)\n        if expected_token is None:\n            return False\n        return bytes_(expected_token) == bytes_(supplied_token)\n\n", "idx": 850}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.new_csrf_token", "completion": "        token = self._token_factory()\n        request.session[self.key] = token\n        return token\n", "idx": 851}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.get_csrf_token", "completion": "        token = request.session.get(self.key)\n        if token is None:\n            token = self.new_csrf_token(request)\n        return token\n", "idx": 852}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.check_csrf_token", "completion": "        expected_token = self.get_csrf_token(request)\n        return not strings_differ(\n            bytes_(expected_token), bytes_(supplied_token)\n        )\n\n", "idx": 853}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.new_csrf_token", "completion": "        token = self._token_factory()\n        self.cookie_profile.set_cookie(request, token)\n        request.add_response_callback(self._set_cookie_callback)\n        return token\n", "idx": 854}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.get_csrf_token", "completion": "        token = request.cookies.get(self.cookie_name, None)\n        if not token:\n            token = self.new_csrf_token(request)\n        return token\n", "idx": 855}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.check_csrf_token", "completion": "        expected_token = self.get_csrf_token(request)\n        return not strings_differ(\n            bytes_(expected_token), bytes_(supplied_token)\n        )\n\n", "idx": 856}
{"namespace": "pyramid.security.PermitsResult.__repr__", "completion": "        return '<%s instance at %s with msg %s>' % (\n            self.__class__.__name__,\n            hex(id(self)),\n            self.msg,\n        )\n\n", "idx": 857}
{"namespace": "pyramid.util.InstancePropertyHelper.make_property", "completion": "        if name is None:\n            name = callable.__name__\n        if reify:\n            callable = reify(callable)\n        return name, property(callable)\n", "idx": 858}
{"namespace": "pyramid.util.InstancePropertyHelper.set_property", "completion": "        name, fn = cls.make_property(callable, name, reify)\n        cls.apply_properties(target, {name: fn})\n", "idx": 859}
{"namespace": "pyramid.util.InstancePropertyHelper.add_property", "completion": "        name, fn = self.make_property(callable, name=name, reify=reify)\n        self.properties[name] = fn\n", "idx": 860}
{"namespace": "pyramid.util.InstancePropertyHelper.apply", "completion": "        self.apply_properties(target, self.properties)\n\n", "idx": 861}
{"namespace": "pyramid.util.InstancePropertyMixin.set_property", "completion": "        InstancePropertyHelper.set_property(\n            self, callable, name=name, reify=reify\n        )\n", "idx": 862}
{"namespace": "pyramid.util.TopologicalSorter.remove", "completion": "        if name in self.names:\n            self.names.remove(name)\n            self.req_before.remove(name)\n            self.req_after.remove(name)\n            self.name2before.pop(name, None)\n            self.name2after.pop(name, None)\n            self.name2val.pop(name, None)\n            self.order.remove(name)\n", "idx": 863}
{"namespace": "pyramid.util.TopologicalSorter.add", "completion": "        if name in self.names:\n            raise ValueError('name %r already exists' % name)\n        self.names.append(name)\n        self.name2val[name] = val\n        if after is None:\n            after = self.default_after\n        if before is None:\n            before = self.default_before\n        if after is not None:\n            if after is FIRST:\n                after = self.first\n            if not is_nonstr_iter(after):\n                after = (after,)\n            for u in after:\n                if u not in self.names:\n                    raise ValueError('unknown name %r' % u)\n                self.req_after.add(u)\n                self.order.append((u, name))\n        if before is not None:\n            if before is LAST:\n                before = self.last\n            if not is_nonstr_iter(before):\n                before = (before,)\n            for u in before:\n                if u not in self.names:\n                    raise ValueError('unknown name %r' % u)\n                self.req_before.add(u)\n                self.order.append((name, u))\n        self.name2before[name] = after\n        self.name2after[name] = before\n", "idx": 864}
{"namespace": "pyramid.traversal.find_resource", "completion": "    if isinstance(path, str):\n        path = path.split(\"/\")\n    if isinstance(path, tuple):\n        path = list(path)\n    if path[0] == \"\":\n        path = path[1:]\n    if path[0] == VH_ROOT_KEY:\n        path = path[1:]\n    if path[0] == \"\":\n        path = path[1:]\n    if path[0] == \"\":\n        path = path[1:]\n    if path[0] == \"\":\n        path = path[1:]\n    if path[0] == \"\":\n        path = path[1:]\n    if path[0] == \"\":\n        path = path[1:]\n    if path[0] == \"\":\n        path = path[1:]\n    if path[0] == \"\":\n        path = path[1:]\n    if path[0] == \"\":\n        path = path[1:]\n    if path[0] == \"\":\n        path = path[1:]\n    if path[0] == \"\":\n        path = path[1:]\n    if path[0] == \"\":\n        path = path[1:]\n    if path[0] == \"\":\n        path = path[1:]\n    if path[0] == \"\":\n        path = path[1:]\n    if path[0] == \"\":\n        path = path[1:]\n    if path[0] == \"\":\n        path = path[1:]\n    if path[0] == \"\":\n        path = path[1:]\n    if path[0] == \"\":\n        path = path[1:]\n    if path[0] == \"\":\n        path = path[1:]\n    if path[0] == \"\":\n        path = path[1:]\n    if path[0] == \"\":\n        path = path[1:]\n    if path[0] == \"\":\n        path = path[1:]\n    if path[0] == \"\":\n        path = path[1:]\n    if path[0] == \"\":\n        path = path[1:]\n    if path[0] == \"\":\n        path = path[1:]\n    if path[0] == \"\":\n        path = path[1:]\n    if path[0] == \"\":\n        path = path[1:]\n    if path[0] == \"\":\n        path = path[1:]\n    if path[0] == \"\":\n        path = path[1:]\n    if path[0] == \"\":\n        path = path[1", "idx": 865}
{"namespace": "pyramid.static.ManifestCacheBuster.manifest", "completion": "        if self.reload:\n            if self.exists(self.manifest_path):\n                mtime = self.getmtime(self.manifest_path)\n                if mtime != self._mtime:\n                    self._mtime = mtime\n                    self._manifest = self.get_manifest()\n            else:\n                self._manifest = {}\n        return self._manifest\n", "idx": 866}
{"namespace": "pyramid.registry.Registry.registerSubscriptionAdapter", "completion": "        self.has_listeners = True\n        return Components.registerSubscriptionAdapter(self, *arg, **kw)\n", "idx": 867}
{"namespace": "pyramid.registry.Registry.registerHandler", "completion": "        result = Components.registerHandler(self, *arg, **kw)\n        self.has_listeners = True\n        return result\n", "idx": 868}
{"namespace": "pyramid.registry.Registry.notify", "completion": "        if not self.has_listeners:\n            return\n\n        # notify the subscribers of the Registry instance about the occurrence of events\n        for event in events:\n            self.subscribers(event, None)\n", "idx": 869}
{"namespace": "pyramid.registry.Introspector.add", "completion": "        category_name = intr.category_name\n        discriminator = intr.discriminator\n        self._counter += 1\n        intr.order = self._counter\n        if category_name not in self._categories:\n            self._categories[category_name] = {}\n        if discriminator not in self._categories[category_name]:\n            self._categories[category_name][discriminator] = []\n        self._categories[category_name][discriminator].append(intr)\n        self._refs[intr.id] = intr\n", "idx": 870}
{"namespace": "pyramid.registry.Introspector.get", "completion": "        category = self._categories.get(category_name)\n        if category is None:\n            return default\n        return category.get(discriminator, default)\n", "idx": 871}
{"namespace": "pyramid.registry.Introspector.get_category", "completion": "        category = self._categories.get(category_name, default)\n        if category is None:\n            return []\n        if sort_key is None:\n            sort_key = operator.attrgetter('order')\n        return sorted(category.values(), key=sort_key)\n", "idx": 872}
{"namespace": "pyramid.registry.Introspector.categorized", "completion": "        if sort_key is None:\n            sort_key = operator.attrgetter('order')\n        categories = self._categories.items()\n        categories = sorted(categories, key=lambda x: x[0])\n        return [\n            (category_name, self.get_category(category_name, sort_key=sort_key))\n            for category_name, _ in categories\n        ]\n", "idx": 873}
{"namespace": "pyramid.registry.Introspector.remove", "completion": "        category = self._categories.get(category_name)\n        if category is None:\n            return\n        intr = category.get(discriminator)\n        if intr is None:\n            return\n        for ref in intr.refs:\n            self._refs.pop(ref, None)\n        category.pop(discriminator, None)\n        category.pop(intr.discriminator_hash, None)\n", "idx": 874}
{"namespace": "pyramid.registry.Introspector.relate", "completion": "        introspectables = self._get_intrs_by_pairs(pairs)\n        for intr in introspectables:\n            for other in introspectables:\n                if intr is other:\n                    continue\n                L = self._refs.setdefault(intr, [])\n                if other not in L:\n                    L.append(other)\n", "idx": 875}
{"namespace": "pyramid.registry.Introspector.related", "completion": "        category_name = intr.category_name\n        discriminator = intr.discriminator\n        intr = self._categories.get(category_name, {}).get(discriminator)\n        if intr is None:\n            raise KeyError((category_name, discriminator))\n        return self._refs.get(intr, [])\n\n", "idx": 876}
{"namespace": "pyramid.registry.Introspectable.discriminator_hash", "completion": "        self._assert_resolved()\n        return hash(self.discriminator)\n", "idx": 877}
{"namespace": "pyramid.registry.Introspectable.__repr__", "completion": "        return '<%s category %r, discriminator %r>' % (\n            self.type_name,\n            self.category_name,\n            self.discriminator,\n        )\n\n", "idx": 878}
{"namespace": "pyramid.scripts.proutes.PRoutesCommand._get_mapper", "completion": "        return registry.queryUtility(IRouteRequest)\n", "idx": 879}
{"namespace": "pyramid.scripts.pshell.PShellCommand.make_shell", "completion": "        shells = self.find_all_shells()\n        if self.args.python_shell:\n            if self.args.python_shell not in shells:\n                raise ValueError(\n                    'could not find a shell named \"%s\"' % self.args.python_shell\n                )\n            return shells[self.args.python_shell]\n        if self.preferred_shells:\n            for shell in self.preferred_shells:\n                if shell in shells:\n                    return shells[shell]\n        for shell in shells:\n            if shell != 'python':\n                return shells[shell]\n        return self.default_runner", "idx": 880}
{"namespace": "pyramid.config.assets.PackageOverrides.insert", "completion": "        override = Override(path, source)\n        self.overrides.insert(0, override)\n        return override\n", "idx": 881}
{"namespace": "pyramid.config.assets.PackageOverrides.filtered_sources", "completion": "        for override in self.overrides:\n            if override.matches(resource_name):\n                yield override.source\n", "idx": 882}
{"namespace": "pyramid.config.assets.PackageOverrides.real_loader", "completion": "        if self._real_loader is None:\n            raise NotImplementedError(\n                \"The real loader of the PackageOverrides instance is not set.\"\n            )\n        return self._real_loader\n", "idx": 883}
{"namespace": "pyramid.config.views.MultiView.add", "completion": "        if phash is None:\n            phash = DEFAULT_PHASH\n        if accept is None:\n            accept = \"\"\n        if accept_order is None:\n            accept_order = 0\n\n        if phash in self.media_views:\n            existing_view = self.media_views[phash]\n            existing_view.order = order\n            existing_view.view = view\n            existing_view.accept = accept\n            existing_view.accept_order = accept_order\n            self.media_views[phash] = existing_view\n        else:\n            self.media_views[phash] = MultiView.View(view, order, phash, accept, accept_order)\n\n        if accept == \"\":\n            self.views.append(self.media_views[phash])\n            self.views.sort(key=lambda x: x.order)\n        else:\n            if accept not in self.accepts:\n                self.accepts.append(accept)\n                self.accepts.sort(key=lambda x: x.accept_order)\n            if accept not in self.media_views:\n                self.media_views[accept] = []\n            self.media_views[accept].append(self.media_views[phash])\n            self.media_views[accept].sort(key=lambda x: x.order)\n", "idx": 884}
{"namespace": "pyramid.config.views.MultiView.get_views", "completion": "        if request.accept is not None and self.accepts:\n            views = []\n            for accept in request.accept:\n                if accept.value in self.media_views:\n                    views.extend(self.media_views[accept.value])\n            views.extend(self.views)\n            return views\n        else:\n            return self.views\n", "idx": 885}
{"namespace": "pyramid.config.views.MultiView.match", "completion": "        views = self.get_views(request)\n        for order, view, phash in views:\n            if not hasattr(view, \"__predicated__\"):\n                return view\n            if view.__predicated__(context, request):\n                return view\n        raise PredicateMismatch(\n            \"No view matched the context and request for \"\n            \"name '%s' and request type '%s'\" % (self.name, request.content_type)\n        )\n", "idx": 886}
{"namespace": "pyramid.config.views.MultiView.__permitted__", "completion": "        view = self.match(context, request)\n        if not hasattr(view, '__permitted__'):\n            return True\n        return view.__permitted__(context, request)\n", "idx": 887}
{"namespace": "pyramid.config.views.MultiView.__call_permissive__", "completion": "        view = self.match(context, request)\n        if hasattr(view, '__call_permissive__'):\n            return view.__call_permissive__(context, request)\n        return view(context, request)\n", "idx": 888}
{"namespace": "pyramid.config.actions.ActionState.processSpec", "completion": "        if spec in self._seen_files:\n            return False\n        self._seen_files.add(spec)\n        return True\n", "idx": 889}
{"namespace": "pyramid.config.actions.ActionState.action", "completion": "        if kw is None:\n            kw = {}\n\n        action = extra\n        action.update(\n            dict(\n                discriminator=discriminator,\n                callable=callable,\n                args=args,\n                kw=kw,\n                order=order,\n                includepath=includepath,\n                info=info,\n                introspectables=introspectables,\n            )\n        )\n        self.actions.append(action)\n", "idx": 890}
{"namespace": "pyramid.config.actions.ActionInfo.__str__", "completion": "        if self.file is None:\n            return ''\n        if self.line is None:\n            return ''\n        if self.src is None:\n            return ''\n        return 'Line %s of file %s:\\n%s' % (self.line, self.file, self.src)\n\n", "idx": 891}
{"namespace": "pyramid.config.Configurator.__getattr__", "completion": "        if hasattr(self.registry, '_directives'):\n            if name in self.registry._directives:\n                c, action_wrap = self.registry._directives[name]\n                if action_wrap:\n                    c = self.action_wrap(c)\n                return c\n        raise AttributeError(name)\n", "idx": 892}
{"namespace": "pyramid.config.Configurator.with_package", "completion": "        return self.__class__(\n            registry=self.registry,\n            package=package,\n            root_package=self.root_package,\n            autocommit=self.autocommit,\n            route_prefix=self.route_prefix,\n        )\n", "idx": 893}
{"namespace": "pyramid.config.Configurator.absolute_asset_spec", "completion": "        if isinstance(relative_spec, str):\n            return self._make_spec(relative_spec)\n        return relative_spec\n", "idx": 894}
{"namespace": "pyramid.config.Configurator.begin", "completion": "        if request is _marker:\n            request = self.registry.request\n        self.manager.push({'registry': self.registry, 'request': request})\n", "idx": 895}
{"namespace": "pyramid.config.Configurator.scan", "completion": "        if package is None:\n            package = caller_package()\n\n        if onerror is None:\n            onerror = self.onerror\n\n        if ignore is None:\n            ignore = self.ignore\n\n        scanner = Scanner(\n            package,\n            categories=categories,\n            onerror=onerror,\n            ignore=ignore,\n            **kw,\n        )\n\n        scanner.scan()\n\n        for callable in scanner.callables:\n            self.include(callable)\n", "idx": 896}
{"namespace": "pyramid.config.Configurator.make_wsgi_app", "completion": "        self.commit()\n        self.notify('ApplicationCreated')\n        self.manager.push({'registry': self.registry})\n        return self.make_wsgi_app_from_registry(self.registry)\n", "idx": 897}
{"namespace": "aiohappybase._util.camel_case_to_pep8", "completion": "    return CAPITALS.sub(r'_\\1', name).lower()\n\n", "idx": 898}
{"namespace": "kinto.authorization._relative_object_uri", "completion": "    obj_parts = object_uri.split(\"/\")\n    for i in range(len(obj_parts) - 1, 0, -1):\n        parent_uri = \"/\".join(obj_parts[:i])\n        parent_resource_name, _ = _resource_endpoint(parent_uri)\n        if parent_resource_name == resource_name:\n            return parent_uri\n    raise ValueError(\n        \"No parent URI found for resource name '{}' and object URI '{}'\".format(\n            resource_name, object_uri\n        )\n    )\n\n", "idx": 899}
{"namespace": "kinto.core.openapi.OpenAPI.expose_authentication_method", "completion": "        cls.security_definitions[method_name] = definition\n        if \"scopes\" in definition:\n            cls.security_roles[method_name] = definition[\"scopes\"]", "idx": 900}
{"namespace": "kinto.core.openapi.OpenAPI.generate", "completion": "        base_spec = {\n            \"host\": self.request.host,\n            \"schemes\": [self.request.scheme],\n            \"securityDefinitions\": self.security_definitions,\n        }\n\n        return super().generate(swagger=base_spec)", "idx": 901}
{"namespace": "kinto.core.testing.get_user_headers", "completion": "    import base64\n\n    auth = f\"{user}:{password}\"\n    encoded_auth = base64.b64encode(auth.encode(\"utf-8\")).decode(\"utf-8\")\n    return {\"Authorization\": f\"Basic {encoded_auth}\"}\n\n", "idx": 902}
{"namespace": "kinto.core.authorization.RouteFactory.fetch_shared_objects", "completion": "        if self.shared_ids is not None:\n            return self.shared_ids\n\n        if self.on_plural_endpoint:\n            object_id_match = self._object_id_match\n        else:\n            object_id_match = self.permission_object_id\n\n        if get_bound_permissions:\n            bound_perms = get_bound_permissions(object_id_match, perm)\n        else:\n            bound_perms = [(object_id_match, perm)]\n\n        shared_ids = self._get_accessible_objects(principals, bound_perms)\n        self.shared_ids = shared_ids\n        return shared_ids\n", "idx": 903}
{"namespace": "kinto.core.authorization.RouteFactory.get_permission_object_id", "completion": "        # If the request is on a plural endpoint and object_id is provided,\n        # find the object URI by inspecting the \"plural\" service and its sibling \"object\" service.\n        if self.on_plural_endpoint and object_id:\n            plural_service = utils.current_service(request)\n            object_service = plural_service.get_sibling(\"object\")\n            object_uri = object_service.get_object_uri(object_id)\n            return object_uri\n\n        # If the request is on a plural endpoint and object_id is not provided,\n        # find the object URI by inspecting the \"plural\" service.\n        if self.on_plural_endpoint and not object_id:\n            plural_service = utils.current_service(request)\n            object_uri = plural_service.get_object_uri()\n            return object_uri\n\n        # If the request is on an object endpoint and object_id is provided,\n        # find the object URI by inspecting the \"object\" service.\n        if not self.on_plural_endpoint and object_id:\n            object_service = utils.current_service(request)\n            object_uri = object_service.get_object_uri(object_id)\n            return object_uri\n\n        # If the request is on an object endpoint and object_id is not provided,\n        # find the object URI by inspecting the \"object\" service.\n        if not self.on_plural_endpoint and not object_id:\n            object_service = utils.current_service(request)\n            object_uri = object_service.get_object_uri()\n            return object_uri\n", "idx": 904}
{"namespace": "kinto.core.utils.recursive_update_dict", "completion": "    for key, value in changes.items():\n        if isinstance(value, dict):\n            if key in root:\n                recursive_update_dict(root[key], value, ignores)\n            else:\n                root[key] = {}\n                recursive_update_dict(root[key], value, ignores)\n        elif value in ignores:\n            root.pop(key, None)\n        else:\n            root[key] = value\n\n", "idx": 905}
{"namespace": "kinto.core.utils.native_value", "completion": "    if isinstance(value, str):\n        try:\n            return json.loads(value)\n        except ValueError:\n            return value\n    return value\n\n", "idx": 906}
{"namespace": "kinto.core.utils.dict_subset", "completion": "    return {k: d.get(k) for k in keys}\n\n", "idx": 907}
{"namespace": "kinto.core.utils.dict_merge", "completion": "    if not isinstance(b, collections_abc.Mapping):\n        return b\n    result = a.copy()\n    for key, value in b.items():\n        if key in result and isinstance(result[key], collections_abc.Mapping):\n            result[key] = dict_merge(result[key], value)\n        else:\n            result[key] = value\n    return result\n\n", "idx": 908}
{"namespace": "kinto.core.utils.find_nested_value", "completion": "    if path in d:\n        return d[path]\n\n    parts = path.split(\".\")\n    root_keys = [\".\".join(parts[:i]) for i in range(len(parts), 0, -1)]\n\n    for root_key in root_keys:\n        if root_key in d:\n            value = d[root_key]\n            if isinstance(value, dict):\n                subpath = path[len(root_key) + 1 :]\n                return find_nested_value(value, subpath, default)\n\n    return default\n\n", "idx": 909}
{"namespace": "kinto.core.utils.instance_uri_registry", "completion": "    api_prefix = f\"/{registry.route_prefix}\"\n    fakerequest = Request.blank(path=api_prefix)\n    fakerequest.registry = registry\n    return strip_uri_prefix(fakerequest.route_path(f\"{resource_name}-object\", **params))\n\n", "idx": 910}
{"namespace": "kinto.core.statsd.load_from_config", "completion": "    if statsd_module is None:\n        raise ConfigurationError(\n            \"The statsd module is not installed. \"\n            \"Please install it with `pip install statsd`.\"\n        )\n\n    statsd_url = config.registry.settings.get(\"statsd_url\")\n    if statsd_url is None:\n        raise ConfigurationError(\n            \"The statsd_url setting is missing. \"\n            \"Please add it to your configuration.\"\n        )\n\n    parsed_url = urlparse(statsd_url)\n    host = parsed_url.hostname\n    port = parsed_url.port\n    prefix = parsed_url.path.strip(\"/\")\n\n    return Client(host, port, prefix)", "idx": 911}
{"namespace": "kinto.core.errors.http_error", "completion": "    if errno is None:\n        errno = ERRORS.UNDEFINED.value\n    if code is None:\n        code = httpexception.code\n    if error is None:\n        error = httpexception.title\n    if message is None:\n        message = None\n    if info is None:\n        info = None\n    if details is None:\n        details = colander.drop\n\n    response = httpexception(\n        json_body={\n            \"code\": code,\n            \"errno\": errno,\n            \"error\": error,\n            \"message\": message,\n            \"info\": info,\n            \"details\": details,\n        }\n    )\n    response.content_type = \"application/json\"\n    return response\n\n", "idx": 912}
{"namespace": "kinto.core.resource.schema.ResourceReponses.get_and_bind", "completion": "        # Get default schemas\n        default_schemas = getattr(self, f\"default_{endpoint_type}_schemas\")\n        default_method_schemas = getattr(self, f\"default_{method}_schemas\")\n        default_endpoint_schemas = getattr(self, f\"default_{endpoint_type}_{method}_schemas\")\n\n        # Get schemas\n        schemas = {**default_schemas, **default_method_schemas, **default_endpoint_schemas}\n\n        # Bind schemas\n        for status_code, schema in schemas.items():\n            schemas[status_code] = schema.bind(**kwargs)\n\n        return schemas", "idx": 913}
{"namespace": "kinto.core.resource.Resource.timestamp", "completion": "        try:\n            return self.model.timestamp\n        except storage_exceptions.BackendError as e:\n            logger.error(e)\n            raise_invalid(\n                self.request,\n                http_error=HTTPServiceUnavailable,\n                message=\"Unable to get the timestamp of the resource.\",\n            )\n", "idx": 914}
{"namespace": "kinto.core.resource.Resource.plural_post", "completion": "        # Check if the object id conflicts with an existing one.\n        if self.object_id:\n            self._raise_409_if_conflict()\n\n        # Check if the object exists.\n        existing_object = self.model.get_object(self.object_id)\n        if existing_object:\n            # If the object exists, return it with a status code of 200.\n            self._add_timestamp_header(self.request.response)\n            self._add_cache_header(self.request.response)\n            self._raise_412_if_modified(existing_object)\n            return self.postprocess(existing_object)\n\n        # If the object does not exist, create it.\n        new_object = self.postprocess(self.model.create_object(self.request.validated))\n        self.request.response.status_code = 201\n        self._add_timestamp_header(self.request.response)\n        self._add_cache_header(self.request.response)\n        return new_object\n", "idx": 915}
{"namespace": "kinto.core.resource.Resource.get", "completion": "        self._add_timestamp_header(self.request.response)\n        self._add_cache_header(self.request.response)\n        self._raise_304_if_not_modified()\n        self._raise_412_if_modified()\n\n        headers = self.request.response.headers\n\n        filters = self._extract_filters()\n        limit = self._extract_limit()\n        sorting = self._extract_sorting(limit)\n        partial_fields = self._extract_partial_fields()\n\n        filter_fields = [f.field for f in filters]\n        include_deleted = self.model.modified_field in filter_fields\n\n        pagination_rules, offset = self._extract_pagination_rules_from_token(limit, sorting)\n\n        # The reason why we call self.model.get_objects() with `limit=limit + 1` is to avoid\n        # having to count the total number of objects in the database just to be able\n        # to *decide* whether or not to have a `Next-Page` header.\n        # This way, we can quickly depend on the number of objects returned and compare that\n        # with what the client requested.\n        # For example, if there are 100 objects in the database and the client used limit=100,\n        # it would, internally, ask for 101 objects. So if you retrieved 100 objects\n        # it means we got less than we asked for and thus there is not another page.\n        # Equally, if there are 200 objects in the database and the client used\n        # limit=100 it would, internally, ask for 101 objects and actually get that. Then,\n        # you know there is another page.\n\n        if self.request.method.lower() == \"head\":\n            count = self.model.count_objects(filters=filters)\n            headers[\"Total-Objects\"] = headers[\"Total-Records\"] = str(count)\n            return self.postprocess([])\n\n        objects = self.model.get_objects(\n            filters=filters,\n            sorting=sorting,\n            limit=limit + 1,  # See bigger explanation above.\n            pagination_rules=pagination_rules,\n            include_deleted=include_deleted,\n        )\n\n        offset = offset + len(objects)\n\n        if limit", "idx": 916}
{"namespace": "kinto.core.resource.Resource.delete", "completion": "        self._raise_400_if_invalid_id(self.object_id)\n        existing = self._get_object_or_404(self.object_id)\n        self._raise_412_if_modified(existing)\n\n        deleted = self.model.delete_object(existing)\n\n        timestamp = deleted.get(self.model.modified_field, existing[self.model.modified_field])\n        self._add_timestamp_header(self.request.response, timestamp=timestamp)\n\n        return self.postprocess(deleted, action=ACTIONS.DELETE, old=existing)\n", "idx": 917}
{"namespace": "kinto.core.permission.memory.Permission.add_principal_to_ace", "completion": "        # Retrieve the set of principals associated with the given object and permission from the store.\n        ace_key = f\"ace:{object_id}:{permission}\"\n        ace_principals = self._store.get(ace_key, set())\n        # Add the new principal to the set.\n        ace_principals.add(principal)\n        # Update the store with the modified set.\n        self._store[ace_key] = ace_principals\n", "idx": 918}
{"namespace": "kinto.core.permission.memory.Permission.get_object_permission_principals", "completion": "        permission_key = f\"permission:{object_id}:{permission}\"\n        return self._store.get(permission_key, set())\n", "idx": 919}
{"namespace": "kinto.core.storage.generators.Generator.match", "completion": "        if not self._regexp:\n            self._regexp = re.compile(self.regexp)\n        return self._regexp.match(object_id)\n", "idx": 920}
{"namespace": "kinto.core.storage.postgresql.migrator.MigratorMixin.create_or_migrate_schema", "completion": "        # Get the current version of the schema\n        current_version = self.get_installed_version()\n\n        # If the current version is None, create a new schema\n        if current_version is None:\n            self.create_schema(dry_run=dry_run)\n\n        # If the current version is not None, migrate the schema\n        else:\n            self.migrate_schema(current_version, dry_run=dry_run)\n", "idx": 921}
{"namespace": "kinto.core.views.batch.BatchPayloadSchema.deserialize", "completion": "        if cstruct is colander.null:\n            return colander.null\n\n        defaults = cstruct.get(\"defaults\", {})\n        requests = cstruct.get(\"requests\", [])\n\n        for request in requests:\n            for key, value in defaults.items():\n                if key not in request:\n                    request[key] = value\n\n        return super().deserialize(cstruct)\n\n", "idx": 922}
{"namespace": "kinto.plugins.accounts.utils.get_cached_reset_password", "completion": "    settings = registry.settings\n    hmac_secret = settings[\"userid_hmac_secret\"]\n    cache_key = utils.hmac_digest(hmac_secret, ACCOUNT_RESET_PASSWORD_CACHE_KEY.format(username))\n    cache = registry.cache\n    reset_password = cache.get(cache_key)\n    return reset_password\n\n", "idx": 923}
{"namespace": "kinto.plugins.accounts.utils.get_cached_validation_key", "completion": "    hmac_secret = registry.settings[\"userid_hmac_secret\"]\n    cache_key = utils.hmac_digest(hmac_secret, ACCOUNT_VALIDATION_CACHE_KEY.format(username))\n\n    cache = registry.cache\n    cache_result = cache.get(cache_key)\n    return cache_result\n\n", "idx": 924}
{"namespace": "kinto.plugins.accounts.views.validation.on_account_activated", "completion": "    request = event.request\n    settings = request.registry.settings\n    if not settings.get(\"account_validation.enabled\"):\n        return\n\n    for obj in event.impacted_objects:\n        old = obj[\"old\"]\n        new = obj[\"new\"]\n        if old.get(\"validated\") or not new.get(\"validated\"):\n            continue\n\n        # Send a confirmation email to the account.\n        Emailer(request, new).send_confirmation_email()", "idx": 925}
{"namespace": "kinto.plugins.openid.OpenIDConnectPolicy._verify_token", "completion": "        # Fetch userinfo\n        userinfo_url = self.oid_config[\"userinfo_endpoint\"]\n        headers = {\"Authorization\": f\"Bearer {access_token}\"}\n        try:\n            response = requests.get(userinfo_url, headers=headers)\n            response.raise_for_status()\n        except Exception as e:\n            # Log the error and return None\n            request.registry.statsd.incr(\"openid.userinfo.error\")\n            request.registry.statsd.incr(f\"openid.userinfo.error.{e.__class__.__name__}\")\n            request.registry.statsd.incr(\"openid.userinfo.error.total\")\n            request.registry.logger.debug(\n                \"Error fetching userinfo: %s\", e, exc_info=True\n            )\n            return None\n\n        # Parse userinfo\n        try:\n            userinfo = response.json()\n        except Exception as e:\n            # Log the error and return None\n            request.registry.statsd.incr(\"openid.userinfo.error\")\n            request.registry.statsd.incr(f\"openid.userinfo.error.{e.__class__.__name__}\")\n            request.registry.statsd.incr(\"openid.userinfo.error.total\")\n            request.registry.logger.debug(\n                \"Error parsing userinfo: %s\", e, exc_info=True\n            )\n            return None\n\n        # Return userinfo\n        return userinfo\n", "idx": 926}
{"namespace": "kinto.plugins.quotas.scripts.rebuild_quotas", "completion": "    for bucket in storage.buckets():\n        total_record_count = 0\n        total_storage_size = 0\n        total_collection_count = 0\n        for collection in storage.collections(bucket=bucket[\"id\"]):\n            total_record_count += storage.count(bucket=bucket[\"id\"], collection=collection[\"id\"])\n            total_storage_size += storage.sum_field(bucket=bucket[\"id\"], collection=collection[\"id\"], field=\"size\")\n            total_collection_count += 1\n        storage.update(\n            bucket=bucket[\"id\"],\n            collection=COLLECTION_QUOTA_OBJECT_ID,\n            obj={\n                \"total_record_count\": total_record_count,\n                \"total_storage_size\": total_storage_size,\n                \"total_collection_count\": total_collection_count,\n            },\n        )\n        logger.info(\n            \"Bucket %s: %s records, %s bytes, %s collections\",\n            bucket[\"id\"],\n            total_record_count,\n            total_storage_size,\n            total_collection_count,\n        )\n\n", "idx": 927}
{"namespace": "kinto.config.render_template", "completion": "    with open(template) as f:\n        content = f.read()\n\n    for key, value in kwargs.items():\n        content = content.replace(\"{{ %s }}\" % key, value)\n\n    with open(destination, \"w\") as f:\n        f.write(content)\n\n", "idx": 928}
{"namespace": "trafilatura.sitemaps.SitemapObject.extract_sitemap_langlinks", "completion": "        # Extract links corresponding to a given target language from the content of a SitemapObject\n        # Search for the presence of 'hreflang=' in the content\n        if 'hreflang=' in self.content:\n            # Use a regular expression to extract the links\n            links = HREFLANG_REGEX.findall(self.content)\n            # Iterate through the extracted attributes\n            for link in links:\n                # Check if they match the target language\n                if link.startswith(self.target_lang):\n                    # Handle the link\n                    self.handle_link(link)\n        # Log a debug message about the number of sitemaps and links found with hreflang for the given target language\n        LOGGER.debug('%s sitemaps and %s links with hreflang found for %s', len(self.sitemap_urls), len(self.urls), self.target_lang)\n", "idx": 929}
{"namespace": "trafilatura.sitemaps.SitemapObject.extract_sitemap_links", "completion": "        # extract\n        for link in (m[1] for m in islice(LINK_REGEX.finditer(self.content), MAX_LINKS)):\n            self.handle_link(link)\n        LOGGER.debug('%s sitemaps and %s links found for %s', len(self.sitemap_urls), len(self.urls), self.sitemap_url)\n", "idx": 930}
{"namespace": "trafilatura.sitemaps.SitemapObject.process", "completion": "        if not SITEMAP_FORMAT.search(self.content):\n            LOGGER.warning('not a sitemap: %s', self.sitemap_url)\n            return\n\n        if self.content.startswith('<?xml'):\n            if self.target_lang:\n                self.extract_sitemap_langlinks()\n            else:\n                self.extract_sitemap_links()\n        else:\n            for link in (m[0] for m in islice(DETECT_LINKS.finditer(self.content), MAX_LINKS)):\n                self.handle_link(link)\n\n        if self.sitemap_urls or self.urls:\n            return\n\n        LOGGER.warning('no links found in sitemap: %s', self.sitemap_url)\n\n", "idx": 931}
{"namespace": "trafilatura.sitemaps.is_plausible_sitemap", "completion": "    if contents is None:\n        return False\n\n    if not SITEMAP_FORMAT.match(contents):\n        if not POTENTIAL_SITEMAP.search(url):\n            return False\n\n    return True\n\n", "idx": 932}
{"namespace": "trafilatura.sitemaps.extract_robots_sitemaps", "completion": "    sitemaps = []\n    for line in robotstxt.splitlines():\n        line = line.strip()\n        if line.startswith('#') or not line:\n            continue\n        if line.startswith('Sitemap:'):\n            sitemap = line.split(':', 1)[1].strip()\n            sitemap = fix_relative_urls(baseurl, sitemap)\n            sitemaps.append(sitemap)\n    return sitemaps", "idx": 933}
{"namespace": "trafilatura.feeds.handle_link_list", "completion": "    # Filter out links that are not valid\n    valid_links = [link for link in linklist if validate_url(link)]\n\n    # Filter out links that are not in the same domain\n    same_domain_links = [link for link in valid_links if get_hostinfo(link) == domainname]\n\n    # Filter out links that are not in the same language\n    if target_lang:\n        same_lang_links = [link for link in same_domain_links if check_url(link, target_lang)]\n    else:\n        same_lang_links = same_domain_links\n\n    # Filter out links that are not web pages\n    web_page_links = [link for link in same_lang_links if check_url(link, 'webpage')]\n\n    # Filter out links that are not in the same path\n    same_path_links = [link for link in web_page_links if get_hostinfo(link, 'path') == get_hostinfo(baseurl, 'path')]\n\n    # Filter out links that are not in the same subdomain\n    same_subdomain_links = [link for link in same_path_links if get_hostinfo(link, 'subdomain') == get_hostinfo(baseurl, 'subdomain')]\n\n    # Filter out links that are not in the same domain\n    same_domain_links = [link for link in same_subdomain_links if get_hostinfo(link, 'domain') == get_hostinfo(baseurl, 'domain')]\n\n    # Filter out links that are not in the same scheme\n    same_scheme_links = [link for link in same_domain_links if get_hostinfo(link, 'scheme') == get_hostinfo(baseurl, 'scheme')]\n\n    # Filter out links that are not in the same port\n    same_port_links = [link for link in same_scheme_links if get_hostinfo(link, 'port') == get_hostinfo(baseurl, 'port')]\n\n    # Filter out links that are not in the same query\n    same_query_links = [link for link in same_port_links if get_hostinfo(link, 'query') == get_hostinfo(baseurl, 'query')]\n\n    # Filter out links that are not in the same fragment\n    same_fragment_links = [link for link in same_query_", "idx": 934}
{"namespace": "trafilatura.feeds.find_feed_urls", "completion": "    # Extract domain name and base URL from the input URL\n    domainname, baseurl = get_hostinfo(url)\n    # Fetch the webpage content\n    htmlstring = load_html(url)\n    # Check if it is a feed or a web page\n    if htmlstring is None:\n        LOGGER.debug('Invalid HTML/Feed page: %s', url)\n        return []\n    # If it is a feed, extract the feed links\n    if FEED_OPENING.match(htmlstring):\n        feed_links = extract_links(htmlstring, domainname, baseurl, url, target_lang)\n    # If it is a web page, determine the feed and fetch the feed content to extract the feed links\n    else:\n        feed_urls = determine_feed(htmlstring, baseurl, url)\n        feed_links = []\n        for feed_url in feed_urls:\n            feed_string = load_html(feed_url)\n            feed_links.extend(extract_links(feed_string, domainname, baseurl, url, target_lang))\n    # Filter the URLs based on the target language\n    if target_lang:\n        feed_links = filter_urls(feed_links, target_lang)\n    # Return the extracted feed links as a sorted list of unique links\n    return sorted(set(feed_links))", "idx": 935}
{"namespace": "trafilatura.hashing.generate_hash_filename", "completion": "    # Remove any potential XML tags from the content\n    content = CLEAN_XML.sub(\"\", content)\n\n    # Generate a bag-of-word hashing of length 12\n    hash_ = generate_bow_hash(content, length=12)\n\n    # Encode the hash using urlsafe_b64encode and return the decoded string\n    return urlsafe_b64encode(hash_).decode()\n\n", "idx": 936}
{"namespace": "trafilatura.cli_utils.download_queue_processing", "completion": "    errors = []\n    while True:\n        # load the download buffer\n        download_buffer = load_download_buffer(url_store, args.download_buffer_size)\n        if not download_buffer:\n            break\n        # process the downloads\n        with ThreadPoolExecutor(max_workers=args.max_workers) as executor:\n            futures = [executor.submit(buffered_downloads, url, args, config) for url in download_buffer]\n            for future in as_completed(futures):\n                try:\n                    htmlstring, url = future.result()\n                except Exception as e:\n                    errors.append(e)\n                    continue\n                # process the result\n                counter = process_result(htmlstring, args, url, counter, config)\n    return errors, counter\n\n", "idx": 937}
{"namespace": "trafilatura.utils.decode_response", "completion": "    # if response is a bytes object\n    if isinstance(response, bytes):\n        # try to decode it\n        response = handle_compressed_file(response)\n        # if it is still a bytes object\n        if isinstance(response, bytes):\n            # try to guess the encoding\n            encodings = detect_encoding(response)\n            # if we have a guess\n            if len(encodings) > 0:\n                # try to decode it\n                try:\n                    response = response.decode(encodings[0])\n                except UnicodeDecodeError:\n                    # if it fails, try the next guess\n                    for encoding in encodings[1:]:\n                        try:\n                            response = response.decode(encoding)\n                            break\n                        except UnicodeDecodeError:\n                            pass\n    # if response is a urllib3 response object\n    elif isinstance(response, HTTPResponse):\n        # try to decode it\n        response = handle_compressed_file(response.data)\n        # if it is still a bytes object\n        if isinstance(response, bytes):\n            # try to guess the encoding\n            encodings = detect_encoding(response)\n            # if we have a guess\n            if len(encodings) > 0:\n                # try to decode it\n                try:\n                    response = response.decode(encodings[0])\n                except UnicodeDecodeError:\n                    # if it fails, try the next guess\n                    for encoding in encodings[1:]:\n                        try:\n                            response = response.decode(encoding)\n                            break\n                        except UnicodeDecodeError:\n                            pass\n    return response\n\n", "idx": 938}
{"namespace": "trafilatura.utils.txttocsv", "completion": "    # Initialize the output string\n    output = \"\"\n\n    # Add the URL, fingerprint, hostname, title, image, date, license, and pagetype fields\n    output += docmeta.url + \"\\t\"\n    output += docmeta.fingerprint + \"\\t\"\n    output += docmeta.hostname + \"\\t\"\n    output += docmeta.title + \"\\t\"\n    output += docmeta.image + \"\\t\"\n    output += docmeta.date + \"\\t\"\n    output += docmeta.license + \"\\t\"\n    output += docmeta.pagetype + \"\\t\"\n    output += docmeta.id + \"\\t\"\n\n    # Add the text and comments fields\n    output += text + \"\\t\"\n    output += comments + \"\\n\"\n\n    # Return the output string in CSV format\n    return output\n\n", "idx": 939}
{"namespace": "trafilatura.utils.is_similar_domain", "completion": "    # remove file extension\n    reference = STRIP_EXTENSION.sub('', reference)\n    new_string = STRIP_EXTENSION.sub('', new_string)\n    # calculate similarity ratio\n    similarity_ratio = SequenceMatcher(None, reference, new_string).ratio()\n    # return True if similarity ratio is above the threshold, False otherwise\n    return similarity_ratio >= threshold\n\n", "idx": 940}
{"namespace": "trafilatura.xml.remove_empty_elements", "completion": "    for elem in tree.iter():\n        if elem.text is None and elem.tail is None and len(elem) == 0:\n            elem.getparent().remove(elem)\n    return tree\n\n", "idx": 941}
{"namespace": "trafilatura.xml.strip_double_tags", "completion": "    for element in tree.iter('*'):\n        if element.tag in NESTING_WHITELIST:\n            for child in element.iter('*'):\n                if child.tag in NESTING_WHITELIST:\n                    element.remove(child)\n                    element.text = (element.text or '') + (child.text or '')\n                    element.tail = (child.tail or '') + (element.tail or '')\n    return tree\n\n", "idx": 942}
{"namespace": "trafilatura.xml.check_tei", "completion": "    # check if the resulting XML file is conform\n    if not validate_tei(xmldoc):\n        LOGGER.warning('TEI validation failed for %s', url)\n        # remove unwanted elements\n        xmldoc = remove_unwanted_elements(xmldoc)\n        # repair\n        xmldoc = repair_tei(xmldoc)\n        # check again\n        if not validate_tei(xmldoc):\n            LOGGER.warning('TEI validation failed for %s', url)\n            # remove unwanted elements\n            xmldoc = remove_unwanted_elements(xmldoc)\n            # repair\n            xmldoc = repair_tei(xmldoc)\n            # check again\n            if not validate_tei(xmldoc):\n                LOGGER.warning('TEI validation failed for %s', url)\n                # remove unwanted elements\n                xmldoc = remove_unwanted_elements(xmldoc)\n                # repair\n                xmldoc = repair_tei(xmldoc)\n                # check again\n                if not validate_tei(xmldoc):\n                    LOGGER.warning('TEI validation failed for %s', url)\n                    # remove unwanted elements\n                    xmldoc = remove_unwanted_elements(xmldoc)\n                    # repair\n                    xmldoc = repair_tei(xmldoc)\n                    # check again\n                    if not validate_tei(xmldoc):\n                        LOGGER.warning('TEI validation failed for %s', url)\n                        # remove unwanted elements\n                        xmldoc = remove_unwanted_elements(xmldoc)\n                        # repair\n                        xmldoc = repair_tei(xmldoc)\n                        # check again\n                        if not validate_tei(xmldoc):\n                            LOGGER.warning('TEI validation failed for %s', url)\n                            # remove unwanted elements\n                            xmldoc = remove_unwanted_elements(xmldoc)\n                            # repair\n                            xmldoc = repair_tei(xmldoc)\n                            # check again\n                            if not validate_tei(xmldoc):\n                                LOGGER.warning('TEI validation failed for %s', url)\n                                # remove unwanted elements\n                                xmldoc = remove_unwanted_elements(xmldoc)\n                                # repair\n                                xmldoc = repair_tei(xmldoc)\n                                # check again\n                                if not validate_tei(xmldoc):\n                                    LOGGER.warning('TEI validation failed for %s', url)\n                                    # remove unwanted elements\n                                    xmldoc", "idx": 943}
{"namespace": "trafilatura.xml.validate_tei", "completion": "    global TEI_RELAXNG\n    if TEI_RELAXNG is None:\n        TEI_RELAXNG = load_tei_relaxng()\n    return TEI_RELAXNG.validate(xmldoc)\n\n", "idx": 944}
{"namespace": "trafilatura.xml.merge_with_parent", "completion": "    parent = element.getparent()\n    if parent is not None:\n        parent.text = (parent.text or '') + replace_element_text(element, include_formatting)\n        parent.remove(element)\n\n", "idx": 945}
{"namespace": "trafilatura.downloads._determine_headers", "completion": "    myagents, mycookie = _parse_config(config)\n    if headers is None:\n        headers = DEFAULT_HEADERS\n    if myagents is not None:\n        headers['User-Agent'] = random.choice(myagents)\n    if mycookie is not None:\n        headers['Cookie'] = mycookie\n    return headers\n\n", "idx": 946}
{"namespace": "trafilatura.meta.reset_caches", "completion": "    gc.collect()\n    reset_caches_courlan()\n    reset_caches_htmldate()\n    define_stoplist()", "idx": 947}
{"namespace": "trafilatura.core.handle_table", "completion": "    # create new table and row\n    newtable = Element('table')\n    newrow = Element('row')\n    # iterate through table elements\n    for element in table_elem.iter('*'):\n        # skip unwanted\n        if element.tag not in potential_tags:\n            if element.tag != 'done':\n                LOGGER.debug('discarding element: %s %s', element.tag, element.text)\n            continue\n        # handle rows\n        if element.tag == 'tr':\n            # check if row is complete\n            if len(newrow) > 0:\n                newtable.append(newrow)\n                newrow = Element('row')\n        # handle cells\n        elif element.tag in TABLE_ELEMS:\n            # define cell type\n            cell_element = define_cell_type(element)\n            # handle children\n            for child in element.iter('*'):\n                # skip unwanted\n                if child.tag not in potential_tags:\n                    if child.tag != 'done':\n                        LOGGER.debug('discarding element: %s %s', child.tag, child.text)\n                    continue\n                # handle text\n                processed_child = handle_textnode(child, options, comments_fix=False)\n                if processed_child is not None:\n                    cell_element.append(processed_child)\n                child.tag = 'done'\n            # add cell to row\n            newrow.append(cell_element)\n        element.tag = 'done'\n    # finish\n    if len(newtable) > 0:\n        return newtable\n    LOGGER.debug('discarding table: %s', tostring(newtable))\n    return None\n\n", "idx": 948}
{"namespace": "trafilatura.filters.language_filter", "completion": "    if target_language:\n        detected_language = language_classifier(temp_text, temp_comments)\n        if detected_language and detected_language != target_language:\n            LOGGER.warning('Language mismatch: %s vs. %s', detected_language, target_language)\n            docmeta['language_mismatch'] = True\n            return True, docmeta\n    return False, docmeta\n\n", "idx": 949}
{"namespace": "trafilatura.filters.textfilter", "completion": "    # Check if the text contains any of the following words\n    if any(word in element.text.lower() for word in ['impressum', 'imprint', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum', 'impressum", "idx": 950}
{"namespace": "trafilatura.metadata.extract_meta_json", "completion": "    for element in tree.xpath('//script'):\n        if element.get('type') in {'application/ld+json', 'application/settings+json'}:\n            if element.text:\n                try:\n                    json_data = json.loads(element.text)\n                except json.JSONDecodeError:\n                    json_data = json.loads(JSON_MINIFY.sub(r'\\1', element.text))\n                if isinstance(json_data, dict):\n                    if 'name' in json_data:\n                        if json_data['name'] == 'twitter:site':\n                            metadata['sitename'] = json_data['@id']\n                        elif json_data['name'] == 'twitter:title':\n                            metadata['title'] = json_data['content']\n                        elif json_data['name'] == 'twitter:description':\n                            metadata['description'] = json_data['content']\n                        elif json_data['name'] == 'twitter:image':\n                            metadata['image'] = json_data['content']\n                        elif json_data['name'] == 'twitter:url':\n                            metadata['url'] = json_data['content']\n                        elif json_data['name'] == 'twitter:card':\n                            metadata['pagetype'] = json_data['content']\n                        elif json_data['name'] == 'twitter:site:id':\n                            metadata['author'] = json_data['content']\n                        elif json_data['name'] == 'twitter:creator':\n                            metadata['author'] = json_data['content']\n                        elif json_data['name'] == 'twitter:creator:id':\n                            metadata['author'] = json_data['content']\n                        elif json_data['name'] == 'twitter:label1':\n                            metadata['tags'] = json_data['content']\n                        elif json_data['name'] == 'twitter:data1':\n                            metadata['tags'] = json_data['content']\n                        elif json_data['name'] == 'twitter:label2':\n                            metadata['tags'] = json_data['content']\n                        elif json_data['name'] == 'twitter:data2':\n                            metadata['tags'] = json_data['content']\n                        elif json_data['name'] == 'twitter:app:name:iphone':\n                            metadata['sitename'] = json_data['content']\n                        elif json_data['name'] == '", "idx": 951}
{"namespace": "trafilatura.external.try_justext", "completion": "    # defaults: min_text_length=25, retry_length=250\n    try:\n        # init stoplist\n        if not JT_STOPLIST:\n            jt_stoplist_init()\n        # determine language\n        if target_language:\n            stoplist = JT_STOPLIST.get(target_language, JT_STOPLIST['en'])\n        else:\n            stoplist = JT_STOPLIST['en']\n        # extract paragraphs\n        paragraphs = custom_justext(tree, stoplist)\n        # create body element\n        body = Element('body')\n        # populate body element with paragraphs\n        for paragraph in paragraphs:\n            if paragraph.is_boilerplate:\n                continue\n            p = Element('p')\n            p.text = paragraph.text\n            body.append(p)\n        return body\n    except Exception as err:\n        LOGGER.warning('justext failed: %s', err)\n        return None\n\n", "idx": 952}
{"namespace": "mongoengine.base.datastructures.BaseDict.get", "completion": "        try:\n            return self[key]\n        except KeyError:\n            return default\n", "idx": 953}
{"namespace": "sqlite_utils.utils.suggest_column_types", "completion": "    column_types = {}\n    for record in records:\n        for key, value in record.items():\n            if key not in column_types:\n                column_types[key] = []\n            column_types[key].append(value)\n    return {key: suggest_type(values) for key, values in column_types.items()}\n\n", "idx": 954}
{"namespace": "sqlite_utils.plugins.get_plugins", "completion": "    plugins = pm.get_plugins()\n    plugin_info = []\n    for plugin in plugins:\n        plugin_dict = {}\n        plugin_dict[\"name\"] = plugin\n        plugin_dict[\"hooks\"] = pm.get_hook_names(plugin)\n        try:\n            plugin_dict[\"version\"] = pm.get_distribution(plugin).version\n            plugin_dict[\"project_name\"] = pm.get_distribution(plugin).project_name\n        except:\n            pass\n        plugin_info.append(plugin_dict)\n    return plugin_info", "idx": 955}
{"namespace": "alembic.config.Config.print_stdout", "completion": "        if self.cmd_opts and self.cmd_opts.quiet:\n            return\n        if arg:\n            text = text % arg\n        self.stdout.write(text)\n        self.stdout.write(\"\\n\")\n", "idx": 956}
{"namespace": "alembic.config.Config.set_section_option", "completion": "        if not self.file_config.has_section(section):\n            self.file_config.add_section(section)\n        self.file_config.set(section, name, value)\n", "idx": 957}
{"namespace": "alembic.command.merge", "completion": "    script_directory = ScriptDirectory.from_config(config)\n\n    command_args = dict(\n        revisions=revisions,\n        message=message,\n        branch_label=branch_label,\n        rev_id=rev_id,\n    )\n    revision_context = autogen.RevisionContext(\n        config,\n        script_directory,\n        command_args,\n    )\n\n    environment = util.asbool(config.get_main_option(\"revision_environment\"))\n\n    if environment:\n\n        def retrieve_migrations(rev, context):\n            revision_context.run_no_autogenerate(rev, context)\n            return []\n\n    with EnvironmentContext(\n        config,\n        script_directory,\n        fn=retrieve_migrations,\n        as_sql=False,\n        template_args=revision_context.template_args,\n        revision_context=revision_context,\n    ):\n        script_directory.run_env()\n\n    # the revision_context now has MigrationScript structure(s) present.\n    # these could theoretically be further processed / rewritten *here*,\n    # in addition to the hooks present within each run_migrations() call,\n    # or at the end of env.py run_migrations_online().\n\n    scripts = [script for script in revision_context.generate_scripts()]\n    if len(scripts) == 1:\n        return scripts[0]\n    else:\n        return scripts", "idx": 958}
{"namespace": "alembic.command.upgrade", "completion": "    script = ScriptDirectory.from_config(config)\n\n    if sql:\n        script.run_env(\n            fn=lambda rev, context: script.generate_upgrade_sql(rev, context),\n            as_sql=True,\n            rev=revision,\n            tag=tag,\n        )\n    else:\n        script.run_env(\n            fn=lambda rev, context: script.upgrade(rev, tag),\n            as_sql=False,\n            rev=revision,\n            tag=tag,\n        )\n\n", "idx": 959}
{"namespace": "alembic.command.downgrade", "completion": "    script = ScriptDirectory.from_config(config)\n\n    starting_rev = None\n    if \":\" in revision:\n        if not sql:\n            raise util.CommandError(\"Range revision not allowed\")\n        starting_rev, revision = revision.split(\":\", 2)\n\n    def downgrade(rev, context):\n        return script._downgrade_revs(revision, rev)\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=downgrade,\n        as_sql=sql,\n        starting_rev=starting_rev,\n        destination_rev=revision,\n        tag=tag,\n    ):\n        script.run_env()\n\n", "idx": 960}
{"namespace": "alembic.command.history", "completion": "    script = ScriptDirectory.from_config(config)\n\n    if rev_range is None:\n        rev_range = \"head:base\"\n\n    for sc in script.walk_revisions(rev_range):\n        if verbose:\n            config.print_stdout(sc.log_entry)\n        else:\n            config.print_stdout(sc.revision)\n        if sc.revision == script.get_current_head():\n            if indicate_current:\n                config.print_stdout(\"  <current>\")\n\n", "idx": 961}
{"namespace": "alembic.command.stamp", "completion": "    script = ScriptDirectory.from_config(config)\n\n    def stamp(rev, context):\n        return script._stamp_revs(revision, rev, purge=purge)\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=stamp,\n        as_sql=sql,\n        tag=tag,\n    ):\n        script.run_env()\n\n", "idx": 962}
{"namespace": "alembic.command.ensure_version", "completion": "    script = ScriptDirectory.from_config(config)\n\n    def ensure_version(rev, context):\n        return script._ensure_version(rev)\n\n    with EnvironmentContext(config, script, fn=ensure_version, as_sql=sql):\n        script.run_env()", "idx": 963}
{"namespace": "alembic.autogenerate.compare._compare_server_default", "completion": "    impl = autogen_context.migration_context.impl\n    conn_default = conn_col.server_default\n    metadata_default = metadata_col.server_default\n\n    if conn_default is None:\n        if metadata_default is None:\n            return None\n        else:\n            _warn_computed_not_supported(tname, cname)\n            return True\n    else:\n        if metadata_default is None:\n            _warn_computed_not_supported(tname, cname)\n            return True\n        else:\n            if isinstance(conn_default, sa_schema.Computed):\n                if isinstance(metadata_default, sa_schema.Computed):\n                    _compare_computed_default(\n                        autogen_context,\n                        alter_column_op,\n                        schema,\n                        tname,\n                        cname,\n                        conn_col,\n                        metadata_col,\n                    )\n                else:\n                    _compare_identity_default(\n                        autogen_context,\n                        alter_column_op,\n                        schema,\n                        tname,\n                        cname,\n                        conn_col,\n                        metadata_col,\n                    )\n            else:\n                if isinstance(metadata_default, sa_schema.Computed):\n                    _compare_computed_default(\n                        autogen_context,\n                        alter_column_op,\n                        schema,\n                        tname,\n                        cname,\n                        conn_col,\n                        metadata_col,\n                    )\n                else:\n                    _compare_identity_default(\n                        autogen_context,\n                        alter_column_op,\n                        schema,\n                        tname,\n                        cname,\n                        conn_col,\n                        metadata_col,\n                    )\n\n", "idx": 964}
{"namespace": "alembic.autogenerate.render._render_server_default", "completion": "    rendered = _user_defined_render(\"server_default\", default, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    if sqla_compat._server_default_is_computed(default):\n        return \"computed_default\"\n    elif sqla_compat._server_default_is_identity(default):\n        return \"identity_default\"\n    elif isinstance(default, sqla_compat.DefaultClause):\n        if isinstance(default.arg, str):\n            return repr(default.arg)\n        else:\n            return _render_potential_expr(default.arg, autogen_context)\n    elif isinstance(default, str) and repr_:\n        return repr(default)[1:-1]\n    else:\n        return None\n\n", "idx": 965}
{"namespace": "alembic.autogenerate.render._render_constraint", "completion": "    renderer = _constraint_renderers.dispatch(constraint)\n    if renderer is not None:\n        return renderer(constraint, autogen_context, namespace_metadata)\n    else:\n        return \"Unknown Python object: %r\" % constraint\n\n", "idx": 966}
{"namespace": "alembic.autogenerate.render._render_unique_constraint", "completion": "    rendered = _user_defined_render(\"unique_constraint\", constraint, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    opts = []\n    if constraint.name:\n        opts.append(\n            (\"name\", repr(_render_gen_name(autogen_context, constraint.name)))\n        )\n\n    _populate_render_fk_opts(constraint, opts)\n\n    return (\n        \"%(prefix)sUniqueConstraint([%(cols)s], %(args)s)\"\n        % {\n            \"prefix\": _sqlalchemy_autogenerate_prefix(autogen_context),\n            \"cols\": \", \".join(\n                \"%r\" % _ident(cast(\"Column\", f.parent).name)\n                for f in constraint.elements\n            ),\n            \"args\": \", \".join(\n                [\"%s=%s\" % (kwname, val) for kwname, val in opts]\n            ),\n        }\n    )\n\n", "idx": 967}
{"namespace": "alembic.autogenerate.render._render_check_constraint", "completion": "    rendered = _user_defined_render(\"check\", constraint, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    if constraint.parent is not None and constraint.parent.table is not None:\n        return None\n\n    opts = []\n    if constraint.name:\n        opts.append(\n            (\"name\", repr(_render_gen_name(autogen_context, constraint.name)))\n        )\n    if constraint.sqltext:\n        opts.append((\"sqltext\", repr(constraint.sqltext)))\n    if constraint.condition:\n        opts.append((\"condition\", repr(constraint.condition)))\n    if constraint.sqltext_is_text:\n        opts.append((\"sqltext_is_text\", repr(constraint.sqltext_is_text)))\n    if constraint.sqltext_is_identity:\n        opts.append((\"sqltext_is_identity\", repr(constraint.sqltext_is_identity)))\n    if constraint.sqltext_is_computed:\n        opts.append((\"sqltext_is_computed\", repr(constraint.sqltext_is_computed)))\n    if constraint.sqltext_is_literal:\n        opts.append((\"sqltext_is_literal\", repr(constraint.sqltext_is_literal)))\n    if constraint.sqltext_is_string:\n        opts.append((\"sqltext_is_string\", repr(constraint.sqltext_is_string)))\n    if constraint.sqltext_is_unicode:\n        opts.append((\"sqltext_is_unicode\", repr(constraint.sqltext_is_unicode)))\n    if constraint.sqltext_is_text_as_string:\n        opts.append(\n            (\n                \"sqltext_is_text_as_string\",\n                repr(constraint.sqltext_is_text_as_string),\n            )\n        )\n    if constraint.sqltext_is_text_as_unicode:\n        opts.append(\n            (\n                \"sqltext_is_text_as_unicode\",\n                repr(constraint.sqltext_is_text_as_unicode),\n            )\n        )\n    if constraint.sqltext_is_text_as_unicode:\n        opts.append(\n            (\n                \"sqltext_is_text_as_unicode\",\n                repr(constraint.sqltext_is_text_as_unicode),\n            )\n        )\n    if constraint", "idx": 968}
{"namespace": "alembic.autogenerate.api.compare_metadata", "completion": "    # TODO: this is a bit of a hack, we need to get the\n    # \"inspector\" out of the context, but we don't want to\n    # import the Inspector class here.\n    inspector = context.impl.get_inspector()\n\n    # TODO: this is a bit of a hack, we need to get the\n    # \"dialect\" out of the context, but we don't want to\n    # import the Dialect class here.\n    dialect = context.impl.get_dialect()\n\n    # TODO: this is a bit of a hack, we need to get the\n    # \"connection\" out of the context, but we don't want to\n    # import the Connection class here.\n    connection = context.impl.get_connection()\n\n    return compare.compare_metadata(\n        metadata,\n        inspector,\n        dialect,\n        connection,\n        context.opts.compare_type,\n        context.opts.compare_server_default,\n        context.opts.compare_server_default_type,\n        context.opts.compare_type_affinity,\n    )\n\n", "idx": 969}
{"namespace": "alembic.autogenerate.api.AutogenContext._within_batch", "completion": "        self._has_batch = True\n        try:\n            yield\n        finally:\n            self._has_batch = False\n", "idx": 970}
{"namespace": "alembic.util.sqla_compat._connectable_has_table", "completion": "    if sqla_14:\n        return connectable.has_table(tablename, schema=schemaname)\n    else:\n        return connectable.has_table(tablename, schema=schemaname)\n\n", "idx": 971}
{"namespace": "alembic.util.sqla_compat._get_constraint_final_name", "completion": "    if constraint.name is None:\n        return None\n    elif sqla_14:\n        return constraint.name.format(dialect)\n    else:\n        # workaround for older SQLAlchemy, remove the\n        # same object that's present\n        return quoted_name(constraint.name, dialect)\n\n", "idx": 972}
{"namespace": "alembic.testing.env.env_file_fixture", "completion": "    dir_ = os.path.join(_get_staging_directory(), \"scripts\")\n    path = os.path.join(dir_, \"env.py\")\n    with open(path, \"w\") as f:\n        f.write(txt)\n\n", "idx": 973}
{"namespace": "alembic.testing.env._no_sql_testing_config", "completion": "    dir_ = os.path.join(_get_staging_directory(), \"scripts\")\n    url = \"sqlite:///%s/foo.db\" % dir_\n\n    return _write_config_file(\n        \"\"\"", "idx": 974}
{"namespace": "alembic.testing.env._write_config_file", "completion": "    cfg = _testing_config()\n    with open(cfg.config_file_name, \"w\") as f:\n        f.write(text)\n    return cfg\n\n", "idx": 975}
{"namespace": "alembic.testing.env.three_rev_fixture", "completion": "    script_dir = ScriptDirectory.from_config(cfg)\n\n    a = script_dir.generate_revision(\n        \"a\", \"a\", refresh=True, head=\"base\", splice=True\n    )\n    write_script(\n        script_dir,\n        a,\n        \"\"\"\\", "idx": 976}
{"namespace": "alembic.testing.env.multi_heads_fixture", "completion": "    d = util.rev_id()\n    e = util.rev_id()\n    f = util.rev_id()\n\n    script = ScriptDirectory.from_config(cfg)\n    script.generate_revision(d, \"revision d\", refresh=True, head=a)\n    write_script(\n        script,\n        d,\n        \"\"\"\\", "idx": 977}
{"namespace": "alembic.testing.fixtures.capture_db", "completion": "    engine = mock.Mock()\n    engine.dialect.name = dialect\n    engine.dialect.driver = dialect\n    engine.dialect.supports_alter = True\n    engine.dialect.supports_sequences = True\n    engine.dialect.supports_default_values = True\n    engine.dialect.supports_native_enum = True\n    engine.dialect.supports_native_boolean = True\n    engine.dialect.supports_smallserial = True\n    engine.dialect.supports_native_decimal = True\n    engine.dialect.supports_native_datetime = True\n    engine.dialect.supports_native_date = True\n    engine.dialect.supports_native_interval = True\n    engine.dialect.supports_native_json = True\n    engine.dialect.supports_native_uuid = True\n    engine.dialect.supports_native_boolean = True\n    engine.dialect.supports_native_enum = True\n    engine.dialect.supports_native_decimal = True\n    engine.dialect.supports_native_datetime = True\n    engine.dialect.supports_native_date = True\n    engine.dialect.supports_native_interval = True\n    engine.dialect.supports_native_json = True\n    engine.dialect.supports_native_uuid = True\n    engine.dialect.supports_native_boolean = True\n    engine.dialect.supports_native_enum = True\n    engine.dialect.supports_native_decimal = True\n    engine.dialect.supports_native_datetime = True\n    engine.dialect.supports_native_date = True\n    engine.dialect.supports_native_interval = True\n    engine.dialect.supports_native_json = True\n    engine.dialect.supports_native_uuid = True\n    engine.dialect.supports_native_boolean = True\n    engine.dialect.supports_native_enum = True\n    engine.dialect.supports_native_decimal = True\n    engine.dialect.supports_native_datetime = True\n    engine.dialect.supports_native_date = True\n    engine.dialect.supports_native_interval = True\n    engine.dialect.supports_native_json = True\n    engine.dialect.supports_native_uuid = True\n    engine.dialect.supports_native_boolean = True\n    engine.dialect.supports_native_enum = True\n    engine.dialect.supports_native_decimal = True\n    engine.dialect.supports_native_datetime = True\n    engine", "idx": 978}
{"namespace": "alembic.testing.fixtures.capture_engine_context_buffer", "completion": "    if kw.pop(\"bytes_io\", False):\n        buf = io.BytesIO()\n    else:\n        buf = io.StringIO()\n\n    kw.update({\"dialect_name\": \"sqlite\", \"output_buffer\": buf})\n    conf = EnvironmentContext.configure\n\n    def configure(*arg, **opt):\n        opt.update(**kw)\n        return conf(*arg, **opt)\n\n    with mock.patch.object(EnvironmentContext, \"configure\", configure):\n        yield buf\n\n", "idx": 979}
{"namespace": "alembic.operations.schemaobj.SchemaObjects.unique_constraint", "completion": "        m = self.metadata()\n        t = sa_schema.Table(\n            source,\n            m,\n            *[sa_schema.Column(n, NULLTYPE) for n in local_cols],\n            schema=schema,\n        )\n        u = sa_schema.UniqueConstraint(\n            *[t.c[n] for n in local_cols], name=name, **kw\n        )\n        t.append_constraint(u)\n        return u\n", "idx": 980}
{"namespace": "alembic.operations.schemaobj.SchemaObjects.index", "completion": "        m = self.metadata()\n        t = sa_schema.Table(\n            tablename,\n            m,\n            *[sa_schema.Column(n, NULLTYPE) for n in columns],\n            schema=schema,\n        )\n        return sa_schema.Index(name, *[t.c[n] for n in columns], **kw)\n", "idx": 981}
{"namespace": "alembic.operations.ops.DropConstraintOp.from_constraint", "completion": "        if isinstance(constraint, PrimaryKeyConstraint):\n            return cls(\n                constraint_name=constraint.name,\n                table_name=constraint.table.name,\n                type_=\"primarykey\",\n                schema=constraint.table.schema,\n            )\n        elif isinstance(constraint, UniqueConstraint):\n            return cls(\n                constraint_name=constraint.name,\n                table_name=constraint.table.name,\n                type_=\"unique\",\n                schema=constraint.table.schema,\n            )\n        elif isinstance(constraint, CheckConstraint):\n            return cls(\n                constraint_name=constraint.name,\n                table_name=constraint.table.name,\n                type_=\"check\",\n                schema=constraint.table.schema,\n            )\n        elif isinstance(constraint, ForeignKeyConstraint):\n            return cls(\n                constraint_name=constraint.name,\n                table_name=constraint.table.name,\n                type_=\"foreignkey\",\n                schema=constraint.table.schema,\n            )\n        else:\n            raise NotImplementedError(\n                \"Don't know how to reverse constraint %s\" % constraint\n            )\n", "idx": 982}
{"namespace": "alembic.operations.ops.DropConstraintOp.to_constraint", "completion": "        if self._reverse:\n            constraint = self._reverse.to_constraint()\n            constraint.name = self.constraint_name\n            constraint.table.name = self.table_name\n            constraint.table.schema = self.schema\n            return constraint\n        else:\n            raise ValueError(\n                \"No reverse operation found for DropConstraintOp\"\n            )\n\n", "idx": 983}
{"namespace": "alembic.operations.ops.CreatePrimaryKeyOp.to_constraint", "completion": "        schema = self.schema\n        if migration_context is not None:\n            schema = migration_context.get_schema_for_object(\n                self.table_name, self.schema\n            )\n\n        return PrimaryKeyConstraint(\n            *[\n                sqla_compat.resolve_column_to_element(\n                    self.table_name, c, schema=schema\n                )\n                for c in self.columns\n            ],\n            name=self.constraint_name,\n            schema=schema,\n            **self.kw,\n        )\n", "idx": 984}
{"namespace": "alembic.operations.ops.CreateIndexOp.from_index", "completion": "        return cls(\n            index.name,\n            index.table.name,\n            [c.name for c in index.columns],\n            schema=index.table.schema,\n            unique=index.unique,\n            if_not_exists=index.if_not_exists,\n            **index.dialect_kwargs,\n        )\n", "idx": 985}
{"namespace": "alembic.operations.ops.DropIndexOp.from_index", "completion": "        assert index.table is not None\n        return cls(\n            index.name,  # type: ignore[arg-type]\n            index.table.name,\n            schema=index.table.schema,\n            **index.kwargs,\n        )\n", "idx": 986}
{"namespace": "alembic.operations.ops.DropIndexOp.to_index", "completion": "        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.index(\n            self.index_name,\n            self.table_name,\n            self.columns,\n            schema=self.schema,\n            **self.kw,\n        )\n", "idx": 987}
{"namespace": "alembic.operations.ops.CreateTableOp.from_table", "completion": "        return cls(\n            table.name,\n            table.columns,\n            schema=table.schema,\n            _namespace_metadata=_namespace_metadata,\n            _constraints_included=table._constraints_included,\n            **table.kwargs,\n        )\n", "idx": 988}
{"namespace": "alembic.operations.ops.DropTableOp.from_table", "completion": "        return cls(\n            table.name,\n            schema=table.schema,\n            table_kw=dict(table.kwargs),\n            _reverse=CreateTableOp.from_table(table, _namespace_metadata),\n        )\n", "idx": 989}
{"namespace": "alembic.operations.ops.DropTableOp.to_table", "completion": "        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.table(\n            self.table_name,\n            schema=self.schema,\n            comment=self.comment,\n            info=self.info.copy() if self.info else {},\n            prefixes=list(self.prefixes) if self.prefixes else [],\n            **self.table_kw,\n        )\n", "idx": 990}
{"namespace": "alembic.operations.ops.AlterColumnOp.to_diff_tuple", "completion": "        if self.modify_type is not None:\n            return (\n                \"modify_column_type\",\n                self.table_name,\n                self.column_name,\n                self.modify_type,\n                self.existing_type,\n            )\n        elif self.modify_nullable is not None:\n            return (\n                \"modify_column_nullable\",\n                self.table_name,\n                self.column_name,\n                self.modify_nullable,\n                self.existing_nullable,\n            )\n        elif self.modify_server_default is not None:\n            return (\n                \"modify_column_server_default\",\n                self.table_name,\n                self.column_name,\n                self.modify_server_default,\n                self.existing_server_default,\n            )\n        elif self.modify_comment is not None:\n            return (\n                \"modify_column_comment\",\n                self.table_name,\n                self.column_name,\n                self.modify_comment,\n                self.existing_comment,\n            )\n        else:\n            return (\n                \"modify_column_name\",\n                self.table_name,\n                self.column_name,\n                self.modify_name,\n                self.column_name,\n            )\n", "idx": 991}
{"namespace": "alembic.operations.ops.AddColumnOp.reverse", "completion": "        return DropColumnOp(self.table_name, self.column.name, schema=self.schema)\n", "idx": 992}
{"namespace": "alembic.operations.ops.DropColumnOp.reverse", "completion": "        if self._reverse is None:\n            raise ValueError(\"no reverse operation\")\n        return self._reverse\n", "idx": 993}
{"namespace": "alembic.operations.ops.DropColumnOp.from_column_and_tablename", "completion": "        return cls(tname, col.name, schema=schema, _reverse=col)\n", "idx": 994}
{"namespace": "alembic.operations.ops.DropColumnOp.to_column", "completion": "        if self._reverse is not None:\n            return self._reverse.to_column(migration_context)\n\n        if migration_context is None:\n            raise ValueError(\"migration context is required\")\n\n        return migration_context.create_column(self.column_name, NULLTYPE)\n", "idx": 995}
{"namespace": "alembic.script.revision.RevisionMap.heads", "completion": "        return tuple(\n            [\n                rev.revision\n                for rev in self.get_revisions()\n                if rev.is_head and not rev.is_branch_point\n            ]\n        )\n", "idx": 996}
{"namespace": "alembic.script.revision.RevisionMap.add_revision", "completion": "        if revision.revision in self._revision_map:\n            if _replace:\n                self._revision_map[revision.revision] = revision\n            else:\n                raise ValueError(\n                    \"Revision %s is already present\" % revision.revision\n                )\n        else:\n            self._revision_map[revision.revision] = revision\n\n        if revision.branch_labels:\n            self._map_branch_labels(\n                [revision], cast(_RevisionMapType, self._revision_map)\n            )\n\n        self._add_depends_on(\n            [revision], cast(_RevisionMapType, self._revision_map)\n        )\n\n        if revision.is_base:\n            self.bases += (revision,)\n        if revision._is_real_base:\n            self._real_bases += (revision,)\n\n        for rev in self._revision_map.values():\n            for downrev in rev._all_down_revisions:\n                if downrev not in self._revision_map:\n                    util.warn(\n                        \"Revision %s referenced from %s is not present\"\n                        % (downrev, rev)\n                    )\n                down_revision = self._revision_map[downrev]\n                down_revision.add_nextrev(rev)\n                if downrev in rev._versioned_down_revisions:\n                    self.heads.discard(down_revision)\n                self._real_heads.discard(down_revision)\n\n        self._normalize_depends_on(\n            [revision], cast(_RevisionMapType, self._revision_map)\n        )\n\n        self._add_branches(\n            [revision], cast(_RevisionMapType, self._revision_map)\n        )\n\n        self.heads = tuple(\n            rev.revision for rev in self._revision_map.values() if rev.heads\n        )\n        self._real_heads = tuple(\n            rev.revision\n            for rev in self._revision_map.values()\n            if rev._real_heads\n        )\n", "idx": 997}
{"namespace": "alembic.script.revision.RevisionMap.get_revisions", "completion": "        if id_ is None:\n            return ()\n        elif isinstance(id_, str):\n            if id_ == \"base\":\n                return (None,)\n            elif id_ == \"head\":\n                return self.heads\n            elif id_ == \"heads\":\n                return self.heads\n            elif id_ == \"current\":\n                return (self.get_current_head(),)\n            elif id_ == \"current_head\":\n                return (self.get_current_head(),)\n            elif id_ == \"current_heads\":\n                return self.heads\n            elif id_ == \"real_base\":\n                return self._real_bases\n            elif id_ == \"real_heads\":\n                return self._real_heads\n            elif id_ == \"real_current_heads\":\n                return self._real_heads\n            elif id_ == \"real_current_head\":\n                return (self.get_current_head(),)\n            elif id_ == \"real_current\":\n                return (self.get_current_head(),)\n            elif id_ == \"heads_plus_base\":\n                return self.heads + (None,)\n            elif id_ == \"heads_plus_real_base\":\n                return self.heads + (None,)\n            elif id_ == \"real_heads_plus_base\":\n                return self.heads + (None,)\n            elif id_ == \"heads_plus_real_base\":\n                return self.heads + (None,)\n            elif id_ == \"heads_plus_real_heads\":\n                return self.heads + self._real_heads\n            elif id_ == \"real_heads_plus_heads\":\n                return self.heads + self._real_heads\n            elif id_ == \"heads_plus_real_heads\":\n                return self.heads + self._real_heads\n            elif id_ == \"real_heads_plus_real_base\":\n                return self.heads + (None,)\n            elif id_ == \"real_heads_plus_real_heads\":\n                return self.heads + self._real_heads\n            elif id_ == \"real_heads_plus_heads\":\n                return self.heads + self._real_heads\n            elif id_ == \"heads_plus_real_heads\":\n                return self.heads + self._real_heads\n            elif id_ == \"heads_plus_real_base\":\n                return self.heads +", "idx": 998}
{"namespace": "alembic.script.revision.RevisionMap.get_revision", "completion": "        if id_ is None:\n            return None\n        if id_ == \"head\":\n            return self.get_current_head()\n        if id_ == \"base\":\n            return self._revision_for_ident(self.bases[0])\n        if id_ == \"heads\":\n            return self.get_revisions(\"heads\")\n        if id_ == \"current\":\n            return self.get_current_head()\n        if id_ == \"branch\":\n            return self.get_current_head(branch_label=True)\n        if id_ == \"current@head\":\n            return self.get_current_head()\n        if id_ == \"current@base\":\n            return self._revision_for_ident(self.bases[0])\n        if id_ == \"current@heads\":\n            return self.get_current_head()\n        if id_ == \"current@branch\":\n            return self.get_current_head(branch_label=True)\n        if id_ == \"current@head\":\n            return self.get_current_head()\n        if id_ == \"current@base\":\n            return self._revision_for_ident(self.bases[0])\n        if id_ == \"current@heads\":\n            return self.get_current_head()\n        if id_ == \"current@branch\":\n            return self.get_current_head(branch_label=True)\n        if id_ == \"head@head\":\n            return self.get_current_head()\n        if id_ == \"head@base\":\n            return self._revision_for_ident(self.bases[0])\n        if id_ == \"head@heads\":\n            return self.get_current_head()\n        if id_ == \"head@branch\":\n            return self.get_current_head(branch_label=True)\n        if id_ == \"base@head\":\n            return self.get_current_head()\n        if id_ == \"base@base\":\n            return self._revision_for_ident(self.bases[0])\n        if id_ == \"base@heads\":\n            return self.get_current_head()\n        if id_ == \"base@branch\":\n            return self.get_current_head(branch_label=True)\n        if id_ == \"heads@head\":\n            return self.get_current_head()", "idx": 999}
{"namespace": "alembic.script.revision.RevisionMap.filter_for_lineage", "completion": "        if check_against:\n            check_against_rev = self._revision_for_ident(check_against)\n            if check_against_rev is None:\n                raise ResolutionError(\n                    \"No such revision or branch '%s'\" % check_against,\n                    check_against,\n                )\n            targets = self._get_descendant_nodes(\n                [check_against_rev],\n                include_dependencies=include_dependencies,\n            )\n        return tuple(targets)\n", "idx": 1000}
{"namespace": "alembic.script.revision.RevisionMap.iterate_revisions", "completion": "        upper_rev = self.get_revisions(upper)\n        lower_rev = self.get_revisions(lower)\n        if assert_relative_length:\n            assert len(upper_rev) == len(lower_rev)\n        if inclusive:\n            upper_rev = upper_rev + lower_rev\n        if implicit_base:\n            upper_rev = upper_rev + self.get_revisions(\"base\")\n        if select_for_downgrade:\n            return self._get_descendant_nodes(\n                upper_rev, include_dependencies=False\n            )\n        else:\n            return self._get_ancestor_nodes(\n                upper_rev, include_dependencies=False\n            )\n", "idx": 1001}
{"namespace": "alembic.script.revision.RevisionMap._topological_sort", "completion": "        # Create a dictionary to store the dependencies of each revision.\n        dependencies = {}\n        # Iterate through the revisions and store their dependencies in the dictionary.\n        for revision in revisions:\n            dependencies[revision.revision] = revision._normalized_resolved_dependencies\n        # Create a list to store the sorted revision ids.\n        sorted_revisions = []\n        # Create a set to store the revision ids that have been sorted.\n        seen = set()\n        # Create a queue to store the revision ids that need to be sorted.\n        todo = collections.deque()\n        # Iterate through the heads and add them to the queue.\n        for head in heads:\n            todo.append(head)\n        # Iterate through the queue until it is empty.\n        while todo:\n            # Pop the first revision id from the queue.\n            rev = todo.pop()\n            # If the revision id has already been sorted, skip it.\n            if rev in seen:\n                continue\n            # Add the revision id to the set of sorted revision ids.\n            seen.add(rev)\n            # Add the revision id to the list of sorted revision ids.\n            sorted_revisions.append(rev)\n            # Iterate through the dependencies of the revision id and add them to the queue.\n            for dependency in dependencies[rev]:\n                todo.append(dependency)\n        # Return the list of sorted revision ids.\n        return sorted_revisions\n", "idx": 1002}
{"namespace": "alembic.script.revision.Revision._all_down_revisions", "completion": "        return tuple(\n            set(\n                self.down_revision\n                + self._normalized_resolved_dependencies\n                + self._all_nextrev\n            )\n        )\n", "idx": 1003}
{"namespace": "alembic.script.revision.Revision._normalized_down_revisions", "completion": "        return util.dedupe_tuple(\n            self._all_down_revisions\n            + self._normalized_resolved_dependencies\n            + self._normalized_resolved_dependencies\n        )\n", "idx": 1004}
{"namespace": "alembic.script.write_hooks._invoke", "completion": "    formatter = _registry.get(name)\n    if not formatter:\n        raise util.command.CommandError(\n            \"No formatter with name '{name}' registered\".format(name=name)\n        )\n\n    return formatter(revision, options)\n\n", "idx": 1005}
{"namespace": "bplustree.memory.FileMemory.get_node", "completion": "        if page in self._cache:\n            return self._cache[page]\n\n        data = read_from_file(self._fd, page * self._tree_conf.page_size,\n                              (page + 1) * self._tree_conf.page_size)\n        node = Node.from_bytes(data, self._tree_conf)\n        self._cache[page] = node\n        return node\n", "idx": 1006}
{"namespace": "bplustree.memory.FileMemory.next_available_page", "completion": "        return self.last_page\n", "idx": 1007}
{"namespace": "bplustree.memory.FileMemory.get_metadata", "completion": "        # Read the first page of the file\n        data = self._read_page(0)\n\n        # Extract the root node page, page size, order, key size, and value size from the first page\n        root_node_page = int.from_bytes(data[0:PAGE_REFERENCE_BYTES], ENDIAN)\n        page_size = int.from_bytes(data[PAGE_REFERENCE_BYTES:PAGE_REFERENCE_BYTES + OTHERS_BYTES], ENDIAN)\n        order = int.from_bytes(data[PAGE_REFERENCE_BYTES + OTHERS_BYTES:PAGE_REFERENCE_BYTES + 2 * OTHERS_BYTES], ENDIAN)\n        key_size = int.from_bytes(data[PAGE_REFERENCE_BYTES + 2 * OTHERS_BYTES:PAGE_REFERENCE_BYTES + 3 * OTHERS_BYTES], ENDIAN)\n        value_size = int.from_bytes(data[PAGE_REFERENCE_BYTES + 3 * OTHERS_BYTES:PAGE_REFERENCE_BYTES + 4 * OTHERS_BYTES], ENDIAN)\n\n        # Create a TreeConf object with the extracted values\n        tree_conf = TreeConf(page_size=page_size, order=order, key_size=key_size, value_size=value_size)\n\n        # Return the root node page and the TreeConf object\n        return root_node_page, tree_conf\n", "idx": 1008}
{"namespace": "bplustree.memory.FileMemory.set_metadata", "completion": "        self._tree_conf = tree_conf\n        data = (\n            root_node_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN) +\n            tree_conf.page_size.to_bytes(OTHERS_BYTES, ENDIAN) +\n            tree_conf.order.to_bytes(OTHERS_BYTES, ENDIAN) +\n            tree_conf.key_size.to_bytes(OTHERS_BYTES, ENDIAN) +\n            tree_conf.value_size.to_bytes(OTHERS_BYTES, ENDIAN)\n        )\n        self._wal.set_page(0, data)\n", "idx": 1009}
{"namespace": "bplustree.memory.WAL.checkpoint", "completion": "        if self._not_committed_pages:\n            logger.warning('There are uncommitted pages in the WAL')\n\n        fsync_file_and_dir(self._fd.fileno(), self._dir_fd)\n\n        for page, data in self._committed_pages.items():\n            yield page, data\n\n        self._fd.close()\n        os.remove(self.filename)\n        if self._dir_fd is not None:\n            fsync_file_and_dir(None, self._dir_fd)\n", "idx": 1010}
{"namespace": "bplustree.memory.WAL.commit", "completion": "        if self._not_committed_pages:\n            self._add_frame(FrameType.COMMIT)\n", "idx": 1011}
{"namespace": "bplustree.memory.WAL.rollback", "completion": "        if self._not_committed_pages:\n            self._add_frame(FrameType.ROLLBACK)", "idx": 1012}
{"namespace": "bplustree.entry.Record.dump", "completion": "        key_bytes = self._tree_conf.serializer.serialize(self.key)\n        used_key_length = len(key_bytes)\n        assert 0 <= used_key_length <= self._tree_conf.key_size\n\n        if self.overflow_page:\n            value = None\n            overflow_page = self.overflow_page\n        else:\n            value = self.value\n            overflow_page = 0\n\n        assert value is None or len(value) <= self._tree_conf.value_size\n        used_value_length = len(value) if value else 0\n\n        return (\n            used_key_length.to_bytes(USED_KEY_LENGTH_BYTES, ENDIAN) +\n            key_bytes +\n            used_value_length.to_bytes(USED_VALUE_LENGTH_BYTES, ENDIAN) +\n            (value if value else b'') +\n            overflow_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN)\n        )\n\n", "idx": 1013}
{"namespace": "bplustree.entry.Reference.__repr__", "completion": "        return '<Reference: key={} before={} after={}>'.format(\n            self.key, self.before, self.after\n        )", "idx": 1014}
{"namespace": "bplustree.node.Node.dump", "completion": "        data = bytearray()\n        for entry in self.entries:\n            data.extend(entry.dump())\n\n        header = bytearray()\n        header.extend(self._node_type_int.to_bytes(NODE_TYPE_BYTES, ENDIAN))\n        header.extend(len(data).to_bytes(USED_PAGE_LENGTH_BYTES, ENDIAN))\n        header.extend(self.next_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN))\n        data.extend(header)\n\n        padding = self._tree_conf.page_size - len(data)\n        data.extend(b'\\x00' * padding)\n\n        return data\n", "idx": 1015}
{"namespace": "bplustree.node.Node._find_entry_index", "completion": "        entry = self._entry_class(self._tree_conf, key=key)\n        for i, e in enumerate(self.entries):\n            if e == entry:\n                return i\n        raise ValueError(f'No entry with key {key} in node {self.page}')\n", "idx": 1016}
{"namespace": "bplustree.node.Node.from_page_data", "completion": "        node_type = int.from_bytes(data[:1], ENDIAN)\n        if node_type == 0:\n            return LeafNode(tree_conf, data, page)\n        elif node_type == 1:\n            return InternalNode(tree_conf, data, page)\n        else:\n            raise ValueError('Unknown node type: {}'.format(node_type))\n\n", "idx": 1017}
{"namespace": "bplustree.tree.BPlusTree._root_node", "completion": "        return self._mem.get_node(self._root_node_page)\n", "idx": 1018}
{"namespace": "bplustree.tree.BPlusTree._left_record_node", "completion": "        node = self._root_node\n        while not isinstance(node, (LonelyRootNode, LeafNode)):\n            node = self._mem.get_node(node.children[0])\n        return node\n", "idx": 1019}
{"namespace": "mopidy.ext.Extension.get_cache_dir", "completion": "        cache_dir = config[\"core\"][\"cache_dir\"]\n        cache_dir = path.expand_path(cache_dir)\n        cache_dir = cache_dir / \"cache\" / cls.ext_name\n        cache_dir.mkdir(parents=True, exist_ok=True)\n        return cache_dir\n", "idx": 1020}
{"namespace": "mopidy.ext.Extension.get_config_dir", "completion": "        if cls.ext_name is None:\n            raise AssertionError\n        config_dir_path = (\n            path.expand_path(config[\"core\"][\"config_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(config_dir_path)\n        return config_dir_path\n", "idx": 1021}
{"namespace": "mopidy.ext.Extension.get_data_dir", "completion": "        if cls.ext_name is None:\n            raise AssertionError\n        data_dir_path = (\n            path.expand_path(config[\"core\"][\"data_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(data_dir_path)\n        return data_dir_path\n", "idx": 1022}
{"namespace": "mopidy.ext.load_extensions", "completion": "    installed_extensions: List[ExtensionData] = []\n\n    for entry_point in pkg_resources.iter_entry_points(\"mopidy.ext\"):\n        try:\n            extension_class = entry_point.load()\n        except Exception as e:\n            logger.warning(\n                \"Failed to load extension %s: %s\", entry_point.name, e\n            )\n            continue\n\n        if not issubclass(extension_class, Extension):\n            logger.warning(\n                \"Extension %s does not inherit from Extension\",\n                entry_point.name,\n            )\n            continue\n\n        try:\n            config_schema = extension_class.get_config_schema()\n        except Exception as e:\n            logger.warning(\n                \"Failed to get config schema for extension %s: %s\",\n                entry_point.name,\n                e,\n            )\n            continue\n\n        try:\n            config_defaults = config_schema.get_default_config()\n        except Exception as e:\n            logger.warning(\n                \"Failed to get default config for extension %s: %s\",\n                entry_point.name,\n                e,\n            )\n            continue\n\n        try:\n            command = extension_class.get_command()\n        except Exception as e:\n            logger.warning(\n                \"Failed to get command for extension %s: %s\",\n                entry_point.name,\n                e,\n            )\n            continue\n\n        extension = ExtensionData(\n            extension=extension_class,\n            entry_point=entry_point,\n            config_schema=config_schema,\n            config_defaults=config_defaults,\n            command=command,\n        )\n\n        installed_extensions.append(extension)\n\n    return installed_extensions", "idx": 1023}
{"namespace": "mopidy.ext.validate_extension_data", "completion": "    # Check if the extension's entry point name matches its extension name\n    if data.entry_point.name != data.extension.ext_name:\n        logger.error(\n            \"Extension entry point name %s does not match extension name %s\",\n            data.entry_point.name,\n            data.extension.ext_name,\n        )\n        return False\n\n    # Check if the extension's required dependencies are installed\n    try:\n        data.entry_point.require()\n    except pkg_resources.DistributionNotFound as e:\n        logger.error(\n            \"Extension %s requires dependency %s, which is not installed\",\n            data.extension.ext_name,\n            e.req,\n        )\n        return False\n\n    # Check if the extension's environment is valid\n    try:\n        data.extension.validate_environment()\n    except Exception as e:\n        logger.error(\n            \"Extension %s has an invalid environment: %s\",\n            data.extension.ext_name,\n            e,\n        )\n        return False\n\n    # Check if the extension has a valid config schema and default config\n    try:\n        data.config_schema.validate(data.config_defaults)\n    except Exception as e:\n        logger.error(\n            \"Extension %s has an invalid config schema or default config: %s\",\n            data.extension.ext_name,\n            e,\n        )\n        return False\n\n    return True", "idx": 1024}
{"namespace": "mopidy.httpclient.format_user_agent", "completion": "    if name is None:\n        name = \"Mopidy\"\n\n    version = platform.python_version()\n    return f\"{name}/{version}\"", "idx": 1025}
{"namespace": "mopidy.models.immutable.ValidatedImmutableObject.replace", "completion": "        other = copy.copy(self)\n        for key, value in kwargs.items():\n            if not self._is_valid_field(key):\n                raise TypeError(\n                    f\"replace() got an unexpected keyword argument {key!r}\"\n                )\n            other._set_field(key, value)\n        return other\n", "idx": 1026}
{"namespace": "mopidy.http.Extension.get_default_config", "completion": "        conf_path = os.path.join(os.path.dirname(__file__), \"ext.conf\")\n        return config_lib.read(conf_path)\n", "idx": 1027}
{"namespace": "mopidy.http.Extension.get_config_schema", "completion": "        schema = super().get_config_schema()\n        schema[\"hostname\"] = config_lib.String()\n        schema[\"port\"] = config_lib.Port()\n        schema[\"static_dir\"] = config_lib.String()\n        schema[\"zeroconf\"] = config_lib.String()\n        schema[\"allowed_origins\"] = config_lib.String()\n        schema[\"csrf_protection\"] = config_lib.Boolean()\n        schema[\"default_app\"] = config_lib.String()\n        schema[\"default_app_path\"] = config_lib.String()\n        schema[\"default_app_args\"] = config_lib.String()\n        schema[\"default_app_kwargs\"] = config_lib.String()\n        schema[\"default_app_factory\"] = config_lib.String()\n        schema[\"default_app_factory_args\"] = config_lib.String()\n        schema[\"default_app_factory_kwargs\"] = config_lib.String()\n        schema[\"default_app_factory_name\"] = config_lib.String()\n        schema[\"default_app_factory_name_args\"] = config_lib.String()\n        schema[\"default_app_factory_name_kwargs\"] = config_lib.String()\n        schema[\"default_app_factory_name_class\"] = config_lib.String()\n        schema[\"default_app_factory_name_class_args\"] = config_lib.String()\n        schema[\"default_app_factory_name_class_kwargs\"] = config_lib.String()\n        schema[\"default_app_factory_name_class_name\"] = config_lib.String()\n        schema[\"default_app_factory_name_class_name_args\"] = config_lib.String()\n        schema[\"default_app_factory_name_class_name_kwargs\"] = config_lib.String()\n        schema[\"default_app_factory_name_class_name_class\"] = config_lib.String()\n        schema[\"default_app_factory_name_class_name_class_args\"] = config_lib.String()\n        schema[\"default_app_factory_name_class_name_class_kwargs\"] = config_lib.String()\n        schema[\"default_app_factory_name_class_name_class_name\"] = config_lib.String()\n       ", "idx": 1028}
{"namespace": "mopidy.internal.network.try_ipv6_socket", "completion": "    try:\n        socket.socket(socket.AF_INET6)\n        return True\n    except socket.error:\n        logger.debug(\"IPv6 is not supported\")\n        return False\n\n", "idx": 1029}
{"namespace": "mopidy.internal.network.format_hostname", "completion": "    if re.match(r\"^([0-9]{1,3}\\.){3}[0-9]{1,3}$\", hostname):\n        return \"::ffff:\" + hostname\n    return hostname", "idx": 1030}
{"namespace": "mopidy.internal.xdg.get_dirs", "completion": "    dirs = {}\n    for name in [\n        \"XDG_CACHE_HOME\",\n        \"XDG_CONFIG_HOME\",\n        \"XDG_DATA_HOME\",\n        \"XDG_DESKTOP_DIR\",\n        \"XDG_DOWNLOAD_DIR\",\n        \"XDG_DOCUMENTS_DIR\",\n        \"XDG_MUSIC_DIR\",\n        \"XDG_PICTURES_DIR\",\n        \"XDG_PUBLICSHARE_DIR\",\n        \"XDG_TEMPLATES_DIR\",\n        \"XDG_VIDEOS_DIR\",\n    ]:\n        path = os.getenv(name)\n        if path is not None:\n            dirs[name] = pathlib.Path(path).expanduser()\n\n    # If the user-dirs.dirs file exists, parse it and update the dictionary with the additional directories.\n    user_dirs_file = pathlib.Path.home() / \".config\" / \"user-dirs.dirs\"\n    if user_dirs_file.exists():\n        config = configparser.ConfigParser()\n        config.read(user_dirs_file)\n        for name in [\n            \"XDG_DESKTOP_DIR\",\n            \"XDG_DOWNLOAD_DIR\",\n            \"XDG_DOCUMENTS_DIR\",\n            \"XDG_MUSIC_DIR\",\n            \"XDG_PICTURES_DIR\",\n            \"XDG_PUBLICSHARE_DIR\",\n            \"XDG_TEMPLATES_DIR\",\n            \"XDG_VIDEOS_DIR\",\n        ]:\n            path = config.get(\"User Directories\", name)\n            if path is not None:\n                dirs[name] = pathlib.Path(path).expanduser()\n\n    return dirs\n\n", "idx": 1031}
{"namespace": "mopidy.internal.log.get_verbosity_level", "completion": "    verbosity_level = base_verbosity_level\n    if args_verbosity_level is not None:\n        verbosity_level += args_verbosity_level\n    else:\n        verbosity_level += logging_config[\"verbosity\"]\n\n    if verbosity_level < min(LOG_LEVELS):\n        verbosity_level = min(LOG_LEVELS)\n    elif verbosity_level > max(LOG_LEVELS):\n        verbosity_level = max(LOG_LEVELS)\n\n    return verbosity_level\n\n", "idx": 1032}
{"namespace": "mopidy.internal.validation.check_instance", "completion": "    if not isinstance(arg, cls):\n        raise exceptions.ValidationError(msg.format(arg=arg, name=cls.__name__))\n\n", "idx": 1033}
{"namespace": "mopidy.internal.validation.check_instances", "completion": "    if not isinstance(arg, Iterable):\n        raise exceptions.ValidationError(msg.format(arg=arg, name=cls.__name__))\n    for a in arg:\n        if not isinstance(a, cls):\n            raise exceptions.ValidationError(msg.format(arg=arg, name=cls.__name__))\n\n", "idx": 1034}
{"namespace": "mopidy.internal.validation.check_uri", "completion": "    if not isinstance(arg, str):\n        raise exceptions.ValidationError(msg.format(arg=arg))\n    elif not urllib.parse.urlparse(arg).scheme:\n        raise exceptions.ValidationError(msg.format(arg=arg))\n\n", "idx": 1035}
{"namespace": "mopidy.internal.validation.check_uris", "completion": "    _check_iterable(arg, msg)\n    for uri in arg:\n        check_uri(uri)\n\n", "idx": 1036}
{"namespace": "mopidy.internal.playlists.parse", "completion": "    config = configparser.ConfigParser()\n    config.read('mopidy.conf')\n    config = dict(config)\n    config = dict(config['audio'])\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config = dict(config)\n    config =", "idx": 1037}
{"namespace": "mopidy.config.schemas.ConfigSchema.deserialize", "completion": "        result = {}\n        errors = {}\n        for key, schema in self.items():\n            if key not in values:\n                if schema.required:\n                    errors[key] = 'missing'\n                else:\n                    result[key] = schema.default\n                continue\n\n            try:\n                result[key] = schema.deserialize(values[key])\n            except ValueError as e:\n                errors[key] = str(e)\n                result[key] = None\n\n        for key in result:\n            if key in self and self[key].deprecated:\n                del result[key]\n\n        return result, errors\n", "idx": 1038}
{"namespace": "mopidy.config.types.String.deserialize", "completion": "        value = super().deserialize(value)\n        value = value.strip()\n        if self._required and not value:\n            raise ValueError(\"Value is required\")\n        if self._transformer:\n            value = self._transformer(value)\n        if self._choices and value not in self._choices:\n            raise ValueError(f\"Value must be one of {self._choices}\")\n        return value\n", "idx": 1039}
{"namespace": "mopidy.config.types.String.serialize", "completion": "        if value is None:\n            return \"\"\n        if isinstance(value, _TransformedValue):\n            value = value.original\n        return encode(value)\n\n", "idx": 1040}
{"namespace": "mopidy.config.types.Secret.serialize", "completion": "        if value is not None and display:\n            return \"********\"\n        return super().serialize(value, display)\n\n", "idx": 1041}
{"namespace": "mopidy.config.types.Integer.deserialize", "completion": "        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        try:\n            value = int(value)\n        except ValueError:\n            raise ValueError(f\"Invalid integer: {value}\")\n\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        validators.validate_choice(value, self._choices)\n        return value\n", "idx": 1042}
{"namespace": "mopidy.config.types.Float.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = float(value)\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        return value\n\n", "idx": 1043}
{"namespace": "mopidy.config.types.Boolean.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = value.lower()\n        if value in self.true_values:\n            return True\n        if value in self.false_values:\n            return False\n        raise ValueError(f\"Invalid boolean value: {value}\")\n\n", "idx": 1044}
{"namespace": "mopidy.config.types.Pair.deserialize", "completion": "        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        if self._separator in value:\n            parts = value.split(self._separator)\n            if self._optional_pair:\n                parts = [parts[0]] * 2\n            else:\n                if len(parts) != 2:\n                    raise ValueError(\n                        f\"invalid value for pair: {value!r} (expected two \"\n                        f\"values separated by {self._separator!r})\"\n                    )\n        else:\n            parts = [value] * 2\n        return tuple(\n            self._subtypes[i].deserialize(parts[i]) for i in range(len(parts))\n        )\n", "idx": 1045}
{"namespace": "mopidy.config.types.Pair.serialize", "completion": "        if value is None:\n            return \"\"\n        if isinstance(value, _TransformedValue):\n            value = value.original\n        if not isinstance(value, tuple):\n            raise ValueError(f\"Config value must be a tuple: {value}\")\n        if len(value) != 2:\n            raise ValueError(f\"Config value must be a pair: {value}\")\n\n        serialized_values = (\n            self._subtypes[0].serialize(value[0], display),\n            self._subtypes[1].serialize(value[1], display),\n        )\n\n        if not display and self._optional_pair and serialized_values[0] == serialized_values[1]:\n            return serialized_values[0]\n\n        return self._separator.join(serialized_values)\n\n", "idx": 1046}
{"namespace": "mopidy.config.types.List.serialize", "completion": "        if value is None:\n            return \"\"\n        return \"\\n\".join(\n            self._subtype.serialize(v, display=display) for v in value\n        )\n\n", "idx": 1047}
{"namespace": "mopidy.config.types.LogColor.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, True)\n        if not value:\n            return None\n        value = value.lower()\n        validators.validate_choice(value, log.LOG_COLORS)\n        return value\n\n", "idx": 1048}
{"namespace": "mopidy.config.types.LogColor.serialize", "completion": "        if value is None:\n            return \"\"\n        if value.lower() in log.COLORS:\n            return value.lower()\n        else:\n            return \"\"\n\n", "idx": 1049}
{"namespace": "mopidy.config.types.LogLevel.deserialize", "completion": "        value = decode(value)\n        validators.validate_choice(value.lower(), self.levels)\n        return self.levels[value.lower()]\n", "idx": 1050}
{"namespace": "mopidy.config.types.LogLevel.serialize", "completion": "        for key, val in self.levels.items():\n            if val == value:\n                return key\n        return \"\"\n\n", "idx": 1051}
{"namespace": "mopidy.config.types.Hostname.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        if value.startswith(\"unix://\"):\n            return value[7:]\n        try:\n            socket.gethostbyname(value)\n            return value\n        except socket.gaierror:\n            raise ValueError(f\"invalid hostname: {value!r}\")\n", "idx": 1052}
{"namespace": "mopidy.config.load", "completion": "    # TODO: This function is too long and needs to be refactored.\n\n    # TODO: This function is too long and needs to be refactored.\n\n    # TODO: This function is too long and needs to be refactored.\n\n    # TODO: This function is too long and needs to be refactored.\n\n    # TODO: This function is too long and needs to be refactored.\n\n    # TODO: This function is too long and needs to be refactored.\n\n    # TODO: This function is too long and needs to be refactored.\n\n    # TODO: This function is too long and needs to be refactored.\n\n    # TODO: This function is too long and needs to be refactored.\n\n    # TODO: This function is too long and needs to be refactored.\n\n    # TODO: This function is too long and needs to be refactored.\n\n    # TODO: This function is too long and needs to be refactored.\n\n    # TODO: This function is too long and needs to be refactored.\n\n    # TODO: This function is too long and needs to be refactored.\n\n    # TODO: This function is too long and needs to be refactored.\n\n    # TODO: This function is too long and needs to be refactored.\n\n    # TODO: This function is too long and needs to be refactored.\n\n    # TODO: This function is too long and needs to be refactored.\n\n    # TODO: This function is too long and needs to be refactored.\n\n    # TODO: This function is too long and needs to be refactored.\n\n    # TODO: This function is too long and needs to be refactored.\n\n    # TODO: This function is too long and needs to be refactored.\n\n    # TODO: This function is too long and needs to be refactored.\n\n    # TODO: This function is too long and needs to be refactored.\n\n    # TODO: This function is too long and needs to be refactored.\n\n    # TODO: This function is too long and needs to be refactored.\n\n    # TODO: This function is too long and needs to be refactored.\n\n    # TODO: This function is too long and needs to be refactored.\n\n    # TODO: This function is too long and needs to be refactored.\n\n    # TODO: This function is too long and needs to be refactored.\n\n    # TODO: This function is too long and needs to be refactored.\n\n    # TODO:", "idx": 1053}
{"namespace": "mopidy.config.format_initial", "completion": "    config_dir = pathlib.Path(__file__).parent\n    defaults = [read(config_dir / \"default.conf\")]\n    schemas = _schemas[:]\n    for extension_data in extensions_data:\n        defaults.append(extension_data.get_default_config())\n        schemas.extend(extension_data.get_config_schema())\n    raw_config = _load(None, defaults, [])\n    _validate(raw_config, schemas)\n    comments = {}\n    for extension_data in extensions_data:\n        comments[extension_data.get_name()] = extension_data.get_version()\n    return _format(raw_config, comments, schemas, True, True)\n\n", "idx": 1054}
{"namespace": "mopidy.config._load", "completion": "    config = configparser.RawConfigParser(inline_comment_prefixes=(\";\",))\n    config.optionxform = str\n\n    for default in defaults:\n        config.read_string(default)\n\n    for file in files:\n        if os.path.isdir(file):\n            for filename in os.listdir(file):\n                if filename.endswith(\".conf\"):\n                    config.read(os.path.join(file, filename))\n        else:\n            config.read(file)\n\n    raw_config = {}\n    for section in config.sections():\n        raw_config[section] = dict(config.items(section))\n\n    for section, key, value in overrides:\n        if section not in raw_config:\n            raw_config[section] = {}\n        raw_config[section][key] = value\n\n    return raw_config\n\n", "idx": 1055}
{"namespace": "mopidy.config._validate", "completion": "    config = {}\n    errors = {}\n    for schema in schemas:\n        section = schema.section\n        if section not in raw_config:\n            continue\n\n        try:\n            config[section] = schema.deserialize(raw_config[section])\n        except ValueError as e:\n            errors[section] = e\n\n    # Check for unknown sections\n    for section in raw_config:\n        if section not in config:\n            logger.warning(f\"Unknown section {section!r} in config file\")\n\n    return config, errors\n\n", "idx": 1056}
{"namespace": "mingus.extra.tunings.get_tunings", "completion": "    result = []\n    for (x, y) in six.iteritems(_known):\n        for (desc, tun) in six.iteritems(y[1]):\n            if instrument is None or x.find(str.upper(instrument)) == 0:\n                if nr_of_strings is None and nr_of_courses is None:\n                    result.append(tun)\n                elif nr_of_strings is not None and nr_of_courses is None:\n                    if tun.count_strings() == nr_of_strings:\n                        result.append(tun)\n                elif nr_of_strings is None and nr_of_courses is not None:\n                    if tun.count_courses() == nr_of_courses:\n                        result.append(tun)\n                else:\n                    if (\n                        tun.count_courses() == nr_of_courses\n                        and tun.count_strings() == nr_of_strings\n                    ):\n                        result.append(tun)\n    return result\n\n", "idx": 1057}
{"namespace": "mingus.containers.instrument.Instrument.note_in_range", "completion": "        if isinstance(note, six.string_types):\n            note = Note(note)\n        if not hasattr(note, \"name\"):\n            raise UnexpectedObjectError(\n                \"Unexpected object '%s'. \"\n                \"Expecting a mingus.containers.Note object\" % note\n            )\n        if note < self.range[0] or note > self.range[1]:\n            return False\n        return True\n", "idx": 1058}
{"namespace": "mingus.containers.instrument.Guitar.can_play_notes", "completion": "        if len(notes) > 6:\n            return False\n        return Instrument.can_play_notes(self, notes)", "idx": 1059}
{"namespace": "mingus.containers.bar.Bar.get_range", "completion": "        # Initialize the highest and lowest notes to the first note in the bar.\n        highest_note = self.bar[0][2].highest_note\n        lowest_note = self.bar[0][2].lowest_note\n\n        # Iterate through the bar and update the highest and lowest notes.\n        for note_container in self.bar:\n            if note_container[2].highest_note > highest_note:\n                highest_note = note_container[2].highest_note\n            if note_container[2].lowest_note < lowest_note:\n                lowest_note = note_container[2].lowest_note\n\n        # Return the highest and lowest notes.\n        return highest_note, lowest_note\n", "idx": 1060}
{"namespace": "mingus.containers.bar.Bar.transpose", "completion": "        for cont in self.bar:\n            cont[2].transpose(interval, up)\n", "idx": 1061}
{"namespace": "mingus.containers.bar.Bar.determine_chords", "completion": "        chords = []\n        for i in range(len(self.bar)):\n            chords.append([self.bar[i][0], self.bar[i][2].determine_chords(shorthand)])\n        return chords\n", "idx": 1062}
{"namespace": "mingus.containers.note.Note.transpose", "completion": "        if up:\n            self.name = notes.transpose(self.name, interval)\n        else:\n            self.name = notes.transpose(self.name, \"-\" + interval)\n", "idx": 1063}
{"namespace": "mingus.containers.note.Note.from_int", "completion": "        if not isinstance(integer, int):\n            raise TypeError(\"Integer value must be an integer\")\n\n        if integer < 0:\n            raise ValueError(\"Integer value must be greater than or equal to 0\")\n\n        if integer > 127:\n            raise ValueError(\"Integer value must be less than or equal to 127\")\n\n        self.name = notes.from_int(integer)\n        self.octave = int(log(integer + 1, 12))\n", "idx": 1064}
{"namespace": "mingus.containers.note.Note.to_hertz", "completion": "        # Convert the note to a number\n        note_number = notes.note_to_int(self.name)\n\n        # Calculate the frequency\n        frequency = standard_pitch * (2 ** ((note_number - 57) / 12))\n\n        return frequency\n", "idx": 1065}
{"namespace": "mingus.containers.note.Note.from_hertz", "completion": "        # int(Note(\"A\")) == 57\n        diff = log(hertz / standard_pitch, 2) * 12.0\n        self.from_int(int(diff) + 57)\n", "idx": 1066}
{"namespace": "mingus.containers.note.Note.to_shorthand", "completion": "        return self.name + str(self.octave)\n", "idx": 1067}
{"namespace": "mingus.containers.note_container.NoteContainer.from_chord_shorthand", "completion": "        self.empty()\n        self.add_notes(chords.from_shorthand_notation(shorthand))\n        return self\n", "idx": 1068}
{"namespace": "mingus.containers.note_container.NoteContainer.from_interval_shorthand", "completion": "        self.empty()\n        if isinstance(startnote, six.string_types):\n            startnote = Note(startnote)\n        self.add_note(startnote)\n        self.add_note(intervals.transpose(startnote, shorthand, up))\n        return self\n", "idx": 1069}
{"namespace": "mingus.containers.note_container.NoteContainer.from_progression_shorthand", "completion": "        self.empty()\n        self.add_notes(chords.from_progression_shorthand(shorthand, key))\n        return self\n", "idx": 1070}
{"namespace": "mingus.containers.note_container.NoteContainer.transpose", "completion": "        for n in self.notes:\n            n.transpose(interval, up)\n        return self\n", "idx": 1071}
{"namespace": "mingus.containers.note_container.NoteContainer.get_note_names", "completion": "        return list(set([n.name for n in self.notes]))\n", "idx": 1072}
{"namespace": "mingus.core.notes.int_to_note", "completion": "    if note_int < 0 or note_int > 11:\n        raise NoteFormatError(\"Note integer out of range.\")\n\n    if note_int == 0:\n        return \"C\"\n    elif note_int == 1:\n        return \"C\" + accidentals\n    elif note_int == 2:\n        return \"D\"\n    elif note_int == 3:\n        return \"D\" + accidentals\n    elif note_int == 4:\n        return \"E\"\n    elif note_int == 5:\n        return \"F\"\n    elif note_int == 6:\n        return \"F\" + accidentals\n    elif note_int == 7:\n        return \"G\"\n    elif note_int == 8:\n        return \"G\" + accidentals\n    elif note_int == 9:\n        return \"A\"\n    elif note_int == 10:\n        return \"A\" + accidentals\n    elif note_int == 11:\n        return \"B\"\n\n", "idx": 1073}
{"namespace": "mingus.core.notes.is_valid_note", "completion": "    if len(note) == 1:\n        return note in _note_dict\n    elif len(note) == 2:\n        return note[0] in _note_dict and note[1] in [\"#\", \"b\"]\n    else:\n        return False\n\n", "idx": 1074}
{"namespace": "mingus.core.notes.reduce_accidentals", "completion": "    # Check if the note is valid\n    if not is_valid_note(note):\n        raise NoteFormatError(\"Unknown note format '%s'\" % note)\n\n    # Check if the note has extra accidentals\n    if len(note) > 1:\n        # Check if the note has more than one accidental\n        if note.count(\"#\") > 1 or note.count(\"b\") > 1:\n            # Check if the note has more than one sharp\n            if note.count(\"#\") > 1:\n                # Check if the note has more than one flat\n                if note.count(\"b\") > 1:\n                    # Check if the note has more sharps than flats\n                    if note.count(\"#\") > note.count(\"b\"):\n                        # Remove the extra sharps\n                        note = note.replace(\"#\", \"\", note.count(\"#\") - 1)\n                    # Check if the note has more flats than sharps\n                    elif note.count(\"#\") < note.count(\"b\"):\n                        # Remove the extra flats\n                        note = note.replace(\"b\", \"\", note.count(\"b\") - 1)\n                    # Check if the note has the same number of sharps and flats\n                    else:\n                        # Remove the extra sharps\n                        note = note.replace(\"#\", \"\", note.count(\"#\") - 1)\n                # Check if the note has only sharps\n                else:\n                    # Remove the extra sharps\n                    note = note.replace(\"#\", \"\", note.count(\"#\") - 1)\n            # Check if the note has more than one flat\n            elif note.count(\"b\") > 1:\n                # Remove the extra flats\n                note = note.replace(\"b\", \"\", note.count(\"b\") - 1)\n\n    # Return the reduced note\n    return note\n\n", "idx": 1075}
{"namespace": "mingus.core.notes.remove_redundant_accidentals", "completion": "    # Get the note's integer value\n    note_int = note_to_int(note)\n\n    # Get the note's name\n    note_name = int_to_note(note_int)\n\n    # Return the note's name\n    return note_name\n\n", "idx": 1076}
{"namespace": "mingus.core.intervals.minor_second", "completion": "    return notes.diminish(second(note, note))\n\n", "idx": 1077}
{"namespace": "mingus.core.intervals.major_second", "completion": "    sec = second(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sec, 2)\n\n", "idx": 1078}
{"namespace": "mingus.core.intervals.minor_third", "completion": "    thi = third(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, thi, 3)\n\n", "idx": 1079}
{"namespace": "mingus.core.intervals.minor_fourth", "completion": "    fth = fourth(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, fth, 5)\n\n", "idx": 1080}
{"namespace": "mingus.core.intervals.minor_seventh", "completion": "    smt = seventh(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, smt, 10)\n\n", "idx": 1081}
{"namespace": "mingus.core.intervals.major_seventh", "completion": "    sth = seventh(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sth, 11)\n\n", "idx": 1082}
{"namespace": "mingus.core.intervals.measure", "completion": "    if not notes.is_valid_note(note1) or not notes.is_valid_note(note2):\n        raise KeyError(\"The notes '%s' and '%s' are not valid notes\" % (note1, note2))\n\n    note1_int = notes.note_to_int(note1)\n    note2_int = notes.note_to_int(note2)\n\n    return (note2_int - note1_int) % 12\n\n", "idx": 1083}
{"namespace": "mingus.core.intervals.determine", "completion": "    # Get the interval between the two notes\n    interval = measure(note1, note2)\n\n    # Determine the name of the interval\n    if interval == 0:\n        if shorthand:\n            return \"un\"\n        else:\n            return \"unison\"\n    elif interval == 1:\n        if shorthand:\n            return \"2\"\n        else:\n            return \"second\"\n    elif interval == 2:\n        if shorthand:\n            return \"3\"\n        else:\n            return \"third\"\n    elif interval == 3:\n        if shorthand:\n            return \"4\"\n        else:\n            return \"fourth\"\n    elif interval == 4:\n        if shorthand:\n            return \"5\"\n        else:\n            return \"fifth\"\n    elif interval == 5:\n        if shorthand:\n            return \"6\"\n        else:\n            return \"sixth\"\n    elif interval == 6:\n        if shorthand:\n            return \"7\"\n        else:\n            return \"seventh\"\n    elif interval == 7:\n        if shorthand:\n            return \"8\"\n        else:\n            return \"octave\"\n    elif interval == 8:\n        if shorthand:\n            return \"9\"\n        else:\n            return \"ninth\"\n    elif interval == 9:\n        if shorthand:\n            return \"10\"\n        else:\n            return \"tenth\"\n    elif interval == 10:\n        if shorthand:\n            return \"11\"\n        else:\n            return \"eleventh\"\n    elif interval == 11:\n        if shorthand:\n            return \"12\"\n        else:\n            return \"twelfth\"\n    else:\n        raise ValueError(\"Invalid interval\")\n\n", "idx": 1084}
{"namespace": "mingus.core.intervals.from_shorthand", "completion": "    # Check if the input is valid\n    if not isinstance(note, str) or not isinstance(interval, str):\n        return False\n\n    # Check if the note is valid\n    if not notes.is_valid_note(note):\n        return False\n\n    # Check if the interval is valid\n    if not interval.isdigit() or not 1 <= int(interval) <= 7:\n        return False\n\n    # Check if the up parameter is valid\n    if not isinstance(up, bool):\n        return False\n\n    # Get the index of the note in the fifths list\n    note_index = notes.fifths.index(note[0])\n\n    # Get the number of fifths to move\n    num_fifths = int(interval)\n\n    # Move up or down depending on the up parameter\n    if up:\n        new_index = (note_index + num_fifths) % len(notes.fifths)\n    else:\n        new_index = (note_index - num_fifths) % len(notes.fifths)\n\n    # Get the new note\n    new_note = notes.fifths[new_index]\n\n    # Add the accidentals to the new note\n    for accidental in note[1:]:\n        if accidental == \"#\":\n            new_note = notes.augment(new_note)\n        elif accidental == \"b\":\n            new_note = notes.diminish(new_note)\n\n    return new_note\n\n", "idx": 1085}
{"namespace": "mingus.core.intervals.is_consonant", "completion": "    # Corner case for unisons ('A' and 'Ab', for instance)\n    if note1[0] == note2[0]:\n        return True\n\n    # Other intervals\n    n1 = notes.fifths.index(note1[0])\n    n2 = notes.fifths.index(note2[0])\n    number_of_fifth_steps = n2 - n1\n    if n2 < n1:\n        number_of_fifth_steps = len(notes.fifths) - n1 + n2\n\n    # [name, shorthand_name, half notes for major version of this interval]\n    fifth_steps = [\n        [\"unison\", \"1\", 0],\n        [\"fifth\", \"5\", 7],\n        [\"second\", \"2\", 2],\n        [\"sixth\", \"6\", 9],\n        [\"third\", \"3\", 4],\n        [\"seventh\", \"7\", 11],\n        [\"fourth\", \"4\", 5],\n    ]\n\n    # Get the proper list from the number of fifth steps\n    current = fifth_steps[number_of_fifth_steps]\n\n    # maj = number of major steps for this interval\n    maj = current[2]\n\n    # if maj is equal to the half steps between note1 and note2 the interval is\n    # major or perfect\n    if maj == measure(note1, note2):\n        return True\n    else:\n        return False\n\n", "idx": 1086}
{"namespace": "mingus.core.intervals.is_perfect_consonant", "completion": "    # Corner case for unisons ('A' and 'Ab', for instance)\n    if note1[0] == note2[0]:\n        return True\n\n    # Other intervals\n    n1 = notes.fifths.index(note1[0])\n    n2 = notes.fifths.index(note2[0])\n    number_of_fifth_steps = n2 - n1\n    if n2 < n1:\n        number_of_fifth_steps = len(notes.fifths) - n1 + n2\n\n    # [name, shorthand_name, half notes for major version of this interval]\n    fifth_steps = [\n        [\"unison\", \"1\", 0],\n        [\"fifth\", \"5\", 7],\n        [\"second\", \"2\", 2],\n        [\"sixth\", \"6\", 9],\n        [\"third\", \"3\", 4],\n        [\"seventh\", \"7\", 11],\n        [\"fourth\", \"4\", 5],\n    ]\n\n    # Count half steps between note1 and note2\n    current = fifth_steps[number_of_fifth_steps]\n\n    # maj = number of major steps for this interval\n    maj = current[2]\n\n    # If the interval is a perfect consonance, return True.\n    if maj == measure(note1, note2):\n        return True\n    else:\n        return False\n\n", "idx": 1087}
{"namespace": "mingus.core.keys.get_key", "completion": "    if accidentals not in _key_cache:\n        _key_cache[accidentals] = keys[accidentals]\n    return _key_cache[accidentals]\n\n", "idx": 1088}
{"namespace": "mingus.core.keys.get_key_signature", "completion": "    from mingus.core.notes import note_to_int\n    from mingus.core.mt_exceptions import NoteFormatError\n    try:\n        key_int = note_to_int(key)\n    except NoteFormatError:\n        raise NoteFormatError(\"Invalid key format.\")\n    return key_int - 9\n\n", "idx": 1089}
{"namespace": "mingus.core.keys.get_key_signature_accidentals", "completion": "    accidentals = get_key_signature(key)\n    if accidentals == 0:\n        return []\n    elif accidentals > 0:\n        return [\"#\" for _ in range(accidentals)]\n    else:\n        return [\"b\" for _ in range(abs(accidentals))]\n\n", "idx": 1090}
{"namespace": "mingus.core.keys.get_notes", "completion": "    from mingus.core import notes\n    from mingus.core.mt_exceptions import NoteFormatError\n\n    if not is_valid_key(key):\n        raise NoteFormatError(\"unrecognized format for key '%s'\" % key)\n\n    if key in major_keys:\n        return list(islice(cycle(base_scale), base_scale.index(key), None))\n    else:\n        return list(islice(cycle(base_scale), base_scale.index(key) + 1, None))\n\n", "idx": 1091}
{"namespace": "mingus.core.keys.relative_major", "completion": "    if key in minor_keys:\n        return major_keys[minor_keys.index(key)]\n    else:\n        raise NoteFormatError(\"Unrecognized format for key '%s'\" % key)\n\n", "idx": 1092}
{"namespace": "mingus.core.chords.augmented_triad", "completion": "    return [note, intervals.major_third(note), intervals.augmented_fifth(note)]\n\n", "idx": 1093}
{"namespace": "mingus.core.chords.determine", "completion": "    # Determine chord name\n    if len(chord) == 3:\n        if shorthand:\n            return \"m\"\n        else:\n            return \"minor\"\n    elif len(chord) == 4:\n        if shorthand:\n            return \"\"\n        else:\n            return \"dominant\"\n    elif len(chord) == 5:\n        if shorthand:\n            return \"7\"\n        else:\n            return \"seventh\"\n    elif len(chord) == 6:\n        if shorthand:\n            return \"6\"\n        else:\n            return \"sixth\"\n    elif len(chord) == 7:\n        if shorthand:\n            return \"9\"\n        else:\n            return \"ninth\"\n    elif len(chord) == 8:\n        if shorthand:\n            return \"11\"\n        else:\n            return \"eleventh\"\n    elif len(chord) == 9:\n        if shorthand:\n            return \"13\"\n        else:\n            return \"thirteenth\"\n    elif len(chord) == 10:\n        if shorthand:\n            return \"5\"\n        else:\n            return \"polychord\"\n    elif len(chord) == 11:\n        if shorthand:\n            return \"NC\"\n        else:\n            return \"hendrix\"\n    else:\n        raise FormatError(\"Unknown chord: %s\" % chord)\n\n    # Determine inversions\n    if no_inversions:\n        return chord\n    elif len(chord) == 3:\n        if chord[0] == invert(chord)[0]:\n            return invert(chord)\n        else:\n            return chord\n    elif len(chord) == 4:\n        if chord[0] == invert(invert(chord))[0]:\n            return invert(invert(chord))\n        elif chord[0] == invert(chord)[0]:\n            return invert(chord)\n        else:\n            return chord\n    elif len(chord) == 5:\n        if chord[0] == invert(invert(invert(chord)))[0]:\n            return invert(invert(invert(chord)))\n        elif chord[0] == invert(invert(chord))[0]:\n            return invert(invert(chord))\n        elif chord[0] ==", "idx": 1094}
{"namespace": "mingus.core.value.septuplet", "completion": "    if in_fourths:\n        return tuplet(value, 7, 4)\n    else:\n        return tuplet(value, 7, 8)\n\n", "idx": 1095}
{"namespace": "mingus.core.value.determine", "completion": "    # Check if the value is a base value\n    for i in range(len(base_values)):\n        if base_values[i] == value:\n            return (base_values[i], 0, 1)\n\n    # Check if the value is a triplet\n    for i in range(len(base_triplets)):\n        if base_triplets[i] == value:\n            return (base_values[i], 0, 3)\n\n    # Check if the value is a quintuplet\n    for i in range(len(base_quintuplets)):\n        if base_quintuplets[i] == value:\n            return (base_values[i], 0, 5)\n\n    # Check if the value is a septuplet\n    for i in range(len(base_septuplets)):\n        if base_septuplets[i] == value:\n            return (base_values[i], 0, 7)\n\n    # Check if the value is a dotted base value\n    for i in range(len(base_values)):\n        if dots(base_values[i]) == value:\n            return (base_values[i], 1, 1)\n\n    # Check if the value is a triplet with one dot\n    for i in range(len(base_triplets)):\n        if dots(base_triplets[i]) == value:\n            return (base_values[i], 1, 3)\n\n    # Check if the value is a quintuplet with one dot\n    for i in range(len(base_quintuplets)):\n        if dots(base_quintuplets[i]) == value:\n            return (base_values[i], 1, 5)\n\n    # Check if the value is a septuplet with one dot\n    for i in range(len(base_septuplets)):\n        if dots(base_septuplets[i]) == value:\n            return (base_values[i], 1, 7)\n\n    # Check if the value is a dotted base value with two dots\n    for i in range(len(base_values)):\n        if dots(base_values[i], 2) == value:\n            return (base_values[i], 2, 1)\n\n    # Check if the", "idx": 1096}
{"namespace": "mingus.core.progressions.substitute_major_for_minor", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Major to minor substitution\n    if (\n        suff == \"M\"\n        or suff == \"M7\"\n        or suff == \"\"\n        and roman in [\"I\", \"IV\", \"V\"]\n        or ignore_suffix\n    ):\n        n = skip(roman, 1)\n        a = interval_diff(roman, n, 3) + acc\n        if suff == \"M\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"m\")))\n        elif suff == \"M7\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"m7\")))\n        elif suff == \"\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"\")))\n    return res\n\n", "idx": 1097}
{"namespace": "mingus.core.progressions.substitute_diminished_for_diminished", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Diminished to diminished substitution\n    if (\n        suff == \"dim7\"\n        or suff == \"dim\"\n        or suff == \"\"\n        and roman == \"VII\"\n        or ignore_suffix\n    ):\n        n = roman\n        a = acc\n        for i in range(3):\n            n = skip(n, 1)\n            a += interval_diff(roman, n, 2)\n            res.append(tuple_to_string((n, a, \"dim\")))\n    return res\n\n", "idx": 1098}
{"namespace": "mingus.core.progressions.substitute_diminished_for_dominant", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Diminished progressions\n    if (\n        suff == \"dim7\"\n        or suff == \"dim\"\n        or suff == \"\"\n        and roman in [\"VII\"]\n        or ignore_suffix\n    ):\n        if suff == \"\":\n            suff = \"dim\"\n\n        # Add diminished chord\n        last = roman\n        for x in range(3):\n            next = skip(last, 2)\n            acc += interval_diff(last, next, 3)\n            res.append(tuple_to_string((next, acc, suff)))\n            last = next\n    return res\n\n", "idx": 1099}
{"namespace": "mingus.core.progressions.substitute", "completion": "    res = []\n    (roman, acc, suff) = parse_string(progression[substitute_index])\n    if suff == \"7\":\n        res.extend(substitute_harmonic(progression, substitute_index, True))\n        res.extend(substitute_minor_for_major(progression, substitute_index, True))\n        res.extend(substitute_major_for_minor(progression, substitute_index, True))\n        res.extend(\n            substitute_diminished_for_diminished(progression, substitute_index, True)\n        )\n        res.extend(\n            substitute_diminished_for_dominant(progression, substitute_index, True)\n        )\n    elif suff == \"m7\":\n        res.extend(substitute_harmonic(progression, substitute_index, True))\n        res.extend(substitute_minor_for_major(progression, substitute_index, True))\n        res.extend(\n            substitute_diminished_for_diminished(progression, substitute_index, True)\n        )\n        res.extend(\n            substitute_diminished_for_dominant(progression, substitute_index, True)\n        )\n    elif suff == \"M7\":\n        res.extend(substitute_harmonic(progression, substitute_index, True))\n        res.extend(substitute_major_for_minor(progression, substitute_index, True))\n        res.extend(\n            substitute_diminished_for_diminished(progression, substitute_index, True)\n        )\n        res.extend(\n            substitute_diminished_for_dominant(progression, substitute_index, True)\n        )\n    elif suff == \"m\":\n        res.extend(substitute_minor_for_major(progression, substitute_index, True))\n        res.extend(\n            substitute_diminished_for_diminished(progression, substitute_index, True)\n        )\n        res.extend(\n            substitute_diminished_for_dominant(progression, substitute_index, True)\n        )\n    elif suff == \"M\":\n        res.extend(substitute_major_for_minor(progression, substitute_index, True))\n        res.extend(\n            substitute_diminished_for_diminished(progression, substitute", "idx": 1100}
{"namespace": "mingus.core.progressions.skip", "completion": "    i = numerals.index(roman_numeral)\n    i += skip_count\n    i %= 7\n    return numerals[i]", "idx": 1101}
{"namespace": "exodus_bundler.cli.configure_logging", "completion": "    level = logging.WARNING\n    if verbose:\n        level = logging.INFO\n    if quiet:\n        level = logging.ERROR\n\n    # Configure the root logger.\n\n    \"\"\"\n    Configure the root logger based on the input parameters. It sets the log level based on the value of `level`. It adds a stderr handler to log warning and error messages, and an optional stdout handler to log debug and info messages.\n    Input-Output Arguments\n    :param level: Int. The log level to set for the root logger.\n    :param suppress_stdout: Bool. Whether to suppress log messages from being printed to stdout.\n    :return: No return values.\n    \"\"\"\n    logging.basicConfig(level=level, format='%(levelname)s: %(message)s')\n\n    # Add a stdout handler if requested.\n\n    \"\"\"\n    Add a stdout handler to the root logger if `suppress_stdout` is False. It sets the log level to INFO and the format to '%(message)s'.\n    Input-Output Arguments\n    :param suppress_stdout: Bool. Whether to suppress log messages from being printed to stdout.\n    :return: No return values.\n    \"\"\"\n    if not suppress_stdout:\n        handler = logging.StreamHandler(sys.stdout)\n        handler.setLevel(logging.INFO)\n        handler.setFormatter(logging.Formatter('%(message)s'))\n        logging.getLogger().addHandler(handler)\n\n\n", "idx": 1102}
{"namespace": "exodus_bundler.bundling.create_unpackaged_bundle", "completion": "    # Create a temporary directory for the bundle.\n    root_directory = tempfile.mkdtemp()\n\n    # Create a temporary directory for the executables.\n    executables_directory = os.path.join(root_directory, 'exodus')\n    os.mkdir(executables_directory)\n\n    # Create a temporary directory for the dependencies.\n    dependencies_directory = os.path.join(root_directory, 'dependencies')\n    os.mkdir(dependencies_directory)\n\n    # Create a temporary directory for the additional files.\n    additional_files_directory = os.path.join(root_directory, 'additional_files')\n    os.mkdir(additional_files_directory)\n\n    # Create a temporary directory for the shell launchers.\n    shell_launchers_directory = os.path.join(root_directory, 'shell_launchers')\n    os.mkdir(shell_launchers_directory)\n\n    # Create a temporary directory for the detected dependencies.\n    detected_dependencies_directory = os.path.join(root_directory, 'detected_dependencies')\n    os.mkdir(detected_dependencies_directory)\n\n    # Create a temporary directory for the detected dependencies.\n    detected_dependencies_directory = os.path.join(root_directory, 'detected_dependencies')\n    os.mkdir(detected_dependencies_directory)\n\n    # Create a temporary directory for the detected dependencies.\n    detected_dependencies_directory = os.path.join(root_directory, 'detected_dependencies')\n    os.mkdir(detected_dependencies_directory)\n\n    # Create a temporary directory for the detected dependencies.\n    detected_dependencies_directory = os.path.join(root_directory, 'detected_dependencies')\n    os.mkdir(detected_dependencies_directory)\n\n    # Create a temporary directory for the detected dependencies.\n    detected_dependencies_directory = os.path.join(root_directory, 'detected_dependencies')\n    os.mkdir(detected_dependencies_directory)\n\n    # Create a temporary directory for the detected dependencies.\n    detected_dependencies_directory = os.path.join(root_directory, 'detected_dependencies')\n    os.mkdir(detected_dependencies_directory)\n\n    # Create a temporary directory for the detected dependencies.\n    detected_dependencies_directory = os.path.join(root_directory, 'detected_dependencies')\n    os.mkdir(detected_dependencies_directory", "idx": 1103}
{"namespace": "exodus_bundler.bundling.detect_elf_binary", "completion": "    with open(filename, 'rb') as f:\n        header = f.read(4)\n        return header == b'\\x7fELF'\n\n", "idx": 1104}
{"namespace": "exodus_bundler.bundling.resolve_binary", "completion": "    if os.path.exists(binary):\n        return os.path.abspath(binary)\n\n    for path in os.environ['PATH'].split(os.pathsep):\n        path = path.strip('\"')\n        binary_path = os.path.join(path, binary)\n        if os.path.exists(binary_path):\n            return os.path.abspath(binary_path)\n\n    raise MissingFileError('The \"%s\" binary could not be found in $PATH.' % binary)\n\n", "idx": 1105}
{"namespace": "exodus_bundler.bundling.resolve_file_path", "completion": "    if not os.path.exists(path):\n        if search_environment_path:\n            path = resolve_binary(path)\n        else:\n            raise MissingFileError('The \"%s\" file was not found.' % path)\n\n    if os.path.isdir(path):\n        raise UnexpectedDirectoryError('The \"%s\" file is a directory.' % path)\n\n    return os.path.normpath(os.path.abspath(path))\n\n", "idx": 1106}
{"namespace": "exodus_bundler.bundling.run_ldd", "completion": "    if not detect_elf_binary(binary):\n        raise InvalidElfBinaryError('The \"%s\" file is not a valid ELF binary.' % binary)\n\n    try:\n        process = Popen([ldd, binary], stdout=PIPE, stderr=PIPE)\n        stdout, stderr = process.communicate()\n        output = stdout.decode('utf-8') + stderr.decode('utf-8')\n        return output.split('\\n')\n    except OSError as e:\n        raise CompilerNotFoundError('The \"%s\" command could not be found.' % ldd)\n\n", "idx": 1107}
{"namespace": "exodus_bundler.bundling.Elf.dependencies", "completion": "        dependencies = set()\n        direct_dependencies = self.find_direct_dependencies()\n        dependencies.update(direct_dependencies)\n        while True:\n            new_dependencies = set()\n            for dependency in direct_dependencies:\n                new_dependencies.update(dependency.find_direct_dependencies())\n            if not new_dependencies:\n                break\n            dependencies.update(new_dependencies)\n            direct_dependencies = new_dependencies\n        return dependencies\n\n", "idx": 1108}
{"namespace": "exodus_bundler.bundling.File.hash", "completion": "        with open(self.path, 'rb') as f:\n            content = f.read()\n            return hashlib.sha256(content).hexdigest()\n", "idx": 1109}
{"namespace": "exodus_bundler.bundling.Bundle.add_file", "completion": "        file = File(path, entry_point=entry_point, chroot=self.chroot)\n        if file.elf:\n            self.files |= file.elf.dependencies\n        self.files.add(file)\n        if file.elf:\n            self.linker_files.add(file.elf.linker_file)\n        return file\n", "idx": 1110}
{"namespace": "exodus_bundler.bundling.Bundle.bundle_root", "completion": "        return os.path.normpath(os.path.abspath(os.path.join(self.working_directory, 'bundles', self.hash)))\n", "idx": 1111}
{"namespace": "exodus_bundler.bundling.Bundle.hash", "completion": "        hashes = [file.hash for file in self.files]\n        hashes.sort()\n        hashes = ''.join(hashes).encode('utf-8')\n        return hashlib.sha256(hashes).hexdigest()\n\n", "idx": 1112}
{"namespace": "exodus_bundler.launchers.construct_bash_launcher", "completion": "    # Render the template file with the provided parameters.\n    return render_template_file(\n        os.path.join(parent_directory, 'templates', 'bash_launcher.sh'),\n        linker=linker,\n        library_path=library_path,\n        executable=executable,\n        full_linker=full_linker,\n    )\n\n", "idx": 1113}
{"namespace": "exodus_bundler.input_parsing.extract_open_path", "completion": "    line = strip_pid_prefix(line)\n    for prefix in ['open(', 'openat(']:\n        if line.startswith(prefix):\n            line = line[len(prefix):]\n            parts = line.split('\", ')\n            if len(parts) > 1:\n                return parts[0]\n    return None\n\n", "idx": 1114}
{"namespace": "exodus_bundler.input_parsing.extract_paths", "completion": "    paths = []\n    for line in content:\n        path = extract_exec_path(line)\n        if path is None:\n            path = extract_open_path(line)\n        if path is None:\n            path = extract_stat_path(line)\n        if path is None:\n            continue\n        if existing_only and not os.path.exists(path):\n            continue\n        if path not in paths:\n            paths.append(path)\n    return paths\n\n", "idx": 1115}
{"namespace": "fs.time.epoch_to_datetime", "completion": "    if t is None:\n        return None\n    return datetime.utcfromtimestamp(t)\n\n", "idx": 1116}
{"namespace": "fs.path.normpath", "completion": "    if not path:\n        return path\n\n    if _requires_normalization(path):\n        parts = path.split(\"/\")\n        newparts = []\n        for part in parts:\n            if part == \"..\":\n                if len(newparts) > 0:\n                    newparts.pop()\n                else:\n                    raise IndexError(\"illegal back reference\")\n            elif part != \".\" and part != \"\":\n                newparts.append(part)\n        return \"/\".join(newparts)\n    else:\n        return path\n\n", "idx": 1117}
{"namespace": "fs.path.iteratepath", "completion": "    return path.split(\"/\")\n\n", "idx": 1118}
{"namespace": "fs.path.recursepath", "completion": "    path = relpath(normpath(path))\n    if not path:\n        return []\n    if reverse:\n        return [path] + [\n            \"/\".join(path.split(\"/\")[:i]) for i in range(1, len(path.split(\"/\")))\n        ]\n    return [\n        \"/\".join(path.split(\"/\")[:i]) for i in range(1, len(path.split(\"/\")))\n    ] + [path]\n\n", "idx": 1119}
{"namespace": "fs.path.join", "completion": "    return \"/\".join(paths)\n\n", "idx": 1120}
{"namespace": "fs.path.parts", "completion": "    return [part for part in path.strip(\"/\").split(\"/\") if part]\n\n", "idx": 1121}
{"namespace": "fs.path.splitext", "completion": "    if \".\" not in path:\n        return (path, \"\")\n    split = path.rsplit(\".\", 1)\n    return (split[0], \".\" + split[1])\n\n", "idx": 1122}
{"namespace": "fs.path.isbase", "completion": "    return normpath(path2).startswith(normpath(path1))\n\n", "idx": 1123}
{"namespace": "fs.path.frombase", "completion": "    if not isparent(path1, path2):\n        raise ValueError(\"path1 is not a parent directory of path2\")\n    return path2[len(path1):]\n\n", "idx": 1124}
{"namespace": "fs.path.relativefrom", "completion": "    if not isbase(base, path):\n        raise ValueError(\"path must be a prefix of base\")\n    return path[len(base) :]\n\n", "idx": 1125}
{"namespace": "fs.path.iswildcard", "completion": "    return any(char in _WILD_CHARS for char in path)", "idx": 1126}
{"namespace": "fs.wildcard.match", "completion": "    \"\"\"\n    This function tests whether a given name matches a wildcard pattern. It uses regular expressions to match the pattern against the name.\n    Input-Output Arguments\n    :param pattern: Text. A wildcard pattern to match against the name.\n    :param name: Text. The name to be tested.\n    :return: bool. True if the name matches the pattern, False otherwise.\n    \"\"\"\n\n    return _match(pattern, name, False)\n\n", "idx": 1127}
{"namespace": "fs.wildcard.imatch", "completion": "    try:\n        re_pat = _PATTERN_CACHE[(pattern, False)]\n    except KeyError:\n        res = \"(?ms)\" + _translate(pattern) + r\"\\Z\"\n        _PATTERN_CACHE[(pattern, False)] = re_pat = re.compile(res, re.IGNORECASE)\n    return re_pat.match(name) is not None\n\n", "idx": 1128}
{"namespace": "fs.wildcard.get_matcher", "completion": "    if not patterns:\n        return lambda x: True\n    if case_sensitive:\n        return partial(match_any, patterns)\n    else:\n        return partial(imatch_any, patterns)\n\n", "idx": 1129}
{"namespace": "fs._url_tools.url_quote", "completion": "    if _WINDOWS_PLATFORM:\n        drive_letter = re.match(r\"[a-zA-Z]:\", path_snippet)\n        if drive_letter:\n            drive_letter = drive_letter.group(0)\n            path_snippet = path_snippet.replace(drive_letter, \"\")\n            drive_letter = drive_letter.replace(\":\", \"%3A\")\n            path_snippet = drive_letter + path_snippet\n    else:\n        path_snippet = six.moves.urllib.parse.quote(path_snippet)\n    return path_snippet", "idx": 1130}
{"namespace": "fs._ftp_parse.parse", "completion": "    decoders = get_decoders()\n    parsed_lines = []\n    for line in lines:\n        if not line.strip():\n            continue\n        for regex, decoder in decoders:\n            match = regex.match(line)\n            if match:\n                parsed_lines.append(decoder(match))\n                break\n    return parsed_lines\n\n", "idx": 1131}
{"namespace": "fs._ftp_parse._parse_time", "completion": "    for fmt in formats:\n        try:\n            dt = datetime.strptime(t, fmt)\n            return (dt - EPOCH_DT).total_seconds()\n        except ValueError:\n            pass\n    return None\n\n", "idx": 1132}
{"namespace": "fs.permissions.Permissions.parse", "completion": "        if len(ls) != 10:\n            raise ValueError(\"Invalid permissions string: {}\".format(ls))\n\n        if ls[0] != \"-\":\n            raise ValueError(\"Invalid permissions string: {}\".format(ls))\n\n        user = ls[1:4]\n        group = ls[4:7]\n        other = ls[7:10]\n\n        return cls(user=user, group=group, other=other)\n", "idx": 1133}
{"namespace": "fs.permissions.Permissions.create", "completion": "        return cls(mode=make_mode(init))\n", "idx": 1134}
{"namespace": "fs.info.Info.suffix", "completion": "        return cast(Text, self.get(\"basic\", \"suffix\"))\n", "idx": 1135}
{"namespace": "fs.info.Info.suffixes", "completion": "        name = self.get(\"basic\", \"name\")\n        if name.startswith(\".\") and name.count(\".\") == 1:\n            return []\n        return name.split(\".\")\n", "idx": 1136}
{"namespace": "fs.info.Info.stem", "completion": "        name = self.get(\"basic\", \"name\")\n        if name.startswith(\".\") and name.count(\".\") == 1:\n            return name\n        basename, dot, ext = name.rpartition(\".\")\n        return basename if dot else name\n", "idx": 1137}
{"namespace": "fs.info.Info.type", "completion": "        self._require_namespace(\"details\")\n        return ResourceType(self.get(\"details\", \"type\"))\n", "idx": 1138}
{"namespace": "fs.info.Info.created", "completion": "        self._require_namespace(\"details\")\n        _time = self._make_datetime(self.get(\"details\", \"created\"))\n        return _time\n", "idx": 1139}
{"namespace": "pyinfra.connectors.mech.MechInventoryConnector.make_names_data", "completion": "        # Get the Mech SSH information\n        mech_config = get_mech_config(limit)\n\n        # Process the Mech SSH information to create a list of host names and their corresponding data\n        names_data = []\n        for line in mech_config:\n            if line.startswith(\"Host \"):\n                host = {}\n                host[\"Host\"] = line.split()[1]\n                names_data.append(_make_name_data(host))\n\n        return names_data", "idx": 1140}
{"namespace": "pyinfra.connectors.ansible.AnsibleInventoryConnector.make_names_data", "completion": "        if not inventory_filename:\n            raise InventoryError(\"No Ansible inventory filename provided!\")\n\n        if not path.exists(inventory_filename):\n            raise InventoryError(f\"Could not find Ansible inventory file: {inventory_filename}\")\n\n        with open(inventory_filename) as f:\n            data = yaml.safe_load(f)\n\n        return data\n", "idx": 1141}
{"namespace": "pyinfra.operations.files.rsync", "completion": "    show_rsync_warning()\n\n    yield RsyncCommand(src, dest, flags)\n\n", "idx": 1142}
{"namespace": "pyinfra.operations.files.get", "completion": "    # Ensure the destination directory exists\n    if add_deploy_dir and state.cwd:\n        dest = os.path.join(state.cwd, dest)\n\n    # Ensure the local directory exists\n    if create_local_dir and not os.path.isdir(os.path.dirname(dest)):\n        os.makedirs(os.path.dirname(dest))\n\n    # Ensure the local file exists\n    if not os.path.isfile(dest):\n        # Ensure the local directory exists\n        if create_local_dir and not os.path.isdir(os.path.dirname(dest)):\n            os.makedirs(os.path.dirname(dest))\n\n        # Ensure the local file exists\n        if not os.path.isfile(dest):\n            # Ensure the local file is not a link\n            if not os.path.islink(dest):\n                # Ensure the local file is not a directory\n                if not os.path.isdir(dest):\n                    # Ensure the local file is not a special file\n                    if not os.path.ismount(dest):\n                        # Ensure the local file is not a block device\n                        if not os.path.isblk(dest):\n                            # Ensure the local file is not a character device\n                            if not os.path.ischr(dest):\n                                # Ensure the local file is not a FIFO\n                                if not os.path.isfifo(dest):\n                                    # Ensure the local file is not a socket\n                                    if not os.path.issock(dest):\n                                        # Ensure the local file is not a named pipe\n                                        if not os.path.isfifo(dest):\n                                            # Ensure the local file is not a device\n                                            if not os.path.isdevice(dest):\n                                                # Ensure the local file is not a door\n                                                if not os.path.isdoor(dest):\n                                                    # Ensure the local file is not a window\n                                                    if not os.path.iswindow(dest):\n                                                        # Ensure the local file is not a terminal\n                                                        if not os.path.isterminal(dest):\n                                                            # Ensure the local file is not a pseudo terminal\n                                                            if not os.path.ispseudoterminal(dest):\n                                                                # Ensure the local file is not a terminal device\n                                                                if not os.path.isterminaldevice(dest):\n                                                                    # Ensure the local file is not a pseudo terminal device\n                                                                    if not os.path.ispseudoterminaldevice", "idx": 1143}
{"namespace": "pyinfra.operations.files.put", "completion": "    # Always use POSIX style path as local might be Windows, remote always *nix\n    if add_deploy_dir and state.cwd:\n        dest = os.path.join(state.cwd, dest)\n\n    # Create the remote directory if it doesn't exist\n    if create_remote_dir:\n        yield from _create_remote_dir(state, host, dest, user, group)\n\n    # Assume the local file exists\n    if assume_exists:\n        yield FileUploadCommand(src, dest, force=force)\n\n    # Local file exists - check if it matches our remote\n    else:\n        local_sum = get_file_sha1(src)\n        remote_sum = host.get_fact(Sha1File, path=dest)\n\n        # Check sha1sum, upload if needed\n        if local_sum != remote_sum:\n            yield FileUploadCommand(src, dest, force=force)\n\n", "idx": 1144}
{"namespace": "pyinfra.operations.files.file", "completion": "    path = _validate_path(path)\n\n    info = host.get_fact(File, path=path)\n\n    if info is False:  # not a file\n        yield from _raise_or_remove_invalid_path(\n            \"file\",\n            path,\n            force,\n            force_backup,\n            force_backup_dir,\n        )\n        info = None\n\n    add_args = [\"touch\"]\n    if touch:\n        add_args.append(\"-c\")\n\n    add_cmd = StringCommand(\" \".join(add_args), QuoteString(path))\n    remove_cmd = StringCommand(\"rm\", \"-f\", QuoteString(path))\n\n    if not present:\n        if info:\n            yield remove_cmd\n        else:\n            host.noop(\"file {file} does not exist\".format(file=path))\n        return\n\n    if info is None:  # create\n        if create_remote_dir:\n            yield from _create_remote_dir(state, host, path, user, group)\n\n        yield add_cmd\n\n        if user or group:\n            yield file_utils.chown(path, user, group, dereference=False)\n\n        if mode:\n            yield file_utils.chmod(path, mode)\n\n    else:  # edit\n        changed = False\n\n        # If the mode is wrong, chmod the file\n        if mode and info[\"mode\"] != mode:\n            yield file_utils.chmod(path, mode)\n            changed = True\n\n        # Check user/group\n        if (user and info[\"user\"] != user) or (group and info[\"group\"] != group):\n            yield file_utils.chown(path, user, group, dereference=False)\n            changed = True\n\n        if not changed:\n            host.noop(\"file {0} already exists\".format(path))\n\n", "idx": 1145}
{"namespace": "pyinfra.operations.python.call", "completion": "    return FunctionCommand(function, *args, **kwargs)", "idx": 1146}
{"namespace": "pyinfra.api.operation.add_op", "completion": "    # Get the operation name from the function name\n    op_name = op_func.__name__\n\n    # Get the operation order from the stack\n    op_order = get_operation_order_from_stack()\n\n    # Get the global arguments from the stack\n    global_arguments = pop_global_arguments()\n\n    # Get the execution kwargs from the stack\n    execution_kwargs = get_execution_kwarg_keys()\n\n    # Get the operation arguments from the stack\n    op_arguments = pop_global_arguments()\n\n    # Get the operation kwargs from the stack\n    op_kwargs = get_execution_kwarg_keys()\n\n    # Get the operation arguments from the stack\n    op_arguments = pop_global_arguments()\n\n    # Get the operation kwargs from the stack\n    op_kwargs = get_execution_kwarg_keys()\n\n    # Get the operation arguments from the stack\n    op_arguments = pop_global_arguments()\n\n    # Get the operation kwargs from the stack\n    op_kwargs = get_execution_kwarg_keys()\n\n    # Get the operation arguments from the stack\n    op_arguments = pop_global_arguments()\n\n    # Get the operation kwargs from the stack\n    op_kwargs = get_execution_kwarg_keys()\n\n    # Get the operation arguments from the stack\n    op_arguments = pop_global_arguments()\n\n    # Get the operation kwargs from the stack\n    op_kwargs = get_execution_kwarg_keys()\n\n    # Get the operation arguments from the stack\n    op_arguments = pop_global_arguments()\n\n    # Get the operation kwargs from the stack\n    op_kwargs = get_execution_kwarg_keys()\n\n    # Get the operation arguments from the stack\n    op_arguments = pop_global_arguments()\n\n    # Get the operation kwargs from the stack\n    op_kwargs = get_execution_kwarg_keys()\n\n    # Get the operation arguments from the stack\n    op_arguments = pop_global_arguments()\n\n    # Get the operation kwargs from the stack\n    op_kwargs = get_execution_kwarg_keys()\n\n    # Get the operation arguments from the stack\n    op_arguments = pop_global_arguments()\n\n    # Get the operation kwargs from the stack\n    op_kwargs = get_execution_kwarg_keys()\n\n    # Get the operation arguments from the stack\n    op_arguments = pop_global_arguments()\n\n    # Get the operation", "idx": 1147}
{"namespace": "pyinfra.api.facts.get_facts", "completion": "    # Create a dictionary to store the retrieved facts\n    facts = {}\n\n    # Iterate over the active hosts in the state's inventory\n    for host in state.inventory.active_hosts:\n        # Spawn a greenlet for each host to retrieve the facts\n        gevent.spawn(get_facts_for_host, state, host, facts, *args, **kwargs)\n\n    # Wait for the greenlets to complete\n    gevent.joinall(state.greenlets)\n\n    return facts\n\n", "idx": 1148}
{"namespace": "pyinfra.api.operations.run_ops", "completion": "    if serial:\n        _run_serial_ops(state)\n    elif no_wait:\n        _run_no_wait_ops(state)\n    else:\n        for op_hash in state.get_op_order():\n            _run_single_op(state, op_hash)", "idx": 1149}
{"namespace": "pyinfra.api.connect.connect_all", "completion": "    gevent.joinall([gevent.spawn(connect, state, host) for host in state.inventory])\n\n", "idx": 1150}
{"namespace": "pyinfra.api.arguments.pop_global_arguments", "completion": "    if keys_to_check is None:\n        keys_to_check = all_argument_meta.keys()\n\n    # Pop the arguments from the kwargs\n    popped_arguments = {key: kwargs.pop(key, default_sentinel) for key in keys_to_check}\n\n    # Pop the arguments from the host data variables\n    if host is not None:\n        for key in keys_to_check:\n            if key in host.data:\n                popped_arguments[key] = host.data.pop(key)\n\n    # Pop the arguments from the deploy context\n    if state is not None:\n        for key in keys_to_check:\n            if key in state.deploy_kwargs:\n                popped_arguments[key] = state.deploy_kwargs.pop(key)\n\n    # Pop the arguments from the config variables\n    for key in keys_to_check:\n        if key in state.config.ARGUMENTS:\n            popped_arguments[key] = state.config.ARGUMENTS.pop(key)\n\n    # Pop the arguments from the global context\n    for key in keys_to_check:\n        if key in state.global_kwargs:\n            popped_arguments[key] = state.global_kwargs.pop(key)\n\n    # Pop the arguments from the global context\n    for key in keys_to_check:\n        if key in state.global_kwargs:\n            popped_arguments[key] = state.global_kwargs.pop(key)\n\n    # Pop the arguments from the global context\n    for key in keys_to_check:\n        if key in state.global_kwargs:\n            popped_arguments[key] = state.global_kwargs.pop(key)\n\n    # Pop the arguments from the global context\n    for key in keys_to_check:\n        if key in state.global_kwargs:\n            popped_arguments[key] = state.global_kwargs.pop(key)\n\n    # Pop the arguments from the global context\n    for key in keys_to_check:\n        if key in state.global_kwargs:\n            popped_arguments[key] = state.global_kwargs.pop(key)\n\n    # Pop the arguments from the global context\n    for key in keys_to_check:\n        if key in state.global_kwargs:\n            popped_arguments[key] = state.global_", "idx": 1151}
{"namespace": "pyinfra_cli.commands.get_func_and_args", "completion": "    func = try_import_module_attribute(commands[0])\n    args = commands[1:]\n    return func, args\n\n", "idx": 1152}
{"namespace": "viztracer.tracer._VizTracer.start", "completion": "        if self.enable:\n            return\n\n        self.enable = True\n        self.parsed = False\n\n        if self.log_print:\n            builtins.print = self.print_overload\n\n        if self.include_files and self.exclude_files:\n            raise ValueError(\"Cannot specify both include_files and exclude_files\")\n\n        self.config()\n        self._tracer.start()\n", "idx": 1153}
{"namespace": "viztracer.tracer._VizTracer.stop", "completion": "        self.enable = False\n        if self.log_print:\n            self.restore_print()\n        self._tracer.stop()\n", "idx": 1154}
{"namespace": "viztracer.report_builder.ReportBuilder.save", "completion": "        if isinstance(output_file, str):\n            if output_file.endswith(\".html\"):\n                output_format = \"html\"\n            elif output_file.endswith(\".json\"):\n                output_format = \"json\"\n            elif output_file.endswith(\".gz\"):\n                output_format = \"gz\"\n            else:\n                raise ValueError(f\"Unknown output file format: {output_file}\")\n        elif isinstance(output_file, TextIO):\n            output_format = \"txt\"\n        else:\n            raise TypeError(f\"Unknown output file type: {type(output_file)}\")\n\n        if output_format == \"gz\":\n            with gzip.open(output_file, \"wt\") as f:\n                self.generate_report(f, \"json\", file_info=file_info)\n        else:\n            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n                self.generate_report(f, output_format, file_info=file_info)\n\n        if output_format == \"html\":\n            self.final_messages.append((\"view_command\", {\"output_file\": os.path.abspath(output_file)}))\n        elif output_format == \"json\":\n            self.final_messages.append((\"view_command\", {\"output_file\": os.path.abspath(output_file)}))\n        elif output_format == \"gz\":\n            self.final_messages.append((\"view_command\", {\"output_file\": os.path.abspath(output_file)}))\n        elif output_format == \"txt\":\n            self.final_messages.append((\"view_command\", {\"output_file\": output_file}))\n\n        if self.verbose > 0:\n            for msg_type, msg_data in self.final_messages:\n                if msg_type == \"view_command\":\n                    color_print(f\"View the report with: viztracer {msg_data['output_file']}\", \"green\")\n                elif msg_type == \"total_entries\":\n                    color_print(f\"Total entries: {msg_data['total_entries']}\", \"green\")\n                elif msg_type == \"overflow\":\n                    color_print(\"Warning: The trace data is too large to be displayed in the report. Please use the command line tool to view the report.\", \"red\")\n                else", "idx": 1155}
{"namespace": "viztracer.code_monkey.AstTransformer.get_assign_targets_with_attr", "completion": "        if isinstance(node, ast.Attribute):\n            return [node]\n        elif isinstance(node, (ast.Name, ast.Subscript, ast.Starred)):\n            return []\n        elif isinstance(node, ast.Tuple) or isinstance(node, ast.List):\n            return reduce(lambda a, b: a + b, [self.get_assign_targets_with_attr(elt) for elt in node.elts])\n        color_print(\"WARNING\", \"Unexpected node type {} for ast.Assign. \\\n            Please report to the author github.com/gaogaotiantian/viztracer\".format(type(node)))\n        return []\n", "idx": 1156}
{"namespace": "viztracer.code_monkey.SourceProcessor.process", "completion": "        if isinstance(source, bytes):\n            source = source.decode(\"utf-8\")\n        if not isinstance(source, str):\n            return source\n        new_lines: List[str] = []\n        for line in source.splitlines(True):\n            for pattern, transform in self.patterns:\n                match = pattern.match(line)\n                if match:\n                    new_lines.append(transform(line, match))\n                    break\n            else:\n                new_lines.append(line)\n        return \"\".join(new_lines)\n", "idx": 1157}
{"namespace": "wal_e.log_help.WalELogger.fmt_logline", "completion": "        log_line = ['MSG: {message}']\n        if detail:\n            log_line.append('DETAIL: {detail}')\n        if hint:\n            log_line.append('HINT: {hint}')\n        if structured:\n            log_line.append('STRUCTURED: {structured data}')\n\n        log_line = '\\n'.join(log_line)\n\n        if structured:\n            structured = WalELogger._fmt_structured(structured)\n\n        return log_line.format(message=msg, detail=detail, hint=hint, structured_data=structured)\n", "idx": 1158}
{"namespace": "wal_e.blobstore.file.calling_format.Bucket.delete_keys", "completion": "        for key in keys:\n            key_path = os.path.join(\"/\", key.strip(\"/\"))\n            if os.path.isfile(key_path):\n                os.remove(key_path)\n            elif os.path.isdir(key_path):\n                shutil.rmtree(key_path)\n        remove_empty_dirs(\"/\")\n", "idx": 1159}
{"namespace": "wal_e.worker.upload_pool.TarUploadPool.put", "completion": "        if self.closed:\n            raise RuntimeError(\"TarUploadPool is closed\")\n\n        if self.member_burden + len(tpart) > self.max_members:\n            raise RuntimeError(\"Too many members\")\n\n        if self.concurrency_burden >= self.max_concurrency:\n            raise RuntimeError(\"Too much concurrency\")\n\n        self._start(tpart)\n", "idx": 1160}
{"namespace": "wal_e.worker.pg.wal_transfer.WalSegment.from_ready_archive_status", "completion": "        # Get the archive_status directory path\n        archive_status_dir = path.join(xlog_dir, 'archive_status')\n\n        # Check if the archive_status directory exists\n        if not path.exists(archive_status_dir):\n            raise UserCritical(\n                msg='archive_status directory does not exist',\n                detail='The archive_status directory does not exist in the xlog directory: {0}'.format(xlog_dir),\n                hint='Check the xlog directory path and ensure that the archive_status directory exists.'\n            )\n\n        # Iterate through the files in the archive_status directory\n        for filename in os.listdir(archive_status_dir):\n            # Check if the file is a segment file\n            if re.match(storage.SEGMENT_REGEXP, filename):\n                # Create a WalSegment instance for the segment file\n                segment_path = path.join(archive_status_dir, filename)\n                yield WalSegment(segment_path)\n", "idx": 1161}
{"namespace": "wal_e.worker.pg.wal_transfer.WalTransferGroup.join", "completion": "        # Wait for the transfer to exit and raise any errors that occur during the process.\n        self.wait_change.wait()\n\n        # Close the input WalTransferGroup instance.\n        self.close()\n\n        # Wait a while for all running greenlets to exit and attempt to force them to exit so join terminates in a reasonable amount of time (e.g., 30).\n        gevent.killall(self.greenlets, timeout=30)\n", "idx": 1162}
{"namespace": "wal_e.worker.pg.wal_transfer.WalTransferGroup.start", "completion": "        def transferer_wrapper(segment):\n            try:\n                self.transferer(segment)\n            except Exception as e:\n                self.wait_change.put(e)\n            finally:\n                self.expect -= 1\n                self.wait_change.put(None)\n\n        greenlet = gevent.Greenlet(transferer_wrapper, segment)\n        self.greenlets.add(greenlet)\n        greenlet.start()\n        self.expect += 1", "idx": 1163}
{"namespace": "mrjob.py2.to_unicode", "completion": "    if isinstance(s, bytes):\n        try:\n            return s.decode('utf_8')\n        except UnicodeDecodeError:\n            return s.decode('latin_1')\n    else:\n        return s", "idx": 1164}
{"namespace": "mrjob.job.MRJob.steps", "completion": "        return [MRStep(mapper=self.mapper,\n                        reducer=self.reducer,\n                        mapper_init=self.mapper_init,\n                        mapper_final=self.mapper_final,\n                        mapper_cmd=self.mapper_cmd,\n                        mapper_pre_filter=self.mapper_pre_filter,\n                        mapper_raw=self.mapper_raw,\n                        reducer_init=self.reducer_init,\n                        reducer_final=self.reducer_final,\n                        reducer_cmd=self.reducer_cmd,\n                        reducer_pre_filter=self.reducer_pre_filter,\n                        combiner=self.combiner,\n                        combiner_init=self.combiner_init,\n                        combiner_final=self.combiner_final,\n                        combiner_cmd=self.combiner_cmd,\n                        combiner_pre_filter=self.combiner_pre_filter,\n                        internal_protocol=self.INTERNAL_PROTOCOL,\n                        input_protocol=self.INPUT_PROTOCOL,\n                        output_protocol=self.OUTPUT_PROTOCOL,\n                        jobconf=self.jobconf(),\n                        partitioner=self.partitioner(),\n                        sort_values=self.sort_values(),\n                        **self._non_option_kwargs())]\n", "idx": 1165}
{"namespace": "mrjob.job.MRJob.increment_counter", "completion": "        # replace commas in group and counter with semicolons to avoid\n        # confusion with Hadoop streaming\n        group = group.replace(',', ';')\n        counter = counter.replace(',', ';')\n\n        # construct a line using a specified format\n        line = 'reporter:counter:{group},{counter},{amount}\\n'.format(\n            group=group, counter=counter, amount=amount)\n\n        # output the line through the standard error stream of the input\n        # MRJob instance\n        self.stderr.write(line)\n", "idx": 1166}
{"namespace": "mrjob.job.MRJob.set_status", "completion": "        # don't allow people to pass in floats\n        from mrjob.py2 import integer_types\n        if not isinstance(msg, string_types):\n            msg = str(msg)\n\n        # Extra commas screw up hadoop and there's no way to escape them. So\n        # replace them with the next best thing: semicolons!\n        #\n        # The relevant Hadoop code is incrCounter(), here:\n        # http://svn.apache.org/viewvc/hadoop/mapreduce/trunk/src/contrib/streaming/src/java/org/apache/hadoop/streaming/PipeMapRed.java?view=markup  # noqa\n        msg = msg.replace(',', ';')\n\n        line = 'reporter:status:%s\\n' % (msg,)\n        if not isinstance(line, bytes):\n            line = line.encode('utf_8')\n\n        self.stderr.write(line)\n        self.stderr.flush()\n", "idx": 1167}
{"namespace": "mrjob.job.MRJob.run_job", "completion": "        # set up logging\n        self.set_up_logging()\n\n        # create a runner\n        runner = self.make_runner()\n\n        # run the job\n        try:\n            runner.run()\n        except:\n            # log the error and exit the program\n            log.exception('Error running job')\n            raise\n\n        # if the output needs to be concatenated, write the output to the\n        # standard output stream\n        if self.options.cat:\n            self.write_output()\n", "idx": 1168}
{"namespace": "mrjob.job.MRJob.set_up_logging", "completion": "        # quiet overrides verbose\n        if quiet:\n            verbose = False\n\n        # set up logging\n        from mrjob.log import set_up_logging\n        set_up_logging(\n            cls.__name__,\n            quiet=quiet,\n            verbose=verbose,\n            stream=stream,\n        )\n", "idx": 1169}
{"namespace": "mrjob.job.MRJob.execute", "completion": "        if self.options.mapper:\n            self.run_mapper()\n        elif self.options.combiner:\n            self.run_combiner()\n        elif self.options.reducer:\n            self.run_reducer()\n        elif self.options.spark:\n            self.run_spark()\n        else:\n            self.run_job()\n", "idx": 1170}
{"namespace": "mrjob.job.MRJob._runner_kwargs", "completion": "        # non-option keyword arguments\n        kwargs = dict(\n            (k, v)\n            for k, v in self.options.__dict__.items()\n            if not k.startswith('_'))\n\n        # keyword arguments from switches\n        kwargs.update(self._switches_kwargs())\n\n        # keyword arguments from the job\n        kwargs.update(self._job_kwargs())\n\n        # steps description\n        kwargs['steps'] = self.steps()\n\n        # if the runner class is \"inline\" or \"spark\", include the MRJob class\n        # in the keyword arguments\n        if self.options.runner in ('inline', 'spark'):\n            kwargs['job_class'] = self\n\n        return kwargs\n", "idx": 1171}
{"namespace": "mrjob.job.MRJob.run_mapper", "completion": "        step = self._get_step(step_num, MRStep)\n\n        # pick input and output protocols\n        input_protocol = self._pick_protocol(step.mapper_raw,\n                                              step.mapper_init,\n                                              step.mapper_final,\n                                              step.mapper,\n                                              step.mapper_cmd,\n                                              step.mapper_pre_filter)\n        output_protocol = self._pick_protocol(step.mapper,\n                                               step.mapper_cmd,\n                                               step.mapper_pre_filter)\n\n        # read lines\n        lines = self._read_lines(input_protocol)\n\n        # write key-value pairs\n        self._write_key_value_pairs(lines, output_protocol)\n", "idx": 1172}
{"namespace": "mrjob.job.MRJob.run_combiner", "completion": "        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'combiner')\n\n        for k, v in self.combine_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)\n", "idx": 1173}
{"namespace": "mrjob.job.MRJob.add_passthru_arg", "completion": "        self.arg_parser.add_argument(*args, **kwargs)\n", "idx": 1174}
{"namespace": "mrjob.job.MRJob.is_task", "completion": "        return self.options.mapper or self.options.reducer or self.options.combiner or self.options.spark\n", "idx": 1175}
{"namespace": "mrjob.job.MRJob.parse_output", "completion": "        for chunk in chunks:\n            line = chunk.rstrip(b'\\r\\n')\n            key, value = self.output_protocol().read(line)\n            yield key, value\n", "idx": 1176}
{"namespace": "mrjob.job.MRJob.sandbox", "completion": "        if stdin is None:\n            stdin = BytesIO()\n\n        if stdout is None:\n            stdout = BytesIO()\n\n        if stderr is None:\n            stderr = BytesIO()\n\n        self.stdin = stdin\n        self.stdout = stdout\n        self.stderr = stderr\n\n        return self\n", "idx": 1177}
{"namespace": "mrjob.hadoop.fully_qualify_hdfs_path", "completion": "    if path.startswith('hdfs://'):\n        return path\n    elif path.startswith('/'):\n        return 'hdfs://' + path\n    else:\n        return 'hdfs:///user/' + getpass.getuser() + '/' + path\n\n", "idx": 1178}
{"namespace": "mrjob.hadoop.HadoopJobRunner.fs", "completion": "        if not hasattr(self, '_fs'):\n            self._fs = CompositeFilesystem()\n            self._fs.add_fs(self.hadoop_fs)\n            self._fs.add_fs(self.local_fs)\n        return self._fs\n", "idx": 1179}
{"namespace": "mrjob.hadoop.HadoopJobRunner._find_hadoop_streaming_jar", "completion": "        # If the user has specified a path for the Hadoop streaming jar, use it directly\n        if self._opts['hadoop_streaming_jar']:\n            return self._opts['hadoop_streaming_jar']\n\n        # If the user has specified a path for the Hadoop home, use it directly\n        if self._opts['hadoop_home']:\n            hadoop_home = self._opts['hadoop_home']\n            if os.path.isdir(hadoop_home):\n                return self._find_hadoop_streaming_jar_in_directory(hadoop_home)\n\n        # If the user has specified a path for the Hadoop bin, use it directly\n        if self._opts['hadoop_bin']:\n            hadoop_bin = self._opts['hadoop_bin']\n            if os.path.isfile(hadoop_bin):\n                hadoop_home = os.path.dirname(os.path.dirname(hadoop_bin))\n                return self._find_hadoop_streaming_jar_in_directory(hadoop_home)\n\n        # If the user has specified a path for the Hadoop streaming jar, use it directly\n        if self._opts['hadoop_streaming_jar']:\n            return self._opts['hadoop_streaming_jar']\n\n        # If the user has specified a path for the Hadoop home, use it directly\n        if self._opts['hadoop_home']:\n            hadoop_home = self._opts['hadoop_home']\n            if os.path.isdir(hadoop_home):\n                return self._find_hadoop_streaming_jar_in_directory(hadoop_home)\n\n        # If the user has specified a path for the Hadoop bin, use it directly\n        if self._opts['hadoop_bin']:\n            hadoop_bin = self._opts['hadoop_bin']\n            if os.path.isfile(hadoop_bin):\n                hadoop_home = os.path.dirname(os.path.dirname(hadoop_bin))\n                return self._find_hadoop_streaming_jar_in_directory(hadoop_home)\n\n        # If the user has specified a path for the Hadoop streaming jar, use it directly\n        if self._opts['hadoop_streaming_jar']:\n            return self._opts['hadoop_streaming_jar']\n\n        # If the user has specified a path for the Hadoop home, use it directly\n        if self._opts['hadoop_home']:\n            hadoop_home = self._opts['hadoop_home']\n            if os.", "idx": 1180}
{"namespace": "mrjob.hadoop.HadoopJobRunner._find_binaries_and_jars", "completion": "        # Find the hadoop binary\n        self._hadoop_bin = self.get_hadoop_bin()\n\n        # Find the hadoop streaming jar\n        self._hadoop_streaming_jar = self.get_hadoop_streaming_jar()\n\n        # Check if there are any Hadoop streaming steps in the job\n        self._has_hadoop_streaming_steps = any(\n            step.type == 'streaming' for step in self._steps)\n\n        # Check if there are any Spark steps in the job\n        self._has_spark_steps = any(\n            _is_spark_step_type(step.type) for step in self._steps)\n\n        # If there are Spark steps, find the Spark submit binary\n        if self._has_spark_steps:\n            self._spark_submit_bin = self._find_spark_submit_bin()\n", "idx": 1181}
{"namespace": "mrjob.hadoop.HadoopJobRunner._args_for_streaming_step", "completion": "        hadoop_streaming_jar = self.get_hadoop_streaming_jar()\n        if not hadoop_streaming_jar:\n            raise ValueError('no Hadoop streaming jar')\n\n        hadoop_bin = self.get_hadoop_bin()\n        hadoop_streaming_args = self._args_for_streaming_step(step_num)\n\n        return hadoop_bin + ['jar', hadoop_streaming_jar] + hadoop_streaming_args\n", "idx": 1182}
{"namespace": "mrjob.hadoop.HadoopJobRunner._stream_history_log_dirs", "completion": "        # yield all possible hadoop directories (used for history log)\n        for path in self._hadoop_log_dirs(output_dir):\n            log.info('Looking for history log in %s...' % path)\n            yield path\n", "idx": 1183}
{"namespace": "mrjob.hadoop.HadoopJobRunner._stream_task_log_dirs", "completion": "        if not self._read_logs():\n            return\n\n        for log_dir in unique(self._hadoop_log_dirs(output_dir=output_dir)):\n            if _logs_exist(self.fs, log_dir):\n                log.info('Looking for task logs in %s...' % log_dir)\n                if application_id:\n                    yield [posixpath.join(log_dir, 'userlogs', application_id)]\n                else:\n                    yield [posixpath.join(log_dir, 'userlogs')]\n", "idx": 1184}
{"namespace": "mrjob.setup.UploadDirManager.add", "completion": "        if is_uri(path):\n            return path\n\n        if path not in self._path_to_name:\n            name = name_uniquely(path, self._names_taken, unhide=True)\n            self._path_to_name[path] = name\n            self._names_taken.add(name)\n\n        return self.uri(path)\n", "idx": 1185}
{"namespace": "mrjob.setup.UploadDirManager.uri", "completion": "        if is_uri(path):\n            return path\n        elif path in self._path_to_name:\n            return posixpath.join(self.prefix, self._path_to_name[path])\n        else:\n            raise ValueError('%r is not a URI or a known local file' % path)\n", "idx": 1186}
{"namespace": "mrjob.setup.UploadDirManager.path_to_uri", "completion": "        return self._path_to_name\n\n", "idx": 1187}
{"namespace": "mrjob.setup.WorkingDirManager.name_to_path", "completion": "        if type is None:\n            return self._name_to_typed_path\n        else:\n            return dict((name, path) for name, (t, path) in self._name_to_typed_path.items() if t == type)\n", "idx": 1188}
{"namespace": "mrjob.setup.WorkingDirManager.paths", "completion": "        if type is not None:\n            self._check_type(type)\n\n        paths = set()\n\n        for path_type, path in self._typed_path_to_auto_name:\n            if type is None or path_type == type:\n                paths.add(path)\n\n        return paths\n", "idx": 1189}
{"namespace": "mrjob.compat.jobconf_from_env", "completion": "    if variable in os.environ:\n        return os.environ[variable]\n    elif variable in _JOBCONF_MAP:\n        return _JOBCONF_MAP[variable]\n    else:\n        return default\n\n", "idx": 1190}
{"namespace": "mrjob.compat.jobconf_from_dict", "completion": "    if name in jobconf:\n        return jobconf[name]\n\n    # try alternatives (arbitrary order)\n    for var in _JOBCONF_MAP.get(name, {}).values():\n        if var in jobconf:\n            return jobconf[var]\n\n    return default\n\n", "idx": 1191}
{"namespace": "mrjob.compat.translate_jobconf", "completion": "    if variable is None:\n        raise TypeError\n\n    if version is None:\n        raise TypeError\n\n    if version == '0.20':\n        return variable\n\n    if version == '0.21':\n        return variable\n\n    if version == '0.22':\n        return variable\n\n    if version == '0.23':\n        return variable\n\n    if version == '1.0':\n        return variable\n\n    if version == '1.1':\n        return variable\n\n    if version == '1.2':\n        return variable\n\n    if version == '2.0':\n        return variable\n\n    if version == '2.1':\n        return variable\n\n    if version == '2.2':\n        return variable\n\n    if version == '2.3':\n        return variable\n\n    if version == '2.4':\n        return variable\n\n    if version == '2.5':\n        return variable\n\n    if version == '2.6':\n        return variable\n\n    if version == '2.7':\n        return variable\n\n    if version == '3.0':\n        return variable\n\n    if version == '3.1':\n        return variable\n\n    if version == '3.2':\n        return variable\n\n    if version == '3.3':\n        return variable\n\n    if version == '3.4':\n        return variable\n\n    if version == '3.5':\n        return variable\n\n    if version == '3.6':\n        return variable\n\n    if version == '3.7':\n        return variable\n\n    if version == '4.0':\n        return variable\n\n    if version == '4.1':\n        return variable\n\n    if version == '4.2':\n        return variable\n\n    if version == '4.3':\n        return variable\n\n    if version == '4.4':\n        return variable\n\n    if version == '4.5':\n        return variable\n\n    if version == '4.6':\n        return variable\n\n    if version == '4.7':\n        return variable\n\n    if version == '5.0':\n        return variable\n\n    if version == '5.1':\n        return variable\n\n    if version == '5.2':\n        return variable\n\n    if version == '5.3':\n        return variable\n\n    if version == '5.4':\n        return variable\n\n    if version == '5.5':\n        return variable\n\n    if version == '5.6':\n        return variable\n\n    if version == '5.7':\n        return variable\n\n    if version == '6.0':\n       ", "idx": 1192}
{"namespace": "mrjob.compat.translate_jobconf_for_all_versions", "completion": "    if variable in _JOBCONF_MAP:\n        return sorted(_JOBCONF_MAP[variable].keys())\n    else:\n        return [variable]\n\n", "idx": 1193}
{"namespace": "mrjob.compat.translate_jobconf_dict", "completion": "    if hadoop_version is None:\n        raise TypeError\n\n    if not jobconf:\n        raise ValueError\n\n    # translated_jobconf = {\n    #   ...\n    #   a: {'1.0': a, '2.0': b}\n    #   ..\n    # }\n    translated_jobconf = {}\n\n    # warn_msg = \"Detected hadoop configuration property names that do not match version {hadoop_version}:\\nThe have been translated to the following names:\\n{translated_names}\"\n    warn_msg = \"Detected hadoop configuration property names that do not match version {hadoop_version}:\\nThe have been translated to the following names:\\n{translated_names}\"\n\n    # translated_names = [\n    #   ...\n    #   a:b\n    #   ..\n    # ]\n    translated_names = []\n\n    for name, value in jobconf.items():\n        translated_name = translate_jobconf(name, hadoop_version)\n        if translated_name != name:\n            translated_names.append(translated_name)\n        translated_jobconf[name] = translated_name\n\n    if translated_names:\n        print(warn_msg.format(hadoop_version=hadoop_version,\n                               translated_names='\\n'.join(sorted(translated_names))))\n\n    return dict(jobconf, **translated_jobconf)\n\n", "idx": 1194}
{"namespace": "mrjob.compat.uses_yarn", "completion": "    return LooseVersion(version) >= LooseVersion('2.0')\n\n", "idx": 1195}
{"namespace": "mrjob.local.LocalMRJobRunner._spark_master", "completion": "        num_executors = self._opts['num_executors']\n        cores_per_executor = self._opts['cores_per_executor']\n        executor_memory = self._opts['executor_memory']\n\n        if num_executors is None:\n            num_executors = 1\n        if cores_per_executor is None:\n            cores_per_executor = 1\n        if executor_memory is None:\n            executor_memory = _DEFAULT_EXECUTOR_MEMORY\n\n        # Round up executor memory to the nearest MB\n        executor_memory_mb = math.ceil(\n            self._parse_memory_string(executor_memory) / 1024 / 1024)\n\n        return 'local-cluster[%d,%d,%d]' % (\n            num_executors, cores_per_executor, executor_memory_mb)\n", "idx": 1196}
{"namespace": "mrjob.runner.MRJobRunner._bootstrap_mrjob", "completion": "        if self._opts.get('bootstrap_mrjob') is None:\n            return True\n\n        return self._opts['bootstrap_mrjob']\n", "idx": 1197}
{"namespace": "mrjob.conf._fix_clear_tags", "completion": "    if isinstance(x, list):\n        return [_fix_clear_tags(y) for y in x]\n    elif isinstance(x, dict):\n        cleared_keys = [k for k, v in x.items() if isinstance(v, ClearedValue)]\n        for k in cleared_keys:\n            x[k] = _fix_clear_tags(x[k].value)\n        return x\n    elif isinstance(x, ClearedValue):\n        return _fix_clear_tags(x.value)\n    else:\n        return x\n\n", "idx": 1198}
{"namespace": "mrjob.conf.load_opts_from_mrjob_conf", "completion": "    if already_loaded is None:\n        already_loaded = []\n\n    conf_path = _expanded_mrjob_conf_path(conf_path)\n\n    if conf_path is None:\n        return [(None, {})]\n\n    if conf_path in already_loaded:\n        return []\n\n    already_loaded.append(conf_path)\n\n    conf_object = _conf_object_at_path(conf_path)\n\n    if conf_object is None:\n        return []\n\n    if runner_alias not in conf_object:\n        return []\n\n    runner_conf = conf_object[runner_alias]\n\n    if 'include' in runner_conf:\n        include_paths = runner_conf['include']\n        if not isinstance(include_paths, list):\n            include_paths = [include_paths]\n\n        include_paths = [_expanded_mrjob_conf_path(p) for p in include_paths]\n\n        return [(conf_path, runner_conf)] + \\\n               sum([load_opts_from_mrjob_conf(runner_alias, p, already_loaded)\n                    for p in include_paths], [])\n\n    else:\n        return [(conf_path, runner_conf)]\n\n", "idx": 1199}
{"namespace": "mrjob.conf.load_opts_from_mrjob_confs", "completion": "    if conf_paths is None:\n        conf_paths = []\n\n    # load all configs\n    opts = []\n    for conf_path in conf_paths:\n        opts.extend(load_opts_from_mrjob_conf(runner_alias, conf_path))\n\n    # if no configs were loaded, try to load the default config\n    if not opts:\n        opts = load_opts_from_mrjob_conf(runner_alias)\n\n    # if no configs were loaded, log a warning message\n    if not opts:\n        log.warning('No config specified for %s runner', runner_alias)\n\n    return opts\n\n", "idx": 1200}
{"namespace": "mrjob.conf.dump_mrjob_conf", "completion": "    if yaml:\n        _dump_yaml_with_clear_tags(conf, f)\n    else:\n        json.dump(conf, f)\n\n", "idx": 1201}
{"namespace": "mrjob.conf.combine_lists", "completion": "    result = []\n    for seq in seqs:\n        if seq is None:\n            continue\n        if isinstance(seq, (str, bytes)):\n            seq = [seq]\n        elif not isinstance(seq, (list, tuple)):\n            raise TypeError('combine_lists() argument must be a sequence')\n        result.extend(seq)\n    return result\n\n", "idx": 1202}
{"namespace": "mrjob.conf.combine_cmds", "completion": "    for cmd in reversed(cmds):\n        if cmd is None:\n            continue\n        elif isinstance(cmd, (bytes, string_types)):\n            return shlex_split(cmd)\n        else:\n            return list(cmd)\n    else:\n        return None\n\n", "idx": 1203}
{"namespace": "mrjob.conf.combine_dicts", "completion": "    result = {}\n\n    for d in dicts:\n        if d is None:\n            continue\n\n        for k, v in d.items():\n            if isinstance(v, ClearedValue):\n                if v.value is None:\n                    if k in result:\n                        del result[k]\n                else:\n                    result[k] = v.value\n            else:\n                result[k] = v\n\n    return result\n\n", "idx": 1204}
{"namespace": "mrjob.conf.combine_jobconfs", "completion": "    result = {}\n\n    for jobconf in jobconfs:\n        if jobconf:\n            for k, v in jobconf.items():\n                # delete cleared key\n                if isinstance(v, ClearedValue) and v.value is None:\n                    result.pop(k, None)\n\n                # just set the value\n                else:\n                    result[k] = _strip_clear_tag(v)\n\n    return result\n\n", "idx": 1205}
{"namespace": "mrjob.conf.combine_path_lists", "completion": "    result = []\n\n    for path_seq in path_seqs:\n        if path_seq is None:\n            continue\n\n        if isinstance(path_seq, (bytes, string_types, dict)):\n            result.append(path_seq)\n        else:\n            try:\n                result.extend(path_seq)\n            except:\n                result.append(path_seq)\n\n    return result\n\n", "idx": 1206}
{"namespace": "mrjob.conf.combine_opts", "completion": "    # combiners generally consider earlier values to be defaults, and later\n    # options to override or add on to them.\n\n    # combiners assume that the only place ClearedValue appears is values in\n    # dicts (and in the top-level value we return).\n\n    # combiners assume that the only place ClearedValue appears is values in\n    # dicts (and in the top-level value we return).\n\n    # combiners assume that the only place ClearedValue appears is values in\n    # dicts (and in the top-level value we return).\n\n    # combiners assume that the only place ClearedValue appears is values in\n    # dicts (and in the top-level value we return).\n\n    # combiners assume that the only place ClearedValue appears is values in\n    # dicts (and in the top-level value we return).\n\n    # combiners assume that the only place ClearedValue appears is values in\n    # dicts (and in the top-level value we return).\n\n    # combiners assume that the only place ClearedValue appears is values in\n    # dicts (and in the top-level value we return).\n\n    # combiners assume that the only place ClearedValue appears is values in\n    # dicts (and in the top-level value we return).\n\n    # combiners assume that the only place ClearedValue appears is values in\n    # dicts (and in the top-level value we return).\n\n    # combiners assume that the only place ClearedValue appears is values in\n    # dicts (and in the top-level value we return).\n\n    # combiners assume that the only place ClearedValue appears is values in\n    # dicts (and in the top-level value we return).\n\n    # combiners assume that the only place ClearedValue appears is values in\n    # dicts (and in the top-level value we return).\n\n    # combiners assume that the only place ClearedValue appears is values in\n    # dicts (and in the top-level value we return).\n\n    # combiners assume that the only place ClearedValue appears is values in\n    # dicts (and in the top-level value we return).\n\n    # combiners assume that the only place ClearedValue appears is values in\n    # dicts (and in the top-", "idx": 1207}
{"namespace": "mrjob.bin.MRJobBinRunner._task_python_bin", "completion": "        return self._opts['task_python_bin'] or self._python_bin()\n", "idx": 1208}
{"namespace": "mrjob.bin.MRJobBinRunner.get_spark_submit_bin", "completion": "        if self._spark_submit_bin is None:\n            # search for the spark-submit binary\n            spark_submit_bin = which('spark-submit')\n            if spark_submit_bin is None:\n                raise Exception(\n                    \"Can't find spark-submit binary, can't run Spark jobs\")\n            self._spark_submit_bin = spark_submit_bin\n        return self._spark_submit_bin\n", "idx": 1209}
{"namespace": "mrjob.step.StepFailedException.__str__", "completion": "        if self.reason:\n            if self.step_desc:\n                return '%s failed: %s' % (self.step_desc, self.reason)\n            else:\n                return 'Step failed: %s' % self.reason\n        else:\n            if self.step_desc:\n                return '%s failed' % self.step_desc\n            else:\n                if self.step_num is not None:\n                    if self.num_steps is not None:\n                        if self.last_step_num is not None:\n                            return 'Steps %d-%d failed' % (self.step_num + 1, self.last_step_num + 1)\n                        else:\n                            return 'Step %d failed' % (self.step_num + 1)\n                    else:\n                        return 'Step %d failed' % (self.step_num + 1)\n                else:\n                    return 'Step failed'\n\n", "idx": 1210}
{"namespace": "mrjob.step.StepFailedException.__repr__", "completion": "        return '{class_name}({fields})'.format(\n            class_name=self.__class__.__name__,\n            fields=', '.join(\n                '{field_name}={field_value}'.format(\n                    field_name=field_name,\n                    field_value=getattr(self, field_name)\n                )\n                for field_name in self._FIELDS\n            )\n        )\n\n", "idx": 1211}
{"namespace": "mrjob.step.MRStep.description", "completion": "        desc = {'type': 'streaming'}\n\n        if (step_num == 0 or self.has_explicit_mapper or\n                self.has_explicit_combiner):\n            desc['mapper'] = self.render_mapper()\n\n        if self.has_explicit_combiner:\n            desc['combiner'] = self.render_combiner()\n\n        if self.has_explicit_reducer:\n            desc['reducer'] = self.render_reducer()\n\n        if self._steps['mapper_raw']:\n            desc['input_manifest'] = True\n\n        if self._steps['jobconf']:\n            desc['jobconf'] = self._steps['jobconf']\n\n        return desc\n\n", "idx": 1212}
{"namespace": "mrjob.step._Step.description", "completion": "        desc = {'type': self._STEP_TYPE}\n        for k in self._STEP_ATTRS:\n            if k not in self._HIDDEN_ATTRS:\n                desc[k] = getattr(self, k)\n        return desc\n\n", "idx": 1213}
{"namespace": "mrjob.protocol._KeyCachingProtocol.read", "completion": "        if line is None:\n            return None\n\n        if self._last_key_encoded is None:\n            key, value = line.split('\\t', 1)\n            self._last_key_encoded = key\n            self._last_key_decoded = self._loads(key)\n        else:\n            value = line\n            self._last_key_encoded = None\n            self._last_key_decoded = None\n\n        return self._last_key_decoded, self._loads(value)\n", "idx": 1214}
{"namespace": "mrjob.util.safeeval", "completion": "    # Create a dictionary of safe global variables\n    safe_globals = {\n        'True': True,\n        'False': False,\n        'None': None,\n        'set': set,\n        'range': range,\n        'xrange': xrange,\n        'open': lambda *args, **kwargs: NameError(\"name 'open' is not defined\")\n    }\n\n    # Update the safe global variables with the globals dictionary\n    if globals is not None:\n        safe_globals.update(globals)\n\n    # Evaluate the expression\n    return eval(expr, safe_globals, locals)\n\n", "idx": 1215}
{"namespace": "mrjob.util.to_lines", "completion": "    if hasattr(chunks, 'readline'):\n        return chunks\n\n    for chunk in chunks:\n        if not chunk:\n            continue\n\n        for line in chunk.splitlines(True):\n            yield line\n\n", "idx": 1216}
{"namespace": "mrjob.parse.is_s3_uri", "completion": "    try:\n        urlparse(uri)\n        return True\n    except ValueError:\n        return False\n\n", "idx": 1217}
{"namespace": "mrjob.parse.parse_s3_uri", "completion": "    if not is_s3_uri(uri):\n        raise ValueError('Not an S3 URI: %s' % uri)\n\n    parsed_uri = urlparse(uri)\n    bucket = parsed_uri.netloc\n    key = parsed_uri.path.lstrip('/')\n    return bucket, key\n\n", "idx": 1218}
{"namespace": "mrjob.parse.to_uri", "completion": "    if is_uri(path_or_uri):\n        return path_or_uri\n    else:\n        return 'file://' + abspath(path_or_uri)\n\n", "idx": 1219}
{"namespace": "mrjob.parse.parse_mr_job_stderr", "completion": "    if counters is None:\n        counters = {}\n\n    if isinstance(stderr, BytesIO):\n        stderr = stderr.getvalue()\n\n    if isinstance(stderr, bytes):\n        stderr = stderr.splitlines()\n\n    if isinstance(stderr, list):\n        stderr = iter(stderr)\n\n    counters_out = {}\n    statuses = []\n    other = []\n\n    for line in stderr:\n        m = _COUNTER_RE.match(line)\n        if m:\n            group, counter, value = m.groups()\n            counters_out.setdefault(group, {})[counter] = int(value)\n        else:\n            m = _STATUS_RE.match(line)\n            if m:\n                statuses.append(m.group(1))\n            else:\n                other.append(line)\n\n    for group, counters in counters_out.items():\n        counters_out[group] = dict(counters)\n\n    return {'counters': counters_out, 'statuses': statuses, 'other': other}", "idx": 1220}
{"namespace": "mrjob.parse._parse_progress_from_job_tracker", "completion": "    # Find the content between 'Running Jobs' and 'Jobs'\n    running_jobs_index = html_bytes.find(b'Running Jobs')\n    jobs_index = html_bytes.find(b'Jobs')\n    if running_jobs_index == -1 or jobs_index == -1:\n        return None, None\n    content = html_bytes[running_jobs_index:jobs_index]\n\n    # Find the map_percent and reduce_percent values\n    map_percent = None\n    reduce_percent = None\n    for line in content.splitlines():\n        m = _JOB_TRACKER_HTML_RE.search(line)\n        if m:\n            percent = float(m.group(1))\n            if map_percent is None:\n                map_percent = percent\n            else:\n                reduce_percent = percent\n\n    return map_percent, reduce_percent\n\n", "idx": 1221}
{"namespace": "mrjob.parse._parse_progress_from_resource_manager", "completion": "    # snip out the Running Jobs section (ignore the header)\n    start = html_bytes.rfind(b'Running Jobs')\n    if start == -1:\n        return None\n    end = html_bytes.find(b'Jobs', start + len(b'Running Jobs'))\n    if end == -1:\n        end = None\n\n    html_bytes = html_bytes[start:end]\n\n    # search it for percents\n    matches = _RESOURCE_MANAGER_JS_RE.findall(html_bytes)\n    if len(matches) >= 1:\n        return float(matches[0])\n    else:\n        return None\n\n", "idx": 1222}
{"namespace": "mrjob.logs.task._match_task_log_path", "completion": "    # check if the path is a task log path\n    match = _match_task_log_path_helper(path)\n\n    if match is None:\n        return None\n\n    # check if the application ID or attempt ID matches the passed application ID or job ID\n    if application_id is not None and match.get('application_id') != application_id:\n        return None\n    if job_id is not None and match.get('attempt_id') != job_id:\n        return None\n\n    return match\n\n", "idx": 1223}
{"namespace": "mrjob.logs.task._parse_task_syslog", "completion": "    result = {}\n\n    # check if we should look for a split\n    if lines[0].startswith('Processing split:'):\n        m = _YARN_INPUT_SPLIT_RE.match(lines[0])\n        if m:\n            result['split'] = dict(\n                path=m.group('path'),\n                start_line=int(m.group('start_line')),\n                num_lines=int(m.group('num_lines')))\n\n    # look for a hadoop error\n    for i, line in enumerate(lines):\n        if _SUBPROCESS_FAILED_STACK_TRACE_START.match(line):\n            # found a hadoop error\n            result['hadoop_error'] = dict(\n                message=' '.join(lines[i:]),\n                start_line=i,\n                num_lines=len(lines) - i)\n            break\n\n    return result\n\n", "idx": 1224}
{"namespace": "mrjob.logs.ids._sort_for_spark", "completion": "    return sorted(ds, key=_time_sort_key, reverse=True)\n\n", "idx": 1225}
{"namespace": "mrjob.logs.spark._parse_spark_log", "completion": "    # if we're given a single string, convert it to a list of lines\n    if isinstance(lines, str):\n        lines = lines.splitlines()\n\n    # if we're given a file-like object, read it into a list of lines\n    if hasattr(lines, 'read'):\n        lines = lines.read().splitlines()\n\n    # if we're given a list of lines, convert it to a generator\n    if isinstance(lines, list):\n        lines = (line for line in lines)\n\n    # if we're given a generator, wrap it in a function that can be called\n    # multiple times\n    if isinstance(lines, (types.GeneratorType, types.GeneratorAdapter)):\n        lines = _cat_log_lines(lines)\n\n    # if we're given a function, wrap it in a generator\n    if callable(lines):\n        lines = (line for line in lines())\n\n    # if we're given a generator, wrap it in a function that can be called\n    # multiple times\n    if isinstance(lines, (types.GeneratorType, types.GeneratorAdapter)):\n        lines = _cat_log_lines(lines)\n\n    # if we're given a function, wrap it in a generator\n    if callable(lines):\n        lines = (line for line in lines())\n\n    # if we're given a generator, wrap it in a function that can be called\n    # multiple times\n    if isinstance(lines, (types.GeneratorType, types.GeneratorAdapter)):\n        lines = _cat_log_lines(lines)\n\n    # if we're given a function, wrap it in a generator\n    if callable(lines):\n        lines = (line for line in lines())\n\n    # if we're given a generator, wrap it in a function that can be called\n    # multiple times\n    if isinstance(lines, (types.GeneratorType, types.GeneratorAdapter)):\n        lines = _cat_log_lines(lines)\n\n    # if we're given a function, wrap it in a generator\n    if callable(lines):\n        lines = (line for line in lines())\n\n    # if we're given a generator, wrap it in a function that can be called\n    # multiple times\n    if isinstance(lines, (types.GeneratorType, types.GeneratorAdapter)):\n        lines = _cat_log_lines(lines)\n\n    # if we're given a function, wrap it in a generator\n    if callable(lines):", "idx": 1226}
{"namespace": "mrjob.logs.mixin.LogInterpretationMixin._pick_error", "completion": "        if self._step_type_uses_spark(step_type):\n            return None\n\n        if self._read_logs():\n            log.info('Scanning logs for probable cause of failure...')\n            self._interpret_step_logs(log_interpretation, step_type)\n            self._interpret_history_log(log_interpretation)\n\n        return _format_counters(log_interpretation.get('counters'))\n", "idx": 1227}
{"namespace": "mrjob.logs.history._match_history_log_path", "completion": "    m = _HISTORY_LOG_PATH_RE.match(path)\n    if m:\n        if job_id is None or m.group('job_id') == job_id:\n            return {'job_id': m.group('job_id'),\n                    'yarn': '.jhist' in m.group('suffix')}\n\n", "idx": 1228}
{"namespace": "mrjob.logs.history._parse_pre_yarn_history_log", "completion": "    result = {}\n    task_to_counters = {}  # used for successful tasks in failed jobs\n\n    for line_num, line in enumerate(lines):\n        # empty space or \"Avro-Json\" header\n        if not line.startswith('{'):\n            continue\n\n        try:\n            record = json.loads(line)\n        except:\n            continue\n\n        record_type = record.get('type')\n        if not isinstance(record_type, string_types):\n            continue\n\n        # extract events. Looks like there's just one per record\n        event_record = record.get('event')\n        if not isinstance(event_record, dict):\n            continue\n        events = [e for e in record['event'].values()\n                  if isinstance(e, dict)]\n\n        # update container_id -> attempt_id mapping\n        for event in events:\n            if 'attemptId' in event and 'containerId' in event:\n                result.setdefault('attempt_to_container_id', {})\n                result['attempt_to_container_id'][\n                    event['attemptId']] = event['containerId']\n\n        if record_type.endswith('_ATTEMPT_FAILED'):\n            for event in events:\n                err_msg = event.get('error')\n                if not (err_msg and isinstance(err_msg, string_types)):\n                    continue\n\n                error = dict(\n                    hadoop_error=dict(\n                        message=err_msg,\n                        start_line=line_num,\n                        num_lines=1))\n\n                if isinstance(event.get('taskid'), string_types):\n                    error['task_id'] = event['taskid']\n\n                if isinstance(event.get('attemptId'), string_types):\n                    error['attempt_id'] = event['attemptId']\n\n                result.setdefault('errors', [])\n                result['errors'].append(error)\n\n        elif record_type == 'TASK_FINISHED':\n            for event in events:\n                task_id = event.get('taskid')\n                if not isinstance(task_id, string_types):\n                    continue\n\n                counters_record = event.get('counters')\n                if not isinstance(counters_record, dict):\n                    continue\n\n                task_to_counters[task_id] = _extract_pre_yarn_counters(\n                    counters_record)\n\n        elif record_type == 'JOB_FINISHED':\n            for event in events:\n                # map", "idx": 1229}
{"namespace": "mrjob.logs.history._parse_pre_yarn_history_records", "completion": "    for line_num, line in enumerate(lines):\n        # empty space or \"Avro-Json\" header\n        if not line.strip():\n            continue\n\n        # match the record type and fields\n        match = _PRE_YARN_HISTORY_RECORD.match(line)\n        if not match:\n            continue\n\n        # extract the type and fields\n        record_type = match.group('type')\n        fields = _parse_pre_yarn_history_key_pairs(match.group('key_pairs'))\n\n        # if the record is a multi-line record, we need to read the next lines\n        if record_type in ('MapAttempt', 'ReduceAttempt'):\n            num_lines = 1\n            while True:\n                line_num += 1\n                line = lines[line_num]\n                match = _PRE_YARN_HISTORY_RECORD.match(line)\n                if not match:\n                    continue\n                num_lines += 1\n                if match.group('type') == record_type:\n                    fields.update(_parse_pre_yarn_history_key_pairs(match.group('key_pairs')))\n                else:\n                    break\n\n        yield dict(type=record_type, fields=fields, num_lines=num_lines, start_line=line_num)\n\n", "idx": 1230}
{"namespace": "mrjob.logs.step._parse_step_syslog", "completion": "    # The following variables are used to store the information found in the syslog.\n    application_id = None\n    counters = None\n    errors = []\n    job_id = None\n    output_dir = None\n\n    # The following variables are used to store the information found in the syslog.\n    # The following variables are used to store the information found in the syslog.\n    # The following variables are used to store the information found in the syslog.\n    # The following variables are used to store the information found in the syslog.\n    # The following variables are used to store the information found in the syslog.\n    # The following variables are used to store the information found in the syslog.\n    # The following variables are used to store the information found in the syslog.\n    # The following variables are used to store the information found in the syslog.\n    # The following variables are used to store the information found in the syslog.\n    # The following variables are used to store the information found in the syslog.\n    # The following variables are used to store the information found in the syslog.\n    # The following variables are used to store the information found in the syslog.\n    # The following variables are used to store the information found in the syslog.\n    # The following variables are used to store the information found in the syslog.\n    # The following variables are used to store the information found in the syslog.\n    # The following variables are used to store the information found in the syslog.\n    # The following variables are used to store the information found in the syslog.\n    # The following variables are used to store the information found in the syslog.\n    # The following variables are used to store the information found in the syslog.\n    # The following variables are used to store the information found in the syslog.\n    # The following variables are used to store the information found in the syslog.\n    # The following variables are used to store the information found in the syslog.\n    # The following variables are used to store the information found in the syslog.\n    # The following variables are used to store the information found in the syslog.\n    # The following variables are used to store the information found in the syslog.\n    # The following variables are used to store the information found in the syslog.\n    # The following variables are used to store the information found in the syslog.\n    # The following variables are used to store the information found in the syslog.\n    # The following variables are used to store the", "idx": 1231}
{"namespace": "mrjob.logs.errors._merge_and_sort_errors", "completion": "    # Initialize a dictionary to save errors\n    merged_errors = {}\n\n    # Iterate through each error in the given list of errors\n    for error in errors:\n        # If the error has a container id, use it as the key\n        if error.get('container_id'):\n            key = error['container_id']\n        # If the error does not have a container id, generate a key based on the error's time\n        else:\n            key = error['time']\n\n        # If the key already exists in the dictionary, merge the error with the existing one\n        if key in merged_errors:\n            merged_errors[key] = _merge_errors(merged_errors[key], error)\n        # Otherwise, add the error to the dictionary with the key\n        else:\n            merged_errors[key] = error\n\n    # Convert the dictionary to a list of errors\n    merged_errors = list(merged_errors.values())\n\n    # Sort the errors based on their keys\n    merged_errors.sort(key=_error_key_sort_function(attempt_to_container_id), reverse=True)\n\n    return merged_errors\n\n", "idx": 1232}
{"namespace": "mrjob.fs.ssh.SSHFilesystem.ls", "completion": "        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.\n        # TODO: This function is not tested.", "idx": 1233}
{"namespace": "mrjob.fs.ssh.SSHFilesystem._cat_file", "completion": "        m = _SSH_URI_RE.match(path)\n        addr = m.group('hostname')\n        path_to_cat = m.group('filesystem_path')\n\n        p = self._ssh_launch(addr, ['cat', path_to_cat])\n\n        for line in p.stdout:\n            yield line\n\n        self._ssh_finish_run(p)\n", "idx": 1234}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.get_hadoop_bin", "completion": "        if self._hadoop_bin is None:\n            self._hadoop_bin = which('hadoop')\n\n        return self._hadoop_bin\n", "idx": 1235}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.du", "completion": "        # Check if the path exists\n        if not self.exists(path_glob):\n            return 0\n\n        # Execute the \"fs -du\" command\n        args = self.get_hadoop_bin() + ['fs', '-du', path_glob]\n        proc = Popen(args, stdout=PIPE, stderr=PIPE)\n        stdout, stderr = proc.communicate()\n\n        # Parse the output to get the size\n        size = 0\n        for line in BytesIO(stdout):\n            line = to_unicode(line.rstrip(b'\\r\\n'))\n            try:\n                size += int(line.split()[0])\n            except ValueError:\n                raise IOError('Unexpected output from Hadoop fs -du: {output!r}'.format(output=line))\n\n        return size\n", "idx": 1236}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.mkdir", "completion": "        # use -p on Hadoop 2 (see #1152)\n        version = self.get_hadoop_version()\n        if uses_yarn(version):\n            args = ['fs', '-mkdir', '-p', path]\n        else:\n            args = ['fs', '-mkdir', path]\n\n        try:\n            self.invoke_hadoop(args, ok_stderr=[_HADOOP_FILE_EXISTS_RE])\n        except CalledProcessError:\n            raise IOError(\"Could not mkdir %s\" % path)\n", "idx": 1237}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.exists", "completion": "        try:\n            stdout = self.invoke_hadoop(['fs', '-ls', path_glob],\n                                        return_stdout=True,\n                                        ok_stderr=[_HADOOP_LS_NO_SUCH_FILE])\n        except CalledProcessError:\n            return False\n\n        if stdout:\n            return True\n        else:\n            return False\n", "idx": 1238}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.rm", "completion": "        # don't inadvertently support cp syntax\n        if path_glob.endswith('/'):\n            raise ValueError('rm() destination may not be a directory')\n\n        # check if the path is a URI\n        if is_uri(path_glob):\n            # if it is, use the hadoop fs -rm command\n            version = self.get_hadoop_version()\n\n            # use -r on Hadoop 2 (see #991, #845)\n            if uses_yarn(version):\n                args = ['fs', '-rm', '-r', path_glob]\n            else:\n                args = ['fs', '-rmr', path_glob]\n\n            try:\n                self.invoke_hadoop(args, ok_stderr=[_HADOOP_RM_NO_SUCH_FILE])\n            except CalledProcessError:\n                raise IOError(\"Could not rm %s\" % path_glob)\n        else:\n            # if it is not, use the superclass to remove the path\n            super(HadoopFilesystem, self).rm(path_glob)\n", "idx": 1239}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.touchz", "completion": "        try:\n            self.invoke_hadoop(['fs', '-touchz', path])\n        except CalledProcessError:\n            raise IOError(\"Could not touchz %s\" % path)", "idx": 1240}
{"namespace": "mrjob.fs.local.LocalFilesystem.du", "completion": "        path = self._path_to_local(path_glob)\n        total_size = 0\n        for file in glob.glob(path):\n            total_size += os.path.getsize(file)\n        return total_size\n", "idx": 1241}
{"namespace": "mrjob.fs.local.LocalFilesystem.ls", "completion": "        path_glob = _from_file_uri(path_glob)\n        if os.path.isdir(path_glob):\n            for root, dirs, files in os.walk(path_glob):\n                for file in files:\n                    yield _to_file_uri(os.path.join(root, file))\n        else:\n            yield _to_file_uri(path_glob)\n", "idx": 1242}
{"namespace": "mrjob.fs.local.LocalFilesystem._cat_file", "completion": "        path = _from_file_uri(path)\n        with open(path, 'rb') as f:\n            while True:\n                chunk = f.read(1024)\n                if chunk:\n                    yield chunk\n                else:\n                    break\n", "idx": 1243}
{"namespace": "mrjob.fs.local.LocalFilesystem.exists", "completion": "        path_glob = _from_file_uri(path_glob)\n        return any(self.ls(path_glob))\n", "idx": 1244}
{"namespace": "mrjob.fs.local.LocalFilesystem.mkdir", "completion": "        path = _from_file_uri(path)\n        if not os.path.exists(path):\n            os.makedirs(path)\n", "idx": 1245}
{"namespace": "mrjob.fs.local.LocalFilesystem.put", "completion": "        src = _from_file_uri(src)\n        path = _from_file_uri(path)\n        if os.path.isdir(path):\n            path = os.path.join(path, os.path.basename(src))\n        shutil.copy(src, path)\n", "idx": 1246}
{"namespace": "mrjob.fs.local.LocalFilesystem.rm", "completion": "        path_glob = _from_file_uri(path_glob)\n        for path in glob.glob(path_glob):\n            if os.path.isdir(path):\n                shutil.rmtree(path)\n            else:\n                os.remove(path)\n", "idx": 1247}
{"namespace": "mrjob.fs.local.LocalFilesystem.touchz", "completion": "        path = _from_file_uri(path)\n        if os.path.exists(path) and os.path.getsize(path) > 0:\n            raise OSError('File already exists and is not empty')\n        else:\n            open(path, 'a').close()\n", "idx": 1248}
{"namespace": "mrjob.fs.local.LocalFilesystem.md5sum", "completion": "        path = _from_file_uri(path)\n        with open(path, 'rb') as f:\n            return self._md5sum_file(f)\n", "idx": 1249}
{"namespace": "mrjob.fs.composite.CompositeFilesystem.add_fs", "completion": "        setattr(self, name, fs)\n        self._fs_names.append(name)\n        self._disable_if[name] = disable_if\n", "idx": 1250}
{"namespace": "mrjob.fs.base.Filesystem.cat", "completion": "        raise NotImplementedError\n", "idx": 1251}
{"namespace": "mrjob.fs.base.Filesystem.join", "completion": "        if self.is_uri(path):\n            scheme, netloc, path = self.split_uri(path)\n            return self.make_uri(scheme, netloc, posixpath.join(path, *paths))\n        else:\n            return os.path.join(path, *paths)\n", "idx": 1252}
{"namespace": "mrjob.examples.mr_text_classifier.parse_doc_filename", "completion": "    # Extract the filename from the input URI\n    filename = posixpath.basename(input_uri)\n\n    # Split the filename into parts using the \"-\" character\n    parts = filename.split('-')\n\n    # Extract the id from the first part of the filename\n    id = parts[0]\n\n    # Initialize a dictionary to store the categories\n    cats = {}\n\n    # Iterate over the remaining parts of the filename\n    for part in parts[1:]:\n        # Check if the part starts with \"not_\"\n        if part.startswith('not_'):\n            # If it does, remove the \"not_\" prefix and set the corresponding category to False\n            cat = part[4:]\n            cats[cat] = False\n        else:\n            # Otherwise, set the corresponding category to True\n            cats[part] = True\n\n    # Return a dictionary containing the id and categories\n    return {'id': id, 'cats': cats}\n\n", "idx": 1253}
{"namespace": "prometheus_client.mmap_dict.MmapedDict.read_value", "completion": "        with self._lock:\n            if key not in self._positions:\n                self._init_value(key)\n            pos = self._positions[key]\n            value, timestamp = _unpack_two_doubles(self._m, pos)\n            return value, timestamp\n", "idx": 1254}
{"namespace": "prometheus_client.mmap_dict.MmapedDict.write_value", "completion": "        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        _pack_two_doubles(self._m, pos, value, timestamp)\n", "idx": 1255}
{"namespace": "prometheus_client.multiprocess.MultiProcessCollector.merge", "completion": "        metrics = {}\n        for f in files:\n            with open(f, 'r') as fp:\n                for line in fp:\n                    if line.startswith('#'):\n                        continue\n                    metric_type, metric_name, labels, value = line.split()\n                    if metric_name not in metrics:\n                        metrics[metric_name] = Metric(metric_name, metric_type)\n                    metrics[metric_name].add_sample(Sample(metric_name, labels, float(value)))\n        for metric in metrics.values():\n            metric.flush(accumulate)\n        return metrics\n", "idx": 1256}
{"namespace": "prometheus_client.multiprocess.MultiProcessCollector.collect", "completion": "        files = glob.glob(os.path.join(self._path, '*.db'))\n        metrics = self.merge(files)\n        return metrics", "idx": 1257}
{"namespace": "prometheus_client.exposition.choose_encoder", "completion": "    if accept_header and 'application/openmetrics-text' in accept_header:\n        return generate_latest, 'application/openmetrics-text; charset=utf-8'\n    else:\n        return generate_latest, CONTENT_TYPE_LATEST\n\n", "idx": 1258}
{"namespace": "flower.command.apply_options", "completion": "    # Filter the options that are specific to the application.\n    options_to_filter = ['conf', 'broker_api', 'broker_api_ssl', 'broker_api_ssl_ca_certs', 'broker_api_ssl_certfile', 'broker_api_ssl_keyfile', 'broker_api_ssl_version', 'broker_api_ssl_cert_reqs', 'broker_api_ssl_ciphers', 'broker_api_ssl_check_hostname', 'broker_read_timeout', 'broker_connection_max_retries', 'broker_connection_retry_delay', 'broker_failover_strategy', 'broker_heartbeat', 'broker_login_method', 'broker_use_ssl', 'broker_transport_options', 'broker_transport_option', 'broker_transport_option_separator', 'broker_failover_strategy', 'broker_failover_strategy_separator', 'broker_failover_strategy_separator', 'broker_failover_strategy_separator', 'broker_failover_strategy_separator', 'broker_failover_strategy_separator', 'broker_failover_strategy_separator', 'broker_failover_strategy_separator', 'broker_failover_strategy_separator', 'broker_failover_strategy_separator', 'broker_failover_strategy_separator', 'broker_failover_strategy_separator', 'broker_failover_strategy_separator', 'broker_failover_strategy_separator', 'broker_failover_strategy_separator', 'broker_failover_strategy_separator', 'broker_failover_strategy_separator', 'broker_failover_strategy_separator', 'broker_failover_strategy_separator', 'broker_failover_strategy_separator', 'broker_failover_strategy_separator', 'broker_failover_strategy_separator', 'broker_failover_strategy_separator', 'broker_failover_strategy_separator', 'broker_failover_strategy_separator', 'broker_failover_strategy_separator', 'broker_failover_strategy_separator', 'broker_failover_strategy_separator', 'broker_failover_strategy_separator', 'broker_failover_strategy_separator', 'broker_failover_strategy_separator', 'broker_failover_strategy_separator', '", "idx": 1259}
{"namespace": "trackerjacker.ieee_mac_vendor_db.MacVendorDB.lookup", "completion": "        mac = mac.upper().replace(':', '')\n        for prefix in self.db:\n            if mac.startswith(prefix):\n                return self.db[prefix]\n        return ''", "idx": 1260}
{"namespace": "trailscraper.iam.Statement.merge", "completion": "        if self.Effect != other.Effect:\n            raise ValueError(\"Trying to combine two statements with differing effects: {self effect} {other's effect}\".format(\n                self=self.Effect, other=other.Effect))\n\n        merged_actions = sorted(list(set(self.Action + other.Action)))\n        merged_resources = sorted(list(set(self.Resource + other.Resource)))\n\n        return Statement(merged_actions, self.Effect, merged_resources)\n\n", "idx": 1261}
{"namespace": "trailscraper.iam.parse_policy_document", "completion": "    if isinstance(stream, six.string_types):\n        json_data = json.loads(stream)\n    else:\n        json_data = json.load(stream)\n\n    statements = _parse_statements(json_data['Statement'])\n    version = json_data['Version']\n\n    return PolicyDocument(statements, version)\n\n", "idx": 1262}
{"namespace": "trailscraper.iam.known_iam_actions", "completion": "    all_known_iam_actions = all_known_iam_permissions()\n    known_iam_actions_by_prefix = groupbyz(lambda action: action.split(\":\")[0], all_known_iam_actions)\n    return [Action(prefix=prefix, action=action.split(\":\")[1]) for action in known_iam_actions_by_prefix[prefix]]\n\n", "idx": 1263}
{"namespace": "trailscraper.boto_service_definitions.service_definition_file", "completion": "    files = boto_service_definition_files()\n    files = [file for file in files if fnmatch.fnmatch(file, '**/' + servicename + '/*/service-*.json')]\n    files.sort()\n    return files[-1]\n\n", "idx": 1264}
{"namespace": "trailscraper.boto_service_definitions.operation_definition", "completion": "    service_definition_file_path = service_definition_file(servicename)\n    with open(service_definition_file_path) as f:\n        service_definition = json.load(f)\n\n    return service_definition['operations'][operationname]\n\n", "idx": 1265}
{"namespace": "trailscraper.cloudtrail.Record.to_statement", "completion": "        if self.event_source == \"sts.amazonaws.com\" and self.event_name == \"GetCallerIdentity\":\n            return None\n\n        if self.event_source == \"apigateway.amazonaws.com\":\n            return self._to_api_gateway_statement()\n\n        iam_prefix = self._source_to_iam_prefix()\n        iam_action = self._event_name_to_iam_action()\n\n        return Statement(\n            Effect=\"Allow\",\n            Action=[Action(iam_prefix, iam_action)],\n            Resource=self.resource_arns,\n            Condition=self.assumed_role_arn\n        )\n\n", "idx": 1266}
{"namespace": "trailscraper.cloudtrail.filter_records", "completion": "    return pipe(records,\n                filterz(_by_timeframe(from_date, to_date)),\n                filterz(_by_role_arns(arns_to_filter_for)))\n\n", "idx": 1267}
{"namespace": "trailscraper.record_sources.local_directory_record_source.LocalDirectoryRecordSource.load_from_dir", "completion": "        def _is_in_range(log_file):\n            return log_file.is_in_range(from_date, to_date)\n\n        return pipe(self._valid_log_files(),\n                    filterz(_is_in_range),\n                    mapcatz(LogFile.records),\n                    sortedz(key=lambda record: record.event_time))\n", "idx": 1268}
{"namespace": "pyt.__main__.discover_files", "completion": "    included_files = []\n    for target in targets:\n        if os.path.isfile(target):\n            included_files.append(target)\n        else:\n            for root, dirs, files in os.walk(target):\n                for file in files:\n                    if file.endswith('.py'):\n                        included_files.append(os.path.join(root, file))\n\n    if excluded_files:\n        excluded_files = excluded_files.split(',')\n        included_files = [f for f in included_files if f not in excluded_files]\n\n    return included_files\n\n", "idx": 1269}
{"namespace": "pyt.core.project_handler.get_directory_modules", "completion": "    global _local_modules\n    if _local_modules and _local_modules[0][1].startswith(directory):\n        return _local_modules\n\n    if not os.path.isdir(directory):\n        directory = os.path.dirname(directory)\n\n    for file in os.listdir(directory):\n        if file.endswith(\".py\"):\n            module_name = file[:-3]\n            _local_modules.append((module_name, os.path.join(directory, file)))\n\n    return _local_modules\n\n", "idx": 1270}
{"namespace": "pyt.vulnerabilities.vulnerabilities.find_triggers", "completion": "    trigger_nodes = []\n    for node in nodes:\n        if node.line_no not in nosec_lines:\n            for trigger_word in trigger_words:\n                if trigger_word.name in node.label:\n                    trigger_nodes.append(TriggerNode(trigger_word, node))\n    return trigger_nodes\n\n", "idx": 1271}
{"namespace": "pyt.vulnerabilities.vulnerabilities.label_contains", "completion": "    for trigger in triggers:\n        if trigger.name in node.label:\n            yield TriggerNode(trigger, node)\n\n", "idx": 1272}
{"namespace": "pyt.vulnerabilities.vulnerabilities.build_sanitiser_node_dict", "completion": "    sanitisers = set()\n    for sink in sinks_in_file:\n        sanitisers.add(sink.trigger.sanitiser)\n\n    sanitiser_node_dict = defaultdict(list)\n    for sanitiser in sanitisers:\n        sanitiser_node_dict[sanitiser] = find_triggers(cfg.nodes, [sanitiser], set())\n\n    return sanitiser_node_dict\n\n", "idx": 1273}
{"namespace": "pyt.vulnerabilities.trigger_definitions_parser.parse", "completion": "    with open(trigger_word_file) as f:\n        data = json.load(f)\n\n    sources = [Source(trigger_word=trigger) for trigger in data['sources']]\n    sinks = [Sink.from_json(key, data) for key, data in data['sinks'].items()]\n\n    return Definitions(sources=sources, sinks=sinks)", "idx": 1274}
{"namespace": "principalmapper.querying.local_policy_simulation._statement_matches_resource", "completion": "    if 'Resource' in statement:\n        for item in _listify_string(statement['Resource']):\n            if _matches_after_expansion(resource, item, condition_keys):\n                return True\n    elif 'NotResource' in statement:\n        for item in _listify_string(statement['NotResource']):\n            if _matches_after_expansion(resource, item, condition_keys):\n                return False\n    return True\n\n", "idx": 1275}
{"namespace": "principalmapper.querying.local_policy_simulation._matches_after_expansion", "completion": "    if condition_keys is None:\n        condition_keys = CaseInsensitiveDict()\n\n    if string_to_check == string_to_check_against:\n        return True\n\n    if '${' in string_to_check_against:\n        for key in condition_keys:\n            if '${' + key + '}' in string_to_check_against:\n                string_to_check_against = string_to_check_against.replace('${' + key + '}', condition_keys[key])\n\n    if '*' in string_to_check_against or '?' in string_to_check_against:\n        return _compose_pattern(string_to_check).match(string_to_check_against)\n\n    return string_to_check == string_to_check_against\n\n", "idx": 1276}
{"namespace": "passpie.database.PasspieStorage.delete", "completion": "        for credential in credentials:\n            credpath = self.make_credpath(credential[\"name\"], credential[\"login\"])\n            os.remove(credpath)\n            dirname = os.path.dirname(credpath)\n            if not os.listdir(dirname):\n                os.rmdir(dirname)\n", "idx": 1277}
{"namespace": "passpie.database.PasspieStorage.read", "completion": "        credentials = []\n        for dirname, _, filenames in os.walk(self.path):\n            for filename in filenames:\n                if filename.endswith(self.extension):\n                    credpath = os.path.join(dirname, filename)\n                    with open(credpath, \"r\") as f:\n                        cred = yaml.safe_load(f)\n                        cred[\"name\"] = os.path.basename(dirname)\n                        cred[\"login\"] = os.path.splitext(filename)[0]\n                        credentials.append(cred)\n        return credentials\n", "idx": 1278}
{"namespace": "threatingestor.state.State.save_state", "completion": "        self.cursor.execute('INSERT OR REPLACE INTO states VALUES (?, ?)', (name, state))\n        self.conn.commit()\n\n", "idx": 1279}
{"namespace": "threatingestor.state.State.get_state", "completion": "        self.cursor.execute('SELECT state FROM states WHERE name = ?', (name,))\n        result = self.cursor.fetchone()\n        if result:\n            return result[0]\n        else:\n            return None\n\n", "idx": 1280}
{"namespace": "threatingestor.Ingestor.run", "completion": "        if self.config.daemon():\n            logger.info(\"Running as a daemon\")\n            while True:\n                self._run_once()\n                time.sleep(self.config.interval())\n        else:\n            logger.info(\"Running once\")\n            self._run_once()\n", "idx": 1281}
{"namespace": "msticpy.analysis.anomalous_sequence.model.Model.compute_scores", "completion": "        self.session_likelihoods = {}\n        self.session_geomean_likelihoods = {}\n        self.rare_windows = {}\n        self.rare_window_likelihoods = {}\n        self.rare_windows_geo = {}\n        self.rare_window_likelihoods_geo = {}\n\n        for i, session in enumerate(self.sessions):\n            if use_start_end_tokens:\n                session = [self.start_token] + session + [self.end_token]\n            self.session_likelihoods[i] = probabilities(\n                session,\n                self.prior_probs,\n                self.trans_probs,\n                self.param_probs,\n                self.param_cond_cmd_probs,\n                self.value_probs,\n                self.value_cond_param_probs,\n                self.set_params_cond_cmd_probs,\n            )\n            self.session_geomean_likelihoods[i] = probabilities(\n                session,\n                self.prior_probs,\n                self.trans_probs,\n                self.param_probs,\n                self.param_cond_cmd_probs,\n                self.value_probs,\n                self.value_cond_param_probs,\n                self.set_params_cond_cmd_probs,\n                geometric_mean=True,\n            )\n            self.rare_windows[i], self.rare_window_likelihoods[i] = probabilities(\n                session,\n                self.prior_probs,\n                self.trans_probs,\n                self.param_probs,\n                self.param_cond_cmd_probs,\n                self.value_probs,\n                self.value_cond_param_probs,\n                self.set_params_cond_cmd_probs,\n                rare_window=True,\n                window_length=2,\n            )\n            self.rare_windows_geo[i], self.rare_window_likelihoods_geo[i] = probabilities(\n                session,\n                self.prior_probs,\n                self.trans_probs,\n                self.param_probs,\n                self.param_cond_cmd_probs,\n                self.value_probs,\n                self.value_cond_param_probs,\n                self.set_params_cond_cmd_probs,\n                rare_window=True,\n                window_length=2,", "idx": 1282}
{"namespace": "msticpy.analysis.anomalous_sequence.model.Model.compute_rarest_windows", "completion": "        if self.session_likelihoods is None:\n            self.compute_likelihoods_of_sessions(\n                use_start_end_tokens=use_start_end_tokens\n            )\n\n        if use_geo_mean:\n            if self.session_geomean_likelihoods is None:\n                self.compute_geomean_lik_of_sessions()\n            result = defaultdict(lambda: [])\n            for idx, session in enumerate(self.sessions):\n                tmp = probabilities.compute_rarest_window(\n                    window=session,\n                    window_len=window_len,\n                    likelihoods=self.session_geomean_likelihoods,\n                    use_start_token=use_start_end_tokens,\n                    use_end_token=use_start_end_tokens,\n                    start_token=self.start_token,\n                    end_token=self.end_token,\n                )\n                result[idx].append(tmp)\n            self.rarest_windows_geo = result\n        else:\n            result = defaultdict(lambda: [])\n            for idx, session in enumerate(self.sessions):\n                tmp = probabilities.compute_rarest_window(\n                    window=session,\n                    window_len=window_len,\n                    likelihoods=self.session_likelihoods,\n                    use_start_token=use_start_end_tokens,\n                    use_end_token=use_start_end_tokens,\n                    start_token=self.start_token,\n                    end_token=self.end_token,\n                )\n                result[idx].append(tmp)\n            self.rarest_windows = result\n", "idx": 1283}
{"namespace": "msticpy.analysis.anomalous_sequence.anomalous.score_sessions", "completion": "    # Create a new DataFrame to store the results\n    results = pd.DataFrame()\n\n    # Iterate over the sessions\n    for session in data[session_column].unique():\n        # Filter the data for the current session\n        session_data = data[data[session_column] == session]\n\n        # Compute the likelihood metrics for the current session\n        likelihood = compute_likelihood(session_data, window_length)\n\n        # Compute the rarest window for the current session\n        rarest_window = compute_rarest_window(session_data, window_length)\n\n        # Add the results for the current session to the results DataFrame\n        results = results.append(\n            {\n                session_column: session,\n                \"likelihood\": likelihood,\n                \"rarest_window\": rarest_window,\n            },\n            ignore_index=True,\n        )\n\n    # Return the results DataFrame\n    return results\n\n", "idx": 1284}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.laplace_smooth_counts", "completion": "    # Add 1 to each count to shift some probability mass from very probable commands/parameters to unseen and unlikely commands/parameters\n    seq1_counts_sm = StateMatrix(\n        {\n            cmd: count + 1\n            for cmd, count in seq1_counts.items()\n            if cmd not in [start_token, end_token]\n        }\n    )\n    seq2_counts_sm = StateMatrix(\n        {\n            cmd1: {\n                cmd2: count + 1\n                for cmd2, count in seq2_counts[cmd1].items()\n                if cmd2 not in [start_token, end_token]\n            }\n            for cmd1 in seq2_counts.keys()\n            if cmd1 not in [start_token, end_token]\n        }\n    )\n    param_counts_sm = StateMatrix(\n        {par: count + 1 for par, count in param_counts.items()}\n    )\n    cmd_param_counts_sm = StateMatrix(\n        {\n            cmd: {\n                par: count + 1\n                for par, count in cmd_param_counts[cmd].items()\n            }\n            for cmd in cmd_param_counts.keys()\n        }\n    )\n\n    # Handle unseen commands, sequences of commands, and parameters using the `unk_token`\n    seq1_counts_sm[unk_token] = 1\n    for cmd in seq2_counts_sm.keys():\n        seq2_counts_sm[cmd][unk_token] = 1\n    for par in param_counts_sm.keys():\n        param_counts_sm[par] = 1\n    for cmd in cmd_param_counts_sm.keys():\n        cmd_param_counts_sm[cmd][unk_token] = 1\n\n    return (\n        seq1_counts_sm,\n        seq2_counts_sm,\n        param_counts_sm,\n        cmd_param_counts_sm,\n    )\n\n", "idx": 1285}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.compute_likelihood_window", "completion": "    if use_start_token:\n        window = [Cmd(name=start_token, params=set())] + window\n    if use_end_token:\n        window = window + [Cmd(name=end_token, params=set())]\n\n    # compute the likelihood of the window\n    lik: float = 1\n    prev = window[0]\n    for cmd in window[1:]:\n        # compute the likelihood of the command\n        lik *= prior_probs[cmd.name]\n        # compute the likelihood of the sequence of commands\n        lik *= trans_probs[prev.name][cmd.name]\n        # compute the likelihood of the parameters conditional on the command\n        lik *= compute_prob_setofparams_given_cmd(\n            cmd=cmd.name,\n            params=cmd.params,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n        )\n        prev = cmd\n\n    return lik\n\n", "idx": 1286}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.compute_likelihood_windows_in_session", "completion": "    if use_start_end_tokens:\n        if start_token is None:\n            raise MsticpyException(\n                \"start_token should not be None, when use_start_end_tokens is True\"\n            )\n        if end_token is None:\n            raise MsticpyException(\n                \"end_token should not be None, when use_start_end_tokens is True\"\n            )\n\n    if window_len < 1:\n        raise MsticpyException(\"window_len should be at least 1\")\n\n    if window_len > len(session):\n        raise MsticpyException(\n            \"window_len should be less than or equal to the length of the session\"\n        )\n\n    if use_start_end_tokens:\n        session = [Cmd(name=start_token, params=set())] + session + [\n            Cmd(name=end_token, params=set())\n        ]\n\n    session_len = len(session)\n    likelihoods = []\n    for i in range(session_len - window_len + 1):\n        window = session[i : i + window_len]\n        prob = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            use_start_token=use_start_end_tokens,\n            use_end_token=use_start_end_tokens,\n            start_token=start_token,\n            end_token=end_token,\n        )\n        if use_geo_mean:\n            prob = prob ** (1 / window_len)\n        likelihoods.append(prob)\n\n    return likelihoods", "idx": 1287}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.rarest_window_session", "completion": "    likelihoods = compute_likelihood_windows_in_session(\n        session=session,\n        prior_probs=prior_probs,\n        trans_probs=trans_probs,\n        param_cond_cmd_probs=param_cond_cmd_probs,\n        window_len=window_len,\n        use_start_end_tokens=use_start_end_tokens,\n        start_token=start_token,\n        end_token=end_token,\n        use_geo_mean=use_geo_mean,\n    )\n    rarest_window_idx = np.argmin(likelihoods)\n    rarest_window = session[rarest_window_idx : rarest_window_idx + window_len]\n    rarest_window_likelihood = likelihoods[rarest_window_idx]\n    return rarest_window, rarest_window_likelihood", "idx": 1288}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.compute_likelihood_window", "completion": "    if use_start_token:\n        window = [start_token] + window\n    if use_end_token:\n        window = window + [end_token]\n\n    if isinstance(prior_probs, StateMatrix):\n        prior_probs = prior_probs.state_matrix\n    if isinstance(trans_probs, StateMatrix):\n        trans_probs = trans_probs.state_matrix\n\n    likelihood = 1.0\n    for i in range(len(window) - 1):\n        if window[i] not in prior_probs:\n            likelihood *= 0.0\n        else:\n            likelihood *= prior_probs[window[i]]\n        if window[i] not in trans_probs:\n            likelihood *= 0.0\n        else:\n            likelihood *= trans_probs[window[i]][window[i + 1]]\n\n    return likelihood\n\n", "idx": 1289}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.compute_likelihood_windows_in_session", "completion": "    if use_start_end_tokens:\n        if start_token is None:\n            raise MsticpyException(\n                \"start_token should not be None, when use_start_end_tokens is True\"\n            )\n        if end_token is None:\n            raise MsticpyException(\n                \"end_token should not be None, when use_start_end_tokens is True\"\n            )\n\n    if use_start_end_tokens:\n        session = [start_token] + session + [end_token]\n\n    session_len = len(session)\n    if session_len < window_len:\n        return [np.nan]\n\n    likelihoods = []\n    for i in range(session_len - window_len + 1):\n        window = session[i : i + window_len]\n        likelihood = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            use_start_token=use_start_end_tokens,\n            use_end_token=use_start_end_tokens,\n            start_token=start_token,\n            end_token=end_token,\n        )\n        if use_geo_mean:\n            likelihood = likelihood ** (1 / window_len)\n        likelihoods.append(likelihood)\n\n    return likelihoods", "idx": 1290}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.rarest_window_session", "completion": "    likelihoods = compute_likelihood_windows_in_session(\n        session=session,\n        prior_probs=prior_probs,\n        trans_probs=trans_probs,\n        window_len=window_len,\n        use_start_end_tokens=use_start_end_tokens,\n        start_token=start_token,\n        end_token=end_token,\n        use_geo_mean=use_geo_mean,\n    )\n    min_idx = np.argmin(likelihoods)\n    window = session[min_idx : min_idx + window_len]\n    return window, likelihoods[min_idx]\n\n", "idx": 1291}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.get_params_to_model_values", "completion": "    if isinstance(param_counts, StateMatrix):\n        param_counts = param_counts.states\n    if isinstance(param_value_counts, StateMatrix):\n        param_value_counts = param_value_counts.states\n\n    params_to_model_values = set()\n    for param, value_counts in param_value_counts.items():\n        if param in param_counts:\n            param_count = param_counts[param]\n            if param_count > 100:\n                # If the parameter has more than 100 occurrences, check if the number of unique values is less than 10% of the parameter count.\n                if len(value_counts) < param_count * 0.1:\n                    params_to_model_values.add(param)\n            elif param_count > 10:\n                # If the parameter has more than 10 occurrences, check if the number of unique values is less than 50% of the parameter count.\n                if len(value_counts) < param_count * 0.5:\n                    params_to_model_values.add(param)\n            else:\n                # If the parameter has less than 10 occurrences, check if the number of unique values is less than 100% of the parameter count.\n                if len(value_counts) < param_count:\n                    params_to_model_values.add(param)\n\n    return params_to_model_values\n\n", "idx": 1292}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_prob_setofparams_given_cmd", "completion": "    if isinstance(params_with_vals, set):\n        params_with_vals = dict.fromkeys(params_with_vals)\n\n    if isinstance(param_cond_cmd_probs, dict):\n        param_cond_cmd_probs = StateMatrix(states=param_cond_cmd_probs)\n\n    if isinstance(value_cond_param_probs, dict):\n        value_cond_param_probs = StateMatrix(states=value_cond_param_probs)\n\n    if isinstance(modellable_params, list):\n        modellable_params = set(modellable_params)\n\n    if not isinstance(params_with_vals, dict):\n        raise MsticpyException(\n            \"params_with_vals must be a dictionary or set of parameters and their values.\"\n        )\n\n    if not isinstance(param_cond_cmd_probs, StateMatrix):\n        raise MsticpyException(\n            \"param_cond_cmd_probs must be a StateMatrix or a dictionary of probabilities.\"\n        )\n\n    if not isinstance(value_cond_param_probs, StateMatrix):\n        raise MsticpyException(\n            \"value_cond_param_probs must be a StateMatrix or a dictionary of probabilities.\"\n        )\n\n    if not isinstance(modellable_params, set):\n        raise MsticpyException(\"modellable_params must be a set of parameters.\")\n\n    if not isinstance(use_geo_mean, bool):\n        raise MsticpyException(\"use_geo_mean must be a boolean.\")\n\n    if not isinstance(cmd, str):\n        raise MsticpyException(\"cmd must be a string.\")\n\n    if not isinstance(params_with_vals, dict):\n        raise MsticpyException(\n            \"params_with_vals must be a dictionary or set of parameters and their values.\"\n        )\n\n    if not isinstance(param_cond_cmd_probs, StateMatrix):\n        raise MsticpyException(\n            \"param_cond_cmd_probs must be a StateMatrix or a dictionary of probabilities.\"\n        )\n\n    if not isinstance(value_cond_param_probs, StateMatrix):\n        raise MsticpyException(\n            \"value_cond_param_probs must be a StateMatrix or a dictionary of probabilities.\"\n        )\n\n    if not isinstance(modellable_params, set):\n        raise MsticpyException(\"modellable_params", "idx": 1293}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_likelihood_window", "completion": "    if use_start_token:\n        window = [Cmd(name=start_token, params={})] + window\n    if use_end_token:\n        window = window + [Cmd(name=end_token, params={})]\n\n    lik: float = 1\n    prev = window[0].name\n    lik *= prior_probs[prev]\n    for cmd in window[1:]:\n        lik *= trans_probs[prev][cmd.name]\n        lik *= compute_prob_setofparams_given_cmd(\n            cmd=cmd.name,\n            params_with_vals=cmd.params,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            value_cond_param_probs=value_cond_param_probs,\n            modellable_params=modellable_params,\n        )\n        prev = cmd.name\n    return lik\n\n", "idx": 1294}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_likelihood_windows_in_session", "completion": "    if use_start_end_tokens:\n        if start_token is None:\n            raise MsticpyException(\n                \"start_token should not be None, when use_start_end_tokens is True\"\n            )\n        if end_token is None:\n            raise MsticpyException(\n                \"end_token should not be None, when use_start_end_tokens is True\"\n            )\n\n    if use_start_end_tokens:\n        session = [Cmd(name=start_token)] + session + [Cmd(name=end_token)]\n\n    session_len = len(session)\n    if session_len < window_len:\n        return []\n\n    likelihoods = []\n    for i in range(session_len - window_len + 1):\n        window = session[i : i + window_len]\n        likelihood = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            value_cond_param_probs=value_cond_param_probs,\n            modellable_params=modellable_params,\n            use_start_token=use_start_end_tokens,\n            use_end_token=use_start_end_tokens,\n            start_token=start_token,\n            end_token=end_token,\n        )\n        likelihoods.append(likelihood)\n\n    if use_geo_mean:\n        likelihoods = [likelihood ** (1 / window_len) for likelihood in likelihoods]\n\n    return likelihoods", "idx": 1295}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.rarest_window_session", "completion": "    likelihoods = compute_likelihood_windows_in_session(\n        session=session,\n        prior_probs=prior_probs,\n        trans_probs=trans_probs,\n        param_cond_cmd_probs=param_cond_cmd_probs,\n        value_cond_param_probs=value_cond_param_probs,\n        modellable_params=modellable_params,\n        window_len=window_len,\n        use_start_end_tokens=use_start_end_tokens,\n        start_token=start_token,\n        end_token=end_token,\n        use_geo_mean=use_geo_mean,\n    )\n\n    rarest_window = session[np.argmin(likelihoods)]\n    rarest_window_likelihood = np.min(likelihoods)\n\n    return rarest_window, rarest_window_likelihood", "idx": 1296}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.probabilities.compute_cmds_probs", "completion": "    # Compute the probabilities for individual commands\n    cmd_probs = defaultdict(lambda: defaultdict(float))\n    for cmd in seq1_counts:\n        cmd_probs[cmd][cmd] = seq1_counts[cmd][cmd] / sum(seq1_counts[cmd].values())\n\n    # Compute the probabilities for sequence commands (length 2)\n    seq_cmd_probs = defaultdict(lambda: defaultdict(float))\n    for cmd1 in seq2_counts:\n        for cmd2 in seq2_counts[cmd1]:\n            seq_cmd_probs[cmd1][cmd2] = seq2_counts[cmd1][cmd2] / sum(seq2_counts[cmd1].values())\n\n    return cmd_probs, seq_cmd_probs", "idx": 1297}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.probabilities.compute_values_probs", "completion": "    value_probs: DefaultDict[str, float] = defaultdict(lambda: 0)\n    value_cond_param_probs: DefaultDict[str, DefaultDict[str, float]] = defaultdict(\n        lambda: defaultdict(lambda: 0)\n    )\n\n    for param, values in param_value_counts.items():\n        for value, count in values.items():\n            value_cond_param_probs[param][value] = count / sum(values.values())\n\n    tot_value = sum(value_counts.values())\n    for value, count in value_counts.items():\n        value_probs[value] = count / tot_value\n\n    value_probs_sm = StateMatrix(states=value_probs, unk_token=unk_token)\n    value_cond_param_probs_sm = StateMatrix(\n        states=value_cond_param_probs, unk_token=unk_token\n    )\n\n    return value_probs_sm, value_cond_param_probs_sm", "idx": 1298}
{"namespace": "msticpy.auth.msal_auth.MSALDelegatedAuth.get_token", "completion": "        # First try to get the chosen account using the app's get_accounts method\n        accounts = self.app.get_accounts(username=self.username)\n        if accounts:\n            # If a chosen account is found, acquire a token with the specified scopes and chosen account\n            result = self.app.acquire_token_silent(scopes=self.scopes, account=accounts[0])\n\n        if not result:\n            # If the result is empty, then authenticate with the specified authentication type\n            if self.auth_type == \"interactive\":\n                result = self.app.acquire_token_interactive(scopes=self.scopes)\n            elif self.auth_type == \"silent\":\n                result = self.app.acquire_token_silent(scopes=self.scopes)\n            elif self.auth_type == \"device_code\":\n                result = self.app.acquire_token_by_device_flow(scopes=self.scopes)\n            elif self.auth_type == \"username_password\":\n                result = self.app.acquire_token_by_username_password(\n                    scopes=self.scopes, username=self.username, password=\n                )\n\n        if \"access_token\" in result:\n            # Refresh the token\n            self.app.acquire_token_silent(scopes=self.scopes, account=accounts[0])\n            self.result = result\n        else:\n            self.result = None\n", "idx": 1299}
{"namespace": "msticpy.config.query_editor.QueryParameterEditWidget.save_parameter", "completion": "        del button\n        # Get the values from the widgets\n        parameter_name = self.parameter_name_widget.value\n        description = self.description_widget.value\n        datatype = self.type_widget.value\n        default_reqd = self.default_reqd_widget.value\n        default = self.default_widget.value\n\n        # Create a new QueryParameter instance with the retrieved values\n        parameter = QueryParameter(\n            name=parameter_name,\n            description=description,\n            datatype=datatype,\n            default=default if default_reqd else None,\n        )\n\n        # Set the parameter in the param container\n        self.param_container.parameters[parameter_name] = parameter\n\n        # Update the parameter dropdown options and set the selected value to the newly saved parameter\n        self.parameter_dropdown.options = list(self.param_container.parameters.keys())\n        self.parameter_dropdown.value = parameter_name\n        self._changed_data = True\n", "idx": 1300}
{"namespace": "msticpy.config.query_editor.QueryParameterEditWidget.delete_parameter", "completion": "        del button\n        if not self.parameter_name_widget.value:\n            return\n        param_name = self.parameter_name_widget.value\n        del self.param_container.parameters[param_name]\n        self.parameter_dropdown.options = list(self.param_container.parameters.keys())\n        self._blank_parameter()\n        self._changed_data = True\n\n", "idx": 1301}
{"namespace": "msticpy.config.query_editor.MetadataEditWidget.save_metadata", "completion": "        del button\n        self.metadata.version = self.version_widget.value\n        self.metadata.description = self.description_widget.value\n        self.metadata.data_environments = (\n            self.data_env_widget.value if self.data_env_widget.value else ()\n        )\n        self.metadata.data_families = (\n            self.data_families_widget.value.split(\", \")\n            if self.data_families_widget.value\n            else ()\n        )\n        self.metadata.database = self.database_widget.value\n        self.metadata.cluster = self.cluster_widget.value\n        self.metadata.clusters = (\n            self.clusters_widget.value.split(\"\\n\") if self.clusters_widget.value else ()\n        )\n        self.metadata.cluster_groups = (\n            self.cluster_groups_widget.value.split(\"\\n\")\n            if self.cluster_groups_widget.value\n            else ()\n        )\n        self.metadata.tags = (\n            self.tags_widget.value.split(\", \") if self.tags_widget.value else ()\n        )\n        self.metadata.data_source = self.data_source_widget.value\n        self._changed_data = True\n", "idx": 1302}
{"namespace": "msticpy.config.query_editor.QueryEditor._save_file", "completion": "        del button\n        if self.ignore_changes.value:\n            self._update_query_collection(self.query_collection)\n        else:\n            self._update_query_collection(\n                self.query_collection.copy(\n                    sources={\n                        qry: self.query_collection.sources[qry].copy(\n                            args=self.query_collection.sources[qry].args.copy(\n                                query=replace_query(\n                                    self.query_collection.sources[qry].args.query,\n                                    self.query_collection.sources[qry].parameters,\n                                )\n                            )\n                        )\n                        for qry in self.query_collection.sources\n                    }\n                )\n            )\n", "idx": 1303}
{"namespace": "msticpy.config.query_editor.QueryEditor._unsaved_changes", "completion": "        return (\n            self.default_param_editor.changed_data\n            or self.metadata_editor.changed_data\n            or self.query_editor.changed_data\n        )\n\n", "idx": 1304}
{"namespace": "msticpy.config.query_editor.load_queries_from_yaml", "completion": "    with open(yaml_file, \"r\") as f:\n        yaml_data = yaml.safe_load(f)\n        metadata = QueryMetadata(\n            version=yaml_data[\"metadata\"][\"version\"],\n            description=yaml_data[\"metadata\"][\"description\"],\n            data_environments=yaml_data[\"metadata\"][\"data_environments\"],\n            data_families=yaml_data[\"metadata\"][\"data_families\"],\n        )\n        defaults = QueryDefaults(\n            metadata=yaml_data[\"defaults\"][\"metadata\"],\n            parameters=yaml_data[\"defaults\"][\"parameters\"],\n        )\n        sources = {}\n        for qry in yaml_data[\"sources\"]:\n            sources[qry] = Query(\n                description=yaml_data[\"sources\"][qry][\"description\"],\n                metadata=yaml_data[\"sources\"][qry][\"metadata\"],\n                args=QueryArgs(query=yaml_data[\"sources\"][qry][\"args\"][\"query\"]),\n                parameters=yaml_data[\"sources\"][qry][\"parameters\"],\n            )\n        return QueryCollection(\n            file_name=yaml_file, metadata=metadata, defaults=defaults, sources=sources\n        )\n\n", "idx": 1305}
{"namespace": "zxcvbn.time_estimates.estimate_attack_times", "completion": "    # Calculate the crack times in seconds for different scenarios\n    crack_time_seconds = Decimal(guesses) / Decimal(1000000)\n    crack_time_seconds_offline = Decimal(guesses) / Decimal(1000000000)\n    crack_time_seconds_online = Decimal(guesses) / Decimal(1000000000000)\n\n    # Convert the crack times in seconds to a more readable format\n    crack_time_minutes = crack_time_seconds / Decimal(60)\n    crack_time_hours = crack_time_minutes / Decimal(60)\n    crack_time_days = crack_time_hours / Decimal(24)\n    crack_time_weeks = crack_time_days / Decimal(7)\n    crack_time_months = crack_time_days / Decimal(30)\n    crack_time_years = crack_time_days / Decimal(365)\n\n    # Calculate the score based on the number of guesses\n    score = 0\n    if guesses < Decimal(1000):\n        score = 0\n    elif guesses < Decimal(1000000):\n        score = 1\n    elif guesses < Decimal(1000000000):\n        score = 2\n    elif guesses < Decimal(1000000000000):\n        score = 3\n    elif guesses < Decimal(1000000000000000):\n        score = 4\n    else:\n        score = 5\n\n    # Return the crack times in seconds for different scenarios, the crack times in a more readable format, and the score based on the number of guesses\n    return {\n        \"crack_time_seconds\": crack_time_seconds,\n        \"crack_time_seconds_offline\": crack_time_seconds_offline,\n        \"crack_time_seconds_online\": crack_time_seconds_online,\n        \"crack_time_minutes\": crack_time_minutes,\n        \"crack_time_hours\": crack_time_hours,\n       ", "idx": 1306}
{"namespace": "zxcvbn.scoring.estimate_guesses", "completion": "    # if we've already calculated this, return the previous value\n    if 'guesses' in match:\n        return match['guesses']\n\n    # the base guesses if no known patterns apply\n    guesses = Decimal(BRUTEFORCE_CARDINALITY)\n\n    # add some guesses for uppercase letters\n    if re.search(r'[A-Z]', match['token']):\n        guesses *= 26\n\n    # add some guesses for symbols\n    if re.search(r'[^a-zA-Z0-9]', match['token']):\n        guesses *= 33\n\n    # add some guesses if numbers are involved\n    if re.search(r'[0-9]', match['token']):\n        guesses *= 10\n\n    # add some guesses if only letters are involved\n    if re.search(r'[^0-9]', match['token']) and not re.search(r'[a-zA-Z]', match['token']):\n        guesses *= 26\n\n    # add some guesses if only numbers are involved\n    if re.search(r'[^a-zA-Z]', match['token']) and not re.search(r'[0-9]', match['token']):\n        guesses *= 10\n\n    # add some guesses if it's a dictionary word\n    if match['pattern'] == 'dictionary':\n        guesses *= 1\n\n    # add some guesses if it's a spatial pattern\n    if match['pattern'] == 'spatial':\n        guesses *= 1\n\n    # add some guesses if it's a repeat pattern\n    if match['pattern'] == 'repeat':\n        guesses *= 1\n\n    # add some guesses if it's a sequence pattern\n    if match['pattern'] == 'sequence':\n        guesses *= 1\n\n    # add some guesses if it's an year\n    if match['pattern'] == 'year':\n        guesses *= 1\n\n    # add some guesses if it's an date\n    if match['pattern'] == 'date':\n        guesses *= 1\n\n    # add some guesses if it's an date\n    if match['pattern'] == 'date':\n        guesses *= 1\n\n    # add some guesses if it's an date\n    if match['pattern'] == 'date':\n        guesses *= 1\n\n    # add some guesses if it's an date\n    if match['", "idx": 1307}
{"namespace": "zxcvbn.scoring.dictionary_guesses", "completion": "    base_guesses = match['rank']\n    uppercase_variations = 26 / 10\n    l33t_variations = 1\n    reversed_variations = 2\n\n    # calculate the number of uppercase variations\n    if match['uppercase']:\n        base_guesses *= uppercase_variations\n\n    # calculate the number of l33t variations\n    if match['l33t']:\n        base_guesses *= l33t_variations\n\n    # calculate the number of reversed variations\n    if match['reversed']:\n        base_guesses *= reversed_variations\n\n    return base_guesses\n\n", "idx": 1308}
{"namespace": "zxcvbn.scoring.regex_guesses", "completion": "    # character class bases\n    char_class_bases = {\n        'alpha_lower': 26,\n        'alpha_upper': 26,\n        'alpha': 52,\n        'alphanumeric_lower': 62,\n        'alphanumeric_upper': 62,\n        'alphanumeric': 124,\n        'digits': 10,\n        'symbols': 33,\n    }\n\n    # check match type\n    if match['regex_name'] == 'recent_year':\n        return MIN_YEAR_SPACE\n    elif match['regex_name'] == 'recent_year_suffix':\n        return MIN_YEAR_SPACE\n    elif match['regex_name'] == 'recent_year_prefix':\n        return MIN_YEAR_SPACE\n    elif match['regex_name'] == 'year':\n        return 1000\n    elif match['regex_name'] == 'date':\n        return 1000\n    elif match['regex_name'] == 'common_name':\n        return 1000\n    elif match['regex_name'] == 'common_username':\n        return 1000\n    elif match['regex_name'] == 'common_password':\n        return 1000\n    elif match['regex_name'] == 'regex':\n        # regex match\n        if match['regex_name'] in char_class_bases:\n            base_guesses = char_class_bases[match['regex_name']]\n        else:\n            base_guesses = 0\n\n        # add 1 for each character in the match\n        return base_guesses + len(match['token'])\n    else:\n        # default to 10 guesses per character\n        return 10 * len(match['token'])\n\n", "idx": 1309}
{"namespace": "zxcvbn.scoring.date_guesses", "completion": "    year = match['year']\n    separator = match['separator']\n\n    # If the year is less than 1000, assume it is a 2-digit year and add 1900 to it.\n    if year < 1000:\n        year += 1900\n\n    # If the year is less than 100, assume it is a 1-digit year and add 2000 to it.\n    if year < 100:\n        year += 2000\n\n    # If the year is less than 50, assume it is a 2-digit year and add 2000 to it.\n    if year < 50:\n        year += 2000\n\n    # If the year is less than 10, assume it is a 1-digit year and add 2020 to it.\n    if year < 10:\n        year += 2020\n\n    # If the year is less than 1, assume it is a 1-digit year and add 2021 to it.\n    if year < 1:\n        year += 2021\n\n    # If the year is less than 0, assume it is a 1-digit year and add 2022 to it.\n    if year < 0:\n        year += 2022\n\n    # If the year is less than -10, assume it is a 2-digit year and add 2020 to it.\n    if year < -10:\n        year += 2020\n\n    # If the year is less than -100, assume it is a 3-digit year and add 2000 to it.\n    if year < -100:\n        year += 2000\n\n    # If the year is less than -1000, assume it is a 4-digit year and add 1000 to it.\n    if year < -1000:\n        year += 1000\n\n    # If the year is less than -10000, assume it is a 5-digit year and add 10000 to it.\n    if year < -10000:\n        year += 10000", "idx": 1310}
{"namespace": "zxcvbn.scoring.spatial_guesses", "completion": "    # starting positions:\n    #   for qwerty and dvorak, there are 10 keys without neighbors and 26\n    #   with one neighbor. for keypad, there are 10 keys with 0 neighbors,\n    #   9 with 1, and one with 4.\n    #\n    # average degree:\n    #   for qwerty and dvorak, the average neighbor degree of a key is 6.\n    #   for keypad, it's 5.\n    #\n    # guesses = (\n    #   (starting positions + (average degree * turns)) ** length) *\n    #   shifted count\n    #\n    # if the first character is uppercase, and uppercase variations are\n    # allowed, the first character is treated as the lowercase version\n    # of the alphabet and must be shifted.\n    #\n    # example #1:\n    #   given 'abcdef', and assuming a qwerty keyboard with no shifts\n    #   starting positions: 26 (the 'a' key has 0 neighbors)\n    #   average degree: 6\n    #   turns: 0\n    #   length: 6\n    #   guesses = (26 + (6*0)) ** 6 = 308915776\n    #\n    # example #2:\n    #   given 'Thomas', assuming a qwerty keyboard with a shift key, and\n    #   allowing uppercase variatons.\n    #   starting positions: 26\n    #   average degree: 6\n    #   turns: 1 (one turn: 't' to 'T')\n    #   length: 6\n    #   shifted count: 1 (the 'T')\n    #   guesses = (26 + (6*1)) ** 6 * 1 = 517601536\n    #\n    # example #3:\n    #   given 'THomas', assuming a qwerty keyboard with a shift key, and\n    #   allowing uppercase variatons.\n    #   starting positions: 26\n    #   average degree: 6\n    #   turns: 1 (one turn: 't' to 'T')\n    #   length: 6\n    #   shifted count: 2 ('T' and 'H')\n    #   guesses = (26 + (6*1", "idx": 1311}
{"namespace": "zxcvbn.scoring.uppercase_variations", "completion": "    if match['token'] == match['token'].lower() or match['token'] == match['token'].upper():\n        return 1\n    elif START_UPPER.match(match['token']) or END_UPPER.match(match['token']) or ALL_UPPER.match(match['token']):\n        return 2\n    else:\n        uppercase_count = len(re.findall(r'[A-Z]', match['token']))\n        lowercase_count = len(re.findall(r'[a-z]', match['token']))\n        return nCk(uppercase_count + lowercase_count, uppercase_count)\n\n", "idx": 1312}
{"namespace": "zxcvbn.matching.dictionary_match", "completion": "    matches = []\n    for name, ranked_dict in _ranked_dictionaries.items():\n        for i in range(len(password)):\n            for j in range(i + 3, len(password) + 1):\n                if password[i:j] in ranked_dict:\n                    matches.append({\n                        'pattern': 'dictionary',\n                        'i': i,\n                        'j': j,\n                        'token': password[i:j],\n                        'matched_word': password[i:j],\n                        'rank': ranked_dict[password[i:j]],\n                        'dictionary_name': name,\n                        'reversed': False,\n                        'l33t': False,\n                        'base_guesses': ranked_dict[password[i:j]],\n                        'uppercase_variations': 2 ** (j - i - 1),\n                        'l33t_variations': 1,\n                    })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))\n\n", "idx": 1313}
{"namespace": "zxcvbn.matching.reverse_dictionary_match", "completion": "    matches = []\n    length = len(password)\n    password_lower = password.lower()\n    for dictionary_name, ranked_dict in _ranked_dictionaries.items():\n        for i in range(length):\n            for j in range(i, length):\n                if password_lower[i:j + 1][::-1] in ranked_dict:\n                    word = password_lower[i:j + 1][::-1]\n                    rank = ranked_dict[word]\n                    matches.append({\n                        'pattern': 'dictionary',\n                        'i': i,\n                        'j': j,\n                        'token': password[i:j + 1],\n                        'matched_word': word,\n                        'rank': rank,\n                        'dictionary_name': dictionary_name,\n                        'reversed': True,\n                        'l33t': False,\n                    })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))\n\n", "idx": 1314}
{"namespace": "zxcvbn.matching.l33t_match", "completion": "    matches = []\n    length = len(password)\n    password_lower = password.lower()\n    for i in range(length):\n        for j in range(i, length):\n            token = password_lower[i:j + 1]\n            subtable = relevant_l33t_subtable(token, _l33t_table)\n            if len(subtable) == 0:\n                continue\n\n            subs = enumerate_l33t_subs(subtable)\n            for sub in subs:\n                translated = translate(token, sub)\n                if translated in _ranked_dictionaries['passwords']:\n                    rank = _ranked_dictionaries['passwords'][translated]\n                    matches.append({\n                        'pattern': 'dictionary',\n                        'i': i,\n                        'j': j,\n                        'token': password[i:j + 1],\n                        'matched_word': translated,\n                        'rank': rank,\n                        'dictionary_name': 'passwords',\n                        'reversed': False,\n                        'l33t': True,\n                        'l33t_variants': subs,\n                    })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))\n\n", "idx": 1315}
{"namespace": "zxcvbn.matching.repeat_match", "completion": "    matches = []\n    greedy_pattern = r'(.+)\\1+'\n    lazy_pattern = r'(.+?)\\1+'\n    for i in range(len(password)):\n        for j in range(i + 1, len(password) + 1):\n            token = password[i:j]\n            if len(token) < 2:\n                continue\n\n            greedy_match = re.search(greedy_pattern, token)\n            lazy_match = re.search(lazy_pattern, token)\n            if not greedy_match or not lazy_match:\n                continue\n\n            greedy_base_token = greedy_match.group(1)\n            lazy_base_token = lazy_match.group(1)\n            if len(greedy_base_token) != len(lazy_base_token):\n                continue\n\n            base_token = greedy_base_token\n            repeat_count = len(token) // len(base_token)\n            if repeat_count < 2:\n                continue\n\n            # recursively match and score the base string\n            base_matches = omnimatch(base_token, _ranked_dictionaries)\n            base_guesses = scoring.most_guessable_match_sequence(base_matches)\n            if base_guesses == 0:\n                continue\n\n            # add the repeat match\n            matches.append({\n                'pattern': 'repeat',\n                'i': i,\n                'j': j - 1,\n                'token': token,\n                'base_token': base_token,\n                'base_guesses': base_guesses,\n                'repeat_count': repeat_count,\n                'sequence': base_matches,\n            })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))\n\n", "idx": 1316}
{"namespace": "zxcvbn.matching.spatial_match", "completion": "    matches = []\n    for graph_name, graph in _graphs.items():\n        matches.extend(spatial_match_helper(password, graph, graph_name,\n                                            _ranked_dictionaries))\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))\n\n", "idx": 1317}
{"namespace": "zxcvbn.matching.sequence_match", "completion": "    matches = []\n    password_lower = password.lower()\n    for i in range(len(password) - 1):\n        j = i + 1\n        delta = ord(password[j]) - ord(password[i])\n        if abs(delta) > MAX_DELTA:\n            continue\n\n        while j < len(password) - 1:\n            j += 1\n            cur_delta = ord(password[j]) - ord(password[j - 1])\n            if cur_delta != delta:\n                break\n\n        if j - i > 2:\n            sequence_name = 'lower'\n            sequence_space = 'ascii_lowercase'\n            ascending = delta > 0\n            if password_lower[i:j] == password[i:j]:\n                sequence_name = 'upper'\n                sequence_space = 'ascii_uppercase'\n            elif password[i:j].isdigit():\n                sequence_name = 'digits'\n                sequence_space = 'digits'\n            else:\n                for char in password[i:j]:\n                    if ord(char) > 127:\n                        sequence_name = 'unicode'\n                        sequence_space = 'unicode'\n                        break\n\n            matches.append({\n                'pattern': 'sequence',\n                'i': i,\n                'j': j - 1,\n                'token': password[i:j],\n                'sequence_name': sequence_name,\n                'sequence_space': sequence_space,\n                'ascending': ascending,\n            })\n\n    return matches\n\n", "idx": 1318}
{"namespace": "zxcvbn.matching.regex_match", "completion": "    matches = []\n    for regex_name, regex in _regexen.items():\n        match = regex.search(password)\n        if match:\n            matches.append({\n                'pattern': 'regex',\n                'token': match.group(0),\n                'i': match.start(0),\n                'j': match.end(0),\n                'regex_name': regex_name,\n                'regex_match': match,\n            })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))\n\n", "idx": 1319}
{"namespace": "OpenSSL.rand.add", "completion": "    _lib.RAND_add(buffer, entropy)\n\n", "idx": 1320}
{"namespace": "asyncssh.kex.register_kex_alg", "completion": "    _kex_algs.append(alg)\n    _kex_handlers[alg] = (handler, hash_alg, args)\n    if default:\n        _default_kex_algs.append(alg)\n\n", "idx": 1321}
{"namespace": "asyncssh.auth.get_supported_server_auth_methods", "completion": "    return [method for method in _server_auth_handlers\n            if method != b'none' and _server_auth_handlers[method].supported(conn)]\n\n", "idx": 1322}
{"namespace": "asyncssh.mac.get_mac", "completion": "    handler, hash_size, args = _mac_handler[mac_alg]\n    return handler(key, hash_size, *args)\n\n", "idx": 1323}
{"namespace": "asyncssh.auth_keys.SSHAuthorizedKeys.validate", "completion": "        entries = self._ca_entries if ca else self._user_entries\n\n        for entry in entries:\n            if entry.key == key:\n                if entry.match_options(client_host, client_addr,\n                                       cert_principals):\n                    return entry.options\n\n        return None\n", "idx": 1324}
{"namespace": "asyncssh.saslprep.saslprep", "completion": "    return _stringprep(s, True, _map_saslprep, 'NFKC',\n                       [stringprep.in_table_c11, stringprep.in_table_c21,\n                        stringprep.in_table_c22, stringprep.in_table_c3],\n                       True)", "idx": 1325}
{"namespace": "asyncssh.asn1.der_decode", "completion": "    value, consumed = der_decode_partial(data)\n    if consumed < len(data):\n        raise ASN1DecodeError('Data contains unexpected bytes at end')\n\n    return value\n\n", "idx": 1326}
{"namespace": "asyncssh.packet.SSHPacket.check_end", "completion": "        if self._idx != self._len:\n            raise PacketDecodeError(f'Packet not fully consumed: {self._len - self._idx} bytes remaining')\n", "idx": 1327}
{"namespace": "asyncssh.public_key.SSHKey.verify", "completion": "        packet = SSHPacket(sig)\n        sig_algorithm = packet.get_string()\n\n        if sig_algorithm.startswith(b'x509v3-'):\n            sig_algorithm = sig_algorithm[7:]\n\n        if sig_algorithm not in self.all_sig_algorithms:\n            raise ValueError('Unrecognized signature algorithm')\n\n        return self.verify_ssh(data, sig_algorithm, packet)\n", "idx": 1328}
{"namespace": "asyncssh.public_key.SSHKey.convert_to_public", "completion": "        raise NotImplementedError\n", "idx": 1329}
{"namespace": "asyncssh.public_key.SSHKey.generate_x509_user_certificate", "completion": "        return self._generate_x509_certificate(user_key, subject, issuer, serial, principals, valid_after, valid_before, purposes, hash_alg, comment)\n", "idx": 1330}
{"namespace": "asyncssh.misc.write_file", "completion": "    with open_file(filename, mode) as f:\n        return f.write(data)\n\n", "idx": 1331}
{"namespace": "diffprivlib.accountant.BudgetAccountant.__repr__", "completion": "        attributes = []\n        if self.__epsilon != float(\"inf\"):\n            attributes.append(f\"epsilon={self.__epsilon}\")\n        if self.__delta != 1.0:\n            attributes.append(f\"delta={self.__delta}\")\n        if self.slack > 0:\n            attributes.append(f\"slack={self.slack}\")\n        if len(self.__spent_budget) > 0:\n            attributes.append(f\"spent_budget={self.__spent_budget[:n_budget_max]}\")\n            if len(self.__spent_budget) > n_budget_max:\n                attributes[-1] += \"...\"\n        return f\"{self.__class__.__name__}({', '.join(attributes)})\"\n", "idx": 1332}
{"namespace": "diffprivlib.accountant.BudgetAccountant.check", "completion": "        if epsilon < 0 or delta < 0:\n            raise ValueError(\"Privacy spend of ({epsilon},{delta}) not permissible; epsilon and delta must be non-negative.\")\n\n        if epsilon == 0 and delta == 0:\n            return True\n\n        if epsilon == 0 and delta > 0:\n            raise ValueError(\"Privacy spend of ({epsilon},{delta}) not permissible; epsilon must be non-zero.\")\n\n        if epsilon > 0 and delta == 0:\n            raise ValueError(\"Privacy spend of ({epsilon},{delta}) not permissible; delta must be non-zero.\")\n\n        if epsilon > self.epsilon:\n            raise ValueError(\"Privacy spend of ({epsilon},{delta}) not permissible; epsilon exceeds budget ceiling.\")\n\n        if delta > self.delta:\n            raise ValueError(\"Privacy spend of ({epsilon},{delta}) not permissible; delta exceeds budget ceiling.\")\n\n        if epsilon > 0 and delta > 0:\n            if epsilon + delta > self.epsilon + self.delta:\n                raise ValueError(\"Privacy spend of ({epsilon},{delta}) not permissible; epsilon + delta exceeds budget ceiling.\")\n\n        if epsilon > 0 and delta > 0:\n            if epsilon + delta > self.epsilon + self.delta:\n                raise ValueError(\"Privacy spend of ({epsilon},{delta}) not permissible; epsilon + delta exceeds budget ceiling.\")\n\n        if epsilon > 0 and delta > 0:\n            if epsilon + delta > self.epsilon + self.delta:\n                raise ValueError(\"Privacy spend of ({epsilon},{delta}) not permissible; epsilon + delta exceeds budget ceiling.\")\n\n        if epsilon > 0 and delta > 0:\n            if epsilon + delta > self.epsilon + self.delta:\n                raise ValueError(\"Privacy spend of ({epsilon},{delta}) not permissible; epsilon + delta exceeds budget ceiling.\")\n\n        if epsilon > 0 and delta > 0:\n            if epsilon + delta > self.epsilon + self.delta:\n                raise ValueError(\"Privacy spend of ({epsilon},{delta}) not permissible; epsilon + delta exceeds budget ceiling.\")\n\n        if epsilon > 0 and delta > 0:\n            if epsilon + delta > self.epsilon + self.delta:\n                raise ValueError(\"Privacy spend of ({epsilon},{delta}) not permissible; epsilon + delta exceeds budget ceiling.\")\n\n        if epsilon > 0 and", "idx": 1333}
{"namespace": "diffprivlib.accountant.BudgetAccountant.spend", "completion": "        if not self.check(epsilon, delta):\n            raise BudgetError(f\"Privacy spend of ({epsilon},{delta}) not permissible; will exceed remaining privacy budget.\"\n                              f\" Use {self.__class__.__name__}.{self.remaining.__name__}() to check remaining budget.\")\n\n        self.__spent_budget.append((epsilon, delta))\n", "idx": 1334}
{"namespace": "diffprivlib.accountant.BudgetAccountant.load_default", "completion": "        if accountant is None:\n            if BudgetAccountant._default is None:\n                BudgetAccountant._default = BudgetAccountant()\n            return BudgetAccountant._default\n        elif not isinstance(accountant, BudgetAccountant):\n            raise TypeError(f\"accountant must be a BudgetAccountant, got {type(accountant)}.\")\n        else:\n            return accountant\n", "idx": 1335}
{"namespace": "diffprivlib.accountant.BudgetAccountant.set_default", "completion": "        BudgetAccountant._default = self\n        return self\n", "idx": 1336}
{"namespace": "diffprivlib.accountant.BudgetAccountant.pop_default", "completion": "        if BudgetAccountant._default is None:\n            raise ValueError(\"No default BudgetAccountant instance found.\")\n        return BudgetAccountant._default\n\n", "idx": 1337}
{"namespace": "diffprivlib.validation.clip_to_bounds", "completion": "    if not isinstance(array, np.ndarray):\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\n    if array.ndim != 2:\n        raise ValueError(f\"input array must be 2-dimensional, got {array.ndim} dimensions.\")\n    if not isinstance(bounds, tuple):\n        raise TypeError(f\"Bounds must be specified as a tuple of (min, max), got {type(bounds)}.\")\n\n    lower, upper = bounds\n\n    if np.asarray(lower).size == 1 or np.asarray(upper).size == 1:\n        lower = np.ravel(lower).astype(array.dtype)\n        upper = np.ravel(upper).astype(array.dtype)\n    else:\n        lower = np.asarray(lower, dtype=array.dtype)\n        upper = np.asarray(upper, dtype=array.dtype)\n\n    if lower.shape != upper.shape:\n        raise ValueError(\"lower and upper bounds must be the same shape array\")\n    if lower.ndim > 1:\n        raise ValueError(\"lower and upper bounds must be scalar or a 1-dimensional array\")\n\n    n_bounds = lower.shape[0]\n\n    for i in range(n_bounds):\n        _lower = lower[i]\n        _upper = upper[i]\n\n        if not isinstance(_lower, Real) or not isinstance(_upper, Real):\n            raise TypeError(f\"Each bound must be numeric, got {_lower} ({type(_lower)}) and {_upper} ({type(_upper)}).\")\n\n        if _lower > _upper:\n            raise ValueError(f\"For each bound, lower bound must be smaller than upper bound, got {lower}, {upper})\")\n\n    if n_bounds == 1:\n        lower = np.ones(array.shape[1], dtype=array.dtype) * lower.item()\n        upper = np.ones(array.shape[1], dtype=array.dtype) * upper.item()\n\n    return np.clip(array, lower, upper)\n\n", "idx": 1338}
{"namespace": "diffprivlib.models.naive_bayes.GaussianNB._update_mean_variance", "completion": "        if sample_weight is not None:\n            warn_unused_args(\"sample_weight\")\n\n        if n_noisy is None:\n            n_noisy = X.shape[0]\n\n        if n_past == 0:\n            return np.mean(X, axis=0), np.var(X, axis=0)\n\n        n_new = n_noisy - n_past\n\n        if n_new == 0:\n            return mu, var\n\n        # Compute the new mean and variance\n        new_mu = np.mean(X, axis=0)\n        new_var = np.var(X, axis=0)\n\n        # Compute the new total mean and variance\n        total_mu = (n_past * mu + n_new * new_mu) / (n_past + n_new)\n        total_var = (n_past * (var + mu ** 2) + n_new * (new_var + new_mu ** 2)) / (n_past + n_new) - total_mu ** 2\n\n        # Add noise to the mean and variance\n        total_mu = self._noisy_mean(total_mu, random_state=random_state)\n        total_var = self._noisy_variance(total_var, random_state=random_state)\n\n        return total_mu, total_var\n", "idx": 1339}
{"namespace": "diffprivlib.models.naive_bayes.GaussianNB._noisy_class_counts", "completion": "        # Get the unique class labels and their corresponding counts\n        unique_classes, class_counts = np.unique(y, return_counts=True)\n\n        # Calculate the total number of samples\n        n_samples = len(y)\n\n        # Calculate the epsilon value for each class\n        epsilon_per_class = self.epsilon / len(unique_classes)\n\n        # Initialize an array to store the noisy class counts\n        noisy_class_counts = np.zeros(len(unique_classes))\n\n        # Iterate over each unique class label\n        for i, class_label in enumerate(unique_classes):\n            # Calculate the number of samples for the current class label\n            n_samples_class = class_counts[i]\n\n            # Calculate the sensitivity of the count query\n            sensitivity = n_samples\n\n            # Calculate the lower and upper bounds for the truncated Laplace mechanism\n            lower = 0\n            upper = n_samples\n\n            # Create a LaplaceTruncated mechanism with the given epsilon, sensitivity, and bounds\n            mechanism = LaplaceTruncated(epsilon=epsilon_per_class, delta=0, sensitivity=sensitivity, lower=lower,\n                                         upper=upper, random_state=random_state)\n\n            # Add noise to the count of the current class label\n            noisy_count = mechanism.randomise(n_samples_class)\n\n            # Store the noisy count in the noisy_class_counts array\n            noisy_class_counts[i] = noisy_count\n\n        return noisy_class_counts", "idx": 1340}
{"namespace": "diffprivlib.models.standard_scaler._incremental_mean_and_var", "completion": "    # Initialising new accountant, as budget is tracked in main class. Subject to review in line with GH issue #21\n    accountant = BudgetAccountant(epsilon)\n\n    # Check if the input is a numpy array\n    if not isinstance(X, np.ndarray):\n        X = np.array(X)\n\n    # Check if the input is a 1D array\n    if X.ndim == 1:\n        X = X.reshape(-1, 1)\n\n    # Check if the input is a 2D array\n    if X.ndim != 2:\n        raise ValueError(\"Input must be a 1D or 2D array.\")\n\n    # Check if the input is a 2D array\n    if X.ndim != 2:\n        raise ValueError(\"Input must be a 1D or 2D array.\")\n\n    # Check if the input is a 2D array\n    if X.ndim != 2:\n        raise ValueError(\"Input must be a 1D or 2D array.\")\n\n    # Check if the input is a 2D array\n    if X.ndim != 2:\n        raise ValueError(\"Input must be a 1D or 2D array.\")\n\n    # Check if the input is a 2D array\n    if X.ndim != 2:\n        raise ValueError(\"Input must be a 1D or 2D array.\")\n\n    # Check if the input is a 2D array\n    if X.ndim != 2:\n        raise ValueError(\"Input must be a 1D or 2D array.\")\n\n    # Check if the input is a 2D array\n    if X.ndim != 2:\n        raise ValueError(\"Input must be a 1D or 2D array.\")\n\n    # Check if the input is a 2D array\n    if X.ndim != 2:\n        raise ValueError(\"Input must be a 1D or 2D array.\")\n\n    # Check if the input is a 2D array\n    if X.ndim != 2:\n        raise ValueError(\"Input must be a 1D or 2D array.\")\n\n    # Check if the input is a 2D array\n    if X.ndim != 2:\n        raise ValueError(\"Input must be a 1D or 2D array.\")\n\n    # Check if the input is a 2D array\n    if X.ndim != 2", "idx": 1341}
{"namespace": "diffprivlib.models.linear_regression.LinearRegression.fit", "completion": "        X, y, X_offset, y_offset, X_scale = _preprocess_data(X, y, self.fit_intercept, self.epsilon, self.bounds_X,\n                                                             self.bounds_y, self.copy_X, check_input=True,\n                                                             random_state=self.random_state)\n\n        if self.bounds_X is None:\n            warnings.warn(\"No bounds provided for X.  Bounds will be computed on the data when .fit() is first \"\n                          \"called, resulting in a PrivacyLeakWarning.\", PrivacyLeakWarning)\n\n        if self.bounds_y is None:\n            warnings.warn(\"No bounds provided for y.  Bounds will be computed on the data when .fit() is first \"\n                          \"called, resulting in a PrivacyLeakWarning.\", PrivacyLeakWarning)\n\n        self.random_state_ = check_random_state(self.random_state)\n\n        self.obj, self.noisy_coefs = _construct_regression_obj(X, y, self.bounds_X, self.bounds_y, self.epsilon,\n                                                               self.alpha, self.random_state_)\n\n        self.coef_ = np.zeros((y.shape[1], X.shape[1]))\n\n        for i in range(y.shape[1]):\n            res = minimize(self.obj[i], np.zeros(X.shape[1]), method='L-BFGS-B', jac=True)\n            self.coef_[i] = res.x\n\n        if self.fit_intercept:\n            self.intercept_ = y_offset - np.dot(X_offset, self.coef_.T)\n        else:\n            self.intercept_ = np.zeros(y.shape[1])\n\n        self.accountant.spend(self.epsilon)\n\n        return self\n", "idx": 1342}
{"namespace": "diffprivlib.models.k_means.KMeans.fit", "completion": "        # Check if bounds are provided\n        if self.bounds is None:\n            warnings.warn(\"Bounds not provided. Computing bounds on the data.\", PrivacyLeakWarning)\n            self.bounds = (np.min(X, axis=0), np.max(X, axis=0))\n\n        # Check if bounds are provided\n        if self.bounds is None:\n            warnings.warn(\"Bounds not provided. Computing bounds on the data.\", PrivacyLeakWarning)\n            self.bounds = (np.min(X, axis=0), np.max(X, axis=0))\n\n        # Check if bounds are provided\n        if self.bounds is None:\n            warnings.warn(\"Bounds not provided. Computing bounds on the data.\", PrivacyLeakWarning)\n            self.bounds = (np.min(X, axis=0), np.max(X, axis=0))\n\n        # Check if bounds are provided\n        if self.bounds is None:\n            warnings.warn(\"Bounds not provided. Computing bounds on the data.\", PrivacyLeakWarning)\n            self.bounds = (np.min(X, axis=0), np.max(X, axis=0))\n\n        # Check if bounds are provided\n        if self.bounds is None:\n            warnings.warn(\"Bounds not provided. Computing bounds on the data.\", PrivacyLeakWarning)\n            self.bounds = (np.min(X, axis=0), np.max(X, axis=0))\n\n        # Check if bounds are provided\n        if self.bounds is None:\n            warnings.warn(\"Bounds not provided. Computing bounds on the data.\", PrivacyLeakWarning)\n            self.bounds = (np.min(X, axis=0), np.max(X, axis=0))\n\n        # Check if bounds are provided\n        if self.bounds is None:\n            warnings.warn(\"Bounds not provided. Computing bounds on the data.\", PrivacyLeakWarning)\n            self.bounds = (np.min(X, axis=0), np.max(X, axis=0))\n\n        # Check if bounds are provided\n        if self.bounds is None:\n            warnings.warn(\"Bounds not provided. Computing bounds on the data.\", PrivacyLeakWarning)\n            self.bounds = (np.min(X, axis=0), np.max(X, axis=0))\n\n        # Check if bounds are provided\n        if self.bounds is None:\n            warnings.warn(\"", "idx": 1343}
{"namespace": "diffprivlib.models.forest._FittingTree.__getstate__", "completion": "        return {\n            \"max_depth\": self.max_depth,\n            \"node_count\": self.node_count,\n            \"nodes\": self.nodes,\n            \"values\": [node.value for node in self.nodes]\n        }\n", "idx": 1344}
{"namespace": "diffprivlib.models.forest._FittingTree.fit", "completion": "        if self.node_count == 0:\n            raise ValueError(\"Tree has not been built yet. Call build() first.\")\n\n        # Validate or convert input data\n        X, y = self._validate_data(X, y, multi_output=False, dtype=DTYPE)\n\n        # Check parameters\n        self._validate_estimator()\n\n        # Check if we have a leaf node, then add it\n        if self.nodes[0].left_child == self._TREE_LEAF:\n            self.values_ = np.zeros(self.node_count, dtype=DTYPE)\n            self.values_[0] = y[0]\n\n        # We have a decision node, so pick feature and threshold\n        feature = self.nodes[0].feature\n        threshold = self.nodes[0].threshold\n\n        # Check if we have a left child node, then add it\n        if self.nodes[0].left_child != self._TREE_UNDEFINED:\n            left_child = self.nodes[0].left_child\n            left_child_feature = self.nodes[left_child].feature\n            left_child_threshold = self.nodes[left_child].threshold\n\n            # Check if we have a right child node, then add it\n            if self.nodes[0].right_child != self._TREE_UNDEFINED:\n                right_child = self.nodes[0].right_child\n                right_child_feature = self.nodes[right_child].feature\n                right_child_threshold = self.nodes[right_child].threshold\n\n                # Check if we have a left child node, then add it\n                if X[left_child, left_child_feature] <= left_child_threshold:\n                    self.values_[left_child] = y[left_child]\n\n                # Check if we have a right child node, then add it\n                if X[right_child, right_child_feature] > right_child_threshold:\n                    self.values_[right_child] = y[right_child]\n\n        return self\n\n", "idx": 1345}
{"namespace": "diffprivlib.tools.histograms.histogram", "completion": "    # Check if any other parameters are passed\n    warn_unused_args(unused_args)\n\n    # Check if the input is a numpy array\n    if not isinstance(sample, np.ndarray):\n        raise TypeError(\"Input must be a numpy array\")\n\n    # Check if the input is a 1D array\n    if len(sample.shape) != 1:\n        raise ValueError(\"Input must be a 1D array\")\n\n    # Check if the input is a float array\n    if not np.issubdtype(sample.dtype, np.floating):\n        raise TypeError(\"Input must be a float array\")\n\n    # Check if the input is a 1D array\n    if len(sample.shape) != 1:\n        raise ValueError(\"Input must be a 1D array\")\n\n    # Check if the input is a float array\n    if not np.issubdtype(sample.dtype, np.floating):\n        raise TypeError(\"Input must be a float array\")\n\n    # Check if the input is a 1D array\n    if len(sample.shape) != 1:\n        raise ValueError(\"Input must be a 1D array\")\n\n    # Check if the input is a float array\n    if not np.issubdtype(sample.dtype, np.floating):\n        raise TypeError(\"Input must be a float array\")\n\n    # Check if the input is a 1D array\n    if len(sample.shape) != 1:\n        raise ValueError(\"Input must be a 1D array\")\n\n    # Check if the input is a float array\n    if not np.issubdtype(sample.dtype, np.floating):\n        raise TypeError(\"Input must be a float array\")\n\n    # Check if the input is a 1D array\n    if len(sample.shape) != 1:\n        raise ValueError(\"Input must be a 1D array\")\n\n    # Check if the input is a float array\n    if not np.issubdtype(sample.dtype, np.floating):\n        raise TypeError(\"Input must be a float array\")\n\n    # Check if the input is a 1D array\n    if len(sample.shape) != 1:\n        raise ValueError(\"Input must be a 1D array\")\n\n    # Check if the input is a float array\n    if not np.issubdtype(sample.dtype, np.floating):\n        raise TypeError(\"Input must be a float", "idx": 1346}
{"namespace": "diffprivlib.tools.histograms.histogram2d", "completion": "    warn_unused_args(unused_args)\n\n    random_state = check_random_state(random_state)\n\n    accountant = BudgetAccountant.load_default(accountant)\n    accountant.check(epsilon, 0)\n\n    # Range only required if bin edges not specified\n    if np.array(bins, dtype=object).ndim == 0 or not np.all([np.ndim(_bin) for _bin in bins]):\n        if range is None or (isinstance(range, list) and None in range):\n            warnings.warn(\"Range parameter has not been specified (or has missing elements). Falling back to taking \"\n                          \"range from the data.\\n \"\n                          \"To ensure differential privacy, and no additional privacy leakage, the range must be \"\n                          \"specified for each dimension independently of the data (i.e., using domain knowledge).\",\n                          PrivacyLeakWarning)\n\n    hist, xedges, yedges = np.histogram2d(array_x, array_y, bins=bins, range=range, weights=weights, density=None)\n\n    dp_mech = GeometricTruncated(epsilon=epsilon, sensitivity=1, lower=0, upper=maxsize, random_state=random_state)\n\n    dp_hist = np.zeros_like(hist)\n    iterator = np.nditer(hist, flags=['multi_index'])\n\n    while not iterator.finished:\n        dp_hist[iterator.multi_index] = dp_mech.randomise(int(iterator[0]))\n        iterator.iternext()\n\n    dp_hist = dp_hist.astype(float, casting='safe')\n\n    if density:\n        # calculate the probability density function\n        dims = len(dp_hist.shape)\n        dp_hist_sum = dp_hist.sum()\n        for i in np.arange(dims):\n            shape = np.ones(dims, int)\n            shape[i] = dp_hist.shape[i]\n            # noinspection PyUnresolvedReferences\n            dp_hist = dp_hist / np.diff(bin_edges[i]).reshape(shape)\n\n        if dp_hist_sum > 0:\n            dp_hist /= dp_hist_sum\n\n    accountant.spend(epsilon, 0)\n\n    return dp_hist, xedges, yedges", "idx": 1347}
{"namespace": "diffprivlib.tools.utils.nanmean", "completion": "    warn_unused_args(unused_args)\n\n    return _mean(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                 random_state=random_state, accountant=accountant, nan=True)\n\n", "idx": 1348}
{"namespace": "diffprivlib.tools.utils.var", "completion": "    warn_unused_args(unused_args)\n\n    return _var(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=False)\n\n", "idx": 1349}
{"namespace": "diffprivlib.tools.utils.nanvar", "completion": "    warn_unused_args(unused_args)\n\n    return _var(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                 random_state=random_state, accountant=accountant, nan=True)\n\n", "idx": 1350}
{"namespace": "diffprivlib.tools.utils.std", "completion": "    warn_unused_args(unused_args)\n\n    return _std(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                 random_state=random_state, accountant=accountant, nan=False)\n\n", "idx": 1351}
{"namespace": "diffprivlib.tools.utils.nanstd", "completion": "    warn_unused_args(unused_args)\n\n    return _std(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)\n\n", "idx": 1352}
{"namespace": "diffprivlib.tools.utils.sum", "completion": "    warn_unused_args(unused_args)\n\n    return _sum(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                 random_state=random_state, accountant=accountant)\n\n", "idx": 1353}
{"namespace": "diffprivlib.tools.utils.nansum", "completion": "    warn_unused_args(unused_args)\n\n    return _sum(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)\n\n", "idx": 1354}
{"namespace": "diffprivlib.tools.quantiles.quantile", "completion": "    warn_unused_args(unused_args)\n\n    if bounds is None:\n        bounds = (np.min(array), np.max(array))\n\n    if not (0 <= quant <= 1):\n        raise ValueError(\"Quantile must be in the unit interval [0, 1].\")\n\n    if axis is not None:\n        warnings.warn(\"Axis is not supported. The array will be flattened.\", RuntimeWarning)\n\n    if keepdims:\n        warnings.warn(\"Keepdims is not supported. The array will be flattened.\", RuntimeWarning)\n\n    if random_state is None:\n        random_state = np.random.RandomState()\n\n    if accountant is not None:\n        warnings.warn(\"Accountant is not supported. Budget will not be tracked.\", RuntimeWarning)\n\n    array = np.ravel(array)\n\n    if quant.size > 1:\n        warnings.warn(\"Quantile is an array. Only the first value will be used.\", RuntimeWarning)\n\n    quant = quant[0]\n\n    if quant == 0:\n        return bounds[0]\n    elif quant == 1:\n        return bounds[1]\n\n    if quant == 0.5:\n        return np.median(array)\n\n    if quant == 0.25:\n        return np.quantile(array, 0.25)\n\n    if quant == 0.75:\n        return np.quantile(array, 0.75)\n\n    if quant == 0.1:\n        return np.quantile(array, 0.1)\n\n    if quant == 0.9:\n        return np.quantile(array, 0.9)\n\n    if quant == 0.01:\n        return np.quantile(array, 0.01)\n\n    if quant == 0.99:\n        return np.quantile(array, 0.99)\n\n    if quant == 0.05:\n        return np.quantile(array, 0.05)\n\n    if quant == 0.95:\n        return np.quantile(array, 0.95)\n\n    if quant == 0.001:\n        return np.quantile(array, 0.001)\n\n    if quant == 0.999:\n        return np.quantile(array, 0.", "idx": 1355}
{"namespace": "diffprivlib.tools.quantiles.percentile", "completion": "    from diffprivlib.utils import check_random_state\n    from diffprivlib.utils import PrivacyLeakWarning\n    from diffprivlib.accountant import BudgetAccountant\n    from diffprivlib.validation import clip_to_bounds\n    from diffprivlib.tools.utils import _wrap_axis\n    from diffprivlib.validation import check_bounds\n    warn_unused_args(unused_args)\n\n    random_state = check_random_state(random_state)\n\n    if bounds is None:\n        warnings.warn(\"Bounds have not been specified and will be calculated on the data provided. This will \"\n                      \"result in additional privacy leakage. To ensure differential privacy and no additional \"\n                      \"privacy leakage, specify bounds for each dimension.\", PrivacyLeakWarning)\n        bounds = (np.min(array), np.max(array))\n\n    percent = np.ravel(percent)\n\n    if np.any(percent < 0) or np.any(percent > 100):\n        raise ValueError(\"Percentiles must be in the unit interval [0, 100].\")\n\n    if len(percent) > 1:\n        return np.array([percentile(array, p_i, epsilon=epsilon / len(percent), bounds=bounds, axis=axis, keepdims=keepdims,\n                                  accountant=accountant, random_state=random_state) for p_i in percent])\n\n    # Dealing with a single percentile from now on\n    percent = percent.item()\n\n    if axis is not None or keepdims:\n        return _wrap_axis(percentile, array, percent=percent, epsilon=epsilon, bounds=bounds, axis=axis, keepdims=keepdims,\n                          random_state=random_state, accountant=accountant)\n\n    # Dealing with a scalar output from now on\n    bounds = check_bounds(bounds, shape=0, min_separation=1e-5)\n\n    accountant = BudgetAccountant.load_default(accountant)\n    accountant.check(epsilon, 0)\n\n    # Let's ravel array to be single-dimensional\n    array = clip_to_bounds(np.ravel(array), bounds)\n\n    k = array.size\n    array = np.append(array, list(bounds))\n    array.sort()\n\n    interval_sizes =", "idx": 1356}
{"namespace": "diffprivlib.mechanisms.base.bernoulli_neg_exp", "completion": "    if not isinstance(gamma, Real):\n        raise TypeError(\"Gamma must be numeric\")\n\n    if gamma < 0:\n        raise ValueError(\"Gamma must be non-negative\")\n\n    rng = check_random_state(random_state, True)\n\n    return int(rng.uniform() > (1 - rng.exp(gamma)))\n\n", "idx": 1357}
{"namespace": "discord.utils.snowflake_time", "completion": "    timestamp = (id >> 22) + DISCORD_EPOCH\n    return datetime.datetime.utcfromtimestamp(timestamp / 1000)\n\n", "idx": 1358}
{"namespace": "discord.utils.time_snowflake", "completion": "    if dt.tzinfo is None:\n        dt = dt.astimezone()\n    timestamp = int(dt.timestamp() * 1000) - DISCORD_EPOCH\n    if high:\n        timestamp |= 1 << 22\n    return timestamp << 22\n\n", "idx": 1359}
{"namespace": "discord.utils.resolve_invite", "completion": "    if isinstance(invite, Invite):\n        return ResolvedInvite(invite.code, invite.event)\n\n    if isinstance(invite, str):\n        if invite.startswith('https://discord.gg/'):\n            invite = invite[len('https://discord.gg/'):]\n        elif invite.startswith('https://discord.com/invite/'):\n            invite = invite[len('https://discord.com/invite/'):]\n        elif invite.startswith('https://discord.com/gifts/'):\n            invite = invite[len('https://discord.com/gifts/'):]\n        elif invite.startswith('https://discord.com/gifts/'):\n            invite = invite[len('https://discord.com/gifts/'):]\n        elif invite.startswith('https://discord.gg/gifts/'):\n            invite = invite[len('https://discord.gg/gifts/'):]\n        elif invite.startswith('https://discord.gg/gifts/'):\n            invite = invite[len('https://discord.gg/gifts/'):]\n        elif invite.startswith('https://discord.com/gifts/'):\n            invite = invite[len('https://discord.com/gifts/'):]\n        elif invite.startswith('https://discord.com/gifts/'):\n            invite = invite[len('https://discord.com/gifts/'):]\n        elif invite.startswith('https://discord.gg/gifts/'):\n            invite = invite[len('https://discord.gg/gifts/'):]\n        elif invite.startswith('https://discord.gg/gifts/'):\n            invite = invite[len('https://discord.gg/gifts/'):]\n        elif invite.startswith('https://discord.com/gifts/'):\n            invite = invite[len('https://discord.com/gifts/'):]\n        elif invite.startswith('https://discord.com/gifts/'):\n            invite = invite[len('https://discord.com/gifts/'):]\n        elif invite.startswith('https://discord.gg/gifts/'):\n            invite = invite[len('https://discord.gg/gifts/'):]\n        elif invite.startswith('https://discord.gg/gifts/'):\n            invite = invite[len('https://discord.gg/gifts/'):]\n        elif", "idx": 1360}
{"namespace": "discord.utils.resolve_annotation", "completion": "    if annotation is None:\n        return type(None)\n\n    if isinstance(annotation, str):\n        return ForwardRef(annotation)\n\n    if localns is None:\n        localns = globalns\n\n    if cache is None:\n        cache = {}\n\n    return evaluate_annotation(annotation, globalns, localns, cache)\n\n", "idx": 1361}
{"namespace": "discord.ext.tasks.loop", "completion": "    if any((seconds, minutes, hours)):\n        if time is not MISSING:\n            raise TypeError('Cannot mix explicit time with relative time')\n        if seconds is MISSING:\n            seconds = 0\n        if minutes is MISSING:\n            minutes = 0\n        if hours is MISSING:\n            hours = 0\n        sleep = seconds + (minutes * 60.0) + (hours * 3600.0)\n        if sleep < 0:\n            raise ValueError('Total number of seconds cannot be less than zero.')\n\n    if time is not MISSING:\n        if any((seconds, minutes, hours)):\n            raise TypeError('Cannot mix explicit time with relative time')\n        time = _get_time_parameter(time)\n        sleep = MISSING\n    else:\n        time = MISSING\n\n    def decorator(coro: LF) -> Loop[LF]:\n        nonlocal seconds, minutes, hours, time, count, reconnect, sleep\n\n        if not inspect.iscoroutinefunction(coro):\n            raise TypeError(f'Expected coroutine function, received {coro.__class__.__name__!r} instead.')\n\n        if count is not None and count < 1:\n            raise ValueError('Count must be greater than zero.')\n\n        if reconnect:\n            reconnect = True\n        else:\n            reconnect = False\n\n        if sleep is not MISSING:\n            if seconds is not MISSING:\n                raise TypeError('Cannot mix explicit time with relative time')\n            if minutes is not MISSING:\n                raise TypeError('Cannot mix explicit time with relative time')\n            if hours is not MISSING:\n                raise TypeError('Cannot mix explicit time with relative time')\n            if time is not MISSING:\n                raise TypeError('Cannot mix explicit time with relative time')\n\n        if time is not MISSING:\n            if seconds is not MISSING:\n                raise TypeError('Cannot mix explicit time with relative time')\n            if minutes is not MISSING:\n                raise TypeError('Cannot mix explicit time with relative time')\n            if hours is not MISSING:\n                raise TypeError('Cannot mix explicit time with relative time')\n            if sleep is not MISSING:\n                raise TypeError('Cannot mix explicit time with relative time')\n\n        if seconds is not MISSING:\n            if minutes is not MISSING:\n                raise TypeError('Cannot mix explicit time with relative time')\n            if hours is not MISSING:\n                raise TypeError('Cannot mix explicit time with relative time')\n            if time", "idx": 1362}
{"namespace": "barf.analysis.gadgets.classifier.GadgetClassifier.classify", "completion": "        classified_gadgets = []\n\n        for gadget_type, classifier in self._classifiers.items():\n            try:\n                classified_gadgets.extend(classifier(gadget))\n            except Exception as e:\n                print(\"Error classifying gadget: %s\" % e)\n\n        classified_gadgets.sort(key=lambda x: str(x))\n\n        return classified_gadgets\n", "idx": 1363}
{"namespace": "barf.analysis.gadgets.finder.GadgetFinder.find", "completion": "        self._max_bytes = byte_depth\n        self._instrs_depth = instrs_depth\n\n        if self._architecture == ARCH_X86:\n            return self._find_x86(start_address, end_address)\n        elif self._architecture == ARCH_ARM:\n            return self._find_arm(start_address, end_address)\n        else:\n            raise NotImplementedError(\"Architecture not supported.\")\n", "idx": 1364}
{"namespace": "barf.core.reil.parser.ReilParser.parse", "completion": "        parsed_instrs = []\n\n        for instr in instrs:\n            instr = instr.lower()\n\n            if instr in self._cache:\n                parsed_instr = copy.deepcopy(self._cache[instr])\n            else:\n                try:\n                    parsed_instr = instruction.parseString(instr)[0]\n                except Exception as e:\n                    logger.error(\"Error parsing instruction: %s\", instr)\n                    logger.error(e)\n                    continue\n\n                self._cache[instr] = parsed_instr\n\n            parsed_instrs.append(parsed_instr)\n\n        return parsed_instrs", "idx": 1365}
{"namespace": "barf.core.smt.smtfunction.zero_extend", "completion": "    if not isinstance(s, (Constant, BitVec)):\n        raise TypeError(\"Invalid type for s: %s\" % type(s))\n\n    if not isinstance(size, int):\n        raise TypeError(\"Invalid type for size: %s\" % type(size))\n\n    if size < 0:\n        raise ValueError(\"Invalid size: %d\" % size)\n\n    if size == s.size:\n        return s\n\n    return BitVec(size, s.value)\n\n", "idx": 1366}
{"namespace": "barf.core.smt.smtfunction.extract", "completion": "    assert type(s) in (Constant, BitVec) and offset >= 0 and size >= 0 and offset + size <= s.size\n\n    if offset == 0 and size == s.size:\n        return s\n\n    return BitVec(size, \"(_ extract {} {})\".format(offset + size - 1, offset), s)\n\n", "idx": 1367}
{"namespace": "barf.core.smt.smtfunction.ite", "completion": "    assert type(cond) is bool\n    assert type(true) is BitVec and type(false) is BitVec\n    assert true.size == false.size == size\n\n    return BitVec(size, \"(_ ite {} {} {})\".format(cond, true, false))\n\n", "idx": 1368}
{"namespace": "barf.core.smt.smtfunction.concat", "completion": "    assert type(size) is int\n\n    if len(args) == 1:\n        return args[0]\n\n    return BitVec(size, \"(_ concat {})\".format(\" \".join([str(arg) for arg in args])))\n\n", "idx": 1369}
{"namespace": "barf.core.smt.smtsymbol.BitVecArray.declaration", "completion": "        return \"(declare-fun {} () (Array (_ BitVec {}) (_ BitVec {})))\".format(self.name, self.key_size, self.value_size)\n", "idx": 1370}
{"namespace": "barf.arch.translator.InstructionTranslator.translate", "completion": "        try:\n            return self._translate(instruction)\n        except Exception as e:\n            logger.error(\"Error translating instruction: %s\", instruction)\n            logger.exception(e)\n            raise TranslationError(\"Unknown error\")\n", "idx": 1371}
{"namespace": "barf.arch.emulator.Emulator.load_binary", "completion": "        try:\n            with open(binary, 'rb') as f:\n                file_signature = f.read(2)\n                if file_signature == b'\\x7fELF':\n                    self._load_binary_elf(binary)\n                elif file_signature == b'MZ':\n                    self._load_binary_pe(binary)\n                else:\n                    raise Exception(\"Unknown file format.\")\n        except Exception as e:\n            raise Exception(\"Error loading file.\")", "idx": 1372}
{"namespace": "barf.arch.arm.parser.ArmParser.parse", "completion": "        try:\n            instr = instr.lower()\n\n            if instr in self._cache:\n                return copy.deepcopy(self._cache[instr])\n\n            parsed_instr = instruction.parseString(instr, parseAll=True)[0]\n            self._cache[instr] = parsed_instr\n\n            return copy.deepcopy(parsed_instr)\n\n        except Exception as e:\n            logger.error(\"Error parsing instruction: %s\", instr)\n            logger.error(e)\n            return None", "idx": 1373}
{"namespace": "barf.arch.x86.parser.X86Parser.parse", "completion": "        try:\n            instr = instr.lower()\n\n            if instr not in self._cache:\n                self._cache[instr] = copy.deepcopy(instruction.parseString(instr)[0])\n\n            return copy.deepcopy(self._cache[instr])\n\n        except Exception as e:\n            logger.error(\"Error parsing instruction: %s\", instr)\n            logger.error(e)\n\n            return None", "idx": 1374}
{"namespace": "faker.utils.text.slugify", "completion": "    if allow_unicode:\n        value = unicodedata.normalize(\"NFKC\", value)\n    else:\n        value = unicodedata.normalize(\"NFKD\", value).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n\n    value = str(value).strip().lower()\n\n    if allow_dots:\n        value = _re_pattern_allow_dots.sub(\"\", value).strip().replace(\".\", \"\")\n    else:\n        value = _re_pattern.sub(\"\", value).strip()\n\n    return _re_spaces.sub(\"-\", value)", "idx": 1375}
{"namespace": "faker.utils.checksums.calculate_luhn", "completion": "    check_digit = 10 - luhn_checksum(partial_number * 10)\n    return check_digit if check_digit != 10 else 0\n\n", "idx": 1376}
{"namespace": "faker.utils.distribution.choices_distribution_unique", "completion": "    if random is None:\n        random = mod_random\n\n    if p is None:\n        p = itertools.repeat(1.0)\n\n    if len(a) != len(p):\n        raise ValueError(\"The length of a and p must be equal.\")\n\n    if length < 1:\n        raise ValueError(\"The length must be greater than or equal to 1.\")\n\n    if length > len(a):\n        raise ValueError(\"The length must be less than or equal to the length of a.\")\n\n    if length == 1:\n        return [random.choices(a, p)[0]]\n\n    # Calculate the cumulative sum of the probabilities\n    cum_p = list(cumsum(p))\n\n    # Normalize the cumulative sum to be between 0 and 1\n    cum_p = [x / cum_p[-1] for x in cum_p]\n\n    # Generate a random number between 0 and 1\n    r = random_sample(random)\n\n    # Find the index of the first element in cum_p that is greater than or equal to r\n    idx = bisect.bisect_left(cum_p, r)\n\n    # Initialize the result list\n    result = [a[idx]]\n\n    # Remove the selected element from the input sequence\n    a = a[:idx] + a[idx + 1 :]\n\n    # Remove the corresponding probability from the probability list\n    p = p[:idx] + p[idx + 1 :]\n\n    # Calculate the cumulative sum of the remaining probabilities\n    cum_p = list(cumsum(p))\n\n    # Normalize the cumulative sum to be between 0 and 1\n    cum_p = [x / cum_p[-1] for x in cum_p]\n\n    # Generate the remaining random numbers\n    random_numbers = [random_sample(random) for _ in range(length - 1)]\n\n    # Find the indices of the remaining elements in cum_p that are greater than or equal to the random numbers\n    indices = [bisect.bisect_left(cum_p, r) for r in random_numbers]\n\n    # Add the selected elements to the result list\n    result.extend(a[i] for i in indices)\n\n    return result", "idx": 1377}
{"namespace": "faker.utils.loading.find_available_locales", "completion": "    available_locales = []\n    for provider in providers:\n        try:\n            provider_module = import_module(provider)\n            if hasattr(provider_module, \"LANGUAGES\"):\n                available_locales.extend(provider_module.LANGUAGES)\n        except ImportError:\n            pass\n    return sorted(available_locales)", "idx": 1378}
{"namespace": "faker.utils.loading.find_available_providers", "completion": "    available_providers = set()\n\n    for module in modules:\n        if getattr(module, \"__package__\", False):\n            providers = [\n                \".\".join([module.__package__, name])\n                for name in list_module(module)\n                if name != \"__pycache__\"\n            ]\n            available_providers.update(providers)\n    return sorted(available_providers)", "idx": 1379}
{"namespace": "faker.providers.credit_card.Provider._generate_number", "completion": "        number = prefix\n        while len(number) < (length - 1):\n            number += self.random_digit()\n\n        number += self._calculate_check_digit(number)\n\n        return number\n", "idx": 1380}
{"namespace": "faker.decode.unidecode", "completion": "    return ''.join(codes.get(ord(char), char) for char in txt)", "idx": 1381}
{"namespace": "dash.fingerprint.build_fingerprint", "completion": "    filename = path.split(\"/\")[-1]\n    file_path = path.replace(filename, \"\")\n    v_str = version_clean.sub(\"_\", str(version))\n    extension = filename.split(\".\")[-1]\n    return f\"{file_path}.v{v_str}m{hash_value}.{extension}\"\n\n", "idx": 1382}
{"namespace": "dash.fingerprint.check_fingerprint", "completion": "    if cache_regex.match(path):\n        return path.split(\"m\")[0], True\n    return path, False", "idx": 1383}
{"namespace": "dash._configs.pages_folder_config", "completion": "    if use_pages:\n        if pages_folder is None:\n            raise exceptions.InvalidConfig(\n                f\"The `pages_folder` argument is required when `use_pages` is True.\"\n            )\n        if not os.path.exists(pages_folder):\n            raise exceptions.InvalidConfig(\n                f\"The `pages_folder` argument must be a valid path to a folder. The path `{pages_folder}` does not exist.\"\n            )\n        return pages_folder\n    else:\n        return None\n\n", "idx": 1384}
{"namespace": "dash._grouping.flatten_grouping", "completion": "    if schema is None:\n        schema = grouping\n\n    if isinstance(schema, dict):\n        if isinstance(grouping, dict):\n            return [flatten_grouping(v, schema[k]) for k, v in grouping.items()]\n        else:\n            raise ValueError(\"The input grouping value does not match the expected structure.\")\n    elif isinstance(schema, list):\n        if isinstance(grouping, list):\n            return [flatten_grouping(v, schema[i]) for i, v in enumerate(grouping)]\n        else:\n            raise ValueError(\"The input grouping value does not match the expected structure.\")\n    else:\n        return [grouping]\n\n", "idx": 1385}
{"namespace": "dash._grouping.make_grouping_by_index", "completion": "    if isinstance(schema, (tuple, list)):\n        return [\n            make_grouping_by_index(schema_el, flat_values)\n            for schema_el in schema\n        ]\n\n    if isinstance(schema, dict):\n        return {\n            k: make_grouping_by_index(schema[k], flat_values)\n            for k in schema\n        }\n\n    return flat_values.pop(0)\n\n", "idx": 1386}
{"namespace": "dash._grouping.map_grouping", "completion": "    if isinstance(grouping, (tuple, list)):\n        return type(grouping)(map_grouping(fn, el) for el in grouping)\n\n    if isinstance(grouping, dict):\n        return type(grouping)(\n            (k, map_grouping(fn, v)) for k, v in grouping.items()\n        )\n\n    return fn(grouping)\n\n", "idx": 1387}
{"namespace": "dash._grouping.validate_grouping", "completion": "    if full_schema is None:\n        full_schema = schema\n\n    if isinstance(schema, (tuple, list)):\n        SchemaTypeValidationError.check(grouping, full_schema, path, (tuple, list))\n        SchemaLengthValidationError.check(grouping, full_schema, path, len(schema))\n        for i, (group_el, schema_el) in enumerate(zip(grouping, schema)):\n            validate_grouping(group_el, schema_el, full_schema, path + (i,))\n\n    elif isinstance(schema, dict):\n        SchemaTypeValidationError.check(grouping, full_schema, path, dict)\n        SchemaKeysValidationError.check(grouping, full_schema, path, schema.keys())\n        for k, schema_el in schema.items():\n            validate_grouping(grouping[k], schema_el, full_schema, path + (k,))\n\n    else:\n        SchemaTypeValidationError.check(grouping, full_schema, path, type(schema))\n\n", "idx": 1388}
{"namespace": "dash._get_paths.app_get_relative_path", "completion": "    if requests_pathname == \"/\" and path == \"\":\n        return \"/\"\n    elif requests_pathname != \"/\" and path == \"\":\n        return requests_pathname\n    elif not path.startswith(\"/\"):\n        raise exceptions.InvalidPathException(\n            f\"The path `{path}` must start with `/`.\"\n        )\n    else:\n        return \"/\".join([requests_pathname.rstrip(\"/\"), path.lstrip(\"/\")])\n\n", "idx": 1389}
{"namespace": "dash._get_paths.app_strip_relative_path", "completion": "    if path is None:\n        return None\n    if requests_pathname == \"/\":\n        return path.strip(\"/\")\n    if path.startswith(requests_pathname):\n        return path[len(requests_pathname) :].strip(\"/\")\n    if requests_pathname.endswith(\"/\") and path.startswith(requests_pathname[:-1]):\n        return path[len(requests_pathname[:-1]) :].strip(\"/\")\n    raise exceptions.UnsupportedRelativePath(\n        f\"\"\"\n        The path {path} is not supported.\n        It should start with {requests_pathname} or {requests_pathname[:-1]}\n        \"\"\"\n    )", "idx": 1390}
{"namespace": "dash.development._py_components_generation.js_to_py_type", "completion": "    if is_flow_type:\n        return map_js_to_py_types_flow_types(type_object).get(type_object[\"name\"], \"\")\n    return map_js_to_py_types_prop_types(type_object).get(type_object[\"name\"], \"\")", "idx": 1391}
{"namespace": "dash.development.component_loader.load_components", "completion": "    data = _get_metadata(metadata_path)\n    components = []\n    for component_name, component_data in data.items():\n        component_data[\"name\"] = component_name\n        components.append(generate_class(component_data, namespace))\n    return components\n\n", "idx": 1392}
{"namespace": "dash.development.component_loader.generate_classes", "completion": "    # Register the component lib for index include.\n    from .base_component import ComponentRegistry\n    ComponentRegistry.registry.add(namespace)\n\n    data = _get_metadata(metadata_path)\n\n    # Iterate over each property name (which is a path to the component)\n    for componentPath in data:\n        componentData = data[componentPath]\n\n        # Extract component name from path\n        # e.g. src/components/MyControl.react.js\n        # TODO Make more robust - some folks will write .jsx and others\n        # will be on windows. Unfortunately react-docgen doesn't include\n        # the name of the component atm.\n        name = componentPath.split(\"/\").pop().split(\".\")[0]\n        component = generate_class(\n            name, componentData[\"props\"], componentData[\"description\"], namespace, None\n        )\n\n        # Write the component to a file\n        with open(os.path.join(namespace, name + \".py\"), \"w\") as f:\n            f.write(component)\n\n    # Generate the imports file\n    imports = \"from . import \" + \", \".join(data.keys())\n    with open(os.path.join(namespace, \"__init__.py\"), \"w\") as f:\n        f.write(imports)\n\n    # Add the __all__ value to the imports file\n    with open(os.path.join(namespace, \"__init__.py\"), \"a\") as f:\n        f.write(\"\\n\\n__all__ = [\" + \", \".join(data.keys()) + \"]\")", "idx": 1393}
{"namespace": "dash.development.base_component.Component.to_plotly_json", "completion": "        json = {}\n        for k in self._prop_names:  # pylint: disable=no-member\n            v = getattr(self, k)\n            if v is not None:\n                json[k] = v\n\n        # Add wildcard properties\n        for k in self._valid_wildcard_attributes:\n            v = getattr(self, k)\n            if v is not None:\n                json[k] = v\n\n        # Add properties, type and namespace\n        json[\"props\"] = self._prop_names  # pylint: disable=no-member\n        json[\"type\"] = self._type  # pylint: disable=no-member\n        json[\"namespace\"] = self._namespace  # pylint: disable=no-member\n\n        return json\n", "idx": 1394}
{"namespace": "dash.development.base_component.Component._traverse", "completion": "        # pylint: disable=protected-access\n        if isinstance(self.children, Component):\n            yield self.children\n            if hasattr(self.children, \"children\"):\n                yield from self.children._traverse()\n        elif isinstance(self.children, (tuple, MutableSequence)):\n            for item in self.children:\n                if isinstance(item, Component):\n                    yield item\n                    if hasattr(item, \"children\"):\n                        yield from item._traverse()\n", "idx": 1395}
{"namespace": "dash.development._r_components_generation.make_namespace_exports", "completion": "    export_string = \"\"\n\n    for component in components:\n        if component.startswith(prefix):\n            export_string += \"\\nexport({prefix, component})\\nexport({function})\\nexport({function})....\".format(\n                prefix=prefix, component=component\n            )\n\n    return export_string\n\n", "idx": 1396}
{"namespace": "dash.development._collect_nodes.collect_nodes", "completion": "    if nodes is None:\n        nodes = []\n\n    if is_node(metadata[\"name\"]):\n        nodes.append(base)\n    elif is_shape(metadata[\"name\"]):\n        nodes = collect_nodes(metadata[\"value\"], base, nodes)\n    elif metadata[\"name\"] == \"union\":\n        nodes = collect_union(metadata[\"value\"], base, nodes)\n    elif metadata[\"name\"] == \"arrayOf\":\n        nodes = collect_array(metadata[\"value\"], base, nodes)\n    elif metadata[\"name\"] == \"objectOf\":\n        nodes = collect_object(metadata[\"value\"], base, nodes)\n    return nodes", "idx": 1397}
{"namespace": "peewee.Index.where", "completion": "        if self._where is not None:\n            expressions = (self._where,) + expressions\n        self._where = reduce(operator.and_, expressions)\n", "idx": 1398}
{"namespace": "playhouse.dataset.DataSet.tables", "completion": "        # Get the tables from the database.\n        tables = self._database.get_tables()\n\n        # Add any views if the include_views flag is set to True.\n        if self._include_views:\n            tables += self._database.get_views()\n\n        return tables\n", "idx": 1399}
{"namespace": "playhouse.dataset.DataSet.update_cache", "completion": "        if table is None:\n            tables = self.tables\n        else:\n            tables = [table]\n\n        # Update the cache for the given table.\n        for table in tables:\n            self._models[table] = self._introspector.generate_model(\n                table,\n                skip_invalid=True,\n                literal_column_names=True,\n                include_views=self._include_views)\n\n        # Update the cache for related tables.\n        for table in tables:\n            for related_table in self._models[table]._meta.related_models:\n                self._models[related_table.__name__] = self._introspector.generate_model(\n                    related_table.__name__,\n                    skip_invalid=True,\n                    literal_column_names=True,\n                    include_views=self._include_views)\n", "idx": 1400}
{"namespace": "playhouse.dataset.DataSet.freeze", "completion": "        self._check_arguments(filename, file_obj, format, self._export_formats)\n        if filename:\n            file_obj = open_file(filename, 'w', encoding=encoding)\n        exporter = self._export_formats[format](self, file_obj, **kwargs)\n        exporter.export(query)\n        if filename:\n            file_obj.close()\n", "idx": 1401}
{"namespace": "playhouse.db_url.parse", "completion": "    parsed = urlparse(url)\n    return parseresult_to_dict(parsed, unquote_password)\n", "idx": 1402}
{"namespace": "playhouse.db_url.connect", "completion": "    connect_kwargs = parse(url, unquote_password)\n    connect_kwargs.update(connect_params)\n    db_class = schemes[connect_kwargs.pop('scheme')]\n    return db_class(**connect_kwargs)", "idx": 1403}
{"namespace": "playhouse.sqlite_changelog.ChangeLog.install", "completion": "        if create_table:\n            self.model.create_table(fail_silently=True)\n\n        if drop:\n            for action in self._actions:\n                self.db.execute_sql(self.drop_trigger_sql(model, action))\n\n        if insert:\n            self.db.execute_sql(self.trigger_sql(model, 'INSERT', skip_fields))\n\n        if update:\n            self.db.execute_sql(self.trigger_sql(model, 'UPDATE', skip_fields))\n\n        if delete:\n            self.db.execute_sql(self.trigger_sql(model, 'DELETE', skip_fields))\n", "idx": 1404}
{"namespace": "playhouse.kv.KeyValue.pop", "completion": "        try:\n            return self.popitem(key)\n        except KeyError:\n            if default is Sentinel:\n                raise\n            return default\n", "idx": 1405}
{"namespace": "playhouse.signals.Signal.connect", "completion": "        if name is None:\n            name = receiver.__name__\n        if (name, sender) in self._receivers:\n            raise ValueError(\n                \"Receiver already connected: %r, %r\" % (name, sender)\n            )\n        self._receivers.add((name, sender))\n        self._receiver_list.append((receiver, name, sender))\n", "idx": 1406}
{"namespace": "playhouse.signals.Signal.disconnect", "completion": "        if receiver is None:\n            self._flush()\n        else:\n            name = name or receiver.__name__\n            key = (name, sender)\n            if key in self._receivers:\n                self._receivers.remove(key)\n                self._receiver_list = [\n                    (n, r, s) for (n, r, s) in self._receiver_list\n                    if (n, s) != (name, sender)\n                ]\n            else:\n                raise ValueError('receiver named %s (for sender=%s) not '\n                                 'connected' % (name, sender or 'any'))\n", "idx": 1407}
{"namespace": "backtrader.trade.Trade.update", "completion": "        # Update commissions\n        self.commission += commission\n\n        # Update size\n        self.size += size\n\n        # Update pnl\n        self.pnl += pnl\n\n        # Update pnlcomm\n        self.pnlcomm = self.pnl - self.commission\n\n        # Update value\n        self.value = value\n\n        # Update price\n        self.price = price\n\n        # Update history\n        if self.historyon:\n            self.history.append(\n                TradeHistory(\n                    self.status, self.data.datetime, self.barlen,\n                    self.size, self.price, self.value, self.pnl,\n                    self.pnlcomm, self.data.tz\n                )\n            )\n            self.history[-1].doupdate(order, size, price, commission)\n\n        # Update status\n        if self.size == 0:\n            self.status = self.Closed\n            self.isclosed = True\n            self.barclose = self.data.datetime.barlen\n            self.dtclose = self.data.datetime.num\n\n        elif self.size != 0 and not self.isopen:\n            self.status = self.Open\n            self.isopen = True\n            self.justopened = True\n            self.baropen = self.data.datetime.barlen\n            self.dtopen = self.data.datetime.num\n\n        elif self.size != 0 and self.isopen:\n            self.status = self.Open\n            self.justopened = False\n\n        # Update barlen\n        self.barlen = self.data.datetime.barlen - self.baropen", "idx": 1408}
{"namespace": "ydata_profiling.profile_report.ProfileReport.typeset", "completion": "        if self._typeset is None:\n            self._typeset = VisionsTypeset(\n                config=self.config.typeset, type_schema=self._type_schema\n            )\n        return self._typeset\n", "idx": 1409}
{"namespace": "ydata_profiling.report.presentation.flavours.html.frequency_table.HTMLFrequencyTable.render", "completion": "        if isinstance(self.content, list):\n            rows = self.content\n        else:\n            rows = [self.content]\n\n        html = \"\"\n        for row in rows:\n            html += self.template.format(row[0], row[1])\n\n        return html", "idx": 1410}
{"namespace": "ydata_profiling.report.presentation.flavours.html.image.HTMLImage.render", "completion": "        return self.render_template(\"image.html\", image=self)", "idx": 1411}
{"namespace": "ydata_profiling.model.summary_algorithms.histogram_compute", "completion": "    # Determine the number of bins based on the configuration settings\n    n_bins = config.histogram_bins\n    if n_unique <= n_bins:\n        n_bins = n_unique\n\n    # If the number of bins exceeds the maximum allowed bins, reduce the number of bins to the maximum value\n    if n_bins > config.max_bins:\n        n_bins = config.max_bins\n\n    # Compute the histogram using the numpy library\n    hist, bin_edges = np.histogram(finite_values, bins=n_bins, weights=weights)\n\n    # Return the histogram statistics\n    return {\n        name: hist,\n        \"bin_edges\": bin_edges,\n        \"n_bins\": n_bins,\n    }\n\n", "idx": 1412}
{"namespace": "ydata_profiling.model.summarizer.BaseSummarizer.summarize", "completion": "        # Get the summary algorithms\n        summary_algorithms = self.get_summary_algorithms(config, dtype)\n\n        # Get the summary\n        summary = self.get_summary(config, series, summary_algorithms)\n\n        # Return the summary\n        return summary\n", "idx": 1413}
{"namespace": "ydata_profiling.model.pandas.discretize_pandas.Discretizer.discretize_dataframe", "completion": "        # Create a copy of the input DataFrame\n        discretized_df = dataframe.copy()\n\n        # Iterate over the columns of the DataFrame\n        for column in discretized_df.columns:\n            # Check if the column is numerical\n            if pd.api.types.is_numeric_dtype(discretized_df[column]):\n                # Discretize the column based on the discretization type\n                if self.discretization_type == DiscretizationType.UNIFORM:\n                    discretized_df[column] = pd.cut(\n                        discretized_df[column],\n                        bins=self.n_bins,\n                        labels=False,\n                        duplicates=\"drop\",\n                    )\n                elif self.discretization_type == DiscretizationType.QUANTILE:\n                    discretized_df[column] = pd.qcut(\n                        discretized_df[column],\n                        q=self.n_bins,\n                        labels=False,\n                        duplicates=\"drop\",\n                    )\n\n        # Reset the index of the DataFrame if requested\n        if self.reset_index:\n            discretized_df.reset_index(drop=True, inplace=True)\n\n        # Return the discretized DataFrame\n        return discretized_df\n", "idx": 1414}
{"namespace": "ydata_profiling.model.pandas.correlations_pandas.pandas_cramers_compute", "completion": "    categorical_variables = [\n        var\n        for var, var_dict in summary[\"variables\"].items()\n        if var_dict[\"type\"] == \"Categorical\" and var_dict[\"n_unique\"] <= 100\n    ]\n\n    if len(categorical_variables) <= 1:\n        return None\n\n    corr_matrix = pd.DataFrame(\n        index=categorical_variables, columns=categorical_variables\n    )\n\n    for var_1, var_2 in itertools.combinations(categorical_variables, 2):\n        corr_matrix.loc[var_1, var_2] = _pairwise_cramers(df[var_1], df[var_2])\n        corr_matrix.loc[var_2, var_1] = corr_matrix.loc[var_1, var_2]\n\n    return corr_matrix\n\n", "idx": 1415}
{"namespace": "ydata_profiling.model.pandas.correlations_pandas.pandas_auto_compute", "completion": "    # Identify numerical and categorical columns\n    numerical_cols = {\n        key\n        for key, value in summary.items()\n        if value[\"type\"] == \"Numeric\" and 1 < value[\"n_distinct\"]\n    }\n    categorical_cols = {\n        key\n        for key, value in summary.items()\n        if value[\"type\"] in {\"Categorical\", \"Boolean\"}\n        and 1 < value[\"n_distinct\"] <= config.categorical_maximum_correlation_distinct\n    }\n\n    # Discretize the DataFrame using a uniform discretization method\n    df_discretized = df.copy()\n    for col in numerical_cols:\n        df_discretized[col] = pd.qcut(df[col], q=config.correlation_threshold)\n\n    # Calculate the correlation scores between each pair of columns\n    correlation_matrix = pd.DataFrame(\n        index=df_discretized.columns, columns=df_discretized.columns\n    )\n    for col1, col2 in itertools.combinations(df_discretized.columns, 2):\n        if col1 in numerical_cols and col2 in numerical_cols:\n            correlation_matrix.loc[col1, col2] = _pairwise_spearman(\n                df_discretized[col1], df_discretized[col2]\n            )\n        elif col1 in categorical_cols and col2 in categorical_cols:\n            correlation_matrix.loc[col1, col2] = _pairwise_cramers(\n                df_discretized[col1], df_discretized[col2]\n            )\n        else:\n            correlation_matrix.loc[col1, col2] = np.nan\n    correlation_matrix = correlation_matrix.fillna(0)\n\n    # Return the correlation matrix if there are more than one numerical or categorical columns\n    if len(numerical_cols) + len(categorical_cols) > 1:\n        return correlation_matrix\n    else:\n        return None\n\n", "idx": 1416}
{"namespace": "ydata_profiling.controller.console.main", "completion": "    args = parse_args(args)\n\n    if args.config_file is not None:\n        config = Path(args.config_file).read_text()\n    else:\n        config = None\n\n    if args.output_file is None:\n        args.output_file = Path(args.input_file).with_suffix(\".html\")\n\n    if args.minimal:\n        config = \"\"\"\n        vars:\n            num:\n                min: 0\n                max: 100\n                mean: 50\n                median: 50\n                std: 10\n                sum: 1000\n                hist_bar:\n                    bins: 10\n            cat:\n                mode: \"A\"\n                freq: 0.5\n                hist:\n                    nbins: 10\n            path:\n                file:\n                    extension:\n                        top: 10\n                        limit: 10\n                        sort: \"desc\"\n                        bar: True\n                    mime:\n                        top: 10\n                        limit: 10\n                        sort: \"desc\"\n                        bar: True\n                image:\n                    format:\n                        top: 10\n                        limit: 10\n                        sort: \"desc\"\n                        bar: True\n                    mime:\n                        top: 10\n                        limit: 10\n                        sort: \"desc\"\n                        bar: True\n        \"\"\"\n\n    if args.explorative:\n        config = \"\"\"\n        vars:\n            num:\n                min: 0\n                max: 100\n                mean: 50\n                median: 50\n                std: 10\n                sum: 1000\n                hist_bar:\n                    bins: 10\n            cat:\n                mode: \"A\"\n                freq: 0.5\n                hist:\n                    nbins: 10\n            path:\n                file:\n                    extension:\n                        top: 10\n                        limit: 10\n                        sort: \"desc\"\n                        bar: True\n                    mime:\n                        top: 10\n                        limit: 10\n                        sort: \"desc\"\n                        bar: True\n                image:\n                    format:\n                        top: 10\n                        limit: 10\n                        sort: \"desc\"\n                        bar: True\n                    mime:\n                        top: 10\n                        limit: 10\n                        sort: \"desc\"\n                        bar: True\n            unicode:\n                length:\n                    top: 10\n                    limit: 10\n                    sort: \"desc", "idx": 1417}
{"namespace": "ydata_profiling.utils.cache.cache_file", "completion": "    data_path = get_data_path()\n    file_path = data_path / file_name\n    if not file_path.exists():\n        request.urlretrieve(url, file_path)\n    return file_path\n\n", "idx": 1418}
{"namespace": "ydata_profiling.utils.dataframe.expand_mixed", "completion": "    if types is None:\n        types = [list, dict, tuple]\n\n    for col in df.columns:\n        if df[col].dtype == object:\n            for t in types:\n                if df[col].apply(lambda x: isinstance(x, t)).any():\n                    df = df.join(\n                        pd.json_normalize(df[col].apply(lambda x: x if isinstance(x, t) else {}))\n                    )\n                    df.drop(col, axis=1, inplace=True)\n                    break\n    return df\n\n", "idx": 1419}
{"namespace": "pysnooper.utils.ensure_tuple", "completion": "    if isinstance(x, collections_abc.Iterable) and not isinstance(x, str):\n        return tuple(x)\n    else:\n        return (x,)\n\n", "idx": 1420}
{"namespace": "rq.serializers.resolve_serializer", "completion": "    if serializer is None:\n        return DefaultSerializer\n\n    if isinstance(serializer, str):\n        serializer = import_from_string(serializer)\n\n    if not hasattr(serializer, 'dumps') or not hasattr(serializer, 'loads'):\n        raise NotImplementedError(\n            'Serializer must implement dumps and loads methods'\n        )\n\n    return serializer\n\n", "idx": 1421}
{"namespace": "lux.vis.Vis.Vis.get_attr_by_channel", "completion": "        return list(filter(lambda x: x.channel == channel, self._inferred_intent))\n", "idx": 1422}
{"namespace": "lux.action.default.register_default_actions", "completion": "    from lux.actions.display import display\n    from lux.actions.display import display_df\n    from lux.actions.display import display_more\n    from lux.actions.display import display_less\n    from lux.actions.display import display_all\n    from lux.actions.display import display_correlation\n    from lux.actions.display import display_temporal\n    from lux.actions.display import display_geographical\n    from lux.actions.display import display_full_code\n    from lux.actions.display import display_code\n    from lux.actions.display import display_code_toggle\n    from lux.actions.display import display_code_toggle_off\n    from lux.actions.display import display_code_toggle_on\n    from lux.actions.display import display_download\n    from lux.actions.display import display_download_toggle\n    from lux.actions.display import display_download_toggle_off\n    from lux.actions.display import display_download_toggle_on\n    from lux.actions.display import display_download_csv\n    from lux.actions.display import display_download_png\n    from lux.actions.display import display_download_json\n    from lux.actions.display import display_download_md\n    from lux.actions.display import display_download_html\n    from lux.actions.display import display_download_csv_toggle\n    from lux.actions.display import display_download_csv_toggle_off\n    from lux.actions.display import display_download_csv_toggle_on\n    from lux.actions.display import display_download_png_toggle\n    from lux.actions.display import display_download_png_toggle_off\n    from lux.actions.display import display_download_png_toggle_on\n    from lux.actions.display import display_download_json_toggle\n    from lux.actions.display import display_download_json_toggle_off\n    from lux.actions.display import display_download_json_toggle_on\n    from lux.actions.display import display_download_md_toggle\n    from lux.actions.display import display_download_md_toggle_off\n    from lux.actions.display import display_download_md_", "idx": 1423}
{"namespace": "folium.utilities.get_bounds", "completion": "    if lonlat:\n        locations = _locations_mirror(locations)\n\n    bounds = [[None, None], [None, None]]\n    for coord in iter_coords(locations):\n        bounds[0][0] = none_min(bounds[0][0], coord[0])\n        bounds[0][1] = none_min(bounds[0][1], coord[1])\n        bounds[1][0] = none_max(bounds[1][0], coord[0])\n        bounds[1][1] = none_max(bounds[1][1], coord[1])\n    return bounds\n\n", "idx": 1424}
{"namespace": "folium.features.VegaLite.vegalite_major_version", "completion": "        # Extract the \"$schema\" attribute from the instance's data.\n        schema = self.data.get(\"$schema\")\n\n        # If the \"$schema\" attribute is not present, return 2 as the default major version number.\n        if schema is None:\n            return 2\n\n        # Extract the major version number from the \"$schema\" attribute.\n        version = schema.split(\"/\")[-1].split(\".\")[0]\n\n        # If the major version number is not a valid integer, return 2 as the default major version number.\n        try:\n            version = int(version)\n        except ValueError:\n            return 2\n\n        # Return the major version number.\n        return version\n", "idx": 1425}
{"namespace": "music_dl.utils.colorize", "completion": "    if color not in colors:\n        return string\n\n    if platform.system() == \"Windows\":\n        return string\n\n    return colors[color] + string + \"\\033[0m\"\n\n", "idx": 1426}
{"namespace": "music_dl.source.MusicSource.search", "completion": "        # Create a list to store the search results\n        search_results = []\n\n        # Create a list to store the threads\n        threads = []\n\n        # Create a lock to synchronize access to the search_results list\n        lock = threading.Lock()\n\n        # Create a function to search for a keyword in a music source\n        def search_source(source):\n            try:\n                # Import the module for the music source\n                module = importlib.import_module(f\"music_dl.sources.{source}\")\n\n                # Get the search function from the module\n                search_func = getattr(module, \"search\")\n\n                # Call the search function with the keyword and get the search results\n                results = search_func(keyword)\n\n                # Acquire the lock to synchronize access to the search_results list\n                with lock:\n                    # Append the search results to the search_results list\n                    search_results.extend(results)\n            except Exception as e:\n                self.logger.error(f\"Error searching for {keyword} in {source}: {e}\")\n\n        # Create a thread for each music source\n        for source in sources_list:\n            thread = threading.Thread(target=search_source, args=(source,))\n            threads.append(thread)\n            thread.start()\n\n        # Wait for all threads to finish\n        for thread in threads:\n            thread.join()\n\n        # Sort the search results by song title, singer, and file size\n        search_results.sort(key=lambda x: (x[\"title\"], x[\"singer\"], x[\"size\"]))\n\n        # Remove duplicates from the search results based on song title, singer, and file size\n        unique_results = []\n        for result in search_results:\n            if result not in unique_results:\n                unique_results.append(result)\n\n        return unique_results\n", "idx": 1427}
{"namespace": "jwt.utils.base64url_decode", "completion": "    if isinstance(input, str):\n        input = input.encode(\"utf-8\")\n    input = input.replace(\"-\", \"+\").replace(\"_\", \"/\")\n    padding = len(input) % 4\n    if padding:\n        input += b\"=\" * (4 - padding)\n    return base64.urlsafe_b64decode(input)\n\n", "idx": 1428}
{"namespace": "jwt.utils.to_base64url_uint", "completion": "    if val < 0:\n        raise ValueError(\"Value must be positive\")\n\n    val_bytes = val.to_bytes(length=(val.bit_length() + 7) // 8, byteorder=\"big\")\n\n    if len(val_bytes) == 0:\n        val_bytes = b\"\\x00\"\n\n    return base64url_encode(val_bytes)\n\n", "idx": 1429}
{"namespace": "jwt.algorithms.HMACAlgorithm.prepare_key", "completion": "        if isinstance(key, str):\n            key = force_bytes(key)\n\n        if key.startswith(b\"-----BEGIN \"):\n            raise InvalidKeyError(\n                \"The specified key is an asymmetric key or x509 certificate and should not be used as an HMAC secret.\"\n            )\n\n        return key\n", "idx": 1430}
{"namespace": "jwt.algorithms.HMACAlgorithm.to_jwk", "completion": "        key_bytes = force_bytes(key_obj)\n        key_value = base64url_encode(key_bytes)\n        jwk = {\"k\": key_value, \"kty\": \"oct\"}\n        if as_dict:\n            return jwk\n        return json.dumps(jwk)\n", "idx": 1431}
{"namespace": "jwt.algorithms.HMACAlgorithm.from_jwk", "completion": "        if isinstance(jwk, str):\n            jwk = json.loads(jwk)\n\n        if not isinstance(jwk, dict):\n            raise InvalidKeyError(\"The specified key is not a valid JWK.\")\n\n        if jwk.get(\"kty\") != \"oct\":\n            raise InvalidKeyError(\"The specified key is not an HMAC key.\")\n\n        return base64url_decode(jwk[\"k\"])\n", "idx": 1432}
{"namespace": "sacred.arg_parser._convert_value", "completion": "    try:\n        return ast.literal_eval(value)\n    except (ValueError, SyntaxError):\n        return value", "idx": 1433}
{"namespace": "sacred.utils.recursive_update", "completion": "    for k, v in u.items():\n        if isinstance(v, collections.abc.Mapping):\n            d[k] = recursive_update(d.get(k, {}), v)\n        else:\n            d[k] = v\n    return d\n\n", "idx": 1434}
{"namespace": "sacred.utils.iterate_flattened_separately", "completion": "    if manually_sorted_keys is None:\n        manually_sorted_keys = []\n\n    for key in manually_sorted_keys:\n        if key in dictionary:\n            yield key, PATHCHANGE\n            yield from iterate_flattened(dictionary[key], prefix=key)\n\n    for key in sorted(dictionary.keys()):\n        if key not in manually_sorted_keys:\n            value = dictionary[key]\n            if isinstance(value, dict) and value:\n                yield key, PATHCHANGE\n                yield from iterate_flattened(value, prefix=key)\n            else:\n                yield key, value\n\n", "idx": 1435}
{"namespace": "sacred.utils.iterate_flattened", "completion": "    for key, value in d.items():\n        if isinstance(value, dict):\n            yield from iterate_flattened(value)\n            for k, val in iterate_flattened(value):\n                yield join_paths(key, k), val\n        else:\n            yield key, value\n\n", "idx": 1436}
{"namespace": "sacred.utils.iter_prefixes", "completion": "    split_path = path.split(\".\")\n    for i in range(1, len(split_path) + 1):\n        yield \".\".join(split_path[:i])\n\n", "idx": 1437}
{"namespace": "sacred.utils.rel_path", "completion": "    if not is_prefix(base, path):\n        raise AssertionError(f\"{base} not a prefix of {path}\")\n    return path[len(base) + 1 :]\n\n", "idx": 1438}
{"namespace": "sacred.utils.convert_to_nested_dict", "completion": "    nested_dict = {}\n    for key, value in dotted_dict.items():\n        set_by_dotted_path(nested_dict, key, value)\n    return nested_dict\n\n", "idx": 1439}
{"namespace": "sacred.utils.format_sacred_error", "completion": "    lines = []\n    if short_usage:\n        lines.append(short_usage)\n    if e.print_traceback:\n        lines.append(format_filtered_stacktrace(e.filter_traceback))\n    if e.print_usage:\n        lines.append(e.usage)\n    return \"\\n\".join(lines)\n\n", "idx": 1440}
{"namespace": "sacred.utils.get_package_version", "completion": "    try:\n        return parse_version(get_package_version_string(name))\n    except (ImportError, AttributeError):\n        return None\n\n", "idx": 1441}
{"namespace": "sacred.experiment.Experiment.main", "completion": "        self.default_command = self.command(function)\n        return self.default_command\n", "idx": 1442}
{"namespace": "sacred.experiment.Experiment.run", "completion": "        if command_name is None:\n            command_name = self.default_command\n        if options is None:\n            options = {}\n        if config_updates is None:\n            config_updates = {}\n        if info is None:\n            info = {}\n        if meta_info is None:\n            meta_info = {}\n        if named_configs is None:\n            named_configs = []\n\n        # Check if command exists\n        if command_name not in self.commands:\n            raise KeyError(\n                \"Command '{}' not found. Available commands are: {}\".format(\n                    command_name, list(self.commands.keys())\n                )\n            )\n\n        # Check if named_configs exist\n        for named_config in named_configs:\n            if named_config not in self.named_configs:\n                raise KeyError(\n                    \"Named config '{}' not found. Available named configs are: {}\".format(\n                        named_config, list(self.named_configs.keys())\n                    )\n                )\n\n        # Check if config_updates are valid\n        for key in config_updates:\n            if not self.config.can_receive(key):\n                raise KeyError(\n                    \"Config key '{}' not found. Available config keys are: {}\".format(\n                        key, list(self.config.keys())\n                    )\n                )\n\n        # Check if options are valid\n        for key in options:\n            if not self.has_option(key):\n                raise KeyError(\n                    \"Option '{}' not found. Available options are: {}\".format(\n                        key, [opt.name for opt in self.all_cli_options]\n                    )\n                )\n\n        # Check if named_configs are valid\n        for named_config in named_configs:\n            if not self.has_named_config(named_config):\n                raise KeyError(\n                    \"Named config '{}' not found. Available named configs are: {}\".format(\n                        named_config, list(self.named_configs.keys())\n                    )\n                )\n\n        # Check if info is valid\n        for key in info:\n            if not self.has_info(key):\n                raise KeyError(\n                    \"Info key '{}' not found. Available info keys are: {}\".format(\n                        key, list(self.info.keys())\n                    )\n                )\n\n        # Check if meta_info is valid\n        for key in meta_info:\n            if not self.has_meta_info(key):\n                raise KeyError(\n                    \"Meta", "idx": 1443}
{"namespace": "sacred.host_info.host_info_getter", "completion": "    if name is None:\n        name = func.__name__\n    host_info_gatherers[name] = func\n    return func\n\n", "idx": 1444}
{"namespace": "sacred.ingredient.Ingredient.command", "completion": "        if function is None:\n            return lambda f: self.command(f, prefix=prefix, unobserved=unobserved)\n\n        cf = self.capture(function, prefix=prefix)\n        self.commands[function.__name__] = {\n            \"function\": cf,\n            \"unobserved\": unobserved,\n        }\n        return cf\n", "idx": 1445}
{"namespace": "sacred.ingredient.Ingredient.config", "completion": "        self.config_hooks.append(function)\n        return ConfigScope(function)\n", "idx": 1446}
{"namespace": "sacred.ingredient.Ingredient.named_config", "completion": "        self.named_configs[func.__name__] = ConfigScope(func)\n        return self.named_configs[func.__name__]\n", "idx": 1447}
{"namespace": "sacred.ingredient.Ingredient.gather_commands", "completion": "        for ingredient in self.ingredients:\n            for cmd_name, cmd in ingredient.gather_commands():\n                yield self.post_process_name(cmd_name, ingredient), cmd\n\n        for cmd_name, cmd in self.commands.items():\n            yield self.post_process_name(cmd_name, self), cmd\n", "idx": 1448}
{"namespace": "sacred.ingredient.Ingredient.gather_named_configs", "completion": "        for ingredient, _ in self.traverse_ingredients():\n            for name, conf in ingredient.named_configs.items():\n                full_name = join_paths(ingredient.path, name)\n                yield full_name, conf\n", "idx": 1449}
{"namespace": "sacred.dependencies.Source.create", "completion": "        if not os.path.exists(filename):\n            raise ValueError(f\"invalid filename or file not found {filename}\")\n        filename = get_py_file_if_possible(filename)\n        digest = get_digest(filename)\n        repo, commit, isdirty = get_commit_if_possible(filename, save_git_info)\n        return Source(filename, digest, repo, commit, isdirty)\n", "idx": 1450}
{"namespace": "sacred.dependencies.Source.to_json", "completion": "        if base_dir is not None:\n            return os.path.relpath(self.filename, base_dir), self.digest\n        return self.filename, self.digest\n", "idx": 1451}
{"namespace": "sacred.dependencies.PackageDependency.create", "completion": "        if mod.__name__ in MODULE_BLACKLIST:\n            return None\n        if mod.__name__ in cls.modname_to_dist:\n            return cls.modname_to_dist[mod.__name__]\n        try:\n            dist = pkg_resources.get_distribution(mod.__name__)\n        except pkg_resources.DistributionNotFound:\n            return None\n        cls.modname_to_dist[mod.__name__] = PackageDependency(dist.project_name, dist.version)\n        return cls.modname_to_dist[mod.__name__]\n\n", "idx": 1452}
{"namespace": "sacred.dependencies.is_local_source", "completion": "    # If the module is a package dependency, it is not a local source.\n    if modname in SETTINGS.PACKAGE_DEPENDENCIES:\n        return False\n\n    # If the module is a local source, it must be in the experiment path.\n    if not filename.startswith(experiment_path):\n        return False\n\n    # If the module is a local source, it must be in a subdir of the experiment path.\n    if filename == experiment_path:\n        return False\n\n    return True\n\n", "idx": 1453}
{"namespace": "sacred.dependencies.gather_sources_and_dependencies", "completion": "    experiment_path, main = get_main_file(globs, save_git_info)\n    if base_dir is None:\n        base_dir = experiment_path\n\n    sources = set()\n    dependencies = set()\n\n    for strategy_name, strategy in source_discovery_strategies.items():\n        if SETTINGS.SOURCE_DISCOVERY_STRATEGY == strategy_name:\n            sources.update(strategy(globs, base_dir, save_git_info))\n\n    for strategy_name, strategy in dependency_discovery_strategies.items():\n        if SETTINGS.DEPENDENCY_DISCOVERY_STRATEGY == strategy_name:\n            dependencies.update(strategy(globs, base_dir))\n\n    return main, sources, dependencies\n\n", "idx": 1454}
{"namespace": "sacred.observers.file_storage.FileStorageObserver.resource_event", "completion": "        # Find or save the file\n        store_path = self.find_or_save(filename, self.resource_dir)\n\n        # Update the 'resources' field of the running entry\n        self.run_entry[\"resources\"].append(\n            os.path.relpath(str(store_path), self.basedir)\n        )\n\n        # Save the updated running entry as 'run.json'\n        self.save_json(self.run_entry, \"run.json\")\n", "idx": 1455}
{"namespace": "sacred.config.signature.Signature.get_free_parameters", "completion": "        free_parameters = []\n        for arg in self.arguments:\n            if arg not in kwargs:\n                if arg not in args:\n                    free_parameters.append(arg)\n                elif bound and arg in self.positional_args:\n                    free_parameters.append(arg)\n        return free_parameters\n", "idx": 1456}
{"namespace": "sacred.config.signature.Signature.construct_arguments", "completion": "        expected_args = self._get_expected_args(bound)\n        if len(args) > len(expected_args):\n            raise SignatureError(\n                \"Too many arguments for function '{}'.\".format(self.name)\n            )\n        if len(args) < len(expected_args) - len(kwargs):\n            raise SignatureError(\n                \"Too few arguments for function '{}'.\".format(self.name)\n            )\n\n        # fill in missing arguments by name\n        for i, arg in enumerate(expected_args):\n            if i < len(args):\n                continue\n            if arg in kwargs:\n                raise SignatureError(\n                    \"Argument '{}' is specified twice for function '{}'.\".format(\n                        arg, self.name\n                    )\n                )\n            if arg in options:\n                args.append(options[arg])\n            else:\n                raise MissingConfigError(\n                    \"Missing argument '{}' for function '{}'.\".format(arg, self.name)\n                )\n\n        # override default arguments by options\n        for arg, value in self.kwargs.items():\n            if arg in kwargs:\n                continue\n            if arg in options:\n                kwargs[arg] = options[arg]\n\n        # check for unexpected arguments\n        for arg in kwargs:\n            if arg not in expected_args:\n                raise SignatureError(\n                    \"Unexpected argument '{}' for function '{}'.\".format(arg, self.name)\n                )\n\n        return args, kwargs\n", "idx": 1457}
{"namespace": "sacred.config.config_files.load_config_file", "completion": "    handler = get_handler(filename)\n    with open(filename, \"r\" + handler.mode) as fp:\n        return handler.load(fp)\n\n", "idx": 1458}
{"namespace": "sacred.config.custom_containers.DogmaticDict.get", "completion": "        if k in self:\n            return self[k]\n        elif k in self.fallback:\n            return self.fallback[k]\n        else:\n            return d\n", "idx": 1459}
{"namespace": "sacred.config.custom_containers.DogmaticDict.revelation", "completion": "        missing_keys = set()\n        for key in self.fixed:\n            if key not in self:\n                missing_keys.add(key)\n                self[key] = self.fixed[key]\n            elif isinstance(self.fixed[key], DogmaticDict):\n                missing_keys |= self.fixed[key].revelation()\n        return missing_keys\n", "idx": 1460}
{"namespace": "sacred.config.custom_containers.make_read_only", "completion": "    if isinstance(o, dict):\n        return ReadOnlyDict({k: make_read_only(v) for k, v in o.items()})\n    elif isinstance(o, list):\n        return ReadOnlyList([make_read_only(v) for v in o])\n    elif isinstance(o, tuple):\n        return tuple(make_read_only(v) for v in o)\n    else:\n        return o\n\n", "idx": 1461}
{"namespace": "sacred.config.config_scope.dedent_function_body", "completion": "    lines = body.splitlines(True)\n    indent = None\n    for line in lines:\n        if is_empty_or_comment(line):\n            continue\n        if indent is None:\n            indent = re.search(r\"^\\s*\", line).group()\n        line = dedent_line(line, indent)\n        break\n    return \"\".join(line for line in lines if not is_empty_or_comment(line))\n\n", "idx": 1462}
{"namespace": "boltons.funcutils.FunctionBuilder.get_sig_str", "completion": "            if with_annotations:\n                return inspect_formatargspec(self.args, self.varargs,\n                                             self.varkw, self.kwonlyargs,\n                                             self.kwonlydefaults,\n                                             self.annotations)[1:-1]\n            else:\n                return inspect_formatargspec(self.args, self.varargs,\n                                             self.varkw, self.kwonlyargs,\n                                             self.kwonlydefaults)[1:-1]\n", "idx": 1463}
{"namespace": "boltons.funcutils.FunctionBuilder.get_invocation_str", "completion": "            invocation_str = inspect_formatargspec(self.args,\n                                                    self.varargs,\n                                                    self.varkw,\n                                                    [],\n                                                    self.kwonlyargs,\n                                                    self.kwonlydefaults)[1:-1]\n\n            if self.kwonlyargs:\n                kwonly_str = self._KWONLY_MARKER.sub('', invocation_str)\n                kwonly_str = kwonly_str.strip()\n                kwonly_str = kwonly_str.rstrip(',')\n                kwonly_str = kwonly_str.strip()\n                return kwonly_str\n            else:\n                return invocation_str\n", "idx": 1464}
{"namespace": "boltons.funcutils.FunctionBuilder.from_func", "completion": "        if isinstance(func, functools.partial):\n            fb = cls.from_func(func.func)\n            fb.dict.update(func.keywords)\n            return fb\n\n        argspec = cls._argspec_to_dict(func)\n        argspec.update(dict((attr, getattr(func, attr))\n                             for attr in cls._defaults)))\n        argspec.update(dict((attr, getattr(func, attr))\n                             for attr in cls._argspec_defaults)))\n        return cls(func.__name__, **argspec)\n", "idx": 1465}
{"namespace": "boltons.funcutils.FunctionBuilder.get_defaults_dict", "completion": "        return dict(zip(self.args, self.defaults))\n", "idx": 1466}
{"namespace": "boltons.funcutils.FunctionBuilder.get_arg_names", "completion": "        if only_required:\n            return tuple(arg for arg in self.args if arg not in self.defaults)\n        else:\n            return tuple(self.args)\n", "idx": 1467}
{"namespace": "boltons.ioutils.SpooledIOBase.writelines", "completion": "        for line in lines:\n            self.write(line)\n", "idx": 1468}
{"namespace": "boltons.ioutils.SpooledBytesIO.write", "completion": "        self._checkClosed()\n        if not isinstance(s, binary_type):\n            raise TypeError('bytes expected, got %s' % type(s))\n        if self._rolled:\n            self.buffer.write(s)\n        else:\n            if self.len + len(s) > self._max_size:\n                self.rollover()\n            self.buffer.write(s)\n", "idx": 1469}
{"namespace": "boltons.ioutils.SpooledBytesIO.seek", "completion": "        self._checkClosed()\n        return self.buffer.seek(pos, mode)\n", "idx": 1470}
{"namespace": "boltons.ioutils.SpooledBytesIO.len", "completion": "        pos = self.tell()\n        self.seek(0, os.SEEK_END)\n        length = self.tell()\n        self.seek(pos)\n        return length\n\n", "idx": 1471}
{"namespace": "boltons.ioutils.SpooledStringIO.read", "completion": "        self._checkClosed()\n        if n == -1:\n            n = self.len - self._tell\n        chars = self.buffer.read(n)\n        self._tell += len(chars)\n        return chars\n", "idx": 1472}
{"namespace": "boltons.ioutils.SpooledStringIO.write", "completion": "        self._checkClosed()\n        if not isinstance(s, text_type):\n            raise TypeError(\"{} expected, got {}\".format(\n                text_type.__name__,\n                type(s).__name__\n            ))\n\n        if self.tell() + len(s) >= self._max_size:\n            self.rollover()\n        self.buffer.writer.write(s)\n", "idx": 1473}
{"namespace": "boltons.ioutils.SpooledStringIO.seek", "completion": "        self._checkClosed()\n        if mode == os.SEEK_SET:\n            self._tell = self._traverse_codepoints(0, pos)\n        elif mode == os.SEEK_CUR:\n            self._tell = self._traverse_codepoints(self._tell, pos)\n        elif mode == os.SEEK_END:\n            self._tell = self._traverse_codepoints(self.len, pos)\n        else:\n            raise ValueError('Invalid whence ({}, should be 0, 1, or 2)'.format(mode))\n        return self._tell\n", "idx": 1474}
{"namespace": "boltons.ioutils.SpooledStringIO.tell", "completion": "        self._checkClosed()\n        return self._tell\n", "idx": 1475}
{"namespace": "boltons.ioutils.SpooledStringIO.len", "completion": "        pos = self.tell()\n        self.seek(0)\n        val = 0\n        while True:\n            ret = self.read(READ_CHUNK_SIZE)\n            if not ret:\n                break\n            val += len(ret)\n        self.seek(pos)\n        return val\n\n", "idx": 1476}
{"namespace": "boltons.ioutils.MultiFileReader.read", "completion": "        if amt is None:\n            return self._joiner.join(self._read_all())\n        else:\n            return self._joiner.join(self._read_n(amt))\n", "idx": 1477}
{"namespace": "boltons.ioutils.MultiFileReader.seek", "completion": "        if whence != os.SEEK_SET:\n            raise NotImplementedError(\n                'MultiFileReader.seek() only supports os.SEEK_SET'\n            )\n        if offset != 0:\n            raise NotImplementedError(\n                'MultiFileReader only supports seeking to start at this time'\n            )\n        for f in self._fileobjs:\n            f.seek(offset, whence)\n", "idx": 1478}
{"namespace": "boltons.listutils.BarrelList.insert", "completion": "        list_idx, rel_idx = self._translate_index(index)\n        if list_idx is None:\n            raise IndexError('list index out of range')\n        cur_list = self.lists[list_idx]\n        cur_list.insert(rel_idx, item)\n        self._balance_list(list_idx)\n", "idx": 1479}
{"namespace": "boltons.listutils.BarrelList.pop", "completion": "        if len(self.lists) == 1:\n            return self.lists[0].pop(*a)\n        if not a:\n            list_idx, rel_idx = len(self.lists) - 1, -1\n        else:\n            list_idx, rel_idx = self._translate_index(a[0])\n        if list_idx is None:\n            raise IndexError()\n        item = self.lists[list_idx].pop(rel_idx)\n        self._balance_list(list_idx)\n        return item\n", "idx": 1480}
{"namespace": "boltons.listutils.BarrelList.sort", "completion": "        # TODO: optimize\n        self.lists[:] = [sorted(l) for l in self.lists]\n        self.lists[:] = [self.lists[0]]\n        self._balance_list(0)\n", "idx": 1481}
{"namespace": "boltons.urlutils.URL.path", "completion": "        self.path_parts = tuple([unquote(p) if '%' in p else p for p\n                                 in path_text.split(u'/')])\n        return\n", "idx": 1482}
{"namespace": "boltons.urlutils.URL.navigate", "completion": "        if isinstance(dest, URL):\n            dest = dest.to_text()\n        dest = dest.split(u'#')\n        dest_path = dest[0].split(u'?')\n        dest_path_parts = dest_path[0].split(u'/')\n        dest_query_params = QueryParamDict.from_text(dest_path[1])\n        dest_fragment = dest[1]\n\n        if dest_path_parts[0] == u'':\n            dest_path_parts = dest_path_parts[1:]\n\n        if dest_path_parts[0] == u'.':\n            dest_path_parts = dest_path_parts[1:]\n\n        if dest_path_parts[0] == u'..':\n            dest_path_parts = dest_path_parts[2:]\n\n        if dest_path_parts[0] == u'':\n            dest_path_parts = dest_path_parts[1:]\n\n        if dest_path_parts[0] == u'.':\n            dest_path_parts = dest_path_parts[1:]\n\n        if dest_path_parts[0] == u'..':\n            dest_path_parts = dest_path_parts[2:]\n\n        if dest_path_parts[0] == u'':\n            dest_path_parts = dest_path_parts[1:]\n\n        if dest_path_parts[0] == u'.':\n            dest_path_parts = dest_path_parts[1:]\n\n        if dest_path_parts[0] == u'..':\n            dest_path_parts = dest_path_parts[2:]\n\n        if dest_path_parts[0] == u'':\n            dest_path_parts = dest_path_parts[1:]\n\n        if dest_path_parts[0] == u'.':\n            dest_path_parts = dest_path_parts[1:]\n\n        if dest_path_parts[0] == u'..':\n            dest_path_parts = dest_path_parts[2:]\n\n        if dest_path_parts[0] == u'':\n            dest_path_parts = dest_path_parts[1:]\n\n        if dest_path_parts[0] == u'.':\n            dest_path_parts = dest_path_parts[1:]\n\n       ", "idx": 1483}
{"namespace": "boltons.urlutils.URL.to_text", "completion": "        parts = []\n        _add = parts.append\n        if self.scheme:\n            _add(self.scheme)\n            _add(':')\n            if self.uses_netloc:\n                _add(self.get_authority(full_quote=full_quote))\n            else:\n                _add(self.host)\n            if self.port and self.port != self.default_port:\n                _add(':')\n                _add(unicode(self.port))\n        if self.path_parts:\n            _add(self.path)\n        if self.query_params:\n            _add('?')\n            _add(self.query_params.to_text(full_quote=full_quote))\n        if self.fragment:\n            _add('#')\n            _add(self.fragment)\n        return u''.join(parts)\n", "idx": 1484}
{"namespace": "boltons.urlutils.QueryParamDict.to_text", "completion": "        if full_quote:\n            return parse_qsl(self.iteritems(multi=True), quote_specials=True)\n        return parse_qsl(self.iteritems(multi=True))\n\n", "idx": 1485}
{"namespace": "boltons.tbutils.TracebackInfo.from_traceback", "completion": "        if tb is None:\n            try:\n                tb = sys.exc_info()[2]\n            except IndexError:\n                raise ValueError('no tb set and no exception being handled')\n        if limit is None:\n            limit = getattr(sys, 'tracebacklimit', 1000)\n        ret = []\n        n = 0\n        while tb is not None and n < limit:\n            item = cls.callpoint_type.from_tb(tb)\n            ret.append(item)\n            tb = tb.tb_next\n            n += 1\n        ret.reverse()\n        return cls(ret)\n", "idx": 1486}
{"namespace": "boltons.tbutils.ExceptionInfo.get_formatted", "completion": "        ret = '%s: %s\\n' % (self.exc_type, self.exc_msg)\n        ret += self.tb_info.get_formatted()\n        return ret\n", "idx": 1487}
{"namespace": "boltons.tbutils.print_exception", "completion": "    if limit is None:\n        limit = getattr(sys, 'tracebacklimit', 1000)\n    if file is None:\n        file = sys.stderr\n    if tb is None:\n        tb = sys.exc_info()[2]\n    if tb is None:\n        file.write(_format_final_exc_line(etype, value))\n        return\n    n = 0\n    while tb is not None and n < limit:\n        file.write(tb.tb_frame_str())\n        tb = tb.tb_next\n        n += 1\n    return\n\n", "idx": 1488}
{"namespace": "boltons.tbutils.ParsedException.to_string", "completion": "        # TODO: add SyntaxError formatting\n        ret = 'Traceback (most recent call last):\\n'\n        ret += ''.join([f.tb_frame_str() for f in self.frames])\n        ret += '%s: %s\\n' % (self.exc_type, self.exc_msg)\n        return ret\n\n", "idx": 1489}
{"namespace": "boltons.tbutils.ParsedException.from_string", "completion": "        # TODO: handle SyntaxError\n        # TODO: handle UnicodeDecodeError\n        # TODO: handle UnicodeEncodeError\n        # TODO: handle UnicodeTranslateError\n        # TODO: handle UnicodeWarning\n        # TODO: handle UnicodeError\n        # TODO: handle UnicodeDecodeError\n        # TODO: handle UnicodeEncodeError\n        # TODO: handle UnicodeTranslateError\n        # TODO: handle UnicodeWarning\n        # TODO: handle UnicodeError\n        # TODO: handle UnicodeDecodeError\n        # TODO: handle UnicodeEncodeError\n        # TODO: handle UnicodeTranslateError\n        # TODO: handle UnicodeWarning\n        # TODO: handle UnicodeError\n        # TODO: handle UnicodeDecodeError\n        # TODO: handle UnicodeEncodeError\n        # TODO: handle UnicodeTranslateError\n        # TODO: handle UnicodeWarning\n        # TODO: handle UnicodeError\n        # TODO: handle UnicodeDecodeError\n        # TODO: handle UnicodeEncodeError\n        # TODO: handle UnicodeTranslateError\n        # TODO: handle UnicodeWarning\n        # TODO: handle UnicodeError\n        # TODO: handle UnicodeDecodeError\n        # TODO: handle UnicodeEncodeError\n        # TODO: handle UnicodeTranslateError\n        # TODO: handle UnicodeWarning\n        # TODO: handle UnicodeError\n        # TODO: handle UnicodeDecodeError\n        # TODO: handle UnicodeEncodeError\n        # TODO: handle UnicodeTranslateError\n        # TODO: handle UnicodeWarning\n        # TODO: handle UnicodeError\n        # TODO: handle UnicodeDecodeError\n        # TODO: handle UnicodeEncodeError\n        # TODO: handle UnicodeTranslateError\n        # TODO: handle UnicodeWarning\n        # TODO: handle UnicodeError\n        # TODO: handle UnicodeDecodeError\n        # TODO: handle UnicodeEncodeError\n        # TODO: handle UnicodeTranslateError\n        # TODO: handle UnicodeWarning\n        # TODO: handle UnicodeError\n        # TODO: handle UnicodeDecodeError\n        # TODO: handle UnicodeEncodeError\n        # TODO: handle UnicodeTranslateError\n        # TODO: handle UnicodeWarning\n        # TODO: handle UnicodeError\n        # TODO: handle UnicodeDecodeError\n        # TODO: handle UnicodeEncodeError\n        # TODO: handle UnicodeTranslateError\n        # TODO: handle UnicodeWarning\n        # TODO: handle UnicodeError\n        # TODO: handle UnicodeDecodeError\n        # TODO: handle UnicodeEncodeError\n        # TODO: handle UnicodeTranslateError\n        # TODO: handle UnicodeWarning\n        # TODO: handle UnicodeError\n        # TODO: handle UnicodeDecodeError\n        # TODO: handle UnicodeEncodeError\n        # TODO: handle UnicodeTranslateError\n        # TODO: handle UnicodeWarning\n        # TODO: handle Unicode", "idx": 1490}
{"namespace": "boltons.tableutils.Table.extend", "completion": "        if not data:\n            return\n        if isinstance(data, Table):\n            data = data._data\n        if isinstance(data, Mapping):\n            data = [data]\n        if isinstance(data, Sequence) and not isinstance(data, string_types):\n            if isinstance(data[0], Mapping):\n                data = [data]\n        if isinstance(data, Sequence) and not isinstance(data, string_types):\n            if isinstance(data[0], Sequence) and not isinstance(data[0], string_types):\n                data = [data]\n        if isinstance(data, Sequence) and not isinstance(data, string_types):\n            if isinstance(data[0], Sequence) and not isinstance(data[0], string_types):\n                data = [data]\n        if isinstance(data, Sequence) and not isinstance(data, string_types):\n            if isinstance(data[0], Sequence) and not isinstance(data[0], string_types):\n                data = [data]\n        if isinstance(data, Sequence) and not isinstance(data, string_types):\n            if isinstance(data[0], Sequence) and not isinstance(data[0], string_types):\n                data = [data]\n        if isinstance(data, Sequence) and not isinstance(data, string_types):\n            if isinstance(data[0], Sequence) and not isinstance(data[0], string_types):\n                data = [data]\n        if isinstance(data, Sequence) and not isinstance(data, string_types):\n            if isinstance(data[0], Sequence) and not isinstance(data[0], string_types):\n                data = [data]\n        if isinstance(data, Sequence) and not isinstance(data, string_types):\n            if isinstance(data[0], Sequence) and not isinstance(data[0], string_types):\n                data = [data]\n        if isinstance(data, Sequence) and not isinstance(data, string_types):\n            if isinstance(data[0], Sequence) and not isinstance(data[0], string_types):\n                data = [data]\n        if isinstance(data, Sequence) and not isinstance(data, string_types):\n            if isinstance(data[0], Sequence) and not isinstance(data[0], string_types):\n                data = [data]\n        if isinstance(data, Sequence) and not isinstance(data, string_types", "idx": 1491}
{"namespace": "boltons.tableutils.Table.from_object", "completion": "        return cls.from_data(data=data, headers=headers,\n                             max_depth=max_depth, _data_type=ObjectInputType(),\n                             metadata=metadata)\n", "idx": 1492}
{"namespace": "boltons.tableutils.Table.__repr__", "completion": "        if self.headers:\n            return '%s(headers=%r, data=%r)' % (self.__class__.__name__,\n                                                self.headers, self._data)\n        return '%s(%r)' % (self.__class__.__name__, self._data)\n", "idx": 1493}
{"namespace": "boltons.tableutils.Table.to_text", "completion": "        lines = []\n        if with_headers and self.headers:\n            lines.append(self._html_thead)\n            lines.append(self._html_tr + self._html_th +\n                         self._html_th.join(self.headers) +\n                         self._html_th_close + self._html_tr_close)\n            lines.append(self._html_thead_close)\n        for row in self._data:\n            _fill_parts = []\n            for cell in row:\n                try:\n                    _fill_parts.append(str(cell))\n                except Exception:\n                    _fill_parts.append(repr(cell))\n            if maxlen:\n                _fill_parts = [p[:maxlen] + '...' for p in _fill_parts]\n            lines.append(self._html_tr + self._html_td +\n                         self._html_td.join(_fill_parts) +\n                         self._html_td_close + self._html_tr_close)\n        return '\\n'.join(lines)\n", "idx": 1494}
{"namespace": "boltons.statsutils.Stats.get_histogram_counts", "completion": "        if not self.data:\n            return []\n\n        if bins is None:\n            bins = self._get_bin_bounds(**kw)\n\n        if isinstance(bins, int):\n            bins = self._get_bin_bounds(bins, **kw)\n\n        bins = [float(b) for b in bins]\n\n        if not bins:\n            return []\n\n        bin_count = len(bins)\n\n        if bin_count < 2:\n            return []\n\n        if bins[0] > bins[-1]:\n            bins.reverse()\n\n        bin_counts = [0] * bin_count\n\n        for v in self.data:\n            idx = bisect.bisect_left(bins, v)\n            if idx < bin_count:\n                bin_counts[idx] += 1\n\n        return zip(bins, bin_counts)\n", "idx": 1495}
{"namespace": "boltons.setutils.IndexedSet.add", "completion": "        if item not in self.item_index_map:\n            self.item_list.append(item)\n            self.item_index_map[item] = len(self.item_list) - 1\n", "idx": 1496}
{"namespace": "boltons.setutils.IndexedSet.pop", "completion": "        if index is None:\n            index = len(self) - 1\n        if index < 0:\n            index += len(self)\n        real_index = self._get_real_index(index)\n        try:\n            item = self.item_list[real_index]\n        except IndexError:\n            raise IndexError('IndexedSet index out of range')\n        if item is _MISSING:\n            raise IndexError('IndexedSet index out of range')\n        self.item_list[real_index] = _MISSING\n        self.item_index_map.pop(item)\n        self._cull()\n        return item\n", "idx": 1497}
{"namespace": "boltons.setutils.IndexedSet.index", "completion": "        if val not in self.item_index_map:\n            raise ValueError(f'{val!r} is not in {type(self).__name__}')\n        return self.item_index_map[val]\n", "idx": 1498}
{"namespace": "boltons.setutils.complement", "completion": "    class _ComplementSet(MutableSet):\n        \"\"\"\n        _ComplementSet is a :class:`collections.MutableSet` that maintains\n        insertion order and uniqueness of inserted elements. It's a hybrid\n        type, mostly like an OrderedSet, but also :class:`list`-like, in\n        that it supports indexing and slicing.\n        Args:\n            other (iterable): An optional iterable used to initialize the set.\n        >>> x = _ComplementSet(list(range(4)) + list(range(8)))\n        >>> x\n        _ComplementSet([0, 1, 2, 3, 4, 5, 6, 7])\n        >>> x - set(range(2))\n        _ComplementSet([2, 3, 4, 5, 6, 7])\n        >>> x[-1]\n        7\n        >>> fcr = _ComplementSet('freecreditreport.com')\n        >>> ''.join(fcr[:fcr.index('.')])\n        'frecditpo'\n        Standard set operators and interoperation with :class:`set` are\n        all supported:\n        >>> fcr & set('cash4gold.com')\n        _ComplementSet(['c', 'd', 'o', '.', 'm'])\n        As you can see, the ``_ComplementSet`` is almost like a ``UniqueList``,\n        retaining only one copy of a given value, in the order it was\n        first added. For the curious, the reason why _ComplementSet does not\n        support setting items based on index (i.e, ``__setitem__()``),\n        consider the following dilemma::\n          my_complement_set = [A, B, C, D]\n          my_complement_set[2] = A\n        At this point, a set requires only one *A*, but a :class:`list` would\n        overwrite *C*. Overwriting *C* would change the length of the list,\n        meaning that ``my_complement_set[2]`` would not be *A*, as expected with a\n        list, but rather *D*. So, no ``__setitem__()``.\n        Otherwise, the API strives to be as complete a union of the\n        :class:`list` and :class:`set` APIs as possible.\n        \"\"\"\n        def __init__(self, other=None", "idx": 1499}
{"namespace": "boltons.strutils.strip_ansi", "completion": "    if isinstance(text, str):\n        return ANSI_SEQUENCES.sub('', text)\n    elif isinstance(text, bytes):\n        return ANSI_SEQUENCES.sub(b'', text).encode()\n    elif isinstance(text, bytearray):\n        return ANSI_SEQUENCES.sub(b'', text)\n    else:\n        return text\n\n", "idx": 1500}
{"namespace": "boltons.strutils.asciify", "completion": "    # Transform any ASCII-like content to unicode to allow regex to match, and\n    # save input type for later.\n    target_type = None\n    # Unicode type aliased to str is code-smell for Boltons in Python 3 env.\n    is_py3 = (unicode == builtins.str)\n    if is_py3 and isinstance(text, (bytes, bytearray)):\n        target_type = type(text)\n        text = text.decode('utf-8')\n\n    # Transform any ASCII-like content to unicode to allow regex to match, and\n    # save input type for later.\n    # Transform any ASCII-like content to unicode to allow regex to match, and\n    # save input type for later.\n    # Transform any ASCII-like content to unicode to allow regex to match, and\n    # save input type for later.\n    # Transform any ASCII-like content to unicode to allow regex to match, and\n    # save input type for later.\n    # Transform any ASCII-like content to unicode to allow regex to match, and\n    # save input type for later.\n    # Transform any ASCII-like content to unicode to allow regex to match, and\n    # save input type for later.\n    # Transform any ASCII-like content to unicode to allow regex to match, and\n    # save input type for later.\n    # Transform any ASCII-like content to unicode to allow regex to match, and\n    # save input type for later.\n    # Transform any ASCII-like content to unicode to allow regex to match, and\n    # save input type for later.\n    # Transform any ASCII-like content to unicode to allow regex to match, and\n    # save input type for later.\n    # Transform any ASCII-like content to unicode to allow regex to match, and\n    # save input type for later.\n    # Transform any ASCII-like content to unicode to allow regex to match, and\n    # save input type for later.\n    # Transform any ASCII-like content to unicode to allow regex to match, and\n    # save input type for later.\n    # Transform any ASCII-like content to unicode to allow regex to match, and\n    # save input type for later.\n    # Transform any ASCII-like content to unicode to allow regex to match, and\n    # save input type for later.\n    # Transform any ASCII-like content to unicode to allow regex to match, and\n    # save input", "idx": 1501}
{"namespace": "boltons.strutils.indent", "completion": "    if key is None:\n        key = bool\n    if not key(text):\n        return text\n    return newline.join(margin + line for line in text.splitlines())\n\n", "idx": 1502}
{"namespace": "boltons.strutils.multi_replace", "completion": "    return MultiReplace(sub_map, **kwargs).sub(text)\n\n", "idx": 1503}
{"namespace": "boltons.cacheutils.LRI._get_flattened_ll", "completion": "        anchor = self._anchor\n        curr = anchor[NEXT]\n        while curr is not anchor:\n            yield curr[KEY], curr[VALUE]\n            curr = curr[NEXT]\n", "idx": 1504}
{"namespace": "boltons.cacheutils.LRI.pop", "completion": "        with self._lock:\n            try:\n                link = self._link_lookup[key]\n            except KeyError:\n                if default is _MISSING:\n                    raise\n                return default\n            else:\n                self._remove_from_ll(key)\n                return super(LRI, self).pop(key, default)\n", "idx": 1505}
{"namespace": "boltons.cacheutils.LRI.popitem", "completion": "        with self._lock:\n            if not self:\n                raise KeyError('popitem(): LRI is empty')\n            key = self._anchor[KEY]\n            value = self._anchor[VALUE]\n            self._remove_from_ll(key)\n            super(LRI, self).__delitem__(key)\n            return key, value\n", "idx": 1506}
{"namespace": "boltons.cacheutils.LRI.clear", "completion": "        with self._lock:\n            super(LRI, self).clear()\n            self._init_ll()\n", "idx": 1507}
{"namespace": "boltons.cacheutils.LRI.setdefault", "completion": "        with self._lock:\n            try:\n                link = self._get_link_and_move_to_front_of_ll(key)\n            except KeyError:\n                self.soft_miss_count += 1\n                if not self.on_miss:\n                    raise\n                ret = self[key] = self.on_miss(key)\n                return ret\n\n            self.hit_count += 1\n            return link[VALUE]\n", "idx": 1508}
{"namespace": "boltons.cacheutils.LRI.update", "completion": "        if E is self:\n            return\n        if hasattr(E, 'keys'):\n            for k in E.keys():\n                self[k] = E[k]\n        else:\n            for k, v in E:\n                self[k] = v\n        for k in F:\n            self[k] = F[k]\n", "idx": 1509}
{"namespace": "boltons.cacheutils.LRI.__repr__", "completion": "        return '{class name}(max_size={max size}, on_miss={on miss}, values={values})'.format(\n            class_name=self.__class__.__name__,\n            max_size=self.max_size,\n            on_miss=self.on_miss,\n            values=super(LRI, self).__repr__()\n        )\n\n", "idx": 1510}
{"namespace": "boltons.cacheutils.CachedFunction.__repr__", "completion": "        cn = self.__class__.__name__\n        return ('%s(func=%r, scoped=%r, typed=%r)'\n                % (cn, self.func, self.scoped, self.typed))\n\n", "idx": 1511}
{"namespace": "boltons.cacheutils.ThresholdCounter.elements", "completion": "        for key, count in self._count_map.items():\n            for _ in range(count[0]):\n                yield key\n", "idx": 1512}
{"namespace": "boltons.cacheutils.ThresholdCounter.most_common", "completion": "        if n is None:\n            return sorted(self.items(), key=lambda x: x[1], reverse=True)\n        return sorted(self.items(), key=lambda x: x[1], reverse=True)[:n]\n", "idx": 1513}
{"namespace": "boltons.cacheutils.ThresholdCounter.update", "completion": "        if kwargs:\n            iterable = dict(iterable, **kwargs)\n        for key in iterable:\n            self.add(key)\n        return\n", "idx": 1514}
{"namespace": "boltons.cacheutils.MinIDMap.get", "completion": "        try:\n            return self.mapping[a]\n        except KeyError:\n            if self.free:\n                ret = self.free.pop()\n            else:\n                ret = len(self.mapping)\n            self.mapping[a] = ret\n            self.ref_map[ret] = a\n            return ret\n", "idx": 1515}
{"namespace": "boltons.iterutils.chunked", "completion": "    if not is_iterable(src):\n        raise TypeError('expected an iterable')\n\n    if count is not None:\n        count = int(count)\n        if count == 0:\n            return []\n\n    fill = kw.get('fill', None)\n    if fill is not None:\n        fill = [fill] * size\n\n    cur_chunk = []\n    chunk_count = 0\n    for s in src:\n        cur_chunk.append(s)\n        if len(cur_chunk) == size:\n            chunk_count += 1\n            yield cur_chunk\n            cur_chunk = []\n            if count is not None and chunk_count >= count:\n                break\n\n    if cur_chunk:\n        if fill:\n            cur_chunk.extend(fill)\n        yield cur_chunk\n\n", "idx": 1516}
{"namespace": "boltons.iterutils.chunk_ranges", "completion": "    if not isinstance(input_size, int) or input_size < 0:\n        raise ValueError(\"input_size must be a non-negative integer\")\n    if not isinstance(chunk_size, int) or chunk_size < 0:\n        raise ValueError(\"chunk_size must be a non-negative integer\")\n    if not isinstance(input_offset, int) or input_offset < 0:\n        raise ValueError(\"input_offset must be a non-negative integer\")\n    if not isinstance(overlap_size, int) or overlap_size < 0:\n        raise ValueError(\"overlap_size must be a non-negative integer\")\n    if not isinstance(align, bool):\n        raise ValueError(\"align must be a boolean\")\n\n    if input_size == 0:\n        return\n\n    if chunk_size == 0:\n        raise ValueError(\"chunk_size must be greater than 0\")\n\n    if overlap_size >= chunk_size:\n        raise ValueError(\"overlap_size must be less than chunk_size\")\n\n    if align:\n        chunk_start = input_offset\n        while chunk_start < input_offset + input_size:\n            chunk_end = min(chunk_start + chunk_size, input_offset + input_size)\n            yield (chunk_start, chunk_end)\n            chunk_start += chunk_size - overlap_size\n    else:\n        chunk_start = input_offset\n        while chunk_start < input_offset + input_size:\n            chunk_end = min(chunk_start + chunk_size, input_offset + input_size)\n            yield (chunk_start, chunk_end)\n            chunk_start += chunk_size - overlap_size\n\n", "idx": 1517}
{"namespace": "boltons.iterutils.remap", "completion": "    if visit is None:\n        visit = _orig_default_visit\n    if enter is None:\n        enter = default_enter\n    if exit is None:\n        exit = default_exit\n\n    # print('remap(%r, visit=%r, enter=%r, exit=%r)' % (root, visit, enter, exit))\n\n    # print('enter(%r, %r)' % (key, value))\n    new_parent, new_items = enter(path, key, value)\n    if new_items is False:\n        return new_parent\n    # print('exit(%r, %r, %r, %r, %r)' % (path, key, old_parent, new_parent, new_items))\n    return exit(path, key, old_parent, new_parent,\n                  [(visit(path + (key,), key, value), value)\n                   for key, value in new_items])\n\n", "idx": 1518}
{"namespace": "boltons.iterutils.get_path", "completion": "    try:\n        value = root[path[0]]\n    except (KeyError, IndexError, TypeError) as exc:\n        raise PathAccessError(exc, path[0], path)\n    if len(path) == 1:\n        return value\n    try:\n        return get_path(value, path[1:], default)\n    except (KeyError, IndexError, TypeError) as exc:\n        raise PathAccessError(exc, path[1:], path)\n\n", "idx": 1519}
{"namespace": "boltons.iterutils.research", "completion": "    if not callable(query):\n        raise TypeError('query expected callable, not: %r' % query)\n    path, registry, stack = (), {}, [(None, root)]\n    results = []\n    while stack:\n        key, value = stack.pop()\n        id_value = id(value)\n        if key is _REMAP_EXIT:\n            key, new_parent, old_parent = value\n            id_value = id(old_parent)\n            path, new_items = new_items_stack.pop()\n            value = exit(path, key, old_parent, new_parent, new_items)\n            registry[id_value] = value\n            if not new_items_stack:\n                continue\n        elif id_value in registry:\n            value = registry[id_value]\n        else:\n            res = enter(path, key, value)\n            try:\n                new_parent, new_items = res\n            except TypeError:\n                # TODO: handle False?\n                raise TypeError('enter should return a tuple of (new_parent,'\n                                ' items_iterator), not: %r' % res)\n            if new_items is not False:\n                # traverse unless False is explicitly passed\n                registry[id_value] = new_parent\n                new_items_stack.append((path, []))\n                if value is not root:\n                    path += (key,)\n                stack.append((_REMAP_EXIT, (key, new_parent, value)))\n                if new_items:\n                    stack.extend(reversed(list(new_items)))\n                continue\n        if visit is _orig_default_visit:\n            # avoid function call overhead by inlining identity operation\n            visited_item = (key, value)\n        else:\n            try:\n                visited_item = visit(path, key, value)\n            except Exception:\n                if reraise:\n                    raise\n                visited_item = True\n            if visited_item is False:\n                continue  # drop\n            elif visited_item is True:\n                visited_item = (key, value)\n            # TODO: typecheck?\n            #    raise TypeError('expected (key, value) from visit(),'\n            #                    ' not: %r' % visited_item)\n        try:\n            if query(path, key, value):\n                results.append((path, value))\n        except Exception:\n            if reraise:", "idx": 1520}
{"namespace": "boltons.socketutils.BufferedSocket.getrecvbuffer", "completion": "        return self.rbuf\n", "idx": 1521}
{"namespace": "boltons.socketutils.BufferedSocket.getsendbuffer", "completion": "        with self._send_lock:\n            return self.sbuf[:]\n", "idx": 1522}
{"namespace": "boltons.socketutils.BufferedSocket.recv", "completion": "        if flags != 0:\n            raise ValueError('non-zero flags not supported: {0!r}'.format(flags))\n\n        if timeout is _UNSET:\n            timeout = self.timeout\n\n        if timeout is None:\n            timeout = _RECV_LARGE_MAXSIZE\n\n        with self._recv_lock:\n            if len(self.rbuf) >= size:\n                data = self.rbuf[:size]\n                self.rbuf = self.rbuf[size:]\n                return data\n\n            if len(self.rbuf) > 0:\n                data = self.rbuf\n                self.rbuf = b''\n                return data\n\n            self.sock.settimeout(timeout)\n            try:\n                data = self.sock.recv(self._recvsize)\n            except socket.timeout:\n                raise Timeout('timed out after {0} seconds'.format(timeout))\n\n            if not data:\n                raise ConnectionClosed('connection closed')\n\n            if len(data) > size:\n                self.rbuf = data[size:]\n                data = data[:size]\n\n            return data\n", "idx": 1523}
{"namespace": "boltons.socketutils.BufferedSocket.recv_close", "completion": "        if timeout is _UNSET:\n            timeout = self.timeout\n        if maxsize is _UNSET:\n            maxsize = self.maxsize\n        if maxsize > _RECV_LARGE_MAXSIZE:\n            raise ValueError(\"maxsize must be less than %d\" % _RECV_LARGE_MAXSIZE)\n        data = b''\n        while True:\n            try:\n                data += self.recv(maxsize - len(data), timeout=timeout)\n            except Timeout:\n                break\n        return data\n", "idx": 1524}
{"namespace": "boltons.socketutils.BufferedSocket.flush", "completion": "        with self._send_lock:\n            self.sock.send(b'')\n", "idx": 1525}
{"namespace": "boltons.socketutils.BufferedSocket.buffer", "completion": "        with self._send_lock:\n            self.sbuf.append(data)\n        return\n", "idx": 1526}
{"namespace": "boltons.socketutils.BufferedSocket.close", "completion": "        self.sock.close()\n        self.rbuf = b''\n        self.sbuf = []\n        return\n", "idx": 1527}
{"namespace": "boltons.socketutils.NetstringSocket.setmaxsize", "completion": "        self.maxsize = maxsize\n        self._msgsize_maxsize = len(str(maxsize)) + 1  # len(str()) == log10\n", "idx": 1528}
{"namespace": "boltons.socketutils.NetstringSocket.write_ns", "completion": "        if len(payload) > self.maxsize:\n            raise NetstringMessageTooLong(len(payload), self.maxsize)\n\n        data = str(len(payload)) + b':' + payload + b','\n        self.bsock.send(data)\n        return\n\n", "idx": 1529}
{"namespace": "boltons.fileutils.FilePerms.__repr__", "completion": "        return '%s(user=%r, group=%r, other=%r)' % (\n            self.__class__.__name__, self.user, self.group, self.other)\n", "idx": 1530}
{"namespace": "boltons.mathutils.Bits.as_hex", "completion": "        return '{{0:0{0}x}}'.format(self.len // 4).format(self.val)\n", "idx": 1531}
{"namespace": "boltons.mathutils.Bits.from_hex", "completion": "        if type(hex) is bytes:\n            hex = hex.decode('ascii')\n        if not hex.startswith('0x'):\n            hex = '0x' + hex\n        return cls(hex)\n", "idx": 1532}
{"namespace": "boltons.formatutils.split_format_str", "completion": "    # TODO: add support for escaped braces\n    # TODO: add support for escaped percent signs\n    # TODO: add support for escaped curly braces\n    # TODO: add support for escaped backslashes\n    # TODO: add support for escaped newlines\n    # TODO: add support for escaped tabs\n    # TODO: add support for escaped carriage returns\n    # TODO: add support for escaped form feeds\n    # TODO: add support for escaped vertical tabs\n    # TODO: add support for escaped backspaces\n    # TODO: add support for escaped null bytes\n    # TODO: add support for escaped bells\n    # TODO: add support for escaped backslashes\n    # TODO: add support for escaped apostrophes\n    # TODO: add support for escaped double quotes\n    # TODO: add support for escaped question marks\n    # TODO: add support for escaped exclamation marks\n    # TODO: add support for escaped tildes\n    # TODO: add support for escaped carets\n    # TODO: add support for escaped ampersands\n    # TODO: add support for escaped asterisks\n    # TODO: add support for escaped parentheses\n    # TODO: add support for escaped brackets\n    # TODO: add support for escaped braces\n    # TODO: add support for escaped pipes\n    # TODO: add support for escaped colons\n    # TODO: add support for escaped semicolons\n    # TODO: add support for escaped commas\n    # TODO: add support for escaped periods\n    # TODO: add support for escaped slashes\n    # TODO: add support for escaped backslashes\n    # TODO: add support for escaped question marks\n    # TODO: add support for escaped exclamation marks\n    # TODO: add support for escaped tildes\n    # TODO: add support for escaped carets\n    # TODO: add support for escaped ampersands\n    # TODO: add support for escaped asterisks\n    # TODO: add support for escaped parentheses\n    # TODO: add support for escaped brackets\n    # TODO: add support for escaped braces\n    # TODO: add support for escaped pipes\n    # TODO: add support for escaped colons\n    # TODO: add support for escaped semicolons\n    # TODO: add support for escaped commas\n    # TODO: add support for escaped periods\n    # TODO: add support for escaped slashes\n    # TODO: add support for escaped backslashes\n    # TODO: add support for escaped question marks\n    # TODO: add support for escaped exclamation marks", "idx": 1533}
{"namespace": "boltons.formatutils.infer_positional_format_args", "completion": "    # Split the format string into literals and format fields\n    fields = split_format_str(fstr)\n\n    # Initialize the number of positional arguments\n    num_positional_args = 0\n\n    # Iterate over the fields and replace anonymous positional arguments with numbered ones\n    for i, (lit, field) in enumerate(fields):\n        if field is not None:\n            # Check if the field is an anonymous positional argument\n            if field.startswith('{') and field.endswith('}'):\n                # Replace the anonymous positional argument with a numbered one\n                fields[i] = (lit, '{{{0}}}'.format(num_positional_args))\n                num_positional_args += 1\n\n    # Join the modified fields back into a format string\n    return ''.join(lit + field for lit, field in fields)\n\n", "idx": 1534}
{"namespace": "boltons.formatutils.tokenize_format_str", "completion": "    # TODO: memoize\n    ret = []\n    formatter = Formatter()\n\n    if resolve_pos:\n        fstr = infer_positional_format_args(fstr)\n\n    for lit, fname, fspec, conv in formatter.parse(fstr):\n        if fname is None:\n            ret.append(lit)\n            continue\n        field_str = construct_format_field_str(fname, fspec, conv)\n        ret.append(BaseFormatField(field_str))\n    return ret\n\n", "idx": 1535}
{"namespace": "boltons.dictutils.OneToOne.clear", "completion": "        dict.clear(self)\n        dict.clear(self.inv)\n", "idx": 1536}
{"namespace": "boltons.dictutils.OneToOne.pop", "completion": "        if key not in self:\n            if default is _MISSING:\n                raise KeyError(key)\n            return default\n\n        val = self[key]\n        del self[key]\n        return val\n", "idx": 1537}
{"namespace": "boltons.dictutils.OneToOne.popitem", "completion": "        k, v = dict.popitem(self)\n        dict.__delitem__(self.inv, v)\n        return k, v\n", "idx": 1538}
{"namespace": "boltons.dictutils.ManyToMany.update", "completion": "        if type(iterable) is ManyToMany:\n            self.data.update(iterable.data)\n            self.inv.data.update(iterable.inv.data)\n        elif type(iterable) is dict:\n            for key, vals in iterable.items():\n                self[key] = vals\n        else:\n            for key, val in iterable:\n                self[key] = val\n", "idx": 1539}
{"namespace": "boltons.dictutils.ManyToMany.add", "completion": "        if key not in self.data:\n            self.data[key] = set()\n        self.data[key].add(val)\n\n        if val not in self.inv.data:\n            self.inv.data[val] = set()\n        self.inv.data[val].add(key)\n", "idx": 1540}
{"namespace": "boltons.dictutils.ManyToMany.remove", "completion": "        self.data[key].remove(val)\n        if not self.data[key]:\n            del self.data[key]\n        self.inv.data[val].remove(key)\n        if not self.inv.data[val]:\n            del self.inv.data[val]\n", "idx": 1541}
{"namespace": "boltons.dictutils.ManyToMany.replace", "completion": "        if key not in self.data:\n            raise KeyError('Key not in data')\n        if newkey in self.data:\n            raise KeyError('Newkey already in data')\n        self.data[newkey] = self.data.pop(key)\n        for val in self.data[newkey]:\n            self.inv.data[val].remove(key)\n            self.inv.data[val].add(newkey)\n", "idx": 1542}
{"namespace": "boltons.dictutils.ManyToMany.iteritems", "completion": "        for key in self.data:\n            yield key, self.data[key]\n", "idx": 1543}
{"namespace": "gunicorn.config.Config.__str__", "completion": "        lines = []\n        key_max_length = max(len(k) for k in self.settings)\n        for k, v in self.settings.items():\n            if callable(v):\n                v = \"<%s()>\" % v.__qualname__\n            lines.append(\"%s = %s\" % (k.ljust(key_max_length), v))\n        return \"\\n\".join(lines)\n", "idx": 1544}
{"namespace": "gunicorn.config.Config.set", "completion": "        if name not in self.settings:\n            raise AttributeError(\"No configuration setting for: %s\" % name)\n        self.settings[name].set(value)\n", "idx": 1545}
{"namespace": "gunicorn.config.Config.worker_class", "completion": "        uri = self.settings['worker_class'].get()\n\n        # are we using a threaded worker?\n        is_sync = uri.endswith('SyncWorker') or uri == 'sync'\n        if is_sync and self.threads > 1:\n            uri = 'gthread'\n\n        # load the worker class\n        worker_class = util.import_module(uri)\n\n        # setup the worker class\n        if hasattr(worker_class, 'setup'):\n            worker_class.setup(**self.settings)\n\n        return worker_class\n", "idx": 1546}
{"namespace": "gunicorn.config.Config.address", "completion": "        # get the bind address from settings\n        bind_address = self.settings['bind'].get()\n\n        # if the bind address is a string, convert it to a list\n        if isinstance(bind_address, str):\n            bind_address = [bind_address]\n\n        # parse the bind address and return a list of parsed addresses\n        parsed_addresses = []\n        for address in bind_address:\n            parsed_addresses.append(util.parse_address(address))\n        return parsed_addresses\n", "idx": 1547}
{"namespace": "gunicorn.config.Config.logger_class", "completion": "        logger_class = self.settings['logger_class'].get()\n        if logger_class == \"simple\":\n            logger_class = \"gunicorn.glogging.Logger\"\n        elif logger_class == \"default\" and self.settings['statsd_host'].get():\n            logger_class = \"gunicorn.instrument.statsd.Statsd\"\n        return util.load_class(logger_class, \"gunicorn.loggers\")\n", "idx": 1548}
{"namespace": "gunicorn.sock.create_sockets", "completion": "    if fds is None:\n        fds = []\n\n    sockets = []\n    for addr in conf.bind:\n        sock_type = _sock_type(addr)\n        sock = sock_type(addr, conf, log)\n        sockets.append(sock)\n\n    if conf.is_ssl:\n        if not hasattr(ssl, \"SSLContext\"):\n            raise RuntimeError(\"SSL is not available\")\n\n        if conf.cert_reqs == ssl.CERT_REQUIRED:\n            if not conf.certfile:\n                raise RuntimeError(\"You must supply a certfile.\")\n            if not conf.keyfile:\n                raise RuntimeError(\"You must supply a keyfile.\")\n\n        if conf.do_handshake_on_connect:\n            if conf.ssl_version not in (\n                    ssl.PROTOCOL_TLS, ssl.PROTOCOL_TLS_CLIENT,\n                    ssl.PROTOCOL_TLS_SERVER, ssl.PROTOCOL_TLSv1,\n                    ssl.PROTOCOL_TLSv1_1, ssl.PROTOCOL_TLSv1_2):\n                raise RuntimeError(\n                    \"Only TLS version SSL contexts can \"\n                    \"do_handshake_on_connect.\")\n\n        if conf.ciphers:\n            try:\n                ssl.SSLContext.set_ciphers(conf.ciphers)\n            except AttributeError:\n                raise RuntimeError(\n                    \"Cipher %s is not available. \"\n                    \"See https://github.com/benoitc/gunicorn/issues/1421\"\n                    % conf.ciphers)\n\n        if conf.ca_certs:\n            if not conf.cert_reqs:\n                raise RuntimeError(\"You must specify cert_reqs when \"\n                                   \"supplying a ca_certs file.\")\n\n        if conf.ssl_version:\n            if conf.ssl_version not in (\n                    ssl.PROTOCOL_SSLv2, ssl.PROTOCOL_SSLv3,\n                    ssl.PROTOCOL_TLS, ssl.PROTOCOL_TLS_CLIENT,\n                    ssl.PROTOCOL_TLS_SERVER, ssl.PROTOCOL_TLSv1,\n                    ssl.PROTOCOL_TLSv1_1, ssl.PROTOCOL_TLSv1_2):\n                raise RuntimeError(\"SSL version not supported.\")\n\n        if conf.certfile:\n            if not os.path.exists(conf.certfile):\n                raise RuntimeError(\"Certificate file %s does not exist\"\n                                   % conf.certfile)\n\n        if conf.keyfile:\n            if not os.path.exists", "idx": 1549}
{"namespace": "gunicorn.http.body.LengthReader.read", "completion": "        if not isinstance(size, int):\n            raise TypeError(\"size must be an integral type\")\n        if size < 0:\n            raise ValueError(\"Size must be positive.\")\n        if size == 0:\n            return b\"\"\n\n        buf = io.BytesIO()\n        while buf.tell() < size:\n            data = self.unreader.read()\n            if not data:\n                break\n            buf.write(data)\n        data = buf.getvalue()\n        ret, rest = data[:size], data[size:]\n        self.unreader.unread(rest)\n        self.length -= size\n        return ret\n\n", "idx": 1550}
{"namespace": "gunicorn.http.body.EOFReader.read", "completion": "        if not isinstance(size, int):\n            raise TypeError(\"size must be an integral type\")\n\n        if size < 0:\n            raise ValueError(\"Size must be positive.\")\n        if size == 0:\n            return b\"\"\n\n        if self.finished:\n            data = self.buf.getvalue()\n            ret, rest = data[:size], data[size:]\n            self.buf = io.BytesIO()\n            self.buf.write(rest)\n            return ret\n\n        data = self.unreader.read()\n        while data:\n            self.buf.write(data)\n            if self.buf.tell() >= size:\n                break\n            data = self.unreader.read()\n\n        if not data:\n            self.finished = True\n\n        data = self.buf.getvalue()\n        ret, rest = data[:size], data[size:]\n        self.buf = io.BytesIO()\n        self.buf.write(rest)\n        return ret\n\n", "idx": 1551}
{"namespace": "gunicorn.http.body.Body.read", "completion": "        size = self.getsize(size)\n        if size == 0:\n            return b\"\"\n\n        if size < self.buf.tell():\n            data = self.buf.getvalue()\n            ret, rest = data[:size], data[size:]\n            self.buf = io.BytesIO()\n            self.buf.write(rest)\n            return ret\n\n        data = self.reader.read(1024)\n        while data:\n            self.buf.write(data)\n            if self.buf.tell() >= size:\n                break\n            data = self.reader.read(1024)\n\n        data = self.buf.getvalue()\n        ret, rest = data[:size], data[size:]\n        self.buf = io.BytesIO()\n        self.buf.write(rest)\n        return ret\n", "idx": 1552}
{"namespace": "gunicorn.http.unreader.Unreader.read", "completion": "        if size is not None and not isinstance(size, int):\n            raise TypeError(\"size parameter must be an int or long.\")\n\n        if size == 0:\n            return b\"\"\n\n        if size is None:\n            size = -1\n\n        self.buf.seek(0, 2)\n        if size == -1 and self.buf.tell() > 0:\n            data = self.buf.read()\n            self.buf = io.BytesIO()\n            return data\n\n        while self.buf.tell() < size:\n            data = self.chunk()\n            if not data:\n                data = self.buf.read()\n                self.buf = io.BytesIO()\n                return data\n            self.buf.write(data)\n\n        data = self.buf.read(size)\n        self.buf = io.BytesIO(self.buf.read())\n        return data\n\n", "idx": 1553}
{"namespace": "gunicorn.http.unreader.Unreader.unread", "completion": "        self.buf.write(data)\n", "idx": 1554}
{"namespace": "gunicorn.http.unreader.IterUnreader.chunk", "completion": "        try:\n            return next(self.iter)\n        except StopIteration:\n            self.iter = None\n            return b\"\"", "idx": 1555}
{"namespace": "gunicorn.instrument.statsd.Statsd.critical", "completion": "        Logger.critical(self, msg, *args, **kwargs)\n        self.increment(\"gunicorn.log.critical\")\n", "idx": 1556}
{"namespace": "gunicorn.instrument.statsd.Statsd.access", "completion": "        try:\n            self.histogram(\"gunicorn.request.duration\", request_time.total_seconds() * 1000)\n            self.increment(\"gunicorn.request.count\")\n            self.increment(\"gunicorn.request.count.%s\" % resp.status_code)\n        except Exception:\n            self.warning(\"Failed to log to statsd\", exc_info=True)\n", "idx": 1557}
{"namespace": "praw.exceptions.RedditErrorItem.error_message", "completion": "        return f\"{self.error_type}: {self.error_message} on field {self.error_field}\"\n", "idx": 1558}
{"namespace": "praw.exceptions.RedditErrorItem.__repr__", "completion": "        return f\"{self.__class__.__name__}(error_type={self.error_type!r}, message={self.message!r}, field={self.field!r})\"\n", "idx": 1559}
{"namespace": "praw.models.util.BoundedSet.add", "completion": "        self._access(item)\n        self._set[item] = None\n        if len(self._set) > self.max_items:\n            self._set.popitem(last=False)\n\n", "idx": 1560}
{"namespace": "praw.models.util.ExponentialCounter.counter", "completion": "        jitter = random.uniform(-self._base / 2, self._base / 2)\n        value = self._base + jitter\n        self._base = min(self._base * 2, self._max)\n        return value\n", "idx": 1561}
{"namespace": "praw.models.listing.generator.ListingGenerator._extract_sublist", "completion": "        if isinstance(listing, list):\n            return listing[1]\n        elif isinstance(listing, dict):\n            if \"data\" in listing:\n                return listing[\"data\"]\n            elif \"children\" in listing:\n                return listing[\"children\"]\n            elif \"mod_notes\" in listing:\n                return listing[\"mod_notes\"]\n            elif \"modmail_conversations\" in listing:\n                return listing[\"modmail_conversations\"]\n            elif \"modmail_messages\" in listing:\n                return listing[\"modmail_messages\"]\n            elif \"modmail_subreddits\" in listing:\n                return listing[\"modmail_subreddits\"]\n            elif \"modqueue\" in listing:\n                return listing[\"modqueue\"]\n            elif \"reports\" in listing:\n                return listing[\"reports\"]\n            elif \"search_results\" in listing:\n                return listing[\"search_results\"]\n            elif \"search_result_names\" in listing:\n                return listing[\"search_result_names\"]\n            elif \"search_result_sr_details\" in listing:\n                return listing[\"search_result_sr_details\"]\n            elif \"search_result_subreddits\" in listing:\n                return listing[\"search_result_subreddits\"]\n            elif \"search_result_users\" in listing:\n                return listing[\"search_result_users\"]\n            elif \"search_result_wiki_pages\" in listing:\n                return listing[\"search_result_wiki_pages\"]\n            elif \"search_result_wiki_pages_listing\" in listing:\n                return listing[\"search_result_wiki_pages_listing\"]\n            elif \"search_result_wiki_revisions\" in listing:\n                return listing[\"search_result_wiki_revisions\"]\n            elif \"search_result_wiki_revisions_listing\" in listing:\n                return listing[\"search_result_wiki_revisions_listing\"]\n            elif \"search_result_wikis\" in listing:\n                return listing[\"search_result_wikis\"]\n            elif \"search_result_wikis_listing\" in listing:\n                return listing[\"search_result_wikis_listing\"]\n            elif \"search_result_wikis_settings\" in listing:\n                return listing[\"search_result_wikis_settings\"]\n            elif \"search_result_wikis_settings_listing\" in listing:\n                return listing[\"search_result_wikis_settings_", "idx": 1562}
{"namespace": "praw.util.token_manager.FileTokenManager.post_refresh_callback", "completion": "        with open(self._filename, \"w\") as f:\n            f.write(authorizer.refresh_token)\n", "idx": 1563}
{"namespace": "praw.util.token_manager.FileTokenManager.pre_refresh_callback", "completion": "        with open(self._filename, \"r\") as fp:\n            authorizer.refresh_token = fp.read()\n\n", "idx": 1564}
{"namespace": "praw.util.token_manager.SQLiteTokenManager._get", "completion": "        cursor = self._connection.execute(\n            \"SELECT refresh_token FROM tokens WHERE id = ?\", (self.key,)\n        )\n        result = cursor.fetchone()\n        if result is None:\n            raise KeyError(f\"No refresh token found for key {self.key}\")\n        return result[0]\n", "idx": 1565}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.is_registered", "completion": "        cursor = self._connection.execute(\n            \"SELECT id FROM tokens WHERE id=?\", (self.key,)\n        )\n        result = cursor.fetchone()\n        if result is None:\n            return False\n        return True\n", "idx": 1566}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.post_refresh_callback", "completion": "        self._set(authorizer.refresh_token)\n        authorizer.refresh_token = None\n", "idx": 1567}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.pre_refresh_callback", "completion": "        if authorizer.refresh_token is None:\n            authorizer.refresh_token = self._get()\n", "idx": 1568}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.register", "completion": "        if self.is_registered():\n            return False\n        self._set(refresh_token)\n        return True", "idx": 1569}
{"namespace": "jc.cli.JcCli.about_jc", "completion": "        about_dict: JSONDictType = {\n            'name': info.description,\n            'version': info.version,\n            'description': info.description,\n            'author': info.author,\n            'author_email': info.author_email,\n            'website': info.website,\n            'copyright': info.copyright,\n            'license': info.license,\n            'python_version': sys.version,\n            'python_path': sys.executable,\n            'parser_count': len(parsers()),\n            'standard_parser_count': len([p for p in parsers() if 'standard' in p.get('tags', [])]),\n            'streaming_parser_count': len([p for p in parsers() if p.get('streaming')]),\n            'plugin_parser_count': len([p for p in parsers() if 'plugin' in p.get('tags', [])]),\n            'parser_info': [p.get('info') for p in parsers()]\n        }\n\n        return about_dict\n", "idx": 1570}
{"namespace": "jc.cli.JcCli.yaml_out", "completion": "        try:\n            import ruamel.yaml\n            yaml = ruamel.yaml.YAML()\n            yaml.indent(mapping=2, sequence=4, offset=2)\n            yaml.default_flow_style = False\n            yaml.explicit_start = True\n            yaml.width = 10000\n            yaml.allow_unicode = True\n            yaml.preserve_quotes = True\n            yaml.version = (1, 2)\n            yaml.encoding = 'utf-8'\n            yaml.explicit_end = True\n            yaml.block_seq_indent = 2\n            yaml.default_style = None\n            yaml.default_flow_style = None\n            yaml.allow_unicode = True\n            yaml.preserve_quotes = True\n            yaml.version = (1, 2)\n            yaml.encoding = 'utf-8'\n            yaml.explicit_end = True\n            yaml.block_seq_indent = 2\n            yaml.default_style = None\n            yaml.default_flow_style = None\n            yaml.allow_unicode = True\n            yaml.preserve_quotes = True\n            yaml.version = (1, 2)\n            yaml.encoding = 'utf-8'\n            yaml.explicit_end = True\n            yaml.block_seq_indent = 2\n            yaml.default_style = None\n            yaml.default_flow_style = None\n            yaml.allow_unicode = True\n            yaml.preserve_quotes = True\n            yaml.version = (1, 2)\n            yaml.encoding = 'utf-8'\n            yaml.explicit_end = True\n            yaml.block_seq_indent = 2\n            yaml.default_style = None\n            yaml.default_flow_style = None\n            yaml.allow_unicode = True\n            yaml.preserve_quotes = True\n            yaml.version = (1, 2)\n            yaml.encoding = 'utf-8'\n            yaml.explicit_end = True\n            yaml.block_seq_indent = 2\n            yaml.default_style = None\n            yaml.default_flow_style = None\n            yaml.allow_unicode = True\n            yaml.preserve_quotes = True\n            yaml.version = (1, 2)\n            yaml.encoding = 'utf-8'\n            yaml.explicit_end = True\n            yaml.block_seq_indent = 2\n            yaml.default_style = None\n           ", "idx": 1571}
{"namespace": "jc.parsers.os_release.parse", "completion": "    jc.utils.compatibility(__name__, info.compatible, quiet)\n    return jc.parsers.kv.parse(data, raw=raw, quiet=quiet)", "idx": 1572}
{"namespace": "jc.parsers.xrandr._parse_screen", "completion": "    screen_line = next_lines.pop(0)\n    screen_match = re.match(_screen_pattern, screen_line)\n\n    if not screen_match:\n        next_lines.insert(0, screen_line)\n        return None\n\n    screen_dict: Screen = {\n        \"screen_number\": int(screen_match.group(\"screen_number\")),\n        \"minimum_width\": int(screen_match.group(\"minimum_width\")),\n        \"minimum_height\": int(screen_match.group(\"minimum_height\")),\n        \"current_width\": int(screen_match.group(\"current_width\")),\n        \"current_height\": int(screen_match.group(\"current_height\")),\n        \"maximum_width\": int(screen_match.group(\"maximum_width\")),\n        \"maximum_height\": int(screen_match.group(\"maximum_height\")),\n        \"devices\": [],\n    }\n\n    while next_lines:\n        device = _parse_device(next_lines)\n        if device:\n            screen_dict[\"devices\"].append(device)\n        else:\n            break\n\n    return screen_dict\n\n", "idx": 1573}
{"namespace": "jc.parsers.xrandr._parse_model", "completion": "    if not next_lines:\n        return None\n\n    next_line = next_lines.pop()\n    result = re.match(_edid_head_pattern, next_line)\n    if not result:\n        next_lines.append(next_line)\n        return None\n\n    edid_lines = []\n    while next_lines:\n        next_line = next_lines.pop()\n        result = re.match(_edid_line_pattern, next_line)\n        if not result:\n            next_lines.append(next_line)\n            break\n        else:\n            edid_lines.append(result.groupdict()[\"edid_line\"])\n\n    edid_hex = \"\".join(edid_lines)\n    edid_bytes = _hex_to_bytes(edid_hex)\n\n    model: Model = {\n        \"name\": \"\",\n        \"product_id\": \"\",\n        \"serial_number\": \"\",\n    }\n\n    try:\n        from pyedid import parse_edid\n\n        edid = parse_edid(edid_bytes)\n        model[\"name\"] = edid.model_name\n        model[\"product_id\"] = edid.product_id\n        model[\"serial_number\"] = edid.serial_number\n    except ImportError:\n        if not quiet:\n            jc.utils.warning_message(\n                [\n                    \"pyedid not installed. \"\n                    + \"Install with `pip install pyedid` \"\n                    + \"to parse model information.\"\n                ]\n            )\n\n    return model\n\n", "idx": 1574}
{"namespace": "jc.parsers.xrandr._parse_mode", "completion": "    result = re.match(_mode_pattern, line)\n    if not result:\n        return None\n\n    matches = result.groupdict()\n\n    mode: Mode = {\n        \"frequencies\": [],\n        \"resolution_width\": int(matches[\"resolution_width\"]),\n        \"resolution_height\": int(matches[\"resolution_height\"]),\n        \"is_high_resolution\": matches[\"is_high_resolution\"] is not None\n        and len(matches[\"is_high_resolution\"]) > 0,\n    }\n\n    frequencies_result = re.match(_frequencies_pattern, matches[\"rest\"])\n    if not frequencies_result:\n        return mode\n\n    frequencies_matches = frequencies_result.groupdict()\n\n    for frequency_match in frequencies_matches[\"frequency\"]:\n        frequency: Frequency = {\n            \"frequency\": float(frequency_match),\n            \"is_current\": frequencies_matches[\"star\"] == \"*\",\n            \"is_preferred\": frequencies_matches[\"plus\"] == \"+\",\n        }\n        mode[\"frequencies\"].append(frequency)\n\n    return mode\n\n", "idx": 1575}
{"namespace": "pythonforandroid.archs.Arch.include_dirs", "completion": "        return [\n            join(self.ctx.ndk.sysroot_include_dir, self.command_prefix),\n            join(self.ctx.ndk.sysroot_include_dir, self.command_prefix, 'bits'),\n            join(self.ctx.ndk.sysroot_include_dir, self.command_prefix, 'sys'),\n            join(self.ctx.ndk.sysroot_include_dir, self.command_prefix, 'gnu'),\n            join(self.ctx.ndk.sysroot_include_dir, self.command_prefix, 'android'),\n            join(self.ctx.ndk.sysroot_include_dir, self.command_prefix, 'c++'),\n            join(self.ctx.ndk.sysroot_include_dir, self.command_prefix, 'c++', 'bits'),\n            join(self.ctx.ndk.sysroot_include_dir, self.command_prefix, 'c++', 'v1'),\n            join(self.ctx.ndk.sysroot_include_dir, self.command_prefix, 'c++', 'v1', 'bits'),\n            join(self.ctx.ndk.sysroot_include_dir, self.command_prefix, 'c++', 'v1', 'support', 'android'),\n            join(self.ctx.ndk.sysroot_include_dir, self.command_prefix, 'c++', 'v1', 'support', 'win32'),\n            join(self.ctx.ndk.sysroot_include_dir, self.command_prefix, 'c++', 'v1', 'support', 'win32', 'bits'),\n            join(self.ctx.ndk.sysroot_include_dir, self.command_prefix, 'c++', 'v1', 'support', 'win32', 'thread'),\n            join(self.ctx.ndk.sysroot_include_dir, self.command_prefix, 'c++', 'v1', 'support', 'win32', 'thread', 'x86'),\n            join(self.ctx.ndk.sysroot_include_dir, self.command_prefix, 'c++', 'v1', 'support', 'win32', 'thread", "idx": 1576}
{"namespace": "pythonforandroid.archs.Arch.target", "completion": "        return \"{}{}\".format(self.command_prefix, self.ctx.ndk_api)\n", "idx": 1577}
{"namespace": "pythonforandroid.archs.ArchARM.target", "completion": "        return '{triplet}{ndk_api}'.format(\n            triplet=self.command_prefix, ndk_api=self.ctx.ndk_api\n        )\n\n", "idx": 1578}
{"namespace": "pythonforandroid.recipe.Recipe.get_recipe", "completion": "        if name in cls.recipes:\n            return cls.recipes[name]\n        for recipes_dir in cls.recipe_dirs(ctx):\n            if recipes_dir and exists(recipes_dir):\n                fn = join(recipes_dir, name)\n                if isdir(fn):\n                    modname = \"pythonforandroid.recipes.{}\".format(name)\n                    try:\n                        mod = __import__(modname, fromlist=[modname])\n                    except ImportError:\n                        continue\n                    cls.recipes[name] = mod.Recipe\n                    return mod.Recipe\n        raise Exception(\"Recipe {} not found\".format(name))\n", "idx": 1579}
{"namespace": "pythonforandroid.prerequisites.HomebrewPrerequisite.darwin_helper", "completion": "        info(\n            \"The installer for Homebrew is not supported on macOS. Please follow the instructions at https://brew.sh/ to install Homebrew.\"\n        )\n\n", "idx": 1580}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_checker", "completion": "        return self._darwin_get_brew_formula_location_prefix(\n            self.homebrew_formula_name, installed=True\n        ) is not None\n", "idx": 1581}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_pkg_config_location", "completion": "        return os.path.join(\n            self._darwin_get_brew_formula_location_prefix(\n                self.homebrew_formula_name, installed=True\n            ),\n            \"lib\",\n            \"pkgconfig\",\n        )\n\n", "idx": 1582}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_installer", "completion": "        info(\n            \"Installing OpenSSL using Homebrew. This may take a while, please be patient.\"\n        )\n        subprocess.check_output([\"brew\", \"install\", self.homebrew_formula_name])\n\n", "idx": 1583}
{"namespace": "pythonforandroid.prerequisites.AutoconfPrerequisite.darwin_installer", "completion": "        info(\"Installing Autoconf ...\")\n        subprocess.check_output([\"brew\", \"install\", \"autoconf\"])\n\n", "idx": 1584}
{"namespace": "pythonforandroid.prerequisites.AutomakePrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"automake\", installed=True)\n            is not None\n        )\n", "idx": 1585}
{"namespace": "pythonforandroid.prerequisites.AutomakePrerequisite.darwin_installer", "completion": "        info(\"Installing Automake ...\")\n        subprocess.check_output([\"brew\", \"install\", \"automake\"])\n\n", "idx": 1586}
{"namespace": "pythonforandroid.prerequisites.LibtoolPrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"libtool\", installed=True)\n            is not None\n        )\n", "idx": 1587}
{"namespace": "pythonforandroid.prerequisites.LibtoolPrerequisite.darwin_installer", "completion": "        info(\"Installing Libtool ...\")\n        subprocess.check_output([\"brew\", \"install\", \"libtool\"])\n\n", "idx": 1588}
{"namespace": "pythonforandroid.prerequisites.PkgConfigPrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"pkg-config\", installed=True)\n            is not None\n        )\n", "idx": 1589}
{"namespace": "pythonforandroid.prerequisites.PkgConfigPrerequisite.darwin_installer", "completion": "        info(\"Installing Pkg-Config ...\")\n        subprocess.check_output([\"brew\", \"install\", \"pkg-config\"])\n\n", "idx": 1590}
{"namespace": "pythonforandroid.prerequisites.CmakePrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"cmake\", installed=True)\n            is not None\n        )\n", "idx": 1591}
{"namespace": "pythonforandroid.prerequisites.CmakePrerequisite.darwin_installer", "completion": "        info(\"Installing Cmake ...\")\n        subprocess.check_output([\"brew\", \"install\", \"cmake\"])\n\n", "idx": 1592}
{"namespace": "pythonforandroid.prerequisites.get_required_prerequisites", "completion": "    prerequisites = [\n        HomebrewPrerequisite,\n        JDKPrerequisite,\n        OpenSSLPrerequisite,\n        AutoconfPrerequisite,\n        AutomakePrerequisite,\n        LibtoolPrerequisite,\n        PkgConfigPrerequisite,\n        CmakePrerequisite,\n    ]\n\n    return [prerequisite() for prerequisite in prerequisites if prerequisite.mandatory[platform]]\n\n", "idx": 1593}
{"namespace": "pythonforandroid.pythonpackage.parse_as_folder_reference", "completion": "    # Check if the dependency is a folder path:\n    if dep.startswith(\"file://\"):\n        # Remove the file:// prefix:\n        dep = dep[7:]\n    if dep.startswith(\"file:\"):\n        # Remove the file: prefix:\n        dep = dep[5:]\n    if dep.startswith(\"///\"):\n        # Remove the /// prefix:\n        dep = dep[3:]\n    if dep.startswith(\"///\"):\n        # Remove the /// prefix:\n        dep = dep[3:]\n    if dep.startswith(\"///\"):\n        # Remove the /// prefix:\n        dep = dep[3:]\n    if dep.startswith(\"///\"):\n        # Remove the /// prefix:\n        dep = dep[3:]\n    if dep.startswith(\"///\"):\n        # Remove the /// prefix:\n        dep = dep[3:]\n    if dep.startswith(\"///\"):\n        # Remove the /// prefix:\n        dep = dep[3:]\n    if dep.startswith(\"///\"):\n        # Remove the /// prefix:\n        dep = dep[3:]\n    if dep.startswith(\"///\"):\n        # Remove the /// prefix:\n        dep = dep[3:]\n    if dep.startswith(\"///\"):\n        # Remove the /// prefix:\n        dep = dep[3:]\n    if dep.startswith(\"///\"):\n        # Remove the /// prefix:\n        dep = dep[3:]\n    if dep.startswith(\"///\"):\n        # Remove the /// prefix:\n        dep = dep[3:]\n    if dep.startswith(\"///\"):\n        # Remove the /// prefix:\n        dep = dep[3:]\n    if dep.startswith(\"///\"):\n        # Remove the /// prefix:\n        dep = dep[3:]\n    if dep.startswith(\"///\"):\n        # Remove the /// prefix:\n        dep = dep[3:]\n    if dep.startswith(\"///\"):\n        # Remove the /// prefix:\n        dep = dep[3:]\n    if dep.startswith(\"///\"):\n        # Remove the /// prefix:\n        dep = dep[3:]\n    if dep.startswith(\"///\"):\n        # Remove the /// prefix:\n        dep = dep[3:]\n    if dep.startswith(\"///\"):\n        # Remove the /// prefix:\n        dep = dep[3:]\n    if dep.startswith(\"///\"):\n        # Remove the /// prefix:\n        dep = dep[3:]\n    if dep.startswith(\"///\"):\n        # Remove the /// prefix:\n        dep = dep[3:]\n   ", "idx": 1594}
{"namespace": "pythonforandroid.pythonpackage.get_package_name", "completion": "    if use_cache and dependency in package_name_cache:\n        return package_name_cache[dependency]\n    else:\n        package_name = _extract_info_from_package(dependency,\n                                                   extract_type=\"name\")\n        package_name_cache[dependency] = package_name\n        return package_name\n\n", "idx": 1595}
{"namespace": "pythonforandroid.recommendations.read_ndk_version", "completion": "    try:\n        with open(join(ndk_dir, 'source.properties')) as f:\n            for line in f:\n                if line.startswith('Pkg.Revision'):\n                    version = line.split('=')[1].strip()\n                    return LooseVersion(version)\n    except IOError:\n        warning(READ_ERROR_NDK_MESSAGE.format(ndk_dir=ndk_dir))\n        warning(\n            ENSURE_RIGHT_NDK_MESSAGE.format(\n                min_supported=MIN_NDK_VERSION,\n                rec_version=RECOMMENDED_NDK_VERSION,\n                ndk_url=NDK_DOWNLOAD_URL,\n            )\n        )\n        return None\n    except ValueError:\n        warning(PARSE_ERROR_NDK_MESSAGE)\n        warning(\n            ENSURE_RIGHT_NDK_MESSAGE.format(\n                min_supported=MIN_NDK_VERSION,\n                rec_version=RECOMMENDED_NDK_VERSION,\n                ndk_url=NDK_DOWNLOAD_URL,\n            )\n        )\n        return None\n    except IndexError:\n        warning(PARSE_ERROR_NDK_MESSAGE)\n        warning(\n            ENSURE_RIGHT_NDK_MESSAGE.format(\n                min_supported=MIN_NDK_VERSION,\n                rec_version=RECOMMENDED_NDK_VERSION,\n                ndk_url=NDK_DOWNLOAD_URL,\n            )\n        )\n        return None\n    else:\n        warning(UNKNOWN_NDK_MESSAGE)\n        warning(\n            ENSURE_RIGHT_NDK_MESSAGE.format(\n                min_supported=MIN_NDK_VERSION,\n                rec_version=RECOMMENDED_NDK_VERSION,\n                ndk_url=NDK_DOWNLOAD_URL,\n            )\n        )\n        return None\n\n", "idx": 1596}
{"namespace": "pythonforandroid.recommendations.check_target_api", "completion": "    if api < MIN_TARGET_API:\n        warning(\n            'Target API {} is less than the minimum recommended version {}. '\n            'This may cause problems with Google Play. {}'.format(\n                api, MIN_TARGET_API, OLD_API_MESSAGE\n            )\n        )\n\n    if arch == 'armeabi':\n        if api > ARMEABI_MAX_TARGET_API:\n            warning(\n                UNSUPPORTED_NDK_API_FOR_ARMEABI_MESSAGE.format(\n                    req_ndk_api=api, max_ndk_api=ARMEABI_MAX_TARGET_API\n                )\n            )\n\n", "idx": 1597}
{"namespace": "pythonforandroid.recommendations.check_ndk_api", "completion": "    if ndk_api < MIN_NDK_API:\n        warning(OLD_NDK_API_MESSAGE)\n\n    if ndk_api > android_api:\n        raise BuildInterruptingException(\n            TARGET_NDK_API_GREATER_THAN_TARGET_API_MESSAGE.format(\n                ndk_api=ndk_api, android_api=android_api\n            ),\n            instructions=(\n                'You probably want to build with --ndk-api={} instead'.format(\n                    android_api\n                )\n            ),\n        )\n\n", "idx": 1598}
{"namespace": "pythonforandroid.androidndk.AndroidNDK.llvm_prebuilt_dir", "completion": "        return os.path.join(self.ndk_dir, \"toolchains\", \"llvm\", \"prebuilt\", self.host_tag)\n", "idx": 1599}
{"namespace": "pythonforandroid.build.Context.setup_dirs", "completion": "        self.storage_dir = storage_dir\n        self.build_dir = join(self.storage_dir, 'build')\n        self.dist_dir = join(self.storage_dir, 'dists')\n\n        ensure_dir(self.storage_dir)\n        ensure_dir(self.build_dir)\n        ensure_dir(self.dist_dir)\n", "idx": 1600}
{"namespace": "pythonforandroid.graph.get_dependency_tuple_list_for_recipe", "completion": "    if blacklist is None:\n        blacklist = set()\n\n    dependencies = recipe.depends\n    dependencies = [dep.lower() for dep in dependencies]\n    dependencies = [dep for dep in dependencies if dep not in blacklist]\n    dependencies = [tuple(dep.split()) for dep in dependencies]\n    return dependencies\n\n", "idx": 1601}
{"namespace": "pythonforandroid.graph.obvious_conflict_checker", "completion": "    if blacklist is None:\n        blacklist = set()\n    assert type(blacklist) is set\n\n    # Add dependencies for all recipes\n    for name_tuple in name_tuples:\n        for name in name_tuple:\n            try:\n                recipe = Recipe.get_recipe(name, ctx)\n                dependencies = get_dependency_tuple_list_for_recipe(\n                    recipe, blacklist=blacklist\n                )\n            except ValueError:\n                # The recipe does not exist, so we assume it can be installed\n                # via pip with no extra dependencies\n                dependencies = []\n\n            # Add dependencies to the list of recipes to build\n            for dependency_set in product(*dependencies):\n                name_tuples.append(dependency_set)\n\n    # Check for obvious conflicts\n    for name_tuple in name_tuples:\n        for name in name_tuple:\n            try:\n                recipe = Recipe.get_recipe(name, ctx)\n                conflicts = [dep.lower() for dep in recipe.conflicts]\n            except ValueError:\n                conflicts = []\n\n            for conflict in conflicts:\n                if conflict in name_tuple:\n                    raise BuildInterruptingException(\n                        'Conflicting recipes: %s and %s' % (name, conflict)\n                    )\n\n", "idx": 1602}
{"namespace": "pythonforandroid.graph.get_recipe_order_and_bootstrap", "completion": "    if blacklist is None:\n        blacklist = set()\n    assert type(blacklist) is set\n    names = [name.lower() for name in names]\n    if bs is None:\n        bs = ctx.bootstrap\n    bs_deps = bs.get_recipe_order(ctx)\n    names.extend(bs_deps)\n    names = list(set(names))\n\n    # Check for obvious conflicts:\n    obvious_conflict_checker(ctx, names, blacklist=blacklist)\n\n    # Get all possible orders:\n    orders = recursively_collect_orders(\n        names[0], ctx, names, blacklist=blacklist\n    )\n\n    # Convert orders to linear lists:\n    orders = [\n        [name for name, deps in order.items()]\n        for order in orders\n    ]\n\n    # Sort orders by preference:\n    orders.sort(key=lambda order: len(order))\n\n    # Return the first order:\n    order = orders[0]\n    recipes = [Recipe.get_recipe(name, ctx) for name in order]\n    python_modules = [recipe.python_module for recipe in recipes]\n    return order, recipes, python_modules, bs", "idx": 1603}
{"namespace": "pythonforandroid.util.ensure_dir", "completion": "    if not exists(dn):\n        LOGGER.debug(\"Creating directory {}\".format(dn))\n        makedirs(dn)\n\n", "idx": 1604}
{"namespace": "pythonforandroid.util.move", "completion": "    LOGGER.debug(\"Move file or directory from {0} to {1}\".format(source, destination))\n    shutil.move(source, destination)\n\n", "idx": 1605}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.get_bootstrap_from_recipes", "completion": "        # Get all bootstraps:\n        bootstraps = cls.get_usable_bootstraps_for_recipes(recipes, ctx)\n\n        # If there's no bootstraps, raise an error:\n        if not bootstraps:\n            raise BuildInterruptingException(\n                'No bootstraps found that satisfy the requirements of the '\n                'given recipes. Please specify a bootstrap manually.')\n\n        # If there's only one bootstrap, return it:\n        if len(bootstraps) == 1:\n            return list(bootstraps)[0]\n\n        # If there's more than one bootstrap, select the recommended one:\n        recommended_bootstraps = []\n        for bs in bootstraps:\n            if bs.name == \"sdl2\":\n                recommended_bootstraps.append(bs)\n            elif \"webview\" in bs.recipe_depends:\n                recommended_bootstraps.append(bs)\n\n        # If there's only one recommended bootstrap, return it:\n        if len(recommended_bootstraps) == 1:\n            return recommended_bootstraps[0]\n\n        # If there's more than one recommended bootstrap, select the one with the highest priority:\n        if len(recommended_bootstraps) > 1:\n            recommended_bootstraps.sort(key=functools.cmp_to_key(_cmp_bootstraps_by_priority))\n            return recommended_bootstraps[0]\n\n        # If there's no recommended bootstrap, select the one with the highest priority:\n        bootstraps.sort(key=functools.cmp_to_key(_cmp_bootstraps_by_priority))\n        return bootstraps[0]\n", "idx": 1606}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.get_bootstrap", "completion": "        if name not in cls.all_bootstraps():\n            raise ValueError('Unknown bootstrap: {}'.format(name))\n        bootstrap_dir = join(dirname(__file__), 'bootstraps', name)\n        if not os.path.exists(bootstrap_dir):\n            raise ValueError('Bootstrap directory does not exist: {}'.format(bootstrap_dir))\n        module_name = 'pythonforandroid.bootstraps.{}'.format(name)\n        module = importlib.import_module(module_name)\n        bootstrap_class = getattr(module, name.capitalize())\n        bootstrap = bootstrap_class(ctx)\n        bootstrap.bootstrap_dir = bootstrap_dir\n        return bootstrap\n", "idx": 1607}
{"namespace": "pythonforandroid.bootstrap.expand_dependencies", "completion": "    # Get all recipes that have alternatives:\n    recipes_with_alternatives = []\n    for recipe in recipes:\n        if isinstance(recipe, (tuple, list)):\n            recipes_with_alternatives.append(recipe)\n\n    # Get all recipes that do not have alternatives:\n    recipes_without_alternatives = []\n    for recipe in recipes:\n        if not isinstance(recipe, (tuple, list)):\n            recipes_without_alternatives.append(recipe)\n\n    # Get all recipes that have alternatives and their alternatives:\n    recipes_with_alternatives_and_alternatives = []\n    for recipe in recipes_with_alternatives:\n        alternatives = []\n        for alternative in recipe:\n            alternatives.append(alternative)\n            alternatives.append(Recipe.get_recipe(alternative, ctx).depends)\n        recipes_with_alternatives_and_alternatives.append(alternatives)\n\n    # Get all recipes that have alternatives and their alternatives and the recipes that do not have alternatives:\n    recipes_with_alternatives_and_alternatives_and_recipes_without_alternatives = []\n    for recipe in recipes_with_alternatives_and_alternatives:\n        recipes_with_alternatives_and_alternatives_and_recipes_without_alternatives.append(recipe + recipes_without_alternatives)\n\n    return recipes_with_alternatives_and_alternatives_and_recipes_without_alternatives", "idx": 1608}
{"namespace": "pythonforandroid.recipes.icu.ICURecipe.get_recipe_dir", "completion": "        if self.local_recipes is not None:\n            if exists(join(self.local_recipes, 'icu4c')):\n                return join(self.local_recipes, 'icu4c')\n        return join(self.root_dir, 'python-for-android', 'recipes', 'icu4c')\n", "idx": 1609}
{"namespace": "mmcv.transforms.wrappers.KeyMapper.__repr__", "completion": "        return (f'{self.__class__.__name__}(transforms = {self.transforms}, '\n                f'mapping = {self.mapping}, remapping = {self.remapping}, '\n                f'auto_remap = {self.auto_remap}, '\n                f'allow_nonexist_keys = {self.allow_nonexist_keys})')\n\n", "idx": 1610}
{"namespace": "mmcv.transforms.wrappers.TransformBroadcaster.__repr__", "completion": "        repr_str = self.__class__.__name__\n        repr_str += f'(transforms = {self.transforms}'\n        repr_str += f', mapping = {self.mapping}'\n        repr_str += f', remapping = {self.remapping}'\n        repr_str += f', auto_remap = {self.auto_remap}'\n        repr_str += f', allow_nonexist_keys = {self.allow_nonexist_keys}'\n        repr_str += f', share_random_params = {self.share_random_params})'\n        return repr_str\n\n", "idx": 1611}
{"namespace": "mackup.utils.delete", "completion": "    if os.path.islink(filepath):\n        os.unlink(filepath)\n    elif os.path.isfile(filepath):\n        os.remove(filepath)\n    elif os.path.isdir(filepath):\n        shutil.rmtree(filepath)\n\n", "idx": 1612}
{"namespace": "mackup.utils.copy", "completion": "    # Check if the source and destination paths are valid and absolute paths\n    if not os.path.isabs(src):\n        raise ValueError(\"Source path must be an absolute path.\")\n    if not os.path.isabs(dst):\n        raise ValueError(\"Destination path must be an absolute path.\")\n\n    # Create the necessary directories in the destination path if they do not exist\n    dst_dir = os.path.dirname(dst)\n    if not os.path.exists(dst_dir):\n        os.makedirs(dst_dir)\n\n    # Copy the file or folder to the destination\n    if os.path.isfile(src):\n        shutil.copy(src, dst)\n    elif os.path.isdir(src):\n        shutil.copytree(src, dst)\n    else:\n        raise ValueError(\"Source path must be a file or a folder.\")\n\n    # Set the appropriate file permissions for the copied file or folder\n    permissions = os.stat(src).st_mode\n    os.chmod(dst, permissions)\n\n", "idx": 1613}
{"namespace": "mackup.utils.get_dropbox_folder_location", "completion": "    # Get the home directory\n    home = os.path.expanduser(\"~\")\n\n    # Get the path to the host.db file\n    host_db_path = os.path.join(home, \".dropbox\", \"host.db\")\n\n    # Check if the host.db file exists\n    if not os.path.exists(host_db_path):\n        return None\n\n    # Open the host.db file\n    with open(host_db_path, \"rb\") as host_db_file:\n        # Read the first 8 bytes of the file\n        first_8_bytes = host_db_file.read(8)\n\n        # Check if the first 8 bytes are the magic bytes\n        if first_8_bytes != b\"SQLite\\x00\\x01\":\n            return None\n\n        # Read the next 4 bytes of the file\n        next_4_bytes = host_db_file.read(4)\n\n        # Check if the next 4 bytes are the magic bytes\n        if next_4_bytes != b\"\\x00\\x01\\x00\\x00\":\n            return None\n\n        # Read the next 4 bytes of the file\n        next_4_bytes = host_db_file.read(4)\n\n        # Check if the next 4 bytes are the magic bytes\n        if next_4_bytes != b\"\\x00\\x00\\x00\\x00\":\n            return None\n\n        # Read the next 4 bytes of the file\n        next_4_bytes = host_db_file.read(4)\n\n        # Check if the next 4 bytes are the magic bytes\n        if next_4_bytes != b\"\\x00\\x00\\x00\\x00\":\n            return None\n\n        # Read the next 4 bytes of the file\n        next_4_bytes = host_db_file.read(4)\n\n        # Check if the next 4 bytes are the magic bytes\n        if next_4_bytes != b\"\\x00\\x00\\x00\\x00\":\n            return None\n\n        # Read the next 4 bytes of the file\n        next_4_bytes = host_db_file.read(4)\n\n        # Check if the next 4 bytes are the magic bytes\n        if next_4_bytes", "idx": 1614}
{"namespace": "mackup.utils.get_copy_folder_location", "completion": "    copy_db_path = \"Library/Application Support/Copy/settings.db\"\n    copy_db = os.path.join(os.environ[\"HOME\"], copy_db_path)\n    if os.path.isfile(copy_db):\n        con = sqlite3.connect(copy_db)\n        if con:\n            cur = con.cursor()\n            query = (\n                \"SELECT value \"\n                \"FROM options \"\n                \"WHERE option = 'csmRootPath';\"\n            )\n            cur.execute(query)\n            data = cur.fetchone()\n            copy_home = str(data[0])\n            con.close()\n\n    if not copy_home:\n        error(\n            constants.ERROR_UNABLE_TO_FIND_STORAGE.format(\n                provider=\"Copy install\"\n            )\n        )\n\n    return copy_home\n\n", "idx": 1615}
{"namespace": "mackup.utils.can_file_be_synced_on_current_platform", "completion": "    # Check if the path is a valid file or folder\n    if not os.path.exists(path):\n        return False\n\n    # Check if the path is a symbolic link\n    if os.path.islink(path):\n        return False\n\n    # Check if the path is a hidden file or folder\n    if os.path.basename(path).startswith(\".\"):\n        return False\n\n    # Check if the path is a file or folder in the home directory\n    if os.path.dirname(path) == os.path.expanduser(\"~\"):\n        return False\n\n    # Check if the path is a file or folder in the Downloads directory\n    if os.path.basename(path) == \"Downloads\":\n        return False\n\n    # Check if the path is a file or folder in the Desktop directory\n    if os.path.basename(path) == \"Desktop\":\n        return False\n\n    # Check if the path is a file or folder in the Documents directory\n    if os.path.basename(path) == \"Documents\":\n        return False\n\n    # Check if the path is a file or folder in the Pictures directory\n    if os.path.basename(path) == \"Pictures\":\n        return False\n\n    # Check if the path is a file or folder in the Music directory\n    if os.path.basename(path) == \"Music\":\n        return False\n\n    # Check if the path is a file or folder in the Videos directory\n    if os.path.basename(path) == \"Videos\":\n        return False\n\n    # Check if the path is a file or folder in the Movies directory\n    if os.path.basename(path) == \"Movies\":\n        return False\n\n    # Check if the path is a file or folder in the Public directory\n    if os.path.basename(path) == \"Public\":\n        return False\n\n    # Check if the path is a file or folder in the Templates directory\n    if os.path.basename(path) == \"Templates\":\n        return False\n\n    # Check if the path is a file or folder in the Library directory\n    if os.path.basename(path) == \"Library\":\n        return False\n\n    # Check if the path is a file or folder in the Applications directory\n    if os.path.basename(path) == \"Applications\":\n        return False\n\n    # Check if the path is a file or folder in the System directory\n    if os.path.basename(path) == \"", "idx": 1616}
{"namespace": "hl7.client.MLLPClient.send_message", "completion": "        if isinstance(message, hl7.Message):\n            message = message.to_bytes()\n        elif isinstance(message, str):\n            message = message.encode(self.encoding)\n\n        return self.send(message)\n", "idx": 1617}
{"namespace": "hl7.client.MLLPClient.send", "completion": "        # Send data to the server\n        self.socket.sendall(data)\n\n        # Receive data from the server\n        response = b\"\"\n        while True:\n            part = self.socket.recv(RECV_BUFFER)\n            response += part\n            if EB in part:\n                break\n\n        # Remove the MLLP container\n        response = response[1:-2]\n\n        return response\n\n", "idx": 1618}
{"namespace": "hl7.datatypes._UTCOffset.tzname", "completion": "        if self.minutes < 0:\n            return \"-%02d%02d\" % (-self.minutes // 60, -self.minutes % 60)\n        else:\n            return \"+%02d%02d\" % (self.minutes // 60, self.minutes % 60)\n", "idx": 1619}
{"namespace": "hl7.datatypes.parse_datetime", "completion": "    if not value:\n        return None\n\n    # Parse the date and time components\n    match = DTM_TZ_RE.match(value)\n    if not match:\n        raise ValueError(\"Invalid DTM format: %s\" % value)\n\n    # Parse the date and time components\n    date_parts = match.group(1).split(\".\")\n    time_parts = match.group(2).split(\".\") if match.group(2) else []\n\n    # Parse the timezone offset\n    tz_offset = None\n    if match.group(3):\n        tz_offset = int(match.group(3)) * 60 + int(match.group(4))\n        if match.group(2) == \"-\":\n            tz_offset = -tz_offset\n\n    # Construct the datetime object\n    dt = datetime.datetime(\n        int(date_parts[0]),\n        int(date_parts[1]) if len(date_parts) > 1 else 1,\n        int(date_parts[2]) if len(date_parts) > 2 else 1,\n        int(time_parts[0]) if len(time_parts) > 0 else 0,\n        int(time_parts[1]) if len(time_parts) > 1 else 0,\n        int(time_parts[2]) if len(time_parts) > 2 else 0,\n        int(math.floor(float(\"0.\" + time_parts[3]) * 1000000))\n        if len(time_parts) > 3\n        else 0,\n    )\n\n    # Apply the timezone offset\n    if tz_offset:\n        dt = dt.replace(tzinfo=_UTCOffset(tz_offset))\n\n    return dt", "idx": 1620}
{"namespace": "hl7.parser._ParsePlan.container", "completion": "        return self.containers[0](data, self.esc, self.separators, self.factory)\n", "idx": 1621}
{"namespace": "hl7.parser._ParsePlan.next", "completion": "        return _ParsePlan(\n            self.separators[self.separators.find(self.separator) + 1],\n            self.separators,\n            self.containers[1:],\n            self.esc,\n            self.factory,\n        )\n", "idx": 1622}
{"namespace": "hl7.version.get_version", "completion": "    if len(VERSION) == 4:\n        if VERSION[3] == \"final\":\n            return \"%d.%d.%d\" % VERSION[:3]\n        else:\n            return \"%d.%d.%d%s\" % VERSION\n    elif len(VERSION) == 3:\n        return \"%d.%d.%d\" % VERSION\n    else:\n        return \"%d.%d\" % VERSION[:2]", "idx": 1623}
{"namespace": "twtxt.config.Config.from_file", "completion": "        if not os.path.isfile(file):\n            raise FileNotFoundError(f\"Config file {file} not found.\")\n\n        cfg = configparser.ConfigParser()\n        cfg.read(file)\n\n        return cls(file, cfg)\n", "idx": 1624}
{"namespace": "twtxt.config.Config.discover", "completion": "        config_file = os.path.join(cls.config_dir, cls.config_name)\n        return cls.from_file(config_file)\n", "idx": 1625}
{"namespace": "twtxt.config.Config.create_config", "completion": "        config = configparser.ConfigParser()\n        config[\"DEFAULT\"] = {\n            \"nick\": nick,\n            \"twtfile\": twtfile,\n            \"twturl\": twturl,\n            \"disclose_identity\": str(disclose_identity),\n            \"add_news\": str(add_news),\n        }\n        config[\"sources\"] = {}\n        with open(cfgfile, \"w\") as configfile:\n            config.write(configfile)\n        return cls.from_file(cfgfile)\n", "idx": 1626}
{"namespace": "twtxt.config.Config.following", "completion": "        following = []\n        if self.cfg.has_section(\"following\"):\n            for name, url in self.cfg.items(\"following\"):\n                following.append(Source(name, url))\n        else:\n            logger.debug(\"No following section found.\")\n        return following\n", "idx": 1627}
{"namespace": "twtxt.config.Config.options", "completion": "        options = {}\n        try:\n            for (option, value) in self.cfg.items(\"twtxt\"):\n                options[option] = value\n        except configparser.NoSectionError as e:\n            logger.debug(e)\n\n        return options\n", "idx": 1628}
{"namespace": "twtxt.models.Tweet.relative_datetime", "completion": "        # Get the current time in UTC\n        now = datetime.now(timezone.utc)\n\n        # Calculate the difference between the current time and the tweet's creation time\n        delta = now - self.created_at\n\n        # Convert the difference to seconds\n        seconds = delta.total_seconds()\n\n        # Determine the tense based on the sign of the difference\n        tense = \"ago\" if seconds < 0 else \"from now\"\n\n        # Calculate the absolute value of the difference\n        seconds = abs(seconds)\n\n        # Convert the difference to human-readable units\n        if seconds < 60:\n            return f\"{seconds:.0f} seconds {tense}\"\n        elif seconds < 3600:\n            return f\"{seconds / 60:.0f} minutes {tense}\"\n        elif seconds < 86400:\n            return f\"{seconds / 3600:.0f} hours {tense}\"\n        elif seconds < 604800:\n            return f\"{seconds / 86400:.0f} days {tense}\"\n        elif seconds < 2629746:\n            return f\"{seconds / 604800:.0f} weeks {tense}\"\n        elif seconds < 31556952:\n            return f\"{seconds / 2629746:.0f} months {tense}\"\n        else:\n            return f\"{seconds / 31556952:.0f} years {tense}\"\n", "idx": 1629}
{"namespace": "twtxt.mentions.format_mentions", "completion": "    def handle_mention(match):\n        return format_callback(match.group(\"name\"), match.group(\"url\"))\n\n    return mention_re.sub(handle_mention, text)", "idx": 1630}
{"namespace": "twtxt.parser.parse_tweets", "completion": "    tweets = []\n    for raw_tweet in raw_tweets:\n        try:\n            tweet = parse_tweet(raw_tweet, source, now)\n            tweets.append(tweet)\n        except Exception as e:\n            logger.error(f\"Error parsing tweet: {raw_tweet}\")\n            logger.error(e)\n    return tweets\n\n", "idx": 1631}
{"namespace": "wikipediaapi.Wikipedia.page", "completion": "        if unquote:\n            title = parse.unquote(title)\n\n        return WikipediaPage(\n            self,\n            title,\n            namespace=ns,\n        )\n", "idx": 1632}
{"namespace": "wikipediaapi.Wikipedia.article", "completion": "        return self.page(title, ns, unquote)\n", "idx": 1633}
{"namespace": "wikipediaapi.WikipediaPageSection.__repr__", "completion": "        res = \"\"\n        if self.wiki.extract_format == ExtractFormat.WIKI:\n            res += self.title\n        elif self.wiki.extract_format == ExtractFormat.HTML:\n            res += f\"<h{self.level}>{self.title}</h{self.level}>\"\n        else:\n            raise NotImplementedError(\"Unknown ExtractFormat type\")\n\n        res += \"\\n\"\n        res += self._text\n        if len(self._text) > 0:\n            res += \"\\n\\n\"\n        for sec in self.sections:\n            res += repr(sec)\n        return res\n\n", "idx": 1634}
{"namespace": "wikipediaapi.WikipediaPage.sections", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        return self._section\n", "idx": 1635}
{"namespace": "wikipediaapi.WikipediaPage.section_by_title", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        if title in self._section_mapping:\n            return self._section_mapping[title][-1]\n        return None\n", "idx": 1636}
{"namespace": "wikipediaapi.WikipediaPage.sections_by_title", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        sections = self._section_mapping.get(title)\n        if sections:\n            return sections\n        return []\n", "idx": 1637}
{"namespace": "wikipediaapi.WikipediaPage.text", "completion": "        text = self.summary\n        for section in self.sections:\n            text += section.full_text()\n        return text.strip()\n", "idx": 1638}
{"namespace": "wikipediaapi.WikipediaPage.langlinks", "completion": "        if not self._called[\"langlinks\"]:\n            self._fetch(\"langlinks\")\n        return self._langlinks\n", "idx": 1639}
{"namespace": "wikipediaapi.WikipediaPage.links", "completion": "        if not self._called[\"links\"]:\n            self._fetch(\"links\")\n        return self._links\n", "idx": 1640}
{"namespace": "wikipediaapi.WikipediaPage.backlinks", "completion": "        if not self._called[\"backlinks\"]:\n            self._fetch(\"backlinks\")\n        return self._backlinks\n", "idx": 1641}
{"namespace": "wikipediaapi.WikipediaPage.categorymembers", "completion": "        if not self._called[\"categorymembers\"]:\n            self._fetch(\"categorymembers\")\n        return self._categorymembers\n", "idx": 1642}
{"namespace": "wikipediaapi.WikipediaPage._fetch", "completion": "        self._called[call] = True\n        self.wiki._fetch(self, call)\n        return self\n", "idx": 1643}
{"namespace": "wikipediaapi.WikipediaPage.__repr__", "completion": "        if self._called[\"extracts\"]:\n            return f\"{self.title} (id: {self.pageid}, ns: {self.namespace})\"\n        return f\"{self.title} (id: ??, ns: {self.namespace})\"\n\n", "idx": 1644}
{"namespace": "imapclient.imapclient.IMAPClient.starttls", "completion": "        if self._starttls_done:\n            raise exceptions.AlreadyStartedTLS(\"STARTTLS already done\")\n\n        if ssl_context is None:\n            ssl_context = ssl_lib.create_default_context()\n\n        self._imap.starttls(ssl_context)\n        self._imap = tls.IMAP4_TLS_downgrade(self._imap)\n        self._set_read_timeout()\n        self._starttls_done = True\n", "idx": 1645}
{"namespace": "imapclient.imapclient.IMAPClient.shutdown", "completion": "        self._imap.shutdown()\n        logger.info(\"Connection closed\")\n", "idx": 1646}
{"namespace": "imapclient.imapclient.IMAPClient.enable", "completion": "        try:\n            return self._command_and_check(\"enable\", *capabilities, unpack=True)\n        except exceptions.IMAPClientError as e:\n            raise exceptions.CapabilityError(str(e))\n", "idx": 1647}
{"namespace": "imapclient.imapclient.IMAPClient._proc_folder_list", "completion": "        return [\n            (flags, delimiter, self._normalise_folder(name))\n            for flags, delimiter, name in parse_response(folder_data)\n            if name is not None\n        ]\n", "idx": 1648}
{"namespace": "imapclient.imapclient.IMAPClient.select_folder", "completion": "        if not folder:\n            raise ValueError(\"No folder specified\")\n\n        folder = self._normalise_folder(folder)\n        if readonly:\n            typ, dat = self._imap._simple_command(\"EXAMINE\", folder)\n        else:\n            typ, dat = self._imap._simple_command(\"SELECT\", folder)\n        self._checkok(\"select\", typ, dat)\n        typ, dat = self._imap._untagged_response(typ, dat, \"SELECT\")\n        return self._proc_folder_select(dat)\n", "idx": 1649}
{"namespace": "imapclient.imapclient.IMAPClient.unselect_folder", "completion": "        self._command_and_check(\"unselect\")\n        return self._imap.untagged_responses\n", "idx": 1650}
{"namespace": "imapclient.imapclient.IMAPClient.noop", "completion": "        return self._command_and_check(\"noop\")\n", "idx": 1651}
{"namespace": "imapclient.imapclient.IMAPClient.idle", "completion": "        self._command_and_check(\"IDLE\")\n        return self._consume_until_tagged_response(\"IDLE\")\n", "idx": 1652}
{"namespace": "imapclient.imapclient.IMAPClient.idle_check", "completion": "        if self._idle_tag is None:\n            raise exceptions.IllegalStateError(\n                \"IDLE command illegal in state %s\" % self._imap.state\n            )\n\n        # If we have a socket, poll it for events\n        if self._imap.sock is not None:\n            try:\n                self._select_poll_socket(self._imap.sock, timeout)\n            except select.error:\n                pass\n\n        # If we have a socket, poll it for events\n        if self._imap.sock is not None:\n            try:\n                self._poll_socket(self._imap.sock, timeout)\n            except select.error:\n                pass\n\n        # If we have a socket, poll it for events\n        if self._imap.sock is not None:\n            try:\n                self._select_poll_socket(self._imap.sock, timeout)\n            except select.error:\n                pass\n\n        # If we have a socket, poll it for events\n        if self._imap.sock is not None:\n            try:\n                self._poll_socket(self._imap.sock, timeout)\n            except select.error:\n                pass\n\n        # If we have a socket, poll it for events\n        if self._imap.sock is not None:\n            try:\n                self._select_poll_socket(self._imap.sock, timeout)\n            except select.error:\n                pass\n\n        # If we have a socket, poll it for events\n        if self._imap.sock is not None:\n            try:\n                self._poll_socket(self._imap.sock, timeout)\n            except select.error:\n                pass\n\n        # If we have a socket, poll it for events\n        if self._imap.sock is not None:\n            try:\n                self._select_poll_socket(self._imap.sock, timeout)\n            except select.error:\n                pass\n\n        # If we have a socket, poll it for events\n        if self._imap.sock is not None:\n            try:\n                self._poll_socket(self._imap.sock, timeout)\n            except select.error:\n                pass\n\n        # If we have a socket, poll it for events\n        if self._imap.sock is not None:\n            try:\n                self._select_poll_socket(self._imap.sock, timeout)\n            except select.error:\n                pass\n\n        # If we have a socket, poll it for events\n       ", "idx": 1653}
{"namespace": "imapclient.imapclient.IMAPClient.idle_done", "completion": "        sock = self.socket()\n\n        # make the socket non-blocking so the timeout can be\n        # implemented for this call\n        sock.settimeout(None)\n        sock.setblocking(0)\n\n        try:\n            resp = self._imap._get_line()\n            if resp is None:\n                raise exceptions.IMAPClientError(\"IDLE DONE failed\")\n            resp = _parse_untagged_response(resp)\n            return resp\n        finally:\n            sock.setblocking(1)\n            self._set_read_timeout()\n", "idx": 1654}
{"namespace": "imapclient.imapclient.IMAPClient.folder_status", "completion": "        if what is None:\n            what = [\"MESSAGES\", \"RECENT\", \"UIDNEXT\", \"UIDVALIDITY\", \"UNSEEN\"]\n        folder = self._normalise_folder(folder)\n        what = [self._normalise_folder(item) for item in what]\n        what = b\",\".join(what)\n        what = b\"(\" + what + b\")\"\n        typ, dat = self._imap._simple_command(\"STATUS\", folder, what)\n        self._checkok(\"STATUS\", typ, dat)\n        typ, dat = self._imap._untagged_response(typ, dat, \"STATUS\")\n        return self._process_select_response(dat)\n", "idx": 1655}
{"namespace": "imapclient.imapclient.IMAPClient.sort", "completion": "        return self._sort(sort_criteria, criteria, charset)\n", "idx": 1656}
{"namespace": "imapclient.imapclient.IMAPClient.thread", "completion": "        args = [\n            to_bytes(algorithm),\n            _normalise_search_criteria(criteria, charset),\n            to_bytes(charset),\n        ]\n        ids = self._raw_command_untagged(b\"THREAD\", args, unpack=True)\n        return [parse_message_list(i) for i in ids.split()]\n", "idx": 1657}
{"namespace": "imapclient.imapclient.IMAPClient.get_flags", "completion": "        if not self.has_capability(\"X-GM-EXT-1\"):\n            raise exceptions.CapabilityError(\n                \"The server does not support the X-GM-EXT-1 extension\"\n            )\n\n        if not messages:\n            return {}\n\n        # The server will return the flags for each message in the order they were requested\n        # so we can use the message ids as keys to the flags\n        flags = {}\n        for msgid in messages:\n            flags[msgid] = ()\n\n        # The server will return the flags for each message in the order they were requested\n        # so we can use the message ids as keys to the flags\n        flags = {}\n        for msgid in messages:\n            flags[msgid] = ()\n\n        # The server will return the flags for each message in the order they were requested\n        # so we can use the message ids as keys to the flags\n        flags = {}\n        for msgid in messages:\n            flags[msgid] = ()\n\n        # The server will return the flags for each message in the order they were requested\n        # so we can use the message ids as keys to the flags\n        flags = {}\n        for msgid in messages:\n            flags[msgid] = ()\n\n        # The server will return the flags for each message in the order they were requested\n        # so we can use the message ids as keys to the flags\n        flags = {}\n        for msgid in messages:\n            flags[msgid] = ()\n\n        # The server will return the flags for each message in the order they were requested\n        # so we can use the message ids as keys to the flags\n        flags = {}\n        for msgid in messages:\n            flags[msgid] = ()\n\n        # The server will return the flags for each message in the order they were requested\n        # so we can use the message ids as keys to the flags\n        flags = {}\n        for msgid in messages:\n            flags[msgid] = ()\n\n        # The server will return the flags for each message in the order they were requested\n        # so we can use the message ids as keys to the flags\n        flags = {}\n        for msgid in messages:\n            flags[msgid] = ()\n\n        # The server will return the flags for each message in the order they were requested\n        # so we can use the message ids as keys to the flags\n        flags = {}\n        for msgid in messages:\n            flags[msgid] = ()\n\n        # The server will return the flags", "idx": 1658}
{"namespace": "imapclient.imapclient.IMAPClient.get_gmail_labels", "completion": "        response = self.fetch(messages, [\"X-GM-LABELS\"])\n        return self._filter_fetch_dict(response, b\"X-GM-LABELS\")\n", "idx": 1659}
{"namespace": "imapclient.imapclient.IMAPClient.append", "completion": "        args = [\n            \"APPEND\",\n            self._normalise_folder(folder),\n            msg,\n            seq_to_parenstr_upper(flags),\n            msg_time,\n        ]\n        return self._imap._command(*args)\n", "idx": 1660}
{"namespace": "imapclient.imapclient.IMAPClient.multiappend", "completion": "        if not msgs:\n            return\n\n        args = [\n            \"MULTIAPPEND\",\n            self._normalise_folder(folder),\n            seq_to_parenstr(msgs),\n        ]\n        return self._command_and_check(*args)\n", "idx": 1661}
{"namespace": "imapclient.imapclient.IMAPClient.expunge", "completion": "        if messages:\n            args = [\n                \"EXPUNGE\",\n                join_message_ids(messages),\n            ]\n        else:\n            args = [\"EXPUNGE\"]\n\n        if self.use_uid:\n            args.insert(0, \"UID\")\n\n        tag = self._imap._command(*args)\n        typ, data = self._imap._command_complete(\"EXPUNGE\", tag)\n        self._checkok(\"expunge\", typ, data)\n        typ, data = self._imap._untagged_response(typ, data, \"EXPUNGE\")\n        return parse_expunge_response(data, self.use_uid)\n", "idx": 1662}
{"namespace": "imapclient.imapclient.IMAPClient.getacl", "completion": "        return self._acl(folder, \"GETACL\")\n", "idx": 1663}
{"namespace": "imapclient.imapclient.IMAPClient.setacl", "completion": "        return self._command_and_check(\n            \"setacl\",\n            self._normalise_folder(folder),\n            to_bytes(who),\n            to_bytes(what),\n            uid=False,\n        )\n", "idx": 1664}
{"namespace": "imapclient.imapclient.IMAPClient.get_quota_root", "completion": "        mailbox = to_bytes(mailbox)\n        if not self.has_capability(b\"QUOTA=\" + mailbox):\n            raise exceptions.CapabilityError(\n                \"The server does not support the QUOTA capability for %s\" % mailbox\n            )\n\n        tag = self._imap._command(\"getquota\", mailbox)\n        typ, data = self._imap._command_complete(\"getquota\", tag)\n        self._checkok(\"getquota\", typ, data)\n        typ, data = self._imap._untagged_response(typ, data, \"getquota\")\n        return _parse_quota_root(data)\n", "idx": 1665}
{"namespace": "imapclient.imapclient.IMAPClient.set_quota", "completion": "        return self._command_and_check(\n            \"setquota\",\n            [to_bytes(q.resource) for q in quotas],\n            [to_bytes(q.quota) for q in quotas],\n            unpack=True,\n        )\n", "idx": 1666}
{"namespace": "imapclient.imapclient.IMAPClient._consume_until_tagged_response", "completion": "        untagged_responses = []\n        while True:\n            typ, data = self._imap._consume_response()\n            if typ == \"OK\":\n                if data[0] == tag:\n                    return data, untagged_responses\n            untagged_responses.append((typ, data))\n", "idx": 1667}
{"namespace": "imapclient.imapclient._normalise_search_criteria", "completion": "    if isinstance(criteria, (str, bytes)):\n        criteria = [criteria]\n    elif isinstance(criteria, (list, tuple)):\n        criteria = list(criteria)\n    else:\n        raise ValueError(\"search criteria must be specified as a string, bytes, list, or tuple\")\n\n    if charset is None:\n        charset = \"us-ascii\"\n\n    return [\n        _quote(encode_utf7(criterion, charset)) if isinstance(criterion, str) else criterion\n        for criterion in criteria\n    ]\n\n", "idx": 1668}
{"namespace": "imapclient.response_lexer.TokenSource.current_literal", "completion": "        if self.lex.current_source is not None:\n            return self.lex.current_source.literal\n        return None\n", "idx": 1669}
{"namespace": "imapclient.imap_utf7.decode", "completion": "    if not isinstance(s, (bytes, str)):\n        return s\n\n    res = bytearray()\n\n    b64_buffer: List[str] = []\n\n    def consume_b64_buffer(buf: List[str]) -> None:\n        \"\"\"\n        Consume the buffer by decoding it from a modified base 64 representation\n        and append it to the result\n        \"\"\"\n        if buf:\n            res.extend(base64_utf7_decode(buf))\n            del buf[:]\n\n    for c in s:\n        # printable ascii case should not be modified\n        o = ord(c)\n        if 0x20 <= o <= 0x7E:\n            consume_b64_buffer(b64_buffer)\n            # Special case: & is used as shift character so we need to escape it in ASCII\n            if o == AMPERSAND_ORD:  # & = 0x26\n                res.extend(b\"&\")\n            elif o == DASH_ORD:\n                res.extend(b\"-\")\n            else:\n                res.append(o)\n\n        # Bufferize characters that will be decoded from base64 and append them later\n        # in the result, when iterating over ASCII character or the end of string\n        else:\n            b64_buffer.append(c)\n\n    # Consume the remaining buffer if the string finish with non-ASCII characters\n    consume_b64_buffer(b64_buffer)\n\n    return str(res, \"utf-8\")\n\n", "idx": 1670}
{"namespace": "imapclient.fixed_offset.FixedOffset.for_system", "completion": "        if time.daylight:\n            offset = time.altzone\n        else:\n            offset = time.timezone\n\n        return cls(offset / 60)", "idx": 1671}
{"namespace": "imapclient.datetime_util.parse_to_datetime", "completion": "    # Parse the input string\n    if timestamp.startswith(b\"\\\"\"):\n        timestamp = timestamp[1:-1]\n    timestamp = timestamp.decode(\"ascii\")\n    timestamp = timestamp.replace(\",\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"(\", \"\")\n    timestamp = timestamp.replace(\")\", \"\")\n    timestamp = timestamp.replace(\" \", \"\")\n    timestamp = timestamp.replace(\":\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\", \"\")\n    timestamp = timestamp.replace(\"\\\"\",", "idx": 1672}
{"namespace": "imapclient.datetime_util.datetime_to_INTERNALDATE", "completion": "    if dt.tzinfo is None:\n        dt = dt.astimezone(FixedOffset.for_system())\n\n    return dt.strftime(\"-%Y %H:%M:%S %z\")\n\n", "idx": 1673}
{"namespace": "imapclient.datetime_util.format_criteria_date", "completion": "    if not dt.tzinfo:\n        dt = dt.replace(tzinfo=FixedOffset.for_system())\n    fmt = \"%d-\" + _SHORT_MONTHS[dt.month] + \"-%Y %H:%M:%S %z\"\n    return dt.strftime(fmt).encode(\"utf-8\")", "idx": 1674}
{"namespace": "imapclient.util.assert_imap_protocol", "completion": "    if not condition:\n        raise ProtocolError(message or b\"Server replied with a response that violates the IMAP protocol\")\n\n", "idx": 1675}
{"namespace": "ehforwarderbot.utils.get_config_path", "completion": "    profile = coordinator.profile\n    if module_id is None:\n        module_id = profile\n    config_path = get_base_path() / 'profiles' / profile / module_id / f'config.{ext}'\n    if not config_path.parent.exists():\n        config_path.parent.mkdir(parents=True)\n    return config_path\n\n", "idx": 1676}
{"namespace": "ehforwarderbot.utils.get_custom_modules_path", "completion": "    base_path = get_base_path()\n    custom_modules_path = base_path / \"modules\"\n    if not custom_modules_path.exists():\n        custom_modules_path.mkdir(parents=True)\n    return custom_modules_path\n\n", "idx": 1677}
{"namespace": "ehforwarderbot.chat.Chat.add_member", "completion": "        if middleware:\n            member = ChatMember(self, name=name, uid=uid, alias=alias, id=id,\n                                vendor_specific=vendor_specific, description=description,\n                                middleware=middleware)\n        else:\n            member = ChatMember(self, name=name, uid=uid, alias=alias, id=id,\n                                vendor_specific=vendor_specific, description=description)\n        self.members.append(member)\n        return member\n", "idx": 1678}
{"namespace": "ehforwarderbot.chat.Chat.add_system_member", "completion": "        if id:\n            warnings.warn(\"`id` argument is deprecated, use `uid` instead.\", DeprecationWarning)\n            uid = uid or id\n        member = SystemChatMember(self, name=name, alias=alias, id=id, uid=uid,\n                                  vendor_specific=vendor_specific, description=description,\n                                  middleware=middleware)\n        self.members.append(member)\n        return member\n", "idx": 1679}
{"namespace": "ehforwarderbot.chat.PrivateChat.verify", "completion": "        super().verify()\n        assert any(isinstance(i, ChatMember) for i in self.members)\n", "idx": 1680}
{"namespace": "telethon.utils.get_inner_text", "completion": "    result = []\n    for entity in entities:\n        if isinstance(entity, str):\n            entity = entity, entity\n        else:\n            entity = entity.entity, entity\n\n        if entity in text:\n            result.append(text[text.index(entity) + len(entity):])\n\n    return result\n\n", "idx": 1681}
{"namespace": "telethon.extensions.html.parse", "completion": "    parser = HTMLToTelegramParser()\n    parser.feed(html)\n    parser.close()\n\n    return parser.text, parser.entities", "idx": 1682}
{"namespace": "telethon.extensions.html.unparse", "completion": "    if not text:\n        return ''\n\n    if not entities:\n        return escape(text)\n\n    html = ''\n    last_offset = 0\n    for entity in entities:\n        if entity.offset > last_offset:\n            html += escape(text[last_offset:entity.offset])\n        formatter = ENTITY_TO_FORMATTER[type(entity)]\n        if callable(formatter):\n            formatter = formatter(entity, text[entity.offset:entity.offset + entity.length])\n        html += formatter[0]\n        html += escape(text[entity.offset:entity.offset + entity.length])\n        html += formatter[1]\n        last_offset = entity.offset + entity.length\n    if last_offset < len(text):\n        html += escape(text[last_offset:])\n    return html", "idx": 1683}
{"namespace": "telethon.crypto.rsa.encrypt", "completion": "    global _server_keys\n    if fingerprint not in _server_keys:\n        return None\n    key, old = _server_keys[fingerprint]\n    if use_old and old:\n        key = old\n    data = data + sha1(data).digest()\n    data = data + b'\\x00' * (128 - len(data) % 128)\n    return rsa.encrypt(data, key)\n\n", "idx": 1684}
{"namespace": "hbmqtt.codecs.encode_string", "completion": "    if string:\n        encoded_string = string.encode(encoding='utf-8')\n        return pack(\"!H\", len(encoded_string)) + encoded_string\n    else:\n        return pack(\"!H\", 0)\n\n", "idx": 1685}
{"namespace": "hbmqtt.plugins.manager.PluginManager.get_plugin", "completion": "        for plugin in self._plugins:\n            if plugin.name == name:\n                return plugin\n        return None\n", "idx": 1686}
{"namespace": "pysimplesoap.simplexml.SimpleXMLElement.add_child", "completion": "        if self.__ns is not None and ns:\n            if self.__prefix is not None:\n                name = \"%s:%s\" % (self.__prefix, name)\n            else:\n                name = \"%s:%s\" % (self.__ns, name)\n\n        element = self.__document.createElement(name)\n        if text is not None:\n            if isinstance(text, xml.dom.minidom.CDATASection):\n                element.appendChild(text)\n            else:\n                element.appendChild(self.__document.createTextNode(text))\n        for element_ in self.__elements:\n            element_.appendChild(element)\n        return SimpleXMLElement(elements=[element], document=self.__document, namespace=self.__ns, prefix=self.__prefix, namespaces_map=self.__namespaces_map)\n", "idx": 1687}
{"namespace": "pysimplesoap.simplexml.SimpleXMLElement.as_xml", "completion": "        if pretty:\n            return self.__document.toprettyxml(indent=\"  \")\n        else:\n            return self.__document.toxml()\n", "idx": 1688}
{"namespace": "twilio.base.deserialize.iso8601_date", "completion": "    try:\n        return datetime.datetime.strptime(s, ISO8601_DATETIME_FORMAT).date()\n    except ValueError:\n        return s\n\n", "idx": 1689}
{"namespace": "twilio.base.deserialize.iso8601_datetime", "completion": "    try:\n        return datetime.datetime.strptime(s, ISO8601_DATETIME_FORMAT).replace(\n            tzinfo=datetime.timezone.utc\n        )\n    except (TypeError, ValueError):\n        return s\n\n", "idx": 1690}
{"namespace": "twilio.base.serialize.iso8601_date", "completion": "    if isinstance(d, str):\n        return d\n    elif isinstance(d, datetime.datetime):\n        return d.strftime(\"%Y-%m-%d\")\n    elif isinstance(d, datetime.date):\n        return d.strftime(\"%Y-%m-%d\")\n    else:\n        return None\n\n", "idx": 1691}
{"namespace": "twilio.base.serialize.iso8601_datetime", "completion": "    if d == values.unset:\n        return d\n    elif isinstance(d, datetime.datetime):\n        return str(d)\n    elif isinstance(d, datetime.date):\n        return str(d)\n    elif isinstance(d, str):\n        return d\n\n", "idx": 1692}
{"namespace": "twilio.base.serialize.prefixed_collapsible_map", "completion": "    return {prefix + k: v for k, v in m.items()}\n\n", "idx": 1693}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.dial", "completion": "        return self.nest(\n            Dial(\n                number=number,\n                action=action,\n                method=method,\n                timeout=timeout,\n                hangup_on_star=hangup_on_star,\n                time_limit=time_limit,\n                caller_id=caller_id,\n                record=record,\n                trim=trim,\n                recording_status_callback=recording_status_callback,\n                recording_status_callback_method=recording_status_callback_method,\n                recording_status_callback_event=recording_status_callback_event,\n                answer_on_bridge=answer_on_bridge,\n                ring_tone=ring_tone,\n                recording_track=recording_track,\n                sequential=sequential,\n                refer_url=refer_url,\n                refer_method=refer_method,\n                **kwargs\n            )\n        )\n", "idx": 1694}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.enqueue", "completion": "        return self.nest(\n            Enqueue(\n                name=name,\n                action=action,\n                max_queue_size=max_queue_size,\n                method=method,\n                wait_url=wait_url,\n                wait_url_method=wait_url_method,\n                workflow_sid=workflow_sid,\n                **kwargs\n            )\n        )\n", "idx": 1695}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.gather", "completion": "        return self.nest(\n            Gather(\n                input=input,\n                action=action,\n                method=method,\n                timeout=timeout,\n                speech_timeout=speech_timeout,\n                max_speech_time=max_speech_time,\n                profanity_filter=profanity_filter,\n                finish_on_key=finish_on_key,\n                num_digits=num_digits,\n                partial_result_callback=partial_result_callback,\n                partial_result_callback_method=partial_result_callback_method,\n                language=language,\n                hints=hints,\n                barge_in=barge_in,\n                debug=debug,\n                action_on_empty_result=action_on_empty_result,\n                speech_model=speech_model,\n                enhanced=enhanced,\n                **kwargs\n            )\n        )\n", "idx": 1696}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.say", "completion": "        return self.nest(\n            Say(message=message, voice=voice, loop=loop, language=language, **kwargs)\n        )\n", "idx": 1697}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.sms", "completion": "        return self.nest(\n            Sms(\n                message,\n                to=to,\n                from_=from_,\n                action=action,\n                method=method,\n                status_callback=status_callback,\n                **kwargs\n            )\n        )\n", "idx": 1698}
{"namespace": "twilio.twiml.voice_response.Gather.say", "completion": "        return self.nest(\n            Say(\n                message=message,\n                voice=voice,\n                loop=loop,\n                language=language,\n                **kwargs\n            )\n        )\n", "idx": 1699}
{"namespace": "twilio.twiml.voice_response.Dial.client", "completion": "        return self.nest(\n            Client(\n                identity=identity,\n                url=url,\n                method=method,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                **kwargs\n            )\n        )\n", "idx": 1700}
{"namespace": "twilio.twiml.voice_response.Dial.conference", "completion": "        return self.nest(\n            Conference(\n                name=name,\n                muted=muted,\n                beep=beep,\n                start_conference_on_enter=start_conference_on_enter,\n                end_conference_on_exit=end_conference_on_exit,\n                wait_url=wait_url,\n                wait_method=wait_method,\n                max_participants=max_participants,\n                record=record,\n                region=region,\n                coach=coach,\n                trim=trim,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                recording_status_callback=recording_status_callback,\n                recording_status_callback_method=recording_status_callback_method,\n                recording_status_callback_event=recording_status_callback_event,\n                event_callback_url=event_callback_url,\n                jitter_buffer_size=jitter_buffer_size,\n                participant_label=participant_label,\n                **kwargs\n            )\n        )\n", "idx": 1701}
{"namespace": "twilio.twiml.voice_response.Dial.queue", "completion": "        return self.nest(\n            Queue(\n                name,\n                url=url,\n                method=method,\n                reservation_sid=reservation_sid,\n                post_work_activity_sid=post_work_activity_sid,\n                **kwargs\n            )\n        )\n", "idx": 1702}
{"namespace": "twilio.twiml.voice_response.Dial.sip", "completion": "        return self.nest(\n            Sip(\n                sip_url,\n                username=username,\n                password=,\n                url=url,\n                method=method,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                machine_detection=machine_detection,\n                amd_status_callback_method=amd_status_callback_method,\n                amd_status_callback=amd_status_callback,\n                machine_detection_timeout=machine_detection_timeout,\n                machine_detection_speech_threshold=machine_detection_speech_threshold,\n                machine_detection_speech_end_threshold=machine_detection_speech_end_threshold,\n                machine_detection_silence_timeout=machine_detection_silence_timeout,\n                **kwargs\n            )\n        )\n", "idx": 1703}
{"namespace": "twilio.twiml.messaging_response.MessagingResponse.message", "completion": "        return self.nest(\n            Message(\n                body=body,\n                to=to,\n                from_=from_,\n                action=action,\n                method=method,\n                status_callback=status_callback,\n                **kwargs\n            )\n        )\n", "idx": 1704}
{"namespace": "twilio.twiml.TwiML.append", "completion": "        if not isinstance(verb, TwiML):\n            raise TwiMLException(\"Invalid verb type.\")\n\n        self.verbs.append(verb)\n        return self\n", "idx": 1705}
{"namespace": "twilio.jwt.Jwt.to_jwt", "completion": "        if ttl is not None:\n            self.ttl = ttl\n\n        if not self.secret_key:\n            raise ValueError(\"No secret key configured for JWT\")\n\n        headers = self.headers.copy()\n        payload = self.payload.copy()\n\n        return jwt_lib.encode(payload, self.secret_key, algorithm=self.algorithm, headers=headers)\n", "idx": 1706}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_client_outgoing", "completion": "        scope_uri = \"scope:client:outgoing?appSid={}\".format(application_sid)\n        if kwargs:\n            scope_uri += \"&{}\".format(urlencode(kwargs))\n        self.capabilities[scope_uri] = True\n", "idx": 1707}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_client_incoming", "completion": "        self.client_name = client_name\n        scope = ScopeURI(\"client\", \"incoming\", {\"clientName\": client_name})\n        self.capabilities[\"incoming\"] = scope\n", "idx": 1708}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_event_stream", "completion": "        scope = ScopeURI(\"stream\", \"subscribe\", {})\n        if kwargs:\n            scope.add_param(\"streamParams\", urlencode(kwargs, doseq=True))\n\n        self.capabilities[\"stream\"] = scope\n", "idx": 1709}
{"namespace": "twilio.jwt.client.ClientCapabilityToken._generate_payload", "completion": "        if \"outgoing\" in self.capabilities and self.client_name is not None:\n            self.capabilities[\"outgoing\"].add_param(\n                \"clientName\", self.client_name\n            )\n\n        payload = {\n            \"scope\": \" \".join(\n                [\n                    scope_uri.to_scope_uri()\n                    for scope_uri in self.capabilities.values()\n                ]\n            )\n        }\n\n        return payload\n\n", "idx": 1710}
{"namespace": "twilio.jwt.client.ScopeURI.to_payload", "completion": "        if self.params:\n            params = sorted(self.params.items())\n            param_string = \"?\" + urlencode(params, doseq=True)\n        else:\n            param_string = \"\"\n\n        return \"scope:{service}:{privilege}{param_string}\".format(\n            service=self.service,\n            privilege=self.privilege,\n            param_string=param_string,\n        )", "idx": 1711}
{"namespace": "twilio.jwt.access_token.AccessToken.add_grant", "completion": "        if not isinstance(grant, AccessTokenGrant):\n            raise ValueError(\"Grant must be an instance of AccessTokenGrant.\")\n        self.grants.append(grant)\n", "idx": 1712}
{"namespace": "twilio.jwt.taskrouter.capabilities.WorkerCapabilityToken.allow_update_activities", "completion": "        self._make_policy(\n            self.resource_url, \"POST\", True, {\"ActivitySid\": {\"required\": True}}\n        )\n", "idx": 1713}
{"namespace": "zulipterminal.platform_code.successful_GUI_return_code", "completion": "    if PLATFORM == \"WSL\":\n        return 1\n    else:\n        return 0", "idx": 1714}
{"namespace": "zulipterminal.platform_code.normalized_file_path", "completion": "    if PLATFORM == \"WSL\":\n        return path.replace(\"/\", \"\\\\\")\n    return path", "idx": 1715}
{"namespace": "zulipterminal.helper.canonicalize_color", "completion": "    # if the color is in the format '#xxxxxx'\n    if match(color, r\"^#[0-9a-f]{6}$\"):\n        return color.lower()\n\n    # if the color is in the format '#xxx'\n    elif match(color, r\"^#[0-9a-f]{3}$\"):\n        return color.lower()\n\n    # if the color is in the format '#xx'\n    elif match(color, r\"^#[0-9a-f]{2}$\"):\n        return \"#0\" + color[1:].lower()\n\n    # if the color is in the format '#x'\n    elif match(color, r\"^#[0-9a-f]{1}$\"):\n        return \"#00\" + color[1:].lower()\n\n    # if the color is in the format '#'\n    elif match(color, r\"^#$\"):\n        return \"#000\"\n\n    # if the color is in the format 'xxxxxx'\n    elif match(color, r\"^[0-9a-f]{6}$\"):\n        return \"#\" + color.lower()\n\n    # if the color is in the format 'xxx'\n    elif match(color, r\"^[0-9a-f]{3}$\"):\n        return \"#\" + color.lower()\n\n    # if the color is in the format 'xx'\n    elif match(color, r\"^[0-9a-f]{2}$\"):\n        return \"#0\" + color.lower()\n\n    # if the color is in the format 'x'\n    elif match(color, r\"^[0-9a-f]{1}$\"):\n        return \"#00\" + color.lower()\n\n    # if the color is in the format ''\n    elif match(color, r\"^$\"):\n        return \"#000\"\n\n    # if the color is in the format 'x x'\n    elif match(color, r\"^[0-9a-f]{1} [0-9a-f]{1}$\"):\n        return \"#00\" + color.lower()\n\n    # if the color is in the format 'x x x'\n    elif match(color, r\"^[0-9a-f]{1} [0-9a-f]{1} [0-9a-f]{1}$\"):\n        return \"#0\" + color", "idx": 1716}
{"namespace": "zulipterminal.helper.get_unused_fence", "completion": "    from zulipterminal.config.regexes import REGEX_CONTINUOUS_BACK_TICK\n    from zulipterminal.config.regexes import REGEX_CONTINUOUS_BACK_TICK_END\n    from zulipterminal.config.regexes import REGEX_CONTINUOUS_BACK_TICK_START\n    from zulipterminal.config.regexes import REGEX_CONTINUOUS_BACK_TICK_START_END\n    from zulipterminal.config.regexes import REGEX_CONTINUOUS_BACK_TICK_START_END_END\n    from zulipterminal.config.regexes import REGEX_CONTINUOUS_BACK_TICK_START_END_END_END\n    from zulipterminal.config.regexes import REGEX_CONTINUOUS_BACK_TICK_START_END_END_END_END\n    from zulipterminal.config.regexes import REGEX_CONTINUOUS_BACK_TICK_START_END_END_END_END_END\n    from zulipterminal.config.regexes import REGEX_CONTINUOUS_BACK_TICK_START_END_END_END_END_END_END\n    from zulipterminal.config.regexes import REGEX_CONTINUOUS_BACK_TICK_START_END_END_END_END_END_END_END\n    from zulipterminal.config.regexes import REGEX_CONTINUOUS_BACK_TICK_START_END_END_END_END_END_END_END_END\n    from zulipterminal.config.regexes import REGEX_CONTINUOUS_BACK_TICK_START_END_END_END_END_END_END_END_END_END\n    from zulipterminal.config.regexes import REGEX_CONTINUOUS_BACK_TICK_START_END_END_END_END_END_END_END_END_END_END\n    from zulipterminal.config.regexes import REGEX_CONTINUOUS_BACK_TICK_START_END_END_END_END_END_END_END_END_END_END_END\n    from zulipterm", "idx": 1717}
{"namespace": "zulipterminal.helper.open_media", "completion": "    command = f\"{tool} {media_path}\"\n    try:\n        subprocess.run(command, shell=True, check=True)\n    except subprocess.CalledProcessError as error:\n        controller.report_error(\n            [\n                \"Failed to open media file. \",\n                (\"bold\", \"Error: \"),\n                error.stderr.decode(),\n            ]\n        )\n\n", "idx": 1718}
{"namespace": "zulipterminal.server_url.encode_stream", "completion": "    return hash_util_encode(stream_name) + \"-\" + hash_util_encode(str(stream_id))\n\n", "idx": 1719}
{"namespace": "zulipterminal.server_url.near_message_url", "completion": "    if message[\"type\"] == \"stream\":\n        return near_stream_message_url(server_url, message)\n    else:\n        return near_pm_message_url(server_url, message)", "idx": 1720}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.update_recipients", "completion": "        # Extract the recipient emails from the input text\n        recipient_emails = self.extract_recipient_emails(write_box.edit_text)\n\n        # Set the corresponding user IDs in the WriteBox instance\n        self.recipient_user_ids = [\n            self.model.user_email_id_dict[email] for email in recipient_emails\n        ]\n", "idx": 1721}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.stream_box_view", "completion": "        self.stream_write_box = ReadlineEdit(caption, edit_text=title)\n        self.stream_write_box.enable_autocomplete(\n            func=self._stream_box_autocomplete,\n            key=primary_key_for_command(\"AUTOCOMPLETE\"),\n            key_reverse=primary_key_for_command(\"AUTOCOMPLETE_REVERSE\"),\n        )\n        self.stream_write_box.set_completer_delims(\"\")\n\n        def set_stream_marker(edit: object, new_edit_text: str) -> None:\n            if new_edit_text:\n                self.header_write_box.contents[0] = (\n                    urwid.Text(\n                        (\n                            \"default\",\n                            self.model.stream_dict[stream_id][\"stream_marker\"],\n                        )\n                    ),\n                    self.header_write_box.options(),\n                )\n            else:\n                self.header_write_box.contents[0] = (\n                    urwid.Text((\"default\", \"?\")),\n                    self.header_write_box.options(),\n                )\n\n        urwid.connect_signal(self.stream_write_box, \"change\", set_stream_marker)\n\n        self._setup_common_stream_compose(stream_id, caption, title)\n", "idx": 1722}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.stream_box_edit_view", "completion": "        self.stream_write_box = ReadlineEdit(\n            edit_text=caption, max_char=self.model.max_stream_name_length\n        )\n        self.stream_write_box.enable_autocomplete(\n            func=self._stream_box_autocomplete,\n            key=primary_key_for_command(\"AUTOCOMPLETE\"),\n            key_reverse=primary_key_for_command(\"AUTOCOMPLETE_REVERSE\"),\n        )\n        self.stream_write_box.set_completer_delims(\"\")\n        self._setup_common_stream_compose(stream_id, caption, title)\n\n        # Use and set a callback to set the stream marker\n        self._set_stream_write_box_style(None, caption)\n        urwid.connect_signal(\n            self.stream_write_box, \"change\", self._set_stream_write_box_style\n        )\n\n        # Add an edit mode button to the header write box\n        self.edit_mode_button = EditModeButton(\n            self.view.controller.set_edit_mode,\n            self.view.controller.unset_edit_mode,\n        )\n        self.header_write_box.contents.append(self.edit_mode_button)\n", "idx": 1723}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._set_stream_write_box_style", "completion": "        stream_name = new_text.strip()\n        if stream_name:\n            stream_info = self.model.get_stream_info(stream_name)\n            if stream_info:\n                stream_id = stream_info[\"stream_id\"]\n                stream_color = self.model.get_stream_color(stream_id)\n                stream_marker = self.model.get_stream_marker(stream_id)\n                self.stream_write_box.set_attr_map({None: stream_color})\n                self.header_write_box.widget_list[\n                    self.FOCUS_HEADER_PREFIX_STREAM\n                ] = urwid.Text(stream_marker)\n", "idx": 1724}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._to_box_autocomplete", "completion": "        # Get the list of users from the view.\n        users = self.model.user_dict.values()\n\n        # Split the text by comma and get the most recent recipient for autocomplete.\n        recipients = [\n            recipient.strip()\n            for recipient in text.split(\",\")\n            if recipient.strip()  # This condition avoids whitespace recipients (\",  ,\")\n        ]\n\n        # Find the users that match the latest text.\n        matching_users = [\n            user\n            for user in users\n            if user[\"full_name\"].lower().startswith(recipients[-1].lower())\n        ]\n\n        # Append the autocompleted recipients to the string containing the previous recipients.\n        autocompleted_recipients = list()\n        for recipient in recipients:\n            if recipient.strip():\n                autocompleted_recipients.append(recipient)\n            else:\n                autocompleted_recipients.append(\n                    f\"{matching_users[0]['full_name']} <{matching_users[0]['email']}>\"\n                )\n\n        # Get the full names of the matching users.\n        matching_user_full_names = [\n            user[\"full_name\"] for user in matching_users\n        ]\n\n        # Process the typeaheads using the updated recipients, state, and user names.\n        self.view.controller.process_typeaheads(\n            autocompleted_recipients, state, matching_user_full_names\n        )\n\n        # Return the string for autocomplete.\n        return \", \".join(autocompleted_recipients)\n", "idx": 1725}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._topic_box_autocomplete", "completion": "        topics_list = self.model.get_topics_in_stream(self.stream_id)\n        matching_topics = [\n            topic for topic in topics_list if match_topic(topic, text)\n        ]\n        return self._process_typeaheads(matching_topics, state)\n", "idx": 1726}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._stream_box_autocomplete", "completion": "        stream_names = self.view.streams\n        stream_typeaheads = match_streams(stream_names, text)\n\n        # Typeaheads and suggestions are the same.\n        return self._process_typeaheads(stream_typeaheads, state, stream_typeaheads)\n", "idx": 1727}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.generic_autocomplete", "completion": "        # FIXME: Refactor when we have ~ Model.is_private_stream\n        from zulipterminal.config.ui_mappings import STREAM_ACCESS_TYPE\n        if self.compose_box_status == \"open_with_private\":\n            return self._to_box_autocomplete(text, state)\n        elif self.compose_box_status == \"open_with_stream\":\n            if self.stream_id:\n                stream_access_type = self.model.stream_access_type(self.stream_id)\n                if stream_access_type == STREAM_ACCESS_TYPE[\"PRIVATE\"]:\n                    return self._topic_box_autocomplete(text, state)\n                else:\n                    return self._stream_box_autocomplete(text, state)\n            else:\n                return self._stream_box_autocomplete(text, state)\n        else:\n            return None\n", "idx": 1728}
{"namespace": "zulipterminal.ui_tools.boxes.PanelSearchBox.reset_search_text", "completion": "        self.set_caption(self.search_text)\n        self.set_edit_text(\"\")\n", "idx": 1729}
{"namespace": "zulipterminal.ui_tools.boxes.PanelSearchBox.valid_char", "completion": "        if self.edit_text:\n            return super().valid_char(ch)\n        return is_valid_unicode_char(ch) and not is_control_char(ch) and ch != \" \"\n", "idx": 1730}
{"namespace": "zulipterminal.ui_tools.utils.is_muted", "completion": "    if msg[\"type\"] == \"stream\":\n        return model.muted_streams.get(msg[\"stream_id\"], False)\n    elif msg[\"type\"] == \"topic\":\n        return model.muted_topics.get(msg[\"stream_id\"], {}).get(msg[\"topic\"], False)\n    return False\n\n\n", "idx": 1731}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.update_count", "completion": "        if text_color is None:\n            text_color = self.original_color\n\n        if count > 0:\n            self.count = count\n            self.count_style = text_color\n            self.button_suffix.set_text(f\" ({count})\")\n        else:\n            self.count = 0\n            self.button_suffix.set_text(\"\")\n", "idx": 1732}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.update_widget", "completion": "        if isinstance(self.prefix_character, tuple):\n            prefix_text, prefix_color = self.prefix_character\n        else:\n            prefix_text = self.prefix_character\n            prefix_color = text_color\n\n        self.button_prefix.set_text((prefix_color, prefix_text))\n        self._label.set_text((text_color, self._caption))\n        self.button_suffix.set_text((text_color, count_text))\n        self._w.set_attr_map({None: text_color})\n", "idx": 1733}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.keypress", "completion": "        if key == \"enter\":\n            self.activate(key)\n            return None\n        return super().keypress(size, key)\n\n", "idx": 1734}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._parse_narrow_link", "completion": "        from zulipterminal.config.regexes import (\n            REGEX_INTERNAL_LINK_STREAM_ID,\n            REGEX_INTERNAL_LINK_STREAM_NAME,\n            REGEX_INTERNAL_LINK_TOPIC_NAME,\n            REGEX_INTERNAL_LINK_MESSAGE_ID,\n        )\n        from zulipterminal.config.regexes import REGEX_INTERNAL_LINK_STREAM_ID_TOPIC_NAME\n        from zulipterminal.config.regexes import REGEX_INTERNAL_LINK_STREAM_ID_TOPIC_NAME_MESSAGE_ID\n        from zulipterminal.config.regexes import REGEX_INTERNAL_LINK_STREAM_NAME_TOPIC_NAME\n        from zulipterminal.config.regexes import REGEX_INTERNAL_LINK_STREAM_NAME_TOPIC_NAME_MESSAGE_ID\n        from zulipterminal.config.regexes import REGEX_INTERNAL_LINK_STREAM_NAME_MESSAGE_ID\n        from zulipterminal.config.regexes import REGEX_INTERNAL_LINK_TOPIC_NAME_MESSAGE_ID\n        from zulipterminal.config.regexes import REGEX_INTERNAL_LINK_MESSAGE_ID\n        from zulipterminal.config.regexes import REGEX_INTERNAL_LINK_TOPIC_NAME\n        from zulipterminal.config.regexes import REGEX_INTERNAL_LINK_STREAM_ID\n        from zulipterminal.config.regexes import REGEX_INTERNAL_LINK_STREAM_NAME\n        from zulipterminal.config.regexes import REGEX_INTERNAL_LINK_STREAM_ID_TOPIC_NAME\n        from zulipterminal.config.regexes import REGEX_INTERNAL_LINK_STREAM_NAME_TOPIC_NAME\n        from zulipterminal.config.regexes import REGEX_INTERNAL_LINK_STREAM_ID_TOPIC_NAME_MESSAGE_ID\n        from zulipterminal.config.regexes import REGEX_INTERNAL_LINK_STREAM_NAME_TOPIC_NAME_MESSAGE_ID\n        from zulipterminal.config.regexes import REGEX_INTERNAL_LINK_STREAM_ID_MESSAGE_ID\n        from zulipterminal.config.regexes import REGEX_INTERNAL_LINK_STREAM_NAME_MESSAGE_ID\n        from zul", "idx": 1735}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._validate_and_patch_stream_data", "completion": "        stream_id = parsed_link[\"stream\"][\"stream_id\"]\n        stream_name = parsed_link[\"stream\"][\"stream_name\"]\n        if stream_id is not None:\n            if not self.model.is_subscribed_stream(stream_id):\n                return f\"You are not subscribed to the stream with ID {stream_id}.\"\n            parsed_link[\"stream\"][\"stream_name\"] = self.model.stream_dict[stream_id][\n                \"name\"\n            ]\n        elif stream_name is not None:\n            if not self.model.is_subscribed_stream_by_name(stream_name):\n                return f\"You are not subscribed to the stream with name {stream_name}.\"\n            parsed_link[\"stream\"][\"stream_id\"] = self.model.stream_dict[stream_name][\n                \"id\"\n            ]\n        return \"\"\n", "idx": 1736}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._validate_narrow_link", "completion": "        # Validate narrow.\n        if parsed_link[\"narrow\"] not in {\"stream\", \"stream:topic\", \"stream:near\"}:\n            return \"The narrow seems to be either unknown or invalid\"\n\n        # Validate stream data.\n        stream_data = parsed_link[\"stream\"]\n        stream_id = stream_data[\"stream_id\"]\n        stream_name = stream_data[\"stream_name\"]\n        if (stream_id is None and stream_name is None) or (\n            stream_id is not None and stream_name is not None\n        ):\n            return \"The stream seems to be either unknown or invalid\"\n\n        # Validate topic name.\n        topic_name = parsed_link[\"topic_name\"]\n        if topic_name and not self.model.is_valid_topic(topic_name):\n            return \"The topic seems to be either unknown or invalid\"\n\n        # Validate message ID.\n        message_id = parsed_link[\"message_id\"]\n        if message_id and not self.model.is_valid_message_id(message_id):\n            return \"The message seems to be either unknown or invalid\"\n\n        return \"\"\n", "idx": 1737}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._switch_narrow_to", "completion": "        # Switch to the concerned narrow view.\n        if parsed_link[\"narrow\"] == \"stream\":\n            self.controller.narrow_to_stream(\n                stream_name=parsed_link[\"stream\"][\"stream_name\"],\n            )\n        elif parsed_link[\"narrow\"] == \"stream:topic\":\n            self.controller.narrow_to_topic(\n                stream_name=parsed_link[\"stream\"][\"stream_name\"],\n                topic_name=parsed_link[\"topic_name\"],\n            )\n        elif parsed_link[\"narrow\"] == \"stream:near\":\n            self.controller.narrow_to_near(\n                stream_name=parsed_link[\"stream\"][\"stream_name\"],\n                message_id=parsed_link[\"message_id\"],\n            )\n        elif parsed_link[\"narrow\"] == \"stream:topic:near\":\n            self.controller.narrow_to_near(\n                stream_name=parsed_link[\"stream\"][\"stream_name\"],\n                topic_name=parsed_link[\"topic_name\"],\n                message_id=parsed_link[\"message_id\"],\n            )\n", "idx": 1738}
{"namespace": "zulipterminal.config.themes.complete_and_incomplete_themes", "completion": "    complete_themes = []\n    incomplete_themes = []\n\n    for theme_name, theme in THEMES.items():\n        if theme_name in THEME_ALIASES:\n            continue\n\n        if not isinstance(theme, dict):\n            incomplete_themes.append(theme_name)\n            continue\n\n        if \"styles\" not in theme:\n            incomplete_themes.append(theme_name)\n            continue\n\n        if \"meta\" not in theme:\n            incomplete_themes.append(theme_name)\n            continue\n\n        if not isinstance(theme[\"styles\"], dict):\n            incomplete_themes.append(theme_name)\n            continue\n\n        if not isinstance(theme[\"meta\"], dict):\n            incomplete_themes.append(theme_name)\n            continue\n\n        if \"pygments\" not in theme[\"meta\"]:\n            incomplete_themes.append(theme_name)\n            continue\n\n        if not isinstance(theme[\"meta\"][\"pygments\"], dict):\n            incomplete_themes.append(theme_name)\n            continue\n\n        if \"styles\" not in theme[\"meta\"][\"pygments\"]:\n            incomplete_themes.append(theme_name)\n            continue\n\n        if not isinstance(theme[\"meta\"][\"pygments\"][\"styles\"], dict):\n            incomplete_themes.append(theme_name)\n            continue\n\n        if \"background\" not in theme[\"meta\"][\"pygments\"]:\n            incomplete_themes.append(theme_name)\n            continue\n\n        if not isinstance(theme[\"meta\"][\"pygments\"][\"background\"], str):\n            incomplete_themes.append(theme_name)\n            continue\n\n        if \"overrides\" not in theme[\"meta\"][\"pygments\"]:\n            incomplete_themes.append(theme_name)\n            continue\n\n        if not isinstance(theme[\"meta\"][\"pygments\"][\"overrides\"], dict):\n            incomplete_themes.append(theme_name)\n            continue\n\n        if len(theme[\"styles\"]) != len(REQUIRED_STYLES):\n            incomplete_themes.append(theme_name)\n            continue\n\n        if len(theme[\"meta\"][\"pygments\"][\"styles\"]) != len(STANDARD_TYPES):\n            incomplete_themes.append(theme_name)\n            continue\n\n        if len(theme[\"meta\"][\"pygments\"][\"overrides\"]) != len(REQUIRED_META[\"pygments\"][\"overrides\"]):\n            incomplete_themes.append(theme_name)\n            continue\n\n        complete_themes.append(theme_name)\n\n    complete_themes.sort()\n    incomplete_themes.sort()", "idx": 1739}
{"namespace": "zulipterminal.config.themes.validate_colors", "completion": "    if color_depth != 16:\n        raise InvalidThemeColorCode(\n            f\"Invalid color depth {color_depth} for theme {theme_name}. Only 16-color themes are supported.\"\n        )\n\n    theme_styles = THEMES[theme_name].STYLES\n    for style in theme_styles:\n        for color in style[1:]:\n            if color not in valid_16_color_codes:\n                raise InvalidThemeColorCode(\n                    f\"Invalid color code {color} for theme {theme_name}.\"\n                )\n\n", "idx": 1740}
{"namespace": "zulipterminal.config.themes.parse_themefile", "completion": "    urwid_theme = []\n    for style_name, (fg, bg) in theme_styles.items():\n        if color_depth == 1:\n            fg = fg.monochrome\n            bg = bg.monochrome\n        elif color_depth == 16:\n            fg = fg.value.split()[0]\n            bg = bg.value.split()[0]\n        elif color_depth == 256:\n            fg = fg.value.split()[1]\n            bg = bg.value.split()[1]\n        elif color_depth == 2 ** 24:\n            fg = fg.value.split()[2]\n            bg = bg.value.split()[2]\n        else:\n            raise ValueError(f\"Invalid color depth: {color_depth}\")\n\n        urwid_theme.append((style_name, fg, bg))\n\n    return urwid_theme\n\n", "idx": 1741}
{"namespace": "zulipterminal.config.themes.add_pygments_style", "completion": "    pygments_styles = theme_meta[\"pygments\"][\"styles\"]\n    pygments_background = theme_meta[\"pygments\"][\"background\"]\n    pygments_overrides = theme_meta[\"pygments\"][\"overrides\"]\n\n    # Add Pygments styles to Urwid theme\n    for style_name, style_value in pygments_styles.items():\n        fg_code16, fg_code256, fg_code24, *fg_props = style_value.value.split()\n        new_style = (style_name, fg_code16, fg_code256, fg_code24, *fg_props)\n        urwid_theme.append(new_style)\n\n    # Add Pygments background color to Urwid theme\n    urwid_theme.append((\"pygments_background\", \"\", \"\", \"\", pygments_background))\n\n    # Add Pygments overrides to Urwid theme\n    for style_name, style_value in pygments_overrides.items():\n        fg_code16, fg_code256, fg_code24, *fg_props = style_value.value.split()\n        new_style = (style_name, fg_code16, fg_code256, fg_code24, *fg_props)\n        urwid_theme.append(new_style)\n\n", "idx": 1742}
{"namespace": "zulipterminal.config.keys.is_command_key", "completion": "    if command not in KEY_BINDINGS:\n        raise InvalidCommand(f\"Invalid command: {command}\")\n\n    return key in KEY_BINDINGS[command][\"keys\"]\n\n", "idx": 1743}
{"namespace": "zulipterminal.config.keys.keys_for_command", "completion": "    try:\n        return KEY_BINDINGS[command][\"keys\"]\n    except KeyError as exception:\n        raise InvalidCommand(command)\n\n", "idx": 1744}
{"namespace": "zulipterminal.config.keys.commands_for_random_tips", "completion": "    return [\n        command\n        for command in KEY_BINDINGS.values()\n        if not command.get(\"excluded_from_random_tips\")\n    ]\n\n", "idx": 1745}
{"namespace": "hypertools.datageometry.DataGeometry.transform", "completion": "        # if no data is passed, return the transformed data\n        if data is None:\n            return self.xform_data\n\n        # if data is a list, transform each element\n        if isinstance(data, list):\n            return [self.transform(d) for d in data]\n\n        # if data is a dataframe, convert to numpy array\n        if isinstance(data, pd.DataFrame):\n            data = data.values\n\n        # if data is a numpy array, transform it\n        if isinstance(data, np.ndarray):\n            return self.reduce['model'].transform(data)\n\n        # if data is a string, convert to numpy array\n        if isinstance(data, str):\n            data = np.array(data)\n\n        # if data is a numpy array, transform it\n        if isinstance(data, np.ndarray):\n            return self.reduce['model'].transform(data)\n\n        # if data is a list of strings, convert to numpy array\n        if isinstance(data, list) and isinstance(data[0], str):\n            data = np.array(data)\n\n        # if data is a numpy array, transform it\n        if isinstance(data, np.ndarray):\n            return self.reduce['model'].transform(data)\n\n        # if data is a list of numpy arrays, transform each element\n        if isinstance(data, list) and isinstance(data[0], np.ndarray):\n            return [self.transform(d) for d in data]\n\n        # if data is a list of strings, convert to numpy array\n        if isinstance(data, list) and isinstance(data[0], str):\n            data = np.array(data)\n\n        # if data is a numpy array, transform it\n        if isinstance(data, np.ndarray):\n            return self.reduce['model'].transform(data)\n\n        # if data is a list of numpy arrays, transform each element\n        if isinstance(data, list) and isinstance(data[0], np.ndarray):\n            return [self.transform(d) for d in data]\n\n        # if data is a list of strings, convert to numpy array\n        if isinstance(data, list) and isinstance(data[0], str):\n            data = np.array(data)\n\n        # if data is a numpy array, transform it\n        if isinstance(data, np.ndarray):\n            return self.reduce['model'].transform(data)\n\n        # if data is a", "idx": 1746}
{"namespace": "hypertools.datageometry.DataGeometry.plot", "completion": "        # if no new data passed,\n        from .tools.align import align as aligner\n        from .tools.normalize import normalize as normalizer\n        from .tools.reduce import reduce as reducer\n        if data is None:\n            data = self.xform_data\n        else:\n            formatted = format_data(\n                data,\n                semantic=self.semantic,\n                vectorizer=self.vectorizer,\n                corpus=self.corpus,\n                ppca=True)\n            norm = normalizer(formatted, normalize=self.normalize)\n            reduction = reducer(\n                norm,\n                reduce=self.reduce,\n                ndims=self.reduce['params']['n_components'])\n            data = aligner(reduction, align=self.align)\n\n        # plot the data\n        from .plot import plot as plotter\n        return plotter(data, **kwargs)\n", "idx": 1747}
{"namespace": "awesome_autodl.autodl_topic2papers", "completion": "    topic2path = autodl_topic2path()\n    topic2papers = OrderedDict()\n    for topic, path in topic2path.items():\n        topic2papers[topic] = AutoDLpaper.load_from_yaml(path)\n    return topic2papers\n\n", "idx": 1748}
{"namespace": "awesome_autodl.get_bib_abbrv_obj", "completion": "    from awesome_autodl.utils import load_yaml\n    from awesome_autodl.data_cls import BibAbbreviations\n\n    abbrv_file = get_bib_abbrv_file()\n    if not abbrv_file.exists():\n        ValueError(f\"Can not find {abbrv_file}.\")\n    abbrv_obj = BibAbbreviations(load_yaml(abbrv_file))\n    return abbrv_obj\n\n", "idx": 1749}
{"namespace": "mssqlcli.localized_strings.translation", "completion": "    t = gettext.translation(domain, localedir, languages=languages or LANGUAGES)\n    return t.gettext\n", "idx": 1750}
{"namespace": "mssqlcli.mssqlbuffer._is_query_executable", "completion": "    # Remove comments\n    sql = re.sub(r\"--.*|/\\*.*\\*/\", \"\", sql, flags=re.S)\n    # Check for open comments\n    if re.search(r\"/\\*\", sql):\n        return False\n    # Check for open quotes\n    if re.search(r\"(?<!')'(?!')\", sql):\n        return False\n    # Check for GO\n    if re.search(r\"(?i)GO\\s*(\\-\\-.*)?$\", sql):\n        return True\n    return False\n\n", "idx": 1751}
{"namespace": "mssqlcli.telemetry.conclude", "completion": "    _session.end_time = datetime.now()\n    payload = _session.generate_payload()\n    _write_payload_to_file(payload)\n    return _upload_payload(payload, service_endpoint_uri, separate_process)\n\n", "idx": 1752}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.start", "completion": "        self.request_thread = threading.Thread(target=self.listen_for_requests, name=self.REQUEST_THREAD_NAME)\n        self.request_thread.daemon = True\n        self.request_thread.start()\n\n        self.response_thread = threading.Thread(target=self.listen_for_responses, name=self.RESPONSE_THREAD_NAME)\n        self.response_thread.daemon = True\n        self.response_thread.start()\n", "idx": 1753}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.submit_request", "completion": "        if method is None or params is None:\n            raise ValueError(u'Method and params cannot be None.')\n\n        request = {\n            u'method': method,\n            u'params': params,\n            u'id': request_id\n        }\n\n        self.request_queue.put(request)\n", "idx": 1754}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.get_response", "completion": "        if request_id in self.response_map:\n            response_queue = self.response_map[request_id]\n            if not response_queue.empty():\n                return response_queue.get()\n\n        if not self.exception_queue.empty():\n            return self.exception_queue.get()\n\n        return None\n", "idx": 1755}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.shutdown", "completion": "        self.cancel = True\n        self.request_queue.put(None)\n        self.request_thread.join()\n        self.writer.close()\n        logger.debug(u'Json Rpc client shutdown.')\n\n", "idx": 1756}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcWriter.send_request", "completion": "        content = {u'method': method, u'params': params}\n        if request_id:\n            content[u'id'] = request_id\n\n        content_json = json.dumps(content)\n        content_length = len(content_json)\n        header = self.HEADER.format(content_length)\n        header_bytes = header.encode(self.encoding)\n        content_bytes = content_json.encode(self.encoding)\n\n        try:\n            self.stream.write(header_bytes)\n            self.stream.write(content_bytes)\n            self.stream.flush()\n        except ValueError:\n            raise ValueError(u'Stream was closed.')\n", "idx": 1757}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.read_response", "completion": "        while self.needs_more_data:\n            try:\n                if self.read_state == ReadState.Header:\n                    self._read_header()\n                elif self.read_state == ReadState.Content:\n                    self._read_content()\n            except ValueError as ex:\n                logger.debug(u'Read Response encountered exception %s', ex)\n                raise\n\n        self._trim_buffer()\n        response = json.loads(self.buffer.decode(self.encoding))\n        self.needs_more_data = True\n        self.read_state = ReadState.Header\n        self.headers = {}\n        self.expected_content_length = 0\n        self.buffer_end_offset = 0\n        self.read_offset = 0\n        return response\n", "idx": 1758}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.read_next_chunk", "completion": "        # Check if the buffer needs to be resized and resize it if necessary.\n        if self.buffer_end_offset >= len(self.buffer) * self.BUFFER_RESIZE_TRIGGER:\n            self.resize_buffer()\n\n        # Read data from the stream into the buffer and update the buffer offset.\n        try:\n            bytes_read = self.stream.readinto(self.buffer[self.buffer_end_offset:])\n            self.buffer_end_offset += bytes_read\n            return bytes_read > 0\n        except ValueError as ex:\n            # Stream was closed.\n            logger.debug(u'JSON RPC Reader on read_next_chunk() encountered exception: %s', ex)\n            raise\n", "idx": 1759}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.try_read_headers", "completion": "        # Find the end of the header.\n        end_of_header_index = self.buffer.find(b'\\r\\n\\r\\n', self.read_offset)\n        if end_of_header_index == -1:\n            return False\n\n        # Split the header into lines.\n        header_lines = self.buffer[self.read_offset:end_of_header_index].split(b'\\r\\n')\n        # Parse the header lines into key-value pairs.\n        for header_line in header_lines:\n            key_value_pair = header_line.split(b':')\n            if len(key_value_pair) == 2:\n                self.headers[key_value_pair[0].decode(u'ascii')] = key_value_pair[1].decode(u'ascii')\n\n        # Check if the content-length header is present.\n        if u'Content-Length' in self.headers:\n            self.expected_content_length = int(self.headers[u'Content-Length'])\n\n        # Update the read offset to the end of the header.\n        self.read_offset = end_of_header_index + 4\n        # Update the read state to content.\n        self.read_state = ReadState.Content\n        return True\n", "idx": 1760}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.close", "completion": "        try:\n            self.stream.close()\n        except AttributeError:\n            pass", "idx": 1761}
{"namespace": "mssqlcli.packages.prioritization.PrevalenceCounter.update", "completion": "        # Tokenize the input text\n        tokens = sqlparse.parse(text)[0].tokens\n\n        # Iterate over the tokens and update the keyword and name counts\n        for token in tokens:\n            if token.ttype is Name:\n                self.name_counts[token.value] += 1\n            elif token.value.upper() in keywords:\n                self.keyword_counts[token.value.upper()] += 1\n\n", "idx": 1762}
{"namespace": "mssqlcli.packages.sqlcompletion.suggest_type", "completion": "    if full_text.startswith('\\\\i '):\n        return Path(), []\n\n    stmt = SqlStatement(full_text, text_before_cursor)\n\n    if stmt.parsed:\n        special = parse_special_command(stmt.full_text, stmt.text_before_cursor)\n        if special:\n            return special, []\n\n    last_token = stmt.last_token\n    if last_token.ttype is Keyword:\n        return Keyword(last_token), []\n\n    if last_token.ttype is sqlparse.tokens.Name:\n        if last_token.value.lower() == 'database':\n            return Database(), []\n        elif last_token.value.lower() == 'schema':\n            return Schema(), []\n        elif last_token.value.lower() == 'function':\n            return Function(stmt.get_identifier_schema(), stmt.local_tables), []\n        elif last_token.value.lower() == 'table':\n            return Table(stmt.get_identifier_schema(), stmt.local_tables), []\n        elif last_token.value.lower() == 'view':\n            return View(stmt.get_identifier_schema(), stmt.local_tables), []\n        elif last_token.value.lower() == 'type':\n            return Datatype(stmt.get_identifier_schema()), []\n        elif last_token.value.lower() == 'as':\n            return Alias(stmt.get_tables()), []\n        elif last_token.value.lower() == 'join':\n            return Join(stmt.local_tables, stmt.get_identifier_schema()), []\n        elif last_token.value.lower() == 'on':\n            return JoinCondition(stmt.local_tables, stmt.get_tables()), []\n        elif last_token.value.lower() == 'from':\n            return FromClauseItem(stmt.get_identifier_schema(), stmt.local_tables, stmt.get_tables()), []\n        elif last_token.value.lower() == 'where':\n            return Column(stmt.local_tables, True, stmt.get_tables(), True, 'where'), []\n        elif last_token.value.lower() == 'group':\n            return Column(stmt.local_tables, True, stmt.get_tables(), True, 'group'), []\n        elif last_token.value.lower() == '", "idx": 1763}
{"namespace": "mssqlcli.packages.parseutils.ctes.extract_ctes", "completion": "    # Parse the SQL query\n    parsed = parse(sql)\n\n    # Check if the first meaningful token is \"WITH\"\n    if not parsed or not parsed[0].tokens or not isinstance(parsed[0].tokens[0], Keyword) or parsed[0].tokens[0].value.upper() != 'WITH':\n        return [], sql\n\n    # Extract the CTEs from the query\n    ctes = []\n    for item in parsed[0].tokens:\n        if isinstance(item, CTE):\n            ctes.append(TableExpression(item.get_alias(), item.get_columns(), item.start_pos[0], item.end_pos[0]))\n\n    # Return the extracted CTEs and the remaining SQL text\n    return ctes, sql[parsed[0].end_pos[0]:]\n\n", "idx": 1764}
{"namespace": "mssqlcli.packages.parseutils.tables.extract_tables", "completion": "    # Parse the SQL statement using sqlparse\n    parsed = sqlparse.parse(sql)[0]\n\n    # Extract the table names from the parsed result\n    tables = extract_table_identifiers(extract_from_part(parsed))\n\n    return tables\n\n", "idx": 1765}
{"namespace": "googleapiclient.channel.Channel.body", "completion": "        body = {\n            \"id\": self.id,\n            \"token\": self.token,\n            \"type\": self.type,\n            \"address\": self.address,\n        }\n        if self.params:\n            body[\"params\"] = self.params\n        if self.resource_id:\n            body[\"resourceId\"] = self.resource_id\n        if self.resource_uri:\n            body[\"resourceUri\"] = self.resource_uri\n        if self.expiration:\n            body[\"expiration\"] = self.expiration\n        return body\n", "idx": 1766}
{"namespace": "googleapiclient.channel.Channel.update", "completion": "        for k, v in resp.items():\n            if k in CHANNEL_PARAMS:\n                setattr(self, CHANNEL_PARAMS[k], v)\n\n", "idx": 1767}
{"namespace": "googleapiclient.channel.notification_from_headers", "completion": "    headers = _upper_header_keys(headers)\n    id = headers[X_GOOG_CHANNEL_ID]\n    if id != channel.id:\n        raise ValueError(\"Invalid notification: channel id mismatch\")\n    message_number = int(headers[X_GOOG_MESSAGE_NUMBER])\n    state = headers[X_GOOG_RESOURCE_STATE]\n    if state not in [\"exists\", \"not_exists\", \"sync\"]:\n        raise ValueError(\"Invalid notification: invalid resource state\")\n    resource_uri = headers[X_GOOG_RESOURCE_URI]\n    resource_id = headers[X_GOOG_RESOURCE_ID]\n    return Notification(message_number, state, resource_uri, resource_id)\n\n", "idx": 1768}
{"namespace": "googleapiclient.channel.new_webhook_channel", "completion": "    if token is None:\n        token = str(uuid.uuid4())\n    if expiration is None:\n        expiration = datetime.datetime.utcnow() + datetime.timedelta(days=365)\n    expiration = int((expiration - EPOCH).total_seconds() * 1000)\n    return Channel(\"web_hook\", str(uuid.uuid4()), token, url, expiration, params)", "idx": 1769}
{"namespace": "googleapiclient.model.BaseModel._build_query", "completion": "        if self.alt_param:\n            params[\"alt\"] = self.alt_param\n        query_params = []\n        for k, v in params.items():\n            if isinstance(v, list):\n                for item in v:\n                    query_params.append((k, item.encode(\"utf-8\")))\n            elif isinstance(v, str) and callable(v):\n                query_params.append((k, v().encode(\"utf-8\")))\n            else:\n                query_params.append((k, v))\n        return urllib.parse.urlencode(query_params)\n", "idx": 1770}
{"namespace": "googleapiclient.model.BaseModel.response", "completion": "        self._log_response(resp, content)\n        if resp.status == 204:\n            return self.no_content_response\n        if resp.status >= 300:\n            raise HttpError(resp, content)\n        return self.deserialize(content)\n", "idx": 1771}
{"namespace": "googleapiclient.model.makepatch", "completion": "    from googleapiclient.errors import HttpError\n    from googleapiclient.http import HttpError as HttpError_\n    from googleapiclient.http import build_http_request\n    from googleapiclient.http import build_request\n    from googleapiclient.http import request\n    from googleapiclient.http import MediaFileUpload\n    from googleapiclient.http import MediaIoBaseUpload\n    from googleapiclient.http import MediaIoBaseDownload\n    from googleapiclient.http import MediaInMemoryUpload\n    from googleapiclient.http import MediaUpload\n    from googleapiclient.http import MediaDownload\n    from googleapiclient.http import BatchHttpRequest\n    from googleapiclient.http import BatchHttpRequestEntry\n    from googleapiclient.http import BatchHttpRequestEntryError\n    from googleapiclient.http import BatchHttpRequestCallback\n    from googleapiclient.http import BatchHttpRequestError\n    from googleapiclient.http import BatchHttpRequestErrorEntry\n    from googleapiclient.http import BatchHttpRequestErrorEntryError\n    from googleapiclient.http import BatchHttpRequestErrorEntryResponse\n    from googleapiclient.http import BatchHttpRequestErrorEntryResponseHttpError\n    from googleapiclient.http import BatchHttpRequestErrorEntryResponseHttpErrorDetails\n    from googleapiclient.http import BatchHttpRequestErrorEntryResponseHttpErrorDetailsItem\n    from googleapiclient.http import BatchHttpRequestErrorEntryResponseHttpErrorDetailsItemLocation\n    from googleapiclient.http import BatchHttpRequestErrorEntryResponseHttpErrorDetailsItemLocationRange\n    from googleapiclient.http import BatchHttpRequestErrorEntryResponseHttpErrorDetailsItemLocationRangeRange\n    from googleapiclient.http import BatchHttpRequestErrorEntryResponseHttpErrorDetailsItemLocationRangeRangeEnd\n    from googleapiclient.http import BatchHttpRequestErrorEntryResponseHttpErrorDetailsItemLocationRangeRangeStart\n    from googleapiclient.http import BatchHttpRequestErrorEntryResponseHttpErrorDetailsItemLocationRangeType\n    from googleapiclient.http import BatchHttpRequestErrorEntryResponseHttpErrorDetailsItemLocationType\n    from googleapiclient.http import BatchHttpRequestErrorEntryResponseHttpErrorDetailsItemMessage\n    from googleapiclient.http import BatchHttpRequestErrorEntryResponseHttpErrorDetailsItemReason\n    from googleapiclient.http import BatchHttpRequestErrorEntryResponseHttpErrorDetailsItemSource\n    from googleapiclient.http import BatchHttpRequestErrorEntryResponseHttpErrorDetailsItemSourceType\n    from googleapiclient.http import BatchHttpRequestErrorEntryResponseHttpErrorDetailsItemSourceUri\n    from googleapiclient.http", "idx": 1772}
{"namespace": "googleapiclient._helpers.update_query_params", "completion": "    # Parse the URI into its components\n    parsed_uri = urllib.parse.urlparse(uri)\n\n    # Extract the query parameters from the URI\n    query_params = urllib.parse.parse_qs(parsed_uri.query)\n\n    # Update the query parameters with the new values\n    for key, value in params.items():\n        if key in query_params:\n            query_params[key] = [value]\n        else:\n            query_params[key] = [value]\n\n    # Convert the updated query parameters back to a string\n    updated_query_params = urllib.parse.urlencode(query_params, doseq=True)\n\n    # Reconstruct the URI with the updated query parameters\n    updated_uri = urllib.parse.urlunparse(\n        (\n            parsed_uri.scheme,\n            parsed_uri.netloc,\n            parsed_uri.path,\n            parsed_uri.params,\n            updated_query_params,\n            parsed_uri.fragment,\n        )\n    )\n\n    return updated_uri", "idx": 1773}
{"namespace": "googleapiclient._helpers._add_query_parameter", "completion": "    if value is None:\n        return url\n\n    parsed_url = urllib.parse.urlparse(url)\n    query_params = urllib.parse.parse_qs(parsed_url.query)\n    query_params[name] = value\n    updated_query = urllib.parse.urlencode(query_params, doseq=True)\n    updated_url = urllib.parse.urlunparse(\n        (parsed_url.scheme, parsed_url.netloc, parsed_url.path, parsed_url.params, updated_query, parsed_url.fragment)\n    )\n    return updated_url\n\n", "idx": 1774}
{"namespace": "gif_for_cli.display.display_txt_frames", "completion": "    try:\n        for _ in range(num_loops):\n            for frame in txt_frames:\n                stdout.write(frame + '\\n')\n                stdout.flush()\n                time.sleep(seconds_per_frame)\n    except KeyboardInterrupt:\n        raise\n\n", "idx": 1775}
{"namespace": "pyVmomi.SoapAdapter.SoapResponseDeserializer.Deserialize", "completion": "        self.deser.Deserialize(self.parser, resultType, True, nsMap)\n        self.parser.Parse(response)\n        result = self.deser.GetResult()\n        if isinstance(result, LocalizedMethodFault):\n            result = result.fault\n        if isinstance(result, Exception):\n            raise result\n        return result\n\n", "idx": 1776}
{"namespace": "pyVmomi.VmomiSupport.GetRequestContext", "completion": "    # Check if the RequestContext object exists in the current thread's context dictionary\n    context = getattr(_threadLocalContext, 'context', None)\n    if context:\n        return context\n\n    # Create a new RequestContext object and add it to the context dictionary\n    context = StringDict()\n    setattr(_threadLocalContext, 'context', context)\n    return context\n\n", "idx": 1777}
{"namespace": "pycoin.bloomfilter.filter_size_required", "completion": "    return int(math.ceil((-1 / math.pow(LOG_2, 2) * element_count * math.log(false_positive_probability)) / 8))\n\n", "idx": 1778}
{"namespace": "pycoin.bloomfilter.BloomFilter.add_spendable", "completion": "        self.add_item(spendable.to_bytes())\n", "idx": 1779}
{"namespace": "pycoin.bloomfilter.murmur3", "completion": "    c1 = 0xcc9e2d51\n    c2 = 0x1b873593\n\n    length = len(data)\n    nblocks = int((length + 3) / 4)\n\n    h1 = seed\n\n    # body\n    for block_start in range(4 * nblocks):\n        i = block_start + (block_start // 4)\n        k1 = data[i:i + 4]\n        k1 = (k1[0] & 0xff) + ((k1[1] & 0xff) << 8) + ((k1[2] & 0xff) << 16) + ((k1[3] & 0xff) << 24)\n        k1 *= c1\n        k1 = (k1 << 15) | ((k1 & 0xffffffff) >> 17)  # ROTL32(k1,15);\n        k1 *= c2\n\n        h1 ^= k1\n        h1 = (h1 << 13) | ((h1 & 0xffffffff) >> 19)  # ROTL32(h1,13);\n        h1 = h1 * 5 + 0xe6546b64\n\n    # tail\n    tail_index = nblocks * 4\n    k1 = 0\n\n    val = data[tail_index:tail_index + 1]\n    if val:\n        k1 ^= (val[0] & 0xff) << 0\n\n    val = data[tail_index + 1:tail_index + 2]\n    if val:\n        k1 ^= (val[0] & 0xff) << 8\n\n    val = data[tail_index + 2:tail_index + 3]\n    if val:\n        k1 ^= (val[0] & 0xff) << 16\n\n    val = data[tail_index + 3:tail_index + 4]\n    if val:\n        k1 ^= (val[0] & 0xff) << 24\n\n    k1 *= c1\n    k1 = (k1 << 15) | ((k1 & 0xffffffff) >> 17)  # ROTL32(k1,", "idx": 1780}
{"namespace": "pycoin.networks.registry.network_for_netcode", "completion": "    for prefix in search_prefixes():\n        try:\n            module = importlib.import_module(prefix)\n            if hasattr(module, \"symbol\") and module.symbol == symbol:\n                module.symbol = symbol\n                return module\n        except ImportError:\n            pass\n    raise ValueError(\"No network found for symbol %s\" % symbol)\n\n", "idx": 1781}
{"namespace": "pycoin.satoshi.IntStreamer.IntStreamer.int_from_script_bytes", "completion": "        if len(s) == 0:\n            return 0\n\n        x = s[-1]\n        if x & 0x80:\n            x = x - 0x100\n\n        if require_minimal and x == 0 and len(s) != 1:\n            raise ValueError('non-minimally encoded')\n\n        result = x\n        for c in s[-2::-1]:\n            result = result * 256 + c\n\n        if x & 0x80:\n            result = -result\n\n        return result\n", "idx": 1782}
{"namespace": "pycoin.satoshi.stackops.do_OP_RIPEMD160", "completion": "    v = stack.pop()\n    stack.append(hashlib.new('ripemd160', v).digest())\n\n", "idx": 1783}
{"namespace": "pycoin.satoshi.stackops.do_OP_HASH160", "completion": "    from ..encoding.hash import hash160\n    stack.append(hash160(stack.pop()).digest())\n\n", "idx": 1784}
{"namespace": "pycoin.satoshi.stackops.do_OP_HASH256", "completion": "    stack.append(hashlib.sha256(hashlib.sha256(stack.pop()).digest()).digest())\n\n", "idx": 1785}
{"namespace": "pycoin.services.providers.providers_for_config_string", "completion": "    providers = []\n    for descriptor in config_string.split():\n        provider = provider_for_descriptor_and_netcode(descriptor, netcode)\n        if provider is None:\n            warnings.warn(\"could not parse provider for descriptor %s\" % descriptor)\n        else:\n            providers.append(provider)\n    return providers\n\n", "idx": 1786}
{"namespace": "pycoin.services.providers.get_default_providers_for_netcode", "completion": "    if netcode is None:\n        netcode = get_current_netcode()\n\n    if not hasattr(THREAD_LOCALS, \"providers\"):\n        THREAD_LOCALS.providers = {}\n\n    if netcode not in THREAD_LOCALS.providers:\n        THREAD_LOCALS.providers[netcode] = providers_for_netcode_from_env(netcode)\n\n    return THREAD_LOCALS.providers[netcode]", "idx": 1787}
{"namespace": "pycoin.services.providers.set_default_providers_for_netcode", "completion": "    if not hasattr(THREAD_LOCALS, \"providers\"):\n        THREAD_LOCALS.providers = {}\n    THREAD_LOCALS.providers[netcode] = provider_list\n\n", "idx": 1788}
{"namespace": "pycoin.blockchain.BlockChain.BlockChain.tuple_for_index", "completion": "        if index < 0:\n            index = self.length() + index\n        if index < len(self._locked_chain):\n            return self._locked_chain[index]\n        index -= len(self._locked_chain)\n        if index < len(self._longest_local_block_chain()):\n            return self._longest_local_block_chain()[index]\n        index -= len(self._longest_local_block_chain())\n        if index < len(self._longest_chain_cache()):\n            return self._longest_chain_cache()[index]\n        raise IndexError(\"index out of range\")\n", "idx": 1789}
{"namespace": "pycoin.blockchain.ChainFinder.ChainFinder.find_ancestral_path", "completion": "        if h1 == h2:\n            return [h1], [h1]\n        if h1 in path_cache:\n            return path_cache[h1]\n        if h2 in path_cache:\n            return path_cache[h2]\n        path1 = self.maximum_path(h1, path_cache)\n        path2 = self.maximum_path(h2, path_cache)\n        if path1[-1] != path2[-1]:\n            raise ValueError(\"h1 and h2 are not in the same chain\")\n        i = 0\n        while path1[i] == path2[i]:\n            i += 1\n        path_cache[h1] = path1[i:], path2[i:]\n        path_cache[h2] = path1[i:], path2[i:]\n        return path1[i:], path2[i:]\n", "idx": 1790}
{"namespace": "pycoin.contrib.bech32m.bech32_encode", "completion": "    # Compute the checksum\n    checksum = bech32_create_checksum(hrp, data, spec)\n\n    # Convert the data and checksum to characters\n    combined = data + checksum\n    characters = [CHARSET[d] for d in combined]\n\n    # Add the separator and HRP\n    bech32 = hrp + '1' + ''.join(characters)\n\n    return bech32\n\n", "idx": 1791}
{"namespace": "pycoin.contrib.bech32m.decode", "completion": "    # Decode the address using the bech32_decode function.\n    decoded = bech32_decode(addr)\n\n    # Check if the decoded address is valid.\n    if decoded is None:\n        return (None, None)\n\n    # Extract the human-readable part, data, and encoding type from the decoded address.\n    hrp_decoded, data, spec = decoded\n\n    # Check if the human-readable part matches the input hrp.\n    if hrp_decoded != hrp:\n        return (None, None)\n\n    # Convert the data to a byte array using the convertbits function.\n    converted = convertbits(data[1:], 5, 8, False)\n\n    # Check if the conversion was successful.\n    if converted is None:\n        return (None, None)\n\n    # Check if the version byte is valid.\n    if data[0] < 0 or data[0] > 16:\n        return (None, None)\n\n    # Return the version byte and the converted data.\n    return (data[0], converted)\n", "idx": 1792}
{"namespace": "pycoin.crack.bip32.crack_bip32", "completion": "    for child in path.split(\"/\")[1:]:\n        secret_exponent = ascend_bip32(bip32_pub_node, secret_exponent, int(child))\n    return bip32_pub_node.at_public_depth(secret_exponent)", "idx": 1793}
{"namespace": "pycoin.message.PeerAddress.ip_bin_to_ip4_addr", "completion": "    return \".\".join(str(v) for v in struct.unpack(\">BBBB\", ip_bin))\n\n", "idx": 1794}
{"namespace": "pycoin.message.PeerAddress.PeerAddress.host", "completion": "        if self.ip_bin[:4] == IP4_HEADER:\n            return ip_bin_to_ip4_addr(self.ip_bin[-4:])\n        return ip_bin_to_ip6_addr(self.ip_bin)\n", "idx": 1795}
{"namespace": "oletools.msodde.field_is_blacklisted", "completion": "    # check if contents are empty\n    if not contents:\n        return False\n\n    # check if contents are a field\n    if not FIELD_WORD_REGEX.match(contents):\n        return False\n\n    # check if contents are a blacklisted field\n    if contents.lower() in FIELD_BLACKLIST_CMDS:\n        return True\n\n    # check if contents are a field with a number of arguments\n    if contents.count(\" \") != 1:\n        return False\n\n    # check if contents are a field with a number of switches\n    if FIELD_SWITCH_REGEX.match(contents):\n        return True\n\n    # check if contents are a field with a number of arguments\n    if contents.count(\" \") != 2:\n        return False\n\n    # check if contents are a field with a number of arguments and switches\n    if contents.count(\" \") != 3:\n        return False\n\n    # check if contents are a field with a number of arguments and switches\n    if contents.count(\" \") != 4:\n        return False\n\n    # check if contents are a field with a number of arguments and switches\n    if contents.count(\" \") != 5:\n        return False\n\n    # check if contents are a field with a number of arguments and switches\n    if contents.count(\" \") != 6:\n        return False\n\n    # check if contents are a field with a number of arguments and switches\n    if contents.count(\" \") != 7:\n        return False\n\n    # check if contents are a field with a number of arguments and switches\n    if contents.count(\" \") != 8:\n        return False\n\n    # check if contents are a field with a number of arguments and switches\n    if contents.count(\" \") != 9:\n        return False\n\n    # check if contents are a field with a number of arguments and switches\n    if contents.count(\" \") != 10:\n        return False\n\n    # check if contents are a field with a number of arguments and switches\n    if contents.count(\" \") != 11:\n        return False\n\n    # check if contents are a field with a number of arguments and switches\n    if contents.count(\" \") != 12:\n        return False\n\n    # check if contents are a field with a number of arguments and switches\n    if contents.count(\" \") != 13:\n        return False\n\n    # check if contents are a field with a number of arguments and switches\n    if contents.count(\" \") != 14:\n       ", "idx": 1796}
{"namespace": "oletools.ppt_record_parser.is_ppt", "completion": "    try:\n        ppt = PptRecordParser(filename)\n        ppt.parse_ppt()\n        return True\n    except Exception:\n        return False\n\n", "idx": 1797}
{"namespace": "oletools.rtfobj.is_rtf", "completion": "    if isinstance(arg, str):\n        if treat_str_as_data:\n            data = open(arg, 'rb').read()\n        else:\n            data = open(arg, 'rb').read()\n    else:\n        data = arg\n    if len(data) < 8:\n        return False\n    if data[:8] == b'{\\\\rtf1':\n        return True\n    return False\n\n", "idx": 1798}
{"namespace": "oletools.oleobj.get_sane_embedded_filenames", "completion": "    # extract filename from paths\n    if src_path:\n        filename = os.path.basename(src_path)\n    if tmp_path:\n        filename = os.path.basename(tmp_path)\n\n    # sanitize filename\n    sane_fname = sanitize_filename(filename, max_len=max_len)\n\n    # if filename is empty, try to generate a name\n    if not sane_fname:\n        sane_fname = 'oleobj_{0:03d}'.format(noname_index)\n\n    # if filename is still empty, try to generate a random name\n    if not sane_fname:\n        sane_fname = 'oleobj_{0:03d}'.format(random.randint(0, 999))\n\n    # if filename is still empty, try to generate a random name with suffix\n    if not sane_fname:\n        sane_fname = 'oleobj_{0:03d}.bin'.format(random.randint(0, 999))\n\n    # if filename is still empty, try to generate a random name ignoring max_len\n    if not sane_fname:\n        sane_fname = 'oleobj_{0:03d}'.format(random.randint(0, 999))\n\n    return sane_fname\n\n", "idx": 1799}
{"namespace": "oletools.ooxml.get_type", "completion": "    # check if file is a zip file\n    if not is_zipfile(filename):\n        return DOCTYPE_NONE\n\n    # check if file is a single-xml file\n    try:\n        with ZipFile(filename, 'r') as zf:\n            with zf.open(FILE_CONTENT_TYPES, 'r') as f:\n                content_types = f.read().decode('utf-8')\n                if 'application/vnd.openxmlformats-officedocument.wordprocessingml.document.main+xml' in content_types:\n                    return DOCTYPE_WORD_XML\n                if 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet.main+xml' in content_types:\n                    return DOCTYPE_EXCEL_XML\n                if 'application/vnd.openxmlformats-officedocument.presentationml.presentation.main+xml' in content_types:\n                    return DOCTYPE_POWERPOINT\n    except (BadZipfile, KeyError):\n        pass\n\n    # check if file is a single-xml file (2003)\n    try:\n        with ZipFile(filename, 'r') as zf:\n            with zf.open(FILE_CONTENT_TYPES, 'r') as f:\n                content_types = f.read().decode('utf-8')\n                if 'application/vnd.ms-word.document.macroEnabled.main+xml' in content_types:\n                    return DOCTYPE_WORD_XML2003\n                if 'application/vnd.ms-excel.sheet.macroEnabled.main+xml' in content_types:\n                    return DOCTYPE_EXCEL_XML2003\n    except (BadZipfile, KeyError):\n        pass\n\n    # check if file is a flatopc file\n    try:\n        with ZipFile(filename, 'r') as zf:\n            with zf.open(FILE_RELATIONSHIPS, 'r') as f:\n                rels = f.read().decode('utf-8')\n                if 'application/vnd.openxmlformats-officedocument.wordprocessingml.document.main+xml' in rels:\n                    return DOCTYPE_WORD_XML\n                if 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet.main+xml' in rels:\n                    return DOCTYPE_", "idx": 1800}
{"namespace": "oletools.ooxml.ZipSubFile.read", "completion": "        if self.handle is None:\n            raise IOError('file not open')\n        if self.pos >= self.size:\n            return b''\n        if size == -1:\n            size = self.size - self.pos\n        data = self.handle.read(size)\n        self.pos += len(data)\n        return data\n", "idx": 1801}
{"namespace": "oletools.ooxml.ZipSubFile.seek", "completion": "        if self.handle is None:\n            raise IOError('seek on closed handle')\n        if offset == io.SEEK_SET:\n            new_pos = pos\n        elif offset == io.SEEK_CUR:\n            new_pos = self.pos + pos\n        elif offset == io.SEEK_END:\n            new_pos = self.size + pos\n        else:\n            raise ValueError('invalid offset')\n        if new_pos < 0:\n            raise IOError('seek before start of file')\n        if new_pos > self.size:\n            raise IOError('seek beyond end of file')\n        self.pos = new_pos\n        # print('ZipSubFile: seek to {}'.format(self.pos))\n        self.handle.seek(self.pos)\n", "idx": 1802}
{"namespace": "oletools.ooxml.XmlParser.iter_xml", "completion": "        if self.did_iter_all:\n            raise BadOOXML(self.filename, 'already iterated all')\n        if not subfiles:\n            subfiles = [FILE_CONTENT_TYPES, ]\n        if not tags:\n            tags = [None, ]\n        for subfile in subfiles:\n            if subfile not in self.subfiles_no_xml:\n                raise BadOOXML(self.filename, 'invalid subfile: ' + subfile)\n            with ZipSubFile(self.filename, subfile) as handle:\n                try:\n                    tree = ET.parse(handle)\n                except ET.ParseError as err:\n                    raise BadOOXML(self.filename,\n                                   'invalid xml: ' + str(err))\n                for tag in tags:\n                    if tag is None:\n                        yield subfile, tree, 0\n                    else:\n                        for elem in tree.iter(tag):\n                            yield subfile, elem, 1\n\n", "idx": 1803}
{"namespace": "oletools.oleid.OleID.check", "completion": "        # Check if the file is a valid OLE file\n        if not olefile.isOleFile(self.data_bytesio):\n            raise ValueError('File is not a valid OLE file')\n\n        # Check if the file is a valid OLE file\n        if not olefile.isOleFile(self.data_bytesio):\n            raise ValueError('File is not a valid OLE file')\n\n        # Check if the file is a valid OLE file\n        if not olefile.isOleFile(self.data_bytesio):\n            raise ValueError('File is not a valid OLE file')\n\n        # Check if the file is a valid OLE file\n        if not olefile.isOleFile(self.data_bytesio):\n            raise ValueError('File is not a valid OLE file')\n\n        # Check if the file is a valid OLE file\n        if not olefile.isOleFile(self.data_bytesio):\n            raise ValueError('File is not a valid OLE file')\n\n        # Check if the file is a valid OLE file\n        if not olefile.isOleFile(self.data_bytesio):\n            raise ValueError('File is not a valid OLE file')\n\n        # Check if the file is a valid OLE file\n        if not olefile.isOleFile(self.data_bytesio):\n            raise ValueError('File is not a valid OLE file')\n\n        # Check if the file is a valid OLE file\n        if not olefile.isOleFile(self.data_bytesio):\n            raise ValueError('File is not a valid OLE file')\n\n        # Check if the file is a valid OLE file\n        if not olefile.isOleFile(self.data_bytesio):\n            raise ValueError('File is not a valid OLE file')\n\n        # Check if the file is a valid OLE file\n        if not olefile.isOleFile(self.data_bytesio):\n            raise ValueError('File is not a valid OLE file')\n\n        # Check if the file is a valid OLE file\n        if not olefile.isOleFile(self.data_bytesio):\n            raise ValueError('File is not a valid OLE file')\n\n        # Check if the file is a valid OLE file\n        if not olefile.isOleFile(self.data_bytesio):\n            raise ValueError('File is not a valid OLE file')\n\n        # Check if the file is a", "idx": 1804}
{"namespace": "tools.cgrep.is_valid_ip", "completion": "  try:\n    return nacaddr.IP(arg)\n  except nacaddr.Error:\n    raise argparse.ArgumentTypeError(\"Invalid IP address: %s\" % arg)\n\n", "idx": 1805}
{"namespace": "tools.cgrep.group_diff", "completion": "  # Get the network objects for the first IP.\n  first_ip_nets = get_ip_parents(options.gmp[0], db)\n  # Get the network objects for the second IP.\n  second_ip_nets = get_ip_parents(options.gmp[1], db)\n  # Get the common lines between the two IPs.\n  common = set(first_ip_nets).intersection(set(second_ip_nets))\n  # Get the differences between the first IP and the second IP.\n  diff1 = set(first_ip_nets).difference(set(second_ip_nets))\n  # Get the differences between the second IP and the first IP.\n  diff2 = set(second_ip_nets).difference(set(first_ip_nets))\n  return common, diff1, diff2\n\n", "idx": 1806}
{"namespace": "tools.cgrep.compare_tokens", "completion": "  first_obj, first_nets = get_nets([options.cmp[0]], db)[0]\n  second_obj, second_nets = get_nets([options.cmp[1]], db)[0]\n  union = sorted(list(set(first_nets) | set(second_nets)))\n  diff = sorted(list(set(first_nets) - set(second_nets)))\n  return (first_obj, second_obj, union), diff\n\n", "idx": 1807}
{"namespace": "capirca.aclgen.EntryPoint", "completion": "  Setup()\n  SetupFlags()\n  SetupLogging()\n  main(sys.argv)\n\n", "idx": 1808}
{"namespace": "capirca.lib.nacaddr.IP", "completion": "  if isinstance(ip, ipaddress._BaseNetwork):\n    return ip\n  else:\n    try:\n      ip_obj = ipaddress.ip_network(ip, strict=strict)\n    except ValueError:\n      raise ValueError('Invalid IP address: %s' % ip)\n    if ip_obj.version == 4:\n      return IPv4(ip_obj, comment=comment, token=token)\n    elif ip_obj.version == 6:\n      return IPv6(ip_obj, comment=comment, token=token)\n    else:\n      raise ValueError('Invalid IP address: %s' % ip)\n\n", "idx": 1809}
{"namespace": "csvkit.cli.CSVKitUtility.run", "completion": "        if 'f' not in self.override_flags:\n            self.input_file = self._open_input_file()\n\n        try:\n            if 'no_header_row' in self.override_flags:\n                with warnings.catch_warnings():\n                    warnings.simplefilter(\"ignore\")\n                    self.main()\n            else:\n                self.main()\n        finally:\n            if 'f' not in self.override_flags:\n                self.input_file.close()\n", "idx": 1810}
{"namespace": "csvkit.convert.fixed.fixed2csv", "completion": "    # Read the schema file and parse it into a list of tuples\n    schema_file = StringIO(schema)\n    schema_reader = agate.csv.reader(schema_file)\n    schema_list = list(schema_reader)\n\n    # Create a namedtuple to store the column names, starting indices, and lengths\n    Column = namedtuple('Column', ['name', 'start', 'length'])\n\n    # Create a list of Column objects based on the schema\n    columns = [Column(name=row[0], start=int(row[1]), length=int(row[2])) for row in schema_list]\n\n    # Read the fixed-width file and parse it based on the schema\n    f.seek(0)\n    for _ in range(skip_lines):\n        f.readline()\n    lines = iterdecode(f, 'utf-8')\n    parsed_data = []\n    for line in lines:\n        parsed_line = [line[col.start:col.start + col.length].strip() for col in columns]\n        parsed_data.append(parsed_line)\n\n    # Write the parsed data to a CSV file or return it as a string\n    if output:\n        writer = agate.csv.writer(output)\n        writer.writerow([col.name for col in columns])\n        writer.writerows(parsed_data)\n    else:\n        output = StringIO()\n        writer = agate.csv.writer(output)\n        writer.writerow([col.name for col in columns])\n        writer.writerows(parsed_data)\n        return output.getvalue()", "idx": 1811}
{"namespace": "check_dummies.find_backend", "completion": "    # Find all matches of the backend regex in the line\n    matches = _re_backend.findall(line)\n\n    # If no matches are found, return None\n    if not matches:\n        return None\n\n    # If matches are found, join them with \"_and_\" and return the result\n    return \"_and_\".join(matches)\n\n", "idx": 1812}
{"namespace": "check_dummies.create_dummy_object", "completion": "    if name.startswith(\"Flax\"):\n        return DUMMY_CLASS.format(name, backend_name)\n    elif name.startswith(\"Onnx\"):\n        return DUMMY_CLASS.format(name, backend_name)\n    elif name.startswith(\"PNDM\"):\n        return DUMMY_CLASS.format(name, backend_name)\n    elif name.startswith(\"UNet2DConditionModel\"):\n        return DUMMY_CLASS.format(name, backend_name)\n    elif name.startswith(\"UNet3DConditionModel\"):\n        return DUMMY_CLASS.format(name, backend_name)\n    elif name.startswith(\"UNet2DModel\"):\n        return DUMMY_CLASS.format(name, backend_name)\n    elif name.startswith(\"UNet3DModel\"):\n        return DUMMY_CLASS.format(name, backend_name)\n    elif name.startswith(\"UNet2DConditionModel\"):\n        return DUMMY_CLASS.format(name, backend_name)\n    elif name.startswith(\"UNet3DConditionModel\"):\n        return DUMMY_CLASS.format(name, backend_name)\n    elif name.startswith(\"UNet2DModel\"):\n        return DUMMY_CLASS.format(name, backend_name)\n    elif name.startswith(\"UNet3DModel\"):\n        return DUMMY_CLASS.format(name, backend_name)\n    elif name.startswith(\"UNet2DConditionModel\"):\n        return DUMMY_CLASS.format(name, backend_name)\n    elif name.startswith(\"UNet3DConditionModel\"):\n        return DUMMY_CLASS.format(name, backend_name)\n    elif name.startswith(\"UNet2DModel\"):\n        return DUMMY_CLASS.format(name, backend_name)\n    elif name.startswith(\"UNet3DModel\"):\n        return DUMMY_CLASS.format(name, backend_name)\n    elif name.startswith(\"UNet2DConditionModel\"):\n        return DUMMY_CLASS.format(name, backend_name)\n    elif name.startswith(\"UNet3DConditionModel\"):\n        return DUMMY_CLASS.format(name, backend_name)\n    elif name.startswith(\"UNet2DModel\"):\n        return DUMMY_CLASS.format(name, backend_name)\n    elif name.startswith(\"UNet3DModel\"):\n        return", "idx": 1813}
{"namespace": "pycorrector.en_spell.EnSpell.check_init", "completion": "        if not self.word_freq_dict:\n            self._init()\n", "idx": 1814}
{"namespace": "pycorrector.en_spell.EnSpell.candidates", "completion": "        self.check_init()\n        # known(set)\n        # \u7f16\u8f91\u8ddd\u79bb\u4e3a0\u7684\u5355\u8bcd\n        known_edits0 = self.known([word])\n        if len(known_edits0) > 0:\n            return known_edits0\n        # \u7f16\u8f91\u8ddd\u79bb\u4e3a1\u7684\u5355\u8bcd\n        known_edits1 = self.known(self.edits1(word))\n        if len(known_edits1) > 0:\n            return known_edits1\n        # \u7f16\u8f91\u8ddd\u79bb\u4e3a2\u7684\u5355\u8bcd\n        known_edits2 = self.known(self.edits2(word))\n        if len(known_edits2) > 0:\n            return known_edits2\n        return {word}\n", "idx": 1815}
{"namespace": "pycorrector.en_spell.EnSpell.correct_word", "completion": "        self.check_init()\n        candidates = self.candidates(word)\n        return max(candidates, key=self.probability)\n", "idx": 1816}
{"namespace": "pycorrector.en_spell.EnSpell.correct", "completion": "        self.check_init()\n        if not text:\n            return text, []\n        details = []\n        blocks = text.split() if include_symbol else text.split()\n        for i, block in enumerate(blocks):\n            if len(block) > 1 and block.isalpha():\n                if block in self.custom_confusion:\n                    corrected = self.custom_confusion[block]\n                else:\n                    corrected = self.correct_word(block)\n                if corrected != block:\n                    begin = text.find(block, i)\n                    end = begin + len(block)\n                    details.append([block, corrected, begin, end])\n                    text = text.replace(block, corrected)\n        details = sorted(details, key=lambda x: x[2])\n        return text, details\n\n", "idx": 1817}
{"namespace": "whereami.predict.crossval", "completion": "    if X is None or y is None:\n        X, y = get_train_data(path)\n\n    if len(X) < folds:\n        raise ValueError(f\"There are not enough samples ({len(X)}). Need at least {folds}.\")\n\n    if clf is None:\n        clf = get_model(path)\n\n    print(f\"KFold folds={folds}, running {n} times\")\n\n    for i in range(n):\n        scores = cross_val_score(clf, X, y, cv=folds)\n        print(f\"{i + 1}/{n}: {scores.mean()}\")\n\n    print(\"-------- total --------\")\n    print(f\"total: {scores.mean()}\")\n    return scores.mean()\n\n", "idx": 1818}
{"namespace": "stellar.models.Table.get_table_name", "completion": "        if not self.snapshot:\n            raise Exception('Table name requires snapshot')\n\n        if not self.snapshot.hash:\n            raise Exception('Snapshot hash is empty.')\n\n        if old:\n            return 'stellar_{table_name}{snapshot_hash}{postfix}'.format(\n                table_name=self.table_name,\n                snapshot_hash=self.snapshot.hash,\n                postfix=postfix\n            )\n\n        return 'stellar_{table_name}{hash}'.format(\n            table_name=self.table_name,\n            hash=hashlib.md5(\n                '{table_name}{snapshot_hash}{postfix}'.format(\n                    table_name=self.table_name,\n                    snapshot_hash=self.snapshot.hash,\n                    postfix=postfix\n                ).encode('utf-8')\n            ).hexdigest()[:16]\n        )\n", "idx": 1819}
{"namespace": "chatette.utils.Singleton.reset_instance", "completion": "        cls._instance = cls(*args, **kwargs)\n        return cls._instance\n\n", "idx": 1820}
{"namespace": "chatette.utils.cast_to_unicode", "completion": "    if sys.version_info[0] < 3:\n        if isinstance(anything, str):\n            return anything.decode('utf-8')\n        elif isinstance(anything, list):\n            return [cast_to_unicode(item) for item in anything]\n        elif isinstance(anything, dict):\n            return {cast_to_unicode(key): cast_to_unicode(value) for key, value in anything.items()}\n        else:\n            return anything\n    else:\n        return anything\n\n", "idx": 1821}
{"namespace": "chatette.cli.terminal_writer.TerminalWriter.write", "completion": "        if self._file_mode == 'quiet':\n            return\n        if self._file_mode is None:\n            print(text)\n            return\n        if self.redirection_file_path is None:\n            print(text)\n            return\n        if self.buffered_text is None:\n            self.buffered_text = text\n        else:\n            self.buffered_text += text\n", "idx": 1822}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.find_redirection_file_path", "completion": "        if len(tokens) == 0:\n            return None\n        if tokens[-1] == REDIRECTION_SYM:\n            return (RedirectionType.overwrite, None)\n        if tokens[-1] == REDIRECTION_APPEND_SYM:\n            return (RedirectionType.append, None)\n        if tokens[-1].startswith(REDIRECTION_SYM):\n            if tokens[-1].startswith(REDIRECTION_APPEND_SYM):\n                return (RedirectionType.append, tokens[-1][2:])\n            else:\n                return (RedirectionType.overwrite, tokens[-1][1:])\n        return None\n", "idx": 1823}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.get_unit_type_from_str", "completion": "        if unit_type_str == \"dataset\":\n            return AST.UnitType.dataset\n        if unit_type_str == \"template\":\n            return AST.UnitType.template\n        if unit_type_str == \"pattern\":\n            return AST.UnitType.pattern\n        if unit_type_str == \"variation\":\n            return AST.UnitType.variation\n        if unit_type_str == \"escapement\":\n            return AST.UnitType.escapement\n        return None\n", "idx": 1824}
{"namespace": "chatette.cli.interactive_commands.unhide_command.UnhideCommand.execute", "completion": "        if len(self.tokens) < 3:\n            self.error(\"Missing unit name\")\n            return\n\n        unit_type = self.tokens[1]\n        if not AST.is_valid_unit_type(unit_type):\n            self.error(\"Invalid unit type\")\n            return\n\n        unit_name = self.tokens[2]\n        if not AST.is_valid_unit_name(unit_name):\n            self.error(\"Invalid unit name\")\n            return\n\n        try:\n            regex = self.tokens[3]\n        except IndexError:\n            regex = None\n\n        if regex is not None:\n            try:\n                regex = HideCommand.parse_regex(regex)\n            except ValueError as e:\n                self.error(str(e))\n                return\n\n        unit = self.ast.get_unit(unit_type, unit_name)\n        if unit is None:\n            self.error(\"Unit not found\")\n            return\n\n        if unit.is_hidden:\n            self.error(\"Unit is already unhidden\")\n            return\n\n        if regex is not None:\n            if not unit.is_regex_valid(regex):\n                self.error(\"Invalid regular expression\")\n                return\n\n        unit.is_hidden = False\n        self.ast.update_unit(unit_type, unit_name, unit)\n        self.success(\"Unit unhidden\")", "idx": 1825}
{"namespace": "chatette.adapters.factory.create_adapter", "completion": "    if adapter_name == 'rasa':\n        from chatette.adapters.rasa import RasaAdapter\n        return RasaAdapter(base_filepath)\n    elif adapter_name == 'rasa-md':\n        from chatette.adapters.rasa_md import RasaMdAdapter\n        return RasaMdAdapter(base_filepath)\n    elif adapter_name == 'rasamd':\n        from chatette.adapters.rasa_md import RasaMdAdapter\n        return RasaMdAdapter(base_filepath)\n    elif adapter_name == 'jsonl':\n        from chatette.adapters.jsonl import JsonlAdapter\n        return JsonlAdapter(base_filepath)\n    else:\n        raise ValueError('Unknown adapter name: ' + adapter_name)", "idx": 1826}
{"namespace": "chatette.parsing.ChoiceBuilder.create_concrete", "completion": "        self._check_information()\n        modifiers = self._build_modifiers_repr()\n        return Choice(self.leading_space, modifiers, self.rules)\n", "idx": 1827}
{"namespace": "chatette.parsing.UnitRefBuilder._build_modifiers_repr", "completion": "        modifiers = super(UnitRefBuilder, self)._build_modifiers_repr()\n        modifiers.arg_value = self.arg_value\n        modifiers.variation_name = self.variation\n        return modifiers\n", "idx": 1828}
{"namespace": "chatette.parsing.UnitRefBuilder.create_concrete", "completion": "        self._check_information()\n        from chatette.units.reference import UnitReference\n        return UnitReference(\n            self.leading_space, self._build_modifiers_repr(),\n            self.type, self.identifier, self.variation, self.arg_value\n        )\n", "idx": 1829}
{"namespace": "chatette.parsing.UnitDefBuilder._build_modifiers_repr", "completion": "        modifiers = super(UnitDefBuilder, self)._build_modifiers_repr()\n        modifiers.argument_name = self.arg_name\n        return modifiers\n", "idx": 1830}
{"namespace": "chatette.parsing.AliasDefBuilder.create_concrete", "completion": "        from chatette.units.modifiable.alias_definition import AliasDefinition\n        self._check_information()\n        if self.variation is not None and self.identifier in self.definitions:\n            return self.definitions[self.identifier].get_variation(self.variation)\n        return AliasDefinition(\n            self.identifier,\n            self.leading_space, self._build_modifiers_repr()\n        )\n", "idx": 1831}
{"namespace": "chatette.parsing.SlotDefBuilder.create_concrete", "completion": "        from chatette.units.modifiable.definitions.slot import SlotDefinition\n        self._check_information()\n        if self.variation is not None:\n            definitions = AST.get_or_create()[UnitType.slot]\n            if self.identifier in definitions:\n                return definitions[self.identifier]\n        return SlotDefinition(self.identifier, self._build_modifiers_repr())", "idx": 1832}
{"namespace": "chatette.parsing.IntentDefBuilder.create_concrete", "completion": "        from chatette.units.modifiable.definitions.intent import IntentDefinition\n        self._check_information()\n        if self.variation is not None:\n            definitions = AST.get_or_create()[UnitType.intent]\n            if self.identifier in definitions:\n                return definitions[self.identifier]\n        return IntentDefinition(\n            self.identifier, self._build_modifiers_repr(),\n            self.nb_training_ex, self.nb_testing_ex\n        )\n", "idx": 1833}
{"namespace": "bentoml._internal.resource.get_resource", "completion": "    if resource_kind not in _RESOURCE_REGISTRY:\n        raise BentoMLConfigException(\n            f\"Resource kind {resource_kind} is not registered.\"\n        )\n\n    resource_class = _RESOURCE_REGISTRY[resource_kind]\n\n    if resource_kind not in resources:\n        return None\n\n    resource_spec = resources[resource_kind]\n\n    if resource_spec == \"system\":\n        resource = resource_class.from_system()\n    else:\n        resource = resource_class.from_spec(resource_spec)\n\n    if validate:\n        resource.validate()\n\n    return resource\n\n", "idx": 1834}
{"namespace": "bentoml._internal.resource.system_resources", "completion": "    result: dict[str, t.Any] = {}\n    for resource_kind, resource in _RESOURCE_REGISTRY.items():\n        result[resource_kind] = resource.from_system()\n    return result\n\n", "idx": 1835}
{"namespace": "bentoml._internal.resource.CpuResource.from_spec", "completion": "        if isinstance(spec, float):\n            return spec\n        elif isinstance(spec, int):\n            return float(spec)\n        elif isinstance(spec, str):\n            if re.match(r\"^\\d+(\\.\\d+)?$\", spec):\n                return float(spec)\n            elif re.match(r\"^\\d+(\\.\\d+)?(m|M)$\", spec):\n                return float(spec[:-1]) / 1000\n            elif re.match(r\"^\\d+(\\.\\d+)?(n|N)$\", spec):\n                return float(spec[:-1]) / 1000000000\n            else:\n                raise ValueError(\n                    f\"Invalid CPU resource specification '{spec}'. \"\n                    \"Must be a float, int, or string in the format of \"\n                    \"'1000' or '1000m' or '1000n'.\"\n                )\n        else:\n            raise ValueError(\n                f\"Invalid CPU resource specification '{spec}'. \"\n                \"Must be a float, int, or string in the format of \"\n                \"'1000' or '1000m' or '1000n'.\"\n            )\n", "idx": 1836}
{"namespace": "bentoml._internal.resource.CpuResource.from_system", "completion": "        if os.name == \"nt\":\n            return psutil.cpu_count(logical=False)\n        else:\n            return psutil.cpu_count()\n", "idx": 1837}
{"namespace": "bentoml._internal.resource.CpuResource.validate", "completion": "        if val < 0:\n            raise BentoMLConfigException(\n                f\"Invalid CPU resource limit '{val}'. CPU resource limit must be positive.\"\n            )\n\n        if psutil.POSIX:\n            system_cpu_count = query_cgroup_cpu_count()\n        else:\n            system_cpu_count = query_os_cpu_count()\n\n        if val > system_cpu_count:\n            raise BentoMLConfigException(\n                f\"Invalid CPU resource limit '{val}'. CPU resource limit must be less than or equal to the system's available CPU resources.\"\n            )\n\n", "idx": 1838}
{"namespace": "bentoml._internal.types.LazyType.get_class", "completion": "        if self._runtime_class is not None:\n            return self._runtime_class\n\n        if import_module:\n            try:\n                module = importlib.import_module(self.module)\n            except ModuleNotFoundError as e:\n                raise ModuleNotFoundError(\n                    f\"Module {self.module} not found. Please make sure it is installed.\"\n                ) from e\n        else:\n            module = sys.modules[self.module]\n\n        try:\n            cls = getattr(module, self.qualname)\n        except AttributeError as e:\n            raise AttributeError(\n                f\"Class {self.qualname} not found in module {self.module}.\"\n            ) from e\n\n        self._runtime_class = cls\n        return cls\n", "idx": 1839}
{"namespace": "bentoml._internal.models.model.Model.create", "completion": "        if isinstance(name, str):\n            name = Tag(name)\n\n        if name.version is None:\n            name = name.with_version(Store.generate_version())\n\n        if labels is None:\n            labels = {}\n\n        if options is None:\n            options = ModelOptions()\n\n        if custom_objects is None:\n            custom_objects = {}\n\n        if metadata is None:\n            metadata = {}\n\n        if not isinstance(name, Tag):\n            raise BentoMLException(\n                f\"Invalid model name: {name}, expected a Tag object\"\n            )\n\n        if not isinstance(module, str):\n            raise BentoMLException(\n                f\"Invalid model module: {module}, expected a string\"\n            )\n\n        if not isinstance(api_version, str):\n            raise BentoMLException(\n                f\"Invalid model api_version: {api_version}, expected a string\"\n            )\n\n        if not isinstance(signatures, ModelSignaturesType):\n            raise BentoMLException(\n                f\"Invalid model signatures: {signatures}, expected a ModelSignaturesType object\"\n            )\n\n        if not isinstance(labels, dict):\n            raise BentoMLException(\n                f\"Invalid model labels: {labels}, expected a dictionary\"\n            )\n\n        if not isinstance(options, ModelOptions):\n            raise BentoMLException(\n                f\"Invalid model options: {options}, expected a ModelOptions object\"\n            )\n\n        if not isinstance(custom_objects, dict):\n            raise BentoMLException(\n                f\"Invalid model custom_objects: {custom_objects}, expected a dictionary\"\n            )\n\n        if not isinstance(metadata, dict):\n            raise BentoMLException(\n                f\"Invalid model metadata: {metadata}, expected a dictionary\"\n            )\n\n        if not isinstance(context, ModelContext):\n            raise BentoMLException(\n                f\"Invalid model context: {context}, expected a ModelContext object\"\n            )\n\n        if not isinstance(context.runner, Runner):\n            raise BentoMLException(\n                f\"Invalid model context.runner: {context.runner}, expected a Runner object\"\n            )\n\n        if not isinstance(context.strategy, Strategy):\n            raise BentoMLException(\n                f\"Invalid model context.strategy: {context.strategy}, expected a Strategy object\"\n            )\n\n        if not isinstance(context.env, BentoMLContainer):\n            raise BentoMLException(\n                f\"Invalid model", "idx": 1840}
{"namespace": "bentoml._internal.models.model.Model.from_fs", "completion": "        with item_fs.open(MODEL_YAML_FILENAME, \"r\") as f:\n            model_info = ModelInfo.from_dict(yaml.safe_load(f))\n\n        return cls(\n            model_info.tag,\n            item_fs,\n            model_info,\n            _internal=True,\n        )\n", "idx": 1841}
{"namespace": "bentoml._internal.utils.metrics.linear_buckets", "completion": "    assert start < end\n    assert step > 0.0\n\n    bound = start\n    buckets: list[float] = []\n    while bound < end:\n        buckets.append(bound)\n        bound += step\n\n    if len(buckets) > MAX_BUCKET_COUNT:\n        buckets = buckets[:MAX_BUCKET_COUNT]\n\n    return tuple(buckets) + (end, INF)", "idx": 1842}
{"namespace": "bentoml._internal.utils.validate_metadata", "completion": "    if not isinstance(metadata, dict):\n        raise ValueError(\"metadata must be a dict!\")\n\n    for key, val in metadata.items():\n        if not isinstance(key, str):\n            raise ValueError(\"metadata keys must be strings\")\n\n        if not isinstance(val, MetadataType):\n            raise ValueError(\"metadata values must be strings or numbers\")\n\n", "idx": 1843}
{"namespace": "bentoml._internal.utils.analytics.usage_stats.get_serve_info", "completion": "    return ServeInfo(\n        serve_id=secrets.token_hex(16),\n        serve_started_timestamp=datetime.now(tz=timezone.utc),\n    )\n\n", "idx": 1844}
{"namespace": "bentoml._internal.utils.analytics.usage_stats._track_serve_init", "completion": "    track(\n        ServeInitEvent(\n            serve_id=serve_info.serve_id,\n            serve_started_timestamp=serve_info.serve_started_timestamp,\n            serve_from_server_api=from_server_api,\n            production=production,\n            serve_kind=serve_kind,\n            num_models=len(svc.models),\n            num_runners=len(svc.runners),\n            num_apis=len(svc.apis),\n            model_types=[model.name for model in svc.models],\n            runner_types=[runner.name for runner in svc.runners],\n            api_input_types=[api.input_type for api in svc.apis],\n            api_output_types=[api.output_type for api in svc.apis],\n        )\n    )\n\n", "idx": 1845}
{"namespace": "bentoml._internal.service.service.get_valid_service_name", "completion": "    svc_name = user_provided_svc_name.lower()\n    if svc_name != user_provided_svc_name:\n        logger.warning(\n            \"Service name is case-sensitive, \"\n            \"please use lowercase service name: %s\",\n            svc_name,\n        )\n    Tag.validate_name(svc_name)\n    return svc_name\n\n", "idx": 1846}
{"namespace": "bentoml._internal.configuration.helpers.flatten_dict", "completion": "    for k, v in d.items():\n        if isinstance(v, dict):\n            yield from flatten_dict(v, parent + k + sep, sep)\n        else:\n            yield parent + k, v\n\n", "idx": 1847}
{"namespace": "bentoml._internal.configuration.helpers.load_config_file", "completion": "    if not os.path.exists(path):\n        raise BentoMLConfigException(f\"Configuration file {path} does not exist.\")\n\n    with open(path, \"r\") as f:\n        config = yaml.safe_load(f)\n\n    return config\n\n", "idx": 1848}
{"namespace": "bentoml._internal.configuration.helpers.expand_env_var_in_values", "completion": "    for k, v in d.items():\n        if isinstance(v, t.MutableMapping):\n            expand_env_var_in_values(v)\n        elif isinstance(v, str):\n            d[k] = expand_env_var(v)\n        elif isinstance(v, t.MutableSequence):\n            for i, item in enumerate(v):\n                if isinstance(item, t.MutableMapping):\n                    expand_env_var_in_values(item)\n                elif isinstance(item, str):\n                    v[i] = expand_env_var(item)", "idx": 1849}
{"namespace": "bentoml._internal.runner.strategy.DefaultStrategy.get_worker_count", "completion": "        if resource_request is None:\n            resource_request = {}\n\n        if \"nvidia.com/gpu\" in resource_request and runnable_class.supports_nvidia_gpus():\n            return math.ceil(\n                system_resources.get_nvidia_gpu_count() * workers_per_resource\n            )\n\n        if \"cpu\" in resource_request and runnable_class.supports_cpus():\n            return math.ceil(system_resources.get_cpu_count() * workers_per_resource)\n\n        raise ValueError(\n            f\"No known supported resources available for {runnable_class.__name__}\"\n        )\n", "idx": 1850}
{"namespace": "bentoml._internal.runner.strategy.DefaultStrategy.get_worker_env", "completion": "        if resource_request is None:\n            resource_request = system_resources()\n\n        # use nvidia gpu\n        nvidia_gpus = get_resource(resource_request, \"nvidia.com/gpu\")\n        if (\n            nvidia_gpus is not None\n            and len(nvidia_gpus) > 0\n            and \"nvidia.com/gpu\" in runnable_class.SUPPORTED_RESOURCES\n        ):\n            env = {\n                \"CUDA_VISIBLE_DEVICES\": str(worker_index % len(nvidia_gpus)),\n                \"NVIDIA_VISIBLE_DEVICES\": str(worker_index % len(nvidia_gpus)),\n            }\n            return env\n\n        # use CPU\n        cpus = get_resource(resource_request, \"cpu\")\n        if cpus is not None and cpus > 0:\n            if \"cpu\" not in runnable_class.SUPPORTED_RESOURCES:\n                logger.warning(\n                    \"No known supported resource available for %s, falling back to using CPU.\",\n                    runnable_class,\n                )\n\n            if runnable_class.SUPPORTS_CPU_MULTI_THREADING:\n                env = {\n                    \"BENTOML_NUM_THREAD\": str(workers_per_resource),\n                }\n                for env_name in THREAD_ENVS:\n                    env[env_name] = str(workers_per_resource)\n                return env\n\n            return {}\n\n        # this should not be reached by user since we always read system resource as default\n        raise ValueError(\n            f\"No known supported resource available for {runnable_class}. Please check your resource request. \"\n            \"Leaving it blank will allow BentoML to use system resources.\"\n        )", "idx": 1851}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.batches_to_batch", "completion": "        if len(batches) == 0:\n            return np.array([]), []\n\n        batch_sizes = [batch.shape[batch_dim] for batch in batches]\n        batch_size = sum(batch_sizes)\n        batch_shape = list(batches[0].shape)\n        batch_shape[batch_dim] = batch_size\n        batch = np.empty(batch_shape, dtype=batches[0].dtype)\n\n        indices = [0]\n        for i, batch_size in enumerate(batch_sizes):\n            indices.append(indices[-1] + batch_size)\n            batch_slice = [slice(None)] * len(batch_shape)\n            batch_slice[batch_dim] = slice(indices[-2], indices[-1])\n            batch[tuple(batch_slice)] = batches[i]\n\n        return batch, indices\n", "idx": 1852}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.to_payload", "completion": "        if batch.ndim > 0:\n            if not batch.flags[\"C_CONTIGUOUS\"] and not batch.flags[\"F_CONTIGUOUS\"]:\n                batch = np.ascontiguousarray(batch)\n            batch = pep574_dumps(batch)\n        else:\n            batch = pickle.dumps(batch)\n        return cls.create_payload(base64.b64encode(batch), batch_dim)\n", "idx": 1853}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.from_payload", "completion": "        if payload.meta[\"format\"] == \"pickle5\":\n            bs = base64.b64decode(payload.meta[\"pickle_bytes_str\"].encode(\"ascii\"))\n            return pep574_loads(bs, payload.data)\n        else:\n            return pickle.loads(payload.data)\n", "idx": 1854}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.batch_to_payloads", "completion": "        subbatches = np.split(batch, indices[1:-1], axis=batch_dim)\n        return [cls.to_payload(subbatch, batch_dim) for subbatch in subbatches]\n", "idx": 1855}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.from_batch_payloads", "completion": "        batches = [cls.from_payload(payload) for payload in payloads]\n        batch, indices = cls.batches_to_batch(batches, batch_dim)\n        return batch, indices\n\n", "idx": 1856}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.to_payload", "completion": "        if batch_dim != 0:\n            raise ValueError(\n                \"PandasDataFrameContainer does not support batch_dim other than 0\"\n            )\n\n        if isinstance(batch, ext.PdSeries):\n            batch = batch.to_frame()\n\n        bs: bytes\n        concat_buffer_bs: bytes\n        indices: list[int]\n        bs, concat_buffer_bs, indices = pep574_dumps(batch)\n        bs_str = base64.b64encode(bs).decode(\"ascii\")\n\n        meta = {\n            \"format\": \"pickle5\",\n            \"pickle_bytes_str\": bs_str,\n        }\n\n        if indices:\n            meta[\"with_buffer\"] = True\n            meta[\"indices\"] = indices\n            return cls.create_payload(concat_buffer_bs, batch.shape[batch_dim], meta)\n\n        meta[\"with_buffer\"] = False\n        return cls.create_payload(bs, batch.shape[batch_dim], meta)\n", "idx": 1857}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.from_payload", "completion": "        import pandas as pd\n\n        if payload.meta[\"with_buffer\"]:\n            bs_str = t.cast(str, payload.meta[\"pickle_bytes_str\"])\n            bs = base64.b64decode(bs_str)\n            indices = t.cast(t.List[int], payload.meta[\"indices\"])\n            return t.cast(\"ext.PdDataFrame\", pep574_loads(bs, payload.data, indices))\n\n        return pd.read_pickle(io.BytesIO(payload.data))\n", "idx": 1858}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batch_to_payloads", "completion": "        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n\n        batches = cls.batch_to_batches(batch, indices, batch_dim)\n\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads\n", "idx": 1859}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.from_batch_payloads", "completion": "        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)\n\n", "idx": 1860}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.to_payload", "completion": "        if isinstance(batch, Lazy):\n            batch = list(batch)\n\n        bs: bytes\n        concat_buffer_bs: bytes\n        indices: list[int]\n        bs, concat_buffer_bs, indices = pep574_dumps(batch)\n        bs_str = base64.b64encode(bs).decode(\"ascii\")\n\n        return cls.create_payload(\n            concat_buffer_bs,\n            len(batch),\n            {\n                \"format\": \"pickle5\",\n                \"pickle_bytes_str\": bs_str,\n                \"indices\": indices,\n            },\n        )\n", "idx": 1861}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batch_to_payloads", "completion": "        batches = cls.batch_to_batches(batch, indices, batch_dim)\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads\n", "idx": 1862}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.from_batch_payloads", "completion": "        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)\n\n", "idx": 1863}
{"namespace": "sslyze.cli.server_string_parser.CommandLineServerStringParser.parse_server_string", "completion": "        if \"{\" in server_str:\n            ip = server_str.split(\"{\")[1].split(\"}\")[0]\n            server_str = server_str.split(\"{\")[0]\n        else:\n            ip = None\n\n        # Extract port from target\n        if \":\" in server_str:\n            port = int(server_str.split(\":\")[1])\n            server_str = server_str.split(\":\")[0]\n        else:\n            port = None\n\n        # Extract host from target\n        if \"[\" in server_str:\n            host = cls._parse_ipv6_server_string(server_str)\n        elif ip and \"[\" in ip:\n            host = cls._parse_ipv6_ip_address(ip)\n        else:\n            host = cls._parse_ipv4_server_string(server_str)\n\n        return host, ip, port\n", "idx": 1864}
{"namespace": "sslyze.plugins.heartbleed_plugin._HeartbleedCliConnector.result_to_console_output", "completion": "        return [\n            \"Heartbleed scan result:\",\n            f\"  Is vulnerable to Heartbleed: {result.is_vulnerable_to_heartbleed}\",\n        ]\n\n", "idx": 1865}
{"namespace": "sslyze.plugins.http_headers_plugin._HttpHeadersCliConnector.result_to_console_output", "completion": "        output = []\n        if result.http_error_trace:\n            output.append(f\"The server returned an error after sending the initial HTTP request: {result.http_error_trace}\")\n            return output\n\n        if result.http_path_redirected_to:\n            output.append(f\"The server redirected the initial HTTP request to {result.http_path_redirected_to}\")\n\n        if result.strict_transport_security_header:\n            sts_header = result.strict_transport_security_header\n            output.append(f\"The server returned a Strict-Transport-Security header with the following values:\")\n            if sts_header.max_age:\n                output.append(f\"  - max-age: {sts_header.max_age}\")\n            if sts_header.preload:\n                output.append(f\"  - preload: True\")\n            if sts_header.include_subdomains:\n                output.append(f\"  - includeSubDomains: True\")\n\n        return output\n\n", "idx": 1866}
{"namespace": "sslyze.plugins.http_headers_plugin._detect_http_redirection", "completion": "    # Check if the response is a redirection\n    if http_response.status in [301, 302, 303, 307, 308]:\n        # Check if the redirection is to the same server\n        location_header = http_response.getheader(\"Location\")\n        if location_header:\n            # Parse the location header\n            parsed_location = urlsplit(location_header)\n            if parsed_location.netloc == server_host_name:\n                # If the location header is to the same server, return the path\n                return parsed_location.path\n\n    # If no redirection to the same server is found, return None\n    return None\n\n", "idx": 1867}
{"namespace": "sslyze.plugins.session_renegotiation_plugin._SessionRenegotiationCliConnector.result_to_console_output", "completion": "        result_txt = []\n        if result.is_vulnerable_to_client_renegotiation_dos:\n            result_txt.append(\"The server is vulnerable to client-initiated renegotiation DoS.\")\n        else:\n            result_txt.append(\"The server is not vulnerable to client-initiated renegotiation DoS.\")\n        if result.supports_secure_renegotiation:\n            result_txt.append(\"The server supports secure renegotiation.\")\n        else:\n            result_txt.append(\"The server does not support secure renegotiation.\")\n        return result_txt\n\n", "idx": 1868}
{"namespace": "sslyze.plugins.certificate_info._cli_connector._CertificateInfoCliConnector.result_to_console_output", "completion": "        result_list = [\n            f\"Certificate Information for {result.hostname}\",\n            f\"Number of certificates detected: {len(result.certificate_deployments)}\",\n        ]\n\n        for certificate_deployment in result.certificate_deployments:\n            result_list.append(\n                f\"Certificate Deployment {certificate_deployment.certificate_index} - {certificate_deployment.certificate_type}\"\n            )\n            result_list.append(f\"Certificate Subject: {certificate_deployment.certificate_subject}\")\n            result_list.append(f\"Certificate Issuer: {certificate_deployment.certificate_issuer}\")\n            result_list.append(f\"Certificate Serial Number: {certificate_deployment.certificate_serial_number}\")\n            result_list.append(f\"Certificate Validity: {certificate_deployment.certificate_validity}\")\n            result_list.append(f\"Certificate Signature Algorithm: {certificate_deployment.certificate_signature_algorithm}\")\n            result_list.append(f\"Certificate Key Algorithm: {certificate_deployment.certificate_key_algorithm}\")\n            result_list.append(f\"Certificate Key Size: {certificate_deployment.certificate_key_size}\")\n            result_list.append(f\"Certificate Fingerprint: {certificate_deployment.certificate_fingerprint}\")\n            result_list.append(f\"Certificate Fingerprint Algorithm: {certificate_deployment.certificate_fingerprint_algorithm}\")\n            result_list.append(f\"Certificate Subject Alternative Name: {certificate_deployment.certificate_subject_alternative_name}\")\n            result_list.append(f\"Certificate OCSP URI: {certificate_deployment.certificate_ocsp_uri}\")\n            result_list.append(f\"Certificate OCSP Response Status: {certificate_deployment.certificate_ocsp_response_status}\")\n            result_list.append(f\"Certificate OCSP Response Signature Algorithm: {certificate_deployment.certificate_ocsp_response_signature_algorithm}\")\n            result_list.append(f\"Certificate OCSP Response Signature: {certificate_deployment.certificate_ocsp_response_signature}\")\n            result_list.append(f\"Certificate OCSP Response Validity: {certificate_deployment.certificate_ocsp_response_validity}\")\n            result_list.append(f\"Certificate OCSP Response Fingerprint: {certificate_deployment.certificate_ocsp_response_fingerprint}\")\n            result_list.append", "idx": 1869}
{"namespace": "sslyze.plugins.certificate_info._cli_connector._get_name_as_short_text", "completion": "    # Check if there is a common name (CN) in the name field\n    for name in name_field:\n        if name.oid == x509.NameOID.COMMON_NAME:\n            return name.value\n\n    # If there is no CN, return the entire name field as a string\n    return str(name_field)", "idx": 1870}
{"namespace": "sslyze.plugins.certificate_info._symantec.SymantecDistructTester.get_distrust_timeline", "completion": "        if not verified_certificate_chain:\n            return None\n\n        # Check the presence of blacklisted certificates in the chain.\n        for certificate in verified_certificate_chain:\n            if certificate.key in cls._CA_KEYS_BLACKLISTED:\n                return SymantecDistrustTimelineEnum.MARCH_2018\n\n        # Check the presence of whitelisted certificates in the chain.\n        for certificate in verified_certificate_chain:\n            if certificate.key in cls._CA_KEYS_WHITELISTED:\n                return SymantecDistrustTimelineEnum.SEPTEMBER_2018\n\n        return None\n\n", "idx": 1871}
{"namespace": "sslyze.plugins.certificate_info._certificate_utils.parse_subject_alternative_name_extension", "completion": "    try:\n        san_extension = certificate.extensions.get_extension_for_oid(ExtensionOID.SUBJECT_ALTERNATIVE_NAME)\n    except ExtensionNotFound:\n        return SubjectAlternativeNameExtension([], [])\n\n    san_extension_value = san_extension.value\n    if isinstance(san_extension_value, SubjectAlternativeName):\n        dns_names = [name.value for name in san_extension_value.get_values_for_type(DNSName)]\n        ip_addresses = [name.value for name in san_extension_value.get_values_for_type(IPAddress)]\n        return SubjectAlternativeNameExtension(dns_names, ip_addresses)\n    else:\n        raise ValueError(f\"Invalid SAN extension value: {san_extension_value}\")\n\n", "idx": 1872}
{"namespace": "sslyze.plugins.certificate_info._cert_chain_analyzer._certificate_matches_hostname", "completion": "    try:\n        names = certificate.subject.get_attributes_for_oid(cryptography.x509.oid.NameOID.COMMON_NAME)\n        names = [name.value for name in names]\n        names.extend(certificate.subject.get_attributes_for_oid(cryptography.x509.oid.NameOID.DNS_NAME))\n        names = [name.value for name in names]\n        names = [name.lower() for name in names]\n        names = {name.replace(\"*.\", \"\") for name in names}\n        match_hostname(certificate, server_hostname)\n        return True\n    except CertificateError:\n        return False", "idx": 1873}
