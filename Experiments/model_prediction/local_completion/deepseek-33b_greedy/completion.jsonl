{"namespace": "benedict.utils.type_util.is_json_serializable", "completion": "    return isinstance(val, (bool, dict, float, int, list, str, tuple, type(None)))\n\n", "idx": 0}
{"namespace": "feedparser.urls.convert_to_idn", "completion": "    url = urllib.parse.urlparse(url)\n    if url.scheme in ('http', 'https'):\n        try:\n            url.hostname.encode('ascii')\n        except UnicodeEncodeError:\n            url = url._replace(netloc=url.netloc.encode('idna').decode('ascii'))\n    return url.geturl()", "idx": 1}
{"namespace": "mistune.toc.add_toc_hook", "completion": "    if min_level < 1:\n        min_level = 1\n    if max_level > 6:\n        max_level = 6\n\n    md.toc_headings = []  # save it to markdown instance.\n\n    def _toc_hook(md, state):\n        if not state['md'].toc:  # toc disabled\n            return\n\n        heading = state['token']\n        if heading.tag[1] >= min_level and heading.tag[1] <= max_level:\n            html_tag = heading.tag\n            if heading_id:\n                id_value = heading_id(heading.text)\n            else:\n                id_value = striptags(heading.text)\n            md.toc_headings.append((id_value, heading.text, html_tag))\n\n    md.before_heading_hooks.append(_toc_hook)", "idx": 2}
{"namespace": "mistune.plugins.table.table_in_quote", "completion": "    md.block.rules.insert(md.block.rules.index('quote') + 1, 'table', TABLE_PATTERN, parse_table)\n    md.block.rules.insert(md.block.rules.index('quote') + 1, 'nptable', NP_TABLE_PATTERN, parse_nptable)\n\n", "idx": 3}
{"namespace": "mistune.plugins.table.table_in_list", "completion": "    md.block.insert_rule(md.block.list_rules, 'table', before='paragraph')\n    md.block.insert_rule(md.block.list_rules, 'nptable', before='paragraph')", "idx": 4}
{"namespace": "xmnlp.utils.parallel_handler", "completion": "    if not isinstance(texts, list):\n        raise ValueError(\"You should pass a list of texts\")\n\n    with futures.ThreadPoolExecutor(n_jobs) as executor:\n        futures_ = [executor.submit(partial(callback, text, **kwargs)) for text in texts]\n        for future in futures.as_completed(futures_):\n            yield future.result()", "idx": 5}
{"namespace": "parsel.utils.shorten", "completion": "    if width < 0:\n        raise ValueError(\"width must be equal or greater than 0\")\n    if width == 0:\n        return suffix\n    if len(text) <= width:\n        return text\n    if width < len(suffix):\n        return suffix[:width]\n    return text[: width - len(suffix)] + suffix", "idx": 6}
{"namespace": "parsel.xpathfuncs.set_xpathfunc", "completion": "    if func is None:\n        etree.FunctionNamespace(None).pop(fname, None)\n    else:\n        etree.FunctionNamespace(None)[fname] = func\n\n", "idx": 7}
{"namespace": "dominate.dom_tag._get_thread_context", "completion": "  context = [threading.current_thread()]\n  if greenlet is not None:\n    context.append(greenlet.getcurrent())\n  return hash(tuple(context))\n\n", "idx": 8}
{"namespace": "dominate.util.system", "completion": "  import subprocess\n  p = subprocess.Popen(cmd, shell=True, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n  out, err = p.communicate(data)\n  return out.decode()\n\n", "idx": 9}
{"namespace": "dominate.util.url_unescape", "completion": "  return unescape(data)", "idx": 10}
{"namespace": "rows.fields.DatetimeField.serialize", "completion": "        if value is None:\n            return \"\"\n\n        return six.text_type(value.isoformat())\n", "idx": 11}
{"namespace": "rows.fields.Field.serialize", "completion": "        if value is None:\n            return None\n        if isinstance(value, six.text_type):\n            return value\n        if isinstance(value, six.binary_type):\n            return value.decode(\"utf-8\")\n        if isinstance(value, (int, float, Decimal)):\n            return six.text_type(value)\n        if isinstance(value, datetime.datetime):\n            return value.isoformat()\n        if isinstance(value, datetime.date):\n            return value.isoformat()\n        if isinstance(value, bool):\n            return six.text_type(value)\n        if isinstance(value, (list, tuple, set)):\n            return json.dumps(value)\n        if isinstance(value, dict):\n            return json.dumps(value)\n        if isinstance(value, six.string_types):\n            return value\n        if isinstance(value, (bytes, bytearray)):\n            return b64encode(value).decode(\"utf-8\")\n        if isinstance(value, Field):\n            return value.serialize()\n        value_error(value, cls)\n", "idx": 12}
{"namespace": "rows.fields.EmailField.serialize", "completion": "        if value is None:\n            return \"\"\n        else:\n            return six.text_type(value)\n", "idx": 13}
{"namespace": "rows.fields.as_string", "completion": "    if isinstance(value, six.text_type):\n        return value\n    elif isinstance(value, six.binary_type):\n        raise ValueError(\"It's binary, not string\")\n    else:\n        return six.text_type(value)\n\n", "idx": 14}
{"namespace": "rows.fields.get_items", "completion": "    return lambda obj: tuple(obj.get(index, None) for index in indexes)", "idx": 15}
{"namespace": "pycorrector.proper_corrector.load_dict_file", "completion": "    words = {}\n    if path and os.path.exists(path):\n        with open(path, 'r', encoding='utf-8') as f:\n            for w in f:\n                w = w.strip()\n                if w.startswith('#'):\n                    continue\n                if w:\n                    ws = w.split()\n                    if len(ws) == 2:\n                        words[ws[0]] = ws[1]\n    return words\n\n", "idx": 16}
{"namespace": "natasha.span.envelop_spans", "completion": "    for envelope in envelopes:\n        chunk = []\n        for span in spans:\n            if span.start >= envelope.start and span.stop <= envelope.stop:\n                chunk.append(span)\n            elif span.start > envelope.stop:\n                break\n        yield chunk", "idx": 17}
{"namespace": "googleapiclient._helpers.parse_unique_urlencoded", "completion": "    # Parse the URL-encoded content\n    parsed_content = urllib.parse.parse_qs(content)\n\n    # Check for repeated keys\n    for key in parsed_content:\n        if len(parsed_content[key]) > 1:\n            raise ValueError(f\"Repeated key '{key}' in URL-encoded content\")\n\n    # Return the parsed content as a dictionary\n    return {key: parsed_content[key][0] for key in parsed_content}", "idx": 18}
{"namespace": "jinja2.async_utils.auto_aiter", "completion": "    if hasattr(iterable, \"__aiter__\"):\n        async for item in t.cast(\"t.AsyncIterable[V]\", iterable):\n            yield item\n\n    else:\n        for item in t.cast(\"t.Iterable[V]\", iterable):\n            yield item", "idx": 19}
{"namespace": "jinja2.utils.consume", "completion": "    deque(iterable, maxlen=0)", "idx": 20}
{"namespace": "pycorrector.utils.tokenizer.segment", "completion": "    if pos:\n        if cut_type == 'word':\n            word_pos_seq = posseg.lcut(sentence)\n            word_seq, pos_seq = [], []\n            for w, p in word_pos_seq:\n                word_seq.append(w)\n                pos_seq.append(p)\n            return word_seq, pos_seq\n        elif cut_type == 'char':\n            word_seq = list(sentence)\n            pos_seq = []\n            for w in word_seq:\n                w_p = posseg.lcut(w)\n                pos_seq.append(w_p[0].flag)\n            return word_seq, pos_seq\n    else:\n        if cut_type == 'word':\n            return jieba.lcut(sentence)\n        elif cut_type == 'char':\n            return list(sentence)\n\n", "idx": 21}
{"namespace": "jinja2.utils.object_type_repr", "completion": "    if obj is None:\n        return \"None\"\n    if obj is Ellipsis:\n        return \"...\"\n    if isinstance(obj, bool):\n        return \"bool\"\n    if isinstance(obj, int):\n        return \"int\"\n    if isinstance(obj, float):\n        return \"float\"\n    if isinstance(obj, complex):\n        return \"complex\"\n    if isinstance(obj, str):\n        return \"str\"\n    if isinstance(obj, bytes):\n        return \"bytes\"\n    if isinstance(obj, bytearray):\n        return \"bytearray\"\n    if isinstance(obj, memoryview):\n        return \"memoryview\"\n    if isinstance(obj, range):\n        return \"range\"\n    if isinstance(obj, set):\n        return \"set\"\n    if isinstance(obj, frozenset):\n        return \"frozenset\"\n    if isinstance(obj, deque):\n        return \"deque\"\n    if isinstance(obj, abc.Mapping):\n        return \"dict\"\n    if isinstance(obj, abc.MutableMapping):\n        return \"dict\"\n    if isinstance(obj, abc.MutableSequence):\n        return \"list\"\n    if isinstance(obj, abc.Sequence):\n        return \"tuple\"\n    if isinstance(obj, abc.ByteString):\n        return \"bytes\"\n    if isinstance(obj, abc.Set):\n        return \"set\"\n    if isinstance(obj, abc.FrozenSet):\n        return \"frozenset\"\n    if isinstance(obj, abc.MappingView):\n        return \"dict_keys\"\n    if isinstance(obj, abc.ItemsView):\n        return \"dict_items\"\n    if isinstance(obj, abc.ValuesView):\n        return \"dict_values\"\n    if isinstance(obj, abc.Awaitable):\n        return \"coroutine\"\n    if isinstance(obj, abc.Coroutine):\n        return \"coroutine\"\n    if isinstance(obj, abc.AsyncIter", "idx": 22}
{"namespace": "jinja2.utils.LRUCache.setdefault", "completion": "        try:\n            return self[key]\n        except KeyError:\n            self[key] = default\n            return default\n", "idx": 23}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_word_freq", "completion": "        word_freq = {}\n        for w in list_of_words:\n            word_freq[w] = word_freq.get(w, 0) + 1\n        return word_freq\n", "idx": 24}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_average_probability_of_words", "completion": "        content_words_count = len(content_words_in_sentence)\n        if content_words_count == 0:\n            return 0\n        content_words_freq = self._compute_word_freq(content_words_in_sentence)\n        content_word_tf = dict((k, v / content_words_count) for (k, v) in content_words_freq.items())\n        content_word_prob = dict((k, word_freq_in_doc.get(k, 0) / content_words_count) for k in content_words_in_sentence)\n        average_prob = sum(content_word_prob.values()) / content_words_count\n        return average_prob\n", "idx": 25}
{"namespace": "sumy.summarizers.lex_rank.LexRankSummarizer._compute_idf", "completion": "        idf_metrics = {}\n        sentences_count = len(sentences)\n\n        for sentence in sentences:\n            for term in sentence:\n                if term not in idf_metrics:\n                    n_j = sum(1 for s in sentences if term in s)\n                    idf_metrics[term] = math.log(sentences_count / (1 + n_j))\n\n        return idf_metrics\n", "idx": 26}
{"namespace": "sumy.summarizers.lex_rank.LexRankSummarizer.cosine_similarity", "completion": "        unique_words1 = frozenset(sentence1)\n        unique_words2 = frozenset(sentence2)\n\n        if not (unique_words1 and unique_words2):\n            return 0.0\n\n        # We create vector of unique words for each sentence.\n        # We assume that a unique words vector is as long as the sentence, where coefficients\n        # are computed as sentence's term frequencies.\n        # Later, this vector is normalized and can be treated as a vector of sentence's features.\n        # compute tf*idf metrics\n        weights1 = [tf1[term] * idf_metrics.get(term, 0.0) for term in unique_words1]\n        weights2 = [tf2[term] * idf_metrics.get(term, 0.0) for term in unique_words2]\n\n        # compute cosine similarity\n        dot_product = numpy.dot(weights1, weights2)\n        norm1 = numpy.linalg.norm(weights1)\n        norm2 = numpy.linalg.norm(weights2)\n\n        if norm1 and norm2:\n            return dot_product / (norm1 * norm2)\n        else:\n            return 0.0\n", "idx": 27}
{"namespace": "sumy.evaluation.rouge._get_ngrams", "completion": "    ngram_set = set()\n    text_length = len(text)\n    max_index_ngram_start = text_length - n\n    for i in range(max_index_ngram_start + 1):\n        ngram_set.add(tuple(text[i:i + n]))\n    return ngram_set\n\n", "idx": 28}
{"namespace": "sumy.evaluation.rouge._split_into_words", "completion": "    words = []\n    for sentence in sentences:\n        if not isinstance(sentence, Sentence):\n            raise ValueError(\"Object in collection must be of type Sentence\")\n        words.extend(sentence.words)\n    return words\n\n", "idx": 29}
{"namespace": "falcon.inspect.register_router", "completion": "    if router_class in _supported_routers:\n        raise ValueError(\n            'Router class {} already registered'.format(router_class)\n        )\n\n    def decorator(func):\n        _supported_routers[router_class] = func\n        return func\n\n    return decorator", "idx": 30}
{"namespace": "falcon.inspect.inspect_compiled_router", "completion": "    routes = []\n    for uri_template, method_map, resource, parameters in router._router._roots:\n        for method, fn in method_map.items():\n            source_info, name = _get_source_info_and_name(fn)\n            info = RouteInfo(\n                method,\n                name,\n                uri_template,\n                source_info,\n                parameters,\n                _is_internal(fn),\n                _get_resource_name(resource),\n            )\n            routes.append(info)\n    return routes\n\n", "idx": 31}
{"namespace": "falcon.inspect._is_internal", "completion": "    module = inspect.getmodule(obj)\n    if module is None:\n        return False\n    return module.__name__.startswith('falcon')\n\n", "idx": 32}
{"namespace": "falcon.cmd.inspect_app.load_app", "completion": "    try:\n        module_name, app_name = args.app_module.split(':')\n    except ValueError:\n        parser.error('The app_module must be in the form of module:instance')\n\n    try:\n        module = importlib.import_module(module_name)\n    except ImportError:\n        parser.error('Could not import module %s' % module_name)\n\n    try:\n        app = getattr(module, app_name)\n    except AttributeError:\n        parser.error('Could not find %s in module %s' % (app_name, module_name))\n\n    if not isinstance(app, falcon.App):\n        try:\n            app = app()\n        except Exception:\n            parser.error('Could not create an instance of %s' % app_name)\n\n    if not isinstance(app, falcon.App):\n        parser.error('%s is not an instance of falcon.App' % app_name)\n\n    return app\n\n", "idx": 33}
{"namespace": "falcon.cmd.inspect_app.make_parser", "completion": "    parser = argparse.ArgumentParser(\n        description='Inspects a Falcon app\\'s routing table.',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument(\n        '-r', '--router',\n        action='store_true',\n        help='Print the router\\'s tree.')\n\n    parser.add_argument(\n        '-v', '--verbose',\n        action='store_true',\n        help='Print the route\\'s method, path, and class.')\n\n    parser.add_argument(\n        '-i', '--internal',\n        action='store_true',\n        help='Include internal/hidden routes.')\n\n    parser.add_argument(\n        'app_module',\n        help='The Python module path to the Falcon app, e.g. \"my_app.app\".')\n\n    return parser\n\n", "idx": 34}
{"namespace": "falcon.util.uri.unquote_string", "completion": "    if not isinstance(quoted, str):\n        raise TypeError(\"Input must be a string\")\n\n    if len(quoted) < 2:\n        return quoted\n\n    if quoted[0] != '\"' or quoted[-1] != '\"':\n        return quoted\n\n    unquoted = \"\"\n    for i in range(1, len(quoted) - 1):\n        if quoted[i] == \"\\\\\":\n            if quoted[i + 1] == \"\\\\\":\n                unquoted += \"\\\\\"\n                i += 1\n            elif quoted[i + 1] == '\"':\n                unquoted += '\"'\n                i += 1\n            elif quoted[i + 1] == \"n\":\n                unquoted += \"\\n\"\n                i += 1\n            elif quoted[i + 1] == \"r\":\n                unquoted += \"\\r\"\n                i += 1\n            elif quoted[i + 1] == \"t\":\n                unquoted += \"\\t\"\n                i += 1\n            else:\n                unquoted += quoted[i]\n        else:\n            unquoted += quoted[i]\n\n    return unquoted", "idx": 35}
{"namespace": "falcon.util.misc.get_argnames", "completion": "    argspec = inspect.getfullargspec(func)\n    return argspec.args + argspec.kwonlyargs\n\n", "idx": 36}
{"namespace": "falcon.testing.client._is_asgi_app", "completion": "    return len(inspect.signature(app).parameters) == 3", "idx": 37}
{"namespace": "falcon.routing.converters.UUIDConverter.convert", "completion": "        try:\n            return uuid.UUID(value)\n        except ValueError:\n            return None", "idx": 38}
{"namespace": "rest_framework_simplejwt.utils.make_utc", "completion": "    if settings.USE_TZ and is_naive(dt):\n        return make_aware(dt, timezone=timezone.utc)\n    return dt\n\n", "idx": 39}
{"namespace": "boto.sdb.db.sequence.fib", "completion": "    if cv is None:\n        return 1\n    return cv + lv\n", "idx": 40}
{"namespace": "boto.s3.website.RoutingRules.add_rule", "completion": "        self.append(rule)\n        return self\n", "idx": 41}
{"namespace": "boto.cloudfront.distribution.Distribution._canned_policy", "completion": "        return '{\"Statement\":[{\"Resource\":\"%(resource)s\",\"Condition\":{\"DateLessThan\":{\"AWS:EpochTime\":%(expires)s}}}]}' % {'resource': resource, 'expires': expires}\n", "idx": 42}
{"namespace": "boto.cloudfront.invalidation.InvalidationBatch.escape", "completion": "        if p[0] != '/':\n            p = '/' + p\n        return urllib.quote(p, safe='/')\n", "idx": 43}
{"namespace": "proxybroker.utils.get_status_code", "completion": "    try:\n        return int(resp[start:stop])\n    except ValueError:\n        raise BadStatusLine(resp)", "idx": 44}
{"namespace": "authlib.oauth2.rfc6749.util.scope_to_list", "completion": "    if isinstance(scope, (tuple, list, set)):\n        return [to_unicode(s) for s in scope]\n    if scope is None:\n        return None\n    return scope.strip().split()\n\n", "idx": 45}
{"namespace": "authlib.common.encoding.to_unicode", "completion": "    if x is None or isinstance(x, str):\n        return x\n    if isinstance(x, bytes):\n        return x.decode(charset, errors)\n    return str(x)\n\n", "idx": 46}
{"namespace": "authlib.common.encoding.to_bytes", "completion": "    if x is None:\n        return None\n    if isinstance(x, bytes):\n        return x\n    if isinstance(x, str):\n        return x.encode(charset, errors)\n    if isinstance(x, int):\n        return str(x).encode(charset, errors)\n    if isinstance(x, float):\n        return str(x).encode(charset, errors)\n    raise TypeError('Expected bytes, str, int, or float')\n\n", "idx": 47}
{"namespace": "authlib.common.encoding.urlsafe_b64decode", "completion": "    s = to_bytes(s)\n    s = s.rstrip(b'\\n')\n    s = s.rstrip(b'=')\n    s = s.replace(b'-', b'+')\n    s = s.replace(b'_', b'/')\n    return base64.b64decode(s)\n\n", "idx": 48}
{"namespace": "csvs_to_sqlite.utils.table_exists", "completion": "    sql = \"\"\"\n        SELECT name\n        FROM sqlite_master\n        WHERE type='table'\n        AND name=?\n    \"\"\"\n    result = conn.execute(sql, (table,)).fetchall()\n    return bool(result)", "idx": 49}
{"namespace": "sqlitedict.SqliteDict.get_tablenames", "completion": "        if not os.path.exists(filename):\n            raise IOError('file {} does not exist'.format(filename))\n        conn = sqlite3.connect(filename)\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n        tables = [row[0] for row in cursor.fetchall()]\n        conn.close()\n        return tables\n", "idx": 50}
{"namespace": "litecli.packages.parseutils.query_starts_with", "completion": "    prefixes = [prefix.lower() for prefix in prefixes]\n    formatted_query = sqlparse.format(query, strip_comments=True).strip().lower()\n    if not formatted_query:\n        return False\n    first_word = formatted_query.split()[0]\n    return first_word in prefixes", "idx": 51}
{"namespace": "rest_framework.negotiation.DefaultContentNegotiation.filter_renderers", "completion": "        filtered_renderers = []\n        for renderer in renderers:\n            if renderer.format == format:\n                filtered_renderers.append(renderer)\n        if not filtered_renderers:\n            raise Http404\n        return filtered_renderers\n", "idx": 52}
{"namespace": "rest_framework.templatetags.rest_framework.as_string", "completion": "    if value is None:\n        return ''\n    return str(value)\n\n", "idx": 53}
{"namespace": "rest_framework.templatetags.rest_framework.add_nested_class", "completion": "    if isinstance(value, dict):\n        return 'class=nested'\n    elif isinstance(value, list):\n        for item in value:\n            if isinstance(item, dict) or isinstance(item, list):\n                return 'class=nested'\n    return ''\n\n", "idx": 54}
{"namespace": "pyramid.session.PickleSerializer.loads", "completion": "        return pickle.loads(bstruct)\n", "idx": 55}
{"namespace": "pyramid.testing.DummySession.flash", "completion": "        if allow_duplicate:\n            self.setdefault(queue, []).append(msg)\n        elif msg not in self.get(queue, []):\n            self.setdefault(queue, []).append(msg)\n", "idx": 56}
{"namespace": "pyramid.testing.DummySession.pop_flash", "completion": "        storage = self.setdefault('_f_' + queue, [])\n        return storage.pop()", "idx": 57}
{"namespace": "pyramid.testing.DummySession.peek_flash", "completion": "        storage = self.get('_f_' + queue, [])\n        return storage", "idx": 58}
{"namespace": "pyramid.testing.DummySession.new_csrf_token", "completion": "        token = '0123456789012345678901234567890123456789'\n        self['_csrf'] = token\n        return token\n", "idx": 59}
{"namespace": "pyramid.view.view_defaults", "completion": "    def decorator(cls):\n        cls.__view_defaults__ = settings\n        return cls", "idx": 60}
{"namespace": "pyramid.util.bytes_", "completion": "    if isinstance(s, str):\n        return s.encode(encoding, errors)\n    return s", "idx": 61}
{"namespace": "pyramid.scripts.common.parse_vars", "completion": "    # Create an empty dictionary\n    d = {}\n\n    # Loop through the list of strings\n    for arg in args:\n\n        # Split the string into a key and value\n        key, value = arg.split('=')\n\n        # Add the key and value to the dictionary\n        d[key] = value\n\n    # Return the dictionary\n    return d\n\n", "idx": 62}
{"namespace": "pyramid.scripts.pviews.PViewsCommand._find_multi_routes", "completion": "        infos = []\n        for route in mapper.get_routes():\n            match = route.match(request.path)\n            if match is not None:\n                infos.append({'match': match, 'route': route})\n        return infos\n", "idx": 63}
{"namespace": "pyramid.scripts.pserve.PServeCommand.guess_server_url", "completion": "        if not server_name:\n            server_name = 'main'\n        settings = loader.get_settings('server:' + server_name, global_conf)\n        port = settings.get('port')\n        if port:\n            return 'http://127.0.0.1:%s' % port\n\n        host = settings.get('host')\n        if host:\n            return 'http://%s' % host\n\n        return None\n", "idx": 64}
{"namespace": "aiohappybase._util.pep8_to_camel_case", "completion": "    chunks = name.split('_')\n    if not initial:\n        chunks[0] = chunks[0].lower()\n    return ''.join(chunks)\n\n", "idx": 65}
{"namespace": "aiohappybase._util.bytes_increment", "completion": "    assert isinstance(b, bytes), \"expected bytes, got %r\" % type(b)\n    b = bytearray(b)\n    for i in range(len(b) - 1, -1, -1):\n        if b[i] != 0xff:\n            b[i] += 1\n            break\n        b[i] = 0\n    else:\n        return None\n    return bytes(b)\n\n", "idx": 66}
{"namespace": "mssqlcli.config.ensure_dir_exists", "completion": "    if not os.path.exists(os.path.dirname(path)):\n        os.makedirs(os.path.dirname(path))", "idx": 67}
{"namespace": "mssqlcli.telemetry._user_id_file_is_old", "completion": "    if not os.path.exists(id_file_path):\n        return True\n\n    file_modified_time = os.path.getmtime(id_file_path)\n    current_time = datetime.now().timestamp()\n    time_difference = current_time - file_modified_time\n    return time_difference > 24 * 60 * 60\n\n", "idx": 68}
{"namespace": "mssqlcli.util.is_command_valid", "completion": "    try:\n        subprocess.check_call(command, stdout=devnull, stderr=devnull)\n    except OSError:\n        return False\n    return True", "idx": 69}
{"namespace": "mssqlcli.packages.parseutils.utils.find_prev_keyword", "completion": "    # Parse the SQL statement\n    parsed = sqlparse.parse(sql)\n\n    # Get the tokens from the parsed statement\n    tokens = parsed[0].tokens\n\n    # Reverse the tokens\n    tokens.reverse()\n\n    # Initialize the keyword and text variables\n    keyword = None\n    text = None\n\n    # Iterate over the tokens\n    for i, token in enumerate(tokens):\n        # If the token is a keyword\n        if token.ttype is Token.Keyword:\n            # If we've skipped the required number of tokens\n            if i > n_skip:\n                # Set the keyword and text variables\n                keyword = token.value\n                text = sql[:-len(token.value)]\n                # Break out of the loop\n                break\n\n    # Return the keyword and text\n    return keyword, text\n\n", "idx": 70}
{"namespace": "trafilatura.settings.use_config", "completion": "    if config is None:\n        config = ConfigParser(allow_no_value=True, inline_comment_prefixes=('#', ';'))\n        if filename is None:\n            filename = Path(__file__).parent / 'settings.cfg'\n        config.read(filename)\n    return config\n\n", "idx": 71}
{"namespace": "pyramid.util.text_", "completion": "    if isinstance(s, bytes):\n        return s.decode(encoding, errors)\n    return s", "idx": 72}
{"namespace": "trafilatura.downloads._parse_config", "completion": "    user_agents = config.get('http', 'user_agents', fallback=DEFAULT_CONFIG['http']['user_agents'])\n    user_agents = [ua.strip() for ua in user_agents.split('\\n') if ua.strip()]\n    cookies = config.get('http', 'cookies', fallback=DEFAULT_CONFIG['http']['cookies'])\n    return user_agents, cookies\n\n", "idx": 73}
{"namespace": "trafilatura.downloads.load_download_buffer", "completion": "    # get the next URLs to download\n    bufferlist = url_store.get_urls()\n    # if the bufferlist is empty, sleep for a while\n    if not bufferlist:\n        sleep(sleep_time)\n    return bufferlist, url_store", "idx": 74}
{"namespace": "trafilatura.metadata.check_authors", "completion": "    author_blacklist = {author.lower() for author in author_blacklist}\n    authors = authors.split(';')\n    new_authors = []\n    for author in authors:\n        if author.lower() not in author_blacklist:\n            new_authors.append(author)\n    if new_authors:\n        return '; '.join(new_authors)\n    return None\n\n", "idx": 75}
{"namespace": "datasette.filters.where_filters", "completion": "    where_clauses = []\n    extra_wheres_for_ui = []\n    if \"_where\" in request.args:\n        if not database.execute_wrapper.user_can(\"execute-sql\"):\n            raise DatasetteError(\n                \"execute-sql permission required to use _where\", status=403\n            )\n        for value in request.args.getlist(\"_where\"):\n            where_clauses.append(value)\n            if detect_json1(value):\n                value = escape_sqlite(value)\n                extra_wheres_for_ui.append(\n                    {\n                        \"value\": value,\n                        \"label\": value,\n                        \"sql\": \"SELECT * FROM $table WHERE {}\".format(value),\n                    }\n                )\n    if where_clauses:\n\n        def inner(columns, index):\n            return FilterArguments(\n                where_clauses, extra_wheres_for_ui, columns, index, datasette\n            )\n\n        return inner\n\n", "idx": 76}
{"namespace": "datasette.utils.path_with_added_args", "completion": "    if path is None:\n        path = request.path\n    if not args:\n        return path\n    args_encoded = urllib.parse.urlencode(args)\n    if \"?\" in path:\n        return f\"{path}&{args_encoded}\"\n    else:\n        return f\"{path}?{args_encoded}\"\n\n", "idx": 77}
{"namespace": "datasette.utils.path_with_replaced_args", "completion": "    query_string = request.query_string\n    if path is None:\n        path = request.path\n    else:\n        if \"?\" in path:\n            bits = path.split(\"?\", 1)\n            path, query_string = bits\n    # args can be a dict or a set\n    if isinstance(args, dict):\n        args = args.items()\n    current = []\n    for key, value in urllib.parse.parse_qsl(query_string):\n        if key not in args:\n            current.append((key, value))\n    current.extend([(key, value) for key, value in args if value is not None])\n    query_string = urllib.parse.urlencode(current)\n    if query_string:\n        query_string = f\"?{query_string}\"\n    return path + query_string\n\n", "idx": 78}
{"namespace": "datasette.utils.format_bytes", "completion": "    if bytes < 1024:\n        return f\"{bytes} bytes\"\n    elif bytes < 1024 * 1024:\n        return f\"{bytes / 1024:.2f} KB\"\n    elif bytes < 1024 * 1024 * 1024:\n        return f\"{bytes / (1024 * 1024):.2f} MB\"\n    elif bytes < 1024 * 1024 * 1024 * 1024:\n        return f\"{bytes / (1024 * 1024 * 1024):.2f} GB\"\n    else:\n        return f\"{bytes / (1024 * 1024 * 1024 * 1024):.2f} TB\"", "idx": 79}
{"namespace": "datasette.utils.actor_matches_allow", "completion": "    if allow is None:\n        return True\n    if isinstance(allow, str):\n        return actor == allow\n    if isinstance(allow, list):\n        return actor in allow\n    if isinstance(allow, dict):\n        for key, value in allow.items():\n            if key not in actor:\n                return False\n            if actor[key] != value:\n                return False\n        return True\n    return False", "idx": 80}
{"namespace": "datasette.utils.resolve_env_secrets", "completion": "    if isinstance(config, dict):\n        for key, value in config.items():\n            if isinstance(value, dict):\n                config[key] = resolve_env_secrets(value, environ)\n            elif isinstance(value, list):\n                config[key] = [resolve_env_secrets(item, environ) for item in value]\n            elif isinstance(value, str):\n                if value.startswith(\"$env:\"):\n                    env_key = value.split(\"$env:\", 1)[1]\n                    config[key] = environ.get(env_key)\n                elif value.startswith(\"$file:\"):\n                    file_name = value.split(\"$file:\", 1)[1]\n                    with open(file_name, \"r\") as file:\n                        config[key] = file.read()\n    elif isinstance(config, list):\n        for index, item in enumerate(config):\n            if isinstance(item, dict):\n                config[index] = resolve_env_secrets(item, environ)\n            elif isinstance(item, list):\n                config[index] = [resolve_env_secrets(subitem, environ) for subitem in item]\n            elif isinstance(item, str):\n                if item.startswith(\"$env:\"):\n                    env_key = item.split(\"$env:\", 1)[1]\n                    config[index] = environ.get(env_key)\n                elif item.startswith(\"$file:\"):\n                    file_name = item.split(\"$file:\", 1)[1]\n                    with open(file_name, \"r\") as file:\n                        config[index] = file.read()\n    return config", "idx": 81}
{"namespace": "datasette.utils.display_actor", "completion": "    if actor is None:\n        return \"anonymous\"\n    if actor.get(\"display_name\"):\n        return actor[\"display_name\"]\n    if actor.get(\"name\"):\n        return actor[\"name\"]\n    if actor.get(\"username\"):\n        return actor[\"username\"]\n    if actor.get(\"login\"):\n        return actor[\"login\"]\n    if actor.get(\"id\"):\n        return actor[\"id\"]\n    return str(actor)", "idx": 82}
{"namespace": "datasette.utils.initial_path_for_datasette", "completion": "    # Get the list of databases\n    databases = await datasette.get_databases()\n\n    # If there is only one database, return the path to that database\n    if len(databases) == 1:\n        return databases[0][\"path\"]\n\n    # If there are multiple databases, check if any of them contain only one table\n    for database in databases:\n        tables = await datasette.get_database(database[\"path\"]).get_tables()\n        if len(tables) == 1:\n            return tables[0][\"path\"]\n\n    # If no database contains only one table, return the path to the instance\n    return datasette.instance_path\n\n", "idx": 83}
{"namespace": "datasette.utils.tilde_decode", "completion": "    # Replace % with a temporary string to avoid accidental decoding of %2f style sequences\n    s = s.replace(\"%\", \"~PERCENT~\")\n    # Decode the tilde-encoded string\n    s = urllib.parse.unquote(s)\n    # Replace the temporary string with %\n    s = s.replace(\"~PERCENT~\", \"%\")\n    return s\n\n", "idx": 84}
{"namespace": "datasette.utils.resolve_routes", "completion": "    for regex, view in routes:\n        match = regex.match(path)\n        if match:\n            return match, view\n    return None", "idx": 85}
{"namespace": "datasette.utils.truncate_url", "completion": "    if len(url) <= length:\n        return url\n\n    # Check if the URL ends with a file extension\n    file_extension = url.split(\".\")[-1]\n    if len(file_extension) > 0 and len(file_extension) <= 4 and \"/\" not in file_extension:\n        # Truncate the URL to the specified length minus the length of the file extension minus 1 (for the dot)\n        truncated_url = url[: length - len(file_extension) - 1]\n        # Add ellipsis and the file extension at the end\n        truncated_url += \"...\" + file_extension\n    else:\n        # Truncate the URL to the specified length minus 3 (for the ellipsis)\n        truncated_url = url[: length - 3]\n        # Add ellipsis at the end\n        truncated_url += \"...\"\n\n    return truncated_url", "idx": 86}
{"namespace": "kinto.core.authorization.groupfinder", "completion": "    if hasattr(request, \"registry\") and hasattr(request.registry, \"permission\"):\n        if not hasattr(request, \"_cache_principals\"):\n            request._cache_principals = request.registry.permission.get_user_principals(\n                user_id=userid\n            )\n        return request._cache_principals\n    return []\n\n", "idx": 87}
{"namespace": "kinto.core.utils.json.dumps", "completion": "        kw.setdefault(\"bytes_mode\", rapidjson.BM_NONE)\n        return rapidjson.dumps(v, **kw)\n", "idx": 88}
{"namespace": "kinto.core.utils.json.loads", "completion": "        kw.setdefault(\"number_mode\", rapidjson.NM_NATIVE)\n        return rapidjson.loads(v, **kw)", "idx": 89}
{"namespace": "kinto.core.utils.hmac_digest", "completion": "    if isinstance(secret, str):\n        secret = secret.encode(encoding)\n    return hmac.new(secret, message.encode(encoding), hashlib.sha256).hexdigest()\n\n", "idx": 90}
{"namespace": "kinto.core.utils.current_service", "completion": "    registry = request.registry\n    services = registry.cornice_services\n    mapper = registry.queryUtility(IRoutesMapper)\n    info = mapper(request)\n    if info is not None:\n        for service in services:\n            if info[\"route\"].pattern == service.pattern:\n                return service\n    return None", "idx": 91}
{"namespace": "kinto.core.utils.prefixed_principals", "completion": "    principals = request.effective_principals\n    if \"system.Authenticated\" not in principals:\n        return principals\n\n    # Remove the unprefixed user id from the effective principals\n    # to avoid conflicts\n    principals.remove(request.selected_userid)\n\n    # Add the prefixed user id to the beginning of the list\n    principals.insert(0, prefixed_userid(request))\n\n    return principals", "idx": 92}
{"namespace": "kinto.plugins.accounts.views.on_account_created", "completion": "    request = event.request\n    settings = request.registry.settings\n    validation_enabled = settings.get(\"account_validation\", False)\n\n    if not validation_enabled:\n        return\n\n    for obj in event.impacted_objects:\n        # Extract username and password from current user\n        username = obj[\"new\"][\"id\"]\n        activation_key = get_cached_validation_key(username, request.registry)\n\n        if activation_key is None:\n            continue\n\n        account_info = {\n            \"username\": username,\n            \"activation_key\": activation_key,\n        }\n\n        Emailer(request).send_activation(account_info)", "idx": 93}
{"namespace": "kinto.plugins.accounts.utils.hash_password", "completion": "    return str(bcrypt.hashpw(password.encode(\"utf-8\"), bcrypt.gensalt()))\n\n", "idx": 94}
{"namespace": "kinto.views.admin.get_parent_uri", "completion": "    if not object_uri.startswith(\"/\"):\n        return \"\"\n    path = object_uri.split(\"/\")\n    if len(path) < 3:\n        return \"\"\n    return \"/\".join(path[:2])", "idx": 95}
{"namespace": "alembic.script.write_hooks.register", "completion": "    def inner(func: Callable) -> Callable:\n        _registry[name] = func\n        return func\n\n    return inner\n\n", "idx": 96}
{"namespace": "mongo_connector.namespace_config.match_replace_regex", "completion": "    match = regex.match(src_namespace)\n    if match:\n        return dest_namespace.replace(\"*\", match.group(1))\n    return None\n\n", "idx": 97}
{"namespace": "mongo_connector.namespace_config.namespace_to_regex", "completion": "    db_name, collection_name = namespace.split(\".\", 1)\n    db_regex = re.compile(db_name.replace(\"*\", \"(.*)\"))\n    collection_regex = re.compile(collection_name.replace(\"*\", \"(.*)\"))\n    return re.compile(db_regex.pattern + \"\\\\.\" + collection_regex.pattern)", "idx": 98}
{"namespace": "mongo_connector.util.long_to_bson_ts", "completion": "    seconds = val >> 32\n    increment = val & 0xffffffff\n    return Timestamp(seconds, increment)", "idx": 99}
{"namespace": "mongo_connector.doc_managers.formatters.DocumentFlattener.format_document", "completion": "        def _kernel(doc):\n            for key in doc:\n                value = doc[key]\n                for new_k, new_v in self.transform_element(key, value):\n                    yield new_k, new_v\n\n        return dict(_kernel(document))", "idx": 100}
{"namespace": "bplustree.memory.open_file_in_dir", "completion": "    if platform.system() == \"Windows\":\n        return open(path, \"rb+\"), None\n    else:\n        dir_fd = os.open(os.path.dirname(path), os.O_RDONLY)\n        file_fd = os.open(path, os.O_RDWR | os.O_CREAT)\n        return os.fdopen(file_fd, \"rb+\"), dir_fd\n\n", "idx": 101}
{"namespace": "bplustree.memory.FileMemory.read_transaction", "completion": "        return ReadTransaction(self)\n", "idx": 102}
{"namespace": "bplustree.utils.pairwise", "completion": "    a, b = itertools.tee(iterable)\n    next(b, None)\n    return zip(a, b)\n\n", "idx": 103}
{"namespace": "bplustree.utils.iter_slice", "completion": "    it = iter(iterable)\n    while True:\n        chunk = tuple(itertools.islice(it, n))\n        if not chunk:\n            return\n        yield chunk, False\n    yield chunk, True", "idx": 104}
{"namespace": "bplustree.serializer.StrSerializer.serialize", "completion": "        data = obj.encode('utf-8')\n        assert len(data) <= key_size, 'String is too long'\n        return data\n", "idx": 105}
{"namespace": "psd_tools.utils.pack", "completion": "    return struct.pack(fmt, *args)\n\n", "idx": 106}
{"namespace": "psd_tools.utils.unpack", "completion": "    fmt = str(\">\" + fmt)\n    return struct.unpack(fmt, data)\n\n", "idx": 107}
{"namespace": "psd_tools.api.numpy_io.get_pattern", "completion": "    height, width = pattern.data[2:4]\n    pattern_array = np.zeros((height, width), dtype=np.uint8)\n    for channel in pattern.data[4:]:\n        pattern_array = pattern_array | _parse_array(channel.data, 1)\n    return pattern_array\n\n", "idx": 108}
{"namespace": "sqlite_utils.utils.maximize_csv_field_size_limit", "completion": "    max_size = sys.maxsize\n    while True:\n        try:\n            csv.field_size_limit(max_size)\n            break\n        except OverflowError:\n            max_size = int(max_size / 10)\n\n", "idx": 109}
{"namespace": "sqlite_utils.utils.column_affinity", "completion": "    if column_type is None:\n        return None\n    column_type = column_type.upper()\n    if column_type == \"INT\":\n        return \"INTEGER\"\n    if column_type == \"CHAR\":\n        return \"TEXT\"\n    if column_type == \"CLOB\":\n        return \"TEXT\"\n    if column_type == \"TEXT\":\n        return \"TEXT\"\n    if column_type == \"BLOB\":\n        return \"BLOB\"\n    if column_type == \"REAL\":\n        return \"REAL\"\n    if column_type == \"FLOA\":\n        return \"REAL\"\n    if column_type == \"DOUB\":\n        return \"REAL\"\n    return None", "idx": 110}
{"namespace": "sqlite_utils.utils.decode_base64_values", "completion": "    for key, value in doc.items():\n        if isinstance(value, dict) and value.get(\"$base64\"):\n            doc[key] = base64.b64decode(value[\"encoded\"])\n    return doc\n\n", "idx": 111}
{"namespace": "sqlite_utils.utils.chunks", "completion": "    it = iter(sequence)\n    while True:\n        chunk = tuple(itertools.islice(it, size))\n        if not chunk:\n            break\n        yield chunk", "idx": 112}
{"namespace": "sqlite_utils.utils.hash_record", "completion": "    if keys is None:\n        keys = record.keys()\n    hash_ = hashlib.sha1()\n    for key in sorted(keys):\n        hash_.update(key.encode(\"utf-8\"))\n        hash_.update(str(record[key]).encode(\"utf-8\"))\n    return hash_.hexdigest()", "idx": 113}
{"namespace": "arctic.decorators._get_host", "completion": "    if not store:\n        return {}\n\n    if isinstance(store, (list, tuple)):\n        store = store[0]\n\n    library_name = store.library_name\n    nodes = ','.join([\"%s:%s\" % (h, p) for h, p in store.arctic._conn.nodes])\n    host = store.arctic._conn.host\n\n    return {'library_name': library_name, 'nodes': nodes, 'host': host}\n\n", "idx": 114}
{"namespace": "arctic.decorators.mongo_retry", "completion": "    @wraps(f)\n    def wrapper(*args, **kwargs):\n        global _in_retry\n        global _retry_count\n        try:\n            return f(*args, **kwargs)\n        except (AutoReconnect, OperationFailure) as e:\n            if 'arctic' in sys.modules:\n                _log_exception(logger, \"MongoDB connection failure: %s\" % e)\n            if _in_retry:\n                raise\n            _in_retry = True\n            _retry_count += 1\n            if _retry_count > _MAX_RETRIES:\n                raise\n            sleep(0.1)\n            return wrapper(*args, **kwargs)\n        finally:\n            _in_retry = False\n\n    return wrapper\n\n", "idx": 115}
{"namespace": "arctic._util.are_equals", "completion": "    try:\n        if isinstance(o1, DataFrame) and isinstance(o2, DataFrame):\n            assert_frame_equal(o1, o2, **kwargs)\n            return True\n        else:\n            return o1 == o2\n    except Exception:\n        return False\n\n", "idx": 116}
{"namespace": "arctic.hooks.register_resolve_mongodb_hook", "completion": "    global _resolve_mongodb_hook\n    _resolve_mongodb_hook = hook\n\n", "idx": 117}
{"namespace": "arctic.hooks.register_log_exception_hook", "completion": "    global _log_exception_hook\n    _log_exception_hook = hook", "idx": 118}
{"namespace": "arctic.hooks.register_get_auth_hook", "completion": "    global _get_auth_hook\n    _get_auth_hook = hook\n\n", "idx": 119}
{"namespace": "arctic.store._version_store_utils._split_arrs", "completion": "    # Initialize an empty list to store the sub-arrays\n    sub_arrs = []\n\n    # Iterate over the slices\n    for i in range(len(slices)):\n        # If it's the first slice, start index is 0\n        if i == 0:\n            start_idx = 0\n        # Otherwise, start index is the end index of the previous slice\n        else:\n            start_idx = slices[i - 1]\n\n        # If it's the last slice, end index is the length of the array\n        if i == len(slices) - 1:\n            end_idx = len(array_2d)\n        # Otherwise, end index is the start index of the current slice\n        else:\n            end_idx = slices[i]\n\n        # Append the sub-array to the list\n        sub_arrs.append(array_2d[start_idx:end_idx])\n\n    # Return the list of sub-arrays\n    return sub_arrs\n\n", "idx": 120}
{"namespace": "arctic.store._version_store_utils.checksum", "completion": "    # Convert the symbol to bytes\n    symbol_bytes = symbol.encode('utf-8')\n\n    # Calculate the checksum using the SHA1 algorithm\n    checksum_bytes = hashlib.sha1(symbol_bytes).digest()\n\n    # Convert the checksum to a Binary object\n    checksum_binary = Binary(checksum_bytes)\n\n    return checksum_binary\n\n", "idx": 121}
{"namespace": "arctic.store.versioned_item.VersionedItem.__str__", "completion": "        return 'VersionedItem(symbol={symbol},library={library},data={data},version={version},metadata={metadata},host={host})'.format(\n            symbol=self.symbol,\n            library=self.library,\n            data=self.data,\n            version=self.version,\n            metadata=self.metadata,\n            host=self.host\n        )", "idx": 122}
{"namespace": "arctic.store._ndarray_store.NdarrayStore._dtype", "completion": "        if metadata is None:\n            metadata = {}\n        if string.startswith('['):\n            return np.dtype(string, metadata=metadata)\n        else:\n            return np.dtype(string)\n", "idx": 123}
{"namespace": "arctic.store._ndarray_store._promote_struct_dtypes", "completion": "    # Check if the fields of dtype1 are a superset of dtype2\n    if not all(field in dtype1.fields for field in dtype2.fields):\n        raise ValueError(\"The fields of dtype1 are not a superset of dtype2\")\n\n    # Promote the data types of the two structured arrays\n    promoted_dtype = []\n    for field in dtype1.fields:\n        if field in dtype2.fields:\n            if dtype1[field] == dtype2[field]:\n                promoted_dtype.append((field, dtype1[field]))\n            else:\n                promoted_dtype.append((field, np.promote_types(dtype1[field], dtype2[field])))\n        else:\n            promoted_dtype.append((field, dtype1[field]))\n\n    return np.dtype(promoted_dtype)\n\n", "idx": 124}
{"namespace": "arctic.chunkstore.passthrough_chunker.PassthroughChunker.exclude", "completion": "        if isinstance(data, DataFrame):\n            return DataFrame()\n        elif isinstance(data, Series):\n            return Series()\n        else:\n            raise ValueError(\"Invalid data type. Expected DataFrame or Series.\")", "idx": 125}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.to_chunks", "completion": "        if not isinstance(df.index, pd.DatetimeIndex):\n            raise ValueError(\"The dataframe/series must have a DatetimeIndex.\")\n\n        if not isinstance(chunk_size, str):\n            raise ValueError(\"The chunk size must be a string.\")\n\n        if func is None:\n            raise ValueError(\"The function must be provided.\")\n\n        if not callable(func):\n            raise ValueError(\"The function must be callable.\")\n\n        if not isinstance(df, pd.DataFrame) and not isinstance(df, pd.Series):\n            raise ValueError(\"The dataframe/series must be a pandas dataframe or series.\")\n\n        if not isinstance(chunk_size, str):\n            raise ValueError(\"The chunk size must be a string.\")\n\n        if not isinstance(func, callable):\n            raise ValueError(\"The function must be callable.\")\n\n        if not isinstance(df, pd.DataFrame) and not isinstance(df, pd.Series):\n            raise ValueError(\"The dataframe/series must be a pandas dataframe or series.\")\n\n        if not isinstance(chunk_size, str):\n            raise ValueError(\"The chunk size must be a string.\")\n\n        if not isinstance(func, callable):\n            raise ValueError(\"The function must be callable.\")\n\n        if not isinstance(df, pd.DataFrame) and not isinstance(df, pd.Series):\n            raise ValueError(\"The dataframe/series must be a pandas dataframe or series.\")\n\n        if not isinstance(chunk_size, str):\n            raise ValueError(\"The chunk size must be a string.\")\n\n        if not isinstance(func, callable):\n            raise ValueError(\"The function must be callable.\")\n\n        if not isinstance(df, pd.DataFrame) and not isinstance(df, pd.Series):\n            raise ValueError(\"The dataframe/series must be a pandas dataframe or series.\")\n\n        if not isinstance(chunk_size, str):\n            raise ValueError(\"The chunk size must be a string.\")", "idx": 126}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.exclude", "completion": "        if isinstance(range_obj, (pd.DatetimeIndex, tuple)):\n            range_obj = DateRange(range_obj[0], range_obj[-1])\n\n        range_obj = to_pandas_closed_closed(range_obj, add_tz=False)\n        start = range_obj.start\n        end = range_obj.end\n\n        if 'date' in data.index.names:\n            return data[~data.index.get_level_values('date').isin(pd.date_range(start, end))]\n        elif 'date' in data.columns:\n            if start and end:\n                return data[(data.date < start) | (data.date > end)]\n            elif start:\n                return data[(data.date < start)]\n            elif end:\n                return data[(data.date > end)]\n            else:\n                return data\n        else:\n            return data", "idx": 127}
{"namespace": "mopidy.httpclient.format_proxy", "completion": "    if not proxy_config:\n\n        return None\n\n    scheme = proxy_config.get(\"scheme\")\n\n    hostname = proxy_config.get(\"hostname\")\n\n    port = proxy_config.get(\"port\")\n\n    username = proxy_config.get(\"username\")\n\n    password = proxy_config.get(\"password\")\n\n    if not scheme or not hostname or not port:\n\n        return None\n\n    proxy = \"{}://{}:{}\".format(scheme, hostname, port)\n\n    if auth and username and password:\n\n        proxy = \"{}:{}@{}\".format(username, password, proxy)\n\n    return proxy\n\n\n", "idx": 128}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.filter", "completion": "        if isinstance(range_obj, (pd.DatetimeIndex, tuple)):\n            range_obj = DateRange(range_obj[0], range_obj[-1])\n        if range_obj.start and range_obj.end:\n            return data[(data.index >= range_obj.start) & (data.index <= range_obj.end)]\n        elif range_obj.start:\n            return data[data.index >= range_obj.start]\n        elif range_obj.end:\n            return data[data.index <= range_obj.end]\n        else:\n            return data\n", "idx": 129}
{"namespace": "mopidy.config.validators.validate_required", "completion": "    if required and not value:\n        raise ValueError('Value is required')", "idx": 130}
{"namespace": "mopidy.config.validators.validate_choice", "completion": "    if value not in choices:\n        names = ', '.join(repr(c) for c in choices)\n        raise ValueError(\"must be one of %s, not %s\" % (names, value))", "idx": 131}
{"namespace": "mopidy.config.validators.validate_minimum", "completion": "    if value < minimum:\n        raise ValueError(f\"{value} must be larger than {minimum}.\")\n\n", "idx": 132}
{"namespace": "mopidy.config.validators.validate_maximum", "completion": "    if maximum is not None and value > maximum:\n        raise ValueError(f\"{value} must be smaller than {maximum}.\")", "idx": 133}
{"namespace": "mopidy.config.schemas._did_you_mean", "completion": "    if len(choices) > 0:\n        return min(\n            choices,\n            key=lambda x: collections.Counter(\n                Levenshtein.distance(name, x) for x in choices\n            )[0],\n        )\n    else:\n        return None", "idx": 134}
{"namespace": "mopidy.config.types.encode", "completion": "    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(char, char.encode(encoding=\"unicode-escape\").decode())\n\n    return value\n\n", "idx": 135}
{"namespace": "mopidy.config.types.decode", "completion": "    if isinstance(value, bytes):\n        value = value.decode(\"utf-8\", \"surrogateescape\")\n    return value.replace(\"\\\\\", \"\\\\\\\\\").replace(\"\\n\", \"\\\\n\").replace(\"\\t\", \"\\\\t\")\n\n", "idx": 136}
{"namespace": "mopidy.config.types.ConfigValue.serialize", "completion": "        if value is None:\n            return \"\"\n        return encode(value)\n", "idx": 137}
{"namespace": "mopidy.config.types.Boolean.serialize", "completion": "        if value is None:\n            return \"false\"\n        if value is True:\n            return \"true\"\n        if value is False:\n            return \"false\"\n        raise ValueError(f\"{value} is not a boolean\")\n\n", "idx": 138}
{"namespace": "hypertools.tools.df2mat.df2mat", "completion": "    # Check if the DataFrame is multi-level\n    if isinstance(data.index, pd.MultiIndex):\n        raise ValueError(\"The DataFrame is multi-level. This function only works with single-level DataFrames.\")\n\n    # Get the column names\n    cols = data.columns\n\n    # Get the column types\n    col_types = data.dtypes\n\n    # Get the column labels\n    labels = []\n\n    # Loop through the columns\n    for col in cols:\n\n        # Check if the column is text\n        if col_types[col] == 'object':\n\n            # Get the unique values in the column\n            unique_vals = data[col].unique()\n\n            # Loop through the unique values\n            for val in unique_vals:\n\n                # Create a new column for each unique value\n                data[col + '_' + val] = (data[col] == val).astype(int)\n\n                # Add the new column label to the list of labels\n                labels.append(col + '_' + val)\n\n            # Drop the original text column\n            data = data.drop(col, axis=1)\n\n        else:\n\n            # Add the column label to the list of labels\n            labels.append(col)\n\n    # Convert the DataFrame to a Numpy array\n    mat = data.values\n\n    # Return the Numpy array and labels\n    if return_labels:\n        return mat, labels\n    else:\n        return mat", "idx": 139}
{"namespace": "hypertools._shared.helpers.center", "completion": "    assert type(x) == list, \"Input must be a list\"\n    return [i - np.mean(x) for i in x]\n\n", "idx": 140}
{"namespace": "hypertools._shared.helpers.group_by_category", "completion": "    if any(isinstance(i, list) for i in vals):\n        vals = [item for sublist in vals for item in sublist]\n    vals = np.array(vals)\n    uniq = sorted(set(vals))\n    return [uniq.index(i) for i in vals]", "idx": 141}
{"namespace": "hypertools._shared.helpers.vals2colors", "completion": "    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n    val_set = list(sorted(set(vals), key=list(vals).index))\n    palette = sns.color_palette(cmap, res)\n    return [palette[val_set.index(val)] for val in vals]\n\n", "idx": 142}
{"namespace": "hypertools._shared.helpers.vals2bins", "completion": "    # flatten if list of lists\n    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n\n    # get palette from seaborn\n    bins = np.digitize(vals, np.linspace(np.min(vals), np.max(vals)+1, res+1)) - 1\n    return bins", "idx": 143}
{"namespace": "hypertools._shared.helpers.interp_array", "completion": "    # Get the number of rows and columns in the input array\n    rows, cols = arr.shape\n\n    # Create an empty array to store the interpolated values\n    interp_arr = np.zeros((rows, interp_val * (cols - 1) + 1))\n\n    # Loop through each row of the input array\n    for i in range(rows):\n        # Get the current row of the input array\n        y = arr[i]\n\n        # Create an array of indices for the x-axis\n        x = np.arange(len(y))\n\n        # Create a PchipInterpolator object with the x and y arrays\n        f = pchip(x, y)\n\n        # Create a new array of indices for the interpolated x-axis\n        x_new = np.linspace(x.min(), x.max(), len(x) * interp_val + 1)\n\n        # Use the PchipInterpolator object to interpolate the new x-axis\n        y_new = f(x_new)\n\n        # Store the interpolated values in the output array\n        interp_arr[i] = y_new\n\n    return interp_arr", "idx": 144}
{"namespace": "hypertools._shared.helpers.parse_args", "completion": "    if type(x) is not list:\n        x = [x]\n    if type(args) is not list:\n        args = [args]\n    if len(x) != len(args):\n        print('Error: x and args must have the same length')\n        sys.exit(1)\n    for i in range(len(args)):\n        if type(args[i]) is list or type(args[i]) is tuple:\n            if len(args[i]) != len(x):\n                print('Error: args must have the same length as x')\n                sys.exit(1)\n    return [(x[i], *args[i]) for i in range(len(x))]", "idx": 145}
{"namespace": "hypertools._shared.helpers.parse_kwargs", "completion": "    kwargs_list = []\n    for i, item in enumerate(x):\n        tmp = {}\n        for k, v in kwargs.items():\n            if isinstance(v, (tuple, list)):\n                if len(v) == len(x):\n                    tmp[k] = v[i]\n                else:\n                    print('Error: arguments must be a list of the same length as x')\n                    sys.exit(1)\n            else:\n                tmp[k] = v\n        kwargs_list.append(tmp)\n    return kwargs_list", "idx": 146}
{"namespace": "gif_for_cli.utils._get_default_display_mode", "completion": "    if \"TERM\" in environ and \"COLORTERM\" in environ:\n        if \"truecolor\" in environ[\"TERM\"] or \"truecolor\" in environ[\"COLORTERM\"]:\n            return \"truecolor\"\n        elif \"256\" in environ[\"TERM\"] or \"256\" in environ[\"COLORTERM\"]:\n            return \"256fgbg\"\n    return \"nocolor\"\n\n", "idx": 147}
{"namespace": "gif_for_cli.utils._pool_type", "completion": "    val = int(val)\n    if val <= 0:\n        raise argparse.ArgumentTypeError('Minimum number of processes is 1')\n    return val", "idx": 148}
{"namespace": "gif_for_cli.generate.utils.get_avg_for_em", "completion": "    r, g, b = 0, 0, 0\n    for i in range(x, x + cell_width):\n        for j in range(y, y + cell_height):\n            r += px[i, j][0]\n            g += px[i, j][1]\n            b += px[i, j][2]\n    r //= cell_width * cell_height\n    g //= cell_width * cell_height\n    b //= cell_width * cell_height\n    return [r, g, b]\n\n", "idx": 149}
{"namespace": "gif_for_cli.generate.utils.process_input_source", "completion": "    if input_source.startswith('https://tenor.com/view/'):\n        gif_id = input_source.split('-')[-1]\n        gif_url = f'https://media.tenor.com/images/gifs/gif_{gif_id}.gif'\n    else:\n        if os.path.isfile(input_source):\n            gif_url = input_source\n        else:\n            params = {\n                'q': input_source,\n                'key': api_key,\n                'limit': 1,\n            }\n            response = requests.get('https://api.tenor.com/v1/search', params=params)\n            try:\n                gif_url = response.json()['results'][0]['media'][0]['gif']['url']\n            except (KeyError, IndexError, JSONDecodeError):\n                raise ValueError('Invalid input source')\n    return gif_url", "idx": 150}
{"namespace": "hypertools._shared.helpers.reshape_data", "completion": "    # Get the unique categories in the hue\n    hue_cats = np.unique(hue)\n\n    # Initialize the reshaped data and labels\n    x_reshaped = []\n    labels_reshaped = []\n\n    # Loop through each category in the hue\n    for i, cat in enumerate(hue_cats):\n        # Get the indices of the data points that belong to the current category\n        idx = np.where(hue == cat)[0]\n\n        # Stack the data points that belong to the current category\n        x_reshaped.append(np.vstack(x[idx]))\n\n        # If labels are provided, stack the labels that belong to the current category\n        if labels is not None:\n            labels_reshaped.append(np.vstack(labels[idx]))\n\n    # If labels are provided, return the reshaped data and labels\n    if labels is not None:\n        return x_reshaped, labels_reshaped\n\n    # Otherwise, return only the reshaped data\n    return x_reshaped", "idx": 151}
{"namespace": "mingus.extra.lilypond.from_Note", "completion": "    if not isinstance(note, Note):\n        raise TypeError(\"The argument to from_Note must be a Note object\")\n    if not isinstance(process_octaves, bool):\n        raise TypeError(\"The argument to from_Note must be a bool\")\n    if not isinstance(standalone, bool):\n        raise TypeError(\"The argument to from_Note must be a bool\")\n\n    if process_octaves:\n        if note.octave > 3:\n            note_str = note.name + \"'\" * (note.octave - 3)\n        elif note.octave < 3:\n            note_str = note.name + \",\" * (3 - note.octave)\n        else:\n            note_str = note.name\n    else:\n        note_str = note.name\n\n    if standalone:\n        return \"\\\\relative c' {%s}\" % note_str\n    else:\n        return note_str\n\n", "idx": 152}
{"namespace": "mingus.extra.tablature._get_qsize", "completion": "    # Find the longest string\n    longest_string = max([len(str(x)) for x in tuning.tuning])\n\n    # Calculate the quarter note size based on the width and longest string\n    qsize = width // (longest_string + 3)\n\n    # Ensure the quarter note size is at least 4\n    if qsize < 4:\n        qsize = 4\n\n    return qsize\n\n", "idx": 153}
{"namespace": "mingus.core.notes.augment", "completion": "    if note[-1] != \"b\":\n        return note + \"#\"\n    else:\n        return note[:-1]\n\n", "idx": 154}
{"namespace": "mingus.core.meter.valid_beat_duration", "completion": "    if duration == 0:\n        return False\n    if duration == 1:\n        return True\n    while duration != 1:\n        if duration % 2 != 0:\n            return False\n        duration /= 2\n    return True\n\n", "idx": 155}
{"namespace": "mingus.core.notes.diminish", "completion": "    if note[-1] != \"#\":\n        return note + \"b\"\n    else:\n        return note[:-1]", "idx": 156}
{"namespace": "mingus.core.intervals.invert", "completion": "    return interval[::-1]", "idx": 157}
{"namespace": "mingus.core.progressions.parse_string", "completion": "    roman_numeral = \"\"\n    acc = 0\n    suffix = \"\"\n    for x in progression:\n        if x in [\"b\", \"#\"]:\n            acc += 1 if x == \"#\" else -1\n        elif x in [\"I\", \"V\", \"i\", \"v\"]:\n            roman_numeral += x\n        else:\n            suffix += x\n    return (roman_numeral, acc, suffix)", "idx": 158}
{"namespace": "exodus_bundler.bundling.bytes_to_int", "completion": "    result = 0\n    if byteorder == 'big':\n        for b in bytes:\n            result = result * 256 + int(b)\n    elif byteorder == 'little':\n        for b in bytes[::-1]:\n            result = result * 256 + int(b)\n    else:\n        raise ValueError(\"Unknown byte order\")\n    return result\n\n", "idx": 159}
{"namespace": "exodus_bundler.templating.render_template", "completion": "    for key, value in context.items():\n        string = string.replace('{{' + key + '}}', value)\n    return string\n\n", "idx": 160}
{"namespace": "exodus_bundler.input_parsing.strip_pid_prefix", "completion": "    # Check if the line starts with the `[pid XXX] ` prefix\n    if line.startswith('[pid '):\n        # Use a regular expression to find the index of the first closing bracket\n        match = re.search(r'\\[\\d+\\]', line)\n        if match:\n            # Remove the `[pid XXX] ` prefix from the line\n            return line[match.end():].strip()\n    # If the line does not start with the `[pid XXX] ` prefix, return the original line\n    return line", "idx": 161}
{"namespace": "fs.path.abspath", "completion": "    if not isabs(path):\n        return \"/\" + path\n    return path\n\n", "idx": 162}
{"namespace": "fs.path.combine", "completion": "    if path2.startswith(\"/\"):\n        raise ValueError(\"Second path is not relative: %r\" % path2)\n    if path1 == \"/\":\n        return \"/\" + path2\n    else:\n        return path1 + \"/\" + path2\n\n", "idx": 163}
{"namespace": "fs.path.split", "completion": "    head, tail = os.path.split(path)\n    if head == '':\n        head = '/'\n    return head, tail\n\n", "idx": 164}
{"namespace": "fs.path.isparent", "completion": "    _path1 = forcedir(abspath(path1))\n    _path2 = forcedir(abspath(path2))\n    return _path1.startswith(_path2)  # longer one is parent\n\n", "idx": 165}
{"namespace": "fs.path.forcedir", "completion": "    if not path.endswith(\"/\"):\n        return path + \"/\"\n    return path\n\n", "idx": 166}
{"namespace": "fs.wildcard.match_any", "completion": "    return any(match(pattern, name) for pattern in patterns)\n\n", "idx": 167}
{"namespace": "fs.wildcard.imatch_any", "completion": "    if not patterns:\n        return True\n    return any(imatch(pattern, name) for pattern in patterns)\n\n", "idx": 168}
{"namespace": "wal_e.cmd.parse_boolean_envvar", "completion": "    if val.lower() in ('false', '0'):\n        return False\n    elif val.lower() in ('true', '1'):\n        return True\n    else:\n        raise ValueError(\"Invalid boolean value: {}\".format(val))", "idx": 169}
{"namespace": "wal_e.log_help.get_log_destinations", "completion": "    log_destinations = os.environ.get('WALE_LOG_DESTINATION', 'stderr,syslog')\n    return log_destinations.split(',')", "idx": 170}
{"namespace": "wal_e.log_help.WalELogger._fmt_structured", "completion": "        # Get the current time.\n        now = datetime.datetime.utcnow()\n\n        # Format the time.\n        time_str = now.strftime('%Y-%m-%dT%H:%M:%S.%f-00')\n\n        # Get the current process id.\n        pid = os.getpid()\n\n        # Format the dictionary.\n        d_str = ' '.join(['{0}={1}'.format(k, v) for k, v in sorted(d.items())])\n\n        # Return the formatted string.\n        return 'time={0} pid={1} {2}'.format(time_str, pid, d_str)\n", "idx": 171}
{"namespace": "wal_e.tar_partition._fsync_files", "completion": "    for filename in filenames:\n        fd = os.open(filename, os.O_RDONLY)\n        os.fsync(fd)\n        os.close(fd)\n\n    dirname = os.path.dirname(filenames[0])\n    fd = os.open(dirname, os.O_RDONLY)\n    os.fsync(fd)\n    os.close(fd)\n\n", "idx": 172}
{"namespace": "wal_e.blobstore.file.calling_format.Bucket.list", "completion": "        path = os.path.join(\"/\", prefix.strip(\"/\"))\n        file_paths = [os.path.join(path, f) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n        return [FileKey(bucket=self, name=f) for f in file_paths]", "idx": 173}
{"namespace": "pyinfra.operations.util.files.unix_path_join", "completion": "    return \"/\".join(str(part).strip(\"/\") for part in path_parts)\n\n", "idx": 174}
{"namespace": "pyinfra.operations.server.shell", "completion": "    if isinstance(commands, str):\n        commands = [commands]\n\n    for command in commands:\n        yield command\n\n", "idx": 175}
{"namespace": "pyinfra.api.util.try_int", "completion": "    try:\n        return int(value)\n    except ValueError:\n        return value\n\n", "idx": 176}
{"namespace": "mrjob.job.MRJob.mr_job_script", "completion": "        try:\n            return inspect.getsourcefile(cls)\n        except TypeError:\n            return None\n", "idx": 177}
{"namespace": "mrjob.compat.map_version", "completion": "    if isinstance(version_map, dict):\n        version_map = sorted(version_map.items(), key=lambda x: x[0])\n\n    for version_tuple in version_map:\n        if version_tuple[0] > version:\n            return version_tuple[1]\n\n    return version_map[-1][1]\n\n", "idx": 178}
{"namespace": "mrjob.conf.combine_values", "completion": "    for value in reversed(values):\n        if value is not None:\n            return value\n\n", "idx": 179}
{"namespace": "mrjob.protocol.BytesProtocol.read", "completion": "        if b'\\t' in line:\n            key, value = line.split(b'\\t', 1)\n        else:\n            key, value = line, None\n\n        return key, value\n", "idx": 180}
{"namespace": "mrjob.protocol.TextProtocol.write", "completion": "        return u'\\t'.join(x for x in (key, value) if x is not None).encode('utf_8')\n\n", "idx": 181}
{"namespace": "mrjob.protocol.TextProtocol.read", "completion": "        try:\n            key_value = line.decode('utf_8').split(u'\\t', 1)\n        except UnicodeDecodeError:\n            key_value = line.decode('latin_1').split(u'\\t', 1)\n\n        if len(key_value) == 1:\n            key_value.append(None)\n\n        return tuple(key_value)\n", "idx": 182}
{"namespace": "mrjob.protocol.TextValueProtocol.read", "completion": "        try:\n            line = line.decode('utf_8')\n        except UnicodeDecodeError:\n            line = line.decode('latin_1')\n\n        return (None, line)\n", "idx": 183}
{"namespace": "mrjob.util.file_ext", "completion": "    filename = filename.lstrip('.')\n    index = filename.find('.')\n    if index == -1:\n        return ''\n    else:\n        return filename[index:]", "idx": 184}
{"namespace": "mrjob.util.cmd_line", "completion": "    return ' '.join(pipes.quote(str(arg)) for arg in args)\n\n", "idx": 185}
{"namespace": "mrjob.util.save_cwd", "completion": "    original_cwd = os.getcwd()\n\n    try:\n        yield\n\n    finally:\n        os.chdir(original_cwd)\n\n", "idx": 186}
{"namespace": "mrjob.util.save_sys_std", "completion": "    original_stdin = sys.stdin\n    original_stdout = sys.stdout\n    original_stderr = sys.stderr\n\n    try:\n        yield\n\n    finally:\n        sys.stdin.flush()\n        sys.stdout.flush()\n        sys.stderr.flush()\n\n        sys.stdin = original_stdin\n        sys.stdout = original_stdout\n        sys.stderr = original_stderr\n\n", "idx": 187}
{"namespace": "mrjob.util.unarchive", "completion": "    if not os.path.exists(dest):\n        os.makedirs(dest)\n\n    if is_zipfile(archive_path):\n        with ZipFile(archive_path) as zf:\n            for name in zf.namelist():\n                if name.endswith('/'):\n                    continue\n                zf.extract(name, dest)\n    else:\n        with tarfile.open(archive_path) as tf:\n            tf.extractall(dest)\n\n", "idx": 188}
{"namespace": "mrjob.util.unique", "completion": "    seen = set()\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            yield item", "idx": 189}
{"namespace": "mrjob.parse.urlparse", "completion": "    if not isinstance(urlstring, str):\n        urlstring = str(urlstring)\n    if not isinstance(scheme, str):\n        scheme = str(scheme)\n    if not isinstance(allow_fragments, bool):\n        allow_fragments = bool(allow_fragments)\n\n    # Split the URL into its components\n    components = urlparse_buggy(urlstring, scheme, allow_fragments, *args, **kwargs)\n\n    # If the URL has a fragment, split it off and add it to the components\n    if allow_fragments and '#' in urlstring:\n        fragment = urlstring.split('#', 1)[1]\n        components = components._replace(fragment=fragment)\n\n    return components\n\n", "idx": 190}
{"namespace": "mrjob.util.which", "completion": "    if path is None:\n        path = os.environ.get('PATH', '')\n\n    for dir in path.split(os.pathsep):\n        if dir == '':\n            continue\n\n        path = os.path.join(dir, cmd)\n\n        if os.path.exists(path):\n            return path\n\n    return None\n\n", "idx": 191}
{"namespace": "sshuttle.ssh.parse_hostport", "completion": "    if not rhostport:\n        return None, None, None, None\n\n    # Check if rhostport is a valid IPv4/v6 address\n    try:\n        ipaddress.ip_address(rhostport)\n        return None, None, None, rhostport\n    except ValueError:\n        pass\n\n    # Check if rhostport is a valid hostname\n    try:\n        socket.gethostbyname(rhostport)\n        return None, None, None, rhostport\n    except socket.gaierror:\n        pass\n\n    # Check if rhostport is a valid ssh alias from ~/.ssh/config\n    try:\n        config = paramiko.SSHConfig()\n        with open(os.path.expanduser(\"~/.ssh/config\")) as f:\n            config.parse(f)\n        host = config.lookup(rhostport)[\"hostname\"]\n        return None, None, None, host\n    except (FileNotFoundError, paramiko.ssh_exception.SSHException):\n        pass\n\n    # Parse rhostport\n    match = re.match(r'^(?:(?P<username>[^:@]+)(?::(?P<password>[^@]+))?@)?(?P<host>[^:@]+)(?::(?P<port>\\d+))?$', rhostport)\n    if not match:\n        raise ValueError(\"Invalid rhostport format\")\n\n    username = match.group('username')\n    password = match.group('password')\n    host = match.group('host')\n    port = match.group('port')\n\n    return username, password, port, host", "idx": 192}
{"namespace": "flower.utils.search.stringified_dict_contains_value", "completion": "    try:\n        return str_dict.find(f'\"{key}\": {value}') != -1\n    except:\n        return False\n\n", "idx": 193}
{"namespace": "flower.utils.abs_path", "completion": "    path = os.path.expanduser(path)\n    if not os.path.isabs(path):\n        path = os.path.join(os.getcwd(), path)\n    return path", "idx": 194}
{"namespace": "flower.utils.strtobool", "completion": "    val = val.lower()\n    if val in ('y', 'yes', 't', 'true', 'on', '1'):\n        return 1\n    elif val in ('n', 'no', 'f', 'false', 'off', '0'):\n        return 0\n    else:\n        raise ValueError(\"invalid truth value %r\" % (val,))", "idx": 195}
{"namespace": "sshuttle.methods.get_method", "completion": "    try:\n        method_module = importlib.import_module(\n            \".%s\" % method_name, \"sshuttle.methods\")\n    except ImportError as e:\n        raise Fatal(\"Error loading method %s: %s\" % (method_name, e))\n    return method_module.Method(method_name)", "idx": 196}
{"namespace": "trailscraper.iam.all_known_iam_permissions", "completion": "    return set(open(os.path.join(os.path.dirname(__file__), 'known-iam-actions.txt')).read().splitlines())\n\n", "idx": 197}
{"namespace": "trailscraper.cloudtrail.parse_records", "completion": "    return list(filterz(lambda x: x is not None, map(_parse_record, json_records)))", "idx": 198}
{"namespace": "pycoin.satoshi.IntStreamer.IntStreamer.int_to_script_bytes", "completion": "        if v == 0:\n            return b\"\"\n        is_negative = (v < 0)\n        if is_negative:\n            v = -v\n        s = bytearray()\n        while v:\n            s.append(v & 0xff)\n            v >>= 8\n        if s[-1] & 0x80:\n            s.append(0x80 if is_negative else 0)\n        elif is_negative:\n            s[-1] |= 0x80\n        s.reverse()\n        return bytes(s)\n", "idx": 199}
{"namespace": "pycoin.satoshi.stackops.do_OP_2DROP", "completion": "    if len(stack) < 2:\n        raise ScriptError(\"OP_2DROP requires 2 elements on the stack\", errno.INVALID_STACK_OPERATION)\n    stack.pop(-1)\n    stack.pop(-1)\n\n", "idx": 200}
{"namespace": "pycoin.satoshi.stackops.do_OP_2DUP", "completion": "    stack.append(stack[-2])\n    stack.append(stack[-2])\n\n", "idx": 201}
{"namespace": "pycoin.satoshi.stackops.do_OP_3DUP", "completion": "    stack.append(stack[-3])\n    stack.append(stack[-3])\n    stack.append(stack[-3])\n\n", "idx": 202}
{"namespace": "trailscraper.s3_download._s3_key_prefixes", "completion": "    delta = to_date - from_date\n    dates = [from_date + datetime.timedelta(days=i) for i in range(delta.days + 1)]\n    return [\n        _s3_key_prefix(prefix, date, account_id, region)\n        for org_id in org_ids\n        for account_id in account_ids\n        for region in regions\n        for date in dates\n    ]\n\n", "idx": 203}
{"namespace": "pycoin.satoshi.stackops.do_OP_2OVER", "completion": "    stack.append(stack[-3])\n    stack.append(stack[-3])\n\n", "idx": 204}
{"namespace": "pycoin.satoshi.stackops.do_OP_2SWAP", "completion": "    stack.append(stack.pop(-3))\n    stack.append(stack.pop(-3))\n\n", "idx": 205}
{"namespace": "pycoin.satoshi.stackops.do_OP_IFDUP", "completion": "    if stack[-1] != 0:\n        stack.append(stack[-1])\n\n", "idx": 206}
{"namespace": "pycoin.satoshi.stackops.do_OP_NIP", "completion": "    stack.pop(-2)\n\n", "idx": 207}
{"namespace": "pycoin.satoshi.stackops.do_OP_TUCK", "completion": "    v1 = stack.pop()\n    v2 = stack.pop()\n    stack.append(v1)\n    stack.append(v2)\n    stack.append(v1)\n\n", "idx": 208}
{"namespace": "pycoin.satoshi.stackops.do_OP_CAT", "completion": "    v1 = stack.pop()\n    v2 = stack.pop()\n    stack.append(v2 + v1)\n\n", "idx": 209}
{"namespace": "pycoin.crack.ecdsa.crack_secret_exponent_from_k", "completion": "    # Calculate the secret exponent\n    secret_exponent = (signed_value - k * generator) * pow(sig, -1, generator) % generator\n\n    # Return the secret exponent\n    return secret_exponent\n\n", "idx": 210}
{"namespace": "pycoin.crack.ecdsa.crack_k_from_sigs", "completion": "    r1, s1 = sig1\n    r2, s2 = sig2\n    return (((val1 - val2) * generator.inverse(s1 - s2)) % generator.order())\n\n", "idx": 211}
{"namespace": "pycoin.message.make_parser_and_packer.standard_streamer", "completion": "    streamer = Streamer(parse_satoshi_int=parse_satoshi_int)\n    for type_code, (parse_function, stream_function) in parsing_functions:\n        streamer.register_array_length_parser(type_code, parse_function)\n        streamer.register_parser(type_code, stream_function)\n    return streamer", "idx": 212}
{"namespace": "pycoin.key.subpaths.subpaths_for_path_range", "completion": "    # Split the path range into its components\n    path_range_parts = path_range.split(\"/\")\n\n    # Initialize an empty list to store the subpaths\n    subpaths = []\n\n    # Iterate over the path range parts\n    for part in path_range_parts:\n        # Check if the part contains a range\n        if \"-\" in part:\n            # Split the part into its start and end values\n            start, end = part.split(\"-\")\n            # Convert the start and end values to integers\n            start = int(start)\n            end = int(end)\n            # Add the subpaths for the range to the list\n            subpaths.append(range(start, end + 1))\n        else:\n            # Add the part as a single subpath\n            subpaths.append([int(part)])\n\n    # Use the itertools.product function to generate all possible combinations of subpaths\n    for subpath in itertools.product(*subpaths):\n        # Join the subpaths into a single path string\n        path = \"/\".join(str(p) for p in subpath)\n        # Check if the path contains any hardening characters\n        if any(c in path for c in hardening_chars):\n            # If it does, yield the path with the hardening character replaced by a question mark\n            yield path.replace(hardening_chars[0], \"?\")\n        else:\n            # If it doesn't, yield the path as is\n            yield path\n\n", "idx": 213}
{"namespace": "pyt.core.project_handler._is_python_file", "completion": "    return path.endswith('.py')", "idx": 214}
{"namespace": "pycoin.encoding.hexbytes.h2b", "completion": "    try:\n        return binascii.unhexlify(h)\n    except:\n        raise ValueError(\"Invalid hexadecimal string\")\n\n", "idx": 215}
{"namespace": "zxcvbn.scoring.calc_average_degree", "completion": "    # Initialize the sum of degrees to zero\n    sum_of_degrees = 0\n\n    # Iterate through the graph and add the number of neighbors of each node to the sum of degrees\n    for node in graph:\n        sum_of_degrees += len(graph[node])\n\n    # Calculate the average degree by dividing the sum of degrees by the number of nodes\n    average_degree = sum_of_degrees / len(graph)\n\n    # Return the average degree\n    return average_degree\n\n", "idx": 216}
{"namespace": "zxcvbn.scoring.nCk", "completion": "    if k > n:\n        return 0\n    m = n\n    o = 1\n    for p in range(1, min(k, n - k) + 1):\n        m -= 1\n        o *= m\n        o //= p\n    return o\n\n", "idx": 217}
{"namespace": "zxcvbn.matching.relevant_l33t_subtable", "completion": "    password_chars = {}\n    for char in password:\n        password_chars[char] = True\n\n    subtable = {}\n    for letter, subs in table.items():\n        if letter in password_chars:\n            subtable[letter] = subs\n\n    return subtable\n\n", "idx": 218}
{"namespace": "zxcvbn.matching.translate", "completion": "    for key, value in chr_map.items():\n        string = string.replace(key, value)\n    return string.split()\n\n", "idx": 219}
{"namespace": "tools.cgrep.get_nets", "completion": "  results = []\n  for obj in objects:\n    try:\n      networks = db.GetNetworks(obj)\n    except naming.UndefinedAddressError:\n      logging.info('%s is an invalid object', obj)\n    else:\n      results.append((obj, networks))\n  return results", "idx": 220}
{"namespace": "tools.cgrep.get_ports", "completion": "  results = []\n  for svc in svc_group:\n    port = db.GetPort(svc)\n    protocol = db.GetProtocol(svc)\n    results.append((svc, '%s/%s' % (port, protocol)))\n  return results", "idx": 221}
{"namespace": "tools.cgrep.compare_ip_token", "completion": "  results = []\n  for ip in options.ip:\n    ip = nacaddr.IP(ip)\n    for obj in get_nets([options.token], db)[0][1]:\n      if ip.version == obj.version:\n        if ip.subnet_of(obj):\n          results.append('%s is in %s' % (ip, options.token))\n        else:\n          results.append('%s is not in %s' % (ip, options.token))\n  return '\\n'.join(results)", "idx": 222}
{"namespace": "tools.cgrep.get_services", "completion": "  port = options.port[0]\n  protocol = options.port[1]\n  results = []\n  for svc in db.services:\n    for p in db.services[svc]:\n      if p.port == port and p.protocol == protocol:\n        results.append(svc)\n  return port, protocol, results", "idx": 223}
{"namespace": "asyncssh.packet.String", "completion": "    if isinstance(value, str):\n        value = value.encode('utf-8')\n\n    return UInt32(len(value)) + value\n\n", "idx": 224}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_cmd_counts", "completion": "    # Add 1 to each of the counts, including the unk_token, to handle unseen commands\n    seq1_counts[unk_token] += 1\n    seq2_counts[start_token][unk_token] += 1\n    seq2_counts[unk_token][unk_token] += 1\n    seq2_counts[unk_token][end_token] += 1\n\n    return seq1_counts, seq2_counts\n\n", "idx": 225}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_param_counts", "completion": "    param_counts_ls = copy.deepcopy(param_counts)\n    cmd_param_counts_ls = copy.deepcopy(cmd_param_counts)\n\n    params: List[str] = list(param_counts_ls.keys()) + [unk_token]\n    for cmd in cmds:\n        for param in params:\n            param_counts_ls[param] += 1\n            cmd_param_counts_ls[cmd][param] += 1\n\n    return param_counts_ls, cmd_param_counts_ls", "idx": 226}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_value_counts", "completion": "    value_counts_ls = copy.deepcopy(value_counts)\n    param_value_counts_ls = copy.deepcopy(param_value_counts)\n\n    values: List[str] = list(value_counts.keys()) + [unk_token]\n    for param in params:\n        for value in values:\n            if value in param_value_counts_ls[param] or value == unk_token:\n                value_counts_ls[value] += 1\n                param_value_counts_ls[param][value] += 1\n\n    return value_counts_ls, param_value_counts_ls", "idx": 227}
{"namespace": "diffprivlib.validation.check_epsilon_delta", "completion": "    if not isinstance(epsilon, Real) or not isinstance(delta, Real):\n        raise TypeError(\"Epsilon and delta must be numeric\")\n\n    if epsilon < 0:\n        raise ValueError(\"Epsilon must be non-negative\")\n\n    if delta < 0 or delta > 1:\n        raise ValueError(\"Delta must be in [0, 1]\")\n\n    if not allow_zero and epsilon + delta == 0:\n        raise ValueError(\"Epsilon and Delta cannot both be zero\")\n\n", "idx": 228}
{"namespace": "diffprivlib.utils.check_random_state", "completion": "    if secure:\n        return secrets.SystemRandom()\n    else:\n        return skl_check_random_state(seed)", "idx": 229}
{"namespace": "diffprivlib.validation.clip_to_norm", "completion": "    if not isinstance(array, np.ndarray):\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\n    if array.ndim != 2:\n        raise ValueError(f\"input array must be 2-dimensional, got {array.ndim} dimensions.\")\n    if not isinstance(clip, Real):\n        raise TypeError(f\"Clip value must be numeric, got {type(clip)}.\")\n    if clip <= 0:\n        raise ValueError(f\"Clip value must be strictly positive, got {clip}.\")\n\n    norms = np.linalg.norm(array, axis=1)\n    multiplier = np.minimum(1, clip / norms)\n    return array * multiplier[:, np.newaxis]\n\n", "idx": 230}
{"namespace": "diffprivlib.models.pca.PCA.fit_transform", "completion": "        X = self._validate_data(X, accept_sparse='csr',\n                                dtype=[np.float64, np.float32],\n                                ensure_min_samples=2,\n                                estimator=self)\n\n        if self.svd_solver == 'auto':\n            # Small problem or only 2 features, just call full PCA\n            if max(X.shape) < 5:\n                return self._fit_full(X, n_components=self.n_components)[2]\n            # For large arrays, automatically switch to randomized SVD\n            # if the input data is not floating point, need to coerce\n            # to np.float64 for randomized SVD in scipy\n            if not X.dtype.kind == 'f':\n                X = X.astype(np.float64)\n\n            # For wide matrices, we use randomized PCA\n            if self.n_components is None:\n                n_components = min(X.shape)\n            else:\n                n_components = self.n_components\n            random_state = check_random_state(self.random_state)\n            U, S, V = randomized_svd(X, n_components,\n                                     n_iter=self.iterated_power,\n                                     random_state=random_state)\n            U, V = svd_flip(U, V)\n\n            if self.whiten:\n                # X_new = X * V = U * np.diag(S) * V\n                X_new = np.dot(U, np.diag(S))\n                X_new /= np.sqrt(S)\n            else:\n                X_new = U\n\n            components_ = V\n            explained_variance_ = (S ** 2) / (n_samples - 1)\n            explained_variance_ratio_ = (explained_variance_ /\n                                         explained_variance_.sum())\n            singular_values_ = S.copy()  # Store the singular values.\n        else:\n            components_, explained_variance_, explained_variance_ratio_, singular", "idx": 231}
{"namespace": "discord.utils.get_slots", "completion": "    if not hasattr(cls, '__slots__'):\n        if hasattr(cls, '__dict__'):\n            for elem in vars(cls):\n                yield elem\n\n    for elem in reversed(cls.__mro__):\n        if hasattr(elem, '__slots__'):\n            for slot in elem.__slots__:\n                yield slot\n\n", "idx": 232}
{"namespace": "discord.utils.is_inside_class", "completion": "    if not hasattr(func, '__qualname__'):\n        return False\n\n    qualname = func.__qualname__\n    if '.' not in qualname:\n        return False\n\n    # Check if the function is defined within a class\n    if '<locals>' in qualname:\n        # If the function is defined within a nested function, check if the class is defined within the outer function\n        outer_func_name = qualname.split('.<locals>')[0]\n        outer_func = globals().get(outer_func_name)\n        if outer_func and hasattr(outer_func, '__qualname__') and '<locals>' not in outer_func.__qualname__:\n            return True\n    else:\n        # If the function is defined within a class, check if the class is defined within a module\n        class_name = qualname.split('.')[0]\n        class_obj = globals().get(class_name)\n        if class_obj and hasattr(class_obj, '__module__') and class_obj.__module__ != '__main__':\n            return True\n\n    return False\n\n", "idx": 233}
{"namespace": "faker.utils.decorators.slugify", "completion": "    def wrapper(*args, **kwargs) -> T:\n        return text.slugify(fn(*args, **kwargs))\n\n    return wrapper\n\n", "idx": 234}
{"namespace": "faker.utils.decorators.slugify_domain", "completion": "    def wrapper(*args: Tuple[T, ...], **kwargs: Dict[str, T]) -> str:\n        return text.slugify(fn(*args, **kwargs), allow_dots=True)\n\n    return wrapper", "idx": 235}
{"namespace": "faker.utils.decorators.slugify_unicode", "completion": "    def wrapper(*args: Tuple[T, ...], **kwargs: Dict[str, T]) -> str:\n        return text.slugify(fn(*args, **kwargs), allow_unicode=True)\n\n    return wrapper", "idx": 236}
{"namespace": "faker.utils.loading.get_path", "completion": "    if getattr(sys, \"frozen\", False):\n        if hasattr(sys, \"_MEIPASS\"):\n            return Path(sys._MEIPASS) / module.__name__\n        else:\n            return Path(module.__file__).parent\n    else:\n        if module.__file__ is None:\n            raise RuntimeError(f\"Can't find path from module `{module}.\")\n        return Path(module.__file__).parent\n\n", "idx": 237}
{"namespace": "faker.utils.checksums.luhn_checksum", "completion": "    def digits_of(number: float) -> List[int]:\n        return [int(d) for d in str(number)]\n\n    digits = digits_of(number)\n    odd_digits = digits[-1::-2]\n    even_digits = digits[-2::-2]\n    checksum = 0\n    checksum += sum(odd_digits)\n    for digit in even_digits:\n        checksum += sum(digits_of(2 * digit))\n    return checksum % 10\n\n", "idx": 238}
{"namespace": "faker.utils.datasets.add_ordereddicts", "completion": "    return OrderedDict(chain.from_iterable(d.items() for d in odicts))", "idx": 239}
{"namespace": "faker.providers.person.pl_PL.checksum_identity_card_number", "completion": "    weights = [7, 3, 1, 0, 7, 3, 1, 7, 3]\n    checksum = 0\n    for i in range(len(characters)):\n        checksum += int(characters[i]) * weights[i]\n    return checksum % 10\n\n", "idx": 240}
{"namespace": "faker.providers.company.pl_PL.regon_checksum", "completion": "    weights = [8, 9, 2, 3, 4, 5, 6, 7]\n    check_digit = sum([weight * digit for weight, digit in zip(weights, digits)]) % 11\n    if check_digit == 10:\n        check_digit = 0\n    return check_digit\n\n", "idx": 241}
{"namespace": "faker.providers.company.ru_RU.calculate_checksum", "completion": "    factors = [3, 7, 2, 4, 10, 3, 5, 9, 4, 6, 8]\n    sum_ = 0\n    for i in range(len(value)):\n        sum_ += int(value[i]) * factors[i]\n    remainder = sum_ % 11\n    if remainder == 0:\n        return \"0\"\n    elif remainder == 1:\n        return \"0\"\n    else:\n        return str(11 - remainder)\n\n", "idx": 242}
{"namespace": "faker.providers.company.pl_PL.local_regon_checksum", "completion": "    weights_for_check_digit = [2, 4, 8, 5, 0, 9, 7, 3, 6, 1, 2, 4, 8]\n    check_digit = 0\n\n    for i in range(0, 12):\n        check_digit += weights_for_check_digit[i] * digits[i]\n\n    check_digit %= 11\n\n    if check_digit == 10:\n        check_digit = 0\n\n    return check_digit\n\n", "idx": 243}
{"namespace": "faker.providers.company.pl_PL.company_vat_checksum", "completion": "    weights_for_check_digit = [6, 5, 7, 2, 3, 4, 5, 6, 7]\n    check_digit = 0\n\n    for i in range(0, 9):\n        check_digit += weights_for_check_digit[i] * digits[i]\n\n    check_digit %= 11\n\n    if check_digit == 10:\n        check_digit = 0\n\n    return check_digit\n\n", "idx": 244}
{"namespace": "faker.providers.company.pt_BR.company_id_checksum", "completion": "    weights = [6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 2]\n    check_digits = [\n        (sum(d * w for d, w in zip(digits, weights)) % 11) % 10,\n        (sum(d * w for d, w in zip(digits + [0], weights)) % 11) % 10,\n    ]\n    return digits + check_digits\n\n", "idx": 245}
{"namespace": "faker.providers.misc.Provider.binary", "completion": "        return self.generator.random.getrandbits(length * 8).to_bytes(length, \"big\")\n", "idx": 246}
{"namespace": "faker.providers.python.Provider.pystr", "completion": "        if min_chars is None:\n            min_chars = max_chars\n        if min_chars < 0 or max_chars < 0:\n            raise ValueError(\"min_chars and max_chars must be positive\")\n        if min_chars > max_chars:\n            raise ValueError(\"min_chars must be less than or equal to max_chars\")\n\n        length = self.random_int(min_chars, max_chars)\n        return prefix + self.generator.text(length) + suffix\n", "idx": 247}
{"namespace": "dash._utils.AttributeDict.set_read_only", "completion": "        if not hasattr(self, \"_read_only\"):\n            self._read_only = {}\n        for name in names:\n            self._read_only[name] = msg\n", "idx": 248}
{"namespace": "dash._utils.AttributeDict.first", "completion": "        for name in names:\n            val = self.get(name)\n            if val:\n                return val\n\n        return self[0]", "idx": 249}
{"namespace": "dash._get_paths.app_get_asset_url", "completion": "    if config.assets_external_path:\n        return config.assets_external_path + path\n    else:\n        return config.requests_pathname_prefix + path\n\n", "idx": 250}
{"namespace": "peewee.sort_models", "completion": "    G = {}\n    for model in models:\n        G[model] = set(model._meta.backrefs)\n\n    def visit(n, visited, stack):\n        if n in visited:\n            return\n        visited.add(n)\n        for m in G[n]:\n            visit(m, visited, stack)\n        stack.append(n)\n\n    visited = set()\n    stack = []\n    for model in models:\n        visit(model, visited, stack)\n    return stack", "idx": 251}
{"namespace": "dash._grouping.grouping_len", "completion": "    if isinstance(grouping, (tuple, list)):\n        return sum(grouping_len(g) for g in grouping)\n\n    if isinstance(grouping, dict):\n        return sum(grouping_len(g) for g in grouping.values())\n\n    return 1\n\n", "idx": 252}
{"namespace": "playhouse.kv.KeyValue.get", "completion": "        try:\n            return self[key]\n        except KeyError:\n            return default", "idx": 253}
{"namespace": "playhouse.kv.KeyValue.setdefault", "completion": "        try:\n            return self[key]\n        except KeyError:\n            self[key] = default\n            return default\n", "idx": 254}
{"namespace": "sslyze.plugins.certificate_info._certificate_utils.get_public_key_sha256", "completion": "    return sha256(certificate.public_key().public_bytes(Encoding.DER, PublicFormat.PKCS1)).digest()", "idx": 255}
{"namespace": "ydata_profiling.compare_reports._compare_title", "completion": "    if len(set(titles)) == 1:\n        return titles[0]\n    else:\n        return f\"{titles[0]} vs {titles[1]}\"", "idx": 256}
{"namespace": "ydata_profiling.report.formatters.fmt_bytesize", "completion": "    for unit in [\"\", \"Ki\", \"Mi\", \"Gi\", \"Ti\", \"Pi\", \"Ei\", \"Zi\"]:\n        if abs(num) < 1024.0:\n            return f\"{num:3.1f} {unit}{suffix}\"\n        num /= 1024.0\n    return f\"{num:.1f} Yi{suffix}\"\n\n", "idx": 257}
{"namespace": "ydata_profiling.report.formatters.fmt_percent", "completion": "    if edge_cases:\n        if math.isnan(value):\n            return \"NaN%\"\n        if math.isinf(value):\n            if value > 0:\n                return \"+Inf%\"\n            return \"-Inf%\"\n    return f\"{value:.1%}\"\n\n", "idx": 258}
{"namespace": "ydata_profiling.report.formatters.fmt_numeric", "completion": "    if isinstance(value, np.integer):\n        return f\"{value:n}\"\n    if isinstance(value, np.floating):\n        return f\"{value:,.{precision}f}\"\n    return f\"{value:n}\"\n\n", "idx": 259}
{"namespace": "ydata_profiling.report.formatters.fmt_array", "completion": "    with np.printoptions(threshold=threshold, edgeitems=threshold):\n        return str(value)\n\n", "idx": 260}
{"namespace": "ydata_profiling.report.formatters.fmt_monotonic", "completion": "    if value == 0:\n        return \"\u21ca\"\n    elif value == 1:\n        return \"\u21c8\"\n    else:\n        return \"\u21d5\"", "idx": 261}
{"namespace": "ydata_profiling.visualisation.plot._plot_pie_chart", "completion": "    # Use the pd.Series indices as category names\n    labels = data.index.values.astype(str)\n\n    # Plot\n    _, ax = plt.subplots(figsize=(7, 2))\n    ax.axis(\"off\")\n\n    # Plot the pie chart\n    wedges, _ = ax.pie(data, colors=colors, startangle=90, counterclock=False)\n\n    # Add labels to the pie chart\n    ax.legend(\n        wedges,\n        labels,\n        fontsize=\"x-large\",\n        loc=\"center left\",\n        bbox_to_anchor=(1, 0, 0.5, 1),\n    )\n\n    # Add percentage labels to the pie chart\n    for wedge, label in zip(wedges, labels):\n        angle = wedge.theta2 - wedge.theta1\n        if angle < 180:\n            angle += angle\n        else:\n            angle -= angle\n        angle = angle / 2.0 + wedge.theta1\n        x = np.cos(np.deg2rad(angle))\n        y = np.sin(np.deg2rad(angle))\n        horizontalalignment = {\n            \"right\": \"left\",\n            \"left\": \"right\",\n        }[x < 0]\n        callout_pos = x + np.sign(x) * 0.4\n        ax.annotate(\n            f\"{label} ({data[label]})\",\n            xy=(x, y),\n            xytext=(1.35 * np.sign(x), 1.4 * y),\n            horizontalalignment=horizontalalignment,\n            arrowprops=dict(\n                facecolor=\"black\", shrink=0.05, width=2, headwidth=8, headlength=8\n            ),\n        )\n\n    # Add a title to the pie chart\n    ax.set_title(\"Category Frequency\", fontsize=\"xx-large\", fontweight=\"bold\")\n\n    # Return the pie chart and legend handler\n    return ax, None", "idx": 262}
{"namespace": "ydata_profiling.visualisation.plot._prepare_heatmap_data", "completion": "    if sortby is not None:\n        if isinstance(sortby, str):\n            sortby = [sortby]\n        dataframe = dataframe.sort_values(sortby)\n\n    if selected_entities is not None:\n        dataframe = dataframe[dataframe[entity_column].isin(selected_entities)]\n\n    if max_entities is not None:\n        dataframe = dataframe.head(max_entities)\n\n    return dataframe\n\n", "idx": 263}
{"namespace": "ydata_profiling.visualisation.plot._create_timeseries_heatmap", "completion": "    fig, ax = plt.subplots(figsize=figsize)\n    ax.set_facecolor(\"white\")\n    ax.set_xlabel(\"\")\n    ax.set_ylabel(\"\")\n    ax.set_yticks([])\n    ax.set_xticks([])\n    ax.set_xticks(np.arange(df.shape[1]) + 0.5, minor=False)\n    ax.set_yticks(np.arange(df.shape[0]) + 0.5, minor=False)\n    ax.invert_yaxis()\n    ax.xaxis.tick_top()\n    ax.set_xticklabels(df.columns, minor=False)\n    ax.set_yticklabels(df.index, minor=False)\n    ax.grid(False, which=\"minor\")\n    ax.grid(False, which=\"major\")\n    ax.imshow(df, interpolation=\"nearest\", cmap=color, aspect=\"auto\")\n    return ax\n\n", "idx": 264}
{"namespace": "ydata_profiling.model.expectation_algorithms.generic_expectations", "completion": "    if name not in batch.columns:\n        summary[name] = \"Column not found\"\n        return name, summary, batch\n\n    if batch[name].isnull().sum() > 0:\n        summary[name] = \"Missing values found\"\n        return name, summary, batch\n\n    if batch[name].nunique() != len(batch[name]):\n        summary[name] = \"Duplicate values found\"\n        return name, summary, batch\n\n    return name, summary, batch", "idx": 265}
{"namespace": "ydata_profiling.model.expectation_algorithms.numeric_expectations", "completion": "    name, summary, batch = generic_expectations(name, summary, batch, *args)\n\n    if summary[\"n_missing\"] == 0:\n        batch.expect_column_values_to_be_of_type(name, \"int\")\n\n    if summary[\"n_missing\"] == 0:\n        batch.expect_column_values_to_be_between(\n            name, summary[\"min\"], summary[\"max\"]\n        )\n\n    if summary[\"n_missing\"] == 0:\n        batch.expect_column_mean_to_be_between(\n            name, summary[\"mean\"] - summary[\"std\"], summary[\"mean\"] + summary[\"std\"]\n        )\n\n    if summary[\"n_missing\"] == 0:\n        batch.expect_column_median_to_be_between(\n            name, summary[\"min\"], summary[\"max\"]\n        )\n\n    if summary[\"n_missing\"] == 0:\n        batch.expect_column_quantile_values_to_be_between(\n            name,\n            quantile=0.25,\n            min_value=summary[\"25%\"],\n            max_value=summary[\"75%\"],\n        )\n\n    return name, summary, batch\n\n", "idx": 266}
{"namespace": "ydata_profiling.model.expectation_algorithms.categorical_expectations", "completion": "    if summary[\"n_distinct\"] < summary[\"p_distinct\"] * summary[\"n_rows\"]:\n        batch.expect_column_values_to_be_in_set(\n            name, summary[\"value_counts_index\"][:-1]\n        )\n\n    return name, summary, batch", "idx": 267}
{"namespace": "ydata_profiling.model.expectation_algorithms.datetime_expectations", "completion": "    if \"min\" in summary:\n        batch.expect_column_values_to_be_between(\n            name,\n            min_value=summary[\"min\"],\n            parse_strings_as_datetimes=True,\n        )\n    if \"max\" in summary:\n        batch.expect_column_values_to_be_between(\n            name,\n            max_value=summary[\"max\"],\n            parse_strings_as_datetimes=True,\n        )\n    return name, summary, batch\n\n", "idx": 268}
{"namespace": "ydata_profiling.model.expectation_algorithms.file_expectations", "completion": "    return name, summary, batch", "idx": 269}
{"namespace": "ydata_profiling.model.pandas.describe_categorical_pandas.word_summary_vc", "completion": "    # Split the string into a list of words\n    words = vc.index.str.split(expand=True).stack()\n\n    # Remove stop words\n    if stop_words:\n        words = words[~words.isin(stop_words)]\n\n    # Count the number of occurrences of each word\n    word_counts = words.value_counts()\n\n    # Sort the words by frequency\n    word_counts = word_counts.sort_values(ascending=False)\n\n    # Return the results as a Series\n    return {\"word_counts\": word_counts}", "idx": 270}
{"namespace": "ydata_profiling.model.pandas.imbalance_pandas.column_imbalance_score", "completion": "    if n_classes == 1:\n        return 0\n    else:\n        return entropy(value_counts, base=2) / log2(n_classes)", "idx": 271}
{"namespace": "django.core.exceptions.ValidationError.messages", "completion": "        if hasattr(self, \"error_dict\"):\n            return sum(dict(self).values(), [])\n        return list(self)\n", "idx": 272}
{"namespace": "django.utils.module_loading.module_has_submodule", "completion": "    try:\n        package.__path__\n    except AttributeError:\n        return False\n    else:\n        full_path = os.path.join(package.__path__[0], module_name)\n        if os.path.exists(full_path) and os.path.isdir(full_path):\n            return True\n        else:\n            return importlib_find(package.__name__ + \".\" + module_name) is not None", "idx": 273}
{"namespace": "django.utils.timezone.get_fixed_timezone", "completion": "    if isinstance(offset, timedelta):\n        offset = offset.seconds // 60\n    return timezone(timedelta(minutes=offset), name=f\"UTC{offset:+d}\")\n\n", "idx": 274}
{"namespace": "django.utils.encoding.filepath_to_uri", "completion": "    # Convert the path to a string if it is a pathlib.Path object\n    path = str(path)\n\n    # Split the path into its components\n    path_parts = path.split(\"/\")\n\n    # Encode each component using the quote function\n    encoded_parts = [quote(part, safe=\"\") for part in path_parts]\n\n    # Join the encoded parts with '/' to form the final URI\n    uri = \"/\".join(encoded_parts)\n\n    return uri", "idx": 275}
{"namespace": "django.utils._os.to_path", "completion": "    if isinstance(value, Path):\n        return value\n    return Path(value)", "idx": 276}
{"namespace": "django.utils.lorem_ipsum.sentence", "completion": "    # Determine the number of comma-separated sections and number of words in each section\n    num_sections = random.randint(1, 10)\n    sect_lens = [random.randint(1, 10) for _ in range(num_sections)]\n\n    # Determine the number of words in the sentence\n    num_words = sum(sect_lens)\n\n    # Break words into sections\n    sections = []\n    i = 0\n    for sect_len in sect_lens:\n        section = WORDS[i : i + sect_len]\n        sections.append(section)\n        i += sect_len\n\n    # Select a number of words from the sections\n    words = []\n    for section in sections:\n        # select 1-3 words from the section, in a random order\n        num_words = random.randint(1, 3)\n        for _ in range(num_words):\n            words.append(random.choice(section))\n\n    # Capitalize the first word, add the period\n    words[0] = words[0].capitalize()\n    words[-1] += random.choice([\".\", \"?\", \"!\"])\n\n    # Add commas\n    comma_indices = random.sample(range(num_words), random.randint(0, num_words - 1))\n    for i in comma_indices:\n        words[i] += \",\"\n\n    return \" \".join(words)\n\n", "idx": 277}
{"namespace": "ydata_profiling.utils.dataframe.sort_column_names", "completion": "    if sort is None:\n        return dct\n    elif sort == \"ascending\":\n        return dict(sorted(dct.items(), key=lambda item: item[0]))\n    elif sort == \"descending\":\n        return dict(sorted(dct.items(), key=lambda item: item[0], reverse=True))\n    else:\n        raise ValueError(\n            f\"Invalid sorting order: {sort}. It should be 'ascending', 'descending' or None.\"\n        )", "idx": 278}
{"namespace": "django.utils.ipv6.is_valid_ipv6_address", "completion": "    try:\n        ipaddress.IPv6Address(ip_str)\n        return True\n    except ValueError:\n        return False", "idx": 279}
{"namespace": "django.utils.http.urlsafe_base64_decode", "completion": "    s = s.encode(\"ascii\")\n    try:\n        return base64.urlsafe_b64decode(s.ljust(len(s) + len(s) % 4, b\"=\"))\n    except BinasciiError:\n        raise ValueError(\"Input is not a valid base64-encoded string\")\n\n", "idx": 280}
{"namespace": "django.utils.http.parse_etags", "completion": "    if etag_str == \"*\":\n        return [\"*\"]\n    etags = []\n    for etag in etag_str.split(\",\"):\n        etag = etag.strip()\n        if not etag:\n            continue\n        if not ETAG_MATCH.match(etag):\n            raise ValueError(\"Invalid ETag: %r\" % etag)\n        etags.append(etag)\n    return etags\n\n", "idx": 281}
{"namespace": "django.utils.http.is_same_domain", "completion": "    if host == pattern:\n        return True\n    elif pattern.startswith(\"*\") and host.endswith(pattern[1:]):\n        return True\n    else:\n        return False\n\n", "idx": 282}
{"namespace": "django.utils.http.content_disposition_header", "completion": "    if as_attachment:\n        return 'attachment; filename=\"%s\"' % filename\n    else:\n        return 'inline; filename=\"%s\"' % filename\n\n", "idx": 283}
{"namespace": "pysnooper.utils.truncate", "completion": "    if len(string) <= max_length:\n        return string\n    else:\n        half_length = (max_length - 3) // 2\n        return string[:half_length] + '...' + string[-half_length:]", "idx": 284}
{"namespace": "pysnooper.variables.needs_parentheses", "completion": "    source_compiled = compile(source, '<string>', 'eval')\n    if source_compiled.co_names:\n        return True\n    source_parentheses = '({})'.format(source)\n    source_parentheses_compiled = compile(source_parentheses, '<string>', 'eval')\n    return source_compiled.co_code != source_parentheses_compiled.co_code\n\n", "idx": 285}
{"namespace": "django.test.utils.extend_sys_path", "completion": "    original_sys_path = sys.path\n    sys.path = list(original_sys_path)\n    for path in paths:\n        sys.path.insert(0, path)\n    try:\n        yield\n    finally:\n        sys.path = original_sys_path\n\n", "idx": 286}
{"namespace": "albumentations.augmentations.functional.normalize_cv2", "completion": "    mean = np.array(mean, dtype=np.float32)\n    denominator = np.array(denominator, dtype=np.float32)\n    if mean.ndim == 1:\n        mean = mean.reshape(1, 1, -1)\n    if denominator.ndim == 1:\n        denominator = denominator.reshape(1, 1, -1)\n\n    img = img.astype(np.float32)\n    img -= mean\n    img /= denominator\n    return img\n\n", "idx": 287}
{"namespace": "albumentations.augmentations.functional.normalize_numpy", "completion": "    if mean.shape and len(mean) != 4 and mean.shape != img.shape:\n        mean = np.array(mean.tolist() + [0] * (4 - len(mean)), dtype=np.float64)\n    if not denominator.shape:\n        denominator = np.array([denominator.tolist()] * 4, dtype=np.float64)\n    elif len(denominator) != 4 and denominator.shape != img.shape:\n        denominator = np.array(denominator.tolist() + [1] * (4 - len(denominator)), dtype=np.float64)\n\n    img = np.ascontiguousarray(img.astype(\"float32\"))\n    img -= mean.astype(np.float64)\n    img *= denominator.astype(np.float64)\n    return img\n\n", "idx": 288}
{"namespace": "albumentations.augmentations.functional.gamma_transform", "completion": "    if img.dtype == np.uint8:\n        invGamma = 1.0 / gamma\n        table = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n        return cv2.LUT(img, table)\n\n    if img.dtype == np.uint16:\n        invGamma = 1.0 / gamma\n        table = np.array([((i / 65535.0) ** invGamma) * 65535 for i in np.arange(0, 65536)]).astype(\"uint16\")\n        return cv2.LUT(img, table)\n\n    if img.dtype == np.float32:\n        return np.power(img, gamma)\n\n    raise ValueError(\"Unsupported image type\")\n\n", "idx": 289}
{"namespace": "albumentations.augmentations.functional.swap_tiles_on_image", "completion": "    # Create a copy of the input image\n    output_image = image.copy()\n\n    # Iterate over the tiles\n    for tile in tiles:\n        # Extract the current and old left-up corner coordinates, height, and width of the tile\n        current_left_up_corner_x, current_left_up_corner_y, old_left_up_corner_x, old_left_up_corner_y, height, width = tile\n\n        # Extract the current tile from the input image\n        current_tile = image[current_left_up_corner_y:current_left_up_corner_y + height, current_left_up_corner_x:current_left_up_corner_x + width]\n\n        # Extract the old tile from the output image\n        old_tile = output_image[old_left_up_corner_y:old_left_up_corner_y + height, old_left_up_corner_x:old_left_up_corner_x + width]\n\n        # Swap the tiles\n        output_image[current_left_up_corner_y:current_left_up_corner_y + height, current_left_up_corner_x:current_left_up_corner_x + width] = old_tile\n        output_image[old_left_up_corner_y:old_left_up_corner_y + height, old_left_up_corner_x:old_left_up_corner_x + width] = current_tile\n\n    return output_image", "idx": 290}
{"namespace": "albumentations.augmentations.geometric.functional.keypoint_rotate", "completion": "    x, y, a, s = keypoint[:4]\n    matrix = cv2.getRotationMatrix2D((cols / 2 - 0.5, rows / 2 - 0.5), angle, 1.0)\n    x, y = cv2.transform(np.array([[[x, y]]]), matrix).squeeze()\n    return x, y, a, s\n\n", "idx": 291}
{"namespace": "albumentations.augmentations.geometric.functional.keypoint_shift_scale_rotate", "completion": "    x, y, a, s = keypoint[:4]\n    center = (cols - 1) * 0.5, (rows - 1) * 0.5\n    matrix = cv2.getRotationMatrix2D(center, angle, scale)\n    matrix[0, 2] += dx * cols\n    matrix[1, 2] += dy * rows\n    x, y = cv2.transform(np.array([[[x, y]]]), matrix).squeeze()\n    return x, y, a + math.radians(angle), s\n\n", "idx": 292}
{"namespace": "albumentations.core.keypoints_utils.angle_to_2pi_range", "completion": "    two_pi = 2 * math.pi\n    return angle % two_pi\n\n", "idx": 293}
{"namespace": "albumentations.augmentations.geometric.functional.rot90", "completion": "    img = np.rot90(img, factor)\n    return np.ascontiguousarray(img)\n\n", "idx": 294}
{"namespace": "albumentations.core.keypoints_utils.convert_keypoints_to_albumentations", "completion": "    return [\n        convert_keypoint_to_albumentations(\n            kp, source_format, rows, cols, check_validity=check_validity, angle_in_degrees=angle_in_degrees\n        )\n        for kp in keypoints\n    ]\n\n", "idx": 295}
{"namespace": "albumentations.core.keypoints_utils.convert_keypoints_from_albumentations", "completion": "    return [\n        convert_keypoint_from_albumentations(kp, target_format, rows, cols, check_validity, angle_in_degrees)\n        for kp in keypoints\n    ]", "idx": 296}
{"namespace": "albumentations.core.transforms_interface.to_tuple", "completion": "    if low is not None and bias is not None:\n        raise ValueError(\"Arguments low and bias are mutually exclusive\")\n\n    if param is None:\n        return param\n\n    if isinstance(param, (int, float, list, tuple, np.ndarray)):\n        if low is None and bias is None:\n            param = -param, +param\n        elif low is None and bias is not None:\n            param = bias - param, bias + param\n        elif low is not None and bias is None:\n            param = (low - param, low + param)\n        else:\n            raise ValueError(\"Arguments low and bias are mutually exclusive\")\n    elif isinstance(param, Sequence):\n        param = tuple(param)\n    else:\n        raise ValueError(\"Argument param must be either scalar or tuple of 2 elements.\")\n\n    return tuple(param)\n\n", "idx": 297}
{"namespace": "albumentations.core.composition.ReplayCompose.replay", "completion": "        if not isinstance(saved_augmentations, dict):\n            raise ValueError(\"saved_augmentations must be a dictionary\")\n\n        if \"transforms\" not in saved_augmentations:\n            raise ValueError(\"saved_augmentations must contain a 'transforms' key\")\n\n        if \"params\" not in saved_augmentations:\n            raise ValueError(\"saved_augmentations must contain a 'params' key\")\n\n        if \"id\" not in saved_augmentations:\n            raise ValueError(\"saved_augmentations must contain an 'id' key\")\n\n        if \"applied\" not in saved_augmentations:\n            raise ValueError(\"saved_augmentations must contain an 'applied' key\")\n\n        if \"__class_fullname__\" not in saved_augmentations:\n            raise ValueError(\"saved_augmentations must contain a '__class_fullname__' key\")\n\n        if saved_augmentations[\"__class_fullname__\"] != ReplayCompose.get_class_fullname():\n            raise ValueError(\n                \"saved_augmentations must be a dictionary containing a ReplayCompose object, \"\n                \"but got {}\".format(saved_augmentations[\"__class_fullname__\"])\n            )\n\n        if saved_augmentations[\"id\"] not in SERIALIZABLE_REGISTRY:\n            raise ValueError(\n                \"saved_augmentations must be a dictionary containing a ReplayCompose object, \"\n                \"but got an object with id {}\".format(saved_augmentations[\"id\"])\n            )\n\n        if saved_augmentations[\"applied\"] is None:\n            raise ValueError(\"saved_augmentations must contain an 'applied' key\")\n\n        if saved_augmentations[\"applied\"] is False:\n            raise ValueError(\"saved_augmentations must contain an 'applied' key with value True\")\n\n        if saved_augmentations[\"params\"] is None:\n            raise ValueError(\"saved_augmentations must contain a 'params' key\")", "idx": 298}
{"namespace": "albumentations.core.serialization.shorten_class_name", "completion": "    if class_fullname.startswith(\"albumentations.\"):\n        return class_fullname[len(\"albumentations.\") :]\n    return class_fullname\n\n", "idx": 299}
{"namespace": "wandb.util.to_forward_slash_path", "completion": "    if platform.system() == \"Windows\":\n        return path.replace(\"\\\\\", \"/\")\n    return path", "idx": 300}
{"namespace": "wandb.util.make_artifact_name_safe", "completion": "    # Replace any characters that are not alphanumeric, dashes, underscores, or dots with underscores\n    cleaned_name = re.sub(r\"[^a-zA-Z0-9\\-\\_\\.]\", \"_\", name)\n\n    # If the length of the cleaned name is greater than 128, truncate it with dots in the middle\n    if len(cleaned_name) > 128:\n        cleaned_name = cleaned_name[:64] + \"...\" + cleaned_name[-64:]\n\n    return cleaned_name\n\n", "idx": 301}
{"namespace": "wandb.sdk.wandb_settings._redact_dict", "completion": "    return {k: redact_str if k in unsafe_keys else v for k, v in d.items()}\n\n", "idx": 302}
{"namespace": "wandb.sdk.launch.builder.build.get_current_python_version", "completion": "    # Get the current Python version\n    python_version = sys.version_info\n\n    # Get the major version of the Python\n    major_version = str(python_version.major)\n\n    # Get the full version of the Python\n    full_version = \".\".join(str(x) for x in python_version)\n\n    # Return the full version and the major version of the Python\n    return full_version, major_version\n\n", "idx": 303}
{"namespace": "wandb.sdk.artifacts.storage_policy.StoragePolicy.lookup_by_name", "completion": "        for sub in cls.__subclasses__():\n            if sub.name() == name:\n                return sub\n        raise NotImplementedError\n", "idx": 304}
{"namespace": "wandb.sdk.lib.runid.generate_id", "completion": "    return \"\".join(\n        secrets.choice(string.ascii_lowercase + string.digits) for _ in range(length)\n    )", "idx": 305}
{"namespace": "wandb.sdk.internal.file_stream.CRDedupeFilePolicy.get_consecutive_offsets", "completion": "        # Sort the offsets (line numbers) in ascending order\n        offsets = sorted(console.keys())\n\n        # Initialize an empty list to store the intervals\n        intervals = []\n\n        # Initialize the start and end of the current interval\n        start = end = offsets[0]\n\n        # Iterate over the sorted offsets\n        for offset in offsets[1:]:\n            # If the current offset is consecutive to the previous one, update the end of the current interval\n            if offset == end + 1:\n                end = offset\n            # If the current offset is not consecutive to the previous one, add the current interval to the list of intervals and start a new interval\n            else:\n                intervals.append([start, end])\n                start = end = offset\n\n        # Add the last interval to the list of intervals\n        intervals.append([start, end])\n\n        return intervals\n", "idx": 306}
{"namespace": "wandb.sdk.internal.system.assets.ipu.IPUStats.sample", "completion": "        try:\n            devices = self._gc_ipu_info.getDevices()\n            metrics = self._gc_ipu_info.getMetrics()\n        except Exception as e:\n            raise Exception(\n                \"Error while getting IPU devices and metrics: {}\".format(e)\n            ) from e\n\n        for device in devices:\n            if device[\"pid\"] != self._pid:\n                continue\n\n            device_name = device[\"name\"]\n            if device_name in self._devices_called:\n                continue\n\n            self._devices_called.add(device_name)\n\n            for key, value in metrics.items():\n                if key not in self.variable_metric_keys:\n                    continue\n\n                metric = self.parse_metric(key, value)\n                if metric is None:\n                    continue\n\n                metric_name, metric_value = metric\n                metric_name = self.name.format(device_name, metric_name)\n                self.samples.append({metric_name: metric_value})\n", "idx": 307}
{"namespace": "csvkit.cleanup.join_rows", "completion": "    rows = list(rows)\n\n    if not rows:\n        return []\n\n    fixed_row_length = len(rows[0])\n\n    for row in rows:\n        if len(row) != fixed_row_length:\n            raise LengthMismatchError('All rows must be the same length.')\n\n    new_row = []\n\n    for i in range(fixed_row_length):\n        new_cell = joiner.join([r[i] for r in rows if r[i]])\n        new_row.append(new_cell)\n\n    return new_row\n\n", "idx": 308}
{"namespace": "csvkit.convert.guess_format", "completion": "    if '.' in filename:\n        ext = filename.split('.')[-1]\n        if ext == 'csv':\n            return 'csv'\n        elif ext == 'dbf':\n            return 'dbf'\n        elif ext == 'fixed':\n            return 'fixed'\n        elif ext == 'xls':\n            return 'xls'\n        elif ext == 'xlsx':\n            return 'xlsx'\n        elif ext == 'js':\n            return 'json'\n        else:\n            return None\n    else:\n        return 'csv'\n\n", "idx": 309}
{"namespace": "folium.utilities.normalize", "completion": "    return re.sub(r\"\\s+\", \"\", rendered).strip()", "idx": 310}
{"namespace": "tpot.gp_deap.initialize_stats_dict", "completion": "    individual.stats = {}\n    individual.stats['generation'] = 0\n    individual.stats['mutation_count'] = 0\n    individual.stats['crossover_count'] = 0\n    individual.stats['predecessor'] = None", "idx": 311}
{"namespace": "bentoml_cli.env_manager.remove_env_arg", "completion": "    return [arg for arg in cmd_args if not re.match(r\"--env(=|\\s+)\", arg)]\n\n", "idx": 312}
{"namespace": "bentoml._internal.utils.uri.path_to_uri", "completion": "    path = os.path.abspath(path)\n    if os.name == \"nt\":\n        return path_to_uri_windows(path)\n    else:\n        return path_to_uri_unix(path)\n\n", "idx": 313}
{"namespace": "bentoml._internal.utils.uri.uri_to_path", "completion": "    parsed_uri = urlparse(uri)\n    if parsed_uri.scheme not in [\"file\", \"\"]:\n        raise ValueError(f\"Unsupported URI scheme: {parsed_uri.scheme}\")\n    path = url2pathname(unquote(parsed_uri.path))\n    if psutil.WINDOWS:\n        path = path.lstrip(\"/\")\n    return path", "idx": 314}
{"namespace": "bentoml._internal.utils.validate_labels", "completion": "    if not isinstance(labels, dict):\n        raise ValueError(f\"labels must be a dictionary, got {type(labels)}\")\n    for k, v in labels.items():\n        if not isinstance(k, str):\n            raise ValueError(f\"label key must be a string, got {type(k)}\")\n        if not isinstance(v, str):\n            raise ValueError(f\"label value must be a string, got {type(v)}\")\n\n", "idx": 315}
{"namespace": "bentoml._internal.configuration.helpers.is_valid_ip_address", "completion": "    try:\n        ipaddress.ip_address(addr)\n        return True\n    except ValueError:\n        return False\n\n", "idx": 316}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batches_to_batch", "completion": "        # Concatenate the batches along the specified batch dimension\n        batch: ext.PdDataFrame = pd.concat(batches, axis=batch_dim)\n\n        # Calculate the indices of the subbatches\n        indices = list(\n            itertools.accumulate(subbatch.shape[batch_dim] for subbatch in batches)\n        )\n        indices = [0] + indices\n\n        # Return the concatenated DataFrame and the indices of the subbatches\n        return batch, indices\n", "idx": 317}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batch_to_batches", "completion": "        import pandas as pd\n\n        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n        return [batch.iloc[i:j] for i, j in zip(indices, indices[1:])]\n", "idx": 318}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batches_to_batch", "completion": "        batch: list[t.Any] = list(itertools.chain.from_iterable(batches))\n        indices = list(\n            itertools.accumulate(subbatch.shape[batch_dim] for subbatch in batches)\n        )\n        indices = [0] + indices\n        return batch, indices\n", "idx": 319}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batch_to_batches", "completion": "        assert (\n            batch_dim == 0\n        ), \"Default Runner DataContainer does not support batch_dim other than 0\"\n        return [batch[indices[i] : indices[i + 1]] for i in range(len(indices) - 1)]\n", "idx": 320}
{"namespace": "jwt.utils.force_bytes", "completion": "    if isinstance(value, str):\n        return value.encode(\"utf-8\")\n    elif isinstance(value, bytes):\n        return value\n    else:\n        raise TypeError(\"Expected a string or bytes-like object\")\n\n", "idx": 321}
{"namespace": "pytube.cli.display_progress_bar", "completion": "    columns = shutil.get_terminal_size().columns\n    max_width = int(columns * scale)\n\n    filled = int(round(max_width * bytes_received / float(filesize)))\n    remaining = max_width - filled\n    bar = ch * filled + \" \" * remaining\n    percent = round(100.0 * bytes_received / float(filesize), 1)\n    text = f\" \u21b3 |{bar}| {percent}%\\r\"\n    sys.stdout.write(text)\n    sys.stdout.flush()", "idx": 322}
{"namespace": "pytube.cli._download", "completion": "    # Get the file size in megabytes\n    filesize = stream.filesize / 1024 / 1024\n    # Print the filename and file size\n    print(f\"{filename} ({filesize:.2f}MB)\")\n    # Download the file to the target location\n    stream.download(output_path=target, filename=filename)\n\n", "idx": 323}
{"namespace": "pytube.cli.display_streams", "completion": "    youtube.register_on_progress_callback(on_progress)\n    streams = youtube.streams.filter(progressive=True)\n    for stream in streams:\n        print(stream)", "idx": 324}
{"namespace": "pytube.cli._unique_name", "completion": "    if media_type == \"audio\":\n        filename = f\"{base}.{subtype}\"\n    else:\n        filename = f\"{base}.{subtype}\"\n    if not os.path.exists(os.path.join(target, filename)):\n        return filename\n    count = 1\n    while True:\n        new_name = f\"{base}_{count}.{subtype}\"\n        if not os.path.exists(os.path.join(target, new_name)):\n            return new_name\n        count += 1\n\n", "idx": 325}
{"namespace": "pytube.cli._print_available_captions", "completion": "    print(\"Available captions:\")\n    for caption in captions:\n        print(f\"{caption.code} ({caption.name})\")", "idx": 326}
{"namespace": "pytube.cipher.throttling_reverse", "completion": "    arr.reverse()\n\n", "idx": 327}
{"namespace": "pytube.helpers.setup_logger", "completion": "    # Create a logger instance\n    logger = logging.getLogger(__name__)\n\n    # Set the severity level of the logs to handle\n    logger.setLevel(level)\n\n    # Create a formatter for the logs\n    formatter = logging.Formatter(\n        \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n    )\n\n    # Create a stream handler for the logs\n    stream_handler = logging.StreamHandler()\n    stream_handler.setFormatter(formatter)\n    logger.addHandler(stream_handler)\n\n    # If a log filename is provided, create a file handler for the logs\n    if log_filename:\n        file_handler = logging.FileHandler(log_filename)\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n\n", "idx": 328}
{"namespace": "pytube.helpers.deprecated", "completion": "    def decorator(func: Callable[..., GenericType]) -> Callable:\n        @functools.wraps(func)\n        def new_func(*args, **kwargs) -> GenericType:\n            warnings.simplefilter(\"always\", DeprecationWarning)\n            warnings.warn(\n                f\"Call to deprecated function {func.__name__}. {reason}\",\n                category=DeprecationWarning,\n                stacklevel=2,\n            )\n            warnings.simplefilter(\"default\", DeprecationWarning)\n            return func(*args, **kwargs)\n\n        return new_func\n\n    return decorator\n\n", "idx": 329}
{"namespace": "pytube.helpers.uniqueify", "completion": "    seen = set()\n    seen_add = seen.add\n    return [x for x in duped_list if not (x in seen or seen_add(x))]", "idx": 330}
{"namespace": "pytube.helpers.target_directory", "completion": "    if output_path is None:\n        output_path = os.getcwd()\n    else:\n        if not os.path.isabs(output_path):\n            output_path = os.path.join(os.getcwd(), output_path)\n    if not os.path.exists(output_path):\n        os.makedirs(output_path)\n    return output_path\n\n", "idx": 331}
{"namespace": "pytube.extract.is_private", "completion": "    private_strings = [\n        'This is a private video.',\n        '/sorry?q=watch_login&redir=',  # noqa: E501\n        '/account_sharing.php'\n    ]\n    for string in private_strings:\n        if string in watch_html:\n            return True\n    return False\n\n", "idx": 332}
{"namespace": "pymc.math.cartesian", "completion": "    arrays = [np.asarray(x) for x in arrays]\n    shape = (len(x) for x in arrays)\n    ix = np.indices(shape, dtype=int)\n    ix = ix.reshape(len(arrays), -1).T\n    for n, arr in enumerate(arrays):\n        ix[:, n] = arrays[n][ix[:, n]]\n    return ix\n\n", "idx": 333}
{"namespace": "pymc.math.log1mexp", "completion": "    if negative_input:\n        return -log1pexp(-x)\n    else:\n        return -log1pexp(x)\n\n", "idx": 334}
{"namespace": "pymc.math.log1mexp_numpy", "completion": "    if not negative_input:\n        warnings.warn(\n            \"pymc.math.log1mexp will expect a negative input in a future \"\n            \"version of PyMC.\\n To suppress this warning set `negative_input=True`\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        x = -x\n\n    return np.log1p(-np.exp(x))\n\n", "idx": 335}
{"namespace": "pymc.util.drop_warning_stat", "completion": "    # Create a new InferenceData object with the same data and attributes as the input object\n    new_idata = arviz.InferenceData(\n        **{\n            key: value\n            for key, value in idata.items()\n            if key not in [\"sample_stats\", \"sample_stats_prior\"]\n        }\n    )\n\n    # Iterate over the sample stats groups in the input object\n    for group in [\"sample_stats\", \"sample_stats_prior\"]:\n        if group in idata:\n            # Create a new dictionary to store the sample stats for the new object\n            new_sample_stats = {}\n\n            # Iterate over the variables in the sample stats group\n            for var_name, var_data in idata[group].items():\n                # Check if the variable is the \"warning\" stat\n                if var_name != \"warning\":\n                    # If it's not the \"warning\" stat, add it to the new dictionary\n                    new_sample_stats[var_name] = var_data\n\n            # Add the new sample stats dictionary to the new InferenceData object\n            new_idata[group] = new_sample_stats\n\n    # Return the new InferenceData object\n    return new_idata", "idx": 336}
{"namespace": "pymc.pytensorf.walk_model", "completion": "    if stop_at_vars is None:\n        stop_at_vars = set()\n\n    seen = set()\n    stack = list(graphs)\n    while stack:\n        v = stack.pop()\n        if v in seen:\n            continue\n        seen.add(v)\n        if v in stop_at_vars:\n            continue\n        yield v\n        stack.extend(expand_fn(v))\n\n", "idx": 337}
{"namespace": "pymc.testing.select_by_precision", "completion": "    if pytensor.config.floatX == \"float64\":\n        return float64\n    else:\n        return float32\n\n", "idx": 338}
{"namespace": "pymc.gp.cov.handle_args", "completion": "    def new_func(x, args):\n        if args is None:\n            return func(x)\n        else:\n            return func(x, *args)\n\n    return new_func", "idx": 339}
{"namespace": "pymc.gp.util.kmeans_inducing_points", "completion": "    # Initialize inducing points using k-means\n    if X.shape[0] <= n_inducing:\n        warnings.warn(\n            \"The number of inducing points should be less than the \"\n            \"number of data points. Otherwise, inducing points will \"\n            \"not be used. n_inducing is set to {0}\".format(X.shape[0])\n        )\n        n_inducing = X.shape[0]\n\n    # Scale data to [0, 1]^d\n    X_max = X.max(0)\n    X_min = X.min(0)\n    X_scaled = (X - X_min) / (X_max - X_min)\n\n    # Use K-means to initialize inducing inputs\n    X_scaled_mean = X_scaled.mean(0)\n    X_scaled_std = X_scaled.std(0)\n    X_scaled_centered = (X_scaled - X_scaled_mean) / X_scaled_std\n    initial_centers, _ = kmeans(X_scaled_centered, n_inducing, **kmeans_kwargs)\n    initial_centers *= X_scaled_std\n    initial_centers += X_scaled_mean\n\n    # Scale back\n    initial_centers *= (X_max - X_min)\n    initial_centers += X_min\n\n    return initial_centers", "idx": 340}
{"namespace": "pymc.pytensorf.floatX", "completion": "    return pytensor.config.floatX(X)\n\n", "idx": 341}
{"namespace": "pymc.distributions.multivariate.posdef", "completion": "    try:\n        np.linalg.cholesky(AA)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n", "idx": 342}
{"namespace": "pymc.distributions.dist_math.multigammaln", "completion": "    if p <= 0:\n        raise ValueError(\"p should be greater than 0\")\n    if p == 1:\n        return gammaln(a)\n    return gammaln(a) + p * (pt.log(pt.pi) / 2.0) + gammaln(pt.tensor((p - 1) / 2.0))\n\n", "idx": 343}
{"namespace": "pymc.distributions.dist_math.incomplete_beta", "completion": "    return pt.betainc(a, b, value)", "idx": 344}
{"namespace": "pymc.sampling.forward.observed_dependent_deterministics", "completion": "    deterministics = model.deterministics\n    observed_rvs = model.observed_RVs\n    basic_rvs = model.basic_RVs\n\n    observed_dependent_deterministics = []\n\n    for deterministic in deterministics:\n        for input_node in deterministic.inputs:\n            if input_node in observed_rvs or input_node in basic_rvs:\n                observed_dependent_deterministics.append(deterministic)\n                break\n\n    return observed_dependent_deterministics\n\n", "idx": 345}
{"namespace": "pymc.smc.kernels.systematic_resampling", "completion": "    n = len(weights)\n    positions = (rng.random() + np.arange(n)) / n\n    index = 0\n    new_indices = np.zeros(n, dtype=int)\n    cumulative_sum = weights[0]\n    for i in range(n):\n        while positions[index] < cumulative_sum:\n            new_indices[i] = index\n            index += 1\n        cumulative_sum += weights[index]\n    return new_indices\n\n", "idx": 346}
{"namespace": "pymc.backends.base._squeeze_cat", "completion": "    if combine:\n        results = np.concatenate(results, axis=0)\n    if squeeze:\n        results = np.squeeze(results)\n    return results\n\n", "idx": 347}
{"namespace": "pymc.logprob.transforms.SimplexTransform.forward", "completion": "        log_value = pt.log(value)\n        sum_log_value = pt.sum(log_value, axis=-1, keepdims=True)\n        return log_value - sum_log_value\n", "idx": 348}
{"namespace": "pymc.logprob.transforms.SimplexTransform.backward", "completion": "        value = pt.as_tensor(value)\n        N = value.shape[-1].astype(value.dtype)\n        shift = pt.sum(value, -1, keepdims=True) / N\n        return pt.concatenate(\n            [pt.exp(value), pt.ones_like(value[..., :1]) / N],\n            -1,\n        )\n", "idx": 349}
{"namespace": "pymc.logprob.utils.walk_model", "completion": "    if stop_at_vars is None:\n        stop_at_vars = set()\n\n    def expand(var):\n        if isinstance(var, MeasurableVariable):\n            if not walk_past_rvs:\n                return []\n        return expand_fn(var)\n\n    def inner_walk(var):\n        if var in stop_at_vars:\n            return []\n        return expand(var)\n\n    for graph in graphs:\n        for var in walk(inner_walk, graph, expand):\n            yield var\n\n", "idx": 350}
{"namespace": "sacred.metrics_logger.linearize_metrics", "completion": "    metrics = {}\n    for metric in logged_metrics:\n        if metric.name not in metrics:\n            metrics[metric.name] = {\n                \"steps\": [],\n                \"values\": [],\n                \"timestamps\": [],\n            }\n        metrics[metric.name][\"steps\"].append(metric.step)\n        metrics[metric.name][\"values\"].append(metric.value)\n        metrics[metric.name][\"timestamps\"].append(metric.timestamp)\n    return metrics", "idx": 351}
{"namespace": "sacred.utils.set_by_dotted_path", "completion": "    if path == \"\":\n        raise ValueError(\"Path cannot be empty\")\n    if \".\" not in path:\n        d[path] = value\n    else:\n        key, rest = path.split(\".\", 1)\n        if key not in d:\n            d[key] = {}\n        set_by_dotted_path(d[key], rest, value)\n\n", "idx": 352}
{"namespace": "sacred.utils.get_by_dotted_path", "completion": "    split_path = path.split(\".\")\n    current_option = d\n    for p in split_path:\n        if p not in current_option:\n            return default\n        current_option = current_option[p]\n    return current_option", "idx": 353}
{"namespace": "pymc.logprob.scan.construct_scan", "completion": "    # Create a scan operation\n    scan_op = Scan(**kwargs)\n\n    # Create a node based on the input and output variables\n    node = scan_op.make_node(\n        *scan_args.inner_inputs,\n        *scan_args.inner_outputs,\n        *scan_args.outer_inputs,\n        *scan_args.outer_outputs,\n    )\n\n    # Return the node outputs and updates\n    return node.outputs, node.updates\n\n", "idx": 354}
{"namespace": "sacred.utils.is_prefix", "completion": "    return path.startswith(pre_path + \".\") or path == pre_path\n\n", "idx": 355}
{"namespace": "sacred.utils.get_inheritors", "completion": "    subclasses = set()\n    for subclass in cls.__subclasses__():\n        subclasses.add(subclass)\n        subclasses.update(get_inheritors(subclass))\n    return subclasses", "idx": 356}
{"namespace": "sacred.utils.convert_camel_case_to_snake_case", "completion": "    s1 = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1_\\2\", name)\n    return re.sub(\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", s1).lower()", "idx": 357}
{"namespace": "sacred.utils.module_exists", "completion": "    return pkgutil.find_loader(modname) is not None\n\n", "idx": 358}
{"namespace": "sacred.utils.apply_backspaces_and_linefeeds", "completion": "    lines = text.split(\"\\n\")\n    for i, line in enumerate(lines):\n        while \"\\b\" in line:\n            index = line.index(\"\\b\")\n            line = line[:index - 1] + line[index + 1:]\n        lines[i] = line\n    return \"\\n\".join(lines)\n\n", "idx": 359}
{"namespace": "sacred.commands.help_for_command", "completion": "    help_text = pydoc.render_doc(command, renderer=pydoc.plaintext)\n    help_text = help_text.replace(\"\\b\", \"\")\n    return help_text", "idx": 360}
{"namespace": "sacred.optional.optional_import", "completion": "    for package_name in package_names:\n        if modules_exist(package_name):\n            return True, importlib.import_module(package_name)\n    return False, None\n\n", "idx": 361}
{"namespace": "sacred.dependencies.get_py_file_if_possible", "completion": "    if pyc_name.endswith((\".py\", \".so\", \".pyd\", \".ipynb\")):\n        return pyc_name\n    py_name = pyc_name[:-1]\n    if os.path.isfile(py_name):\n        return py_name\n    return pyc_name\n\n", "idx": 362}
{"namespace": "sacred.config.custom_containers.DogmaticDict.update", "completion": "        if iterable is not None:\n            if hasattr(iterable, 'keys'):\n                for k, v in iterable.items():\n                    self[k] = v\n            else:\n                for k, v in iterable:\n                    self[k] = v\n        for k, v in kwargs.items():\n            self[k] = v\n", "idx": 363}
{"namespace": "sacred.config.config_scope.is_empty_or_comment", "completion": "    line = line.strip()\n    return not line or line.startswith(SETTINGS.CONFIG.COMMENT)", "idx": 364}
{"namespace": "boltons.funcutils.copy_function", "completion": "    if not isinstance(orig, FunctionType):\n        raise TypeError('orig must be a function, not %s' % type(orig))\n    if not _IS_PY2:\n        # Python 3\n        if not orig.__code__.co_freevars:\n            # No free variables, so we can just use the built-in copy.\n            return orig.__copy__()\n    # Python 2\n    # TODO: handle slots?\n    # TODO: handle __closure__?\n    # TODO: handle __dict__?\n    # TODO: handle __annotations__?\n    # TODO: handle __kwdefaults__?\n    # TODO: handle __defaults__?\n    # TODO: handle __kwdefaults__?\n    # TODO: handle __annotations__?\n    # TODO: handle __kwdefaults__?\n    # TODO: handle __annotations__?\n    # TODO: handle __kwdefaults__?\n    # TODO: handle __annotations__?\n    # TODO: handle __kwdefaults__?\n    # TODO: handle __annotations__?\n    # TODO: handle __kwdefaults__?\n    # TODO: handle __annotations__?\n    # TODO: handle __kwdefaults__?\n    # TODO: handle __annotations__?\n    # TODO: handle __kwdefaults__?\n    # TODO: handle __annotations__?\n    # TODO: handle __kwdefaults__?\n    # TODO: handle __annotations__?\n    # TODO: handle __kwdefaults__?\n    # TODO: handle __annotations__?\n    # TODO: handle __kwdefaults__?\n    # TODO: handle __annotations__?\n    # TODO: handle __kwdefaults__?\n    # TODO: handle __annotations__?\n    # TODO: handle __kwdefaults__?\n    # TODO: handle __annotations__?\n    # TODO: handle __kwdefaults__?\n    # TODO: handle __annotations__?\n    # TODO: handle __kwdefaults__?\n    # TODO: handle __annotations__?\n    # TODO: handle __kwdefaults__?\n    # TODO: handle __annotations__?\n    # TODO: handle __kwdefaults__?\n    # TODO", "idx": 365}
{"namespace": "sacred.config.config_scope.dedent_line", "completion": "    if line.startswith(indent):\n        return line[len(indent) :]\n    else:\n        return line\n\n", "idx": 366}
{"namespace": "boltons.funcutils.format_invocation", "completion": "    if kwargs is None:\n        kwargs = {}\n    if kw:\n        kwargs.update(kw)\n    if kwargs:\n        kwargs = ', '.join('{}={}'.format(k, v) for k, v in kwargs.items())\n    else:\n        kwargs = ''\n    if args:\n        args = ', '.join(map(repr, args))\n    else:\n        args = ''\n    if kwargs:\n        kwargs = ', ' + kwargs\n    return '{}({}{})'.format(name, args, kwargs)\n\n", "idx": 367}
{"namespace": "boltons.listutils.SplayList.shift", "completion": "        item = self.pop(item_index)\n        self.insert(dest_index, item)\n", "idx": 368}
{"namespace": "boltons.strutils.gzip_bytes", "completion": "    return zlib.compress(bytestring, level)\n\n", "idx": 369}
{"namespace": "boltons.strutils.is_uuid", "completion": "    if isinstance(obj, uuid.UUID):\n        return obj.version == version\n    try:\n        return uuid.UUID(obj, version=version).version == version\n    except ValueError:\n        return False\n\n", "idx": 370}
{"namespace": "boltons.strutils.parse_int_list", "completion": "    # Split the input range string into individual elements\n    elements = range_string.split(delim)\n\n    # Initialize an empty list to store the parsed integers\n    parsed_ints = []\n\n    # Iterate over each element in the input range string\n    for element in elements:\n        # Check if the element contains a range delimiter\n        if range_delim in element:\n            # Split the element into start and end values\n            start, end = element.split(range_delim)\n            # Convert the start and end values to integers\n            start, end = int(start), int(end)\n            # Add all integers in the range [start, end] to the parsed_ints list\n            parsed_ints.extend(range(start, end + 1))\n        else:\n            # Convert the element to an integer and add it to the parsed_ints list\n            parsed_ints.append(int(element))\n\n    # Sort the parsed_ints list in ascending order\n    parsed_ints.sort()\n\n    # Return the sorted list of parsed integers\n    return parsed_ints\n\n", "idx": 371}
{"namespace": "boltons.cacheutils.ThresholdCounter.get", "completion": "        return self._count_map.get(key, [default, 0])[0]\n", "idx": 372}
{"namespace": "boltons.iterutils.backoff_iter", "completion": "    if not isinstance(start, (int, float)):\n        raise TypeError('start must be a number')\n    if not isinstance(stop, (int, float)):\n        raise TypeError('stop must be a number')\n    if start <= 0:\n        raise ValueError('start must be positive')\n    if stop <= 0:\n        raise ValueError('stop must be positive')\n    if start >= stop:\n        raise ValueError('start must be less than stop')\n    if factor <= 0:\n        raise ValueError('factor must be positive')\n    if count is not None and not isinstance(count, int):\n        raise TypeError('count must be an integer')\n    if count is not None and count < 0:\n        raise ValueError('count must be positive')\n    if jitter and not isinstance(jitter, (int, float)):\n        raise TypeError('jitter must be a number')\n    if jitter and not -1.0 <= jitter <= 1.0:\n        raise ValueError('jitter must be between -1.0 and 1.0')\n\n    if count is None:\n        count = int(math.ceil(math.log(stop / start, factor)))\n    elif count == 'repeat':\n        count = None\n\n    if jitter:\n        if jitter > 0:\n            jitter_func = lambda x: x * (1.0 + jitter * random.random())\n        else:\n            jitter_func = lambda x: x * (1.0 - jitter * random.random())\n    else:\n        jitter_func = lambda x: x\n\n    cur = start\n    for _ in xrange(count):\n        yield jitter_func(cur)\n        cur *= factor\n        if cur >= stop:\n            break\n    if cur < stop:\n        yield stop\n\n", "idx": 373}
{"namespace": "boltons.cacheutils.cached", "completion": "    def decorator(func):\n        return CachedFunction(func, cache, scoped=scoped, typed=typed, key=key)\n    return decorator\n\n", "idx": 374}
{"namespace": "boltons.timeutils.total_seconds", "completion": "    return (td.microseconds + (td.seconds + td.days * 24 * 3600) * 10**6) / 10**6\n\n", "idx": 375}
{"namespace": "boltons.gcutils.get_all", "completion": "    # TODO: type survey\n    # TODO: docstring\n    # TODO: test\n    # TODO: optimize\n    # TODO: add support for built-in types\n    # TODO: add support for types that don't have a __dict__ attribute\n    # TODO: add support for types that don't have a __class__ attribute\n    # TODO: add support for types that don't have a __bases__ attribute\n    # TODO: add support for types that don't have a __subclasses__ attribute\n    # TODO: add support for types that don't have a __mro__ attribute\n    # TODO: add support for types that don't have a __base__ attribute\n    # TODO: add support for types that don't have a __subclasshook__ attribute\n    # TODO: add support for types that don't have a __subclasscheck__ attribute\n    # TODO: add support for types that don't have a __subclasshook__ attribute\n    # TODO: add support for types that don't have a __subclasscheck__ attribute\n    # TODO: add support for types that don't have a __subclasshook__ attribute\n    # TODO: add support for types that don't have a __subclasscheck__ attribute\n    # TODO: add support for types that don't have a __subclasshook__ attribute\n    # TODO: add support for types that don't have a __subclasscheck__ attribute\n    # TODO: add support for types that don't have a __subclasshook__ attribute\n    # TODO: add support for types that don't have a __subclasscheck__ attribute\n    # TODO: add support for types that don't have a __subclasshook__ attribute\n    # TODO: add support for types that don't have a __subclasscheck__ attribute\n    # TODO: add support for types that don't have a __subclasshook__ attribute\n    # TODO: add support for types that don't have a __subclasscheck__ attribute\n    # TODO: add support for types that don't have a __subclasshook__ attribute\n    # TODO: add support for types that don't have a __subclasscheck__ attribute\n    # TODO: add support for types", "idx": 376}
{"namespace": "boltons.timeutils.daterange", "completion": "    if isinstance(step, timedelta):\n        step = step.days\n    elif isinstance(step, tuple):\n        step = timedelta(*step).days\n    if step == 0:\n        raise ValueError('step cannot be 0')\n    if step < 0:\n        if stop is None:\n            raise ValueError('stop cannot be None when step is negative')\n        if start < stop:\n            raise ValueError('start must be greater than stop when step is negative')\n    elif step > 0:\n        if stop is None:\n            raise ValueError('stop cannot be None when step is positive')\n        if start > stop:\n            raise ValueError('start must be less than stop when step is positive')\n    if inclusive:\n        if stop is None:\n            raise ValueError('stop cannot be None when inclusive is True')\n        if step < 0:\n            while start >= stop:\n                yield start\n                start += step\n        else:\n            while start <= stop:\n                yield start\n                start += step\n    else:\n        if step < 0:\n            while start > stop:\n                yield start\n                start += step\n        else:\n            while start < stop:\n                yield start\n                start += step\n\n", "idx": 377}
{"namespace": "boltons.mathutils.clamp", "completion": "    return min(max(x, lower), upper)\n\n", "idx": 378}
{"namespace": "boltons.mathutils.ceil", "completion": "    if options is None:\n        return _ceil(x)\n    else:\n        options = sorted(options)\n        for option in options:\n            if option >= x:\n                return option\n        return options[-1]\n\n", "idx": 379}
{"namespace": "boltons.formatutils.get_format_args", "completion": "    # TODO: memoize\n    pos_args, kw_args = [], []\n    # look for {: or {![ or {. or {[ or {}\n    start, end, prev_end = 0, 0, 0\n    for match in _pos_farg_re.finditer(fstr):\n        start, end, group = match.start(), match.end(), match.group()\n        if prev_end < start:\n            ret += fstr[prev_end:start]\n        prev_end = end\n        if group == '{{' or group == '}}':\n            ret += group\n            continue\n        ret += '{%s%s' % (max_anon, group[1:])\n        max_anon += 1\n    ret += fstr[prev_end:]\n    return ret\n\n", "idx": 380}
{"namespace": "boltons.mathutils.floor", "completion": "    if options is None:\n        return _floor(x)\n    options = sorted(options)\n    i = bisect.bisect_right(options, x)\n    if i == 0:\n        raise ValueError(\"no floor options less than or equal to: %r\" % x)\n    return options[i - 1]", "idx": 381}
{"namespace": "boltons.dictutils.OneToOne.setdefault", "completion": "        if key in self:\n            return self[key]\n        self[key] = default\n        return default\n", "idx": 382}
{"namespace": "boltons.dictutils.OneToOne.update", "completion": "        if isinstance(dict_or_iterable, dict):\n            for key, val in dict_or_iterable.items():\n                self[key] = val\n        else:\n            for key, val in dict_or_iterable:\n                self[key] = val\n        for key, val in kw.items():\n            self[key] = val\n", "idx": 383}
{"namespace": "boltons.dictutils.ManyToMany.get", "completion": "        return self.data.get(key, default)\n", "idx": 384}
{"namespace": "boltons.dictutils.FrozenDict.updated", "completion": "        if a:\n            if a[0] is _OTO_INV_MARKER:\n                self.inv = a[1]\n                dict.__init__(self, [(v, k) for k, v in self.inv.items()])\n                return\n            elif a[0] is _OTO_UNIQUE_MARKER:\n                a, raise_on_dupe = a[1:], True\n\n        dict.__init__(self, *a, **kw)\n        self.inv = self.__class__(_OTO_INV_MARKER, self)\n\n        if len(self) == len(self.inv):\n            # if lengths match, that means everything's unique\n            return\n\n        if not raise_on_dupe:\n            dict.clear(self)\n            dict.update(self, [(v, k) for k, v in self.inv.items()])\n            return\n\n        # generate an error message if the values aren't 1:1\n\n        val_multidict = {}\n        for k, v in self.items():\n            val_multidict.setdefault(v, []).append(k)\n\n        dupes = dict([(v, k_list) for v, k_list in\n                      val_multidict.items() if len(k_list) > 1])\n\n        raise ValueError('expected unique values, got multiple keys for'\n                         ' the following values: %r' % dupes)\n", "idx": 385}
{"namespace": "boltons.dictutils.subdict", "completion": "    if keep is None:\n        keep = d.keys()\n    if drop is None:\n        drop = []\n    return dict([(k, d[k]) for k in keep if k not in drop])", "idx": 386}
{"namespace": "boltons.dictutils.FrozenDict.__repr__", "completion": "        cn = self.__class__.__name__\n        dict_repr = dict.__repr__(self)\n        return f'{cn}({dict_repr})'\n", "idx": 387}
{"namespace": "gunicorn.config.validate_callable", "completion": "    if isinstance(val, str):\n        try:\n            module_name, obj_name = val.rsplit(\".\", 1)\n            module = importlib.import_module(module_name)\n            val = getattr(module, obj_name)\n        except (ImportError, AttributeError):\n            raise TypeError(\"Invalid callable: %s\" % val)\n\n    if not callable(val):\n        raise TypeError(\"Invalid callable: %s\" % val)\n\n    if arity != -1 and len(inspect.signature(val).parameters) != arity:\n        raise TypeError(\"Invalid callable: %s\" % val)\n\n    return val\n\n", "idx": 388}
{"namespace": "gunicorn.config.get_default_config_file", "completion": "    path = os.path.join(util.getcwd(), 'gunicorn.conf.py')\n    if os.path.exists(path):\n        return path\n    return None\n\n", "idx": 389}
{"namespace": "gunicorn.util.is_ipv6", "completion": "    try:\n        socket.inet_pton(socket.AF_INET6, addr)\n    except socket.error:\n        return False\n    return True", "idx": 390}
{"namespace": "gunicorn.systemd.listen_fds", "completion": "    if \"LISTEN_PID\" not in os.environ or os.environ[\"LISTEN_PID\"] != str(os.getpid()):\n        return 0\n\n    if \"LISTEN_FDS\" not in os.environ:\n        return 0\n\n    if unset_environment:\n        del os.environ[\"LISTEN_PID\"]\n        del os.environ[\"LISTEN_FDS\"]\n\n    return int(os.environ[\"LISTEN_FDS\"])\n\n", "idx": 391}
{"namespace": "gunicorn.util.http_date", "completion": "    if timestamp is None:\n        timestamp = time.time()\n    return email.utils.formatdate(timestamp, usegmt=True)\n\n", "idx": 392}
{"namespace": "gunicorn.util.parse_address", "completion": "    # Check if the address is a unix socket\n    if netloc.startswith('unix:'):\n        # Remove the 'unix:' prefix\n        netloc = netloc[5:]\n        # If the address is a file descriptor, return the file descriptor\n        if netloc.startswith('fd:'):\n            fd = int(netloc[3:])\n            return fd\n        # Otherwise, return the unix socket path\n        else:\n            return netloc\n\n    # Check if the address is a file descriptor\n    if netloc.startswith('fd:'):\n        # Remove the 'fd:' prefix\n        netloc = netloc[3:]\n        # Return the file descriptor\n        return int(netloc)\n\n    # Check if the address is a TCP address\n    if netloc.startswith('tcp:'):\n        # Remove the 'tcp:' prefix\n        netloc = netloc[4:]\n\n    # Split the address into host and port\n    host, port = netloc.split(':')\n\n    # If the port is not specified, use the default port\n    if not port:\n        port = default_port\n\n    # Return the host and port\n    return host, int(port)\n\n", "idx": 393}
{"namespace": "gunicorn.util.to_bytestring", "completion": "    if isinstance(value, bytes):\n        return value\n    return value.encode(encoding)\n\n", "idx": 394}
{"namespace": "gunicorn.util.warn", "completion": "    warnings.warn(msg, RuntimeWarning, stacklevel=2)", "idx": 395}
{"namespace": "gunicorn.util.split_request_uri", "completion": "    # Check if the URI starts with \"//\"\n    if uri.startswith(\"//\"):\n        # Add a temporary dot prefix to the URI\n        uri = \".\" + uri\n\n    # Split the URI into its components\n    scheme, netloc, path, query, fragment = urllib.parse.urlsplit(uri)\n\n    # Remove the temporary dot prefix from the path\n    if path.startswith(\".\"):\n        path = path[1:]\n\n    # Return the components of the URI\n    return scheme, netloc, path, query, fragment", "idx": 396}
{"namespace": "praw.models.listing.listing.ModNoteListing.after", "completion": "        if not self.has_next_page:\n            return None\n        return self.end_cursor", "idx": 397}
{"namespace": "praw.models.util.permissions_string", "completion": "    if permissions is None:\n        return \"+all\"\n    known_permissions = set(known_permissions)\n    permissions = set(permissions)\n    additions = permissions - known_permissions\n    subtractions = known_permissions - permissions\n    return \",\".join(\n        sorted(\n            [\"+\" + permission for permission in additions]\n            + [\"-\" + permission for permission in subtractions]\n            + [\n                permission\n                for permission in permissions.intersection(known_permissions)\n                if not permission.startswith(\"+\") and not permission.startswith(\"-\")\n            ]\n        )\n    )\n\n", "idx": 398}
{"namespace": "jc.cli.JcCli.json_out", "completion": "        if self.mono:\n            return json.dumps(self.data_out, separators=self.json_separators, indent=self.json_indent)\n\n        class JcStyle(Style):\n            styles: CustomColorType = self.custom_colors\n\n        return str(highlight(json.dumps(self.data_out, separators=self.json_separators, indent=self.json_indent), JsonLexer(), Terminal256Formatter(style=JcStyle))[0:-1])\n", "idx": 399}
{"namespace": "pythonforandroid.pythonpackage.transform_dep_for_pip", "completion": "    if \"@\" in dependency and \"://\" in dependency:\n        return dependency\n    else:\n        return dependency.replace(\" \", \"==\")\n\n", "idx": 400}
{"namespace": "pythonforandroid.graph.fix_deplist", "completion": "    new_deps = []\n    for dep in deps:\n        if isinstance(dep, str):\n            new_deps.append((dep.lower(),))\n        else:\n            new_deps.append(tuple(map(str.lower, dep)))\n    return new_deps\n\n", "idx": 401}
{"namespace": "pythonforandroid.util.walk_valid_filens", "completion": "    for root, dirs, files in walk(base_dir):\n        # Remove invalid directories\n        for invalid_dir_name in invalid_dir_names:\n            if invalid_dir_name in dirs:\n                dirs.remove(invalid_dir_name)\n\n        # Yield valid files\n        for file in files:\n            full_path = join(root, file)\n            if not any(fnmatch(full_path, pattern) for pattern in invalid_file_patterns):\n                yield full_path\n\n", "idx": 402}
{"namespace": "pythonforandroid.bootstrap._cmp_bootstraps_by_priority", "completion": "    a_priority = default_recipe_priorities.index(a.name) if a.name in default_recipe_priorities else len(default_recipe_priorities)\n    b_priority = default_recipe_priorities.index(b.name) if b.name in default_recipe_priorities else len(default_recipe_priorities)\n    if a_priority == b_priority:\n        return cmp(a.name, b.name)\n    return a_priority - b_priority", "idx": 403}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.all_bootstraps", "completion": "        bootstraps_dir = join(cls.ctx.root_dir, 'bootstraps')\n        bootstraps = set()\n        for bootstrap_dir in listdir(bootstraps_dir):\n            if isdir(join(bootstraps_dir, bootstrap_dir)):\n                bootstraps.add(bootstrap_dir)\n        return bootstraps\n", "idx": 404}
{"namespace": "mmcv.image.colorspace._convert_input_type_range", "completion": "    img_type = img.dtype\n    img = img.astype(np.float32)\n    if img_type == np.float32:\n        pass\n    elif img_type == np.uint8:\n        img /= 255.\n    else:\n        raise TypeError('The img type should be np.float32 or np.uint8, ' f'but got {img_type}')\n    return img\n\n", "idx": 405}
{"namespace": "mackup.utils.error", "completion": "    print(\"Error: {}\".format(message))\n    sys.exit(1)", "idx": 406}
{"namespace": "mmcv.image.colorspace._convert_output_type_range", "completion": "    if dst_type not in (np.uint8, np.float32):\n        raise TypeError('The dst_type should be np.float32 or np.uint8, '\n                        f'but got {dst_type}')\n    if dst_type == np.uint8:\n        img = img.round()\n    else:\n        img /= 255.\n    return img.astype(dst_type)\n\n", "idx": 407}
{"namespace": "mackup.utils.is_process_running", "completion": "    try:\n        subprocess.check_output([\"pgrep\", process_name])\n        return True\n    except subprocess.CalledProcessError:\n        return False", "idx": 408}
{"namespace": "stellar.operations._get_pid_column", "completion": "    server_version = raw_conn.engine.dialect.server_version_info\n    if server_version[0] == 10 and server_version[1] == 3:\n        return 'pid'\n    else:\n        return 'id'", "idx": 409}
{"namespace": "imapclient.imap_utf7.encode", "completion": "    if isinstance(s, str):\n        return s.encode(\"utf-7\")\n    return s\n\n", "idx": 410}
{"namespace": "imapclient.version._imapclient_version_string", "completion": "    major, minor, micro, releaselevel = vinfo\n    v = \"%d.%d.%d\" % (major, minor, micro)\n    if releaselevel != \"final\":\n        v += \"-\" + releaselevel\n    return v", "idx": 411}
{"namespace": "telethon.helpers.generate_key_data_from_nonce", "completion": "    server_nonce = server_nonce.to_bytes(16, 'little', signed=True)\n    new_nonce = new_nonce.to_bytes(32, 'little', signed=True)\n\n    hash1 = sha1(new_nonce + server_nonce).digest()\n    hash2 = sha1(server_nonce + new_nonce).digest()\n    hash3 = sha1(new_nonce + new_nonce).digest()\n\n    key = hash1 + hash2[:12]\n    iv = hash2[12:20] + hash3 + new_nonce[:4]\n\n    return key, iv\n\n", "idx": 412}
{"namespace": "hbmqtt.codecs.bytes_to_int", "completion": "    return int.from_bytes(data, byteorder='big')\n\n", "idx": 413}
{"namespace": "zulipterminal.helper.display_error_if_present", "completion": "    if response.get(\"result\") == \"error\":\n        if hasattr(controller, \"view\"):\n            controller.view.report_error(response[\"msg\"])\n\n", "idx": 414}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._decode_message_id", "completion": "        try:\n            return int(message_id)\n        except ValueError:\n            return None\n", "idx": 415}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton.handle_narrow_link", "completion": "        parsed_link = self._parse_narrow_link(self.link)\n        validation_error = self._validate_narrow_link(parsed_link)\n        if validation_error:\n            self.view.footer.show_error_status(validation_error)\n        else:\n            self._switch_narrow_to(parsed_link)\n\n", "idx": 416}
{"namespace": "zulipterminal.config.color.color_properties", "completion": "    class ColorProperties(Enum):\n        pass\n\n    for color in colors:\n        for p in prop:\n            setattr(ColorProperties, f\"{color.name}_{p}\", f\"{color.value}_{p}\")\n\n    return ColorProperties\n\n", "idx": 417}
{"namespace": "twilio.base.deserialize.decimal", "completion": "    if d is None:\n        return d\n    try:\n        return Decimal(d, context=BasicContext)\n    except (TypeError, ValueError):\n        return d", "idx": 418}
{"namespace": "twilio.base.deserialize.integer", "completion": "    try:\n        return int(i)\n    except (TypeError, ValueError):\n        return i", "idx": 419}
{"namespace": "twilio.base.serialize.object", "completion": "    if isinstance(obj, str):\n        return obj\n    try:\n        return json.dumps(obj)\n    except (TypeError, ValueError):\n        return obj", "idx": 420}
{"namespace": "twilio.base.serialize.map", "completion": "    return [serialize_func(e) for e in lst]", "idx": 421}
{"namespace": "twilio.base.obsolete.deprecated_method", "completion": "    def deprecated_method_wrapper(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            warnings.warn(\n                \"{} is a deprecated method. Please use {} instead.\".format(\n                    func.__name__, new_func.__name__\n                ),\n                DeprecationWarning,\n            )\n            return func(*args, **kwargs)\n\n        return wrapper\n\n    if new_func is None:\n        return deprecated_method_wrapper\n    else:\n        return deprecated_method_wrapper(new_func)", "idx": 422}
{"namespace": "chatette.utils.sample_indulgent", "completion": "    if nb_items > len(array):\n        return deepcopy(array)\n    else:\n        return sample(array, nb_items)\n\n", "idx": 423}
{"namespace": "chatette.utils.rchop", "completion": "    if string.endswith(ending):\n        return string[:-len(ending)]\n    return string\n\n", "idx": 424}
{"namespace": "chatette.utils.str_to_bool", "completion": "    if text == 'True':\n        return True\n    elif text == 'False':\n        return False\n    else:\n        raise ValueError(\"The input string is not 'True' or 'False'.\")", "idx": 425}
{"namespace": "chatette.utils.min_if_exist", "completion": "    if n1 is None:\n        return n2\n    if n2 is None:\n        return n1\n    return min(n1, n2)\n\n", "idx": 426}
{"namespace": "chatette.utils.append_to_list_in_dict", "completion": "    if key in dict_of_lists:\n        dict_of_lists[key].append(value)\n    else:\n        dict_of_lists[key] = [value]", "idx": 427}
{"namespace": "chatette.utils.extend_list_in_dict", "completion": "    if key not in dict_of_lists:\n        dict_of_lists[key] = values\n    else:\n        dict_of_lists[key].extend(values)", "idx": 428}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy._is_end_regex", "completion": "        if word.endswith('$'):\n            word = rchop(word, 1)\n            if word.endswith(VARIATION_SYM):\n                word = rchop(word, 1)\n            if word.endswith(ESCAPEMENT_SYM):\n                word = rchop(word, 1)\n            if word.endswith(REGEX_SYM):\n                return True\n        return False\n", "idx": 429}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.execute", "completion": "        raise NotImplementedError(\"This method should be overriden by subclasses.\")", "idx": 430}
{"namespace": "aioxmpp.network.group_and_order_srv_records", "completion": "    if rng is None:\n        rng = random.Random()\n\n    records = sorted(all_records, key=lambda x: (x[0], rng.random()))\n\n    for prio, records in itertools.groupby(records, lambda x: x[0]):\n        records = sorted(records, key=lambda x: x[1])\n        for _, _, record in records:\n            yield record", "idx": 431}
{"namespace": "aioxmpp.nonza.StreamFeatures.get_feature", "completion": "        try:\n            return self[feature_cls]\n        except KeyError:\n            return default", "idx": 432}
{"namespace": "aioxmpp.connector.XMPPOverTLSConnector._context_factory_factory", "completion": "        ssl_context = metadata.ssl_context_factory()\n        if hasattr(ssl_context, \"set_alpn_protocols\"):\n            ssl_context.set_alpn_protocols([\"xmpp-client\"])\n        verifier.setup_context(ssl_context, None)\n        return ssl_context\n", "idx": 433}
{"namespace": "aioxmpp.xmltestutils.element_path", "completion": "    path = []\n    while el is not upto:\n        path.append(el.tag)\n        el = el.getparent()\n    path.reverse()\n    return '/'.join(path)", "idx": 434}
{"namespace": "aioxmpp.structs.JID.fromstr", "completion": "        if not s:\n            raise ValueError(\"JID must not be empty\")\n\n        if s[0] == \"@\":\n            raise ValueError(\"JID must not start with @\")\n\n        if s[-1] == \"/\":\n            raise ValueError(\"JID must not end with /\")\n\n        if \"@\" in s:\n            localpart, domain = s.rsplit(\"@\", 1)\n        else:\n            localpart = None\n            domain = s\n\n        if \"/\" in domain:\n            domain, resource = domain.rsplit(\"/\", 1)\n        else:\n            resource = None\n\n        return cls(localpart, domain, resource, strict=strict)\n\n", "idx": 435}
{"namespace": "aioxmpp.security_layer.extract_python_dict_from_x509", "completion": "    result = {}\n    for i in range(x509.get_extension_count()):\n        ext = x509.get_extension(i)\n        if ext.get_short_name() == b\"subjectAltName\":\n            result[\"subjectAltName\"] = ext.__str__()\n        if ext.get_short_name() == b\"subject\":\n            result[\"subject\"] = ext.__str__()\n    return result\n\n", "idx": 436}
{"namespace": "aioxmpp.security_layer.extract_blob", "completion": "    # Extract the ASN.1 blob from the certificate\n    asn1_blob = x509.to_cryptography().public_bytes(\n        encoding=serialization.Encoding.DER)\n    return asn1_blob", "idx": 437}
{"namespace": "aioxmpp.security_layer.blob_to_pyasn1", "completion": "    return pyasn1.codec.der.decoder.decode(blob, asn1Spec=pyasn1_modules.rfc2459.Certificate())", "idx": 438}
{"namespace": "aioxmpp.security_layer.extract_pk_blob_from_pyasn1", "completion": "    return pyasn1.codec.der.encoder.encode(\n        pyasn1_struct.getComponentByName(\"tbsCertificate\").getComponentByName(\"subjectPublicKeyInfo\")\n    )", "idx": 439}
{"namespace": "aioxmpp.callbacks.AdHocSignal.ASYNC_WITH_LOOP", "completion": "        if loop is None:\n            loop = asyncio.get_event_loop()\n        return functools.partial(cls._async_wrapper, loop=loop)\n", "idx": 440}
{"namespace": "aioxmpp.callbacks.AdHocSignal.SPAWN_WITH_LOOP", "completion": "        if loop is None:\n            loop = asyncio.get_event_loop()\n\n        def create_wrapper(f):\n            if not asyncio.iscoroutinefunction(f):\n                raise TypeError(\"must be coroutine, got {f}\".format(f=f))\n            return functools.partial(cls._spawn_wrapper, f, loop)\n\n        return create_wrapper\n", "idx": 441}
{"namespace": "aioxmpp.callbacks.first_signal", "completion": "    fut = asyncio.Future()\n\n    def on_signal(*args):\n        if len(args) > 0:\n            arg, = args\n        else:\n            arg = None\n        if not fut.done():\n            if isinstance(arg, Exception):\n                fut.set_exception(arg)\n            else:\n                fut.set_result(arg)\n        return True\n\n    for signal in signals:\n        signal.connect(on_signal)\n\n    return fut", "idx": 442}
{"namespace": "aioxmpp.tasks.TaskPool.spawn", "completion": "", "idx": 443}
{"namespace": "aioxmpp.protocol.send_and_wait_for", "completion": "    fut = asyncio.Future(loop=xmlstream._loop)\n\n    def cb_wrapper(msg):\n        if fut.done():\n            return\n        fut.set_result(msg)\n        if cb is not None:\n            cb(msg)\n\n    xmlstream.stanza_parser.add_class(wait_for, cb_wrapper)\n    xmlstream.send_xso(send)\n    try:\n        return await asyncio.wait_for(fut, timeout, loop=xmlstream._loop)\n    except asyncio.TimeoutError:\n        raise TimeoutError(\"Timeout waiting for response\")\n    finally:\n        xmlstream.stanza_parser.remove_class(wait_for, cb_wrapper)\n\n", "idx": 444}
{"namespace": "aioxmpp.testutils.run_coroutine_with_peer", "completion": "    if not loop:\n        loop = asyncio.get_event_loop()\n    fut = asyncio.Future()\n    fut.set_result(None)\n    fut2 = asyncio.ensure_future(coroutine)\n    fut3 = asyncio.ensure_future(peer_coroutine)\n    loop.run_until_complete(\n        asyncio.wait_for(\n            asyncio.gather(fut, fut2, fut3),\n            timeout=timeout))\n    return fut2.result()", "idx": 445}
{"namespace": "aioxmpp.testutils.make_listener", "completion": "    mock = unittest.mock.Mock()\n    for name in dir(instance):\n        if name.startswith(\"_\"):\n            continue\n        attr = getattr(instance, name)\n        if not isinstance(attr, callbacks.Signal):\n            continue\n        setattr(mock, name, unittest.mock.Mock())\n    return mock", "idx": 446}
{"namespace": "aioxmpp.vcard.service.VCardService.set_vcard", "completion": "        iq = aioxmpp.IQ(\n            type_=aioxmpp.IQType.SET,\n            to=jid,\n            payload=vcard,\n        )\n\n        await self.client.send(iq)", "idx": 447}
{"namespace": "aioxmpp.rsm.xso.ResultSetMetadata.limit", "completion": "        result = copy.deepcopy(self)\n        result.max_ = max_\n        return result\n", "idx": 448}
{"namespace": "aioxmpp.muc.service.Room.features", "completion": "        return self._features\n", "idx": 449}
{"namespace": "aioxmpp.xso.query.EvaluationContext.eval_bool", "completion": "        return bool(self.eval(expr))", "idx": 450}
{"namespace": "aioxmpp.xso.query._BoolOpMixin.eval", "completion": "        for leaf in self.eval_leaf(ec):\n            if leaf:\n                yield True\n                return\n        yield False\n\n", "idx": 451}
{"namespace": "aioxmpp.xso.model.drop_handler", "completion": "    depth = 1\n    while depth > 0:\n        ev_type, *ev_args = yield\n        if ev_type == \"start\":\n            depth += 1\n        elif ev_type == \"end\":\n            depth -= 1\n\n", "idx": 452}
{"namespace": "aioxmpp.xso.model.guard", "completion": "    depth = 1\n    while depth:\n        try:\n            ev = yield\n        except GeneratorExit:\n            try:\n                dest.close()\n            except StopIteration as e:\n                return e.value\n            raise\n        if ev[0] == \"start\":\n            depth += 1\n        elif ev[0] == \"end\":\n            depth -= 1\n        dest.send(ev)\n    try:\n        dest.close()\n    except StopIteration as e:\n        return e.value\n    raise\n\n", "idx": 453}
{"namespace": "aioxmpp.xso.model.capture_events", "completion": "    try:\n        while True:\n            ev = yield\n            dest.append(ev)\n            try:\n                receiver.send(ev)\n            except StopIteration as exc:\n                return exc.value\n    finally:\n        dest.clear()", "idx": 454}
{"namespace": "aioxmpp.xso.model.events_to_sax", "completion": "    for ev_type, *ev_args in events:\n        if ev_type == \"start\":\n            dest.startElementNS(ev_args[0], ev_args[1], ev_args[2])\n        elif ev_type == \"text\":\n            dest.characters(ev_args[0])\n        elif ev_type == \"end\":\n            dest.endElementNS(ev_args[0], ev_args[1])\n        else:\n            raise ValueError(\"unknown event type {ev_type}\".format(\n                ev_type=ev_type))", "idx": 455}
{"namespace": "aioxmpp.adhoc.service.AdHocClient.get_command_info", "completion": "        disco = self.dependencies[aioxmpp.disco.DiscoClient]\n        response = await disco.query_info(\n            peer_jid,\n            node=namespaces.xep0050_commands + \"#\" + command_name,\n        )\n        return response\n", "idx": 456}
{"namespace": "aioxmpp.entitycaps.caps115.build_identities_string", "completion": "    identities = [\n        b\"<\" + identity.category + b\"/\" + identity.type + b\"/\" + identity.lang + b\">\"\n        for identity in identities\n    ]\n    if len(identities) != len(set(identities)):\n        raise ValueError(\"duplicate identities\")\n    return b\"\".join(sorted(identities))\n\n", "idx": 457}
{"namespace": "aioxmpp.entitycaps.caps115.build_features_string", "completion": "    features = [escape(feature).encode(\"utf-8\") for feature in features]\n    if len(set(features)) != len(features):\n        raise ValueError(\"duplicate feature\")\n\n    features.sort()\n    features.append(b\"\")\n    return b\"<\".join(features)\n\n", "idx": 458}
{"namespace": "aioxmpp.entitycaps.caps115.build_forms_string", "completion": "    # Process the input forms and build a list of forms\n    forms = [\n        b\"/\".join([\n            escape(form.type_).encode(\"utf-8\"),\n            escape(form.ns).encode(\"utf-8\"),\n        ])\n        for form in forms\n    ]\n\n    # Check if there are duplicate forms\n    if len(set(forms)) != len(forms):\n        raise ValueError(\"duplicate form\")\n\n    # Sort the forms and build a string based on the sorted forms\n    forms.sort()\n    forms.append(b\"\")\n    return b\"<\".join(forms)\n\n", "idx": 459}
{"namespace": "aioxmpp.entitycaps.caps115.Key.path", "completion": "        return pathlib.Path(\"hashes\", self.algo, urllib.parse.quote(self.node))\n\n", "idx": 460}
{"namespace": "aioxmpp.entitycaps.caps390._process_features", "completion": "    return b\"\".join(\n        feature.encode(\"ascii\") + b\"\\x00\"\n        for feature in sorted(features)\n    )\n\n", "idx": 461}
{"namespace": "aioxmpp.entitycaps.caps390._process_identities", "completion": "    parts = [\n        _process_identity(identity)\n        for identity in identities\n    ]\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1c\"\n\n", "idx": 462}
{"namespace": "aioxmpp.entitycaps.caps390._process_extensions", "completion": "    parts = [\n        _process_form(form)\n        for form in exts\n    ]\n\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1c\"", "idx": 463}
{"namespace": "aioxmpp.entitycaps.caps390._calculate_hash", "completion": "    if algo == \"sha-1\":\n        return aioxmpp.hashes.sha1(hash_input).hexdigest()\n    elif algo == \"sha-256\":\n        return aioxmpp.hashes.sha256(hash_input).hexdigest()\n    elif algo == \"sha-384\":\n        return aioxmpp.hashes.sha384(hash_input).hexdigest()\n    elif algo == \"sha-512\":\n        return aioxmpp.hashes.sha512(hash_input).hexdigest()\n    else:\n        raise ValueError(\"Unsupported hash algorithm: {}\".format(algo))\n\n", "idx": 464}
{"namespace": "aioxmpp.entitycaps.caps390.Key.node", "completion": "        return \"\".join([\n            self.algo,\n            \"+\",\n            base64.b64encode(self.digest).decode(\"ascii\"),\n        ])\n", "idx": 465}
{"namespace": "aioxmpp.entitycaps.caps390.Key.path", "completion": "        return \"{}.{}\".format(\n            base64.b64encode(self.digest).decode(\"ascii\"),\n            self.algo,\n        )\n", "idx": 466}
{"namespace": "aioxmpp.entitycaps.caps390.Implementation.extract_keys", "completion": "        if presence.xep0390_caps is None:\n            return ()\n\n        return (\n            Key(\n                algo=presence.xep0390_caps.hash,\n                digest=base64.b64decode(presence.xep0390_caps.ver),\n            ),\n        )\n", "idx": 467}
{"namespace": "aioxmpp.roster.service.RosterClient.approve", "completion": "        self.client.send(\n            stanza.Presence(\n                type_=structs.PresenceType.SUBSCRIBED,\n                to=peer_jid\n            )\n        )\n", "idx": 468}
{"namespace": "aioxmpp.roster.service.RosterClient.subscribe", "completion": "        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.SUBSCRIBE,\n                            to=peer_jid)\n        )", "idx": 469}
{"namespace": "aioxmpp.roster.service.RosterClient.unsubscribe", "completion": "        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.UNSUBSCRIBE,\n                            to=peer_jid)\n        )", "idx": 470}
{"namespace": "aioxmpp.forms.fields.BoundSingleValueField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass\n", "idx": 471}
{"namespace": "aioxmpp.forms.fields.BoundMultiValueField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass\n", "idx": 472}
{"namespace": "aioxmpp.forms.fields.BoundOptionsField.options", "completion": "        try:\n            del self._options\n        except AttributeError:\n            pass\n", "idx": 473}
{"namespace": "aioxmpp.forms.fields.BoundSelectField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass\n", "idx": 474}
{"namespace": "aioxmpp.forms.fields.BoundMultiSelectField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass\n", "idx": 475}
{"namespace": "cupy.random._generator.reset_states", "completion": "    global _random_states\n    _random_states = {}\n\n", "idx": 476}
{"namespace": "cupy.random._generator._check_and_get_dtype", "completion": "    if dtype is None:\n        return numpy.float32\n    if isinstance(dtype, numpy.dtype):\n        if dtype.kind not in '?bBhHiIlLqQfd':\n            raise TypeError('Unsupported data type: {}'.format(dtype))\n        return dtype\n    if isinstance(dtype, str):\n        if dtype not in '?bBhHiIlLqQfd':\n            raise TypeError('Unsupported data type: {}'.format(dtype))\n        return numpy.dtype(dtype)\n    if isinstance(dtype, type):\n        if dtype not in (bool, numpy.bool_,\n                         numpy.int8, numpy.int16, numpy.int32, numpy.int64,\n                         numpy.uint8, numpy.uint16, numpy.uint32, numpy.uint64,\n                         numpy.float16, numpy.float32, numpy.float64):\n            raise TypeError('Unsupported data type: {}'.format(dtype))\n        return numpy.dtype(dtype)\n    raise TypeError('Unsupported data type: {}'.format(dtype))", "idx": 477}
{"namespace": "cupy_builder._command.filter_files_by_extension", "completion": "    sources_with_extension = []\n    sources_without_extension = []\n\n    for source in sources:\n        if source.endswith(extension):\n            sources_with_extension.append(source)\n        else:\n            sources_without_extension.append(source)\n\n    return sources_with_extension, sources_without_extension\n\n", "idx": 478}
{"namespace": "datasets.table._in_memory_arrow_table_from_file", "completion": "    with pa.memory_map(filename, \"r\") as source:\n        return pa.ipc.open_file(source).read_all()\n\n", "idx": 479}
{"namespace": "datasets.table._in_memory_arrow_table_from_buffer", "completion": "    buffer_reader = pa.BufferReader(buffer)\n    opened_stream = pa.ipc.open_stream(buffer_reader)\n    pa_table = opened_stream.read_all()\n    return pa_table\n\n", "idx": 480}
{"namespace": "datasets.table._interpolation_search", "completion": "    if not arr:\n        raise IndexError(\"The array is empty\")\n    if x < arr[0]:\n        raise IndexError(\"The query is outside the array values\")\n    if x >= arr[-1]:\n        return len(arr) - 1\n    lo = 0\n    hi = len(arr) - 1\n    while lo < hi:\n        mid = (lo + hi) // 2\n        if arr[mid] <= x < arr[mid + 1]:\n            return mid\n        elif arr[mid] < x:\n            lo = mid + 1\n        else:\n            hi = mid\n    raise IndexError(\"The query is outside the array values\")\n\n", "idx": 481}
{"namespace": "datasets.data_files._is_inside_unrequested_special_dir", "completion": "    # Check if the path is inside an unrequested special directory\n    if any(\n        matched_rel_path.startswith(special_dir)\n        for special_dir in [\n            \"__MACOSX\",\n            \".DS_Store\",\n            \".git\",\n            \".gitattributes\",\n            \".gitignore\",\n            \".hg\",\n            \".hgignore\",\n            \".hgsub\",\n            \".hgsubstate\",\n            \".svn\",\n            \"__pycache__\",\n            \"CVS\",\n            \"RCS\",\n            \"SCCS\",\n            \"_darcs\",\n            \"_sgbak\",\n            \"bzr\",\n            \"bzrignore\",\n            \"bzr-tags\",\n            \"bzr-templates\",\n            \"bzr-trunk\",\n            \"bzr-branch\",\n            \"bzr-branches\",\n            \"bzr-shelves\",\n            \"bzr-shelves-v1\",\n            \"bzr-shelves-v1-0\",\n            \"bzr-shelves-v1-1\",\n            \"bzr-shelves-v1-2\",\n            \"bzr-shelves-v1-3\",\n            \"bzr-shelves-v1-4\",\n            \"bzr-shelves-v1-5\",\n            \"bzr-shelves-v1-6\",\n            \"bzr-shelves-v1-7\",\n            \"bzr-shelves-v1-8\",\n            \"bzr-shelves-v1-9\",\n            \"bzr-shelves-v1-10\",\n            \"bzr-shelves-v1-11\",\n            \"bzr-shelves-v1-12\",\n            \"bzr-shelves-v1-13\",\n            \"bzr-shelves-v1-14\",\n            \"bzr-shelves-v1", "idx": 482}
{"namespace": "datasets.data_files._is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir", "completion": "    # We just need to check if every hidden directories from the path is present explicly in the pattern.\n    # Since we assume that the path matches the pattern, it's equivalent to counting that both\n    # the parent path and the parent pattern have the same number of hidden directories.\n    data_dirs_to_ignore_in_path = [part for part in PurePath(matched_rel_path).parent.parts if part.startswith(\".\")]\n    data_dirs_to_ignore_in_pattern = [part for part in PurePath(pattern).parent.parts if part.startswith(\".\")]\n    return len(data_dirs_to_ignore_in_path) != len(data_dirs_to_ignore_in_pattern)\n\n", "idx": 483}
{"namespace": "datasets.iterable_dataset._batch_to_examples", "completion": "    return [{col: batch[col][i] for col in batch} for i in range(len(batch[list(batch.keys())[0]]))]\n\n", "idx": 484}
{"namespace": "datasets.iterable_dataset._examples_to_batch", "completion": "    columns = {k: None for k in examples[0]}\n    arrays = [[] for _ in columns]\n    for example in examples:\n        for i, (column, array) in enumerate(zip(columns, arrays)):\n            array.append(example[column])\n    return dict(zip(columns, arrays))\n\n", "idx": 485}
{"namespace": "datasets.iterable_dataset.RandomlyCyclingMultiSourcesExamplesIterable._iter_random_indices", "completion": "        if p is None:\n            p = np.full(num_sources, 1 / num_sources)\n        while True:\n            yield from rng.choice(num_sources, size=random_batch_size, p=p, replace=True)\n", "idx": 486}
{"namespace": "datasets.iterable_dataset.BufferShuffledExamplesIterable._iter_random_indices", "completion": "        while True:\n            yield from (int(i) for i in rng.integers(0, buffer_size, size=random_batch_size))\n", "idx": 487}
{"namespace": "datasets.iterable_dataset.IterableDataset.remove_columns", "completion": "        if isinstance(column_names, str):\n            column_names = [column_names]\n        return self.map(remove_columns_fn, remove_columns=column_names)", "idx": 488}
{"namespace": "datasets.dataset_dict.DatasetDict.with_format", "completion": "        self._check_values_type()\n        return DatasetDict(\n            {\n                k: dataset.with_format(\n                    type=type, columns=columns, output_all_columns=output_all_columns, **format_kwargs\n                )\n                for k, dataset in self.items()\n            }\n        )\n", "idx": 489}
{"namespace": "datasets.dataset_dict.DatasetDict.with_transform", "completion": "        dataset = copy.deepcopy(self)\n        dataset.set_transform(transform=transform, columns=columns, output_all_columns=output_all_columns)\n        return dataset\n", "idx": 490}
{"namespace": "datasets.dataset_dict.DatasetDict.align_labels_with_mapping", "completion": "        self._check_values_type()\n        return DatasetDict({k: dataset.align_labels_with_mapping(label2id=label2id, label_column=label_column) for k, dataset in self.items()})\n", "idx": 491}
{"namespace": "datasets.dataset_dict.IterableDatasetDict.map", "completion": "        return IterableDatasetDict(\n            {\n                k: dataset.map(\n                    function=function,\n                    with_indices=with_indices,\n                    input_columns=input_columns,\n                    batched=batched,\n                    batch_size=batch_size,\n                    drop_last_batch=drop_last_batch,\n                    remove_columns=remove_columns,\n                    fn_kwargs=fn_kwargs,\n                )\n                for k, dataset in self.items()\n            }\n        )\n", "idx": 492}
{"namespace": "datasets.dataset_dict.IterableDatasetDict.filter", "completion": "        return IterableDatasetDict(\n            {\n                k: dataset.filter(\n                    function=function,\n                    with_indices=with_indices,\n                    input_columns=input_columns,\n                    batched=batched,\n                    batch_size=batch_size,\n                    fn_kwargs=fn_kwargs,\n                )\n                for k, dataset in self.items()\n            }\n        )", "idx": 493}
{"namespace": "datasets.arrow_dataset.Dataset.num_rows", "completion": "        if self._indices is not None:\n            return self._indices.num_rows\n        return self._data.num_rows\n", "idx": 494}
{"namespace": "datasets.filesystems.extract_path_from_uri", "completion": "    if \"://\" in dataset_path:\n        dataset_path = dataset_path.split(\"://\")[1]\n    return dataset_path\n\n", "idx": 495}
{"namespace": "datasets.filesystems.is_remote_filesystem", "completion": "    if isinstance(fs, fsspec.asyn.AsyncFileSystem):\n        fs = fs._ofs\n    return fs.protocol != \"file\"\n\n", "idx": 496}
{"namespace": "datasets.utils.file_utils.hash_url_to_filename", "completion": "    url_bytes = url.encode(\"utf-8\")\n    url_hash = sha256(url_bytes)\n    filename = url_hash.hexdigest()\n\n    if etag:\n        etag_bytes = etag.encode(\"utf-8\")\n        etag_hash = sha256(etag_bytes)\n        filename += \".\" + etag_hash.hexdigest()\n\n    # detect if file is h5\n    if url[-3:] == \".h5\":\n        filename += \".h5\"\n\n    return filename\n\n", "idx": 497}
{"namespace": "datasets.utils.hub.hf_hub_url", "completion": "    if version.parse(hfh.__version__) < version.parse(\"0.11.0\"):\n        # Old version of huggingface_hub don't handle special characters in path\n        path = quote(path, safe=\"/:\")\n    url = hfh.hf_hub_url(repo_id=repo_id, filename=path, revision=revision)\n    return url", "idx": 498}
{"namespace": "datasets.utils.sharding._number_of_shards_in_gen_kwargs", "completion": "    number_of_shards = None\n    for key, value in gen_kwargs.items():\n        if isinstance(value, list):\n            if number_of_shards is None:\n                number_of_shards = len(value)\n            elif number_of_shards != len(value):\n                raise ValueError(\n                    f\"The number of shards in the gen_kwargs is not consistent. The number of shards in the {key} list is {len(value)} but the number of shards in the previous list is {number_of_shards}.\"\n                )\n    return number_of_shards\n\n", "idx": 499}
{"namespace": "datasets.utils.sharding._distribute_shards", "completion": "    if num_shards < max_num_jobs:\n        return [range(i, i + 1) for i in range(num_shards)]\n    else:\n        shard_indices = np.arange(num_shards)\n        shard_indices = np.array_split(shard_indices, max_num_jobs)\n        return [range(indices[0], indices[-1] + 1) for indices in shard_indices]", "idx": 500}
{"namespace": "datasets.utils.py_utils.temporary_assignment", "completion": "    old_value = getattr(obj, attr)\n    setattr(obj, attr, value)\n    try:\n        yield\n    finally:\n        setattr(obj, attr, old_value)\n\n", "idx": 501}
{"namespace": "datasets.utils.extract.TarExtractor.extract", "completion": "        os.makedirs(output_path, exist_ok=True)\n        tar_file = tarfile.open(input_path)\n        tar_file.extractall(output_path, members=cls.safemembers(tar_file, output_path))\n        tar_file.close()\n\n", "idx": 502}
{"namespace": "datasets.utils.extract.Extractor.infer_extractor_format", "completion": "        magic_number_length = cls._get_magic_number_max_length()\n        magic_number = cls._read_magic_number(path, magic_number_length=magic_number_length)\n        for extractor_format, extractor in cls.extractors.items():\n            if extractor.is_extractable(path, magic_number=magic_number):\n                return extractor_format\n        return \"\"\n", "idx": 503}
{"namespace": "datasets.utils.py_utils.asdict", "completion": "    if is_dataclass(obj):\n        return asdict(obj.__dict__)\n    elif isinstance(obj, tuple) and hasattr(obj, \"_fields\"):\n        return type(obj)(*[asdict(v) for v in obj])\n    elif isinstance(obj, (list, tuple)):\n        return type(obj)(asdict(v) for v in obj)\n    elif isinstance(obj, dict):\n        return type(obj)((asdict(k), asdict(v)) for k, v in obj.items())\n    else:\n        return copy.deepcopy(obj)\n\n", "idx": 504}
{"namespace": "datasets.utils.metadata.MetadataConfigs.from_dataset_card_data", "completion": "        if cls.FIELD_NAME in dataset_card_data:\n            metadata_configs = dataset_card_data[cls.FIELD_NAME]\n            for metadata_config in metadata_configs.values():\n                cls._raise_if_data_files_field_not_valid(metadata_config)\n            return cls(metadata_configs)\n        else:\n            return cls()", "idx": 505}
{"namespace": "pymorphy2.analyzer.lang_dict_path", "completion": "    lang_paths = _lang_dict_paths()\n    if lang in lang_paths:\n        return lang_paths[lang]\n    else:\n        raise ValueError(\n            \"Unknown dictionary language '%s'. \"\n            \"Use pymorphy2.get_available_language_dicts() to get a list of available languages.\" % lang\n        )\n\n", "idx": 506}
{"namespace": "pymorphy2.dawg.assert_can_create", "completion": "    if not EXTENSION_AVAILABLE:\n        raise NotImplementedError(\"DAWG extension is not available\")", "idx": 507}
{"namespace": "pymorphy2.opencorpora_dict.compile._to_paradigm", "completion": "    stem = lexeme[0][0]\n    prefixes = [word_form[:-len(stem)] for word_form, _ in lexeme]\n\n    if not all(prefix in paradigm_prefixes for prefix in prefixes):\n        stem = ''\n        prefixes = [''] * len(lexeme)\n\n    paradigm = [\n        (word_form[len(prefix):], tag, prefix)\n        for (word_form, tag), prefix in izip(lexeme, prefixes)\n    ]\n\n    return stem, paradigm\n\n", "idx": 508}
{"namespace": "pymorphy2.units.by_analogy.KnownPrefixAnalyzer.tag", "completion": "        result = []\n        for prefix, unprefixed_word in self.possible_splits(word_lower):\n            method = (self, prefix)\n\n            tags = self.morph.tag(unprefixed_word)\n            for tag in tags:\n\n                if not tag.is_productive():\n                    continue\n\n                tag = add_tag_if_not_seen(tag, result, seen_tags)\n                tag.method = method\n\n        return result\n", "idx": 509}
{"namespace": "pymorphy2.units.by_analogy.UnknownPrefixAnalyzer.tag", "completion": "        result = []\n        for prefix, unprefixed_word in word_splits(word_lower):\n            for tag in self.dict_analyzer.tag(unprefixed_word, unprefixed_word, seen_tags):\n                if not tag.is_productive():\n                    continue\n                add_tag_if_not_seen(tag, result, seen_tags)\n        return result", "idx": 510}
{"namespace": "benedict.dicts.keylist.keylist_util.get_item", "completion": "    if not keys:\n        return d\n    key = keys[0]\n    subkeys = keys[1:]\n    try:\n        _, value = _get_item_key_and_value(d, key)\n        if not subkeys:\n            return value\n        return get_item(value, subkeys)\n    except (IndexError, KeyError, TypeError):\n        return (None,) * len(subkeys)\n\n", "idx": 511}
{"namespace": "benedict.dicts.keylist.keylist_util.set_item", "completion": "    item = d\n    for key in keys[:-1]:\n        item = _get_or_new_item_value(item, key, keys)\n    _set_item_value(item, keys[-1], value)", "idx": 512}
{"namespace": "benedict.dicts.keypath.keypath_util._split_key_indexes", "completion": "    if not key or not isinstance(key, str) or not key.endswith(\"]\"):\n        return [key]\n    if \"[\" not in key:\n        return [key]\n    indexes = []\n    while key.endswith(\"]\"):\n        match = re.search(KEY_INDEX_RE, key)\n        if not match:\n            break\n        index = match.group(1)\n        key = key[: -(len(match.group(0)))]\n        indexes.insert(0, index)\n    if key:\n        indexes.insert(0, key)\n    return indexes\n\n", "idx": 513}
{"namespace": "feedparser.urls.make_safe_absolute_uri", "completion": "    if not base:\n        return rel\n    if not rel:\n        return base\n    if not isinstance(base, str):\n        raise TypeError('base must be a string')\n    if not isinstance(rel, str):\n        raise TypeError('rel must be a string')\n    base = convert_to_idn(base)\n    rel = convert_to_idn(rel)\n    base = _urljoin(base, rel)\n    if urllib.parse.urlsplit(base)[0] not in ACCEPTABLE_URI_SCHEMES:\n        return ''\n    return base\n\n", "idx": 514}
{"namespace": "feedparser.api._open_resource", "completion": "    if isinstance(url_file_stream_or_string, io.IOBase):\n        # It's already an open file, so just return it\n        return url_file_stream_or_string\n\n    if isinstance(url_file_stream_or_string, str):\n        # It's a string, so we need to convert it to a file-like object\n        if url_file_stream_or_string.startswith(\"http://\") or url_file_stream_or_string.startswith(\"https://\"):\n            # It's a URL, so we need to open it with urllib2\n            import urllib.request\n            opener = urllib.request.build_opener(*handlers)\n            request = urllib.request.Request(url_file_stream_or_string)\n            if etag:\n                request.add_header(\"If-None-Match\", etag)\n            if modified:\n                if isinstance(modified, tuple):\n                    # Convert the modified time tuple to a string\n                    modified = time.strftime(\"%a, %d %b %Y %H:%M:%S GMT\", modified)\n                request.add_header(\"If-Modified-Since\", modified)\n            if agent:\n                request.add_header(\"User-Agent\", agent)\n            if referrer:\n                request.add_header(\"Referer\", referrer)\n            if request_headers:\n                for key, value in request_headers.items():\n                    request.add_header(key, value)\n            try:\n                response = opener.open(request)\n            except urllib.error.HTTPError as e:\n                if e.code == 304:\n                    # The content has not changed, so return None\n                    return None\n                else:\n                    # An error occurred, so raise an exception\n                    raise urllib.error.URLError(e.reason)\n            if result:\n                result[\"status\"] = response.status\n                result[\"etag\"] = response.getheader(\"ETag\")\n                result[\"modified\"] = response.getheader(\"Last-", "idx": 515}
{"namespace": "feedparser.http._build_urllib2_request", "completion": "    # Create a request object\n    request = urllib.request.Request(url)\n\n    # Add headers to the request object\n    request.add_header('User-Agent', agent)\n    if accept_header:\n        request.add_header('Accept', accept_header)\n    if etag:\n        request.add_header('If-None-Match', etag)\n    if modified:\n        if isinstance(modified, datetime.datetime):\n            modified = modified.strftime('%a, %d %b %Y %H:%M:%S GMT')\n        request.add_header('If-Modified-Since', modified)\n    if referrer:\n        request.add_header('Referer', referrer)\n    if auth:\n        request.add_header('Authorization', auth)\n    if request_headers:\n        for key, value in request_headers.items():\n            request.add_header(key, value)\n\n    return request\n\n", "idx": 516}
{"namespace": "pylatex.utils.dumps_list", "completion": "    if mapper is None:\n        mapper = []\n    elif callable(mapper):\n        mapper = [mapper]\n\n    if not isinstance(mapper, list):\n        raise ValueError(\"mapper must be a list of callables\")\n\n    if escape:\n        l = [escape_latex(s) for s in l]\n\n    for m in mapper:\n        l = [m(s) for s in l]\n\n    if as_content:\n        l = [pylatex.base_classes.LatexObject.dumps_as_content(s) for s in l]\n\n    return NoEscape(token.join(l))", "idx": 517}
{"namespace": "pylatex.utils._latex_item_to_string", "completion": "    if isinstance(item, pylatex.base_classes.LatexObject):\n        if as_content:\n            return item.dumps_as_content()\n        return item.dumps()\n\n    s = str(item)\n\n    if escape:\n        s = escape_latex(s)\n\n    return s", "idx": 518}
{"namespace": "mistune.markdown.Markdown.read", "completion": "        if state is None:\n            state = self.block.state_cls()\n\n        with open(filepath, 'rb') as f:\n            s = f.read()\n\n        s = s.decode(encoding)\n        return self.parse(s, state)", "idx": 519}
{"namespace": "mistune.create_markdown", "completion": "    if renderer == 'html':\n        renderer = HTMLRenderer(escape=escape, hard_wrap=hard_wrap)\n    if plugins:\n        for plugin in plugins:\n            plugin(renderer)\n    return Markdown(renderer)", "idx": 520}
{"namespace": "parsel.utils.extract_regex", "completion": "    if isinstance(regex, str):\n        regex = re.compile(regex, re.UNICODE)\n\n    if isinstance(text, bytes):\n        text = text.decode(\"utf-8\")\n\n    if replace_entities:\n        text = w3lib_replace_entities(text)\n\n    matches = regex.finditer(text)\n    strings = []\n    for match in matches:\n        if match.lastgroup is not None:\n            start, end = match.span(match.lastgroup)\n            strings.append(text[start:end])\n        elif match.groups():\n            strings.extend(match.groups())\n        else:\n            strings.append(match.group())\n\n    return flatten(strings)\n\n", "idx": 521}
{"namespace": "dominate.dom_tag.dom_tag.render", "completion": "    # pylint: disable=bad-indentation, bad-whitespace, missing-docstring\n\n    def _render(tag, indent, pretty, xhtml):\n      '''\n      Render the DOM tag and return the rendered output as a string. It recursively renders the tag and its children.\n      Input-Output Arguments\n      :param tag: dom_tag. An instance of the dom_tag class.\n      :param indent: String. The string used for indentation. Defaults to two spaces.\n      :param pretty: Bool. Whether to add line breaks and indentation for a prettier output. Defaults to True.\n      :param xhtml: Bool. Whether to use XHTML syntax. Defaults to False.\n      :return: String. The rendered output of the DOM tag.\n      '''\n\n      # pylint: disable=bad-indentation, bad-whitespace, missing-docstring\n\n      def _render_attr(attr, value):\n        '''\n        Render the attribute and return the rendered output as a string.\n        Input-Output Arguments\n        :param attr: String. The attribute name.\n        :param value: String. The attribute value.\n        :return: String. The rendered output of the attribute.\n        '''\n\n        # pylint: disable=bad-indentation, bad-whitespace, missing-docstring\n\n        if value is True:\n          return ' %s' % attr\n        elif value is False:\n          return ''\n        elif value is None:\n          return ''\n        else:\n          return ' %s=\"%s\"' % (attr, util.escape(value, quote=True))\n\n      # pylint: enable=bad-indentation, bad-whitespace, missing-docstring\n\n      # pylint: enable=bad-indentation, bad-whitespace, missing-docstring\n\n      # pylint: enable=bad-indentation, bad-whitespace, missing-docstring\n\n      # pylint: enable=bad-indentation, bad-whitespace, missing-docstring\n\n      # pylint: enable=bad-", "idx": 522}
{"namespace": "dominate.util.include", "completion": "  with open(f, 'r') as fp:\n    return fp.read()\n\n", "idx": 523}
{"namespace": "dominate.util.unescape", "completion": "  # TODO: This is a very basic implementation. It only handles a limited set of entities.\n  #       It should be extended to handle all HTML entities.\n  #       Also, it should be optimized for performance.\n  #       It should also handle unicode data.\n  #       It should also handle non-ascii data.\n  #       It should also handle non-string data.\n  #       It should also handle non-unicode data.\n  #       It should also handle non-string data.\n  #       It should also handle non-unicode data.\n  #       It should also handle non-string data.\n  #       It should also handle non-unicode data.\n  #       It should also handle non-string data.\n  #       It should also handle non-unicode data.\n  #       It should also handle non-string data.\n  #       It should also handle non-unicode data.\n  #       It should also handle non-string data.\n  #       It should also handle non-unicode data.\n  #       It should also handle non-string data.\n  #       It should also handle non-unicode data.\n  #       It should also handle non-string data.\n  #       It should also handle non-unicode data.\n  #       It should also handle non-string data.\n  #       It should also handle non-unicode data.\n  #       It should also handle non-string data.\n  #       It should also handle non-unicode data.\n  #       It should also handle non-string data.\n  #       It should also handle non-unicode data.\n  #       It should also handle non-string data.\n  #       It should also handle non-unicode data.\n  #       It should also handle non-string data.\n  #       It should also handle non-unicode data.\n  #       It should also handle non-string data.\n  #       It should also handle non-unicode data.\n  #       It should also handle non-string data.\n  #       It should also handle non-unicode data.\n  #       It should also handle non-string data.\n  #       It should also", "idx": 524}
{"namespace": "onlinejudge_command.pretty_printers._tokenize_line", "completion": "    tokens = _tokenize_str(line)\n    if tokens and tokens[-1].type == _PrettyTokenType.WHITESPACE:\n        tokens[-1] = _PrettyToken(_PrettyTokenType.BODY, tokens[-1].value[:-1])\n        tokens.append(_PrettyToken(_PrettyTokenType.WHITESPACE, ' '))\n    if tokens and tokens[-1].type == _PrettyTokenType.BODY:\n        tokens.append(_PrettyToken(_PrettyTokenType.NEWLINE, '\\n'))\n    return tokens\n\n", "idx": 525}
{"namespace": "onlinejudge_command.pretty_printers._render_tokens", "completion": "    if font_bold is None:\n        font_bold = lambda s: s\n    if font_dim is None:\n        font_dim = lambda s: s\n    if font_red is None:\n        font_red = lambda s: s\n    if font_blue is None:\n        font_blue = lambda s: s\n    if font_normal is None:\n        font_normal = lambda s: s\n\n    rendered = ''\n    for token in tokens:\n        if token.type == _PrettyTokenType.BODY:\n            rendered += token.value\n        elif token.type == _PrettyTokenType.BODY_HIGHLIGHT_LEFT:\n            rendered += font_bold(font_red(token.value))\n        elif token.type == _PrettyTokenType.BODY_HIGHLIGHT_RIGHT:\n            rendered += font_bold(font_blue(token.value))\n        elif token.type == _PrettyTokenType.WHITESPACE:\n            rendered += font_dim(font_normal(_replace_whitespace(token.value)))\n        elif token.type == _PrettyTokenType.NEWLINE:\n            rendered += token.value\n        elif token.type == _PrettyTokenType.HINT:\n            rendered += font_dim(font_normal(token.value))\n        elif token.type == _PrettyTokenType.LINENO:\n            rendered += font_dim(font_normal(token.value))\n        elif token.type == _PrettyTokenType.OTHERS:\n            rendered += font_dim(font_normal(token.value))\n        else:\n            assert False\n    return rendered\n\n", "idx": 526}
{"namespace": "onlinejudge_command.pretty_printers._tokenize_file_content_without_snipping", "completion": "    tokens, text = _decode_with_recovery(content)\n    for line in text.splitlines(keepends=True):\n        tokens += _tokenize_line(line)\n    tokens = _warn_if_empty(tokens)\n    return tokens\n\n", "idx": 527}
{"namespace": "jinja2.environment.Environment.get_template", "completion": "        if isinstance(name, Template):\n            return name\n\n        if parent is not None:\n            name = self.join_path(name, parent)\n\n        return self._load_template(name, globals)\n", "idx": 528}
{"namespace": "jinja2.environment.Environment.from_string", "completion": "        if template_class is None:\n            template_class = self.template_class\n        if isinstance(source, str):\n            source = self._parse(source, None, None)\n        return template_class.from_code(\n            self, source, globals, self.template_class.name\n        )\n", "idx": 529}
{"namespace": "jinja2.environment.Template.render", "completion": "        if self.environment.is_async:\n            return self.render_async(*args, **kwargs).get()\n        return self.render_sync(*args, **kwargs)\n", "idx": 530}
{"namespace": "jinja2.utils.generate_lorem_ipsum", "completion": "    words = (\n        \"lorem\",\n        \"ipsum\",\n        \"dolor\",\n        \"sit\",\n        \"amet\",\n        \"consectetuer\",\n        \"adipiscing\",\n        \"elit\",\n        \"sed\",\n        \"diam\",\n        \"nonummy\",\n        \"nibh\",\n        \"euismod\",\n        \"tincidunt\",\n        \"ut\",\n        \"laoreet\",\n        \"dolore\",\n        \"magna\",\n        \"aliquam\",\n        \"erat\",\n    )\n\n    def generate_paragraph(n: int, min: int, max: int) -> str:\n        \"\"\"\n        This function generates a paragraph of Lorem Ipsum text based on the given parameters. It creates a specified number of words, each containing a random number of characters within a given range. The generated text can be returned as plain text or as HTML paragraphs. It adds a comma after every 3 to 8 words, and a period after every 10 to 20 words. Every paragraph ends with a period. It also automatically capitalizes the first word of each sentence. If the text is rendered as HTML, it also adds a \"\\n\" between each paragraph. Otherwise, it adds a \"\\n\\n\" between each paragraph.\n        Input-Output Arguments\n        :param n: int. The number of words to generate.\n        :param min: int. The minimum number of characters in each word.\n        :param max: int. The maximum number of characters in each word.\n        :return: str. The generated Lorem Ipsum text. If html is True, the text is returned as HTML paragraphs. Otherwise, it is returned as plain text.\n        \"\"\"\n        paragraph = \"\"\n        for i in range(n):\n            if i == 0:\n                word = words[randrange(len(words))].capitalize()\n            else:\n                word = words[randrange(len(words))]\n            if i % 3 == 0 and i % 10 != 0:\n                word += \",\"\n            elif i % 10 == 0:\n                word += \".\"\n            paragraph += word", "idx": 531}
{"namespace": "jinja2.utils.LRUCache.clear", "completion": "        with self._wlock:\n            self._mapping.clear()\n            self._queue.clear()\n", "idx": 532}
{"namespace": "jinja2.utils.LRUCache.items", "completion": "        with self._wlock:\n            return [(key, self._mapping[key]) for key in reversed(self._queue)]\n", "idx": 533}
{"namespace": "jinja2.idtracking.symbols_for_node", "completion": "    sym = Symbols(parent=parent_symbols)\n    visitor = FrameSymbolVisitor(sym)\n    visitor.visit(node)\n    return sym\n\n", "idx": 534}
{"namespace": "jinja2.idtracking.Symbols.find_ref", "completion": "        if name in self.refs:\n            return self.refs[name]\n\n        if self.parent is not None:\n            return self.parent.find_ref(name)\n\n        return None\n", "idx": 535}
{"namespace": "jinja2.idtracking.Symbols.dump_stores", "completion": "        stores: t.Dict[str, str] = {}\n        for name in self.stores:\n            if name in self.refs:\n                stores[name] = self.refs[name]\n        if self.parent is not None:\n            stores.update(self.parent.dump_stores())\n        return stores", "idx": 536}
{"namespace": "jinja2.meta.find_undeclared_variables", "completion": "    codegen = TrackingCodeGenerator(ast.environment)\n    codegen.visit(ast)\n    return codegen.undeclared_identifiers", "idx": 537}
{"namespace": "jinja2.loaders.split_template_path", "completion": "    if os.sep in template or (os.altsep and os.altsep in template):\n        raise TemplateNotFound(template)\n    if os.path.pardir in template:\n        raise TemplateNotFound(template)\n    return template.split(\"/\")\n\n", "idx": 538}
{"namespace": "jinja2.bccache.MemcachedBytecodeCache.load_bytecode", "completion": "        key = self.prefix + bucket.key\n        try:\n            bytecode = self.client.get(key)\n        except Exception:\n            if not self.ignore_memcache_errors:\n                raise\n        else:\n            if bytecode is not None:\n                bucket.bytecode_from_string(bytecode)\n", "idx": 539}
{"namespace": "jinja2.bccache.MemcachedBytecodeCache.dump_bytecode", "completion": "        try:\n            self.client.set(\n                self.prefix + bucket.key, bucket.bytecode_to_string(), self.timeout\n            )\n        except Exception:\n            if not self.ignore_memcache_errors:\n                raise", "idx": 540}
{"namespace": "sumy.utils.get_stop_words", "completion": "    normalized_language = normalize_language(language)\n    path = join(\"stopwords\", normalized_language + \".txt\")\n    try:\n        return frozenset(to_unicode(pkgutil.get_data(\"sumy\", path).decode(\"utf-8\")).splitlines())\n    except IOError:\n        raise LookupError(\"Stop-words are not available for language %s.\" % language)", "idx": 541}
{"namespace": "sumy._compat.to_bytes", "completion": "    if isinstance(object, bytes):\n        return object\n    elif isinstance(object, unicode):\n        return object.encode(\"utf-8\")\n    else:\n        return to_unicode(object).encode(\"utf-8\")\n\n", "idx": 542}
{"namespace": "sumy._compat.to_unicode", "completion": "    if isinstance(object, unicode):\n        return object\n    elif isinstance(object, bytes):\n        return object.decode(\"utf-8\")\n    else:\n        # try decode instance to unicode\n        return instance_to_unicode(object)\n\n", "idx": 543}
{"namespace": "sumy.summarizers.lsa.LsaSummarizer._create_dictionary", "completion": "        words = map(self.normalize_word, document.words)\n        # words = [self.normalize_word(word) for word in document.words]\n        unique_words = frozenset(self._get_unique_words(words))\n\n        return dict((word, idx) for idx, word in enumerate(unique_words))\n", "idx": 544}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._get_content_words_in_sentence", "completion": "        normalized_words = map(self.normalize_word, sentence.words)\n        normalized_content_words = self._filter_out_stop_words(normalized_words)\n        return self._stem_words(normalized_content_words)\n", "idx": 545}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._get_all_content_words_in_doc", "completion": "        all_words = self._get_all_words_in_doc(sentences)\n        content_words = self._filter_out_stop_words(all_words)\n        normalized_content_words = self._normalize_words(content_words)\n        return normalized_content_words\n", "idx": 546}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_tf", "completion": "        content_words = self._get_all_content_words_in_doc(sentences)\n        content_words_count = len(content_words)\n        content_words_freq = self._compute_word_freq(content_words)\n\n        content_word_tf = dict.fromkeys(content_words_freq.keys(), 0)\n        for word, freq in content_words_freq.items():\n            content_word_tf[word] = freq / content_words_count\n\n        return content_word_tf\n", "idx": 547}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_ratings", "completion": "        word_freq = self._compute_tf(sentences)\n        ratings = {}\n        sentences_as_words = [self._get_content_words_in_sentence(s) for s in sentences]\n        for i in range(len(sentences)):\n            index = self._find_index_of_best_sentence(word_freq, sentences_as_words)\n            ratings[sentences[index]] = i\n            word_freq = self._update_tf(word_freq, sentences_as_words[index])\n            del sentences_as_words[index]\n        return ratings", "idx": 548}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.cue_method", "completion": "        method = self._build_cue_method_instance()\n        return self._get_best_sentences(document.sentences, sentences_count,\n            method.rate_sentences(document, bonus_word_value, stigma_word_value))\n", "idx": 549}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.key_method", "completion": "        summarization_method = self._build_key_method_instance()\n        return summarization_method(document, sentences_count, weight)\n", "idx": 550}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.title_method", "completion": "        summarization_method = self._build_title_method_instance()\n        return summarization_method(document, sentences_count)\n", "idx": 551}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.location_method", "completion": "        summarization_method = self._build_location_method_instance()\n        return summarization_method(document, sentences_count, w_h, w_p1, w_p2, w_s1, w_s2)\n", "idx": 552}
{"namespace": "sumy.summarizers.reduction.ReductionSummarizer.rate_sentences", "completion": "        sentences = document.sentences\n        length = len(sentences)\n\n        # Create a dictionary to store the ratings for each sentence\n        ratings = defaultdict(float)\n\n        # Create a dictionary to store the similarity between each pair of sentences\n        similarities = defaultdict(dict)\n\n        # Iterate over all pairs of sentences\n        for i, j in combinations(range(length), 2):\n            # Calculate the similarity between the two sentences\n            similarity = self.sentences_similarity(sentences[i], sentences[j])\n\n            # Store the similarity in the dictionary\n            similarities[i][j] = similarity\n            similarities[j][i] = similarity\n\n        # Iterate over all pairs of sentences\n        for i, j in combinations(range(length), 2):\n            # Calculate the rating for the two sentences\n            rating = self.rate_sentence_pair(similarities[i][j], similarities[i], similarities[j])\n\n            # Store the rating in the dictionary\n            ratings[i] += rating\n            ratings[j] += rating\n\n        # Return the ratings dictionary\n        return ratings\n", "idx": 553}
{"namespace": "sumy.summarizers.reduction.ReductionSummarizer._to_words_set", "completion": "        words = map(self.normalize_word, sentence.words)\n        return [self.stem_word(w) for w in words if w not in self._stop_words]\n", "idx": 554}
{"namespace": "sumy.summarizers.text_rank.TextRankSummarizer._to_words_set", "completion": "        words = map(self.normalize_word, sentence.words)\n        return [self.stem_word(w) for w in words if w not in self._stop_words]\n", "idx": 555}
{"namespace": "sumy.summarizers.kl.KLSummarizer.compute_tf", "completion": "        content_words = self._get_all_content_words_in_doc(sentences)\n        content_words_count = len(content_words)\n        content_words_freq = self._compute_word_freq(content_words)\n\n        content_word_tf = dict((w, f / content_words_count) for w, f in content_words_freq.items())\n        return content_word_tf\n", "idx": 556}
{"namespace": "sumy.evaluation.rouge._get_word_ngrams", "completion": "    assert (n > 0)\n    assert (len(sentences) > 0)\n\n    # Collects the n-grams for each sentence.\n    words = _split_into_words(sentences)\n    return _get_ngrams(n, words)", "idx": 557}
{"namespace": "sumy.evaluation.rouge._len_lcs", "completion": "    table = _lcs(x, y)\n    n, m = _get_index_of_lcs(x, y)\n    return table[n, m]\n\n", "idx": 558}
{"namespace": "sumy.evaluation.rouge._recon_lcs", "completion": "    i, j = _get_index_of_lcs(x, y)\n    table = _lcs(x, y)\n\n    def _recon(i, j):\n        if i == 0 or j == 0:\n            return []\n        elif x[i - 1] == y[j - 1]:\n            return _recon(i - 1, j - 1) + [(x[i - 1], i)]\n        elif table[i - 1, j] > table[i, j - 1]:\n            return _recon(i - 1, j)\n        else:\n            return _recon(i, j - 1)\n\n    recon_tuple = tuple(map(lambda x: x[0], _recon(i, j)))\n    return recon_tuple\n\n", "idx": 559}
{"namespace": "sumy.evaluation.rouge._union_lcs", "completion": "    if len(evaluated_sentences) <= 0:\n        raise (ValueError(\"Collections must contain at least 1 sentence.\"))\n\n    lcs_union = set()\n    reference_words = _split_into_words([reference_sentence])\n    combined_lcs_length = 0\n    for eval_s in evaluated_sentences:\n        evaluated_words = _split_into_words([eval_s])\n        lcs = set(_recon_lcs(reference_words, evaluated_words))\n        combined_lcs_length += len(lcs)\n        lcs_union = lcs_union.union(lcs)\n\n    union_lcs_count = len(lcs_union)\n    union_lcs_value = union_lcs_count / combined_lcs_length\n    return union_lcs_value\n\n", "idx": 560}
{"namespace": "sumy.parsers.html.HtmlParser.from_file", "completion": "        with open(file_path, \"rb\") as file:\n            return cls(file.read(), tokenizer, url)\n", "idx": 561}
{"namespace": "sumy.parsers.plaintext.PlaintextParser.document", "completion": "        document = ObjectDocumentModel()\n\n        current_paragraph = Paragraph(self._tokenizer)\n        for line in self._text.splitlines():\n            line = line.strip()\n\n            if not line:\n                if current_paragraph.is_valid():\n                    document.add_paragraph(current_paragraph)\n                current_paragraph = Paragraph(self._tokenizer)\n                continue\n\n            sentence = Sentence(line, self._tokenizer, is_heading=self._is_heading(line))\n            current_paragraph.add_sentence(sentence)\n\n        if current_paragraph.is_valid():\n            document.add_paragraph(current_paragraph)\n\n        return document\n", "idx": 562}
{"namespace": "sumy.nlp.tokenizers.Tokenizer.to_sentences", "completion": "        extra_abbreviations = self.LANGUAGE_EXTRA_ABREVS.get(self.language, [])\n        self._sentence_tokenizer._params.abbrev_types.update(extra_abbreviations)\n        sentences = self._sentence_tokenizer.tokenize(to_unicode(paragraph))\n        return tuple(map(unicode.strip, sentences))\n", "idx": 563}
{"namespace": "sumy.nlp.stemmers.null_stemmer", "completion": "    return unicode(object).lower()\n\n", "idx": 564}
{"namespace": "rows.fields.BinaryField.serialize", "completion": "        if value is None:\n            return \"\"\n        elif isinstance(value, six.binary_type):\n            try:\n                return b64encode(value).decode(\"ascii\")\n            except (binascii.Error, UnicodeDecodeError):\n                return value\n        else:\n            value_error(value, cls)\n", "idx": 565}
{"namespace": "rows.fields.BoolField.deserialize", "completion": "        value = super(BoolField, cls).deserialize(value, *args, **kwargs)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        value = str(value).lower()\n        if value in cls.TRUE_VALUES:\n            return True\n        elif value in cls.FALSE_VALUES:\n            return False\n        else:\n            raise ValueError(\"Value is not boolean\")\n\n", "idx": 566}
{"namespace": "rows.fields.DateField.serialize", "completion": "        if value is None:\n            return \"\"\n        else:\n            return value.strftime(cls.OUTPUT_FORMAT)\n", "idx": 567}
{"namespace": "rows.fields.DateField.deserialize", "completion": "        value = super(DateField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n        try:\n            value = datetime.datetime.strptime(value, cls.INPUT_FORMAT)\n        except ValueError:\n            value_error(value, cls)\n        return datetime.date(value.year, value.month, value.day)\n\n", "idx": 568}
{"namespace": "rows.fields.TextField.deserialize", "completion": "        value = super(TextField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            return six.text_type(value)\n\n", "idx": 569}
{"namespace": "rows.fields.EmailField.deserialize", "completion": "        value = super(EmailField, cls).deserialize(value)\n        if value is None or value == \"\":\n            return None\n\n        match = cls.EMAIL_REGEXP.match(value)\n        if match:\n            return match.group(0)\n        else:\n            raise ValueError(\"Invalid email format\")\n\n", "idx": 570}
{"namespace": "rows.fields.JSONField.deserialize", "completion": "        value = super(JSONField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            return json.loads(value)\n\n", "idx": 571}
{"namespace": "falcon.http_error.HTTPError.to_json", "completion": "        if handler is None:\n            import json\n\n            handler = json\n\n        return handler.dumps(self.to_dict()).encode('utf-8')\n", "idx": 572}
{"namespace": "falcon.inspect.inspect_app", "completion": "    routes = inspect_routes(app)\n    static_routes = inspect_static_routes(app)\n    sinks = inspect_sinks(app)\n    error_handlers = inspect_error_handlers(app)\n    middleware = inspect_middleware(app)\n    return AppInfo(\n        routes=routes,\n        static_routes=static_routes,\n        sinks=sinks,\n        error_handlers=error_handlers,\n        middleware=middleware,\n    )\n\n", "idx": 573}
{"namespace": "falcon.inspect.inspect_routes", "completion": "    router = app._router\n    routes = []\n    for route in router._roots:\n        routes.extend(_inspect_tree(route, []))\n    return routes\n\n", "idx": 574}
{"namespace": "falcon.inspect.inspect_static_routes", "completion": "    static_routes = []\n    for static_route, _ in app._static_routes:\n        uri_template = static_route._uri_template\n        if uri_template.startswith('file://'):\n            uri_template = uri_template[7:]\n        static_routes.append(StaticRouteInfo(uri_template, static_route._fs_path))\n    return static_routes\n\n", "idx": 575}
{"namespace": "falcon.inspect.inspect_sinks", "completion": "    sinks = []\n    for sink in app._sinks:\n        info = SinkInfo(sink._method, sink._uri_template, sink._handler)\n        sinks.append(info)\n    return sinks\n\n", "idx": 576}
{"namespace": "falcon.inspect.inspect_error_handlers", "completion": "    error_handlers = []\n    for exc, handlers in app._error_handlers.items():\n        for handler in handlers:\n            source_info, name = _get_source_info_and_name(handler)\n            info = ErrorHandlerInfo(exc, name, source_info)\n            error_handlers.append(info)\n    return error_handlers\n\n", "idx": 577}
{"namespace": "falcon.inspect.inspect_middleware", "completion": "    middleware_tree = _prepare_middleware_tree(app)\n    middleware_classes = _get_middleware_classes(app)\n    return MiddlewareInfo(middleware_tree, middleware_classes)\n\n", "idx": 578}
{"namespace": "falcon.inspect.InspectVisitor.process", "completion": "        visit_name = instance.__visit_name__\n        visit_method = getattr(self, 'visit_' + visit_name, None)\n        if visit_method is None:\n            raise RuntimeError(\n                'No visit method found for {}'.format(instance.__visit_name__)\n            )\n        return visit_method(instance)\n\n", "idx": 579}
{"namespace": "falcon.request.Request.forwarded", "completion": "        if self._cached_forwarded is None:\n            try:\n                self._cached_forwarded = parse_forwarded(self.env['HTTP_FORWARDED'])\n            except KeyError:\n                self._cached_forwarded = None\n\n        return self._cached_forwarded\n", "idx": 580}
{"namespace": "falcon.request.Request.client_accepts_msgpack", "completion": "        return self.client_accepts('application/x-msgpack') or self.client_accepts('application/msgpack')\n", "idx": 581}
{"namespace": "falcon.request.Request.content_length", "completion": "        try:\n            content_length = self.env['CONTENT_LENGTH']\n        except KeyError:\n            return None\n\n        try:\n            content_length = int(content_length)\n        except ValueError:\n            raise errors.HTTPBadRequest(\n                title='Invalid Content-Length header',\n                description=(\n                    'The Content-Length header value must be a positive integer.'\n                ),\n            )\n\n        if content_length < 0:\n            raise errors.HTTPBadRequest(\n                title='Invalid Content-Length header',\n                description=(\n                    'The Content-Length header value must be a positive integer.'\n                ),\n            )\n\n        return content_length\n", "idx": 582}
{"namespace": "falcon.request.Request.bounded_stream", "completion": "        if self._bounded_stream is None:\n            self._bounded_stream = BoundedStream(\n                self.stream, self.content_length\n            )\n\n        return self._bounded_stream\n", "idx": 583}
{"namespace": "falcon.request.Request.uri", "completion": "        if self._cached_uri is None:\n            self._cached_uri = '{0}://{1}{2}'.format(\n                self.scheme, self.netloc, self.relative_uri\n            )\n\n        return self._cached_uri\n", "idx": 584}
{"namespace": "falcon.request.Request.forwarded_uri", "completion": "        if self._cached_forwarded_uri is None:\n            # PERF: For small numbers of items, '+' is faster\n            # than ''.join(...). Concatenation is also generally\n            # faster than formatting.\n            value = self.forwarded_scheme + '://' + self.forwarded_host + self.relative_uri\n\n            self._cached_forwarded_uri = value\n\n        return self._cached_forwarded_uri\n", "idx": 585}
{"namespace": "falcon.request.Request.relative_uri", "completion": "        if self._cached_relative_uri is None:\n            # PERF: For small numbers of items, '+' is faster\n            # than ''.join(...). Concatenation is also generally\n            # faster than formatting.\n            value = self.app + self.path\n\n            if self.query_string:\n                value += '?' + self.query_string\n\n            self._cached_relative_uri = value\n\n        return self._cached_relative_uri\n", "idx": 586}
{"namespace": "falcon.request.Request.prefix", "completion": "        if self._cached_prefix is None:\n            self._cached_prefix = self.scheme + '://' + self.netloc + self.app\n\n        return self._cached_prefix\n", "idx": 587}
{"namespace": "falcon.request.Request.forwarded_prefix", "completion": "        if self._cached_forwarded_prefix is None:\n            self._cached_forwarded_prefix = (\n                self.forwarded_scheme + '://' + self.forwarded_host + self.app\n            )\n\n        return self._cached_forwarded_prefix\n", "idx": 588}
{"namespace": "falcon.request.Request.host", "completion": "        # Try to get the host information from the 'HTTP_HOST' header\n        host = self.env.get('HTTP_HOST')\n\n        # If the 'HTTP_HOST' header is not found, get the host information from the 'SERVER_NAME' field\n        if host is None:\n            host = self.env.get('SERVER_NAME')\n\n        # Return the host information\n        return host\n", "idx": 589}
{"namespace": "falcon.request.Request.subdomain", "completion": "        host = self.host\n        if host.startswith('['):\n            # IPv6 address\n            host = host.split(']')[0][1:]\n\n        subdomain, sep, remainder = host.partition('.')\n        if sep:\n            return subdomain\n        else:\n            return None\n", "idx": 590}
{"namespace": "falcon.request.Request.headers", "completion": "        if self._cached_headers is None:\n            self._cached_headers = {}\n            for key, value in self.env.items():\n                if key.startswith('HTTP_'):\n                    self._cached_headers[key[5:].replace('_', '-')] = value\n\n        return self._cached_headers.copy()\n", "idx": 591}
{"namespace": "falcon.request.Request.remote_addr", "completion": "        try:\n            return self.env['REMOTE_ADDR']\n        except KeyError:\n            return '127.0.0.1'\n", "idx": 592}
{"namespace": "falcon.request.Request.client_accepts", "completion": "        if self.accept is None:\n            return False\n\n        # NOTE(kgriffs): We use mimeparse.parse_media_range() here\n        # instead of mimetypes.parse_media_range() because the latter\n        # is not RFC-compliant.\n        parsed_accept = mimeparse.parse_media_range(self.accept)\n        parsed_media_type = mimeparse.parse_media_range(media_type)\n\n        return mimeparse.match_mime_type(parsed_media_type, parsed_accept)\n", "idx": 593}
{"namespace": "falcon.request.Request.client_prefers", "completion": "        accept = self.accept\n\n        # PERF(kgriffs): Usually the following will be true, so\n        # try it first.\n        if accept == '*/*':\n            return media_types[0]\n\n        # Fall back to full-blown parsing\n        try:\n            return mimeparse.best_match(media_types, accept)\n        except ValueError:\n            return None\n", "idx": 594}
{"namespace": "falcon.request.Request.get_header", "completion": "        name = name.upper().replace('-', '_')\n\n        try:\n            return self.env['HTTP_' + name]\n        except KeyError:\n            if required:\n                raise errors.HTTPBadRequest(\n                    'Missing header', '{0} header is required'.format(name)\n                )\n\n            return default\n", "idx": 595}
{"namespace": "falcon.request.Request.get_cookie_values", "completion": "        if self._cookies is None:\n            header_value = self.get_header('Cookie')\n            if header_value:\n                self._cookies = helpers.parse_cookie_header(header_value)\n            else:\n                self._cookies = {}\n\n        return self._cookies.get(name)\n", "idx": 596}
{"namespace": "falcon.response.Response.unset_cookie", "completion": "        if self._cookies is None:\n            return\n\n        if not is_ascii_encodable(name):\n            raise KeyError('name is not ascii encodable')\n\n        if domain:\n            self._cookies[name]['domain'] = domain\n\n        if path:\n            self._cookies[name]['path'] = path\n\n        self._cookies[name]['expires'] = 'Thu, 01 Jan 1970 00:00:00 GMT'\n        self._cookies[name]['max-age'] = 0\n        self._cookies[name]['secure'] = True\n        self._cookies[name]['httponly'] = True\n        self._cookies[name]['samesite'] = 'Lax'\n        self._cookies[name][''] = ''\n", "idx": 597}
{"namespace": "falcon.response.Response.get_header", "completion": "        name = name.lower()\n\n        if name == 'set-cookie':\n            raise HeaderNotSupported(\n                'get_header() does not support multiple values for '\n                'Set-Cookie'\n            )\n\n        try:\n            return format_header_value_list(self._headers[name])\n        except KeyError:\n            return default\n", "idx": 598}
{"namespace": "falcon.response.Response.set_header", "completion": "        # NOTE(kgriffs): normalize name by lowercasing it\n        name = name.lower()\n\n        if name == 'set-cookie':\n            raise HeaderNotSupported('Setting Set-Cookie is not currently supported.')\n\n        self._headers[name] = value\n", "idx": 599}
{"namespace": "falcon.response.Response.delete_header", "completion": "        # NOTE(kgriffs): normalize name by lowercasing it\n        name = name.lower()\n\n        if name == 'set-cookie':\n            raise HeaderNotSupported('This method cannot be used to set cookies')\n\n        if name in self._headers:\n            del self._headers[name]\n", "idx": 600}
{"namespace": "falcon.cmd.inspect_app.route_main", "completion": "    print(\n        'The \"falcon-print-routes\" command is deprecated. '\n        'Please use \"falcon-inspect-app\"',\n        file=sys.stderr,\n    )\n    print(\n        'Please use \"falcon-inspect-app\"',\n        file=sys.stderr,\n    )\n    main()", "idx": 601}
{"namespace": "falcon.util.uri.decode", "completion": "    if not encoded_uri:\n        return encoded_uri\n\n    if _cy_decode:\n        return _cy_decode(encoded_uri, unquote_plus)\n\n    # NOTE(kgriffs): This is a bit faster than using a regex, and we\n    # only need to do it once here, so we don't lose any performance\n    # by using a regex instead.\n    tokens = encoded_uri.split('%')\n\n    if len(tokens) == 1:\n        return encoded_uri\n\n    return _join_tokens(tokens)\n\n", "idx": 602}
{"namespace": "falcon.util.structures.ETag.dumps", "completion": "        if self.is_weak:\n            return 'W/' + self\n        return self", "idx": 603}
{"namespace": "falcon.util.structures.ETag.loads", "completion": "        if not etag_str:\n            raise ValueError('Invalid ETag string')\n\n        if etag_str[0] == 'W':\n            if etag_str[1] != '/':\n                raise ValueError('Invalid ETag string')\n\n            if etag_str[-1] != '\"':\n                raise ValueError('Invalid ETag string')\n\n            return cls(etag_str[2:-1], is_weak=True)\n\n        if etag_str[0] != '\"':\n            raise ValueError('Invalid ETag string')\n\n        if etag_str[-1] != '\"':\n            raise ValueError('Invalid ETag string')\n\n        return cls(etag_str[1:-1])\n", "idx": 604}
{"namespace": "falcon.util.misc.secure_filename", "completion": "    if isinstance(filename, str):\n        from unicodedata import normalize\n\n        filename = normalize('NFKD', filename).encode('ascii', 'ignore')\n        filename = filename.decode('ascii')\n\n    for sep in os.path.sep, os.path.altsep:\n        if sep:\n            filename = filename.replace(sep, ' ')\n    filename = str(_filename_ascii_strip_re.sub('', '_'.join(filename.split()))).strip(\n        '._'\n    )\n\n    # On Windows, the following characters are not allowed in filenames.\n    filename = (\n        filename.replace('\"', \"'\")\n        .replace(':', ' -')\n        .replace('<', '(')\n        .replace('>', ')')\n        .replace('|', '-')\n        .replace('\\0', '')\n    )\n\n    return filename", "idx": 605}
{"namespace": "falcon.asgi.reader.BufferedReader.peek", "completion": "        if size == -1 or size is None:\n            return self._buffer[self._buffer_pos :]\n\n        if size <= 0:\n            return b''\n\n        if size <= self._max_join_size:\n            if self._buffer_len > self._buffer_pos:\n                if size <= self._buffer_len - self._buffer_pos:\n                    return self._buffer[self._buffer_pos : self._buffer_pos + size]\n\n                result = [self._buffer[self._buffer_pos :]]\n                remaining = size - (self._buffer_len - self._buffer_pos)\n            else:\n                result = []\n                remaining = size\n\n            async for chunk in self._source:\n                chunk_len = len(chunk)\n                if remaining < chunk_len:\n                    result.append(chunk[:remaining])\n                    self._prepend_buffer(chunk[remaining:])\n                    break\n\n                result.append(chunk)\n                remaining -= chunk_len\n                if remaining == 0:  # pragma: no py39,py310 cover\n                    break\n\n            # PERF(vytas): local1 + local2 is faster than self._attr\n            #   (still true on CPython 3.8)\n            return result[0] if len(result) == 1 else b''.join(result)\n\n        # NOTE(vytas): size > self._max_join_size\n        if self._buffer_len > self._buffer_pos:\n            if size <= self._buffer_len - self._buffer_pos:\n                return self._buffer[self._buffer_pos : self._buffer_pos + size]\n\n            result = [self._buffer[self._buffer_pos :]]\n            remaining = size - (self._buffer_len - self._buffer_pos)\n        else:\n            result = []\n            remaining = size\n\n        async for chunk in self._source:\n            chunk_len = len(chunk)\n            if remaining < chunk_len:\n                result.append(chunk[:remaining])\n                self._prepend_buffer(chunk[rem", "idx": 606}
{"namespace": "falcon.asgi.reader.BufferedReader.read_until", "completion": "        return await self._read_from(\n            self._iter_delimited(delimiter, size_hint=size or 0), size\n        )\n", "idx": 607}
{"namespace": "falcon.routing.converters.IntConverter.convert", "completion": "        if self._num_digits is not None:\n            if len(value) != self._num_digits:\n                return None\n\n        if value.strip() != value:\n            return None\n\n        try:\n            value = int(value)\n        except ValueError:\n            return None\n\n        if self._min is not None and value < self._min:\n            return None\n\n        if self._max is not None and value > self._max:\n            return None\n\n        return value\n\n", "idx": 608}
{"namespace": "falcon.routing.converters.DateTimeConverter.convert", "completion": "        try:\n            return strptime(value, self._format_string)\n        except ValueError:\n            return None\n\n", "idx": 609}
{"namespace": "falcon.routing.util.map_http_methods", "completion": "    # NOTE(kgriffs): We could also use inspect.getmembers() here, but it\n    # gets *all* methods, which means we'd have to filter out the\n    # responders we don't care about, which is an extra step that\n    # isn't necessary.\n    responder_names = responders.get_responder_names(resource)\n\n    if suffix:\n        responder_names = [\n            name + suffix for name in responder_names\n            if name.upper() not in constants.METHOD_MAP\n        ]\n\n    return {\n        method: getattr(resource, name)\n        for name in responder_names\n        for method in constants.METHOD_MAP[name.upper()]\n    }", "idx": 610}
{"namespace": "falcon.routing.static._BoundedFile.read", "completion": "        if size < 0:\n            size = self.remaining\n        if size > self.remaining:\n            size = self.remaining\n        data = self.fh.read(size)\n        self.remaining -= size\n        return data\n\n", "idx": 611}
{"namespace": "authlib.oauth2.rfc6749.util.list_to_scope", "completion": "    if scope is None:\n        return None\n    if isinstance(scope, (set, tuple, list)):\n        return \" \".join([to_unicode(s) for s in scope])\n    if isinstance(scope, dict):\n        return \" \".join([to_unicode(k) for k in scope])\n    if scope == \"\":\n        return scope\n    if utils.IS_PY3 and isinstance(scope, bytes):\n        return to_unicode(scope)\n    return to_unicode(scope)\n\n", "idx": 612}
{"namespace": "authlib.oauth2.rfc6749.util.extract_basic_authorization", "completion": "    auth = headers.get('Authorization')\n    if not auth or ' ' not in auth:\n        return None, None\n    auth_type, auth_token = auth.split(None, 1)\n    if auth_type.lower() != 'basic':\n        return None, None\n    try:\n        auth_token = base64.b64decode(to_unicode(auth_token).encode('ascii'))\n    except (binascii.Error, ValueError):\n        return None, None\n    try:\n        username, password = auth_token.decode('utf-8').split(':', 1)\n    except ValueError:\n        username = auth_token\n        password = None\n    return username, password\n\n", "idx": 613}
{"namespace": "authlib.oauth2.rfc6749.parameters.prepare_grant_uri", "completion": "    url = urlparse.urlparse(uri)\n    query = url.query\n    if query:\n        query += '&'\n    query += 'response_type=' + response_type\n    query += '&client_id=' + client_id\n    if redirect_uri:\n        query += '&redirect_uri=' + redirect_uri\n    if scope:\n        query += '&scope=' + list_to_scope(scope)\n    if state:\n        query += '&state=' + state\n    for k, v in kwargs.items():\n        query += '&' + k + '=' + v\n    return urlparse.urlunparse(\n        (url.scheme, url.netloc, url.path, url.params, query, url.fragment))\n\n", "idx": 614}
{"namespace": "authlib.oauth2.rfc6749.parameters.parse_authorization_code_response", "completion": "    params = dict(urlparse.parse_qsl(urlparse.urlparse(uri).query))\n    if state and params.get('state') != state:\n        raise MismatchingStateException()\n    error = params.get('error')\n    if error:\n        description = params.get('error_description')\n        uri = params.get('error_uri')\n        raise Exception(error, description, uri)\n    return {\n        'code': params.get('code'),\n        'state': params.get('state'),\n    }", "idx": 615}
{"namespace": "authlib.oauth2.rfc6749.parameters.parse_implicit_response", "completion": "    fragment = urlparse.urlparse(uri).fragment\n    params = dict(urlparse.parse_qsl(fragment))\n\n    if 'access_token' not in params:\n        raise MissingTokenException()\n\n    params_state = params.get('state')\n    if state and params_state != state:\n        raise MismatchingStateException()\n\n    return params", "idx": 616}
{"namespace": "authlib.common.encoding.json_b64encode", "completion": "    if isinstance(text, dict):\n        text = json.dumps(text)\n    return urlsafe_b64encode(text.encode('utf-8')).decode('utf-8')\n\n", "idx": 617}
{"namespace": "authlib.jose.util.extract_header", "completion": "    header_data = extract_segment(header_segment, error_cls)\n    try:\n        header = json_loads(to_unicode(header_data))\n    except ValueError as e:\n        raise error_cls('Invalid header string: {}'.format(e))\n    if not isinstance(header, dict):\n        raise error_cls('Header must be a json object')\n    return header\n\n", "idx": 618}
{"namespace": "twitter.models.TwitterModel.AsDict", "completion": "        data = {}\n        for key, value in self.__dict__.items():\n            if isinstance(value, (list, tuple, set)):\n                data[key] = []\n                for item in value:\n                    if hasattr(item, 'AsDict'):\n                        data[key].append(item.AsDict())\n                    else:\n                        data[key].append(item)\n            elif isinstance(value, TwitterModel):\n                data[key] = value.AsDict()\n            else:\n                data[key] = value\n        return data\n", "idx": 619}
{"namespace": "twitter.models.TwitterModel.NewFromJsonDict", "completion": "        # Create a new instance of the class\n        instance = cls(**kwargs)\n\n        # Iterate over the key/value pairs in the data dictionary\n        for (key, value) in data.items():\n\n            # If the value is a list, we need to create a list to hold the\n            # objects created by the NewFromJsonDict() method, i.e., if it\n            # inherits from TwitterModel. If the item in the list doesn't\n            # support the NewFromJsonDict() method, then we assign the value\n            # directly. An example being a list of Media objects contained\n            # within a Status object.\n            if isinstance(value, (list, tuple, set)):\n                setattr(instance, key, list())\n                for subobj in value:\n                    if getattr(instance, key, None):\n                        getattr(instance, key).append(\n                            getattr(instance, key, None).NewFromJsonDict(subobj))\n                    else:\n                        getattr(instance, key).append(subobj)\n\n            # Not a list, *but still a subclass of TwitterModel* and\n            # and we can assign the data[key] directly with the NewFromJsonDict()\n            # method of the object. An example being a Status object contained\n            # within a User object.\n            elif getattr(instance, key, None):\n                setattr(instance, key,\n                        getattr(instance, key, None).NewFromJsonDict(value))\n\n            # If the value doesn't have a NewFromJsonDict() method, i.e., it's\n            # not something that subclasses TwitterModel, then we can use direct\n            # assigment.\n            else:\n                setattr(instance, key, value)\n\n        return instance\n", "idx": 620}
{"namespace": "twitter.api.Api._TweetTextWrap", "completion": "        if len(status) > char_lim:\n            raise TwitterError(\"Text must be less than or equal to CHARACTER_LIMIT characters.\")\n\n        tweets = []\n        line = \"\"\n        for word in status.split():\n            if len(word) > char_lim:\n                raise TwitterError(\"Text must be less than or equal to CHARACTER_LIMIT characters.\")\n            if len(line) + len(word) > char_lim:\n                tweets.append(line)\n                line = word\n            else:\n                line += word + \" \"\n        tweets.append(line)\n        return tweets", "idx": 621}
{"namespace": "databases.importer.import_from_string", "completion": "    if not isinstance(import_str, str):\n        raise ImportFromStringError(\n            f\"Import string '{import_str}' is not a string.\"\n        )\n\n    module_str, _, attrs_str = import_str.partition(\":\")\n\n    if not module_str or not attrs_str:\n        raise ImportFromStringError(\n            f\"Import string '{import_str}' must be in format '<module>:<attribute>'.\"\n        )\n\n    try:\n        module = importlib.import_module(module_str)\n    except ImportError as e:\n        if \".\" not in module_str:\n            raise ImportFromStringError(\n                f\"Could not import module '{module_str}'. Are you sure it is installed?\"\n            ) from e\n\n        module_name, _, obj_name = module_str.rpartition(\".\")\n        modules = module_name.split(\".\")\n\n        obj = None\n\n        for i in range(len(modules), 0, -1):\n            try:\n                obj = importlib.import_module(\".\".join(modules[:i]))\n                break\n            except ImportError:\n                continue\n\n        if obj is None:\n            raise ImportFromStringError(\n                f\"Could not import module '{module_str}'. Are you sure it is installed?\"\n            )\n\n        for attr in attrs_str.split(\".\"):\n            try:\n                obj = getattr(obj, attr)\n            except AttributeError:\n                raise ImportFromStringError(\n                    f\"Attribute '{attr}' not found in module '{module_str}'.\"\n                ) from None\n\n        return obj\n\n    try:\n        return getattr(module, attrs_str)\n    except AttributeError:\n        raise ImportFromStringError(\n            f\"Attribute '{attrs_str}' not found in module '{module_str}'.\"\n        ) from None", "idx": 622}
{"namespace": "rest_framework.reverse.reverse", "completion": "    scheme = get_url_scheme(api_settings)\n    url = _reverse(viewname, args, kwargs, request, format, **extra)\n\n    if not url:\n        return url\n\n    url = build_absolute_uri(url, request)\n    url = preserve_builtin_query_params(url, request)\n\n    if scheme and url:\n        return build_absolute_uri(url, request)\n\n    return url\n\n", "idx": 623}
{"namespace": "rest_framework.serializers.Serializer.fields", "completion": "        # If 'fields' is already set on the class, use it instead of\n        # building it dynamically.\n        if getattr(self, '_fields', None) is not None:\n            return self._fields\n\n        fields = OrderedDict()\n        for key, value in self.get_fields().items():\n            fields[key] = value\n            value.field_name = key\n            value.parent = self\n        return fields\n", "idx": 624}
{"namespace": "rest_framework.parsers.JSONParser.parse", "completion": "        parser_context = parser_context or {}\n        encoding = parser_context.get('encoding', settings.DEFAULT_CHARSET)\n\n        try:\n            decoded_stream = codecs.getreader(encoding)(stream)\n            parse_constant = None if self.strict else None\n            return renderers.JSONParser().parse(decoded_stream, parse_constant)\n        except ValueError as exc:\n            raise ParseError('JSON parse error - %s' % str(exc))\n\n", "idx": 625}
{"namespace": "rest_framework.parsers.FileUploadParser.get_filename", "completion": "        view = parser_context['view']\n        request = parser_context['request']\n        filename = view.kwargs.get(view.lookup_url_kwarg or 'filename', None)\n        if not filename:\n            content_disposition = request.META.get('HTTP_CONTENT_DISPOSITION')\n            if content_disposition:\n                match = re.search(r'filename=[\\'\"]?(.*?)[\\'\"]?', content_disposition)\n                if match:\n                    filename = match.group(1)\n        return filename", "idx": 626}
{"namespace": "rest_framework.fields.is_simple_callable", "completion": "    if not callable(obj):\n        return False\n\n    if inspect.isbuiltin(obj):\n        raise BuiltinSignatureError(\n            'Built-in function signatures are not inspectable. '\n            'You must provide the argument names for \\'%s\\'.' % obj.__name__\n        )\n\n    if isinstance(obj, functools.partial):\n        return True\n\n    signature = inspect.signature(obj)\n    for param in signature.parameters.values():\n        if param.kind not in (inspect.Parameter.POSITIONAL_ONLY, inspect.Parameter.VAR_POSITIONAL):\n            return False\n    return True\n\n", "idx": 627}
{"namespace": "rest_framework.fields.Field.bind", "completion": "        self.field_name = field_name\n        self.parent = parent\n\n        # In case the field has been declared with a dotted source, we\n        # store the source on the field and use that when getting the value\n        # and setting it.\n        if self.source is not None and not callable(self.source):\n            self.source_attrs = self.source.split('.')\n        else:\n            self.source_attrs = None\n\n        # If the field is a `SerializerMethodField`, then we need to\n        # store the method name so we can call it later.\n        if isinstance(self, SerializerMethodField):\n            self.method_name = self.source\n\n        # If the field is a `HyperlinkedIdentityField`, then we need to\n        # store the view name so we can reverse it later.\n        if isinstance(self, HyperlinkedIdentityField):\n            self.view_name = self.source\n\n        # If the field is a `HyperlinkedRelatedField`, then we need to\n        # store the view name so we can reverse it later.\n        if isinstance(self, HyperlinkedRelatedField):\n            self.view_name = self.source\n\n        # If the field is a `HyperlinkedRelatedField`, then we need to\n        # store the lookup field so we can reverse it later.\n        if isinstance(self, HyperlinkedRelatedField):\n            self.lookup_field = self.source\n\n        # If the field is a `HyperlinkedRelatedField`, then we need to\n        # store the lookup url kwarg so we can reverse it later.\n        if isinstance(self, HyperlinkedRelatedField):\n            self.lookup_url_kwarg = self.source\n\n        # If the field is a `HyperlinkedRelatedField`, then we need to\n        # store the lookup url kwarg so we can reverse it later.\n        if isinstance(self, HyperlinkedRelatedField):\n            self.lookup_url_kwarg = self.source\n\n        # If the field is a `HyperlinkedRelatedField`, then we need to", "idx": 628}
{"namespace": "rest_framework.fields.Field.run_validation", "completion": "        (is_empty_value, data) = self.validate_empty_values(data)\n        if is_empty_value:\n            return data\n\n        value = self.to_internal_value(data)\n        self.run_validators(value)\n        return value\n", "idx": 629}
{"namespace": "rest_framework.fields.Field.root", "completion": "        root = self\n        while root.parent is not None:\n            root = root.parent\n        return root\n", "idx": 630}
{"namespace": "rest_framework.fields.CharField.run_validation", "completion": "        (is_empty_value, data) = self.validate_empty_values(data)\n        if is_empty_value:\n            return data\n        if not self.allow_blank and data.strip() == '':\n            self.fail('blank')\n        return super().run_validation(data)\n", "idx": 631}
{"namespace": "rest_framework.fields.CharField.to_internal_value", "completion": "        if isinstance(data, bool):\n            raise TypeError(\n                'Incorrect type. Expected a string, but got a boolean.')\n        if not isinstance(data, (str, int, float)):\n            raise TypeError(\n                'Incorrect type. Expected a string, but got %s' % type(data).__name__)\n        if self.trim_whitespace:\n            data = data.strip()\n        return str(data)\n", "idx": 632}
{"namespace": "rest_framework.fields.DecimalField.to_internal_value", "completion": "        if isinstance(data, str) and len(data) > self.MAX_STRING_LENGTH:\n            self.fail('max_string_length')\n\n        try:\n            if self.localize:\n                data = localize_input(data)\n            if self.coerce_to_string:\n                data = str(data)\n            return decimal.Decimal(data)\n        except (TypeError, decimal.InvalidOperation):\n            self.fail('invalid')\n", "idx": 633}
{"namespace": "rest_framework.fields.DateTimeField.to_representation", "completion": "        if value in (None, ''):\n            return None\n\n        output_format = getattr(self, 'format', api_settings.DATETIME_FORMAT)\n        if output_format is None or isinstance(value, str):\n            return value\n\n        value = self.enforce_timezone(value)\n        return value.strftime(output_format)\n\n", "idx": 634}
{"namespace": "rest_framework.fields.ChoiceField.iter_options", "completion": "        return iter_options(self.grouped_choices, self.html_cutoff, self.html_cutoff_text)\n", "idx": 635}
{"namespace": "rest_framework.fields.MultipleChoiceField.get_value", "completion": "        if self.field_name not in dictionary:\n            if getattr(self.root, 'partial', False):\n                return []\n            return self.get_default()\n        data = dictionary.get(self.field_name, empty)\n        if html.is_html_input(data):\n            # HTML forms will represent empty fields as '', and cannot\n            # represent None or False values directly.\n            if data == '':\n                if getattr(self.root, 'partial', False):\n                    return []\n                if self.required:\n                    self.fail('required')\n                return []\n            if data == '[]':\n                return []\n            if data == 'false':\n                return []\n            if data == 'true':\n                return [self.choices[0][0]]\n            return [value for value in data.split(',') if value]\n        return data\n", "idx": 636}
{"namespace": "rest_framework.exceptions._get_error_details", "completion": "    if isinstance(data, list):\n        ret = [\n            _get_error_details(item, default_code) for item in data\n        ]\n        if isinstance(data, ReturnList):\n            return ReturnList(ret, serializer=data.serializer)\n        return ret\n    elif isinstance(data, dict):\n        ret = {\n            key: _get_error_details(value, default_code)\n            for key, value in data.items()\n        }\n        if isinstance(data, ReturnDict):\n            return ReturnDict(ret, serializer=data.serializer)\n        return ret\n\n    text = force_str(data)\n    code = getattr(data, 'code', default_code)\n    return ErrorDetail(text, code)\n\n", "idx": 637}
{"namespace": "rest_framework.exceptions.server_error", "completion": "    data = {\n        'error': 'Server Error (500)'\n    }\n    return JsonResponse(data, status=status.HTTP_500_INTERNAL_SERVER_ERROR)", "idx": 638}
{"namespace": "rest_framework.exceptions.bad_request", "completion": "    data = {\n        'error': 'Bad Request (400)'\n    }\n    return JsonResponse(data, status=status.HTTP_400_BAD_REQUEST)\n\n", "idx": 639}
{"namespace": "rest_framework.relations.RelatedField.iter_options", "completion": "        queryset = self.get_queryset()\n        if queryset is None:\n            # Ensure that field.choices returns something sensible\n            # even when accessed with a read-only field.\n            return\n\n        for item in queryset:\n            yield (\n                self.to_representation(item),\n                self.display_value(item)\n            )\n", "idx": 640}
{"namespace": "rest_framework.relations.PrimaryKeyRelatedField.to_internal_value", "completion": "        queryset = self.get_queryset()\n        if queryset is None:\n            raise Exception('Writable related fields must include a `queryset` argument.')\n\n        try:\n            pk = self.pk_field.to_internal_value(data)\n        except ObjectTypeError:\n            self.fail('incorrect_type', data_type=type(data).__name__)\n        except (ObjectValueError, ValueError):\n            self.fail('incorrect_type', data_type=type(data).__name__)\n\n        try:\n            return queryset.get(pk=pk)\n        except ObjectDoesNotExist:\n            self.fail('does_not_exist', pk_value=smart_str(data))\n        except (TypeError, ValueError):\n            self.fail('incorrect_type', data_type=type(data).__name__)\n", "idx": 641}
{"namespace": "rest_framework.relations.PrimaryKeyRelatedField.to_representation", "completion": "        if self.pk_field is not None:\n            return self.pk_field.to_representation(value.pk)\n        return value.pk\n", "idx": 642}
{"namespace": "rest_framework.relations.SlugRelatedField.to_internal_value", "completion": "        queryset = self.get_queryset()\n        try:\n            return queryset.get(**{self.slug_field: data})\n        except ObjectDoesNotExist:\n            self.fail('does_not_exist', slug_name=self.slug_field, value=smart_str(data))\n        except (TypeError, ValueError):\n            self.fail('invalid')\n", "idx": 643}
{"namespace": "rest_framework.templatetags.rest_framework.add_query_param", "completion": "    dict_ = request.GET.copy()\n    dict_[key] = val\n    return escape(iri_to_uri(request.build_absolute_uri('?' + dict_.urlencode())))", "idx": 644}
{"namespace": "rest_framework.utils.mediatypes._MediaType.match", "completion": "        if self.full_type == '*/*':\n            return True\n        if self.main_type == '*':\n            return self.sub_type == other.sub_type\n        if self.sub_type == '*':\n            return self.main_type == other.main_type\n        return self.full_type == other.full_type\n", "idx": 645}
{"namespace": "rest_framework.utils.mediatypes._MediaType.precedence", "completion": "        if self.main_type == '*':\n            return 0\n        elif self.sub_type == '*':\n            return 1\n        elif self.params:\n            return 2\n        else:\n            return 3", "idx": 646}
{"namespace": "rest_framework.utils.mediatypes._MediaType.__str__", "completion": "        ret = self.full_type\n        for param, value in self.params.items():\n            if param == 'q':\n                value = '%.1f' % value\n            ret += '; %s=%s' % (param, value)\n        return ret", "idx": 647}
{"namespace": "asyncpg._testbase.TestCase.assertLoopErrorHandlerCalled", "completion": "        def handler(loop, context):\n            self.__unhandled_exceptions.append(context)\n\n        self.loop.set_exception_handler(handler)\n        try:\n            yield\n        finally:\n            self.loop.set_exception_handler(self.loop_exception_handler)\n\n        for context in self.__unhandled_exceptions:\n            if re.search(msg_re, context['message']):\n                return\n\n        raise AssertionError(\n            'no exception with message matching {} was logged'.format(\n                repr(msg_re)))\n", "idx": 648}
{"namespace": "csvs_to_sqlite.utils.refactor_dataframes", "completion": "    for column, (table_name, value_column) in foreign_keys.items():\n        lookup_table = LookupTable(conn, table_name, value_column, index_fts)\n        for df in dataframes:\n            if column in df.columns:\n                df[column] = df[column].apply(lookup_table.id_for_value)\n    return dataframes\n\n", "idx": 649}
{"namespace": "sqlitedict.SqliteDict.iteritems", "completion": "        GET_ITEMS = 'SELECT key, value FROM \"%s\" ORDER BY rowid' % self.tablename\n        for key, value in self.conn.select(GET_ITEMS):\n            yield self.decode_key(key), self.decode(value)\n", "idx": 650}
{"namespace": "sqlitedict.SqliteDict.update", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to write to read-only SqliteDict')\n\n        if items:\n            items = [(self.encode_key(k), self.encode(v)) for k, v in items]\n            UPDATE_ITEMS = 'REPLACE INTO \"%s\" (key, value) VALUES (?,?)' % self.tablename\n            self.conn.executemany(UPDATE_ITEMS, items)\n        if kwds:\n            self.update(kwds)\n        if self.autocommit:\n            self.commit()\n", "idx": 651}
{"namespace": "sqlitedict.SqliteDict.clear", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to clear read-only SqliteDict')\n\n        CLEAR_TABLE = 'DELETE FROM \"%s\"' % self.tablename\n        self.conn.execute(CLEAR_TABLE)\n        if self.autocommit:\n            self.commit()\n", "idx": 652}
{"namespace": "sqlitedict.SqliteDict.commit", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to commit to read-only SqliteDict')\n\n        self.conn.commit(blocking)\n", "idx": 653}
{"namespace": "sqlitedict.SqliteDict.terminate", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to terminate read-only SqliteDict')\n        self.close()\n        if self.filename != \":memory:\":\n            try:\n                os.remove(self.filename)\n            except Exception:\n                pass", "idx": 654}
{"namespace": "boto.utils.retry_url", "completion": "    # Create a proxy handler\n    proxy_handler = urllib.request.ProxyHandler({})\n\n    # Create an opener with the proxy handler\n    opener = urllib.request.build_opener(proxy_handler)\n\n    # Install the opener\n    urllib.request.install_opener(opener)\n\n    # Set the timeout value\n    if timeout is not None:\n        socket.setdefaulttimeout(timeout)\n\n    # Initialize the result variable\n    result = None\n\n    # Loop through the number of retries\n    for i in range(num_retries):\n        try:\n            # Open the URL and read the result\n            result = urllib.request.urlopen(url).read()\n            break\n        except urllib.error.HTTPError as e:\n            # If the error is a 404 and retry_on_404 is False, raise the error\n            if e.code == 404 and not retry_on_404:\n                raise\n            # If the error is a 404 and retry_on_404 is True, wait for 1 second and try again\n            elif e.code == 404 and retry_on_404:\n                time.sleep(1)\n            # If the error is not a 404, raise the error\n            else:\n                raise\n\n    # Return the result\n    return result", "idx": 655}
{"namespace": "boto.utils.LazyLoadMetadata.values", "completion": "        self._materialize()\n        return super(LazyLoadMetadata, self).values()\n", "idx": 656}
{"namespace": "boto.utils.get_instance_userdata", "completion": "    userdata_url = _build_instance_metadata_url(url, version, 'user-data/')\n    try:\n        data = retry_url(userdata_url, num_retries=num_retries, timeout=timeout)\n        if data:\n            if sep:\n                return dict([line.split(sep, 1) for line in data.split('\\n')])\n            else:\n                return data\n        else:\n            return None\n    except urllib.error.URLError:\n        return None", "idx": 657}
{"namespace": "boto.utils.pythonize_name", "completion": "    s1 = _first_cap_regex.sub(r'\\1_\\2', name)\n    s2 = _number_cap_regex.sub(r'\\1_\\2', s1)\n    s3 = _end_cap_regex.sub(r'\\1_\\2', s2)\n    return s3.lower()\n\n", "idx": 658}
{"namespace": "boto.cloudsearchdomain.connect_to_region", "completion": "    from boto.cloudsearchdomain.layer1 import CloudSearchDomainConnection\n    return CloudSearchDomainConnection(region=RegionInfo(name=region_name,\n                                                        endpoint='cloudsearchdomain.%s.amazonaws.com' % region_name),\n                                        **kw_params)", "idx": 659}
{"namespace": "boto.redshift.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)", "idx": 660}
{"namespace": "boto.support.connect_to_region", "completion": "    from boto.support.layer1 import SupportConnection\n    return SupportConnection(region=RegionInfo(name=region_name,\n                                               endpoint='support.%s.amazonaws.com' % region_name),\n                             **kw_params)", "idx": 661}
{"namespace": "boto.configservice.connect_to_region", "completion": "    from boto.configservice.layer1 import ConfigServiceConnection\n    return ConfigServiceConnection(region=RegionInfo(name=region_name,\n                                                     endpoint='configservice.%s.amazonaws.com' % region_name),\n                                    **kw_params)", "idx": 662}
{"namespace": "boto.cloudhsm.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)", "idx": 663}
{"namespace": "boto.logs.connect_to_region", "completion": "    from boto.logs.layer1 import CloudWatchLogsConnection\n    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)", "idx": 664}
{"namespace": "boto.cloudsearch.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)", "idx": 665}
{"namespace": "boto.glacier.job.Job.download_to_fileobj", "completion": "        num_chunks = self._calc_num_chunks(chunk_size)\n        self._download_to_fileob(output_file, num_chunks, chunk_size,\n                                 verify_hashes, retry_exceptions)\n", "idx": 666}
{"namespace": "boto.glacier.utils.minimum_part_size", "completion": "    if size_in_bytes > MAXIMUM_NUMBER_OF_PARTS * default_part_size:\n        raise ValueError(\"File size exceeds maximum allowed archive size\")\n    return max(default_part_size, int(math.ceil(float(size_in_bytes) / MAXIMUM_NUMBER_OF_PARTS)))\n\n", "idx": 667}
{"namespace": "boto.glacier.utils.chunk_hashes", "completion": "    # If the bytestring is empty, return a list with a single hash of an empty bytestring\n    if not bytestring:\n        return [hashlib.sha256(b'').digest()]\n\n    # Initialize an empty list to store the hashes\n    hashes = []\n\n    # Iterate over the bytestring in chunks of size chunk_size\n    for i in range(0, len(bytestring), chunk_size):\n        # Get the current chunk\n        chunk = bytestring[i:i+chunk_size]\n\n        # Calculate the SHA256 hash of the chunk\n        hash = hashlib.sha256(chunk).digest()\n\n        # Append the hash to the list of hashes\n        hashes.append(hash)\n\n    # Return the list of hashes\n    return hashes\n\n", "idx": 668}
{"namespace": "boto.glacier.utils.compute_hashes_from_fileobj", "completion": "    # Initialize the linear and tree hash objects\n    linear_hash = hashlib.sha256()\n    tree_hash = hashlib.sha256()\n\n    # Read the file in chunks and update the linear and tree hashes\n    while True:\n        chunk = fileobj.read(chunk_size)\n        if not chunk:\n            break\n        linear_hash.update(chunk)\n        tree_hash.update(chunk)\n\n    # Compute the tree hash\n    tree_hash_value = tree_hash.hexdigest()\n\n    # Return the linear and tree hashes in hexadecimal format\n    return linear_hash.hexdigest(), tree_hash_value", "idx": 669}
{"namespace": "boto.glacier.concurrent.ConcurrentTransferer._calculate_required_part_size", "completion": "        # Calculate the number of parts required to transfer the data\n        num_parts = int(math.ceil(float(total_size) / self._part_size))\n\n        # Calculate the final part size to be used for concurrent transfer\n        final_part_size = int(math.ceil(float(total_size) / num_parts))\n\n        # Return the total number of parts and the final part size\n        return num_parts, final_part_size\n", "idx": 670}
{"namespace": "boto.glacier.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)", "idx": 671}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.update", "completion": "        if self.id is None:\n            return\n        updated = self.connection.get_all_network_interfaces(\n            [self.id],\n            dry_run=dry_run)[0]\n        if updated is not None:\n            self._update(updated)\n        elif validate:\n            raise ValueError('%s is not a valid NetworkInterface ID' % self.id)\n        return self.status\n", "idx": 672}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.attach", "completion": "        params = {\n            'NetworkInterfaceId': self.id,\n            'InstanceId': instance_id,\n            'DeviceIndex': device_index,\n        }\n        if dry_run:\n            params['DryRun'] = dry_run\n        try:\n            self.connection.attach_network_interface(**params)\n            return True\n        except BotoClientError as e:\n            if e.error_code == 'InvalidParameterValue':\n                raise ValueError(e.message)\n            else:\n                raise e\n", "idx": 673}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.detach", "completion": "        return self.connection.detach_network_interface(\n            self.id,\n            force,\n            dry_run=dry_run\n        )", "idx": 674}
{"namespace": "boto.ec2.address.Address.release", "completion": "        params = {}\n        if self.allocation_id:\n            params['allocationId'] = self.allocation_id\n        elif self.public_ip:\n            params['publicIp'] = self.public_ip\n        else:\n            raise ValueError('Invalid address')\n        if dry_run:\n            params['dryRun'] = 'true'\n        return self.connection.get_status('ReleaseAddress', params,\n                                          verb='POST')", "idx": 675}
{"namespace": "boto.ec2.address.Address.associate", "completion": "        if self.allocation_id:\n            return self.connection.associate_address(\n                allocation_id=self.allocation_id,\n                instance_id=instance_id,\n                network_interface_id=network_interface_id,\n                private_ip_address=private_ip_address,\n                allow_reassociation=allow_reassociation,\n                dry_run=dry_run)\n        else:\n            return self.connection.associate_address(\n                public_ip=self.public_ip,\n                instance_id=instance_id,\n                network_interface_id=network_interface_id,\n                private_ip_address=private_ip_address,\n                allow_reassociation=allow_reassociation,\n                dry_run=dry_run)\n", "idx": 676}
{"namespace": "boto.ec2.address.Address.disassociate", "completion": "        if self.allocation_id:\n            return self.connection.disassociate_address(\n                public_ip=self.public_ip,\n                allocation_id=self.allocation_id,\n                dry_run=dry_run\n            )\n        return self.connection.disassociate_address(\n            public_ip=self.public_ip,\n            dry_run=dry_run\n        )", "idx": 677}
{"namespace": "boto.ec2.ec2object.TaggedEC2Object.add_tags", "completion": "        if dry_run:\n            return\n        if not isinstance(tags, dict):\n            raise TypeError('tags must be a dict')\n        if not tags:\n            return\n        self.connection.create_tags([self.id], tags)\n        self.tags.update(tags)\n", "idx": 678}
{"namespace": "boto.ec2.ec2object.TaggedEC2Object.remove_tags", "completion": "        status = self.connection.delete_tags(\n            [self.id],\n            tags,\n            dry_run=dry_run\n        )\n        if self.tags is None:\n            self.tags = TagSet()\n        self.tags.update(tags)", "idx": 679}
{"namespace": "boto.ec2.connection.EC2Connection.get_all_instance_status", "completion": "        params = {}\n        if instance_ids:\n            self.build_list_params(params, instance_ids, 'InstanceId')\n        if filters:\n            self.build_filter_params(params, filters)\n        if dry_run:\n            params['DryRun'] = 'true'\n        if max_results is not None:\n            params['MaxResults'] = max_results\n        if next_token:\n            params['NextToken'] = next_token\n        if include_all_instances:\n            params['IncludeAllInstances'] = 'true'\n        return self.get_list('DescribeInstanceStatus', params,\n                             [('item', InstanceStatus)], verb='POST')\n", "idx": 680}
{"namespace": "boto.ec2.volume.Volume.update", "completion": "        if self.id:\n            try:\n                volume = self.connection.get_all_volumes(\n                    volume_ids=[self.id],\n                    filters={'dry-run': dry_run})[0]\n                self._update(volume)\n            except EC2ResponseError as error:\n                if error.code == 'InvalidVolume.NotFound':\n                    if validate:\n                        raise ValueError('%s is not a valid Volume ID' % self.id)\n                else:\n                    raise\n        return self.status", "idx": 681}
{"namespace": "boto.ec2.volume.Volume.attach", "completion": "        return self.connection.attach_volume(self.id, instance_id, device, dry_run=dry_run)\n", "idx": 682}
{"namespace": "boto.ec2.volume.Volume.detach", "completion": "        return self.connection.detach_volume(\n            self.id,\n            instance_id,\n            device,\n            dry_run=dry_run\n        )", "idx": 683}
{"namespace": "boto.ec2.volume.Volume.create_snapshot", "completion": "        return self.connection.create_snapshot(\n            self.id,\n            description,\n            dry_run=dry_run\n        )", "idx": 684}
{"namespace": "boto.ec2.volume.Volume.attachment_state", "completion": "        return self.attach_data.status", "idx": 685}
{"namespace": "boto.ec2.securitygroup.SecurityGroup.add_rule", "completion": "        if self.vpc_id:\n            raise BotoClientError(\n                'Cannot add rules to a VPC Security Group.')\n        else:\n            self.rules.append(\n                IPPermission(\n                    ip_protocol=ip_protocol,\n                    from_port=from_port,\n                    to_port=to_port,\n                    src_group_name=src_group_name,\n                    src_group_owner_id=src_group_owner_id,\n                    cidr_ip=cidr_ip,\n                    src_group_group_id=src_group_group_id,\n                    dry_run=dry_run\n                )\n            )\n", "idx": 686}
{"namespace": "boto.ec2.connect_to_region", "completion": "    for region in regions(**kw_params):\n        if region.name == region_name:\n            return region.connect(**kw_params)", "idx": 687}
{"namespace": "boto.ec2.cloudwatch.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 688}
{"namespace": "boto.ec2.autoscale.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 689}
{"namespace": "boto.ec2.elb.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 690}
{"namespace": "boto.ec2.elb.ELBConnection.get_all_load_balancers", "completion": "        params = {}\n        if load_balancer_names:\n            self.build_list_params(params, load_balancer_names, 'LoadBalancerNames.member.%d')\n        if marker:\n            params['Marker'] = marker\n        return self.get_list('DescribeLoadBalancers', params,\n                             [('DescribeLoadBalancersResult', 'LoadBalancerDescriptions'),\n                              ('LoadBalancerDescriptions', 'member')],\n                             LoadBalancer)\n", "idx": 691}
{"namespace": "boto.ec2.elb.ELBConnection.disable_availability_zones", "completion": "        params = {'LoadBalancerName': load_balancer_name}\n        self.build_list_params(params, zones_to_remove,\n                               'AvailabilityZones.member.%d')\n        obj = self.get_object('DisableAvailabilityZonesForLoadBalancer',\n                              params, LoadBalancerZones)\n        return obj.zones\n", "idx": 692}
{"namespace": "boto.awslambda.connect_to_region", "completion": "    from boto.awslambda.layer1 import AWSLambdaConnection\n    return AWSLambdaConnection(region=RegionInfo(name=region_name,\n                                                 endpoint='awslambda.%s.amazonaws.com' % (region_name,)))", "idx": 693}
{"namespace": "boto.cognito.identity.connect_to_region", "completion": "    from boto.cognito.identity.layer1 import CognitoIdentityConnection\n    return CognitoIdentityConnection(region=RegionInfo(name=region_name,\n                                                      endpoint='cognito-identity.%s.amazonaws.com' % (region_name,)),\n                                      **kw_params)", "idx": 694}
{"namespace": "boto.cognito.sync.connect_to_region", "completion": "    from boto.cognito.sync.layer1 import CognitoSyncConnection\n    return CognitoSyncConnection(region=RegionInfo(name=region_name,\n                                                   endpoint='cognito-sync.%s.amazonaws.com' % region_name),\n                                  **kw_params)", "idx": 695}
{"namespace": "boto.cloudformation.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)", "idx": 696}
{"namespace": "boto.route53.zone.Zone.find_records", "completion": "        name = self.route53connection._make_qualified(name)\n        if identifier is not None:\n            try:\n                int(identifier[1])\n                identifier = (identifier[0], identifier[1])\n            except:\n                identifier = (identifier[0], identifier[1])\n        records = ResourceRecordSets(self.route53connection, self.id)\n        records.find_records(name, type, desired, all, identifier)\n        if len(records) == 0:\n            return None\n        elif len(records) == 1:\n            return records[0]\n        else:\n            return records", "idx": 697}
{"namespace": "boto.route53.domains.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)", "idx": 698}
{"namespace": "boto.s3.key.Key.get_contents_to_filename", "completion": "        with open(filename, 'wb') as fp:\n            self.get_contents_to_file(fp, headers, cb, num_cb, torrent,\n                                      version_id, res_download_handler,\n                                      response_headers)\n", "idx": 699}
{"namespace": "boto.s3.cors.CORSConfiguration.add_rule", "completion": "        rule = CORSRule(allowed_method=allowed_method,\n                        allowed_origin=allowed_origin,\n                        id=id,\n                        allowed_header=allowed_header,\n                        max_age_seconds=max_age_seconds,\n                        expose_header=expose_header)\n        self.append(rule)", "idx": 700}
{"namespace": "boto.s3.bucket.Bucket.get_key", "completion": "        if version_id:\n            key_name = '%s?versionId=%s' % (key_name, version_id)\n        key = self.key_class(self, key_name)\n        if validate:\n            try:\n                key.get_contents_as_string(headers=headers,\n                                           response_headers=response_headers)\n            except boto.exception.S3ResponseError:\n                return None\n        return key\n", "idx": 701}
{"namespace": "boto.s3.bucket.Bucket.new_key", "completion": "        return self.key_class(self, key_name)\n", "idx": 702}
{"namespace": "boto.s3.bucket.Bucket.delete_key", "completion": "        provider = self.connection.provider\n        query_args = 'delete'\n        if version_id:\n            query_args += '&versionId=%s' % version_id\n        if mfa_token:\n            headers[provider.mfa_header] = ' '.join(mfa_token)\n        response = self.connection.make_request('DELETE', self.name, key_name,\n                                                headers=headers,\n                                                query_args=query_args)\n        body = response.read()\n        if response.status == 204:\n            k = self.key_class(self)\n            k.name = key_name\n            k.handle_version_headers(response)\n            k.handle_encryption_headers(response)\n            k.handle_restore_headers(response)\n            k.handle_storage_class_header(response)\n            k.handle_addl_headers(response.getheaders())\n            return k\n        else:\n            raise provider.storage_response_error(response.status,\n                                                  response.reason,\n                                                  body)\n", "idx": 703}
{"namespace": "boto.s3.bucket.Bucket.get_tags", "completion": "        response = self.connection.make_request('GET', self.name,\n                                                query_args='tagging',\n                                                headers=headers)\n        body = response.read()\n        boto.log.debug(body)\n        if response.status == 200:\n            tags = Tags()\n            h = handler.XmlHandler(tags, self)\n            if not isinstance(body, bytes):\n                body = body.encode('utf-8')\n            xml.sax.parseString(body, h)\n            return tags\n        else:\n            raise self.connection.provider.storage_response_error(\n                response.status, response.reason, body)\n", "idx": 704}
{"namespace": "boto.s3.connection.S3Connection._required_auth_capability", "completion": "        return ['hmac-v4-s3']\n", "idx": 705}
{"namespace": "boto.s3.connection.S3Connection.generate_url_sigv4", "completion": "        if not iso_date:\n            iso_date = boto.utils.get_current_iso_date()\n        if not headers:\n            headers = {}\n        if not response_headers:\n            response_headers = {}\n        if not bucket:\n            bucket = self.default_bucket\n        if not key:\n            key = ''\n        if not version_id:\n            version_id = ''\n        if not method:\n            method = 'GET'\n        if not force_http:\n            force_http = False\n\n        # Construct the request parameters\n        request_parameters = {\n            'X-Amz-Algorithm': 'AWS4-HMAC-SHA256',\n            'X-Amz-Credential': self.provider.get_credential_string(),\n            'X-Amz-Date': iso_date,\n            'X-Amz-Expires': str(expires_in),\n            'X-Amz-SignedHeaders': 'host',\n            'X-Amz-Signature': '',\n            'X-Amz-VersionId': version_id\n        }\n\n        # Construct the canonical request\n        canonical_request = self._auth_handler.build_canonical_request(\n            method,\n            self.calling_format.build_auth_path(bucket, key),\n            headers,\n            request_parameters,\n            self.calling_format.build_host(self.server_name(), bucket),\n            self.calling_format.build_path_base(bucket, key),\n            response_headers\n        )\n\n        # Sign the canonical request\n        signature = self._auth_handler.sign_string(canonical_request)\n\n        # Build the presigned URL\n        url = self.calling_format.build_url_base(\n            self,\n            'https' if not force_http else 'http',\n            self.server_name(),\n            bucket,\n            key\n        )\n        url += '?' + '&'.join(\n            ['%s=%s' % (k, v) for k, v in request_parameters.items()", "idx": 706}
{"namespace": "boto.s3.lifecycle.Lifecycle.add_rule", "completion": "        rule = Rule(id, prefix, status, expiration, transition)\n        self.append(rule)", "idx": 707}
{"namespace": "boto.s3.website.WebsiteConfiguration.to_xml", "completion": "        xml = ['<?xml version=\"1.0\" encoding=\"UTF-8\"?>']\n        if self.suffix is not None:\n            xml.append(tag('IndexDocument', tag('Suffix', self.suffix)))\n        if self.error_key is not None:\n            xml.append(tag('ErrorDocument', tag('Key', self.error_key)))\n        if self.redirect_all_requests_to is not None:\n            xml.append(tag('RedirectAllRequestsTo',\n                           self.redirect_all_requests_to.to_xml()))\n        if self.routing_rules is not None:\n            xml.append(self.routing_rules.to_xml())\n        return ''.join(xml)", "idx": 708}
{"namespace": "boto.s3.website.RoutingRules.to_xml", "completion": "        parts = ['<RoutingRules>']\n        for rule in self:\n            parts.append(rule.to_xml())\n        parts.append('</RoutingRules>')\n        return ''.join(parts)", "idx": 709}
{"namespace": "boto.s3.website.RoutingRule.when", "completion": "        condition = Condition(key_prefix=key_prefix,\n                              http_error_code=http_error_code)\n        return cls(condition=condition)\n", "idx": 710}
{"namespace": "boto.s3.website.RoutingRule.then_redirect", "completion": "        self.redirect = Redirect(hostname=hostname, protocol=protocol,\n                                 replace_key=replace_key,\n                                 replace_key_prefix=replace_key_prefix,\n                                 http_redirect_code=http_redirect_code)\n        return self", "idx": 711}
{"namespace": "boto.s3.connect_to_region", "completion": "    if 'host' in kw_params:\n        from boto.s3.connection import S3Connection\n        region = S3RegionInfo(name=region_name, endpoint=kw_params['host'])\n        del kw_params['host']\n        return region.connect(**kw_params)\n    else:\n        return connect_to_region(region_name, **kw_params)", "idx": 712}
{"namespace": "boto.directconnect.connect_to_region", "completion": "    from boto.directconnect.layer1 import DirectConnectConnection\n    return DirectConnectConnection(region=RegionInfo(name=region_name,\n                                                     endpoint='directconnect.%s.amazonaws.com' % (region_name.lower())),\n                                   **kw_params)", "idx": 713}
{"namespace": "boto.rds.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 714}
{"namespace": "boto.datapipeline.connect_to_region", "completion": "    from boto.datapipeline.layer1 import DataPipelineConnection\n    return DataPipelineConnection(region=region_name, **kw_params)", "idx": 715}
{"namespace": "boto.dynamodb.batch.Batch.to_dict", "completion": "        request_dict = {\n            'RequestItems': {\n                self.table.name: {\n                    'Keys': [self.table.format_keys(k) for k in self.keys],\n                    'ConsistentRead': self.consistent_read,\n                }\n            },\n            'ReturnConsumedCapacity': 'TOTAL'\n        }\n        if self.attributes_to_get:\n            request_dict['RequestItems'][self.table.name]['AttributesToGet'] = self.attributes_to_get\n        return request_dict", "idx": 716}
{"namespace": "boto.dynamodb.batch.BatchList.to_dict", "completion": "        batch_list = {}\n        for batch in self:\n            table_name, batch_dict = batch.to_dict()\n            if table_name not in batch_list:\n                batch_list[table_name] = []\n            batch_list[table_name].append(batch_dict)\n        return batch_list", "idx": 717}
{"namespace": "boto.dynamodb.types.Dynamizer.encode", "completion": "        dynamodb_type = self._get_dynamodb_type(attr)\n        if dynamodb_type == 'N':\n            return self._encode_n(attr)\n        elif dynamodb_type == 'S':\n            return self._encode_s(attr)\n        elif dynamodb_type == 'NS':\n            return self._encode_ns(attr)\n        elif dynamodb_type == 'SS':\n            return self._encode_ss(attr)\n        elif dynamodb_type == 'B':\n            return self._encode_b(attr)\n        elif dynamodb_type == 'BS':\n            return self._encode_bs(attr)\n        elif dynamodb_type == 'M':\n            return self._encode_m(attr)\n        elif dynamodb_type == 'L':\n            return self._encode_l(attr)\n        elif dynamodb_type == 'NULL':\n            return self._encode_null(attr)\n        elif dynamodb_type == 'BOOL':\n            return self._encode_bool(attr)\n        else:\n            raise TypeError('Unsupported type \"%s\" for value \"%s\"' % (type(attr), attr))\n", "idx": 718}
{"namespace": "boto.dynamodb.types.Dynamizer.decode", "completion": "        if len(attr.keys()) > 1:\n            return attr\n        if 'S' in attr:\n            return self._decode_s(attr)\n        if 'N' in attr:\n            return self._decode_n(attr)\n        if 'SS' in attr:\n            return self._decode_ss(attr)\n        if 'NS' in attr:\n            return self._decode_ns(attr)\n        if 'B' in attr:\n            return self._decode_b(attr)\n        if 'BS' in attr:\n            return self._decode_bs(attr)\n        if 'NULL' in attr:\n            return self._decode_null(attr)\n        if 'BOOL' in attr:\n            return self._decode_bool(attr)\n        if 'M' in attr:\n            return self._decode_m(attr)\n        if 'L' in attr:\n            return self._decode_l(attr)\n        return attr\n", "idx": 719}
{"namespace": "boto.dynamodb.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)", "idx": 720}
{"namespace": "boto.beanstalk.connect_to_region", "completion": "    from boto.beanstalk.layer1 import Layer1\n    return Layer1(region=RegionInfo(name=region_name, endpoint='elasticbeanstalk.us-east-1.amazonaws.com'), **kw_params)", "idx": 721}
{"namespace": "boto.swf.connect_to_region", "completion": "    return boto.swf.layer1.Layer1(region=RegionInfo(name=region_name, endpoint=REGION_ENDPOINTS[region_name]), **kw_params)", "idx": 722}
{"namespace": "boto.opsworks.regions", "completion": "    return [\n        RegionInfo(name='us-east-1', endpoint='opsworks.us-east-1.amazonaws.com'),\n        RegionInfo(name='us-west-2', endpoint='opsworks.us-west-2.amazonaws.com'),\n        RegionInfo(name='us-west-1', endpoint='opsworks.us-west-1.amazonaws.com'),\n        RegionInfo(name='eu-west-1', endpoint='opsworks.eu-west-1.amazonaws.com'),\n        RegionInfo(name='eu-central-1', endpoint='opsworks.eu-central-1.amazonaws.com'),\n        RegionInfo(name='ap-southeast-1', endpoint='opsworks.ap-southeast-1.amazonaws.com'),\n        RegionInfo(name='ap-southeast-2', endpoint='opsworks.ap-southeast-2.amazonaws.com'),\n        RegionInfo(name='ap-northeast-1', endpoint='opsworks.ap-northeast-1.amazonaws.com'),\n        RegionInfo(name='ap-northeast-2', endpoint='opsworks.ap-northeast-2.amazonaws.com'),\n        RegionInfo(name='sa-east-1', endpoint='opsworks.sa-east-1.amazonaws.com'),\n    ]", "idx": 723}
{"namespace": "boto.opsworks.connect_to_region", "completion": "    from boto.opsworks.layer1 import OpsWorksConnection\n    return OpsWorksConnection(region=RegionInfo(name=region_name,\n                                               endpoint='opsworks.%s.amazonaws.com' % (region_name,)))", "idx": 724}
{"namespace": "boto.sqs.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)", "idx": 725}
{"namespace": "boto.rds2.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)", "idx": 726}
{"namespace": "boto.cloudsearch2.connect_to_region", "completion": "    from boto.cloudsearch2.layer1 import CloudSearchConnection\n    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)", "idx": 727}
{"namespace": "boto.cloudtrail.connect_to_region", "completion": "    from boto.cloudtrail.layer1 import CloudTrailConnection\n    return CloudTrailConnection(region=RegionInfo(name=region_name,\n                                                 endpoint='cloudtrail.%s.amazonaws.com' % region_name),\n                                **kw_params)", "idx": 728}
{"namespace": "boto.elasticache.connect_to_region", "completion": "    from boto.elasticache.layer1 import ElastiCacheConnection\n    return ElastiCacheConnection(region=RegionInfo(name=region_name,\n                                                  endpoint='elasticache.%s.amazonaws.com' % (region_name,)),\n                                 **kw_params)", "idx": 729}
{"namespace": "boto.ses.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)", "idx": 730}
{"namespace": "boto.codedeploy.connect_to_region", "completion": "    from boto.codedeploy.layer1 import CodeDeployConnection\n    return CodeDeployConnection(region=RegionInfo(name=region_name,\n                                                 endpoint='codedeploy.%s.amazonaws.com' % region_name),\n                                **kw_params)", "idx": 731}
{"namespace": "boto.sts.credentials.Credentials.to_dict", "completion": "        return {\n            'access_key': self.access_key,\n            'secret_key': self.secret_key,\n            'session_token': self.session_token,\n            'expiration': self.expiration,\n            'request_id': self.request_id,\n        }\n", "idx": 732}
{"namespace": "boto.sts.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 733}
{"namespace": "boto.machinelearning.connect_to_region", "completion": "    from boto.machinelearning.layer1 import MachineLearningConnection\n    return MachineLearningConnection(region=RegionInfo(name=region_name,\n                                                       endpoint='machinelearning.us-east-1.amazonaws.com'),\n                                      **kw_params)", "idx": 734}
{"namespace": "boto.vpc.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)", "idx": 735}
{"namespace": "boto.vpc.VPCConnection.get_all_vpc_peering_connections", "completion": "        params = {}\n        if vpc_peering_connection_ids:\n            self.build_list_params(params, vpc_peering_connection_ids, 'VpcPeeringConnectionId')\n        if filters:\n            self.build_filter_params(params, filters)\n        if dry_run:\n            params['DryRun'] = 'true'\n        return self.get_list('DescribeVpcPeeringConnections', params, [('item', VpcPeeringConnection)])\n", "idx": 736}
{"namespace": "boto.kinesis.connect_to_region", "completion": "    from boto.kinesis.layer1 import KinesisConnection\n    return KinesisConnection(region=RegionInfo(name=region_name,\n                                              endpoint='kinesis.%s.amazonaws.com' % region_name),\n                             **kw_params)", "idx": 737}
{"namespace": "boto.ec2containerservice.connect_to_region", "completion": "    from boto.ec2containerservice.layer1 import EC2ContainerServiceConnection\n    return EC2ContainerServiceConnection(region=region_name, **kw_params)", "idx": 738}
{"namespace": "boto.dynamodb2.table.Table._introspect_indexes", "completion": "        indexes = []\n\n        for field in raw_indexes:\n            if field['Projection']['ProjectionType'] == 'ALL':\n                index_klass = AllIndex\n            elif field['Projection']['ProjectionType'] == 'KEYS_ONLY':\n                index_klass = KeysOnlyIndex\n            elif field['Projection']['ProjectionType'] == 'INCLUDE':\n                index_klass = IncludeIndex\n                kwargs['includes'] = field['Projection']['NonKeyAttributes']\n            else:\n                raise exceptions.UnknownIndexFieldError(\n                    \"%s was seen, but is unknown. Please report this at \"\n                    \"https://github.com/boto/boto/issues.\" % \\\n                    field['Projection']['ProjectionType']\n                )\n\n            name = field['IndexName']\n            kwargs['parts'] = self._introspect_schema(field['KeySchema'], None)\n            indexes.append(index_klass(name, **kwargs))\n\n        return indexes\n", "idx": 739}
{"namespace": "boto.dynamodb2.table.Table.describe", "completion": "        raw_data = self.connection.describe_table(self.table_name)\n        self.schema = self._introspect_schema(\n            raw_data['Table']['KeySchema'],\n            raw_data['Table']['AttributeDefinitions']\n        )\n        self.indexes = self._introspect_indexes(\n            raw_data['Table'].get('LocalSecondaryIndexes', [])\n        )\n        self.global_indexes = self._introspect_global_indexes(\n            raw_data['Table'].get('GlobalSecondaryIndexes', [])\n        )\n        self.throughput = {\n            'read': raw_data['Table']['ProvisionedThroughput']['ReadCapacityUnits'],\n            'write': raw_data['Table']['ProvisionedThroughput']['WriteCapacityUnits'],\n        }\n        return raw_data\n", "idx": 740}
{"namespace": "boto.dynamodb2.table.Table.update", "completion": "        if throughput is not None:\n            self.throughput = throughput\n\n        if global_indexes is not None:\n            self.global_indexes = global_indexes\n\n        # Prep the schema.\n        raw_schema = []\n        attr_defs = []\n        seen_attrs = set()\n\n        for field in self.schema:\n            raw_schema.append(field.schema())\n            # Build the attributes off what we know.\n            seen_attrs.add(field.name)\n            attr_defs.append(field.definition())\n\n        raw_throughput = {\n            'ReadCapacityUnits': int(self.throughput['read']),\n            'WriteCapacityUnits': int(self.throughput['write']),\n        }\n        kwargs = {}\n\n        kwarg_map = {\n            'indexes': 'local_secondary_indexes',\n            'global_indexes': 'global_secondary_indexes',\n        }\n        for index_attr in ('indexes', 'global_indexes'):\n            table_indexes = getattr(self, index_attr)\n            if table_indexes:\n                raw_indexes = []\n                for index_field in table_indexes:\n                    raw_indexes.append(index_field.schema())\n                    # Make sure all attributes specified in the indexes are\n                    # added to the definition\n                    for field in index_field.parts:\n                        if field.name not in seen_attrs:\n                            seen_attrs.add(field.name)\n                            attr_defs.append(field.definition())\n\n                kwargs[kwarg_map[index_attr]] = raw_indexes\n\n        self.connection.update_table(\n            table_name=self.table_name,\n            attribute_definitions=attr_defs,\n            key_schema=raw_schema,\n            provisioned_throughput=raw_throughput,\n            **kwargs\n        )\n        return True\n", "idx": 741}
{"namespace": "boto.dynamodb2.table.Table.create_global_secondary_index", "completion": "        if not isinstance(global_index, GlobalBaseIndexField):\n            raise TypeError('global_index must be a subclass of GlobalBaseIndexField')\n\n        self.connection.update_table(\n            self.table_name,\n            global_secondary_index_updates=[\n                {\n                    \"Create\": global_index.schema(),\n                },\n            ],\n        )\n\n        self.describe()\n\n        return True\n", "idx": 742}
{"namespace": "boto.dynamodb2.table.Table.delete_global_secondary_index", "completion": "        if global_index_name:\n            gsi_data = []\n\n            gsi_data.append({\n                \"Delete\": {\n                    \"IndexName\": global_index_name\n                }\n            })\n\n            self.connection.update_table(\n                self.table_name,\n                global_secondary_index_updates=gsi_data\n            )\n\n            return True\n        else:\n            msg = 'You need to provide the global index name to ' \\\n                  'delete_global_secondary_index method'\n            boto.log.error(msg)\n\n            return False\n", "idx": 743}
{"namespace": "boto.dynamodb2.table.Table.update_global_secondary_index", "completion": "        if global_indexes:\n            gsi_data = []\n\n            for gsi_name, gsi_throughput in global_indexes.items():\n                gsi_data.append({\n                    \"Update\": {\n                        \"IndexName\": gsi_name,\n                        \"ProvisionedThroughput\": {\n                            \"ReadCapacityUnits\": int(gsi_throughput['read']),\n                            \"WriteCapacityUnits\": int(gsi_throughput['write']),\n                        },\n                    },\n                })\n\n            self.connection.update_table(\n                self.table_name,\n                global_secondary_index_updates=gsi_data,\n            )\n\n            return True\n        else:\n            msg = 'You need to provide the global_indexes to ' \\\n                  'update_global_secondary_index method'\n            boto.log.error(msg)\n\n            return False", "idx": 744}
{"namespace": "boto.dynamodb2.table.Table.delete", "completion": "        self.connection.delete_table(self.table_name)\n        return True", "idx": 745}
{"namespace": "boto.dynamodb2.table.Table.get_item", "completion": "        raw_key = self._encode_keys(kwargs)\n        raw_attributes = []\n\n        if attributes:\n            for attribute in attributes:\n                raw_attributes.append(attribute)\n\n        raw_result = self.connection.get_item(\n            self.table_name,\n            raw_key,\n            consistent_read=consistent,\n            attributes_to_get=raw_attributes,\n        )\n\n        if 'Item' not in raw_result:\n            raise exceptions.ItemNotFound(\n                \"Item not found: %s\" % raw_key\n            )\n\n        return Item(self, raw_result['Item'])\n", "idx": 746}
{"namespace": "boto.dynamodb2.table.Table.has_item", "completion": "        raw_key = self._encode_keys(kwargs)\n        item_data = self.connection.get_item(\n            self.table_name,\n            raw_key,\n            attributes_to_get=attributes,\n            consistent_read=consistent\n        )\n        if 'Item' not in item_data:\n            return False\n        return True\n", "idx": 747}
{"namespace": "boto.dynamodb2.table.Table._put_item", "completion": "        if expects is None:\n            expects = {}\n        raw_data = {}\n\n        for key, value in item_data.items():\n            raw_data[key] = self._dynamizer.encode(value)\n\n        self.connection.put_item(\n            self.table_name,\n            raw_data,\n            expected=expects,\n        )\n        return True\n", "idx": 748}
{"namespace": "boto.dynamodb2.table.Table.delete_item", "completion": "        raw_key = self._encode_keys(kwargs)\n        kwargs = {}\n\n        if expected is not None:\n            kwargs['expected'] = expected\n\n        if conditional_operator is not None:\n            kwargs['conditional_operator'] = conditional_operator\n\n        self.connection.delete_item(self.table_name, raw_key, **kwargs)\n        return True\n", "idx": 749}
{"namespace": "boto.dynamodb2.table.Table.get_key_fields", "completion": "        if not self.schema:\n            self.describe()\n        return [field.name for field in self.schema]\n", "idx": 750}
{"namespace": "boto.dynamodb2.table.Table._build_filters", "completion": "        filters = {}\n        for key, value in filter_kwargs.items():\n            if '__' in key:\n                field, operator = key.split('__')\n                if operator not in using:\n                    raise exceptions.InvalidQueryOperatorError(\n                        \"The operator %s is not valid. Please report this at \"\n                        \"https://github.com/boto/boto/issues.\" % operator\n                    )\n                filters[field] = {\n                    using[operator]: self._dynamizer.encode(value)\n                }\n            else:\n                filters[key] = {\n                    'EQ': self._dynamizer.encode(value)\n                }\n        return filters\n", "idx": 751}
{"namespace": "boto.dynamodb2.table.Table.batch_get", "completion": "        results = ResultSet(\n            max_page_size=self.max_batch_get\n        )\n        kwargs = {\n            'keys': keys,\n            'consistent': consistent,\n            'attributes': attributes,\n        }\n        results.to_call(self._batch_get, **kwargs)\n        return results\n", "idx": 752}
{"namespace": "boto.dynamodb2.table.Table.count", "completion": "        return self.connection.describe_table(self.table_name)['Table']['ItemCount']", "idx": 753}
{"namespace": "boto.dynamodb2.table.BatchTable.put_item", "completion": "        item = Item(self.table, data=data)\n        self._to_put.append(item)\n\n        if len(self._to_put) >= self.table.max_batch_get:\n            self.flush()\n", "idx": 754}
{"namespace": "boto.dynamodb2.table.BatchTable.delete_item", "completion": "        self._to_delete.append(kwargs)\n\n        if self.should_flush():\n            self.flush()\n", "idx": 755}
{"namespace": "boto.dynamodb2.table.BatchTable.flush", "completion": "        if self._to_put:\n            # Prepare the data to be inserted.\n            items = {\n                self.table.table_name: [\n                    {\n                        'PutRequest': {\n                            'Item': self._dynamizer.encode(item)\n                        }\n                    } for item in self._to_put\n                ]\n            }\n            self._to_put = []\n            self.table.connection.batch_write_item(request_items=items)\n\n        if self._to_delete:\n            # Prepare the data to be deleted.\n            items = {\n                self.table.table_name: [\n                    {\n                        'DeleteRequest': {\n                            'Key': self._dynamizer.encode(item)\n                        }\n                    } for item in self._to_delete\n                ]\n            }\n            self._to_delete = []\n            self.table.connection.batch_write_item(request_items=items)\n\n        return True\n", "idx": 756}
{"namespace": "boto.dynamodb2.table.BatchTable.resend_unprocessed", "completion": "        while self._unprocessed:\n            batch_data = {\n                self.table.table_name: [\n                    # We'll insert data here shortly.\n                ],\n            }\n\n            for item in self._unprocessed:\n                if 'PutRequest' in item:\n                    batch_data[self.table.table_name].append(item)\n                elif 'DeleteRequest' in item:\n                    batch_data[self.table.table_name].append(item)\n\n            resp = self.table.connection.batch_write_item(batch_data)\n            self.handle_unprocessed(resp)", "idx": 757}
{"namespace": "boto.dynamodb2.fields.BaseSchemaField.definition", "completion": "        return {\n            'AttributeName': self.name,\n            'AttributeType': self.attr_type\n        }", "idx": 758}
{"namespace": "boto.dynamodb2.fields.BaseIndexField.definition", "completion": "        return [\n            {\n                'AttributeName': part.name,\n                'AttributeType': part.data_type,\n            }\n            for part in self.parts\n        ]\n", "idx": 759}
{"namespace": "boto.dynamodb2.fields.BaseIndexField.schema", "completion": "        key_schema = []\n\n        for part in self.parts:\n            key_schema.append({\n                'AttributeName': part.name,\n                'KeyType': part.attr_type,\n            })\n\n        return {\n            'IndexName': self.name,\n            'KeySchema': key_schema,\n            'Projection': {\n                'ProjectionType': self.projection_type,\n            },\n        }", "idx": 760}
{"namespace": "boto.dynamodb2.fields.GlobalBaseIndexField.schema", "completion": "        schema_data = super(GlobalBaseIndexField, self).schema()\n        schema_data['ProvisionedThroughput'] = self.throughput\n        return schema_data", "idx": 761}
{"namespace": "boto.dynamodb2.fields.GlobalIncludeIndex.schema", "completion": "        schema_data = super(GlobalIncludeIndex, self).schema()\n        schema_data.update(super(GlobalBaseIndexField, self).schema())\n        return schema_data", "idx": 762}
{"namespace": "boto.dynamodb2.items.Item.get_keys", "completion": "        keys = {}\n        for key in self.table.key_fields:\n            keys[key] = self[key]\n        return keys", "idx": 763}
{"namespace": "boto.dynamodb2.items.Item.get_raw_keys", "completion": "        key_fields = self.table.get_key_fields()\n        key_data = {}\n\n        for key in key_fields:\n            key_data[key] = self._dynamizer.encode(self[key])\n\n        return key_data\n", "idx": 764}
{"namespace": "boto.dynamodb2.items.Item.build_expects", "completion": "        if fields is None:\n            fields = self.keys()\n\n        expects = {}\n\n        for field in fields:\n            if field not in self:\n                expects[field] = {\n                    'Exists': False,\n                }\n            elif self[field] is NEWVALUE:\n                expects[field] = {\n                    'Exists': True,\n                }\n            elif self[field] != self._orig_data.get(field, None):\n                expects[field] = {\n                    'Value': self._dynamizer.encode(self[field]),\n                }\n\n        return expects\n", "idx": 765}
{"namespace": "boto.dynamodb2.items.Item.prepare_full", "completion": "        data = {}\n        for key, value in self._data.items():\n            data[key] = self._dynamizer.encode(value)\n        return data\n", "idx": 766}
{"namespace": "boto.dynamodb2.items.Item.prepare_partial", "completion": "        # This doesn't save on its own. Rather, we prepare the datastructure\n        # and hand-off to the table to handle creation/update.\n        final_data = {}\n        fields = set()\n\n        # Determine the alterations made to the fields\n        alterations = self._determine_alterations()\n\n        # Encode the values for the fields that were added\n        for key, value in alterations['adds'].items():\n            final_data[key] = {\n                'Action': 'PUT',\n                'Value': self._dynamizer.encode(value),\n            }\n            fields.add(key)\n\n        # Encode the values for the fields that were changed\n        for key, value in alterations['changes'].items():\n            final_data[key] = {\n                'Action': 'PUT',\n                'Value': self._dynamizer.encode(value),\n            }\n            fields.add(key)\n\n        # Encode the values for the fields that were deleted\n        for key in alterations['deletes']:\n            final_data[key] = {\n                'Action': 'DELETE',\n            }\n            fields.add(key)\n\n        return final_data, fields", "idx": 767}
{"namespace": "boto.dynamodb2.items.Item.partial_save", "completion": "        if not self.needs_save():\n            return False\n\n        data, fields = self.prepare_partial()\n        key_data = self.get_raw_keys()\n        table_name = self.table.get_name()\n\n        try:\n            self.table._connection.update_item(\n                TableName=table_name,\n                Key=key_data,\n                AttributeUpdates=data,\n            )\n        except Exception as e:\n            raise e\n\n        self.mark_clean()\n        return True\n", "idx": 768}
{"namespace": "boto.dynamodb2.items.Item.save", "completion": "        if not overwrite and not self.needs_save():\n            return False\n\n        key = self.get_keys()\n        final_data = self.prepare_full()\n\n        if not final_data:\n            return False\n\n        # Remove the key(s) from the ``final_data`` if present.\n        # They should only be present if this is a new item, in which\n        # case we shouldn't be sending as part of the data to update.\n        for fieldname, value in key.items():\n            if fieldname in final_data:\n                del final_data[fieldname]\n\n        # Build expectations of only the fields we're planning to update.\n        expects = self.build_expects()\n        returned = self.table._put_item(key, final_data, expects=expects)\n        # Mark the object as clean.\n        self.mark_clean()\n        return returned", "idx": 769}
{"namespace": "boto.dynamodb2.items.Item.delete", "completion": "        key = self.get_keys()\n        self.table._delete_item(key)\n        return True", "idx": 770}
{"namespace": "boto.dynamodb2.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)", "idx": 771}
{"namespace": "litecli.packages.parseutils.extract_tables", "completion": "    # Parse the SQL statement using the sqlparse library\n    parsed = sqlparse.parse(sql)[0]\n\n    # Extract the table names from the parsed result\n    from_stream = extract_from_part(parsed)\n    tables = list(extract_table_identifiers(from_stream))\n\n    # Return the list of table names\n    return tables", "idx": 772}
{"namespace": "litecli.packages.parseutils.queries_start_with", "completion": "    for query in queries:\n        if query_starts_with(query, prefixes):\n            return True\n    return False", "idx": 773}
{"namespace": "litecli.packages.parseutils.is_destructive", "completion": "    # List of keywords that are commonly associated with destructive actions\n    destructive_keywords = [\n        \"ALTER\",\n        \"ATTACH\",\n        \"BEGIN\",\n        \"COMMIT\",\n        \"CREATE\",\n        \"DELETE\",\n        \"DETACH\",\n        \"DROP\",\n        \"END\",\n        \"INSERT\",\n        \"PRAGMA\",\n        \"REINDEX\",\n        \"RELEASE\",\n        \"REPLACE\",\n        \"ROLLBACK\",\n        \"SAVEPOINT\",\n        \"TRUNCATE\",\n        \"UPDATE\",\n        \"VACUUM\",\n    ]\n\n    # Check if any of the queries start with a destructive keyword\n    for query in sqlparse.split(queries):\n        if query and query_starts_with(query, destructive_keywords) is True:\n            return True\n\n    return False", "idx": 774}
{"namespace": "litecli.packages.completion_engine.suggest_type", "completion": "    # If the text before the cursor is empty, then the suggestion type is 'default' and the suggestion scope is None.\n    if not text_before_cursor:\n        return [{'type': 'default', 'schema': None}]\n\n    # If the text before the cursor is not empty, then we need to parse the text before the cursor to determine the suggestion type and scope.\n    # We use the sqlparse library to parse the text before the cursor.\n    # We use the last_word function to get the last word in the text before the cursor.\n    # We use the extract_tables function to get the tables in the text before the cursor.\n    # We use the find_prev_keyword function to get the previous keyword in the text before the cursor.\n    # We use the parse_special_command function to get the special command in the text before the cursor.\n    # We use the parse_special_command function to get the special command in the text before the cursor.\n    # We use the parse_special_command function to get the special command in the text before the cursor.\n    # We use the parse_special_command function to get the special command in the text before the cursor.\n    # We use the parse_special_command function to get the special command in the text before the cursor.\n    # We use the parse_special_command function to get the special command in the text before the cursor.\n    # We use the parse_special_command function to get the special command in the text before the cursor.\n    # We use the parse_special_command function to get the special command in the text before the cursor.\n    # We use the parse_special_command function to get the special command in the text before the cursor.\n    # We use the parse_special_command function to get the special command in the text before the cursor.\n    # We use the parse_special_command function to get the special command in the text before the cursor.\n    # We use the parse_special_command function to get the special command in the text before the cursor.\n    # We use the parse_special_command function to get the special command in the text before the cursor.\n    # We use the parse_special_command function to get the special command in the", "idx": 775}
{"namespace": "datasette.plugins.get_plugins", "completion": "    plugins = []\n    for plugin in pm.get_plugins():\n        name = plugin.name\n        hooks = [hook.name for hook in pm.get_hookcallers(plugin)]\n        static_path = None\n        templates_path = None\n        version = None\n        project_name = None\n        if hasattr(plugin, \"static_mount_dir\"):\n            static_path = plugin.static_mount_dir()\n        if hasattr(plugin, \"templates_mount_dir\"):\n            templates_path = plugin.templates_mount_dir()\n        if hasattr(plugin, \"__version__\"):\n            version = plugin.__version__\n        if hasattr(plugin, \"__datasette_project_name__\"):\n            project_name = plugin.__datasette_project_name__\n        plugins.append(\n            {\n                \"name\": name,\n                \"hooks\": hooks,\n                \"static_path\": static_path,\n                \"templates_path\": templates_path,\n                \"version\": version,\n                \"project_name\": project_name,\n            }\n        )\n    return plugins", "idx": 776}
{"namespace": "datasette.facets.ColumnFacet.suggest", "completion": "        row_count = await self.get_row_count()\n        columns = await self.get_columns(self.sql, self.params)\n        facet_size = self.get_facet_size()\n        suggested_facets = []\n        for column in columns:\n            sql = f\"select {escape_sqlite(column)}, count(*) from ({self.sql}) group by {escape_sqlite(column)}\"\n            results = await self.ds.execute(self.database, sql, self.params)\n            distinct_values = len(results.rows)\n            if distinct_values > 0 and distinct_values < row_count:\n                if distinct_values <= facet_size:\n                    if any(count > 1 for value, count in results.rows):\n                        suggested_facets.append(\n                            {\n                                \"name\": column,\n                                \"toggle_url\": path_with_added_args(\n                                    self.request, {\"_facet\": column}\n                                ),\n                            }\n                        )\n        return suggested_facets\n", "idx": 777}
{"namespace": "datasette.facets.ColumnFacet.facet_results", "completion": "        facet_size = self.get_facet_size()\n        facet_results = []\n        facets_timed_out = []\n        for config in self.get_configs():\n            config = config[\"config\"]\n            column = config[\"simple\"]\n            label = config.get(\"label\")\n            if not label:\n                label = column\n            # TODO: Support custom SQL here\n            sql = f\"select {escape_sqlite(column)} as value, count(*) as n from ({self.sql})\"\n            if config.get(\"sql\"):\n                sql = config[\"sql\"]\n            sql += \" group by value\"\n            sql += f\" limit {facet_size + 1}\"\n            try:\n                facet_values = await self.ds.execute(\n                    self.database,\n                    sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n            except QueryInterrupted:\n                facets_timed_out.append(column)\n                continue\n            # If we got more than facet_size results, truncate and set a flag\n            truncated = False\n            if len(facet_values) > facet_size:\n                truncated = True\n                facet_values = facet_values[:-1]\n            for facet_value in facet_values:\n                value = facet_value[\"value\"]\n                count = facet_value[\"n\"]\n                # TODO: Support custom SQL here\n                toggle_url = self.ds.absolute_url(\n                    self.request,\n                    self.ds.urls.path(\n                        path_with_removed_args(\n                            self.request, {\"_facet\": column, f\"{column}\": value}\n                        )\n                    ),\n                )\n                facet_results.append(\n                    {\n                        \"value\": value,\n                        \"label\": label,\n                        \"count\": count,\n                        \"toggle_url\": toggle_url,\n                        \"selected\": self.request.args.get(column) == value,", "idx": 778}
{"namespace": "datasette.facets.ArrayFacet.suggest", "completion": "        row_count = await self.get_row_count()\n        columns = await self.get_columns(self.sql, self.params)\n        facet_size = self.get_facet_size()\n        suggested_facets = []\n        already_enabled = [c[\"config\"][\"simple\"] for c in self.get_configs()]\n        for column in columns:\n            if column in already_enabled:\n                continue\n            suggested_facet_sql = \"\"\"\n                select {column} as value, count(*) as n from (\n                    {sql}\n                ) where value is not null\n                group by value\n                limit {limit}\n            \"\"\".format(\n                column=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            distinct_values = None\n            try:\n                distinct_values = await self.ds.execute(\n                    self.database,\n                    suggested_facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_suggest_time_limit_ms\"),\n                )\n                num_distinct_values = len(distinct_values)\n                if (\n                    1 < num_distinct_values < row_count\n                    and num_distinct_values <= facet_size\n                    # And at least one has n > 1\n                    and any(r[\"n\"] > 1 for r in distinct_values)\n                ):\n                    suggested_facets.append(\n                        {\n                            \"name\": column,\n                            \"toggle_url\": self.ds.absolute_url(\n                                self.request,\n                                self.ds.urls.path(\n                                    path_with_added_args(\n                                        self.request, {\"_facet\": column}\n                                    )\n                                ),\n                            ),\n                        }\n                    )\n            except QueryInterrupted:\n                continue\n        return suggested_facets\n", "idx": 779}
{"namespace": "datasette.facets.ArrayFacet.facet_results", "completion": "        facet_results = []\n        facets_timed_out = []\n\n        qs_pairs = self.get_querystring_pairs()\n\n        facet_size = self.get_facet_size()\n        for source_and_config in self.get_configs():\n            config = source_and_config[\"config\"]\n            source = source_and_config[\"source\"]\n            column = config.get(\"column\") or config[\"simple\"]\n            facet_sql = \"\"\"\n                select json_each.value as value, count(*) as count from (\n                    {sql}\n                ) join json_each({column})\n                where {column} is not null and {column} != ''\n                group by json_each.value order by count desc, value limit {limit}\n            \"\"\".format(\n                column=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            try:\n                facet_rows_results = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n                facet_results_values = []\n                facet_results.append(\n                    {\n                        \"name\": column,\n                        \"type\": self.type,\n                        \"hideable\": source != \"metadata\",\n                        \"toggle_url\": self.ds.urls.path(\n                            path_with_removed_args(self.request, {\"_facet_array\": column})\n                        ),\n                        \"results\": facet_results_values,\n                        \"truncated\": len(facet_rows_results) > facet_size,\n                    }\n                )\n                facet_rows = facet_rows_results.rows[:facet_size]\n                for row in facet_rows:\n                    column_qs = column\n                    if column.startswith(\"_\"):\n                        column_qs = \"{}__exact\".format(column)\n                    selected = (column_q", "idx": 780}
{"namespace": "datasette.facets.DateFacet.facet_results", "completion": "        facet_results = []\n        facets_timed_out = []\n\n        facet_size = self.get_facet_size()\n        for source_and_config in self.get_configs():\n            config = source_and_config[\"config\"]\n            source = source_and_config[\"source\"]\n            column = config.get(\"column\") or config[\"simple\"]\n            facet_sql = \"\"\"\n                select\n                    date({col}) as value,\n                    count(*) as count\n                from (\n                    {sql}\n                )\n                where\n                    {col} is not null and\n                    {col} != '' and\n                    {col} glob '????-??-*'\n                group by\n                    value\n                order by\n                    count(*) desc,\n                    value\n                limit {limit}\n            \"\"\".format(\n                col=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            try:\n                facet_rows_results = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n                facet_results_values = []\n                facet_results.append(\n                    {\n                        \"name\": column,\n                        \"type\": self.type,\n                        \"results\": facet_results_values,\n                        \"hideable\": source != \"metadata\",\n                        \"toggle_url\": self.ds.urls.path(\n                            path_with_removed_args(\n                                self.request, {\"_facet_date\": column}\n                            )\n                        ),\n                        \"truncated\": len(facet_rows_results) > facet_size,\n                    }\n                )\n                facet_rows = facet_rows_results.rows[:facet_size]\n                pairs = self.get_querystring_pairs()\n                for row in facet_rows:\n                    value = str(row[\"value\"])\n                    selected = (f\"", "idx": 781}
{"namespace": "datasette.app.Datasette.invoke_startup", "completion": "        if self._startup_invoked:\n            return\n        self._startup_invoked = True\n        await self.refresh_schemas()\n        await self.client.refresh_metadata()\n        await self.client.refresh_permissions()\n        await self.client.refresh_settings()\n        await self.client.refresh_plugins()\n        await self.client.refresh_databases()\n        await self.client.refresh_versions()\n        await self.client.refresh_plugins()\n        await self.client.refresh_metadata()\n        await self.client.refresh_permissions()\n        await self.client.refresh_settings()\n        await self.client.refresh_plugins()\n        await self.client.refresh_databases()\n        await self.client.refresh_versions()\n        await self.client.refresh_plugins()\n        await self.client.refresh_metadata()\n        await self.client.refresh_permissions()\n        await self.client.refresh_settings()\n        await self.client.refresh_plugins()\n        await self.client.refresh_databases()\n        await self.client.refresh_versions()\n        await self.client.refresh_plugins()\n        await self.client.refresh_metadata()\n        await self.client.refresh_permissions()\n        await self.client.refresh_settings()\n        await self.client.refresh_plugins()\n        await self.client.refresh_databases()\n        await self.client.refresh_versions()\n        await self.client.refresh_plugins()\n        await self.client.refresh_metadata()\n        await self.client.refresh_permissions()\n        await self.client.refresh_settings()\n        await self.client.refresh_plugins()\n        await self.client.refresh_databases()\n        await self.client.refresh_versions()\n        await self.client.refresh_plugins()\n        await self.client.refresh_metadata()\n        await self.client.refresh_permissions()\n        await self.client.refresh_", "idx": 782}
{"namespace": "datasette.app.Datasette.get_database", "completion": "        if route:\n            for database in self.databases.values():\n                if database.route == route:\n                    return database\n        for database in self.databases.values():\n            if database.name != \"_internal\" and (name is None or database.name == name):\n                return database\n", "idx": 783}
{"namespace": "datasette.app.Datasette.add_database", "completion": "        # Create a copy of the existing databases\n        databases = dict(self.databases)\n\n        # Assign a unique name to the new database\n        if name is None:\n            name = db.name\n        if name in databases:\n            i = 1\n            while True:\n                new_name = \"{}_{}\".format(name, i)\n                if new_name not in databases:\n                    name = new_name\n                    break\n                i += 1\n\n        # Assign the name and route to the new database\n        db.name = name\n        db.route = route or name\n\n        # Add the new database to the copied databases dictionary\n        databases[name] = db\n\n        # Assign the copied dictionary back to the instance\n        self.databases = databases\n\n        return db\n", "idx": 784}
{"namespace": "datasette.app.Datasette.ensure_permissions", "completion": "        for permission in permissions:\n            if isinstance(permission, str):\n                action = permission\n                resource = None\n            else:\n                action, resource = permission\n            if not await self.permission_allowed(\n                actor=actor, action=action, resource=resource, default=False\n            ):\n                raise Forbidden(\n                    \"Permission denied: {} on {}\".format(action, resource or \"\")\n                )\n", "idx": 785}
{"namespace": "datasette.app.Datasette.check_visibility", "completion": "        assert actor is None or isinstance(actor, dict), \"actor must be None or a dict\"\n        if permissions is not None:\n            await self.ensure_permissions(actor, permissions)\n        if action is not None and resource is not None:\n            permissions = [(action, resource)]\n        elif action is not None:\n            permissions = [action]\n        else:\n            permissions = []\n        for permission in permissions:\n            if isinstance(permission, str):\n                action = permission\n                resource = None\n            elif isinstance(permission, (tuple, list)) and len(permission) == 2:\n                action, resource = permission\n            else:\n                assert (\n                    False\n                ), \"permission should be string or tuple of two items: {}\".format(\n                    repr(permission)\n                )\n            ok = await self.permission_allowed(\n                actor,\n                action,\n                resource=resource,\n                default=None,\n            )\n            if ok is not None:\n                if ok:\n                    return True, False\n                else:\n                    raise Forbidden(action)\n        return False, False\n", "idx": 786}
{"namespace": "datasette.app.Datasette.render_template", "completion": "        if not self._startup_invoked:\n            raise StartupError(\"Datasette.render_template() called before startup\")\n        if context is None:\n            context = {}\n        context.update(\n            {\n                \"datasette\": self,\n                \"request\": request,\n                \"view_name\": view_name,\n                \"settings\": self.settings_dict(),\n                \"urls\": self.urls,\n                \"crumbs\": await self._crumb_items(request),\n                \"messages\": self._show_messages(request),\n                \"version\": __version__,\n                \"asgi_version\": asgi_version,\n                \"asgi_csrf\": asgi_csrf,\n                \"asgi_static\": asgi_static,\n                \"asgi_send\": asgi_send,\n                \"asgi_send_file\": asgi_send_file,\n                \"asgi_send_redirect\": asgi_send_redirect,\n                \"asgi_lifespan\": AsgiLifespan,\n                \"asgi_run_on_first_request\": AsgiRunOnFirstRequest,\n                \"asgi_tracer\": AsgiTracer,\n                \"asgi_trace_debug\": self.setting(\"trace_debug\"),\n                \"asgi_template_debug\": self.setting(\"template_debug\"),\n                \"asgi_csrf_token\": request.scope.get(\"asgi_csrf_token\"),\n                \"asgi_csrf_cookie_name\": request.scope.get(\"asgi_csrf_cookie_name\"),\n                \"asgi_csrf_header_name\": request.scope.get(\"asgi_csrf_header_name\"),\n                \"asgi_csrf_field_name\": request.scope.get(\"asgi_csrf_field_name\"),\n                \"asgi_csrf_cookie_domain\": request.scope.get(\"asgi_csrf_cookie_domain\"),\n                \"asgi_csrf_cookie_path\": request.scope.get(\"asgi_csrf_cookie_path\"),\n                \"asgi", "idx": 787}
{"namespace": "datasette.app.DatasetteClient.get", "completion": "        path = self._fix(path)\n        async with httpx.AsyncClient(app=self.app, base_url=\"http://testserver\") as client:\n            return await client.get(path, **kwargs)\n", "idx": 788}
{"namespace": "datasette.utils.asgi.Request.full_path", "completion": "        if self.query_string:\n            return self.path + \"?\" + self.query_string\n        else:\n            return self.path\n", "idx": 789}
{"namespace": "datasette.utils.asgi.Request.post_body", "completion": "        body = b\"\"\n        more_body = True\n        while more_body:\n            message = await self.receive()\n            body += message.get(\"body\", b\"\")\n            more_body = message.get(\"more_body\", False)\n        return body\n", "idx": 790}
{"namespace": "datasette.utils.asgi.Request.fake", "completion": "        path, _, query_string = path_with_query_string.partition(\"?\")\n        return cls(\n            {\n                \"type\": \"http\",\n                \"method\": method,\n                \"path\": path,\n                \"query_string\": query_string.encode(\"latin-1\"),\n                \"url_route\": {\"kwargs\": url_vars or {}},\n                \"scheme\": scheme,\n            },\n            None,\n        )\n\n", "idx": 791}
{"namespace": "datasette.utils.asgi.Response.asgi_send", "completion": "        headers = {k: v for k, v in self.headers.items()}\n        headers[\"content-type\"] = self.content_type\n        for key, value in self._set_cookie_headers:\n            headers.setdefault(\"set-cookie\", []).append(value)\n        await send(\n            {\n                \"type\": \"http.response.start\",\n                \"status\": self.status,\n                \"headers\": [\n                    [key.encode(\"utf-8\"), value.encode(\"utf-8\")]\n                    for key, value in headers.items()\n                ],\n            }\n        )\n        await send(\n            {\n                \"type\": \"http.response.body\",\n                \"body\": self.body.encode(\"utf-8\"),\n                \"more_body\": False,\n            }\n        )\n", "idx": 792}
{"namespace": "datasette.utils.asgi.Response.set_cookie", "completion": "        if samesite not in SAMESITE_VALUES:\n            raise ValueError(\n                \"samesite must be one of {}\".format(\", \".join(SAMESITE_VALUES))\n            )\n        cookie = Morsel()\n        cookie.set(key, value, value)\n        if max_age is not None:\n            cookie[\"max-age\"] = max_age\n        if expires is not None:\n            cookie[\"expires\"] = expires\n        cookie[\"path\"] = path\n        if domain is not None:\n            cookie[\"domain\"] = domain\n        if secure:\n            cookie[\"secure\"] = True\n        if httponly:\n            cookie[\"httponly\"] = True\n        cookie[\"samesite\"] = samesite\n        self._set_cookie_headers.append(cookie.output(header=\"\"))", "idx": 793}
{"namespace": "datasette.utils.asgi.Response.html", "completion": "        return cls(body, status, headers, content_type=\"text/html; charset=utf-8\")\n", "idx": 794}
{"namespace": "datasette.utils.asgi.Response.text", "completion": "        return cls(\n            body,\n            status=status,\n            headers=headers,\n            content_type=\"text/plain; charset=utf-8\",\n        )\n", "idx": 795}
{"namespace": "datasette.utils.asgi.Response.json", "completion": "        return cls(\n            json.dumps(body, default=default),\n            status=status,\n            headers=headers,\n            content_type=\"application/json; charset=utf-8\",\n        )\n", "idx": 796}
{"namespace": "datasette.utils.asgi.Response.redirect", "completion": "        headers = headers or {}\n        headers[\"Location\"] = path\n        return cls(\"\", status=status, headers=headers)\n\n", "idx": 797}
{"namespace": "datasette.utils.validate_sql_select", "completion": "    sql = sql.lower()\n    for allowed_sql_re in allowed_sql_res:\n        if allowed_sql_re.match(sql):\n            return\n    for disallowed_sql_re, error_message in disallawed_sql_res:\n        if disallowed_sql_re.search(sql):\n            raise InvalidSql(error_message)\n    raise InvalidSql(\"Statement did not match any allowed patterns\")", "idx": 798}
{"namespace": "datasette.utils.escape_css_string", "completion": "    s = s.replace(\"\\r\\n\", \"\\n\")\n    return _css_re.sub(lambda m: \"\\\\\" + hex(ord(m.group(0)))[1:], s)\n\n", "idx": 799}
{"namespace": "datasette.utils.detect_fts", "completion": "    # Check if the table has a corresponding FTS virtual table\n    fts_table = f\"{table}_fts\"\n    if conn.execute(f\"SELECT name FROM sqlite_master WHERE type='table' AND name='{fts_table}'\").fetchone():\n        return fts_table\n    return None\n\n", "idx": 800}
{"namespace": "datasette.utils.is_url", "completion": "    if not isinstance(value, str):\n        return False\n    if not value.startswith(\"http://\") and not value.startswith(\"https://\"):\n        return False\n    if whitespace_re.search(value):\n        return False\n    return True\n\n", "idx": 801}
{"namespace": "datasette.utils.to_css_class", "completion": "    if css_class_re.match(s):\n        return s\n    s = css_invalid_chars_re.sub(\"\", s)\n    return f\"{s}_{hashlib.md5(s.encode('utf-8')).hexdigest()[:6]}\"\n\n", "idx": 802}
{"namespace": "datasette.utils.escape_fts", "completion": "    terms = [term for term in _escape_fts_re.split(query) if term]\n    terms = list(OrderedDict.fromkeys(terms))\n    if query.count('\"') % 2 == 1:\n        terms.append('\"\"')\n    return \" \".join('\"{}\"'.format(term) for term in terms)\n\n", "idx": 803}
{"namespace": "datasette.utils.check_connection", "completion": "    try:\n        conn.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n    except sqlite3.OperationalError as e:\n        if \"no such table: sqlite_master\" in str(e):\n            raise SpatialiteConnectionProblem(\n                \"This database does not appear to be a Spatialite database\"\n            )\n        elif \"unable to open database file\" in str(e):\n            raise ConnectionProblem(\n                \"Could not open database file. Check that the file exists and is not locked by another process.\"\n            )\n        elif \"database is locked\" in str(e):\n            raise ConnectionProblem(\n                \"Database is locked. Check that the database is not being accessed by another process.\"\n            )\n        elif \"database disk image is malformed\" in str(e):\n            raise ConnectionProblem(\n                \"Database disk image is malformed. Check that the database file is not corrupted.\"\n            )\n        elif \"unable to open database file\" in str(e):\n            raise ConnectionProblem(\n                \"Unable to open database file. Check that the file exists and is not locked by another process.\"\n            )\n        elif \"file is not a database\" in str(e):\n            raise ConnectionProblem(\n                \"File is not a database. Check that the file is a valid SQLite database file.\"\n            )\n        elif \"database disk image is malformed\" in str(e):\n            raise ConnectionProblem(\n                \"Database disk image is malformed. Check that the database file is not corrupted.\"\n            )\n        elif \"unable to open database file\" in str(e):\n            raise ConnectionProblem(\n                \"Unable to open database file. Check that the file exists and is not locked by another process.\"\n            )\n        elif \"file is not a database\" in str(e):\n            raise ConnectionProblem(\n                \"File is not a database. Check that the file is a valid SQLite database file.\"\n            )\n        elif \"database disk image is malformed\" in str(e):\n            raise ConnectionProblem(\n                \"Database disk image is malformed. Check that the database file is not corrupted.\"", "idx": 804}
{"namespace": "datasette.utils.parse_metadata", "completion": "    try:\n        return json.loads(content)\n    except ValueError:\n        pass\n    try:\n        return yaml.safe_load(content)\n    except yaml.YAMLError:\n        pass\n    raise BadMetadataError(\"Invalid metadata format\")\n\n", "idx": 805}
{"namespace": "datasette.utils.call_with_supported_arguments", "completion": "    parameters = inspect.signature(fn).parameters.keys()\n    call_with = []\n    for parameter in parameters:\n        if parameter not in kwargs:\n            raise TypeError(\n                \"{} requires parameters {}, missing: {}\".format(\n                    fn, tuple(parameters), set(parameters) - set(kwargs.keys())\n                )\n            )\n        call_with.append(kwargs[parameter])\n    return fn(*call_with)", "idx": 806}
{"namespace": "datasette.utils.derive_named_parameters", "completion": "    sql = sql.rstrip(\";\")\n    named_parameters = _re_named_parameter.findall(sql)\n    try:\n        explain_results = await db.execute(\n            \"explain \" + sql, dict((p, None) for p in named_parameters)\n        )\n    except sqlite3.OperationalError:\n        return named_parameters\n    variables = [\n        row[\"variable\"]\n        for row in explain_results\n        if row[\"variable\"] and row[\"variable\"].startswith(\":\")\n    ]\n    return [v[1:] for v in variables]\n\n", "idx": 807}
{"namespace": "pyramid.path.Resolver.get_package_name", "completion": "        if self.package is CALLER_PACKAGE:\n            return package_name(caller_module())\n        return package_name(self.package)\n", "idx": 808}
{"namespace": "pyramid.path.Resolver.get_package", "completion": "        if self.package is CALLER_PACKAGE:\n            package = caller_package()\n        else:\n            package = self.package\n        return package\n", "idx": 809}
{"namespace": "pyramid.path.DottedNameResolver.resolve", "completion": "        if ':' in dotted:\n            return self.resolve_pkg_resources_style(dotted)\n        else:\n            return self.resolve_zope_dottedname_style(dotted)\n", "idx": 810}
{"namespace": "pyramid.path.DottedNameResolver.maybe_resolve", "completion": "        if not isinstance(dotted, str):\n            return dotted\n        package = self.get_package()\n        return self.resolve(dotted, package)\n", "idx": 811}
{"namespace": "pyramid.path.PkgResourcesAssetDescriptor.abspath", "completion": "        return self.pkg_resources.resource_filename(self.pkg_name, self.path)\n", "idx": 812}
{"namespace": "pyramid.renderers.render_to_response", "completion": "    if package is None:\n        package = caller_package()\n    helper = RendererHelper(\n        name=renderer_name, package=package, registry=request.registry\n    )\n    return helper.render_to_response(value, request, response)", "idx": 813}
{"namespace": "pyramid.renderers.JSON.add_adapter", "completion": "        self.components.registerAdapter(adapter, (type_or_iface,), IJSONAdapter)\n", "idx": 814}
{"namespace": "pyramid.renderers.RendererHelper.settings", "completion": "        settings = self.registry.settings.get('renderers', {})\n        if settings is None:\n            settings = {}\n        return settings\n", "idx": 815}
{"namespace": "pyramid.renderers.RendererHelper.render_view", "completion": "        system = {\n            'view': view,\n            'renderer_name': self.name,\n            'renderer_info': self,\n            'context': context,\n            'request': request,\n            'csrf_token': get_csrf_token(request),\n        }\n        result = self.renderer(context, system)\n        response.body = result\n", "idx": 816}
{"namespace": "pyramid.renderers.RendererHelper.render", "completion": "", "idx": 817}
{"namespace": "pyramid.renderers.RendererHelper.render_to_response", "completion": "        result = self.render(value, system_values, request=request)\n        response = request.response\n        response.body = result\n        return response", "idx": 818}
{"namespace": "pyramid.renderers.RendererHelper.clone", "completion": "        if name is None:\n            name = self.name\n        if package is None:\n            package = self.package\n        if registry is None:\n            registry = self.registry\n        return RendererHelper(name=name, package=package, registry=registry)", "idx": 819}
{"namespace": "pyramid.urldispatch.RoutesMapper.get_routes", "completion": "        routes = self.routelist[:]\n        if include_static:\n            routes.extend(self.static_routes)\n        return routes\n", "idx": 820}
{"namespace": "pyramid.urldispatch.RoutesMapper.connect", "completion": "        route = Route(\n            name, pattern, factory, predicates, pregenerator\n        )\n        self.routes[name] = route\n        if static is True:\n            self.static_routes.append(route)\n        else:\n            self.routes[name] = route\n            self.routelist.append(route)\n        return route\n", "idx": 821}
{"namespace": "pyramid.testing.DummyTemplateRenderer.assert_", "completion": "        for k, v in kw.items():\n            if k not in self._received:\n                raise AssertionError(\n                    'Key %s not found in renderer' % k)\n            if self._received[k] != v:\n                raise AssertionError(\n                    'Key %s value %s != %s' % (k, self._received[k], v))\n        return True", "idx": 822}
{"namespace": "pyramid.testing.DummyResource.__getitem__", "completion": "        return self.subs[name]", "idx": 823}
{"namespace": "pyramid.testing.DummyResource.clone", "completion": "        if __name__ is _marker:\n            __name__ = self.__name__\n        if __parent__ is _marker:\n            __parent__ = self.__parent__\n        kw = self.kw.copy()\n        kw.update(kw)\n        return self.__class__(__name__, __parent__, **kw)", "idx": 824}
{"namespace": "pyramid.testing.DummySession.get_csrf_token", "completion": "        if '_csrft_' not in self:\n            return self.new_csrf_token()\n        return self['_csrft_']", "idx": 825}
{"namespace": "pyramid.testing.DummyRequest.response", "completion": "        return self.registry.response_factory(self)\n", "idx": 826}
{"namespace": "pyramid.testing.DummyRendererFactory.add", "completion": "        self.renderers[spec] = renderer\n", "idx": 827}
{"namespace": "pyramid.authorization.ACLAuthorizationPolicy.principals_allowed_by_permission", "completion": "        return self.helper.principals_allowed_by_permission(context, permission)", "idx": 828}
{"namespace": "pyramid.url.URLMethodsMixin.route_url", "completion": "        registry = get_current_registry(self.environ)\n        route = registry.introspector.get('routes', route_name)\n        if route is None:\n            raise KeyError(route_name)\n        route_url = route['object']\n        app_url, qs, frag = parse_url_overrides(self, kw)\n        return route_url(app_url, *elements, **kw) + qs + frag\n", "idx": 829}
{"namespace": "pyramid.predicates.CustomPredicate.text", "completion": "        if hasattr(self.func, '__text__'):\n            return self.func.__text__\n        return 'custom predicate %s' % self.func.__name__\n", "idx": 830}
{"namespace": "pyramid.threadlocal.ThreadLocalManager.pop", "completion": "        if self.stack:\n            return self.stack.pop()\n        else:\n            return self.default\n", "idx": 831}
{"namespace": "pyramid.threadlocal.ThreadLocalManager.get", "completion": "        if self.stack:\n            return self.stack[-1]\n        return self.default()", "idx": 832}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.authenticated_userid", "completion": "        debug = self.debug\n        userid = self.unauthenticated_userid(request)\n        if userid is None:\n            debug and self._log(\n                'call to unauthenticated_userid returned None; returning None',\n                'authenticated_userid',\n                request,\n            )\n            return None\n        if self._clean_principal(userid) is None:\n            debug and self._log(\n                (\n                    'use of userid %r is disallowed by any built-in Pyramid '\n                    'security policy, returning None' % userid\n                ),\n                'authenticated_userid',\n                request,\n            )\n            return None\n\n        if self.callback is None:\n            debug and self._log(\n                'there was no groupfinder callback; returning %r' % (userid,),\n                'authenticated_userid',\n                request,\n            )\n            return userid\n        callback_ok = self.callback(userid, request)\n        if callback_ok is not None:  # is not None\n            debug and self._log(\n                'groupfinder callback returned %r; returning %r'\n                % (callback_ok, userid),\n                'authenticated_userid',\n                request,\n            )\n            return userid\n        debug and self._log(\n            'groupfinder callback returned None; returning None',\n            'authenticated_userid',\n            request,\n        )\n        return None\n", "idx": 833}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.unauthenticated_userid", "completion": "        identity = self._get_identity(request)\n        if identity is None:\n            return None\n        return identity['repoze.who.userid']", "idx": 834}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.forget", "completion": "        identifier = self._get_identifier(request)\n        if identifier is None:\n            return []\n        environ = request.environ\n        return identifier.forget(environ)", "idx": 835}
{"namespace": "pyramid.authentication.AuthTktAuthenticationPolicy.unauthenticated_userid", "completion": "        self.debug and self._log(\n            'unauthenticated_userid called', 'unauthenticated_userid', request\n        )\n        userid = self.cookie.identify(request)\n        self.debug and self._log(\n            'unauthenticated_userid returning %r' % (userid,),\n            'unauthenticated_userid',\n            request,\n        )\n        return userid", "idx": 836}
{"namespace": "pyramid.authentication.SessionAuthenticationHelper.remember", "completion": "        session = request.session\n        session[self.userid_key] = userid\n        return []\n", "idx": 837}
{"namespace": "pyramid.authentication.SessionAuthenticationHelper.forget", "completion": "        if self.userid_key in request.session:\n            del request.session[self.userid_key]\n        return []\n", "idx": 838}
{"namespace": "pyramid.authentication.BasicAuthAuthenticationPolicy.unauthenticated_userid", "completion": "        authorization = request.authorization\n        if authorization is not None:\n            try:\n                authmeth, auth = authorization\n            except ValueError:  # not enough values to unpack\n                return None\n            if authmeth.lower() == 'basic':\n                try:\n                    auth = auth.strip().decode('base64')\n                except binascii.Error:  # can't decode\n                    return None\n                try:\n                    login, password = auth.split(':', 1)\n                except ValueError:  # not enough values to unpack\n                    return None\n                return self.check(login, password, request)\n        return None\n", "idx": 839}
{"namespace": "pyramid.request.CallbackMethodsMixin._process_response_callbacks", "completion": "        for callback in self.response_callbacks:\n            callback(self, response)", "idx": 840}
{"namespace": "pyramid.request.CallbackMethodsMixin._process_finished_callbacks", "completion": "        callbacks = self.finished_callbacks\n        while callbacks:\n            callback = callbacks.popleft()\n            callback(self)", "idx": 841}
{"namespace": "pyramid.request.Request.session", "completion": "        if self.registry.session_factory is None:\n            raise ConfigurationError(\n                \"No session factory registered.  Use the \"\n                \"'pyramid.session_factory' configuration directive to register \"\n                \"one.\"\n            )\n        return self.registry.session_factory(self)\n", "idx": 842}
{"namespace": "pyramid.request.RequestLocalCache.get_or_create", "completion": "        if creator is None:\n            creator = self._creator\n        if creator is None:\n            raise ValueError('creator must be provided')\n        value = self._store.get(request, self.NO_VALUE)\n        if value is self.NO_VALUE:\n            value = creator(request)\n            self._store[request] = value\n        return value\n", "idx": 843}
{"namespace": "pyramid.request.RequestLocalCache.set", "completion": "        if request in self._store:\n            self._store[request] = value\n        else:\n            self._store[request] = value\n            request.add_finished_callback(self._remove_request)\n", "idx": 844}
{"namespace": "pyramid.i18n.Localizer.pluralize", "completion": "        if self.pluralizer is None:\n            self.pluralizer = Pluralizer(self.translations)\n        return self.pluralizer(singular, plural, n, domain=domain, mapping=mapping)", "idx": 845}
{"namespace": "pyramid.i18n.Translations.load", "completion": "        if locales is None:\n            locales = []\n        if not isinstance(locales, (list, tuple)):\n            locales = [locales]\n        locales = [str(locale) for locale in locales]\n        if not locales:\n            locales = [get_locale_name(None)]\n        for locale in locales:\n            for dirpath, dirnames, filenames in os.walk(dirname):\n                for filename in filenames:\n                    if filename.endswith('.mo'):\n                        if filename.startswith(domain):\n                            if locale in filename:\n                                filepath = os.path.join(dirpath, filename)\n                                with open(filepath, 'rb') as fileobj:\n                                    return cls(fileobj, domain)\n        return gettext.NullTranslations()\n", "idx": 846}
{"namespace": "pyramid.i18n.Translations.add", "completion": "        if not isinstance(translations, Translations):\n            raise TypeError(\n                'translations must be an instance of Translations'\n            )\n        if translations.domain != self.domain:\n            if translations.domain in self._domains:\n                raise ValueError(\n                    'domain %r already exists in catalog' % translations.domain\n                )\n            self._domains[translations.domain] = translations\n            return self\n        if merge:\n            for key, value in translations._catalog.items():\n                if key in self._catalog:\n                    self._catalog[key] = self._catalog[key] + value\n                else:\n                    self._catalog[key] = value\n        else:\n            self._catalog.update(translations._catalog)\n        self.files.extend(translations.files)\n        return self\n", "idx": 847}
{"namespace": "pyramid.i18n.Translations.merge", "completion": "        for message in translations._catalog.values():\n            if message.id in self._catalog:\n                self._catalog[message.id] = message\n        self.files.extend(translations.files)\n        return self", "idx": 848}
{"namespace": "pyramid.i18n.LocalizerRequestMixin.locale_name", "completion": "        return negotiate_locale_name(self)", "idx": 849}
{"namespace": "pyramid.csrf.LegacySessionCSRFStoragePolicy.check_csrf_token", "completion": "        expected_token = self.get_csrf_token(request)\n        if expected_token is None:\n            return False\n        return not strings_differ(bytes_(expected_token), bytes_(supplied_token))", "idx": 850}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.new_csrf_token", "completion": "        token = self._token_factory()\n        request.session[self.key] = token\n        return token\n", "idx": 851}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.get_csrf_token", "completion": "        if self.key not in request.session:\n            return self.new_csrf_token(request)\n        return request.session[self.key]\n", "idx": 852}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.check_csrf_token", "completion": "        expected_token = self.get_csrf_token(request)\n        return not strings_differ(\n            bytes_(expected_token), bytes_(supplied_token)\n        )", "idx": 853}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.new_csrf_token", "completion": "        token = self._token_factory()\n        self.cookie_profile.set_cookie(request, token)\n        request.add_response_callback(self.cookie_profile.set_cookie)\n        return token\n", "idx": 854}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.get_csrf_token", "completion": "        token = request.cookies.get(self.cookie_name, None)\n        if not token:\n            token = self.new_csrf_token(request)\n        return token\n", "idx": 855}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.check_csrf_token", "completion": "        expected_token = self.get_csrf_token(request)\n        return not strings_differ(\n            bytes_(expected_token), bytes_(supplied_token)\n        )", "idx": 856}
{"namespace": "pyramid.security.PermitsResult.__repr__", "completion": "        return '<%s instance at %s with msg %s>' % (\n            self.__class__.__name__,\n            id(self),\n            self.msg,\n        )", "idx": 857}
{"namespace": "pyramid.util.InstancePropertyHelper.make_property", "completion": "        if name is None:\n            name = callable.__name__\n        if reify:\n            callable = reify(callable)\n        return name, property(callable)\n", "idx": 858}
{"namespace": "pyramid.util.InstancePropertyHelper.set_property", "completion": "        name, fn = cls.make_property(callable, name, reify)\n        cls.apply_properties(target, {name: fn})\n", "idx": 859}
{"namespace": "pyramid.util.InstancePropertyHelper.add_property", "completion": "        prop = self.make_property(callable, name=name, reify=reify)\n        self.properties[prop[0]] = prop[1]\n", "idx": 860}
{"namespace": "pyramid.util.InstancePropertyHelper.apply", "completion": "        self.apply_properties(target, self.properties.items())", "idx": 861}
{"namespace": "pyramid.util.InstancePropertyMixin.set_property", "completion": "        InstancePropertyHelper.set_property(self, callable, name=name, reify=reify)\n", "idx": 862}
{"namespace": "pyramid.util.TopologicalSorter.remove", "completion": "        if name not in self.names:\n            return\n        self.names.remove(name)\n        self.req_before.discard(name)\n        self.req_after.discard(name)\n        self.name2before.pop(name, None)\n        self.name2after.pop(name, None)\n        self.name2val.pop(name, None)\n        self.order.remove(name)\n", "idx": 863}
{"namespace": "pyramid.util.TopologicalSorter.add", "completion": "        if name in self.names:\n            raise ValueError('duplicate name %s' % name)\n        self.names.append(name)\n        self.name2val[name] = val\n        if after is not None:\n            if after is FIRST:\n                after = self.first\n            else:\n                after = as_sorted_tuple(after)\n            self.name2after[name] = after\n            self.req_after.add(name)\n            for u in after:\n                self.order.append((u, name))\n        if before is not None:\n            if before is LAST:\n                before = self.last\n            else:\n                before = as_sorted_tuple(before)\n            self.name2before[name] = before\n            self.req_before.add(name)\n            for u in before:\n                self.order.append((name, u))\n", "idx": 864}
{"namespace": "pyramid.traversal.find_resource", "completion": "    if isinstance(path, str):\n        if path.startswith(\"/\"):\n            path = path.split(\"/\")[1:]\n        else:\n            path = path.split(\"/\")\n    else:\n        if path[0] == \"\":\n            path = path[1:]\n    for segment in path:\n        resource = resource[segment]\n    return resource\n\n", "idx": 865}
{"namespace": "pyramid.static.ManifestCacheBuster.manifest", "completion": "        if self.reload:\n            if self.exists(self.manifest_path):\n                mtime = self.getmtime(self.manifest_path)\n                if mtime != self._mtime:\n                    self._manifest = self.get_manifest()\n                    self._mtime = mtime\n            else:\n                self._manifest = {}\n        return self._manifest\n", "idx": 866}
{"namespace": "pyramid.registry.Registry.registerSubscriptionAdapter", "completion": "        self.has_listeners = True\n        return super(Registry, self).registerSubscriptionAdapter(*arg, **kw)", "idx": 867}
{"namespace": "pyramid.registry.Registry.registerHandler", "completion": "        result = Components.registerHandler(self, *arg, **kw)\n        self.has_listeners = True\n        return result", "idx": 868}
{"namespace": "pyramid.registry.Registry.notify", "completion": "        if not self.has_listeners:\n            return\n        for event in events:\n            for subscriber in self.subscribers([event], None):\n                subscriber(event)", "idx": 869}
{"namespace": "pyramid.registry.Introspector.add", "completion": "        # Increment the counter\n        self._counter += 1\n\n        # Get the category name and discriminator from the introspectable object\n        category_name = intr.category_name\n        discriminator = intr.discriminator\n\n        # Create a new category if it doesn't exist\n        if category_name not in self._categories:\n            self._categories[category_name] = []\n\n        # Add the introspectable object to the category\n        self._categories[category_name].append(intr)\n\n        # Create a new reference if it doesn't exist\n        if discriminator not in self._refs:\n            self._refs[discriminator] = {}\n\n        # Add the introspectable object to the reference\n        self._refs[discriminator][category_name] = intr\n\n        # Set the order of the introspectable object\n        intr.order = self._counter\n", "idx": 870}
{"namespace": "pyramid.registry.Introspector.get", "completion": "        category = self._categories.get(category_name)\n        if category is None:\n            return default\n        return category.get(discriminator, default)\n", "idx": 871}
{"namespace": "pyramid.registry.Introspector.get_category", "completion": "        category = self._categories.setdefault(category_name, {})\n        if sort_key is None:\n            sort_key = operator.attrgetter('order')\n        return [\n            {'introspectable': intr, 'related': intr.related}\n            for intr in sorted(category.values(), key=sort_key)\n        ]\n", "idx": 872}
{"namespace": "pyramid.registry.Introspector.categorized", "completion": "        if sort_key is None:\n            sort_key = operator.attrgetter('order')\n        result = []\n        for category_name, category in self._categories.items():\n            values = category.values()\n            values = sorted(set(values), key=sort_key)\n            result.append(\n                (\n                    category_name,\n                    [\n                        {'introspectable': intr, 'related': self.related(intr)}\n                        for intr in values\n                    ],\n                )\n            )\n        return result", "idx": 873}
{"namespace": "pyramid.registry.Introspector.remove", "completion": "        category = self._categories.setdefault(category_name, {})\n        intr = category.get(discriminator)\n        if intr is not None:\n            for ref in self._refs.get(intr, ()):\n                ref.remove(intr)\n            del category[intr.discriminator]\n            del category[intr.discriminator_hash]\n            del self._refs[intr]\n            del intr", "idx": 874}
{"namespace": "pyramid.registry.Introspector.relate", "completion": "        introspectables = self._get_intrs_by_pairs(pairs)\n        for intr in introspectables:\n            self._refs.setdefault(intr, [])\n        for i, intr in enumerate(introspectables):\n            for intr2 in introspectables[i + 1 :]:\n                self._refs[intr].append(intr2)\n                self._refs[intr2].append(intr)\n", "idx": 875}
{"namespace": "pyramid.registry.Introspector.related", "completion": "        category_name = intr.category_name\n        discriminator = intr.discriminator\n        intr = self._categories.get(category_name, {}).get(discriminator)\n        if intr is None:\n            raise KeyError((category_name, discriminator))\n        return self._refs.get(intr, [])", "idx": 876}
{"namespace": "pyramid.registry.Introspectable.discriminator_hash", "completion": "        self._assert_resolved()\n        return hash(self.discriminator)\n", "idx": 877}
{"namespace": "pyramid.registry.Introspectable.__repr__", "completion": "        self._assert_resolved()\n        return '<%s category %r, discriminator %r>' % (\n            self.type_name,\n            self.category_name,\n            self.discriminator,\n        )", "idx": 878}
{"namespace": "pyramid.scripts.proutes.PRoutesCommand._get_mapper", "completion": "        return registry.getUtility(IRoutesMapper)\n", "idx": 879}
{"namespace": "pyramid.scripts.pshell.PShellCommand.make_shell", "completion": "        shells = self.find_all_shells()\n        if self.args.python_shell:\n            shell_name = self.args.python_shell.lower()\n            if shell_name in shells:\n                return shells[shell_name]\n            else:\n                raise ValueError('could not find a shell named \"%s\"' % shell_name)\n        for shell_name in self.preferred_shells:\n            if shell_name in shells:\n                return shells[shell_name]\n        if shells:\n            return list(shells.values())[0]\n        return self.default_runner", "idx": 880}
{"namespace": "pyramid.config.assets.PackageOverrides.insert", "completion": "        if path.endswith(os.sep):\n            override = DirectoryOverride(path, source)\n        else:\n            override = FileOverride(path, source)\n        self.overrides.insert(0, override)\n        return override\n", "idx": 881}
{"namespace": "pyramid.config.assets.PackageOverrides.filtered_sources", "completion": "        for override in self.overrides:\n            if override.matches(resource_name):\n                yield override.filtered_source(resource_name)\n", "idx": 882}
{"namespace": "pyramid.config.assets.PackageOverrides.real_loader", "completion": "        if self._real_loader is None:\n            raise NotImplementedError(\n                \"This PackageOverrides instance does not have a real loader\"\n            )\n        return self._real_loader\n", "idx": 883}
{"namespace": "pyramid.config.views.MultiView.add", "completion": "        if phash is None:\n            phash = DEFAULT_PHASH\n        if accept is None:\n            if view in self.views:\n                self.views.remove(view)\n            self.views.append(view)\n            self.views.sort(key=lambda v: v.order)\n        else:\n            if accept not in self.media_views:\n                self.media_views[accept] = []\n            if view in self.media_views[accept]:\n                self.media_views[accept].remove(view)\n            self.media_views[accept].append(view)\n            self.media_views[accept].sort(key=lambda v: v.order)\n            if accept not in self.accepts:\n                self.accepts.append(accept)\n                self.accepts.sort(key=accept_order)\n", "idx": 884}
{"namespace": "pyramid.config.views.MultiView.get_views", "completion": "        if request.accept and self.accepts:\n            views = []\n            for accept in request.accept:\n                views.extend(self.media_views.get(accept, []))\n            views.extend(self.views)\n            return views\n        return self.views\n", "idx": 885}
{"namespace": "pyramid.config.views.MultiView.match", "completion": "        views = self.get_views(request)\n        for _, view, _ in views:\n            if not hasattr(view, '__predicated__'):\n                return view\n            if view.__predicated__(context, request):\n                return view\n        raise PredicateMismatch()\n", "idx": 886}
{"namespace": "pyramid.config.views.MultiView.__permitted__", "completion": "        view = self.match(context, request)\n        if not hasattr(view, '__permitted__'):\n            return True\n        return view.__permitted__(context, request)", "idx": 887}
{"namespace": "pyramid.config.views.MultiView.__call_permissive__", "completion": "        view = self.match(context, request)\n        if hasattr(view, '__call_permissive__'):\n            return view.__call_permissive__(context, request)\n        return view(context, request)\n", "idx": 888}
{"namespace": "pyramid.config.actions.ActionState.processSpec", "completion": "        if spec in self._seen_files:\n            return False\n        self._seen_files.add(spec)\n        return True\n", "idx": 889}
{"namespace": "pyramid.config.actions.ActionState.action", "completion": "        if kw is None:\n            kw = {}\n\n        action = extra\n        action.update(\n            dict(\n                discriminator=discriminator,\n                callable=callable,\n                args=args,\n                kw=kw,\n                order=order,\n                info=info,\n                includepath=includepath,\n                introspectables=introspectables,\n            )\n        )\n        self.actions.append(action)\n", "idx": 890}
{"namespace": "pyramid.config.actions.ActionInfo.__str__", "completion": "        if self.src is None:\n            return ''\n        if is_nonstr_iter(self.src):\n            src = '\\n'.join(self.src)\n        else:\n            src = self.src\n        return 'Line %s of file %s:\\n%s' % (\n            self.line,\n            self.file,\n            textwrap.indent(src, '  '),\n        )", "idx": 891}
{"namespace": "pyramid.config.Configurator.__getattr__", "completion": "        if name in self.registry._directives:\n            c, action_wrap = self.registry._directives[name]\n            if action_wrap:\n                c = self.action(c)\n            return c.__get__(self)\n        raise AttributeError(name)", "idx": 892}
{"namespace": "pyramid.config.Configurator.with_package", "completion": "        # Create a new instance of the Configurator class with the same registry\n        new_configurator = Configurator(registry=self.registry)\n\n        # Set the package for the new instance\n        new_configurator.package = package\n\n        # Copy other attributes from the current instance\n        new_configurator.basepath = self.basepath\n        new_configurator.includepath = self.includepath\n        new_configurator.info = self.info\n        new_configurator.object_description = self.object_description\n        new_configurator.introspectable = self.introspectable\n        new_configurator.inspect = self.inspect\n        new_configurator.name_resolver = self.name_resolver\n        new_configurator.package_name = self.package_name\n        new_configurator.package = self.package\n        new_configurator.root_package = self.root_package\n        new_configurator.registry = self.registry\n        new_configurator.autocommit = self.autocommit\n        new_configurator.route_prefix = self.route_prefix\n        new_configurator.introspection = self.introspection\n\n        return new_configurator\n", "idx": 893}
{"namespace": "pyramid.config.Configurator.absolute_asset_spec", "completion": "        if not isinstance(relative_spec, str):\n            return relative_spec\n\n        if relative_spec.startswith('.'):\n            relative_spec = relative_spec[1:]\n\n        package, filename = resolve_asset_spec(relative_spec, self.package_name)\n        if package is None:\n            return filename  # absolute filename\n        return '%s:%s' % (package, filename)\n", "idx": 894}
{"namespace": "pyramid.config.Configurator.begin", "completion": "        if request is _marker:\n            request = self.registry.get('request', _marker)\n        self.manager.push({'registry': self.registry, 'request': request})\n", "idx": 895}
{"namespace": "pyramid.config.Configurator.scan", "completion": "        if package is None:\n            package = caller_package()\n        if isinstance(package, str):\n            package = self.maybe_dotted(package)\n        if not isinstance(package, (type(os), type(self))):\n            package = package.__name__\n        if isinstance(package, type(os)):\n            package = package.__name__\n        if isinstance(package, type(self)):\n            package = package.__module__\n        if isinstance(package, str):\n            package = self.name_resolver.maybe_resolve(package)\n        if not isinstance(package, (type(os), type(self))):\n            package = package.__name__\n        if isinstance(package, type(os)):\n            package = package.__name__\n        if isinstance(package, type(self)):\n            package = package.__module__\n        if isinstance(package, str):\n            package = self.name_resolver.maybe_resolve(package)\n        if not isinstance(package, (type(os), type(self))):\n            package = package.__name__\n        if isinstance(package, type(os)):\n            package = package.__name__\n        if isinstance(package, type(self)):\n            package = package.__module__\n        if isinstance(package, str):\n            package = self.name_resolver.maybe_resolve(package)\n        if not isinstance(package, (type(os), type(self))):\n            package = package.__name__\n        if isinstance(package, type(os)):\n            package = package.__name__\n        if isinstance(package, type(self)):\n            package = package.__module__\n        if isinstance(package, str):\n            package = self.name_resolver.maybe_resolve(package)\n        if not isinstance(package, (type(os), type(self))):\n            package = package.__name__\n        if isinstance(package, type(os)):\n            package = package.__name__\n        if isinstance(package, type(self)):\n            package = package.__module__\n        if isinstance(package", "idx": 896}
{"namespace": "pyramid.config.Configurator.make_wsgi_app", "completion": "        self.commit()\n        self.registry.registerUtility(self.registry, IRegistry)\n        self.registry.registerUtility(self.registry, IDebugLogger)\n        self.registry.registerUtility(self.registry, IIntrospector)\n        self.registry.registerUtility(self.registry, ISettings)\n        self.registry.registerUtility(self.registry, IAuthorizationPolicy)\n        self.registry.registerUtility(self.registry, IAuthenticationPolicy)\n        self.registry.registerUtility(self.registry, ISecurityPolicy)\n        self.registry.registerUtility(self.registry, IViewMapperFactory)\n        self.registry.registerUtility(self.registry, IViewMapper)\n        self.registry.registerUtility(self.registry, IView)\n        self.registry.registerUtility(self.registry, IViewClassifier)\n        self.registry.registerUtility(self.registry, IViewRenderer)\n        self.registry.registerUtility(self.registry, IViewRendererFactory)\n        self.registry.registerUtility(self.registry, IViewRendererInfo)\n        self.registry.registerUtility(self.registry, IViewMapper)\n        self.registry.registerUtility(self.registry, IViewMapperFactory)\n        self.registry.registerUtility(self.registry, IViewMapper)\n        self.registry.registerUtility(self.registry, IViewMapperFactory)\n        self.registry.registerUtility(self.registry, IViewMapper)\n        self.registry.registerUtility(self.registry, IViewMapperFactory)\n        self.registry.registerUtility(self.registry, IViewMapper)\n        self.registry.registerUtility(self.registry, IViewMapperFactory)\n        self.registry.registerUtility(self.registry, IViewMapper)", "idx": 897}
{"namespace": "aiohappybase._util.camel_case_to_pep8", "completion": "    return CAPITALS.sub(lambda x: '_' + x.group(0).lower(), name)\n\n", "idx": 898}
{"namespace": "kinto.authorization._relative_object_uri", "completion": "    obj_parts = object_uri.split(\"/\")\n    parent_uri = None\n    for i in range(0, len(obj_parts), 2):\n        parent_uri = \"/\".join(obj_parts[: i + 2])\n        parent_resource_name, plural_endpoint = _resource_endpoint(parent_uri)\n        if plural_endpoint:\n            # /buckets/bid/collections -> /buckets/bid\n            parent_uri = parent_uri[:-1]\n        if resource_name == parent_resource_name:\n            return parent_uri\n\n    raise ValueError(f\"{resource_name} not found in {object_uri}\")\n\n", "idx": 899}
{"namespace": "kinto.core.openapi.OpenAPI.expose_authentication_method", "completion": "        cls.security_definitions[method_name] = definition\n        cls.security_roles[method_name] = definition[\"scopes\"]", "idx": 900}
{"namespace": "kinto.core.openapi.OpenAPI.generate", "completion": "        base_spec = {\n            \"host\": self.request.host,\n            \"schemes\": [self.request.scheme],\n            \"securityDefinitions\": self.security_definitions,\n        }\n        return super().generate(swagger=base_spec)", "idx": 901}
{"namespace": "kinto.core.testing.get_user_headers", "completion": "    credentials = \"%s:%s\" % (user, password)\n    token = base64.b64encode(credentials.encode(\"ascii\")).decode(\"ascii\")\n    return {\"Authorization\": \"Basic %s\" % token}", "idx": 902}
{"namespace": "kinto.core.authorization.RouteFactory.fetch_shared_objects", "completion": "        if self.shared_ids is None:\n            self.shared_ids = self._get_accessible_objects(\n                principals,\n                self._object_id_match,\n                perm,\n                get_bound_permissions,\n            )\n        return self.shared_ids", "idx": 903}
{"namespace": "kinto.core.authorization.RouteFactory.get_permission_object_id", "completion": "        if self.on_plural_endpoint:\n            if object_id is None:\n                # If the request is on a plural endpoint and object_id is not provided,\n                # use the current object id.\n                object_id = self.current_object[\"id\"]\n            # If the request is on a plural endpoint and object_id is provided,\n            # find the object URI by inspecting the \"plural\" service and its sibling \"object\" service.\n            plural_service = request.registry.services[\"plural\"]\n            object_service = request.registry.services[\"object\"]\n            object_uri = plural_service.get_object_uri(object_id, object_service)\n            return object_uri\n        else:\n            # If the request is not on a plural endpoint, use the current object id.\n            return self.current_object[\"id\"]\n", "idx": 904}
{"namespace": "kinto.core.utils.recursive_update_dict", "completion": "    for k, v in changes.items():\n        if isinstance(v, dict):\n            if k not in root:\n                root[k] = {}\n            recursive_update_dict(root[k], v, ignores)\n        elif v in ignores:\n            root.pop(k, None)\n        else:\n            root[k] = v\n\n", "idx": 905}
{"namespace": "kinto.core.utils.native_value", "completion": "    if isinstance(value, str):\n        try:\n            return json.loads(value)\n        except ValueError:\n            return value\n    return value", "idx": 906}
{"namespace": "kinto.core.utils.dict_subset", "completion": "    return {k: v for k, v in d.items() if k in keys}\n\n", "idx": 907}
{"namespace": "kinto.core.utils.dict_merge", "completion": "    if not isinstance(a, dict) or not isinstance(b, dict):\n        raise TypeError(\"Both inputs must be dictionaries\")\n\n    merged = a.copy()\n\n    for key, value in b.items():\n        if key in a and isinstance(a[key], dict) and isinstance(value, dict):\n            merged[key] = dict_merge(a[key], value)\n        else:\n            merged[key] = value\n\n    return merged", "idx": 908}
{"namespace": "kinto.core.utils.find_nested_value", "completion": "    if path in d:\n        return d[path]\n\n    path_parts = path.split(\".\")\n    root_keys = [\".\".join(path_parts[:i]) for i in range(len(path_parts), 0, -1)]\n\n    for root_key in root_keys:\n        if root_key in d and isinstance(d[root_key], dict):\n            subpath = path[len(root_key) + 1 :]\n            return find_nested_value(d[root_key], subpath, default)\n\n    return default\n\n", "idx": 909}
{"namespace": "kinto.core.utils.instance_uri_registry", "completion": "    fakerequest = Request.blank(path=\"/\")\n    fakerequest.registry = registry\n    return instance_uri(fakerequest, resource_name, **params)", "idx": 910}
{"namespace": "kinto.core.statsd.load_from_config", "completion": "    if statsd_module is None:\n        raise ConfigurationError(\"Please install Kinto with monitoring dependencies (e.g. statsd package)\")\n\n    settings = config.get_settings()\n    statsd_url = settings[\"statsd_url\"]\n    url = urlparse(statsd_url)\n\n    return Client(url.hostname, url.port, url.path[1:])", "idx": 911}
{"namespace": "kinto.core.errors.http_error", "completion": "    if errno is None:\n        errno = ERRORS.UNDEFINED.value\n\n    if code is None:\n        code = httpexception.code\n\n    if error is None:\n        error = httpexception.title\n\n    error_body = {\n        \"errno\": errno,\n        \"code\": code,\n        \"error\": error,\n        \"message\": message,\n        \"info\": info,\n        \"details\": details,\n    }\n\n    response = httpexception(body=error_body)\n    response.content_type = \"application/json\"\n    return response\n\n", "idx": 912}
{"namespace": "kinto.core.resource.schema.ResourceReponses.get_and_bind", "completion": "        # Get the default schemas for the endpoint type and method\n        default_schemas = getattr(self, f\"default_{endpoint_type}_schemas\")\n        default_method_schemas = getattr(self, f\"default_{method}_schemas\")\n\n        # Get the endpoint-specific schemas for the endpoint type\n        endpoint_schemas = getattr(self, f\"{endpoint_type}_schemas\", {})\n\n        # Get the method-specific schemas for the method\n        method_schemas = getattr(self, f\"{endpoint_type}_{method}_schemas\", {})\n\n        # Merge the default and endpoint-specific schemas\n        schemas = {**default_schemas, **endpoint_schemas}\n\n        # Merge the method-specific schemas into the merged schemas\n        schemas.update(method_schemas)\n\n        # Merge the default method schemas into the merged schemas\n        schemas.update(default_method_schemas)\n\n        # Create a dictionary of status codes mapping cloned and binded responses\n        responses = {}\n        for status, schema in schemas.items():\n            # Clone the schema\n            cloned_schema = schema.clone()\n\n            # Bind the schema with the provided keyword arguments\n            cloned_schema.bind(**kwargs)\n\n            # Add the cloned and binded schema to the responses dictionary\n            responses[status] = cloned_schema\n\n        return responses", "idx": 913}
{"namespace": "kinto.core.resource.Resource.timestamp", "completion": "        try:\n            return self.model.timestamp\n        except storage_exceptions.ReadOnlyError as e:\n            logger.error(e)\n            raise_invalid(self.request, ERRORS.READONLY)\n", "idx": 914}
{"namespace": "kinto.core.resource.Resource.plural_post", "completion": "        self._raise_412_if_modified(obj={})\n        self._raise_400_if_invalid_id(self.object_id)\n\n        if self.object_id:\n            existing = self.model.get_object(self.object_id)\n            if existing is not None:\n                return self.postprocess(existing)\n\n        new = self.model.create_object(self.request.validated[\"data\"])\n        self.request.response.status_code = 201\n        return self.postprocess(new)\n", "idx": 915}
{"namespace": "kinto.core.resource.Resource.get", "completion": "        self._raise_404_if_not_found()\n        self._raise_304_if_not_modified()\n        self._raise_412_if_modified()\n\n        partial_fields = self._extract_partial_fields()\n\n        obj = self.model.get_object(self.object_id)\n\n        if partial_fields:\n            obj = dict_subset(obj, partial_fields)\n\n        self._add_timestamp_header(self.request.response)\n        self._add_cache_header(self.request.response)\n\n        return self.postprocess(obj)", "idx": 916}
{"namespace": "kinto.core.resource.Resource.delete", "completion": "        self._raise_400_if_invalid_id(self.object_id)\n        obj = self._get_object_or_404(self.object_id)\n        self._raise_412_if_modified(obj)\n\n        self.model.delete_object(self.object_id)\n\n        timestamp = obj[self.model.modified_field]\n        self._add_timestamp_header(self.request.response, timestamp=timestamp)\n\n        return self.postprocess(obj, action=ACTIONS.DELETE)", "idx": 917}
{"namespace": "kinto.core.permission.memory.Permission.add_principal_to_ace", "completion": "        object_key = f\"object:{object_id}\"\n        ace_key = f\"{object_key}:{permission}\"\n        ace_principals = self._store.get(ace_key, set())\n        ace_principals.add(principal)\n        self._store[ace_key] = ace_principals\n", "idx": 918}
{"namespace": "kinto.core.permission.memory.Permission.get_object_permission_principals", "completion": "        permission_key = f\"permission:{object_id}:{permission}\"\n        return self._store.get(permission_key, set())\n", "idx": 919}
{"namespace": "kinto.core.storage.generators.Generator.match", "completion": "        if self._regexp is None:\n            self._regexp = re.compile(self.regexp)\n        return self._regexp.match(object_id) is not None\n", "idx": 920}
{"namespace": "kinto.core.storage.postgresql.migrator.MigratorMixin.create_or_migrate_schema", "completion": "        current_version = self.get_installed_version()\n        if current_version is None:\n            logger.info(\n                \"No %s schema version is currently installed.\", self.name\n            )\n            if not dry_run:\n                self.create_schema()\n        elif current_version == self.schema_version:\n            logger.info(\n                \"%s schema is already up-to-date (version %s)\",\n                self.name,\n                self.schema_version,\n            )\n        else:\n            logger.info(\n                \"Migrating %s schema from version %s to %s\",\n                self.name,\n                current_version,\n                self.schema_version,\n            )\n            if not dry_run:\n                self.migrate_schema(current_version)\n", "idx": 921}
{"namespace": "kinto.core.views.batch.BatchPayloadSchema.deserialize", "completion": "        # Deserialize the data\n        deserialized = super().deserialize(cstruct)\n\n        # Merge the defaults with the requests\n        defaults = deserialized.get(\"defaults\", {})\n        requests = deserialized.get(\"requests\", [])\n        for request in requests:\n            request.update(defaults)\n\n        # Return the deserialized data\n        return deserialized", "idx": 922}
{"namespace": "kinto.plugins.accounts.utils.get_cached_reset_password", "completion": "    settings = registry.settings\n    hmac_secret = settings[\"userid_hmac_secret\"]\n    cache_key = utils.hmac_digest(hmac_secret, ACCOUNT_RESET_PASSWORD_CACHE_KEY.format(username))\n\n    cache = registry.cache\n    reset_password = cache.get(cache_key)\n    return reset_password\n\n", "idx": 923}
{"namespace": "kinto.plugins.accounts.utils.get_cached_validation_key", "completion": "    hmac_secret = registry.settings[\"userid_hmac_secret\"]\n    cache_key = utils.hmac_digest(hmac_secret, ACCOUNT_VALIDATION_CACHE_KEY.format(username))\n\n    cache = registry.cache\n    cache_result = cache.get(cache_key)\n    return cache_result\n\n", "idx": 924}
{"namespace": "kinto.plugins.accounts.views.validation.on_account_activated", "completion": "    if not event.request.registry.settings.get(\"account_validation.enable\"):\n        return\n\n    for impacted in event.impacted_objects:\n        if (\n            impacted.get(\"old\", {}).get(\"validated\")\n            or not impacted.get(\"new\", {}).get(\"validated\")\n        ):\n            continue\n\n        Emailer(event.request, impacted[\"new\"]).send_confirmation_email()", "idx": 925}
{"namespace": "kinto.plugins.openid.OpenIDConnectPolicy._verify_token", "completion": "        # Fetch userinfo\n        userinfo_endpoint = self.oid_config[\"userinfo_endpoint\"]\n        headers = {\"Authorization\": f\"Bearer {access_token}\"}\n        try:\n            resp = requests.get(userinfo_endpoint, headers=headers)\n            resp.raise_for_status()\n        except requests.exceptions.HTTPError as e:\n            self.logger.debug(f\"Error while fetching userinfo: {e}\")\n            return None\n\n        # Parse userinfo\n        try:\n            payload = resp.json()\n        except ValueError as e:\n            self.logger.debug(f\"Error while parsing userinfo: {e}\")\n            return None\n\n        return payload", "idx": 926}
{"namespace": "kinto.plugins.quotas.scripts.rebuild_quotas", "completion": "    for bucket in storage.list_all(collection_id=\"bucket\"):\n        bucket_id = bucket[\"id\"]\n        bucket_path = f\"/buckets/{bucket_id}\"\n        bucket_info = storage.get(\n            collection_id=\"bucket\", object_id=bucket_id, parent_id=\"\"\n        )\n        bucket_info[\"data\"][\"record_count\"] = 0\n        bucket_info[\"data\"][\"storage_size\"] = 0\n        bucket_info[\"data\"][\"collection_count\"] = 0\n\n        for collection in storage.list_all(\n            collection_id=\"collection\", parent_id=bucket_path\n        ):\n            collection_id = collection[\"id\"]\n            collection_path = f\"{bucket_path}/collections/{collection_id}\"\n            collection_info = storage.get(\n                collection_id=\"collection\", object_id=collection_id, parent_id=bucket_path\n            )\n            bucket_info[\"data\"][\"collection_count\"] += 1\n\n            for record in paginated(\n                storage, collection_id=\"record\", parent_id=collection_path, sorting=[OLDEST_FIRST]\n            ):\n                bucket_info[\"data\"][\"record_count\"] += 1\n                bucket_info[\"data\"][\"storage_size\"] += record_size(record)\n\n        if not dry_run:\n            storage.update(\n                collection_id=\"bucket\",\n                object_id=bucket_id,\n                parent_id=\"\",\n                record=bucket_info[\"data\"],\n                with_deleted=False,\n            )\n\n        logger.info(\n            f\"Bucket {bucket_id} has {bucket_info['data']['record_count']} records, \"\n            f\"{bucket_info['data']['storage_size']} bytes, \"\n            f\"{bucket_info['data']['collection_count']} collections.\"\n        )\n\n", "idx": 927}
{"namespace": "kinto.config.render_template", "completion": "    template = os.path.join(HERE, template)\n    with codecs.open(template, encoding=\"utf-8\") as f:\n        raw_template = f.read()\n\n    rendered = raw_template.format(**kwargs)\n\n    with codecs.open(destination, \"w+\", encoding=\"utf-8\") as f:\n        f.write(rendered)\n\n", "idx": 928}
{"namespace": "trafilatura.sitemaps.SitemapObject.extract_sitemap_langlinks", "completion": "        # extract links with hreflang\n        attributes = HREFLANG_REGEX.findall(self.content)\n        for attribute in attributes:\n            if attribute == self.target_lang:\n                for link in DETECT_LINKS.findall(attribute):\n                    self.handle_link(link)\n        LOGGER.debug('%s sitemaps and %s links with hreflang found for %s', len(self.sitemap_urls), len(self.urls), self.target_lang)", "idx": 929}
{"namespace": "trafilatura.sitemaps.SitemapObject.extract_sitemap_links", "completion": "        # extract\n        for match in islice(LINK_REGEX.finditer(self.content), MAX_LINKS):\n            self.handle_link(match[1])\n        LOGGER.debug('%s sitemaps and %s links found for %s', len(self.sitemap_urls), len(self.urls), self.sitemap_url)", "idx": 930}
{"namespace": "trafilatura.sitemaps.SitemapObject.process", "completion": "        # check if sitemap is plausible\n        if not SITEMAP_FORMAT.search(self.content):\n            return\n        # try to extract links from a TXT file\n        if self.content.startswith('http'):\n            for link in DETECT_LINKS.findall(self.content):\n                self.handle_link(link)\n            return\n        # iterate through content to find links\n        for link in DETECT_LINKS.findall(self.content):\n            self.handle_link(link)\n        # if content matches the format of an XML sitemap and a target language is specified, extract language links from the sitemap\n        if self.target_lang and SCRUB_REGEX.sub('', self.sitemap_url).endswith('.xml'):\n            self.extract_sitemap_langlinks()\n        # if there are sitemap URLs or URLs extracted from the sitemap, return\n        if self.sitemap_urls or self.urls:\n            return\n        # extract links from the sitemap\n        self.extract_sitemap_links()\n\n", "idx": 931}
{"namespace": "trafilatura.sitemaps.is_plausible_sitemap", "completion": "    # check if the URL ends with .xml\n    if url.endswith('.xml'):\n        return True\n    # check if the URL ends with .txt\n    if url.endswith('.txt'):\n        return True\n    # check if the URL ends with .gz\n    if url.endswith('.gz'):\n        return True\n    # check if the URL ends with .xml.gz\n    if url.endswith('.xml.gz'):\n        return True\n    # check if the URL ends with .txt.gz\n    if url.endswith('.txt.gz'):\n        return True\n    # check if the URL ends with .xml.gz\n    if url.endswith('.xml.gz'):\n        return True\n    # check if the URL ends with .txt.gz\n    if url.endswith('.txt.gz'):\n        return True\n    # check if the URL ends with .xml.gz\n    if url.endswith('.xml.gz'):\n        return True\n    # check if the URL ends with .txt.gz\n    if url.endswith('.txt.gz'):\n        return True\n    # check if the URL ends with .xml.gz\n    if url.endswith('.xml.gz'):\n        return True\n    # check if the URL ends with .txt.gz\n    if url.endswith('.txt.gz'):\n        return True\n    # check if the URL ends with .xml.gz\n    if url.endswith('.xml.gz'):\n        return True\n    # check if the URL ends with .txt.gz\n    if url.endswith('.txt.gz'):\n        return True\n    # check if the URL ends with .xml.gz\n    if url.endswith('.xml.gz'):\n        return True\n    # check if the URL ends with .txt.gz\n    if url.endswith('.txt.gz'):\n        return True\n    # check if the URL ends with .xml.gz\n    if url.endswith('.xml.gz'):\n        return True\n    # check if the URL ends with .txt.gz\n    if", "idx": 932}
{"namespace": "trafilatura.sitemaps.extract_robots_sitemaps", "completion": "    sitemaps = []\n    for line in robotstxt.splitlines():\n        line = line.strip()\n        if line.startswith('#') or not line:\n            continue\n        if line.startswith('Sitemap:'):\n            sitemap_url = line.split(':', 1)[1].strip()\n            sitemap_url = clean_url(sitemap_url, baseurl)\n            if sitemap_url:\n                sitemaps.append(sitemap_url)\n    return sitemaps", "idx": 933}
{"namespace": "trafilatura.feeds.handle_link_list", "completion": "    # check links\n    linklist = [check_url(url, baseurl) for url in linklist]\n    # filter links\n    linklist = [url for url in linklist if url and validate_url(url, domainname, target_lang)]\n    # fix relative URLs\n    linklist = [fix_relative_urls(url, baseurl) for url in linklist]\n    # remove duplicates\n    linklist = list(set(linklist))\n    # remove links pointing to feeds\n    linklist = [url for url in linklist if not BLACKLIST.search(url)]\n    # return list\n    return linklist\n\n", "idx": 934}
{"namespace": "trafilatura.feeds.find_feed_urls", "completion": "    # get domain name and base URL\n    domainname, baseurl = get_hostinfo(url)\n    # fetch webpage\n    htmlstring, _ = load_html(url)\n    # check if it's a feed\n    if htmlstring is None:\n        LOGGER.debug('Empty feed: %s', domainname)\n        return []\n    # check if it's a feed\n    if FEED_OPENING.match(htmlstring):\n        feed_links = extract_links(htmlstring, domainname, baseurl, url, target_lang)\n    # otherwise, try to find feeds\n    else:\n        feed_urls = determine_feed(htmlstring, baseurl, url)\n        feed_links = []\n        for feed_url in feed_urls:\n            feed_string, _ = load_html(feed_url)\n            feed_links.extend(extract_links(feed_string, domainname, baseurl, url, target_lang))\n    # refine\n    output_links = handle_link_list(feed_links, domainname, baseurl, target_lang)\n    output_links = [l for l in output_links if l != url and l.count('/') > 2]\n    # log result\n    if feed_links:\n        LOGGER.debug('Links found: %s of which %s valid', len(feed_links), len(output_links))\n    else:\n        LOGGER.debug('Invalid feed for %s', domainname)\n    return sorted(set(output_links))", "idx": 935}
{"namespace": "trafilatura.hashing.generate_hash_filename", "completion": "    content = re.sub(CLEAN_XML, \"\", content)\n    return urlsafe_b64encode(generate_bow_hash(content, length=12)).decode()\n\n", "idx": 936}
{"namespace": "trafilatura.cli_utils.download_queue_processing", "completion": "    # Initialize the list of errors\n    errors = []\n\n    # Load the download buffer\n    download_buffer = load_download_buffer(url_store, args)\n\n    # Process the downloads\n    for url, htmlstring in download_buffer.items():\n        # Process the result\n        counter = process_result(htmlstring, args, url, counter, config)\n\n    # Return the list of errors and the updated counter\n    return errors, counter\n\n", "idx": 937}
{"namespace": "trafilatura.utils.decode_response", "completion": "    if isinstance(response, HTTPResponse):\n        # read the first chunk\n        filecontent = response.read(15000)\n        # check if the response is compressed\n        filecontent = handle_compressed_file(filecontent)\n    else:\n        filecontent = response\n    # guess encoding\n    encodings = detect_encoding(filecontent)\n    # decode\n    if len(encodings) > 0:\n        # try first encoding\n        try:\n            return filecontent.decode(encodings[0])\n        except UnicodeDecodeError:\n            # try next encoding\n            if len(encodings) > 1:\n                return filecontent.decode(encodings[1])\n    # fallback\n    return filecontent.decode('utf-8', errors='ignore')\n\n", "idx": 938}
{"namespace": "trafilatura.utils.txttocsv", "completion": "    # Create a CSV writer object\n    csv_writer = csv.writer(sys.stdout, delimiter='\\t')\n\n    # Write the header row\n    csv_writer.writerow(['URL', 'fingerprint', 'hostname', 'title', 'image', 'date', 'license', 'pagetype', 'ID', 'text', 'comments'])\n\n    # Write the data rows\n    csv_writer.writerow([docmeta.get('url'), docmeta.get('fingerprint'), docmeta.get('hostname'), docmeta.get('title'), docmeta.get('image'), docmeta.get('date'), docmeta.get('license'), docmeta.get('pagetype'), docmeta.get('id'), text, comments])\n\n    # Return the result as a string\n    return csv_writer.writerow()", "idx": 939}
{"namespace": "trafilatura.utils.is_similar_domain", "completion": "    # remove file extension\n    reference = STRIP_EXTENSION.sub('', reference)\n    new_string = STRIP_EXTENSION.sub('', new_string)\n    # compare similarity\n    similarity = SequenceMatcher(None, reference, new_string).ratio()\n    return similarity >= threshold\n\n", "idx": 940}
{"namespace": "trafilatura.xml.remove_empty_elements", "completion": "    for elem in tree.iter('*'):\n        if not elem.getchildren() and not elem.text and not elem.tail:\n            elem.getparent().remove(elem)\n    return tree\n\n", "idx": 941}
{"namespace": "trafilatura.xml.strip_double_tags", "completion": "    for element in tree.iter('*'):\n        if element.tag in NESTING_WHITELIST:\n            for child in element:\n                if child.tag in NESTING_WHITELIST:\n                    element.remove(child)\n                    element.text = (element.text or '') + child.text\n                    element.tail = (element.tail or '') + child.tail\n    return tree\n\n", "idx": 942}
{"namespace": "trafilatura.xml.check_tei", "completion": "    # check if the XML document is conform\n    if not validate_tei(xmldoc):\n        # if not, scrub the XML document\n        xmldoc = scrub_tei(xmldoc, url)\n    return xmldoc\n\n", "idx": 943}
{"namespace": "trafilatura.xml.validate_tei", "completion": "    global TEI_RELAXNG\n    if TEI_RELAXNG is None:\n        with lzma.open(TEI_SCHEMA, 'rb') as file:\n            TEI_RELAXNG = load_pickle(file)\n    return TEI_RELAXNG(xmldoc)\n\n", "idx": 944}
{"namespace": "trafilatura.xml.merge_with_parent", "completion": "    parent = element.getparent()\n    if parent is not None:\n        parent.text = (parent.text or '') + replace_element_text(element, include_formatting)\n        parent.remove(element)\n\n", "idx": 945}
{"namespace": "trafilatura.downloads._determine_headers", "completion": "    myagents, mycookie = _parse_config(config)\n    if myagents is not None:\n        headers = headers or {}\n        headers['User-Agent'] = random.choice(myagents)\n    if mycookie is not None:\n        headers = headers or {}\n        headers['Cookie'] = mycookie\n    return headers or DEFAULT_HEADERS\n\n", "idx": 946}
{"namespace": "trafilatura.meta.reset_caches", "completion": "    # clear caches for justext\n    define_stoplist(stoplist=None, language='en')\n\n    # clear caches for htmldate\n    reset_caches_htmldate()\n\n    # clear caches for courlan\n    reset_caches_courlan()\n\n    # clear gc\n    gc.collect()", "idx": 947}
{"namespace": "trafilatura.core.handle_table", "completion": "    # create new table and row\n    newtable = Element('table')\n    newrow = Element('row')\n    # strip structural elements\n    strip_elements(table_elem, 'thead', 'tbody', 'tfoot')\n    # iterate through sub-elements\n    for child in table_elem.iter('*'):\n        # rows and cell elements within the table\n        if child.tag == 'tr':\n            # check for previously processed row\n            if len(newrow) > 0:\n                newtable.append(newrow)\n                newrow = Element('row')\n        elif child.tag in TABLE_ELEMS:\n            # define cell type\n            cell_element = define_cell_type(child)\n            # handle child elements without further descendants\n            if len(child) == 0:\n                processed_child = process_node(child, options)\n                if processed_child is not None:\n                    cell_element.text = processed_child.text\n                    cell_element.tail = processed_child.tail\n            # handle child elements with further descendants\n            else:\n                for subelem in child.iterdescendants('*'):\n                    # related to table elements\n                    if subelem.tag in TABLE_ALL:\n                        processed_subchild = handle_textnode(subelem, options, comments_fix=False)\n                        if processed_subchild is not None:\n                            cell_element.append(processed_subchild)\n                    subelem.tag = 'done'\n            # add cell to row\n            newrow.append(cell_element)\n        child.tag = 'done'\n    # finish\n    if len(newrow) > 0:\n        newtable.append(newrow)\n    if len(newtable) > 0:\n        return newtable\n    return None\n\n", "idx": 948}
{"namespace": "trafilatura.filters.language_filter", "completion": "    if target_language:\n        result = language_classifier(temp_text, temp_comments)\n        if result != target_language:\n            LOGGER.warning('Language %s is not %s', result, target_language)\n            docmeta.language = result\n            return True, docmeta\n    return False, docmeta", "idx": 949}
{"namespace": "trafilatura.filters.textfilter", "completion": "    # check for unwanted text\n    if element.text is not None:\n        if RE_FILTER.match(element.text):\n            return True\n    return False", "idx": 950}
{"namespace": "trafilatura.metadata.extract_meta_json", "completion": "    for element in tree.xpath('//script[@type=\"application/ld+json\" or @type=\"application/settings+json\"]'):\n        if element.text:\n            json_text = JSON_MINIFY.sub('', element.text)\n            try:\n                json_data = json.loads(json_text)\n            except json.JSONDecodeError:\n                json_data = json.loads(json_text.replace('\"', '\\\\\"'))\n            if isinstance(json_data, list):\n                for item in json_data:\n                    if isinstance(item, dict):\n                        extract_meta_json_item(item, metadata)\n            elif isinstance(json_data, dict):\n                extract_meta_json_item(json_data, metadata)\n    return metadata\n\n", "idx": 951}
{"namespace": "trafilatura.external.try_justext", "completion": "    # init\n    body = Element('body')\n    # determine language\n    if target_language is None:\n        target_language = 'en'\n    if target_language not in JUSTEXT_LANGUAGES:\n        target_language = 'en'\n    # extract paragraphs\n    try:\n        paragraphs = custom_justext(tree, JUSTEXT_STOPLISTS[target_language])\n        for paragraph in paragraphs:\n            if not paragraph.is_boilerplate:\n                body.append(paragraph.to_html_tree())\n    except Exception as err:\n        LOGGER.warning('justext failed: %s', err)\n    return body\n\n", "idx": 952}
{"namespace": "mongoengine.base.datastructures.BaseDict.get", "completion": "        if key in self:\n            return self[key]\n        return default\n", "idx": 953}
{"namespace": "sqlite_utils.utils.suggest_column_types", "completion": "    column_types = {}\n    for record in records:\n        for key, value in record.items():\n            if key not in column_types:\n                column_types[key] = set()\n            column_types[key].add(type(value))\n    return suggest_column_types_from_dict(column_types)\n\n", "idx": 954}
{"namespace": "sqlite_utils.plugins.get_plugins", "completion": "    plugins = pm.list_plugin_distinfo()\n    out = []\n    for name, (plugin, dist) in plugins:\n        d = {\"name\": name}\n        hooks = pm.get_hookcallers(plugin)\n        d[\"hooks\"] = [h.name for h in hooks]\n        if dist:\n            d[\"version\"] = dist.version\n            d[\"project_name\"] = dist.project_name\n        out.append(d)\n    return out", "idx": 955}
{"namespace": "alembic.config.Config.print_stdout", "completion": "        if self.cmd_opts and self.cmd_opts.quiet:\n            return\n        if arg:\n            text = text % arg\n        self.stdout.write(text)\n        self.stdout.write(\"\\n\")\n        self.stdout.flush()\n", "idx": 956}
{"namespace": "alembic.config.Config.set_section_option", "completion": "        if not self.file_config.has_section(section):\n            self.file_config.add_section(section)\n        self.file_config.set(section, name, value)\n", "idx": 957}
{"namespace": "alembic.command.merge", "completion": "    script_directory = ScriptDirectory.from_config(config)\n\n    command_args = dict(\n        message=message,\n        autogenerate=True,\n        sql=False,\n        head=\"head\",\n        splice=False,\n        branch_label=branch_label,\n        version_path=None,\n        rev_id=rev_id,\n        depends_on=None,\n    )\n    revision_context = autogen.RevisionContext(\n        config,\n        script_directory,\n        command_args,\n    )\n\n    def retrieve_migrations(rev, context):\n        revision_context.run_autogenerate(rev, context)\n        return []\n\n    with EnvironmentContext(\n        config,\n        script_directory,\n        fn=retrieve_migrations,\n        as_sql=False,\n        template_args=revision_context.template_args,\n        revision_context=revision_context,\n    ):\n        script_directory.run_env()\n\n    # the revision_context now has MigrationScript structure(s) present.\n\n    migration_script = revision_context.generated_revisions[-1]\n    diffs = migration_script.upgrade_ops.as_diffs()\n    if diffs:\n        raise util.AutogenerateDiffsDetected(\n            f\"New upgrade operations detected: {diffs}\"\n        )\n    else:\n        config.print_stdout(\"No new upgrade operations detected.\")", "idx": 958}
{"namespace": "alembic.command.upgrade", "completion": "    script = ScriptDirectory.from_config(config)\n\n    starting_rev = None\n    if \":\" not in revision:\n        starting_rev = script.get_revision(revision)\n    else:\n        start, end = revision.split(\":\", 2)\n        if not start:\n            start = None\n        starting_rev = script.get_revision(start)\n        ending_rev = script.get_revision(end)\n        if ending_rev is not None and ending_rev.revision != end:\n            raise util.CommandError(\n                \"Incorrect end revision (%s) - should be head or a valid \"\n                \"revision identifier\" % end\n            )\n\n    def upgrade(rev, context):\n        return script._upgrade_revs(revision, rev)\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=upgrade,\n        as_sql=sql,\n        starting_rev=starting_rev,\n        destination_rev=ending_rev,\n        tag=tag,\n    ):\n        script.run_env()", "idx": 959}
{"namespace": "alembic.command.downgrade", "completion": "    script = ScriptDirectory.from_config(config)\n\n    starting_rev = None\n    if \":\" in revision:\n        if not sql:\n            raise util.CommandError(\"Range revision not allowed\")\n        starting_rev, revision = revision.split(\":\", 2)\n\n    def downgrade(rev, context):\n        return script._downgrade_revs(revision, rev)\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=downgrade,\n        as_sql=sql,\n        starting_rev=starting_rev,\n        destination_rev=revision,\n        tag=tag,\n    ):\n        script.run_env()", "idx": 960}
{"namespace": "alembic.command.history", "completion": "    script = ScriptDirectory.from_config(config)\n\n    def _display_history(rev, context):\n        for sc in script.walk_revisions(\n            rev_range or \"base\", rev\n        ):\n            config.print_stdout(\n                sc.cmd_format(\n                    verbose,\n                    include_branches=True,\n                    include_doc=True,\n                    include_parents=True,\n                    include_indicate_current=indicate_current,\n                )\n            )\n        return []\n\n    with EnvironmentContext(config, script, fn=_display_history):\n        script.run_env()\n\n", "idx": 961}
{"namespace": "alembic.command.stamp", "completion": "    script = ScriptDirectory.from_config(config)\n\n    starting_rev = None\n    if \":\" in revision:\n        if not sql:\n            raise util.CommandError(\"Range revision not allowed\")\n        starting_rev, revision = revision.split(\":\", 2)\n\n    def do_stamp(rev, context):\n        return script._stamp_revs(revision, rev, purge=purge)\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=do_stamp,\n        as_sql=sql,\n        starting_rev=starting_rev,\n        destination_rev=revision,\n        tag=tag,\n    ):\n        script.run_env()", "idx": 962}
{"namespace": "alembic.command.ensure_version", "completion": "    script = ScriptDirectory.from_config(config)\n\n    def ensure_version(rev, context):\n        return script._ensure_version_table(rev)\n\n    with EnvironmentContext(\n        config, script, fn=ensure_version, as_sql=sql, destination_rev=None\n    ):\n        script.run_env()", "idx": 963}
{"namespace": "alembic.autogenerate.compare._compare_server_default", "completion": "    conn_default = _render_server_default_for_compare(\n        conn_col.server_default, autogen_context\n    )\n    metadata_default = _render_server_default_for_compare(\n        metadata_col.server_default, autogen_context\n    )\n\n    if conn_default is None:\n        conn_default = \"\"\n    if metadata_default is None:\n        metadata_default = \"\"\n\n    if conn_default != metadata_default:\n        if (\n            sqla_compat._server_default_is_computed(\n                metadata_col.server_default, conn_col.server_default\n            )\n            and sqla_compat._nullability_might_be_unset(metadata_col)\n            or (\n                sqla_compat._server_default_is_identity(\n                    metadata_col.server_default, conn_col.server_default\n                )\n            )\n        ):\n            log.info(\n                \"Ignoring server_default change on identity column '%s.%s'\",\n                tname,\n                cname,\n            )\n        else:\n            alter_column_op.modify_server_default = metadata_col.server_default\n            log.info(\n                \"Detected server_default change from %r to %r on '%s.%s'\",\n                conn_default,\n                metadata_default,\n                tname,\n                cname,\n            )\n\n", "idx": 964}
{"namespace": "alembic.autogenerate.render._render_server_default", "completion": "    rendered = _user_defined_render(\"server_default\", default, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    if default is None:\n        return None\n\n    if sqla_compat._server_default_is_computed(default):\n        return \"Computed()\"\n\n    if sqla_compat._server_default_is_identity(default):\n        return \"Identity()\"\n\n    if isinstance(default, sql.DefaultClause):\n        if isinstance(default.arg, str):\n            return _render_potential_expr(default, autogen_context)\n        else:\n            return repr(default.arg)\n\n    if isinstance(default, str) and repr_:\n        return default[1:-1]\n\n    return repr(default)\n\n", "idx": 965}
{"namespace": "alembic.autogenerate.render._render_constraint", "completion": "    renderer = _constraint_renderers.dispatch(constraint)\n    if renderer is not None:\n        return renderer(constraint, autogen_context, namespace_metadata)\n    else:\n        return \"<unknown Python object %r>\" % constraint\n\n", "idx": 966}
{"namespace": "alembic.autogenerate.render._render_unique_constraint", "completion": "    rendered = _user_defined_render(\"unique_constraint\", constraint, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    return _uq_constraint(constraint, autogen_context, False)\n\n", "idx": 967}
{"namespace": "alembic.autogenerate.render._render_check_constraint", "completion": "    rendered = _user_defined_render(\"check\", constraint, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    if constraint.table is not None:\n        for col in constraint.table.columns:\n            if col.type.python_type == constraint.sqltext:\n                return None\n\n    opts = []\n    if constraint.name:\n        opts.append(\n            (\"name\", repr(_render_gen_name(autogen_context, constraint.name)))\n        )\n\n    return \"%(prefix)sCheckConstraint(%(sql)s, %(args)s)\" % {\n        \"prefix\": _sqlalchemy_autogenerate_prefix(autogen_context),\n        \"sql\": repr(constraint.sqltext),\n        \"args\": \", \".join(\n            [\"%s=%s\" % (kwname, val) for kwname, val in opts]\n        ),\n    }", "idx": 968}
{"namespace": "alembic.autogenerate.api.compare_metadata", "completion": "    # TODO: This function is not yet implemented.\n    raise NotImplementedError(\"This function is not yet implemented.\")\n\n", "idx": 969}
{"namespace": "alembic.autogenerate.api.AutogenContext._within_batch", "completion": "        self._has_batch = True\n        try:\n            yield\n        finally:\n            self._has_batch = False\n", "idx": 970}
{"namespace": "alembic.util.sqla_compat._connectable_has_table", "completion": "    if sqla_14:\n        return connectable.dialect.has_table(connectable, tablename, schemaname)\n    else:\n        return connectable.dialect.has_table(connectable.connection, tablename, schemaname)\n\n", "idx": 971}
{"namespace": "alembic.util.sqla_compat._get_constraint_final_name", "completion": "    if constraint.name is None:\n        return None\n    elif sqla_14:\n        return constraint.name._format_for_dialect(dialect)\n    else:\n        return constraint.name._compiler_dispatch(dialect)\n\n", "idx": 972}
{"namespace": "alembic.testing.env.env_file_fixture", "completion": "    dir_ = os.path.join(_get_staging_directory(), \"scripts\")\n    path = os.path.join(dir_, \"env.py\")\n    with open(path, \"w\") as f:\n        f.write(txt)\n\n", "idx": 973}
{"namespace": "alembic.testing.env._no_sql_testing_config", "completion": "    dir_ = os.path.join(_get_staging_directory(), \"scripts\")\n    url = \"%s://user:pass@host/db\" % dialect\n\n    return _write_config_file(\n        \"\"\"", "idx": 974}
{"namespace": "alembic.testing.env._write_config_file", "completion": "    cfg = _testing_config()\n    with open(cfg.config_file_name, \"w\") as f:\n        f.write(text)\n    return cfg", "idx": 975}
{"namespace": "alembic.testing.env.three_rev_fixture", "completion": "    scriptdir = ScriptDirectory.from_config(cfg)\n\n    a = scriptdir.generate_revision(\n        \"A\",\n        \"Revision A\",\n        head=\"base\",\n        refresh=True,\n        splice=True,\n        branch_labels=\"heads\",\n    )\n    b = scriptdir.generate_revision(\n        \"B\",\n        \"Revision B\",\n        head=\"base\",\n        refresh=True,\n        splice=True,\n        branch_labels=\"heads\",\n    )\n    c = scriptdir.generate_revision(\n        \"C\",\n        \"Revision C\",\n        head=\"base\",\n        refresh=True,\n        splice=True,\n        branch_labels=\"heads\",\n    )\n\n    write_script(\n        scriptdir,\n        a,\n        \"\"\"\n        from alembic import op\n        import sqlalchemy as sa\n\n        def upgrade():\n            op.create_table('table_a',\n                sa.Column('id', sa.Integer, primary_key=True),\n                sa.Column('data', sa.String(50))\n            )\n\n        def downgrade():\n            op.drop_table('table_a')\n        \"\"\",\n    )\n\n    write_script(\n        scriptdir,\n        b,\n        \"\"\"\n        from alembic import op\n        import sqlalchemy as sa\n\n        def upgrade():\n            op.create_table('table_b',\n                sa.Column('id', sa.Integer, primary_key=True),\n                sa.Column('data', sa.String(50))\n            )\n\n        def downgrade():\n            op.drop_table('table_b')\n        \"\"\",\n    )\n\n    write_script(\n        scriptdir,\n        c,\n        \"\"\"\n        from alembic import op\n        import sqlalchemy as sa\n\n        def upgrade():\n            op.create_table('table_c',\n                sa.Column('id', sa.Integer, primary_key=True),\n                sa.Column('data', sa", "idx": 976}
{"namespace": "alembic.testing.env.multi_heads_fixture", "completion": "    d = util.rev_id()\n    e = util.rev_id()\n    f = util.rev_id()\n\n    script = ScriptDirectory.from_config(cfg)\n    script.generate_revision(d, \"revision d\", refresh=True, head=a)\n    write_script(\n        script,\n        d,\n        \"\"\"\\", "idx": 977}
{"namespace": "alembic.testing.fixtures.capture_db", "completion": "    engine = mock.Mock(spec=create_engine(dialect))\n    buf = io.StringIO()\n    engine.execute.side_effect = lambda *args, **kw: buf.write(str(args[0]))\n    return engine, buf", "idx": 978}
{"namespace": "alembic.testing.fixtures.capture_engine_context_buffer", "completion": "    if kw.pop(\"bytes_io\", False):\n        buf = io.BytesIO()\n    else:\n        buf = io.StringIO()\n\n    kw.update({\"dialect_name\": \"sqlite\", \"output_buffer\": buf})\n    conf = EnvironmentContext.configure\n\n    def configure(*arg, **opt):\n        opt.update(**kw)\n        return conf(*arg, **opt)\n\n    with mock.patch.object(EnvironmentContext, \"configure\", configure):\n        yield buf\n\n", "idx": 979}
{"namespace": "alembic.operations.schemaobj.SchemaObjects.unique_constraint", "completion": "        m = self.metadata()\n        t1 = sa_schema.Table(\n            source,\n            m,\n            *[sa_schema.Column(n, NULLTYPE) for n in local_cols],\n            schema=schema,\n        )\n        u = sa_schema.UniqueConstraint(*local_cols, name=name, **kw)\n        t1.append_constraint(u)\n        return u\n", "idx": 980}
{"namespace": "alembic.operations.schemaobj.SchemaObjects.index", "completion": "        m = self.metadata()\n        t = sa_schema.Table(tablename, m, schema=schema)\n        i = sa_schema.Index(name, *columns, **kw)\n        t.append_constraint(i)\n        return i\n", "idx": 981}
{"namespace": "alembic.operations.ops.DropConstraintOp.from_constraint", "completion": "        if isinstance(constraint, ForeignKeyConstraint):\n            return cls(\n                constraint.name,\n                constraint.table.name,\n                \"foreignkey\",\n                schema=constraint.table.schema,\n            )\n        else:\n            return cls(\n                constraint.name,\n                constraint.table.name,\n                constraint.__visit_name__,\n                schema=constraint.table.schema,\n            )\n", "idx": 982}
{"namespace": "alembic.operations.ops.DropConstraintOp.to_constraint", "completion": "        if self._reverse is not None:\n            constraint = self._reverse.to_constraint()\n            constraint.name = self.constraint_name\n            constraint.table.name = self.table_name\n            constraint.table.schema = self.schema\n            return constraint\n        else:\n            raise ValueError(\n                \"Cannot convert DropConstraintOp to Constraint without reverse operation\"\n            )", "idx": 983}
{"namespace": "alembic.operations.ops.CreatePrimaryKeyOp.to_constraint", "completion": "        schema = sqla_compat._get_schema_from_migration_context(\n            migration_context\n        )\n        table = sqla_compat._get_table_from_migration_context(\n            migration_context, self.table_name, schema\n        )\n        return PrimaryKeyConstraint(\n            self.constraint_name,\n            table,\n            self.columns,\n            **self.kw,\n        )\n", "idx": 984}
{"namespace": "alembic.operations.ops.CreateIndexOp.from_index", "completion": "        return cls(\n            index.name,\n            index.table.name,\n            index.columns,\n            schema=index.table.schema,\n            unique=index.unique,\n            if_not_exists=index.dialect_kwargs.get(\"if_not_exists\"),\n            **index.dialect_kwargs,\n        )\n", "idx": 985}
{"namespace": "alembic.operations.ops.DropIndexOp.from_index", "completion": "        assert index.table is not None\n        return cls(\n            index.name,  # type: ignore[arg-type]\n            index.table.name,\n            sqla_compat._get_index_expressions(index),\n            schema=index.table.schema,\n            unique=index.unique,\n            **index.kwargs,\n        )\n", "idx": 986}
{"namespace": "alembic.operations.ops.DropIndexOp.to_index", "completion": "        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        idx = schema_obj.index(\n            self.index_name,\n            self.table_name,\n            self.columns,\n            schema=self.schema,\n            unique=self.unique,\n            **self.kw,\n        )\n        return idx\n", "idx": 987}
{"namespace": "alembic.operations.ops.CreateTableOp.from_table", "completion": "        return cls(\n            table.name,\n            table.columns.values(),\n            schema=table.schema,\n            _namespace_metadata=_namespace_metadata,\n            _constraints_included=True,\n            info=table.info,\n            comment=table.comment,\n            prefixes=table.prefixes,\n            **table.dialect_kwargs,\n        )\n", "idx": 988}
{"namespace": "alembic.operations.ops.DropTableOp.from_table", "completion": "        if _namespace_metadata is None:\n            _namespace_metadata = table.metadata\n\n        return cls(\n            table.name,\n            schema=table.schema,\n            table_kw=table.kwargs,\n            _reverse=CreateTableOp.from_table(table, _namespace_metadata=_namespace_metadata),\n            comment=table.comment,\n            info=dict(table.info),\n            prefixes=list(table._prefixes),\n        )\n", "idx": 989}
{"namespace": "alembic.operations.ops.DropTableOp.to_table", "completion": "        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.table(\n            self.table_name,\n            *self._reverse.columns if self._reverse else [],\n            schema=self.schema,\n            prefixes=list(self.prefixes) if self.prefixes else [],\n            comment=self.comment,\n            info=self.info.copy() if self.info else {},\n            **self.table_kw,\n        )\n", "idx": 990}
{"namespace": "alembic.operations.ops.AlterColumnOp.to_diff_tuple", "completion": "        if self.modify_type:\n            return (\n                \"modify_type\",\n                self.to_column(),\n                self.existing_type,\n            )\n        elif self.modify_nullable:\n            return (\n                \"modify_nullable\",\n                self.to_column(),\n                self.existing_nullable,\n            )\n        elif self.modify_server_default:\n            return (\n                \"modify_server_default\",\n                self.to_column(),\n                self.existing_server_default,\n            )\n        elif self.modify_comment:\n            return (\n                \"modify_comment\",\n                self.to_column(),\n                self.existing_comment,\n            )\n        else:\n            return (\"modify_name\", self.to_column(), self.modify_name)\n", "idx": 991}
{"namespace": "alembic.operations.ops.AddColumnOp.reverse", "completion": "        return DropColumnOp(\n            self.table_name,\n            self.column.name,\n            schema=self.schema,\n            existing_type=self.column.type,\n            existing_server_default=self.column.server_default,\n            existing_nullable=self.column.nullable,\n            existing_comment=self.column.comment,\n            **self.kw,\n        )\n", "idx": 992}
{"namespace": "alembic.operations.ops.DropColumnOp.reverse", "completion": "        if self._reverse:\n            return self._reverse\n        else:\n            raise ValueError(\n                \"Cannot reverse operation: \"\n                \"reverse operation not available\"\n            )\n", "idx": 993}
{"namespace": "alembic.operations.ops.DropColumnOp.from_column_and_tablename", "completion": "        return cls(tname, col.name, schema=schema)\n", "idx": 994}
{"namespace": "alembic.operations.ops.DropColumnOp.to_column", "completion": "        if self._reverse:\n            return self._reverse.column\n        else:\n            schema_obj = schemaobj.SchemaObjects(migration_context)\n            return schema_obj.column(self.column_name, NULLTYPE)\n", "idx": 995}
{"namespace": "alembic.script.revision.RevisionMap.heads", "completion": "        self.revision_map  # noqa\n        return tuple(\n            rev.revision\n            for rev in self.revision_map.values()\n            if rev.is_head\n        )\n", "idx": 996}
{"namespace": "alembic.script.revision.RevisionMap.add_revision", "completion": "        if revision.revision in self._map:\n            if _replace:\n                del self._map[revision.revision]\n            else:\n                util.warn(\n                    \"Revision %s is present more than once\" % revision.revision\n                )\n        self._map[revision.revision] = revision\n        if revision.branch_labels:\n            self._has_branch_labels.add(revision)\n\n        self._heads.add(revision)\n        self._real_heads.add(revision)\n        if revision.is_base:\n            self._bases += (revision,)\n        if revision._is_real_base:\n            self._real_bases += (revision,)\n\n        self._map_branch_labels(self._has_branch_labels, self._map)\n\n        self._add_depends_on(self._all_revisions, self._map)\n\n        for rev in self._map.values():\n            for downrev in rev._versioned_down_revisions:\n                if downrev not in self._map:\n                    util.warn(\n                        \"Revision %s referenced from %s is not present\"\n                        % (downrev, rev)\n                    )\n                down_revision = self._map[downrev]\n                down_revision.add_nextrev(rev)\n                if downrev in rev._versioned_down_revisions:\n                    self._heads.discard(down_revision)\n                self._real_heads.discard(down_revision)\n\n        self._normalize_depends_on(self._all_revisions, self._map)\n        self._detect_cycles(\n            self._map, self._heads, self._bases, self._real_heads, self._real_bases\n        )\n\n        self.heads = tuple(rev.revision for rev in self._heads)\n        self._real_heads = tuple(rev.revision for rev in self._real_heads)\n        self.bases = tuple(rev.revision for rev in self", "idx": 997}
{"namespace": "alembic.script.revision.RevisionMap.get_revisions", "completion": "        if id_ is None:\n            return ()\n        elif isinstance(id_, str):\n            if id_ == \"head\":\n                return self.heads\n            elif id_ == \"base\":\n                return self.bases\n            elif id_ == \"heads\":\n                return self.heads\n            elif id_ == \"bases\":\n                return self.bases\n            elif id_ == \"current\":\n                return (self.get_current_head(),)\n            elif id_ == \"all\":\n                return tuple(self._revision_map.keys())\n            elif id_ == \"branches\":\n                return tuple(self.branches)\n            elif id_ == \"legacy\":\n                return tuple(self.legacy)\n            elif id_ == \"pending\":\n                return tuple(self.pending)\n            elif id_ == \"current+pending\":\n                return self.current + self.pending\n            elif id_ == \"current+legacy\":\n                return self.current + self.legacy\n            elif id_ == \"current+legacy+pending\":\n                return self.current + self.legacy + self.pending\n            elif id_ == \"current+legacy+pending+branches\":\n                return self.current + self.legacy + self.pending + self.branches\n            elif id_ == \"current+legacy+pending+branches+heads\":\n                return self.current + self.legacy + self.pending + self.branches + self.heads\n            elif id_ == \"current+legacy+pending+branches+heads+base\":\n                return self.current + self.legacy + self.pending + self.branches + self.heads + self.bases\n            elif id_ == \"current+legacy+pending+branches+heads+base+all\":\n                return self.current + self.legacy + self.pending + self.branches + self.heads + self.bases + self.all\n            elif id_ == \"current+legacy+pending+branches+heads+base+all+head\":\n                return self.current", "idx": 998}
{"namespace": "alembic.script.revision.RevisionMap.get_revision", "completion": "        if id_ is None:\n            return None\n        if id_ == \"head\":\n            return self.get_current_head()\n        if id_ == \"base\":\n            return self.bases[0]\n        if id_ == \"heads\":\n            return self.heads\n        if id_ == \"bases\":\n            return self.bases\n        if id_ == \"current\":\n            return self.get_current_head()\n        if id_ == \"current@head\":\n            return self.get_current_head()\n        if id_ == \"current@heads\":\n            return self.heads\n        if id_ == \"current@base\":\n            return self.bases[0]\n        if id_ == \"current@bases\":\n            return self.bases\n        if id_ == \"current@heads+current@base\":\n            return self.heads + self.bases\n        if id_ == \"current@heads+current@bases\":\n            return self.heads + self.bases\n        if id_ == \"current@heads+current@head\":\n            return self.heads + self.heads\n        if id_ == \"current@heads+current@heads\":\n            return self.heads + self.heads\n        if id_ == \"current@bases+current@base\":\n            return self.bases + self.bases\n        if id_ == \"current@bases+current@bases\":\n            return self.bases + self.bases\n        if id_ == \"current@bases+current@head\":\n            return self.bases + self.heads\n        if id_ == \"current@bases+current@heads\":\n            return self.bases + self.heads\n        if id_ == \"current@head+current@base\":\n            return self.heads + self.bases\n        if id_ == \"current@head+current@bases\":\n            return self.heads + self.bases\n        if id_ == \"current@head+current@head\":\n            return self.heads + self.heads\n        if id", "idx": 999}
{"namespace": "alembic.script.revision.RevisionMap.filter_for_lineage", "completion": "        if check_against is None:\n            return tuple(targets)\n\n        targets = set(targets)\n\n        for rev in list(targets):\n            assert rev\n            if targets.intersection(\n                self._get_descendant_nodes(\n                    [rev],\n                    include_dependencies=include_dependencies,\n                )\n            ).difference([rev]):\n                targets.discard(rev)\n        return tuple(targets)\n", "idx": 1000}
{"namespace": "alembic.script.revision.RevisionMap.iterate_revisions", "completion": "        # Ensure that the upper and lower revision identifiers are not None\n        if upper is None:\n            raise ValueError(\"upper revision identifier cannot be None\")\n        if lower is None:\n            raise ValueError(\"lower revision identifier cannot be None\")\n\n        # Ensure that the upper and lower revision identifiers are not the same\n        if upper == lower:\n            raise ValueError(\"upper and lower revision identifiers cannot be the same\")\n\n        # Ensure that the upper and lower revision identifiers are not the same\n        if upper == lower:\n            raise ValueError(\"upper and lower revision identifiers cannot be the same\")\n\n        # Ensure that the upper and lower revision identifiers are not the same\n        if upper == lower:\n            raise ValueError(\"upper and lower revision identifiers cannot be the same\")\n\n        # Ensure that the upper and lower revision identifiers are not the same\n        if upper == lower:\n            raise ValueError(\"upper and lower revision identifiers cannot be the same\")\n\n        # Ensure that the upper and lower revision identifiers are not the same\n        if upper == lower:\n            raise ValueError(\"upper and lower revision identifiers cannot be the same\")\n\n        # Ensure that the upper and lower revision identifiers are not the same\n        if upper == lower:\n            raise ValueError(\"upper and lower revision identifiers cannot be the same\")\n\n        # Ensure that the upper and lower revision identifiers are not the same\n        if upper == lower:\n            raise ValueError(\"upper and lower revision identifiers cannot be the same\")\n\n        # Ensure that the upper and lower revision identifiers are not the same\n        if upper == lower:\n            raise ValueError(\"upper and lower revision identifiers cannot be the same\")\n\n        # Ensure that the upper and lower revision identifiers are not the same\n        if upper == lower:\n            raise ValueError(\"upper and lower revision identifiers cannot be the same\")\n\n        # Ensure that the upper and lower revision identifiers are not the same\n        if upper == lower:\n            raise ValueError(\"upper and lower revision identifiers cannot be the same\")\n\n        # Ensure that the upper and lower revision identifiers are not the same\n        if upper ==", "idx": 1001}
{"namespace": "alembic.script.revision.RevisionMap._topological_sort", "completion": "        # Create a dictionary to store the in-degree of each revision\n        in_degree = {revision: 0 for revision in revisions}\n\n        # Create a dictionary to store the children of each revision\n        children = {revision: set() for revision in revisions}\n\n        # Iterate through the revisions and update the in-degree and children dictionaries\n        for revision in revisions:\n            for child in revision.nextrev:\n                children[revision].add(child)\n                in_degree[child] += 1\n\n        # Create a queue to store the revisions with in-degree 0\n        queue = collections.deque(\n            [revision for revision in revisions if in_degree[revision] == 0]\n        )\n\n        # Create a list to store the sorted revisions\n        sorted_revisions = []\n\n        # Perform the topological sort\n        while queue:\n            # Pop a revision from the queue\n            revision = queue.popleft()\n\n            # Add the revision to the sorted list\n            sorted_revisions.append(revision)\n\n            # Decrement the in-degree of the children of the revision\n            for child in children[revision]:\n                in_degree[child] -= 1\n\n                # If the in-degree of the child becomes 0, add it to the queue\n                if in_degree[child] == 0:\n                    queue.append(child)\n\n        # Check if there are any cycles in the graph\n        if any(in_degree.values()):\n            raise CycleDetected(\n                [revision.revision for revision in revisions]\n            )\n\n        # Return the sorted revisions\n        return [revision.revision for revision in sorted_revisions]\n", "idx": 1002}
{"namespace": "alembic.script.revision.Revision._all_down_revisions", "completion": "        return tuple(\n            set(\n                util.to_tuple(self.down_revision)\n                + util.to_tuple(self._resolved_dependencies)\n            )\n        )\n", "idx": 1003}
{"namespace": "alembic.script.revision.Revision._normalized_down_revisions", "completion": "        return util.dedupe_tuple(\n            util.to_tuple(self.down_revision, default=())\n            + self._normalized_resolved_dependencies\n        )\n", "idx": 1004}
{"namespace": "alembic.script.write_hooks._invoke", "completion": "    try:\n        fn = _registry[name]\n    except KeyError:\n        raise util.CommandError(f\"No formatter with name '{name}' registered\")\n\n    return fn(revision, options)\n\n", "idx": 1005}
{"namespace": "bplustree.memory.FileMemory.get_node", "completion": "        with self._lock.gen_rlock():\n            node = self._cache.get(page)\n            if node is not None:\n                return node\n\n            data = read_from_file(self._fd, page * self._tree_conf.page_size,\n                                 (page + 1) * self._tree_conf.page_size)\n            node = Node.from_data(data, self._tree_conf)\n            self._cache[page] = node\n            return node\n", "idx": 1006}
{"namespace": "bplustree.memory.FileMemory.next_available_page", "completion": "        return self.last_page\n", "idx": 1007}
{"namespace": "bplustree.memory.FileMemory.get_metadata", "completion": "        data = self._read_page(0)\n        root_node_page = int.from_bytes(\n            data[:PAGE_REFERENCE_BYTES], ENDIAN)\n        page_size = int.from_bytes(\n            data[PAGE_REFERENCE_BYTES:PAGE_REFERENCE_BYTES * 2], ENDIAN)\n        order = int.from_bytes(\n            data[PAGE_REFERENCE_BYTES * 2:PAGE_REFERENCE_BYTES * 2 + FRAME_TYPE_BYTES], ENDIAN)\n        key_size = int.from_bytes(\n            data[PAGE_REFERENCE_BYTES * 2 + FRAME_TYPE_BYTES:PAGE_REFERENCE_BYTES * 2 + FRAME_TYPE_BYTES * 2], ENDIAN)\n        value_size = int.from_bytes(\n            data[PAGE_REFERENCE_BYTES * 2 + FRAME_TYPE_BYTES * 2:PAGE_REFERENCE_BYTES * 2 + FRAME_TYPE_BYTES * 3], ENDIAN)\n        tree_conf = TreeConf(page_size, order, key_size, value_size)\n        return root_node_page, tree_conf\n", "idx": 1008}
{"namespace": "bplustree.memory.FileMemory.set_metadata", "completion": "        data = root_node_page.to_bytes(\n            PAGE_REFERENCE_BYTES, ENDIAN\n        ) + tree_conf.page_size.to_bytes(\n            OTHERS_BYTES, ENDIAN\n        ) + tree_conf.order.to_bytes(\n            OTHERS_BYTES, ENDIAN\n        ) + tree_conf.key_size.to_bytes(\n            OTHERS_BYTES, ENDIAN\n        ) + tree_conf.value_size.to_bytes(\n            OTHERS_BYTES, ENDIAN\n        )\n        self._write_page(0, data)\n        self._tree_conf = tree_conf\n", "idx": 1009}
{"namespace": "bplustree.memory.WAL.checkpoint", "completion": "        if self._not_committed_pages:\n            logger.warning('WAL has uncommitted pages, '\n                           'the B+Tree was not closed properly')\n        fsync_file_and_dir(self._fd.fileno(), self._dir_fd)\n        self._fd.seek(0)\n        for page, data in self._read_committed_pages():\n            yield page, data\n        self._fd.close()\n        os.remove(self.filename)\n        if self._dir_fd is not None:\n            os.fsync(self._dir_fd)\n", "idx": 1010}
{"namespace": "bplustree.memory.WAL.commit", "completion": "        if self._not_committed_pages:\n            self._add_frame(FrameType.COMMIT)\n", "idx": 1011}
{"namespace": "bplustree.memory.WAL.rollback", "completion": "        if self._not_committed_pages:\n            self._add_frame(FrameType.ROLLBACK)\n", "idx": 1012}
{"namespace": "bplustree.entry.Record.dump", "completion": "        key = self._tree_conf.serializer.serialize(self.key)\n        used_key_length = len(key)\n        assert 0 <= used_key_length <= self._tree_conf.key_size\n\n        if self.overflow_page:\n            value = None\n        else:\n            value = self.value\n            assert len(value) <= self._tree_conf.value_size\n\n        used_value_length = len(value) if value else 0\n        assert 0 <= used_value_length <= self._tree_conf.value_size\n\n        overflow_page = self.overflow_page if self.overflow_page else 0\n\n        return (\n            used_key_length.to_bytes(USED_KEY_LENGTH_BYTES, ENDIAN) +\n            key +\n            used_value_length.to_bytes(USED_VALUE_LENGTH_BYTES, ENDIAN) +\n            value +\n            overflow_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN)\n        )\n", "idx": 1013}
{"namespace": "bplustree.entry.Reference.__repr__", "completion": "        return '<Reference: key={} before={} after={}>'.format(\n            self.key, self.before, self.after\n        )", "idx": 1014}
{"namespace": "bplustree.node.Node.dump", "completion": "        data = bytearray()\n        for entry in self.entries:\n            data.extend(entry.dump())\n\n        used_page_length = len(data)\n        header = bytearray()\n        header.extend(self._node_type_int.to_bytes(NODE_TYPE_BYTES, ENDIAN))\n        header.extend(used_page_length.to_bytes(USED_PAGE_LENGTH_BYTES, ENDIAN))\n        if self.next_page is None:\n            self.next_page = 0\n        header.extend(self.next_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN))\n\n        data[0:0] = header\n        padding_length = self._tree_conf.page_size - len(data)\n        data.extend(b'\\x00' * padding_length)\n        return data\n", "idx": 1015}
{"namespace": "bplustree.node.Node._find_entry_index", "completion": "        entry = self._entry_class(self._tree_conf, key=key)\n        return bisect.bisect_left(self.entries, entry)\n", "idx": 1016}
{"namespace": "bplustree.node.Node.from_page_data", "completion": "        node_type_int = int.from_bytes(data[:NODE_TYPE_BYTES], ENDIAN)\n        if node_type_int == RecordNode._node_type_int:\n            return RecordNode(tree_conf, data, page)\n        elif node_type_int == ReferenceNode._node_type_int:\n            return ReferenceNode(tree_conf, data, page)\n        else:\n            raise ValueError('Unknown node type {}'.format(node_type_int))\n\n", "idx": 1017}
{"namespace": "bplustree.tree.BPlusTree._root_node", "completion": "        return self._mem.get_node(self._root_node_page)\n", "idx": 1018}
{"namespace": "bplustree.tree.BPlusTree._left_record_node", "completion": "        node = self._root_node\n        while not isinstance(node, (LonelyRootNode, LeafNode)):\n            node = self._mem.get_node(node.children[0])\n        return node\n", "idx": 1019}
{"namespace": "mopidy.ext.Extension.get_cache_dir", "completion": "        cache_dir = path.get_cache_dir(config) / cls.ext_name\n        cache_dir.mkdir(parents=True, exist_ok=True)\n        return cache_dir\n", "idx": 1020}
{"namespace": "mopidy.ext.Extension.get_config_dir", "completion": "        if cls.ext_name is None:\n            raise AssertionError\n        config_dir_path = (\n            path.expand_path(config[\"core\"][\"config_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(config_dir_path)\n        return config_dir_path\n", "idx": 1021}
{"namespace": "mopidy.ext.Extension.get_data_dir", "completion": "        if cls.ext_name is None:\n            raise AssertionError\n        data_dir_path = (\n            path.expand_path(config[\"core\"][\"data_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(data_dir_path)\n        return data_dir_path\n", "idx": 1022}
{"namespace": "mopidy.ext.load_extensions", "completion": "    installed_extensions = []\n    for entry_point in pkg_resources.iter_entry_points(group=\"mopidy.ext\"):\n        try:\n            extension_class = entry_point.load()\n            if not issubclass(extension_class, Extension):\n                logger.warning(\n                    \"Skipping extension %s. Not a subclass of Extension\",\n                    extension_class,\n                )\n                continue\n        except ImportError as e:\n            logger.warning(\n                \"Skipping extension %s. Import failed with exception: %s\",\n                entry_point.name,\n                e,\n            )\n            continue\n\n        extension = extension_class()\n        config_schema = extension.get_config_schema()\n        config_defaults = config_schema.defaults()\n        command = extension.get_command()\n\n        installed_extensions.append(\n            ExtensionData(\n                extension=extension,\n                entry_point=entry_point,\n                config_schema=config_schema,\n                config_defaults=config_defaults,\n                command=command,\n            )\n        )\n\n    return installed_extensions", "idx": 1023}
{"namespace": "mopidy.ext.validate_extension_data", "completion": "    if data.extension.ext_name != data.entry_point.name:\n        logger.warning(\n            \"Extension name does not match entry point name: %s != %s\",\n            data.extension.ext_name,\n            data.entry_point.name,\n        )\n        return False\n\n    if not data.extension.validate_environment():\n        logger.warning(\n            \"Extension %s has invalid environment, skipping.\",\n            data.extension.ext_name,\n        )\n        return False\n\n    if not data.config_schema.validate(data.config_defaults):\n        logger.warning(\n            \"Extension %s has invalid config, skipping.\", data.extension.ext_name\n        )\n        return False\n\n    return True", "idx": 1024}
{"namespace": "mopidy.httpclient.format_user_agent", "completion": "    if name:\n        return f\"{name} {get_version()} Python/{platform.python_version()}\"\n    else:\n        return f\"Mopidy/{get_version()} Python/{platform.python_version()}\"", "idx": 1025}
{"namespace": "mopidy.models.immutable.ValidatedImmutableObject.replace", "completion": "        other = copy.copy(self)\n        for key, value in kwargs.items():\n            if not self._is_valid_field(key):\n                raise TypeError(\n                    f\"replace() got an unexpected keyword argument {key}\")\n            other._set_field(key, value)\n        return other", "idx": 1026}
{"namespace": "mopidy.http.Extension.get_default_config", "completion": "        conf_file = os.path.join(os.path.dirname(__file__), \"ext.conf\")\n        return config_lib.read(conf_file)\n", "idx": 1027}
{"namespace": "mopidy.http.Extension.get_config_schema", "completion": "        schema = super().get_config_schema()\n        schema[\"hostname\"] = config_lib.Hostname()\n        schema[\"port\"] = config_lib.Port()\n        schema[\"static_dir\"] = config_lib.Path(optional=True)\n        schema[\"zeroconf\"] = config_lib.String(optional=True)\n        schema[\"allowed_origins\"] = config_lib.List(optional=True)\n        schema[\"csrf_protection\"] = config_lib.Boolean(optional=True)\n        schema[\"default_app\"] = config_lib.String(optional=True)\n        schema[\"default_app_config\"] = config_lib.Dict(optional=True)\n        schema[\"default_app_config\"][\"enabled\"] = config_lib.Boolean(optional=True)\n        schema[\"default_app_config\"][\"name\"] = config_lib.String(optional=True)\n        schema[\"default_app_config\"][\"config\"] = config_lib.Dict(optional=True)\n        schema[\"default_app_config\"][\"config\"][\"enabled\"] = config_lib.Boolean(\n            optional=True\n        )\n        schema[\"default_app_config\"][\"config\"][\"name\"] = config_lib.String(optional=True)\n        schema[\"default_app_config\"][\"config\"][\"config\"] = config_lib.Dict(\n            optional=True\n        )\n        schema[\"default_app_config\"][\"config\"][\"config\"][\"enabled\"] = config_lib.Boolean(\n            optional=True\n        )\n        schema[\"default_app_config\"][\"config\"][\"config\"][\"name\"] = config_lib.String(\n            optional=True\n        )\n        schema[\"default_app_config\"][\"config\"][\"config\"][\"config\"] = config_lib.Dict(\n            optional=True\n        )\n        schema[\"default_app_config\"][\"config\"][\"config\"][\"config\"][\"enabled\"] = config_lib.Boolean(\n            optional=True\n        )\n        schema[\"default_app_config\"][\"config\"][\"config\"][\"config\"][\"name\"] = config_lib.String(\n            optional=True", "idx": 1028}
{"namespace": "mopidy.internal.network.try_ipv6_socket", "completion": "    try:\n        socket.socket(socket.AF_INET6)\n        return True\n    except OSError:\n        logger.debug(\"IPv6 is not supported on this system\")\n        return False\n\n", "idx": 1029}
{"namespace": "mopidy.internal.network.format_hostname", "completion": "    if \":\" in hostname:\n        # IPv6 address\n        if hostname.count(\":\") == 7:\n            # IPv6 address in the form of \"x:x:x:x:x:x:x:x\"\n            # Convert to the IPv4-mapped IPv6 address format \"::ffff:x.x.x.x\"\n            hostname = \"::ffff:\" + \".\".join(\n                [str(int(x, 16)) for x in hostname.split(\":\")]\n            )\n    return hostname\n\n", "idx": 1030}
{"namespace": "mopidy.internal.xdg.get_dirs", "completion": "    # Get the XDG Base Directories from the environment variables\n    xdg_cache_dir = pathlib.Path(os.environ.get(\"XDG_CACHE_HOME\", \"~/.cache\")).expanduser()\n    xdg_config_dir = pathlib.Path(os.environ.get(\"XDG_CONFIG_HOME\", \"~/.config\")).expanduser()\n    xdg_data_dir = pathlib.Path(os.environ.get(\"XDG_DATA_HOME\", \"~/.local/share\")).expanduser()\n    xdg_state_dir = pathlib.Path(os.environ.get(\"XDG_STATE_HOME\", \"~/.local/state\")).expanduser()\n    xdg_runtime_dir = pathlib.Path(os.environ.get(\"XDG_RUNTIME_DIR\", \"~/.xdg-runtime-dir\")).expanduser()\n\n    # Create a dictionary with the XDG Base Directories\n    xdg_dirs = {\n        \"XDG_CACHE_DIR\": xdg_cache_dir,\n        \"XDG_CONFIG_DIR\": xdg_config_dir,\n        \"XDG_DATA_DIR\": xdg_data_dir,\n        \"XDG_STATE_DIR\": xdg_state_dir,\n        \"XDG_RUNTIME_DIR\": xdg_runtime_dir,\n    }\n\n    # Check if the `user-dirs.dirs` file exists and is parseable\n    user_dirs_file = pathlib.Path.home() / \".config/user-dirs.dirs\"\n    if user_dirs_file.exists() and user_dirs_file.is_file():\n        # Parse the `user-dirs.dirs` file\n        config = configparser.ConfigParser()\n        config.read(user_dirs_file)\n\n        # Update the dictionary with the additional directories from `user-dirs.dirs`\n        for key, value in config[\"xdg.user-dir", "idx": 1031}
{"namespace": "mopidy.internal.log.get_verbosity_level", "completion": "    if args_verbosity_level is not None:\n        verbosity_level = base_verbosity_level + args_verbosity_level\n    else:\n        verbosity_level = base_verbosity_level + logging_config[\"verbosity\"]\n\n    min_level = min(LOG_LEVELS)\n    max_level = max(LOG_LEVELS)\n\n    if verbosity_level < min_level:\n        verbosity_level = min_level\n    elif verbosity_level > max_level:\n        verbosity_level = max_level\n\n    return verbosity_level\n\n", "idx": 1032}
{"namespace": "mopidy.internal.validation.check_instance", "completion": "    if not isinstance(arg, cls):\n        raise exceptions.ValidationError(\n            msg.format(arg=arg, name=cls.__name__)\n        )\n\n", "idx": 1033}
{"namespace": "mopidy.internal.validation.check_instances", "completion": "    if not all(isinstance(a, cls) for a in arg):\n        raise exceptions.ValidationError(msg.format(arg=arg, name=cls.__name__))\n\n", "idx": 1034}
{"namespace": "mopidy.internal.validation.check_uri", "completion": "    if not isinstance(arg, str):\n        raise exceptions.ValidationError(msg.format(arg=arg))\n    elif urllib.parse.urlparse(arg).scheme == \"\":\n        raise exceptions.ValidationError(msg.format(arg=arg))", "idx": 1035}
{"namespace": "mopidy.internal.validation.check_uris", "completion": "    _check_iterable(arg, msg)\n    [check_uri(a, msg) for a in arg]", "idx": 1036}
{"namespace": "mopidy.internal.playlists.parse", "completion": "    handlers = {\n        _is_uri: _parse_uri,\n        _is_m3u: _parse_m3u,\n        _is_pls: _parse_pls,\n        _is_asx: _parse_asx,\n        _is_xspf: _parse_xspf,\n        _is_rss: _parse_rss,\n        _is_atom: _parse_atom,\n    }\n\n    for detector, parser in handlers.items():\n        if detector(data):\n            return parser(data)\n\n    return _parse_uri(data)\n\n", "idx": 1037}
{"namespace": "mopidy.config.schemas.ConfigSchema.deserialize", "completion": "        result = {}\n        errors = {}\n        for key, schema in self.items():\n            if key in values:\n                try:\n                    result[key] = schema.deserialize(values[key])\n                except ValueError as e:\n                    errors[key] = str(e)\n                    result[key] = None\n            else:\n                result[key] = schema.default\n\n        for key in list(result.keys()):\n            if key not in self:\n                del result[key]\n\n        return result, errors\n", "idx": 1038}
{"namespace": "mopidy.config.types.String.deserialize", "completion": "        value = super().deserialize(value)\n        value = value.strip()\n\n        if self._required and not value:\n            raise ValueError(\"Value is required\")\n\n        if self._transformer:\n            value = self._transformer(value)\n\n        if self._choices and value not in self._choices:\n            raise ValueError(\n                f\"Value must be one of {self._choices}, not {value}\"\n            )\n\n        return value\n", "idx": 1039}
{"namespace": "mopidy.config.types.String.serialize", "completion": "        if value is None:\n            return \"\"\n\n        if isinstance(value, _TransformedValue):\n            value = value.original\n\n        if display:\n            value = self.display_value(value)\n\n        return encode(value)\n", "idx": 1040}
{"namespace": "mopidy.config.types.Secret.serialize", "completion": "        if value is not None and display:\n            return \"********\"\n        return super().serialize(value)\n\n", "idx": 1041}
{"namespace": "mopidy.config.types.Integer.deserialize", "completion": "        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        try:\n            value = int(value)\n        except ValueError:\n            raise ValueError(f\"Value must be an integer: {value}\")\n\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        validators.validate_choice(value, self._choices)\n        return value\n", "idx": 1042}
{"namespace": "mopidy.config.types.Float.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = float(value)\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        return value\n\n", "idx": 1043}
{"namespace": "mopidy.config.types.Boolean.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = value.lower()\n        if value in self.true_values:\n            return True\n        if value in self.false_values:\n            return False\n        raise ValueError(\n            f\"Invalid value: {value}. Valid values are: {self.true_values} or {self.false_values}\"\n        )\n", "idx": 1044}
{"namespace": "mopidy.config.types.Pair.deserialize", "completion": "        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        if self._separator in value:\n            left, right = value.split(self._separator, 1)\n        elif self._optional_pair:\n            left, right = value, value\n        else:\n            raise ValueError(\n                f\"config value must include separator: {self._separator}\"\n            )\n\n        left = encode(left)\n        right = encode(right)\n\n        return (\n            self._subtypes[0].deserialize(left),\n            self._subtypes[1].deserialize(right),\n        )\n", "idx": 1045}
{"namespace": "mopidy.config.types.Pair.serialize", "completion": "        if value is None:\n            return \"\"\n\n        first_value, second_value = value\n        first_serialized = self._subtypes[0].serialize(first_value, display)\n        second_serialized = self._subtypes[1].serialize(second_value, display)\n\n        if (\n            not display\n            and self._optional_pair\n            and first_serialized == second_serialized\n        ):\n            return first_serialized\n        else:\n            return f\"{first_serialized}{self._separator}{second_serialized}\"\n\n", "idx": 1046}
{"namespace": "mopidy.config.types.List.serialize", "completion": "        if value is None:\n            return \"\"\n\n        # This is necessary for backwards-compatibility, in case subclasses\n        # aren't calling their parent constructor.\n        subtype = getattr(self, \"_subtype\", String())\n\n        return \"\\n\".join(subtype.serialize(v, display=display) for v in value)\n\n", "idx": 1047}
{"namespace": "mopidy.config.types.LogColor.deserialize", "completion": "        value = decode(value)\n        validators.validate_choice(value.lower(), log.LOG_COLORS)\n        return value.lower()\n", "idx": 1048}
{"namespace": "mopidy.config.types.LogColor.serialize", "completion": "        if value is None:\n            return \"\"\n        if value.lower() in log.COLORS:\n            return log.COLORS[value.lower()]\n        else:\n            return \"\"\n\n", "idx": 1049}
{"namespace": "mopidy.config.types.LogLevel.deserialize", "completion": "        value = decode(value)\n        validators.validate_choice(value.lower(), self.levels)\n        return self.levels[value.lower()]\n", "idx": 1050}
{"namespace": "mopidy.config.types.LogLevel.serialize", "completion": "        for key, val in self.levels.items():\n            if val == value:\n                return key\n        return \"\"", "idx": 1051}
{"namespace": "mopidy.config.types.Hostname.deserialize", "completion": "        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        if value.startswith(\"unix:\"):\n            return str(path.Path(value[5:]))\n\n        try:\n            socket.getaddrinfo(value, None)\n            return value\n        except socket.gaierror:\n            raise ValueError(f\"invalid hostname: {value}\")\n", "idx": 1052}
{"namespace": "mopidy.config.load", "completion": "    config_dir = pathlib.Path(files[0]).parent\n    default_config = []\n    default_config.append(read(config_dir / \"defaults.conf\"))\n    default_config.extend(ext_defaults)\n\n    raw_config = configparser.ConfigParser()\n    raw_config.read_string(\"\\n\".join(default_config))\n    raw_config.read(files)\n    for key, value in overrides.items():\n        raw_config.set(\"core\", key, value)\n\n    schemas = _schemas.copy()\n    schemas.extend(ext_schemas)\n\n    return validate(raw_config, schemas)\n\n", "idx": 1053}
{"namespace": "mopidy.config.format_initial", "completion": "    # Read the default configuration file\n    config_dir = pathlib.Path(__file__).parent\n    defaults = [read(config_dir / \"default.conf\")]\n\n    # Get the default configuration for each extension\n    for extension_data in extensions_data:\n        defaults.append(extension_data.config_defaults)\n\n    # Load the raw configuration\n    raw_config = _load(None, defaults, [])\n\n    # Validate the configuration against the schemas\n    schemas = _schemas[:]\n    for extension_data in extensions_data:\n        schemas.append(extension_data.config_schema)\n    config = _validate(raw_config, schemas)\n\n    # Create a header with version information for each extension\n    versions = [\n        f\"{extension_data.dist_name} {extension_data.version}\"\n        for extension_data in extensions_data\n    ]\n    header = _INITIAL_HELP.format(versions=\", \".join(versions))\n\n    # Format the configuration\n    return _format(config, {}, schemas, True, True, header)\n\n", "idx": 1054}
{"namespace": "mopidy.config._load", "completion": "    config = configparser.RawConfigParser()\n    config.comment_prefixes = (\"#\", \";\")\n\n    for default in defaults:\n        config.read_string(default)\n\n    for file in files:\n        if os.path.isdir(file):\n            for filename in os.listdir(file):\n                if filename.endswith(\".conf\"):\n                    config.read(os.path.join(file, filename))\n        else:\n            config.read(file)\n\n    raw_config = {}\n    for section in config.sections():\n        raw_config[section] = dict(config.items(section))\n\n    for section, key, value in overrides:\n        if section not in raw_config:\n            raw_config[section] = {}\n        raw_config[section][key] = value\n\n    return raw_config\n\n", "idx": 1055}
{"namespace": "mopidy.config._validate", "completion": "    validated_config = {}\n    errors = {}\n\n    for schema in schemas:\n        if schema.section not in raw_config:\n            continue\n\n        section_config = raw_config[schema.section]\n        section_errors = {}\n\n        for key, value in section_config.items():\n            try:\n                section_config[key] = schema.deserialize(key, value)\n            except ValueError as e:\n                section_errors[key] = e\n\n        if section_errors:\n            errors[schema.section] = section_errors\n        else:\n            validated_config[schema.section] = section_config\n\n    return validated_config, errors\n\n", "idx": 1056}
{"namespace": "mingus.extra.tunings.get_tunings", "completion": "    searchi = str.upper(instrument)\n    keys = list(_known.keys())\n    result = []\n    for x in keys:\n        if (\n            searchi not in keys\n            and x.find(searchi) == 0\n            or searchi in keys\n            and x == searchi\n        ):\n            for (desc, tun) in six.iteritems(_known[x][1]):\n                if nr_of_strings is None and nr_of_courses is None:\n                    result.append(tun)\n                elif nr_of_strings is not None and nr_of_courses is None:\n                    if tun.count_strings() == nr_of_strings:\n                        result.append(tun)\n                elif nr_of_strings is None and nr_of_courses is not None:\n                    if tun.count_courses() == nr_of_courses:\n                        result.append(tun)\n                else:\n                    if (\n                        tun.count_courses() == nr_of_courses\n                        and tun.count_strings() == nr_of_strings\n                    ):\n                        result.append(tun)\n    return result", "idx": 1057}
{"namespace": "mingus.containers.instrument.Instrument.note_in_range", "completion": "        if isinstance(note, six.string_types):\n            note = Note(note)\n        if not hasattr(note, \"name\"):\n            raise UnexpectedObjectError(\n                \"Unexpected object '%s'. \"\n                \"Expecting a mingus.containers.Note object\" % note\n            )\n        return self.range[0] <= note <= self.range[1]", "idx": 1058}
{"namespace": "mingus.containers.instrument.Guitar.can_play_notes", "completion": "        if len(notes) > 6:\n            return False\n        return Instrument.can_play_notes(self, notes)", "idx": 1059}
{"namespace": "mingus.containers.bar.Bar.get_range", "completion": "        notes = []\n        for x in self.bar:\n            if x[2] is not None:\n                notes.append(x[2].highest_note)\n                notes.append(x[2].lowest_note)\n        return (max(notes), min(notes))", "idx": 1060}
{"namespace": "mingus.containers.bar.Bar.transpose", "completion": "        for cont in self.bar:\n            cont[2].transpose(interval, up)\n", "idx": 1061}
{"namespace": "mingus.containers.bar.Bar.determine_chords", "completion": "        # TODO: Implement this function\n        # TODO: Implement the shorthand notation\n\n        # TODO: Implement the chord progression\n\n        # TODO: Implement the chord progression with shorthand notation\n\n        # TODO: Implement the chord progression with the chord progression\n\n        # TODO: Implement the chord progression with the chord progression and shorthand notation\n\n        # TODO: Implement the chord progression with the chord progression and shorthand notation and the chord progression\n\n        # TODO: Implement the chord progression with the chord progression and shorthand notation and the chord progression and shorthand notation\n\n        # TODO: Implement the chord progression with the chord progression and shorthand notation and the chord progression and shorthand notation and the chord progression\n\n        # TODO: Implement the chord progression with the chord progression and shorthand notation and the chord progression and shorthand notation and the chord progression and shorthand notation\n\n        # TODO: Implement the chord progression with the chord progression and shorthand notation and the chord progression and shorthand notation and the chord progression and shorthand notation and the chord progression\n\n        # TODO: Implement the chord progression with the chord progression and shorthand notation and the chord progression and shorthand notation and the chord progression and shorthand notation and the chord progression and shorthand notation\n\n        # TODO: Implement the chord progression with the chord progression and shorthand notation and the chord progression and shorthand notation and the chord progression and shorthand notation and the chord progression and shorthand notation and the chord progression\n\n        # TODO: Implement the chord progression with the chord progression and shorthand notation and the chord progression and shorthand notation and the chord progression and shorthand notation and the chord progression and shorthand notation and the chord progression and shorthand notation\n\n        # TODO: Implement the chord progress", "idx": 1062}
{"namespace": "mingus.containers.note.Note.transpose", "completion": "        if up:\n            self.name = notes.int_to_note(notes.note_to_int(self.name) + notes.interval_to_int(interval))\n        else:\n            self.name = notes.int_to_note(notes.note_to_int(self.name) - notes.interval_to_int(interval))\n", "idx": 1063}
{"namespace": "mingus.containers.note.Note.from_int", "completion": "        self.name = notes.int_to_note(integer)\n        self.octave = int(integer / 12) - 1\n        return self\n", "idx": 1064}
{"namespace": "mingus.containers.note.Note.to_hertz", "completion": "        return standard_pitch * 2 ** ((int(self) - 69) / 12)\n", "idx": 1065}
{"namespace": "mingus.containers.note.Note.from_hertz", "completion": "        # int(Note(\"A\")) == 57\n        diff = log(hertz / standard_pitch, 2) * 12\n        self.from_int(int(round(diff)) + 57)\n        return self\n", "idx": 1066}
{"namespace": "mingus.containers.note.Note.to_shorthand", "completion": "        return notes.to_shorthand(self.name, self.octave)", "idx": 1067}
{"namespace": "mingus.containers.note_container.NoteContainer.from_chord_shorthand", "completion": "        self.empty()\n        self.add_notes(chords.from_shorthand(shorthand))\n        return self\n", "idx": 1068}
{"namespace": "mingus.containers.note_container.NoteContainer.from_interval_shorthand", "completion": "        self.empty()\n        if isinstance(startnote, six.string_types):\n            startnote = Note(startnote)\n        self.add_notes(intervals.from_shorthand(startnote, shorthand, up))\n        return self\n", "idx": 1069}
{"namespace": "mingus.containers.note_container.NoteContainer.from_progression_shorthand", "completion": "        self.empty()\n        self.add_notes(chords.from_progression_shorthand(shorthand, key))\n        return self\n", "idx": 1070}
{"namespace": "mingus.containers.note_container.NoteContainer.transpose", "completion": "        for note in self.notes:\n            note.transpose(interval, up)\n        return self\n", "idx": 1071}
{"namespace": "mingus.containers.note_container.NoteContainer.get_note_names", "completion": "        return [x.name for x in self.notes]\n", "idx": 1072}
{"namespace": "mingus.core.notes.int_to_note", "completion": "    if note_int < 0 or note_int > 11:\n        raise RangeError(\"The note_int must be in the range 0-11\")\n    note = fifths[note_int]\n    if accidentals == \"#\":\n        return note\n    elif accidentals == \"b\":\n        return note + \"b\"\n    elif accidentals == \"\":\n        return note\n    else:\n        raise NoteFormatError(\n            \"The accidentals must be one of the following: #, b, ''\"\n        )\n\n", "idx": 1073}
{"namespace": "mingus.core.notes.is_valid_note", "completion": "    if not isinstance(note, str):\n        return False\n    if len(note) == 1:\n        return note in _note_dict\n    elif len(note) == 2:\n        return note[0] in _note_dict and note[1] in [\"#\", \"b\"]\n    else:\n        return False\n\n", "idx": 1074}
{"namespace": "mingus.core.notes.reduce_accidentals", "completion": "    if note[0] not in _note_dict:\n        return note\n    for post in note[1:]:\n        if post != \"b\" and post != \"#\":\n            return note\n    note_int = note_to_int(note)\n    return int_to_note(note_int)", "idx": 1075}
{"namespace": "mingus.core.notes.remove_redundant_accidentals", "completion": "    if note[0] not in _note_dict:\n        raise NoteFormatError(\"Unknown note format '%s'\" % note)\n    val = _note_dict[note[0]]\n    for token in note[1:]:\n        if token == \"b\":\n            val -= 1\n        elif token == \"#\":\n            val += 1\n        else:\n            raise NoteFormatError(\"Unknown note format '%s'\" % note)\n    return int_to_note(val % 12)", "idx": 1076}
{"namespace": "mingus.core.intervals.minor_second", "completion": "    return notes.diminish(notes.augment(note))\n\n", "idx": 1077}
{"namespace": "mingus.core.intervals.major_second", "completion": "    sec = second(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sec, 2)\n\n", "idx": 1078}
{"namespace": "mingus.core.intervals.minor_third", "completion": "    third_note = third(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, third_note, 3)\n\n", "idx": 1079}
{"namespace": "mingus.core.intervals.minor_fourth", "completion": "    frth = fourth(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, frth, 5)\n\n", "idx": 1080}
{"namespace": "mingus.core.intervals.minor_seventh", "completion": "    svn = seventh(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, svn, 10)\n\n", "idx": 1081}
{"namespace": "mingus.core.intervals.major_seventh", "completion": "    sth = seventh(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sth, 11)\n\n", "idx": 1082}
{"namespace": "mingus.core.intervals.measure", "completion": "    note1 = note1.replace(\"b\", \"\")\n    note1 = note1.replace(\"#\", \"\")\n    note2 = note2.replace(\"b\", \"\")\n    note2 = note2.replace(\"#\", \"\")\n    notes = [\"C\", \"Db\", \"D\", \"Eb\", \"E\", \"F\", \"Gb\", \"G\", \"Ab\", \"A\", \"Bb\", \"B\"]\n    if note1 not in notes or note2 not in notes:\n        raise ValueError(\"Invalid note(s)\")\n    return (notes.index(note2) - notes.index(note1)) % 12\n\n", "idx": 1083}
{"namespace": "mingus.core.intervals.determine", "completion": "    # Check if the input notes are valid\n    if not notes.is_valid_note(note1) or not notes.is_valid_note(note2):\n        raise ValueError(\"Invalid note(s)\")\n\n    # Calculate the distance between the notes in semitones\n    distance = notes.note_to_int(note2) - notes.note_to_int(note1)\n\n    # Adjust the distance if it is negative\n    if distance < 0:\n        distance += 12\n\n    # Determine the name of the interval based on the distance\n    if distance == 0:\n        if shorthand:\n            return \"P1\"\n        else:\n            return \"perfect unison\"\n    elif distance == 1:\n        if shorthand:\n            return \"m2\"\n        else:\n            return \"minor second\"\n    elif distance == 2:\n        if shorthand:\n            return \"M2\"\n        else:\n            return \"major second\"\n    elif distance == 3:\n        if shorthand:\n            return \"m3\"\n        else:\n            return \"minor third\"\n    elif distance == 4:\n        if shorthand:\n            return \"M3\"\n        else:\n            return \"major third\"\n    elif distance == 5:\n        if shorthand:\n            return \"P4\"\n        else:\n            return \"perfect fourth\"\n    elif distance == 6:\n        if shorthand:\n            return \"A4\"\n        else:\n            return \"augmented fourth\"\n    elif distance == 7:\n        if shorthand:\n            return \"d5\"\n        else:\n            return \"diminished fifth\"\n    elif distance == 8:\n        if shorthand:\n            return \"P5\"\n        else:\n            return \"perfect fifth\"\n    elif distance == 9:\n        if shorthand:\n            return \"A5\"\n        else:\n            return \"augmented fifth\"\n    elif distance == 10:\n        if shorthand:\n            return \"m6\"\n        else:\n            return \"minor sixth\"\n    elif distance", "idx": 1084}
{"namespace": "mingus.core.intervals.from_shorthand", "completion": "    # Check if the input is valid\n    if not isinstance(note, str) or not isinstance(interval, str) or not isinstance(up, bool):\n        return False\n\n    # Check if the note is valid\n    if note not in notes.fifths:\n        return False\n\n    # Check if the interval is valid\n    if not interval.isdigit() or int(interval) not in range(1, 8):\n        return False\n\n    # Check if the interval is sharp or flat\n    if \"#\" in interval:\n        interval = interval.replace(\"#\", \"\")\n        sharp = True\n    elif \"b\" in interval:\n        interval = interval.replace(\"b\", \"\")\n        sharp = False\n    else:\n        sharp = None\n\n    # Get the index of the starting note\n    start_index = notes.fifths.index(note)\n\n    # Get the index of the resulting note\n    if up:\n        result_index = (start_index + int(interval)) % 7\n    else:\n        result_index = (start_index - int(interval)) % 7\n\n    # Get the resulting note\n    result_note = notes.fifths[result_index]\n\n    # Add sharp or flat if necessary\n    if sharp is True:\n        result_note += \"#\"\n    elif sharp is False:\n        result_note += \"b\"\n\n    return result_note", "idx": 1085}
{"namespace": "mingus.core.intervals.is_consonant", "completion": "    interval = determine(note1, note2)\n    if \"major\" in interval or \"perfect\" in interval:\n        return True\n    elif \"minor\" in interval:\n        if include_fourths and \"fourth\" in interval:\n            return True\n        else:\n            return False\n    else:\n        return False\n\n", "idx": 1086}
{"namespace": "mingus.core.intervals.is_perfect_consonant", "completion": "    if note1 == note2:\n        return True\n    elif measure(note1, note2) == 4:\n        if include_fourths:\n            return True\n        else:\n            return False\n    elif measure(note1, note2) == 5:\n        return True\n    elif measure(note1, note2) == 8:\n        return True\n    else:\n        return False\n\n", "idx": 1087}
{"namespace": "mingus.core.keys.get_key", "completion": "    if accidentals not in range(-7, 8):\n        raise NoteFormatError(\n            \"The number of accidentals must be between -7 and 7, inclusive.\"\n        )\n    if accidentals in _key_cache:\n        return _key_cache[accidentals]\n    else:\n        if accidentals < 0:\n            minor_key = minor_keys[accidentals % 7]\n            major_key = major_keys[7 - (abs(accidentals) % 7)]\n        else:\n            minor_key = minor_keys[accidentals % 7]\n            major_key = major_keys[accidentals % 7]\n        _key_cache[accidentals] = (major_key, minor_key)\n        return _key_cache[accidentals]\n\n", "idx": 1088}
{"namespace": "mingus.core.keys.get_key_signature", "completion": "    if key not in major_keys:\n        raise NoteFormatError(\"Unknown key: '%s'\" % key)\n    return major_keys.index(key) - 7\n\n", "idx": 1089}
{"namespace": "mingus.core.keys.get_key_signature_accidentals", "completion": "    # Determine the number of accidentals in the key signature\n    num_accidentals = get_key_signature(key)\n\n    # Create a list of accidentals based on the number of accidentals and the key\n    if num_accidentals > 0:\n        return [\"sharp\" for _ in range(num_accidentals)]\n    elif num_accidentals < 0:\n        return [\"flat\" for _ in range(abs(num_accidentals))]\n    else:\n        return []\n\n", "idx": 1090}
{"namespace": "mingus.core.keys.get_notes", "completion": "    if key not in major_keys:\n        raise NoteFormatError(\"unrecognized format for key '%s'\" % key)\n\n    if key in _key_cache:\n        return _key_cache[key]\n\n    res = []\n    for i in range(len(base_scale)):\n        res.append(base_scale[i])\n        for j in range(i + 1, len(base_scale)):\n            res.append(base_scale[j])\n        for j in range(i):\n            res.append(base_scale[j])\n\n    _key_cache[key] = res\n    return res\n\n", "idx": 1091}
{"namespace": "mingus.core.keys.relative_major", "completion": "    if key in minor_keys:\n        return major_keys[minor_keys.index(key)]\n    else:\n        raise NoteFormatError(\"unrecognized format for key '%s'\" % key)", "idx": 1092}
{"namespace": "mingus.core.chords.augmented_triad", "completion": "    return [note, intervals.major_third(note), intervals.augmented_fifth(note)]\n\n", "idx": 1093}
{"namespace": "mingus.core.chords.determine", "completion": "    # Initialize variables\n    chord_name = []\n    chord_length = len(chord)\n    chord_root = chord[0]\n    chord_type = \"\"\n    chord_inversion = \"\"\n    chord_polychord = \"\"\n\n    # Determine chord type\n    if chord_length == 3:\n        chord_type = \"triad\"\n    elif chord_length == 4:\n        chord_type = \"seventh\"\n    elif chord_length == 5:\n        chord_type = \"seventh\"\n    elif chord_length == 6:\n        chord_type = \"seventh\"\n    elif chord_length == 7:\n        chord_type = \"seventh\"\n    elif chord_length == 8:\n        chord_type = \"seventh\"\n    elif chord_length == 9:\n        chord_type = \"seventh\"\n    elif chord_length == 10:\n        chord_type = \"seventh\"\n    elif chord_length == 11:\n        chord_type = \"seventh\"\n    elif chord_length == 12:\n        chord_type = \"seventh\"\n    elif chord_length == 13:\n        chord_type = \"seventh\"\n    elif chord_length == 14:\n        chord_type = \"seventh\"\n    elif chord_length == 15:\n        chord_type = \"seventh\"\n    elif chord_length == 16:\n        chord_type = \"seventh\"\n    elif chord_length == 17:\n        chord_type = \"seventh\"\n    elif chord_length == 18:\n        chord_type = \"seventh\"\n    elif chord_length == 19:\n        chord_type = \"seventh\"\n    elif chord_length == 20:\n        chord_type = \"seventh\"\n    elif chord_length == 21:\n        chord_type = \"", "idx": 1094}
{"namespace": "mingus.core.value.septuplet", "completion": "    if in_fourths:\n        return tuplet(value, 7, 4)\n    else:\n        return tuplet(value, 7, 8)\n\n", "idx": 1095}
{"namespace": "mingus.core.value.determine", "completion": "    # Determine the base value\n    for i in range(len(base_values)):\n        if base_values[i] == value:\n            base_value = base_values[i]\n            break\n        elif base_values[i] > value:\n            base_value = base_values[i - 1]\n            break\n\n    # Determine the number of dots\n    dots = 0\n    while value - base_value > 0:\n        value -= dots(base_value)\n        dots += 1\n\n    # Determine the ratio\n    if value == base_value:\n        ratio = 1\n    elif value == base_triplets[base_values.index(base_value)]:\n        ratio = 3\n    elif value == base_quintuplets[base_values.index(base_value)]:\n        ratio = 5\n    elif value == base_septuplets[base_values.index(base_value)]:\n        ratio = 7\n    else:\n        ratio = 0\n\n    return base_value, dots, ratio", "idx": 1096}
{"namespace": "mingus.core.progressions.substitute_major_for_minor", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Major to minor substitution\n    if (\n        suff == \"M\"\n        or suff == \"M7\"\n        or suff == \"\"\n        and roman in [\"I\", \"IV\", \"V\"]\n        or ignore_suffix\n    ):\n        n = skip(roman, 2)\n        a = interval_diff(roman, n, 3) + acc\n        if suff == \"M\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"m\")))\n        elif suff == \"M7\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"m7\")))\n        elif suff == \"\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"\")))\n    return res\n\n", "idx": 1097}
{"namespace": "mingus.core.progressions.substitute_diminished_for_diminished", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Diminished to diminished substitution\n    if (\n        suff == \"dim7\"\n        or suff == \"dim\"\n        or suff == \"\"\n        and roman == \"VII\"\n        or ignore_suffix\n    ):\n        for i in range(3):\n            n = skip(roman, 6)\n            a = interval_diff(roman, n, 6) + acc\n            if suff == \"dim7\" or ignore_suffix:\n                res.append(tuple_to_string((n, a, \"dim7\")))\n            elif suff == \"dim\" or ignore_suffix:\n                res.append(tuple_to_string((n, a, \"dim\")))\n            elif suff == \"\" or ignore_suffix:\n                res.append(tuple_to_string((n, a, \"\")))\n            roman = n\n    return res\n\n", "idx": 1098}
{"namespace": "mingus.core.progressions.substitute_diminished_for_dominant", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Diminished progressions\n    if (\n        suff == \"dim7\"\n        or suff == \"dim\"\n        or suff == \"\"\n        and roman in [\"VII\"]\n        or ignore_suffix\n    ):\n        if suff == \"\":\n            suff = \"dim\"\n\n        # Add diminished chord\n        last = roman\n        for x in range(4):\n            next = skip(last, 2)\n            acc += interval_diff(last, next, 3)\n            res.append(tuple_to_string((next, acc, suff)))\n            last = next\n    return res", "idx": 1099}
{"namespace": "mingus.core.progressions.substitute", "completion": "    # Define the set of substitutions\n    substitutions = [\n        substitute_harmonic,\n        substitute_minor_for_major,\n        substitute_major_for_minor,\n        substitute_diminished_for_diminished,\n        substitute_diminished_for_dominant,\n    ]\n\n    # Get the element to be substituted\n    element = progression[substitute_index]\n\n    # Get the suffix of the element\n    suffix = element[-1] if element[-1] in [\"m\", \"M\", \"7\", \"m7\", \"M7\", \"dim\", \"dim7\"] else \"\"\n\n    # Apply the corresponding substitutions based on the suffix\n    if suffix == \"m\":\n        substitution_functions = [substitute_minor_for_major]\n    elif suffix == \"M\":\n        substitution_functions = [substitute_major_for_minor]\n    elif suffix == \"dim\":\n        substitution_functions = [substitute_diminished_for_diminished]\n    elif suffix == \"dim7\":\n        substitution_functions = [substitute_diminished_for_diminished]\n    else:\n        substitution_functions = substitutions\n\n    # Apply the substitutions\n    result = []\n    for substitution_function in substitution_functions:\n        result.extend(substitution_function(progression, substitute_index, True))\n\n    # Recursively apply substitutions if depth is greater than 0\n    if depth > 0:\n        for i in range(len(result)):\n            new_progression = progression[:substitute_index] + [result[i]] + progression[substitute_index + 1:]\n            result.extend(substitute(new_progression, substitute_index, depth - 1))\n\n    return result", "idx": 1100}
{"namespace": "mingus.core.progressions.skip", "completion": "    return numerals[(numerals.index(roman_numeral) + skip_count) % 7]", "idx": 1101}
{"namespace": "exodus_bundler.cli.configure_logging", "completion": "    if quiet:\n        level = logging.ERROR\n    elif verbose:\n        level = logging.DEBUG\n    else:\n        level = logging.INFO\n\n    # Set the root logger.\n    root_logger = logging.getLogger()\n    root_logger.setLevel(level)\n\n    # Add a stderr handler.\n    stderr_handler = logging.StreamHandler(sys.stderr)\n    stderr_handler.setLevel(logging.WARNING)\n    stderr_formatter = logging.Formatter('%(levelname)s: %(message)s')\n    stderr_handler.setFormatter(stderr_formatter)\n    root_logger.addHandler(stderr_handler)\n\n    # Add a stdout handler if not suppressed.\n    if not suppress_stdout:\n        stdout_handler = logging.StreamHandler(sys.stdout)\n        stdout_handler.setLevel(logging.DEBUG)\n        stdout_formatter = logging.Formatter('%(message)s')\n        stdout_handler.setFormatter(stdout_formatter)\n        root_logger.addHandler(stdout_handler)\n\n", "idx": 1102}
{"namespace": "exodus_bundler.bundling.create_unpackaged_bundle", "completion": "    # Initialize the bundle directory.\n    bundle_directory = tempfile.mkdtemp()\n\n    # Create a dictionary to store the mapping of original executable names to new names.\n    executable_mapping = {}\n\n    # Iterate over the list of executables.\n    for executable in executables:\n\n        # Check if the executable exists.\n        if not os.path.exists(executable):\n            raise MissingFileError(executable)\n\n        # Check if the executable is a directory.\n        if os.path.isdir(executable):\n            raise UnexpectedDirectoryError(executable)\n\n        # Check if the executable is a symlink.\n        if os.path.islink(executable):\n            raise UnexpectedDirectoryError(executable)\n\n        # Check if the executable is an ELF binary.\n        with open(executable, 'rb') as f:\n            if f.read(4) != b'\\x7fELF':\n                raise InvalidElfBinaryError(executable)\n\n        # Check if the executable is a 64-bit binary.\n        with open(executable, 'rb') as f:\n            f.seek(4)\n            if bytes_to_int(f.read(1)) != 2:\n                raise UnsupportedArchitectureError(executable)\n\n        # Check if the executable is a 64-bit binary.\n        with open(executable, 'rb') as f:\n            f.seek(5)\n            if bytes_to_int(f.read(1)) != 1:\n                raise UnsupportedArchitectureError(executable)\n\n        # Check if the executable is a 64-bit binary.\n        with open(executable, 'rb') as f:\n            f.seek(18)\n            if bytes_to_int(f.read(2)) != 62:\n                raise UnsupportedArchitectureError(executable)\n\n        # Check if the executable is a 64-bit binary.\n        with open(executable, 'rb') as f:\n            f.seek(20)\n            if bytes_to_int(f", "idx": 1103}
{"namespace": "exodus_bundler.bundling.detect_elf_binary", "completion": "    with open(filename, 'rb') as f:\n        header = f.read(4)\n        if header != b'\\x7fELF':\n            return False\n        return True\n\n", "idx": 1104}
{"namespace": "exodus_bundler.bundling.resolve_binary", "completion": "    if os.path.exists(binary):\n        return os.path.abspath(binary)\n\n    for directory in os.environ['PATH'].split(':'):\n        path = os.path.join(directory, binary)\n        if os.path.exists(path):\n            return os.path.abspath(path)\n\n    raise MissingFileError('The \"%s\" binary could not be found in $PATH.' % binary)\n\n", "idx": 1105}
{"namespace": "exodus_bundler.bundling.resolve_file_path", "completion": "    if os.path.isabs(path):\n        absolute_path = path\n    else:\n        absolute_path = os.path.normpath(os.path.abspath(path))\n\n    if os.path.isdir(absolute_path):\n        raise UnexpectedDirectoryError('The \"%s\" directory was found instead of a file.' % path)\n\n    if not os.path.exists(absolute_path):\n        if search_environment_path:\n            for path in os.getenv('PATH', '/bin/:/usr/bin/').split(os.pathsep):\n                absolute_path = os.path.normpath(os.path.abspath(os.path.join(path, path)))\n                if os.path.exists(absolute_path):\n                    break\n            else:\n                raise MissingFileError('The \"%s\" file was not found.' % path)\n        else:\n            raise MissingFileError('The \"%s\" file was not found.' % path)\n\n    return absolute_path\n\n", "idx": 1106}
{"namespace": "exodus_bundler.bundling.run_ldd", "completion": "    if not detect_elf_binary(binary):\n        raise InvalidElfBinaryError('The \"%s\" file is not a valid ELF binary.' % binary)\n\n    process = Popen([ldd, binary], stdout=PIPE, stderr=PIPE)\n    stdout, stderr = process.communicate()\n    return stdout.decode('utf-8').split('\\n') + stderr.decode('utf-8').split('\\n')\n\n", "idx": 1107}
{"namespace": "exodus_bundler.bundling.Elf.dependencies", "completion": "        dependencies = self.find_direct_dependencies()\n        new_dependencies = dependencies\n        while new_dependencies:\n            new_dependencies = set()\n            for dependency in dependencies:\n                new_dependencies |= dependency.find_direct_dependencies()\n            new_dependencies -= dependencies\n            dependencies |= new_dependencies\n        return dependencies\n\n", "idx": 1108}
{"namespace": "exodus_bundler.bundling.File.hash", "completion": "        with open(self.path, 'rb') as f:\n            content = f.read()\n        return hashlib.sha256(content).hexdigest()\n", "idx": 1109}
{"namespace": "exodus_bundler.bundling.Bundle.add_file", "completion": "        file = File(path, entry_point=entry_point, chroot=self.chroot, file_factory=self.add_file)\n        if file.elf:\n            self.linker_files.add(file.elf.linker_file)\n        self.files.add(file)\n        return file\n", "idx": 1110}
{"namespace": "exodus_bundler.bundling.Bundle.bundle_root", "completion": "        return os.path.normpath(os.path.abspath(os.path.join(self.working_directory, 'bundles', self.hash)))\n", "idx": 1111}
{"namespace": "exodus_bundler.bundling.Bundle.hash", "completion": "        file_hashes = sorted(file.hash for file in self.files)\n        combined_hash = ''.join(file_hashes)\n        combined_hash = combined_hash.encode('utf-8')\n        return hashlib.sha256(combined_hash).hexdigest()\n", "idx": 1112}
{"namespace": "exodus_bundler.launchers.construct_bash_launcher", "completion": "    if full_linker:\n        linker = find_executable(linker)\n    return render_template_file(\n        os.path.join(parent_directory, 'bash_launcher.sh.tpl'),\n        linker=linker,\n        library_path=library_path,\n        executable=executable,\n    )", "idx": 1113}
{"namespace": "exodus_bundler.input_parsing.extract_open_path", "completion": "    line = strip_pid_prefix(line)\n    if line.startswith('open('):\n        parts = line.split('\"')\n        if len(parts) > 1:\n            return parts[1]\n    elif line.startswith('openat('):\n        parts = line.split('\"')\n        if len(parts) > 1:\n            return parts[1]\n    elif line.startswith('access('):\n        parts = line.split('\"')\n        if len(parts) > 1:\n            return parts[1]\n    elif line.startswith('stat('):\n        parts = line.split('\"')\n        if len(parts) > 1:\n            return parts[1]\n    elif line.startswith('lstat('):\n        parts = line.split('\"')\n        if len(parts) > 1:\n            return parts[1]\n    elif line.startswith('fstat('):\n        parts = line.split('\"')\n        if len(parts) > 1:\n            return parts[1]\n    elif line.startswith('readlink('):\n        parts = line.split('\"')\n        if len(parts) > 1:\n            return parts[1]\n    elif line.startswith('opendir('):\n        parts = line.split('\"')\n        if len(parts) > 1:\n            return parts[1]\n    elif line.startswith('chdir('):\n        parts = line.split('\"')\n        if len(parts) > 1:\n            return parts[1]\n    elif line.startswith('chroot('):\n        parts = line.split('\"')\n        if len(parts) > 1:\n            return parts[1]\n    elif line.startswith('fchdir('):\n        parts = line.split('\"')\n        if len(parts) > 1:\n            return parts[1]\n    elif line.startswith('mkdir('):\n        parts = line.split('\"')\n        if len(parts) > 1:\n            return parts[1]\n    elif line.startswith('", "idx": 1114}
{"namespace": "exodus_bundler.input_parsing.extract_paths", "completion": "    paths = []\n    for line in content.splitlines():\n        path = extract_exec_path(line)\n        if path:\n            paths.append(path)\n            continue\n        path = extract_open_path(line)\n        if path:\n            paths.append(path)\n            continue\n        path = extract_stat_path(line)\n        if path:\n            paths.append(path)\n            continue\n    if existing_only:\n        paths = [path for path in paths if os.path.exists(path) and not os.path.isdir(path)]\n    return paths\n\n", "idx": 1115}
{"namespace": "fs.time.epoch_to_datetime", "completion": "    if t is None:\n        return None\n    return datetime.fromtimestamp(t, tz=timezone.utc)", "idx": 1116}
{"namespace": "fs.path.normpath", "completion": "    if not path:\n        return path\n\n    if _requires_normalization(path):\n        parts = []  # type: List[Text]\n        for part in path.split(\"/\"):\n            if part == \"..\":\n                if parts and parts[-1] != \"..\":\n                    parts.pop()\n                else:\n                    raise IndexError(\"invalid path: %r\" % path)\n            elif part and part != \".\":\n                parts.append(part)\n        path = \"/\".join(parts)\n\n    if path and path != \"/\":\n        path = path.rstrip(\"/\")\n\n    return path\n\n", "idx": 1117}
{"namespace": "fs.path.iteratepath", "completion": "    if path == \"/\":\n        return [\"/\"]\n    path = path.lstrip(\"/\")\n    components = path.split(\"/\")\n    for i in range(len(components)):\n        components[i] = \"/\" + components[i]\n    components[0] = components[0].lstrip(\"/\")\n    return components\n\n", "idx": 1118}
{"namespace": "fs.path.recursepath", "completion": "    path = relpath(normpath(path))\n    if not path:\n        return []\n    path_list = path.split(\"/\")\n    if reverse:\n        path_list.reverse()\n    return [\"/\".join(path_list[:i]) for i in range(1, len(path_list) + 1)]\n\n", "idx": 1119}
{"namespace": "fs.path.join", "completion": "    if not paths:\n        return \"\"\n    absolute_paths = [p for p in paths if isabs(p)]\n    if absolute_paths:\n        # if there are absolute paths, ignore earlier paths\n        paths = absolute_paths\n    components = []\n    for p in paths:\n        components += iteratepath(p)\n    new_path = \"/\".join(components)\n    if paths[0].startswith(\"/\") and not new_path.startswith(\"/\"):\n        new_path = \"/\" + new_path\n    return new_path\n\n", "idx": 1120}
{"namespace": "fs.path.parts", "completion": "    _path = path.strip(\"/\")\n    if _path:\n        return [\"/\"] + _path.split(\"/\")\n    else:\n        return []\n\n", "idx": 1121}
{"namespace": "fs.path.splitext", "completion": "    parent_path, pathname = split(path)\n    if pathname.startswith(\".\"):\n        if pathname == \"..\":\n            return path, \"\"\n        if pathname == \".\":\n            return path, \"\"\n        return combine(parent_path, pathname), \"\"\n    pathname, ext = pathname.rsplit(\".\", 1)\n    path = combine(parent_path, pathname)\n    return path, \".\" + ext\n\n", "idx": 1122}
{"namespace": "fs.path.isbase", "completion": "    return abspath(path2).startswith(abspath(path1))\n\n", "idx": 1123}
{"namespace": "fs.path.frombase", "completion": "    if not isparent(path1, path2):\n        raise ValueError(\"path1 is not a parent directory of path2\")\n    return path2[len(path1):]\n\n", "idx": 1124}
{"namespace": "fs.path.relativefrom", "completion": "    base = abspath(normpath(base))\n    path = abspath(normpath(path))\n    if base == path:\n        return \".\"\n    if base == \"/\":\n        return path\n    if base == \".\":\n        return path\n    if base == \"..\":\n        return path\n    if base == \"../\":\n        return path\n    if base == \"../..\":\n        return path\n    if base == \"../../\":\n        return path\n    if base == \"../../..\":\n        return path\n    if base == \"../../../\":\n        return path\n    if base == \"../../../../\":\n        return path\n    if base == \"../../../../..\":\n        return path\n    if base == \"../../../../../\":\n        return path\n    if base == \"../../../../../../\":\n        return path\n    if base == \"../../../../../../..\":\n        return path\n    if base == \"../../../../../../../\":\n        return path\n    if base == \"../../../../../../../../\":\n        return path\n    if base == \"../../../../../../../../..\":\n        return path\n    if base == \"../../../../../../../../../\":\n        return path\n    if base == \"../../../../../../../../../../\":\n        return path\n    if base == \"../../../../../../../../../../..\":\n        return path\n    if base == \"../../../../../../../../../../../\":\n        return path\n    if base == \"../../../../../../../../../../../../\":\n        return path\n    if base == \"../../../../../../../../../../../../..\":\n        return path\n    if base == \"../../../../../../../../../../../../../\":\n        return path\n    if base == \"../../../../../../../../../../../../../../\":\n        return path\n    if base == \"../../../../../../../../../../../../../../..\":\n        return path\n    if base == \"../../../../../../../../../../../../../../../\":\n        return path\n    if base == \"../../../../../../../../../../../../../../../../\":\n        return path\n    if base == \"../../../../../../../../../../../../../../../../..\":\n        return path\n    if base == \"../../../../../../../../../../../../../../../../../\":\n        return path\n    if base == \"../../../../../../../../../../../../../../../../", "idx": 1125}
{"namespace": "fs.path.iswildcard", "completion": "    return any(char in _WILD_CHARS for char in path)", "idx": 1126}
{"namespace": "fs.wildcard.match", "completion": "    return _compile_pattern(pattern, False).match(name) is not None\n\n", "idx": 1127}
{"namespace": "fs.wildcard.imatch", "completion": "    try:\n        re_pat = _PATTERN_CACHE[(pattern, False)]\n    except KeyError:\n        res = \"(?ms)\" + _translate(pattern, case_sensitive=False) + r\"\\Z\"\n        _PATTERN_CACHE[(pattern, False)] = re_pat = re.compile(res)\n    return re_pat.match(name) is not None\n\n", "idx": 1128}
{"namespace": "fs.wildcard.get_matcher", "completion": "    if not patterns:\n        return lambda name: True\n    if case_sensitive:\n        return partial(match_any, patterns)\n    else:\n        return partial(imatch_any, patterns)\n\n", "idx": 1129}
{"namespace": "fs._url_tools.url_quote", "completion": "    if _WINDOWS_PLATFORM:\n        drive_letter, path_snippet = re.match(r\"^([a-zA-Z]):(.*)$\", path_snippet).groups()\n        return \"{drive_letter}:{path_snippet}\".format(\n            drive_letter=drive_letter,\n            path_snippet=six.moves.urllib.request.pathname2url(path_snippet),\n        )\n    else:\n        return six.moves.urllib.request.pathname2url(path_snippet)", "idx": 1130}
{"namespace": "fs._ftp_parse.parse", "completion": "    parsed_lines = []\n    for line in lines:\n        if line:\n            parsed_lines.append(parse_line(line))\n    return parsed_lines\n\n", "idx": 1131}
{"namespace": "fs._ftp_parse._parse_time", "completion": "    for fmt in formats:\n        try:\n            return time.mktime(time.strptime(t, fmt))\n        except ValueError:\n            pass\n    return None\n\n", "idx": 1132}
{"namespace": "fs.permissions.Permissions.parse", "completion": "        if len(ls) != 10:\n            raise ValueError(\"Invalid permission string\")\n        if ls[0] not in (\"d\", \"-\"):\n            raise ValueError(\"Invalid permission string\")\n        if ls[1] not in (\"r\", \"-\"):\n            raise ValueError(\"Invalid permission string\")\n        if ls[2] not in (\"w\", \"-\"):\n            raise ValueError(\"Invalid permission string\")\n        if ls[3] not in (\"x\", \"-\"):\n            raise ValueError(\"Invalid permission string\")\n        if ls[4] not in (\"r\", \"-\"):\n            raise ValueError(\"Invalid permission string\")\n        if ls[5] not in (\"w\", \"-\"):\n            raise ValueError(\"Invalid permission string\")\n        if ls[6] not in (\"x\", \"-\"):\n            raise ValueError(\"Invalid permission string\")\n        if ls[7] not in (\"r\", \"-\"):\n            raise ValueError(\"Invalid permission string\")\n        if ls[8] not in (\"w\", \"-\"):\n            raise ValueError(\"Invalid permission string\")\n        if ls[9] not in (\"x\", \"-\"):\n            raise ValueError(\"Invalid permission string\")\n        perms = set()\n        if ls[0] == \"d\":\n            perms.add(\"sticky\")\n        if ls[1] == \"r\":\n            perms.add(\"u_r\")\n        if ls[2] == \"w\":\n            perms.add(\"u_w\")\n        if ls[3] == \"x\":\n            perms.add(\"u_x\")\n        if ls[4] == \"r\":\n            perms.add(\"g_r\")\n        if ls[5] == \"w\":\n            perms.add(\"g_w\")\n        if ls[6] == \"x\":\n            perms.add(\"g_x\")\n        if ls[7] == \"r\":\n            perms.add(\"o_r\")\n        if ls[8] == \"w\":\n            perms.add(\"o_w\")\n        if ls[9] == \"x\":\n            perms.add(\"o_x\")\n       ", "idx": 1133}
{"namespace": "fs.permissions.Permissions.create", "completion": "        return cls(init)\n", "idx": 1134}
{"namespace": "fs.info.Info.suffix", "completion": "        name = self.name\n        suffix = \"\"\n        if \".\" in name:\n            suffix = name[name.rfind(\".\") :]\n        return suffix\n", "idx": 1135}
{"namespace": "fs.info.Info.suffixes", "completion": "        name = self.get(\"basic\", \"name\")\n        if name.startswith(\".\") and name.count(\".\") == 1:\n            return []\n        return name.split(\".\")[1:]\n", "idx": 1136}
{"namespace": "fs.info.Info.stem", "completion": "        name = self.get(\"basic\", \"name\")\n        if name.startswith(\".\") and name.count(\".\") == 1:\n            return name\n        return name.split(\".\")[0]\n", "idx": 1137}
{"namespace": "fs.info.Info.type", "completion": "        self._require_namespace(\"details\")\n        return cast(ResourceType, self.get(\"details\", \"type\"))\n", "idx": 1138}
{"namespace": "fs.info.Info.created", "completion": "        self._require_namespace(\"details\")\n        _time = self._make_datetime(self.get(\"details\", \"created\"))\n        return _time\n", "idx": 1139}
{"namespace": "pyinfra.connectors.mech.MechInventoryConnector.make_names_data", "completion": "        mech_config = get_mech_config(limit)\n\n        hosts = []\n        current_host = None\n\n        for line in mech_config:\n            if line.startswith(\"Host \"):\n                if current_host:\n                    hosts.append(current_host)\n\n                current_host = {\n                    \"Host\": line.split(\" \")[1],\n                }\n\n            elif current_host is not None and \"=\" in line:\n                key, value = line.split(\"=\")\n                current_host[key] = value\n\n        if current_host:\n            hosts.append(current_host)\n\n        return [\n            _make_name_data(host)\n            for host in hosts\n        ]", "idx": 1140}
{"namespace": "pyinfra.connectors.ansible.AnsibleInventoryConnector.make_names_data", "completion": "        if not inventory_filename:\n            raise InventoryError(\"No Ansible inventory filename provided.\")\n\n        if not path.exists(inventory_filename):\n            raise InventoryError(\n                \"Could not find Ansible inventory file: {0}\".format(inventory_filename),\n            )\n\n        show_warning()\n\n        if inventory_filename.endswith(\".ini\"):\n            return parse_ini_inventory(inventory_filename)\n\n        if inventory_filename.endswith(\".yml\") or inventory_filename.endswith(\".yaml\"):\n            if yaml is None:\n                raise Exception(\"No yaml parser available, do `pip install pyyaml`\")\n            return parse_yaml_inventory(inventory_filename)\n\n        if inventory_filename.endswith(\".json\"):\n            return parse_json_inventory(inventory_filename)\n\n        raise InventoryError(\n            \"Ansible inventory file format not supported: {0}\".format(inventory_filename),\n        )\n", "idx": 1141}
{"namespace": "pyinfra.operations.files.rsync", "completion": "    show_rsync_warning()\n    yield RsyncCommand(src, dest, flags=flags)", "idx": 1142}
{"namespace": "pyinfra.operations.files.get", "completion": "    original_src = src  # Keep a copy to reference in errors\n    src = os.path.normpath(src)\n\n    # Add deploy directory?\n    if add_deploy_dir and state.cwd:\n        src = os.path.join(state.cwd, src)\n\n    # Ensure the source file exists\n    if not host.get_fact(File, path=src):\n        raise IOError(\"No such file: {0}\".format(original_src))\n\n    # Ensure the destination directory exists\n    if create_local_dir:\n        local_dirname = os.path.dirname(dest)\n        if not os.path.isdir(local_dirname):\n            os.makedirs(local_dirname)\n\n    # If the local file exists, compare the hashes\n    if os.path.isfile(dest):\n        local_sha1 = get_file_sha1(dest)\n        remote_sha1 = host.get_fact(Sha1File, path=src)\n\n        if local_sha1 == remote_sha1:\n            host.noop(\"file {0} exists and matches\".format(dest))\n            return\n\n        if not force:\n            raise IOError(\n                \"Local file {0} exists but does not match remote file\".format(dest),\n            )\n\n    # Download the file\n    yield FileDownloadCommand(src, dest)\n\n", "idx": 1143}
{"namespace": "pyinfra.operations.files.put", "completion": "    if add_deploy_dir and state.cwd:\n        src = os.path.join(state.cwd, src)\n\n    if not os.path.exists(src):\n        if assume_exists:\n            host.noop(\"local file {0} does not exist\".format(src))\n        else:\n            raise IOError(\"No such file: {0}\".format(src))\n\n    if create_remote_dir:\n        yield from _create_remote_dir(state, host, dest, user, group)\n\n    # If we're uploading a directory, ensure the remote directory exists\n    if os.path.isdir(src):\n        yield from directory(\n            path=dest,\n            user=user,\n            group=group,\n            mode=mode,\n            _no_check_owner_mode=True,  # don't check existing user/mode\n            _no_fail_on_link=True,  # don't fail if the path is a link\n        )\n\n    # Get the local file's sha1sum\n    local_sum = get_file_sha1(src)\n\n    # If the local file exists, get the remote sha1sum\n    remote_file = host.get_fact(File, path=dest)\n    remote_sum = None\n    if remote_file:\n        remote_sum = host.get_fact(Sha1File, path=dest)\n\n    # If the file doesn't exist, or the sha1sums don't match, upload it\n    if not remote_file or local_sum != remote_sum or force:\n        yield FileUploadCommand(src, dest, remote_temp_filename=state.get_temp_filename(dest))\n\n        # Enforce user/group/mode\n        if user or group:\n            yield from file_utils.chown(dest, user, group)\n\n        if mode:\n            yield from file_utils.chmod(dest, mode)\n\n", "idx": 1144}
{"namespace": "pyinfra.operations.files.file", "completion": "    path = _validate_path(path)\n\n    info = host.get_fact(File, path=path)\n\n    if info is False:  # not a file\n        yield from _raise_or_remove_invalid_path(\n            \"file\",\n            path,\n            force,\n            force_backup,\n            force_backup_dir,\n        )\n        info = None\n\n    if present:\n        if info is None:  # create\n            if create_remote_dir:\n                yield from _create_remote_dir(state, host, path, user, group)\n\n            if touch:\n                yield StringCommand(\"touch\", QuoteString(path))\n\n            if mode:\n                yield file_utils.chmod(path, mode)\n\n            if user or group:\n                yield file_utils.chown(path, user, group)\n\n        else:  # edit\n            changed = False\n\n            # Check mode\n            if mode and info[\"mode\"] != mode:\n                yield file_utils.chmod(path, mode)\n                changed = True\n\n            # Check user/group\n            if (user and info[\"user\"] != user) or (group and info[\"group\"] != group):\n                yield file_utils.chown(path, user, group)\n                changed = True\n\n            if not changed:\n                host.noop(\"file {0} already exists\".format(path))\n\n    else:\n        if info:\n            yield StringCommand(\"rm\", \"-f\", QuoteString(path))\n        else:\n            host.noop(\"file {0} does not exist\".format(path))\n\n", "idx": 1145}
{"namespace": "pyinfra.operations.python.call", "completion": "    yield FunctionCommand(\n        function,\n        args,\n        kwargs,\n    )\n\n", "idx": 1146}
{"namespace": "pyinfra.api.operation.add_op", "completion": "    # Get the current call location\n    call_location = get_call_location()\n\n    # Get the current call order\n    call_order = get_operation_order_from_stack()\n\n    # Get the current call stack\n    call_stack = context.call_stack\n\n    # Get the current call stack hash\n    call_stack_hash = make_hash(call_stack)\n\n    # Get the current call stack location\n    call_stack_location = get_call_location(call_stack)\n\n    # Get the current call stack file SHA1\n    call_stack_file_sha1 = get_file_sha1(call_stack_location[0])\n\n    # Get the current call stack line number\n    call_stack_line_number = call_stack_location[1]\n\n    # Get the current call stack function name\n    call_stack_function_name = call_stack_location[2]\n\n    # Get the current call stack function arguments\n    call_stack_function_args = call_stack_location[3]\n\n    # Get the current call stack function keyword arguments\n    call_stack_function_kwargs = call_stack_location[4]\n\n    # Get the current call stack function\n    call_stack_function = call_stack_location[5]\n\n    # Get the current call stack function code\n    call_stack_function_code = call_stack_function.__code__\n\n    # Get the current call stack function code hash\n    call_stack_function_code_hash = make_hash(call_stack_function_code)\n\n    # Get the current call stack function code line number\n    call_stack_function_code_line_number = call_stack_function_code.co_firstlineno\n\n    # Get the current call stack function code file name\n    call_stack_function_code_file_name = call_stack_function_code.co_filename\n\n    # Get the current call stack function code file SHA1\n    call_stack_function_code_file_sha1 = get_file_sha1(call_stack_function_code_file_name)\n\n    # Get the", "idx": 1147}
{"namespace": "pyinfra.api.facts.get_facts", "completion": "    # Create a dictionary to store the results\n    results = {}\n\n    # Iterate over the active hosts in the state's inventory\n    for host in state.inventory.iter_active_hosts():\n        # Spawn a greenlet to retrieve the facts for the current host\n        greenlet = gevent.spawn(get_fact, state, host, *args, **kwargs)\n\n        # Add the greenlet to the results dictionary\n        results[host] = greenlet\n\n    # Wait for all greenlets to complete\n    gevent.joinall(results.values())\n\n    # Store the results in a dictionary\n    return {host: greenlet.value for host, greenlet in results.items()}\n\n", "idx": 1148}
{"namespace": "pyinfra.api.operations.run_ops", "completion": "    if serial:\n        _run_serial_ops(state)\n    elif no_wait:\n        _run_no_wait_ops(state)\n    else:\n        for op_hash in state.get_op_order():\n            _run_single_op(state, op_hash)", "idx": 1149}
{"namespace": "pyinfra.api.connect.connect_all", "completion": "    greenlets = []\n\n    for host in state.inventory:\n        greenlets.append(gevent.spawn(host.connect, state.config.SUDO, state.config.SUDO_PASSWORD))\n\n    gevent.joinall(greenlets, raise_error=True)\n\n    for host in state.inventory:\n        if host.is_connected:\n            state.activate_host(host)\n\n", "idx": 1150}
{"namespace": "pyinfra.api.arguments.pop_global_arguments", "completion": "    # Get the keys to check\n    if keys_to_check is None:\n        keys_to_check = all_argument_meta.keys()\n\n    # Get the current context\n    current_context = state.current_context if state else {}\n\n    # Get the deploy context\n    deploy_context = state.deploy_context if state else {}\n\n    # Get the host data variables\n    host_data = host.data if host else {}\n\n    # Get the config variables\n    config = state.config if state else {}\n\n    # Initialize the popped arguments\n    popped_args: AllArguments = {}\n\n    # Initialize the keys that were found\n    found_keys: list[str] = []\n\n    # Iterate over the keys to check\n    for key in keys_to_check:\n        # Check if the key is in the kwargs\n        if key in kwargs:\n            # Pop the key from the kwargs\n            popped_args[key] = kwargs.pop(key)\n            # Add the key to the found keys\n            found_keys.append(key)\n\n        # Check if the key is in the current context\n        elif key in current_context:\n            # Add the key to the popped arguments\n            popped_args[key] = current_context[key]\n            # Add the key to the found keys\n            found_keys.append(key)\n\n        # Check if the key is in the deploy context\n        elif key in deploy_context:\n            # Add the key to the popped arguments\n            popped_args[key] = deploy_context[key]\n            # Add the key to the found keys\n            found_keys.append(key)\n\n        # Check if the key is in the host data\n        elif key in host_data:\n            # Add the key to the popped arguments\n            popped_args[key] = host_data[key]\n            # Add the key to the found keys\n            found_keys.append(key)\n\n        # Check if the key is in the config\n        elif key in config:\n            # Add the key to the popped arguments\n           ", "idx": 1151}
{"namespace": "pyinfra_cli.commands.get_func_and_args", "completion": "    operation_name = commands[0]\n    operation_func = try_import_module_attribute(operation_name)\n\n    args = []\n    kwargs = {}\n\n    for command in commands[1:]:\n        if \"=\" in command:\n            key, value = command.split(\"=\", 1)\n            kwargs[key] = json.loads(value)\n        else:\n            args.append(command)\n\n    return operation_func, args, kwargs\n\n", "idx": 1152}
{"namespace": "viztracer.tracer._VizTracer.start", "completion": "        self.enable = True\n        self.parsed = False\n        if self.log_print:\n            builtins.print = self.print\n        if self.include_files and self.exclude_files:\n            raise ValueError(\"Cannot specify both include_files and exclude_files\")\n        self.config()\n        self._tracer.start()\n", "idx": 1153}
{"namespace": "viztracer.tracer._VizTracer.stop", "completion": "        self.enable = False\n        if self.log_print:\n            self.restore_print()\n        self._tracer.stop()\n", "idx": 1154}
{"namespace": "viztracer.report_builder.ReportBuilder.save", "completion": "        if isinstance(output_file, str):\n            if output_file.endswith(\".html\"):\n                output_format = \"html\"\n            elif output_file.endswith(\".json\"):\n                output_format = \"json\"\n            elif output_file.endswith(\".gz\"):\n                output_format = \"gz\"\n            else:\n                raise ValueError(\"Unknown output format\")\n            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n                self.generate_report(f, output_format, file_info)\n        else:\n            self.generate_report(output_file, \"html\", file_info)\n        self.final_messages.append((\"view_command\", {\"output_file\": os.path.abspath(output_file.name)}))\n        for msg, kwargs in self.final_messages:\n            color_print(msg, **kwargs)", "idx": 1155}
{"namespace": "viztracer.code_monkey.AstTransformer.get_assign_targets_with_attr", "completion": "        if isinstance(node, ast.Attribute):\n            return [node]\n        elif isinstance(node, (ast.Name, ast.Subscript, ast.Starred)):\n            return []\n        elif isinstance(node, (ast.Tuple, ast.List)):\n            return reduce(lambda a, b: a + b, [self.get_assign_targets_with_attr(elt) for elt in node.elts])\n        color_print(\"WARNING\", \"Unexpected node type {} for ast.Assign. \\\n            Please report to the author github.com/gaogaotiantian/viztracer\".format(type(node)))\n        return []\n", "idx": 1156}
{"namespace": "viztracer.code_monkey.SourceProcessor.process", "completion": "        if isinstance(source, bytes):\n            source = source.decode(\"utf-8\")\n        if not isinstance(source, str):\n            return source\n        new_source: List[str] = []\n        for line in source.split(\"\\n\"):\n            for pattern, transform in self.patterns:\n                if re.match(pattern, line):\n                    new_source.append(transform(line))\n                    break\n            else:\n                new_source.append(line)\n        return \"\\n\".join(new_source)\n", "idx": 1157}
{"namespace": "wal_e.log_help.WalELogger.fmt_logline", "completion": "        log_line = []\n        if msg:\n            log_line.append(\"MSG: {0}\".format(msg))\n        if detail:\n            log_line.append(\"DETAIL: {0}\".format(detail))\n        if hint:\n            log_line.append(\"HINT: {0}\".format(hint))\n        if structured:\n            log_line.append(\"STRUCTURED: {0}\".format(\n                WalELogger._fmt_structured(structured)))\n        return '\\n'.join(log_line)\n", "idx": 1158}
{"namespace": "wal_e.blobstore.file.calling_format.Bucket.delete_keys", "completion": "        for key in keys:\n            key.delete()\n        remove_empty_dirs(self.name)", "idx": 1159}
{"namespace": "wal_e.worker.upload_pool.TarUploadPool.put", "completion": "        if self.closed:\n            raise Exception('Pool closed')\n\n        # Check for errors in previously submitted greenlets.\n        while self.wait_change.balance < 0:\n            val = self.wait_change.get()\n            assert isinstance(val, Exception)\n            raise val\n\n        # Check if there are enough resources to start an upload.\n        if self.concurrency_burden >= self.max_concurrency:\n            raise Exception('Too much concurrency')\n\n        if self.member_burden >= self.max_members:\n            raise Exception('Too much work')\n\n        # Start the upload.\n        self._start(tpart)\n", "idx": 1160}
{"namespace": "wal_e.worker.pg.wal_transfer.WalSegment.from_ready_archive_status", "completion": "        # Get the path to the archive_status directory\n        archive_status_dir = path.join(xlog_dir, 'archive_status')\n\n        # Iterate through the files in the archive_status directory\n        for f in os.listdir(archive_status_dir):\n            # Check if the file is a segment file\n            if re.match(storage.SEGMENT_REGEXP, f):\n                # Create a WalSegment instance for the segment file\n                yield WalSegment(path.join(xlog_dir, f))\n", "idx": 1161}
{"namespace": "wal_e.worker.pg.wal_transfer.WalTransferGroup.join", "completion": "        self.closed = True\n        self.wait_change.put(None)\n\n        # Wait for all greenlets to exit.\n        gevent.joinall(self.greenlets, timeout=30)\n\n        # Attempt to force greenlets to exit.\n        for greenlet in self.greenlets:\n            greenlet.kill()\n\n        # Wait for all greenlets to exit.\n        gevent.joinall(self.greenlets, timeout=30)\n\n        # Raise any errors that occurred during the transfer.\n        for greenlet in self.greenlets:\n            if greenlet.exception:\n                raise greenlet.exception\n", "idx": 1162}
{"namespace": "wal_e.worker.pg.wal_transfer.WalTransferGroup.start", "completion": "        self.expect += 1\n        greenlet = gevent.Greenlet(self.transferer, segment)\n        self.greenlets.add(greenlet)\n        greenlet.start()\n", "idx": 1163}
{"namespace": "mrjob.py2.to_unicode", "completion": "    if isinstance(s, bytes):\n        return s.decode('utf-8', 'ignore')\n    else:\n        return s", "idx": 1164}
{"namespace": "mrjob.job.MRJob.steps", "completion": "        # create a dictionary of redefined methods\n        redefined_methods = {\n            'mapper': self.mapper,\n            'reducer': self.reducer,\n            'combiner': self.combiner,\n            'mapper_init': self.mapper_init,\n            'mapper_final': self.mapper_final,\n            'mapper_cmd': self.mapper_cmd,\n            'mapper_pre_filter': self.mapper_pre_filter,\n            'mapper_raw': self.mapper_raw,\n            'reducer_init': self.reducer_init,\n            'reducer_final': self.reducer_final,\n            'reducer_cmd': self.reducer_cmd,\n            'reducer_pre_filter': self.reducer_pre_filter,\n            'combiner_init': self.combiner_init,\n            'combiner_final': self.combiner_final,\n            'combiner_cmd': self.combiner_cmd,\n            'combiner_pre_filter': self.combiner_pre_filter,\n            'spark': self.spark,\n        }\n\n        # create a dictionary of redefined methods that are not None\n        redefined_methods = {k: v for k, v in redefined_methods.items() if v is not None}\n\n        # create a dictionary of kwargs for MRStep\n        kwargs = {\n            'mapper': 'mapper',\n            'reducer': 'reducer',\n            'combiner': 'combiner',\n            'mapper_init': 'mapper_init',\n            'mapper_final': 'mapper_final',\n            'mapper_cmd': 'mapper_cmd',\n            'mapper_pre_filter': 'mapper_pre_filter',\n            'mapper_raw': 'mapper_raw',\n            'reducer_init': 'reducer_init',\n            'reducer_final': 'reducer_final',\n            'reducer_cmd': 'reducer_cmd',\n            'reducer", "idx": 1165}
{"namespace": "mrjob.job.MRJob.increment_counter", "completion": "        group = group.replace(',', ';')\n        counter = counter.replace(',', ';')\n        line = 'reporter:counter:%s,%s,%d\\n' % (group, counter, amount)\n        self.stderr.write(line)", "idx": 1166}
{"namespace": "mrjob.job.MRJob.set_status", "completion": "        # The message should be a string\n        if not isinstance(msg, string_types):\n            msg = str(msg)\n\n        # The message should not contain any newline characters\n        msg = msg.replace('\\n', ' ')\n\n        # The message should not contain any commas\n        msg = msg.replace(',', ';')\n\n        # The message should be encoded as UTF-8\n        if not isinstance(msg, bytes):\n            msg = msg.encode('utf_8')\n\n        # The message should be prefixed with \"reporter:status:\"\n        msg = 'reporter:status:%s\\n' % msg\n\n        # Write the message to the standard error stream\n        self.stderr.write(msg)\n\n        # Flush the standard error stream to ensure that the message is immediately sent to the Hadoop streaming\n        self.stderr.flush()\n", "idx": 1167}
{"namespace": "mrjob.job.MRJob.run_job", "completion": "        # set up logging\n        self.configure_logging()\n\n        # create a runner\n        runner = self.make_runner()\n\n        # run the job\n        try:\n            runner.run()\n        except Exception as e:\n            self.log.exception('Job failed: %r', e)\n            sys.exit(1)\n\n        # concatenate the output if necessary\n        if self.options.cat_output:\n            for path in runner.fs.cat_output():\n                for line in to_lines(path):\n                    self.stdout.write(line)\n                    self.stdout.write('\\n')\n", "idx": 1168}
{"namespace": "mrjob.job.MRJob.set_up_logging", "completion": "        if quiet:\n            log_level = logging.ERROR\n        elif verbose:\n            log_level = logging.DEBUG\n        else:\n            log_level = logging.INFO\n\n        if stream is None:\n            stream = sys.stderr\n\n        # set up logging for the 'mrjob' logger\n        mrjob_logger = logging.getLogger('mrjob')\n        mrjob_logger.setLevel(log_level)\n        mrjob_logger.addHandler(logging.StreamHandler(stream))\n\n        # set up logging for the '__main__' logger\n        main_logger = logging.getLogger('__main__')\n        main_logger.setLevel(log_level)\n        main_logger.addHandler(logging.StreamHandler(stream))\n", "idx": 1169}
{"namespace": "mrjob.job.MRJob.execute", "completion": "        if self.options.mapper:\n            self.run_mapper()\n        elif self.options.combiner:\n            self.run_combiner()\n        elif self.options.reducer:\n            self.run_reducer()\n        else:\n            self.run_job()\n", "idx": 1170}
{"namespace": "mrjob.job.MRJob._runner_kwargs", "completion": "        # build extra_args\n        raw_args = _parse_raw_args(self.arg_parser, self._cl_args)\n\n        extra_args = []\n\n        for dest, option_string, args in raw_args:\n            if dest in self._file_arg_dests:\n                extra_args.append(option_string)\n                extra_args.append(parse_legacy_hash_path('file', args[0]))\n            elif dest in self._passthru_arg_dests:\n                # special case for --hadoop-args=-verbose etc.\n                if (option_string and len(args) == 1 and\n                        args[0].startswith('-')):\n                    extra_args.append('%s=%s' % (option_string, args[0]))\n                else:\n                    if option_string:\n                        extra_args.append(option_string)\n                    extra_args.extend(args)\n\n        # max_output_files is added by _add_runner_args() but can only\n        # be set from the command line, so we add it here (see #2040)\n        return dict(\n            conf_paths=self.options.conf_paths,\n            extra_args=extra_args,\n            hadoop_input_format=self.hadoop_input_format(),\n            hadoop_output_format=self.hadoop_output_format(),\n            input_paths=self.options.args,\n            max_output_files=self.options.max_output_files,\n            mr_job_script=self.mr_job_script(),\n            output_dir=self.options.output_dir,\n            partitioner=self.partitioner(),\n            stdin=self.stdin,\n            step_output_dir=self.options.step_output_dir,\n        )\n", "idx": 1171}
{"namespace": "mrjob.job.MRJob.run_mapper", "completion": "        step = self._get_step(step_num, MRStep)\n        if step.mapper_raw:\n            self._run_mapper_raw(step)\n        else:\n            self._run_mapper(step)\n", "idx": 1172}
{"namespace": "mrjob.job.MRJob.run_combiner", "completion": "        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'combiner')\n\n        for k, v in self.combine_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)\n", "idx": 1173}
{"namespace": "mrjob.job.MRJob.add_passthru_arg", "completion": "        pass_opt = self.arg_parser.add_argument(*args, **kwargs)\n\n        self._passthru_arg_dests.add(pass_opt.dest)\n", "idx": 1174}
{"namespace": "mrjob.job.MRJob.is_task", "completion": "        return self.options.run_mapper or self.options.run_combiner or self.options.run_reducer or self.options.run_spark", "idx": 1175}
{"namespace": "mrjob.job.MRJob.parse_output", "completion": "        for chunk in chunks:\n            for line in to_lines(decompress(BytesIO(chunk), '')):\n                yield self.output_protocol().read(line)\n", "idx": 1176}
{"namespace": "mrjob.job.MRJob.sandbox", "completion": "        if stdin is None:\n            stdin = BytesIO()\n        if stdout is None:\n            stdout = BytesIO()\n        if stderr is None:\n            stderr = BytesIO()\n\n        self.stdin = stdin\n        self.stdout = stdout\n        self.stderr = stderr\n\n        return self", "idx": 1177}
{"namespace": "mrjob.hadoop.fully_qualify_hdfs_path", "completion": "    if path.startswith('hdfs://'):\n        return path\n    elif path.startswith('/'):\n        return 'hdfs://' + path\n    else:\n        return 'hdfs:///user/{}/{}'.format(getpass.getuser(), path)\n\n", "idx": 1178}
{"namespace": "mrjob.hadoop.HadoopJobRunner.fs", "completion": "        if not hasattr(self, '_fs'):\n            self._fs = CompositeFilesystem(\n                'hdfs://',\n                'file://',\n            )\n        return self._fs\n", "idx": 1179}
{"namespace": "mrjob.hadoop.HadoopJobRunner._find_hadoop_streaming_jar", "completion": "        for directory in self._opts['hadoop_streaming_jar_dirs']:\n            log.info('Looking for Hadoop streaming jar in %s...' % directory)\n            for root, dirs, files in os.walk(directory):\n                for file in files:\n                    if _HADOOP_STREAMING_JAR_RE.match(file):\n                        return os.path.join(root, file)\n        return None", "idx": 1180}
{"namespace": "mrjob.hadoop.HadoopJobRunner._find_binaries_and_jars", "completion": "        # Load the Hadoop binary\n        self.fs.hadoop.get_hadoop_bin()\n\n        # Check if there are Hadoop streaming steps or Spark steps in the job\n        if any(step_type in self._STEP_TYPES for step_type in self._step_types):\n            # Load the Hadoop streaming jar\n            self.get_hadoop_streaming_jar()\n\n            # Load the Spark submit binary\n            self.fs.hadoop.get_spark_submit_bin()\n", "idx": 1181}
{"namespace": "mrjob.hadoop.HadoopJobRunner._args_for_streaming_step", "completion": "        hadoop_bin = self.get_hadoop_bin()\n        hadoop_streaming_jar = self.get_hadoop_streaming_jar()\n\n        if not hadoop_streaming_jar:\n            raise ValueError('no Hadoop streaming jar')\n\n        args = [hadoop_bin[0], 'jar', hadoop_streaming_jar]\n\n        args.extend(self._args_for_hadoop_streaming_step(step_num))\n\n        return args\n", "idx": 1182}
{"namespace": "mrjob.hadoop.HadoopJobRunner._stream_history_log_dirs", "completion": "        if not self._should_read_logs():\n            return\n\n        for hadoop_log_dir in unique(self._hadoop_log_dirs(output_dir)):\n            log.info('Looking for history log in %s...' % hadoop_log_dir)\n            if self.fs.exists(hadoop_log_dir):\n                yield [hadoop_log_dir]\n", "idx": 1183}
{"namespace": "mrjob.hadoop.HadoopJobRunner._stream_task_log_dirs", "completion": "        if not self._read_logs():\n            return\n\n        for log_dir in unique(self._hadoop_log_dirs(output_dir=output_dir)):\n            if _logs_exist(self.fs, log_dir):\n                if application_id:\n                    directory = posixpath.join(\n                        log_dir, 'userlogs', application_id)\n                else:\n                    directory = posixpath.join(log_dir, 'userlogs')\n\n                log.info('Looking for task logs in %s...' % directory)\n                yield [directory]\n", "idx": 1184}
{"namespace": "mrjob.setup.UploadDirManager.add", "completion": "        if is_uri(path):\n            return path\n\n        if path in self._path_to_name:\n            return self.uri(path)\n\n        name = name_uniquely(path, self._names_taken, unhide=True)\n        self._path_to_name[path] = name\n        self._names_taken.add(name)\n\n        return self.uri(path)\n", "idx": 1185}
{"namespace": "mrjob.setup.UploadDirManager.uri", "completion": "        if is_uri(path):\n            return path\n\n        if path not in self._path_to_name:\n            raise ValueError('%r is not a URI or a known local file' % path)\n\n        return posixpath.join(self.prefix, self._path_to_name[path])\n", "idx": 1186}
{"namespace": "mrjob.setup.UploadDirManager.path_to_uri", "completion": "        return {path: self.uri(path) for path in self._path_to_name}", "idx": 1187}
{"namespace": "mrjob.setup.WorkingDirManager.name_to_path", "completion": "        if type is None:\n            return dict((name, path)\n                        for (path_type, path), name in\n                        self._name_to_typed_path.iteritems())\n        else:\n            return dict((name, path)\n                        for (path_type, path), name in\n                        self._name_to_typed_path.iteritems()\n                        if path_type == type)", "idx": 1188}
{"namespace": "mrjob.setup.WorkingDirManager.paths", "completion": "        if type is not None:\n            self._check_type(type)\n\n        paths = set()\n        for path_type, path in self._typed_path_to_auto_name:\n            if type is None or path_type == type:\n                paths.add(path)\n\n        for typed_path in self._name_to_typed_path.values():\n            if type is None or typed_path[0] == type:\n                paths.add(typed_path[1])\n\n        return paths\n", "idx": 1189}
{"namespace": "mrjob.compat.jobconf_from_env", "completion": "    # Check if the variable exists in the environment\n    value = os.environ.get(variable)\n    if value is not None:\n        return value\n\n    # Try alternative variable names based on the mapping dictionary\n    compat_map = _JOBCONF_MAP.get(variable)\n    if compat_map is not None:\n        for version, alt_variable in compat_map.items():\n            value = os.environ.get(alt_variable)\n            if value is not None:\n                log.warning('Using deprecated jobconf variable %s. '\n                            'Please use %s instead.',\n                            alt_variable, variable)\n                return value\n\n    # Return the default value if the variable is not found\n    return default\n\n", "idx": 1190}
{"namespace": "mrjob.compat.jobconf_from_dict", "completion": "    # try variable verbatim first\n    if name in jobconf:\n        return jobconf[name]\n\n    # try alternatives (arbitrary order)\n    for var in _JOBCONF_MAP.get(name, {}).values():\n        if var in jobconf:\n            return jobconf[var]\n\n    return default\n\n", "idx": 1191}
{"namespace": "mrjob.compat.translate_jobconf", "completion": "    if variable in _JOBCONF_MAP:\n        return map_version(version, _JOBCONF_MAP[variable])\n    else:\n        return variable", "idx": 1192}
{"namespace": "mrjob.compat.translate_jobconf_for_all_versions", "completion": "    if variable in _JOBCONF_MAP:\n        return sorted(_JOBCONF_MAP[variable].values())\n    else:\n        return [variable]", "idx": 1193}
{"namespace": "mrjob.compat.translate_jobconf_dict", "completion": "    if hadoop_version is None:\n        raise TypeError\n\n    translated_jobconf = {}\n    for key, value in jobconf.items():\n        if key in _JOBCONF_MAP:\n            translated_key = map_version(hadoop_version, _JOBCONF_MAP[key])\n            if translated_key != key:\n                log.warning(\n                    'Detected hadoop configuration property names that do not match version %s:\\n'\n                    'The have been translated to the following names:\\n%s',\n                    hadoop_version,\n                    '\\n'.join(sorted(\n                        '%s:%s' % (key, value)\n                        for key, value in _JOBCONF_MAP.items())))\n            translated_jobconf[translated_key] = value\n        else:\n            translated_jobconf[key] = value\n\n    return translated_jobconf\n\n", "idx": 1194}
{"namespace": "mrjob.compat.uses_yarn", "completion": "    if version is None:\n        raise TypeError\n\n    if isinstance(version, string_types):\n        version = LooseVersion(version)\n\n    return version >= LooseVersion('2.0')", "idx": 1195}
{"namespace": "mrjob.local.LocalMRJobRunner._spark_master", "completion": "        # TODO: this should probably take step_num, to allow for\n        # step-specific jobconf\n        num_cores = self._opts['num_cores']\n        num_executors = self._opts['num_executors']\n        executor_memory = self._opts['executor_memory']\n\n        if num_executors is None:\n            num_executors = num_cores\n\n        if executor_memory is None:\n            executor_memory = _DEFAULT_EXECUTOR_MEMORY\n\n        # convert executor_memory to MB\n        executor_memory_mb = int(cmd_line.parse_size(executor_memory)) / 1024 / 1024\n\n        # round up to the nearest MB\n        executor_memory_mb = int(math.ceil(executor_memory_mb))\n\n        return 'local-cluster[{},{},{}]'.format(\n            num_executors, num_cores // num_executors, executor_memory_mb)\n", "idx": 1196}
{"namespace": "mrjob.runner.MRJobRunner._bootstrap_mrjob", "completion": "        if self._opts['bootstrap_mrjob'] is None:\n            return True\n        else:\n            return self._opts['bootstrap_mrjob']\n", "idx": 1197}
{"namespace": "mrjob.conf._fix_clear_tags", "completion": "    if isinstance(x, list):\n        return [_fix_clear_tags(v) for v in x]\n    elif isinstance(x, dict):\n        return {k: _fix_clear_tags(v) for k, v in x.items() if not isinstance(k, ClearedValue)}\n    elif isinstance(x, ClearedValue):\n        return _fix_clear_tags(x.value)\n    else:\n        return x\n\n", "idx": 1198}
{"namespace": "mrjob.conf.load_opts_from_mrjob_conf", "completion": "    if already_loaded is None:\n        already_loaded = []\n\n    conf_path = _expanded_mrjob_conf_path(conf_path)\n\n    if conf_path is None:\n        return [(None, {})]\n\n    if conf_path in already_loaded:\n        return []\n\n    already_loaded.append(conf_path)\n\n    conf_object = _conf_object_at_path(conf_path)\n\n    if conf_object is None:\n        return [(conf_path, {})]\n\n    if 'runners' not in conf_object:\n        return [(conf_path, {})]\n\n    if runner_alias not in conf_object['runners']:\n        return [(conf_path, {})]\n\n    if 'conf_paths' in conf_object:\n        conf_paths = conf_object['conf_paths']\n        if isinstance(conf_paths, string_types):\n            conf_paths = [conf_paths]\n        return [(conf_path, conf_object['runners'][runner_alias])\n                for conf_path in conf_paths] + \\\n            [(conf_path, {}) for conf_path in conf_paths\n             for conf_object in [_conf_object_at_path(conf_path)\n                                 for conf_path in conf_paths]\n             if conf_object is not None]\n    else:\n        return [(conf_path, conf_object['runners'][runner_alias])]\n\n", "idx": 1199}
{"namespace": "mrjob.conf.load_opts_from_mrjob_confs", "completion": "    if conf_paths is None:\n        conf_paths = [find_mrjob_conf()]\n\n    conf_paths = [_expanded_mrjob_conf_path(conf_path)\n                  for conf_path in conf_paths]\n\n    confs = []\n    for conf_path in conf_paths:\n        confs.extend(load_opts_from_mrjob_conf(runner_alias, conf_path))\n\n    if not confs:\n        log.warning('No config specified for %s runner' % runner_alias)\n\n    return confs\n\n", "idx": 1200}
{"namespace": "mrjob.conf.dump_mrjob_conf", "completion": "    if yaml:\n        _dump_yaml_with_clear_tags(conf, f)\n    else:\n        json.dump(conf, f, indent=4, sort_keys=True)\n\n", "idx": 1201}
{"namespace": "mrjob.conf.combine_lists", "completion": "    result = []\n    for seq in seqs:\n        if seq is not None:\n            if isinstance(seq, string_types):\n                result.append(seq)\n            else:\n                try:\n                    result.extend(seq)\n                except TypeError:\n                    result.append(seq)\n    return result\n\n", "idx": 1202}
{"namespace": "mrjob.conf.combine_cmds", "completion": "    for cmd in reversed(cmds):\n        if cmd is None:\n            continue\n\n        if isinstance(cmd, string_types):\n            return shlex_split(cmd)\n        else:\n            return list(cmd)\n\n    return None\n\n", "idx": 1203}
{"namespace": "mrjob.conf.combine_dicts", "completion": "    result = {}\n\n    for d in dicts:\n        if d is None:\n            continue\n\n        for k, v in d.items():\n            if isinstance(v, ClearedValue) and v.value is None:\n                result.pop(k, None)\n            else:\n                result[k] = v\n\n    return result\n\n", "idx": 1204}
{"namespace": "mrjob.conf.combine_jobconfs", "completion": "    result = {}\n    for jobconf in jobconfs:\n        if jobconf:\n            for k, v in jobconf.items():\n                if v is None:\n                    result.pop(k, None)\n                else:\n                    result[k] = _strip_clear_tag(v)\n    return result\n\n", "idx": 1205}
{"namespace": "mrjob.conf.combine_path_lists", "completion": "    result = []\n\n    for path_seq in path_seqs:\n        if path_seq is None:\n            continue\n\n        if isinstance(path_seq, string_types):\n            path_seq = [path_seq]\n\n        for path in path_seq:\n            path = expand_path(path)\n\n            if '*' in path:\n                result.extend(glob.glob(path))\n            else:\n                result.append(path)\n\n    return result\n\n", "idx": 1206}
{"namespace": "mrjob.conf.combine_opts", "completion": "    # collect all keys from the dictionaries that are not wrapped in `ClearedValue`\n    keys = set(k for d in opts_list for k, v in d.items() if not isinstance(v, ClearedValue))\n\n    # initialize the result dictionary\n    result = {}\n\n    # iterate through each key\n    for key in keys:\n        # get the values for the key from all the dictionaries\n        values = [d.get(key) for d in opts_list]\n\n        # get the sub-combiner for the key, or use the default combine_values function\n        sub_combiner = combiners.get(key, combine_values)\n\n        # process the values using the sub-combiner\n        processed_value = sub_combiner(*values)\n\n        # store the processed value with the key in the result dictionary\n        result[key] = processed_value\n\n    # return the result dictionary\n    return result\n\n", "idx": 1207}
{"namespace": "mrjob.bin.MRJobBinRunner._task_python_bin", "completion": "        return self._opts['task_python_bin'] or self._python_bin()\n", "idx": 1208}
{"namespace": "mrjob.bin.MRJobBinRunner.get_spark_submit_bin", "completion": "        if not self._spark_submit_bin:\n            self._spark_submit_bin = which('spark-submit')\n\n        return self._spark_submit_bin\n", "idx": 1209}
{"namespace": "mrjob.step.StepFailedException.__str__", "completion": "        if self.reason:\n            return '%s failed: %s' % (self.step_desc, self.reason)\n        else:\n            return '%s failed' % self.step_desc\n\n", "idx": 1210}
{"namespace": "mrjob.step.StepFailedException.__repr__", "completion": "        return '%s(%s)' % (\n            self.__class__.__name__,\n            ', '.join('%s=%r' % (f, getattr(self, f)) for f in self._FIELDS))", "idx": 1211}
{"namespace": "mrjob.step.MRStep.description", "completion": "        desc = {'type': 'streaming'}\n\n        if (step_num == 0 or\n                self.has_explicit_mapper or\n                self.has_explicit_combiner):\n            desc['mapper'] = self.render_mapper()\n\n        if self.has_explicit_combiner:\n            desc['combiner'] = self.render_combiner()\n\n        if self.has_explicit_reducer:\n            desc['reducer'] = self.render_reducer()\n\n        if self._steps['mapper_raw']:\n            desc['input_manifest'] = True\n\n        if self._steps['jobconf']:\n            desc['jobconf'] = self._steps['jobconf']\n\n        return desc\n\n", "idx": 1212}
{"namespace": "mrjob.step._Step.description", "completion": "        desc = {'type': self._STEP_TYPE}\n        for k in self._STEP_ATTRS:\n            if k not in self._HIDDEN_ATTRS:\n                desc[k] = getattr(self, k)\n        return desc\n\n", "idx": 1213}
{"namespace": "mrjob.protocol._KeyCachingProtocol.read", "completion": "        key_encoded, value_encoded = line.split('\\t', 1)\n        self._last_key_encoded = key_encoded\n        self._last_key_decoded = self._loads(self._last_key_encoded)\n        value_decoded = self._loads(value_encoded)\n        return self._last_key_decoded, value_decoded\n", "idx": 1214}
{"namespace": "mrjob.util.safeeval", "completion": "    if globals is None:\n        globals = {}\n    if locals is None:\n        locals = {}\n\n    safe_globals = {\n        'True': True,\n        'False': False,\n        'None': None,\n        'set': set,\n        'range': range,\n        'xrange': xrange,\n        'open': NameError('name \\'open\\' is not defined')\n    }\n\n    safe_globals.update(globals)\n\n    safe_locals = {\n        'True': True,\n        'False': False,\n        'None': None,\n        'set': set,\n        'range': range,\n        'xrange': xrange,\n        'open': NameError('name \\'open\\' is not defined')\n    }\n\n    safe_locals.update(locals)\n\n    return eval(expr, safe_globals, safe_locals)", "idx": 1215}
{"namespace": "mrjob.util.to_lines", "completion": "    if hasattr(chunks, 'readline'):\n        return chunks\n\n    for chunk in chunks:\n        for line in chunk.splitlines(True):\n            yield line\n\n", "idx": 1216}
{"namespace": "mrjob.parse.is_s3_uri", "completion": "    try:\n        return urlparse(uri).scheme == 's3'\n    except ValueError:\n        return False\n\n", "idx": 1217}
{"namespace": "mrjob.parse.parse_s3_uri", "completion": "    try:\n        parsed_uri = urlparse(uri)\n    except ValueError:\n        raise ValueError('Could not parse S3 URI: %s' % uri)\n\n    if parsed_uri.scheme != 's3':\n        raise ValueError('Not an S3 URI: %s' % uri)\n\n    if not parsed_uri.netloc:\n        raise ValueError('Bucket missing: %s' % uri)\n\n    if not parsed_uri.path:\n        raise ValueError('Key missing: %s' % uri)\n\n    return parsed_uri.netloc, parsed_uri.path.lstrip('/')\n\n", "idx": 1218}
{"namespace": "mrjob.parse.to_uri", "completion": "    if is_uri(path_or_uri):\n        return path_or_uri\n    else:\n        return pathname2url(abspath(path_or_uri))", "idx": 1219}
{"namespace": "mrjob.parse.parse_mr_job_stderr", "completion": "    if counters is None:\n        counters = {}\n\n    if isinstance(stderr, bytes):\n        stderr = BytesIO(stderr)\n\n    if isinstance(stderr, BytesIO):\n        stderr = stderr.readlines()\n\n    statuses = []\n    other = []\n\n    for line in stderr:\n        line = line.strip()\n\n        match = _COUNTER_RE.match(line)\n        if match:\n            group, counter, amount = match.groups()\n            if group not in counters:\n                counters[group] = {}\n            counters[group][counter] = int(amount)\n            continue\n\n        match = _STATUS_RE.match(line)\n        if match:\n            statuses.append(match.group(1))\n            continue\n\n        other.append(line)\n\n    return {\n        'counters': counters,\n        'statuses': statuses,\n        'other': other,\n    }", "idx": 1220}
{"namespace": "mrjob.parse._parse_progress_from_job_tracker", "completion": "    # This function is used to parse the progress of a running job from the HTML content of a job tracker.\n    # It extracts the map_percent and reduce_percent values as floats and returns them.\n    # The content searched is extracted between 'Running Jobs' and 'Jobs' in the HTML content.\n    # If the HTML content does not contain the necessary information, it returns (None, None).\n    # Input-Output Arguments\n    # :param html_bytes: bytes. The HTML content of the job tracker.\n    # :return: Tuple of floats. The map_percent and reduce_percent values extracted from the HTML content, or (None, None) if the information is not found.\n\n    # Search for the content between 'Running Jobs' and 'Jobs' in the HTML content\n    match = re.search(\n        br'Running Jobs.*?Jobs', html_bytes, re.DOTALL | re.IGNORECASE)\n\n    # If the content is not found, return (None, None)\n    if not match:\n        return None, None\n\n    # Extract the content between 'Running Jobs' and 'Jobs'\n    content = match.group(0)\n\n    # Search for the map_percent and reduce_percent values in the content\n    match = _JOB_TRACKER_HTML_RE.search(content)\n\n    # If the values are not found, return (None, None)\n    if not match:\n        return None, None\n\n    # Extract the map_percent and reduce_percent values as floats\n    map_percent, reduce_percent = [float(x) for x in match.groups()]\n\n    # Return the map_percent and reduce_percent values\n    return map_percent, reduce_percent\n\n", "idx": 1221}
{"namespace": "mrjob.parse._parse_progress_from_resource_manager", "completion": "    # snip out the Running Jobs section (ignore the header)\n    start = html_bytes.rfind(b'Running Jobs')\n    if start == -1:\n        return None, None\n    end = html_bytes.find(b'Jobs', start + len(b'Running Jobs'))\n    if end == -1:\n        end = None\n\n    html_bytes = html_bytes[start:end]\n\n    # search it for percents\n    matches = _JOB_TRACKER_HTML_RE.findall(html_bytes)\n    if len(matches) >= 2:\n        return float(matches[0]), float(matches[1])\n    else:\n        return None, None\n\n", "idx": 1222}
{"namespace": "mrjob.logs.task._match_task_log_path", "completion": "    # check if the path is a pre-YARN task log path\n    match = _PRE_YARN_TASK_LOG_PATH_RE.search(path)\n    if match:\n        # check if the attempt ID matches the passed job ID\n        if job_id and match.group('job_id') != job_id:\n            return None\n        # return the match as a dictionary\n        return {\n            'application_id': None,\n            'attempt_id': match.group('attempt_id'),\n            'log_type': match.group('log_type'),\n            'path': path,\n        }\n\n    # check if the path is a YARN task log path\n    match = _YARN_TASK_LOG_PATH_RE.search(path)\n    if match:\n        # check if the application ID matches the passed application ID\n        if application_id and match.group('application_id') != application_id:\n            return None\n        # return the match as a dictionary\n        return {\n            'application_id': match.group('application_id'),\n            'container_id': match.group('container_id'),\n            'log_type': match.group('log_type'),\n            'path': path,\n        }\n\n    # if the path is not a task log path, return None\n    return None\n\n", "idx": 1223}
{"namespace": "mrjob.logs.task._parse_task_syslog", "completion": "    result = {}\n\n    # check for a split\n    for line in lines:\n        m = _YARN_INPUT_SPLIT_RE.match(line)\n        if m:\n            result['split'] = dict(\n                path=m.group('path'),\n                start_line=int(m.group('start_line')),\n                num_lines=int(m.group('num_lines')))\n            break\n\n    # check for a Hadoop error\n    for line in lines:\n        if _SPARK_APP_MASTER_LOGGER in line:\n            # this is a Hadoop error\n            m = _SPARK_APP_EXITED_RE.search(line)\n            if m:\n                # this is a Hadoop error\n                result['hadoop_error'] = dict(\n                    message=line,\n                    num_lines=1)\n                break\n\n    return result\n\n", "idx": 1224}
{"namespace": "mrjob.logs.ids._sort_for_spark", "completion": "    return sorted(\n        ds,\n        key=lambda d: (\n            d.get('cluster'),\n            d.get('step'),\n            d.get('attempt'),\n            d.get('started_at'),\n        ),\n        reverse=True,\n    )\n\n", "idx": 1225}
{"namespace": "mrjob.logs.spark._parse_spark_log", "completion": "    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this\n    # TODO: add a test for this", "idx": 1226}
{"namespace": "mrjob.logs.mixin.LogInterpretationMixin._pick_error", "completion": "        if self._step_type_uses_spark(step_type):\n            return None\n\n        if self._read_logs():\n            log.info('Scanning logs for probable cause of failure...')\n            self._interpret_step_logs(log_interpretation, step_type)\n            self._interpret_history_log(log_interpretation)\n\n        return _pick_error(log_interpretation)\n", "idx": 1227}
{"namespace": "mrjob.logs.history._match_history_log_path", "completion": "    m = _HISTORY_LOG_PATH_RE.match(path)\n    if not m:\n        return None\n\n    if job_id and m.group('job_id') != job_id:\n        return None\n\n    return {\n        'job_id': m.group('job_id'),\n        'yarn': '.jhist' in m.group('suffix'),\n    }\n\n", "idx": 1228}
{"namespace": "mrjob.logs.history._parse_pre_yarn_history_log", "completion": "    result = {}\n    task_to_counters = {}  # used for successful tasks in failed jobs\n\n    for record in _parse_pre_yarn_history_records(lines):\n        record_type = record.get('type')\n        if not isinstance(record_type, string_types):\n            continue\n\n        if record_type == 'Task':\n            if 'COUNTERS' in record and 'TASKID' in record:\n                counters = _extract_pre_yarn_counters(record['COUNTERS'])\n                task_to_counters[record['TASKID']] = counters\n\n        elif record_type == 'FAILED':\n            if 'ERROR' in record and record['ERROR']:\n                error = dict(\n                    hadoop_error=dict(\n                        message=record['ERROR'],\n                        start_line=record['start_line'],\n                        num_lines=record['num_lines']))\n\n                if 'TASKATTEMPTID' in record:\n                    error['attempt_id'] = record['TASKATTEMPTID']\n\n                result.setdefault('errors', [])\n                result['errors'].append(error)\n\n    # if job failed, patch together counters from successful tasks\n    if 'counters' not in result and task_to_counters:\n        result['counters'] = _sum_counters(*task_to_counters.values())\n\n    return result\n\n", "idx": 1229}
{"namespace": "mrjob.logs.history._parse_pre_yarn_history_records", "completion": "    for line_num, line in enumerate(lines):\n        m = _PRE_YARN_HISTORY_RECORD.match(line)\n        if not m:\n            continue\n\n        record = dict(\n            fields={},\n            num_lines=1,\n            start_line=line_num,\n            type=m.group('type'))\n\n        for key_pair in m.group('key_pairs').split():\n            m = _PRE_YARN_HISTORY_KEY_PAIR.match(key_pair)\n            if not m:\n                continue\n\n            key = m.group('key')\n            value = _PRE_YARN_HISTORY_ESCAPE_RE.sub(\n                r'\\1', m.group('escaped_value'))\n\n            record['fields'][key] = value\n\n        yield record\n\n", "idx": 1230}
{"namespace": "mrjob.logs.step._parse_step_syslog", "completion": "    #: The application_id is the unique identifier for the application.\n    application_id = None\n\n    #: The counters are the various counters that are tracked by the application.\n    counters = {}\n\n    #: The errors are the errors that are encountered during the execution of the application.\n    errors = []\n\n    #: The job_id is the unique identifier for the job.\n    job_id = None\n\n    #: The output_dir is the directory where the output of the application is stored.\n    output_dir = None\n\n    #: The progress is the progress of the job.\n    progress = None\n\n    #: The task_attempt_ids are the unique identifiers for the task attempts.\n    task_attempt_ids = []\n\n    #: The task_attempt_ids_by_task_id is a dictionary that maps task_id to task_attempt_id.\n    task_attempt_ids_by_task_id = {}\n\n    #: The task_ids are the unique identifiers for the tasks.\n    task_ids = []\n\n    #: The task_ids_by_attempt_id is a dictionary that maps task_attempt_id to task_id.\n    task_ids_by_attempt_id = {}\n\n    #: The task_progress is the progress of the tasks.\n    task_progress = {}\n\n    #: The task_progress_by_attempt_id is a dictionary that maps task_attempt_id to task_progress.\n    task_progress_by_attempt_id = {}\n\n    #: The task_progress_by_task_id is a dictionary that maps task_id to task_progress.\n    task_progress_by_task_id = {}\n\n    #: The task_progress_by_task_id_and_attempt_id is a dictionary that maps task_id and task_attempt_id to task_progress.\n    task_progress_by_task_id_and_attempt_id = {}\n\n    #: The task_progress_by_task_id_and_attempt_id_and_progress is a", "idx": 1231}
{"namespace": "mrjob.logs.errors._merge_and_sort_errors", "completion": "    merged_errors = {}\n\n    for error in errors:\n        if error.get('container_id'):\n            key = error['container_id']\n        else:\n            key = error['time']\n\n        merged_errors[key] = _merge_error(\n            merged_errors.get(key), error, attempt_to_container_id)\n\n    return sorted(\n        merged_errors.values(),\n        key=_sort_key, reverse=True)\n\n", "idx": 1232}
{"namespace": "mrjob.fs.ssh.SSHFilesystem.ls", "completion": "        # Extract the hostname and filesystem path from the path_glob\n        match = _SSH_URI_RE.match(path_glob)\n        hostname = match.group('hostname')\n        filesystem_path = match.group('filesystem_path')\n\n        # Execute the \"find\" command to list all the files in the specified path\n        args = ['find', filesystem_path, '-type', 'f']\n        p = self._ssh_launch(hostname, args)\n\n        # Read the output of the \"find\" command and yield the file paths\n        for line in p.stdout:\n            yield 'ssh://' + hostname + line.strip()\n\n        # Close the file handles and do error handling\n        self._ssh_finish_run(p)\n", "idx": 1233}
{"namespace": "mrjob.fs.ssh.SSHFilesystem._cat_file", "completion": "        m = _SSH_URI_RE.match(path)\n        addr = m.group('hostname')\n        path_to_cat = m.group('filesystem_path')\n\n        p = self._ssh_launch(\n            addr, ['cat', path_to_cat])\n\n        for line in p.stdout:\n            yield line\n\n        self._ssh_finish_run(p)\n", "idx": 1234}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.get_hadoop_bin", "completion": "        if self._hadoop_bin is None:\n            self._hadoop_bin = which('hadoop')\n\n        return self._hadoop_bin\n", "idx": 1235}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.du", "completion": "        # Execute the \"fs -du\" command using the Hadoop binary\n        args = self.get_hadoop_bin() + ['fs', '-du', path_glob]\n        stdout = self.invoke_hadoop(args, return_stdout=True)\n\n        # Parse the output to get the size\n        try:\n            size = int(stdout.split()[0])\n        except (IndexError, ValueError):\n            raise IOError('Unexpected output from Hadoop fs -du: %r' % stdout)\n\n        return size", "idx": 1236}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.mkdir", "completion": "        version = self.get_hadoop_version()\n\n        # use mkdir -p on Hadoop 2 (see #1152)\n        if uses_yarn(version):\n            args = ['fs', '-mkdir', '-p', path]\n        else:\n            args = ['fs', '-mkdir', path]\n\n        try:\n            self.invoke_hadoop(args, ok_stderr=[_HADOOP_FILE_EXISTS_RE])\n        except CalledProcessError:\n            raise IOError(\"Could not mkdir %s\" % path)\n", "idx": 1237}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.exists", "completion": "        try:\n            self.invoke_hadoop(['fs', '-ls', path_glob],\n                                ok_stderr=[_HADOOP_LS_NO_SUCH_FILE])\n        except CalledProcessError as e:\n            if e.returncode == -1 or e.returncode == 255:\n                return False\n            else:\n                raise IOError(\"Could not check path %s\" % path_glob)\n        return True\n", "idx": 1238}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.rm", "completion": "        if not is_uri(path_glob):\n            super(HadoopFilesystem, self).rm(path_glob)\n            return\n\n        version = self.get_hadoop_version()\n\n        # use -R on Hadoop 2 (see #1152)\n        if uses_yarn(version):\n            args = ['fs', '-rm', '-R', path_glob]\n        else:\n            args = ['fs', '-rmr', path_glob]\n\n        try:\n            self.invoke_hadoop(args, ok_stderr=[_HADOOP_RM_NO_SUCH_FILE])\n        except CalledProcessError:\n            raise IOError(\"Could not rm %s\" % path_glob)", "idx": 1239}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.touchz", "completion": "        try:\n            self.invoke_hadoop(['fs', '-touchz', path])\n        except CalledProcessError:\n            raise IOError(\"Could not touchz %s\" % path)", "idx": 1240}
{"namespace": "mrjob.fs.local.LocalFilesystem.du", "completion": "        path_glob = self._match_single_path_glob(path_glob)\n        total_size = 0\n        for file_path in glob.glob(path_glob):\n            total_size += os.path.getsize(file_path)\n        return total_size\n", "idx": 1241}
{"namespace": "mrjob.fs.local.LocalFilesystem.ls", "completion": "        path_glob = _from_file_uri(path_glob)\n\n        if os.path.isdir(path_glob):\n            for root, dirs, files in os.walk(path_glob):\n                for path in files:\n                    yield 'file://%s' % os.path.join(root, path)\n        else:\n            yield 'file://%s' % path_glob\n", "idx": 1242}
{"namespace": "mrjob.fs.local.LocalFilesystem._cat_file", "completion": "        path = _from_file_uri(path)\n        with open(path, 'rb') as f:\n            while True:\n                chunk = f.read(self.blocksize)\n                if not chunk:\n                    break\n                yield chunk\n", "idx": 1243}
{"namespace": "mrjob.fs.local.LocalFilesystem.exists", "completion": "        path_glob = _from_file_uri(path_glob)\n        return bool(glob.glob(path_glob))\n", "idx": 1244}
{"namespace": "mrjob.fs.local.LocalFilesystem.mkdir", "completion": "        path = _from_file_uri(path)\n        if not os.path.isdir(path):\n            os.mkdir(path)\n", "idx": 1245}
{"namespace": "mrjob.fs.local.LocalFilesystem.put", "completion": "        src = _from_file_uri(src)\n        path = _from_file_uri(path)\n        shutil.copy(src, path)\n", "idx": 1246}
{"namespace": "mrjob.fs.local.LocalFilesystem.rm", "completion": "        path_glob = _from_file_uri(path_glob)\n        for path in glob.glob(path_glob):\n            if os.path.isdir(path):\n                shutil.rmtree(path)\n            else:\n                os.remove(path)\n", "idx": 1247}
{"namespace": "mrjob.fs.local.LocalFilesystem.touchz", "completion": "        path = _from_file_uri(path)\n        if os.path.exists(path):\n            if os.path.getsize(path) != 0:\n                raise OSError('File already exists and is not empty: %s' % path)\n        else:\n            open(path, 'w').close()\n", "idx": 1248}
{"namespace": "mrjob.fs.local.LocalFilesystem.md5sum", "completion": "        path = _from_file_uri(path)\n        with open(path, 'rb') as f:\n            return self._md5sum_file(f)\n", "idx": 1249}
{"namespace": "mrjob.fs.composite.CompositeFilesystem.add_fs", "completion": "        setattr(self, name, fs)\n        self._fs_names.append(name)\n        self._disable_if[name] = disable_if\n", "idx": 1250}
{"namespace": "mrjob.fs.base.Filesystem.cat", "completion": "        for path in self.ls(path_glob):\n            yield self.read(path)\n            yield b''", "idx": 1251}
{"namespace": "mrjob.fs.base.Filesystem.join", "completion": "        if not path:\n            return path\n\n        if not paths:\n            return path\n\n        if path.startswith('hdfs://'):\n            scheme, netloc, path = self._split_uri(path)\n            path = posixpath.join(path, *paths)\n            return self._join_uri(scheme, netloc, path)\n\n        return os.path.join(path, *paths)", "idx": 1252}
{"namespace": "mrjob.examples.mr_text_classifier.parse_doc_filename", "completion": "    # Extract the filename from the input URI\n    filename = posixpath.basename(input_uri)\n\n    # Split the filename into its components\n    id, cats = filename.split('.', 1)\n\n    # Initialize an empty dictionary to store the parsed information\n    parsed = dict(id=id)\n\n    # Split the categories into a list\n    cats = cats.split('-')\n\n    # Iterate over the categories\n    for cat in cats:\n        # Check if the category is a negated category\n        if cat.startswith('not_'):\n            # If it is, add the category to the dictionary with a value of False\n            parsed[cat[4:]] = False\n        else:\n            # If it is not, add the category to the dictionary with a value of True\n            parsed[cat] = True\n\n    # Return the parsed information\n    return parsed\n\n", "idx": 1253}
{"namespace": "prometheus_client.mmap_dict.MmapedDict.read_value", "completion": "        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        return _unpack_two_doubles(self._m, pos + 4)[0]\n", "idx": 1254}
{"namespace": "prometheus_client.mmap_dict.MmapedDict.write_value", "completion": "        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        _pack_two_doubles(self._m, pos, value, timestamp)\n", "idx": 1255}
{"namespace": "prometheus_client.multiprocess.MultiProcessCollector.merge", "completion": "        merged = defaultdict(dict)\n        for file in files:\n            with open(file, 'r') as f:\n                data = json.load(f)\n            for name, samples in data.items():\n                for sample in samples:\n                    if name not in merged:\n                        merged[name][sample['labels']] = sample\n                    else:\n                        if sample['labels'] not in merged[name]:\n                            merged[name][sample['labels']] = sample\n                        else:\n                            if sample['type'] == 'histogram':\n                                if accumulate:\n                                    merged[name][sample['labels']]['values'][0] += sample['values'][0]\n                                    merged[name][sample['labels']]['values'][1] += sample['values'][1]\n                                    merged[name][sample['labels']]['values'][2] += sample['values'][2]\n                                    merged[name][sample['labels']]['values'][3] += sample['values'][3]\n                                    merged[name][sample['labels']]['values'][4] += sample['values'][4]\n                                else:\n                                    merged[name][sample['labels']] = sample\n                            else:\n                                merged[name][sample['labels']]['value'] += sample['value']\n        return merged\n", "idx": 1256}
{"namespace": "prometheus_client.multiprocess.MultiProcessCollector.collect", "completion": "        files = glob.glob(os.path.join(self._path, '*.db'))\n        return MultiProcessCollector.merge(files, accumulate=True)", "idx": 1257}
{"namespace": "prometheus_client.exposition.choose_encoder", "completion": "    if accept_header is not None and 'application/openmetrics-text' in accept_header:\n        return generate_latest, CONTENT_TYPE_LATEST\n    return generate_latest, CONTENT_TYPE_LATEST\n\n", "idx": 1258}
{"namespace": "flower.command.apply_options", "completion": "    # Filter out options that are specific to the application\n    argv = [arg for arg in argv if not arg.startswith('--app')]\n\n    # Parse the command line to get the \"--conf\" option\n    parse_command_line(argv=argv)\n\n    # Parse the configuration file\n    if options.conf:\n        parse_config_file(options.conf)\n\n    # Parse the command line again to update the options\n    parse_command_line(argv=argv)\n\n", "idx": 1259}
{"namespace": "trackerjacker.ieee_mac_vendor_db.MacVendorDB.lookup", "completion": "        mac = mac.upper().replace(':', '')\n        for prefix in self.db:\n            if mac.startswith(prefix):\n                return self.db[prefix]\n        return ''", "idx": 1260}
{"namespace": "trailscraper.iam.Statement.merge", "completion": "        if self.Effect != other.Effect:\n            raise ValueError(\"Trying to combine two statements with differing effects: {} {}\".format(self.Effect, other.Effect))\n\n        merged_actions = sorted(list(set(self.Action + other.Action)))\n        merged_resources = sorted(list(set(self.Resource + other.Resource)))\n\n        return Statement(merged_actions, self.Effect, merged_resources)", "idx": 1261}
{"namespace": "trailscraper.iam.parse_policy_document", "completion": "    if isinstance(stream, six.string_types):\n        json_data = json.loads(stream)\n    else:\n        json_data = json.load(stream)\n\n    statements = _parse_statements(json_data['Statement'])\n    version = json_data['Version']\n\n    return PolicyDocument(Statement=statements, Version=version)\n\n", "idx": 1262}
{"namespace": "trailscraper.iam.known_iam_actions", "completion": "    all_known_iam_permissions = all_known_iam_permissions()\n    actions = pipe(all_known_iam_permissions,\n                   mapz(lambda action: _parse_action(action)),\n                   groupbyz(lambda action: action.prefix))\n    return actions[prefix]", "idx": 1263}
{"namespace": "trailscraper.boto_service_definitions.service_definition_file", "completion": "    files = boto_service_definition_files()\n    files = [file for file in files if fnmatch.fnmatch(file, '**/' + servicename + '/*/service-*.json')]\n    files.sort()\n    return files[-1]\n\n", "idx": 1264}
{"namespace": "trailscraper.boto_service_definitions.operation_definition", "completion": "    service_definition_file_path = service_definition_file(servicename)\n    with open(service_definition_file_path, 'r') as service_definition_file:\n        service_definition = json.load(service_definition_file)\n\n    return service_definition['operations'][operationname]", "idx": 1265}
{"namespace": "trailscraper.cloudtrail.Record.to_statement", "completion": "        if self.event_source == \"sts.amazonaws.com\" and self.event_name == \"GetCallerIdentity\":\n            return None\n\n        if self.event_source == \"apigateway.amazonaws.com\":\n            return self._to_api_gateway_statement()\n\n        iam_prefix = self._source_to_iam_prefix()\n        iam_action = self._event_name_to_iam_action()\n\n        return Statement(\n            Effect=\"Allow\",\n            Action=[Action(iam_prefix, iam_action)],\n            Resource=self.resource_arns\n        )\n\n", "idx": 1266}
{"namespace": "trailscraper.cloudtrail.filter_records", "completion": "    return pipe(records,\n                filterz(_by_timeframe(from_date, to_date)),\n                filterz(_by_role_arns(arns_to_filter_for)))", "idx": 1267}
{"namespace": "trailscraper.record_sources.local_directory_record_source.LocalDirectoryRecordSource.load_from_dir", "completion": "        records = []\n        for log_file in self._valid_log_files():\n            if log_file.contains_events_within(from_date, to_date):\n                records.extend(log_file.load_records())\n        return records\n", "idx": 1268}
{"namespace": "pyt.__main__.discover_files", "completion": "    included_files = []\n    excluded_files = excluded_files.split(',')\n\n    for target in targets:\n        if os.path.isfile(target):\n            if target.endswith('.py') and target not in excluded_files:\n                included_files.append(os.path.abspath(target))\n        elif os.path.isdir(target):\n            for root, _, files in os.walk(target):\n                for file in files:\n                    if file.endswith('.py') and file not in excluded_files:\n                        file_path = os.path.abspath(os.path.join(root, file))\n                        included_files.append(file_path)\n        else:\n            log.warning('Invalid target: %s', target)\n\n    for file_path in included_files:\n        log.debug('Discovered file: %s', file_path)\n\n    return included_files\n\n", "idx": 1269}
{"namespace": "pyt.core.project_handler.get_directory_modules", "completion": "    global _local_modules\n    if _local_modules and os.path.dirname(_local_modules[0][1]) == directory:\n        return _local_modules\n    if not os.path.isdir(directory):\n        directory = os.path.dirname(directory)\n    for file in os.listdir(directory):\n        if file.endswith(\".py\"):\n            module_name = file[:-3]\n            file_path = os.path.join(directory, file)\n            _local_modules.append((module_name, file_path))\n    return _local_modules\n\n", "idx": 1270}
{"namespace": "pyt.vulnerabilities.vulnerabilities.find_triggers", "completion": "    trigger_nodes = []\n    for node in nodes:\n        if node.line_number not in nosec_lines:\n            for word in trigger_words:\n                if word.label in node.label:\n                    trigger_nodes.append(TriggerNode(word, cfg_node=node))\n    return trigger_nodes\n\n", "idx": 1271}
{"namespace": "pyt.vulnerabilities.vulnerabilities.label_contains", "completion": "    for trigger in triggers:\n        if trigger.label in node.label:\n            yield TriggerNode(trigger, node)", "idx": 1272}
{"namespace": "pyt.vulnerabilities.vulnerabilities.build_sanitiser_node_dict", "completion": "    sanitisers = extract_sanitisers(sinks_in_file)\n    sanitiser_node_dict = defaultdict(list)\n    for sanitiser in sanitisers:\n        sanitiser_node_dict[sanitiser] = find_sanitiser_nodes(cfg, sanitiser)\n    return sanitiser_node_dict\n\n", "idx": 1273}
{"namespace": "pyt.vulnerabilities.trigger_definitions_parser.parse", "completion": "    with open(trigger_word_file) as fd:\n        contents = fd.read()\n\n    definitions = json.loads(contents)\n\n    sources = {\n        key: Source(**data)\n        for key, data in definitions['sources'].items()\n    }\n    sinks = {\n        key: Sink.from_json(key, data)\n        for key, data in definitions['sinks'].items()\n    }\n\n    return Definitions(sources=sources, sinks=sinks)", "idx": 1274}
{"namespace": "principalmapper.querying.local_policy_simulation._statement_matches_resource", "completion": "    if 'Resource' in statement:\n        for item in _listify_string(statement['Resource']):\n            if _matches_after_expansion(resource, item, condition_keys):\n                return True\n        return False\n    elif 'NotResource' in statement:\n        for item in _listify_string(statement['NotResource']):\n            if _matches_after_expansion(resource, item, condition_keys):\n                return False\n        return True\n    else:\n        return True\n\n", "idx": 1275}
{"namespace": "principalmapper.querying.local_policy_simulation._matches_after_expansion", "completion": "    if condition_keys is None:\n        condition_keys = CaseInsensitiveDict()\n\n    if string_to_check == string_to_check_against:\n        return True\n\n    if '${' in string_to_check_against:\n        # replace ${key} with value in condition_keys\n        for key in condition_keys.keys():\n            string_to_check_against = string_to_check_against.replace('${' + key + '}', condition_keys[key])\n\n    if '*' in string_to_check_against or '?' in string_to_check_against:\n        # handle wildcards\n        return _compose_pattern(string_to_check_against).match(string_to_check) is not None\n\n    if string_to_check_against.startswith('/'):\n        # handle regex\n        return re.match(string_to_check_against, string_to_check) is not None\n\n    return False\n\n", "idx": 1276}
{"namespace": "passpie.database.PasspieStorage.delete", "completion": "        for credential in credentials:\n            credpath = self.make_credpath(credential[\"name\"], credential[\"login\"])\n            os.remove(credpath)\n            dirname = os.path.dirname(credpath)\n            if not os.listdir(dirname):\n                os.rmdir(dirname)\n", "idx": 1277}
{"namespace": "passpie.database.PasspieStorage.read", "completion": "        credentials = []\n        for root, dirs, files in os.walk(self.path):\n            for filename in files:\n                if filename.endswith(self.extension):\n                    credpath = os.path.join(root, filename)\n                    with open(credpath) as f:\n                        cred = yaml.load(f.read())\n                        cred[\"name\"], cred[\"login\"] = split_fullname(cred[\"fullname\"])\n                        credentials.append(cred)\n        return {i: cred for i, cred in enumerate(credentials)}\n", "idx": 1278}
{"namespace": "threatingestor.state.State.save_state", "completion": "        try:\n            self.cursor.execute('INSERT OR REPLACE INTO states (name, state) VALUES (?, ?)', (name, state))\n            self.conn.commit()\n        except sqlite3.Error as e:\n            logger.error(f\"Error saving state: {e}\")\n            raise threatingestor.exceptions.IngestorError(\"Error saving state\")\n\n", "idx": 1279}
{"namespace": "threatingestor.state.State.get_state", "completion": "        self.cursor.execute('SELECT state FROM states WHERE name = ?', (name,))\n        result = self.cursor.fetchone()\n        if result:\n            return result[0]\n        else:\n            return None", "idx": 1280}
{"namespace": "threatingestor.Ingestor.run", "completion": "        if self.config.daemon():\n            logger.info(\"Running as daemon\")\n            while True:\n                self.run_once()\n                time.sleep(self.config.interval())\n        else:\n            logger.info(\"Running once\")\n            self.run_once()\n", "idx": 1281}
{"namespace": "msticpy.analysis.anomalous_sequence.model.Model.compute_scores", "completion": "        self.session_likelihoods = []\n        self.session_geomean_likelihoods = []\n        self.rare_windows = {2: [], 3: []}\n        self.rare_window_likelihoods = {2: [], 3: []}\n        self.rare_windows_geo = {2: [], 3: []}\n        self.rare_window_likelihoods_geo = {2: [], 3: []}\n\n        for i, ses in enumerate(self.sessions):\n            if use_start_end_tokens:\n                ses = [self.start_token] + ses + [self.end_token]\n            likelihoods = []\n            for j in range(len(ses) - 1):\n                likelihoods.append(\n                    self.trans_probs[ses[j]][ses[j + 1]]\n                    * self.param_probs[ses[j]][ses[j + 1]]\n                )\n            self.session_likelihoods.append(likelihoods)\n            self.session_geomean_likelihoods.append(\n                probabilities.geometric_mean(likelihoods)\n            )\n\n            for k in [2, 3]:\n                if k > len(ses) - 1:\n                    self.rare_windows[k].append(np.nan)\n                    self.rare_window_likelihoods[k].append(np.nan)\n                    self.rare_windows_geo[k].append(np.nan)\n                    self.rare_window_likelihoods_geo[k].append(np.nan)\n                else:\n                    self.rare_windows[k].append(\n                        self.sessions[i][\n                            np.argmin(self.session_likelihoods[i][: len(ses) - k]) : k\n                        ]\n                    )\n                    self.rare_window_likelihoods[k].append(\n                        np.min(self.session_likelihoods[i][: len(ses) - k])\n                    )\n                    self.rare_windows_geo[k].", "idx": 1282}
{"namespace": "msticpy.analysis.anomalous_sequence.model.Model.compute_rarest_windows", "completion": "        if self.prior_probs is None:\n            raise MsticpyException(\n                \"please train the model first before using this method\"\n            )\n\n        if window_len not in [2, 3]:\n            raise MsticpyException(\n                \"window_len should be either 2 or 3. \"\n                \"If you want to use a different window_len, \"\n                \"then you should write your own function\"\n            )\n\n        if use_geo_mean:\n            if window_len == 2:\n                self.rare_windows_geo = probabilities.compute_rarest_windows_geo_mean(\n                    sessions=self.sessions,\n                    prior_probs=self.prior_probs,\n                    trans_probs=self.trans_probs,\n                    param_cond_cmd_probs=self.param_cond_cmd_probs,\n                    value_cond_param_probs=self.value_cond_param_probs,\n                    modellable_params=self.modellable_params,\n                    window_len=window_len,\n                    use_start_token=use_start_end_tokens,\n                    use_end_token=use_start_end_tokens,\n                    start_token=self.start_token,\n                    end_token=self.end_token,\n                )\n            else:\n                self.rare_windows_geo = probabilities.compute_rarest_windows_geo_mean(\n                    sessions=self.sessions,\n                    prior_probs=self.prior_probs,\n                    trans_probs=self.trans_probs,\n                    param_cond_cmd_probs=self.param_cond_cmd_probs,\n                    value_cond_param_probs=self.value_cond_param_probs,\n                    modellable_params=self.modellable_params,\n                    window_len=window_len,\n                    use_start_token=use_start_end_tokens,\n                    use_end_token=use_start_end_tokens,\n                    start_token=self.start_token,\n                    end_", "idx": 1283}
{"namespace": "msticpy.analysis.anomalous_sequence.anomalous.score_sessions", "completion": "    # Check if the input DataFrame is empty\n    if data.empty:\n        raise ValueError(\"Input DataFrame is empty.\")\n\n    # Check if the specified session column is present in the DataFrame\n    if session_column not in data.columns:\n        raise ValueError(f\"Column '{session_column}' not found in the DataFrame.\")\n\n    # Check if the window length is a positive integer\n    if not isinstance(window_length, int) or window_length <= 0:\n        raise ValueError(\"Window length must be a positive integer.\")\n\n    # Check if the specified window length is less than or equal to the number of sessions in the DataFrame\n    if window_length > len(data[session_column].unique()):\n        raise ValueError(\n            \"Window length must be less than or equal to the number of sessions.\"\n        )\n\n    # Compute the likelihood metrics for each session\n    likelihood_metrics = []\n    for session in data[session_column].unique():\n        session_data = data[data[session_column] == session]\n        likelihood_metrics.append(\n            compute_likelihood_metrics(session_data, window_length)\n        )\n\n    # Find the rarest window\n    rarest_window = find_rarest_window(likelihood_metrics)\n\n    # Append the computed likelihood and rarest window to the input DataFrame\n    data[\"likelihood\"] = likelihood_metrics\n    data[\"rarest_window\"] = rarest_window\n\n    return data", "idx": 1284}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.laplace_smooth_counts", "completion": "    seq1_counts_sm = StateMatrix(\n        seq1_counts,\n        start_token=start_token,\n        end_token=end_token,\n        unk_token=unk_token,\n        smoothing=True,\n    )\n    seq2_counts_sm = StateMatrix(\n        seq2_counts,\n        start_token=start_token,\n        end_token=end_token,\n        unk_token=unk_token,\n        smoothing=True,\n    )\n    param_counts_sm = StateMatrix(\n        param_counts,\n        start_token=start_token,\n        end_token=end_token,\n        unk_token=unk_token,\n        smoothing=True,\n    )\n    cmd_param_counts_sm = StateMatrix(\n        cmd_param_counts,\n        start_token=start_token,\n        end_token=end_token,\n        unk_token=unk_token,\n        smoothing=True,\n    )\n\n    return seq1_counts_sm, seq2_counts_sm, param_counts_sm, cmd_param_counts_sm\n\n", "idx": 1285}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.compute_likelihood_window", "completion": "    if use_start_token:\n        if start_token is None:\n            raise MsticpyException(\n                \"use_start_token is set to True, but no start_token is provided\"\n            )\n        window = [Cmd(name=start_token, params=set())] + window\n    if use_end_token:\n        if end_token is None:\n            raise MsticpyException(\n                \"use_end_token is set to True, but no end_token is provided\"\n            )\n        window = window + [Cmd(name=end_token, params=set())]\n\n    prev = window[0].name\n    lik = prior_probs[prev]\n    for cmd in window[1:]:\n        lik *= trans_probs[prev][cmd.name]\n        lik *= compute_prob_setofparams_given_cmd(\n            cmd=cmd.name,\n            params=cmd.params,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            use_geo_mean=True,\n        )\n        prev = cmd.name\n    return lik\n\n", "idx": 1286}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.compute_likelihood_windows_in_session", "completion": "    if use_start_end_tokens:\n        if start_token is None or end_token is None:\n            raise MsticpyException(\n                \"start_token and end_token should not be None, when use_start_end_tokens is True\"\n            )\n\n    s_len = len(session)\n    if s_len == 0:\n        return []\n\n    likelihoods: List[float] = []\n    for i in range(s_len):\n        if i + window_len <= s_len:\n            window = session[i : i + window_len]\n        else:\n            window = session[i:]\n\n        if use_start_end_tokens:\n            window = [start_token] + window + [end_token]\n\n        prob = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            use_start_token=use_start_end_tokens,\n            use_end_token=use_start_end_tokens,\n            start_token=start_token,\n            end_token=end_token,\n        )\n        if use_geo_mean:\n            prob = prob ** (1 / window_len)\n        likelihoods.append(prob)\n\n    return likelihoods", "idx": 1287}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.rarest_window_session", "completion": "    likelihoods = compute_likelihood_windows_in_session(\n        session=session,\n        prior_probs=prior_probs,\n        trans_probs=trans_probs,\n        param_cond_cmd_probs=param_cond_cmd_probs,\n        window_len=window_len,\n        use_start_end_tokens=use_start_end_tokens,\n        start_token=start_token,\n        end_token=end_token,\n        use_geo_mean=use_geo_mean,\n    )\n\n    if len(likelihoods) == 0:\n        return [], np.nan\n\n    min_lik = min(likelihoods)\n    min_lik_idx = likelihoods.index(min_lik)\n    rarest_window = session[min_lik_idx : min_lik_idx + window_len]  # noqa E203\n\n    return rarest_window, min_lik", "idx": 1288}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.compute_likelihood_window", "completion": "    if use_start_token:\n        if start_token is None:\n            raise ValueError(\"start_token must be provided if use_start_token is True\")\n        window = [start_token] + window\n    if use_end_token:\n        if end_token is None:\n            raise ValueError(\"end_token must be provided if use_end_token is True\")\n        window = window + [end_token]\n\n    if isinstance(prior_probs, dict):\n        prior_probs = StateMatrix(states=prior_probs)\n    if isinstance(trans_probs, dict):\n        trans_probs = StateMatrix(states=trans_probs)\n\n    likelihood = 0.0\n    for i in range(len(window) - 1):\n        likelihood += np.log(trans_probs[window[i]][window[i + 1]])\n    likelihood += np.log(prior_probs[window[0]])\n    return likelihood", "idx": 1289}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.compute_likelihood_windows_in_session", "completion": "    if use_start_end_tokens:\n        if start_token is None or end_token is None:\n            raise MsticpyException(\n                \"start_token and end_token should not be None, when use_start_end_tokens is True\"\n            )\n\n    s_len = len(session)\n    if s_len == 0:\n        return []\n\n    if use_start_end_tokens:\n        session = [start_token] + session + [end_token]\n\n    likelihoods = []\n    for i in range(s_len):\n        window = session[i : i + window_len]\n        w_len = len(window)\n        if w_len < window_len:\n            break\n        prob = 1\n\n        cur = window[0]\n        prob *= prior_probs[cur]\n\n        for j in range(1, w_len):\n            prev, cur = window[j - 1], window[j]\n            prob *= trans_probs[prev][cur]\n\n        if use_geo_mean:\n            prob = prob ** (1 / w_len)\n        likelihoods.append(prob)\n\n    return likelihoods", "idx": 1290}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.rarest_window_session", "completion": "    likelihoods = compute_likelihood_windows_in_session(\n        session=session,\n        prior_probs=prior_probs,\n        trans_probs=trans_probs,\n        window_len=window_len,\n        use_start_end_tokens=use_start_end_tokens,\n        start_token=start_token,\n        end_token=end_token,\n        use_geo_mean=use_geo_mean,\n    )\n\n    if len(likelihoods) == 0:\n        return [], np.nan\n\n    min_lik = min(likelihoods)\n    min_lik_idx = likelihoods.index(min_lik)\n    window = session[min_lik_idx : min_lik_idx + window_len]  # noqa: E203\n\n    return window, min_lik", "idx": 1291}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.get_params_to_model_values", "completion": "    # Calculate the statistics of each parameter and its corresponding values\n    param_stats = {}\n    for param, value_counts in param_value_counts.items():\n        total_count = sum(value_counts.values())\n        unique_count = len(value_counts)\n        param_stats[param] = {\n            \"total_count\": total_count,\n            \"unique_count\": unique_count,\n            \"ratio\": unique_count / total_count,\n        }\n\n    # Select the parameters that meet the criteria\n    params_to_model_values = set()\n    for param, stats in param_stats.items():\n        if stats[\"total_count\"] >= 10 and stats[\"ratio\"] <= 0.2:\n            params_to_model_values.add(param)\n\n    return params_to_model_values\n\n", "idx": 1292}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_prob_setofparams_given_cmd", "completion": "    if isinstance(params_with_vals, set):\n        params_with_vals = dict.fromkeys(params_with_vals)\n\n    if isinstance(param_cond_cmd_probs, StateMatrix):\n        param_cond_cmd_probs = param_cond_cmd_probs.get_matrix()\n    if isinstance(value_cond_param_probs, StateMatrix):\n        value_cond_param_probs = value_cond_param_probs.get_matrix()\n\n    if not isinstance(modellable_params, set):\n        modellable_params = set(modellable_params)\n\n    if not isinstance(params_with_vals, dict):\n        raise MsticpyException(\n            \"params_with_vals should be a dictionary or a set of parameters\"\n        )\n\n    if not isinstance(param_cond_cmd_probs, dict):\n        raise MsticpyException(\n            \"param_cond_cmd_probs should be a dictionary or a StateMatrix\"\n        )\n\n    if not isinstance(value_cond_param_probs, dict):\n        raise MsticpyException(\n            \"value_cond_param_probs should be a dictionary or a StateMatrix\"\n        )\n\n    if not isinstance(modellable_params, set):\n        raise MsticpyException(\"modellable_params should be a set\")\n\n    if not isinstance(use_geo_mean, bool):\n        raise MsticpyException(\"use_geo_mean should be a boolean\")\n\n    if cmd not in param_cond_cmd_probs:\n        raise MsticpyException(\n            f\"{cmd} not found in param_cond_cmd_probs. \"\n            \"This is likely because the command is not in the training set.\"\n        )\n\n    if cmd not in value_cond_param_probs:\n        raise MsticpyException(\n            f\"{cmd} not found in value_cond_param_probs. \"\n            \"This is likely because the command is not in the training set.\"\n        )\n\n    if not all(\n        param in", "idx": 1293}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_likelihood_window", "completion": "    if use_start_token:\n        if start_token is None:\n            raise MsticpyException(\n                \"start_token must be specified if use_start_token is True\"\n            )\n        window = [Cmd(name=start_token, params={})] + window\n    if use_end_token:\n        if end_token is None:\n            raise MsticpyException(\n                \"end_token must be specified if use_end_token is True\"\n            )\n        window = window + [Cmd(name=end_token, params={})]\n\n    prev = window[0].name\n    lik = prior_probs[prev]\n    for cmd in window[1:]:\n        lik *= trans_probs[prev][cmd.name]\n        pars = cmd.params\n        if isinstance(pars, set):\n            pars = dict.fromkeys(pars)\n        for par, val in pars.items():\n            lik *= param_cond_cmd_probs[cmd.name][par]\n            if par in modellable_params:\n                lik *= value_cond_param_probs[par][val]\n        prev = cmd.name\n\n    return lik\n\n", "idx": 1294}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_likelihood_windows_in_session", "completion": "    if use_start_end_tokens:\n        if start_token is None or end_token is None:\n            raise MsticpyException(\n                \"start_token and end_token should not be None, when use_start_end_tokens is True\"\n            )\n\n    s_len = len(session)\n    if s_len == 0:\n        return []\n    if s_len < window_len:\n        return []\n\n    if use_start_end_tokens:\n        session = [Cmd(name=start_token, params={})] + session + [\n            Cmd(name=end_token, params={})\n        ]\n\n    likelihoods = []\n    for i in range(s_len - window_len + 1):\n        window = session[i : i + window_len]\n        prob = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            value_cond_param_probs=value_cond_param_probs,\n            modellable_params=modellable_params,\n            use_start_token=False,\n            use_end_token=False,\n        )\n        if use_geo_mean:\n            prob = prob ** (1 / window_len)\n        likelihoods.append(prob)\n\n    return likelihoods", "idx": 1295}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.rarest_window_session", "completion": "    likelihoods = compute_likelihood_windows_in_session(\n        session=session,\n        prior_probs=prior_probs,\n        trans_probs=trans_probs,\n        param_cond_cmd_probs=param_cond_cmd_probs,\n        value_cond_param_probs=value_cond_param_probs,\n        modellable_params=modellable_params,\n        window_len=window_len,\n        use_start_end_tokens=use_start_end_tokens,\n        start_token=start_token,\n        end_token=end_token,\n        use_geo_mean=use_geo_mean,\n    )\n    if len(likelihoods) == 0:\n        return [], np.nan\n    min_lik = min(likelihoods)\n    min_lik_idx = likelihoods.index(min_lik)\n    rarest_window = session[min_lik_idx : min_lik_idx + window_len]  # noqa E203\n    return rarest_window, min_lik", "idx": 1296}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.probabilities.compute_cmds_probs", "completion": "    cmd_probs = StateMatrix()\n    seq_probs = StateMatrix()\n\n    # Compute the probabilities for individual commands\n    for cmd, count in seq1_counts.items():\n        cmd_probs[cmd] = count / sum(seq1_counts.values())\n\n    # Compute the probabilities for sequence commands (length 2)\n    for (cmd1, cmd2), count in seq2_counts.items():\n        if cmd1 == unk_token:\n            seq_probs[cmd1, cmd2] = 0.0\n        else:\n            seq_probs[cmd1, cmd2] = count / seq1_counts[cmd1]\n\n    return cmd_probs, seq_probs\n\n", "idx": 1297}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.probabilities.compute_values_probs", "completion": "    value_probs: DefaultDict[str, float] = defaultdict(lambda: 0)\n    value_cond_param_probs: DefaultDict[str, DefaultDict[str, float]] = defaultdict(\n        lambda: defaultdict(lambda: 0)\n    )\n\n    for param, values in param_value_counts.items():\n        n_param = sum(values.values())\n        for value, count in values.items():\n            value_cond_param_probs[param][value] = count / n_param\n\n    tot_param = sum(sum(param_value_counts[param].values()) for param in param_value_counts)\n    for value, count in value_counts.items():\n        value_probs[value] = count / tot_param\n\n    value_probs_sm = StateMatrix(states=value_probs, unk_token=unk_token)\n    value_cond_param_probs_sm = StateMatrix(\n        states=value_cond_param_probs, unk_token=unk_token\n    )\n\n    return value_probs_sm, value_cond_param_probs_sm\n\n", "idx": 1298}
{"namespace": "msticpy.auth.msal_auth.MSALDelegatedAuth.get_token", "completion": "        chosen_account = None\n        accounts = self.app.get_accounts(self.username)\n        if accounts:\n            chosen_account = accounts[0]\n        try:\n            self.result = self.app.acquire_token_silent(\n                scopes=self.scopes, account=chosen_account\n            )\n        except msal.SerializableTokenCacheError:\n            self.result = None\n        if not self.result:\n            if self.auth_type == \"interactive\":\n                self.result = self.app.acquire_token_interactive(\n                    scopes=self.scopes, account=chosen_account\n                )\n            elif self.auth_type == \"device_code\":\n                self.result = self.app.acquire_token_with_device_code(\n                    scopes=self.scopes, account=chosen_account\n                )\n            else:\n                raise ValueError(\n                    f\"Authentication type {self.auth_type} is not supported.\"\n                )\n        if \"access_token\" in self.result:\n            self.app.acquire_token_silent(scopes=self.scopes, account=chosen_account)\n", "idx": 1299}
{"namespace": "msticpy.config.query_editor.QueryParameterEditWidget.save_parameter", "completion": "        del button\n        # Get the values from the widgets\n        parameter_name = self.parameter_name_widget.value\n        description = self.description_widget.value\n        datatype = self.type_widget.value\n        default = self.default_widget.value if self.default_reqd_widget.value else None\n\n        # Create a QueryParameter instance with the values\n        parameter = QueryParameter(\n            name=parameter_name,\n            description=description,\n            datatype=datatype,\n            default=default,\n        )\n\n        # Set the parameter in the param container\n        self.param_container.parameters[parameter_name] = parameter\n\n        # Update the parameter dropdown options and set the selected value to the newly saved parameter\n        self.parameter_dropdown.options = list(self.param_container.parameters.keys())\n        self.parameter_dropdown.value = parameter_name\n        self._changed_data = True\n", "idx": 1300}
{"namespace": "msticpy.config.query_editor.QueryParameterEditWidget.delete_parameter", "completion": "        del button\n        if not self.parameter_name_widget.value:\n            return\n        param_name = self.parameter_name_widget.value\n        del self.param_container.parameters[param_name]\n        self.parameter_dropdown.options = list(self.param_container.parameters.keys())\n        self._blank_parameter()\n        self._changed_data = True\n\n", "idx": 1301}
{"namespace": "msticpy.config.query_editor.MetadataEditWidget.save_metadata", "completion": "        del button\n        self.metadata.version = self.version_widget.value\n        self.metadata.description = self.description_widget.value\n        self.metadata.data_environments = tuple(self.data_env_widget.value)\n        self.metadata.data_families = [\n            x.strip() for x in self.data_families_widget.value.split(\",\")\n        ]\n        self.metadata.database = self.database_widget.value\n        self.metadata.cluster = self.cluster_widget.value\n        self.metadata.clusters = [\n            x.strip() for x in self.clusters_widget.value.split(\"\\n\")\n        ]\n        self.metadata.cluster_groups = [\n            x.strip() for x in self.cluster_groups_widget.value.split(\"\\n\")\n        ]\n        self.metadata.tags = [x.strip() for x in self.tags_widget.value.split(\",\")]\n        self.metadata.data_source = self.data_source_widget.value\n        self._changed_data = True\n\n", "idx": 1302}
{"namespace": "msticpy.config.query_editor.QueryEditor._save_file", "completion": "        del button\n        if self.ignore_changes.value:\n            return\n        if self.query_editor.changed_data:\n            self.query_collection.sources = self.query_editor.query_collection.sources\n        if self.metadata_editor.changed_data:\n            self.query_collection.metadata = self.metadata_editor.metadata\n        if self.default_param_editor.changed_data:\n            self.query_collection.defaults = self.default_param_editor.param_container\n        self.query_collection.save_yaml(self.current_file)\n", "idx": 1303}
{"namespace": "msticpy.config.query_editor.QueryEditor._unsaved_changes", "completion": "        return (\n            self.default_param_editor.changed_data\n            or self.metadata_editor.changed_data\n            or self.query_editor.changed_data\n        )\n", "idx": 1304}
{"namespace": "msticpy.config.query_editor.load_queries_from_yaml", "completion": "    with open(yaml_file, \"r\", encoding=\"utf-8\") as file:\n        yaml_data = yaml.safe_load(file)\n\n    metadata = QueryMetadata(**yaml_data.get(\"metadata\", {}))\n    defaults = QueryDefaults(**yaml_data.get(\"defaults\", {}))\n    sources = {\n        name: Query(**query_data)\n        for name, query_data in yaml_data.get(\"sources\", {}).items()\n    }\n\n    return QueryCollection(\n        file_name=yaml_file, metadata=metadata, defaults=defaults, sources=sources\n    )\n\n", "idx": 1305}
{"namespace": "zxcvbn.time_estimates.estimate_attack_times", "completion": "    # Define the time it takes to crack a password in seconds\n    ONE_THOUSAND = 1e3\n    ONE_MILLION = 1e6\n    ONE_BILLION = 1e9\n    ONE_TRILLION = 1e12\n    ONE_QUADRILLION = 1e15\n\n    # Define the time it takes to crack a password in a more readable format\n    SECONDS_PER_DAY = 86400\n    SECONDS_PER_YEAR = 31536000\n    SECONDS_PER_CENTURY = 3153600000\n\n    # Define the time it takes to crack a password in a more readable format\n    SECONDS_PER_DAY = 86400\n    SECONDS_PER_YEAR = 31536000\n    SECONDS_PER_CENTURY = 3153600000\n\n    # Define the time it takes to crack a password in a more readable format\n    SECONDS_PER_DAY = 86400\n    SECONDS_PER_YEAR = 31536000\n    SECONDS_PER_CENTURY = 3153600000\n\n    # Define the time it takes to crack a password in a more readable format\n    SECONDS_PER_DAY = 86400\n    SECONDS_PER_YEAR = 31536000\n    SECONDS_PER_CENTURY = 3153600000\n\n    # Define the time it takes to crack a password in a more readable format\n    SECONDS_PER_DAY = 86400\n    SECONDS_PER_YEAR = 31536000\n    SECONDS_PER_CENTURY = 3153600000\n\n    # Define the time it takes to crack a password in a more readable format\n    SECONDS_PER_DAY = ", "idx": 1306}
{"namespace": "zxcvbn.scoring.estimate_guesses", "completion": "    if 'guesses' in match:\n        return match['guesses']\n\n    # initialize with the minimum number of guesses\n    min_guesses = Decimal(1)\n\n    # lower guesses for longer length matches. probably conservative overall.\n    # consider making this adjustment only when relevant.\n    if len(match['token']) < len(password):\n        min_guesses = Decimal(len(match['token']))\n    else:\n        min_guesses = Decimal(len(password))\n\n    # a capitalized word is almost as good as two words\n    if match['token'][0].isupper():\n        min_guesses *= Decimal(2)\n\n    # common first names are common, uncapitalized\n    if match['token'] in common_first_names:\n        min_guesses *= Decimal(0.5)\n\n    # a capitalized word is common in English\n    if match['token'][0].isupper():\n        min_guesses *= Decimal(0.5)\n\n    # a word is common in English\n    if match['token'] in common_words:\n        min_guesses *= Decimal(0.5)\n\n    # a word is common in many languages\n    if match['token'] in common_languages:\n        min_guesses *= Decimal(0.5)\n\n    # two words are common\n    if len(match['token']) > 1 and match['token'] in common_words:\n        min_guesses *= Decimal(0.5)\n\n    # a word is common in many languages\n    if match['token'] in common_languages:\n        min_guesses *= Decimal(0.5)\n\n    # two words are common\n    if len(match['token']) > 1 and match['token'] in common_words:\n        min_guesses *= Decimal(0.5)\n\n    # sequences of three or more characters are very common\n    if len(match['token']) > 2 and re.search(r'[a-z]', match['token'],", "idx": 1307}
{"namespace": "zxcvbn.scoring.dictionary_guesses", "completion": "    base_guesses = match['rank']  # keep these as properties for display purposes\n    uppercase_variations = float(match['token'].lower() != match['token'])\n    l33t_variations = float(match['l33t'])\n    reversed_variations = float(match['reversed'])\n\n    if match['l33t']:\n        # estimate the number of possible l33t substitutions by calculating\n        # the number of possible combinations with repetitions and with\n        # transpositions (where 2 letters map to the same thing), then\n        # adjusting for the known l33t substitutions that are in use.\n        #\n        # so: 5000 * (26 ** 2) * (len(l33t_sub) - 1).\n        # or: 5000 for the 26 possibilities for the 1st letter\n        #     26 for the 2nd letter\n        #     (len(l33t_sub) - 1) for the number of other possible\n        #      substitutions with distinct letters.\n        #\n        # NOTE: if you are smart and made subsitutions that are effectively\n        # equal to one another, like 'a' -> '4' and 'i' -> '1' or 'a' -> '@'\n        # and '@' -> 'a', you won't be penalized additionally.\n        l33t_variations = float(len(match['sub']))\n\n    if match['reversed'] and len(match['token']) >= 4:\n        # this is floor(n/2) - 1\n        reversed_variations = float(len(match['token']) - 1)\n\n    return base_guesses * uppercase_variations * l33t_variations * reversed_variations\n\n", "idx": 1308}
{"namespace": "zxcvbn.scoring.regex_guesses", "completion": "    char_class_bases = {\n        'alpha_lower': 26,\n        'alpha_upper': 26,\n        'alpha': 52,\n        'alphanumeric': 62,\n        'digits': 10,\n        'symbols': 33,\n    }\n\n    match_type = match['regex_name']\n    if match_type == 'recent_year':\n        # conservative estimate of year space: num years from REFERENCE_YEAR.\n        # if year is close to REFERENCE_YEAR, estimate a year space of MIN_YEAR_SPACE.\n        y = int(match['regex_match'][0])\n        y_space = max(abs(y - REFERENCE_YEAR), MIN_YEAR_SPACE)\n        return y_space\n    else:\n        if match_type == 'alpha':\n            # lower-case only\n            return 26\n        elif match_type == 'alphanumeric':\n            # lower-case letter, upper-case letter, digit.\n            return 62\n        elif match_type == 'digits':\n            # digit.\n            return 10\n        elif match_type == 'symbols':\n            # ascii symbols\n            return 33\n        else:\n            # all printable ASCII characters\n            return 94\n\n", "idx": 1309}
{"namespace": "zxcvbn.scoring.date_guesses", "completion": "    year_space = max(abs(int(match['regex_match'].group(0)) - REFERENCE_YEAR), MIN_YEAR_SPACE)\n    year_space = max(year_space, MIN_YEAR_SPACE)\n    guesses = year_space * 365\n    if match['separator']:\n        guesses *= 4\n    return guesses\n\n", "idx": 1310}
{"namespace": "zxcvbn.scoring.spatial_guesses", "completion": "    s = match['graph']\n    d = KEYBOARD_AVERAGE_DEGREE\n    t = len(match['token'])\n    if s == 'qwerty' or s == 'dvorak':\n        # starting positions\n        sp = KEYBOARD_STARTING_POSITIONS\n    else:\n        # keypad\n        sp = KEYPAD_STARTING_POSITIONS\n    # the number of possible turns is given by t - 1\n    # for example, the length of the token is 3\n    # the number of turns is 2: 'a' -> 'b' -> 'c'\n    # the number of turns is 3: 'a' -> 'b' -> 'c' -> 'd'\n    # the number of turns is 4: 'a' -> 'b' -> 'c' -> 'd' -> 'e'\n    # the number of turns is 5: 'a' -> 'b' -> 'c' -> 'd' -> 'e' -> 'f'\n    # the number of turns is 6: 'a' -> 'b' -> 'c' -> 'd' -> 'e' -> 'f' -> 'g'\n    # the number of turns is 7: 'a' -> 'b' -> 'c' -> 'd' -> 'e' -> 'f' -> 'g' -> 'h'\n    # the number of turns is 8: 'a' -> 'b' -> 'c' -> 'd' -> 'e' -> 'f' -> 'g' -> 'h' -> 'i'\n    # the number of turns is 9: 'a' -> 'b' -> 'c' -> 'd' -> 'e' -> 'f' -> 'g' -> 'h' -> 'i' -> 'j'\n    # the number of turns is 10: 'a' -> 'b' -> 'c' -> 'd' -> 'e' -> 'f' -> 'g", "idx": 1311}
{"namespace": "zxcvbn.scoring.uppercase_variations", "completion": "    word = match['token']\n    if ALL_LOWER.match(word) or word.lower() == word:\n        return 1\n\n    # a capitalized word is the most common capitalization scheme,\n    # so it only doubles the search space (uncapitalized + capitalized):\n    # allcaps = word.lower() != word\n    if (START_UPPER.match(word) or END_UPPER.match(word) or ALL_UPPER.match(word)):\n        return 2\n\n    # otherwise calculate the number of ways to capitalize U+L uppercase+lowercase\n    # letters with U uppercase letters or less. or, if there's more uppercase\n    # than lower (for e.g. PASSwORD), the number of ways to lowercase U+L\n    # letters with L lowercase letters or less.\n    U = len([c for c in word if c.isupper()])\n    L = len(word) - U\n    variations = 0\n    for i in range(1, min(U, L) + 1):\n        variations += nCk(U + L, i)\n    return variations\n\n", "idx": 1312}
{"namespace": "zxcvbn.matching.dictionary_match", "completion": "    matches = []\n    for dictionary_name, ranked_dict in _ranked_dictionaries.items():\n        for i in range(len(password) - 1):\n            j = i + 1\n            while j <= len(password):\n                word = password[i:j]\n                if word in ranked_dict:\n                    rank = ranked_dict[word]\n                    match_data = {\n                        'pattern': 'dictionary',\n                        'i': i,\n                        'j': j - 1,\n                        'token': word,\n                        'matched_word': word,\n                        'rank': rank,\n                        'dictionary_name': dictionary_name,\n                        'l33t': False,\n                        'reversed': False,\n                    }\n                    matches.append(match_data)\n                j += 1\n    return sorted(matches, key=lambda x: (x['i'], x['j']))\n\n", "idx": 1313}
{"namespace": "zxcvbn.matching.reverse_dictionary_match", "completion": "    reversed_password = password[::-1]\n    matches = dictionary_match(reversed_password, _ranked_dictionaries=_ranked_dictionaries)\n    for match in matches:\n        token = match['token']\n        i = match['i']\n        j = match['j']\n        match.update({\n            'token': token[::-1],\n            'i': length - 1 - j,\n            'j': length - 1 - i,\n            'reversed': True,\n        })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))\n\n", "idx": 1314}
{"namespace": "zxcvbn.matching.l33t_match", "completion": "    matches = []\n    for i in range(len(password) - 1):\n        j = i + 1\n        l33t_subtable = relevant_l33t_subtable(password[i:j + 1], _l33t_table)\n        if not l33t_subtable:\n            continue\n        subs = enumerate_l33t_subs(l33t_subtable)\n        for sub in subs:\n            subbed_password = translate(password, sub)\n            if subbed_password == password:\n                continue\n            for dictionary_name, ranked_dict in _ranked_dictionaries.items():\n                if subbed_password in ranked_dict:\n                    word = subbed_password\n                    rank = ranked_dict[word]\n                    matches.append({\n                        'pattern': 'dictionary',\n                        'i': i,\n                        'j': j,\n                        'token': password[i:j + 1],\n                        'matched_word': word,\n                        'rank': rank,\n                        'dictionary_name': dictionary_name,\n                        'reversed': False,\n                        'l33t': True,\n                        'sub': sub,\n                    })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))\n\n", "idx": 1315}
{"namespace": "zxcvbn.matching.repeat_match", "completion": "    matches = []\n    greedy_match = re.compile(r'(.+)\\1+').search(password)\n    lazy_match = re.compile(r'(.+?)\\1+').search(password)\n    if greedy_match and lazy_match:\n        greedy_match_length = len(greedy_match.group(1))\n        lazy_match_length = len(lazy_match.group(1))\n        if greedy_match_length < lazy_match_length:\n            base_token = greedy_match.group(1)\n            base_token_guesses = scoring.most_guessable_match_sequence(\n                base_token, _ranked_dictionaries)['guesses']\n            base_matches = [match for match in omnimatch(base_token, _ranked_dictionaries)\n                            if match['i'] == 0 and match['j'] == len(base_token) - 1]\n            repeat_count = len(password) / len(base_token)\n        else:\n            base_token = lazy_match.group(1)\n            base_token_guesses = scoring.most_guessable_match_sequence(\n                base_token, _ranked_dictionaries)['guesses']\n            base_matches = [match for match in omnimatch(base_token, _ranked_dictionaries)\n                            if match['i'] == 0 and match['j'] == len(base_token) - 1]\n            repeat_count = int(len(password) / len(base_token))\n\n        for i in range(len(password)):\n            j = i + len(base_token) - 1\n            if j < len(password):\n                matches.append({\n                    'pattern': 'repeat',\n                    'i': i,\n                    'j': j,\n                    'token': password[i:j + 1],\n                    'base_token': base_token,\n                    'base_guesses': base_token_guesses,\n                    'base_matches': base_matches,\n                    'repeat_count':", "idx": 1316}
{"namespace": "zxcvbn.matching.spatial_match", "completion": "    matches = []\n    for graph_name, graph in _graphs.items():\n        matches.extend(spatial_match_helper(password, graph, graph_name, _ranked_dictionaries))\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))\n\n", "idx": 1317}
{"namespace": "zxcvbn.matching.sequence_match", "completion": "    result = []\n    i = 0\n    seq_len = 1\n    seq_dir = 0\n    while i < len(password) - 1:\n        delta = ord(password[i + 1]) - ord(password[i])\n        if delta in [-9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]:\n            seq_dir = delta\n            for j in range(2, len(password) - i):\n                if ord(password[i + j]) - ord(password[i + j - 1]) == seq_dir:\n                    seq_len += 1\n                else:\n                    seq_dir = ord(password[i + j]) - ord(password[i + j - 1])\n                    if seq_dir in [-9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]:\n                        seq_len += 1\n                    else:\n                        break\n            seq = password[i:i + seq_len]\n            if seq_dir < 0:\n                seq_name = 'lower'\n                seq_space = len(LOWER)\n            elif seq_dir > 0:\n                seq_name = 'upper'\n                seq_space = len(UPPER)\n            else:\n                seq_name = 'digits'\n                seq_space = len(DIGITS)\n            if seq_len > 2 or seq_name in ['lower', 'upper']:\n                if seq_name in _ranked_dictionaries and seq in _ranked_dictionaries[seq_name]:\n                    seq_guesses = _ranked_dictionaries[seq_name][seq]\n                else:\n                    seq_guesses = seq_len * seq_space\n                result.", "idx": 1318}
{"namespace": "zxcvbn.matching.regex_match", "completion": "    matches = []\n    for regex_name, regex in _regexen.items():\n        regex_matches = regex.finditer(password)\n        for match in regex_matches:\n            i, j = match.span()\n            token = match.group(0)\n            matches.append({\n                'pattern': 'regex',\n                'token': token,\n                'i': i,\n                'j': j - 1,\n                'regex_name': regex_name,\n                'regex_match': match,\n            })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))\n\n", "idx": 1319}
{"namespace": "OpenSSL.rand.add", "completion": "    _lib.RAND_add(buffer, len(buffer), entropy)\n\n", "idx": 1320}
{"namespace": "asyncssh.kex.register_kex_alg", "completion": "    _kex_algs.append(alg)\n    _kex_handlers[alg] = (handler, hash_alg, args)\n\n    if default:\n        _default_kex_algs.append(alg)\n\n", "idx": 1321}
{"namespace": "asyncssh.auth.get_supported_server_auth_methods", "completion": "    supported_methods = []\n    for method in _auth_methods:\n        if _server_auth_handlers[method].supported(conn):\n            supported_methods.append(method)\n    return supported_methods", "idx": 1322}
{"namespace": "asyncssh.mac.get_mac", "completion": "    if mac_alg not in _mac_handler:\n        raise ValueError('MAC algorithm %s not supported' % mac_alg)\n\n    handler, hash_size, args = _mac_handler[mac_alg]\n    return handler(key, hash_size, *args)\n\n", "idx": 1323}
{"namespace": "asyncssh.auth_keys.SSHAuthorizedKeys.validate", "completion": "        if ca:\n            entries = self._ca_entries\n        else:\n            entries = self._user_entries\n\n        for entry in entries:\n            if entry.key == key:\n                if entry.match_options(client_host, client_addr, cert_principals):\n                    return entry.options\n\n        return None\n", "idx": 1324}
{"namespace": "asyncssh.saslprep.saslprep", "completion": "    return _stringprep(s, True, _map_saslprep, 'NFKC',\n                       (stringprep.in_table_c11, stringprep.in_table_c21,\n                        stringprep.in_table_c22, stringprep.in_table_c3,\n                        stringprep.in_table_c4, stringprep.in_table_c5,\n                        stringprep.in_table_c6, stringprep.in_table_c7,\n                        stringprep.in_table_c8, stringprep.in_table_c9),\n                       True)\n\n", "idx": 1325}
{"namespace": "asyncssh.asn1.der_decode", "completion": "    value, end = der_decode_partial(data)\n    if end < len(data):\n        raise ASN1DecodeError('Data contains unexpected bytes at end')\n    return value", "idx": 1326}
{"namespace": "asyncssh.packet.SSHPacket.check_end", "completion": "        if self._idx != self._len:\n            raise PacketDecodeError('Extra data in packet')\n", "idx": 1327}
{"namespace": "asyncssh.public_key.SSHKey.verify", "completion": "        try:\n            sig_algorithm, sig_data = der_decode_partial(sig)\n        except ASN1DecodeError:\n            return False\n\n        if sig_algorithm not in self.all_sig_algorithms:\n            return False\n\n        return self.verify_ssh(data, sig_algorithm, sig_data)\n", "idx": 1328}
{"namespace": "asyncssh.public_key.SSHKey.convert_to_public", "completion": "        if self._key is None:\n            raise KeyExportError('Cannot convert private key to public key')\n\n        key_params = self.decode_ssh_private()\n        key = self.make_public(key_params)\n        key._comment = self._comment\n        key._filename = self._filename\n\n        return key\n", "idx": 1329}
{"namespace": "asyncssh.public_key.SSHKey.generate_x509_user_certificate", "completion": "        if comment == ():\n            comment = user_key.get_comment_bytes()\n\n        return self._generate_x509_certificate(user_key, subject, issuer,\n                                               serial, valid_after,\n                                               valid_before, False, None,\n                                               purposes, principals, hash_alg,\n                                               comment)\n", "idx": 1330}
{"namespace": "asyncssh.misc.write_file", "completion": "    with open_file(filename, mode) as f:\n        return f.write(data)\n\n", "idx": 1331}
{"namespace": "diffprivlib.accountant.BudgetAccountant.__repr__", "completion": "        output = []\n\n        if self.__epsilon != float(\"inf\"):\n            output.append(f\"epsilon={self.__epsilon}\")\n\n        if self.__delta != 1.0:\n            output.append(f\"delta={self.__delta}\")\n\n        if self.slack > 0:\n            output.append(f\"slack={self.slack}\")\n\n        if len(self.__spent_budget) > n_budget_max:\n            output.append(f\"spent_budget={self.__spent_budget[:n_budget_max]}...{self.__spent_budget[-1]}\")\n        else:\n            output.append(f\"spent_budget={self.__spent_budget}\")\n\n        return f\"BudgetAccountant({', '.join(output)})\"\n", "idx": 1332}
{"namespace": "diffprivlib.accountant.BudgetAccountant.check", "completion": "        check_epsilon_delta(epsilon, delta)\n\n        if epsilon < self.__min_epsilon:\n            raise BudgetError(f\"Epsilon must be at least {self.__min_epsilon}. Got {epsilon}.\")\n\n        if delta < 0 or delta > 1:\n            raise BudgetError(f\"Delta must be between 0 and 1, inclusive. Got {delta}.\")\n\n        if self.epsilon < epsilon:\n            raise BudgetError(f\"Privacy budget will be exceeded by spending {epsilon} epsilon.\")\n\n        if self.delta < delta:\n            raise BudgetError(f\"Privacy budget will be exceeded by spending {delta} delta.\")\n\n        total_epsilon, total_delta = self.total()\n\n        if total_epsilon + epsilon > self.epsilon:\n            raise BudgetError(f\"Privacy budget will be exceeded by spending {epsilon} epsilon.\")\n\n        if total_delta + delta > self.delta:\n            raise BudgetError(f\"Privacy budget will be exceeded by spending {delta} delta.\")\n\n        return True\n", "idx": 1333}
{"namespace": "diffprivlib.accountant.BudgetAccountant.spend", "completion": "        check_epsilon_delta(epsilon, delta)\n        if self.epsilon == float(\"inf\") and self.delta == 1:\n            return self\n\n        if 0 < epsilon < self.__min_epsilon:\n            raise ValueError(f\"Epsilon must be at least {self.__min_epsilon} if non-zero, got {epsilon}.\")\n\n        if Budget(self.epsilon, self.delta) >= self.total(spent_budget=self.spent_budget + [(epsilon, delta)]):\n            self.__spent_budget.append((epsilon, delta))\n            return self\n\n        raise BudgetError(f\"Privacy spend of ({epsilon},{delta}) not permissible; will exceed remaining privacy budget.\"\n                          f\" Use {self.__class__.__name__}.{self.remaining.__name__}() to check remaining budget.\")\n", "idx": 1334}
{"namespace": "diffprivlib.accountant.BudgetAccountant.load_default", "completion": "        if accountant is None:\n            if BudgetAccountant._default is None:\n                BudgetAccountant._default = BudgetAccountant()\n            return BudgetAccountant._default\n        elif isinstance(accountant, BudgetAccountant):\n            return accountant\n        else:\n            raise TypeError(\"accountant must be a BudgetAccountant or None\")\n", "idx": 1335}
{"namespace": "diffprivlib.accountant.BudgetAccountant.set_default", "completion": "        BudgetAccountant._default = self\n        return self\n", "idx": 1336}
{"namespace": "diffprivlib.accountant.BudgetAccountant.pop_default", "completion": "        default = BudgetAccountant._default\n        BudgetAccountant._default = None\n        return default\n\n", "idx": 1337}
{"namespace": "diffprivlib.validation.clip_to_bounds", "completion": "    if not isinstance(array, np.ndarray):\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\n    if array.ndim != 2:\n        raise ValueError(f\"input array must be 2-dimensional, got {array.ndim} dimensions.\")\n    if not isinstance(bounds, tuple):\n        raise TypeError(f\"Bounds must be specified as a tuple of (min, max), got {type(bounds)}.\")\n    if not isinstance(shape, Integral):\n        raise TypeError(f\"shape parameter must be integer-valued, got {type(shape)}.\")\n\n    lower, upper = bounds\n\n    if np.asarray(lower).size == 1 or np.asarray(upper).size == 1:\n        lower = np.ravel(lower).astype(dtype)\n        upper = np.ravel(upper).astype(dtype)\n    else:\n        lower = np.asarray(lower, dtype=dtype)\n        upper = np.asarray(upper, dtype=dtype)\n\n    if lower.shape != upper.shape:\n        raise ValueError(\"lower and upper bounds must be the same shape array\")\n    if lower.ndim > 1:\n        raise ValueError(\"lower and upper bounds must be scalar or a 1-dimensional array\")\n    if lower.size not in (1, shape):\n        raise ValueError(f\"lower and upper bounds must have {shape or 1} element(s), got {lower.size}.\")\n\n    n_bounds = lower.shape[0]\n\n    for i in range(n_bounds):\n        _lower = lower[i]\n        _upper = upper[i]\n\n        if not isinstance(_lower, Real) or not isinstance(_upper, Real):\n            raise TypeError(f\"Each bound must be numeric, got {_lower} ({type(_lower)}) and {_upper} ({type(_upper)}).\")\n\n        if _lower > _upper:\n            raise ValueError(f\"For each bound, lower bound must be smaller than upper bound, got {", "idx": 1338}
{"namespace": "diffprivlib.models.naive_bayes.GaussianNB._update_mean_variance", "completion": "        if sample_weight is not None:\n            warn_unused_args(\"sample_weight\")\n\n        if n_noisy is None:\n            n_noisy = X.shape[0]\n\n        n_total = n_past + n_noisy\n\n        # If we have a sample weight, we need to weight the data points\n        if sample_weight is not None:\n            n_total = np.average(sample_weight, weights=n_noisy)\n\n        # Calculate the new mean\n        # The new mean is calculated by taking a weighted average of the old mean and the new data points\n        # The weight of the old mean is the number of samples in the old mean divided by the total number of samples\n        # The weight of the new data points is the number of samples in the new data points divided by the total number of samples\n        # The new mean is then calculated by taking the weighted average of the old mean and the new data points\n        # The new mean is calculated by taking the weighted average of the old mean and the new data points\n        # The weight of the old mean is the number of samples in the old mean divided by the total number of samples\n        # The weight of the new data points is the number of samples in the new data points divided by the total number of samples\n        # The new mean is then calculated by taking the weighted average of the old mean and the new data points\n        # The new mean is calculated by taking the weighted average of the old mean and the new data points\n        # The weight of the old mean is the number of samples in the old mean divided by the total number of samples\n        # The weight of the new data points is the number of samples in the new data points divided by the total number of samples\n        # The new mean is then calculated by taking the weighted average of the old mean and the new data points\n        # The new mean is calculated by taking the weighted average of the old mean and the new data points\n        # The weight of the old mean is the number of samples in the old mean divided by the total number of samples\n        # The weight of the new data points is the number of samples in the new data points divided by the total number of samples\n        # The new mean is then calculated by taking the weighted", "idx": 1339}
{"namespace": "diffprivlib.models.naive_bayes.GaussianNB._noisy_class_counts", "completion": "        # Calculate the actual class counts\n        class_counts = np.bincount(y)\n\n        # Calculate the total number of samples\n        n_samples = y.shape[0]\n\n        # Calculate the sensitivity of the class counts\n        sensitivity = 1\n\n        # Calculate the privacy budget for each class count\n        epsilon_per_class = self.epsilon / len(class_counts)\n\n        # Add noise to the class counts using the Laplace mechanism\n        noisy_class_counts = np.zeros_like(class_counts)\n        for i in range(len(class_counts)):\n            # Create a Laplace mechanism with the calculated privacy budget\n            laplace_mechanism = LaplaceBoundedDomain(epsilon=epsilon_per_class, sensitivity=sensitivity, lower=0, upper=n_samples, random_state=random_state)\n\n            # Add noise to the actual class count\n            noisy_class_counts[i] = laplace_mechanism.randomise(class_counts[i])\n\n        return noisy_class_counts", "idx": 1340}
{"namespace": "diffprivlib.models.standard_scaler._incremental_mean_and_var", "completion": "    # Initialising new accountant, as budget is tracked in main class. Subject to review in line with GH issue #21\n    acc = BudgetAccountant(epsilon, 0)\n\n    # Checking if the input dataset is empty\n    if X.size == 0:\n        return last_mean, last_variance, last_sample_count\n\n    # Checking if the input dataset is a single value\n    if X.size == 1:\n        return X[0], 0.0, 1\n\n    # Checking if the input dataset is a single value\n    if X.size == 1:\n        return X[0], 0.0, 1\n\n    # Checking if the input dataset is a single value\n    if X.size == 1:\n        return X[0], 0.0, 1\n\n    # Checking if the input dataset is a single value\n    if X.size == 1:\n        return X[0], 0.0, 1\n\n    # Checking if the input dataset is a single value\n    if X.size == 1:\n        return X[0], 0.0, 1\n\n    # Checking if the input dataset is a single value\n    if X.size == 1:\n        return X[0], 0.0, 1\n\n    # Checking if the input dataset is a single value\n    if X.size == 1:\n        return X[0], 0.0, 1\n\n    # Checking if the input dataset is a single value\n    if X.size == 1:\n        return X[0], 0.0, 1\n\n    # Checking if the input dataset is a single value\n    if X.size == 1:\n        return X[0], 0.0, 1\n\n    # Checking if the input dataset is a single value\n    if X.size == 1:\n        return X[0], 0.0, 1\n\n    # Checking if the input dataset is a single value\n    if X.size == 1:\n        return X[0], 0.0, 1\n\n    # Checking if the input dataset", "idx": 1341}
{"namespace": "diffprivlib.models.linear_regression.LinearRegression.fit", "completion": "        X, y, X_offset, y_offset, X_scale = _preprocess_data(X, y, self.fit_intercept, self.epsilon, self.bounds_X,\n                                                             self.bounds_y, self.copy_X, True, self.random_state)\n\n        if sample_weight is not None:\n            warnings.warn(\"sample_weight is not supported by this class.\")\n\n        if self.bounds_X is None:\n            self.bounds_X = (X.min(axis=0), X.max(axis=0))\n\n        if self.bounds_y is None:\n            self.bounds_y = (y.min(axis=0), y.max(axis=0))\n\n        obj, noisy_coefs = _construct_regression_obj(X, y, self.bounds_X, self.bounds_y, self.epsilon, self.alpha,\n                                                     self.random_state)\n\n        self.coef_ = np.zeros((X.shape[1], y.shape[1] if y.ndim > 1 else 1))\n        self.intercept_ = np.zeros(y.shape[1] if y.ndim > 1 else 1)\n\n        for i in range(y.shape[1] if y.ndim > 1 else 1):\n            res = minimize(obj[i], self.coef_[:, i], method=\"L-BFGS-B\", jac=True)\n            self.coef_[:, i] = res.x\n\n        if y.ndim == 1:\n            self.coef_ = self.coef_.ravel()\n\n        if self.fit_intercept:\n            self.intercept_ = y_offset - np.dot(X_offset, self.coef_.T)\n\n        self.accountant.spend(epsilon=self.epsilon)\n\n        return self\n", "idx": 1342}
{"namespace": "diffprivlib.models.k_means.KMeans.fit", "completion": "        # Check if the input data is valid\n        if not isinstance(X, np.ndarray):\n            raise TypeError(\"X must be a numpy array\")\n\n        # Check if the input data is valid\n        if not isinstance(X, np.ndarray):\n            raise TypeError(\"X must be a numpy array\")\n\n        # Check if the input data is valid\n        if not isinstance(X, np.ndarray):\n            raise TypeError(\"X must be a numpy array\")\n\n        # Check if the input data is valid\n        if not isinstance(X, np.ndarray):\n            raise TypeError(\"X must be a numpy array\")\n\n        # Check if the input data is valid\n        if not isinstance(X, np.ndarray):\n            raise TypeError(\"X must be a numpy array\")\n\n        # Check if the input data is valid\n        if not isinstance(X, np.ndarray):\n            raise TypeError(\"X must be a numpy array\")\n\n        # Check if the input data is valid\n        if not isinstance(X, np.ndarray):\n            raise TypeError(\"X must be a numpy array\")\n\n        # Check if the input data is valid\n        if not isinstance(X, np.ndarray):\n            raise TypeError(\"X must be a numpy array\")\n\n        # Check if the input data is valid\n        if not isinstance(X, np.ndarray):\n            raise TypeError(\"X must be a numpy array\")\n\n        # Check if the input data is valid\n        if not isinstance(X, np.ndarray):\n            raise TypeError(\"X must be a numpy array\")\n\n        # Check if the input data is valid\n        if not isinstance(X, np.ndarray):\n            raise TypeError(\"X must be a numpy array\")\n\n        # Check if the input data is valid\n        if not isinstance(X, np.ndarray):\n            raise TypeError(\"X must be a numpy array\")\n\n        # Check if the input data is valid\n        if not isinstance(X, np.ndarray):\n            raise TypeError(\"X", "idx": 1343}
{"namespace": "diffprivlib.models.forest._FittingTree.__getstate__", "completion": "        return {\n            \"max_depth\": self.max_depth,\n            \"node_count\": self.node_count,\n            \"nodes\": self.nodes,\n            \"values\": self.values,\n        }\n", "idx": 1344}
{"namespace": "diffprivlib.models.forest._FittingTree.fit", "completion": "        if not self.nodes:\n            raise ValueError(\"Tree has not been built yet. Call build() first.\")\n\n        # Apply tree to input data to determine leaves\n        leaves = self.apply(X)\n\n        # Calculate unique leaves\n        unique_leaves = np.unique(leaves)\n\n        # Initialize array to store values for each leaf\n        self.values_ = np.zeros((len(unique_leaves), len(self.classes)), dtype=int)\n\n        # Populate values for real leaves\n        for leaf in unique_leaves:\n            if leaf != self._TREE_LEAF:\n                leaf_samples = X[leaves == leaf]\n                leaf_targets = y[leaves == leaf]\n                leaf_values, _ = np.histogram(leaf_targets, bins=len(self.classes), range=(0, len(self.classes)))\n                self.values_[leaf] = leaf_values\n\n        # Populate values for empty leaves\n        for leaf in unique_leaves:\n            if leaf != self._TREE_LEAF:\n                if np.sum(self.values_[leaf]) == 0:\n                    self.values_[leaf] = self.random_state.permutation(self.classes)\n\n        # Assign calculated values to tree\n        for node in self.nodes:\n            if node.left_child == self._TREE_LEAF:\n                node.left_child = self.values_[node.left_child]\n            if node.right_child == self._TREE_LEAF:\n                node.right_child = self.values_[node.right_child]\n\n        return self\n", "idx": 1345}
{"namespace": "diffprivlib.tools.histograms.histogram", "completion": "    # Check inputs\n    if not isinstance(sample, np.ndarray):\n        sample = np.asarray(sample)\n\n    if accountant is None:\n        accountant = BudgetAccountant()\n\n    if random_state is None:\n        random_state = check_random_state(None)\n\n    # Check for unused arguments\n    warn_unused_args(unused_args, \"histogram\")\n\n    # Check for valid epsilon\n    if epsilon < 0:\n        raise ValueError(\"Epsilon must be non-negative\")\n\n    # Check for valid bins\n    if bins < 1:\n        raise ValueError(\"Bins must be a positive integer\")\n\n    # Check for valid range\n    if range is not None:\n        if range[0] > range[1]:\n            raise ValueError(\"Range must be a tuple of two numbers, where the first number is smaller than the second number\")\n\n    # Check for valid weights\n    if weights is not None:\n        if not isinstance(weights, np.ndarray):\n            weights = np.asarray(weights)\n\n        if weights.shape != sample.shape:\n            raise ValueError(\"Weights must be of the same shape as the input data\")\n\n    # Check for valid density\n    if density is not None:\n        if not isinstance(density, bool):\n            raise ValueError(\"Density must be a boolean\")\n\n    # Check for valid random_state\n    if not isinstance(random_state, np.random.RandomState):\n        random_state = check_random_state(random_state)\n\n    # Check for valid accountant\n    if not isinstance(accountant, BudgetAccountant):\n        raise ValueError(\"Accountant must be a BudgetAccountant\")\n\n    # Check for valid unused_args\n    if unused_args:\n        raise ValueError(\"Unused arguments: {}\".format(unused_args))\n\n    # Check for valid sample\n    if not isinstance(sample, np.ndarray):\n        sample = np.asarray(sample)\n\n    # Check for valid epsilon\n    if epsilon < 0:", "idx": 1346}
{"namespace": "diffprivlib.tools.histograms.histogram2d", "completion": "    warn_unused_args(unused_args)\n\n    random_state = check_random_state(random_state)\n\n    accountant = BudgetAccountant.load_default(accountant)\n    accountant.check(epsilon, 0)\n\n    # Range only required if bin edges not specified\n    if np.array(bins, dtype=object).ndim == 0 or not np.all([np.ndim(_bin) for _bin in bins]):\n        if range is None or (isinstance(range, list) and None in range):\n            warnings.warn(\"Range parameter has not been specified (or has missing elements). Falling back to taking \"\n                          \"range from the data.\\n \"\n                          \"To ensure differential privacy, and no additional privacy leakage, the range must be \"\n                          \"specified for each dimension independently of the data (i.e., using domain knowledge).\",\n                          PrivacyLeakWarning)\n\n    hist, bin_edges_x, bin_edges_y = np.histogram2d(array_x, array_y, bins=bins, range=range, weights=weights, density=None)\n\n    dp_mech = GeometricTruncated(epsilon=epsilon, sensitivity=1, lower=0, upper=maxsize, random_state=random_state)\n\n    dp_hist = np.zeros_like(hist)\n    iterator = np.nditer(hist, flags=['multi_index'])\n\n    while not iterator.finished:\n        dp_hist[iterator.multi_index] = dp_mech.randomise(int(iterator[0]))\n        iterator.iternext()\n\n    dp_hist = dp_hist.astype(float, casting='safe')\n\n    if density:\n        # calculate the probability density function\n        dims = len(dp_hist.shape)\n        dp_hist_sum = dp_hist.sum()\n        for i in np.arange(dims):\n            shape = np.ones(dims, int)\n            shape[i] = dp_hist.shape[i]\n           ", "idx": 1347}
{"namespace": "diffprivlib.tools.utils.nanmean", "completion": "    warn_unused_args(unused_args)\n\n    return _mean(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                 random_state=random_state, accountant=accountant, nan=True)\n\n", "idx": 1348}
{"namespace": "diffprivlib.tools.utils.var", "completion": "    warn_unused_args(unused_args)\n\n    return _var(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=False)\n\n", "idx": 1349}
{"namespace": "diffprivlib.tools.utils.nanvar", "completion": "    warn_unused_args(unused_args)\n\n    return _var(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)\n\n", "idx": 1350}
{"namespace": "diffprivlib.tools.utils.std", "completion": "    warn_unused_args(unused_args)\n\n    return _std(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=False)\n\n", "idx": 1351}
{"namespace": "diffprivlib.tools.utils.nanstd", "completion": "    warn_unused_args(unused_args)\n\n    return _std(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)\n\n", "idx": 1352}
{"namespace": "diffprivlib.tools.utils.sum", "completion": "    warn_unused_args(unused_args)\n\n    return _sum(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant)\n\n", "idx": 1353}
{"namespace": "diffprivlib.tools.utils.nansum", "completion": "    warn_unused_args(unused_args)\n\n    return _sum(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)\n\n", "idx": 1354}
{"namespace": "diffprivlib.tools.quantiles.quantile", "completion": "    # Check random state\n    if random_state is not None:\n        np.random.seed(random_state)\n\n    # Check quantile\n    if isinstance(quant, (list, tuple, np.ndarray)):\n        quant = np.array(quant)\n\n    if isinstance(quant, np.ndarray):\n        if quant.ndim == 0:\n            quant = quant[np.newaxis]\n\n        if quant.ndim > 1:\n            raise ValueError(\"Quantile must be a scalar or 1-dimensional array.\")\n\n        if (quant < 0.0).any() or (quant > 1.0).any():\n            raise ValueError(\"Quantile must be in the unit interval [0, 1].\")\n\n    else:\n        quant = np.array([quant])\n\n    # Check bounds\n    if bounds is not None:\n        if len(bounds) != 2:\n            raise ValueError(\"Bounds must be of length 2.\")\n\n        if bounds[0] > bounds[1]:\n            raise ValueError(\"Bounds must be of the form (min, max).\")\n\n    # Check axis\n    if axis is None:\n        axis = tuple(range(array.ndim))\n    elif isinstance(axis, int):\n        axis = (axis,)\n\n    # Check keepdims\n    if not isinstance(keepdims, bool):\n        raise TypeError(\"keepdims must be a boolean.\")\n\n    # Check accountant\n    if accountant is not None:\n        if not isinstance(accountant, BudgetAccountant):\n            raise TypeError(\"accountant must be a BudgetAccountant.\")\n\n    # Check unused arguments\n    warn_unused_args(unused_args, \"quantile\")\n\n    # Process array\n    array = np.asarray(array)\n    array = np.ravel(array)\n\n    # Check array\n    if array.ndim == 0:\n        raise ValueError(\"Array must be at least 1-dimensional.\")\n\n    # Check quantile\n    if (quant < 0.0).any() or (quant > 1.0).any():", "idx": 1355}
{"namespace": "diffprivlib.tools.quantiles.percentile", "completion": "    quant = np.ravel(percent)\n\n    if np.any(quant < 0) or np.any(quant > 100):\n        raise ValueError(\"Percentiles must be in the range [0, 100].\")\n\n    if len(quant) > 1:\n        return np.array([percentile(array, p_i, epsilon=epsilon / len(quant), bounds=bounds, axis=axis, keepdims=keepdims,\n                                   accountant=accountant, random_state=random_state) for p_i in quant])\n\n    quant = quant.item() / 100\n\n    return quantile(array, quant, epsilon=epsilon, bounds=bounds, axis=axis, keepdims=keepdims,\n                    random_state=random_state, accountant=accountant)", "idx": 1356}
{"namespace": "diffprivlib.mechanisms.base.bernoulli_neg_exp", "completion": "    rng = check_random_state(random_state)\n    if rng.random() < np.exp(-gamma):\n        return 1\n    else:\n        return 0", "idx": 1357}
{"namespace": "discord.utils.snowflake_time", "completion": "    return datetime.datetime.utcfromtimestamp(((id >> 22) + DISCORD_EPOCH) / 1000)\n\n", "idx": 1358}
{"namespace": "discord.utils.time_snowflake", "completion": "    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=datetime.timezone.utc)\n    timestamp = int(dt.timestamp() * 1000 - DISCORD_EPOCH)\n    return (timestamp << 22) + (2**22 - 1 if high else 0)\n\n", "idx": 1359}
{"namespace": "discord.utils.resolve_invite", "completion": "    if isinstance(invite, Invite):\n        return ResolvedInvite(invite.code, invite.guild.id)\n    elif isinstance(invite, str):\n        if invite.startswith('https://discord.gg/'):\n            invite = invite.split('/')[-1]\n        if invite.startswith('https://discord.com/invite/'):\n            invite = invite.split('/')[-1]\n        if invite.startswith('https://discord.com/events/'):\n            invite = invite.split('/')[-1]\n        if invite.startswith('https://discord.com/api/oauth2/authorize?client_id='):\n            invite = invite.split('/')[-1]\n        if invite.startswith('https://discord.com/api/oauth2/authorize?client_id='):\n            invite = invite.split('/')[-1]\n        if invite.startswith('https://discord.com/api/oauth2/authorize?client_id='):\n            invite = invite.split('/')[-1]\n        if invite.startswith('https://discord.com/api/oauth2/authorize?client_id='):\n            invite = invite.split('/')[-1]\n        if invite.startswith('https://discord.com/api/oauth2/authorize?client_id='):\n            invite = invite.split('/')[-1]\n        if invite.startswith('https://discord.com/api/oauth2/authorize?client_id='):\n            invite = invite.split('/')[-1]\n        if invite.startswith('https://discord.com/api/oauth2/authorize?client_id='):\n            invite = invite.split('/')[-1]\n        if invite.startswith('https://discord.com/api/oauth2/authorize?client_id='):\n            invite = invite.split('/')[-1]\n        if invite.startswith('https://discord.com/api", "idx": 1360}
{"namespace": "discord.utils.resolve_annotation", "completion": "    if annotation is None:\n        return type(None)\n    if isinstance(annotation, str):\n        return ForwardRef(annotation)\n    if localns is not None:\n        namespace = localns\n    else:\n        namespace = globalns\n    if cache is None:\n        cache = {}\n    return evaluate_annotation(annotation, namespace, namespace, cache)", "idx": 1361}
{"namespace": "discord.ext.tasks.loop", "completion": "    def decorator(func: LF) -> Loop[LF]:\n        return Loop(func, seconds=seconds, minutes=minutes, hours=hours, time=time, count=count, reconnect=reconnect)\n\n    return decorator", "idx": 1362}
{"namespace": "barf.analysis.gadgets.classifier.GadgetClassifier.classify", "completion": "        # Classify the gadget using each classifier.\n        classified_gadgets = []\n        for gadget_type, classifier in self._classifiers.items():\n            try:\n                classified_gadgets.extend(classifier(gadget))\n            except Exception as e:\n                print(\"Error classifying gadget:\", gadget)\n                print(\"Error message:\", e)\n                import traceback\n                traceback.print_exc()\n\n        # Sort the classified gadgets by their string representation.\n        classified_gadgets.sort(key=str)\n\n        return classified_gadgets\n", "idx": 1363}
{"namespace": "barf.analysis.gadgets.finder.GadgetFinder.find", "completion": "        # Set the maximum number of bytes and the depth of instructions to be considered.\n        self._max_bytes = byte_depth\n        self._instrs_depth = instrs_depth\n\n        # Call the appropriate method based on the architecture to find the candidates.\n        if self._architecture == ARCH_X86:\n            candidates = self._find_x86(start_address, end_address)\n        elif self._architecture == ARCH_ARM:\n            candidates = self._find_arm(start_address, end_address)\n        else:\n            raise NotImplementedError(\"Architecture not supported.\")\n\n        # Sort the candidates based on their addresses.\n        candidates.sort(key=lambda x: x.address)\n\n        # Return the sorted list of candidates.\n        return candidates\n", "idx": 1364}
{"namespace": "barf.core.reil.parser.ReilParser.parse", "completion": "        parsed_instrs = []\n\n        for instr in instrs:\n            instr = instr.lower()\n\n            if instr in self._cache:\n                parsed_instr = copy.deepcopy(self._cache[instr])\n            else:\n                try:\n                    parsed_instr = instruction.parseString(instr)[0]\n\n                    self._cache[instr] = parsed_instr\n                except Exception as e:\n                    logger.error(\"Error parsing instruction: %s\", instr)\n\n                    continue\n\n            parsed_instrs.append(parsed_instr)\n\n        return parsed_instrs", "idx": 1365}
{"namespace": "barf.core.smt.smtfunction.zero_extend", "completion": "    if not isinstance(s, (Constant, BitVec)):\n        raise TypeError(\"s must be a Constant or BitVec\")\n\n    if size < s.size:\n        raise ValueError(\"size must be greater than or equal to s.size\")\n\n    if size == s.size:\n        return s\n\n    return BitVec(size, s.name, s.smt_symbol, s.solver)\n\n", "idx": 1366}
{"namespace": "barf.core.smt.smtfunction.extract", "completion": "    assert type(s) in (Constant, BitVec) and size + offset <= s.size\n\n    if offset == 0 and size == s.size:\n        return s\n\n    return BitVec(size, \"(_ extract {} {})\".format(offset + size - 1, offset), s)\n\n", "idx": 1367}
{"namespace": "barf.core.smt.smtfunction.ite", "completion": "    assert type(cond) == bool\n    assert type(true) in (Constant, BitVec) and size == true.size\n    assert type(false) in (Constant, BitVec) and size == false.size\n\n    return BitVec(size, \"(ite {} {} {})\".format(cond, true, false), cond, true, false)\n\n", "idx": 1368}
{"namespace": "barf.core.smt.smtfunction.concat", "completion": "    assert all(type(arg) is BitVec for arg in args)\n\n    if len(args) == 1:\n        return args[0]\n\n    return BitVec(size, \"concat\", *args)", "idx": 1369}
{"namespace": "barf.core.smt.smtsymbol.BitVecArray.declaration", "completion": "        return \"(declare-fun {:s} () (Array (_ BitVec {:d}) (_ BitVec {:d})))\".format(self.name, self.key_size, self.value_size)\n", "idx": 1370}
{"namespace": "barf.arch.translator.InstructionTranslator.translate", "completion": "        try:\n            return self._translate(instruction)\n        except Exception as e:\n            logger.exception(e)\n            raise TranslationError(\"Unknown error\")\n", "idx": 1371}
{"namespace": "barf.arch.emulator.Emulator.load_binary", "completion": "        try:\n            with open(binary, 'rb') as f:\n                signature = f.read(4)\n\n                if signature == b'\\x7fELF':\n                    self._load_binary_elf(binary)\n                elif signature == b'MZ':\n                    self._load_binary_pe(binary)\n                else:\n                    raise Exception(\"Unknown file format.\")\n        except Exception as e:\n            raise Exception(\"Error loading file.\") from e", "idx": 1372}
{"namespace": "barf.arch.arm.parser.ArmParser.parse", "completion": "        instr = instr.lower()\n\n        if instr in self._cache:\n            return copy.deepcopy(self._cache[instr])\n\n        try:\n            parsed_instr = instruction.parseString(instr)[0]\n            self._cache[instr] = parsed_instr\n            return copy.deepcopy(parsed_instr)\n        except Exception as e:\n            logger.error(\"Error parsing instruction: %s\", instr)\n            logger.error(e)\n            return None", "idx": 1373}
{"namespace": "barf.arch.x86.parser.X86Parser.parse", "completion": "        try:\n            instr = instr.lower()\n\n            if instr not in self._cache:\n                self._cache[instr] = copy.deepcopy(instruction.parseString(instr)[0])\n\n            return copy.deepcopy(self._cache[instr])\n\n        except Exception as e:\n            logger.error(\"Error parsing instruction: %s\", instr)\n            logger.error(e)\n\n            return None", "idx": 1374}
{"namespace": "faker.utils.text.slugify", "completion": "    if allow_dots:\n        pattern = _re_pattern_allow_dots\n    else:\n        pattern = _re_pattern\n    value = str(value)\n    if allow_unicode:\n        value = unicodedata.normalize(\"NFKC\", value)\n    else:\n        value = unicodedata.normalize(\"NFKD\", value).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n    value = re.sub(pattern, \"\", value).strip().lower()\n    return re.sub(_re_spaces, \"-\", value)", "idx": 1375}
{"namespace": "faker.utils.checksums.calculate_luhn", "completion": "    check_digit = luhn_checksum(partial_number * 10)\n    if check_digit == 0:\n        return check_digit\n    return 10 - check_digit", "idx": 1376}
{"namespace": "faker.utils.distribution.choices_distribution_unique", "completion": "    if random is None:\n        random = mod_random\n\n    if p is None:\n        # Default to a uniform distribution\n        p = [1 / len(a)] * len(a)\n\n    assert len(a) == len(p), \"The number of elements must match the number of probabilities.\"\n\n    # Normalize the probabilities\n    total = sum(p)\n    p = [x / total for x in p]\n\n    # Cumulative distribution function\n    cdf = list(cumsum(p))\n\n    # Generate unique choices\n    choices = set()\n    while len(choices) < length:\n        choices.add(a[bisect.bisect(cdf, random_sample(random))])\n\n    return list(choices)\n\n", "idx": 1377}
{"namespace": "faker.utils.loading.find_available_locales", "completion": "    available_locales = set()\n\n    for provider_path in providers:\n        provider_module = import_module(provider_path)\n        if getattr(provider_module, \"localized\", False):\n            langs = list(getattr(provider_module, \"Provider\").langs)\n            available_locales.update(langs)\n\n    return sorted(available_locales)", "idx": 1378}
{"namespace": "faker.utils.loading.find_available_providers", "completion": "    available_providers = set()\n\n    for locale_path in modules:\n        locale_module = import_module(locale_path)\n        if getattr(locale_module, \"localized\", False):\n            providers = list_module(locale_module)\n            available_providers.update(providers)\n    return sorted(available_providers)", "idx": 1379}
{"namespace": "faker.providers.credit_card.Provider._generate_number", "completion": "        number = prefix\n        number += \"\".join([self.random_digit() for _ in range(length - len(prefix))])\n        number += self._calculate_luhn(number)\n        return number\n", "idx": 1380}
{"namespace": "faker.decode.unidecode", "completion": "    return \"\".join(codes.get(ord(c), c) for c in txt)", "idx": 1381}
{"namespace": "dash.fingerprint.build_fingerprint", "completion": "    filename, extension = path.rsplit(\".\", 1)\n    file_path = filename.rsplit(\"/\", 1)[0]\n    v_str = version_clean.sub(\"_\", str(version))\n    return f\"{file_path}.v{v_str}m{hash_value}.{extension}\"\n\n", "idx": 1382}
{"namespace": "dash.fingerprint.check_fingerprint", "completion": "    path_parts = path.split(\"/\")\n    filename, extension = path_parts[-1].split(\".\", 1)\n    file_path = \"/\".join(path_parts[:-1] + [filename])\n\n    if cache_regex.match(filename):\n        return file_path, True\n\n    return path, False", "idx": 1383}
{"namespace": "dash._configs.pages_folder_config", "completion": "    if use_pages:\n        if not pages_folder:\n            pages_folder = os.path.join(\n                os.path.dirname(os.getcwd()), \"pages\"\n            )\n\n        if not os.path.isdir(pages_folder):\n            raise exceptions.InvalidConfig(\n                f\"{name} `pages_folder` not found. {pages_folder}\"\n            )\n\n    return pages_folder", "idx": 1384}
{"namespace": "dash._grouping.flatten_grouping", "completion": "    if schema is None:\n        schema = grouping\n\n    if isinstance(schema, (tuple, list)):\n        if not isinstance(grouping, (tuple, list)):\n            raise InvalidCallbackReturnValue(\n                \"The grouping value must be a tuple or list \"\n                \"when a schema is provided. \"\n                f\"The grouping value was a {type(grouping)}.\"\n            )\n        if len(grouping) != len(schema):\n            raise InvalidCallbackReturnValue(\n                \"The grouping value must have the same length as the schema. \"\n                f\"The grouping value had length {len(grouping)} \"\n                f\"and the schema had length {len(schema)}.\"\n            )\n        return [\n            element\n            for group, schema in zip(grouping, schema)\n            for element in flatten_grouping(group, schema)\n        ]\n    elif isinstance(schema, dict):\n        if not isinstance(grouping, dict):\n            raise InvalidCallbackReturnValue(\n                \"The grouping value must be a dict \"\n                \"when a schema is provided. \"\n                f\"The grouping value was a {type(grouping)}.\"\n            )\n        if set(grouping.keys()) != set(schema.keys()):\n            raise InvalidCallbackReturnValue(\n                \"The grouping value must have the same keys as the schema. \"\n                f\"The grouping value had keys {set(grouping.keys())} \"\n                f\"and the schema had keys {set(schema.keys())}.\"\n            )\n        return [\n            element\n            for key, group in grouping.items()\n            for element in flatten_grouping(group, schema[key])\n        ]\n    else:\n        return [grouping]\n\n", "idx": 1385}
{"namespace": "dash._grouping.make_grouping_by_index", "completion": "    if isinstance(schema, (tuple, list)):\n        grouping = []\n        for sub_schema in schema:\n            sub_grouping, flat_values = make_grouping_by_index(sub_schema, flat_values)\n            grouping.append(sub_grouping)\n        return grouping, flat_values\n\n    if isinstance(schema, dict):\n        grouping = {}\n        for k in schema:\n            grouping[k], flat_values = make_grouping_by_index(schema[k], flat_values)\n        return grouping, flat_values\n\n    return flat_values[0], flat_values[1:]\n\n", "idx": 1386}
{"namespace": "dash._grouping.map_grouping", "completion": "    if isinstance(grouping, (tuple, list)):\n        return list(map_grouping(fn, el) for el in grouping)\n\n    if isinstance(grouping, dict):\n        return {k: map_grouping(fn, v) for k, v in grouping.items()}\n\n    return fn(grouping)\n\n", "idx": 1387}
{"namespace": "dash._grouping.validate_grouping", "completion": "    if full_schema is None:\n        full_schema = schema\n\n    if isinstance(schema, (tuple, list)):\n        SchemaTypeValidationError.check(grouping, full_schema, path, type(schema))\n        SchemaLengthValidationError.check(grouping, full_schema, path, len(schema))\n        for i, (g, s) in enumerate(zip(grouping, schema)):\n            validate_grouping(g, s, full_schema, path + (i,))\n\n    elif isinstance(schema, dict):\n        SchemaTypeValidationError.check(grouping, full_schema, path, type(schema))\n        SchemaKeysValidationError.check(grouping, full_schema, path, set(schema.keys()))\n        for k, s in schema.items():\n            validate_grouping(grouping[k], s, full_schema, path + (k,))\n\n    else:\n        SchemaTypeValidationError.check(grouping, full_schema, path, type(schema))\n\n", "idx": 1388}
{"namespace": "dash._get_paths.app_get_relative_path", "completion": "    if requests_pathname == \"/\" and path == \"\":\n        return \"/\"\n    elif requests_pathname != \"/\" and path == \"\":\n        return requests_pathname\n    elif not path.startswith(\"/\"):\n        raise exceptions.RelativePathTraversalException(\n            \"Paths must start with '/' but found '{}'\".format(path)\n        )\n    else:\n        return \"/\".join(\n            [\n                requests_pathname.rstrip(\"/\"),\n                path.lstrip(\"/\"),\n            ]\n        )", "idx": 1389}
{"namespace": "dash._get_paths.app_strip_relative_path", "completion": "    if requests_pathname != \"/\" and not path.startswith(requests_pathname.rstrip(\"/\")):\n        return path\n    if requests_pathname.endswith(\"/\") and not path.endswith(\"/\"):\n        return path\n    return path[len(requests_pathname) :]", "idx": 1390}
{"namespace": "dash.development._py_components_generation.js_to_py_type", "completion": "    if is_flow_type:\n        return map_js_to_py_types_flow_types(type_object).get(\n            type_object[\"name\"], lambda: \"\"\n        )(indent_num)\n    return map_js_to_py_types_prop_types(type_object).get(type_object[\"name\"], lambda: \"\")()", "idx": 1391}
{"namespace": "dash.development.component_loader.load_components", "completion": "    # Load the metadata\n    metadata = _get_metadata(metadata_path)\n\n    # Register the component library for index inclusion\n    # dash.register_page(\n    #     __name__,\n    #     path=\"/\",\n    #     title=\"Component Library\",\n    #     description=\"A collection of React components that can be used in Dash applications.\",\n    # )\n\n    # Iterate over each component in the metadata\n    components = []\n    for component_name in metadata:\n        # Extract the component name\n        component_metadata = metadata[component_name]\n\n        # Generate a class for the component\n        component_class = generate_class(component_name, component_metadata)\n\n        # Add the component class to the list\n        components.append(component_class)\n\n    return components", "idx": 1392}
{"namespace": "dash.development.component_loader.generate_classes", "completion": "    # Register the component lib for index include.\n    from .base_component import ComponentRegistry\n    ComponentRegistry.registry.add(namespace)\n\n    data = _get_metadata(metadata_path)\n\n    # Iterate over each property name (which is a path to the component)\n    for componentPath in data:\n        componentData = data[componentPath]\n\n        # Extract component name from path\n        # e.g. src/components/MyControl.react.js\n        # TODO Make more robust - some folks will write .jsx and others\n        # will be on windows. Unfortunately react-docgen doesn't include\n        # the name of the component atm.\n        name = componentPath.split(\"/\").pop().split(\".\")[0]\n        generate_class(\n            name, componentData[\"props\"], componentData[\"description\"], namespace, None\n        )\n\n    # Generate imports file\n    imports_file = os.path.join(namespace, \"__init__.py\")\n    with open(imports_file, \"w\") as f:\n        f.write(\"from .base_component import ComponentRegistry\\n\")\n        f.write(\"from .{} import *\\n\".format(namespace))\n        f.write(\"__all__ = [\")\n        for name in data:\n            component_name = name.split(\"/\").pop().split(\".\")[0]\n            f.write('\"{}\",\\n'.format(component_name))\n        f.write(\"]\\n\")\n        f.write(\"ComponentRegistry.registry.add('{}')\\n\".format(namespace))", "idx": 1393}
{"namespace": "dash.development.base_component.Component.to_plotly_json", "completion": "        props = {}\n        for k, v in self._prop_names.items():\n            if k in self._valid_wildcard_attributes:\n                continue\n            if v is not None:\n                props[k] = v\n\n        for k, v in self._valid_wildcard_attributes.items():\n            if v is not None:\n                props[k] = v\n\n        props[\"type\"] = self._type\n        props[\"namespace\"] = self._namespace\n\n        return props\n", "idx": 1394}
{"namespace": "dash.development.base_component.Component._traverse", "completion": "        if isinstance(self, Component):\n            if hasattr(self, \"children\"):\n                if isinstance(self.children, Component):\n                    yield from self.children._traverse()\n                elif isinstance(self.children, (tuple, MutableSequence)):\n                    for item in self.children:\n                        if isinstance(item, Component):\n                            yield from item._traverse()\n\n        yield self\n", "idx": 1395}
{"namespace": "dash.development._r_components_generation.make_namespace_exports", "completion": "    export_string = \"\"\n    for component in components:\n        if (\n            component.startswith(\"_\")\n            or component.startswith(\".\")\n            or component.endswith(\"_\")\n            or component.endswith(\".\")\n        ):\n            continue\n        if component == \"DashComponent\":\n            continue\n        if component == \"DashComponentWithProps\":\n            continue\n        if component == \"DashComponentWithContext\":\n            continue\n        if component == \"DashComponentWithPropsWithContext\":\n            continue\n        if component == \"DashComponentWithPropsWithContextWithRef\":\n            continue\n        if component == \"DashComponentWithPropsWithRef\":\n            continue\n        if component == \"DashComponentWithRef\":\n            continue\n        if component == \"DashComponentWithContextWithRef\":\n            continue\n        if component == \"DashComponentWithContextWithRef\":\n            continue\n        if component == \"DashComponentWithRefWithContext\":\n            continue\n        if component == \"DashComponentWithRefWithContextWithRef\":\n            continue\n        if component == \"DashComponentWithRefWithRef\":\n            continue\n        if component == \"DashComponentWithRefWithRefWithRef\":\n            continue\n        if component == \"DashComponentWithRefWithRefWithRefWithRef\":\n            continue\n        if component == \"DashComponentWithRefWithRefWithRefWithRefWithRef\":\n            continue\n        if component == \"DashComponentWithRefWithRefWithRefWithRefWithRefWithRef\":\n            continue\n        if component == \"DashComponentWithRefWithRefWithRefWithRefWithRefWithRefWithRef\":\n            continue\n        if component == \"DashComponentWithRefWithRefWithRefWithRefWithRefWithRefWithRefWithRef\":\n            continue\n        if component == \"DashComponentWithRefWithRefWithRefWithRefWithRefWithRefWithRefWithRefWithRef\":\n            continue\n        if component == \"DashComponentWithRefWithRefWithRefWithRefWithRefWithRefWithRefWithRefWithRefWithRef\":\n            continue\n        if component == \"DashComponentWithRefWithRefWithRefWithRefWithRefWithRefWithRefWithRefWithRefWith", "idx": 1396}
{"namespace": "dash.development._collect_nodes.collect_nodes", "completion": "    if nodes is None:\n        nodes = []\n    for key, value in metadata.items():\n        if is_node(value[\"name\"]):\n            nodes.append(base + key)\n        elif is_shape(value[\"name\"]):\n            nodes = collect_nodes(value[\"value\"], base + key + \".\", nodes)\n        elif value[\"name\"] == \"arrayOf\":\n            nodes = collect_array(value, base + key, nodes)\n        elif value[\"name\"] == \"union\":\n            nodes = collect_union(value[\"value\"], base + key, nodes)\n        elif value[\"name\"] == \"objectOf\":\n            nodes = collect_object(value, base + key, nodes)\n    return nodes", "idx": 1397}
{"namespace": "peewee.Index.where", "completion": "        if self._where is not None:\n            expressions = (self._where,) + expressions\n        self._where = reduce(operator.and_, expressions)\n", "idx": 1398}
{"namespace": "playhouse.dataset.DataSet.tables", "completion": "        tables = self._introspector.get_table_names()\n        if self._include_views:\n            tables += self._introspector.get_view_names()\n        return tables\n", "idx": 1399}
{"namespace": "playhouse.dataset.DataSet.update_cache", "completion": "        if table is None:\n            tables = self.tables\n        else:\n            tables = [table]\n\n        # Update the cache for the given table and its related tables.\n        for table in tables:\n            self._introspector.update_cache(table)\n\n        # Generate and update the models in the cache based on the updated cache.\n        self._models = self._introspector.generate_models(\n            skip_invalid=True,\n            literal_column_names=True,\n            include_views=self._include_views)\n", "idx": 1400}
{"namespace": "playhouse.dataset.DataSet.freeze", "completion": "        self._check_arguments(filename, file_obj, format, self._export_formats)\n        if filename:\n            file_obj = open_file(filename, 'w', encoding=encoding)\n        exporter = self._export_formats[format](self, file_obj, **kwargs)\n        exporter.export(query)\n        if filename:\n            file_obj.close()\n", "idx": 1401}
{"namespace": "playhouse.db_url.parse", "completion": "    parsed = urlparse(url)\n    return parseresult_to_dict(parsed, unquote_password)", "idx": 1402}
{"namespace": "playhouse.db_url.connect", "completion": "    parsed = urlparse(url)\n    connect_kwargs = parseresult_to_dict(parsed, unquote_password)\n    connect_kwargs.update(connect_params)\n    scheme = schemes.get(parsed.scheme)\n    if scheme is None:\n        if parsed.scheme.startswith('postgres'):\n            scheme = schemes.get('postgres')\n        else:\n            raise ValueError('Unknown database scheme: %s' % parsed.scheme)\n    return scheme(**connect_kwargs)", "idx": 1403}
{"namespace": "playhouse.sqlite_changelog.ChangeLog.install", "completion": "        if create_table:\n            self.model.create_table()\n\n        if drop:\n            if insert:\n                self.db.execute_sql(self.drop_trigger_sql(model, 'INSERT'))\n            if update:\n                self.db.execute_sql(self.drop_trigger_sql(model, 'UPDATE'))\n            if delete:\n                self.db.execute_sql(self.drop_trigger_sql(model, 'DELETE'))\n\n        if insert:\n            self.db.execute_sql(self.trigger_sql(model, 'INSERT', skip_fields))\n        if update:\n            self.db.execute_sql(self.trigger_sql(model, 'UPDATE', skip_fields))\n        if delete:\n            self.db.execute_sql(self.trigger_sql(model, 'DELETE', skip_fields))", "idx": 1404}
{"namespace": "playhouse.kv.KeyValue.pop", "completion": "        with self._database.atomic():\n            try:\n                value = self[key]\n                del self[key]\n                return value\n            except KeyError:\n                if default is not Sentinel:\n                    return default\n                raise\n", "idx": 1405}
{"namespace": "playhouse.signals.Signal.connect", "completion": "        if name is None:\n            name = receiver.__name__\n        if name in self._receivers:\n            raise ValueError('Receiver with name %s already exists' % name)\n        self._receivers.add(name)\n        self._receiver_list.append((receiver, name, sender))\n", "idx": 1406}
{"namespace": "playhouse.signals.Signal.disconnect", "completion": "        if receiver is None:\n            if name is None:\n                raise ValueError('a receiver or a name must be provided')\n            key = (name, sender)\n            if key not in self._receivers:\n                raise ValueError('receiver named %s (for sender=%s) is not '\n                                 'connected' % (name, sender or 'any'))\n            for i, (n, r, s) in enumerate(self._receiver_list):\n                if (n, s) == key:\n                    del self._receiver_list[i]\n                    self._receivers.remove(key)\n                    break\n        else:\n            for i, (n, r, s) in enumerate(self._receiver_list):\n                if r == receiver:\n                    del self._receiver_list[i]\n                    self._receivers.remove((n, s))\n                    break\n            else:\n                raise ValueError('receiver %s is not connected' % receiver)\n", "idx": 1407}
{"namespace": "backtrader.trade.Trade.update", "completion": "        # Update the commissions\n        self.commission += commission\n\n        # Update the size\n        self.size += size\n\n        # Check if the trade was opened\n        if not self.isopen:\n            self.isopen = True\n            self.justopened = True\n            self.baropen = self.data.current_bar\n            self.dtopen = self.data.datetime[0]\n\n        # Update the trade length\n        self.barlen = self.data.current_bar - self.baropen\n\n        # Check if the position was closed\n        if self.size == 0:\n            self.isclosed = True\n            self.barclose = self.data.current_bar\n            self.dtclose = self.data.datetime[0]\n\n        # Update the average price\n        if abs(size) > abs(self.size):\n            self.price = price\n        else:\n            self.price = (self.price * abs(self.size) + price * abs(size)) / (abs(self.size) + abs(size))\n\n        # Update the value\n        self.value = self.price * self.size\n\n        # Update the pnl\n        self.pnl = self.value - self.commission\n\n        # Update the pnlcomm\n        self.pnlcomm = self.pnl - self.commission\n\n        # Update the history if needed\n        if self.historyon:\n            self.history.append(\n                TradeHistory(\n                    self.status,\n                    self.data.datetime[0],\n                    self.barlen,\n                    self.size,\n                    self.price,\n                    self.value,\n                    self.pnl,\n                    self.pnlcomm,\n                    self.data.tz\n                )\n            )\n            self.history[-1].doupdate(order, size, price, commission)\n\n        # Update the status\n        if self.size > 0:\n            self.status = self.Open\n        elif self.size < 0:\n            self.status = self.Closed\n        else:\n            self.status = self.Created\n\n        # Update", "idx": 1408}
{"namespace": "ydata_profiling.profile_report.ProfileReport.typeset", "completion": "        if self._typeset is None:\n            self._typeset = VisionsTypeset(\n                config=self.config.vars.typeset,\n                type_schema=self._type_schema,\n            )\n        return self._typeset\n", "idx": 1409}
{"namespace": "ydata_profiling.report.presentation.flavours.html.frequency_table.HTMLFrequencyTable.render", "completion": "        if self.content is None:\n            return \"\"\n\n        if isinstance(self.content, list):\n            return self.render_list()\n        else:\n            return self.render_row()", "idx": 1410}
{"namespace": "ydata_profiling.report.presentation.flavours.html.image.HTMLImage.render", "completion": "        template_name = \"diagram.html\"\n        template_context = {\"image\": self.content}\n        return self.render_template(template_name, **template_context)", "idx": 1411}
{"namespace": "ydata_profiling.model.summary_algorithms.histogram_compute", "completion": "    n_bins = config.plot.histogram.bins\n    if n_bins > config.plot.histogram.max_bins:\n        n_bins = config.plot.histogram.max_bins\n\n    if n_unique <= n_bins:\n        n_bins = n_unique\n\n    histogram = np.histogram(finite_values, bins=n_bins, weights=weights)\n    histogram_bins = histogram[1]\n    histogram_bins = (histogram_bins[1:] + histogram_bins[:-1]) / 2.0\n\n    return {\n        f\"{name}_bins\": histogram_bins,\n        f\"{name}_counts\": histogram[0],\n    }\n\n", "idx": 1412}
{"namespace": "ydata_profiling.model.summarizer.BaseSummarizer.summarize", "completion": "        raise NotImplementedError\n\n", "idx": 1413}
{"namespace": "ydata_profiling.model.pandas.discretize_pandas.Discretizer.discretize_dataframe", "completion": "        discretized_dataframe = dataframe.copy()\n        for column in discretized_dataframe.columns:\n            if np.issubdtype(discretized_dataframe[column].dtype, np.number):\n                discretized_dataframe[column] = self.discretize_column(\n                    discretized_dataframe[column]\n                )\n        if self.reset_index:\n            discretized_dataframe.reset_index(drop=True, inplace=True)\n        return discretized_dataframe\n", "idx": 1414}
{"namespace": "ydata_profiling.model.pandas.correlations_pandas.pandas_cramers_compute", "completion": "    threshold = config.categorical_maximum_correlation_distinct\n\n    categorical_variables = {\n        key\n        for key, value in summary.items()\n        if value[\"type\"] == \"Categorical\"\n        and value[\"n_distinct\"] <= threshold\n    }\n\n    if len(categorical_variables) <= 1:\n        return None\n\n    correlation_matrix = pd.DataFrame(\n        np.ones((len(categorical_variables), len(categorical_variables))),\n        index=categorical_variables,\n        columns=categorical_variables,\n    )\n\n    for (name1, data1), (name2, data2) in itertools.combinations(\n        df[categorical_variables].items(), 2\n    ):\n        confusion_matrix = pd.crosstab(data1, data2)\n        corr = _cramers_corrected_stat(confusion_matrix, correction=True)\n        correlation_matrix.loc[name2, name1] = corr\n        correlation_matrix.loc[name1, name2] = corr\n\n    return correlation_matrix\n\n", "idx": 1415}
{"namespace": "ydata_profiling.model.pandas.correlations_pandas.pandas_auto_compute", "completion": "    # Identify numerical and categorical columns\n    numerical_columns = {\n        key\n        for key, value in summary.items()\n        if value[\"type\"] == \"Numeric\" and 1 < value[\"n_distinct\"]\n    }\n    categorical_columns = {\n        key\n        for key, value in summary.items()\n        if value[\"type\"] in {\"Categorical\", \"Boolean\"}\n        and 1 < value[\"n_distinct\"] <= config.categorical_maximum_correlation_distinct\n    }\n\n    # Check if there are enough columns to compute correlations\n    if len(numerical_columns) + len(categorical_columns) <= 1:\n        return None\n\n    # Discretize the DataFrame using a uniform discretization method\n    df_discretized = df.copy()\n    for column in df_discretized.columns:\n        if column in numerical_columns:\n            df_discretized[column] = pd.cut(\n                df_discretized[column], bins=config.categorical_maximum_correlation_distinct\n            )\n\n    # Calculate the correlation scores between each pair of columns\n    correlation_scores = {}\n    for col1, col2 in itertools.combinations(df_discretized.columns, 2):\n        if col1 in numerical_columns and col2 in numerical_columns:\n            correlation_scores[(col1, col2)] = _pairwise_spearman(\n                df_discretized[col1], df_discretized[col2]\n            )\n        elif col1 in categorical_columns and col2 in categorical_columns:\n            correlation_scores[(col1, col2)] = _pairwise_cramers(\n                df_discretized[col1], df_discretized[col2]\n            )\n\n    # Create a correlation matrix from the correlation scores\n    correlation_matrix = pd.DataFrame(\n        np.nan, index=df_discretized.columns, columns=df_discretized.columns\n    )\n    for (col1, col2), score in", "idx": 1416}
{"namespace": "ydata_profiling.controller.console.main", "completion": "    args = parse_args(args)\n\n    if args.output_file is None:\n        args.output_file = Path(args.input_file).with_suffix(\".html\")\n\n    if args.silent:\n        ProfileReport(\n            args.input_file,\n            minimal=args.minimal,\n            explorative=args.explorative,\n            pool_size=args.pool_size,\n            title=args.title,\n            config_file=args.config_file,\n            infer_dtypes=args.infer_dtypes,\n        ).to_file(output_file=args.output_file)\n    else:\n        ProfileReport(\n            args.input_file,\n            minimal=args.minimal,\n            explorative=args.explorative,\n            pool_size=args.pool_size,\n            title=args.title,\n            config_file=args.config_file,\n            infer_dtypes=args.infer_dtypes,\n        ).to_notebook_if_not_none()", "idx": 1417}
{"namespace": "ydata_profiling.utils.cache.cache_file", "completion": "    data_path = get_data_path()\n    data_path.mkdir(exist_ok=True)\n\n    file_path = data_path / file_name\n\n    if not file_path.exists():\n        response = request.urlopen(url)\n        with open(file_path, \"wb\") as f:\n            f.write(response.read())\n\n    return file_path\n\n", "idx": 1418}
{"namespace": "ydata_profiling.utils.dataframe.expand_mixed", "completion": "    if types is None:\n        types = [list, dict, tuple]\n\n    for col in df.columns:\n        if df[col].apply(lambda x: type(x) in types).any():\n            df = df.explode(col)\n            df = df.rename(columns={col: f\"{col}_exploded\"})\n            df = df.reset_index(drop=True)\n\n    return df", "idx": 1419}
{"namespace": "pysnooper.utils.ensure_tuple", "completion": "    if isinstance(x, collections_abc.Iterable) and not isinstance(x, str):\n        return tuple(x)\n    else:\n        return (x,)", "idx": 1420}
{"namespace": "rq.serializers.resolve_serializer", "completion": "    if serializer is None:\n        return DefaultSerializer\n    elif isinstance(serializer, str):\n        return resolve_serializer(import_from_string(serializer))\n    elif isinstance(serializer, type):\n        if not hasattr(serializer, 'dumps') or not hasattr(serializer, 'loads'):\n            raise NotImplementedError(\n                f'Serializer must implement `dumps` and `loads` methods. Got {serializer}'\n            )\n        return serializer\n    else:\n        raise ValueError(\n            f'Serializer must be either a path to a serializer, a SerializerProtocol implementation, or None. Got {serializer}'\n        )", "idx": 1421}
{"namespace": "lux.vis.Vis.Vis.get_attr_by_channel", "completion": "        return list(filter(lambda x: x.channel == channel, self._inferred_intent))\n", "idx": 1422}
{"namespace": "lux.action.default.register_default_actions", "completion": "    from lux.action.filter import register_filter_action\n    from lux.action.custom import register_custom_action\n    from lux.action.enhance import register_enhance_action\n    from lux.action.generalize import register_generalize_action\n    from lux.action.recommendation import register_recommendation_action\n    from lux.action.correlation import register_correlation_action\n    from lux.action.univariate import register_univariate_action\n    from lux.action.stop_execution import register_stop_execute_action\n\n    from lux.vis.VisList import VisList\n    from lux.vis.Vis import Vis\n\n    def no_vis(vis: Vis):\n        return len(vis) == 0\n\n    def multi_vis(vis: VisList):\n        return len(vis) > 1\n\n    def one_filter_vis(vis: VisList):\n        return (\n            len(vis) == 1\n            and vis[0].get_attr_by_channel(\"filter\") is not None\n            and len(vis[0]) > 0\n        )\n\n    def one_vis(vis: VisList):\n        return len(vis) == 1\n\n    def one_or_zero_vis(vis: VisList):\n        return len(vis) < 2\n\n    def two_or_more_vis(vis: VisList):\n        return len(vis) >= 2\n\n    def at_least_two_vis(vis: VisList):\n        return len(vis) >= 2\n\n    def multi_filter_vis(vis: VisList):\n        return (\n            len(vis) >= 1\n            and vis[0].get_attr_by_channel(\"filter\") is not None\n            and len(vis[0]) > 0\n        )\n\n    def multi_vis_one_filter(vis: VisList):\n        return (\n            len(vis) >= 1\n            and vis[0].get_attr_by_channel(\"filter\") is not None\n            and len(vis[0]) > 0\n        )\n\n    def at_least_two_filter_vis(", "idx": 1423}
{"namespace": "folium.utilities.get_bounds", "completion": "    if lonlat:\n        locations = _locations_mirror(locations)\n    bounds = [[None, None], [None, None]]\n    for coord in iter_coords(locations):\n        for i in (0, 1):\n            bounds[i][0] = none_min(bounds[i][0], coord[i])\n            bounds[i][1] = none_max(bounds[i][1], coord[i])\n    return bounds", "idx": 1424}
{"namespace": "folium.features.VegaLite.vegalite_major_version", "completion": "        schema = self.data.get(\"$schema\", \"https://vega.github.io/schema/vega-lite/v2.json\")\n        return int(schema.split(\"/\")[-1].split(\".\")[0])\n", "idx": 1425}
{"namespace": "music_dl.utils.colorize", "completion": "    if color not in colors:\n        raise ValueError(\"Unsupported color: %s\" % color)\n    if platform.system() == \"Windows\":\n        return string\n    return colors[color] + string + \"\\033[0m\"\n\n", "idx": 1426}
{"namespace": "music_dl.source.MusicSource.search", "completion": "        # Create a list of threads to search for the keyword in each source\n        threads = []\n        for source in sources_list:\n            t = threading.Thread(target=self.search_in_source, args=(source, keyword))\n            threads.append(t)\n\n        # Start all threads\n        for t in threads:\n            t.start()\n\n        # Wait for all threads to finish\n        for t in threads:\n            t.join()\n\n        # Sort and remove duplicates from the search results based on song title, singer, and file size\n        songs = self.sort_and_remove_duplicates(self.songs)\n\n        return songs\n", "idx": 1427}
{"namespace": "jwt.utils.base64url_decode", "completion": "    if isinstance(input, str):\n        input = input.encode(\"ascii\")\n\n    rem = len(input) % 4\n\n    if rem > 0:\n        input += b\"=\" * (4 - rem)\n\n    return base64.urlsafe_b64decode(input)\n\n", "idx": 1428}
{"namespace": "jwt.utils.to_base64url_uint", "completion": "    if val < 0:\n        raise ValueError(\"Must be a positive integer\")\n\n    int_bytes = val.to_bytes((val.bit_length() + 7) // 8, byteorder=\"big\")\n\n    if len(int_bytes) == 0:\n        int_bytes = b\"\\x00\"\n\n    return base64url_encode(int_bytes)\n\n", "idx": 1429}
{"namespace": "jwt.algorithms.HMACAlgorithm.prepare_key", "completion": "        if isinstance(key, str):\n            key = key.encode(\"utf-8\")\n\n        if isinstance(key, bytes):\n            if key.startswith(b\"-----BEGIN\"):\n                raise InvalidKeyError(\n                    \"The specified key is an asymmetric key or x509 certificate and \"\n                    \"should not be used as an HMAC secret.\"\n                )\n            return key\n\n        raise TypeError(\"Expecting a string- or bytes-formatted key.\")\n", "idx": 1430}
{"namespace": "jwt.algorithms.HMACAlgorithm.to_jwk", "completion": "        key_obj = force_bytes(key_obj)\n        jwk = {\"k\": base64url_encode(key_obj), \"kty\": \"oct\"}\n        if as_dict:\n            return jwk\n        return json.dumps(jwk)\n", "idx": 1431}
{"namespace": "jwt.algorithms.HMACAlgorithm.from_jwk", "completion": "        if isinstance(jwk, str):\n            jwk = json.loads(jwk)\n\n        if jwk[\"kty\"] != \"oct\":\n            raise InvalidKeyError(\"Invalid key type\")\n\n        return base64url_decode(jwk[\"k\"])\n", "idx": 1432}
{"namespace": "sacred.arg_parser._convert_value", "completion": "    try:\n        return ast.literal_eval(value)\n    except (ValueError, SyntaxError):\n        if strict_parsing:\n            raise\n        return value", "idx": 1433}
{"namespace": "sacred.utils.recursive_update", "completion": "    for k, v in u.items():\n        if isinstance(v, collections.abc.Mapping):\n            d[k] = recursive_update(d.get(k, {}), v)\n        else:\n            d[k] = v\n    return d\n\n", "idx": 1434}
{"namespace": "sacred.utils.iterate_flattened_separately", "completion": "    if manually_sorted_keys is None:\n        manually_sorted_keys = []\n    for key in manually_sorted_keys:\n        if key in dictionary:\n            yield key, dictionary[key]\n    for key, value in sorted(dictionary.items()):\n        if isinstance(value, dict) and value:\n            yield key, PATHCHANGE\n            for k, v in iterate_flattened_separately(value):\n                yield join_paths(key, k), v\n        else:\n            yield key, value\n    for key, value in sorted(dictionary.items()):\n        if key not in manually_sorted_keys and not isinstance(value, dict):\n            yield key, value\n\n", "idx": 1435}
{"namespace": "sacred.utils.iterate_flattened", "completion": "    for key, value in d.items():\n        if isinstance(value, dict):\n            for k, val in iterate_flattened(value):\n                yield join_paths(key, k), val\n        else:\n            yield key, value", "idx": 1436}
{"namespace": "sacred.utils.iter_prefixes", "completion": "    split_path = path.split(\".\")\n    for i in range(1, len(split_path) + 1):\n        yield \".\".join(split_path[:i])\n\n", "idx": 1437}
{"namespace": "sacred.utils.rel_path", "completion": "    if not is_prefix(base, path):\n        raise AssertionError(f\"{base} not a prefix of {path}\")\n    return path[len(base) + 1 :]", "idx": 1438}
{"namespace": "sacred.utils.convert_to_nested_dict", "completion": "    nested_dict = {}\n    for key, value in dotted_dict.items():\n        current_dict = nested_dict\n        for part in key.split(\".\"):\n            current_dict = current_dict.setdefault(part, {})\n        current_dict[\"\"] = value\n    return nested_dict\n\n", "idx": 1439}
{"namespace": "sacred.utils.format_sacred_error", "completion": "    lines = []\n    if short_usage:\n        lines.append(short_usage)\n    if e.print_traceback:\n        lines.append(format_filtered_stacktrace(e.filter_traceback))\n    else:\n        lines.append(\"{}: {}\".format(type(e).__name__, str(e)))\n    return \"\\n\".join(lines)\n\n", "idx": 1440}
{"namespace": "sacred.utils.get_package_version", "completion": "    try:\n        return parse_version(pkg_resources.get_distribution(name).version)\n    except pkg_resources.DistributionNotFound:\n        return parse_version(\"0.0.0\")\n\n", "idx": 1441}
{"namespace": "sacred.experiment.Experiment.main", "completion": "        self.default_command = self.capture(function)\n        return self.default_command\n", "idx": 1442}
{"namespace": "sacred.experiment.Experiment.run", "completion": "        if command_name is None:\n            command_name = self.default_command\n        if command_name is None:\n            raise RuntimeError(\n                \"No command specified and no default command found. \"\n                \"Please specify a command or use the @ex.main decorator.\"\n            )\n        if command_name not in self.commands:\n            raise KeyError(\n                \"Command '{}' not found. Available commands: {}\".format(\n                    command_name, \", \".join(self.commands)\n                )\n            )\n        if options is None:\n            options = {}\n        if config_updates is None:\n            config_updates = {}\n        if info is None:\n            info = {}\n        if meta_info is None:\n            meta_info = {}\n        config_updates = get_config_updates(\n            self.all_cli_options, options, config_updates\n        )\n        for hook in self.option_hooks:\n            hook(options=options)\n        config_updates = self.config.update(config_updates)\n        if named_configs:\n            config_updates = self.config.named_config(named_configs).update(\n                config_updates\n            )\n        if self.interactive:\n            config_updates = self.config.update(\n                {\n                    \"interactive\": True,\n                    \"captured_out_filter\": self.captured_out_filter,\n                }\n            )\n        run = create_run(\n            self,\n            command_name,\n            config_updates,\n            info,\n            meta_info,\n            options,\n            self.observers,\n            self.additional_host_info,\n        )\n        self.current_run = run\n        try:\n            run.run()\n        finally:\n            self.current_run = None\n        return run\n", "idx": 1443}
{"namespace": "sacred.host_info.host_info_getter", "completion": "    if name is None:\n        name = func.__name__\n    host_info_gatherers[name] = func\n    return func\n\n", "idx": 1444}
{"namespace": "sacred.ingredient.Ingredient.command", "completion": "        if function in self.commands:\n            return function\n        captured_function = create_captured_function(function, prefix=prefix)\n        self.commands[captured_function.__name__] = captured_function\n        return captured_function\n", "idx": 1445}
{"namespace": "sacred.ingredient.Ingredient.config", "completion": "        if function in self.config_hooks:\n            return function\n        self.config_hooks.append(function)\n        return function\n", "idx": 1446}
{"namespace": "sacred.ingredient.Ingredient.named_config", "completion": "        config_scope = ConfigScope(func)\n        self.named_configs[func.__name__] = config_scope\n        return config_scope\n", "idx": 1447}
{"namespace": "sacred.ingredient.Ingredient.gather_commands", "completion": "        for ingredient in self.ingredients:\n            for cmd_name, cmd in ingredient.gather_commands():\n                yield self.post_process_name(cmd_name, ingredient), cmd\n\n        for cmd_name, cmd in self.commands.items():\n            yield cmd_name, cmd", "idx": 1448}
{"namespace": "sacred.ingredient.Ingredient.gather_named_configs", "completion": "        for ingredient, _ in self.traverse_ingredients():\n            for name, conf in ingredient.named_configs.items():\n                full_name = join_paths(ingredient.path, name)\n                yield full_name, conf\n", "idx": 1449}
{"namespace": "sacred.dependencies.Source.create", "completion": "        if not os.path.exists(filename):\n            raise ValueError(f\"invalid filename or file not found {filename}\")\n        filename = get_py_file_if_possible(filename)\n        digest = get_digest(filename)\n        repo, commit, is_dirty = get_commit_if_possible(filename, save_git_info)\n        return Source(filename, digest, repo, commit, is_dirty)\n", "idx": 1450}
{"namespace": "sacred.dependencies.Source.to_json", "completion": "        if base_dir is not None:\n            filename = os.path.relpath(self.filename, base_dir)\n        else:\n            filename = self.filename\n        return filename, self.digest\n", "idx": 1451}
{"namespace": "sacred.dependencies.PackageDependency.create", "completion": "        if not cls.modname_to_dist:\n            for dist in pkg_resources.working_set:\n                for top_level_mod in dist.get_metadata(\"top_level.txt\"):\n                    cls.modname_to_dist[top_level_mod] = dist\n        if mod.__name__ in cls.modname_to_dist:\n            dist = cls.modname_to_dist[mod.__name__]\n            return PackageDependency(dist.project_name, dist.version)\n        else:\n            return PackageDependency(mod.__name__, None)\n\n", "idx": 1452}
{"namespace": "sacred.dependencies.is_local_source", "completion": "    # check if the module is a local source file\n    if filename.startswith(experiment_path):\n        return True\n\n    # check if the module is a package dependency\n    for prefix in iter_prefixes(modname):\n        if prefix in MODULE_BLACKLIST:\n            return False\n\n    return False\n\n", "idx": 1453}
{"namespace": "sacred.dependencies.gather_sources_and_dependencies", "completion": "    experiment_path, main = get_main_file(globs, save_git_info)\n    if base_dir is None:\n        base_dir = experiment_path\n    sources = source_discovery_strategies[SETTINGS.SOURCE_DISCOVERY](\n        globs, base_dir, save_git_info\n    )\n    if main is not None:\n        sources.add(main)\n    dependencies = dependency_discovery_strategies[SETTINGS.DEPENDENCY_DISCOVERY](\n        globs, base_dir\n    )\n    if \"numpy\" in sys.modules:\n        dependencies.add(PackageDependency.create(sys.modules[\"numpy\"]))\n    return main, sources, dependencies", "idx": 1454}
{"namespace": "sacred.observers.file_storage.FileStorageObserver.resource_event", "completion": "        store_path = self.find_or_save(filename, self.resource_dir)\n        relative_source = os.path.relpath(str(store_path), self.basedir)\n        self.run_entry[\"resources\"].append(relative_source)\n        self.save_json(self.run_entry, \"run.json\")", "idx": 1455}
{"namespace": "sacred.config.signature.Signature.get_free_parameters", "completion": "        # Check if the signature is bound to an instance\n        if bound:\n            # If the signature is bound to an instance, the first argument is the instance itself\n            # and should not be included in the free parameters\n            args = args[1:]\n\n        # Get the list of arguments that have been passed\n        passed_args = list(args) + list(kwargs.keys())\n\n        # Get the list of all arguments in the signature\n        all_args = self.arguments\n\n        # If the signature has a variable-length argument, remove it from the list of all arguments\n        if self.vararg_name:\n            all_args = all_args[:-1]\n\n        # If the signature has a keyword-only argument, remove it from the list of all arguments\n        if self.kw_wildcard_name:\n            all_args = all_args[:-1]\n\n        # Get the list of free parameters by removing the arguments that have been passed\n        free_params = [arg for arg in all_args if arg not in passed_args]\n\n        return free_params\n", "idx": 1456}
{"namespace": "sacred.config.signature.Signature.construct_arguments", "completion": "        expected_args = self._get_expected_args(bound)\n        # Check if there are too many arguments\n        if len(args) > len(expected_args):\n            raise SignatureError(\n                \"Too many arguments for function '{}'.\".format(self.name)\n            )\n\n        # Check if there are conflicting values for a parameter in both args and kwargs\n        for arg, value in zip(expected_args, args):\n            if arg in kwargs and kwargs[arg] != value:\n                raise SignatureError(\n                    \"Conflicting values for parameter '{}' in function '{}'.\".format(\n                        arg, self.name\n                    )\n                )\n\n        # Fill in missing arguments using options\n        for arg in expected_args[len(args) :]:\n            if arg in options:\n                kwargs[arg] = options[arg]\n            elif arg in self.kwargs:\n                kwargs[arg] = self.kwargs[arg]\n            else:\n                raise MissingConfigError(\n                    \"Missing parameter '{}' for function '{}'.\".format(arg, self.name)\n                )\n\n        # Check if there are unexpected arguments\n        for arg in kwargs:\n            if arg not in expected_args:\n                raise SignatureError(\n                    \"Unexpected parameter '{}' for function '{}'.\".format(arg, self.name)\n                )\n\n        # Check if there are unfilled parameters at the end\n        for arg in expected_args[len(args) :]:\n            if arg not in kwargs:\n                raise SignatureError(\n                    \"Missing parameter '{}' for function '{}'.\".format(arg, self.name)\n                )\n\n        # Construct the args list and kwargs dictionary\n        args = list(args)\n        for arg in expected_args[len(args) :]:\n            args.append(kwargs[arg])\n            del kwargs[arg]\n\n        return args, kwargs\n", "idx": 1457}
{"namespace": "sacred.config.config_files.load_config_file", "completion": "    handler = get_handler(filename)\n    with open(filename, \"r\" + handler.mode) as f:\n        return handler.load(f)\n\n", "idx": 1458}
{"namespace": "sacred.config.custom_containers.DogmaticDict.get", "completion": "        if dict.__contains__(self, k):\n            return dict.__getitem__(self, k)\n        elif k in self.fallback:\n            if k in self.fixed:\n                return self.fixed[k]\n            else:\n                return self.fallback[k]\n        return d", "idx": 1459}
{"namespace": "sacred.config.custom_containers.DogmaticDict.revelation", "completion": "        missing_keys = set()\n        for key in self.fixed:\n            if key not in self:\n                missing_keys.add(key)\n                self[key] = self.fixed[key]\n            elif isinstance(self[key], DogmaticDict):\n                sub_missing_keys = self[key].revelation()\n                missing_keys |= {join_paths(key, m) for m in sub_missing_keys}\n        return missing_keys", "idx": 1460}
{"namespace": "sacred.config.custom_containers.make_read_only", "completion": "    if isinstance(o, dict):\n        return ReadOnlyDict({k: make_read_only(v) for k, v in o.items()})\n    elif isinstance(o, list):\n        return ReadOnlyList([make_read_only(v) for v in o])\n    elif isinstance(o, tuple):\n        return tuple(make_read_only(v) for v in o)\n    else:\n        return o", "idx": 1461}
{"namespace": "sacred.config.config_scope.dedent_function_body", "completion": "    lines = body.splitlines()\n    indent = \"\"\n    for line in lines:\n        if not is_empty_or_comment(line):\n            indent = \"\".join(\n                [\n                    a\n                    for a, b in zip(line, lines[0])\n                    if a == b and not a.isspace()\n                ]\n            )\n            break\n    return \"\\n\".join([dedent_line(line, indent) for line in lines])\n\n", "idx": 1462}
{"namespace": "boltons.funcutils.FunctionBuilder.get_sig_str", "completion": "            if with_annotations:\n                return inspect_formatargspec(self.args, self.varargs,\n                                             self.varkw, self.defaults,\n                                             self.kwonlyargs,\n                                             self.kwonlydefaults,\n                                             self.annotations)\n            else:\n                return inspect_formatargspec(self.args, self.varargs,\n                                             self.varkw, self.defaults,\n                                             self.kwonlyargs,\n                                             self.kwonlydefaults)\n", "idx": 1463}
{"namespace": "boltons.funcutils.FunctionBuilder.get_invocation_str", "completion": "            # Get the arguments and keyword-only arguments of the FunctionBuilder instance\n            args = self.args\n            kwonlyargs = self.kwonlyargs\n\n            # If there are no arguments and no keyword-only arguments, return an empty string\n            if not args and not kwonlyargs:\n                return ''\n\n            # If there are no keyword-only arguments, return the arguments as a string\n            if not kwonlyargs:\n                return inspect_formatargspec(args, self.varargs, self.varkw, [])[1:-1]\n\n            # If there are keyword-only arguments, remove the * from the signature\n            sig_str = inspect_formatargspec(args, self.varargs, self.varkw, [], kwonlyargs, {}, {})[1:-1]\n            sig_str = _KWONLY_MARKER.sub(', ', sig_str)\n\n            # Return the invocation string\n            return sig_str\n", "idx": 1464}
{"namespace": "boltons.funcutils.FunctionBuilder.from_func", "completion": "        if isinstance(func, functools.partial):\n            func = func.func\n        fb = cls(func.__name__, **cls._argspec_to_dict(func))\n        fb.doc = func.__doc__\n        fb.module = func.__module__\n        fb.body = inspect.getsource(func)\n        fb.dict = dir_dict(func)\n        fb.is_async = _inspect_iscoroutinefunction(func)\n        return fb\n", "idx": 1465}
{"namespace": "boltons.funcutils.FunctionBuilder.get_defaults_dict", "completion": "        defaults_dict = {}\n        for i, arg in enumerate(self.args):\n            if i < len(self.defaults):\n                defaults_dict[arg] = self.defaults[i]\n            else:\n                defaults_dict[arg] = None\n        return defaults_dict", "idx": 1466}
{"namespace": "boltons.funcutils.FunctionBuilder.get_arg_names", "completion": "        if only_required:\n            return tuple(self.args)\n        else:\n            return tuple(self.args + self.kwonlyargs)\n", "idx": 1467}
{"namespace": "boltons.ioutils.SpooledIOBase.writelines", "completion": "        for line in lines:\n            self.write(line)\n", "idx": 1468}
{"namespace": "boltons.ioutils.SpooledBytesIO.write", "completion": "        self._checkClosed()\n        if not isinstance(s, binary_type):\n            raise TypeError(\"bytes expected, got %s\" % type(s))\n        if self.len + len(s) > self._max_size:\n            self.rollover()\n        self.buffer.write(s)\n", "idx": 1469}
{"namespace": "boltons.ioutils.SpooledBytesIO.seek", "completion": "        self._checkClosed()\n        return self.buffer.seek(pos, mode)\n", "idx": 1470}
{"namespace": "boltons.ioutils.SpooledBytesIO.len", "completion": "        pos = self.tell()\n        if self._rolled:\n            self.seek(0, os.SEEK_END)\n            length = self.tell()\n            self.seek(pos)\n        else:\n            length = len(self.buffer.getvalue())\n        return length\n\n", "idx": 1471}
{"namespace": "boltons.ioutils.SpooledStringIO.read", "completion": "        self._checkClosed()\n        if n == -1:\n            n = self.len\n        if n < 0:\n            raise IOError(EINVAL, \"Negative read count not allowed\")\n        if self._rolled:\n            self.seek(0)\n            val = self.buffer.read(n)\n        else:\n            val = self.buffer.read(n)\n        self._tell += len(val)\n        return val\n", "idx": 1472}
{"namespace": "boltons.ioutils.SpooledStringIO.write", "completion": "        self._checkClosed()\n        if not isinstance(s, text_type):\n            raise TypeError(\"{} expected, got {}\".format(\n                text_type.__name__,\n                type(s).__name__\n            ))\n\n        if self.tell() + len(s) >= self._max_size:\n            self.rollover()\n        self.buffer.write(s)\n        self._tell = self.tell() + len(s)\n", "idx": 1473}
{"namespace": "boltons.ioutils.SpooledStringIO.seek", "completion": "        self._checkClosed()\n        if mode == os.SEEK_SET:\n            self._tell = pos\n        elif mode == os.SEEK_CUR:\n            self._tell = self._traverse_codepoints(self._tell, pos)\n        elif mode == os.SEEK_END:\n            self._tell = self.len + pos\n        else:\n            raise ValueError(\"Invalid whence ({}, should be 0, 1, or 2)\".format(mode))\n        return self._tell\n", "idx": 1474}
{"namespace": "boltons.ioutils.SpooledStringIO.tell", "completion": "        self._checkClosed()\n        return self._tell\n", "idx": 1475}
{"namespace": "boltons.ioutils.SpooledStringIO.len", "completion": "        pos = self.tell()\n        if self._rolled:\n            self.seek(0)\n            val = os.fstat(self.fileno()).st_size\n        else:\n            self.seek(0, os.SEEK_END)\n            val = self.tell()\n        self.seek(pos)\n        return val\n\n", "idx": 1476}
{"namespace": "boltons.ioutils.MultiFileReader.read", "completion": "        if amt is None:\n            amt = -1\n        if amt < 0:\n            amt = float('inf')\n        ret = []\n        while amt > 0:\n            if self._index >= len(self._fileobjs):\n                break\n            f = self._fileobjs[self._index]\n            data = f.read(amt)\n            if not data:\n                self._index += 1\n                continue\n            ret.append(data)\n            amt -= len(data)\n        return self._joiner.join(ret)\n", "idx": 1477}
{"namespace": "boltons.ioutils.MultiFileReader.seek", "completion": "        if whence != os.SEEK_SET:\n            raise NotImplementedError('MultiFileReader.seek() only supports os.SEEK_SET')\n        if offset != 0:\n            raise NotImplementedError('MultiFileReader only supports seeking to start at this time')\n        for f in self._fileobjs:\n            f.seek(0)\n        self._index = 0\n", "idx": 1478}
{"namespace": "boltons.listutils.BarrelList.insert", "completion": "        if index < 0:\n            index += len(self)\n        if index < 0:\n            index = 0\n        if index > len(self):\n            index = len(self)\n        list_idx, rel_idx = self._translate_index(index)\n        if list_idx is None:\n            self.lists.append([item])\n        else:\n            cur_list = self.lists[list_idx]\n            cur_list.insert(rel_idx, item)\n            self._balance_list(list_idx)\n", "idx": 1479}
{"namespace": "boltons.listutils.BarrelList.pop", "completion": "        if len(a) == 0:\n            if len(self.lists) == 1:\n                return self.lists[0].pop()\n            else:\n                return self.lists[-1].pop()\n        else:\n            index = a[0]\n            if len(self.lists) == 1:\n                return self.lists[0].pop(index)\n            else:\n                list_idx, rel_idx = self._translate_index(index)\n                if list_idx is None:\n                    raise IndexError()\n                return self.lists[list_idx].pop(rel_idx)\n", "idx": 1480}
{"namespace": "boltons.listutils.BarrelList.sort", "completion": "        self.lists = [sorted(l) for l in self.lists]\n        self._balance_list(0)\n", "idx": 1481}
{"namespace": "boltons.urlutils.URL.path", "completion": "        if path_text.startswith('/'):\n            path_text = path_text[1:]\n        self.path_parts = tuple([unquote(p) if '%' in p else p\n                                 for p in path_text.split(u'/')])\n", "idx": 1482}
{"namespace": "boltons.urlutils.URL.navigate", "completion": "        if isinstance(dest, URL):\n            dest = dest.to_text()\n        if not isinstance(dest, unicode):\n            dest = to_unicode(dest)\n        if dest.startswith(u'//'):\n            dest = self.scheme + u':' + dest\n        elif dest.startswith(u'/'):\n            dest = self.scheme + u'://' + self.host + dest\n        elif dest.startswith(u'?'):\n            dest = self.to_text() + dest\n        elif dest.startswith(u'#'):\n            dest = self.to_text() + dest\n        elif dest.startswith(u'mailto:'):\n            dest = dest\n        else:\n            dest = self.to_text() + '/' + dest\n        return URL(dest).normalize()\n", "idx": 1483}
{"namespace": "boltons.urlutils.URL.to_text", "completion": "        parts = []\n        _add = parts.append\n        if self.scheme:\n            _add(self.scheme)\n            _add(u':')\n        if self.uses_netloc:\n            _add(u'//')\n            _add(self.get_authority(full_quote=full_quote,\n                                    with_userinfo=True))\n        if self.path_parts:\n            _add(u'/')\n            _add(self.path)\n        if self.query_params:\n            _add(u'?')\n            _add(self.query_params.to_text(full_quote=full_quote))\n        if self.fragment:\n            _add(u'#')\n            _add(quote_fragment_part(self.fragment, full_quote=full_quote))\n        return u''.join(parts)\n", "idx": 1484}
{"namespace": "boltons.urlutils.QueryParamDict.to_text", "completion": "        ret = []\n        _add = ret.append\n        for k, v in self.iteritems(multi=True):\n            _add(quote_query_part(k, full_quote=full_quote))\n            _add('=')\n            _add(quote_query_part(v, full_quote=full_quote))\n            _add('&')\n        if ret:\n            ret.pop()\n        return u''.join(ret)", "idx": 1485}
{"namespace": "boltons.tbutils.TracebackInfo.from_traceback", "completion": "        if tb is None:\n            tb = sys.exc_info()[2]\n            if tb is None:\n                raise ValueError('no tb set and no exception being handled')\n        ret = []\n        if limit is None:\n            limit = getattr(sys, 'tracebacklimit', 1000)\n        n = 0\n        while tb is not None and n < limit:\n            item = cls.callpoint_type.from_tb(tb)\n            ret.append(item)\n            tb = tb.tb_next\n            n += 1\n        return cls(ret)\n", "idx": 1486}
{"namespace": "boltons.tbutils.ExceptionInfo.get_formatted", "completion": "        ret = 'Traceback (most recent call last):\\n'\n        ret += self.tb_info.get_formatted()\n        ret += '%s: %s\\n' % (self.exc_type, self.exc_msg)\n        return ret", "idx": 1487}
{"namespace": "boltons.tbutils.print_exception", "completion": "    if file is None:\n        file = sys.stderr\n    for line in format_exception_only(etype, value):\n        file.write(line)\n    traceback.print_stack(tb, limit=limit, file=file)\n\n", "idx": 1488}
{"namespace": "boltons.tbutils.ParsedException.to_string", "completion": "        # Initialize the output string\n        output = \"\"\n\n        # Add the exception type and message to the output\n        output += f\"{self.exc_type}: {self.exc_msg}\\n\"\n\n        # Iterate over the frames in the traceback\n        for frame in self.frames:\n            # Add the file path, line number, and function name to the output\n            output += f\"  File \\\"{frame['filepath']}\\\", line {frame['lineno']}, in {frame['funcname']}\\n\"\n\n        # Return the formatted output\n        return output", "idx": 1489}
{"namespace": "boltons.tbutils.ParsedException.from_string", "completion": "        if isinstance(tb_str, bytes):\n            tb_str = tb_str.decode('utf-8')\n        lines = tb_str.splitlines()\n        frames = []\n        exc_type = None\n        exc_msg = None\n        for line in lines:\n            if line.startswith('Traceback (most recent call last):'):\n                continue\n            m = _frame_re.match(line)\n            if m:\n                frames.append(m.groupdict())\n                continue\n            if line.startswith('  File \"'):\n                continue\n            if line.startswith('    '):\n                frames[-1]['source_line'] = line[4:]\n                continue\n            if exc_type is None:\n                exc_type, exc_msg = line.split(':', 1)\n                exc_type = exc_type.strip()\n                exc_msg = exc_msg.strip()\n        return cls(exc_type, exc_msg, frames)\n", "idx": 1490}
{"namespace": "boltons.tableutils.Table.extend", "completion": "        if not data:\n            return\n        self._data.extend(data)\n        self._width = max(self._width, len(self._data[-1]))\n        self.fill()\n", "idx": 1491}
{"namespace": "boltons.tableutils.Table.from_object", "completion": "        return cls.from_data(data=data, headers=headers,\n                             max_depth=max_depth, _data_type=ObjectInputType(),\n                             metadata=metadata)\n", "idx": 1492}
{"namespace": "boltons.tableutils.Table.__repr__", "completion": "        if self.headers:\n            return '{0}(headers={1}, data={2})'.format(\n                type(self).__name__, self.headers, self._data)\n        return '{0}({1})'.format(type(self).__name__, self._data)\n", "idx": 1493}
{"namespace": "boltons.tableutils.Table.to_text", "completion": "        # Get the maximum length of each column\n        max_lengths = [max(len(str(cell)) for cell in column) for column in zip(*self._data)]\n\n        # If maxlen is specified, truncate the data in each cell to maxlen\n        if maxlen is not None:\n            for i, column in enumerate(zip(*self._data)):\n                for j, cell in enumerate(column):\n                    if len(str(cell)) > maxlen:\n                        self._data[j][i] = str(cell)[:maxlen] + '...'\n\n        # Convert each cell to a string\n        for i, column in enumerate(zip(*self._data)):\n            for j, cell in enumerate(column):\n                self._data[j][i] = str(cell)\n\n        # Pad each cell with spaces to be centered in the cell\n        for i, column in enumerate(zip(*self._data)):\n            for j, cell in enumerate(column):\n                self._data[j][i] = cell.center(max_lengths[i])\n\n        # Add the header row to the data\n        if with_headers:\n            self._data.insert(0, self.headers)\n\n        # Format the data in a table-like structure\n        table_str = ''\n        for i, row in enumerate(self._data):\n            table_str += ' | '.join(row) + '\\n'\n            if i == 0 and with_headers:\n                table_str += '-' * len(table_str) + '\\n'\n\n        return table_str", "idx": 1494}
{"namespace": "boltons.statsutils.Stats.get_histogram_counts", "completion": "        if bins is None:\n            bins = self._get_bin_bounds(**kw)\n        elif isinstance(bins, int):\n            bins = self._get_bin_bounds(bins, **kw)\n\n        bin_counts = [0] * len(bins)\n        for v in self.data:\n            bin_counts[bisect.bisect_left(bins, v)] += 1\n\n        return list(zip(bins, bin_counts))\n", "idx": 1495}
{"namespace": "boltons.setutils.IndexedSet.add", "completion": "        if item in self.item_index_map:\n            return\n        self.item_index_map[item] = len(self.item_list)\n        self.item_list.append(item)\n", "idx": 1496}
{"namespace": "boltons.setutils.IndexedSet.pop", "completion": "        if index is None:\n            index = -1\n        else:\n            index = self._get_real_index(index)\n        item = self.item_list[index]\n        if index == len(self) - 1:\n            self.item_list.pop()\n            del self.item_index_map[item]\n        else:\n            self.item_list[index] = _MISSING\n            del self.item_index_map[item]\n            self._add_dead(index)\n        self._cull()\n        return item\n", "idx": 1497}
{"namespace": "boltons.setutils.IndexedSet.index", "completion": "        try:\n            return self.item_index_map[val]\n        except KeyError:\n            raise ValueError('%r is not in %s' % (val, self.__class__.__name__))\n", "idx": 1498}
{"namespace": "boltons.setutils.complement", "completion": "    class _ComplementSet(set):\n        def __init__(self, wrapped):\n            self.wrapped = wrapped\n\n        def __contains__(self, item):\n            return item not in self.wrapped\n\n        def __iter__(self):\n            return iter(self.wrapped)\n\n        def __len__(self):\n            return len(self.wrapped)\n\n        def __repr__(self):\n            return 'complement(%r)' % self.wrapped\n\n    return _ComplementSet(wrapped)", "idx": 1499}
{"namespace": "boltons.strutils.strip_ansi", "completion": "    if isinstance(text, bytes):\n        return ANSI_SEQUENCES.sub(b'', text)\n    elif isinstance(text, bytearray):\n        return bytearray(ANSI_SEQUENCES.sub(b'', text))\n    else:\n        return ANSI_SEQUENCES.sub('', text)\n\n", "idx": 1500}
{"namespace": "boltons.strutils.asciify", "completion": "    if isinstance(text, str):\n        text = text.decode('utf-8')\n    if not isinstance(text, unicode):\n        raise TypeError('Expected a unicode or str, got %s' % type(text))\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore' if ignore else 'replace')\n    return text\n\n", "idx": 1501}
{"namespace": "boltons.strutils.indent", "completion": "    return newline.join(margin + line if key(line) else line for line in text.splitlines())", "idx": 1502}
{"namespace": "boltons.strutils.multi_replace", "completion": "    options = {\n        'regex': False,\n        'flags': 0,\n    }\n    options.update(kwargs)\n    m = MultiReplace(sub_map, **options)\n    return m.sub(text)", "idx": 1503}
{"namespace": "boltons.cacheutils.LRI._get_flattened_ll", "completion": "        # TODO: This function is not used anywhere.\n        # TODO: This function is not tested.\n\n        # TODO: This function is not documented.\n\n        # TODO: This function is not used anywhere.\n\n        # TODO: This function is not tested.\n\n        # TODO: This function is not documented.\n\n        # TODO: This function is not used anywhere.\n\n        # TODO: This function is not tested.\n\n        # TODO: This function is not documented.\n\n        # TODO: This function is not used anywhere.\n\n        # TODO: This function is not tested.\n\n        # TODO: This function is not documented.\n\n        # TODO: This function is not used anywhere.\n\n        # TODO: This function is not tested.\n\n        # TODO: This function is not documented.\n\n        # TODO: This function is not used anywhere.\n\n        # TODO: This function is not tested.\n\n        # TODO: This function is not documented.\n\n        # TODO: This function is not used anywhere.\n\n        # TODO: This function is not tested.\n\n        # TODO: This function is not documented.\n\n        # TODO: This function is not used anywhere.\n\n        # TODO: This function is not tested.\n\n        # TODO: This function is not documented.\n\n        # TODO: This function is not used anywhere.\n\n        # TODO: This function is not tested.\n\n        # TODO: This function is not documented.\n\n        # TODO: This function is not used anywhere.\n\n        # TODO: This function is not tested.\n\n        # TODO: This function is not documented.\n\n        # TODO: This function is not used anywhere.\n\n        # TODO: This function is not tested.\n\n        # TODO: This function is not documented.\n\n        # TODO: This function is not used anywhere.\n\n        # TODO: This function is not tested.\n\n        # TODO: This function is not documented.\n\n        # TODO: This function is not used anywhere.\n\n        # TODO: This function is not tested.\n\n        # TODO: This function is not documented.\n\n        # TODO: This function is not used anywhere.\n\n        # TODO: This", "idx": 1504}
{"namespace": "boltons.cacheutils.LRI.pop", "completion": "        with self._lock:\n            try:\n                link = self._link_lookup[key]\n            except KeyError:\n                if default is _MISSING:\n                    raise\n                return default\n            else:\n                self._remove_from_ll(key)\n                return link[VALUE]\n", "idx": 1505}
{"namespace": "boltons.cacheutils.LRI.popitem", "completion": "        with self._lock:\n            key, value = super(LRI, self).popitem()\n            self._remove_from_ll(key)\n            return key, value\n", "idx": 1506}
{"namespace": "boltons.cacheutils.LRI.clear", "completion": "        with self._lock:\n            super(LRI, self).clear()\n            self._init_ll()\n            self.hit_count = self.miss_count = self.soft_miss_count = 0\n", "idx": 1507}
{"namespace": "boltons.cacheutils.LRI.setdefault", "completion": "        with self._lock:\n            try:\n                return self[key]\n            except KeyError:\n                self.soft_miss_count += 1\n                return self.setdefault(key, default)\n", "idx": 1508}
{"namespace": "boltons.cacheutils.LRI.update", "completion": "        if hasattr(E, 'keys'):\n            for k in E.keys():\n                self[k] = E[k]\n        else:\n            for k, v in E:\n                self[k] = v\n        for k in F:\n            self[k] = F[k]\n", "idx": 1509}
{"namespace": "boltons.cacheutils.LRI.__repr__", "completion": "        return '{}(max_size={}, on_miss={}, values={})'.format(\n            self.__class__.__name__,\n            self.max_size,\n            self.on_miss,\n            super(LRI, self).__repr__()\n        )", "idx": 1510}
{"namespace": "boltons.cacheutils.CachedFunction.__repr__", "completion": "        cn = self.__class__.__name__\n        if self.scoped or self.typed:\n            return '%s(func=%r, scoped=%r, typed=%r)' % (cn, self.func,\n                                                          self.scoped, self.typed)\n        return '%s(func=%r)' % (cn, self.func)\n\n", "idx": 1511}
{"namespace": "boltons.cacheutils.ThresholdCounter.elements", "completion": "        for key, value in self._count_map.items():\n            for i in range(value[0]):\n                yield key\n", "idx": 1512}
{"namespace": "boltons.cacheutils.ThresholdCounter.most_common", "completion": "        if n is None:\n            return sorted(self.iteritems(), key=lambda x: x[1], reverse=True)\n        else:\n            return heapq.nlargest(n, self.iteritems(), key=lambda x: x[1])\n", "idx": 1513}
{"namespace": "boltons.cacheutils.ThresholdCounter.update", "completion": "        if isinstance(iterable, Mapping):\n            for key, count in iterable.items():\n                self.add(key, count)\n        else:\n            for key in iterable:\n                self.add(key)\n        for key, count in kwargs.items():\n            self.add(key, count)\n", "idx": 1514}
{"namespace": "boltons.cacheutils.MinIDMap.get", "completion": "        try:\n            return self.mapping[a]\n        except KeyError:\n            pass\n\n        if self.free:\n            i = self.free.pop()\n        else:\n            i = len(self.mapping)\n\n        self.mapping[a] = i\n        self.ref_map[i] = weakref.ref(a, self._on_destroy)\n        return i\n", "idx": 1515}
{"namespace": "boltons.iterutils.chunked", "completion": "    if not is_iterable(src):\n        raise TypeError('expected an iterable')\n\n    if count is not None:\n        count = int(count)\n\n    if size < 1:\n        raise ValueError('size must be greater than 0')\n\n    if count is not None and count < 1:\n        raise ValueError('count must be greater than 0')\n\n    fill = kw.get('fill', None)\n\n    if fill is not None and not is_scalar(fill):\n        raise TypeError('fill must be a scalar')\n\n    if fill is not None and count is not None:\n        raise ValueError('count and fill cannot be used together')\n\n    if fill is not None:\n        fill = [fill]\n\n    if count is not None:\n        return list(chunked_iter(src, size, count, fill))\n\n    return list(chunked_iter(src, size, fill=fill))\n\n", "idx": 1516}
{"namespace": "boltons.iterutils.chunk_ranges", "completion": "    if not isinstance(input_size, int):\n        raise TypeError('input_size must be an integer')\n    if not isinstance(chunk_size, int):\n        raise TypeError('chunk_size must be an integer')\n    if not isinstance(input_offset, int):\n        raise TypeError('input_offset must be an integer')\n    if not isinstance(overlap_size, int):\n        raise TypeError('overlap_size must be an integer')\n    if not isinstance(align, bool):\n        raise TypeError('align must be a boolean')\n\n    if input_size < 0:\n        raise ValueError('input_size must be non-negative')\n    if chunk_size < 0:\n        raise ValueError('chunk_size must be non-negative')\n    if input_offset < 0:\n        raise ValueError('input_offset must be non-negative')\n    if overlap_size < 0:\n        raise ValueError('overlap_size must be non-negative')\n    if chunk_size < overlap_size:\n        raise ValueError('chunk_size must be greater than or equal to overlap_size')\n\n    if input_size == 0:\n        return\n\n    if align:\n        input_offset = (input_offset // (chunk_size - overlap_size)) * (chunk_size - overlap_size)\n\n    start = input_offset\n    while start < input_size:\n        end = min(start + chunk_size, input_size)\n        yield start, end\n        start = end - overlap_size\n\n", "idx": 1517}
{"namespace": "boltons.iterutils.remap", "completion": "    reraise_visit = kwargs.get('reraise_visit', True)\n    path = kwargs.get('path', ())\n    if visit is None:\n        visit = default_visit\n    if enter is None:\n        enter = default_enter\n    if exit is None:\n        exit = default_exit\n    new_parent, iterator = enter(path, None, root)\n    if iterator is False:\n        return new_parent\n    new_items = []\n    for key, value in iterator:\n        if visit is not None:\n            try:\n                key, value = visit(path, key, value)\n            except Exception as e:\n                if reraise_visit:\n                    raise\n                else:\n                    continue\n        if value is _REMAP_EXIT:\n            return _REMAP_EXIT\n        if isinstance(value, basestring):\n            new_items.append((key, value))\n        elif isinstance(value, Mapping):\n            new_items.append((key, remap(value, visit, enter, exit,\n                                        path=path + (key,),\n                                        reraise_visit=reraise_visit)))\n        elif isinstance(value, Sequence):\n            new_items.append((key, remap(value, visit, enter, exit,\n                                        path=path + (key,),\n                                        reraise_visit=reraise_visit)))\n        elif isinstance(value, Set):\n            new_items.append((key, remap(value, visit, enter, exit,\n                                        path=path + (key,),\n                                        reraise_visit=reraise_visit)))\n        else:\n            new_items.append((key, value))\n    return exit(path, None, root, new_parent, new_items)\n\n", "idx": 1518}
{"namespace": "boltons.iterutils.get_path", "completion": "    if not isinstance(path, tuple):\n        raise TypeError('expected tuple path, not: %r' % path)\n    if not path:\n        return root\n    cur = root\n    for i, seg in enumerate(path):\n        try:\n            cur = cur[seg]\n        except (KeyError, IndexError, TypeError) as exc:\n            if default is _UNSET:\n                raise PathAccessError(exc, seg, path)\n            else:\n                return default\n    return cur", "idx": 1519}
{"namespace": "boltons.iterutils.research", "completion": "    if not callable(query):\n        raise TypeError('query expected callable, not: %r' % query)\n\n    path, registry, stack = (), {}, [(None, root)]\n    results = []\n    while stack:\n        key, value = stack.pop()\n        id_value = id(value)\n        if id_value in registry:\n            value = registry[id_value]\n        else:\n            res = default_enter(path, key, value)\n            try:\n                new_parent, new_items = res\n            except TypeError:\n                # TODO: handle False?\n                raise TypeError('enter should return a tuple of (new_parent,'\n                                ' items_iterator), not: %r' % res)\n            if new_items is not False:\n                # traverse unless False is explicitly passed\n                registry[id_value] = new_parent\n                if value is not root:\n                    path += (key,)\n                stack.append((_REMAP_EXIT, (key, new_parent, value)))\n                if new_items:\n                    stack.extend(reversed(list(new_items)))\n                continue\n        try:\n            if query(path, key, value):\n                results.append((path, value))\n        except Exception:\n            if reraise:\n                raise\n    return results", "idx": 1520}
{"namespace": "boltons.socketutils.BufferedSocket.getrecvbuffer", "completion": "        return self.rbuf\n", "idx": 1521}
{"namespace": "boltons.socketutils.BufferedSocket.getsendbuffer", "completion": "        with self._send_lock:\n            return self.sbuf[:]\n", "idx": 1522}
{"namespace": "boltons.socketutils.BufferedSocket.recv", "completion": "        if flags != 0:\n            raise ValueError('non-zero flags not supported: {flags}'.format(flags=flags))\n\n        if timeout is _UNSET:\n            timeout = self.timeout\n\n        with self._recv_lock:\n            if len(self.rbuf) >= size:\n                data, self.rbuf = self.rbuf[:size], self.rbuf[size:]\n                return data\n\n            if self.rbuf:\n                data, self.rbuf = self.rbuf, b''\n                return data\n\n            if timeout is not None:\n                self.sock.settimeout(timeout)\n\n            try:\n                data = self.sock.recv(size)\n            except socket.timeout:\n                raise Timeout('timed out receiving {size} bytes'.format(size=size))\n\n            if len(data) > size:\n                self.rbuf = data[size:]\n                return data[:size]\n\n            return data\n", "idx": 1523}
{"namespace": "boltons.socketutils.BufferedSocket.recv_close", "completion": "        if timeout is _UNSET:\n            timeout = self.timeout\n        if maxsize is _UNSET:\n            maxsize = self.maxsize\n        if maxsize > _RECV_LARGE_MAXSIZE:\n            raise ValueError(\"maxsize too large: %r\" % maxsize)\n        with self._recv_lock:\n            if self.rbuf:\n                data, self.rbuf = self.rbuf, b''\n            else:\n                data = b''\n            self.sock.settimeout(timeout)\n            while True:\n                try:\n                    data += self.sock.recv(self._recvsize)\n                except socket.timeout:\n                    raise Timeout(timeout)\n                if len(data) > maxsize:\n                    raise MessageTooLong(maxsize)\n                if not data:\n                    break\n        return data\n", "idx": 1524}
{"namespace": "boltons.socketutils.BufferedSocket.flush", "completion": "        with self._send_lock:\n            self.sock.send(b'')\n", "idx": 1525}
{"namespace": "boltons.socketutils.BufferedSocket.buffer", "completion": "        with self._send_lock:\n            self.sbuf.append(data)\n", "idx": 1526}
{"namespace": "boltons.socketutils.BufferedSocket.close", "completion": "        with self._send_lock:\n            self.sock.close()\n            self.rbuf = b''\n            self.sbuf = []\n", "idx": 1527}
{"namespace": "boltons.socketutils.NetstringSocket.setmaxsize", "completion": "        self.maxsize = maxsize\n        self._msgsize_maxsize = len(str(maxsize)) + 1\n", "idx": 1528}
{"namespace": "boltons.socketutils.NetstringSocket.write_ns", "completion": "        size = len(payload)\n        if size > self.maxsize:\n            raise NetstringMessageTooLong(size, self.maxsize)\n        size_prefix = str(size).encode('ascii')\n        self.bsock.send(size_prefix + b':' + payload + b',')\n\n", "idx": 1529}
{"namespace": "boltons.fileutils.FilePerms.__repr__", "completion": "        return '%s(user=%r, group=%r, other=%r)' % (\n            self.__class__.__name__, self.user, self.group, self.other)\n", "idx": 1530}
{"namespace": "boltons.mathutils.Bits.as_hex", "completion": "        template = '{{0:0{0}x}}'.format(int(ceil(self.len / 4)))\n        return template.format(self.val)\n", "idx": 1531}
{"namespace": "boltons.mathutils.Bits.from_hex", "completion": "        if type(hex) is bytes:\n            hex = hex.decode('ascii')\n        if not hex.startswith('0x'):\n            hex = '0x' + hex\n        return cls(hex)\n", "idx": 1532}
{"namespace": "boltons.formatutils.split_format_str", "completion": "    # Initialize variables\n    literal_str = ''\n    field_str = ''\n    field_list = []\n    in_field = False\n\n    # Iterate over the characters in the format string\n    for i, c in enumerate(fstr):\n        # If the current character is a left brace, start a new field\n        if c == '{':\n            # If we are already in a field, add the current field to the list\n            if in_field:\n                field_list.append((literal_str, field_str))\n                literal_str = ''\n                field_str = ''\n            # Start a new field\n            in_field = True\n            field_str += c\n        # If the current character is a right brace, end the current field\n        elif c == '}':\n            # If we are in a field, add the current field to the list\n            if in_field:\n                field_list.append((literal_str, field_str + c))\n                literal_str = ''\n                field_str = ''\n            # End the current field\n            in_field = False\n        # If we are in a field, add the current character to the field string\n        elif in_field:\n            field_str += c\n        # If we are not in a field, add the current character to the literal string\n        else:\n            literal_str += c\n\n    # If we are in a field at the end of the string, add the current field to the list\n    if in_field:\n        field_list.append((literal_str, field_str))\n\n    return field_list\n\n", "idx": 1533}
{"namespace": "boltons.formatutils.infer_positional_format_args", "completion": "    # Split the format string into a list of strings\n    fstr_list = split_format_str(fstr)\n\n    # Initialize a counter for the numbered positional arguments\n    counter = 0\n\n    # Iterate over the list of strings\n    for i, (lit, field_str) in enumerate(fstr_list):\n        # If the string is a field string, replace the anonymous positional argument with a numbered one\n        if field_str:\n            # Increment the counter\n            counter += 1\n            # Replace the anonymous positional argument with a numbered one\n            fstr_list[i] = (lit, field_str.replace('{}', '{0}').format(counter))\n\n    # Join the list of strings into a single string\n    return ''.join(lit + field_str for lit, field_str in fstr_list)\n\n", "idx": 1534}
{"namespace": "boltons.formatutils.tokenize_format_str", "completion": "    # TODO: memoize\n    if resolve_pos:\n        fstr = infer_positional_format_args(fstr)\n    formatter = Formatter()\n    ret = []\n    for lit, fname, fspec, conv in formatter.parse(fstr):\n        if fname is None:\n            ret.append((lit, None))\n            continue\n        field_str = construct_format_field_str(fname, fspec, conv)\n        ret.append((lit, BaseFormatField(field_str)))\n    return ret\n\n", "idx": 1535}
{"namespace": "boltons.dictutils.OneToOne.clear", "completion": "        dict.clear(self)\n        dict.clear(self.inv)\n", "idx": 1536}
{"namespace": "boltons.dictutils.OneToOne.pop", "completion": "        if key not in self:\n            if default is _MISSING:\n                raise KeyError(key)\n            return default\n        val = self[key]\n        del self[key]\n        return val\n", "idx": 1537}
{"namespace": "boltons.dictutils.OneToOne.popitem", "completion": "        key, val = dict.popitem(self)\n        dict.__delitem__(self.inv, val)\n        return key, val\n", "idx": 1538}
{"namespace": "boltons.dictutils.ManyToMany.update", "completion": "        if isinstance(iterable, ManyToMany):\n            for key, vals in iterable.data.items():\n                self[key] = vals\n            return\n        if hasattr(iterable, 'keys'):\n            for key in iterable.keys():\n                self[key] = iterable[key]\n            return\n        for key, vals in iterable:\n            self[key] = vals\n", "idx": 1539}
{"namespace": "boltons.dictutils.ManyToMany.add", "completion": "        if key not in self.data:\n            self.data[key] = set()\n        self.data[key].add(val)\n        if val not in self.inv.data:\n            self.inv.data[val] = set()\n        self.inv.data[val].add(key)\n        return\n", "idx": 1540}
{"namespace": "boltons.dictutils.ManyToMany.remove", "completion": "        self.data[key].remove(val)\n        if not self.data[key]:\n            del self.data[key]\n        self.inv.data[val].remove(key)\n        if not self.inv.data[val]:\n            del self.inv.data[val]\n", "idx": 1541}
{"namespace": "boltons.dictutils.ManyToMany.replace", "completion": "        if key not in self.data:\n            return\n        vals = self.data.pop(key)\n        self.data[newkey] = vals\n        for val in vals:\n            self.inv.data[val].remove(key)\n            self.inv.data[val].add(newkey)\n", "idx": 1542}
{"namespace": "boltons.dictutils.ManyToMany.iteritems", "completion": "        for key in self.data:\n            yield key, self[key]\n", "idx": 1543}
{"namespace": "gunicorn.config.Config.__str__", "completion": "        lines = []\n        key_max_length = max(map(len, self.settings.keys()))\n        for key, value in self.settings.items():\n            if callable(value):\n                value = \"<%s()>\" % value.__qualname__\n            lines.append(\"%s = %s\" % (key.ljust(key_max_length), value))\n        return \"\\n\".join(lines)\n", "idx": 1544}
{"namespace": "gunicorn.config.Config.set", "completion": "        if name not in self.settings:\n            raise AttributeError(\"No configuration setting for: %s\" % name)\n        self.settings[name].set(value)\n", "idx": 1545}
{"namespace": "gunicorn.config.Config.worker_class", "completion": "        uri = self.settings['worker_class'].get()\n\n        # are we using a threaded worker?\n        is_sync = uri.endswith('SyncWorker') or uri == 'sync'\n        if is_sync and self.threads > 1:\n            uri = 'gthread'\n\n        # load the worker class\n        worker_class = util.import_app(uri)\n\n        # setup the worker class\n        if hasattr(worker_class, 'setup'):\n            worker_class.setup()\n\n        return worker_class\n", "idx": 1546}
{"namespace": "gunicorn.config.Config.address", "completion": "        # Get the bind address from settings\n        bind = self.settings['bind'].get()\n\n        # If bind is a string, split it by comma and strip whitespace\n        if isinstance(bind, str):\n            bind = [x.strip() for x in bind.split(',')]\n\n        # If bind is a list, return it as is\n        if isinstance(bind, list):\n            return bind\n\n        # If bind is not a string or list, raise a TypeError\n        raise TypeError(\"bind must be a string or list\")\n", "idx": 1547}
{"namespace": "gunicorn.config.Config.logger_class", "completion": "        if self.settings['logger_class'].get() == \"simple\":\n            if self.settings['statsd_host'].get() is not None:\n                self.settings['logger_class'].set(\"gunicorn.instrument.statsd.Statsd\")\n            return util.import_app(\"gunicorn.glogging.Logger\")\n\n        logger_class = util.import_app(\n            self.settings['logger_class'].get(),\n            default=\"gunicorn.glogging.Logger\",\n            section=\"gunicorn.loggers\")\n\n        if hasattr(logger_class, \"setup\"):\n            logger_class.setup(self)\n\n        return logger_class\n", "idx": 1548}
{"namespace": "gunicorn.sock.create_sockets", "completion": "    sockets = []\n    if fds is None:\n        fds = []\n    for addr in conf.addresses:\n        sock_type = _sock_type(addr)\n        if sock_type is UnixSocket:\n            if not conf.is_ssl:\n                sock = sock_type(addr, conf, log)\n                sockets.append(sock)\n            else:\n                raise ValueError(\"SSL not supported for Unix sockets\")\n        else:\n            if not conf.is_ssl:\n                sock = sock_type(addr, conf, log)\n                sockets.append(sock)\n            else:\n                if len(fds) == 0:\n                    raise ValueError(\"No file descriptors provided for SSL\")\n                fd = fds.pop(0)\n                sock = sock_type(addr, conf, log, fd=fd)\n                sockets.append(sock)\n    return sockets", "idx": 1549}
{"namespace": "gunicorn.http.body.LengthReader.read", "completion": "        if not isinstance(size, int):\n            raise TypeError(\"size must be an integral type\")\n        if size < 0:\n            raise ValueError(\"Size must be positive.\")\n        if size == 0:\n            return b\"\"\n\n        buf = io.BytesIO()\n        while buf.tell() < size:\n            data = self.unreader.read()\n            if not data:\n                raise NoMoreData()\n            buf.write(data)\n\n        data = buf.getvalue()\n        ret, rest = data[:size], data[size:]\n        self.unreader.unread(rest)\n        self.length -= size\n        return ret\n\n", "idx": 1550}
{"namespace": "gunicorn.http.body.EOFReader.read", "completion": "        if not isinstance(size, int):\n            raise TypeError(\"size must be an integral type\")\n\n        if size < 0:\n            raise ValueError(\"Size must be positive.\")\n\n        if size == 0:\n            return b\"\"\n\n        if self.finished:\n            data = self.buf.getvalue()\n            ret, rest = data[:size], data[size:]\n            self.buf = io.BytesIO()\n            self.buf.write(rest)\n            return ret\n\n        while self.buf.tell() < size:\n            data = self.unreader.read()\n            if not data:\n                self.finished = True\n                break\n            self.buf.write(data)\n\n        data = self.buf.getvalue()\n        ret, rest = data[:size], data[size:]\n        self.buf = io.BytesIO()\n        self.buf.write(rest)\n        return ret", "idx": 1551}
{"namespace": "gunicorn.http.body.Body.read", "completion": "        size = self.getsize(size)\n        if size == 0:\n            return b\"\"\n\n        if size < self.buf.tell():\n            data = self.buf.getvalue()\n            ret, rest = data[:size], data[size:]\n            self.buf = io.BytesIO()\n            self.buf.write(rest)\n            return ret\n\n        data = self.reader.read()\n        while data:\n            self.buf.write(data)\n            if self.buf.tell() > size:\n                break\n            data = self.reader.read()\n\n        data = self.buf.getvalue()\n        ret, rest = data[:size], data[size:]\n        self.buf = io.BytesIO()\n        self.buf.write(rest)\n        return ret\n", "idx": 1552}
{"namespace": "gunicorn.http.unreader.Unreader.read", "completion": "        if not isinstance(size, (int, long)):\n            raise TypeError(\"size parameter must be an int or long\")\n        if size == 0:\n            return b\"\"\n        if size < 0:\n            size = None\n\n        self.buf.seek(0, os.SEEK_END)\n        if size is None and self.buf.tell():\n            return self.read(self.buf.tell())\n\n        while True:\n            m = self.chunk()\n            if not m:\n                break\n            self.buf.write(m)\n\n        self.buf.seek(0, os.SEEK_SET)\n        data = self.buf.read(size)\n        self.buf = io.BytesIO()\n        if m:\n            self.buf.write(m)\n        return data\n", "idx": 1553}
{"namespace": "gunicorn.http.unreader.Unreader.unread", "completion": "        self.buf.seek(0, os.SEEK_END)\n        self.buf.write(data)\n", "idx": 1554}
{"namespace": "gunicorn.http.unreader.IterUnreader.chunk", "completion": "        if self.iter is None:\n            return b\"\"\n        try:\n            return next(self.iter)\n        except StopIteration:\n            self.iter = None\n            return b\"\"", "idx": 1555}
{"namespace": "gunicorn.instrument.statsd.Statsd.critical", "completion": "        Logger.critical(self, msg, *args, **kwargs)\n        self.incr(\"gunicorn.log.critical\")\n", "idx": 1556}
{"namespace": "gunicorn.instrument.statsd.Statsd.access", "completion": "        self.histogram(\"gunicorn.request.duration\", request_time.total_seconds() * 1000)\n        self.increment(\"gunicorn.request.count\", 1)\n        status_code = resp.status_code\n        if isinstance(status_code, str):\n            status_code = int(status_code.split(None, 1)[0])\n        self.increment(\"gunicorn.request.status.%s\" % status_code, 1)\n", "idx": 1557}
{"namespace": "praw.exceptions.RedditErrorItem.error_message", "completion": "        if self.error_message:\n            return f\"{self.error_type}: {self.message} on field {self.field}\"\n        return self.error_type\n", "idx": 1558}
{"namespace": "praw.exceptions.RedditErrorItem.__repr__", "completion": "        return (\n            f\"{self.__class__.__name__}(error_type={self.error_type}, \"\n            f\"message={self.message}, field={self.field})\"\n        )", "idx": 1559}
{"namespace": "praw.models.util.BoundedSet.add", "completion": "        self._set[item] = None\n        self._set.move_to_end(item)\n        if len(self._set) > self.max_items:\n            self._set.popitem(last=False)\n", "idx": 1560}
{"namespace": "praw.models.util.ExponentialCounter.counter", "completion": "        jitter = random.uniform(-self._max / 32, self._max / 32)\n        value = self._base + jitter\n        if self._base <= self._max / 2:\n            self._base *= 2\n        else:\n            self._base = self._max\n        return value", "idx": 1561}
{"namespace": "praw.models.listing.generator.ListingGenerator._extract_sublist", "completion": "        if isinstance(listing, list):\n            return listing[1]\n        elif isinstance(listing, dict):\n            if \"t2\" in listing:\n                return listing[\"t2\"]\n            elif \"t1\" in listing:\n                return listing[\"t1\"]\n            elif \"t3\" in listing:\n                return listing[\"t3\"]\n            elif \"t4\" in listing:\n                return listing[\"t4\"]\n            elif \"t5\" in listing:\n                return listing[\"t5\"]\n            elif \"t6\" in listing:\n                return listing[\"t6\"]\n            elif \"t8\" in listing:\n                return listing[\"t8\"]\n            elif \"t9\" in listing:\n                return listing[\"t9\"]\n            elif \"t10\" in listing:\n                return listing[\"t10\"]\n            elif \"t11\" in listing:\n                return listing[\"t11\"]\n            elif \"t12\" in listing:\n                return listing[\"t12\"]\n            elif \"t13\" in listing:\n                return listing[\"t13\"]\n            elif \"t14\" in listing:\n                return listing[\"t14\"]\n            elif \"t15\" in listing:\n                return listing[\"t15\"]\n            elif \"t16\" in listing:\n                return listing[\"t16\"]\n            elif \"t17\" in listing:\n                return listing[\"t17\"]\n            elif \"t18\" in listing:\n                return listing[\"t18\"]\n            elif \"t19\" in listing:\n                return listing[\"t19\"]\n            elif \"t20\" in listing:\n                return listing[\"t20\"]\n            elif \"t21\" in listing:\n                return listing[\"t21\"]\n            elif \"t22\" in listing:\n                return listing[\"t22\"]\n            elif \"t23\" in listing:\n                return listing[\"t23\"]\n            elif \"t24\" in listing:\n                return listing[\"t24\"]\n            elif \"t25\" in listing:\n                return listing[\"t25\"]\n            elif \"t26", "idx": 1562}
{"namespace": "praw.util.token_manager.FileTokenManager.post_refresh_callback", "completion": "        with open(self._filename, \"w\") as fp:\n            fp.write(authorizer.refresh_token)\n", "idx": 1563}
{"namespace": "praw.util.token_manager.FileTokenManager.pre_refresh_callback", "completion": "        if authorizer.refresh_token is None:\n            with open(self._filename) as fp:\n                authorizer.refresh_token = fp.read().strip()", "idx": 1564}
{"namespace": "praw.util.token_manager.SQLiteTokenManager._get", "completion": "        cursor = self._connection.execute(\n            \"SELECT refresh_token FROM tokens WHERE id = ?\", (self.key,)\n        )\n        row = cursor.fetchone()\n        if row is None:\n            raise KeyError(self.key)\n        return row[0]\n", "idx": 1565}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.is_registered", "completion": "        cursor = self._connection.execute(\n            \"SELECT refresh_token FROM tokens WHERE id=?\", (self.key,)\n        )\n        result = cursor.fetchone()\n        if result is None:\n            return False\n        return True\n", "idx": 1566}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.post_refresh_callback", "completion": "        self._set(authorizer.refresh_token)\n        authorizer.refresh_token = None\n", "idx": 1567}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.pre_refresh_callback", "completion": "        authorizer.refresh_token = self._get()", "idx": 1568}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.register", "completion": "        try:\n            self._get()\n            return False\n        except KeyError:\n            self._set(refresh_token)\n            return True", "idx": 1569}
{"namespace": "jc.cli.JcCli.about_jc", "completion": "        return {\n            'jc_lib_name': info.description,\n            'jc_lib_version': info.version,\n            'jc_lib_description': info.description,\n            'jc_lib_author': info.author,\n            'jc_lib_author_email': info.author_email,\n            'jc_lib_website': info.website,\n            'jc_lib_copyright': info.copyright,\n            'jc_lib_license': info.license,\n            'python_version': sys.version,\n            'python_path': sys.executable,\n            'parser_count': len(parsers),\n            'standard_parser_count': len(all_parser_info(show_hidden=False, show_deprecated=False)),\n            'streaming_parser_count': len(all_parser_info(show_hidden=False, show_deprecated=False, streaming_only=True)),\n            'plugin_parser_count': len(all_parser_info(show_hidden=False, show_deprecated=False, plugin_only=True)),\n            'all_parser_info': all_parser_info(show_hidden=False, show_deprecated=False)\n        }\n", "idx": 1570}
{"namespace": "jc.cli.JcCli.yaml_out", "completion": "        try:\n            import ruamel.yaml\n            from ruamel.yaml.representer import RoundTripRepresenter\n            from ruamel.yaml.scalarstring import LiteralScalarString\n            from ruamel.yaml.scalarstring import PreservedScalarString\n            from ruamel.yaml.scalarstring import SingleQuotedScalarString\n            from ruamel.yaml.scalarstring import DoubleQuotedScalarString\n            from ruamel.yaml.scalarstring import FoldedScalarString\n            from ruamel.yaml.scalarstring import PlainScalarString\n            from ruamel.yaml.scalarstring import SingleQuotedScalarString\n            from ruamel.yaml.scalarstring import DoubleQuotedScalarString\n            from ruamel.yaml.scalarstring import FoldedScalarString\n            from ruamel.yaml.scalarstring import PlainScalarString\n            from ruamel.yaml.scalarstring import SingleQuotedScalarString\n            from ruamel.yaml.scalarstring import DoubleQuotedScalarString\n            from ruamel.yaml.scalarstring import FoldedScalarString\n            from ruamel.yaml.scalarstring import PlainScalarString\n            from ruamel.yaml.scalarstring import SingleQuotedScalarString\n            from ruamel.yaml.scalarstring import DoubleQuotedScalarString\n            from ruamel.yaml.scalarstring import FoldedScalarString\n            from ruamel.yaml.scalarstring import PlainScalarString\n            from ruamel.yaml.scalarstring import SingleQuotedScalarString\n            from ruamel.yaml.scalarstring import DoubleQuotedScalarString\n            from ruamel.yaml.scalarstring import FoldedScalarString\n            from ruamel.yaml.scalarstring import PlainScalarString\n            from ruamel.yaml.scalarstring import SingleQuotedScalarString\n            from ruamel.yaml.scalarstring import DoubleQuotedScalarString\n            from ruamel.yaml.scalarstring import FoldedScalarString\n           ", "idx": 1571}
{"namespace": "jc.parsers.os_release.parse", "completion": "    jc.utils.compatibility(__name__, info.compatible, quiet)\n    jc.utils.input_type_check(data)\n\n    raw_output: JSONDictType = {}\n\n    if jc.utils.has_data(data):\n\n        for line in filter(None, data.splitlines()):\n            key, val = line.split('=', maxsplit=1)\n            key = key.strip()\n            val = val.strip().strip('\"')\n            raw_output[key] = val\n\n    return raw_output if raw else _process(raw_output)", "idx": 1572}
{"namespace": "jc.parsers.xrandr._parse_screen", "completion": "    if not next_lines:\n        return None\n\n    line = next_lines.pop(0)\n    screen_match = re.match(_screen_pattern, line)\n    if not screen_match:\n        next_lines.insert(0, line)\n        return None\n\n    screen = screen_match.groupdict()\n    screen[\"devices\"] = []\n\n    while next_lines:\n        device = _parse_device(next_lines)\n        if device:\n            screen[\"devices\"].append(device)\n        else:\n            break\n\n    return screen\n\n", "idx": 1573}
{"namespace": "jc.parsers.xrandr._parse_model", "completion": "    if not next_lines:\n        return None\n\n    next_line = next_lines.pop()\n    if not re.match(_edid_head_pattern, next_line):\n        next_lines.append(next_line)\n        return None\n\n    edid_lines: List[str] = []\n    while next_lines:\n        next_line = next_lines.pop()\n        result = re.match(_edid_line_pattern, next_line)\n        if not result:\n            next_lines.append(next_line)\n            break\n        edid_lines.append(result.groupdict()[\"edid_line\"])\n\n    edid_bytes = _hex_to_bytes(edid_lines, quiet)\n    if not edid_bytes:\n        return None\n\n    model: Model = {\n        \"name\": \"\",\n        \"product_id\": \"\",\n        \"serial_number\": \"\",\n    }\n    try:\n        from pyedidlib.parser import EDIDParser\n\n        parser = EDIDParser(edid_bytes)\n        model[\"name\"] = parser.get_name()\n        model[\"product_id\"] = parser.get_product_id()\n        model[\"serial_number\"] = parser.get_serial_number()\n    except ImportError:\n        if not quiet:\n            jc.utils.warning_message(\n                [\n                    \"pyedidlib is not installed. \"\n                    + \"Please install it to get the model name, \"\n                    + \"product id and serial number.\"\n                ]\n            )\n\n    return model\n\n", "idx": 1574}
{"namespace": "jc.parsers.xrandr._parse_mode", "completion": "    result = re.match(_mode_pattern, line)\n    if not result:\n        return None\n\n    matches = result.groupdict()\n\n    mode: Mode = {\n        \"resolution_width\": int(matches[\"resolution_width\"]),\n        \"resolution_height\": int(matches[\"resolution_height\"]),\n        \"is_high_resolution\": matches[\"is_high_resolution\"] is not None\n        and len(matches[\"is_high_resolution\"]) > 0,\n        \"frequencies\": [],\n    }\n\n    frequencies_result = re.match(_frequencies_pattern, matches[\"rest\"])\n    if not frequencies_result:\n        return None\n\n    frequencies_matches = frequencies_result.groupdict()\n    frequencies_str = frequencies_matches[\"frequency\"]\n    frequencies = frequencies_str.split(\" \")\n    for frequency in frequencies:\n        if not frequency:\n            continue\n        mode[\"frequencies\"].append(\n            {\n                \"frequency\": float(frequency),\n                \"is_current\": frequency.endswith(\"*\"),\n                \"is_preferred\": frequency.endswith(\"+\"),\n            }\n        )\n\n    return mode\n\n", "idx": 1575}
{"namespace": "pythonforandroid.archs.Arch.include_dirs", "completion": "        return [\n            join(self.ctx.ndk.sysroot_include_dir, self.command_prefix),\n            join(self.ctx.ndk.sysroot_include_dir, 'arch-{arch}'.format(arch=self.arch)),\n        ]\n", "idx": 1576}
{"namespace": "pythonforandroid.archs.Arch.target", "completion": "        return \"{}-linux-android{}\".format(\n            self.command_prefix,\n            self.ctx.ndk_api)\n", "idx": 1577}
{"namespace": "pythonforandroid.archs.ArchARM.target", "completion": "        return '{triplet}{ndk_api}'.format(\n            triplet=self.command_prefix, ndk_api=self.ctx.ndk_api\n        )\n\n", "idx": 1578}
{"namespace": "pythonforandroid.recipe.Recipe.get_recipe", "completion": "        if name in cls.recipes:\n            return cls.recipes[name]\n        for recipes_dir in cls.recipe_dirs(ctx):\n            if recipes_dir and exists(recipes_dir):\n                for name in listdir(recipes_dir):\n                    if name in ('__pycache__', ):\n                        continue\n                    fn = join(recipes_dir, name)\n                    if isdir(fn):\n                        if name == 'pythonforandroid':\n                            continue\n                        try:\n                            mod = __import__(\n                                'pythonforandroid.recipes.{}'.format(name),\n                                fromlist=['Recipe'])\n                            cls.recipes[name] = mod.Recipe()\n                            return cls.recipes[name]\n                        except ImportError:\n                            pass\n        return None", "idx": 1579}
{"namespace": "pythonforandroid.prerequisites.HomebrewPrerequisite.darwin_helper", "completion": "        info(\n            \"Please install Homebrew by following the instructions at https://brew.sh/\"\n        )", "idx": 1580}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_checker", "completion": "        return self._darwin_get_brew_formula_location_prefix(\n            self.homebrew_formula_name, installed=True\n        ) is not None\n", "idx": 1581}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_pkg_config_location", "completion": "        return os.path.join(\n            self._darwin_get_brew_formula_location_prefix(\n                self.homebrew_formula_name, installed=True\n            ),\n            \"lib/pkgconfig\",\n        )", "idx": 1582}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_installer", "completion": "        # Check if Homebrew is installed\n        if not shutil.which(\"brew\"):\n            error(\"Homebrew is not installed. Please install Homebrew and try again.\")\n            return\n\n        # Check if OpenSSL is already installed\n        if self.darwin_checker():\n            info(\"OpenSSL is already installed.\")\n            return\n\n        # Install OpenSSL using Homebrew\n        info(\"Installing OpenSSL using Homebrew...\")\n        subprocess.run([\"brew\", \"install\", self.homebrew_formula_name])\n\n        # Check if OpenSSL was installed successfully\n        if not self.darwin_checker():\n            error(\"Failed to install OpenSSL using Homebrew.\")\n            return\n\n        info(\"OpenSSL was installed successfully using Homebrew.\")", "idx": 1583}
{"namespace": "pythonforandroid.prerequisites.AutoconfPrerequisite.darwin_installer", "completion": "        info(\"Installing Autoconf ...\")\n        subprocess.check_output([\"brew\", \"install\", \"autoconf\"])", "idx": 1584}
{"namespace": "pythonforandroid.prerequisites.AutomakePrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"automake\", installed=True)\n            is not None\n        )\n", "idx": 1585}
{"namespace": "pythonforandroid.prerequisites.AutomakePrerequisite.darwin_installer", "completion": "        info(\"Installing Automake ...\")\n        subprocess.check_output([\"brew\", \"install\", \"automake\"])", "idx": 1586}
{"namespace": "pythonforandroid.prerequisites.LibtoolPrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"libtool\", installed=True)\n            is not None\n        )\n", "idx": 1587}
{"namespace": "pythonforandroid.prerequisites.LibtoolPrerequisite.darwin_installer", "completion": "        info(\"Installing Libtool ...\")\n        subprocess.check_output([\"brew\", \"install\", \"libtool\"])", "idx": 1588}
{"namespace": "pythonforandroid.prerequisites.PkgConfigPrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"pkg-config\", installed=True)\n            is not None\n        )\n", "idx": 1589}
{"namespace": "pythonforandroid.prerequisites.PkgConfigPrerequisite.darwin_installer", "completion": "        info(\"Installing Pkg-Config ...\")\n        subprocess.check_output([\"brew\", \"install\", \"pkg-config\"])", "idx": 1590}
{"namespace": "pythonforandroid.prerequisites.CmakePrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"cmake\", installed=True)\n            is not None\n        )\n", "idx": 1591}
{"namespace": "pythonforandroid.prerequisites.CmakePrerequisite.darwin_installer", "completion": "        info(\"Installing CMake ...\")\n        subprocess.check_output([\"brew\", \"install\", \"cmake\"])", "idx": 1592}
{"namespace": "pythonforandroid.prerequisites.get_required_prerequisites", "completion": "    prerequisites = [\n        HomebrewPrerequisite,\n        JDKPrerequisite,\n        OpenSSLPrerequisite,\n        AutoconfPrerequisite,\n        AutomakePrerequisite,\n        LibtoolPrerequisite,\n        PkgConfigPrerequisite,\n        CmakePrerequisite,\n    ]\n\n    if platform == \"darwin\":\n        return [prerequisite() for prerequisite in prerequisites]\n    else:\n        return [\n            prerequisite()\n            for prerequisite in prerequisites\n            if not prerequisite.mandatory[platform]\n        ]", "idx": 1593}
{"namespace": "pythonforandroid.pythonpackage.parse_as_folder_reference", "completion": "    # Check if the dependency reference is a file:// URL\n    if dep.startswith(\"file://\"):\n        # Parse the URL and get the path\n        url = urlparse(dep)\n        path = urlunquote(url.path)\n        # Check if the path is absolute\n        if os.path.isabs(path):\n            # Return the path\n            return path\n        else:\n            # Return the absolute path\n            return os.path.abspath(path)\n    else:\n        # Return None\n        return None", "idx": 1594}
{"namespace": "pythonforandroid.pythonpackage.get_package_name", "completion": "    if use_cache and dependency in package_name_cache:\n        # Check if cache is still valid:\n        if package_name_cache[dependency][1] > time.time():\n            return package_name_cache[dependency][0]\n\n    # Extract package name:\n    package_name = _extract_info_from_package(dependency, \"name\")\n\n    # Update cache:\n    package_name_cache[dependency] = (package_name, time.time() + 60)\n\n    return package_name", "idx": 1595}
{"namespace": "pythonforandroid.recommendations.read_ndk_version", "completion": "    source_properties_file = join(ndk_dir, 'source.properties')\n    try:\n        with open(source_properties_file) as fh:\n            for line in fh:\n                if line.startswith('Pkg.Revision'):\n                    version = line.split('=')[1].strip()\n                    return LooseVersion(version)\n    except IOError:\n        warning(PARSE_ERROR_NDK_MESSAGE)\n        warning(ENSURE_RIGHT_NDK_MESSAGE.format(\n            min_supported=MIN_NDK_VERSION,\n            rec_version=RECOMMENDED_NDK_VERSION,\n            ndk_url=NDK_DOWNLOAD_URL,\n        ))\n        return\n    except Exception:\n        warning(UNKNOWN_NDK_MESSAGE)\n        warning(ENSURE_RIGHT_NDK_MESSAGE.format(\n            min_supported=MIN_NDK_VERSION,\n            rec_version=RECOMMENDED_NDK_VERSION,\n            ndk_url=NDK_DOWNLOAD_URL,\n        ))\n        return", "idx": 1596}
{"namespace": "pythonforandroid.recommendations.check_target_api", "completion": "    if api < MIN_TARGET_API:\n        warning(OLD_API_MESSAGE)\n    if arch == 'armeabi' and api > ARMEABI_MAX_TARGET_API:\n        raise BuildInterruptingException(\n            UNSUPPORTED_NDK_API_FOR_ARMEABI_MESSAGE.format(\n                req_ndk_api=api, max_ndk_api=ARMEABI_MAX_TARGET_API\n            )\n        )", "idx": 1597}
{"namespace": "pythonforandroid.recommendations.check_ndk_api", "completion": "    if ndk_api > android_api:\n        raise BuildInterruptingException(\n            TARGET_NDK_API_GREATER_THAN_TARGET_API_MESSAGE.format(\n                ndk_api=ndk_api, android_api=android_api\n            ),\n            instructions='You probably want to build with --ndk-api={} instead'.format(\n                android_api\n            ),\n        )\n\n    if ndk_api < MIN_NDK_API:\n        warning(OLD_NDK_API_MESSAGE)\n        warning(\n            'Minimum supported NDK API is {}, but newer versions may work.'.format(\n                RECOMMENDED_NDK_API\n            )\n        )", "idx": 1598}
{"namespace": "pythonforandroid.androidndk.AndroidNDK.llvm_prebuilt_dir", "completion": "        return os.path.join(\n            self.ndk_dir, \"toolchains\", \"llvm\", \"prebuilt\", self.host_tag\n        )\n", "idx": 1599}
{"namespace": "pythonforandroid.build.Context.setup_dirs", "completion": "        self.storage_dir = storage_dir\n        self.build_dir = join(self.storage_dir, 'build')\n        self.dist_dir = join(self.storage_dir, 'dist')\n        ensure_dir(self.storage_dir)\n        ensure_dir(self.build_dir)\n        ensure_dir(self.dist_dir)\n", "idx": 1600}
{"namespace": "pythonforandroid.graph.get_dependency_tuple_list_for_recipe", "completion": "    if blacklist is None:\n        blacklist = set()\n\n    # Get the dependencies of the recipe\n    deps = recipe.depends\n\n    # Filter out blacklisted items and turn lowercase\n    deps = [dep for dep in deps if dep.lower() not in blacklist]\n\n    # Convert the dependencies into tuples\n    deps = [(dep,) if isinstance(dep, str) else tuple(dep) for dep in deps]\n\n    return deps", "idx": 1601}
{"namespace": "pythonforandroid.graph.obvious_conflict_checker", "completion": "    if blacklist is None:\n        blacklist = set()\n    assert type(blacklist) is set\n\n    # Add dependencies for all recipes\n    for name_tuple in name_tuples:\n        for name in name_tuple:\n            try:\n                recipe = Recipe.get_recipe(name, ctx)\n                dependencies = get_dependency_tuple_list_for_recipe(\n                    recipe, blacklist=blacklist\n                )\n                recipe.depends = dependencies\n            except ValueError:\n                # The recipe does not exist, so we assume it can be installed\n                # via pip with no extra dependencies\n                recipe.depends = []\n\n    # Check for obvious conflicts\n    for name_tuple in name_tuples:\n        for name in name_tuple:\n            try:\n                recipe = Recipe.get_recipe(name, ctx)\n                conflicts = [dep.lower() for dep in recipe.conflicts]\n            except ValueError:\n                conflicts = []\n\n            if any([c in blacklist for c in conflicts]):\n                raise BuildInterruptingException(\n                    \"Conflict detected: {} conflicts with {}\".format(\n                        name, blacklist.intersection(conflicts)\n                    )\n                )\n\n            if any([c in blacklist for c in recipe.depends]):\n                raise BuildInterruptingException(\n                    \"Conflict detected: {} conflicts with {}\".format(\n                        name, blacklist.intersection(recipe.depends)\n                    )\n                )\n\n            if any([c in blacklist for c in recipe.opt_depends]):\n                raise BuildInterruptingException(\n                    \"Conflict detected: {} conflicts with {}\".format(\n                        name, blacklist.intersection(recipe.opt_depends)\n                    )\n                )\n\n            if any([c in blacklist for c in recipe.python_depends]):\n                raise BuildInterruptingException(\n                    \"Conflict detected: {} conflicts with {}\".format(\n                        name, blacklist.intersection(recipe.python_depends)\n                    )\n                )\n\n            if", "idx": 1602}
{"namespace": "pythonforandroid.graph.get_recipe_order_and_bootstrap", "completion": "    if blacklist is None:\n        blacklist = set()\n    assert type(blacklist) is set\n    names = [name.lower() for name in names]\n    names = set(names) - blacklist\n    if bs is not None:\n        names.update(bs.get_bootstrap_dependencies())\n\n    # Check for obvious conflicts:\n    obvious_conflict_checker(ctx, names, blacklist=blacklist)\n\n    # Get all possible orders:\n    orders = recursively_collect_orders(\n        'python2', ctx, names, orders=[RecipeOrder(ctx)],\n        blacklist=blacklist\n    )\n\n    # Convert each order graph into a linear list:\n    orders = [list(find_order(order)) for order in orders]\n\n    # Sort the orders based on preference:\n    orders.sort(key=lambda order: (\n        # Prefer the order that has the most recipes:\n        -len(order),\n        # Prefer the order that has the most dependencies:\n        -sum([len(order[name]) for name in order]),\n        # Prefer the order that has the most dependencies that are\n        # already in the list of recipes to build:\n        -sum([len(set(order[name]) & names) for name in order]),\n        # Prefer the order that has the most dependencies that are\n        # already in the list of recipes to build, and are also\n        # dependencies of the bootstrap instance:\n        -sum([len(set(order[name]) & names) for name in order\n              if bs is not None and name in bs.get_bootstrap_dependencies()]),\n        # Prefer the order that has the most dependencies that are\n        # already in the list of recipes to build, and are also\n        # dependencies of the bootstrap instance, and are also\n        # dependencies of the bootstrap instance's dependencies:\n        -sum([len(set(order[name]) & names) for name in order\n              if bs is not None and name in bs.get_bootstrap_dependencies()\n              and any([dep in bs.get_bootstrap_dependencies()", "idx": 1603}
{"namespace": "pythonforandroid.util.ensure_dir", "completion": "    if not exists(dn):\n        LOGGER.debug(\"Creating directory {}\".format(dn))\n        makedirs(dn)", "idx": 1604}
{"namespace": "pythonforandroid.util.move", "completion": "    LOGGER.debug(\"Move {} to {}\".format(source, destination))\n    shutil.move(source, destination)", "idx": 1605}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.get_bootstrap_from_recipes", "completion": "        # Get all bootstraps that can be chosen automatically:\n        acceptable_bootstraps = cls.get_usable_bootstraps_for_recipes(recipes, ctx)\n\n        # If there are no acceptable bootstraps, raise an exception:\n        if not acceptable_bootstraps:\n            raise ValueError(\n                'No bootstraps found that can be chosen automatically.')\n\n        # If there's only one acceptable bootstrap, return it:\n        if len(acceptable_bootstraps) == 1:\n            return acceptable_bootstraps.pop()\n\n        # If there are multiple acceptable bootstraps, select the one with the highest priority:\n        bootstraps_by_priority = sorted(\n            acceptable_bootstraps, key=_cmp_bootstraps_by_priority)\n        return bootstraps_by_priority[-1]\n", "idx": 1606}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.get_bootstrap", "completion": "        bootstrap_dir = join(ctx.root_dir, 'bootstraps', name)\n        if not os.path.exists(bootstrap_dir):\n            raise ValueError('Bootstrap {} does not exist'.format(name))\n        bootstrap_class_name = 'Bootstrap' + name.capitalize()\n        bootstrap_module = importlib.import_module(\n            'pythonforandroid.bootstraps.' + name)\n        bootstrap_class = getattr(bootstrap_module, bootstrap_class_name)\n        bootstrap = bootstrap_class(ctx)\n        bootstrap.bootstrap_dir = bootstrap_dir\n        return bootstrap\n", "idx": 1607}
{"namespace": "pythonforandroid.bootstrap.expand_dependencies", "completion": "    # Get all recipes:\n    all_recipes = Recipe.get_recipe_names()\n\n    # Get all alternatives:\n    all_alternatives = Recipe.get_alternatives()\n\n    # Get all dependencies:\n    all_dependencies = Recipe.get_dependencies()\n\n    # Get all conflicts:\n    all_conflicts = Recipe.get_conflicts()\n\n    # Get all requirements:\n    all_requirements = Recipe.get_requirements()\n\n    # Get all bootstraps:\n    all_bootstraps = Bootstrap.all_bootstraps()\n\n    # Get all bootstraps:\n    all_bootstraps = Bootstrap.all_bootstraps()\n\n    # Get all bootstraps:\n    all_bootstraps = Bootstrap.all_bootstraps()\n\n    # Get all bootstraps:\n    all_bootstraps = Bootstrap.all_bootstraps()\n\n    # Get all bootstraps:\n    all_bootstraps = Bootstrap.all_bootstraps()\n\n    # Get all bootstraps:\n    all_bootstraps = Bootstrap.all_bootstraps()\n\n    # Get all bootstraps:\n    all_bootstraps = Bootstrap.all_bootstraps()\n\n    # Get all bootstraps:\n    all_bootstraps = Bootstrap.all_bootstraps()\n\n    # Get all bootstraps:\n    all_bootstraps = Bootstrap.all_bootstraps()\n\n    # Get all bootstraps:\n    all_bootstraps = Bootstrap.all_bootstraps()\n\n    # Get all bootstraps:\n    all_bootstraps = Bootstrap.all_bootstraps()\n\n    # Get all bootstraps:\n    all_bootstraps = Bootstrap.all_bootstraps()\n\n    # Get all bootstraps:\n    all_bootstraps = Bootstrap.all_bootstraps()\n\n    # Get all bootstraps:\n    all_bootstraps = Bootstrap.all_bootstraps()\n\n    # Get all bootstraps:\n    all_bootstraps = Bootstrap.all_bootstraps()\n\n    # Get all", "idx": 1608}
{"namespace": "pythonforandroid.recipes.icu.ICURecipe.get_recipe_dir", "completion": "        if self.local_recipes:\n            recipe_dir = join(self.local_recipes, self.name)\n            if isdir(recipe_dir):\n                return recipe_dir\n        return join(self.ctx.root_dir, 'recipes', self.name)\n", "idx": 1609}
{"namespace": "mmcv.transforms.wrappers.KeyMapper.__repr__", "completion": "        return (f'{self.__class__.__name__}('\n                f'transforms = {self.transforms}, '\n                f'mapping = {self.mapping}, '\n                f'remapping = {self.remapping}, '\n                f'auto_remap = {self.auto_remap}, '\n                f'allow_nonexist_keys = {self.allow_nonexist_keys})')\n\n", "idx": 1610}
{"namespace": "mmcv.transforms.wrappers.TransformBroadcaster.__repr__", "completion": "        repr_str = self.__class__.__name__\n        repr_str += f'(transforms = {self.transforms}'\n        repr_str += f', mapping = {self.mapping}'\n        repr_str += f', remapping = {self.remapping}'\n        repr_str += f', auto_remap = {self.auto_remap}'\n        repr_str += f', allow_nonexist_keys = {self.allow_nonexist_keys}'\n        repr_str += f', share_random_params = {self.share_random_params})'\n        return repr_str\n\n", "idx": 1611}
{"namespace": "mackup.utils.delete", "completion": "    if os.path.islink(filepath):\n        os.remove(filepath)\n    elif os.path.isfile(filepath):\n        os.remove(filepath)\n    elif os.path.isdir(filepath):\n        shutil.rmtree(filepath)\n\n", "idx": 1612}
{"namespace": "mackup.utils.copy", "completion": "    # Check if the source and destination paths are valid and absolute paths\n    if not os.path.isabs(src):\n        raise ValueError(\"Source path must be an absolute path\")\n    if not os.path.isabs(dst):\n        raise ValueError(\"Destination path must be an absolute path\")\n\n    # Create the necessary directories in the destination path\n    os.makedirs(os.path.dirname(dst), exist_ok=True)\n\n    # Copy the file or folder\n    if os.path.isfile(src):\n        shutil.copy(src, dst)\n    elif os.path.isdir(src):\n        shutil.copytree(src, dst)\n    else:\n        raise ValueError(\"Source path must be a file or a folder\")\n\n    # Set the appropriate file permissions for the copied file or folder\n    if os.path.isfile(dst):\n        os.chmod(dst, stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH)\n    elif os.path.isdir(dst):\n        for root, dirs, files in os.walk(dst):\n            for d in dirs:\n                os.chmod(os.path.join(root, d), stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO)\n            for f in files:\n                os.chmod(os.path.join(root, f), stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO)\n\n", "idx": 1613}
{"namespace": "mackup.utils.get_dropbox_folder_location", "completion": "    # Get the path to the Dropbox host.db file\n    dropbox_host_db_path = os.path.join(os.path.expanduser(\"~\"), \".dropbox\", \"host.db\")\n\n    # Check if the file exists\n    if not os.path.exists(dropbox_host_db_path):\n        error(\"Dropbox folder not found.\")\n\n    # Connect to the host.db file\n    connection = sqlite3.connect(dropbox_host_db_path)\n    cursor = connection.cursor()\n\n    # Get the Dropbox home path\n    cursor.execute(\"SELECT value FROM config WHERE key = 'user'\")\n    dropbox_home_path = cursor.fetchone()[0]\n\n    # Decode the Dropbox home path\n    dropbox_home_path = base64.b64decode(dropbox_home_path).decode(\"utf-8\")\n\n    # Close the connection\n    connection.close()\n\n    # Return the Dropbox home path\n    return dropbox_home_path", "idx": 1614}
{"namespace": "mackup.utils.get_copy_folder_location", "completion": "    copy_db_path = \"Library/Application Support/CopyAgent/settings.db\"\n    yosemite_copy_db_path = (\n        \"Library/Application Support/CopyAgent/\" \"user_default/settings.db\"\n    )\n    yosemite_copy_db = os.path.join(os.environ[\"HOME\"], yosemite_copy_db_path)\n    if os.path.isfile(yosemite_copy_db):\n        copy_db_path = yosemite_copy_db\n\n    copy_home = None\n\n    copy_db = os.path.join(os.environ[\"HOME\"], copy_db_path)\n    if os.path.isfile(copy_db):\n        con = sqlite3.connect(copy_db)\n        if con:\n            cur = con.cursor()\n            query = (\n                \"SELECT value \"\n                \"FROM options \"\n                \"WHERE option = 'csmRootPath';\"\n            )\n            cur.execute(query)\n            data = cur.fetchone()\n            copy_home = str(data[0])\n            con.close()\n\n    if not copy_home:\n        error(\n            constants.ERROR_UNABLE_TO_FIND_STORAGE.format(\n                provider=\"Copy install\"\n            )\n        )\n\n    return copy_home", "idx": 1615}
{"namespace": "mackup.utils.can_file_be_synced_on_current_platform", "completion": "    # If the path is a symlink, check if it's a broken link\n    if os.path.islink(path):\n        if not os.path.exists(path):\n            return False\n\n    # If the path is a directory, check if it's a virtualenv\n    if os.path.isdir(path):\n        if os.path.basename(path) == \"bin\":\n            return False\n\n    # If the path is a file, check if it's a binary file\n    if os.path.isfile(path):\n        if not os.access(path, os.X_OK):\n            return False\n\n    # If the path is a file, check if it's a binary file\n    if os.path.isfile(path):\n        if not os.access(path, os.X_OK):\n            return False\n\n    # If the path is a file, check if it's a binary file\n    if os.path.isfile(path):\n        if not os.access(path, os.X_OK):\n            return False\n\n    # If the path is a file, check if it's a binary file\n    if os.path.isfile(path):\n        if not os.access(path, os.X_OK):\n            return False\n\n    # If the path is a file, check if it's a binary file\n    if os.path.isfile(path):\n        if not os.access(path, os.X_OK):\n            return False\n\n    # If the path is a file, check if it's a binary file\n    if os.path.isfile(path):\n        if not os.access(path, os.X_OK):\n            return False\n\n    # If the path is a file, check if it's a binary file\n    if os.path.isfile(path):\n        if not os.access(path, os.X_OK):\n            return False\n\n    # If the path is a file, check if it's a binary file\n    if os.path.isfile(path):\n        if not os.access(path, os.X_", "idx": 1616}
{"namespace": "hl7.client.MLLPClient.send_message", "completion": "        if isinstance(message, hl7.Message):\n            message = message.to_er7()\n        if isinstance(message, str):\n            message = message.encode(self.encoding)\n        if not isinstance(message, bytes):\n            raise TypeError(\"message must be a str, unicode, or hl7.Message\")\n        if message.startswith(SB):\n            message = message[1:]\n        if not message.endswith(CR):\n            message += CR\n        self.send(SB + message + EB + CR)\n        return self.receive()\n", "idx": 1617}
{"namespace": "hl7.client.MLLPClient.send", "completion": "        self.socket.sendall(data)\n        response = self.socket.recv(RECV_BUFFER)\n        return response", "idx": 1618}
{"namespace": "hl7.datatypes._UTCOffset.tzname", "completion": "        return \"{0}{1:02d}{2:02d}\".format(\n            \"+\" if self.minutes > 0 else \"-\",\n            abs(self.minutes) // 60,\n            abs(self.minutes) % 60,\n        )\n", "idx": 1619}
{"namespace": "hl7.datatypes.parse_datetime", "completion": "    if not value:\n        return None\n\n    # Parse the date and time components of the input string\n    match = DTM_TZ_RE.match(value)\n    if not match:\n        raise ValueError(\"Invalid DTM format\")\n\n    # Extract the date and time components from the match object\n    date_time = match.group(1)\n    date_time = date_time.split(\".\")\n    date_time = [int(x) for x in date_time]\n\n    # Extract the timezone offset from the match object\n    tz_offset = match.group(2)\n    if tz_offset:\n        tz_offset = int(tz_offset)\n    else:\n        tz_offset = 0\n\n    # Create a datetime object with the parsed date and time components\n    dt = datetime.datetime(*date_time)\n\n    # Set the timezone offset of the datetime object\n    dt = dt.replace(tzinfo=_UTCOffset(tz_offset))\n\n    return dt\n\n", "idx": 1620}
{"namespace": "hl7.parser._ParsePlan.container", "completion": "        return self.containers[0](\n            sequence=data, esc=self.esc, separators=self.separators, factory=self.factory\n        )\n", "idx": 1621}
{"namespace": "hl7.parser._ParsePlan.next", "completion": "        return _ParsePlan(\n            self.separators[self.separators.find(self.separator) + 1],\n            self.separators,\n            self.containers[1:],\n            self.esc,\n            self.factory,\n        )", "idx": 1622}
{"namespace": "hl7.version.get_version", "completion": "    if len(VERSION) > 4 or VERSION[3] != \"final\":\n        if VERSION[3] == \"dev\":\n            return \"{}.dev\".format(get_main_version())\n        else:\n            return \"{} {}\".format(get_main_version(), VERSION[3])\n    else:\n        return get_main_version()\n\n", "idx": 1623}
{"namespace": "twtxt.config.Config.from_file", "completion": "        if not os.path.exists(file):\n            raise ValueError(\"Config file %s does not exist.\" % file)\n\n        cfg = configparser.ConfigParser()\n        cfg.read(file)\n\n        return cls(file, cfg)\n", "idx": 1624}
{"namespace": "twtxt.config.Config.discover", "completion": "        file = os.path.join(cls.config_dir, cls.config_name)\n        return cls.from_file(file)\n", "idx": 1625}
{"namespace": "twtxt.config.Config.create_config", "completion": "        config_parser = configparser.ConfigParser()\n        config_parser.add_section(\"twtxt\")\n        config_parser.set(\"twtxt\", \"nick\", nick)\n        config_parser.set(\"twtxt\", \"twtfile\", twtfile)\n        config_parser.set(\"twtxt\", \"twturl\", twturl)\n        config_parser.set(\"twtxt\", \"disclose_identity\", str(disclose_identity))\n        config_parser.add_section(\"following\")\n        config_parser.set(\"following\", \"twtxtter\", \"https://buckket.org/twtxt.txt\")\n        if add_news:\n            config_parser.set(\"following\", \"twtxtnews\", \"https://twtxt.readthedocs.io/en/latest/news.txt\")\n        with open(cfgfile, \"w\") as config_file:\n            config_parser.write(config_file)\n        return cls.from_file(cfgfile)\n", "idx": 1626}
{"namespace": "twtxt.config.Config.following", "completion": "        following = []\n        if \"following\" in self.cfg.sections():\n            for nick, url in self.cfg.items(\"following\"):\n                source = Source(nick, url)\n                following.append(source)\n        else:\n            logger.debug(\"No sources in `following` section of the config.\")\n        return following\n", "idx": 1627}
{"namespace": "twtxt.config.Config.options", "completion": "        try:\n            return dict(self.cfg.items(\"twtxt\"))\n        except configparser.NoSectionError as e:\n            logger.debug(e)\n            return {}\n", "idx": 1628}
{"namespace": "twtxt.models.Tweet.relative_datetime", "completion": "        now = datetime.now(timezone.utc)\n        delta = now - self.created_at\n        tense = \"ago\"\n        if delta.days < 0:\n            tense = \"from now\"\n        return humanize.naturaldelta(delta) + \" \" + tense\n\n", "idx": 1629}
{"namespace": "twtxt.mentions.format_mentions", "completion": "    def handle_mention(match):\n        name = match.group(1)\n        url = match.group(2)\n        return format_callback(name, url)\n\n    return mention_re.sub(handle_mention, text)", "idx": 1630}
{"namespace": "twtxt.parser.parse_tweets", "completion": "    tweets = []\n    now = now or datetime.now(timezone.utc)\n    for line in raw_tweets:\n        try:\n            tweet = parse_tweet(line, source, now)\n        except (ValueError, OverflowError) as e:\n            logger.debug(e)\n            continue\n        tweets.append(tweet)\n    return tweets\n\n", "idx": 1631}
{"namespace": "wikipediaapi.Wikipedia.page", "completion": "        if unquote:\n            title = parse.unquote(title)\n\n        return WikipediaPage(\n            self,\n            title,\n            namespace2int(ns),\n        )\n", "idx": 1632}
{"namespace": "wikipediaapi.Wikipedia.article", "completion": "        if unquote:\n            title = parse.unquote(title)\n\n        return WikipediaPage(self, title=title, ns=ns, language=self.language)\n", "idx": 1633}
{"namespace": "wikipediaapi.WikipediaPageSection.__repr__", "completion": "        return (\n            \"<WikipediaPageSection \"\n            + \"title='{}', level={}, text='{}', \"\n            + \"subsections={}, subsections={}>\"\n        ).format(\n            self.title,\n            self.level,\n            self.text,\n            len(self.sections),\n            self.sections,\n        )", "idx": 1634}
{"namespace": "wikipediaapi.WikipediaPage.sections", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        return self._section\n", "idx": 1635}
{"namespace": "wikipediaapi.WikipediaPage.section_by_title", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        sections = self._section_mapping.get(title, [])\n        if sections:\n            return sections[-1]\n        return None\n", "idx": 1636}
{"namespace": "wikipediaapi.WikipediaPage.sections_by_title", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        return self._section_mapping.get(title, [])\n", "idx": 1637}
{"namespace": "wikipediaapi.WikipediaPage.text", "completion": "        text = self.summary\n        for section in self.sections:\n            text += section.full_text()\n        return text.strip()\n", "idx": 1638}
{"namespace": "wikipediaapi.WikipediaPage.langlinks", "completion": "        if not self._called[\"langlinks\"]:\n            self._fetch(\"langlinks\")\n        return self._langlinks\n", "idx": 1639}
{"namespace": "wikipediaapi.WikipediaPage.links", "completion": "        if not self._called[\"links\"]:\n            self._fetch(\"links\")\n        return self._links\n", "idx": 1640}
{"namespace": "wikipediaapi.WikipediaPage.backlinks", "completion": "        if not self._called[\"backlinks\"]:\n            self._fetch(\"backlinks\")\n        return self._backlinks\n", "idx": 1641}
{"namespace": "wikipediaapi.WikipediaPage.categorymembers", "completion": "        if not self._called[\"categorymembers\"]:\n            self._fetch(\"categorymembers\")\n        return self._categorymembers\n", "idx": 1642}
{"namespace": "wikipediaapi.WikipediaPage._fetch", "completion": "        getattr(self.wiki, call)(self)\n        self._called[call] = True\n        return self", "idx": 1643}
{"namespace": "wikipediaapi.WikipediaPage.__repr__", "completion": "        if any(self._called.values()):\n            return \"{title} (id: {pageid}, ns: {ns})\".format(\n                title=self.title,\n                pageid=self.pageid,\n                ns=self.ns,\n            )\n        return \"{title} (id: ??, ns: {ns})\".format(\n            title=self.title,\n            ns=self.ns,\n        )", "idx": 1644}
{"namespace": "imapclient.imapclient.IMAPClient.starttls", "completion": "        if self._starttls_done:\n            raise exceptions.IMAPClientAbortError(\n                \"STARTTLS already done for this connection\"\n            )\n\n        if not self.ssl:\n            raise exceptions.IMAPClientAbortError(\n                \"STARTTLS only makes sense when connecting over SSL\"\n            )\n\n        if ssl_context is None:\n            ssl_context = ssl_lib.create_default_context()\n\n        self._imap.starttls(ssl_context)\n        self._starttls_done = True\n        self._set_read_timeout()\n\n        return self._get_response()\n", "idx": 1645}
{"namespace": "imapclient.imapclient.IMAPClient.shutdown", "completion": "        self._imap.shutdown()\n        logger.debug(\"Connection closed\")\n", "idx": 1646}
{"namespace": "imapclient.imapclient.IMAPClient.enable", "completion": "        typ, data = self._imap._simple_command(\"ENABLE\", *capabilities)\n        self._checkok(\"enable\", typ, data)\n        return data[0].split()\n", "idx": 1647}
{"namespace": "imapclient.imapclient.IMAPClient._proc_folder_list", "completion": "        folder_data = [f for f in folder_data if f is not None and f != b\"\"]\n        if not folder_data:\n            return []\n        return [\n            (\n                flags,\n                to_unicode(delimiter),\n                decode_utf7(name) if self.folder_encode else to_unicode(name),\n            )\n            for flags, delimiter, name in parse_response(folder_data)\n        ]\n", "idx": 1648}
{"namespace": "imapclient.imapclient.IMAPClient.select_folder", "completion": "        if self._imap.state != \"SELECTED\":\n            typ, data = self._imap._simple_command(\"SELECT\", folder, readonly)\n            self._checkok(\"select\", typ, data)\n            self._imap.state = \"SELECTED\"\n            self._imap.mailbox = folder\n            self._imap.readonly = readonly\n            self._imap.recent = 0\n            self._imap.uidnext = 0\n            self._imap.uidvalidity = 0\n            self._imap.uid_plus = False\n            self._imap.uid_persistent = False\n            self._imap.uid_non_persistent = False\n            self._imap.highestmodseq = 0\n            self._imap.modseq_highest = 0\n            self._imap.modseq_lowest = 0\n            self._imap.modseq_current = 0\n            self._imap.modseq_cache = {}\n            self._imap.modseq_cache_per_folder = {}\n            self._imap.modseq_cache_per_folder[folder] = {}\n            self._imap.modseq_cache_per_folder[folder][0] = {}\n            self._imap.modseq_cache_per_folder[folder][0][\"HIGHESTMODSEQ\"] = 0\n            self._imap.modseq_cache_per_folder[folder][0][\"LOWESTMODSEQ\"] = 0\n            self._imap.modseq_cache_per_folder[folder][0][\"CURRENTMODSEQ\"] = 0\n            self._imap.modseq_cache_per_folder[folder][0][\"MODIFIED_UIDS\"] = []\n            self._imap.modseq_cache_per_folder[folder][0][\"REMOVED_UIDS\"] = []\n            self._imap.modseq_cache_per_folder[folder][0][\"ADDED_UIDS\"] = []\n            self._imap.modseq_cache_per_folder[folder][0][\"READ_UIDS\"] = []\n            self._imap.mod", "idx": 1649}
{"namespace": "imapclient.imapclient.IMAPClient.unselect_folder", "completion": "        self._command_and_check(\"unselect\")\n        return self._process_unselect_response(self._imap.untagged_responses)\n", "idx": 1650}
{"namespace": "imapclient.imapclient.IMAPClient.noop", "completion": "        return self._command_and_check(\"noop\")\n", "idx": 1651}
{"namespace": "imapclient.imapclient.IMAPClient.idle", "completion": "        if self._idle_tag is not None:\n            raise exceptions.IllegalStateError(\"Already in IDLE mode\")\n\n        self._idle_tag = self._imap._command(\"IDLE\")\n        self._imap._expect_untagged_response(b\"+\", self._idle_tag)\n        logger.debug(\"< IDLE\")\n", "idx": 1652}
{"namespace": "imapclient.imapclient.IMAPClient.idle_check", "completion": "        if self._idle_tag is None:\n            raise exceptions.IllegalStateError(\"Not in IDLE mode\")\n\n        if POLL_SUPPORT:\n            poll_func = self._poll_socket\n        else:\n            poll_func = self._select_poll_socket\n\n        while True:\n            if poll_func(self.socket(), timeout):\n                resp = self._imap._get_response()\n                if resp is not None:\n                    if resp[0] == self._idle_tag:\n                        return self._process_idle_response(resp)\n                    else:\n                        raise exceptions.IMAPClientError(\n                            \"Unexpected IDLE response: %s\" % resp\n                        )\n            else:\n                return []\n", "idx": 1653}
{"namespace": "imapclient.imapclient.IMAPClient.idle_done", "completion": "        if self._idle_tag is None:\n            raise exceptions.IllegalStateError(\"Not in IDLE mode\")\n        self._imap._simple_command(\"DONE\")\n        self._idle_tag = None\n        return self._consume_until_tagged_response(self._idle_tag, \"DONE\")\n", "idx": 1654}
{"namespace": "imapclient.imapclient.IMAPClient.folder_status", "completion": "        if what is None:\n            what = [\"MESSAGES\", \"RECENT\", \"UIDNEXT\", \"UIDVALIDITY\", \"UNSEEN\"]\n        self.select_folder(folder)\n        typ, data = self._imap._simple_command(\"STATUS\", folder, \"(\" + \" \".join(what) + \")\")\n        self._checkok(\"status\", typ, data)\n        typ, data = self._imap._untagged_response(typ, data, \"STATUS\")\n        return dict(parse_response(data))\n", "idx": 1655}
{"namespace": "imapclient.imapclient.IMAPClient.sort", "completion": "        args = []\n        if charset:\n            args.extend([b\"CHARSET\", to_bytes(charset)])\n        args.extend(_normalise_search_criteria(criteria, charset))\n        args.extend(_normalise_search_criteria(sort_criteria, charset))\n\n        try:\n            data = self._raw_command_untagged(b\"SORT\", args)\n        except imaplib.IMAP4.error as e:\n            # Make BAD IMAP responses easier to understand to the user, with a link to the docs\n            m = re.match(r\"SORT command error: BAD \\[(.+)\\]\", str(e))\n            if m:\n                raise exceptions.InvalidCriteriaError(\n                    \"{original_msg}\\n\\n\"\n                    \"This error may have been caused by a syntax error in the criteria: \"\n                    \"{criteria}\\nPlease refer to the documentation for more information \"\n                    \"about search criteria syntax..\\n\"\n                    \"https://imapclient.readthedocs.io/en/master/#imapclient.IMAPClient.search\".format(\n                        original_msg=m.group(1),\n                        criteria='\"%s\"' % criteria\n                        if not isinstance(criteria, list)\n                        else criteria,\n                    )\n                )\n\n            # If the exception is not from a BAD IMAP response, re-raise as-is\n            raise\n\n        return parse_message_list(data)\n", "idx": 1656}
{"namespace": "imapclient.imapclient.IMAPClient.thread", "completion": "        args = [\n            to_bytes(algorithm),\n            to_bytes(charset),\n        ]\n        args.extend(_normalise_search_criteria(criteria, charset))\n        ids = self._raw_command_untagged(b\"THREAD\", args, unpack=True)\n        return parse_message_list(ids)", "idx": 1657}
{"namespace": "imapclient.imapclient.IMAPClient.get_flags", "completion": "        if not messages:\n            return {}\n        if not isinstance(messages, list):\n            raise TypeError(\"messages must be a list\")\n        if not all(isinstance(msg, int) for msg in messages):\n            raise TypeError(\"messages must be a list of integers\")\n\n        # Fetch the flags for the specified messages\n        typ, data = self._imap._simple_command(\n            \"FETCH\",\n            seq_to_parenstr(messages),\n            \"(FLAGS)\",\n        )\n        self._checkok(\"fetch\", typ, data)\n\n        # Parse the response and return the flags for each message\n        flags_dict = {}\n        for msg_id, msg_data in parse_fetch_response(data):\n            flags_dict[msg_id] = msg_data.get(\"FLAGS\", ())\n\n        return flags_dict\n", "idx": 1658}
{"namespace": "imapclient.imapclient.IMAPClient.get_gmail_labels", "completion": "        response = self.fetch(messages, [\"X-GM-LABELS\"])\n        return self._filter_fetch_dict(response, b\"X-GM-LABELS\")\n", "idx": 1659}
{"namespace": "imapclient.imapclient.IMAPClient.append", "completion": "        if msg_time is None:\n            msg_time = datetime.now()\n        else:\n            msg_time = datetime_to_INTERNALDATE(msg_time)\n\n        args = [\n            self._normalise_folder(folder),\n            msg_time,\n            seq_to_parenstr_upper(flags),\n            len(msg),\n        ]\n        typ, data = self._imap._simple_command(\"APPEND\", *args)\n        self._checkok(\"append\", typ, data)\n        return data[0]\n", "idx": 1660}
{"namespace": "imapclient.imapclient.IMAPClient.multiappend", "completion": "        if not msgs:\n            return {}\n\n        # Create a list of arguments for each message\n        args_list = []\n        for msg in msgs:\n            if isinstance(msg, str):\n                # If msg is a string, create a list with the message and no flags or date\n                args_list.append([to_bytes(msg)])\n            elif isinstance(msg, dict):\n                # If msg is a dictionary, create a list with the message, flags, and date (if provided)\n                args = [to_bytes(msg[\"msg\"])]\n                if \"flags\" in msg:\n                    args.append(seq_to_parenstr(msg[\"flags\"]))\n                if \"date\" in msg:\n                    args.append(datetime_to_INTERNALDATE(msg[\"date\"]))\n                args_list.append(args)\n            else:\n                raise ValueError(\"Invalid message type: %s\" % type(msg))\n\n        # Create the MULTIAPPEND command and arguments\n        cmd = \"MULTIAPPEND\"\n        args = [self._normalise_folder(folder)]\n        for msg_args in args_list:\n            args.extend(msg_args)\n\n        # Send the MULTIAPPEND command and get the response\n        tag = self._imap._command(cmd, *args)\n        typ, data = self._imap._command_complete(cmd, tag)\n        self._checkok(cmd, typ, data)\n        typ, data = self._imap._untagged_response(typ, data, cmd)\n        return data[0]\n", "idx": 1661}
{"namespace": "imapclient.imapclient.IMAPClient.expunge", "completion": "        if messages is None:\n            messages = self.search([\"DELETED\"])\n        if self.use_uid:\n            return self._command_and_check(\"uid expunge\", join_message_ids(messages))\n        else:\n            return self._command_and_check(\"expunge\", join_message_ids(messages))\n", "idx": 1662}
{"namespace": "imapclient.imapclient.IMAPClient.getacl", "completion": "        return self._command_and_check(\"getacl\", self._normalise_folder(folder))\n", "idx": 1663}
{"namespace": "imapclient.imapclient.IMAPClient.setacl", "completion": "        return self._command_and_check(\n            \"setacl\", self._normalise_folder(folder), who, what\n        )\n", "idx": 1664}
{"namespace": "imapclient.imapclient.IMAPClient.get_quota_root", "completion": "        data = self._command_and_check(\"getquotaroot\", self._normalise_folder(mailbox))\n        return _parse_quota_root(data)\n", "idx": 1665}
{"namespace": "imapclient.imapclient.IMAPClient.set_quota", "completion": "        args = []\n        for quota in quotas:\n            args.append(to_bytes(quota.quota_root))\n            args.append(to_bytes(quota.resource))\n            args.append(to_bytes(quota.limit))\n        return self._command_and_check(\"setquota\", *args, unpack=True)\n", "idx": 1666}
{"namespace": "imapclient.imapclient.IMAPClient._consume_until_tagged_response", "completion": "        untagged = []\n        while True:\n            typ, data = self._imap._get_response()\n            if typ == b\"OK\":\n                return data[0], untagged\n            elif typ == b\"NO\":\n                raise exceptions.IMAPClientError(\n                    \"%s failed: %s\" % (command, to_unicode(data[0]))\n                )\n            elif typ == b\"BAD\":\n                raise exceptions.IMAPClientError(\n                    \"%s failed: %s\" % (command, to_unicode(data[0]))\n                )\n            elif typ == b\"BYE\":\n                raise exceptions.IMAPClientAbortError(\n                    \"%s failed: %s\" % (command, to_unicode(data[0]))\n                )\n            elif typ == b\"PREAUTH\":\n                raise exceptions.IMAPClientAbortError(\n                    \"%s failed: %s\" % (command, to_unicode(data[0]))\n                )\n            elif typ == tag:\n                return data[0], untagged\n            else:\n                untagged.append((typ, data))\n", "idx": 1667}
{"namespace": "imapclient.imapclient._normalise_search_criteria", "completion": "    if not criteria:\n        raise ValueError(\"No criteria specified\")\n\n    if isinstance(criteria, int):\n        return [b\"SEQ\", str(criteria).encode(\"ascii\")]\n\n    if isinstance(criteria, str):\n        criteria = [criteria]\n\n    if isinstance(criteria, bytes):\n        criteria = [criteria]\n\n    if isinstance(criteria, (list, tuple)):\n        if not criteria:\n            raise ValueError(\"No criteria specified\")\n        if isinstance(criteria[0], (list, tuple)):\n            return [_normalise_search_criteria(c, charset) for c in criteria]\n        if isinstance(criteria[0], int):\n            return [b\"SEQ\", str(criteria[0]).encode(\"ascii\")]\n        if isinstance(criteria[0], str):\n            return [_quote(criteria[0])]\n        if isinstance(criteria[0], bytes):\n            return [_quote(criteria[0])]\n        if isinstance(criteria[0], datetime):\n            return [b\"SINCE\", datetime_to_INTERNALDATE(criteria[0])]\n        if isinstance(criteria[0], date):\n            return [b\"SINCE\", datetime_to_INTERNALDATE(datetime.combine(criteria[0], time()))]\n        if isinstance(criteria[0], timedelta):\n            return [b\"SINCE\", datetime_to_INTERNALDATE(datetime.now() - criteria[0])]\n        if isinstance(criteria[0], Callable):\n            return [b\"SEARCHCHARSET\", to_bytes(charset), criteria[0]]\n        raise ValueError(\"Unknown criteria type: %s\" % criteria[0])\n\n    raise ValueError(\"Unknown criteria type: %s\" % criteria)\n\n", "idx": 1668}
{"namespace": "imapclient.response_lexer.TokenSource.current_literal", "completion": "        if self.src is not None:\n            return self.src.literal\n        return None\n", "idx": 1669}
{"namespace": "imapclient.imap_utf7.decode", "completion": "    if not isinstance(s, (bytes, str)):\n        return s\n\n    res = []\n    b64_buffer = []\n    b64_mode = False\n\n    def consume_b64_buffer(buf: List[str]) -> None:\n        \"\"\"\n        Consume the buffer by decoding it from a modified base 64 representation\n        and surround it with shift characters & and -\n        \"\"\"\n        if buf:\n            res.append(base64_utf7_decode(buf))\n            del buf[:]\n\n    for c in s:\n        if isinstance(c, str):\n            c = ord(c)\n\n        # ASCII 7-bit characters\n        if c < 0x80:\n            if b64_mode:\n                consume_b64_buffer(b64_buffer)\n                b64_mode = False\n\n            if c == AMPERSAND_ORD:\n                b64_mode = True\n            elif c == DASH_ORD:\n                b64_mode = False\n            else:\n                res.append(chr(c))\n\n        # Non-ASCII characters\n        else:\n            if b64_mode:\n                b64_buffer.append(c)\n            else:\n                res.append(chr(c))\n\n    # Consume the remaining buffer if the string finish with non-ASCII characters\n    consume_b64_buffer(b64_buffer)\n\n    return \"\".join(res)\n\n", "idx": 1670}
{"namespace": "imapclient.fixed_offset.FixedOffset.for_system", "completion": "        if time.daylight:\n            if time.localtime().tm_isdst:\n                return cls(time.altzone / 60)\n            else:\n                return cls(time.timezone / 60)\n        else:\n            return cls(time.timezone / 60)", "idx": 1671}
{"namespace": "imapclient.datetime_util.parse_to_datetime", "completion": "    if timestamp is None:\n        return None\n\n    if isinstance(timestamp, datetime):\n        return timestamp\n\n    if isinstance(timestamp, bytes):\n        timestamp = timestamp.decode(\"ascii\")\n\n    if \" \" in timestamp:\n        date_part, time_part = timestamp.split(\" \", 1)\n    else:\n        date_part, time_part = timestamp, \"00:00:00\"\n\n    # Date part\n    if date_part.startswith(\"-\"):\n        # Date is in the old format: DD-Mon-YYYY\n        day, month, year = date_part[1:].split(\"-\")\n        month = _SHORT_MONTHS.index(month)\n    else:\n        # Date is in the new format: YYYY-MM-DD\n        year, month, day = date_part.split(\"-\")\n\n    # Time part\n    hour, minute, second = time_part.split(\":\")\n\n    # Timezone part\n    if \"+\" in timestamp:\n        # Timezone is in the format: +HHMM\n        tz_sign, tz_hour, tz_min = timestamp.split(\"+\")\n        tz_hour = int(tz_hour)\n        tz_min = int(tz_min)\n        tz_offset = FixedOffset(tz_hour * 60 + tz_min)\n    elif \"-\" in timestamp:\n        # Timezone is in the format: -HHMM\n        tz_sign, tz_hour, tz_min = timestamp.split(\"-\")\n        tz_hour = int(tz_hour)\n        tz_min = int(tz_min)\n        tz_offset = FixedOffset(-(tz_hour * 60 + tz_min))\n    else:\n        # No timezone\n        tz_offset = None\n\n    # Create datetime object\n    dt = datetime(\n        int(year),\n        int(month),\n        int(day),\n        int(hour),\n        int(minute),\n        int(second),\n        tzinfo", "idx": 1672}
{"namespace": "imapclient.datetime_util.datetime_to_INTERNALDATE", "completion": "    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=FixedOffset.for_system())\n    return dt.strftime(\"%d-%b-%Y %H:%M:%S %z\")\n\n", "idx": 1673}
{"namespace": "imapclient.datetime_util.format_criteria_date", "completion": "    if isinstance(dt, datetime):\n        return dt.strftime(\"%d-%b-%Y\").encode(\"ascii\")\n    else:\n        return dt.strftime(\"%d-%b-%Y\").encode(\"ascii\")", "idx": 1674}
{"namespace": "imapclient.util.assert_imap_protocol", "completion": "    if not condition:\n        raise AssertionError(\n            \"Server replied with a response that violates the IMAP protocol\"\n            + (\": %s\" % message.decode(\"ascii\") if message is not None else \"\")\n        )\n\n", "idx": 1675}
{"namespace": "ehforwarderbot.utils.get_config_path", "completion": "    profile = coordinator.profile\n    if module_id is None:\n        module_id = profile\n    config_path = get_base_path() / 'profiles' / profile / module_id / 'config'\n    if not config_path.exists():\n        config_path.mkdir(parents=True)\n    return config_path / f'config.{ext}'", "idx": 1676}
{"namespace": "ehforwarderbot.utils.get_custom_modules_path", "completion": "    base_path = get_base_path()\n    channel_path = base_path / 'modules'\n    if not channel_path.exists():\n        channel_path.mkdir(parents=True)\n    return channel_path", "idx": 1677}
{"namespace": "ehforwarderbot.chat.Chat.add_member", "completion": "        if id:\n            warnings.warn(\"`id` argument is deprecated, use `uid` instead.\", DeprecationWarning)\n            uid = id\n        m = ChatMember(self, name=name, alias=alias, id=id, uid=uid,\n                       vendor_specific=vendor_specific, description=description,\n                       middleware=middleware)\n        self.members.append(m)\n        return m\n", "idx": 1678}
{"namespace": "ehforwarderbot.chat.Chat.add_system_member", "completion": "        if id:\n            warnings.warn(\"`id` argument is deprecated, use `uid` instead.\", DeprecationWarning)\n            uid = uid or id\n        member = SystemChatMember(self, name=name, alias=alias, uid=uid,\n                                  vendor_specific=vendor_specific, description=description,\n                                  middleware=middleware)\n        self.members.append(member)\n        return member\n", "idx": 1679}
{"namespace": "ehforwarderbot.chat.PrivateChat.verify", "completion": "        super().verify()\n        for i in self.members:\n            assert isinstance(i, ChatMember)\n", "idx": 1680}
{"namespace": "telethon.utils.get_inner_text", "completion": "    result = []\n    for entity in entities:\n        if isinstance(entity, types.MessageEntityBold):\n            result.append(text[entity.offset:entity.offset + entity.length])\n        elif isinstance(entity, types.MessageEntityItalic):\n            result.append(text[entity.offset:entity.offset + entity.length])\n        elif isinstance(entity, types.MessageEntityCode):\n            result.append(text[entity.offset:entity.offset + entity.length])\n        elif isinstance(entity, types.MessageEntityPre):\n            result.append(text[entity.offset:entity.offset + entity.length])\n        elif isinstance(entity, types.MessageEntityTextUrl):\n            result.append(text[entity.offset:entity.offset + entity.length])\n        elif isinstance(entity, types.MessageEntityMention):\n            result.append(text[entity.offset:entity.offset + entity.length])\n        elif isinstance(entity, types.MessageEntityMentionName):\n            result.append(text[entity.offset:entity.offset + entity.length])\n        elif isinstance(entity, types.MessageEntityHashtag):\n            result.append(text[entity.offset:entity.offset + entity.length])\n        elif isinstance(entity, types.MessageEntityCashtag):\n            result.append(text[entity.offset:entity.offset + entity.length])\n        elif isinstance(entity, types.MessageEntityBotCommand):\n            result.append(text[entity.offset:entity.offset + entity.length])\n        elif isinstance(entity, types.MessageEntityUrl):\n            result.append(text[entity.offset:entity.offset + entity.length])\n        elif isinstance(entity, types.MessageEntityEmail):\n            result.append(text[entity.offset:entity.offset + entity.length])\n        elif isinstance(entity, types.MessageEntityPhone):\n            result.append(text[entity.offset:entity.offset + entity.length])\n        elif isinstance(entity, types.MessageEntityCashtag):\n            result.append(text[entity.offset:entity.offset +", "idx": 1681}
{"namespace": "telethon.extensions.html.parse", "completion": "    parser = HTMLToTelegramParser()\n    parser.feed(html)\n    parser.close()\n    return parser.text, parser.entities\n\n", "idx": 1682}
{"namespace": "telethon.extensions.html.unparse", "completion": "    if not text:\n        return text\n    if not entities:\n        return escape(text)\n\n    # We need to sort the entities by offset, and then by length.\n    # This is so we can process them in the correct order.\n    entities = sorted(entities, key=lambda e: (e.offset, -e.length))\n\n    # We need to split the text into chunks, so we can apply the formatting\n    # to the correct parts.\n    chunks = []\n    offset = 0\n    for entity in entities:\n        if offset > entity.offset:\n            continue\n        if offset < entity.offset:\n            chunks.append(text[offset:entity.offset])\n            offset = entity.offset\n\n        formatter = ENTITY_TO_FORMATTER.get(type(entity), None)\n        if formatter is None:\n            continue\n\n        if isinstance(formatter, tuple):\n            start, end = formatter\n            chunks.append(start)\n            chunks.append(text[offset:offset + entity.length])\n            chunks.append(end)\n        else:\n            chunks.append(formatter(entity, text[offset:offset + entity.length]))\n\n        offset += entity.length\n\n    if offset < len(text):\n        chunks.append(text[offset:])\n\n    return ''.join(chunks)", "idx": 1683}
{"namespace": "telethon.crypto.rsa.encrypt", "completion": "    global _server_keys\n    if fingerprint not in _server_keys:\n        return None\n\n    key, old = _server_keys[fingerprint]\n    if use_old and not old:\n        return None\n\n    data = TLObject.serialize_bytes(data)\n    data += sha1(data).digest()\n    data = rsa.core.encrypt_int(\n        int.from_bytes(data, 'big', signed=False),\n        key.e, key.n\n    )\n    data = TLObject.serialize_bytes(get_byte_array(data))\n    return data\n\n", "idx": 1684}
{"namespace": "hbmqtt.codecs.encode_string", "completion": "    byte_str = string.encode(encoding='utf-8')\n    return pack(\"H\", len(byte_str)) + byte_str\n\n", "idx": 1685}
{"namespace": "hbmqtt.plugins.manager.PluginManager.get_plugin", "completion": "        for plugin in self._plugins:\n            if plugin.name == name:\n                return plugin\n        return None", "idx": 1686}
{"namespace": "pysimplesoap.simplexml.SimpleXMLElement.add_child", "completion": "        if ns is True:\n            ns = self.__ns\n        elif ns is False or self.__ns is None:\n            ns = None\n        if text is None:\n            child = self.__document.createElementNS(ns, name)\n        elif isinstance(text, basestring):\n            child = self.__document.createElementNS(ns, name)\n            child.appendChild(self.__document.createTextNode(text))\n        elif isinstance(text, xml.dom.minidom.CDATASection):\n            child = self.__document.createElementNS(ns, name)\n            child.appendChild(text)\n        else:\n            raise TypeError(\"text must be a string or CDATASection\")\n        self.__elements[0].appendChild(child)\n        return SimpleXMLElement(elements=[child], document=self.__document,\n                                namespace=self.__ns, prefix=self.__prefix,\n                                namespaces_map=self.__namespaces_map, jetty=self.__jetty)\n", "idx": 1687}
{"namespace": "pysimplesoap.simplexml.SimpleXMLElement.as_xml", "completion": "        if pretty:\n            xml_string = self.__document.toprettyxml(indent=\"  \")\n        else:\n            xml_string = self.__document.toxml()\n\n        if filename:\n            with open(filename, \"w\") as f:\n                f.write(xml_string)\n\n        return xml_string\n", "idx": 1688}
{"namespace": "twilio.base.deserialize.iso8601_date", "completion": "    try:\n        return datetime.datetime.strptime(s, ISO8601_DATE_FORMAT).date()\n    except ValueError:\n        return s\n\n", "idx": 1689}
{"namespace": "twilio.base.deserialize.iso8601_datetime", "completion": "    try:\n        return datetime.datetime.strptime(s, ISO8601_DATETIME_FORMAT).replace(\n            tzinfo=datetime.timezone.utc\n        )\n    except (TypeError, ValueError):\n        return s\n\n", "idx": 1690}
{"namespace": "twilio.base.serialize.iso8601_date", "completion": "    if isinstance(d, datetime.datetime):\n        return d.strftime(\"%Y-%m-%d\")\n    elif isinstance(d, datetime.date):\n        return d.strftime(\"%Y-%m-%d\")\n    elif isinstance(d, str):\n        return d\n    else:\n        return None\n\n", "idx": 1691}
{"namespace": "twilio.base.serialize.iso8601_datetime", "completion": "    if d == values.unset:\n        return d\n    elif isinstance(d, datetime.datetime):\n        return d.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n    elif isinstance(d, datetime.date):\n        return str(d)\n    elif isinstance(d, str):\n        return d\n\n", "idx": 1692}
{"namespace": "twilio.base.serialize.prefixed_collapsible_map", "completion": "    return {prefix + k: v for k, v in m.items()}\n\n", "idx": 1693}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.dial", "completion": "        return self.nest(\n            Dial(\n                number=number,\n                action=action,\n                method=method,\n                timeout=timeout,\n                hangup_on_star=hangup_on_star,\n                time_limit=time_limit,\n                caller_id=caller_id,\n                record=record,\n                trim=trim,\n                recording_status_callback=recording_status_callback,\n                recording_status_callback_method=recording_status_callback_method,\n                recording_status_callback_event=recording_status_callback_event,\n                answer_on_bridge=answer_on_bridge,\n                ring_tone=ring_tone,\n                recording_track=recording_track,\n                sequential=sequential,\n                refer_url=refer_url,\n                refer_method=refer_method,\n                **kwargs\n            )\n        )\n", "idx": 1694}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.enqueue", "completion": "        return self.nest(\n            Enqueue(\n                name=name,\n                action=action,\n                max_queue_size=max_queue_size,\n                method=method,\n                wait_url=wait_url,\n                wait_url_method=wait_url_method,\n                workflow_sid=workflow_sid,\n                **kwargs\n            )\n        )\n", "idx": 1695}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.gather", "completion": "        return self.nest(\n            Gather(\n                input=input,\n                action=action,\n                method=method,\n                timeout=timeout,\n                speech_timeout=speech_timeout,\n                max_speech_time=max_speech_time,\n                profanity_filter=profanity_filter,\n                finish_on_key=finish_on_key,\n                num_digits=num_digits,\n                partial_result_callback=partial_result_callback,\n                partial_result_callback_method=partial_result_callback_method,\n                language=language,\n                hints=hints,\n                barge_in=barge_in,\n                debug=debug,\n                action_on_empty_result=action_on_empty_result,\n                speech_model=speech_model,\n                enhanced=enhanced,\n                **kwargs\n            )\n        )\n", "idx": 1696}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.say", "completion": "        return self.nest(\n            Say(message=message, voice=voice, loop=loop, language=language, **kwargs)\n        )", "idx": 1697}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.sms", "completion": "        return self.nest(\n            Sms(\n                message,\n                to=to,\n                from_=from_,\n                action=action,\n                method=method,\n                status_callback=status_callback,\n                **kwargs\n            )\n        )", "idx": 1698}
{"namespace": "twilio.twiml.voice_response.Gather.say", "completion": "        return self.nest(Say(message=message, voice=voice, loop=loop, language=language, **kwargs))\n", "idx": 1699}
{"namespace": "twilio.twiml.voice_response.Dial.client", "completion": "        return self.nest(\n            Client(\n                identity=identity,\n                url=url,\n                method=method,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                **kwargs\n            )\n        )\n", "idx": 1700}
{"namespace": "twilio.twiml.voice_response.Dial.conference", "completion": "        return self.nest(\n            Conference(\n                name,\n                muted=muted,\n                beep=beep,\n                start_conference_on_enter=start_conference_on_enter,\n                end_conference_on_exit=end_conference_on_exit,\n                wait_url=wait_url,\n                wait_method=wait_method,\n                max_participants=max_participants,\n                record=record,\n                region=region,\n                coach=coach,\n                trim=trim,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                recording_status_callback=recording_status_callback,\n                recording_status_callback_method=recording_status_callback_method,\n                recording_status_callback_event=recording_status_callback_event,\n                event_callback_url=event_callback_url,\n                jitter_buffer_size=jitter_buffer_size,\n                participant_label=participant_label,\n                **kwargs\n            )\n        )\n", "idx": 1701}
{"namespace": "twilio.twiml.voice_response.Dial.queue", "completion": "        return self.nest(\n            Queue(\n                name,\n                url=url,\n                method=method,\n                reservation_sid=reservation_sid,\n                post_work_activity_sid=post_work_activity_sid,\n                **kwargs\n            )\n        )\n", "idx": 1702}
{"namespace": "twilio.twiml.voice_response.Dial.sip", "completion": "        return self.nest(\n            Sip(\n                sip_url,\n                username=username,\n                password=password,\n                url=url,\n                method=method,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                machine_detection=machine_detection,\n                amd_status_callback_method=amd_status_callback_method,\n                amd_status_callback=amd_status_callback,\n                machine_detection_timeout=machine_detection_timeout,\n                machine_detection_speech_threshold=machine_detection_speech_threshold,\n                machine_detection_speech_end_threshold=machine_detection_speech_end_threshold,\n                machine_detection_silence_timeout=machine_detection_silence_timeout,\n                **kwargs\n            )\n        )\n", "idx": 1703}
{"namespace": "twilio.twiml.messaging_response.MessagingResponse.message", "completion": "        return self.nest(\n            Message(\n                body=body,\n                to=to,\n                from_=from_,\n                action=action,\n                method=method,\n                status_callback=status_callback,\n                **kwargs\n            )\n        )\n\n", "idx": 1704}
{"namespace": "twilio.twiml.TwiML.append", "completion": "        if isinstance(verb, (str, bytes)):\n            raise TwiMLException(\"Only nested verb calls are accepted as arguments\")\n\n        self.verbs.append(verb)\n        return self\n", "idx": 1705}
{"namespace": "twilio.jwt.Jwt.to_jwt", "completion": "        if not self.secret_key:\n            raise ValueError(\"No secret key configured for JWT\")\n\n        headers = self.headers.copy()\n        payload = self.payload.copy()\n\n        if ttl:\n            payload[\"exp\"] = int(time.time()) + ttl\n\n        return jwt_lib.encode(\n            payload, self.secret_key, algorithm=self.algorithm, headers=headers\n        )\n", "idx": 1706}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_client_outgoing", "completion": "        scope_uri = \"scope:client:outgoing?appSid={}\".format(application_sid)\n        if kwargs:\n            scope_uri += \"&\" + urlencode(kwargs)\n        self.capabilities[\"outgoing\"] = {\"application_sid\": application_sid, \"scope\": scope_uri}\n", "idx": 1707}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_client_incoming", "completion": "        self.client_name = client_name\n        scope = ScopeURI(\"client\", \"incoming\", {\"clientName\": self.client_name})\n        self.capabilities[\"incoming\"] = scope\n", "idx": 1708}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_event_stream", "completion": "        scope = ScopeURI(\"stream\", \"subscribe\", {\"path\": \"/2010-04-01/Events\"})\n        if kwargs:\n            scope.add_param(\"params\", urlencode(kwargs, doseq=True))\n\n        self.capabilities[\"events\"] = scope\n", "idx": 1709}
{"namespace": "twilio.jwt.client.ClientCapabilityToken._generate_payload", "completion": "        if \"outgoing\" in self.capabilities and self.client_name is not None:\n            self.capabilities[\"outgoing\"].add_param(\"clientName\", self.client_name)\n\n        scope_uris = [str(scope_uri) for scope_uri in self.capabilities.values()]\n        return {\"scope\": \" \".join(scope_uris)}", "idx": 1710}
{"namespace": "twilio.jwt.client.ScopeURI.to_payload", "completion": "        if self.params:\n            sorted_params = sorted(self.params.items())\n            encoded_params = urlencode(sorted_params)\n            param_string = \"?{}\".format(encoded_params)\n        else:\n            param_string = \"\"\n\n        return \"scope:{}:{}{}\".format(self.service, self.privilege, param_string)", "idx": 1711}
{"namespace": "twilio.jwt.access_token.AccessToken.add_grant", "completion": "        if not isinstance(grant, AccessTokenGrant):\n            raise ValueError(\"Grant must be an instance of AccessTokenGrant.\")\n\n        self.grants.append(grant)\n", "idx": 1712}
{"namespace": "twilio.jwt.taskrouter.capabilities.WorkerCapabilityToken.allow_update_activities", "completion": "        self._make_policy(self.workspace_url + \"/Activities\", \"POST\", True, {\n            \"ActivitySid\": {\"required\": True}\n        })\n", "idx": 1713}
{"namespace": "zulipterminal.platform_code.successful_GUI_return_code", "completion": "    if PLATFORM == \"WSL\":\n        return 1\n    else:\n        return 0", "idx": 1714}
{"namespace": "zulipterminal.platform_code.normalized_file_path", "completion": "    if PLATFORM == \"WSL\":\n        return path.replace(\"/\", \"\\\\\")", "idx": 1715}
{"namespace": "zulipterminal.helper.canonicalize_color", "completion": "    # Check if the color string is in the format '#xxxxxx'\n    if match(r\"^#[0-9a-f]{6}$\", color, flags=ASCII):\n        # If it is, convert it to the format '#xxx' with lowercase hex digits\n        return f\"#{color[1:].lower()}\"\n\n    # Check if the color string is in the format '#xxx'\n    elif match(r\"^#[0-9a-f]{3}$\", color, flags=ASCII):\n        # If it is, convert it to the format '#xxx' with lowercase hex digits\n        return f\"#{color[1:].lower()}\"\n\n    # If the color string is not in either format, return it as is\n    return color\n\n", "idx": 1716}
{"namespace": "zulipterminal.helper.get_unused_fence", "completion": "    from zulipterminal.config.regexes import REGEX_FENCE\n    matches = findall(REGEX_FENCE, content)\n    max_length = max(len(match) for match in matches) if matches else 0\n    return \"`\" * (max_length + 1)\n\n", "idx": 1717}
{"namespace": "zulipterminal.helper.open_media", "completion": "    command = [tool, media_path]\n    try:\n        subprocess.run(command, check=True)\n    except subprocess.CalledProcessError as e:\n        controller.report_error(f\"Error opening media: {e}\")\n\n", "idx": 1718}
{"namespace": "zulipterminal.server_url.encode_stream", "completion": "    stream_name = stream_name.replace(\" \", \"-\")\n    return f\"{stream_name}-{stream_id}\"\n\n", "idx": 1719}
{"namespace": "zulipterminal.server_url.near_message_url", "completion": "    if message[\"type\"] == \"stream\":\n        return near_stream_message_url(server_url, message)\n    else:\n        return near_pm_message_url(server_url, message)", "idx": 1720}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.update_recipients", "completion": "        # Extract recipient emails from the input text\n        recipient_emails = [\n            email.strip()\n            for email in write_box.edit_text.split(\",\")\n            if email.strip()\n        ]\n\n        # Set the corresponding user IDs in the WriteBox instance\n        self.recipient_user_ids = [\n            self.model.user_id_email_dict[email] for email in recipient_emails\n        ]\n\n        # Update the typing recipient user IDs\n        self.typing_recipient_user_ids = [\n            user_id\n            for user_id in self.recipient_user_ids\n            if user_id != self.model.user_id\n        ]\n", "idx": 1721}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.stream_box_view", "completion": "        self.stream_write_box = ReadlineEdit(edit_text=caption)\n        self.stream_write_box.enable_autocomplete(\n            func=self._stream_box_autocomplete,\n            key=primary_key_for_command(\"AUTOCOMPLETE\"),\n            key_reverse=primary_key_for_command(\"AUTOCOMPLETE_REVERSE\"),\n        )\n        self.stream_write_box.set_completer_delims(\"\")\n\n        def set_stream_marker(edit: object, new_edit_text: str) -> None:\n            self.stream_write_box.set_edit_marker(\n                self.model.stream_dict[stream_id][\"color\"]\n            )\n\n        urwid.connect_signal(self.stream_write_box, \"change\", set_stream_marker)\n\n        self._setup_common_stream_compose(stream_id, caption, title)\n", "idx": 1722}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.stream_box_edit_view", "completion": "        self.stream_write_box = ReadlineEdit(\n            edit_text=caption, max_char=self.model.max_stream_name_length\n        )\n        self.stream_write_box.enable_autocomplete(\n            func=self._stream_box_autocomplete,\n            key=primary_key_for_command(\"AUTOCOMPLETE\"),\n            key_reverse=primary_key_for_command(\"AUTOCOMPLETE_REVERSE\"),\n        )\n        self.stream_write_box.set_completer_delims(\"\")\n        self._setup_common_stream_compose(stream_id, caption, title)\n\n        # Use and set a callback to set the stream marker\n        self._set_stream_write_box_style(None, caption)\n        urwid.connect_signal(\n            self.stream_write_box, \"change\", self._set_stream_write_box_style\n        )\n\n        # Add edit mode button to header write box\n        self.header_write_box.contents.insert(\n            self.FOCUS_HEADER_BOX_EDIT,\n            (EditModeButton(self.view), self.options()),\n        )\n\n        # Set focus to message box\n        self.focus_position = self.FOCUS_CONTAINER_MESSAGE\n", "idx": 1723}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._set_stream_write_box_style", "completion": "        # Check if the input text is a valid stream name\n        if self.model.is_valid_stream(new_text):\n            # Retrieve the corresponding stream information\n            stream_info = self.model.stream_dict[new_text]\n            # Set the color of the stream write box based on the stream color\n            self.stream_write_box.set_attr_map({None: stream_info[\"color\"]})\n            # Set the stream marker in the header write box\n            self.header_write_box.widget_list[0] = urwid.Text(\n                (stream_info[\"color\"], stream_info[\"stream_marker\"])\n            )\n        else:\n            # Set the color of the stream write box to invalid\n            self.stream_write_box.set_attr_map({None: \"invalid\"})\n            # Set the stream marker in the header write box to an empty string\n            self.header_write_box.widget_list[0] = urwid.Text(\"\")\n", "idx": 1724}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._to_box_autocomplete", "completion": "        users = self.view.users\n        text_list = text.split(\",\")\n        latest_text = text_list[-1].strip()\n        matching_users = [\n            user\n            for user in users\n            if user[\"full_name\"].startswith(latest_text)\n            or user[\"email\"].startswith(latest_text)\n        ]\n        autocompleted_recipients = \", \".join(text_list[:-1])\n        if autocompleted_recipients:\n            autocompleted_recipients += \", \"\n        autocompleted_recipients += \", \".join(\n            [f\"{user['full_name']} <{user['email']}>\" for user in matching_users]\n        )\n        user_names = [user[\"full_name\"] for user in matching_users]\n        return self.typeahead_autocomplete(\n            autocompleted_recipients, state, user_names\n        )\n", "idx": 1725}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._topic_box_autocomplete", "completion": "        # Get the list of topic names from the model\n        topic_names = self.model.get_topics_in_current_stream()\n\n        # Match the input text with the topic names to generate typeaheads\n        matching_topics = [\n            topic for topic in topic_names if match_topics(topic, text)\n        ]\n\n        # Process the typeaheads and return them as suggestions\n        return self._process_typeaheads(matching_topics, state, topic_names)\n", "idx": 1726}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._stream_box_autocomplete", "completion": "        stream_names = self.view.streams\n        stream_typeaheads = match_stream(stream_names, text)\n\n        # Typeaheads and suggestions are the same.\n        return self._process_typeaheads(stream_typeaheads, state, stream_typeaheads)\n", "idx": 1727}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.generic_autocomplete", "completion": "        # Check for prefixes in the text and call the corresponding autocomplete function\n        if text.startswith(\"@\"):\n            autocomplete_func = self._user_box_autocomplete\n        elif text.startswith(\"#\"):\n            autocomplete_func = self._stream_box_autocomplete\n        elif text.startswith(\":\"):\n            autocomplete_func = self._emoji_box_autocomplete\n        elif text.startswith(\"!\"):\n            autocomplete_func = self._command_box_autocomplete\n        elif text.startswith(\"^\"):\n            autocomplete_func = self._time_mention_box_autocomplete\n        else:\n            return None\n\n        # Call the autocomplete function and get the suggestions\n        suggestions = autocomplete_func(text, state)\n\n        # Process the suggestions and return the updated text with the autocomplete suggestion\n        return self._process_typeaheads(suggestions, state, suggestions)\n", "idx": 1728}
{"namespace": "zulipterminal.ui_tools.boxes.PanelSearchBox.reset_search_text", "completion": "        self.set_caption(self.search_text)\n        self.set_edit_text(\"\")\n", "idx": 1729}
{"namespace": "zulipterminal.ui_tools.boxes.PanelSearchBox.valid_char", "completion": "        if self.edit_text:\n            return super().valid_char(ch)\n        else:\n            return unicodedata.category(ch)[0] == \"L\" and ch not in [\n                \" \",\n                \"\\t\",\n                \"\\n\",\n                \"\\r\",\n                \"\\f\",\n                \"\\v\",\n            ]\n", "idx": 1730}
{"namespace": "zulipterminal.ui_tools.utils.is_muted", "completion": "    if is_private_message(msg):\n        return False\n    if model.is_muted_stream(msg[\"display_recipient\"], msg[\"subject\"]):\n        return True\n    return False\n\n", "idx": 1731}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.update_count", "completion": "        if text_color is None:\n            text_color = self.original_color\n\n        self.count = count\n        self.update_widget(self.count_style, self.count_text, text_color)\n", "idx": 1732}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.update_widget", "completion": "        if isinstance(self.prefix_character, tuple):\n            prefix_character, prefix_character_color = self.prefix_character\n            self.button_prefix.set_text((prefix_character_color, prefix_character))\n        else:\n            self.button_prefix.set_text(self.prefix_character)\n\n        self._label.set_text(count_text)\n        self.button_suffix.set_text(\"\")\n        self._w.set_attr_map({None: text_color})\n", "idx": 1733}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.keypress", "completion": "        if is_command_key(\"ENTER\", key):\n            self.activate(key)\n            return None\n        else:\n            return super().keypress(size, key)\n\n", "idx": 1734}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._parse_narrow_link", "completion": "        from zulipterminal.config.regexes import (\n            REGEX_INTERNAL_LINK_STREAM_ID,\n            REGEX_INTERNAL_LINK_TOPIC,\n        )\n\n        parsed_link: ParsedNarrowLink = {}\n        parsed_url = urlparse(link)\n        if parsed_url.fragment:\n            fragment = parsed_url.fragment\n            if fragment.startswith(\"narrow/stream/\"):\n                # a. narrow/stream/[{stream_id}-]{stream-name}\n                # b. narrow/stream/[{stream_id}-]{stream-name}/near/{message_id}\n                # c. narrow/stream/[{stream_id}-]{stream-name}/topic/{encoded.20topic.20name}\n                # d. narrow/stream/[{stream_id}-]{stream-name}/topic/{encoded.20topic.20name}/near/{message_id}\n                narrow_components = fragment.split(\"/\")\n                if len(narrow_components) >= 3:\n                    parsed_link[\"narrow\"] = narrow_components[2]\n                    if len(narrow_components) >= 4:\n                        parsed_link[\"stream\"] = cls._decode_stream_data(\n                            narrow_components[3]\n                        )\n                        if len(narrow_components) >= 5:\n                            if narrow_components[4] == \"near\":\n                                parsed_link[\"message_id\"] = cls._decode_message_id(\n                                    narrow_components[5]\n                                )\n                            elif narrow_components[4] == \"topic\":\n                                parsed_link[\"topic_name\"] = hash_util_decode(\n                                    narrow_components[5]\n                                )\n                                if len(narrow_components) >= 6:\n                                    if narrow_components[6] == \"near\":\n                                        parsed_link[\"message_id\"] = cls._decode_message_id(\n                                            narrow_components[7]\n                                        )\n        return parsed_link\n", "idx": 1735}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._validate_and_patch_stream_data", "completion": "        stream_data = parsed_link[\"stream\"]\n        stream_id = stream_data[\"stream_id\"]\n        stream_name = stream_data[\"stream_name\"]\n\n        if stream_id is None and stream_name is None:\n            return \"Invalid stream data in the link.\"\n\n        if stream_id is not None:\n            if not self.model.is_user_subscribed_to_stream(stream_id):\n                return \"You are not subscribed to this stream.\"\n        else:\n            stream_id = self.model.get_stream_id(stream_name)\n            if stream_id is None:\n                return \"Invalid stream name in the link.\"\n            parsed_link[\"stream\"][\"stream_id\"] = stream_id\n\n        return \"\"\n", "idx": 1736}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._validate_narrow_link", "completion": "        model = self.model\n        narrow = parsed_link[\"narrow\"]\n        stream_id = parsed_link[\"stream\"][\"stream_id\"]\n        stream_name = parsed_link[\"stream\"][\"stream_name\"]\n        topic_name = parsed_link.get(\"topic_name\")\n        message_id = parsed_link.get(\"message_id\")\n\n        if narrow == \"stream\":\n            if not model.is_user_subscribed_to_stream(stream_id):\n                return \"The stream seems to be either unknown or unsubscribed\"\n\n        elif narrow == \"stream:topic\":\n            if not model.is_user_subscribed_to_stream(stream_id):\n                return \"The stream seems to be either unknown or unsubscribed\"\n            if not model.is_valid_topic(stream_id, topic_name):\n                return \"The topic seems to be unknown\"\n\n        elif narrow == \"stream:near\":\n            if not model.is_user_subscribed_to_stream(stream_id):\n                return \"The stream seems to be either unknown or unsubscribed\"\n            if not model.is_valid_near_message_id(stream_id, message_id):\n                return \"The message seems to be unknown\"\n\n        elif narrow == \"stream:topic:near\":\n            if not model.is_user_subscribed_to_stream(stream_id):\n                return \"The stream seems to be either unknown or unsubscribed\"\n            if not model.is_valid_topic(stream_id, topic_name):\n                return \"The topic seems to be unknown\"\n            if not model.is_valid_near_message_id(stream_id, message_id):\n                return \"The message seems to be unknown\"\n\n        return \"\"\n", "idx": 1737}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._switch_narrow_to", "completion": "        controller = self.controller\n        stream_id = parsed_link[\"stream\"][\"stream_id\"]\n        stream_name = parsed_link[\"stream\"][\"stream_name\"]\n\n        if parsed_link[\"narrow\"] == \"stream\":\n            controller.narrow_to_stream(stream_id=stream_id, stream_name=stream_name)\n        elif parsed_link[\"narrow\"] == \"stream:topic\":\n            topic_name = parsed_link[\"topic_name\"]\n            controller.narrow_to_topic(stream_id=stream_id, stream_name=stream_name, topic_name=topic_name)\n        elif parsed_link[\"narrow\"] == \"stream:near\":\n            message_id = parsed_link[\"message_id\"]\n            controller.narrow_to_near(stream_id=stream_id, stream_name=stream_name, message_id=message_id)\n        elif parsed_link[\"narrow\"] == \"stream:topic:near\":\n            topic_name = parsed_link[\"topic_name\"]\n            message_id = parsed_link[\"message_id\"]\n            controller.narrow_to_near(stream_id=stream_id, stream_name=stream_name, topic_name=topic_name, message_id=message_id)\n", "idx": 1738}
{"namespace": "zulipterminal.config.themes.complete_and_incomplete_themes", "completion": "    complete_themes = []\n    incomplete_themes = []\n\n    for theme_name, theme_data in THEMES.items():\n        if is_complete_theme(theme_data):\n            complete_themes.append(theme_name)\n        else:\n            incomplete_themes.append(theme_name)\n\n    complete_themes.sort()\n    incomplete_themes.sort()\n\n    return complete_themes, incomplete_themes\n\n", "idx": 1739}
{"namespace": "zulipterminal.config.themes.validate_colors", "completion": "    theme_styles = THEMES[theme_name].STYLES\n    invalid_colors = []\n    for style in theme_styles:\n        if style[1] not in valid_16_color_codes:\n            invalid_colors.append(style[1])\n        if style[2] not in valid_16_color_codes:\n            invalid_colors.append(style[2])\n    if invalid_colors:\n        raise InvalidThemeColorCode(\n            f\"Invalid color codes found in theme '{theme_name}': {invalid_colors}\"\n        )\n\n", "idx": 1740}
{"namespace": "zulipterminal.config.themes.parse_themefile", "completion": "    urwid_theme = []\n    for style_name, (fg, bg) in theme_styles.items():\n        if color_depth == 1:\n            fg = \"default\"\n            bg = \"default\"\n        elif color_depth == 16:\n            fg = fg.split()[0]\n            bg = bg.split()[0]\n        elif color_depth == 256:\n            fg = fg.split()[0]\n            bg = bg.split()[0]\n        elif color_depth == 2 ** 24:\n            fg = fg.split()[0]\n            bg = bg.split()[0]\n        else:\n            raise ValueError(f\"Invalid color depth: {color_depth}\")\n\n        if style_name is None:\n            urwid_theme.append((None, fg, bg))\n        else:\n            urwid_theme.append((style_name, fg, bg))\n\n    return urwid_theme\n\n", "idx": 1741}
{"namespace": "zulipterminal.config.themes.add_pygments_style", "completion": "    pygments_styles = theme_meta[\"pygments\"][\"styles\"]\n    background = theme_meta[\"pygments\"][\"background\"]\n    overrides = theme_meta[\"pygments\"][\"overrides\"]\n\n    # Add background color to Pygments styles\n    pygments_styles[\"background_color\"] = background.value\n\n    # Add Pygments styles to Urwid theme\n    for style_name, style_value in pygments_styles.items():\n        if style_name in STANDARD_TYPES:\n            urwid_theme.append((style_name, style_value, style_value))\n\n    # Add overrides to Urwid theme\n    for style_name, style_value in overrides.items():\n        if style_name in STANDARD_TYPES:\n            urwid_theme.append((style_name, style_value, style_value))", "idx": 1742}
{"namespace": "zulipterminal.config.keys.is_command_key", "completion": "    if command not in KEY_BINDINGS:\n        raise InvalidCommand(f\"{command} is not a valid command.\")\n\n    return key in KEY_BINDINGS[command][\"keys\"]\n\n", "idx": 1743}
{"namespace": "zulipterminal.config.keys.keys_for_command", "completion": "    try:\n        return KEY_BINDINGS[command][\"keys\"]\n    except KeyError as exception:\n        raise InvalidCommand(command)\n\n", "idx": 1744}
{"namespace": "zulipterminal.config.keys.commands_for_random_tips", "completion": "    return [\n        command\n        for command in KEY_BINDINGS.values()\n        if not command.get(\"excluded_from_random_tips\", False)\n    ]", "idx": 1745}
{"namespace": "hypertools.datageometry.DataGeometry.transform", "completion": "        if data is None:\n            return self.xform_data\n        else:\n            if isinstance(data, list):\n                data = list(map(convert_text, data))\n            else:\n                data = convert_text(data)\n            return self.reduce['model'].transform(data, **self.reduce['params'])\n", "idx": 1746}
{"namespace": "hypertools.datageometry.DataGeometry.plot", "completion": "        from .plot.plot import plot\n        if data is None:\n            data = self.xform_data\n        return plot(data, **kwargs)", "idx": 1747}
{"namespace": "awesome_autodl.autodl_topic2papers", "completion": "    topic2path = autodl_topic2path()\n    topic2papers = OrderedDict()\n    for topic, path in topic2path.items():\n        topic2papers[topic] = AutoDLpaper.load_yaml(path)\n    return topic2papers", "idx": 1748}
{"namespace": "awesome_autodl.get_bib_abbrv_obj", "completion": "    from awesome_autodl.utils import load_yaml, dump_yaml\n    from awesome_autodl.bib_abbrv import BibAbbreviations\n\n    bib_abbrv_file = get_bib_abbrv_file()\n    if not bib_abbrv_file.exists():\n        ValueError(f\"Can not find bib_abbrv_file at {bib_abbrv_file}\")\n    bib_abbrv_obj = BibAbbreviations(bib_abbrv_file)\n    return bib_abbrv_obj", "idx": 1749}
{"namespace": "mssqlcli.localized_strings.translation", "completion": "    if languages is None:\n        languages = LANGUAGES\n\n    return gettext.translation(domain, localedir, languages=languages)\n\n", "idx": 1750}
{"namespace": "mssqlcli.mssqlbuffer._is_query_executable", "completion": "    # remove all comments from the string\n    sql = sqlparse.format(sql, strip_comments=True).strip()\n\n    # check if the statement is a complete command\n    if sql.endswith('GO'):\n        # check if the statement is surrounded by an open quote\n        if sql.count(\"'\") % 2 == 1 or sql.count('\"') % 2 == 1:\n            return False\n        return True\n\n    # check if the statement has an open comment\n    if sql.count('/*') != sql.count('*/'):\n        return False\n\n    return False", "idx": 1751}
{"namespace": "mssqlcli.telemetry.conclude", "completion": "    _session.end_time = datetime.now()\n    payload = _session.generate_payload()\n    if payload:\n        _output_payload(payload)\n        if separate_process:\n            return _upload_payload_in_separate_process(payload, service_endpoint_uri)\n        else:\n            return _upload_payload(payload, service_endpoint_uri)\n\n", "idx": 1752}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.start", "completion": "        self.request_thread = threading.Thread(target=self.request_listener, name=self.REQUEST_THREAD_NAME)\n        self.request_thread.daemon = True\n        self.request_thread.start()\n\n        self.response_thread = threading.Thread(target=self.response_listener, name=self.RESPONSE_THREAD_NAME)\n        self.response_thread.daemon = True\n        self.response_thread.start()\n", "idx": 1753}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.submit_request", "completion": "        if method is None:\n            raise ValueError(u'Method cannot be None.')\n        if params is None:\n            raise ValueError(u'Params cannot be None.')\n\n        request = {\n            u'jsonrpc': u'2.0',\n            u'method': method,\n            u'params': params,\n            u'id': request_id\n        }\n\n        self.request_queue.put(request)\n", "idx": 1754}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.get_response", "completion": "        if request_id in self.response_map:\n            if not self.response_map[request_id].empty():\n                return self.response_map[request_id].get()\n            elif not self.exception_queue.empty():\n                raise self.exception_queue.get()\n            else:\n                return None\n        elif owner_uri in self.response_map:\n            if not self.response_map[owner_uri].empty():\n                return self.response_map[owner_uri].get()\n            elif not self.exception_queue.empty():\n                raise self.exception_queue.get()\n            else:\n                return None\n        else:\n            if not self.exception_queue.empty():\n                raise self.exception_queue.get()\n            else:\n                return None\n", "idx": 1755}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.shutdown", "completion": "        self.cancel = True\n        self.request_queue.put(None)\n        self.request_thread.join()\n        self.writer.close()\n        logger.debug(u'Json Rpc client shutdown.')", "idx": 1756}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcWriter.send_request", "completion": "        content = {u'method': method, u'params': params, u'id': request_id}\n        content_json = json.dumps(content)\n        content_bytes = content_json.encode(self.encoding)\n        content_length = len(content_bytes)\n        header = self.HEADER.format(content_length)\n        header_bytes = header.encode(self.encoding)\n\n        try:\n            self.stream.write(header_bytes)\n            self.stream.write(content_bytes)\n        except ValueError:\n            # Stream was closed externally.\n            raise\n", "idx": 1757}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.read_response", "completion": "        while self.needs_more_data:\n            self._read_from_stream()\n            self._read_header()\n            self._read_content()\n\n        # Trim buffer.\n        self._trim_buffer()\n\n        # Parse content.\n        content = self.buffer[self.read_offset:self.buffer_end_offset].decode(self.encoding)\n        self.read_offset = self.buffer_end_offset\n        self.needs_more_data = False\n\n        return json.loads(content)\n", "idx": 1758}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.read_next_chunk", "completion": "        # Check if the buffer needs to be resized\n        if self.buffer_end_offset == len(self.buffer):\n            self.resize_buffer()\n\n        # Read data from the stream into the buffer\n        try:\n            bytes_read = self.stream.readinto(self.buffer[self.buffer_end_offset:])\n            if bytes_read == 0:\n                # Stream is empty or closed externally\n                raise ValueError(u'Stream is empty or closed externally')\n\n            # Update the buffer offset\n            self.buffer_end_offset += bytes_read\n            return True\n        except ValueError as ex:\n            # Stream is empty or closed externally\n            logger.debug(u'JSON RPC Reader on read_next_chunk() encountered exception: %s', ex)\n            raise\n", "idx": 1759}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.try_read_headers", "completion": "        # Find the end of the header.\n        end_of_header = self.buffer.find(b'\\r\\n\\r\\n', self.read_offset)\n        if end_of_header == -1:\n            # We don't have enough data to form a header.\n            return False\n\n        # Extract the header content.\n        header_content = self.buffer[self.read_offset:end_of_header]\n        self.read_offset = end_of_header + 4\n\n        # Split the header content by new line.\n        header_lines = header_content.split(b'\\r\\n')\n\n        # Extract the key-value pairs from the header lines.\n        for header_line in header_lines:\n            key, value = header_line.split(b': ')\n            self.headers[key.decode(self.encoding)] = value.decode(self.encoding)\n\n        # Check if the 'content-length' header is present.\n        if 'content-length' in self.headers:\n            self.expected_content_length = int(self.headers['content-length'])\n\n        # Set the read state to Content.\n        self.read_state = ReadState.Content\n\n        return True\n", "idx": 1760}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.close", "completion": "        try:\n            self.stream.close()\n        except AttributeError:\n            pass", "idx": 1761}
{"namespace": "mssqlcli.packages.prioritization.PrevalenceCounter.update", "completion": "        keywords = get_keywords(text)\n        for kw in keywords:\n            self.keyword_counts[kw] += 1\n\n        names = get_names(text)\n        for name in names:\n            self.name_counts[name] += 1\n", "idx": 1762}
{"namespace": "mssqlcli.packages.sqlcompletion.suggest_type", "completion": "    if full_text.startswith('\\\\i '):\n        return Path, None\n\n    sql_statement = SqlStatement(full_text, text_before_cursor)\n\n    if sql_statement.parsed is None:\n        return None, None\n\n    if sql_statement.parsed.token_first().value.lower() == 'select':\n        return suggest_type_select(sql_statement)\n    elif sql_statement.parsed.token_first().value.lower() == 'insert':\n        return suggest_type_insert(sql_statement)\n    elif sql_statement.parsed.token_first().value.lower() == 'update':\n        return suggest_type_update(sql_statement)\n    elif sql_statement.parsed.token_first().value.lower() == 'delete':\n        return suggest_type_delete(sql_statement)\n    elif sql_statement.parsed.token_first().value.lower() == 'create':\n        return suggest_type_create(sql_statement)\n    elif sql_statement.parsed.token_first().value.lower() == 'drop':\n        return suggest_type_drop(sql_statement)\n    elif sql_statement.parsed.token_first().value.lower() == 'alter':\n        return suggest_type_alter(sql_statement)\n    elif sql_statement.parsed.token_first().value.lower() == 'set':\n        return suggest_type_set(sql_statement)\n    elif sql_statement.parsed.token_first().value.lower() == 'use':\n        return suggest_type_use(sql_statement)\n    elif sql_statement.parsed.token_first().value.lower() == 'show':\n        return suggest_type_show(sql_statement)\n    elif sql_statement.parsed.token_first().value.lower() == 'describe':\n        return suggest_type_describe(sql_statement)\n    elif sql_statement.parsed.token_first().value.lower() == 'explain':\n        return suggest_type_explain(sql_statement)\n    elif sql_statement.parsed.token_first().", "idx": 1763}
{"namespace": "mssqlcli.packages.parseutils.ctes.extract_ctes", "completion": "    # Parse the SQL query using the sqlparse library\n    parsed = parse(sql)\n\n    # Check if the first meaningful token is \"WITH\", which indicates the presence of CTEs\n    if not parsed[0].tokens[0].is_keyword or parsed[0].tokens[0].value.upper() != 'WITH':\n        return [], sql\n\n    # Extract the CTEs from the query\n    ctes = []\n    for statement in parsed[0].tokens[2:]:\n        if statement.ttype is CTE:\n            # Extract the CTE name and columns\n            cte_name = statement.tokens[0].value\n            cte_columns = [token.value for token in statement.tokens[2].tokens if token.ttype is Identifier]\n            # Extract the CTE start and stop positions\n            cte_start = statement.tokens[0].start\n            cte_stop = statement.tokens[-1].stop\n            # Create a TableExpression namedtuple for the CTE\n            cte = TableExpression(cte_name, cte_columns, cte_start, cte_stop)\n            ctes.append(cte)\n\n    # Strip the CTEs from the SQL query\n    remaining_sql = sql[:ctes[0].start] + sql[ctes[-1].stop:]\n\n    return ctes, remaining_sql\n\n", "idx": 1764}
{"namespace": "mssqlcli.packages.parseutils.tables.extract_tables", "completion": "    # Parse the SQL statement using the sqlparse library\n    parsed = sqlparse.parse(sql)[0]\n\n    # Extract the table names from the parsed result\n    tables = extract_table_identifiers(extract_from_part(parsed))\n\n    # Return the extracted table names as a tuple of TableReference namedtuples\n    return tuple(tables)", "idx": 1765}
{"namespace": "googleapiclient.channel.Channel.body", "completion": "        body = {\n            \"id\": self.id,\n            \"token\": self.token,\n            \"type\": self.type,\n            \"address\": self.address,\n        }\n        if self.params:\n            body[\"params\"] = self.params\n        if self.resource_id:\n            body[\"resourceId\"] = self.resource_id\n        if self.resource_uri:\n            body[\"resourceUri\"] = self.resource_uri\n        if self.expiration:\n            body[\"expiration\"] = self.expiration\n        return body\n", "idx": 1766}
{"namespace": "googleapiclient.channel.Channel.update", "completion": "        for k, v in resp.items():\n            if k in CHANNEL_PARAMS:\n                setattr(self, CHANNEL_PARAMS[k], v)\n", "idx": 1767}
{"namespace": "googleapiclient.channel.notification_from_headers", "completion": "    headers = _upper_header_keys(headers)\n    message_number = int(headers[X_GOOG_MESSAGE_NUMBER])\n    state = headers[X_GOOG_RESOURCE_STATE]\n    resource_uri = headers[X_GOOG_RESOURCE_URI]\n    resource_id = headers[X_GOOG_RESOURCE_ID]\n    channel_id = headers[X_GOOG_CHANNEL_ID]\n\n    if channel_id != channel.id:\n        raise InvalidNotificationError(\n            \"Channel id in notification header (%s) does not match channel id in channel (%s)\"\n            % (channel_id, channel.id)\n        )\n\n    return Notification(message_number, state, resource_uri, resource_id)", "idx": 1768}
{"namespace": "googleapiclient.channel.new_webhook_channel", "completion": "    if expiration is not None:\n        expiration = int((expiration - EPOCH).total_seconds() * 1000)\n    if token is None:\n        token = str(uuid.uuid4())\n    return Channel(\n        \"web_hook\",\n        str(uuid.uuid4()),\n        token,\n        url,\n        expiration=expiration,\n        params=params,\n    )", "idx": 1769}
{"namespace": "googleapiclient.model.BaseModel._build_query", "completion": "        if self.alt_param and self.alt_param in params:\n            params = params.copy()\n            params[\"alt\"] = params[self.alt_param]\n            del params[self.alt_param]\n        query_tuples = []\n        for k, v in params.items():\n            if isinstance(v, list):\n                for el in v:\n                    query_tuples.append((k, el.encode(\"utf-8\")))\n            elif isinstance(v, str) and callable(v):\n                query_tuples.append((k, v().encode(\"utf-8\")))\n            else:\n                query_tuples.append((k, v))\n        return urllib.parse.urlencode(query_tuples)\n", "idx": 1770}
{"namespace": "googleapiclient.model.BaseModel.response", "completion": "        if resp.status == 204:\n            return self.no_content_response\n        if resp.status >= 300:\n            raise errors.HttpError(resp, content)\n        if content:\n            content = self.deserialize(content)\n        self._log_response(resp, content)\n        return content", "idx": 1771}
{"namespace": "googleapiclient.model.makepatch", "completion": "    patch = {}\n    for key in modified:\n        if key not in original:\n            patch[key] = modified[key]\n        elif isinstance(original[key], dict) and isinstance(modified[key], dict):\n            sub_patch = makepatch(original[key], modified[key])\n            if sub_patch:\n                patch[key] = sub_patch\n        elif original[key] != modified[key]:\n            patch[key] = modified[key]\n    return patch", "idx": 1772}
{"namespace": "googleapiclient._helpers.update_query_params", "completion": "    # Parse the URI into its components\n    parsed_uri = urllib.parse.urlparse(uri)\n\n    # Parse the query parameters into a dictionary\n    query_params = urllib.parse.parse_qs(parsed_uri.query)\n\n    # Check if any of the keys in the new query parameters are already in the URI\n    for key in params.keys():\n        if key in query_params:\n            raise ValueError(\n                \"URL-encoded content contains a repeated value: %s -> %s\"\n                % (key, \", \".join(params[key]))\n            )\n\n    # Update the query parameters with the new values\n    query_params.update(params)\n\n    # Encode the updated query parameters back into a string\n    new_query_string = urllib.parse.urlencode(query_params, doseq=True)\n\n    # Reconstruct the URI with the updated query parameters\n    new_uri = urllib.parse.urlunparse(\n        (\n            parsed_uri.scheme,\n            parsed_uri.netloc,\n            parsed_uri.path,\n            parsed_uri.params,\n            new_query_string,\n            parsed_uri.fragment,\n        )\n    )\n\n    return new_uri", "idx": 1773}
{"namespace": "googleapiclient._helpers._add_query_parameter", "completion": "    if value is None:\n        return url\n    parsed_url = urllib.parse.urlparse(url)\n    query_params = urllib.parse.parse_qs(parsed_url.query)\n    query_params[name] = value\n    new_query = urllib.parse.urlencode(query_params, doseq=True)\n    new_url = urllib.parse.urlunparse(\n        (\n            parsed_url.scheme,\n            parsed_url.netloc,\n            parsed_url.path,\n            parsed_url.params,\n            new_query,\n            parsed_url.fragment,\n        )\n    )\n    return new_url", "idx": 1774}
{"namespace": "gif_for_cli.display.display_txt_frames", "completion": "    if num_loops is None:\n        while True:\n            for txt_frame in txt_frames:\n                stdout.write(txt_frame)\n                stdout.write(\"\\n\")\n                stdout.flush()\n                time.sleep(seconds_per_frame)\n    else:\n        for _ in range(num_loops):\n            for txt_frame in txt_frames:\n                stdout.write(txt_frame)\n                stdout.write(\"\\n\")\n                stdout.flush()\n                time.sleep(seconds_per_frame)\n\n", "idx": 1775}
{"namespace": "pyVmomi.SoapAdapter.SoapResponseDeserializer.Deserialize", "completion": "        parser = ParserCreate(namespace_separator=NS_SEP)\n        self.deser.Deserialize(parser, resultType, nsMap=nsMap)\n        if isinstance(response, six.binary_type) or isinstance(response, six.text_type):\n            parser.Parse(response)\n        else:\n            parser.ParseFile(response)\n        result = self.deser.GetResult()\n        if self.soapFaultTag in self.deser.stack:\n            raise self.deser.stack[self.soapFaultTag]\n        return result\n\n", "idx": 1776}
{"namespace": "pyVmomi.VmomiSupport.GetRequestContext", "completion": "    if not hasattr(_threadLocalContext, 'context'):\n        _threadLocalContext.context = StringDict()\n    return _threadLocalContext.context\n\n", "idx": 1777}
{"namespace": "pycoin.bloomfilter.filter_size_required", "completion": "    size = (-1 / LOG_2 ** 2 * element_count * math.log(false_positive_probability)) / 8\n    return int(min(size, 36000))\n\n", "idx": 1778}
{"namespace": "pycoin.bloomfilter.BloomFilter.add_spendable", "completion": "        self.add_item(spendable.script_hash())\n", "idx": 1779}
{"namespace": "pycoin.bloomfilter.murmur3", "completion": "    c1 = 0xcc9e2d51\n    c2 = 0x1b873593\n    r1 = 15\n    r2 = 13\n    m = 5\n    n = 0xe6546b64\n\n    hash_ = seed\n\n    num_blocks = len(data) // 4\n    for block_start in range(0, num_blocks * 4, 4):\n        # little endian load order\n        k = struct.unpack('<L', data[block_start:block_start + 4])[0]\n        k *= c1\n        k = (k << r1) | (k >> (32 - r1))  # rotate left\n        k *= c2\n\n        hash_ ^= k\n        hash_ = (hash_ << r2) | (hash_ >> (32 - r2))  # rotate left\n        hash_ = hash_ * m + n\n\n    # Handle the last few bytes of the input array\n    extra_bytes = len(data) % 4\n    if extra_bytes >= 3:\n        hash_ ^= data[-3] << 16\n    if extra_bytes >= 2:\n        hash_ ^= data[-2] << 8\n    if extra_bytes >= 1:\n        hash_ ^= data[-1]\n\n    hash_ ^= len(data)\n    hash_ ^= hash_ >> 16\n    hash_ *= 0x85ebca6b\n    hash_ ^= hash_ >> 13\n    hash_ *= 0xc2b2ae35\n    hash_ ^= hash_ >> 16\n\n    return hash_ & 0xffffffff", "idx": 1780}
{"namespace": "pycoin.networks.registry.network_for_netcode", "completion": "    for prefix in search_prefixes():\n        try:\n            module_name = \"%s.%s\" % (prefix, symbol)\n            module = importlib.import_module(module_name)\n            network = module.network\n            if network.symbol == symbol:\n                network.module_name = module_name\n                return network\n        except ImportError:\n            pass\n    raise ValueError(\"can't find network for netcode %s\" % symbol)\n\n", "idx": 1781}
{"namespace": "pycoin.satoshi.IntStreamer.IntStreamer.int_from_script_bytes", "completion": "        if len(s) == 0:\n            return 0\n        s = s[::-1]\n        i = s[0]\n        if require_minimal and i & 0x7f == 0:\n            if len(s) == 1 or s[1] & 0x80 == 0:\n                raise ScriptError(\n                    \"Script number did not minimally encode\")\n        v = i & 0x7f\n        if i & 0x80 != 0:\n            v = -v\n        for i in s[1:]:\n            v <<= 8\n            v += i\n        return v\n", "idx": 1782}
{"namespace": "pycoin.satoshi.stackops.do_OP_RIPEMD160", "completion": "    stack.append(hashlib.new(\"ripemd160\", stack.pop()).digest())\n\n", "idx": 1783}
{"namespace": "pycoin.satoshi.stackops.do_OP_HASH160", "completion": "    stack.append(hash160(stack.pop()))\n\n", "idx": 1784}
{"namespace": "pycoin.satoshi.stackops.do_OP_HASH256", "completion": "    stack.append(hashlib.sha256(stack.pop()).digest())", "idx": 1785}
{"namespace": "pycoin.services.providers.providers_for_config_string", "completion": "    providers = []\n    for descriptor in config_string.split():\n        provider = provider_for_descriptor_and_netcode(descriptor, netcode)\n        if provider is None:\n            warnings.warn(\"can't parse provider for %s\" % descriptor)\n        else:\n            providers.append(provider)\n    return providers\n\n", "idx": 1786}
{"namespace": "pycoin.services.providers.get_default_providers_for_netcode", "completion": "    if netcode is None:\n        netcode = get_current_netcode()\n    if not hasattr(THREAD_LOCALS, \"providers_for_netcode\"):\n        THREAD_LOCALS.providers_for_netcode = {}\n    if netcode not in THREAD_LOCALS.providers_for_netcode:\n        THREAD_LOCALS.providers_for_netcode[netcode] = providers_for_netcode_from_env(netcode)\n    return THREAD_LOCALS.providers_for_netcode[netcode]", "idx": 1787}
{"namespace": "pycoin.services.providers.set_default_providers_for_netcode", "completion": "    if not hasattr(THREAD_LOCALS, \"providers\"):\n        THREAD_LOCALS.providers = {}\n    THREAD_LOCALS.providers[netcode] = provider_list", "idx": 1788}
{"namespace": "pycoin.blockchain.BlockChain.BlockChain.tuple_for_index", "completion": "        if index < 0:\n            index += len(self)\n        if index < len(self._locked_chain):\n            return self._locked_chain[index]\n        if self._longest_chain_cache is None:\n            self._longest_chain_cache = self._longest_local_block_chain()\n        if index < len(self._longest_chain_cache):\n            the_hash, parent_hash, difficulty = self._longest_chain_cache[index - len(self._locked_chain)]\n            return the_hash, parent_hash, self.weight_lookup[the_hash]\n        raise IndexError(\"block %d out of range\" % index)\n", "idx": 1789}
{"namespace": "pycoin.blockchain.ChainFinder.ChainFinder.find_ancestral_path", "completion": "        # Check if the path between h1 and h2 is already cached\n        if (h1, h2) in path_cache:\n            return path_cache[(h1, h2)]\n\n        # Find the maximum path from h1\n        path1 = self.maximum_path(h1, path_cache)\n\n        # Find the maximum path from h2\n        path2 = self.maximum_path(h2, path_cache)\n\n        # Find the common ancestor\n        common_ancestor = None\n        for i in range(min(len(path1), len(path2))):\n            if path1[i] != path2[i]:\n                common_ancestor = path1[i - 1]\n                break\n\n        # If no common ancestor is found, return None\n        if common_ancestor is None:\n            return None\n\n        # Find the ancestral path from h1 to the common ancestor\n        ancestral_path1 = path1[:path1.index(common_ancestor)]\n\n        # Find the ancestral path from h2 to the common ancestor\n        ancestral_path2 = path2[:path2.index(common_ancestor)]\n\n        # Cache the ancestral path\n        path_cache[(h1, h2)] = (ancestral_path1, ancestral_path2)\n\n        return ancestral_path1, ancestral_path2", "idx": 1790}
{"namespace": "pycoin.contrib.bech32m.bech32_encode", "completion": "    spec = Encoding.BECH32 if spec == \"BECH32\" else Encoding.BECH32M\n    combined = data + bech32_create_checksum(hrp, data, spec)\n    return hrp + '1' + ''.join([CHARSET[d] for d in combined])\n\n", "idx": 1791}
{"namespace": "pycoin.contrib.bech32m.decode", "completion": "    hrpgot, data, spec = bech32_decode(addr)\n    if hrpgot != hrp:\n        return (None, None)\n    decoded = convertbits(data[1:], 5, 8, False)\n    if decoded is None or len(decoded) < 2 or len(decoded) > 40:\n        return (None, None)\n    if data[0] > 16:\n        return (None, None)\n    if data[0] == 0 and len(decoded) != 20 and len(decoded) != 32:\n        return (None, None)\n    if data[0] == 0 and spec != Encoding.BECH32:\n        return (None, None)\n    if data[0] != 0 and spec != Encoding.BECH32M:\n        return (None, None)\n    return (data[0], decoded)\n\n", "idx": 1792}
{"namespace": "pycoin.crack.bip32.crack_bip32", "completion": "    for child in path:\n        secret_exponent = ascend_bip32(bip32_pub_node, secret_exponent, child)\n        bip32_pub_node = bip32_pub_node.subkey_for_path(str(child))\n    return bip32_pub_node.root().subkey_for_path(path)", "idx": 1793}
{"namespace": "pycoin.message.PeerAddress.ip_bin_to_ip4_addr", "completion": "    return \".\".join(str(c) for c in ip_bin)\n\n", "idx": 1794}
{"namespace": "pycoin.message.PeerAddress.PeerAddress.host", "completion": "        if self.ip_bin.startswith(IP4_HEADER):\n            return ip_bin_to_ip4_addr(self.ip_bin)\n        return ip_bin_to_ip6_addr(self.ip_bin)\n", "idx": 1795}
{"namespace": "oletools.msodde.field_is_blacklisted", "completion": "    # split into components\n    parts = FIELD_WORD_REGEX.findall(contents)\n    if not parts:\n        return False\n    cmd = parts[0].lower()\n    if cmd not in FIELD_BLACKLIST_CMDS:\n        return False\n    # check if number of arguments is correct\n    blacklist_entry = [field for field in FIELD_BLACKLIST if field[0] == cmd][0]\n    n_required_args = blacklist_entry[1]\n    n_optional_args = blacklist_entry[2]\n    n_args = len(parts) - 1\n    if n_args < n_required_args or n_args > n_required_args + n_optional_args:\n        return False\n    # check if switches are correct\n    switches = parts[1:]\n    switches_with_args = blacklist_entry[3]\n    switches_without_args = blacklist_entry[4]\n    for switch in switches:\n        if switch not in switches_with_args and switch not in switches_without_args:\n            return False\n        if switch in switches_with_args and not FIELD_SWITCH_REGEX.match(switch):\n            return False\n    # check if format switches are correct\n    format_switches = blacklist_entry[5]\n    if format_switches:\n        for switch in switches:\n            if switch in format_switches:\n                if not FIELD_SWITCH_REGEX.match(switch):\n                    return False\n    return True\n\n", "idx": 1796}
{"namespace": "oletools.ppt_record_parser.is_ppt", "completion": "    try:\n        ppt_parse(filename)\n        return True\n    except Exception:\n        return False\n\n", "idx": 1797}
{"namespace": "oletools.rtfobj.is_rtf", "completion": "    if treat_str_as_data:\n        data = arg\n    else:\n        with open(arg, 'rb') as f:\n            data = f.read(4)\n    return data.startswith(RTF_MAGIC)\n\n", "idx": 1798}
{"namespace": "oletools.oleobj.get_sane_embedded_filenames", "completion": "    # extract filename from path\n    if filename:\n        filename = os.path.basename(filename)\n    if src_path:\n        src_path = os.path.basename(src_path)\n    if tmp_path:\n        tmp_path = os.path.basename(tmp_path)\n\n    # sanitize filename\n    filename = sanitize_filename(filename, max_len=max_len)\n    src_path = sanitize_filename(src_path, max_len=max_len)\n    tmp_path = sanitize_filename(tmp_path, max_len=max_len)\n\n    # generate candidates\n    candidates = []\n    if filename:\n        candidates.append(filename)\n    if src_path:\n        candidates.append(src_path)\n    if tmp_path:\n        candidates.append(tmp_path)\n\n    # remove suffix\n    candidates = [os.path.splitext(c)[0] for c in candidates]\n\n    # add suffix\n    candidates = [c + os.path.splitext(filename)[1] for c in candidates]\n\n    # add random suffix\n    candidates.append(candidates[0] + str(random.randint(0, 1000000)))\n\n    # add noname\n    candidates.append(\"noname\" + str(noname_index))\n\n    # return candidates\n    return candidates\n\n", "idx": 1799}
{"namespace": "oletools.ooxml.get_type", "completion": "    # Check if the file is a zip file\n    if not is_zipfile(filename):\n        return DOCTYPE_NONE\n\n    # Open the zip file\n    try:\n        with ZipFile(filename, 'r') as zip_file:\n            # Check if the file contains the required files\n            if not all(file_name in zip_file.namelist() for file_name in [FILE_CONTENT_TYPES, FILE_RELATIONSHIPS]):\n                return DOCTYPE_NONE\n\n            # Read the content types file\n            content_types_xml = zip_file.read(FILE_CONTENT_TYPES)\n            content_types_root = ET.fromstring(content_types_xml)\n\n            # Check if the file is a Word document\n            if any(content_type.attrib.get('ContentType') in CONTENT_TYPES_WORD for content_type in content_types_root.findall('{*}Override')):\n                return DOCTYPE_WORD\n\n            # Check if the file is an Excel spreadsheet\n            if any(content_type.attrib.get('ContentType') in CONTENT_TYPES_EXCEL for content_type in content_types_root.findall('{*}Override')):\n                return DOCTYPE_EXCEL\n\n            # Check if the file is a PowerPoint presentation\n            if any(content_type.attrib.get('ContentType') in CONTENT_TYPES_PPT for content_type in content_types_root.findall('{*}Override')):\n                return DOCTYPE_POWERPOINT\n\n            # If none of the above conditions are met, the file is not a Word, Excel, or PowerPoint document\n            return DOCTYPE_NONE\n\n    except BadZipfile:\n        return DOCTYPE_NONE\n\n", "idx": 1800}
{"namespace": "oletools.ooxml.ZipSubFile.read", "completion": "        if self.closed:\n            raise IOError('file is closed')\n        if self.pos == self.size:\n            return b''\n        if size == -1:\n            size = self.size - self.pos\n        if size > self.size - self.pos:\n            size = self.size - self.pos\n        data = self.handle.read(size)\n        self.pos += len(data)\n        return data\n", "idx": 1801}
{"namespace": "oletools.ooxml.ZipSubFile.seek", "completion": "        if self.handle is None:\n            raise IOError('seek on closed handle')\n        if offset == io.SEEK_SET:\n            new_pos = pos\n        elif offset == io.SEEK_CUR:\n            new_pos = self.pos + pos\n        elif offset == io.SEEK_END:\n            new_pos = self.size + pos\n        else:\n            raise ValueError('invalid offset')\n        if new_pos < 0:\n            raise ValueError('negative position')\n        if new_pos > self.size:\n            raise ValueError('position beyond end of file')\n        if new_pos == self.pos:\n            return\n        if new_pos > self.pos:\n            # print('ZipSubFile: seek forward {0} bytes'.format(new_pos - self.pos))\n            self.read(new_pos - self.pos)\n        else:\n            # print('ZipSubFile: seek backward {0} bytes'.format(self.pos - new_pos))\n            self.reset()\n            self.read(new_pos)\n        self.pos = new_pos\n", "idx": 1802}
{"namespace": "oletools.ooxml.XmlParser.iter_xml", "completion": "        if not self.did_iter_all:\n            raise BadOOXML(self.filename, 'did not iterate all files')\n\n        for subfile, handle in self.iter_files(subfiles):\n            if subfile is None:\n                # single xml file\n                subfile = self.filename\n            try:\n                tree = ET.parse(handle)\n            except ET.ParseError as err:\n                raise BadOOXML(self.filename,\n                               'invalid xml in {0}: {1}'\n                               .format(subfile, str(err)))\n            root = tree.getroot()\n            for elem, depth in self._iter_xml_elem(root, need_children, tags):\n                yield subfile, elem, depth\n", "idx": 1803}
{"namespace": "oletools.oleid.OleID.check", "completion": "        if self.ole is None:\n            self.ole = olefile.OleFileIO(self.data_bytesio)\n        self.indicators = []\n        self.check_ole()\n        self.check_vba()\n        self.check_flash()\n        self.check_excel()\n        self.check_word()\n        self.check_ppt()\n        self.check_encryption()\n        self.check_suminfo()\n        self.check_ole_streams()\n        self.check_ole_summary()\n        self.check_ole_metadata()\n        self.check_ole_vba_macros()\n        self.check_ole_vba_macros_summary()\n        self.check_ole_vba_macros_code()\n        self.check_ole_vba_macros_code_summary()\n        self.check_ole_vba_macros_code_strings()\n        self.check_ole_vba_macros_code_strings_summary()\n        self.check_ole_vba_macros_code_strings_summary_by_length()\n        self.check_ole_vba_macros_code_strings_summary_by_length_and_type()\n        self.check_ole_vba_macros_code_strings_summary_by_length_and_type_and_value()\n        self.check_ole_vba_macros_code_strings_summary_by_length_and_type_and_value_and_risk()\n        self.check_ole_vba_macros_code_strings_summary_by_length_and_type_and_value_and_risk_and_description()\n        self.check_ole_vba_macros_code_strings_summary_by_length_and_type_and_value_and_risk_and_description_and_name()\n        self.check_ole_vba_macros_code_strings_summary_by_length_and_type_and_value_and_risk_and_description_and_name_and_hide_if_", "idx": 1804}
{"namespace": "tools.cgrep.is_valid_ip", "completion": "  try:\n    return nacaddr.IP(arg)\n  except nacaddr.error.AddrFormatError:\n    raise argparse.ArgumentTypeError('%s is not a valid IP address' % arg)\n\n", "idx": 1805}
{"namespace": "tools.cgrep.group_diff", "completion": "  first_obj, sec_obj = options.gmp\n  first_obj_groups = get_ip_parents(first_obj, db)\n  sec_obj_groups = get_ip_parents(sec_obj, db)\n  first_obj_groups_names = [x[0] for x in first_obj_groups]\n  sec_obj_groups_names = [x[0] for x in sec_obj_groups]\n  common = set(first_obj_groups_names).intersection(sec_obj_groups_names)\n  diff1 = set(first_obj_groups_names).difference(sec_obj_groups_names)\n  diff2 = set(sec_obj_groups_names).difference(first_obj_groups_names)\n  return common, diff1, diff2\n\n", "idx": 1806}
{"namespace": "tools.cgrep.compare_tokens", "completion": "  first_obj, sec_obj = options.cmp\n  first_nets = get_nets([first_obj], db)[0][1]\n  sec_nets = get_nets([sec_obj], db)[0][1]\n  meta = (first_obj, sec_obj, sorted(list(set(first_nets) | set(sec_nets))))\n  results = sorted(list(set(first_nets) - set(sec_nets)))\n  return meta, results\n\n", "idx": 1807}
{"namespace": "capirca.aclgen.EntryPoint", "completion": "  SetupFlags()\n  app.run(main)", "idx": 1808}
{"namespace": "capirca.lib.nacaddr.IP", "completion": "  if isinstance(ip, ipaddress._BaseNetwork):\n    if ip.version == 4:\n      return IPv4(ip, comment, token)\n    elif ip.version == 6:\n      return IPv6(ip, comment, token)\n  else:\n    ip = ipaddress.ip_network(ip, strict=strict)\n    if ip.version == 4:\n      return IPv4(ip, comment, token)\n    elif ip.version == 6:\n      return IPv6(ip, comment, token)\n\n", "idx": 1809}
{"namespace": "csvkit.cli.CSVKitUtility.run", "completion": "        if 'f' not in self.override_flags:\n            self.input_file = self._open_input_file(self.args.file)\n\n        with warnings.catch_warnings():\n            if self.args.no_header_row:\n                warnings.filterwarnings('ignore', category=RequiredHeaderError)\n\n            self.main()\n\n        if 'f' not in self.override_flags:\n            self.input_file.close()\n", "idx": 1810}
{"namespace": "csvkit.convert.fixed.fixed2csv", "completion": "    # Parse the schema file\n    schema = list(agate.csv.reader(schema))\n\n    # Create a namedtuple class for each row\n    Row = namedtuple('Row', [c[0] for c in schema])\n\n    # Read the fixed-width file\n    rows = []\n    for line in iterdecode(f, 'utf-8'):\n        if skip_lines > 0:\n            skip_lines -= 1\n            continue\n\n        # Parse the line using the schema\n        row = Row(*(line[int(c[1]):int(c[1]) + int(c[2])].strip() for c in schema))\n\n        # Add the row to the list of rows\n        rows.append(row)\n\n    # Create a CSV writer\n    if output is None:\n        output = StringIO()\n    writer = agate.csv.writer(output, **kwargs)\n\n    # Write the header row\n    writer.writerow(rows[0]._fields)\n\n    # Write the data rows\n    for row in rows:\n        writer.writerow(row)\n\n    # Return the output if it was provided, otherwise return None\n    if output is not None:\n        return None\n    else:\n        return output.getvalue()", "idx": 1811}
{"namespace": "check_dummies.find_backend", "completion": "    # Search for backends in the line\n    backends = _re_backend.findall(line)\n\n    # If no backends are found, return None\n    if not backends:\n        return None\n\n    # If only one backend is found, return it\n    if len(backends) == 1:\n        return backends[0]\n\n    # If multiple backends are found, return them joined by \"_and_\"\n    return \"_and_\".join(backends)", "idx": 1812}
{"namespace": "check_dummies.create_dummy_object", "completion": "    if name.isupper():\n        return DUMMY_CONSTANT.format(name, backend_name)\n    elif name.islower():\n        return DUMMY_FUNCTION.format(name, backend_name)\n    else:\n        return DUMMY_CLASS.format(name, backend_name)", "idx": 1813}
{"namespace": "pycorrector.en_spell.EnSpell.check_init", "completion": "        if not self.word_freq_dict:\n            self._init()\n", "idx": 1814}
{"namespace": "pycorrector.en_spell.EnSpell.candidates", "completion": "        self.check_init()\n        return (self.known([word]) or self.known(self.edits1(word)) or self.known(self.edits2(word)) or [word])\n", "idx": 1815}
{"namespace": "pycorrector.en_spell.EnSpell.correct_word", "completion": "        self.check_init()\n        candidates = self.candidates(word)\n        candidates_with_prob = [(c, self.probability(c)) for c in candidates]\n        candidates_with_prob.sort(key=operator.itemgetter(1), reverse=True)\n        return candidates_with_prob[0][0]\n", "idx": 1816}
{"namespace": "pycorrector.en_spell.EnSpell.correct", "completion": "        self.check_init()\n        details = []\n        if not text:\n            return text, details\n        if not include_symbol:\n            text = re.sub('[^0-9A-Za-z\\u4e00-\\u9fa5]', '', text)\n        blocks = whitespace_tokenize(text)\n        for block in blocks:\n            block_corrected = ''\n            if len(block) > 1 and re.search('[a-zA-Z]', block):\n                if block in self.custom_confusion:\n                    block_corrected = self.custom_confusion[block]\n                else:\n                    block_corrected = self.correct_word(block)\n            else:\n                block_corrected = block\n            details.append([block, block_corrected, 0, 0])\n            block_corrected = ' ' + block_corrected + ' '\n            text = text.replace(block, block_corrected)\n        details = sorted(details, key=lambda i: i[2])\n        corrected_text = text.replace('', '').replace('", "idx": 1817}
{"namespace": "whereami.predict.crossval", "completion": "    if X is None or y is None:\n        X, y = get_train_data(path)\n    if len(X) < folds:\n        raise ValueError(\n            \"There are not enough samples ({}). Need at least {}.\".format(\n                len(X), folds\n            )\n        )\n    if clf is None:\n        clf = get_model(path)\n    print(\"KFold folds={}, running {} times\".format(folds, n))\n    for i in range(n):\n        scores = cross_val_score(clf, X, y, cv=folds)\n        print(\"{}/{}: {}\".format(i + 1, n, sum(scores) / folds))\n    print(\"-------- total --------\")\n    print(sum(scores) / folds)\n    return sum(scores) / folds\n\n", "idx": 1818}
{"namespace": "stellar.models.Table.get_table_name", "completion": "        if not self.snapshot:\n            raise Exception('Table name requires snapshot')\n        if not self.snapshot.hash:\n            raise Exception('Snapshot hash is empty.')\n        table_name = 'stellar_{table_name}{snapshot_hash}{postfix}'.format(\n            table_name=self.table_name,\n            snapshot_hash=self.snapshot.hash,\n            postfix=postfix\n        )\n        if old:\n            return table_name\n        return 'stellar_' + hashlib.md5(\n            '{table_name}|{snapshot_hash}|{postfix}'.format(\n                table_name=self.table_name,\n                snapshot_hash=self.snapshot.hash,\n                postfix=postfix\n            ).encode('utf-8')\n        ).hexdigest()[:16]\n", "idx": 1819}
{"namespace": "chatette.utils.Singleton.reset_instance", "completion": "        cls._instance = None\n        return cls.get_or_create(*args, **kwargs)\n\n", "idx": 1820}
{"namespace": "chatette.utils.cast_to_unicode", "completion": "    if sys.version_info[0] == 2:\n        if isinstance(anything, str):\n            return unicode(anything, \"utf-8\")\n        elif isinstance(anything, list):\n            return [cast_to_unicode(elem) for elem in anything]\n        elif isinstance(anything, dict):\n            return {cast_to_unicode(k): cast_to_unicode(v) for k, v in anything.items()}\n        else:\n            return anything\n    else:\n        return anything\n\n", "idx": 1821}
{"namespace": "chatette.cli.terminal_writer.TerminalWriter.write", "completion": "        if self._file_mode == 'quiet':\n            return\n        if self._file_mode is None:\n            print(text)\n            return\n        if self.buffered_text is None:\n            self.buffered_text = text\n        else:\n            self.buffered_text += text\n", "idx": 1822}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.find_redirection_file_path", "completion": "        if len(tokens) == 0:\n            return None\n        if tokens[-1] == REDIRECTION_SYM:\n            return (RedirectionType.overwrite, None)\n        if tokens[-1] == REDIRECTION_APPEND_SYM:\n            return (RedirectionType.append, None)\n        if tokens[-1].startswith(REDIRECTION_SYM):\n            return (RedirectionType.overwrite, tokens[-1][1:])\n        if tokens[-1].startswith(REDIRECTION_APPEND_SYM):\n            return (RedirectionType.append, tokens[-1][2:])\n        return None\n", "idx": 1823}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.get_unit_type_from_str", "completion": "        if unit_type_str == \"alias\":\n            return AST.UnitType.alias\n        elif unit_type_str == \"slot\":\n            return AST.UnitType.slot\n        elif unit_type_str == \"intent\":\n            return AST.UnitType.intent\n        elif unit_type_str == \"rule\":\n            return AST.UnitType.rule\n        elif unit_type_str == \"example\":\n            return AST.UnitType.example\n        else:\n            return None\n", "idx": 1824}
{"namespace": "chatette.cli.interactive_commands.unhide_command.UnhideCommand.execute", "completion": "        if len(self.command_tokens) < 3:\n            self.print_error_message(\n                \"Expected at least 2 arguments after 'unhide'.\"\n            )\n            return\n\n        unit_type = self.command_tokens[1]\n        if unit_type not in AST.get_all_types():\n            self.print_error_message(\n                \"Unknown unit type '\" + unit_type + \"'.\"\n            )\n            return\n\n        try:\n            regex = self.command_tokens[2]\n        except IndexError:\n            regex = None\n\n        if regex is None:\n            self.unhide_all_units(unit_type)\n        else:\n            self.unhide_unit(unit_type, regex)\n", "idx": 1825}
{"namespace": "chatette.adapters.factory.create_adapter", "completion": "    if adapter_name == \"rasa\":\n        from chatette.adapters.rasa import RasaWriter\n        return RasaWriter(base_filepath)\n    elif adapter_name == \"rasa-md\":\n        from chatette.adapters.rasa_md import RasaMdWriter\n        return RasaMdWriter(base_filepath)\n    elif adapter_name == \"rasamd\":\n        from chatette.adapters.rasa_md import RasaMdWriter\n        return RasaMdWriter(base_filepath, markdown=True)\n    elif adapter_name == \"jsonl\":\n        from chatette.adapters.jsonl import JsonListWriter\n        return JsonListWriter(base_filepath)\n    else:\n        raise ValueError(\"Unknown adapter name '\" + adapter_name + \"'.\")", "idx": 1826}
{"namespace": "chatette.parsing.ChoiceBuilder.create_concrete", "completion": "        self._check_information()\n        modifiers = self._build_modifiers_repr()\n        return Choice(self.leading_space, modifiers, self.rules)\n\n", "idx": 1827}
{"namespace": "chatette.parsing.UnitRefBuilder._build_modifiers_repr", "completion": "        modifiers = super(UnitRefBuilder, self)._build_modifiers_repr()\n        if self.arg_value is not None:\n            modifiers.arg_value = self.arg_value\n        if self.variation is not None:\n            modifiers.variation = self.variation\n        return modifiers\n", "idx": 1828}
{"namespace": "chatette.parsing.UnitRefBuilder.create_concrete", "completion": "        self._check_information()\n        if self.type == UnitType.alias:\n            from chatette.units.modifiable.alias import Alias\n            return Alias(\n                self.leading_space, self._build_modifiers_repr(),\n                self.identifier\n            )\n        elif self.type == UnitType.slot:\n            from chatette.units.modifiable.slot import Slot\n            return Slot(\n                self.leading_space, self._build_modifiers_repr(),\n                self.identifier\n            )\n        elif self.type == UnitType.intent:\n            from chatette.units.modifiable.intent import Intent\n            return Intent(\n                self.leading_space, self._build_modifiers_repr(),\n                self.identifier\n            )\n        else:\n            raise ValueError(\n                \"Tried to create a concrete unit reference with an \" + \\\n                \"unknown type: \" + str(self.type)\n            )\n\n", "idx": 1829}
{"namespace": "chatette.parsing.UnitDefBuilder._build_modifiers_repr", "completion": "        modifiers = super(UnitDefBuilder, self)._build_modifiers_repr()\n        modifiers.argument_name = self.arg_name\n        return modifiers\n", "idx": 1830}
{"namespace": "chatette.parsing.AliasDefBuilder.create_concrete", "completion": "        from chatette.units.modifiable.alias import AliasDefinition\n        self._check_information()\n        if self.variation is not None:\n            return self.definitions[self.identifier].get_variation(self.variation)\n        return AliasDefinition(\n            self.identifier,\n            self.leading_space, self._build_modifiers_repr()\n        )\n", "idx": 1831}
{"namespace": "chatette.parsing.SlotDefBuilder.create_concrete", "completion": "        from chatette.units.modifiable.definitions.slot import SlotDefinition\n        self._check_information()\n        if self.variation is not None:\n            definitions = AST.get_or_create()[UnitType.slot]\n            if self.identifier in definitions:\n                return definitions[self.identifier]\n        return SlotDefinition(self.identifier, self._build_modifiers_repr())\n", "idx": 1832}
{"namespace": "chatette.parsing.IntentDefBuilder.create_concrete", "completion": "        from chatette.units.modifiable.definitions.intent import IntentDefinition\n        self._check_information()\n        if self.variation is not None:\n            definitions = AST.get_or_create()[UnitType.intent]\n            if self.identifier in definitions:\n                return definitions[self.identifier]\n        return IntentDefinition(\n            self.identifier, self._build_modifiers_repr(),\n            self.nb_training_ex, self.nb_testing_ex\n        )\n", "idx": 1833}
{"namespace": "bentoml._internal.resource.get_resource", "completion": "    if resource_kind not in _RESOURCE_REGISTRY:\n        raise BentoMLConfigException(\n            f\"Resource kind {resource_kind} is not registered. Registered resource kinds are: {list(_RESOURCE_REGISTRY.keys())}\"\n        )\n\n    resource_cls = _RESOURCE_REGISTRY[resource_kind]\n\n    if resource_kind not in resources:\n        return None\n\n    resource_spec = resources[resource_kind]\n\n    if resource_spec == \"system\":\n        resource = resource_cls.from_system()\n    else:\n        resource = resource_cls.from_spec(resource_spec)\n\n    if validate:\n        resource.validate()\n\n    return resource\n\n", "idx": 1834}
{"namespace": "bentoml._internal.resource.system_resources", "completion": "    result = {}\n    for resource_kind in _RESOURCE_REGISTRY:\n        resource = _RESOURCE_REGISTRY[resource_kind]\n        result[resource_kind] = resource.from_system()\n    return result\n\n", "idx": 1835}
{"namespace": "bentoml._internal.resource.CpuResource.from_spec", "completion": "        if isinstance(spec, float):\n            return spec\n        if isinstance(spec, int):\n            return float(spec)\n        if isinstance(spec, str):\n            if spec.endswith(\"m\"):\n                return int(spec[:-1]) / 1000\n            if spec.endswith(\"%\"):\n                return int(spec[:-1]) / 100 * psutil.cpu_count()\n            return float(spec)\n        raise ValueError(f\"Invalid CPU spec: {spec}\")\n", "idx": 1836}
{"namespace": "bentoml._internal.resource.CpuResource.from_system", "completion": "        if os.name == \"nt\":\n            return float(psutil.cpu_count())\n        else:\n            return float(psutil.cpu_count(logical=False))\n", "idx": 1837}
{"namespace": "bentoml._internal.resource.CpuResource.validate", "completion": "        if val < 0:\n            raise BentoMLConfigException(\n                f\"CPU resource limit must be non-negative, got {val}\"\n            )\n\n        system_cpu_count = cls.from_system()\n        if val > system_cpu_count:\n            raise BentoMLConfigException(\n                f\"CPU resource limit {val} exceeds system CPU count {system_cpu_count}\"\n            )\n\n", "idx": 1838}
{"namespace": "bentoml._internal.types.LazyType.get_class", "completion": "        if self._runtime_class is not None:\n            return self._runtime_class\n\n        if import_module:\n            module = __import__(self.module, fromlist=[self.qualname])\n        else:\n            module = sys.modules[self.module]\n\n        self._runtime_class = getattr(module, self.qualname)\n        return self._runtime_class\n", "idx": 1839}
{"namespace": "bentoml._internal.models.model.Model.create", "completion": "        if isinstance(name, str):\n            name = Tag(name)\n\n        if labels is None:\n            labels = {}\n\n        if options is None:\n            options = ModelOptions()\n\n        if metadata is None:\n            metadata = {}\n\n        if custom_objects is None:\n            custom_objects = {}\n\n        if not isinstance(signatures, ModelSignatures):\n            signatures = ModelSignatures(**signatures)\n\n        info = ModelInfo(\n            name=name,\n            module=module,\n            api_version=api_version,\n            signatures=signatures,\n            labels=labels,\n            options=options,\n            metadata=metadata,\n            context=context,\n        )\n\n        model_fs = fs.open_fs(f\"mem://{name.name}\")\n\n        model = cls(\n            tag=name,\n            model_fs=model_fs,\n            info=info,\n            custom_objects=custom_objects,\n            _internal=True,\n        )\n\n        model.save()\n\n        return model\n", "idx": 1840}
{"namespace": "bentoml._internal.models.model.Model.from_fs", "completion": "        with item_fs.open(MODEL_YAML_FILENAME, \"r\") as f:\n            info = ModelInfo.from_dict(yaml.safe_load(f))\n\n        return cls(\n            info.tag,\n            item_fs,\n            info,\n            _internal=True,\n        )\n", "idx": 1841}
{"namespace": "bentoml._internal.utils.metrics.linear_buckets", "completion": "    assert start > 0.0\n    assert start < end\n    assert step > 0.0\n\n    bound = start\n    buckets: list[float] = []\n    while bound < end:\n        buckets.append(bound)\n        bound += step\n\n    if len(buckets) > MAX_BUCKET_COUNT:\n        buckets = buckets[:MAX_BUCKET_COUNT]\n\n    return tuple(buckets) + (end, INF)", "idx": 1842}
{"namespace": "bentoml._internal.utils.validate_metadata", "completion": "    if not isinstance(metadata, dict):\n        raise ValueError(\"metadata must be a dict\")\n\n    for key, val in metadata.items():\n        if not isinstance(key, str):\n            raise ValueError(\"metadata keys must be strings\")\n\n        if not isinstance(val, (str, int, float, bool, date, datetime, time, timedelta)):\n            raise ValueError(\n                \"metadata values must be one of str, int, float, bool, date, datetime, time, timedelta\"\n            )\n\n", "idx": 1843}
{"namespace": "bentoml._internal.utils.analytics.usage_stats.get_serve_info", "completion": "    # The serve ID is generated by combining the current timestamp with a random string.\n    # The timestamp is converted to an integer to ensure uniqueness.\n    # The random string is generated using the secrets module, which provides a cryptographically secure source of randomness.\n    # The serve ID is then hashed using the SHA256 algorithm to ensure its uniqueness.\n    # The serve ID is then truncated to 16 characters for brevity.\n    # The timestamp is converted back to a datetime object for further use.\n    # The function returns an instance of the ServeInfo class, containing the serve ID and the timestamp of token creation.\n    serve_id = hashlib.sha256(\n        str(int(datetime.now().timestamp()) + secrets.token_hex(16)).encode()\n    ).hexdigest()[:16]\n    serve_started_timestamp = datetime.fromtimestamp(\n        int(serve_id[:10]), tz=timezone.utc\n    )\n    return ServeInfo(serve_id, serve_started_timestamp)\n\n", "idx": 1844}
{"namespace": "bentoml._internal.utils.analytics.usage_stats._track_serve_init", "completion": "    track(\n        ServeInitEvent(\n            serve_id=serve_info.serve_id,\n            from_server_api=from_server_api,\n            production=production,\n            serve_kind=serve_kind,\n            serve_started_timestamp=serve_info.serve_started_timestamp,\n            num_models=len(svc.models),\n            num_runners=len(svc.runners),\n            num_apis=len(svc.apis),\n            model_types=[m.model_type for m in svc.models],\n            runner_types=[r.runner_type for r in svc.runners],\n            api_input_types=[a.input_type for a in svc.apis],\n            api_output_types=[a.output_type for a in svc.apis],\n        )\n    )\n\n", "idx": 1845}
{"namespace": "bentoml._internal.service.service.get_valid_service_name", "completion": "    if user_provided_svc_name.lower() != user_provided_svc_name:\n        logger.warning(\n            f\"Service name {user_provided_svc_name} is not lowercase. Converting to lowercase.\"\n        )\n        user_provided_svc_name = user_provided_svc_name.lower()\n\n    # validate the service name\n    Tag(user_provided_svc_name)\n\n    return user_provided_svc_name\n\n", "idx": 1846}
{"namespace": "bentoml._internal.configuration.helpers.flatten_dict", "completion": "    for k, v in d.items():\n        if any(p in k for p in punctuation):\n            k = f'\"{k}\"'\n        new_key = parent + sep + k if parent else k\n        if isinstance(v, t.MutableMapping):\n            yield from flatten_dict(v, new_key, sep=sep)\n        else:\n            yield new_key, v\n\n", "idx": 1847}
{"namespace": "bentoml._internal.configuration.helpers.load_config_file", "completion": "    if not os.path.exists(path):\n        raise BentoMLConfigException(f\"Configuration file {path} does not exist.\")\n    with open(path, \"rb\") as f:\n        config = yaml.safe_load(f)\n    return config\n\n", "idx": 1848}
{"namespace": "bentoml._internal.configuration.helpers.expand_env_var_in_values", "completion": "    for k, v in d.items():\n        if isinstance(v, t.MutableMapping):\n            expand_env_var_in_values(v)\n        elif isinstance(v, str):\n            d[k] = expand_env_var(v)\n        elif isinstance(v, t.Sequence):\n            for i, item in enumerate(v):\n                if isinstance(item, str):\n                    v[i] = expand_env_var(item)\n        else:\n            continue\n\n", "idx": 1849}
{"namespace": "bentoml._internal.runner.strategy.DefaultStrategy.get_worker_count", "completion": "        if resource_request is None:\n            resource_request = {}\n\n        if \"nvidia.com/gpu\" in resource_request:\n            if runnable_class.SUPPORT_GPU:\n                return math.ceil(\n                    resource_request[\"nvidia.com/gpu\"] * workers_per_resource\n                )\n            else:\n                raise ValueError(\n                    f\"{runnable_class.__name__} does not support GPU.\"\n                )\n        elif \"cpu\" in resource_request:\n            if runnable_class.SUPPORT_CPU:\n                return math.ceil(resource_request[\"cpu\"] * workers_per_resource)\n            else:\n                raise ValueError(f\"{runnable_class.__name__} does not support CPU.\")\n        else:\n            raise ValueError(\n                f\"{runnable_class.__name__} does not support any known resources.\"\n            )\n", "idx": 1850}
{"namespace": "bentoml._internal.runner.strategy.DefaultStrategy.get_worker_env", "completion": "        if resource_request is None:\n            resource_request = system_resources()\n\n        # use nvidia gpu\n        nvidia_gpus = get_resource(resource_request, \"nvidia.com/gpu\")\n        if (\n            nvidia_gpus is not None\n            and len(nvidia_gpus) > 0\n            and \"nvidia.com/gpu\" in runnable_class.SUPPORTED_RESOURCES\n        ):\n            return {\n                \"CUDA_VISIBLE_DEVICES\": str(nvidia_gpus[worker_index % len(nvidia_gpus)]),\n            }\n\n        # use CPU\n        cpus = get_resource(resource_request, \"cpu\")\n        if cpus is not None and cpus > 0:\n            if \"cpu\" not in runnable_class.SUPPORTED_RESOURCES:\n                logger.warning(\n                    \"No known supported resource available for %s, falling back to using CPU.\",\n                    runnable_class,\n                )\n\n            if runnable_class.SUPPORTS_CPU_MULTI_THREADING:\n                if isinstance(workers_per_resource, float):\n                    raise ValueError(\n                        \"Fractional CPU multi threading support is not yet supported.\"\n                    )\n                num_threads = math.ceil(cpus / workers_per_resource)\n                return {env: str(num_threads) for env in THREAD_ENVS}\n\n            return {}\n\n        # this should not be reached by user since we always read system resource as default\n        raise ValueError(\n            f\"No known supported resource available for {runnable_class}. Please check your resource request. \"\n            \"Leaving it blank will allow BentoML to use system resources.\"\n        )", "idx": 1851}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.batches_to_batch", "completion": "        if batch_dim != 0:\n            raise NotImplementedError(\n                \"batch_dim must be 0 for NdarrayContainer.batches_to_batch\"\n            )\n\n        batch_size = sum(len(batch) for batch in batches)\n        batch = np.empty((batch_size, *batches[0].shape[1:]), dtype=batches[0].dtype)\n        indices = [0]\n        for i, subbatch in enumerate(batches):\n            batch[indices[-1] : indices[-1] + len(subbatch)] = subbatch\n            indices.append(indices[-1] + len(subbatch))\n        return batch, indices[:-1]\n", "idx": 1852}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.to_payload", "completion": "        if batch.ndim != 0:\n            if not batch.flags.c_contiguous and not batch.flags.f_contiguous:\n                batch = np.ascontiguousarray(batch)\n            pickle_bytes = pep574_dumps(batch)\n        else:\n            pickle_bytes = pickle.dumps(batch)\n        pickle_bytes_str = base64.b64encode(pickle_bytes).decode(\"utf-8\")\n        return cls.create_payload(pickle_bytes_str, batch_dim, batch.shape)\n", "idx": 1853}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.from_payload", "completion": "        if payload.meta[\"format\"] == \"pickle5\":\n            bs = base64.b64decode(payload.meta[\"pickle_bytes_str\"].encode(\"ascii\"))\n            return fixed_torch_loads(bs, payload.meta[\"indices\"], payload.data)\n        else:\n            return pickle.loads(payload.data)\n", "idx": 1854}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.batch_to_payloads", "completion": "        subbatches = cls.batch_to_batches(batch, indices, batch_dim)\n        return [cls.to_payload(subbatch, batch_dim) for subbatch in subbatches]\n", "idx": 1855}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.from_batch_payloads", "completion": "        batches = [cls.from_payload(payload) for payload in payloads]\n        batch, indices = cls.batches_to_batch(batches, batch_dim)\n        return batch, indices\n\n", "idx": 1856}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.to_payload", "completion": "        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n\n        if isinstance(batch, pd.Series):\n            batch = batch.to_frame()\n\n        meta = {\"format\": \"pickle5\"}\n\n        bs: bytes\n        concat_buffer_bs: bytes\n        indices: list[int]\n        bs, concat_buffer_bs, indices = pep574_dumps(batch)\n        bs_str = base64.b64encode(bs).decode(\"ascii\")\n\n        if indices:\n            meta[\"with_buffer\"] = True\n            meta[\"pickle_bytes_str\"] = bs_str\n            meta[\"indices\"] = indices\n            data = concat_buffer_bs\n        else:\n            meta[\"with_buffer\"] = False\n            data = bs\n\n        return cls.create_payload(\n            data,\n            batch.shape[batch_dim],\n            meta,\n        )\n", "idx": 1857}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.from_payload", "completion": "        import pandas as pd\n\n        format = payload.meta.get(\"format\", \"default\")\n        if format == \"pickle5\":\n            if payload.meta[\"with_buffer\"]:\n                bs_str = t.cast(str, payload.meta[\"pickle_bytes_str\"])\n                bs = base64.b64decode(bs_str)\n                indices = t.cast(t.List[int], payload.meta[\"indices\"])\n                return t.cast(\"ext.PdDataFrame\", pep574_loads(bs, payload.data, indices))\n            else:\n                return t.cast(\"ext.PdDataFrame\", fixed_torch_loads(payload.data))\n\n        return pd.read_pickle(io.BytesIO(payload.data))\n", "idx": 1858}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batch_to_payloads", "completion": "        batches = cls.batch_to_batches(batch, indices, batch_dim)\n\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads\n", "idx": 1859}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.from_batch_payloads", "completion": "        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)\n\n", "idx": 1860}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.to_payload", "completion": "        if isinstance(batch, t.Generator):\n            batch = list(batch)\n\n        bs = pickle.dumps(batch)\n        batch_size = len(batch)\n        return cls.create_payload(bs, batch_size)\n", "idx": 1861}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batch_to_payloads", "completion": "        batches = cls.batch_to_batches(batch, indices, batch_dim)\n\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads\n", "idx": 1862}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.from_batch_payloads", "completion": "        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)", "idx": 1863}
{"namespace": "sslyze.cli.server_string_parser.CommandLineServerStringParser.parse_server_string", "completion": "        ip = None\n        if \"{\" in server_str:\n            ip_end = server_str.find(\"}\")\n            if ip_end == -1:\n                raise InvalidServerStringError(server_str, cls.SERVER_STRING_ERROR_BAD_PORT)\n\n            ip = server_str[1:ip_end]\n            server_str = server_str[ip_end + 1 :]\n\n        if \"[\" in server_str:\n            return cls._parse_ipv6_server_string(server_str)\n\n        if ip and \"[\" in ip:\n            return cls._parse_ipv6_ip_address(ip)\n\n        return cls._parse_ipv4_server_string(server_str)\n", "idx": 1864}
{"namespace": "sslyze.plugins.heartbleed_plugin._HeartbleedCliConnector.result_to_console_output", "completion": "        result_as_txt = [cls._format_title(cls._cli_option)]\n        if result.is_vulnerable_to_heartbleed:\n            heartbleed_txt = \"VULNERABLE - Server is vulnerable to Heartbleed\"\n        else:\n            heartbleed_txt = \"OK - Server is not vulnerable to Heartbleed\"\n        result_as_txt.append(heartbleed_txt)\n        return result_as_txt\n\n", "idx": 1865}
{"namespace": "sslyze.plugins.http_headers_plugin._HttpHeadersCliConnector.result_to_console_output", "completion": "        result_as_txt = [cls._format_title(\"HTTP Security Headers\")]\n\n        if result.http_error_trace:\n            result_as_txt.append(cls._format_field(\"\", \"Error when sending the HTTP request\"))\n            for line in result.http_error_trace.format(chain=False):\n                result_as_txt.append(cls._format_field(\"\", line))\n            return result_as_txt\n\n        if result.http_path_redirected_to:\n            result_as_txt.append(\n                cls._format_field(\"\", \"HTTP request was redirected to {}\".format(result.http_path_redirected_to))\n            )\n\n        if result.strict_transport_security_header:\n            result_as_txt.append(cls._format_title(\"Strict-Transport-Security Header\"))\n            result_as_txt.append(\n                cls._format_field(\n                    \"Max Age:\",\n                    \"{} seconds\".format(result.strict_transport_security_header.max_age)\n                    if result.strict_transport_security_header.max_age\n                    else \"None\",\n                )\n            )\n            result_as_txt.append(\n                cls._format_field(\n                    \"Preload:\",\n                    \"Yes\" if result.strict_transport_security_header.preload else \"No\",\n                )\n            )\n            result_as_txt.append(\n                cls._format_field(\n                    \"Include Subdomains:\",\n                    \"Yes\" if result.strict_transport_security_header.include_subdomains else \"No\",\n                )\n            )\n\n        return result_as_txt\n\n", "idx": 1866}
{"namespace": "sslyze.plugins.http_headers_plugin._detect_http_redirection", "completion": "    # Check if the response contains a redirection to the same server\n    if http_response.status == 301 or http_response.status == 302:\n        # Get the location header\n        location_header = http_response.getheader(\"Location\")\n        if location_header:\n            # Parse the location header\n            parsed_location = urlsplit(location_header)\n            # Check if the location is the same as the server\n            if parsed_location.hostname == server_host_name and parsed_location.port == server_port:\n                # Return the path to the new location\n                return parsed_location.path\n    # Return None if no redirection to the same server is found\n    return None\n\n", "idx": 1867}
{"namespace": "sslyze.plugins.session_renegotiation_plugin._SessionRenegotiationCliConnector.result_to_console_output", "completion": "        result_txt = []\n        result_txt.append(\"VULNERABLE TO CLIENT-INITIATED RENEGOTIATION DoS (CVE-2009-3555):\")\n        result_txt.append(cls._format_bool_output(result.is_vulnerable_to_client_renegotiation_dos))\n        result_txt.append(\"Secure Renegotiation:\")\n        result_txt.append(cls._format_bool_output(result.supports_secure_renegotiation))\n        return result_txt\n\n", "idx": 1868}
{"namespace": "sslyze.plugins.certificate_info._cli_connector._CertificateInfoCliConnector.result_to_console_output", "completion": "        result_txt = [\n            f\"\\nScan results for {result.server_location.hostname}:{result.server_location.port} - {result.scan_command.get_title()}\"\n        ]\n\n        # Add the hostname sent for SNI\n        if result.server_location.tls_server_name_indication:\n            result_txt.append(f\"\\n\\tTLS Server Name Indication: {result.server_location.tls_server_name_indication}\")\n\n        # Add the number of certificates detected\n        result_txt.append(f\"\\n\\tNumber of certificates detected: {len(result.certificate_deployments)}\")\n\n        # Iterate through each certificate deployment\n        for cert_deployment in result.certificate_deployments:\n            result_txt.append(cls._format_certificate_deployment(cert_deployment))\n\n        return result_txt\n", "idx": 1869}
{"namespace": "sslyze.plugins.certificate_info._cli_connector._get_name_as_short_text", "completion": "    if name_field.get_attributes_for_oid(x509.NameOID.COMMON_NAME):\n        return name_field.get_attributes_for_oid(x509.NameOID.COMMON_NAME)[0].value\n    else:\n        return str(name_field)", "idx": 1870}
{"namespace": "sslyze.plugins.certificate_info._symantec.SymantecDistructTester.get_distrust_timeline", "completion": "        # Check if the certificate chain is empty\n        if not verified_certificate_chain:\n            return None\n\n        # Get the first certificate in the chain\n        first_certificate = verified_certificate_chain[0]\n\n        # Check if the first certificate is a Symantec root certificate\n        if not cls._is_symantec_root_certificate(first_certificate):\n            return None\n\n        # Check if the first certificate is blacklisted\n        if cls._is_blacklisted(first_certificate):\n            return SymantecDistrustTimelineEnum.MARCH_2018\n\n        # Check if the first certificate is whitelisted\n        if cls._is_whitelisted(first_certificate):\n            return SymantecDistrustTimelineEnum.SEPTEMBER_2018\n\n        # If the first certificate is neither blacklisted nor whitelisted, check the second certificate\n        if len(verified_certificate_chain) > 1:\n            second_certificate = verified_certificate_chain[1]\n\n            # Check if the second certificate is blacklisted\n            if cls._is_blacklisted(second_certificate):\n                return SymantecDistrustTimelineEnum.MARCH_2018\n\n            # Check if the second certificate is whitelisted\n            if cls._is_whitelisted(second_certificate):\n                return SymantecDistrustTimelineEnum.SEPTEMBER_2018\n\n        # If neither the first nor the second certificate is blacklisted or whitelisted, return None\n        return None\n", "idx": 1871}
{"namespace": "sslyze.plugins.certificate_info._certificate_utils.parse_subject_alternative_name_extension", "completion": "    try:\n        san_extension = certificate.extensions.get_extension_for_oid(\n            ExtensionOID.SUBJECT_ALTERNATIVE_NAME\n        )\n    except ExtensionNotFound:\n        return SubjectAlternativeNameExtension(dns_names=[], ip_addresses=[])\n\n    san_extension_value = cast(SubjectAlternativeName, san_extension.value)\n    dns_names = [\n        str(dns_name) for dns_name in san_extension_value.get_values_for_type(DNSName)\n    ]\n    ip_addresses = [\n        str(ip_address)\n        for ip_address in san_extension_value.get_values_for_type(IPAddress)\n    ]\n\n    return SubjectAlternativeNameExtension(\n        dns_names=dns_names, ip_addresses=ip_addresses\n    )\n\n", "idx": 1872}
{"namespace": "sslyze.plugins.certificate_info._cert_chain_analyzer._certificate_matches_hostname", "completion": "    try:\n        # Extract the names from the certificate\n        names = certificate.subject.get_attributes_for_oid(NameOID.COMMON_NAME)\n        # Create a dictionary with the properly formatted names\n        names_dict = {name.value: name.value for name in names}\n        # Check if the server_hostname matches any of the names in the certificate\n        match_hostname(certificate, server_hostname)\n        return True\n    except CertificateError:\n        return False", "idx": 1873}
