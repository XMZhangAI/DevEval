{"namespace": "benedict.utils.type_util.is_json_serializable", "completion": "    import json\n    try:\n        json.dumps(val)\n        return True\n    except (TypeError, OverflowError):\n        return False\n\n", "idx": 0}
{"namespace": "feedparser.urls.convert_to_idn", "completion": "    try:\n        url.encode('ascii')\n    except UnicodeEncodeError:\n        parts = urllib.parse.urlsplit(url)\n        try:\n            host = parts[1].encode('idna').decode('ascii')\n        except UnicodeError:\n            pass\n        else:\n            if host.startswith('['):\n                # IPv6 addresses are not converted\n                return url\n            url = urllib.parse.urlunsplit((parts[0], host, parts[2], parts[3], parts[4]))\n    return url\n\n", "idx": 1}
{"namespace": "mistune.toc.add_toc_hook", "completion": "    if not heading_id:\n        heading_id = _default_heading_id\n\n    def _add_toc_item(toc, level, item):\n        if min_level <= level <= max_level:\n            toc.append((level, item))\n\n    def _add_toc_hook(state, _):\n        state.env['toc'] = []\n        state.add_toc_item = _add_toc_item\n\n    def _add_toc_ul(state, content):\n        if not hasattr(state.env, 'toc'):\n            return content\n\n        toc = state.env.toc\n        ul = toc_to_ul(toc, heading_id)\n        content.append(ul)\n\n    md.treeprocessors.register(TocTreeprocessor(md), 'toc', 20)\n    md.treeprocessors.register(TocHTMLTreeprocessor(md), 'toc', 20)\n    md.before_env_init(_add_toc_hook)\n    md.before_finalize(_add_toc_ul)\n\n", "idx": 2}
{"namespace": "mistune.plugins.table.table_in_quote", "completion": "    md.block.rules.table = md.block.rules.table[:2] + md.block.rules.table[2:]\n    md.block.rules.nptable = md.block.rules.nptable[:2] + md.block.rules.nptable[2:]\n\n", "idx": 3}
{"namespace": "mistune.plugins.table.table_in_list", "completion": "    md.block.insert_rule(md.block.list_rules, 'table', before='paragraph')\n    md.block.insert_rule(md.block.list_rules, 'nptable', before='paragraph')", "idx": 4}
{"namespace": "xmnlp.utils.parallel_handler", "completion": "    if not isinstance(texts, list):\n        raise ValueError(\"You should pass a list of texts\")\n\n    with futures.ThreadPoolExecutor(max_workers=n_jobs) as executor:\n        for result in executor.map(partial(callback, **kwargs), texts):\n            yield result", "idx": 5}
{"namespace": "parsel.utils.shorten", "completion": "    if width < 0:\n        raise ValueError(\"width must be equal or greater than 0\")\n\n    if not text:\n        return text\n\n    if len(text) > abs(width):\n        if width < 0:\n            return suffix\n        else:\n            return text[:width - len(suffix)] + suffix\n\n    return text", "idx": 6}
{"namespace": "parsel.xpathfuncs.set_xpathfunc", "completion": "    ns = etree.FunctionNamespace(None)\n    if func is not None:\n        ns[fname] = func\n    else:\n        del ns[fname]\n\n", "idx": 7}
{"namespace": "dominate.dom_tag._get_thread_context", "completion": "  if greenlet is not None:\n    return hash((threading.current_thread(), greenlet.getcurrent()))\n  else:\n    return hash(threading.current_thread())\n\n", "idx": 8}
{"namespace": "dominate.util.system", "completion": "  import subprocess\n  import shlex\n\n  if data is None:\n    p = subprocess.Popen(shlex.split(cmd), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n  else:\n    p = subprocess.Popen(shlex.split(cmd), stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE)\n    p.stdin.write(data)\n\n  out, err = p.communicate()\n  return out.decode('utf-8')\n\n", "idx": 9}
{"namespace": "dominate.util.url_unescape", "completion": "  def _repl(m):\n    return unichr(int(m.group(1), 16))\n\n  return re.sub(r'%(?P<hex>[0-9A-Fa-f]{2})', _repl, data)\n\n", "idx": 10}
{"namespace": "rows.fields.DatetimeField.serialize", "completion": "        if value is None:\n            return \"\"\n\n        return six.text_type(value.strftime(\"%Y-%m-%d %H:%M:%S\"))\n", "idx": 11}
{"namespace": "rows.fields.Field.serialize", "completion": "        if value is None:\n            return None\n        return str(value)\n", "idx": 12}
{"namespace": "rows.fields.EmailField.serialize", "completion": "        if value is None:\n            return \"\"\n        else:\n            return six.text_type(value)\n", "idx": 13}
{"namespace": "rows.fields.as_string", "completion": "    if isinstance(value, six.binary_type):\n        return value.decode(\"utf-8\")\n    elif isinstance(value, six.text_type):\n        return value\n    else:\n        return six.text_type(value)\n\n", "idx": 14}
{"namespace": "rows.fields.get_items", "completion": "    def get_item(obj):\n        return tuple(obj[index] if index in obj else None for index in indexes)\n\n    return get_item\n\n", "idx": 15}
{"namespace": "pycorrector.proper_corrector.load_dict_file", "completion": "    words = {}\n    if path and os.path.exists(path):\n        with open(path, 'r', encoding='utf-8') as f:\n            for w in f:\n                w = w.strip()\n                if w.startswith('#'):\n                    continue\n                if w.count(' ') == 1:\n                    parts = w.split()\n                    if parts and len(parts) == 2 and parts[0] and parts[1]:\n                        words[parts[0]] = parts[1]\n    return words\n\n", "idx": 16}
{"namespace": "natasha.span.envelop_spans", "completion": "    span_index = 0\n    envelope_index = 0\n    while span_index < len(spans) and envelope_index < len(envelopes):\n        if spans[span_index].start < envelopes[envelope_index].start:\n            span_index += 1\n        elif spans[span_index].start > envelopes[envelope_index].stop:\n            envelope_index += 1\n        else:\n            start = spans[span_index].start\n            stop = spans[span_index].stop\n            while span_index < len(spans) and spans[span_index].stop <= envelopes[envelope_index].stop:\n                stop = spans[span_index].stop\n                span_index += 1\n            yield Span(start, stop, envelopes[envelope_index].type)\n\n", "idx": 17}
{"namespace": "googleapiclient._helpers.parse_unique_urlencoded", "completion": "    parsed_content = urllib.parse.parse_qs(content)\n    for key in parsed_content:\n        if len(parsed_content[key]) > 1:\n            raise ValueError(\"URL-encoded content contains a repeated key: {}\".format(key))\n    return {key: value[0] for key, value in parsed_content.items()}\n\n", "idx": 18}
{"namespace": "jinja2.async_utils.auto_aiter", "completion": "    if hasattr(iterable, \"__aiter__\"):\n        async for item in iterable:\n            yield item\n    else:\n        for item in iterable:\n            yield item", "idx": 19}
{"namespace": "jinja2.utils.consume", "completion": "    for _ in iterable:\n        pass\n\n", "idx": 20}
{"namespace": "pycorrector.utils.tokenizer.segment", "completion": "    if pos:\n        if cut_type == 'word':\n            words = posseg.lcut(sentence)\n            words = [i for i in words if i.flag != 'x']\n            return words\n        elif cut_type == 'char':\n            chars = list(sentence)\n            return chars\n    else:\n        if cut_type == 'word':\n            words = jieba.lcut(sentence)\n            return words\n        elif cut_type == 'char':\n            chars = list(sentence)\n            return chars\n\n", "idx": 21}
{"namespace": "jinja2.utils.object_type_repr", "completion": "    if obj is Ellipsis:\n        return \"Ellipsis\"\n    elif obj is NotImplemented:\n        return \"NotImplemented\"\n    elif obj is None:\n        return \"None\"\n    elif obj is False:\n        return \"False\"\n    elif obj is True:\n        return \"True\"\n\n    module = type(obj).__module__\n    if module is None or module == str.__class__.__module__:\n        return type(obj).__name__\n    return f\"{module}.{type(obj).__name__}\"\n\n", "idx": 22}
{"namespace": "jinja2.utils.LRUCache.setdefault", "completion": "        try:\n            return self[key]\n        except KeyError:\n            self[key] = default\n            return default\n", "idx": 23}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_word_freq", "completion": "        word_freq = {}\n        for w in list_of_words:\n            if w in word_freq:\n                word_freq[w] += 1\n            else:\n                word_freq[w] = 1\n        return word_freq\n", "idx": 24}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_average_probability_of_words", "completion": "        content_words_count = len(content_words_in_sentence)\n        if content_words_count == 0:\n            return 0\n        doc_word_count = sum(word_freq_in_doc[w] for w in content_words_in_sentence)\n        return doc_word_count / content_words_count\n", "idx": 25}
{"namespace": "sumy.summarizers.lex_rank.LexRankSummarizer._compute_idf", "completion": "        idf_metrics = {}\n        sentences_count = len(sentences)\n\n        for sentence in sentences:\n            for term in sentence:\n                if term not in idf_metrics:\n                    idf_metrics[term] = 1\n                else:\n                    idf_metrics[term] += 1\n\n        for term, count in idf_metrics.items():\n            idf_metrics[term] = math.log(sentences_count / float(count))\n\n        return idf_metrics\n", "idx": 26}
{"namespace": "sumy.summarizers.lex_rank.LexRankSummarizer.cosine_similarity", "completion": "        common_words = frozenset(sentence1) & frozenset(sentence2)\n\n        numerator = 0.0\n        for term in common_words:\n            numerator += tf1[term]*tf2[term] * idf_metrics[term]**2\n\n        denominator1 = sum((tf1[t]*idf_metrics[t])**2 for t in sentence1)\n        denominator2 = sum((tf2[t]*idf_metrics[t])**2 for t in sentence2)\n\n        if denominator1 > 0 and denominator2 > 0:\n            return numerator / (math.sqrt(denominator1) * math.sqrt(denominator2))\n        else:\n            return 0.0\n", "idx": 27}
{"namespace": "sumy.evaluation.rouge._get_ngrams", "completion": "    ngram_set = set()\n    text_length = len(text)\n    max_index_ngram_start = text_length - n\n    for i in range(max_index_ngram_start + 1):\n        ngram_set.add(tuple(text[i:i + n]))\n    return ngram_set\n\n", "idx": 28}
{"namespace": "sumy.evaluation.rouge._split_into_words", "completion": "    if not isinstance(sentences, list):\n        raise ValueError(\"Input parameter 'sentences' must be a list\")\n\n    if len(sentences) == 0:\n        raise ValueError(\"Input parameter 'sentences' cannot be an empty list\")\n\n    for sentence in sentences:\n        if not isinstance(sentence, Sentence):\n            raise ValueError(\"Object in collection must be of type Sentence\")\n\n    words = []\n    for sentence in sentences:\n        for word in sentence.words:\n            words.append(word)\n\n    return words\n\n", "idx": 29}
{"namespace": "falcon.inspect.register_router", "completion": "    def _register_router(inspect_function):\n        if router_class in _supported_routers:\n            raise ValueError(\n                'Router class {} is already registered'.format(router_class)\n            )\n        _supported_routers[router_class] = inspect_function\n        return inspect_function\n\n    return _register_router\n\n", "idx": 30}
{"namespace": "falcon.inspect.inspect_compiled_router", "completion": "    routes = []\n    for path, method_map in router._roots.items():\n        for method, resource in method_map.items():\n            name = resource.__class__.__name__\n            source_info = _get_source_info(resource)\n            uri_template = path.decode()\n            route = RouteInfo(name, source_info, uri_template, [])\n            routes.append(route)\n    return routes\n\n", "idx": 31}
{"namespace": "falcon.inspect._is_internal", "completion": "    return obj.__module__.startswith('falcon.')\n\n", "idx": 32}
{"namespace": "falcon.cmd.inspect_app.load_app", "completion": "    try:\n        module, instance = args.app_module.split(':', 1)\n    except ValueError:\n        parser.error('Invalid app_module: %s' % args.app_module)\n\n    try:\n        mod = importlib.import_module(module)\n    except ImportError:\n        parser.error('Could not import module: %s' % module)\n\n    try:\n        app = getattr(mod, instance)\n    except AttributeError:\n        parser.error('Could not get instance \"%s\" from module: %s' % (instance, module))\n\n    if not isinstance(app, falcon.App):\n        try:\n            app = app()\n        except Exception:\n            pass\n\n    if not isinstance(app, falcon.App):\n        parser.error('App is not an instance of falcon.App: %s' % args.app_module)\n\n    return app\n\n", "idx": 33}
{"namespace": "falcon.cmd.inspect_app.make_parser", "completion": "    parser = argparse.ArgumentParser(description='Print out the routes of an App instance.')\n    parser.add_argument('-r', '--router', action='store_true', help='Print the router of the application.')\n    parser.add_argument('-v', '--verbose', action='store_true', help='Print the routes of the application in a verbose way.')\n    parser.add_argument('-i', '--internal', action='store_true', help='Print the routes of the application including the internal ones.')\n    parser.add_argument('app_module', help='The application module.')\n    return parser\n\n", "idx": 34}
{"namespace": "falcon.util.uri.unquote_string", "completion": "    if not isinstance(quoted, str):\n        raise TypeError('Input must be a string.')\n\n    if len(quoted) < 2:\n        return quoted\n\n    if quoted[0] != '\"' or quoted[-1] != '\"':\n        return quoted\n\n    # Remove the quotes\n    unquoted = quoted[1:-1]\n\n    # Replace escaped characters with their unescaped version\n    unquoted = unquoted.replace('\\\\\\\\', '\\\\')\n    unquoted = unquoted.replace('\\\\\"', '\"')\n\n    return unquoted\n\n", "idx": 35}
{"namespace": "falcon.util.misc.get_argnames", "completion": "    sig = inspect.signature(func)\n    args = [\n        p.name for p in sig.parameters.values()\n        if p.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD\n    ]\n    return args\n\n", "idx": 36}
{"namespace": "falcon.testing.client._is_asgi_app", "completion": "    if not callable(app):\n        return False\n\n    try:\n        signature = inspect.signature(app)\n    except (ValueError, TypeError):\n        # inspect.signature fails in some cases\n        return False\n\n    # NOTE(kgriffs): Pyramid's Configurator is a callable, but it won't\n    #   be recognized as an ASGI app because it won't accept the\n    #   required number of arguments.\n    #\n    #   We don't check for the exact class name to avoid a strict\n    #   Pyramid dependency.\n    #\n    # NOTE(CaselIT): Pyramid's testing framework wraps the WSGI app\n    #   in a callable that takes three arguments, so we need to check\n    #   for that case.\n    if 'Configurator' in str(app.__class__):\n        return False\n\n    # NOTE(kgriffs): The framework may have passed in a partial, so we\n    #   need to introspect the original callable.\n    while isinstance(app, functools.partial):\n        app = app.func\n\n    # NOTE(kgriffs): We don't care about the \"self\" or \"cls\" arguments as those\n    #   are simply instance and class methods, respectively.\n    required_args = [\n        p\n        for p in signature.parameters.values()\n        if p.name != 'self' and p.name != 'cls' and p.default == inspect._empty\n    ]  # how to check for required arguments\n\n    return len(required_args) == 3\n\n", "idx": 37}
{"namespace": "falcon.routing.converters.UUIDConverter.convert", "completion": "        try:\n            return uuid.UUID(value)\n        except ValueError:\n            return None", "idx": 38}
{"namespace": "rest_framework_simplejwt.utils.make_utc", "completion": "    if settings.USE_TZ and is_naive(dt):\n        return make_aware(dt, timezone.utc)\n    return dt\n\n", "idx": 39}
{"namespace": "boto.sdb.db.sequence.fib", "completion": "    return cv + lv\n\n", "idx": 40}
{"namespace": "boto.s3.website.RoutingRules.add_rule", "completion": "        self.append(rule)\n        return self\n", "idx": 41}
{"namespace": "boto.cloudfront.distribution.Distribution._canned_policy", "completion": "        return '{\"Statement\":[{\"Resource\":\"%(resource)s\",\"Condition\":{\"DateLessThan\":{\"AWS:EpochTime\":%(expires)s}}}]}' % {\"resource\": resource, \"expires\": expires}\n", "idx": 42}
{"namespace": "boto.cloudfront.invalidation.InvalidationBatch.escape", "completion": "        if not p.startswith('/'):\n            p = '/' + p\n        return urllib.parse.quote(p, safe='/*')\n", "idx": 43}
{"namespace": "proxybroker.utils.get_status_code", "completion": "    try:\n        return int(resp[start:stop])\n    except Exception:\n        return 400\n\n", "idx": 44}
{"namespace": "authlib.oauth2.rfc6749.util.scope_to_list", "completion": "    if isinstance(scope, (set, tuple, list)):\n        return [to_unicode(s) for s in scope]\n    if scope is None:\n        return scope\n    return scope.split(\" \")\n\n", "idx": 45}
{"namespace": "authlib.common.encoding.to_unicode", "completion": "    if x is None:\n        return None\n    if isinstance(x, str):\n        return x\n    if isinstance(x, (int, float)):\n        return str(x)\n    if isinstance(x, bytes):\n        return x.decode(charset, errors)\n    return str(x)\n\n", "idx": 46}
{"namespace": "authlib.common.encoding.to_bytes", "completion": "    if x is None:\n        return None\n    if isinstance(x, bytes):\n        return x\n    if isinstance(x, str):\n        return x.encode(charset, errors)\n    if isinstance(x, (int, float)):\n        return str(x).encode(charset, errors)\n    raise TypeError('Unsupported type %s' % type(x))\n\n", "idx": 47}
{"namespace": "authlib.common.encoding.urlsafe_b64decode", "completion": "    s = to_bytes(s)\n    s += b'=' * (-len(s) % 4)\n    return base64.urlsafe_b64decode(s)\n\n", "idx": 48}
{"namespace": "csvs_to_sqlite.utils.table_exists", "completion": "    c = conn.cursor()\n    c.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='{}'\".format(table))\n    return True if c.fetchone() else False\n\n", "idx": 49}
{"namespace": "sqlitedict.SqliteDict.get_tablenames", "completion": "        # Check if the file exists.\n        if not os.path.exists(filename):\n            raise IOError('file {} does not exist'.format(filename))\n\n        # Connect to the database.\n        conn = sqlite3.connect(filename)\n\n        # Get the names of the tables in the database.\n        cursor = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n        tablenames = [x[0] for x in cursor.fetchall()]\n\n        # Close the connection to the database.\n        conn.close()\n\n        # Return the names of the tables in the database.\n        return tablenames\n", "idx": 50}
{"namespace": "litecli.packages.parseutils.query_starts_with", "completion": "    # Convert all prefixes to lowercase\n    prefixes = [prefix.lower() for prefix in prefixes]\n\n    # Format the query to lowercase and remove comments\n    query = query.lower()\n    query = remove_comments(query)\n\n    # If the formatted query is empty, return False\n    if not query:\n        return False\n\n    # Split the query into words\n    words = query.split()\n\n    # Return True if the first word of the query is in the list of prefixes\n    return words[0] in prefixes\n\n", "idx": 51}
{"namespace": "rest_framework.negotiation.DefaultContentNegotiation.filter_renderers", "completion": "        renderers = [renderer for renderer in renderers if format in renderer.format]\n        if not renderers:\n            raise Http404\n        return renderers\n", "idx": 52}
{"namespace": "rest_framework.templatetags.rest_framework.as_string", "completion": "    if value is None:\n        return \"\"\n    return str(value)\n\n", "idx": 53}
{"namespace": "rest_framework.templatetags.rest_framework.add_nested_class", "completion": "    if isinstance(value, dict):\n        return 'class=nested'\n    elif isinstance(value, list):\n        for item in value:\n            if isinstance(item, dict):\n                return 'class=nested'\n    return ''\n\n", "idx": 54}
{"namespace": "pyramid.session.PickleSerializer.loads", "completion": "        try:\n            return pickle.loads(bstruct)\n        except Exception as e:\n            raise ValueError(e)\n", "idx": 55}
{"namespace": "pyramid.testing.DummySession.flash", "completion": "        if not allow_duplicate and msg in self.get(queue, []):\n            return\n\n        self.setdefault(queue, []).append(msg)\n\n", "idx": 56}
{"namespace": "pyramid.testing.DummySession.pop_flash", "completion": "        storage = self.pop('_f_' + queue, [])\n        return storage\n", "idx": 57}
{"namespace": "pyramid.testing.DummySession.peek_flash", "completion": "        storage = self.get('_f_' + queue, [])\n        return storage\n\n", "idx": 58}
{"namespace": "pyramid.testing.DummySession.new_csrf_token", "completion": "        token = '0123456789012345678901234567890123456789'\n        self['csrf_token'] = token\n        return token\n\n", "idx": 59}
{"namespace": "pyramid.view.view_defaults", "completion": "    def wrapper(wrapped):\n        wrapped.__view_defaults__ = settings\n        return wrapped\n\n    return wrapper\n\n", "idx": 60}
{"namespace": "pyramid.util.bytes_", "completion": "    if isinstance(s, str):\n        return s.encode(encoding, errors)\n    return s\n\n", "idx": 61}
{"namespace": "pyramid.scripts.common.parse_vars", "completion": "    vars = {}\n    for arg in args:\n        if '=' in arg:\n            key, value = arg.split('=')\n            vars[key] = value\n    return vars\n\n", "idx": 62}
{"namespace": "pyramid.scripts.pviews.PViewsCommand._find_multi_routes", "completion": "        infos = []\n        for route in mapper.get_routes():\n            match = route.match(request)\n            if match is not None:\n                info = {'match': match, 'route': route}\n                infos.append(info)\n        return infos\n", "idx": 63}
{"namespace": "pyramid.scripts.pserve.PServeCommand.guess_server_url", "completion": "        if not server_name:\n            server_name = 'main'\n\n        settings = loader.get_settings('server:%s' % server_name, global_conf)\n        port = settings.get('port')\n        if port:\n            return 'http://127.0.0.1:%s' % port\n", "idx": 64}
{"namespace": "aiohappybase._util.pep8_to_camel_case", "completion": "    chunks = name.split('_')\n    converted = ''.join(chunk.capitalize() for chunk in chunks)\n    return converted if initial else converted[0].lower() + converted[1:]\n\n", "idx": 65}
{"namespace": "aiohappybase._util.bytes_increment", "completion": "    assert isinstance(b, bytes)\n    b = bytearray(b)\n    for i in range(len(b) - 1, -1, -1):\n        if b[i] != 0xff:\n            b[i] += 1\n            return bytes(b[:i+1])\n    return None\n\n", "idx": 66}
{"namespace": "mssqlcli.config.ensure_dir_exists", "completion": "    try:\n        if not exists(path):\n            os.makedirs(path)\n    except OSError as exception:\n        if exception.errno != errno.EEXIST:\n            raise\n\n", "idx": 67}
{"namespace": "mssqlcli.telemetry._user_id_file_is_old", "completion": "    if not os.path.exists(id_file_path):\n        return True\n\n    file_modified_time = datetime.fromtimestamp(os.path.getmtime(id_file_path))\n    now = datetime.now()\n    return (now - file_modified_time) > timedelta(hours=24)\n\n", "idx": 68}
{"namespace": "mssqlcli.util.is_command_valid", "completion": "    try:\n        with open(os.devnull, 'w') as DEVNULL:\n            subprocess.call([command], stdout=DEVNULL, stderr=DEVNULL)\n        return True\n    except OSError:\n        return False\n", "idx": 69}
{"namespace": "mssqlcli.packages.parseutils.utils.find_prev_keyword", "completion": "    if not sql:\n        return '', ''\n\n    # Strip newlines\n    sql = sql.replace('\\n', ' ')\n\n    # Strip everything after the first comment marker\n    sql = sql.split('--')[0]\n\n    # Strip everything after the first /* marker\n    sql = sql.split('/*')[0]\n\n    # Strip everything after the first ; marker\n    sql = sql.split(';')[0]\n\n    # Strip everything after the last . marker\n    sql = sql.rstrip(' .')\n\n    # Strip everything after the last , marker\n    sql = sql.rstrip(',')\n\n    # Strip everything after the last ) marker\n    sql = sql.rstrip(')')\n\n    # Strip everything after the last ( marker\n    sql = sql.rstrip('(')\n\n    # Strip everything after the last : marker\n    sql = sql.rstrip(':')\n\n    # Strip everything after the last :: marker\n    sql = sql.rstrip('::')\n\n    # Strip everything after the last ; marker\n    sql = sql.rstrip(';')\n\n    # Strip everything after the last ) marker\n    sql = sql.rstrip(')')\n\n    # Strip everything after the last , marker\n    sql = sql.rstrip(',')\n\n    # Strip everything after the last : marker\n    sql = sql.rstrip(':')\n\n    # Strip everything after the last :: marker\n    sql = sql.rstrip('::')\n\n    # Strip everything after the last ; marker\n    sql = sql.rstrip(';')\n\n    # Strip everything after the last ) marker\n    sql = sql.rstrip(')')\n\n    # Strip everything after the last , marker\n    sql = sql.rstrip(',')\n\n    # Strip everything after the last : marker\n    sql = sql.rstrip(':')\n\n    # Strip everything after the last :: marker\n    sql = sql.rstrip('::')\n\n    # Strip everything after the last ; marker\n    sql = sql.rstrip(';')\n\n    # Strip everything after", "idx": 70}
{"namespace": "trafilatura.settings.use_config", "completion": "    if config is None:\n        if filename is None:\n            filename = Path(__file__).parent / \"settings.cfg\"\n        config = ConfigParser()\n        config.read(filename)\n    return config\n\n", "idx": 71}
{"namespace": "pyramid.util.text_", "completion": "    if isinstance(s, bytes):\n        return s.decode(encoding, errors)\n    return s\n\n", "idx": 72}
{"namespace": "trafilatura.downloads._parse_config", "completion": "    user_agents = config.get('http', 'user_agents')\n    user_agents = user_agents.split('\\n')\n    user_agents = [agent.strip() for agent in user_agents if agent]\n    cookies = config.get('http', 'cookies')\n    return user_agents, cookies\n\n", "idx": 73}
{"namespace": "trafilatura.downloads.load_download_buffer", "completion": "    bufferlist = []\n    while True:\n        if url_store.is_empty():\n            sleep(sleep_time)\n            continue\n        else:\n            bufferlist = url_store.get_urls()\n            break\n\n    return bufferlist, url_store\n\n", "idx": 74}
{"namespace": "trafilatura.metadata.check_authors", "completion": "    if not authors:\n        return None\n\n    new_authors = []\n    for author in authors.split(';'):\n        author = author.strip()\n        if author.lower() not in author_blacklist:\n            new_authors.append(author)\n\n    if new_authors:\n        return '; '.join(new_authors)\n\n    return None\n\n", "idx": 75}
{"namespace": "datasette.filters.where_filters", "completion": "    where_clauses = []\n    extra_wheres_for_ui = []\n    if \"_where\" in request.args:\n        if not request.args[\"_where\"]:\n            raise BadRequest(\"_where= cannot be empty\")\n        if not detect_json1(database):\n            raise DatasetteError(\"This feature requires SQLite 1.1.8 or newer\", status=400)\n        if not datasette.permission_allowed(\n            request.actor, \"execute-sql\", database, default=True\n        ):\n            raise DatasetteError(\"permission_denied\", status=403)\n        try:\n            json_clauses = json.loads(request.args[\"_where\"])\n        except json.decoder.JSONDecodeError:\n            raise DatasetteError(\"Invalid JSON\", status=400)\n        if not isinstance(json_clauses, list):\n            raise DatasetteError(\"JSON list expected\", status=400)\n        for clause in json_clauses:\n            if not isinstance(clause, dict) or len(clause) != 1:\n                raise DatasetteError(\"Invalid input\", status=400)\n            key = list(clause.keys())[0]\n            value = clause[key]\n            if not isinstance(value, str):\n                raise DatasetteError(\"Invalid input\", status=400)\n            where_clauses.append(f\"{escape_sqlite(key)} = {escape_sqlite(value)}\")\n        where_clauses.append(\"1=1\")\n        extra_wheres_for_ui = json_clauses\n    def inner(table, request, where_clauses, where_params, view_name):\n        \"\"\"\n        The function is used to handle the \"_where\" parameter in a request. If the \"_where\" parameter is present in the request parameters, it checks if the user has permission to execute SQL. If no permission is granted, it raises a 403 error. If permission is granted, it adds the values of the \"_where\" parameter to the where_clauses list and generates a separate UI element for each value, which is added to the", "idx": 76}
{"namespace": "datasette.utils.path_with_added_args", "completion": "    if path is None:\n        path = request.path\n    if args is None:\n        args = {}\n    if args:\n        path = append_querystring(path, urllib.parse.urlencode(args))\n    return path\n\n", "idx": 77}
{"namespace": "datasette.utils.path_with_replaced_args", "completion": "    query_string = request.query_string\n    if path is None:\n        path = request.path\n    else:\n        if \"?\" in path:\n            bits = path.split(\"?\", 1)\n            path, query_string = bits\n    # args can be a dict or a set\n    current = []\n    if isinstance(args, set):\n\n        def should_replace(key, value):\n            return key in args\n\n    elif isinstance(args, dict):\n        # Must match key AND value\n        def should_replace(key, value):\n            return args.get(key) == value\n\n    for key, value in urllib.parse.parse_qsl(query_string):\n        if not should_replace(key, value):\n            current.append((key, value))\n    current.extend([(key, value) for key, value in args.items() if value is not None])\n    query_string = urllib.parse.urlencode(current)\n    if query_string:\n        query_string = f\"?{query_string}\"\n    return path + query_string\n\n", "idx": 78}
{"namespace": "datasette.utils.format_bytes", "completion": "    if bytes is None:\n        return \"0B\"\n\n    if not isinstance(bytes, int):\n        raise TypeError(\"The input must be an integer.\")\n\n    if bytes == 0:\n        return \"0B\"\n\n    units = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\")\n    units_index = int(math.log(abs(bytes), 1024))\n\n    if units_index + 1 > len(units):\n        raise OverflowError(\"The input is too large to convert.\")\n\n    return f\"{bytes / (1024 ** units_index):.2f}{units[units_index]}\"\n\n", "idx": 79}
{"namespace": "datasette.utils.actor_matches_allow", "completion": "    if allow is True:\n        return True\n    if allow is False:\n        return False\n    if isinstance(allow, str):\n        return allow == actor\n    if isinstance(allow, list):\n        return actor in allow\n    if isinstance(allow, dict):\n        if actor in allow:\n            return actor_matches_allow(actor, allow[actor])\n        return False\n    raise Exception(f\"Unexpected allow value: {allow}\")\n\n", "idx": 80}
{"namespace": "datasette.utils.resolve_env_secrets", "completion": "    if isinstance(config, dict):\n        for key, value in config.items():\n            if isinstance(value, dict):\n                if list(value.keys()) == [\"$env\"]:\n                    config[key] = environ.get(value[\"$env\"])\n                elif list(value.keys()) == [\"$file\"]:\n                    with open(value[\"$file\"]) as f:\n                        config[key] = f.read()\n                else:\n                    config[key] = resolve_env_secrets(value, environ)\n            elif isinstance(value, list):\n                for i, v in enumerate(value):\n                    if isinstance(v, dict):\n                        if list(v.keys()) == [\"$env\"]:\n                            value[i] = environ.get(v[\"$env\"])\n                        elif list(v.keys()) == [\"$file\"]:\n                            with open(v[\"$file\"]) as f:\n                                value[i] = f.read()\n                        else:\n                            value[i] = resolve_env_secrets(v, environ)\n            elif isinstance(value, str):\n                if value.startswith(\"$env:\"):\n                    config[key] = environ.get(value[5:])\n                elif value.startswith(\"$file:\"):\n                    with open(value[6:]) as f:\n                        config[key] = f.read()\n    elif isinstance(config, list):\n        for i, v in enumerate(config):\n            if isinstance(v, dict):\n                if list(v.keys()) == [\"$env\"]:\n                    config[i] = environ.get(v[\"$env\"])\n                elif list(v.keys()) == [\"$file\"]:\n                    with open(v[\"$file\"]) as f:\n                        config[i] = f.read()\n                else:\n                    config[i] = resolve_env_secrets(v, environ)\n    return config\n\n", "idx": 81}
{"namespace": "datasette.utils.display_actor", "completion": "    if actor is None:\n        return \"Anonymous User\"\n    if \"display\" in actor:\n        return actor[\"display\"]\n    if \"name\" in actor:\n        return actor[\"name\"]\n    if \"username\" in actor:\n        return actor[\"username\"]\n    if \"login\" in actor:\n        return actor[\"login\"]\n    if \"id\" in actor:\n        return str(actor[\"id\"])\n    return str(actor)\n\n", "idx": 82}
{"namespace": "datasette.utils.initial_path_for_datasette", "completion": "    if len(datasette._metadata[\"databases\"]) == 1:\n        database = list(datasette._metadata[\"databases\"].values())[0]\n        if len(database[\"tables\"].keys()) == 1:\n            return \"/{}/{}\".format(\n                list(datasette._metadata[\"databases\"].keys())[0],\n                list(database[\"tables\"].keys())[0],\n            )\n        else:\n            return \"/{}\".format(list(datasette._metadata[\"databases\"].keys())[0])\n    else:\n        return \"/\"\n\n", "idx": 83}
{"namespace": "datasette.utils.tilde_decode", "completion": "    return urllib.parse.unquote(s.replace(\"%\", \"$$\")).replace(\"$$\", \"%\")\n\n", "idx": 84}
{"namespace": "datasette.utils.resolve_routes", "completion": "    for regex, view in routes:\n        match = regex.match(path)\n        if match:\n            return match, view\n    return None, None\n\n", "idx": 85}
{"namespace": "datasette.utils.truncate_url", "completion": "    if len(url) <= length:\n        return url\n    else:\n        url_parts = url.split(\"/\")\n        if len(url_parts[-1]) > 4 and url_parts[-1][-4] == \".\":\n            return \"/\".join(url_parts[:-1]) + \"/...\" + url_parts[-1]\n        else:\n            return \"/\".join(url_parts[:-1]) + \"/...\"\n\n", "idx": 86}
{"namespace": "kinto.core.authorization.groupfinder", "completion": "    # If the permission backend is not configured, return an empty list.\n    if not request.registry.permission:\n        return []\n\n    # If the permission backend is configured, query the permission backend only once per request.\n    if not hasattr(request, \"_has_principals_for_\" + userid):\n        setattr(request, \"_has_principals_for_\" + userid, request.registry.permission.get_user_principals(userid))\n\n    # Return the list of principals fetched from the permission backend.\n    return getattr(request, \"_has_principals_for_\" + userid)\n\n", "idx": 87}
{"namespace": "kinto.core.utils.json.dumps", "completion": "        return rapidjson.dumps(v, bytes_mode=rapidjson.BM_NONE, **kw)\n", "idx": 88}
{"namespace": "kinto.core.utils.json.loads", "completion": "        kw.setdefault(\"number_mode\", rapidjson.NM_NATIVE)\n        return rapidjson.loads(v, **kw)\n\n", "idx": 89}
{"namespace": "kinto.core.utils.hmac_digest", "completion": "    if isinstance(secret, str):\n        secret = secret.encode(encoding)\n    return hmac.new(secret, message.encode(encoding), hashlib.sha256).hexdigest()\n\n", "idx": 90}
{"namespace": "kinto.core.utils.current_service", "completion": "    try:\n        services = request.registry.cornice_services\n    except AttributeError:\n        return None\n\n    for service in services:\n        route = service.get_route(request)\n        if route is not None:\n            return service\n\n    return None\n\n", "idx": 91}
{"namespace": "kinto.core.utils.prefixed_principals", "completion": "    principals = request.effective_principals\n    if Authenticated not in principals:\n        return principals\n    else:\n        principals.remove(Authenticated)\n        principals.insert(0, Authenticated)\n        principals.insert(1, prefixed_userid(request))\n        return principals\n\n", "idx": 92}
{"namespace": "kinto.plugins.accounts.views.on_account_created", "completion": "    # Check if account validation is enabled\n    settings = event.request.registry.settings\n    if settings.get(\"account_validation\", False):\n        # Iterate through each impacted object\n        for obj in event.impacted_objects:\n            # Retrieve account information\n            user_email = obj[\"new\"][\"id\"]\n            activation_key = obj[\"new\"][\"activation_key\"]\n            # If the activation key is not found, skip to the next impacted object\n            if activation_key is None:\n                continue\n            # Send an email to the user\n            Emailer.send_activation(event.request, user_email, activation_key)", "idx": 93}
{"namespace": "kinto.plugins.accounts.utils.hash_password", "completion": "    password = password.encode(\"utf-8\")\n    hashed = bcrypt.hashpw(password, bcrypt.gensalt())\n    return hashed.decode(\"utf-8\")\n\n", "idx": 94}
{"namespace": "kinto.views.admin.get_parent_uri", "completion": "    path = object_uri.split(\"/\")\n    if len(path) < 3:\n        return \"\"\n    return path[1]\n\n", "idx": 95}
{"namespace": "alembic.script.write_hooks.register", "completion": "    def decorate(fn):\n        _registry[name] = fn\n        return fn\n\n    return decorate\n\n", "idx": 96}
{"namespace": "mongo_connector.namespace_config.match_replace_regex", "completion": "    match = regex.match(src_namespace)\n    if match:\n        return dest_namespace.replace(\"*\", match.group(1))\n    return None\n\n", "idx": 97}
{"namespace": "mongo_connector.namespace_config.namespace_to_regex", "completion": "    # Split the namespace into database and collection names.\n    db_name, coll_name = namespace.split(\".\", 1)\n    # Create a regular expression for the database and collection names.\n    db_regex = re.escape(db_name).replace(\"\\\\*\", \".*\")\n    coll_regex = re.escape(coll_name).replace(\"\\\\*\", \".*\")\n    # Compile the regular expression object.\n    return re.compile(\"^\" + db_regex + \"\\\\.\" + coll_regex + \"$\")\n\n", "idx": 98}
{"namespace": "mongo_connector.util.long_to_bson_ts", "completion": "    return Timestamp(int(val >> 32), int(val & 0xffffffff))\n\n", "idx": 99}
{"namespace": "mongo_connector.doc_managers.formatters.DocumentFlattener.format_document", "completion": "        def _kernel(doc):\n            for key in doc:\n                value = doc[key]\n                for new_k, new_v in self.transform_element(key, value):\n                    yield new_k, new_v\n\n        return dict(_kernel(document))\n\n", "idx": 100}
{"namespace": "bplustree.memory.open_file_in_dir", "completion": "    if platform.system() == 'Windows':\n        return (open(path, 'rb+'), None)\n    else:\n        dir_path = os.path.dirname(path)\n        if not os.path.exists(dir_path):\n            os.makedirs(dir_path)\n        return (open(path, 'rb+'), os.open(dir_path, os.O_RDONLY))\n\n", "idx": 101}
{"namespace": "bplustree.memory.FileMemory.read_transaction", "completion": "        return ReadTransaction(self)\n", "idx": 102}
{"namespace": "bplustree.utils.pairwise", "completion": "    a, b = itertools.tee(iterable)\n    next(b, None)\n    return zip(a, b)\n\n", "idx": 103}
{"namespace": "bplustree.utils.iter_slice", "completion": "    it = iter(iterable)\n    while True:\n        chunk = tuple(itertools.islice(it, n))\n        if not chunk:\n            return\n        last = len(chunk) < n\n        yield chunk, last\n\n", "idx": 104}
{"namespace": "bplustree.serializer.StrSerializer.serialize", "completion": "        data = obj.encode('utf-8')\n        assert len(data) <= key_size\n        return data\n", "idx": 105}
{"namespace": "psd_tools.utils.pack", "completion": "    return struct.pack(\">\" + fmt, *args)\n\n", "idx": 106}
{"namespace": "psd_tools.utils.unpack", "completion": "    fmt = str(\">\" + fmt)\n    return struct.unpack(fmt, data)\n\n", "idx": 107}
{"namespace": "psd_tools.api.numpy_io.get_pattern", "completion": "    width = pattern.data[2]\n    height = pattern.data[3]\n    pattern_array = np.zeros((height, width, 3), dtype=np.uint8)\n    for channel in pattern.channels:\n        if channel.id == b'Patt':\n            pattern_array[:, :, 0] = _parse_array(channel.data, 8)\n        elif channel.id == b'PattMatte':\n            pattern_array[:, :, 1] = _parse_array(channel.data, 8)\n        elif channel.id == b'PattStroke':\n            pattern_array[:, :, 2] = _parse_array(channel.data, 8)\n    return pattern_array\n\n", "idx": 108}
{"namespace": "sqlite_utils.utils.maximize_csv_field_size_limit", "completion": "    csv.field_size_limit(sys.maxsize)\n    while True:\n        try:\n            csv.field_size_limit(csv.field_size_limit() + 1)\n        except OverflowError:\n            break\n\n", "idx": 109}
{"namespace": "sqlite_utils.utils.column_affinity", "completion": "    if column_type is None:\n        return \"NUMERIC\"\n    column_type = column_type.upper()\n    if column_type == \"\":\n        return \"NUMERIC\"\n    if \"INT\" in column_type:\n        return \"INTEGER\"\n    if \"CHAR\" in column_type or \"CLOB\" in column_type or \"TEXT\" in column_type:\n        return \"TEXT\"\n    if \"BLOB\" in column_type:\n        return \"NONE\"\n    if \"REAL\" in column_type or \"FLOA\" in column_type or \"DOUB\" in column_type:\n        return \"REAL\"\n    if \"NUMR\" in column_type or \"BOO\" in column_type:\n        return \"NUMERIC\"\n    return \"NUMERIC\"\n\n", "idx": 110}
{"namespace": "sqlite_utils.utils.decode_base64_values", "completion": "    for key in doc:\n        if isinstance(doc[key], dict):\n            if doc[key].get(\"$base64\") is True:\n                doc[key] = base64.b64decode(doc[key][\"encoded\"])\n            else:\n                doc[key] = decode_base64_values(doc[key])\n    return doc\n\n", "idx": 111}
{"namespace": "sqlite_utils.utils.chunks", "completion": "    iterator = iter(sequence)\n    for item in iterator:\n        yield itertools.chain([item], itertools.islice(iterator, size - 1))\n\n", "idx": 112}
{"namespace": "sqlite_utils.utils.hash_record", "completion": "    if keys is None:\n        keys = record.keys()\n\n    hash_object = hashlib.sha1()\n    for key in sorted(keys):\n        hash_object.update(str(record[key]).encode(\"utf-8\"))\n    return hash_object.hexdigest()\n\n", "idx": 113}
{"namespace": "arctic.decorators._get_host", "completion": "    if store is None:\n        return None\n\n    if not store:\n        return None\n\n    if isinstance(store, (list, tuple)):\n        store = store[0]\n\n    host = {\n        'library': store.library,\n        'nodes': store.mongo_host,\n        'host': store.host\n    }\n\n    return host\n\n", "idx": 114}
{"namespace": "arctic.decorators.mongo_retry", "completion": "    @wraps(f)\n    def wrapper(*args, **kwargs):\n        global _in_retry\n        global _retry_count\n        if _in_retry:\n            return f(*args, **kwargs)\n        _in_retry = True\n        _retry_count += 1\n        try:\n            return f(*args, **kwargs)\n        except (AutoReconnect, OperationFailure, DuplicateKeyError, BulkWriteError) as e:\n            if _retry_count > _MAX_RETRIES:\n                _retry_count = 0\n                _in_retry = False\n                raise e\n            else:\n                logger.warning(\"MongoDB error: {}\".format(e))\n                logger.warning(\"Retrying...\")\n                sleep(1)\n                return wrapper(*args, **kwargs)\n        except ServerSelectionTimeoutError as e:\n            logger.error(\"MongoDB error: {}\".format(e))\n            logger.error(\"Retrying...\")\n            sleep(1)\n            return wrapper(*args, **kwargs)\n        except Exception as e:\n            _retry_count = 0\n            _in_retry = False\n            _log_exception(e, f.__name__, args, kwargs, _get_host(args[0]))\n            raise e\n        finally:\n            _retry_count = 0\n            _in_retry = False\n\n    return wrapper", "idx": 115}
{"namespace": "arctic._util.are_equals", "completion": "    try:\n        if isinstance(o1, DataFrame) and isinstance(o2, DataFrame):\n            assert_frame_equal(o1, o2, **kwargs)\n        else:\n            assert o1 == o2\n        return True\n    except:\n        return False\n\n", "idx": 116}
{"namespace": "arctic.hooks.register_resolve_mongodb_hook", "completion": "    global _resolve_mongodb_hook\n    _resolve_mongodb_hook = hook\n\n", "idx": 117}
{"namespace": "arctic.hooks.register_log_exception_hook", "completion": "    global _log_exception_hook\n    _log_exception_hook = hook\n\n", "idx": 118}
{"namespace": "arctic.hooks.register_get_auth_hook", "completion": "    global _get_auth_hook\n    _get_auth_hook = hook\n\n", "idx": 119}
{"namespace": "arctic.store._version_store_utils._split_arrs", "completion": "    return [array_2d[slices[i]:slices[i + 1]] for i in range(len(slices) - 1)]\n\n", "idx": 120}
{"namespace": "arctic.store._version_store_utils.checksum", "completion": "    # Create a string with the symbol and the keys of the dictionary.\n    keys = sorted(doc.keys())\n    keys_str = symbol + ''.join(keys)\n\n    # Create a string with the values of the dictionary.\n    values = [doc[k] for k in keys]\n    values_str = ''.join(map(str, values))\n\n    # Create a string with the symbol, the keys and the values of the dictionary.\n    data_str = keys_str + values_str\n\n    # Calculate the checksum.\n    checksum = hashlib.sha1(data_str.encode('utf-8')).digest()\n\n    # Return the checksum as a Binary object.\n    return Binary(checksum)\n\n", "idx": 121}
{"namespace": "arctic.store.versioned_item.VersionedItem.__str__", "completion": "        return \"VersionedItem(symbol={symbol},library={library},data={data},version={version},metadata={metadata},host={host})\".format(\n            symbol=self.symbol,\n            library=self.library,\n            data=self.data,\n            version=self.version,\n            metadata=self.metadata,\n            host=self.host\n        )\n", "idx": 122}
{"namespace": "arctic.store._ndarray_store.NdarrayStore._dtype", "completion": "        if metadata is None:\n            metadata = {}\n\n        if string.startswith('['):\n            return np.dtype(string, metadata)\n        else:\n            return np.dtype(string)\n", "idx": 123}
{"namespace": "arctic.store._ndarray_store._promote_struct_dtypes", "completion": "    # Check if the fields of dtype1 are a superset of dtype2.\n    if not set(dtype2.fields).issubset(set(dtype1.fields)):\n        raise ValueError(\"The fields of the second structured array are not a subset of the fields of the first structured array.\")\n\n    # Promote the data types of the two structured arrays.\n    dtype = np.dtype([(name, (dtype1[name], dtype2[name])) for name in dtype1.names])\n\n    return dtype\n\n", "idx": 124}
{"namespace": "arctic.chunkstore.passthrough_chunker.PassthroughChunker.exclude", "completion": "        return DataFrame()\n", "idx": 125}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.to_chunks", "completion": "        if not isinstance(df.index, pd.DatetimeIndex):\n            raise ValueError('The index of the dataframe must be a pandas DatetimeIndex.')\n\n        if not isinstance(chunk_size, str):\n            raise ValueError('The chunk_size must be a string.')\n\n        if func is None:\n            raise ValueError('The function cannot be None.')\n\n        if not callable(func):\n            raise ValueError('The function must be callable.')\n\n        if not isinstance(df, (pd.DataFrame, pd.Series)):\n            raise ValueError('The dataframe must be a pandas DataFrame or Series.')\n\n        if isinstance(df, pd.Series):\n            df = df.to_frame()\n\n        if df.index.name is None:\n            df.index.name = 'date'\n\n        if df.index.name != 'date':\n            raise ValueError('The index of the dataframe must be named \"date\".')\n\n        if df.index.tz is not None:\n            raise ValueError('The index of the dataframe must be a tz-naive datetime index.')\n\n        if df.index.freq is None:\n            raise ValueError('The frequency of the index must be set.')\n\n        if df.index.freq != chunk_size:\n            raise ValueError('The frequency of the index must be the same as the chunk_size.')\n\n        if df.index.inferred_freq is None:\n            raise ValueError('The frequency of the index must be inferrable.')\n\n        if df.index.inferred_freq != chunk_size:\n            raise ValueError('The frequency of the index must be the same as the chunk_size.')\n\n        if df.index.inferred_type != 'datetime64':\n            raise ValueError('The index must be a datetime index.')\n\n        if df.index.is_monotonic is False:\n            raise ValueError('The index must be monotonic.')\n\n        if df.index.is_unique is False:\n            raise ValueError('The index must be unique.')\n\n        if df.index.", "idx": 126}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.exclude", "completion": "        if isinstance(range_obj, (pd.DatetimeIndex, tuple)):\n            range_obj = DateRange(range_obj[0], range_obj[-1])\n\n        range_obj = to_pandas_closed_closed(range_obj, add_tz=False)\n        start = range_obj.start\n        end = range_obj.end\n\n        if 'date' in data.index.names:\n            return data[data.index.get_level_values('date') < start]\n        elif 'date' in data.columns:\n            if start and end:\n                return data[(data.date < start) | (data.date > end)]\n            elif start:\n                return data[(data.date < start)]\n            elif end:\n                return data[(data.date > end)]\n            else:\n                return data\n        else:\n            return data\n", "idx": 127}
{"namespace": "mopidy.httpclient.format_proxy", "completion": "    if not proxy_config:\n        return None\n\n    scheme = proxy_config.get('scheme')\n    host = proxy_config.get('hostname')\n    port = proxy_config.get('port')\n    username = proxy_config.get('username')\n    password = proxy_config.get('password')\n\n    if not all((scheme, host, port)):\n        return None\n\n    proxy_string = '%s://%s:%s' % (scheme, host, port)\n\n    if auth and all((username, password)):\n        proxy_string = '%s:%s@%s' % (username, password, proxy_string)\n\n    return proxy_string\n\n", "idx": 128}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.filter", "completion": "        if isinstance(range_obj, (pd.DatetimeIndex, tuple)):\n            range_obj = DateRange(range_obj[0], range_obj[-1])\n        if range_obj.start and range_obj.end:\n            return data[(data.index.get_level_values('date') >= range_obj.start) & (data.index.get_level_values('date') <= range_obj.end)]\n        elif range_obj.start:\n            return data[data.index.get_level_values('date') >= range_obj.start]\n        elif range_obj.end:\n            return data[data.index.get_level_values('date') <= range_obj.end]\n        else:\n            return data\n", "idx": 129}
{"namespace": "mopidy.config.validators.validate_required", "completion": "    if required and not value:\n        raise ValueError('Value is required.')\n\n", "idx": 130}
{"namespace": "mopidy.config.validators.validate_choice", "completion": "    if value not in choices:\n        raise ValueError(f\"must be one of {choices}, not {value}.\")\n\n", "idx": 131}
{"namespace": "mopidy.config.validators.validate_minimum", "completion": "    if value < minimum:\n        raise ValueError(f\"{value!r} must be larger than {minimum!r}.\")\n\n", "idx": 132}
{"namespace": "mopidy.config.validators.validate_maximum", "completion": "    if maximum is not None and value > maximum:\n        raise ValueError(f\"{value!r} must be smaller than {maximum!r}.\")\n\n", "idx": 133}
{"namespace": "mopidy.config.schemas._did_you_mean", "completion": "    if choices is None or len(choices) == 0:\n        return None\n\n    choices = [c.lower() for c in choices]\n    name = name.lower()\n\n    choices_with_distance = sorted([(c, levenshtein_distance(name, c)) for c in choices], key=lambda x: x[1])\n    min_distance = choices_with_distance[0][1]\n    if min_distance <= 3:\n        return choices_with_distance[0][0]\n    return None\n\n", "idx": 134}
{"namespace": "mopidy.config.types.encode", "completion": "    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(\n            char, char.encode(encoding=\"unicode-escape\").decode()\n        )\n\n    return value\n\n", "idx": 135}
{"namespace": "mopidy.config.types.decode", "completion": "    if isinstance(value, bytes):\n        value = value.decode(\"utf-8\", \"surrogateescape\")\n    return re.sub(r\"\\\\u00(..)\", lambda m: chr(int(m.group(1), 16)), value)\n\n", "idx": 136}
{"namespace": "mopidy.config.types.ConfigValue.serialize", "completion": "        if value is None:\n            return \"\"\n        elif isinstance(value, DeprecatedValue):\n            return encode(value.original)\n        elif isinstance(value, _TransformedValue):\n            return encode(value.original) if display else encode(value.transformed)\n        else:\n            return encode(value)\n", "idx": 137}
{"namespace": "mopidy.config.types.Boolean.serialize", "completion": "        if value is None:\n            return \"false\"\n        if value:\n            return \"true\"\n        return \"false\"\n\n", "idx": 138}
{"namespace": "hypertools.tools.df2mat.df2mat", "completion": "    # Check if the input is a DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"Input is not a Pandas DataFrame\")\n\n    # Check if the index is a single-level index\n    if isinstance(data.index, pd.MultiIndex):\n        raise TypeError(\"Input DataFrame has a Multi-level index. This function only works with single-level indices.\")\n\n    # Get the column labels\n    labels = data.columns\n\n    # Transform the text columns into binary vectors\n    data = pd.get_dummies(data)\n\n    # Convert the DataFrame into a Numpy array\n    data = data.values\n\n    # Return the Numpy array and the column labels if return_labels is set to True\n    if return_labels:\n        return data, labels\n\n    # Otherwise, just return the Numpy array\n    else:\n        return data\n\n", "idx": 139}
{"namespace": "hypertools._shared.helpers.center", "completion": "    assert isinstance(x, list), \"Input argument should be a list\"\n    return x - np.mean(x)\n\n", "idx": 140}
{"namespace": "hypertools._shared.helpers.group_by_category", "completion": "    assert type(vals) is list, \"Input values must be a list\"\n    if any(isinstance(i, list) for i in vals):\n        vals = list(itertools.chain.from_iterable(vals))\n    vals = sorted(set(vals))\n    return [vals.index(i) for i in vals]\n\n", "idx": 141}
{"namespace": "hypertools._shared.helpers.vals2colors", "completion": "    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n    val_set = list(sorted(set(vals), key=list(vals).index))\n    colors = sns.color_palette(cmap, res)\n    return [colors[val_set.index(val)] for val in vals]\n\n", "idx": 142}
{"namespace": "hypertools._shared.helpers.vals2bins", "completion": "    # flatten if list of lists\n    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n\n    # map values to bins\n    ranks = np.digitize(vals, np.linspace(np.min(vals), np.max(vals)+1, res+1)) - 1\n    return ranks\n\n", "idx": 143}
{"namespace": "hypertools._shared.helpers.interp_array", "completion": "    # Check if the input array is a list.\n    if type(arr) is list:\n        arr = np.array(arr)\n\n    # Check if the input array is a numpy array.\n    if type(arr) is not np.ndarray:\n        print(\"Error: The input array is not a list or numpy array.\")\n        sys.exit()\n\n    # Check if the input array is a 1D array.\n    if len(arr.shape) != 1:\n        print(\"Error: The input array is not a 1D array.\")\n        sys.exit()\n\n    # Check if the interpolation value is an integer.\n    if type(interp_val) is not int:\n        print(\"Error: The interpolation value is not an integer.\")\n        sys.exit()\n\n    # Check if the interpolation value is a positive integer.\n    if interp_val < 0:\n        print(\"Error: The interpolation value is not a positive integer.\")\n        sys.exit()\n\n    # Check if the length of the input array is greater than or equal to the interpolation value.\n    if len(arr) < interp_val:\n        print(\"Error: The length of the input array is less than the interpolation value.\")\n        sys.exit()\n\n    # Interpolate the input array.\n    x = np.linspace(0, len(arr), len(arr))\n    f = pchip(x, arr)\n    x_new = np.linspace(0, len(arr), interp_val * len(arr))\n    arr_new = f(x_new)\n\n    return arr_new\n\n", "idx": 144}
{"namespace": "hypertools._shared.helpers.parse_args", "completion": "    if isinstance(args, list) or isinstance(args, tuple):\n        if len(args) != len(x):\n            print(\"Error: The length of the list of arguments is not the same as that of the list of elements\")\n            sys.exit()\n        else:\n            return list(zip(x, args))\n    else:\n        return list(zip(x, itertools.repeat(args)))\n\n", "idx": 145}
{"namespace": "hypertools._shared.helpers.parse_kwargs", "completion": "    kwargs_list = []\n    for i, item in enumerate(x):\n        tmp = {}\n        for key, value in kwargs.items():\n            if isinstance(value, (tuple, list)):\n                if len(value) == len(x):\n                    tmp[key] = value[i]\n                else:\n                    print('Error: arguments must be a list of the same length as x')\n                    sys.exit(1)\n            else:\n                tmp[key] = value\n        kwargs_list.append(tmp)\n    return kwargs_list\n\n", "idx": 146}
{"namespace": "gif_for_cli.utils._get_default_display_mode", "completion": "    if \"truecolor\" in environ.get(\"TERM\", \"\") or \"truecolor\" in environ.get(\"COLORTERM\", \"\"):\n        return \"truecolor\"\n    elif \"256\" in environ.get(\"TERM\", \"\") or \"256\" in environ.get(\"COLORTERM\", \"\"):\n        return \"256fgbg\"\n    else:\n        return \"nocolor\"\n\n", "idx": 147}
{"namespace": "gif_for_cli.utils._pool_type", "completion": "    val = int(val)\n    if val <= 0:\n        raise argparse.ArgumentTypeError(\"Please provide a positive integer\")\n    return val\n\n", "idx": 148}
{"namespace": "gif_for_cli.generate.utils.get_avg_for_em", "completion": "    avg = [0, 0, 0]\n    for i in range(cell_height):\n        for j in range(cell_width):\n            avg[0] += px[x + j, y + i][0]\n            avg[1] += px[x + j, y + i][1]\n            avg[2] += px[x + j, y + i][2]\n    avg[0] /= cell_height * cell_width\n    avg[1] /= cell_height * cell_width\n    avg[2] /= cell_height * cell_width\n    return avg\n\n", "idx": 149}
{"namespace": "gif_for_cli.generate.utils.process_input_source", "completion": "    if input_source.startswith(\"https://tenor.com/view/\"):\n        gif_id = input_source.split(\"-\")[-1].split(\".\")[0]\n        gif_url = \"https://media.tenor.com/v1/gifs/{0}\".format(gif_id)\n    else:\n        if os.path.isfile(input_source):\n            with open(input_source, \"rb\") as f:\n                gif_url = f.read()\n        else:\n            url = \"https://api.tenor.com/v1/random?q={0}&key={1}\".format(input_source, api_key)\n            response = requests.get(url)\n            try:\n                gif_url = response.json()[\"results\"][0][\"media\"][0][\"gif\"][\"url\"]\n            except (KeyError, JSONDecodeError):\n                raise Exception(\"Invalid API key or search query.\")\n\n    return gif_url", "idx": 150}
{"namespace": "hypertools._shared.helpers.reshape_data", "completion": "    # Get the unique categories in the hue\n    hue_categories = np.unique(hue)\n\n    # Get the number of categories in the hue\n    hue_categories_len = len(hue_categories)\n\n    # Get the number of samples in the input data\n    x_len = len(x)\n\n    # Get the number of samples in the labels\n    labels_len = len(labels)\n\n    # Get the number of samples in the hue\n    hue_len = len(hue)\n\n    # Check if the hue is a list of lists\n    if isinstance(hue[0], list):\n\n        # Check if the hue is a list of lists of the same length as the input data\n        if len(hue[0]) == x_len:\n\n            # Check if the labels is a list of lists\n            if isinstance(labels[0], list):\n\n                # Check if the labels is a list of lists of the same length as the input data\n                if len(labels[0]) == x_len:\n\n                    # Check if the hue is a list of lists of the same length as the labels\n                    if len(hue[0]) == len(labels[0]):\n\n                        # Reshape the input data based on the categories in the hue\n                        x_reshape = [np.stack(np.array(x)[np.where(np.array(hue) == hue_category)]) for hue_category in hue_categories]\n\n                        # Reshape the labels based on the categories in the hue\n                        labels_reshape = [np.stack(np.array(labels)[np.where(np.array(hue) == hue_category)]) for hue_category in hue_categories]\n\n                    else:\n                        print(\"Error: the hue is a list of lists of the same length as the input data, but the labels is a list of lists of the same length as the input data.\")\n                        sys.exit(1)\n\n                else:\n                    print(\"Error: the hue is a list of lists of the same", "idx": 151}
{"namespace": "mingus.extra.lilypond.from_Note", "completion": "    if not isinstance(note, Note):\n        raise TypeError(\"Input must be a Note object.\")\n\n    if not isinstance(process_octaves, bool):\n        raise TypeError(\"process_octaves must be a boolean.\")\n\n    if not isinstance(standalone, bool):\n        raise TypeError(\"standalone must be a boolean.\")\n\n    if process_octaves:\n        return _note_to_lilypond(note, standalone)\n    else:\n        return _note_to_lilypond_no_octaves(note, standalone)\n\n", "idx": 152}
{"namespace": "mingus.extra.tablature._get_qsize", "completion": "    # Find the longest string\n    maxlen = 0\n    for string in tuning.tuning:\n        if len(string) > maxlen:\n            maxlen = len(string)\n\n    # Find the longest note\n    maxnote = 0\n    for string in tuning.tuning:\n        for note in string:\n            if len(note) > maxnote:\n                maxnote = len(note)\n\n    # Find the longest fret\n    maxfret = 0\n    for string in tuning.tuning:\n        for note in string:\n            if len(note) > maxfret:\n                maxfret = len(note)\n\n    # Find the longest tuning name\n    maxtuning = 0\n    for tuning in tuning.tuning:\n        if len(tuning.name) > maxtuning:\n            maxtuning = len(tuning.name)\n\n    # Find the longest tuning description\n    maxtuningdesc = 0\n    for tuning in tuning.tuning:\n        if len(tuning.description) > maxtuningdesc:\n            maxtuningdesc = len(tuning.description)\n\n    # Find the longest instrument name\n    maxinstrument = 0\n    for instrument in tuning.instrument:\n        if len(instrument.name) > maxinstrument:\n            maxinstrument = len(instrument.name)\n\n    # Find the longest instrument description\n    maxinstrumentdesc = 0\n    for instrument in tuning.instrument:\n        if len(instrument.description) > maxinstrumentdesc:\n            maxinstrumentdesc = len(instrument.description)\n\n    # Find the longest instrument tuning name\n    maxinstrumenttuning = 0\n    for instrument in tuning.instrument:\n        if len(instrument.tuning.name) > maxinstrumenttuning:\n            maxinstrumenttuning = len(instrument.tuning.name)\n\n    # Find the longest instrument tuning description\n    maxinstrumenttuningdesc = 0\n    for instrument in tuning.instrument:\n        if len(instrument", "idx": 153}
{"namespace": "mingus.core.notes.augment", "completion": "    if note[-1] != \"b\":\n        return note + \"#\"\n    else:\n        return note[:-1]\n\n", "idx": 154}
{"namespace": "mingus.core.meter.valid_beat_duration", "completion": "    return int(round(log(duration, 2))) == log(duration, 2)\n\n", "idx": 155}
{"namespace": "mingus.core.notes.diminish", "completion": "    if note[-1] != \"#\":\n        return note + \"b\"\n    else:\n        return note[:-1]\n\n", "idx": 156}
{"namespace": "mingus.core.intervals.invert", "completion": "    return interval[::-1]\n\n", "idx": 157}
{"namespace": "mingus.core.progressions.parse_string", "completion": "    # Check for empty string\n    if progression == \"\":\n        return (\"\", 0, \"\")\n\n    # Check for roman numeral\n    for numeral in numerals:\n        if progression.startswith(numeral):\n            return (numeral, 0, progression[len(numeral):])\n\n    # Check for accidentals\n    for i in range(len(progression)):\n        if progression[i] == \"b\":\n            return (\"\", -i, progression[i:])\n        elif progression[i] == \"#\":\n            return (\"\", i, progression[i:])\n\n    # If no roman numeral or accidentals are found, return the string\n    return (progression, 0, \"\")\n\n", "idx": 158}
{"namespace": "exodus_bundler.bundling.bytes_to_int", "completion": "    return int.from_bytes(bytes, byteorder=byteorder)\n\n", "idx": 159}
{"namespace": "exodus_bundler.templating.render_template", "completion": "    for key, value in context.items():\n        string = string.replace('{{' + key + '}}', value)\n    return string\n\n", "idx": 160}
{"namespace": "exodus_bundler.input_parsing.strip_pid_prefix", "completion": "    pid_prefix_regex = re.compile(r'^\\[pid \\d+\\] ')\n    return pid_prefix_regex.sub('', line)", "idx": 161}
{"namespace": "fs.path.abspath", "completion": "    if not path.startswith(\"/\"):\n        return \"/\" + path\n    return path\n\n", "idx": 162}
{"namespace": "fs.path.combine", "completion": "    if not path1:\n        return path2\n    if not path2:\n        return path1\n    if path1[-1] == \"/\":\n        return path1 + path2\n    return path1 + \"/\" + path2\n\n", "idx": 163}
{"namespace": "fs.path.split", "completion": "    if \"/\" not in path:\n        return (\"\", path)\n    split_path = path.rsplit(\"/\", 1)\n    return (split_path[0] or \"/\", split_path[1])\n\n", "idx": 164}
{"namespace": "fs.path.isparent", "completion": "    _path1 = forcedir(abspath(path1))\n    _path2 = forcedir(abspath(path2))\n    return _path1 == _path2 or _path2.startswith(_path1)\n\n", "idx": 165}
{"namespace": "fs.path.forcedir", "completion": "    if not path.endswith(\"/\"):\n        return path + \"/\"\n    return path\n\n", "idx": 166}
{"namespace": "fs.wildcard.match_any", "completion": "    if not patterns:\n        return True\n    return any(match(pattern, name) for pattern in patterns)\n\n", "idx": 167}
{"namespace": "fs.wildcard.imatch_any", "completion": "    if not patterns:\n        return True\n    return any(imatch(pattern, name) for pattern in patterns)\n\n", "idx": 168}
{"namespace": "wal_e.cmd.parse_boolean_envvar", "completion": "    if val.lower() in ('false', '0'):\n        return False\n    elif val.lower() in ('true', '1'):\n        return True\n    else:\n        raise ValueError('Invalid boolean environment variable value: {}'.format(val))\n\n", "idx": 169}
{"namespace": "wal_e.log_help.get_log_destinations", "completion": "    log_destinations = os.getenv('WALE_LOG_DESTINATION', 'stderr,syslog')\n    log_destinations = log_destinations.split(',')\n    return log_destinations\n\n", "idx": 170}
{"namespace": "wal_e.log_help.WalELogger._fmt_structured", "completion": "        # Format the dictionary as {k1:v1, k2:v2}\n        structured_data = ' '.join('{0}={1}'.format(k, v) for k, v in sorted(d.items()))\n\n        # Add the time and pid\n        time = datetime.datetime.utcnow().strftime('time=%Y-%m-%dT%H:%M:%S.%f-00')\n        pid = 'pid={0}'.format(os.getpid())\n\n        # Return the formatted string\n        return ' '.join([time, pid, structured_data])\n", "idx": 171}
{"namespace": "wal_e.tar_partition._fsync_files", "completion": "    for filename in filenames:\n        with open(filename, 'a'):\n            os.fsync(filename)\n\n        dirfd = os.open(os.path.dirname(filename), os.O_DIRECTORY)\n        os.fsync(dirfd)\n        os.close(dirfd)\n\n", "idx": 172}
{"namespace": "wal_e.blobstore.file.calling_format.Bucket.list", "completion": "        path = os.path.join(\"/\", prefix.strip(\"/\"))\n        if not os.path.exists(path):\n            return []\n        if os.path.isfile(path):\n            return [FileKey(bucket=self, name=prefix)]\n        files = []\n        for root, dirs, files in os.walk(path):\n            for f in files:\n                files.append(FileKey(bucket=self, name=os.path.join(root, f)))\n        return files", "idx": 173}
{"namespace": "pyinfra.operations.util.files.unix_path_join", "completion": "    return '/'.join(part.rstrip('/') for part in path_parts)\n\n", "idx": 174}
{"namespace": "pyinfra.operations.server.shell", "completion": "    if isinstance(commands, str):\n        commands = [commands]\n\n    for command in commands:\n        yield command\n\n", "idx": 175}
{"namespace": "pyinfra.api.util.try_int", "completion": "    try:\n        return int(value)\n    except ValueError:\n        return value\n\n", "idx": 176}
{"namespace": "mrjob.job.MRJob.mr_job_script", "completion": "        if cls.__module__ == '__main__':\n            return sys.argv[0]\n        else:\n            return sys.modules[cls.__module__].__file__\n", "idx": 177}
{"namespace": "mrjob.compat.map_version", "completion": "    if isinstance(version_map, dict):\n        version_map = sorted((LooseVersion(k), v) for k, v in version_map.items())\n\n    for v, value in version_map:\n        if version < v:\n            return value\n\n    return version_map[-1][1]\n\n", "idx": 178}
{"namespace": "mrjob.conf.combine_values", "completion": "    for v in reversed(values):\n        if v is not None:\n            return v\n    else:\n        return None\n\n", "idx": 179}
{"namespace": "mrjob.protocol.BytesProtocol.read", "completion": "        try:\n            key, value = line.split(b'\\t', 1)\n        except ValueError:\n            key, value = line, None\n        return key, value\n", "idx": 180}
{"namespace": "mrjob.protocol.TextProtocol.write", "completion": "        return u'\\t'.join(x for x in (key, value) if x is not None).encode('utf_8')\n\n", "idx": 181}
{"namespace": "mrjob.protocol.TextProtocol.read", "completion": "        try:\n            line = line.decode('utf_8')\n        except UnicodeDecodeError:\n            line = line.decode('latin_1')\n        key_value = line.split('\\t', 1)\n        if len(key_value) == 1:\n            key_value.append(None)\n\n        return tuple(key_value)\n", "idx": 182}
{"namespace": "mrjob.protocol.TextValueProtocol.read", "completion": "        try:\n            return (None, line.decode('utf_8'))\n        except UnicodeDecodeError:\n            return (None, line.decode('latin_1'))\n", "idx": 183}
{"namespace": "mrjob.util.file_ext", "completion": "    if filename.startswith('.'):\n        filename = filename[1:]\n    if '.' in filename:\n        return '.' + filename.split('.')[-1]\n    else:\n        return ''\n\n", "idx": 184}
{"namespace": "mrjob.util.cmd_line", "completion": "    return ' '.join(quote_cmd_arg(arg) for arg in args)\n\n", "idx": 185}
{"namespace": "mrjob.util.save_cwd", "completion": "    original_cwd = os.getcwd()\n    try:\n        yield\n    finally:\n        os.chdir(original_cwd)\n\n", "idx": 186}
{"namespace": "mrjob.util.save_sys_std", "completion": "    saved_stdin = sys.stdin\n    saved_stdout = sys.stdout\n    saved_stderr = sys.stderr\n\n    try:\n        yield\n\n    finally:\n        sys.stdin.flush()\n        sys.stdout.flush()\n        sys.stderr.flush()\n        sys.stdin = saved_stdin\n        sys.stdout = saved_stdout\n        sys.stderr = saved_stderr\n\n", "idx": 187}
{"namespace": "mrjob.util.unarchive", "completion": "    if tarfile.is_tarfile(archive_path):\n        with tarfile.open(archive_path, 'r') as archive:\n            archive.extractall(dest)\n    elif is_zipfile(archive_path):\n        with ZipFile(archive_path) as archive:\n            for name in archive.namelist():\n                # the zip spec specifies that front slashes are always\n                # used as directory separators\n                dest_path = os.path.join(dest, *name.split('/'))\n\n                # now, split out any dirname and filename and create\n                # one and/or the other\n                dirname, filename = os.path.split(dest_path)\n                if dirname and not os.path.exists(dirname):\n                    os.makedirs(dirname)\n                if filename:\n                    with open(dest_path, 'wb') as dest_file:\n                        dest_file.write(archive.read(name))\n    else:\n        raise IOError('Unrecognized archive type: %s' % (archive_path,))\n\n", "idx": 188}
{"namespace": "mrjob.util.unique", "completion": "    seen = set()\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            yield item\n\n", "idx": 189}
{"namespace": "mrjob.parse.urlparse", "completion": "    # Python 2.7's urlparse can't handle URLs that contain fragments (pieces\n    # after a #). This method can handle all URLs.\n    if allow_fragments:\n        parsed = urlparse_buggy(urlstring, scheme, *args, **kwargs)\n        if parsed.fragment:\n            # The result of urlparse is immutable, so we have to convert it to a\n            # mutable object.\n            parsed = ParseResult(\n                parsed.scheme, parsed.netloc, parsed.path, parsed.params,\n                parsed.query, parsed.fragment)._asdict()\n            # We split the fragment into its own parameter.\n            parsed['fragment-path'], parsed['fragment-query'] = \\\n                _split_fragment(parsed['fragment'])\n            parsed = ParseResult(**parsed)\n        return parsed\n    else:\n        return urlparse_buggy(urlstring, scheme, *args, **kwargs)\n\n", "idx": 190}
{"namespace": "mrjob.util.which", "completion": "    if path is None:\n        path = os.environ.get('PATH', os.defpath)\n    if not path:\n        return None\n    if os.path.sep not in cmd:\n        cmd = find_executable(cmd, path)\n    if cmd is not None and os.path.isfile(cmd) and os.access(cmd, os.X_OK):\n        return cmd\n    return None\n\n", "idx": 191}
{"namespace": "sshuttle.ssh.parse_hostport", "completion": "    if not rhostport:\n        return None, None, None, None\n\n    if \"@\" in rhostport:\n        userpass, hostport = rhostport.split(\"@\", 1)\n    else:\n        userpass = None\n        hostport = rhostport\n\n    if \":\" in hostport:\n        host, port = hostport.rsplit(\":\", 1)\n    else:\n        host = hostport\n        port = None\n\n    if userpass:\n        if \":\" in userpass:\n            user, password = userpass.split(\":\", 1)\n        else:\n            user = userpass\n            password = None\n    else:\n        user = None\n        password = None\n\n    return user, password, host, port\n\n", "idx": 192}
{"namespace": "flower.utils.search.stringified_dict_contains_value", "completion": "    if key not in str_dict:\n        return False\n    if value in str_dict:\n        return True\n    else:\n        return value in str_dict[key].replace(' ', '')\n\n", "idx": 193}
{"namespace": "flower.utils.abs_path", "completion": "    path = os.path.expanduser(path)\n    if not os.path.isabs(path):\n        path = os.path.join(os.getcwd(), path)\n    return path", "idx": 194}
{"namespace": "flower.utils.strtobool", "completion": "    val = val.lower()\n    if val in ('y', 'yes', 't', 'true', 'on', '1'):\n        return 1\n    elif val in ('n', 'no', 'f', 'false', 'off', '0'):\n        return 0\n    else:\n        raise ValueError(\"invalid truth value %r\" % (val,))", "idx": 195}
{"namespace": "sshuttle.methods.get_method", "completion": "    try:\n        module = importlib.import_module(\"sshuttle.methods.%s\" % method_name)\n    except ImportError:\n        raise Fatal(\"Method '%s' not found.\" % method_name)\n    method = module.Method()\n    return method", "idx": 196}
{"namespace": "trailscraper.iam.all_known_iam_permissions", "completion": "    # Get the path of the current python script.\n    current_path = os.path.dirname(os.path.realpath(__file__))\n    # Join the path with the name of the file containing the known IAM actions.\n    known_iam_actions_file_path = os.path.join(current_path, 'known-iam-actions.txt')\n    # Open the file.\n    with open(known_iam_actions_file_path, 'r') as known_iam_actions_file:\n        # Return the set of lines in the file.\n        return set(known_iam_actions_file.read().splitlines())\n\n", "idx": 197}
{"namespace": "trailscraper.cloudtrail.parse_records", "completion": "    return [record for record in map(_parse_record, json_records) if record is not None]\n\n", "idx": 198}
{"namespace": "pycoin.satoshi.IntStreamer.IntStreamer.int_to_script_bytes", "completion": "        if v == 0:\n            return b''\n        if v < 0:\n            v = -v\n        ba = bytearray()\n        while v > 0:\n            ba.append(v & 0xff)\n            v >>= 8\n        if ba[-1] & 0x80:\n            ba.append(0x80)\n        ba.reverse()\n        return bytes(ba)\n", "idx": 199}
{"namespace": "pycoin.satoshi.stackops.do_OP_2DROP", "completion": "    stack.pop()\n    stack.pop()\n\n", "idx": 200}
{"namespace": "pycoin.satoshi.stackops.do_OP_2DUP", "completion": "    x1 = stack.pop()\n    x2 = stack.pop()\n    stack.append(x1)\n    stack.append(x2)\n    stack.append(x1)\n    stack.append(x2)\n\n", "idx": 201}
{"namespace": "pycoin.satoshi.stackops.do_OP_3DUP", "completion": "    stack.append(stack[-3])\n    stack.append(stack[-3])\n    stack.append(stack[-3])\n\n", "idx": 202}
{"namespace": "trailscraper.s3_download._s3_key_prefixes", "completion": "    # Calculate the delta between the two dates.\n    delta = to_date - from_date\n\n    # Generate a list of dates based on the delta.\n    dates = [from_date + datetime.timedelta(days=i) for i in range(delta.days + 1)]\n\n    # Create a list of S3 key prefixes based on the organization IDs, account IDs, regions, and dates.\n    if org_ids:\n        return [\n            _s3_key_prefix_for_org_trails(prefix, date, org_id, account_id, region)\n            for org_id in org_ids\n            for account_id in account_ids\n            for region in regions\n            for date in dates\n        ]\n    return [\n        _s3_key_prefix(prefix, date, account_id, region)\n        for account_id in account_ids\n        for region in regions\n        for date in dates\n    ]\n\n", "idx": 203}
{"namespace": "pycoin.satoshi.stackops.do_OP_2OVER", "completion": "    stack.append(stack[-4])\n    stack.append(stack[-4])\n\n", "idx": 204}
{"namespace": "pycoin.satoshi.stackops.do_OP_2SWAP", "completion": "    stack.append(stack.pop(-3))\n    stack.append(stack.pop(-3))\n\n", "idx": 205}
{"namespace": "pycoin.satoshi.stackops.do_OP_IFDUP", "completion": "    if stack[-1] != 0:\n        stack.append(stack[-1])\n\n", "idx": 206}
{"namespace": "pycoin.satoshi.stackops.do_OP_NIP", "completion": "    stack.pop(-2)\n\n", "idx": 207}
{"namespace": "pycoin.satoshi.stackops.do_OP_TUCK", "completion": "    v1 = stack.pop()\n    v2 = stack.pop()\n    stack.append(v1)\n    stack.append(v2)\n    stack.append(v1)\n\n", "idx": 208}
{"namespace": "pycoin.satoshi.stackops.do_OP_CAT", "completion": "    v1 = stack.pop()\n    v2 = stack.pop()\n    stack.append(v2 + v1)\n\n", "idx": 209}
{"namespace": "pycoin.crack.ecdsa.crack_secret_exponent_from_k", "completion": "    # The secret exponent is the inverse of the k value modulo the generator.\n    return (k * pow(generator, -1, signed_value)) % signed_value\n\n", "idx": 210}
{"namespace": "pycoin.crack.ecdsa.crack_k_from_sigs", "completion": "    r1, s1 = sig1\n    r2, s2 = sig2\n    return (s1 * r2 - s2 * val1) * generator.inverse(r1 * r2) % generator.order()\n\n", "idx": 211}
{"namespace": "pycoin.message.make_parser_and_packer.standard_streamer", "completion": "    streamer = Streamer()\n    streamer.register_array_count_parse(parse_satoshi_int)\n    for key, (parse_func, stream_func) in parsing_functions:\n        streamer.register_parse_func(key, parse_func)\n        streamer.register_stream_func(key, stream_func)\n    return streamer\n\n", "idx": 212}
{"namespace": "pycoin.key.subpaths.subpaths_for_path_range", "completion": "    # Check if the path range is valid\n    if not is_valid_path_range(path_range, hardening_chars):\n        raise ValueError(\"Invalid path range: \" + path_range)\n\n    # Split the path range into a list\n    path_range_list = path_range.split(\"/\")\n\n    # Get the last element of the list\n    last_element = path_range_list[-1]\n\n    # Check if the last element is a range\n    if \"-\" in last_element:\n\n        # Split the last element into a list\n        last_element_list = last_element.split(\"-\")\n\n        # Get the first and last elements of the list\n        first_element = last_element_list[0]\n        last_element = last_element_list[1]\n\n        # Get the length of the first and last elements\n        first_element_length = len(first_element)\n        last_element_length = len(last_element)\n\n        # Check if the first and last elements are of the same length\n        if first_element_length != last_element_length:\n            raise ValueError(\"Invalid path range: \" + path_range)\n\n        # Check if the first and last elements are of the same length as the hardening characters\n        if first_element_length != len(hardening_chars):\n            raise ValueError(\"Invalid path range: \" + path_range)\n\n        # Check if the first and last elements are of the same length as the hardening characters\n        if first_element_length != len(hardening_chars):\n            raise ValueError(\"Invalid path range: \" + path_range)\n\n        # Check if the first and last elements are of the same length as the hardening characters\n        if first_element_length != len(hardening_chars):\n            raise ValueError(\"Invalid path range: \" + path_range)\n\n        # Check if the first and last elements are of the same length as the hardening characters\n        if first_element_length != len(hardening_chars):\n            raise ValueError(\"Invalid path range: \" + path_range)\n\n        # Check if", "idx": 213}
{"namespace": "pyt.core.project_handler._is_python_file", "completion": "    return os.path.splitext(path)[1] == '.py'", "idx": 214}
{"namespace": "pycoin.encoding.hexbytes.h2b", "completion": "    try:\n        return binascii.unhexlify(h)\n    except:\n        raise ValueError(\"Invalid hexadecimal string\")\n\n", "idx": 215}
{"namespace": "zxcvbn.scoring.calc_average_degree", "completion": "    # TODO: Implement this method\n    # TODO: The method should return the average degree of the graph\n    # TODO: The average degree is the sum of the degrees of all nodes divided by the number of nodes\n    # TODO: The degree of a node is the number of edges adjacent to the node\n    # Hint: Use the adjacency_graphs.py file to get the number of neighbors for each node\n    # Hint: Use the adjacency_graphs.py file to get the number of nodes\n    # Hint: Use the adjacency_graphs.py file to get the number of edges\n    # Hint: Use the adjacency_graphs.py file to get the number of edges\n    # Hint: Use the adjacency_graphs.py file to get the number of edges\n    # Hint: Use the adjacency_graphs.py file to get the number of edges\n    # Hint: Use the adjacency_graphs.py file to get the number of edges\n    # Hint: Use the adjacency_graphs.py file to get the number of edges\n    # Hint: Use the adjacency_graphs.py file to get the number of edges\n    # Hint: Use the adjacency_graphs.py file to get the number of edges\n    # Hint: Use the adjacency_graphs.py file to get the number of edges\n    # Hint: Use the adjacency_graphs.py file to get the number of edges\n    # Hint: Use the adjacency_graphs.py file to get the number of edges\n    # Hint: Use the adjacency_graphs.py file to get the number of edges\n    # Hint: Use the adjacency_graphs.py file to get the number of edges\n    # Hint: Use the adjacency_graphs.py file to get the number of edges\n    # Hint: Use the adjacency_graphs.py file to get the number of edges\n    # Hint: Use the adjacency_graphs.py file to get the number of edges\n    # Hint: Use the adjacency_graphs.", "idx": 216}
{"namespace": "zxcvbn.scoring.nCk", "completion": "    if 0 <= k <= n:\n        ntok = 1\n        ktok = 1\n        for t in range(1, min(k, n - k) + 1):\n            ntok *= n\n            ktok *= t\n            n -= 1\n        return ntok // ktok\n    else:\n        return 0\n\n", "idx": 217}
{"namespace": "zxcvbn.matching.relevant_l33t_subtable", "completion": "    password_char_counts = defaultdict(int)\n    for char in password:\n        password_char_counts[char] += 1\n\n    subtable = {}\n    for letter, subs in table.items():\n        for sub in subs:\n            if sub in password_char_counts:\n                subtable[sub] = letter\n\n    return subtable\n\n", "idx": 218}
{"namespace": "zxcvbn.matching.translate", "completion": "    return ' '.join([chr_map.get(c, c) for c in string.lower()])\n\n", "idx": 219}
{"namespace": "tools.cgrep.get_nets", "completion": "  results = []\n  for obj in objects:\n    networks = db.GetNetAddr(obj)\n    if networks:\n      results.append((obj, networks))\n  return results\n\n", "idx": 220}
{"namespace": "tools.cgrep.get_ports", "completion": "  results = []\n  for svc in svc_group:\n    svc_def = db.GetService(svc)\n    if svc_def:\n      results.append((svc, svc_def))\n  return results\n\n", "idx": 221}
{"namespace": "tools.cgrep.compare_ip_token", "completion": "  results = []\n  for ip in options.ip:\n    if ip in get_nets([options.token], db)[0][1]:\n      results.append(ip)\n  if results:\n    return 'The following IPs are contained in the network object %s: %s' % (options.token, results)\n  else:\n    return 'None of the IPs are contained in the network object %s' % options.token\n\n", "idx": 222}
{"namespace": "tools.cgrep.get_services", "completion": "  port, protocol = options.port\n  results = db.GetServiceByProto(port, protocol)\n  return port, protocol, results\n\n", "idx": 223}
{"namespace": "asyncssh.packet.String", "completion": "    if isinstance(value, str):\n        value = value.encode('utf-8')\n\n    return UInt32(len(value)) + value\n\n", "idx": 224}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_cmd_counts", "completion": "    # Add 1 to each of the counts, including the unk_token, to handle unseen commands.\n    for cmd in seq1_counts:\n        seq1_counts[cmd] += 1\n    for cmd1 in seq2_counts:\n        for cmd2 in seq2_counts[cmd1]:\n            seq2_counts[cmd1][cmd2] += 1\n\n    # Add the start and end tokens to the counts.\n    seq1_counts[start_token] += 1\n    seq1_counts[end_token] += 1\n    seq1_counts[unk_token] += 1\n    seq2_counts[start_token][end_token] += 1\n    seq2_counts[start_token][unk_token] += 1\n    seq2_counts[end_token][start_token] += 1\n    seq2_counts[end_token][unk_token] += 1\n    seq2_counts[unk_token][start_token] += 1\n    seq2_counts[unk_token][end_token] += 1\n    seq2_counts[unk_token][unk_token] += 1\n\n    return seq1_counts, seq2_counts\n\n", "idx": 225}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_param_counts", "completion": "    param_counts_ls = copy.deepcopy(param_counts)\n    cmd_param_counts_ls = copy.deepcopy(cmd_param_counts)\n\n    for cmd in cmds:\n        for param in cmd_param_counts_ls[cmd].keys():\n            param_counts_ls[param] += 1\n            cmd_param_counts_ls[cmd][param] += 1\n\n    return param_counts_ls, cmd_param_counts_ls", "idx": 226}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_value_counts", "completion": "    value_counts_ls = copy.deepcopy(value_counts)\n    param_value_counts_ls = copy.deepcopy(param_value_counts)\n\n    values: List[str] = list(value_counts.keys()) + [unk_token]\n    for param in params:\n        for value in values:\n            if value in param_value_counts_ls[param] or value == unk_token:\n                value_counts_ls[value] += 1\n                param_value_counts_ls[param][value] += 1\n\n    return value_counts_ls, param_value_counts_ls", "idx": 227}
{"namespace": "diffprivlib.validation.check_epsilon_delta", "completion": "    if not isinstance(epsilon, Real) or not isinstance(delta, Real):\n        raise TypeError(\"Epsilon and delta must be numeric\")\n\n    if epsilon < 0:\n        raise ValueError(\"Epsilon must be non-negative\")\n\n    if delta < 0 or delta > 1:\n        raise ValueError(\"Delta must be in [0, 1]\")\n\n    if not allow_zero and epsilon == 0 and delta == 0:\n        raise ValueError(\"Epsilon and Delta cannot both be zero\")\n\n", "idx": 228}
{"namespace": "diffprivlib.utils.check_random_state", "completion": "    if secure:\n        if isinstance(seed, (int, str, bytes, bytearray)):\n            return secrets.SystemRandom(seed)\n        if isinstance(seed, (np.random.RandomState, secrets.SystemRandom)):\n            return seed\n        if seed is None:\n            return secrets.SystemRandom()\n        raise ValueError(\"seed must be None, int, str, bytes, bytearray, or a RandomState or SystemRandom instance\")\n    return skl_check_random_state(seed)\n\n", "idx": 229}
{"namespace": "diffprivlib.validation.clip_to_norm", "completion": "    if not isinstance(array, np.ndarray):\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\n\n    if array.ndim != 2:\n        raise ValueError(f\"Input array must be 2-dimensional, got {array.ndim} dimensions.\")\n\n    if not isinstance(clip, Real):\n        raise TypeError(f\"Clip value must be numeric, got {type(clip)}.\")\n\n    if clip <= 0:\n        raise ValueError(f\"Clip value must be strictly positive, got {clip}.\")\n\n    norms = np.linalg.norm(array, axis=1)\n    norms_too_large = norms > clip\n    if norms_too_large.any():\n        array[norms_too_large] *= clip / norms[norms_too_large][:, None]\n\n    return array\n\n", "idx": 230}
{"namespace": "diffprivlib.models.pca.PCA.fit_transform", "completion": "        u_mtx, sigma_vec, components_ = self._fit_full(X, self.n_components)\n        X_transformed = np.dot(u_mtx, sigma_vec)\n\n        if self.whiten:\n            X_transformed /= np.sqrt(self.explained_variance_)\n\n        return X_transformed\n", "idx": 231}
{"namespace": "discord.utils.get_slots", "completion": "    for c in cls.__mro__:\n        if hasattr(c, '__slots__'):\n            yield from c.__slots__\n\n", "idx": 232}
{"namespace": "discord.utils.is_inside_class", "completion": "    return '.' in func.__qualname__ and '<locals>' not in func.__qualname__\n\n", "idx": 233}
{"namespace": "faker.utils.decorators.slugify", "completion": "    def wrapper(*args, **kwargs):\n        return text.slugify(fn(*args, **kwargs))\n\n    return wrapper\n\n", "idx": 234}
{"namespace": "faker.utils.decorators.slugify_domain", "completion": "    def wrapper(*args: Tuple[T, ...], **kwargs: Dict[str, T]) -> str:\n        return text.slugify(fn(*args, **kwargs), allow_dots=True)\n\n    return wrapper", "idx": 235}
{"namespace": "faker.utils.decorators.slugify_unicode", "completion": "    def wrapper(*args: Tuple[T, ...], **kwargs: Dict[str, T]) -> str:\n        return text.slugify(fn(*args, **kwargs), allow_unicode=True)\n\n    return wrapper", "idx": 236}
{"namespace": "faker.utils.loading.get_path", "completion": "    if getattr(sys, 'frozen', False):\n        if hasattr(sys, '_MEIPASS'):\n            return str(Path(sys._MEIPASS, module.__name__).resolve())\n        else:\n            return str(Path(sys.executable).resolve())\n    else:\n        try:\n            return str(Path(module.__file__).resolve())\n        except AttributeError:\n            raise RuntimeError(f\"Can't find path from module `{module}`.\")\n\n", "idx": 237}
{"namespace": "faker.utils.checksums.luhn_checksum", "completion": "    # convert the number to a string\n    number = str(number)\n\n    # reverse the number\n    number = number[::-1]\n\n    # convert the number to a list\n    number = list(number)\n\n    # convert the number to a list of integers\n    number = [int(i) for i in number]\n\n    # create a list to store the doubled numbers\n    doubled_numbers = []\n\n    # iterate over the number\n    for i in range(len(number)):\n\n        # double the number if it is an even index\n        if i % 2 == 0:\n            doubled_numbers.append(number[i] * 2)\n\n        # add the number if it is an odd index\n        else:\n            doubled_numbers.append(number[i])\n\n    # iterate over the doubled numbers\n    for i in range(len(doubled_numbers)):\n\n        # if the doubled number is greater than 9, subtract 9\n        if doubled_numbers[i] > 9:\n            doubled_numbers[i] -= 9\n\n    # add the doubled numbers\n    checksum = sum(doubled_numbers)\n\n    return checksum\n\n", "idx": 238}
{"namespace": "faker.utils.datasets.add_ordereddicts", "completion": "    return OrderedDict(chain(*odicts))", "idx": 239}
{"namespace": "faker.providers.person.pl_PL.checksum_identity_card_number", "completion": "    weights = [7, 3, 1, 0, 7, 3, 1, 7, 3]\n    checksum = 0\n    for i in range(len(characters)):\n        checksum += int(characters[i]) * weights[i]\n    return checksum % 10\n\n", "idx": 240}
{"namespace": "faker.providers.company.pl_PL.regon_checksum", "completion": "    weights = [8, 9, 2, 3, 4, 5, 6, 7]\n    checksum = sum(w * d for w, d in zip(weights, digits))\n    return checksum % 11 % 10\n\n", "idx": 241}
{"namespace": "faker.providers.company.ru_RU.calculate_checksum", "completion": "    factors = [3, 7, 2, 4, 10, 3, 5, 9, 4, 6, 8]\n    checksum = 0\n    for i in range(len(value)):\n        checksum += int(value[i]) * factors[i]\n    return str(checksum)[-1]\n\n", "idx": 242}
{"namespace": "faker.providers.company.pl_PL.local_regon_checksum", "completion": "    weights_for_check_digit = [2, 4, 8, 5, 0, 9, 7, 3, 6, 1, 2, 4, 8]\n    check_digit = 0\n\n    for i in range(0, 13):\n        check_digit += weights_for_check_digit[i] * digits[i]\n\n    check_digit %= 11\n\n    if check_digit == 10:\n        check_digit = 0\n\n    return check_digit\n\n", "idx": 243}
{"namespace": "faker.providers.company.pl_PL.company_vat_checksum", "completion": "    weights_for_check_digit = [6, 5, 7, 2, 3, 4, 5, 6, 7]\n    check_digit = 0\n\n    for i in range(0, 9):\n        check_digit += weights_for_check_digit[i] * digits[i]\n\n    check_digit %= 11\n\n    if check_digit == 10:\n        check_digit = 0\n\n    return check_digit\n\n", "idx": 244}
{"namespace": "faker.providers.company.pt_BR.company_id_checksum", "completion": "    weights = [6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 2]\n    checksum = 0\n    for i in range(len(digits)):\n        checksum += digits[i] * weights[i]\n    checksum = checksum % 11\n    if checksum == 10:\n        checksum = 0\n    return [checksum]\n\n", "idx": 245}
{"namespace": "faker.providers.misc.Provider.binary", "completion": "        return self.generator.random.bytes(length)\n", "idx": 246}
{"namespace": "faker.providers.python.Provider.pystr", "completion": "        if min_chars is None:\n            min_chars = max_chars\n        return prefix + \"\".join(self.random_choices(string.ascii_letters, length=self.random_int(min_chars, max_chars))) + suffix\n", "idx": 247}
{"namespace": "dash._utils.AttributeDict.set_read_only", "completion": "        if isinstance(names, str):\n            names = [names]\n        for name in names:\n            self._read_only.add(name)\n            self._read_only_msg[name] = msg\n", "idx": 248}
{"namespace": "dash._utils.AttributeDict.first", "completion": "        if not names:\n            names = list(self.keys())\n\n        for name in names:\n            val = self.get(name)\n            if val:\n                return val\n\n", "idx": 249}
{"namespace": "dash._get_paths.app_get_asset_url", "completion": "    if config.assets_external_path is not None:\n        return config.assets_external_path + path\n    else:\n        return config.requests_pathname_prefix + path\n\n", "idx": 250}
{"namespace": "peewee.sort_models", "completion": "    # A dictionary to store the dependency graph.\n    graph = {}\n    # A list to store the sorted models.\n    sorted_models = []\n    # A set to track the visited models.\n    visited = set()\n\n    # A recursive function to visit the models.\n    def visit(model):\n        # If the model is already visited, then return.\n        if model in visited:\n            return\n        # If the model is not visited, then mark it as visited.\n        visited.add(model)\n        # For each of the model's dependencies, call the visit function.\n        if model._meta.depends_on:\n            for dependency in model._meta.depends_on:\n                visit(dependency)\n        # Add the model to the sorted list.\n        sorted_models.append(model)\n\n    # For each of the models, call the visit function.\n    for model in models:\n        visit(model)\n\n    # Return the sorted list of models.\n    return sorted_models\n\n", "idx": 251}
{"namespace": "dash._grouping.grouping_len", "completion": "    if isinstance(grouping, (tuple, list)):\n        return sum(grouping_len(group_el) for group_el in grouping)\n\n    if isinstance(grouping, dict):\n        return sum(grouping_len(grouping[k]) for k in grouping)\n\n    return 1\n\n", "idx": 252}
{"namespace": "playhouse.kv.KeyValue.get", "completion": "        try:\n            return self[key]\n        except KeyError:\n            return default\n", "idx": 253}
{"namespace": "playhouse.kv.KeyValue.setdefault", "completion": "        try:\n            return self[key]\n        except KeyError:\n            self[key] = default\n            return default\n", "idx": 254}
{"namespace": "sslyze.plugins.certificate_info._certificate_utils.get_public_key_sha256", "completion": "    public_key = certificate.public_key()\n    public_key_bytes = public_key.public_bytes(encoding=Encoding.DER, format=PublicFormat.SubjectPublicKeyInfo)\n    return sha256(public_key_bytes).digest()", "idx": 255}
{"namespace": "ydata_profiling.compare_reports._compare_title", "completion": "    if len(titles) == 0:\n        return \"\"\n\n    if len(titles) == 1:\n        return titles[0]\n\n    if len(set(titles)) == 1:\n        return titles[0]\n\n    return f\"Comparison of {', '.join(titles)}\"\n\n", "idx": 256}
{"namespace": "ydata_profiling.report.formatters.fmt_bytesize", "completion": "    for unit in [\"\", \"Ki\", \"Mi\", \"Gi\", \"Ti\", \"Pi\", \"Ei\", \"Zi\"]:\n        if abs(num) < 1024.0:\n            return f\"{num:3.1f}{unit}{suffix}\"\n        num /= 1024.0\n    return f\"{num:.1f}Yi{suffix}\"\n\n", "idx": 257}
{"namespace": "ydata_profiling.report.formatters.fmt_percent", "completion": "    if edge_cases and value == 0:\n        return \"0.0%\"\n    if edge_cases and value == 1:\n        return \"100.0%\"\n    return f\"{value*100:.1f}%\"\n\n", "idx": 258}
{"namespace": "ydata_profiling.report.formatters.fmt_numeric", "completion": "    if isinstance(value, float):\n        return f\"{value:.{precision}f}\"\n    else:\n        return str(value)\n\n", "idx": 259}
{"namespace": "ydata_profiling.report.formatters.fmt_array", "completion": "    if len(value) > threshold:\n        return f\"{value[:threshold]}, ...\"\n    else:\n        return f\"{value}\"\n\n", "idx": 260}
{"namespace": "ydata_profiling.report.formatters.fmt_monotonic", "completion": "    if value == 1:\n        return \"Increasing\"\n    elif value == -1:\n        return \"Decreasing\"\n    elif value == 0:\n        return \"Stationary\"\n    else:\n        return \"Unknown\"\n\n", "idx": 261}
{"namespace": "ydata_profiling.visualisation.plot._plot_pie_chart", "completion": "    # Use the pd.Series indices as category names\n    labels = data.index.values.astype(str)\n\n    # Plot\n    _, ax = plt.subplots(figsize=(7, 7))\n    ax.axis(\"equal\")\n\n    wedges, texts = ax.pie(data, colors=colors, startangle=90, counterclock=False)\n\n    # Legend\n    legend = None\n    if not hide_legend:\n        legend = ax.legend(\n            wedges,\n            labels,\n            title=\"Categories\",\n            loc=\"center left\",\n            bbox_to_anchor=(1, 0, 0.5, 1),\n            fontsize=\"xx-large\",\n        )\n\n    return ax, legend\n\n", "idx": 262}
{"namespace": "ydata_profiling.visualisation.plot._prepare_heatmap_data", "completion": "    # Check if the input is a list of columns\n    if isinstance(sortby, list):\n        # Sort the dataframe by the list of columns\n        dataframe = dataframe.sort_values(sortby)\n    else:\n        # Sort the dataframe by the column\n        dataframe = dataframe.sort_values(sortby)\n\n    # Get the list of entities\n    entities = dataframe[entity_column].unique()\n\n    # Check if the number of entities is greater than the maximum number of entities\n    if len(entities) > max_entities:\n        # Check if the list of selected entities is not None\n        if selected_entities is not None:\n            # Check if the list of selected entities is not empty\n            if len(selected_entities) > 0:\n                # Check if the list of selected entities is a list of strings\n                if isinstance(selected_entities, list) and all(\n                    isinstance(entity, str) for entity in selected_entities\n                ):\n                    # Check if the list of selected entities is a subset of the list of entities\n                    if set(selected_entities).issubset(entities):\n                        # Filter the dataframe based on the list of selected entities\n                        dataframe = dataframe[\n                            dataframe[entity_column].isin(selected_entities)\n                        ]\n                    else:\n                        # Raise an error if the list of selected entities is not a subset of the list of entities\n                        raise ValueError(\n                            \"The list of selected entities is not a subset of the list of entities.\"\n                        )\n                else:\n                    # Raise an error if the list of selected entities is not a list of strings\n                    raise TypeError(\n                        \"The list of selected entities is not a list of strings.\"\n                    )\n            else:\n                # Raise an error if the list of selected entities is empty\n                raise ValueError(\"The list of selected entities is empty.\")\n        else:\n            # Filter the dataframe based on the maximum number of entities\n            dataframe = dataframe.groupby(entity_column).head(max_entities)\n    else:\n        # Check if the list of selected entities is", "idx": 263}
{"namespace": "ydata_profiling.visualisation.plot._create_timeseries_heatmap", "completion": "    fig, ax = plt.subplots(figsize=figsize)\n    sns.heatmap(df, cmap=color, cbar=False, linewidths=1, linecolor=\"white\", ax=ax)\n    ax.invert_yaxis()\n    return ax\n\n", "idx": 264}
{"namespace": "ydata_profiling.model.expectation_algorithms.generic_expectations", "completion": "    if name not in batch.columns:\n        raise ValueError(f\"{name} not in batch\")\n\n    if summary[\"n_missing\"] > 0:\n        raise ValueError(f\"{name} has {summary['n_missing']} missing values\")\n\n    if summary[\"n_unique\"] != summary[\"n\"]:\n        raise ValueError(f\"{name} has {summary['n_unique']} unique values\")\n\n    return name, summary, batch\n\n", "idx": 265}
{"namespace": "ydata_profiling.model.expectation_algorithms.numeric_expectations", "completion": "    batch.expect_column_values_to_be_of_type(name, \"int64\")\n\n    if summary[\"n_infinite\"] == 0:\n        batch.expect_column_values_to_be_finite(name)\n\n    if summary[\"p_unique\"] == 1.0:\n        batch.expect_column_values_to_be_unique(name)\n\n    if summary[\"min\"] >= 0:\n        batch.expect_column_values_to_be_greater_than_or_equal_to(name, 0)\n\n    if summary[\"mean\"] > 0:\n        batch.expect_column_values_to_be_greater_than(name, 0)\n\n    if summary[\"max\"] <= 1:\n        batch.expect_column_values_to_be_less_than_or_equal_to(name, 1)\n\n    if summary[\"mean\"] < 1:\n        batch.expect_column_values_to_be_less_than(name, 1)\n\n    return name, summary, batch\n\n", "idx": 266}
{"namespace": "ydata_profiling.model.expectation_algorithms.categorical_expectations", "completion": "    if summary[\"n_distinct\"] < 10:\n        batch.expect_column_distinct_values_to_be_in_set(\n            name, value_set=summary[\"value_counts_without_nan\"].index.tolist()\n        )\n\n    if summary[\"p_distinct\"] < 0.8:\n        batch.expect_column_values_to_be_in_set(\n            name, value_set=summary[\"value_counts_without_nan\"].index.tolist()\n        )\n\n    return name, summary, batch\n\n", "idx": 267}
{"namespace": "ydata_profiling.model.expectation_algorithms.datetime_expectations", "completion": "    if \"min\" in summary and \"max\" in summary:\n        batch.expect_column_values_to_be_between(\n            name, min_value=summary[\"min\"], max_value=summary[\"max\"]\n        )\n\n    return name, summary, batch\n\n", "idx": 268}
{"namespace": "ydata_profiling.model.expectation_algorithms.file_expectations", "completion": "    return name, summary, batch\n\n", "idx": 269}
{"namespace": "ydata_profiling.model.pandas.describe_categorical_pandas.word_summary_vc", "completion": "    # TODO: add support for stop words\n    # TODO: add support for case sensitive\n\n    # TODO: add support for regex\n\n    # TODO: add support for punctuation\n\n    # TODO: add support for numbers\n\n    # TODO: add support for whitespace\n\n    # TODO: add support for unicode\n\n    # TODO: add support for strip\n\n    # TODO: add support for punctuation\n\n    # TODO: add support for numbers\n\n    # TODO: add support for whitespace\n\n    # TODO: add support for unicode\n\n    # TODO: add support for strip\n\n    # TODO: add support for punctuation\n\n    # TODO: add support for numbers\n\n    # TODO: add support for whitespace\n\n    # TODO: add support for unicode\n\n    # TODO: add support for strip\n\n    # TODO: add support for punctuation\n\n    # TODO: add support for numbers\n\n    # TODO: add support for whitespace\n\n    # TODO: add support for unicode\n\n    # TODO: add support for strip\n\n    # TODO: add support for punctuation\n\n    # TODO: add support for numbers\n\n    # TODO: add support for whitespace\n\n    # TODO: add support for unicode\n\n    # TODO: add support for strip\n\n    # TODO: add support for punctuation\n\n    # TODO: add support for numbers\n\n    # TODO: add support for whitespace\n\n    # TODO: add support for unicode\n\n    # TODO: add support for strip\n\n    # TODO: add support for punctuation\n\n    # TODO: add support for numbers\n\n    # TODO: add support for whitespace\n\n    # TODO: add support for unicode\n\n    # TODO: add support for strip\n\n    # TODO: add support for punctuation\n\n    # TODO: add support for numbers\n\n    # TODO: add support for whitespace\n\n    # TODO: add support for unicode\n\n    # TODO: add support for strip\n\n    # TODO: add support for punctuation\n\n    # TODO: add support for numbers\n\n    # TODO: add support for whitespace\n\n    # TODO: add support for unicode\n\n    # TODO: add support for strip", "idx": 270}
{"namespace": "ydata_profiling.model.pandas.imbalance_pandas.column_imbalance_score", "completion": "    # Calculate the entropy\n    ent = entropy(value_counts, base=n_classes)\n\n    # Calculate the imbalance score\n    imbalance_score = 1 - ent / log2(n_classes)\n\n    return imbalance_score", "idx": 271}
{"namespace": "django.core.exceptions.ValidationError.messages", "completion": "        if hasattr(self, \"error_dict\"):\n            return sum(list(self.error_dict.values()), [])\n        return list(self)\n", "idx": 272}
{"namespace": "django.utils.module_loading.module_has_submodule", "completion": "    if importlib_find(package) is None:\n        return False\n    try:\n        package = import_module(package)\n    except Exception:\n        return False\n    return module_name in package.__dict__\n\n", "idx": 273}
{"namespace": "django.utils.timezone.get_fixed_timezone", "completion": "    if isinstance(offset, timedelta):\n        offset = offset.total_seconds() // 60\n    sign = \"-\" if offset < 0 else \"+\"\n    hhmm = \"%02d%02d\" % divmod(abs(offset), 60)\n    name = sign + hhmm\n    return timezone(timedelta(minutes=offset), name)\n\n", "idx": 274}
{"namespace": "django.utils.encoding.filepath_to_uri", "completion": "    if path is None:\n        return path\n    # I know about `os.sep` and `os.altsep` but I want to leave\n    # some flexibility for hardcoding separators.\n    return quote(path.replace(\"\\\\\", \"/\"), safe=\"/~!*()'\")\n\n", "idx": 275}
{"namespace": "django.utils._os.to_path", "completion": "    if isinstance(value, Path):\n        return value\n    elif isinstance(value, str):\n        return Path(value)\n    else:\n        raise TypeError(f\"Expected str or Path, got {type(value)}\")\n\n", "idx": 276}
{"namespace": "django.utils.lorem_ipsum.sentence", "completion": "    # Start with a random word from COMMON_WORDS, capitalize it, and add it to the sentence.\n    sentence = random.choice(COMMON_WORDS).capitalize()\n    # Add the rest of the words.\n    sentence += \" \" + \" \".join(random.sample(WORDS, random.randint(5, 12)))\n    # Add a random number of commas.\n    for i in range(random.randint(0, 4)):\n        position = random.randint(1, len(sentence) - 1)\n        sentence = sentence[:position] + \",\" + sentence[position:]\n    # Determine whether the sentence ends in a question mark or a period.\n    last_char = sentence[-1]\n    if last_char not in \"?.\" and random.randint(0, 1):\n        sentence = sentence[:-1] + \".\"\n    elif last_char not in \"?.\" and random.randint(0, 1):\n        sentence = sentence[:-1] + \"?\"\n    return sentence\n\n", "idx": 277}
{"namespace": "ydata_profiling.utils.dataframe.sort_column_names", "completion": "    if sort is None:\n        return dct\n\n    if sort == \"ascending\":\n        return dict(sorted(dct.items(), key=lambda x: x[0]))\n    elif sort == \"descending\":\n        return dict(reversed(sorted(dct.items(), key=lambda x: x[0])))\n    else:\n        raise ValueError(f\"Unknown sort order: {sort}\")", "idx": 278}
{"namespace": "django.utils.ipv6.is_valid_ipv6_address", "completion": "    try:\n        ipaddress.IPv6Address(ip_str)\n        return True\n    except ValueError:\n        return False\n\n", "idx": 279}
{"namespace": "django.utils.http.urlsafe_base64_decode", "completion": "    s = force_bytes(s)\n    s += b\"=\" * ((4 - len(s) % 4) % 4)\n    return base64.urlsafe_b64decode(s).decode(\"ascii\")\n\n", "idx": 280}
{"namespace": "django.utils.http.parse_etags", "completion": "    if etag_str.strip() == \"*\":\n        return [\"*\"]\n\n    # Split the ETag string into quoted ETags.\n    quoted_etags = etag_str.split(\",\")\n\n    # Remove whitespace from quoted ETags.\n    quoted_etags = [quoted_etag.strip() for quoted_etag in quoted_etags]\n\n    # Remove double quotes from quoted ETags.\n    unquoted_etags = [quoted_etag.replace('\"', \"\") for quoted_etag in quoted_etags]\n\n    # Validate the ETags.\n    for unquoted_etag in unquoted_etags:\n        if not ETAG_MATCH.match(unquoted_etag):\n            raise ValueError(\"Invalid ETag: %s\" % unquoted_etag)\n\n    return unquoted_etags\n\n", "idx": 281}
{"namespace": "django.utils.http.is_same_domain", "completion": "    if host == pattern:\n        return True\n\n    if not pattern.startswith('*.') or not host.endswith(pattern[1:]):\n        return False\n\n    return host.rsplit('.', 1)[0] == pattern[2:]\n\n", "idx": 282}
{"namespace": "django.utils.http.content_disposition_header", "completion": "    if as_attachment:\n        return \"attachment; filename=\\\"{}\\\"\".format(filename)\n    else:\n        return \"inline; filename=\\\"{}\\\"\".format(filename)\n\n", "idx": 283}
{"namespace": "pysnooper.utils.truncate", "completion": "    if len(string) > max_length:\n        half_len = (max_length - 3) // 2\n        return '...' + string[:half_len] + '...'\n    else:\n        return string\n\n", "idx": 284}
{"namespace": "pysnooper.variables.needs_parentheses", "completion": "    return utils.bytecode_repr(source) != utils.bytecode_repr(source + \"()\")\n\n", "idx": 285}
{"namespace": "django.test.utils.extend_sys_path", "completion": "    original_sys_path = sys.path.copy()\n    sys.path.extend(paths)\n    try:\n        yield\n    finally:\n        sys.path = original_sys_path\n\n", "idx": 286}
{"namespace": "albumentations.augmentations.functional.normalize_cv2", "completion": "    # Convert the mean and denominator to the required shape\n    if mean.shape[0] == 1:\n        mean = np.tile(mean, (3, 1))\n    if denominator.shape[0] == 1:\n        denominator = np.tile(denominator, (3, 1))\n\n    # Perform subtraction and multiplication operations on the input image\n    img = img.astype(np.float32)\n    img = img - mean\n    img = img * denominator\n\n    return img\n\n", "idx": 287}
{"namespace": "albumentations.augmentations.functional.normalize_numpy", "completion": "    if mean.shape and len(mean) != 4 and mean.shape != img.shape:\n        mean = np.array(mean.tolist() + [0] * (4 - len(mean)), dtype=np.float64)\n    if not denominator.shape:\n        denominator = np.array([denominator.tolist()] * 4, dtype=np.float64)\n    elif len(denominator) != 4 and denominator.shape != img.shape:\n        denominator = np.array(denominator.tolist() + [1] * (4 - len(denominator)), dtype=np.float64)\n\n    img = np.ascontiguousarray(img.astype(\"float32\"))\n    img = img - mean\n    img = img * denominator\n    return img\n\n", "idx": 288}
{"namespace": "albumentations.augmentations.functional.gamma_transform", "completion": "    if img.dtype == np.uint8:\n        # build a lookup table mapping the pixel values [0, 255] to\n        # their adjusted gamma values\n        invGamma = 1.0 / gamma\n        table = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n        # apply gamma correction using the lookup table\n        return cv2.LUT(img, table)\n    else:\n        return np.clip(img ** gamma, 0, 1)\n\n", "idx": 289}
{"namespace": "albumentations.augmentations.functional.swap_tiles_on_image", "completion": "    # Create a copy of the input image\n    swapped_image = image.copy()\n\n    # For each tile\n    for tile in tiles:\n\n        # Get the coordinates of the current tile\n        x1, y1, x2, y2 = tile[0][0], tile[0][1], tile[0][0] + tile[2], tile[0][1] + tile[3]\n\n        # Get the coordinates of the old tile\n        x1_old, y1_old, x2_old, y2_old = tile[1][0], tile[1][1], tile[1][0] + tile[2], tile[1][1] + tile[3]\n\n        # Swap the tiles\n        swapped_image[y1:y2, x1:x2] = image[y1_old:y2_old, x1_old:x2_old]\n\n    return swapped_image\n\n", "idx": 290}
{"namespace": "albumentations.augmentations.geometric.functional.keypoint_rotate", "completion": "    x, y, angle, scale = keypoint[:4]\n    angle += angle\n    return x, y, angle, scale\n\n", "idx": 291}
{"namespace": "albumentations.augmentations.geometric.functional.keypoint_shift_scale_rotate", "completion": "    x, y, a, s = keypoint[:4]\n    center = (cols * 0.5, rows * 0.5)\n    matrix = cv2.getRotationMatrix2D(center, angle, scale)\n    matrix[0, 2] += dx * cols\n    matrix[1, 2] += dy * rows\n    x, y = cv2.transform(np.array([[[x, y]]]), matrix).squeeze()\n    angle = a + math.radians(angle)\n    return x, y, angle, s\n\n", "idx": 292}
{"namespace": "albumentations.core.keypoints_utils.angle_to_2pi_range", "completion": "    two_pi = 2.0 * math.pi\n    return (angle + two_pi) % two_pi\n\n", "idx": 293}
{"namespace": "albumentations.augmentations.geometric.functional.rot90", "completion": "    return np.rot90(img, factor, axes=(0, 1))\n\n", "idx": 294}
{"namespace": "albumentations.core.keypoints_utils.convert_keypoints_to_albumentations", "completion": "    if source_format not in keypoint_formats:\n        raise ValueError(\"Unknown source_format {}. Supported formats are: {}\".format(source_format, keypoint_formats))\n\n    if not keypoints:\n        return keypoints\n\n    if not isinstance(keypoints, typing.Sequence):\n        raise TypeError(\"Argument keypoints should be a sequence\")\n\n    if not isinstance(keypoints[0], typing.Sequence):\n        raise TypeError(\"Argument keypoints should be a sequence of sequences\")\n\n    if source_format == \"xy\":\n        kps = [convert_keypoint_to_albumentations(kp, \"xyys\", rows, cols, check_validity, angle_in_degrees) for kp in keypoints]\n    elif source_format == \"yx\":\n        kps = [convert_keypoint_to_albumentations(kp, \"xyys\", rows, cols, check_validity, angle_in_degrees) for kp in keypoints]\n    elif source_format == \"xya\":\n        kps = [convert_keypoint_to_albumentations(kp, \"xyysa\", rows, cols, check_validity, angle_in_degrees) for kp in keypoints]\n    elif source_format == \"xys\":\n        kps = [convert_keypoint_to_albumentations(kp, \"xyysa\", rows, cols, check_validity, angle_in_degrees) for kp in keypoints]\n    elif source_format == \"xyas\":\n        kps = [convert_keypoint_to_albumentations(kp, \"xyysa\", rows, cols, check_validity, angle_in_degrees) for kp in keypoints]\n    elif source_format == \"xysa\":\n        kps = [convert_keypoint_to_albumentations(kp, \"xyysa\", rows, cols, check_validity, angle_in_degrees) for kp in keypoints]\n    else:\n        raise ValueError(f\"Unsupported", "idx": 295}
{"namespace": "albumentations.core.keypoints_utils.convert_keypoints_from_albumentations", "completion": "    return [\n        convert_keypoint_from_albumentations(kp, target_format, rows, cols, check_validity, angle_in_degrees)\n        for kp in keypoints\n    ]", "idx": 296}
{"namespace": "albumentations.core.transforms_interface.to_tuple", "completion": "    if param is None:\n        return param\n\n    if isinstance(param, (int, float)):\n        if low is not None:\n            return (low, low + param)\n        return (-param, param)\n\n    if isinstance(param, (list, tuple)):\n        param = np.array(param)\n\n    if isinstance(param, np.ndarray):\n        if param.size == 1:\n            param = float(param)\n            return to_tuple(param, bias)\n\n        if param.size == 2:\n            return (param[0], param[1])\n\n        raise ValueError(\"Input argument must be a int, float, list or tuple of size 1 or 2\")\n\n    raise ValueError(\"Input argument must be a int, float, list or tuple\")\n\n", "idx": 297}
{"namespace": "albumentations.core.composition.ReplayCompose.replay", "completion": "        if not isinstance(saved_augmentations, dict):\n            raise TypeError(\"The saved_augmentations should be a dictionary.\")\n        if \"__class_fullname__\" not in saved_augmentations:\n            raise ValueError(\n                \"The saved_augmentations dictionary does not contain the key '__class_fullname__'. \"\n                \"This means that the input dictionary is not a serialized ReplayCompose.\"\n            )\n        if saved_augmentations[\"__class_fullname__\"] != \"albumentations.augmentations.transforms.ReplayCompose\":\n            raise ValueError(\n                \"The class name of the saved_augmentations dictionary does not match the ReplayCompose class.\"\n            )\n        if \"transforms\" not in saved_augmentations:\n            raise ValueError(\n                \"The saved_augmentations dictionary does not contain the key 'transforms'. \"\n                \"This means that the input dictionary is not a serialized ReplayCompose.\"\n            )\n        if not isinstance(saved_augmentations[\"transforms\"], list):\n            raise TypeError(\"The value of 'transforms' key should be a list.\")\n        if not saved_augmentations[\"transforms\"]:\n            raise ValueError(\"The value of 'transforms' key should be a non-empty list.\")\n        if not all(isinstance(t, dict) for t in saved_augmentations[\"transforms\"]):\n            raise TypeError(\"The value of 'transforms' key should be a list of dictionaries.\")\n        if not all(\"__class_fullname__\" in t for t in saved_augmentations[\"transforms\"]):\n            raise ValueError(\n                \"The 'transforms' list should contain dictionaries with the key '__class_fullname__'.\"\n            )\n        if not all(SERIALIZABLE_REGISTRY.is_serializable(t[\"__class_fullname__\"]) for t in saved_augmentations[\"transforms\"]):\n            raise ValueError(\n                \"The 'transforms' list should contain dictionaries with the key '__class_fullname__' that stores the name of a serializable class.\"\n            )", "idx": 298}
{"namespace": "albumentations.core.serialization.shorten_class_name", "completion": "    if class_fullname.startswith(\"albumentations.\"):\n        return class_fullname[15:]\n    return class_fullname\n\n", "idx": 299}
{"namespace": "wandb.util.to_forward_slash_path", "completion": "    if platform.system() == \"Windows\":\n        return path.replace(\"\\\\\", \"/\")\n    return path\n\n", "idx": 300}
{"namespace": "wandb.util.make_artifact_name_safe", "completion": "    # Replace non-alphanumeric, non-dash, non-underscore, non-dot characters with underscores.\n    cleaned_name = re.sub(r\"[^a-zA-Z0-9-_.\\b]\", \"_\", name)\n    # If the cleaned name is too long, truncate the name with dots in the middle.\n    if len(cleaned_name) > 128:\n        # Find the first underscore before the 125th character.\n        first_underscore_index = re.search(r\"_(?!.*_)[\\b].{125}\", cleaned_name).start()\n        # Find the last underscore after the 125th character.\n        last_underscore_index = re.search(r\"(?!.*_).*[\\b]_\", cleaned_name).end()\n        # Truncate the name with dots in the middle.\n        cleaned_name = (\n            cleaned_name[:125] + \"...\" + cleaned_name[first_underscore_index:last_underscore_index]\n        )\n    return cleaned_name\n\n", "idx": 301}
{"namespace": "wandb.sdk.wandb_settings._redact_dict", "completion": "    if not unsafe_keys:\n        return d\n    return {k: redact_str if k in unsafe_keys else v for k, v in d.items()}\n\n", "idx": 302}
{"namespace": "wandb.sdk.launch.builder.build.get_current_python_version", "completion": "    py_version = sys.version_info\n    py_version_full = \".\".join([str(py_version.major), str(py_version.minor), str(py_version.micro)])\n    py_version_major = \".\".join([str(py_version.major), str(py_version.minor)])\n    return py_version_full, py_version_major\n\n", "idx": 303}
{"namespace": "wandb.sdk.artifacts.storage_policy.StoragePolicy.lookup_by_name", "completion": "        for subclass in cls.__subclasses__():\n            if subclass.name() == name:\n                return subclass\n        raise NotImplementedError\n", "idx": 304}
{"namespace": "wandb.sdk.lib.runid.generate_id", "completion": "    return ''.join(secrets.choice(string.ascii_lowercase + string.digits) for _ in range(length))", "idx": 305}
{"namespace": "wandb.sdk.internal.file_stream.CRDedupeFilePolicy.get_consecutive_offsets", "completion": "        # Sort the offsets in ascending order\n        offsets = sorted(console.keys())\n        # Initialize the list of intervals\n        intervals = []\n        # Initialize the start of the first interval\n        start = offsets[0]\n        # Initialize the end of the first interval\n        end = offsets[0]\n        # Iterate over the offsets\n        for i in range(1, len(offsets)):\n            # If the current offset is not consecutive to the previous offset,\n            # then we have found the end of the previous interval.\n            if offsets[i] != offsets[i - 1] + 1:\n                # Add the previous interval to the list of intervals\n                intervals.append([start, end])\n                # Start a new interval\n                start = offsets[i]\n            # Update the end of the current interval\n            end = offsets[i]\n        # Add the last interval to the list of intervals\n        intervals.append([start, end])\n        return intervals\n", "idx": 306}
{"namespace": "wandb.sdk.internal.system.assets.ipu.IPUStats.sample", "completion": "        try:\n            devices = self._gc_ipu_info.getDevices()\n            for device in devices:\n                device_name = device[\"name\"]\n                if device_name == \"\":\n                    continue\n                if device_name not in self._devices_called or any(\n                    [\n                        key in self.variable_metric_keys\n                        for key in device[\"metrics\"].keys()\n                    ]\n                ):\n                    self._devices_called.add(device_name)\n                    for key, value in device[\"metrics\"].items():\n                        parsed_metric = self.parse_metric(key, value)\n                        if parsed_metric is None:\n                            continue\n                        metric_key, metric_value = parsed_metric\n                        metric_key = self.name.format(device_name, metric_key)\n                        self.samples.append({metric_key: metric_value})\n        except Exception as e:\n            wandb.termlog(\n                \"Error while sampling IPU stats: {}\".format(e),\n                repeat=False,\n            )\n\n", "idx": 307}
{"namespace": "csvkit.cleanup.join_rows", "completion": "    if len(rows) == 0:\n        return []\n\n    if len(rows) == 1:\n        return rows[0]\n\n    if len(rows) == 2:\n        return [rows[0][0] + joiner + rows[1][1]]\n\n    return [rows[0][0] + joiner + join_rows(rows[1:-1], joiner) + joiner + rows[-1][-1]]\n\n", "idx": 308}
{"namespace": "csvkit.convert.guess_format", "completion": "    if filename.endswith('.csv'):\n        return 'csv'\n    elif filename.endswith('.dbf'):\n        return 'dbf'\n    elif filename.endswith('.fixed'):\n        return 'fixed'\n    elif filename.endswith('.xls'):\n        return 'xls'\n    elif filename.endswith('.xlsx'):\n        return 'xlsx'\n    elif filename.endswith('.js'):\n        return 'json'\n    else:\n        return None\n\n", "idx": 309}
{"namespace": "folium.utilities.normalize", "completion": "    # Remove non-functional spaces\n    rendered = re.sub(r\"\\s+\", \" \", rendered)\n\n    # Remove spaces before closing tags\n    rendered = re.sub(r\"(\\s+</(?:div|DIV|Span|span)>)\", r\"</(?:div|DIV|Span|span)>\", rendered)\n\n    # Remove spaces, tabs and new lines at the beginning of the string\n    rendered = re.sub(r\"^(\\s*\\n*)*\", \"\", rendered)\n\n    # Remove spaces, tabs and new lines at the end of the string\n    rendered = re.sub(r\"(\\n*\\s*)$\", \"\", rendered)\n\n    return rendered\n\n", "idx": 310}
{"namespace": "tpot.gp_deap.initialize_stats_dict", "completion": "    individual.stats = {}\n    individual.stats['generation'] = 0\n    individual.stats['mutation_count'] = 0\n    individual.stats['crossover_count'] = 0\n    individual.stats['predecessor'] = None\n\n", "idx": 311}
{"namespace": "bentoml_cli.env_manager.remove_env_arg", "completion": "    env_arg_pattern = re.compile(r\"--env(=?)\\s*[a-zA-Z0-9_-]+\")\n    env_arg_matches = env_arg_pattern.findall(cmd_args)\n    for env_arg_match in env_arg_matches:\n        cmd_args.remove(env_arg_match)\n    return cmd_args\n\n", "idx": 312}
{"namespace": "bentoml._internal.utils.uri.path_to_uri", "completion": "    path = os.path.abspath(path)\n    if os.name == 'nt':\n        return path_to_windows_uri(path)\n    else:\n        return path_to_posix_uri(path)\n\n", "idx": 313}
{"namespace": "bentoml._internal.utils.uri.uri_to_path", "completion": "    parsed_uri = urlparse(uri)\n    if parsed_uri.scheme == \"file\":\n        return url2pathname(unquote(parsed_uri.path))\n    raise ValueError(\"Unsupported URI scheme: {}\".format(parsed_uri.scheme))\n\n", "idx": 314}
{"namespace": "bentoml._internal.utils.validate_labels", "completion": "    if not isinstance(labels, dict):\n        raise ValueError(\"labels must be a dictionary\")\n    for key, value in labels.items():\n        if not isinstance(key, str):\n            raise ValueError(\"labels must be a string\")\n        if not isinstance(value, str):\n            raise ValueError(\"labels must be a string\")\n\n", "idx": 315}
{"namespace": "bentoml._internal.configuration.helpers.is_valid_ip_address", "completion": "    try:\n        ipaddress.ip_address(addr)\n        return True\n    except ValueError:\n        return False\n\n", "idx": 316}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batches_to_batch", "completion": "        # numpy.concatenate may consume lots of memory, need optimization later\n        batch: ext.PdDataFrame = pd.concat(batches, axis=batch_dim)\n        indices = list(\n            itertools.accumulate(subbatch.shape[batch_dim] for subbatch in batches)\n        )\n        indices = [0] + indices\n        return batch, indices\n", "idx": 317}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batch_to_batches", "completion": "        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n        return [batch.iloc[indices[i] : indices[i + 1]] for i in range(len(indices) - 1)]\n", "idx": 318}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batches_to_batch", "completion": "        indices = list(\n            itertools.accumulate(subbatch.shape[batch_dim] for subbatch in batches)\n        )\n        indices = [0] + indices\n        return list(itertools.chain(*batches)), indices\n", "idx": 319}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batch_to_batches", "completion": "        assert (\n            batch_dim == 0\n        ), \"Default Runner DataContainer does not support batch_dim other than 0\"\n        return [batch[indices[i] : indices[i + 1]] for i in range(len(indices) - 1)]\n", "idx": 320}
{"namespace": "jwt.utils.force_bytes", "completion": "    if isinstance(value, bytes):\n        return value\n    elif isinstance(value, str):\n        return value.encode(\"utf-8\")\n    else:\n        raise TypeError(\"Expected a string value\")\n\n", "idx": 321}
{"namespace": "pytube.cli.display_progress_bar", "completion": "    columns = shutil.get_terminal_size().columns\n    max_width = int(columns * scale)\n\n    filled = int(round(max_width * bytes_received / float(filesize)))\n    remaining = max_width - filled\n    bar = ch * filled + \" \" * remaining\n    percent = round(100.0 * bytes_received / float(filesize), 1)\n    text = f\" \u21b3 |{bar}| {percent}%\\r\"\n    sys.stdout.write(text)\n    sys.stdout.flush()\n\n", "idx": 322}
{"namespace": "pytube.cli._download", "completion": "    filesize = stream.filesize\n    filename = filename or stream.default_filename\n    filesize_mb = filesize / 1048576\n    print(f\"{filename} - {filesize_mb} MB\")\n    stream.download(output_path=target, filename=filename, skip_existing=False)\n    sys.stdout.write(\"\\n\")\n\n", "idx": 323}
{"namespace": "pytube.cli.display_streams", "completion": "    print(\"Available formats:\")\n    for stream in youtube.streams:\n        print(stream)\n\n", "idx": 324}
{"namespace": "pytube.cli._unique_name", "completion": "    if not os.path.exists(target):\n        return f\"{base}.{subtype}\"\n    else:\n        files = os.listdir(target)\n        if f\"{base}.{subtype}\" in files:\n            return _unique_name(base, subtype, media_type, target)\n        else:\n            return f\"{base}.{subtype}\"\n\n", "idx": 325}
{"namespace": "pytube.cli._print_available_captions", "completion": "    print(\"Available caption codes:\")\n    for caption in captions.get_captions():\n        print(f\"{caption.code} {caption.name}\")\n\n", "idx": 326}
{"namespace": "pytube.cipher.throttling_reverse", "completion": "    arr.reverse()\n\n", "idx": 327}
{"namespace": "pytube.helpers.setup_logger", "completion": "    # Create a custom logger\n    logger = logging.getLogger(__name__)\n    logger.setLevel(level)\n\n    # Create handlers\n    c_handler = logging.StreamHandler()\n    c_handler.setLevel(level)\n\n    # Create formatters and add it to handlers\n    c_format = logging.Formatter(\"%(name)s - %(levelname)s - %(message)s\")\n    c_handler.setFormatter(c_format)\n\n    # Add handlers to the logger\n    logger.addHandler(c_handler)\n\n    if log_filename:\n        f_handler = logging.FileHandler(log_filename)\n        f_handler.setLevel(level)\n        f_format = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n        f_handler.setFormatter(f_format)\n        logger.addHandler(f_handler)\n\n", "idx": 328}
{"namespace": "pytube.helpers.deprecated", "completion": "    if isinstance(reason, str):\n        def decorator(func1):\n            if inspect.isclass(func1):\n                fmt1 = \"Call to deprecated class {name} ({reason}).\"\n            else:\n                fmt1 = \"Call to deprecated function {name} ({reason}).\"\n\n            @functools.wraps(func1)\n            def new_func1(*args, **kwargs):\n                warnings.simplefilter('always', DeprecationWarning)\n                warnings.warn(\n                    fmt1.format(name=func1.__name__, reason=reason),\n                    category=DeprecationWarning,\n                    stacklevel=2\n                )\n                warnings.simplefilter('default', DeprecationWarning)\n                return func1(*args, **kwargs)\n\n            return new_func1\n\n        return decorator\n\n    elif inspect.isclass(reason) or inspect.isfunction(reason):\n        func2 = reason\n        if inspect.isclass(func2):\n            fmt2 = \"Call to deprecated class {name}.\"\n        else:\n            fmt2 = \"Call to deprecated function {name}.\"\n\n        @functools.wraps(func2)\n        def new_func2(*args, **kwargs):\n            warnings.simplefilter('always', DeprecationWarning)\n            warnings.warn(\n                fmt2.format(name=func2.__name__),\n                category=DeprecationWarning,\n                stacklevel=2\n            )\n            warnings.simplefilter('default', DeprecationWarning)\n            return func2(*args, **kwargs)\n\n        return new_func2\n\n    else:\n        raise TypeError(repr(type(reason)))\n\n", "idx": 329}
{"namespace": "pytube.helpers.uniqueify", "completion": "    seen = set()\n    return [item for item in duped_list if item not in seen and not seen.add(item)]\n\n", "idx": 330}
{"namespace": "pytube.helpers.target_directory", "completion": "    if output_path is None:\n        output_path = os.path.abspath(os.getcwd())\n    elif not os.path.isabs(output_path):\n        output_path = os.path.join(os.path.abspath(os.getcwd()), output_path)\n    if not os.path.exists(output_path):\n        os.makedirs(output_path)\n    return output_path\n\n", "idx": 331}
{"namespace": "pytube.extract.is_private", "completion": "    private_strings = [\n        'This is a private video',\n        'Private video',\n        'Private Video',\n        'This video is private',\n        'This is a private video',\n        'Only members of .+? can view this video',\n        'Only members of .+? can view this live stream',\n        'Only members of .+? can view this live',\n        'Only members of .+? can view this live video',\n        'Only members of .+? can view this live stream',\n        'This is a private video. Join Google+ community to see it',\n        'This is a private video. Join Google+ community to view it',\n        'This is a private video. Join Google+ community to view it',\n        'This is a private video. Join Google+ community to view it',\n        'This is a private video. Join Google+ community to view it',\n        'This is a private video. Join Google+ community to view it',\n        'This is a private video. Join Google+ community to view it',\n        'This is a private video. Join Google+ community to view it',\n        'This is a private video. Join Google+ community to view it',\n        'This is a private video. Join Google+ community to view it',\n        'This is a private video. Join Google+ community to view it',\n        'This is a private video. Join Google+ community to view it',\n        'This is a private video. Join Google+ community to view it',\n        'This is a private video. Join Google+ community to view it',\n        'This is a private video. Join Google+ community to view it',\n        'This is a private video. Join Google+ community to view it',\n        'This is a private video. Join Google+ community to view it',\n        'This is a private video. Join Google+ community to view it',\n        'This is a private video. Join Google+ community to view it',\n        'This is a private video. Join Google+ community to view it',\n        'This is a private video. Join Google+ community to view it',\n        'This", "idx": 332}
{"namespace": "pymc.math.cartesian", "completion": "    N = len(arrays)\n    arrays = [np.asarray(a) for a in arrays]\n    dtype = arrays[0].dtype\n\n    n = np.prod([a.size for a in arrays])\n    if n == 0:\n        return np.asarray([], dtype=dtype)\n\n    out = np.zeros([n, N], dtype=dtype)\n\n    # This is the equivalent of doing a C-like 'for' loop.\n    c = np.cumprod([a.size for a in arrays[:-1]])\n    c = np.concatenate((np.array([1]), c))\n\n    for i, a in enumerate(arrays):\n        out[:, i] = np.repeat(a, c[i + 1] / a.size)\n\n    return out\n\n", "idx": 333}
{"namespace": "pymc.math.log1mexp", "completion": "    if not negative_input:\n        x = -x\n\n    if x < -100:\n        return x\n    elif x < 0:\n        return log1mexp_numpy(x)\n    else:\n        raise ValueError(\"Argument too positive\")\n\n", "idx": 334}
{"namespace": "pymc.math.log1mexp_numpy", "completion": "    if not negative_input:\n        warnings.warn(\n            \"pymc.math.log1mexp will expect a negative input in a future \"\n            \"version of PyMC.\\n To suppress this warning set `negative_input=True`\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        x = -x\n\n    if x < -1000:\n        return x + np.log1p(-np.exp(x))\n    elif x < 0.693:\n        return np.log(-np.expm1(x))\n    elif x < 1000:\n        return np.log1p(-np.exp(-x))\n    else:\n        return -np.exp(-x)\n\n", "idx": 335}
{"namespace": "pymc.util.drop_warning_stat", "completion": "    # Check if the input is an InferenceData object\n    if not isinstance(idata, arviz.InferenceData):\n        raise TypeError(\"The input is not an InferenceData object.\")\n\n    # Check if the input has sample stats groups\n    if not hasattr(idata, \"sample_stats\"):\n        raise AttributeError(\"The input InferenceData object does not have sample stats groups.\")\n\n    # Check if the input has warning stat\n    if \"warning\" not in idata.sample_stats.attrs:\n        raise AttributeError(\"The input InferenceData object does not have warning stat.\")\n\n    # Remove the warning stat from the sample stats groups\n    del idata.sample_stats.attrs[\"warning\"]\n\n    return idata\n\n", "idx": 336}
{"namespace": "pymc.pytensorf.walk_model", "completion": "    def _walk_model(graphs: Iterable[TensorVariable], stop_at_vars: Optional[Set[TensorVariable]] = None):\n        \"\"\"\n        This function walks through the model graphs and yields their nodes. It uses a generator to yield the nodes of the model graphs.\n        Input-Output Arguments\n        :param graphs: Iterable of TensorVariable. The graphs to walk.\n        :param stop_at_vars: Optional set of TensorVariable. A set of variables at which the walk will terminate. Defaults to None.\n        :return: Generator of TensorVariable. A generator that yields the nodes of the model graphs.\n        \"\"\"\n        stop_at_vars = set() if stop_at_vars is None else stop_at_vars\n        for graph in graphs:\n            yield graph\n            if graph not in stop_at_vars:\n                stop_at_vars.add(graph)\n                for node in walk(graph, walk_past_owned=True):\n                    if node in stop_at_vars:\n                        raise StopIteration\n                    yield node\n                    if node not in stop_at_vars:\n                        stop_at_vars.add(node)\n                    for var in expand_fn(node):\n                        if var in stop_at_vars:\n                            raise StopIteration\n                        yield var\n                        if var not in stop_at_vars:\n                            stop_at_vars.add(var)\n\n    return _walk_model(graphs, stop_at_vars)\n\n", "idx": 337}
{"namespace": "pymc.testing.select_by_precision", "completion": "    if pytensor.config.floatX == \"float32\":\n        return float32\n    else:\n        return float64\n\n", "idx": 338}
{"namespace": "pymc.gp.cov.handle_args", "completion": "    def inner(*args):\n        if len(args) == 1:\n            return func(args[0])\n        elif len(args) == 2:\n            return func(args[0], *args[1])\n        else:\n            raise ValueError(\"Function must be called with one or two arguments\")\n\n    return inner\n\n", "idx": 339}
{"namespace": "pymc.gp.util.kmeans_inducing_points", "completion": "    # Initialize the inducing points\n    m = X.shape[0]\n    if n_inducing > m:\n        warnings.warn(\n            \"The number of inducing points {} is larger than the number of inputs {}. Using all inputs instead.\".format(\n                n_inducing, m\n            )\n        )\n        return X\n\n    # Initialize the inducing points\n    centroids, _ = kmeans(X, n_inducing, **kmeans_kwargs)\n    return centroids\n\n", "idx": 340}
{"namespace": "pymc.pytensorf.floatX", "completion": "    if isinstance(X, np.ndarray):\n        return np.asarray(X, dtype=pt.config.floatX)\n    elif isinstance(X, TensorVariable):\n        return TensorType(pt.config.floatX)(X)\n    else:\n        raise TypeError(\"The input should be either a numpy array or a PyTensor tensor.\")\n\n", "idx": 341}
{"namespace": "pymc.distributions.multivariate.posdef", "completion": "    try:\n        np.linalg.cholesky(AA)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n", "idx": 342}
{"namespace": "pymc.distributions.dist_math.multigammaln", "completion": "    if p == 1:\n        return gammaln(a)\n    else:\n        return (p / (p - 1)) * gammaln(a) - gammaln(p) + p * np.sum(gammaln(a - (1.0 / p)), axis=-1)\n\n", "idx": 343}
{"namespace": "pymc.distributions.dist_math.incomplete_beta", "completion": "    return pt.betainc(a, b, value)\n\n", "idx": 344}
{"namespace": "pymc.sampling.forward.observed_dependent_deterministics", "completion": "    # Get the deterministics, observed random variables, and basic random variables from the model\n    deterministics = model.deterministics\n    observed_rvs = model.observed_RVs\n    basic_rvs = model.basic_RVs\n\n    # Get the observed variables\n    observed_vars = set()\n    for rv in observed_rvs:\n        observed_vars.add(rv)\n    for rv in basic_rvs:\n        if rv.owner:\n            for input_rv in rv.owner.inputs:\n                if input_rv in observed_vars:\n                    observed_vars.add(rv)\n\n    # Get the deterministics that depend directly on observed variables\n    dependent_deterministics = []\n    for deterministic in deterministics:\n        if deterministic in observed_vars:\n            dependent_deterministics.append(deterministic)\n            continue\n        for input_rv in deterministic.owner.inputs:\n            if input_rv in observed_vars:\n                dependent_deterministics.append(deterministic)\n                break\n\n    return dependent_deterministics\n\n", "idx": 345}
{"namespace": "pymc.smc.kernels.systematic_resampling", "completion": "    # Normalize weights\n    weights = weights / np.sum(weights)\n\n    # Generate random number\n    random_number = rng.uniform(0, 1)\n\n    # Generate indices\n    new_indices = np.arange(len(weights))\n\n    # Sort weights and indices\n    new_indices = new_indices[np.argsort(weights)[::-1]]\n\n    # Generate index\n    index = 0\n\n    # Initialize cumulative sum\n    cumulative_sum = weights[new_indices[index]]\n\n    # Loop over weights\n    for i in range(len(weights)):\n\n        # Check if random number is smaller than cumulative sum\n        if random_number < cumulative_sum:\n\n            # Return index\n            return new_indices[index]\n\n        # Increment index\n        index += 1\n\n        # Increment cumulative sum\n        cumulative_sum += weights[new_indices[index]]\n\n    # Return last index\n    return new_indices[index]\n\n", "idx": 346}
{"namespace": "pymc.backends.base._squeeze_cat", "completion": "    if combine:\n        results = np.concatenate(results)\n        if squeeze:\n            results = _squeeze(results)\n    else:\n        if squeeze:\n            results = [_squeeze(res) for res in results]\n    return results\n\n", "idx": 347}
{"namespace": "pymc.logprob.transforms.SimplexTransform.forward", "completion": "        # Calculate the log of the input value\n        log_value = pt.log(value)\n\n        # Calculate the sum of the log values\n        sum_log_value = pt.sum(log_value, axis=-1, keepdims=True)\n\n        # Return the transformed value\n        return log_value - sum_log_value\n", "idx": 348}
{"namespace": "pymc.logprob.transforms.SimplexTransform.backward", "completion": "        value = pt.as_tensor(value)\n        exp_value = pt.exp(value)\n        N = value.shape[-1].astype(value.dtype)\n        shift = pt.sum(exp_value, -1, keepdims=True)\n        return exp_value / shift\n", "idx": 349}
{"namespace": "pymc.logprob.utils.walk_model", "completion": "    def expand(var: TensorVariable) -> List[TensorVariable]:\n        if stop_at_vars is not None and var in stop_at_vars:\n            return []\n        if not walk_past_rvs and isinstance(var, MeasurableVariable):\n            return []\n        return expand_fn(var)\n\n    return walk(graphs, expand, walk_past_rvs)\n\n", "idx": 350}
{"namespace": "sacred.metrics_logger.linearize_metrics", "completion": "    metrics = {}\n    for entry in logged_metrics:\n        if entry.name not in metrics:\n            metrics[entry.name] = {\"steps\": [], \"values\": [], \"timestamps\": []}\n        metrics[entry.name][\"steps\"].append(entry.step)\n        metrics[entry.name][\"values\"].append(entry.value)\n        metrics[entry.name][\"timestamps\"].append(entry.timestamp)\n    return metrics", "idx": 351}
{"namespace": "sacred.utils.set_by_dotted_path", "completion": "    split_path = path.split(\".\")\n    current = d\n    for p in split_path[:-1]:\n        if p not in current:\n            current[p] = {}\n        current = current[p]\n    current[split_path[-1]] = value\n\n", "idx": 352}
{"namespace": "sacred.utils.get_by_dotted_path", "completion": "    split_path = path.split(\".\")\n    current_option = d\n    for p in split_path:\n        if p not in current_option:\n            return default\n        current_option = current_option[p]\n    return current_option\n\n", "idx": 353}
{"namespace": "pymc.logprob.scan.construct_scan", "completion": "    # Create the scan op\n    scan_op = scan_op_from_scan(scan_args, **kwargs)\n\n    # Create the scan node\n    scan_node = scan_op(*scan_args.inputs)\n\n    # Get the scan node outputs\n    scan_node_outputs = scan_node.outputs\n\n    # Get the scan node updates\n    scan_node_updates = OrderedUpdates(scan_node.op.outputs)\n\n    return scan_node_outputs, scan_node_updates\n\n", "idx": 354}
{"namespace": "sacred.utils.is_prefix", "completion": "    pre_path_list = pre_path.split('.')\n    path_list = path.split('.')\n    if len(pre_path_list) > len(path_list):\n        return False\n    else:\n        for i in range(len(pre_path_list)):\n            if pre_path_list[i] != path_list[i]:\n                return False\n        return True\n\n", "idx": 355}
{"namespace": "sacred.utils.get_inheritors", "completion": "    subclasses = set()\n    work = [cls]\n    while work:\n        parent = work.pop()\n        for child in parent.__subclasses__():\n            if child not in subclasses:\n                subclasses.add(child)\n                work.append(child)\n    return subclasses\n\n", "idx": 356}
{"namespace": "sacred.utils.convert_camel_case_to_snake_case", "completion": "    s1 = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1_\\2\", name)\n    return re.sub(\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", s1).lower()\n\n", "idx": 357}
{"namespace": "sacred.utils.module_exists", "completion": "    try:\n        loader = pkgutil.find_loader(modname)\n    except Exception:\n        return False\n    return loader is not None\n\n", "idx": 358}
{"namespace": "sacred.utils.apply_backspaces_and_linefeeds", "completion": "    if not text:\n        return text\n\n    # remove backspaces\n    cursor = 0\n    output = \"\"\n    for i in range(len(text)):\n        if text[i] == \"\\b\":\n            cursor = max(0, cursor - 1)\n        elif text[i] == \"\\r\":\n            cursor = 0\n        else:\n            output += text[i]\n            cursor += 1\n\n    # remove linefeeds\n    lines = output.splitlines()\n    last_line = None\n    for line in lines:\n        if line != \"\":\n            last_line = line\n\n    return last_line\n\n", "idx": 359}
{"namespace": "sacred.commands.help_for_command", "completion": "    # Get the help text for the command.\n    help_text = pydoc.render_doc(command, title=\"Help for %s\")\n\n    # Remove backspaces from the help text.\n    help_text = help_text.replace(\"\\b\", \"\")\n\n    # Return the help text.\n    return help_text\n\n", "idx": 360}
{"namespace": "sacred.optional.optional_import", "completion": "    for package_name in package_names:\n        if modules_exist(package_name):\n            return True, importlib.import_module(package_name)\n    return False, None\n\n", "idx": 361}
{"namespace": "sacred.dependencies.get_py_file_if_possible", "completion": "    if pyc_name.endswith(\".py\"):\n        return pyc_name\n    elif pyc_name.endswith(\".so\") or pyc_name.endswith(\".pyd\") or pyc_name.endswith(\".ipynb\"):\n        return pyc_name.replace(\".so\", \".py\").replace(\".pyd\", \".py\").replace(\".ipynb\", \".py\")\n    elif os.path.exists(pyc_name[:-1]):\n        return pyc_name[:-1]\n    else:\n        return pyc_name\n\n", "idx": 362}
{"namespace": "sacred.config.custom_containers.DogmaticDict.update", "completion": "        if iterable is not None:\n            try:\n                for k, v in iterable.items():\n                    self[k] = v\n            except AttributeError:\n                for k, v in iterable:\n                    self[k] = v\n\n        for k, v in kwargs.items():\n            self[k] = v\n", "idx": 363}
{"namespace": "sacred.config.config_scope.is_empty_or_comment", "completion": "    line = line.strip()\n    if line == \"\" or line.startswith(\"#\"):\n        return True\n    else:\n        return False\n\n", "idx": 364}
{"namespace": "boltons.funcutils.copy_function", "completion": "    ret = FunctionType(orig.__code__,\n                       orig.__globals__,\n                       name=orig.__name__,\n                       argdefs=getattr(orig, \"__defaults__\", None),\n                       closure=getattr(orig, \"__closure__\", None))\n    if copy_dict:\n        ret.__dict__.update(orig.__dict__)\n    return ret\n\n", "idx": 365}
{"namespace": "sacred.config.config_scope.dedent_line", "completion": "    if line.startswith(indent):\n        return line[len(indent) :]\n    return line\n\n", "idx": 366}
{"namespace": "boltons.funcutils.format_invocation", "completion": "    if kwargs is None:\n        kwargs = {}\n    args_str = ', '.join(map(repr, args))\n    kwargs_str = ', '.join('{}={!r}'.format(k, v) for k, v in sorted(kwargs.items()))\n    if kwargs:\n        if args:\n            return '{}({}, {})'.format(name, args_str, kwargs_str)\n        else:\n            return '{}({})'.format(name, kwargs_str)\n    else:\n        if args:\n            return '{}({})'.format(name, args_str)\n        else:\n            return '{}()'.format(name)\n\n", "idx": 367}
{"namespace": "boltons.listutils.SplayList.shift", "completion": "        item = self.pop(item_index)\n        self.insert(dest_index, item)\n\n", "idx": 368}
{"namespace": "boltons.strutils.gzip_bytes", "completion": "    return zlib.compress(bytestring, level)\n\n", "idx": 369}
{"namespace": "boltons.strutils.is_uuid", "completion": "    if not isinstance(obj, uuid.UUID):\n        try:\n            obj = uuid.UUID(obj)\n        except (TypeError, ValueError, AttributeError):\n            return False\n    if version and obj.version != version:\n        return False\n    return True\n\n", "idx": 370}
{"namespace": "boltons.strutils.parse_int_list", "completion": "    output_list = []\n    range_list = [x.strip() for x in range_string.split(delim)]\n    for i in range_list:\n        if range_delim in i:\n            r = i.split(range_delim)\n            if len(r) == 2 and r[0].isdigit() and r[1].isdigit():\n                output_list += range(int(r[0]), int(r[1]) + 1)\n        elif i.isdigit():\n            output_list.append(int(i))\n    return sorted(output_list)\n\n", "idx": 371}
{"namespace": "boltons.cacheutils.ThresholdCounter.get", "completion": "        return self._count_map.get(key, [default])[0]\n", "idx": 372}
{"namespace": "boltons.iterutils.backoff_iter", "completion": "    if not start:\n        raise ValueError('start must be non-zero')\n    if not stop:\n        raise ValueError('stop must be non-zero')\n    if not factor:\n        raise ValueError('factor must be non-zero')\n    if jitter:\n        if jitter is True:\n            jitter = 1.0\n        if jitter < -1.0 or jitter > 1.0:\n            raise ValueError('jitter must be between -1.0 and 1.0')\n    if start > stop:\n        start, stop = stop, start\n    if count is None:\n        count = int(math.ceil(math.log(stop / start, factor)))\n    if count < 0:\n        raise ValueError('count must be non-negative')\n    if count == 0:\n        return\n    if jitter:\n        jitter = (jitter * (stop - start)) / 2.0\n    for i in xrange(count):\n        yield start + jitter * random.random()\n        start *= factor\n\n", "idx": 373}
{"namespace": "boltons.cacheutils.cached", "completion": "    def decorator(func):\n        return CachedFunction(func, cache, scoped=scoped, typed=typed, key=key)\n    return decorator\n\n", "idx": 374}
{"namespace": "boltons.timeutils.total_seconds", "completion": "    return (td.microseconds + (td.seconds + td.days * 24 * 3600) * 10**6) / 10**6\n\n", "idx": 375}
{"namespace": "boltons.gcutils.get_all", "completion": "    type_all = []\n    for obj in gc.get_objects():\n        # we can't rely on isinstance for classes that\n        # were created in a different process, so we use\n        # the 'issubclass' borg\n        if type(obj) is type_obj:\n            type_all.append(obj)\n        elif include_subtypes and isinstance(obj, type_obj):\n            type_all.append(obj)\n    return type_all\n\n", "idx": 376}
{"namespace": "boltons.timeutils.daterange", "completion": "    if isinstance(step, tuple):\n        step = timedelta(days=360 * step[0] + 30 * step[1] + step[2])\n    elif isinstance(step, int):\n        step = timedelta(days=step)\n    elif not isinstance(step, timedelta):\n        raise TypeError('step must be an int, tuple, or timedelta.')\n\n    if inclusive:\n        while start <= stop:\n            yield start\n            start += step\n    else:\n        while start < stop:\n            yield start\n            start += step\n\n", "idx": 377}
{"namespace": "boltons.mathutils.clamp", "completion": "    return max(min(x, upper), lower)\n\n", "idx": 378}
{"namespace": "boltons.mathutils.ceil", "completion": "    if options is None:\n        return _ceil(x)\n    else:\n        return min([i for i in options if i >= x])\n\n", "idx": 379}
{"namespace": "boltons.formatutils.get_format_args", "completion": "    # TODO: memoize\n    # TODO: infer positional args for 2.6 compat\n    # TODO: infer type from format string\n    # TODO: infer type from value\n    # TODO: handle multiple format specifiers for a single arg\n    # TODO: handle nested format specifiers\n    ret = []\n    for lit, fstr in split_format_str(infer_positional_format_args(fstr)):\n        if fstr is None:\n            continue\n        fname, fspec = fstr[1:-1].split(':', 1)\n        if fname == '':\n            ret.append(('_positional', str))\n        else:\n            ret.append((fname, _TYPE_MAP.get(fspec, str)))\n    return ret\n\n", "idx": 380}
{"namespace": "boltons.mathutils.floor", "completion": "    if options is None:\n        return _floor(x)\n    options = sorted(options)\n    i = bisect.bisect_right(options, x)\n    if not i:\n        raise ValueError(\"no floor options less than or equal to: %r\" % x)\n    return options[i - 1]\n\n", "idx": 381}
{"namespace": "boltons.dictutils.OneToOne.setdefault", "completion": "        if key not in self:\n            dict.__setitem__(self, key, default)\n            dict.__setitem__(self, default, key)\n        return dict.__getitem__(self, key)\n", "idx": 382}
{"namespace": "boltons.dictutils.OneToOne.update", "completion": "        if isinstance(dict_or_iterable, dict):\n            for k, v in dict_or_iterable.items():\n                self[k] = v\n        elif hasattr(dict_or_iterable, 'keys'):\n            for k in dict_or_iterable.keys():\n                self[k] = dict_or_iterable[k]\n        else:\n            for k, v in dict_or_iterable:\n                self[k] = v\n        for k, v in kw.items():\n            self[k] = v\n", "idx": 383}
{"namespace": "boltons.dictutils.ManyToMany.get", "completion": "        return self.data.get(key, default)\n", "idx": 384}
{"namespace": "boltons.dictutils.FrozenDict.updated", "completion": "        other = subdict(self, *a, **kw)\n        return type(self)(self, **other)\n", "idx": 385}
{"namespace": "boltons.dictutils.subdict", "completion": "    if keep is None:\n        keep = list(d.keys())\n    if drop is None:\n        drop = []\n    return {k: d[k] for k in keep if k not in drop and k in d}\n\n", "idx": 386}
{"namespace": "boltons.dictutils.FrozenDict.__repr__", "completion": "        return f'{self.__class__.__name__}({dict.__repr__(self)})'\n", "idx": 387}
{"namespace": "gunicorn.config.validate_callable", "completion": "    def _validate_callable(val):\n        if isinstance(val, str):\n            try:\n                val = import_string(val)\n            except ImportError as e:\n                raise TypeError(\"Invalid value: %s\" % e)\n\n        if not callable(val):\n            raise TypeError(\"Value must be a callable object.\")\n\n        if not -1 < arity < len(inspect.getfullargspec(val).args):\n            raise TypeError(\"Value must be a callable object that takes exactly %s arguments (%s given).\" % (arity, len(inspect.getfullargspec(val).args)))\n\n        return val\n\n    return _validate_callable\n\n", "idx": 388}
{"namespace": "gunicorn.config.get_default_config_file", "completion": "    cfg_file = os.path.join(os.getcwd(), \"gunicorn.conf.py\")\n    if os.path.exists(cfg_file):\n        return cfg_file\n\n", "idx": 389}
{"namespace": "gunicorn.util.is_ipv6", "completion": "    try:\n        socket.inet_pton(socket.AF_INET6, addr)\n    except socket.error:\n        return False\n    return True\n\n", "idx": 390}
{"namespace": "gunicorn.systemd.listen_fds", "completion": "    if os.environ.get(\"LISTEN_PID\", \"\") != str(os.getpid()):\n        return 0\n\n    try:\n        return int(os.environ.get(\"LISTEN_FDS\", 0))\n    except ValueError:\n        return 0\n\n", "idx": 391}
{"namespace": "gunicorn.util.http_date", "completion": "    if timestamp is None:\n        timestamp = time.time()\n    year, month, day, hh, mm, ss, wd, y, z = time.gmtime(timestamp)\n    s = \"%s, %02d %3s %4d %02d:%02d:%02d GMT\" % (\n        _weekdayname[wd],\n        day,\n        _monthname[month],\n        year,\n        hh,\n        mm,\n        ss,\n    )\n    return s\n\n", "idx": 392}
{"namespace": "gunicorn.util.parse_address", "completion": "    if netloc.startswith(\"unix:\"):\n        return (netloc.split(\"unix:\")[1], None)\n\n    if netloc.isdigit():\n        try:\n            fd = int(netloc)\n        except ValueError:\n            pass\n        else:\n            return (fd, None)\n\n    # get host\n    if '[' in netloc and ']' in netloc:\n        host = netloc.split(']')[0][1:].lower()\n    elif ':' in netloc:\n        host = netloc.split(':')[0].lower()\n    elif netloc == \"\":\n        host = \"0.0.0.0\"\n    else:\n        host = netloc.lower()\n\n    # get port\n    netloc = netloc.split(']')[-1]\n    if \":\" in netloc:\n        port = netloc.split(':', 1)[1]\n        if not port.isdigit():\n            raise RuntimeError(\"%r is not a valid port number.\" % port)\n        port = int(port)\n    else:\n        port = default_port\n\n    return (host, port)\n\n", "idx": 393}
{"namespace": "gunicorn.util.to_bytestring", "completion": "    if isinstance(value, bytes):\n        return value\n\n    if not isinstance(value, str):\n        value = str(value)\n\n    return value.encode(encoding)\n\n", "idx": 394}
{"namespace": "gunicorn.util.warn", "completion": "    sys.stderr.write(\"WARNING: %s\\n\" % msg)\n\n", "idx": 395}
{"namespace": "gunicorn.util.split_request_uri", "completion": "    # If the URI starts with \"//\", it is considered as a relative URI, but the function considers it as an absolute path.\n    # It uses a temporary dot prefix to work around this behavior.\n    if uri.startswith(\"//\"):\n        uri = \".\" + uri\n\n    # Split the URI into its components.\n    scheme, authority, path, query, fragment = urllib.parse.urlsplit(uri)\n\n    # If the URI was prefixed with a dot, remove it.\n    if scheme == \".\":\n        scheme = \"\"\n\n    # Return the components of the URI.\n    return scheme, authority, path, query, fragment\n\n", "idx": 396}
{"namespace": "praw.models.listing.listing.ModNoteListing.after", "completion": "        return getattr(self, \"end_cursor\", None) if getattr(self, \"has_next_page\", False) else None\n\n", "idx": 397}
{"namespace": "praw.models.util.permissions_string", "completion": "    if permissions is None:\n        return \"+all\"\n\n    additions = set()\n    removals = set()\n\n    for permission in permissions:\n        if permission.startswith(\"+\"):\n            additions.add(permission[1:])\n        elif permission.startswith(\"-\"):\n            removals.add(permission[1:])\n        else:\n            additions.add(permission)\n\n    for permission in known_permissions:\n        if permission not in additions:\n            removals.add(permission)\n\n    return \",\".join(\n        [\n            (\"+\" + permission) if permission in additions else (\"-\" + permission)\n            for permission in known_permissions\n        ]\n    )\n\n", "idx": 398}
{"namespace": "jc.cli.JcCli.json_out", "completion": "        if self.pretty:\n            if self.ascii_only:\n                return json.dumps(self.data_out, indent=self.indent, separators=(',', ': '), ensure_ascii=True)\n            return json.dumps(self.data_out, indent=self.indent, separators=(',', ': '), ensure_ascii=False)\n\n        if self.ascii_only:\n            return json.dumps(self.data_out, separators=(',', ':'), ensure_ascii=True)\n        return json.dumps(self.data_out, separators=(',', ':'), ensure_ascii=False)\n", "idx": 399}
{"namespace": "pythonforandroid.pythonpackage.transform_dep_for_pip", "completion": "    if \"@\" in dependency and \"://\" in dependency:\n        return dependency.replace(\"@\", \"/\").replace(\"://\", \"@\")\n    return dependency\n\n", "idx": 400}
{"namespace": "pythonforandroid.graph.fix_deplist", "completion": "    for i in range(len(deps)):\n        if isinstance(deps[i], str):\n            deps[i] = (deps[i],)\n        deps[i] = tuple(map(lambda x: x.lower(), deps[i]))\n    return deps\n\n", "idx": 401}
{"namespace": "pythonforandroid.util.walk_valid_filens", "completion": "    for root, dirs, files in walk(base_dir):\n        for name in files:\n            file_path = join(root, name)\n            if any([n in root for n in invalid_dir_names]):\n                continue\n            if any([fnmatch(file_path, pattern) for pattern in invalid_file_patterns]):\n                continue\n            yield file_path\n\n", "idx": 402}
{"namespace": "pythonforandroid.bootstrap._cmp_bootstraps_by_priority", "completion": "    if a.priority != b.priority:\n        return b.priority - a.priority\n    return a.name > b.name\n\n", "idx": 403}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.all_bootstraps", "completion": "        bootstraps_dir = join(cls.ctx.root_dir, 'bootstraps')\n        if not os.path.isdir(bootstraps_dir):\n            return set()\n        bootstraps = set()\n        for name in os.listdir(bootstraps_dir):\n            filen = join(bootstraps_dir, name)\n            if isdir(filen):\n                bootstraps.add(name)\n        return bootstraps\n", "idx": 404}
{"namespace": "mmcv.image.colorspace._convert_input_type_range", "completion": "    img_type = img.dtype\n    img = img.astype(np.float32)\n    if img_type == np.float32:\n        pass\n    elif img_type == np.uint8:\n        img /= 255.\n    else:\n        raise TypeError(f'The img type should be np.float32 or np.uint8, but got {img_type}')\n    return img\n\n", "idx": 405}
{"namespace": "mackup.utils.error", "completion": "    print(\"ERROR: {}\".format(message))\n    sys.exit(1)\n\n", "idx": 406}
{"namespace": "mmcv.image.colorspace._convert_output_type_range", "completion": "    if dst_type not in (np.uint8, np.float32):\n        raise TypeError('The dst_type should be np.float32 or np.uint8, '\n                        f'but got {dst_type}')\n    if dst_type == np.uint8:\n        img = img.round()\n    else:\n        img /= 255.\n    return img.astype(dst_type)\n\n", "idx": 407}
{"namespace": "mackup.utils.is_process_running", "completion": "    try:\n        subprocess.check_output([\"pgrep\", process_name])\n        return True\n    except subprocess.CalledProcessError:\n        return False\n\n", "idx": 408}
{"namespace": "stellar.operations._get_pid_column", "completion": "    server_version = raw_conn.execute('SHOW VARIABLES LIKE \"version\"').fetchone()[1]\n    server_version_number = re.search(r'(\\d+)\\.(\\d+)\\.(\\d+)', server_version).group(0)\n    if server_version_number < '5.7.0':\n        return 'ID'\n    else:\n        return 'processlist_id'\n\n", "idx": 409}
{"namespace": "imapclient.imap_utf7.encode", "completion": "    if isinstance(s, str):\n        s = s.encode()\n\n    if b'&' in s:\n        s = s.replace(b'&', b'&-')\n\n    encoded = bytearray()\n    base64_buffer = bytearray()\n    for char in s:\n        if 0x20 <= char <= 0x7e:\n            if base64_buffer:\n                encoded += b'&' + base64.b64encode(bytes(base64_buffer)) + b'-'\n                base64_buffer.clear()\n            encoded.append(char)\n        elif char == ord(b'-'):\n            if base64_buffer:\n                encoded += b'&' + base64.b64encode(bytes(base64_buffer)) + b'-'\n                base64_buffer.clear()\n            encoded += b'&-'\n        else:\n            base64_buffer.append(char)\n\n    if base64_buffer:\n        encoded += b'&' + base64.b64encode(bytes(base64_buffer)) + b'-'\n\n    return bytes(encoded)\n\n", "idx": 410}
{"namespace": "imapclient.version._imapclient_version_string", "completion": "    major, minor, micro, releaselevel = vinfo\n    version = f\"{major}.{minor}\"\n    if micro:\n        version += f\".{micro}\"\n    if releaselevel != \"final\":\n        version += f\"-{releaselevel}\"\n    return version\n\n", "idx": 411}
{"namespace": "telethon.helpers.generate_key_data_from_nonce", "completion": "    server_nonce_bytes = struct.pack('<Q', server_nonce)\n    new_nonce_bytes = struct.pack('<Q', new_nonce)\n    hash1 = hashlib.sha1(server_nonce_bytes).digest()\n    hash2 = hashlib.sha1(new_nonce_bytes).digest()\n    hash3 = hashlib.sha1(server_nonce_bytes + new_nonce_bytes).digest()\n    key = hash1 + hash2[:12]\n    iv = hash2[12:] + hash3 + new_nonce_bytes[:4]\n    return key, iv\n\n", "idx": 412}
{"namespace": "hbmqtt.codecs.bytes_to_int", "completion": "    return int.from_bytes(data, 'big')\n\n", "idx": 413}
{"namespace": "zulipterminal.helper.display_error_if_present", "completion": "    if len(response[\"messages\"]) > 0:\n        controller.report_error(response[\"messages\"][0][\"content\"])\n\n", "idx": 414}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._decode_message_id", "completion": "        try:\n            return int(message_id)\n        except ValueError:\n            return None\n", "idx": 415}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton.handle_narrow_link", "completion": "        parsed_link = self._parse_narrow_link(self.link)\n        error = self._validate_narrow_link(parsed_link)\n\n        if error:\n            self.view.set_footer_text(error, 3)\n        else:\n            self._switch_narrow_to(parsed_link)\n\n", "idx": 416}
{"namespace": "zulipterminal.config.color.color_properties", "completion": "    def add_property(base_enum: Any, prop: str) -> Any:\n        \"\"\"\n        This function adds a property to an Enum. It creates a new Enum with the given property and returns it.\n        Input-Output Arguments\n        :param base_enum: Any. The Enum to which the property is to be added.\n        :param prop: str. The property to be added to the Enum.\n        :return: Any. The updated Enum with the added property.\n        \"\"\"\n        return type(\n            prop.upper(),\n            (),\n            {\n                \"__getattr__\": lambda *args: getattr(base_enum, args[1]),\n                \"__getitem__\": lambda *args: getattr(base_enum, args[1]),\n                \"__call__\": lambda *args: getattr(base_enum, args[1]),\n                \"property\": prop.lower(),\n            },\n        )\n\n    updated_colors = {}\n    for color in colors:\n        color_name = color.name.lower()\n        color_value = color.value\n        for p in prop:\n            color_name = color_name + \"_\" + p.lower()\n            color_value = color_value + \":\" + p.lower()\n        updated_colors[color_name] = color_value\n    return type(\"Color\", (), updated_colors)\n\n", "idx": 417}
{"namespace": "twilio.base.deserialize.decimal", "completion": "    if d is None or d == \"\":\n        return d\n    return BasicContext.post_calculate(Decimal(d))", "idx": 418}
{"namespace": "twilio.base.deserialize.integer", "completion": "    try:\n        return int(i)\n    except ValueError:\n        return i", "idx": 419}
{"namespace": "twilio.base.serialize.object", "completion": "    if isinstance(obj, dict):\n        return json.dumps(obj)\n    else:\n        return obj\n\n", "idx": 420}
{"namespace": "twilio.base.serialize.map", "completion": "    if lst == values.unset:\n        return lst\n\n    if not isinstance(lst, list):\n        raise TypeError(\n            \"The input should be a list, but it was a {}\".format(type(lst))\n        )\n\n    return [serialize_func(e) for e in lst]", "idx": 421}
{"namespace": "twilio.base.obsolete.deprecated_method", "completion": "    def deprecated_method_wrapper(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            if new_func is not None:\n                warnings.warn(\n                    \"Function {} is deprecated, use {} instead.\".format(func.__name__, new_func),\n                    category=DeprecationWarning,\n                    stacklevel=2,\n                )\n            else:\n                warnings.warn(\n                    \"Function {} is deprecated.\".format(func.__name__),\n                    category=DeprecationWarning,\n                    stacklevel=2,\n                )\n            return func(*args, **kwargs)\n\n        return wrapper\n\n    return deprecated_method_wrapper\n\n", "idx": 422}
{"namespace": "chatette.utils.sample_indulgent", "completion": "    if nb_items <= len(array):\n        return sample(array, nb_items)\n    return deepcopy(array)\n\n", "idx": 423}
{"namespace": "chatette.utils.rchop", "completion": "    if string.endswith(ending):\n        return string[:-len(ending)]\n    return string\n\n", "idx": 424}
{"namespace": "chatette.utils.str_to_bool", "completion": "    if text.lower() == 'true':\n        return True\n    if text.lower() == 'false':\n        return False\n    raise ValueError(\"Cannot convert '\" + text + \"' to boolean.\")\n\n", "idx": 425}
{"namespace": "chatette.utils.min_if_exist", "completion": "    if n1 is None:\n        return n2\n    if n2 is None:\n        return n1\n    return min(n1, n2)\n", "idx": 426}
{"namespace": "chatette.utils.append_to_list_in_dict", "completion": "    if key in dict_of_lists:\n        dict_of_lists[key].append(value)\n    else:\n        dict_of_lists[key] = [value]\n\n", "idx": 427}
{"namespace": "chatette.utils.extend_list_in_dict", "completion": "    if key not in dict_of_lists:\n        dict_of_lists[key] = values\n    else:\n        dict_of_lists[key].extend(values)\n", "idx": 428}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy._is_end_regex", "completion": "        return re.match(r'\\\\/(g?i?|i?g?)$', word) is not None\n", "idx": 429}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.execute", "completion": "        raise NotImplementedError(\"Should be overriden\")\n\n", "idx": 430}
{"namespace": "aioxmpp.network.group_and_order_srv_records", "completion": "    if rng is None:\n        rng = random\n\n    records = {}\n    for record in all_records:\n        priority, weight, hostname, port = record\n        if priority not in records:\n            records[priority] = []\n        records[priority].append((weight, hostname, port))\n\n    ordered_records = []\n    for priority in sorted(records.keys()):\n        records_for_priority = records[priority]\n        rng.shuffle(records_for_priority)\n        records_for_priority.sort(key=lambda x: x[0])\n        ordered_records.extend(records_for_priority)\n\n    return [(hostname, port) for weight, hostname, port in ordered_records]\n\n", "idx": 431}
{"namespace": "aioxmpp.nonza.StreamFeatures.get_feature", "completion": "        try:\n            return self[feature_cls]\n        except KeyError:\n            return default\n\n", "idx": 432}
{"namespace": "aioxmpp.connector.XMPPOverTLSConnector._context_factory_factory", "completion": "        def context_factory():\n            \"\"\"\n            This function creates a context factory for the XMPPOverTLSConnector. It sets the ALPN protocol to \"xmpp-client\" if the ssl_context has the set_alpn_protos method. It also sets up the context with the verifier and returns the ssl_context.\n            Input-Output Arguments\n            :param self: XMPPOverTLSConnector. An instance of the XMPPOverTLSConnector class.\n            :param logger: The logger to be used for logging.\n            :param metadata: The metadata to be used for creating the ssl context.\n            :param verifier: The verifier to be used for setting up the context.\n            :return: The context factory function.\n            \"\"\"\n\n            ssl_context = metadata.ssl_context_factory()\n            if hasattr(ssl_context, \"set_alpn_protocols\"):\n                ssl_context.set_alpn_protocols([\"xmpp-client\"])\n            verifier.setup_context(ssl_context, None)\n            return ssl_context\n\n        return context_factory\n", "idx": 433}
{"namespace": "aioxmpp.xmltestutils.element_path", "completion": "    if upto is None:\n        upto = el.getroottree().getroot()\n\n    path = []\n    while el is not upto:\n        parent = el.getparent()\n        path.append(el.tag + \"[\" + str(parent.index(el)) + \"]\")\n        el = parent\n\n    return \"/\".join(reversed(path))\n\n", "idx": 434}
{"namespace": "aioxmpp.structs.JID.fromstr", "completion": "        if not s:\n            raise ValueError(\"JID must not be empty\")\n\n        if s.startswith(\"(\"):\n            raise ValueError(\"JID must not start with '('\")\n\n        if s.endswith(\")\"):\n            raise ValueError(\"JID must not end with ')'\")\n\n        if s.startswith(\"@\"):\n            raise ValueError(\"JID must not start with '@'\")\n\n        if s.endswith(\"/\"):\n            raise ValueError(\"JID must not end with '/'\")\n\n        if s.startswith(\"//\"):\n            raise ValueError(\"JID must not start with '//'\")\n\n        if s.endswith(\"//\"):\n            raise ValueError(\"JID must not end with '//'\")\n\n        if s.startswith(\"//@\"):\n            raise ValueError(\"JID must not start with '//@'\")\n\n        if s.startswith(\"//@/\"):\n            raise ValueError(\"JID must not start with '//@/'\")\n\n        if s.startswith(\"//@//\"):\n            raise ValueError(\"JID must not start with '//@//'\")\n\n        if s.startswith(\"//@//@\"):\n            raise ValueError(\"JID must not start with '//@//@'\")\n\n        if s.startswith(\"//@//@/\"):\n            raise ValueError(\"JID must not start with '//@//@/'\")\n\n        if s.startswith(\"//@//@//\"):\n            raise ValueError(\"JID must not start with '//@//@//'\")\n\n        if s.startswith(\"//@//@//@\"):\n            raise ValueError(\"JID must not start with '//@//@//@'\")\n\n        if s.startswith(\"//@//@//@/\"):\n            raise ValueError(\"JID must not start with '//@//@//@/'\")\n\n        if s.startswith(\"//@//@//@//\"):\n            raise ValueError(\"JID must not start with '//@//@//@//'\")\n\n        if s.startswith(\"//@", "idx": 435}
{"namespace": "aioxmpp.security_layer.extract_python_dict_from_x509", "completion": "    result = {}\n    result[\"subject\"] = x509.get_subject().get_components()\n    result[\"subjectAltName\"] = []\n    for i in range(x509.get_extension_count()):\n        ext = x509.get_extension(i)\n        if \"subjectAltName\" in str(ext.get_short_name()):\n            result[\"subjectAltName\"] = ext.__getstate__()[0]\n    return result\n\n", "idx": 436}
{"namespace": "aioxmpp.security_layer.extract_blob", "completion": "    result = b\"\"\n    for i in range(0, x509.get_extension_count()):\n        ext = x509.get_extension(i)\n        sn = ext.get_short_name()\n        if sn != b\"subjectAltName\":\n            continue\n        data = pyasn1.codec.der.decoder.decode(\n            ext.get_data(),\n            asn1Spec=pyasn1_modules.rfc2459.SubjectAltName())[0]\n        for name in data:\n            dNSName = name.getComponentByPosition(2)\n            if dNSName is None:\n                continue\n            if hasattr(dNSName, \"isValue\") and not dNSName.isValue:\n                continue\n            result += dNSName\n    return result\n\n", "idx": 437}
{"namespace": "aioxmpp.security_layer.blob_to_pyasn1", "completion": "    decoder = pyasn1.codec.der.decoder.decode\n    return decoder(blob, asn1Spec=pyasn1_modules.rfc2459.Certificate())[0]\n\n", "idx": 438}
{"namespace": "aioxmpp.security_layer.extract_pk_blob_from_pyasn1", "completion": "    return pyasn1.codec.der.encoder.encode(\n        pyasn1_struct.getComponentByName(\"tbsCertificate\").getComponentByName(\"subjectPublicKeyInfo\").getComponentByName(\"subjectPublicKey\"))\n\n", "idx": 439}
{"namespace": "aioxmpp.callbacks.AdHocSignal.ASYNC_WITH_LOOP", "completion": "        if loop is None:\n            loop = asyncio.get_event_loop()\n        return functools.partial(cls._async_wrapper, loop)\n", "idx": 440}
{"namespace": "aioxmpp.callbacks.AdHocSignal.SPAWN_WITH_LOOP", "completion": "        if loop is None:\n            loop = asyncio.get_event_loop()\n\n        def spawn(f):\n            if not asyncio.iscoroutinefunction(f):\n                raise TypeError(\"must be a coroutine function, got {!r}\".format(f))\n            task = asyncio.ensure_future(f, loop=loop)\n            task.add_done_callback(log_spawned)\n            return task\n\n        return spawn\n", "idx": 441}
{"namespace": "aioxmpp.callbacks.first_signal", "completion": "    async def first_signal_awaitable(*args):\n        if isinstance(args[0], Exception):\n            raise args[0]\n        return args[0]\n\n    for signal in signals:\n        signal.connect(first_signal_awaitable)\n\n    return first_signal_awaitable", "idx": 442}
{"namespace": "aioxmpp.tasks.TaskPool.spawn", "completion": "        return asyncio.create_task(coro_fun(*args, **kwargs))", "idx": 443}
{"namespace": "aioxmpp.protocol.send_and_wait_for", "completion": "    if not isinstance(xmlstream, XMLStream):\n        raise TypeError(\"xmlstream must be of type XMLStream\")\n    if not isinstance(send, xso.XSO):\n        raise TypeError(\"send must be of type XSO\")\n    if not isinstance(wait_for, xso.XSO):\n        raise TypeError(\"wait_for must be of type XSO\")\n    if timeout is not None and not isinstance(timeout, int):\n        raise TypeError(\"timeout must be of type int\")\n    if cb is not None and not callable(cb):\n        raise TypeError(\"cb must be a callable\")\n\n    # Send the message\n    xmlstream.send(send)\n\n    # Wait for the response\n    try:\n        response = await xmlstream.wait_for(wait_for, timeout)\n    except asyncio.TimeoutError:\n        raise TimeoutError(\"Timed out waiting for response\")\n\n    # Call the callback function\n    if cb is not None:\n        cb(response)\n\n    return response\n\n", "idx": 444}
{"namespace": "aioxmpp.testutils.run_coroutine_with_peer", "completion": "    if not loop:\n        loop = asyncio.get_event_loop()\n\n    local_future = asyncio.ensure_future(coroutine, loop=loop)\n    peer_future = asyncio.ensure_future(peer_coroutine, loop=loop)\n\n    try:\n        return loop.run_until_complete(\n            asyncio.wait_for(\n                asyncio.gather(\n                    local_future,\n                    peer_future),\n                timeout=timeout))\n    except asyncio.TimeoutError:\n        raise asyncio.TimeoutError(\n            \"Timeout while waiting for coroutines to complete: \"\n            \"local future: %s, peer future: %s\" % (\n                local_future,\n                peer_future))\n\n", "idx": 445}
{"namespace": "aioxmpp.testutils.make_listener", "completion": "    listener = unittest.mock.Mock()\n    for signal in callbacks.get_signals(instance):\n        setattr(listener, signal.name, unittest.mock.Mock())\n        signal.register(getattr(listener, signal.name))\n    return listener\n\n", "idx": 446}
{"namespace": "aioxmpp.vcard.service.VCardService.set_vcard", "completion": "        iq = aioxmpp.IQ(\n            type_=aioxmpp.IQType.SET,\n            to=jid,\n            payload=vcard,\n        )\n\n        await self.client.send(iq)", "idx": 447}
{"namespace": "aioxmpp.rsm.xso.ResultSetMetadata.limit", "completion": "        result = copy.copy(self)\n        result.max_ = max_\n        return result\n", "idx": 448}
{"namespace": "aioxmpp.muc.service.Room.features", "completion": "        return self._features\n", "idx": 449}
{"namespace": "aioxmpp.xso.query.EvaluationContext.eval_bool", "completion": "        return bool(expr.eval(self))\n\n", "idx": 450}
{"namespace": "aioxmpp.xso.query._BoolOpMixin.eval", "completion": "        for value in self.expr.eval(ec):\n            sub_ec = copy.copy(ec)\n            sub_ec.set_toplevel_object(value)\n            if self.op(sub_ec.eval_bool(self.other)):\n                yield value\n\n", "idx": 451}
{"namespace": "aioxmpp.xso.model.drop_handler", "completion": "    depth = 0\n    while True:\n        ev_type, *ev_args = yield\n        if ev_type == \"start\":\n            depth += 1\n        elif ev_type == \"end\":\n            depth -= 1\n        if depth == 0:\n            break\n\n", "idx": 452}
{"namespace": "aioxmpp.xso.model.guard", "completion": "    try:\n        result = dest.send(ev_args)\n    except Exception as err:\n        try:\n            dest.throw(err)\n        except StopIteration as err:\n            result = err.value\n        except Exception:\n            raise\n    else:\n        try:\n            dest.throw(StopIteration(result))\n        except StopIteration:\n            pass\n    return result\n\n", "idx": 453}
{"namespace": "aioxmpp.xso.model.capture_events", "completion": "    try:\n        while True:\n            value = yield from receiver\n            dest.append(value)\n    except:\n        dest.clear()\n        raise\n    finally:\n        dest.clear()\n\n", "idx": 454}
{"namespace": "aioxmpp.xso.model.events_to_sax", "completion": "    for ev_type, *ev_args in events:\n        if ev_type == \"start\":\n            dest.startElementNS(ev_args[0], None, ev_args[2])\n        elif ev_type == \"text\":\n            dest.characters(ev_args[0])\n        elif ev_type == \"end\":\n            dest.endElementNS(ev_args[0], None)\n        else:\n            raise ValueError(\"unknown event type: {}\".format(ev_type))\n\n", "idx": 455}
{"namespace": "aioxmpp.adhoc.service.AdHocClient.get_command_info", "completion": "        disco = self.dependencies[aioxmpp.disco.DiscoClient]\n        response = await disco.query_info(\n            peer_jid,\n            node=command_name,\n        )\n        return response.info\n", "idx": 456}
{"namespace": "aioxmpp.entitycaps.caps115.build_identities_string", "completion": "    # Process each identity in the list and encode it into a byte string.\n    identities_list = [identity.to_bytes() for identity in identities]\n\n    # Check for duplicate identities and sort the identities.\n    identities_list = list(set(identities_list))\n    identities_list.sort()\n\n    # Join the identities into a single byte string.\n    identities_string = b\"<\".join(identities_list)\n\n    return identities_string\n\n", "idx": 457}
{"namespace": "aioxmpp.entitycaps.caps115.build_features_string", "completion": "    features = [\n        escape(feature).encode(\"utf-8\")\n        for feature in features\n    ]\n\n    if len(set(features)) != len(features):\n        raise ValueError(\"duplicate feature\")\n\n    features.sort()\n    features.append(b\"\")\n    return b\"<\".join(features)\n\n", "idx": 458}
{"namespace": "aioxmpp.entitycaps.caps115.build_forms_string", "completion": "    forms = [\n        b\"/\".join([\n            escape(form.type_).encode(\"utf-8\"),\n            escape(form.label or \"\").encode(\"utf-8\"),\n            escape(form.value or \"\").encode(\"utf-8\"),\n            escape(form.options or \"\").encode(\"utf-8\"),\n        ])\n        for form in forms\n    ]\n\n    if len(set(forms)) != len(forms):\n        raise ValueError(\"duplicate form\")\n\n    forms.sort()\n    forms.append(b\"\")\n    return b\"<\".join(forms)\n\n", "idx": 459}
{"namespace": "aioxmpp.entitycaps.caps115.Key.path", "completion": "        return pathlib.Path(\n            \"hashes\",\n            urllib.parse.quote(self.node),\n            self.algo\n        )\n\n", "idx": 460}
{"namespace": "aioxmpp.entitycaps.caps390._process_features", "completion": "    return b\" \".join(feature.encode(\"ascii\") for feature in features)\n\n", "idx": 461}
{"namespace": "aioxmpp.entitycaps.caps390._process_identities", "completion": "    parts = [\n        _process_identity(identity)\n        for identity in identities\n    ]\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1c\"\n\n", "idx": 462}
{"namespace": "aioxmpp.entitycaps.caps390._process_extensions", "completion": "    parts = [\n        _process_form(ext)\n        for ext in exts\n    ]\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1c\"\n\n", "idx": 463}
{"namespace": "aioxmpp.entitycaps.caps390._calculate_hash", "completion": "    if algo == \"sha-1\":\n        return aioxmpp.hashes.sha1(hash_input)\n    elif algo == \"sha-224\":\n        return aioxmpp.hashes.sha224(hash_input)\n    elif algo == \"sha-256\":\n        return aioxmpp.hashes.sha256(hash_input)\n    elif algo == \"sha-384\":\n        return aioxmpp.hashes.sha384(hash_input)\n    elif algo == \"sha-512\":\n        return aioxmpp.hashes.sha512(hash_input)\n    else:\n        raise ValueError(\"Unsupported hash algorithm: %s\" % algo)\n\n", "idx": 464}
{"namespace": "aioxmpp.entitycaps.caps390.Key.node", "completion": "        return base64.b64encode(self.digest).decode(\"utf-8\")\n", "idx": 465}
{"namespace": "aioxmpp.entitycaps.caps390.Key.path", "completion": "        return \"caps/{}.{}.{}\".format(\n            base64.b64encode(self.digest).decode(\"ascii\"),\n            self.algo,\n            \"key\"\n        )\n", "idx": 466}
{"namespace": "aioxmpp.entitycaps.caps390.Implementation.extract_keys", "completion": "        caps = presence.xep0390_caps\n        if caps is None:\n            return\n\n        for algo in self.__algorithms:\n            digest = _calculate_hash(algo, _get_hash_input(caps))\n            yield Key(algo, digest)\n\n", "idx": 467}
{"namespace": "aioxmpp.roster.service.RosterClient.approve", "completion": "        self.client.send(stanza.Presence(\n            type_=structs.PresenceType.SUBSCRIBED,\n            to=peer_jid\n        ))\n", "idx": 468}
{"namespace": "aioxmpp.roster.service.RosterClient.subscribe", "completion": "        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.SUBSCRIBE,\n                            to=peer_jid)\n        )\n", "idx": 469}
{"namespace": "aioxmpp.roster.service.RosterClient.unsubscribe", "completion": "        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.UNSUBSCRIBE,\n                            to=peer_jid)\n        )\n", "idx": 470}
{"namespace": "aioxmpp.forms.fields.BoundSingleValueField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass\n", "idx": 471}
{"namespace": "aioxmpp.forms.fields.BoundMultiValueField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass\n", "idx": 472}
{"namespace": "aioxmpp.forms.fields.BoundOptionsField.options", "completion": "        try:\n            del self._options\n        except AttributeError:\n            pass\n", "idx": 473}
{"namespace": "aioxmpp.forms.fields.BoundSelectField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass\n", "idx": 474}
{"namespace": "aioxmpp.forms.fields.BoundMultiSelectField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass\n", "idx": 475}
{"namespace": "cupy.random._generator.reset_states", "completion": "    _random_states = {}\n\n", "idx": 476}
{"namespace": "cupy.random._generator._check_and_get_dtype", "completion": "    if dtype is None:\n        return numpy.dtype('l')\n    elif isinstance(dtype, int):\n        return numpy.dtype(dtype)\n    elif isinstance(dtype, str):\n        return numpy.dtype(dtype)\n    elif isinstance(dtype, type):\n        return numpy.dtype(dtype)\n    elif isinstance(dtype, numpy.dtype):\n        return dtype\n    else:\n        raise ValueError('Unsupported data type: {}'.format(dtype))\n\n", "idx": 477}
{"namespace": "cupy_builder._command.filter_files_by_extension", "completion": "    filtered_files = list(filter(lambda file: file.endswith(extension), sources))\n    filtered_out_files = list(filter(lambda file: not file.endswith(extension), sources))\n    return filtered_files, filtered_out_files\n\n", "idx": 478}
{"namespace": "datasets.table._in_memory_arrow_table_from_file", "completion": "    stream = pa.input_stream(filename)\n    try:\n        # This code is adapted from https://arrow.apache.org/docs/python/ipc.html#ipc\n        reader = pa.ipc.open_stream(stream)\n        pa_table = reader.read_all()\n        return pa_table\n    finally:\n        stream.close()\n\n", "idx": 479}
{"namespace": "datasets.table._in_memory_arrow_table_from_buffer", "completion": "    in_memory_stream = pa.BufferReader(buffer)\n    opened_stream = pa.ipc.open_stream(in_memory_stream)\n    pa_table = opened_stream.read_all()\n    return pa_table\n\n", "idx": 480}
{"namespace": "datasets.table._interpolation_search", "completion": "    i, j = 0, len(arr) - 1\n    while i < j and arr[i] <= x < arr[j]:\n        k = i + (j - i) // 2\n        if arr[k] <= x:\n            i = k + 1\n        else:\n            j = k\n    if arr[i] > x:\n        raise IndexError(\"x out of array\")\n    return i\n\n", "idx": 481}
{"namespace": "datasets.data_files._is_inside_unrequested_special_dir", "completion": "    if not pattern.endswith(\"**\"):\n        pattern += \"**\"\n    pattern = pattern.replace(\"**\", r\"(\\/.*)?\")\n    pattern = pattern.replace(\"*\", r\"\\w*\")\n    pattern = pattern.replace(\"?\", r\"\\w\")\n    pattern = pattern.replace(\"[!\", \"[^\")\n    pattern = pattern.replace(\"[\", r\"[\\w\")\n    pattern = pattern.replace(\"]\", r\"]\\w\")\n    pattern = pattern.replace(\"+\", r\"\\w+\")\n    pattern = pattern.replace(\".\", r\"\\.\")\n    pattern = pattern.replace(\"(\", r\"\\(\")\n    pattern = pattern.replace(\")\", r\"\\)\")\n    pattern = pattern.replace(\"{\", r\"\\{\")\n    pattern = pattern.replace(\"}\", r\"\\}\")\n    pattern = pattern.replace(\"|\", r\"\\|\")\n    pattern = pattern.replace(\"^\", r\"\\^\")\n    pattern = pattern.replace(\"$\", r\"\\$\")\n    pattern = pattern.replace(\"-\", r\"\\-\")\n    pattern = pattern.replace(\"[\", r\"\\[\")\n    pattern = pattern.replace(\"]\", r\"\\]\")\n    pattern = pattern.replace(\"\\\\\", r\"\\\\\")\n    pattern = pattern.replace(\"/\", r\"\\/\")\n    pattern = pattern.replace(\"!\", r\"\\!\")\n    pattern = pattern.replace(\"&\", r\"\\&\")\n    pattern = pattern.replace(\"~\", r\"\\~\")\n    pattern = pattern.replace(\"#\", r\"\\#\")\n    pattern = pattern.replace(\"@\", r\"\\@\")\n    pattern = pattern.replace(\"%\", r\"\\%\")\n    pattern = pattern.replace(\"=\", r\"\\=\")\n    pattern = pattern.replace(\"<\", r\"\\<\")\n    pattern = pattern.replace(\">\", r\"\\>\")\n    pattern = pattern.replace(\"`\", r\"\\`\")\n    pattern = pattern.replace(\":\", r\"\\:\")\n    pattern = pattern.replace(\";\", r\"\\;\")\n    pattern = pattern.replace(\",\", r\"\\,\")\n    pattern = pattern.replace(\"'\", r\"\\'\")\n    pattern = pattern.replace('\"", "idx": 482}
{"namespace": "datasets.data_files._is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir", "completion": "    return (\n        matched_rel_path.startswith(\".\")\n        or _is_inside_unrequested_special_dir(matched_rel_path, pattern)\n    )\n\n", "idx": 483}
{"namespace": "datasets.iterable_dataset._batch_to_examples", "completion": "    return [dict(zip(batch, t)) for t in zip(*batch.values())]\n\n", "idx": 484}
{"namespace": "datasets.iterable_dataset._examples_to_batch", "completion": "    columns = set()\n    for example in examples:\n        columns.update(example.keys())\n    columns = sorted(list(columns))\n    batch = {column: [] for column in columns}\n    for example in examples:\n        for column in columns:\n            batch[column].append(example.get(column, None))\n    return batch\n\n", "idx": 485}
{"namespace": "datasets.iterable_dataset.RandomlyCyclingMultiSourcesExamplesIterable._iter_random_indices", "completion": "        while True:\n            if p is None:\n                p = np.ones(num_sources) / num_sources\n            random_indices = rng.choice(num_sources, size=random_batch_size, p=p)\n            for random_index in random_indices:\n                yield random_index\n", "idx": 486}
{"namespace": "datasets.iterable_dataset.BufferShuffledExamplesIterable._iter_random_indices", "completion": "        buffer = list(range(buffer_size))\n        while True:\n            rng.shuffle(buffer)\n            for i in range(buffer_size // random_batch_size):\n                yield buffer[i * random_batch_size : (i + 1) * random_batch_size]\n", "idx": 487}
{"namespace": "datasets.iterable_dataset.IterableDataset.remove_columns", "completion": "        if isinstance(column_names, str):\n            column_names = [column_names]\n        return self.map(partial(_remove_columns_fn, column_names=column_names), remove_columns=column_names)\n", "idx": 488}
{"namespace": "datasets.dataset_dict.DatasetDict.with_format", "completion": "        self._check_values_type()\n        return DatasetDict(\n            {\n                k: dataset.with_format(type=type, columns=columns, output_all_columns=output_all_columns, **format_kwargs)\n                for k, dataset in self.items()\n            }\n        )\n", "idx": 489}
{"namespace": "datasets.dataset_dict.DatasetDict.with_transform", "completion": "        dataset = copy.deepcopy(self)\n        dataset.set_format(\"custom\", columns=columns, output_all_columns=output_all_columns, transform=transform)\n        return dataset\n", "idx": 490}
{"namespace": "datasets.dataset_dict.DatasetDict.align_labels_with_mapping", "completion": "        self._check_values_type()\n        return DatasetDict({k: dataset.align_labels_with_mapping(label2id, label_column) for k, dataset in self.items()})\n", "idx": 491}
{"namespace": "datasets.dataset_dict.IterableDatasetDict.map", "completion": "        return IterableDatasetDict({k: dataset.map(\n            function=function,\n            with_indices=with_indices,\n            input_columns=input_columns,\n            batched=batched,\n            batch_size=batch_size,\n            drop_last_batch=drop_last_batch,\n            remove_columns=remove_columns,\n            fn_kwargs=fn_kwargs,\n        ) for k, dataset in self.items()})\n", "idx": 492}
{"namespace": "datasets.dataset_dict.IterableDatasetDict.filter", "completion": "        return IterableDatasetDict(\n            {\n                k: dataset.filter(\n                    function=function,\n                    with_indices=with_indices,\n                    input_columns=input_columns,\n                    batched=batched,\n                    batch_size=batch_size,\n                    fn_kwargs=fn_kwargs,\n                )\n                for k, dataset in self.items()\n            }\n        )\n", "idx": 493}
{"namespace": "datasets.arrow_dataset.Dataset.num_rows", "completion": "        if self._indices is not None:\n            return len(self._indices)\n        return self._data.num_rows\n", "idx": 494}
{"namespace": "datasets.filesystems.extract_path_from_uri", "completion": "    if \"://\" in dataset_path:\n        dataset_path = dataset_path.split(\"://\")[1]\n    return dataset_path\n\n", "idx": 495}
{"namespace": "datasets.filesystems.is_remote_filesystem", "completion": "    if isinstance(fs, fsspec.AbstractFileSystem):\n        protocol = getattr(fs, \"protocol\", None)\n        if isinstance(protocol, str):\n            protocol = protocol.lower()\n            if protocol in (\"http\", \"https\", \"s3\", \"gs\", \"gcs\", \"hdfs\", \"webhdfs\", \"ssh\", \"ftp\", \"sftp\", \"smb\", \"file\"):\n                return True\n    return False\n\n", "idx": 496}
{"namespace": "datasets.utils.file_utils.hash_url_to_filename", "completion": "    url_bytes = url.encode(\"utf-8\")\n    url_hash = sha256(url_bytes)\n    filename = url_hash.hexdigest()\n\n    if etag:\n        etag_bytes = etag.encode(\"utf-8\")\n        etag_hash = sha256(etag_bytes)\n        filename += \".\" + etag_hash.hexdigest()\n\n    if url.endswith(\".h5\"):\n        filename += \".h5\"\n\n    return filename\n\n", "idx": 497}
{"namespace": "datasets.utils.hub.hf_hub_url", "completion": "    if revision is None:\n        revision = \"main\"\n\n    if version.parse(hfh.__version__) < version.parse(\"0.11.0\"):\n        path = quote(path)\n\n    return hfh.hf_hub_url(repo_id, path, revision)", "idx": 498}
{"namespace": "datasets.utils.sharding._number_of_shards_in_gen_kwargs", "completion": "    # Get the number of shards\n    number_of_shards = len(gen_kwargs['n_agents'])\n\n    # Check if the number of shards is the same for all the lists in the dictionary\n    for key in gen_kwargs:\n        if isinstance(gen_kwargs[key], list):\n            if len(gen_kwargs[key]) != number_of_shards:\n                raise ValueError('The number of shards in the gen_kwargs is not the same for all the lists.')\n\n    return number_of_shards\n\n", "idx": 499}
{"namespace": "datasets.utils.sharding._distribute_shards", "completion": "    num_jobs = min(num_shards, max_num_jobs)\n    shards_per_job = num_shards // num_jobs\n    shards_per_job_remainder = num_shards % num_jobs\n    shard_index_range_per_job = []\n    for i in range(num_jobs):\n        start_index = i * shards_per_job\n        end_index = start_index + shards_per_job\n        if i < shards_per_job_remainder:\n            end_index += 1\n        shard_index_range_per_job.append(range(start_index, end_index))\n    return shard_index_range_per_job\n\n", "idx": 500}
{"namespace": "datasets.utils.py_utils.temporary_assignment", "completion": "    # Get the original value of the attribute.\n    original_value = getattr(obj, attr)\n    # Set the attribute to the new value.\n    setattr(obj, attr, value)\n    try:\n        yield\n    finally:\n        # Set the attribute back to the original value.\n        setattr(obj, attr, original_value)\n\n", "idx": 501}
{"namespace": "datasets.utils.extract.TarExtractor.extract", "completion": "        # Create the output directory if it does not exist.\n        Path(output_path).mkdir(parents=True, exist_ok=True)\n\n        # Open the tar file.\n        with tarfile.open(input_path) as tar:\n            tar.extractall(output_path, members=TarExtractor.safemembers(tar, output_path))\n\n", "idx": 502}
{"namespace": "datasets.utils.extract.Extractor.infer_extractor_format", "completion": "        magic_number_length = cls._get_magic_number_max_length()\n        magic_number = cls._read_magic_number(path, magic_number_length)\n        for extractor_format, extractor in cls.extractors.items():\n            if extractor.is_extractable(path, magic_number=magic_number):\n                return extractor_format\n        return \"\"\n", "idx": 503}
{"namespace": "datasets.utils.py_utils.asdict", "completion": "    if is_dataclass(obj):\n        return asdict(obj.__dict__)\n    elif isinstance(obj, (list, tuple)):\n        return [asdict(item) for item in obj]\n    elif isinstance(obj, dict):\n        return {key: asdict(value) for key, value in obj.items()}\n    elif isinstance(obj, (int, float, str, bytes, bytearray, bool, type(None))):\n        return obj\n    elif isinstance(obj, (np.ndarray)):\n        return obj.tolist()\n    else:\n        return str(obj)\n\n", "idx": 504}
{"namespace": "datasets.utils.metadata.MetadataConfigs.from_dataset_card_data", "completion": "        if dataset_card_data.get(cls.FIELD_NAME) is not None:\n            metadata_configs = dataset_card_data[cls.FIELD_NAME]\n            for metadata_config in metadata_configs:\n                cls._raise_if_data_files_field_not_valid(metadata_config)\n            return cls(metadata_configs)\n        else:\n            return cls()\n", "idx": 505}
{"namespace": "pymorphy2.analyzer.lang_dict_path", "completion": "    lang_paths = _lang_dict_paths()\n    if lang not in lang_paths:\n        raise ValueError(\"Language '%s' not supported. Supported languages: %s\" % (lang, ', '.join(sorted(lang_paths))))\n    return lang_paths[lang]\n\n", "idx": 506}
{"namespace": "pymorphy2.dawg.assert_can_create", "completion": "    if not EXTENSION_AVAILABLE:\n        raise NotImplementedError(\"The extension is not available. Please install the extension.\")\n\n", "idx": 507}
{"namespace": "pymorphy2.opencorpora_dict.compile._to_paradigm", "completion": "    stem = \"\"\n    paradigm = []\n    for word_form, tag in lexeme:\n        prefix = \"\"\n        suffix = word_form\n        for pref in paradigm_prefixes:\n            if word_form.startswith(pref):\n                prefix = pref\n                suffix = word_form[len(pref):]\n                break\n        if not stem:\n            stem = suffix\n        if stem != suffix:\n            stem = \"\"\n            break\n        paradigm.append((suffix, tag, prefix))\n    return stem, tuple(paradigm)\n\n", "idx": 508}
{"namespace": "pymorphy2.units.by_analogy.KnownPrefixAnalyzer.tag", "completion": "        result = []\n        for prefix, unprefixed_word in self.possible_splits(word_lower):\n            method = (self, prefix)\n\n            tag = self.morph.tag(unprefixed_word)\n            for tag_data in tag:\n                if tag_data.is_productive():\n                    tag_data = add_tag_if_not_seen(tag_data, seen_tags, method)\n                    result.append(tag_data)\n\n        return result\n", "idx": 509}
{"namespace": "pymorphy2.units.by_analogy.UnknownPrefixAnalyzer.tag", "completion": "        result = []\n        for prefix, unprefixed_word in word_splits(word_lower):\n            for tag in self.dict_analyzer.tag(unprefixed_word, unprefixed_word, seen_tags):\n                if not tag.is_productive():\n                    continue\n                add_tag_if_not_seen(tag, result, seen_tags)\n        return result", "idx": 510}
{"namespace": "benedict.dicts.keylist.keylist_util.get_item", "completion": "    if not type_util.is_dict(d):\n        return (None,) * len(keys)\n\n    if not type_util.is_list_or_tuple(keys):\n        keys = [keys]\n\n    item = d\n    for key in keys:\n        try:\n            item = _get_item_key_and_value(item, key)[1]\n        except (IndexError, KeyError):\n            return (None,) * len(keys)\n\n    return item\n\n", "idx": 511}
{"namespace": "benedict.dicts.keylist.keylist_util.set_item", "completion": "    item = d\n    for key in keys[:-1]:\n        item = _get_or_new_item_value(item, key, keys[-1])\n    _set_item_value(item, keys[-1], value)\n\n", "idx": 512}
{"namespace": "benedict.dicts.keypath.keypath_util._split_key_indexes", "completion": "    if key.startswith(\"[\") and key.endswith(\"]\"):\n        key_indexes = re.findall(KEY_INDEX_RE, key)\n        return key_indexes\n    else:\n        return [key]\n\n", "idx": 513}
{"namespace": "feedparser.urls.make_safe_absolute_uri", "completion": "    if not ACCEPTABLE_URI_SCHEMES:\n        return ''\n    if not base:\n        if not rel:\n            return ''\n        return rel\n    if not rel:\n        return base\n    uri = _urljoin(base, rel)\n    # enforce acceptable schemes\n    try:\n        scheme = urllib.parse.urlparse(uri)[0]\n    except ValueError:\n        return ''\n    if scheme and scheme not in ACCEPTABLE_URI_SCHEMES:\n        return ''\n    return uri\n\n", "idx": 514}
{"namespace": "feedparser.api._open_resource", "completion": "    # If we were passed a string, treat it as a URL, file-like object or file name\n    if isinstance(url_file_stream_or_string, str):\n        # Check if it's a URL\n        if urllib.parse.urlparse(url_file_stream_or_string)[0] in ('http', 'https', 'ftp'):\n            # Check if it's a feed, based on the URL\n            if url_file_stream_or_string.rstrip('/').endswith('/'):\n                url_file_stream_or_string += 'index.xml'\n            # Check if it's a feed, based on the headers returned by the server\n            try:\n                # Create a urllib2 opener\n                opener = urllib.request.build_opener(*handlers)\n                # Add a HTTP ETag header, if available\n                if etag:\n                    opener.addheaders.append(('If-None-Match', etag))\n                # Add a HTTP If-Modified-Since header, if available\n                if modified:\n                    if isinstance(modified, tuple):\n                        modified = _format_date_time(modified)\n                    opener.addheaders.append(('If-Modified-Since', modified))\n                # Add a custom User-Agent header, if available\n                if agent:\n                    opener.addheaders.append(('User-Agent', agent))\n                # Add a custom Referer header, if available\n                if referrer:\n                    opener.addheaders.append(('Referer', referrer))\n                # Add any additional headers\n                if request_headers:\n                    for key, value in request_headers.items():\n                        opener.addheaders.append((key, value))\n                # Open the URL, using the opener\n                result['url'] = url_file_stream_or_string\n                result['bozo'] = 1\n                try:\n                    return opener.open(url_file_stream_or_string)\n                except urllib.error.HTTPError as err:\n                    # Check if we got a 301 or", "idx": 515}
{"namespace": "feedparser.http._build_urllib2_request", "completion": "    request = urllib.request.Request(url)\n    request.add_header('User-Agent', agent)\n    if accept_header is not None:\n        request.add_header('Accept', accept_header)\n    if etag:\n        request.add_header('If-None-Match', etag)\n    if modified:\n        if isinstance(modified, datetime.datetime):\n            modified = modified.utctimetuple()\n        modified = time.strftime('%a, %d %b %Y %H:%M:%S GMT', modified)\n        request.add_header('If-Modified-Since', modified)\n    if referrer:\n        request.add_header('Referer', referrer)\n    if auth:\n        request.add_header('Authorization', auth)\n    if request_headers:\n        for key, value in list(request_headers.items()):\n            request.add_header(key, value)\n    return request\n\n", "idx": 516}
{"namespace": "pylatex.utils.dumps_list", "completion": "    if mapper is None:\n        mapper = []\n    elif not _is_iterable(mapper):\n        mapper = [mapper]\n\n    if escape:\n        mapper.append(escape_latex)\n\n    mapper = [m for m in mapper if m is not None]\n\n    if as_content:\n        mapper.append(dumps_as_content)\n\n    def _map(obj):\n        for m in mapper:\n            obj = m(obj)\n        return obj\n\n    return NoEscape(token.join([_map(o) for o in l]))\n\n", "idx": 517}
{"namespace": "pylatex.utils._latex_item_to_string", "completion": "    if isinstance(item, pylatex.base_classes.LatexObject):\n        if as_content:\n            return item.dumps_as_content()\n        else:\n            return item.dumps()\n\n    if not isinstance(item, str):\n        item = str(item)\n\n    if escape:\n        item = escape_latex(item)\n\n    return item\n\n", "idx": 518}
{"namespace": "mistune.markdown.Markdown.read", "completion": "        if state is None:\n            state = self.block.state_cls()\n\n        with open(filepath, 'r', encoding=encoding) as f:\n            content = f.read()\n\n        return self.parse(content, state)\n", "idx": 519}
{"namespace": "mistune.create_markdown", "completion": "    if plugins is None:\n        plugins = []\n\n    if isinstance(renderer, str):\n        renderer = import_plugin(renderer)\n\n    if not issubclass(renderer, BaseRenderer):\n        raise TypeError('renderer must be a subclass of BaseRenderer')\n\n    for plugin in plugins:\n        plugin = import_plugin(plugin)\n        renderer.use(plugin)\n\n    return Markdown(escape=escape, hard_wrap=hard_wrap, renderer=renderer)\n\n", "idx": 520}
{"namespace": "parsel.utils.extract_regex", "completion": "    if isinstance(regex, str):\n        regex = re.compile(regex, re.UNICODE)\n\n    if regex.flags & re.UNICODE == 0:\n        raise ValueError(\"extract_regex only supports Unicode patterns\")\n\n    matches = regex.finditer(text)\n    result = []\n    for match in matches:\n        if \"extract\" in match.groupdict():\n            result.append(match.group(\"extract\"))\n        elif match.groups():\n            result.extend(match.groups())\n        else:\n            result.append(match.group())\n\n    if replace_entities:\n        result = [w3lib_replace_entities(s) for s in result]\n\n    return result\n\n", "idx": 521}
{"namespace": "dominate.dom_tag.dom_tag.render", "completion": "    # Get the tag name\n    tag_name = type(self).__name__\n\n    # Get the attributes\n    attributes = self.attributes\n\n    # Get the children\n    children = self.children\n\n    # Get the closing tag\n    closing_tag = ' /' if self.is_single else ''\n\n    # Get the opening tag\n    if xhtml:\n      opening_tag = '<{tag}{closing_tag}>'\n    else:\n      opening_tag = '<{tag}{attributes}{closing_tag}>'\n\n    # Render the opening tag\n    output = opening_tag.format(\n        tag=tag_name,\n        attributes=''.join(' %s=\"%s\"' % (k, v) for k, v in attributes.items()),\n        closing_tag=closing_tag)\n\n    # Render the children\n    if children:\n      # Add a newline and indent if necessary\n      if pretty:\n        output += '\\n' + indent\n\n      # Render the children\n      for child in children:\n        # Render the child\n        if isinstance(child, dom_tag):\n          # Render the child tag\n          output += child.render(indent=indent, pretty=pretty, xhtml=xhtml)\n        else:\n          # Render the child string\n          output += unicode(child)\n\n        # Add a newline and indent if necessary\n        if pretty:\n          output += '\\n' + indent\n\n      # Remove the last newline and indent if necessary\n      if pretty:\n        output = output[:-len(indent)]\n\n    # Get the closing tag\n    if not self.is_single:\n      output += '</%s>' % tag_name\n\n    # Return the output\n    return output\n\n", "idx": 522}
{"namespace": "dominate.util.include", "completion": "  with open(f, 'r') as f:\n    return f.read()\n\n", "idx": 523}
{"namespace": "dominate.util.unescape", "completion": "  def replace_entity(match):\n    ent = match.group(1)\n    if ent[0] == \"#\":\n      if ent[1] == 'x' or ent[1] == 'X':\n        return unichr(int(ent[2:], 16))\n      else:\n        return unichr(int(ent[1:]))\n    else:\n      cp = _unescape.get(ent)\n      if cp:\n        return unichr(cp)\n      else:\n        return match.group(0)\n\n  return re.sub(r\"&(#?[xX]?(?:[0-9a-fA-F]+|\\w{1,8}));\", replace_entity, data)\n\n", "idx": 524}
{"namespace": "onlinejudge_command.pretty_printers._tokenize_line", "completion": "    tokens = []\n    l = 0\n    while l < len(line):\n        r = l + 1\n        while r < len(line) and line[l] == line[r]:\n            r += 1\n        if line[l] == '\\n':\n            typ = _PrettyTokenType.NEWLINE\n        elif line[l] in ' \\t':\n            typ = _PrettyTokenType.WHITESPACE\n        else:\n            typ = _PrettyTokenType.BODY\n        tokens.append(_PrettyToken(typ, line[l:r]))\n        l = r\n    return tokens\n\n", "idx": 525}
{"namespace": "onlinejudge_command.pretty_printers._render_tokens", "completion": "    # Set default values for font functions if they are not provided.\n    if font_bold is None:\n        font_bold = lambda s: s\n    if font_dim is None:\n        font_dim = lambda s: s\n    if font_red is None:\n        font_red = lambda s: s\n    if font_blue is None:\n        font_blue = lambda s: s\n    if font_normal is None:\n        font_normal = lambda s: s\n\n    # Apply different font styles to the tokens based on their type.\n    rendered = []\n    for token in tokens:\n        if token.type == _PrettyTokenType.BODY:\n            rendered.append(font_normal(token.value))\n        elif token.type == _PrettyTokenType.BODY_HIGHLIGHT_LEFT:\n            rendered.append(font_bold(token.value))\n        elif token.type == _PrettyTokenType.BODY_HIGHLIGHT_RIGHT:\n            rendered.append(font_bold(token.value))\n        elif token.type == _PrettyTokenType.WHITESPACE:\n            rendered.append(font_dim(_replace_whitespace(token.value)))\n        elif token.type == _PrettyTokenType.NEWLINE:\n            rendered.append(font_dim(token.value))\n        elif token.type == _PrettyTokenType.HINT:\n            rendered.append(font_red(token.value))\n        elif token.type == _PrettyTokenType.LINENO:\n            rendered.append(font_blue(token.value))\n        elif token.type == _PrettyTokenType.OTHERS:\n            rendered.append(font_normal(token.value))\n        else:\n            raise ValueError('unknown token type')\n\n    # Concatenate all the formatted tokens into a single string and return it.\n    return ''.join(rendered)\n\n", "idx": 526}
{"namespace": "onlinejudge_command.pretty_printers._tokenize_file_content_without_snipping", "completion": "    tokens = []\n    try:\n        text = content.decode()\n    except UnicodeDecodeError as e:\n        tokens.append(_PrettyToken(_PrettyTokenType.HINT, str(e)))\n        text = content.decode(errors='replace')\n    for line in text.splitlines(keepends=True):\n        tokens += _tokenize_line(line)\n    tokens = _warn_if_empty(tokens)\n    return tokens\n\n", "idx": 527}
{"namespace": "jinja2.environment.Environment.get_template", "completion": "        if isinstance(name, Template):\n            template = name\n            name = template.name\n        else:\n            if parent is not None:\n                name = self.join_path(name, parent)\n            template = self._load_template(name, globals)\n\n        if parent is not None:\n            template.globals[\"parent\"] = self.get_template(parent)\n\n        return template\n", "idx": 528}
{"namespace": "jinja2.environment.Environment.from_string", "completion": "        if template_class is None:\n            template_class = self.template_class\n        if isinstance(source, template_class):\n            return source\n        return template_class(self, source, globals)\n", "idx": 529}
{"namespace": "jinja2.environment.Template.render", "completion": "        context = self.new_context(*args, **kwargs)\n        return self.render_with_context(context)\n", "idx": 530}
{"namespace": "jinja2.utils.generate_lorem_ipsum", "completion": "    from random import randint\n\n    # Create a list of words to use in the Lorem Ipsum text.\n    words = [\n        \"lorem\",\n        \"ipsum\",\n        \"dolor\",\n        \"sit\",\n        \"amet\",\n        \"consectetur\",\n        \"adipiscing\",\n        \"elit\",\n        \"sed\",\n        \"do\",\n        \"eiusmod\",\n        \"tempor\",\n        \"incididunt\",\n        \"ut\",\n        \"labore\",\n        \"et\",\n        \"dolore\",\n        \"magna\",\n        \"aliqua\",\n        \"ut\",\n        \"enim\",\n        \"ad\",\n        \"minim\",\n        \"veniam\",\n        \"quis\",\n        \"nostrud\",\n        \"exercitation\",\n        \"ullamco\",\n        \"laboris\",\n        \"nisi\",\n        \"ut\",\n        \"aliquip\",\n        \"ex\",\n        \"ea\",\n        \"commodo\",\n        \"consequat\",\n        \"duis\",\n        \"aute\",\n        \"irure\",\n        \"in\",\n        \"reprehenderit\",\n        \"in\",\n        \"voluptate\",\n        \"velit\",\n        \"esse\",\n        \"cillum\",\n        \"eu\",\n        \"fugiat\",\n        \"nulla\",\n        \"pariatur\",\n        \"excepteur\",\n        \"sint\",\n        \"occaecat\",\n        \"cupidatat\",\n        \"non\",\n        \"proident\",\n        \"sunt\",\n        \"in\",\n        \"culpa\",\n        \"qui\",\n        \"officia\",\n        \"deserunt\",\n        \"mollit\",\n        \"anim\",\n        \"id\",\n        \"est\",\n        \"laborum\",\n    ]\n\n    # Create a list of sentences to use in the Lorem Ipsum text.\n    sentences = [\n        \"Lorem\",\n        \"ipsum\",\n        \"dolor\",\n        \"sit\",\n       ", "idx": 531}
{"namespace": "jinja2.utils.LRUCache.clear", "completion": "        with self._wlock:\n            self._mapping.clear()\n            self._queue.clear()\n", "idx": 532}
{"namespace": "jinja2.utils.LRUCache.items", "completion": "        with self._wlock:\n            return [(key, self._mapping[key]) for key in reversed(self._queue)]\n", "idx": 533}
{"namespace": "jinja2.idtracking.symbols_for_node", "completion": "    sym = Symbols(parent=parent_symbols)\n    visitor = FrameSymbolVisitor(sym)\n    visitor.visit(node)\n    return sym\n\n", "idx": 534}
{"namespace": "jinja2.idtracking.Symbols.find_ref", "completion": "        if name in self.refs:\n            return self.refs[name]\n\n        if self.parent is not None:\n            return self.parent.find_ref(name)\n\n        return None\n", "idx": 535}
{"namespace": "jinja2.idtracking.Symbols.dump_stores", "completion": "        stores = {}\n        for store in self.stores:\n            stores[store] = self.ref(store)\n\n        if self.parent is not None:\n            parent_stores = self.parent.dump_stores()\n            for store in parent_stores:\n                if store not in stores:\n                    stores[store] = parent_stores[store]\n\n        return stores\n\n", "idx": 536}
{"namespace": "jinja2.meta.find_undeclared_variables", "completion": "    code_gen = TrackingCodeGenerator(ast.environment)\n    code_gen.visit(ast)\n    return code_gen.undeclared_identifiers", "idx": 537}
{"namespace": "jinja2.loaders.split_template_path", "completion": "    pieces = []\n    for piece in template.split(\"/\"):\n        if (\n            os.path.sep in piece\n            or os.path.altsep is not None\n            and os.path.altsep in piece\n            or os.path.pardir in piece\n            or os.path.curdir in piece\n        ):\n            return []\n        elif piece and piece != \".\":\n            pieces.append(piece)\n    return pieces\n\n", "idx": 538}
{"namespace": "jinja2.bccache.MemcachedBytecodeCache.load_bytecode", "completion": "        try:\n            bucket.bytecode_from_string(self.client.get(self.prefix + bucket.key))\n        except Exception:\n            if not self.ignore_memcache_errors:\n                raise\n", "idx": 539}
{"namespace": "jinja2.bccache.MemcachedBytecodeCache.dump_bytecode", "completion": "        try:\n            self.client.set(\n                self.prefix + bucket.key,\n                bucket.bytecode_to_string(),\n                self.timeout,\n            )\n        except Exception:\n            if not self.ignore_memcache_errors:\n                raise\n", "idx": 540}
{"namespace": "sumy.utils.get_stop_words", "completion": "    language = normalize_language(language)\n    if not isinstance(language, string_types):\n        raise TypeError(\"Given language parameter must be a string.\")\n\n    if language not in STOP_WORDS:\n        raise LookupError(\"Stop words for the given language are not available.\")\n\n    return frozenset(STOP_WORDS[language])\n\n", "idx": 541}
{"namespace": "sumy._compat.to_bytes", "completion": "    if isinstance(object, bytes):\n        return object\n    elif isinstance(object, unicode):\n        return object.encode(\"utf-8\")\n    else:\n        return str(object)\n\n", "idx": 542}
{"namespace": "sumy._compat.to_unicode", "completion": "    if isinstance(object, unicode):\n        return object\n    elif isinstance(object, bytes):\n        return object.decode(\"utf-8\")\n    else:\n        # try decode instance to unicode\n        return instance_to_unicode(object)\n\n", "idx": 543}
{"namespace": "sumy.summarizers.lsa.LsaSummarizer._create_dictionary", "completion": "        dictionary = {}\n        for word in document.words:\n            word = self.normalize_word(word)\n            if word not in self._stop_words:\n                if word in dictionary:\n                    dictionary[word] += 1\n                else:\n                    dictionary[word] = 1\n\n        return dictionary\n", "idx": 544}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._get_content_words_in_sentence", "completion": "        normalized_words = self._normalize_words(sentence.words)\n        content_words = self._filter_out_stop_words(normalized_words)\n        stemmed_words = self._stem_words(content_words)\n        return stemmed_words\n", "idx": 545}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._get_all_content_words_in_doc", "completion": "        all_words = self._get_all_words_in_doc(sentences)\n        content_words = self._get_content_words_in_sentence(all_words)\n        return content_words\n", "idx": 546}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_tf", "completion": "        content_words = self._get_all_content_words_in_doc(sentences)\n        content_words_count = len(content_words)\n        content_words_freq = self._compute_word_freq(content_words)\n        content_words_tf = {w:freq/content_words_count for w, freq in content_words_freq.items()}\n        return content_words_tf\n", "idx": 547}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_ratings", "completion": "        word_freq = self._compute_tf(sentences)\n        ratings = {}\n        content_words_in_sentences = [self._get_content_words_in_sentence(s) for s in sentences]\n        for i in range(len(sentences)):\n            word_freq = self._update_tf(word_freq, content_words_in_sentences[i])\n            best_sentence_index = self._find_index_of_best_sentence(word_freq, content_words_in_sentences)\n            ratings[sentences[best_sentence_index]] = i * -1\n            del sentences[best_sentence_index]\n            del content_words_in_sentences[best_sentence_index]\n        return ratings", "idx": 548}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.cue_method", "completion": "        cue_method = self._build_cue_method_instance(bonus_word_value, stigma_word_value)\n        return cue_method.summarize(document, sentences_count)\n", "idx": 549}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.key_method", "completion": "        summarization_method = self._build_key_method_instance()\n        return summarization_method(document, sentences_count, weight)\n", "idx": 550}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.title_method", "completion": "        summarization_method = self._build_title_method_instance()\n        return summarization_method(document, sentences_count)\n", "idx": 551}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.location_method", "completion": "        summarization_method = self._build_location_method_instance()\n        return summarization_method(document, sentences_count, w_h, w_p1, w_p2, w_s1, w_s2)\n", "idx": 552}
{"namespace": "sumy.summarizers.reduction.ReductionSummarizer.rate_sentences", "completion": "        ratings = defaultdict(float)\n        compressed_text = self._compute_compressed_text(document)\n        (token_weights, token_count) = self._compute_token_weights(compressed_text)\n        self._rate_sentences_by_text_rank(document, token_weights, ratings)\n        self._rate_sentences_by_headings(document, ratings)\n        self._rate_sentences_by_sentence_position(document, ratings)\n        self._normalize_ratings(ratings, token_count)\n        return ratings\n", "idx": 553}
{"namespace": "sumy.summarizers.reduction.ReductionSummarizer._to_words_set", "completion": "        words = map(self.normalize_word, sentence.words)\n        words = [w for w in words if w not in self._stop_words]\n        return frozenset(words)\n", "idx": 554}
{"namespace": "sumy.summarizers.text_rank.TextRankSummarizer._to_words_set", "completion": "        words = self.tokenize(sentence)\n        words = [self.normalize_word(w) for w in words]\n        words = [w for w in words if w not in self._stop_words]\n        return set(words)\n", "idx": 555}
{"namespace": "sumy.summarizers.kl.KLSummarizer.compute_tf", "completion": "        content_words = self._get_all_content_words_in_doc(sentences)\n        content_words_count = len(content_words)\n        content_words_freq = self._compute_word_freq(content_words)\n        content_words_tf = {k: v / content_words_count for k, v in content_words_freq.items()}\n        return content_words_tf\n", "idx": 556}
{"namespace": "sumy.evaluation.rouge._get_word_ngrams", "completion": "    ngram_set = set()\n    for sentence in sentences:\n        words = sentence.words\n        for i in range(len(words) - n + 1):\n            ngram_set.add(tuple(words[i:i + n]))\n    return ngram_set\n\n", "idx": 557}
{"namespace": "sumy.evaluation.rouge._len_lcs", "completion": "    table = _lcs(x, y)\n    i, j = _get_index_of_lcs(x, y)\n    return table[i][j]\n\n", "idx": 558}
{"namespace": "sumy.evaluation.rouge._recon_lcs", "completion": "    n, m = _get_index_of_lcs(x, y)\n    table = _lcs(x, y)\n\n    def _recon(i, j):\n        \"\"\"\n        Private function that reconstructs the LCS from the table\n        :param i: integer. The index in the x sequence.\n        :param j: integer. The index in the y sequence.\n        :return: List of words. The LCS of x[:i] and y[:j]\n        \"\"\"\n        if i == 0 or j == 0:\n            return []\n        elif x[i - 1] == y[j - 1]:\n            return _recon(i - 1, j - 1) + [x[i - 1]]\n        elif table[i - 1, j] > table[i, j - 1]:\n            return _recon(i - 1, j)\n        else:\n            return _recon(i, j - 1)\n\n    return _recon(n, m)\n\n", "idx": 559}
{"namespace": "sumy.evaluation.rouge._union_lcs", "completion": "    union_lcs = set()\n    union_count = 0\n    ref_words = _split_into_words([reference_sentence])\n    combined_lcs_length = 0\n    for eval_s in evaluated_sentences:\n        eval_words = _split_into_words([eval_s])\n        lcs = set(_recon_lcs(ref_words, eval_words))\n        union_lcs.update(lcs)\n        combined_lcs_length += len(lcs)\n\n    union_count = len(union_lcs)\n    return float(combined_lcs_length) / len(ref_words)\n\n", "idx": 560}
{"namespace": "sumy.parsers.html.HtmlParser.from_file", "completion": "        with open(file_path, \"rb\") as f:\n            return cls(f.read(), tokenizer, url)\n", "idx": 561}
{"namespace": "sumy.parsers.plaintext.PlaintextParser.document", "completion": "        # Create a new document object.\n        document = ObjectDocumentModel()\n\n        # Iterate through each line of the input text.\n        for line in self._text.splitlines():\n\n            # If the line is empty, it means that the paragraph has ended.\n            if not line:\n                # Create a new paragraph object.\n                paragraph = Paragraph()\n                # Add the paragraph to the document.\n                document.add_paragraph(paragraph)\n                # Skip to the next line.\n                continue\n\n            # If the line is not empty, it means that the paragraph has not ended.\n            # If the line is in upper case, it means that it is a heading.\n            if line.isupper():\n                # Create a new heading object.\n                heading = self._create_heading(line)\n                # Add the heading to the paragraph.\n                document.current_paragraph.add_heading(heading)\n                # Skip to the next line.\n                continue\n\n            # If the line is not empty and not in upper case, it means that it is a sentence.\n            # Create a new sentence object.\n            sentence = self._create_sentence(line)\n            # Add the sentence to the paragraph.\n            document.current_paragraph.add_sentence(sentence)\n\n        return document\n", "idx": 562}
{"namespace": "sumy.nlp.tokenizers.Tokenizer.to_sentences", "completion": "        # update abbreviations\n        abbreviations = self.LANGUAGE_EXTRA_ABREVS.get(self._language, [])\n        self._sentence_tokenizer._params.abbrev_types.update(abbreviations)\n\n        # tokenize\n        sentences = self._sentence_tokenizer.tokenize(paragraph)\n        return tuple(sentences)\n", "idx": 563}
{"namespace": "sumy.nlp.stemmers.null_stemmer", "completion": "    return str(object).lower()\n\n", "idx": 564}
{"namespace": "rows.fields.BinaryField.serialize", "completion": "        if value is None:\n            return \"\"\n        elif isinstance(value, cls.TYPE):\n            try:\n                return b64encode(value).decode(\"ascii\")\n            except (UnicodeDecodeError, TypeError):\n                return value\n        else:\n            value_error(value, cls)\n", "idx": 565}
{"namespace": "rows.fields.BoolField.deserialize", "completion": "        value = super(BoolField, cls).deserialize(value, *args, **kwargs)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        elif isinstance(value, six.text_type):\n            value = value.lower()\n            if value in cls.TRUE_VALUES:\n                return True\n            elif value in cls.FALSE_VALUES:\n                return False\n            else:\n                value_error(value, cls)\n        else:\n            value_error(value, cls)\n\n", "idx": 566}
{"namespace": "rows.fields.DateField.serialize", "completion": "        if value is None:\n            return \"\"\n        elif isinstance(value, cls.TYPE):\n            return value.strftime(cls.OUTPUT_FORMAT)\n        else:\n            value_error(value, cls)\n", "idx": 567}
{"namespace": "rows.fields.DateField.deserialize", "completion": "        value = super(DateField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n        try:\n            value = datetime.datetime.strptime(value, cls.INPUT_FORMAT)\n            return datetime.date(value.year, value.month, value.day)\n        except ValueError:\n            value_error(value, cls)\n\n", "idx": 568}
{"namespace": "rows.fields.TextField.deserialize", "completion": "        value = super(TextField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            return six.text_type(value)\n\n", "idx": 569}
{"namespace": "rows.fields.EmailField.deserialize", "completion": "        value = super(EmailField, cls).deserialize(value)\n        if value is None or value == \"\":\n            return None\n\n        if not cls.EMAIL_REGEXP.match(value):\n            value_error(value, cls)\n        return value\n\n", "idx": 570}
{"namespace": "rows.fields.JSONField.deserialize", "completion": "        value = super(JSONField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            return json.loads(value)\n\n", "idx": 571}
{"namespace": "falcon.http_error.HTTPError.to_json", "completion": "        if handler is None:\n            import json\n            handler = json\n\n        return handler.dumps(self.to_dict()).encode()\n", "idx": 572}
{"namespace": "falcon.inspect.inspect_app", "completion": "    return AppInfo(app)\n\n", "idx": 573}
{"namespace": "falcon.inspect.inspect_routes", "completion": "    routes = []\n    for uri, method_map in app._router._roots.items():\n        for method, resource in method_map.items():\n            routes.append(\n                RouteInfo(\n                    uri,\n                    method,\n                    resource,\n                    resource.params,\n                    resource.options,\n                    resource.resource_args,\n                    resource.resource_name,\n                    resource.resource_type,\n                )\n            )\n    return routes\n\n", "idx": 574}
{"namespace": "falcon.inspect.inspect_static_routes", "completion": "    static_routes = []\n    for path, resource in app._static_routes:\n        static_routes.append(StaticRouteInfo(path, resource))\n    return static_routes\n\n", "idx": 575}
{"namespace": "falcon.inspect.inspect_sinks", "completion": "    sinks = []\n    for sink in app._sinks:\n        info = SinkInfo(sink._prefix, sink._sink)\n        sinks.append(info)\n    return sinks\n\n", "idx": 576}
{"namespace": "falcon.inspect.inspect_error_handlers", "completion": "    error_handlers = []\n    for error_code, error_handler in app._error_handlers.items():\n        source_info, name = _get_source_info_and_name(error_handler)\n        info = ErrorHandlerInfo(error_code, name, source_info)\n        error_handlers.append(info)\n    return error_handlers\n\n", "idx": 577}
{"namespace": "falcon.inspect.inspect_middleware", "completion": "    middleware_info = MiddlewareInfo()\n    middleware_info.middleware_tree = _get_middleware_tree(app._middleware)\n    middleware_info.middleware_classes = _get_middleware_classes(app._middleware)\n    return middleware_info\n\n", "idx": 578}
{"namespace": "falcon.inspect.InspectVisitor.process", "completion": "        method_name = 'visit_' + instance.__visit_name__\n        method = getattr(self, method_name, None)\n        if method is None:\n            raise RuntimeError(\n                '{} has no method {}'.format(self.__class__.__name__, method_name)\n            )\n        return method(instance)\n\n", "idx": 579}
{"namespace": "falcon.request.Request.forwarded", "completion": "        forwarded = self._cached_forwarded\n\n        if forwarded is None:\n            forwarded = self._cached_forwarded = self._parse_forwarded()\n\n        return forwarded\n", "idx": 580}
{"namespace": "falcon.request.Request.client_accepts_msgpack", "completion": "        return self.client_accepts('application/x-msgpack') or self.client_accepts('application/msgpack')\n", "idx": 581}
{"namespace": "falcon.request.Request.content_length", "completion": "        try:\n            value = self.env['CONTENT_LENGTH']\n        except KeyError:\n            return None\n\n        try:\n            value_as_int = int(value)\n        except ValueError:\n            raise errors.HeaderNotValid('CONTENT_LENGTH', value)\n\n        if value_as_int < 0:\n            raise errors.HeaderNotValid('CONTENT_LENGTH', value)\n\n        return value_as_int\n", "idx": 582}
{"namespace": "falcon.request.Request.bounded_stream", "completion": "        if not self._bounded_stream:\n            self._bounded_stream = self._get_wrapped_wsgi_input()\n\n        return self._bounded_stream\n", "idx": 583}
{"namespace": "falcon.request.Request.uri", "completion": "        if self._cached_uri is None:\n            scheme = self.scheme\n            netloc = self.netloc\n            relative_uri = self.relative_uri\n            self._cached_uri = ''.join([scheme, '://', netloc, relative_uri])\n\n        return self._cached_uri\n", "idx": 584}
{"namespace": "falcon.request.Request.forwarded_uri", "completion": "        if self._cached_forwarded_uri is None:\n            # PERF: For small numbers of items, '+' is faster\n            # than ''.join(...). Concatenation is also generally\n            # faster than formatting.\n            value = self.forwarded_scheme + '://' + self.forwarded_host + self.relative_uri\n\n            self._cached_forwarded_uri = value\n\n        return self._cached_forwarded_uri\n", "idx": 585}
{"namespace": "falcon.request.Request.relative_uri", "completion": "        if self._cached_relative_uri is None:\n            path = self.path\n            if self.query_string:\n                path += '?' + self.query_string\n\n            # PERF: For small numbers of items, '+' is faster\n            # than ''.join(...). Concatenation is also generally\n            # faster than formatting.\n            value = self.root_path + path\n\n            self._cached_relative_uri = value\n\n        return self._cached_relative_uri\n", "idx": 586}
{"namespace": "falcon.request.Request.prefix", "completion": "        if self._cached_prefix is None:\n            self._cached_prefix = '{}://{}{}'.format(self.scheme, self.netloc, self.app)\n\n        return self._cached_prefix\n", "idx": 587}
{"namespace": "falcon.request.Request.forwarded_prefix", "completion": "        if self._cached_forwarded_prefix is None:\n            self._cached_forwarded_prefix = (\n                self.forwarded_scheme + '://' + self.forwarded_host + self.app\n            )\n\n        return self._cached_forwarded_prefix\n", "idx": 588}
{"namespace": "falcon.request.Request.host", "completion": "        # PERF: For small numbers of items, '+' is faster than ''.join(...).\n        # Concatenation is also faster than formatting.\n        host = self.env.get('HTTP_HOST')\n        if host is not None:\n            return host\n\n        # NOTE(kgriffs): According to PEP-3333, this header is required\n        # to be in all HTTP requests.\n        return self.env['SERVER_NAME']\n", "idx": 589}
{"namespace": "falcon.request.Request.subdomain", "completion": "        host = self.host\n        if '.' in host:\n            subdomain, sep, remainder = host.partition('.')\n            if sep:\n                return subdomain\n", "idx": 590}
{"namespace": "falcon.request.Request.headers", "completion": "        if self._cached_headers is None:\n            headers = self._cached_headers = {}\n\n            env = self.env\n            for name, value in env.items():\n                if name.startswith('HTTP_'):\n                    # NOTE(kgriffs): Don't take the time to fix the case\n                    # since headers are supposed to be case-insensitive\n                    # anyway.\n                    headers[name[5:].replace('_', '-')] = value\n\n                elif name in WSGI_CONTENT_HEADERS:\n                    headers[name.replace('_', '-')] = value\n\n        return self._cached_headers.copy()\n", "idx": 591}
{"namespace": "falcon.request.Request.remote_addr", "completion": "        return self.env.get('REMOTE_ADDR', '127.0.0.1')\n", "idx": 592}
{"namespace": "falcon.request.Request.client_accepts", "completion": "        accept = self.accept\n\n        # If the client accepts anything\n        if accept == '*/*' or accept == '*/*;q=0.5':\n            return True\n\n        # If the client doesn't accept the specified media type\n        if media_type not in accept:\n            return False\n\n        # If there is no q parameter, accept the media type\n        if ';q=' not in accept:\n            return True\n\n        # If there is a q parameter, make sure it has a value of 1 to accept the media type\n        else:\n            accept_list = accept.split(\";\")\n            q_index = accept_list.index(\"q=\")\n            q_value = accept_list[q_index + 1]\n\n            if q_value == '1':\n                return True\n            else:\n                return False\n", "idx": 593}
{"namespace": "falcon.request.Request.client_prefers", "completion": "        try:\n            # NOTE(kgriffs): parse_accept_header is new in wsgiref 4.0\n            parsed = mimeparse.parse_accept_header(self.accept)\n        except ValueError:\n            # ValueError is raised if the accept header is malformed\n            return media_types[0]\n\n        try:\n            # NOTE(kgriffs): Best match will be put first\n            preferred_type = mimeparse.best_match(media_types, parsed)\n        except ValueError:\n            # ValueError is raised if no match is found\n            return media_types[0]\n\n        return preferred_type\n", "idx": 594}
{"namespace": "falcon.request.Request.get_header", "completion": "        # NOTE(kgriffs): Normalize the header name\n        uc_name = name.upper().replace('-', '_')\n        if uc_name in self.env:\n            return self.env[uc_name]\n\n        if required:\n            raise errors.HTTPMissingHeader(name)\n\n        return default\n", "idx": 595}
{"namespace": "falcon.request.Request.get_cookie_values", "completion": "        if self._cookies is None:\n            self._cookies = cookies.RequestCookies(self.env)\n\n        return self._cookies.get_values(name)\n", "idx": 596}
{"namespace": "falcon.response.Response.unset_cookie", "completion": "        if self._cookies is None:\n            self._cookies = http_cookies.SimpleCookie()\n\n        self._cookies[name] = ''\n        self._cookies[name]['expires'] = -1\n        self._cookies[name]['max-age'] = 0\n\n        if domain:\n            self._cookies[name]['domain'] = domain\n\n        if path:\n            self._cookies[name]['path'] = path\n", "idx": 597}
{"namespace": "falcon.response.Response.get_header", "completion": "        name = name.lower()\n        if name == 'set-cookie':\n            raise HeaderNotSupported('The \"Set-Cookie\" header cannot be retrieved.')\n        if name in self._headers:\n            return self._headers[name]\n        return default\n", "idx": 598}
{"namespace": "falcon.response.Response.set_header", "completion": "        # NOTE(kgriffs): Normalize name by lowercasing it\n        name = name.lower()\n\n        if name == 'set-cookie':\n            raise HeaderNotSupported('Setting Set-Cookie is not currently supported.')\n\n        # NOTE(kgriffs): Normalize value by stripping whitespace\n        # Insert a space in case the value is entirely composed of tokens\n        value = ' '.join(value.split())\n\n        self._headers[name] = value\n", "idx": 599}
{"namespace": "falcon.response.Response.delete_header", "completion": "        # NOTE(kgriffs): normalize name by lowercasing it\n        name = name.lower()\n\n        if name == 'set-cookie':\n            raise HeaderNotSupported('This method cannot be used to delete cookies')\n\n        if name in self._headers:\n            del self._headers[name]\n", "idx": 600}
{"namespace": "falcon.cmd.inspect_app.route_main", "completion": "    print('The \"falcon-print-routes\" command is deprecated.')\n    print('Please use \"falcon-inspect-app\"')\n    main()\n\n", "idx": 601}
{"namespace": "falcon.util.uri.decode", "completion": "    if encoded_uri is None:\n        return None\n\n    if not encoded_uri:\n        return ''\n\n    if unquote_plus:\n        return _cy_decode(encoded_uri)\n\n    tokens = encoded_uri.split('%')\n    decoded_uri = tokens[0]\n    for token in tokens[1:]:\n        token_partial = token[:2]\n        try:\n            decoded_uri += _HEX_TO_BYTE[token_partial] + token[2:]\n        except KeyError:\n            # malformed percentage like \"x=%\" or \"y=%+\"\n            decoded_uri += '%' + token\n\n    return decoded_uri\n\n", "idx": 602}
{"namespace": "falcon.util.structures.ETag.dumps", "completion": "        if self.is_weak:\n            return \"W/\" + str(self)\n        else:\n            return str(self)\n", "idx": 603}
{"namespace": "falcon.util.structures.ETag.loads", "completion": "        if etag_str.startswith(('W/', 'w/')):\n            is_weak = True\n            etag_str = etag_str[2:]\n        else:\n            is_weak = False\n\n        if etag_str[:1] == etag_str[-1:] == '\"':\n            etag_str = etag_str[1:-1]\n\n        return cls(etag_str, is_weak)\n\n", "idx": 604}
{"namespace": "falcon.util.misc.secure_filename", "completion": "    filename = unicodedata.normalize('NFKD', filename).encode('ascii', 'ignore').decode('ascii')\n    filename = filename.replace(' ', '_')\n    if filename.startswith('.'):\n        filename = '_' + filename[1:]\n    return filename\n\n", "idx": 605}
{"namespace": "falcon.asgi.reader.BufferedReader.peek", "completion": "        if size < -1:\n            raise ValueError('size must be greater than or equal to -1')\n\n        if size == -1:\n            size = self._buffer_len - self._buffer_pos\n\n        if size > self._buffer_len - self._buffer_pos:\n            async for chunk in self._source:\n                self._buffer += chunk\n                self._buffer_len += len(chunk)\n                if size <= self._buffer_len - self._buffer_pos:\n                    break\n\n        return self._buffer[self._buffer_pos : self._buffer_pos + size]\n", "idx": 606}
{"namespace": "falcon.asgi.reader.BufferedReader.read_until", "completion": "        return await self._read_from(\n            self._iter_delimited(delimiter, size_hint=size or 0), size\n        )\n", "idx": 607}
{"namespace": "falcon.routing.converters.IntConverter.convert", "completion": "        if self._num_digits is not None and len(value) != self._num_digits:\n            return None\n\n        if value.strip() != value:\n            return None\n\n        try:\n            value = int(value)\n        except ValueError:\n            return None\n\n        if self._min is not None and value < self._min:\n            return None\n\n        if self._max is not None and value > self._max:\n            return None\n\n        return value\n\n", "idx": 608}
{"namespace": "falcon.routing.converters.DateTimeConverter.convert", "completion": "        try:\n            return strptime(value, self._format_string)\n        except ValueError:\n            return None\n\n", "idx": 609}
{"namespace": "falcon.routing.util.map_http_methods", "completion": "    method_map = {}\n\n    for method in constants.COMBINED_METHODS:\n        try:\n            responder = resource.get_responder(method)\n        except AttributeError:\n            # NOTE(kgriffs): This particular type of error is not usually\n            # a problem, since we do not require that a resource implement\n            # all of the responders. For example, it is not uncommon to have\n            # a resource that only handles POST and PATCH.\n            pass\n        else:\n            if callable(responder):\n                if suffix is None:\n                    method_map[method] = responder\n                else:\n                    try:\n                        method_map[method] = getattr(resource, responder.__name__ + suffix)\n                    except AttributeError:\n                        pass\n\n    return method_map\n\n", "idx": 610}
{"namespace": "falcon.routing.static._BoundedFile.read", "completion": "        if size < 0:\n            size = self.remaining\n        else:\n            size = min(size, self.remaining)\n\n        data = self.fh.read(size)\n        self.remaining -= len(data)\n        return data\n\n", "idx": 611}
{"namespace": "authlib.oauth2.rfc6749.util.list_to_scope", "completion": "    if scope is None:\n        return None\n    elif isinstance(scope, (set, tuple, list)):\n        return \" \".join(scope)\n    else:\n        return to_unicode(scope)\n\n", "idx": 612}
{"namespace": "authlib.oauth2.rfc6749.util.extract_basic_authorization", "completion": "    auth_header = headers.get('Authorization')\n    if not auth_header:\n        return None, None\n\n    parts = auth_header.split(None, 1)\n    if len(parts) != 2:\n        return None, None\n\n    if parts[0].lower() != 'basic':\n        return None, None\n\n    auth_token = parts[1]\n    auth_token = auth_token.encode('ascii')\n    try:\n        auth_token = base64.b64decode(auth_token)\n    except (TypeError, binascii.Error):\n        return None, None\n\n    auth_token = auth_token.decode('ascii')\n    username, password = auth_token.split(':', 1)\n    return username, password", "idx": 613}
{"namespace": "authlib.oauth2.rfc6749.parameters.prepare_grant_uri", "completion": "    params = [\n        ('response_type', response_type),\n        ('client_id', client_id)\n    ]\n\n    if redirect_uri:\n        params.append(('redirect_uri', redirect_uri))\n    if scope:\n        params.append(('scope', list_to_scope(scope)))\n    if state:\n        params.append(('state', state))\n\n    for k in kwargs:\n        if kwargs[k]:\n            params.append((to_unicode(k), kwargs[k]))\n\n    return add_params_to_qs(uri, params)\n\n", "idx": 614}
{"namespace": "authlib.oauth2.rfc6749.parameters.parse_authorization_code_response", "completion": "    params = urlparse.parse_qs(urlparse.urlparse(uri).query)\n    if 'code' not in params:\n        raise MissingCodeException()\n    if state and state != params['state'][0]:\n        raise MismatchingStateException()\n    return params", "idx": 615}
{"namespace": "authlib.oauth2.rfc6749.parameters.parse_implicit_response", "completion": "    fragment = urlparse.urlparse(uri).fragment\n    params = dict(urlparse.parse_qsl(fragment, keep_blank_values=True))\n\n    if 'access_token' not in params:\n        raise MissingTokenException()\n\n    if 'token_type' not in params:\n        raise MissingTokenTypeException()\n\n    if state and 'state' not in params:\n        raise MissingStateException()\n\n    return params", "idx": 616}
{"namespace": "authlib.common.encoding.json_b64encode", "completion": "    if isinstance(text, dict):\n        text = json_dumps(text)\n\n    text = to_bytes(text)\n    text = base64.urlsafe_b64encode(text)\n    text = to_unicode(text, 'ascii')\n\n    return text\n\n", "idx": 617}
{"namespace": "authlib.jose.util.extract_header", "completion": "    try:\n        header_data = urlsafe_b64decode(header_segment)\n        header = json_loads(header_data)\n    except (ValueError, TypeError):\n        raise error_cls('Invalid header')\n\n    if not isinstance(header, dict):\n        raise error_cls('Header must be a dictionary')\n\n    return header\n\n", "idx": 618}
{"namespace": "twitter.models.TwitterModel.AsDict", "completion": "        data = {}\n\n        for (key, val) in self.param_defaults.items():\n            small_key = key[1:]\n            if isinstance(val, list):\n                data[small_key] = []\n                for item in val:\n                    if hasattr(item, 'AsDict'):\n                        data[small_key].append(item.AsDict())\n                    else:\n                        data[small_key].append(item)\n            elif isinstance(val, tuple):\n                data[small_key] = []\n                for item in val:\n                    if hasattr(item, 'AsDict'):\n                        data[small_key].append(item.AsDict())\n                    else:\n                        data[small_key].append(item)\n            elif isinstance(val, set):\n                data[small_key] = []\n                for item in val:\n                    if hasattr(item, 'AsDict'):\n                        data[small_key].append(item.AsDict())\n                    else:\n                        data[small_key].append(item)\n            elif hasattr(val, 'AsDict'):\n                data[small_key] = val.AsDict()\n            else:\n                data[small_key] = val\n\n        return data\n", "idx": 619}
{"namespace": "twitter.models.TwitterModel.NewFromJsonDict", "completion": "        json_data = data.copy()\n        if kwargs:\n            json_data.update(kwargs)\n        return cls(**json_data)\n\n", "idx": 620}
{"namespace": "twitter.api.Api._TweetTextWrap", "completion": "        # Split the status into words\n        words = status.split()\n\n        # Initialize the list of tweets\n        tweets = []\n\n        # Initialize the current line\n        line = ''\n\n        # Iterate through the words\n        for word in words:\n\n            # If the word is longer than the character limit, raise an exception\n            if len(word) > char_lim:\n                raise Exception('Word is too long to be tweeted.')\n\n            # If the length of the line plus the length of the word plus the length of a space exceeds the character limit\n            if len(line) + len(word) + 1 > char_lim:\n\n                # Append the line to the list of tweets\n                tweets.append(line)\n\n                # Start a new line with the current word\n                line = word\n\n            # If the length of the line plus the length of the word plus the length of a space does not exceed the character limit\n            else:\n\n                # If the line is empty\n                if line == '':\n\n                    # Set the line to the word\n                    line = word\n\n                # If the line is not empty\n                else:\n\n                    # Append the word to the line with a space\n                    line += ' ' + word\n\n        # Append the last line to the list of tweets\n        tweets.append(line)\n\n        # Return the list of tweets\n        return tweets\n", "idx": 621}
{"namespace": "databases.importer.import_from_string", "completion": "    try:\n        module_str, _, attrs_str = import_str.partition(\":\")\n        module = importlib.import_module(module_str)\n        try:\n            return functools.reduce(getattr, attrs_str.split(\".\"), module)\n        except AttributeError as e:\n            raise ImportFromStringError(\n                f\"Unable to import {import_str!r} ({e})\"\n            )\n    except ImportError as e:\n        raise ImportFromStringError(\n            f\"Unable to import {import_str!r} ({e})\"\n        )", "idx": 622}
{"namespace": "rest_framework.reverse.reverse", "completion": "    url = django_reverse(viewname, args=args, kwargs=kwargs, **extra)\n    return preserve_builtin_query_params(url, request)\n", "idx": 623}
{"namespace": "rest_framework.serializers.Serializer.fields", "completion": "        # Retrieve the field instances from the declared fields on the class.\n        # By retrieving them from `self._declared_fields` we can ensure that we\n        # get the values as declared on the class, rather than the updated values\n        # on the instance, which may not be available until the metaclass has\n        # created the instance.\n        declared_fields = copy.deepcopy(self._declared_fields)\n        for field_name, field in declared_fields.items():\n            if not isinstance(field, Field):\n                continue\n            field.field_name = field_name\n        return declared_fields\n", "idx": 624}
{"namespace": "rest_framework.parsers.JSONParser.parse", "completion": "        parser_context = parser_context or {}\n        encoding = parser_context.get('encoding', settings.DEFAULT_CHARSET)\n\n        try:\n            decoded_stream = codecs.getreader(encoding)(stream).read()\n            return renderers.JSONRenderer().loads(decoded_stream)\n        except ValueError as exc:\n            raise ParseError('JSON parse error - %s' % str(exc))\n\n", "idx": 625}
{"namespace": "rest_framework.parsers.FileUploadParser.get_filename", "completion": "        # Try to get a filename from URL parameters\n        try:\n            filename = parser_context['args']['filename']\n            return filename\n        except KeyError:\n            pass\n\n        # If no filename is found in the URL, parse the Content-Disposition header\n        try:\n            content_disposition = parser_context['request'].META['HTTP_CONTENT_DISPOSITION']\n            filename = content_disposition.split('filename=')[-1]\n            if filename[0] == filename[-1] == '\"':\n                filename = filename[1:-1]\n            return filename\n        except (KeyError, IndexError):\n            pass\n\n        # If no filename is found in the header, return None\n        return None\n\n", "idx": 626}
{"namespace": "rest_framework.fields.is_simple_callable", "completion": "    if not callable(obj):\n        return False\n\n    try:\n        signature = inspect.signature(obj)\n    except ValueError:\n        raise BuiltinSignatureError(\n            'Built-in function signatures are not inspectable. '\n            'Wrap the function in a `functools.partial` object.'\n        )\n\n    for param in signature.parameters.values():\n        if param.default == inspect.Parameter.empty and param.kind not in (\n            inspect.Parameter.VAR_POSITIONAL,\n            inspect.Parameter.VAR_KEYWORD\n        ):\n            return False\n\n    return True\n\n", "idx": 627}
{"namespace": "rest_framework.fields.Field.bind", "completion": "        # In order to enforce a consistent style, we error if a redundant\n        # 'source' argument has been used. For example:\n        # my_field = serializer.CharField(source='my_field')\n        assert self.source != field_name, (\n            \"It is redundant to specify `source='%s'` on field '%s' in \"\n            \"serializer '%s', because it is the same as the field name. \"\n            \"Remove the `source` keyword argument.\" %\n            (field_name, self.__class__.__name__, parent.__class__.__name__)\n        )\n\n        self.field_name = field_name\n        self.parent = parent\n\n        # self.source should default to the field name.\n        if self.source is None:\n            self.source = field_name\n\n        # self.label should default to self.field_name.title().\n        if self.label is None:\n            self.label = self.field_name.title()\n\n        # self.source_attrs is a list of attributes that need to be looked up\n        # when serializing the instance, or populating the validated data.\n        if self.source == '*':\n            self.source_attrs = []\n        else:\n            self.source_attrs = self.source.split('.')\n", "idx": 628}
{"namespace": "rest_framework.fields.Field.run_validation", "completion": "        (is_empty_value, data) = self.validate_empty_values(data)\n        if is_empty_value:\n            return data\n        value = self.to_internal_value(data)\n        self.run_validators(value)\n        return value\n", "idx": 629}
{"namespace": "rest_framework.fields.Field.root", "completion": "        root = self\n        while hasattr(root, 'parent'):\n            root = root.parent\n        return root\n", "idx": 630}
{"namespace": "rest_framework.fields.CharField.run_validation", "completion": "        if data == '' or (self.trim_whitespace and str(data).strip() == ''):\n            if not self.allow_blank:\n                self.fail('blank')\n            return ''\n        return super().run_validation(data)\n", "idx": 631}
{"namespace": "rest_framework.fields.CharField.to_internal_value", "completion": "        if isinstance(data, bool) or not isinstance(data, (str, int, float)):\n            self.fail('invalid')\n        value = str(data)\n        return value.strip() if self.trim_whitespace else value\n", "idx": 632}
{"namespace": "rest_framework.fields.DecimalField.to_internal_value", "completion": "        if isinstance(data, str) and len(data) > self.MAX_STRING_LENGTH:\n            self.fail('max_string_length')\n\n        try:\n            value = connection.ops.value_to_db_decimal(data, self.max_digits, self.decimal_places)\n        except (TypeError, ValueError, OverflowError, InvalidOperation):\n            self.fail('invalid')\n        return value\n", "idx": 633}
{"namespace": "rest_framework.fields.DateTimeField.to_representation", "completion": "        if not value:\n            return None\n\n        output_format = getattr(self, 'format', api_settings.DATETIME_FORMAT)\n\n        if output_format is None or isinstance(value, str):\n            return value\n\n        # Apply make_aware before strftime so that local time is converted to UTC\n        # for timezones that are west of UTC.\n        value = self.enforce_timezone(value)\n        value = value.strftime(output_format)\n        return value\n\n", "idx": 634}
{"namespace": "rest_framework.fields.ChoiceField.iter_options", "completion": "        # If the grouped choices are not empty, then iterate over them.\n        if bool(self.grouped_choices):\n            for (optgroup_label, optgroup) in self.grouped_choices:\n                for (option_value, option_label) in optgroup:\n                    # If the cutoff value is not None, then check if the length of the option group is greater than the cutoff value.\n                    if self.html_cutoff is not None and len(optgroup) > self.html_cutoff:\n                        # If the cutoff value is not None, then check if the length of the option group is greater than the cutoff value.\n                        if option_value is not None:\n                            # If the cutoff value is not None, then check if the length of the option group is greater than the cutoff value.\n                            yield (option_value, self.html_cutoff_text, self.display_choice_strings_to_values.get(option_value))\n                            # Break out of the loop.\n                            break\n                    else:\n                        # If the cutoff value is None, then yield the option group.\n                        yield (option_value, option_label, self.display_choice_strings_to_values.get(option_value))\n        # If the grouped choices are empty, then iterate over the choices.\n        else:\n            for (option_value, option_label) in self.choices:\n                # If the cutoff value is not None, then check if the length of the option group is greater than the cutoff value.\n                if self.html_cutoff is not None and len(self.choices) > self.html_cutoff:\n                    # If the cutoff value is not None, then check if the length of the option group is greater than the cutoff value.\n                    if option_value is not None:\n                        # If the cutoff value is not None, then check if the length of the option group is greater than the cutoff value.\n                        yield (option_value, self.html_cutoff_text, self.display_choice_strings_to_values.get(option_value", "idx": 635}
{"namespace": "rest_framework.fields.MultipleChoiceField.get_value", "completion": "        if self.field_name not in dictionary:\n            if self.partial:\n                return empty\n        if html.is_html_input(dictionary):\n            return dictionary.getlist(self.field_name)\n        return dictionary.get(self.field_name)\n", "idx": 636}
{"namespace": "rest_framework.exceptions._get_error_details", "completion": "    if isinstance(data, dict):\n        ret = {}\n        for key, value in data.items():\n            # Here we check if the value is an error object. If so, we can apply the default code.\n            if isinstance(value, ErrorDetail):\n                ret[key] = ErrorDetail(value.string, code=value.code or default_code)\n            else:\n                ret[key] = _get_error_details(value, default_code)\n        return ret\n    elif isinstance(data, (list, tuple)):\n        ret = []\n        for item in data:\n            # Here we check if the value is an error object. If so, we can apply the default code.\n            if isinstance(item, ErrorDetail):\n                ret.append(ErrorDetail(item.string, code=item.code or default_code))\n            else:\n                ret.append(_get_error_details(item, default_code))\n        return ret\n    elif isinstance(data, str):\n        return ErrorDetail(data, code=default_code)\n    return data\n\n", "idx": 637}
{"namespace": "rest_framework.exceptions.server_error", "completion": "    return JsonResponse({'error': 'Internal Server Error'}, status=500)\n\n", "idx": 638}
{"namespace": "rest_framework.exceptions.bad_request", "completion": "    data = {\n        'error': 'Bad Request (400)'\n    }\n    return JsonResponse(data, status=status.HTTP_400_BAD_REQUEST)\n\n", "idx": 639}
{"namespace": "rest_framework.relations.RelatedField.iter_options", "completion": "        # If the field is a ManyRelatedField, then we need to iterate over the\n        # child_relation.\n        if isinstance(self, ManyRelatedField):\n            return self.child_relation.iter_options()\n\n        # Otherwise, we can just iterate over the options of the field.\n        return iter_options(self)\n", "idx": 640}
{"namespace": "rest_framework.relations.PrimaryKeyRelatedField.to_internal_value", "completion": "        if self.pk_field is not None:\n            data = self.pk_field.to_internal_value(data)\n\n        queryset = self.get_queryset()\n        try:\n            return queryset.get(pk=data)\n        except ObjectDoesNotExist:\n            self.fail('does_not_exist', pk_value=data)\n        except (TypeError, ValueError):\n            self.fail('incorrect_type', data_type=type(data).__name__)\n", "idx": 641}
{"namespace": "rest_framework.relations.PrimaryKeyRelatedField.to_representation", "completion": "        if self.pk_field is not None:\n            return self.pk_field.to_representation(value.pk)\n        return value.pk\n\n", "idx": 642}
{"namespace": "rest_framework.relations.SlugRelatedField.to_internal_value", "completion": "        # If the data is a string, try to get the object based on the slug field.\n        if isinstance(data, str):\n            try:\n                return self.get_queryset().get(**{self.slug_field: data})\n            except ObjectDoesNotExist:\n                self.fail('does_not_exist', slug_name=self.slug_field, value=smart_str(data))\n            except (TypeError, ValueError):\n                self.fail('invalid')\n\n        # If the data is not a string, then it is invalid.\n        self.fail('invalid')\n", "idx": 643}
{"namespace": "rest_framework.templatetags.rest_framework.add_query_param", "completion": "    dict_ = request.GET.copy()\n    dict_[key] = val\n    path = iri_to_uri(request.get_full_path())\n    return escape(\"{path}?{query}\".format(path=path, query=dict_.urlencode()))\n\n", "idx": 644}
{"namespace": "rest_framework.utils.mediatypes._MediaType.match", "completion": "        # If the main types do not match, return False\n        if self.main_type != '*' and self.main_type != other.main_type:\n            return False\n\n        # If the subtypes do not match, return False\n        if self.sub_type != '*' and self.sub_type != other.sub_type:\n            return False\n\n        # If the two MediaType objects have different parameters, return False\n        if self.params != other.params:\n            return False\n\n        # If all the above checks pass, return True\n        return True\n", "idx": 645}
{"namespace": "rest_framework.utils.mediatypes._MediaType.precedence", "completion": "        if self.main_type == '*':\n            if self.sub_type != '*':\n                return 1\n            return 0\n        elif self.sub_type == '*':\n            return 1\n        elif self.params or (not self.params and other.params):\n            return 3\n        return 2", "idx": 646}
{"namespace": "rest_framework.utils.mediatypes._MediaType.__str__", "completion": "        return self.full_type + ''.join([';%s=%s' % (key, val) for key, val in self.params.items()])\n", "idx": 647}
{"namespace": "asyncpg._testbase.TestCase.assertLoopErrorHandlerCalled", "completion": "        # Set the exception handler for the loop.\n        self.loop.set_exception_handler(self.loop_exception_handler)\n\n        # Execute the code block.\n        yield\n\n        # Check if any of the logged messages match the given regular expression.\n        if not self.__unhandled_exceptions:\n            raise AssertionError(\n                'no loop exception was raised')\n\n        for context in self.__unhandled_exceptions:\n            if re.search(msg_re, context['message']):\n                return\n\n        # If no matching message is found, raise an AssertionError.\n        raise AssertionError(\n            'no loop exception was raised with message matching {}'.format(\n                msg_re))\n", "idx": 648}
{"namespace": "csvs_to_sqlite.utils.refactor_dataframes", "completion": "    # Create a lookup table for each foreign key\n    lookup_tables = {}\n    for column, (table_name, value_column) in foreign_keys.items():\n        lookup_tables[column] = LookupTable(conn, table_name, value_column, index_fts)\n\n    # Replace each foreign key with the corresponding value from the lookup table\n    for df in dataframes:\n        for column, lookup_table in lookup_tables.items():\n            if column in df.columns:\n                df[column] = df[column].apply(lookup_table.id_for_value)\n\n    return dataframes\n\n", "idx": 649}
{"namespace": "sqlitedict.SqliteDict.iteritems", "completion": "        GET_ITEMS = 'SELECT key, value FROM \"%s\" ORDER BY rowid' % self.tablename\n        for key, value in self.conn.select(GET_ITEMS):\n            yield self.decode_key(key), self.decode(value)\n", "idx": 650}
{"namespace": "sqlitedict.SqliteDict.update", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to update read-only SqliteDict')\n\n        if hasattr(items, 'keys'):\n            items = items.items()\n        self.conn.executemany('REPLACE INTO \"%s\" (key, value) VALUES (?, ?)' % self.tablename,\n                              ((self.encode_key(k), self.encode(v)) for k, v in items))\n        if kwds:\n            self.update(kwds)\n        if self.autocommit:\n            self.commit()\n", "idx": 651}
{"namespace": "sqlitedict.SqliteDict.clear", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to clear a read-only SqliteDict')\n\n        CLEAR_ALL = 'DELETE FROM \"%s\"' % self.tablename\n        self.conn.commit()\n        self.conn.execute(CLEAR_ALL)\n        if self.autocommit:\n            self.commit()\n", "idx": 652}
{"namespace": "sqlitedict.SqliteDict.commit", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to commit read-only SqliteDict')\n\n        self.conn.commit(blocking=blocking)\n", "idx": 653}
{"namespace": "sqlitedict.SqliteDict.terminate", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to delete read-only SqliteDict')\n\n        self.close()\n        if self.filename == \":memory:\":\n            return\n        try:\n            os.remove(self.filename)\n        except OSError:\n            pass\n", "idx": 654}
{"namespace": "boto.utils.retry_url", "completion": "    if not provider:\n        provider = boto.provider.get_default()\n    if provider.proxy:\n        proxy_handler = urllib.request.ProxyHandler({'http': provider.proxy})\n        opener = urllib.request.build_opener(proxy_handler)\n        urllib.request.install_opener(opener)\n    for i in range(0, num_retries):\n        try:\n            req = urllib.request.Request(url)\n            r = urllib.request.urlopen(req, timeout=timeout)\n            return r.read()\n        except urllib.error.HTTPError as e:\n            # We don't retry errors not related to a missing instance\n            if e.code == 404 and not retry_on_404:\n                raise\n        except urllib.error.URLError as e:\n            # We don't retry if there is a network error\n            if isinstance(e.reason, socket.gaierror):\n                raise\n        except socket.timeout as e:\n            # We don't retry on a socket timeout\n            raise\n        except ssl.SSLError as e:\n            # We don't retry on an SSLError\n            raise\n        except httplib.BadStatusLine as e:\n            # We don't retry on a BadStatusLine\n            raise\n        time.sleep(2 ** i)\n    return urllib.request.urlopen(req, timeout=timeout).read()\n\n", "idx": 655}
{"namespace": "boto.utils.LazyLoadMetadata.values", "completion": "        self._materialize()\n        return super(LazyLoadMetadata, self).values()\n", "idx": 656}
{"namespace": "boto.utils.get_instance_userdata", "completion": "    user_data = {}\n    user_data_url = _build_instance_metadata_url(url, version, 'user-data')\n    try:\n        user_data = retry_url(user_data_url, num_retries=num_retries, timeout=timeout)\n        if sep:\n            user_data = dict(l.split(sep, 1) for l in user_data.splitlines() if l)\n    except urllib.error.URLError:\n        return None\n    return user_data\n\n", "idx": 657}
{"namespace": "boto.utils.pythonize_name", "completion": "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n\n", "idx": 658}
{"namespace": "boto.cloudsearchdomain.connect_to_region", "completion": "    from boto.cloudsearchdomain.layer1 import CloudSearchDomainConnection\n    region = RegionInfo(name=region_name,\n                        endpoint='cloudsearch.us-east-1.amazonaws.com',\n                        connection_cls=CloudSearchDomainConnection)\n    return region.connect(**kw_params)", "idx": 659}
{"namespace": "boto.redshift.connect_to_region", "completion": "    from boto.redshift.layer1 import RedshiftConnection\n    region = RegionInfo(name=region_name, endpoint='redshift.us-east-1.amazonaws.com', connection_cls=RedshiftConnection)\n    return region.connect(**kw_params)", "idx": 660}
{"namespace": "boto.support.connect_to_region", "completion": "    from boto.support.layer1 import SupportConnection\n    return SupportConnection(region=RegionInfo(None, region_name, **kw_params))", "idx": 661}
{"namespace": "boto.configservice.connect_to_region", "completion": "    from boto.configservice.layer1 import ConfigServiceConnection\n    return ConfigServiceConnection(region=RegionInfo(name=region_name, endpoint='config.us-east-1.amazonaws.com'), **kw_params)", "idx": 662}
{"namespace": "boto.cloudhsm.connect_to_region", "completion": "    from boto.cloudhsm import layer1\n    return layer1.connect_to_region(region_name, **kw_params)", "idx": 663}
{"namespace": "boto.logs.connect_to_region", "completion": "    from boto.logs.layer1 import CloudWatchLogsConnection\n    return CloudWatchLogsConnection(region=region_name, **kw_params)", "idx": 664}
{"namespace": "boto.cloudsearch.connect_to_region", "completion": "    from boto.cloudsearch.layer1 import Layer1\n    return Layer1(region=RegionInfo(None, region_name), **kw_params)", "idx": 665}
{"namespace": "boto.glacier.job.Job.download_to_fileobj", "completion": "        num_chunks = self._calc_num_chunks(chunk_size)\n        self._download_to_fileob(output_file, num_chunks, chunk_size,\n                                 verify_hashes, retry_exceptions)\n", "idx": 666}
{"namespace": "boto.glacier.utils.minimum_part_size", "completion": "    if size_in_bytes < 0:\n        raise ValueError('Archive size should be a positive number.')\n\n    if size_in_bytes > MAXIMUM_NUMBER_OF_PARTS * default_part_size:\n        raise ValueError('Archive size of %s bytes exceeds the maximum allowed size of %s bytes.' % (size_in_bytes, MAXIMUM_NUMBER_OF_PARTS * default_part_size))\n\n    if size_in_bytes <= MAXIMUM_NUMBER_OF_PARTS * default_part_size:\n        return default_part_size\n\n    return _calculate_part_size(size_in_bytes, MAXIMUM_NUMBER_OF_PARTS)\n\n", "idx": 667}
{"namespace": "boto.glacier.utils.chunk_hashes", "completion": "    chunk_size = int(chunk_size)\n    chunk_hashes = []\n    chunk_value = bytestring\n    while chunk_value:\n        chunk_value = chunk_value[chunk_size:]\n        chunk_hashes.append(hashlib.sha256(chunk_value).digest())\n    return chunk_hashes\n\n", "idx": 668}
{"namespace": "boto.glacier.utils.compute_hashes_from_fileobj", "completion": "    # Initialize the hashers\n    linear_hasher = hashlib.sha256()\n    tree_hasher = hashlib.sha256()\n\n    # Read the first chunk\n    data = fileobj.read(chunk_size)\n\n    # While there is data remaining, update the hashers and read the next chunk\n    while data:\n        linear_hasher.update(data)\n        tree_hasher.update(data)\n        data = fileobj.read(chunk_size)\n\n    # Finalize the tree hash\n    while len(tree_hasher.digest()) > chunk_size:\n        tree_hasher = hashlib.sha256(tree_hasher.digest())\n\n    # Return the hashes in hexadecimal format\n    return linear_hasher.hexdigest(), tree_hasher.hexdigest()\n\n", "idx": 669}
{"namespace": "boto.glacier.concurrent.ConcurrentTransferer._calculate_required_part_size", "completion": "        # The minimum part size is 1 MiB.\n        min_part_size = 1024 * 1024\n        if self._part_size < min_part_size:\n            self._part_size = min_part_size\n        num_parts = int(math.ceil(total_size / float(self._part_size)))\n        if num_parts == 1:\n            self._part_size = total_size\n        else:\n            self._part_size = int(math.floor(total_size / float(num_parts)))\n        return (num_parts, self._part_size)\n", "idx": 670}
{"namespace": "boto.glacier.connect_to_region", "completion": "    from boto.glacier.layer2 import Layer2\n    return Layer2(**kw_params)", "idx": 671}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.update", "completion": "        rs = self.connection.get_all_network_interfaces(\n            [self.id],\n            dry_run=dry_run\n        )\n        if len(rs) > 0:\n            self._update(rs[0])\n            return self.status\n        elif validate:\n            raise ValueError('%s is not a valid ENI ID.' % self.id)\n        else:\n            return None\n", "idx": 672}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.attach", "completion": "        if not self.connection:\n            import boto\n            self.connection = boto.connect_vpc(**self.connection_args)\n        rs = self.connection.attach_network_interface(\n            self.id,\n            instance_id,\n            device_index,\n            dry_run=dry_run\n        )\n        if rs:\n            self.attachment = rs\n            return True\n        else:\n            return False\n", "idx": 673}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.detach", "completion": "        return self.connection.detach_network_interface(\n            self.attachment.id,\n            force,\n            dry_run=dry_run\n        )\n", "idx": 674}
{"namespace": "boto.ec2.address.Address.release", "completion": "        return self.connection.release_address(self.public_ip, self.allocation_id, dry_run)\n", "idx": 675}
{"namespace": "boto.ec2.address.Address.associate", "completion": "        if self.allocation_id:\n            return self.connection.associate_address(\n                instance_id=instance_id,\n                allocation_id=self.allocation_id,\n                network_interface_id=network_interface_id,\n                private_ip_address=private_ip_address,\n                allow_reassociation=allow_reassociation,\n                dry_run=dry_run\n            )\n        else:\n            return self.connection.associate_address(\n                instance_id=instance_id,\n                public_ip=self.public_ip,\n                network_interface_id=network_interface_id,\n                private_ip_address=private_ip_address,\n                allow_reassociation=allow_reassociation,\n                dry_run=dry_run\n            )\n", "idx": 676}
{"namespace": "boto.ec2.address.Address.disassociate", "completion": "        if self.association_id:\n            return self.connection.disassociate_address(\n                association_id=self.association_id,\n                dry_run=dry_run\n            )\n        return self.connection.disassociate_address(\n            public_ip=self.public_ip,\n            dry_run=dry_run\n        )\n", "idx": 677}
{"namespace": "boto.ec2.ec2object.TaggedEC2Object.add_tags", "completion": "        params = {'ResourceId.0': self.id}\n        for i, key in enumerate(tags):\n            params['Tag.%d.Key' % i] = key\n            params['Tag.%d.Value' % i] = tags[key]\n        if dry_run:\n            params['DryRun'] = 'true'\n        return self.connection.get_status('CreateTags', params, verb='POST')\n", "idx": 678}
{"namespace": "boto.ec2.ec2object.TaggedEC2Object.remove_tags", "completion": "        status = self.connection.delete_tags(\n            [self.id],\n            tags,\n            dry_run=dry_run\n        )\n        if self.tags is not None:\n            self.tags.remove(tags)\n", "idx": 679}
{"namespace": "boto.ec2.connection.EC2Connection.get_all_instance_status", "completion": "        params = {}\n        if instance_ids:\n            self.build_list_params(params, instance_ids, 'InstanceId')\n        if filters:\n            self.build_filter_params(params, filters)\n        if max_results:\n            params['MaxResults'] = max_results\n        if next_token:\n            params['NextToken'] = next_token\n        if include_all_instances:\n            params['IncludeAllInstances'] = 'true'\n        if dry_run:\n            params['DryRun'] = 'true'\n        return self.get_list('DescribeInstanceStatus', params,\n                             [('item', InstanceStatus)], verb='POST')\n", "idx": 680}
{"namespace": "boto.ec2.volume.Volume.update", "completion": "        if self.connection is not None:\n            return self.connection.get_all_volumes([self.id], dry_run=dry_run)[0]\n        else:\n            if validate:\n                raise ValueError('No connection.')\n            else:\n                return self\n", "idx": 681}
{"namespace": "boto.ec2.volume.Volume.attach", "completion": "        return self.connection.attach_volume(self.id, instance_id, device, dry_run=dry_run)\n", "idx": 682}
{"namespace": "boto.ec2.volume.Volume.detach", "completion": "        return self.connection.detach_volume(\n            self.id,\n            force,\n            dry_run=dry_run\n        )\n", "idx": 683}
{"namespace": "boto.ec2.volume.Volume.create_snapshot", "completion": "        return self.connection.create_snapshot(self.id, description, dry_run)\n", "idx": 684}
{"namespace": "boto.ec2.volume.Volume.attachment_state", "completion": "        return self.attach_data.status\n\n", "idx": 685}
{"namespace": "boto.ec2.securitygroup.SecurityGroup.add_rule", "completion": "        self.rules.add_rule(ip_protocol, from_port, to_port,\n                            src_group_name, src_group_owner_id, cidr_ip,\n                            src_group_group_id)\n", "idx": 686}
{"namespace": "boto.ec2.connect_to_region", "completion": "    for region in regions(**kw_params):\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None\n\n", "idx": 687}
{"namespace": "boto.ec2.cloudwatch.connect_to_region", "completion": "    if 'host' in kw_params:\n        host = kw_params.pop('host')\n    else:\n        if region_name in RegionData:\n            host = RegionData[region_name]\n        else:\n            host = 'monitoring.us-east-1.amazonaws.com'\n    return CloudWatchConnection(host=host, **kw_params)\n\n", "idx": 688}
{"namespace": "boto.ec2.autoscale.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None\n\n", "idx": 689}
{"namespace": "boto.ec2.elb.connect_to_region", "completion": "    if 'host' in kw_params:\n        host = kw_params['host']\n    else:\n        if region_name in RegionData:\n            host = RegionData[region_name]\n        else:\n            host = 'elasticloadbalancing.%s.amazonaws.com' % (region_name,)\n    return ELBConnection(**kw_params)\n\n", "idx": 690}
{"namespace": "boto.ec2.elb.ELBConnection.get_all_load_balancers", "completion": "        params = {}\n        if load_balancer_names:\n            self.build_list_params(params, load_balancer_names,\n                                   'LoadBalancerNames.member.%d')\n        if marker:\n            params['Marker'] = marker\n        return self.get_list('DescribeLoadBalancers', params,\n                             [('member', LoadBalancer)])\n", "idx": 691}
{"namespace": "boto.ec2.elb.ELBConnection.disable_availability_zones", "completion": "        params = {'LoadBalancerName': load_balancer_name}\n        self.build_list_params(params, zones_to_remove,\n                               'AvailabilityZones.member.%d')\n        obj = self.get_object('DisableAvailabilityZonesForLoadBalancer',\n                              params, LoadBalancerZones)\n        return obj.zones\n", "idx": 692}
{"namespace": "boto.awslambda.connect_to_region", "completion": "    from boto.awslambda.layer1 import AWSLambdaConnection\n    region = RegionInfo(name=region_name, endpoint='awslambda.us-east-1.amazonaws.com', connection_cls=AWSLambdaConnection)\n    return region.connect(**kw_params)", "idx": 693}
{"namespace": "boto.cognito.identity.connect_to_region", "completion": "    from boto.cognito.identity.layer1 import CognitoIdentityConnection\n    return CognitoIdentityConnection.connect(**kw_params)", "idx": 694}
{"namespace": "boto.cognito.sync.connect_to_region", "completion": "    from boto.cognito.sync.layer1 import CognitoSyncConnection\n    return CognitoSyncConnection(region=RegionInfo(None, region_name, **kw_params))", "idx": 695}
{"namespace": "boto.cloudformation.connect_to_region", "completion": "    if region_name in RegionData:\n        region = RegionInfo(\n            name=region_name,\n            endpoint=RegionData[region_name],\n            connection_cls=CloudFormationConnection\n        )\n    else:\n        return None\n    return region.connect(**kw_params)", "idx": 696}
{"namespace": "boto.route53.zone.Zone.find_records", "completion": "        name = self.route53connection._make_qualified(name)\n        records = self.get_records(name, type, identifier)\n        if len(records) == 0:\n            return None\n        if len(records) == desired or all:\n            return records\n        else:\n            raise TooManyRecordsException(desired, len(records))\n", "idx": 697}
{"namespace": "boto.route53.domains.connect_to_region", "completion": "    from boto.route53.domains.layer1 import Route53DomainsConnection\n    return Route53DomainsConnection(**kw_params)", "idx": 698}
{"namespace": "boto.s3.key.Key.get_contents_to_filename", "completion": "        fp = open(filename, 'wb')\n        self.get_contents_to_file(fp, headers, cb, num_cb, torrent,\n                                  version_id, res_download_handler,\n                                  response_headers)\n        fp.close()\n", "idx": 699}
{"namespace": "boto.s3.cors.CORSConfiguration.add_rule", "completion": "        rule = CORSRule(allowed_method, allowed_origin, id, allowed_header, max_age_seconds, expose_header)\n        self.append(rule)\n", "idx": 700}
{"namespace": "boto.s3.bucket.Bucket.get_key", "completion": "        return self.connection.get_key(self.name, key_name, headers, version_id,\n                                       response_headers, validate)\n", "idx": 701}
{"namespace": "boto.s3.bucket.Bucket.new_key", "completion": "        if not key_name:\n            key_name = ''\n        return self.key_class(self, key_name)\n", "idx": 702}
{"namespace": "boto.s3.bucket.Bucket.delete_key", "completion": "        if not key_name:\n            raise ValueError('Empty key names are not allowed')\n        return self._delete_key_internal(key_name, headers=headers,\n                                         version_id=version_id,\n                                         mfa_token=mfa_token)\n", "idx": 703}
{"namespace": "boto.s3.bucket.Bucket.get_tags", "completion": "        query_args = 'tagging'\n        response = self.connection.make_request('GET', self.name, headers=headers, query_args=query_args)\n        body = response.read()\n        if response.status == 200:\n            tags = Tags()\n            h = handler.XmlHandler(tags, self)\n            if not isinstance(body, bytes):\n                body = body.encode('utf-8')\n            xml.sax.parseString(body, h)\n            return tags\n        else:\n            raise self.connection.provider.storage_response_error(response.status, response.reason, body)\n", "idx": 704}
{"namespace": "boto.s3.connection.S3Connection._required_auth_capability", "completion": "        if self.anon:\n            return ['anon']\n        else:\n            return ['s3']\n", "idx": 705}
{"namespace": "boto.s3.connection.S3Connection.generate_url_sigv4", "completion": "        if headers is None:\n            headers = {}\n        if response_headers is None:\n            response_headers = {}\n\n        if iso_date is None:\n            iso_date = time.strftime(\"%Y%m%dT%H%M%SZ\", time.gmtime(time.time() + expires_in))\n\n        # Get the bucket name and key from the url path if they aren't specified.\n        if not bucket and not key:\n            bucket, key = self.get_bucket_and_name_from_url(key)\n\n        # Get the bucket location to use when building the query string.\n        if bucket:\n            bucket_location = self.get_bucket_location(bucket)\n        else:\n            bucket_location = ''\n\n        # Get the query string to use for generating the signature.\n        query_string = self.build_query_string(method, bucket, key, headers,\n                                               expires_in, response_headers,\n                                               version_id, bucket_location)\n\n        # Generate the canonical request to use when generating the string to sign.\n        canonical_request = self.build_canonical_request(method, bucket, key, query_string, headers,\n                                                         iso_date, bucket_location)\n\n        # Generate the string to sign.\n        string_to_sign = self.build_string_to_sign(iso_date, canonical_request)\n\n        # Generate the signature.\n        signature = self._auth_handler.sign_string(string_to_sign)\n\n        # Generate the final presigned URL.\n        return self.build_presigned_url(method, bucket, key, query_string, headers,\n                                        expires_in, response_headers, version_id,\n                                        signature, iso_date, bucket_location,\n                                        force_http)\n", "idx": 706}
{"namespace": "boto.s3.lifecycle.Lifecycle.add_rule", "completion": "        rule = Rule(id, prefix, status, expiration, transition)\n        self.append(rule)\n", "idx": 707}
{"namespace": "boto.s3.website.WebsiteConfiguration.to_xml", "completion": "        parts = ['<WebsiteConfiguration>']\n        if self.suffix is not None:\n            parts.append(tag('IndexDocument', tag('Suffix', self.suffix)))\n        if self.error_key is not None:\n            parts.append(tag('ErrorDocument',\n                             tag('Key', self.error_key)))\n        if self.redirect_all_requests_to is not None:\n            parts.append(self.redirect_all_requests_to.to_xml())\n        if self.routing_rules is not None:\n            parts.append(self.routing_rules.to_xml())\n        parts.append('</WebsiteConfiguration>')\n        return ''.join(parts)\n\n", "idx": 708}
{"namespace": "boto.s3.website.RoutingRules.to_xml", "completion": "        parts = ['<RoutingRules>']\n        for rule in self:\n            parts.append(rule.to_xml())\n        parts.append('</RoutingRules>')\n        return ''.join(parts)\n\n", "idx": 709}
{"namespace": "boto.s3.website.RoutingRule.when", "completion": "        condition = Condition(key_prefix=key_prefix, http_error_code=http_error_code)\n        return cls(condition=condition)\n", "idx": 710}
{"namespace": "boto.s3.website.RoutingRule.then_redirect", "completion": "        self.redirect = Redirect(hostname=hostname, protocol=protocol, replace_key=replace_key,\n                                 replace_key_prefix=replace_key_prefix, http_redirect_code=http_redirect_code)\n        return self\n", "idx": 711}
{"namespace": "boto.s3.connect_to_region", "completion": "    if 'host' in kw_params:\n        host = kw_params.pop('host')\n        if 'port' in kw_params:\n            port = kw_params.pop('port')\n        else:\n            port = None\n        if 'is_secure' in kw_params:\n            is_secure = kw_params.pop('is_secure')\n        else:\n            is_secure = None\n        region = S3RegionInfo(\n            name='custom',\n            endpoint=host,\n            connection_cls=S3Connection\n        )\n        if is_secure is not None:\n            region.is_secure = is_secure\n        else:\n            region.is_secure = (port == 443)\n    else:\n        for region in regions():\n            if region.name == region_name:\n                break\n        else:\n            raise S3ResponseError(\"Region %s does not exist\" % region_name)\n    return region.connect(**kw_params)\n\n", "idx": 712}
{"namespace": "boto.directconnect.connect_to_region", "completion": "    from boto.directconnect.layer1 import DirectConnectConnection\n    return DirectConnectConnection(region=RegionInfo(name=region_name,\n                                                     endpoint='directconnect.us-east-1.amazonaws.com'),\n                                   **kw_params)", "idx": 713}
{"namespace": "boto.rds.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None\n\n", "idx": 714}
{"namespace": "boto.datapipeline.connect_to_region", "completion": "    from boto.datapipeline import layer1\n    from boto.regioninfo import get_region_info\n    if 'host' not in kw_params:\n        region = get_region_info(name=region_name)\n        kw_params['host'] = region.endpoint\n    return layer1.DataPipelineConnection(**kw_params)", "idx": 715}
{"namespace": "boto.dynamodb.batch.Batch.to_dict", "completion": "        return {'Keys': [self.table.layer1.build_key_from_values(key) for key in self.keys],\n                'AttributesToGet': self.attributes_to_get,\n                'ConsistentRead': self.consistent_read}\n\n", "idx": 716}
{"namespace": "boto.dynamodb.batch.BatchList.to_dict", "completion": "        op_list = []\n        for batch in self:\n            op_list.append(batch.to_dict())\n        return op_list\n\n", "idx": 717}
{"namespace": "boto.dynamodb.types.Dynamizer.encode", "completion": "        dynamodb_type = self._get_dynamodb_type(attr)\n        dynamodb_encoder = getattr(self, '_encode_%s' % dynamodb_type)\n        return dynamodb_encoder(attr)\n", "idx": 718}
{"namespace": "boto.dynamodb.types.Dynamizer.decode", "completion": "        if len(attr) == 1:\n            dynamodb_type = list(attr.keys())[0]\n            try:\n                decoder = getattr(self, '_decode_%s' % dynamodb_type.lower())\n            except AttributeError:\n                raise ValueError(\"Unable to decode dynamodb type: %s\" %\n                                 dynamodb_type)\n            return decoder(attr[dynamodb_type])\n        else:\n            return attr\n", "idx": 719}
{"namespace": "boto.dynamodb.connect_to_region", "completion": "    from boto.dynamodb.layer2 import Layer2\n    return Layer2(region=RegionInfo(None, region_name, **kw_params))", "idx": 720}
{"namespace": "boto.beanstalk.connect_to_region", "completion": "    from boto.beanstalk.layer1 import Layer1\n    return Layer1(region=RegionInfo(None, region_name, **kw_params))", "idx": 721}
{"namespace": "boto.swf.connect_to_region", "completion": "    if 'host' not in kw_params:\n        kw_params['host'] = REGION_ENDPOINTS[region_name]\n    return boto.swf.layer1.Layer1Connection(**kw_params)", "idx": 722}
{"namespace": "boto.opsworks.regions", "completion": "    regions = [RegionInfo(name='us-east-1',\n                          endpoint='opsworks.us-east-1.amazonaws.com',\n                          connection_cls=OpsWorksConnection),\n               RegionInfo(name='us-west-1',\n                          endpoint='opsworks.us-west-1.amazonaws.com',\n                          connection_cls=OpsWorksConnection),\n               RegionInfo(name='us-west-2',\n                          endpoint='opsworks.us-west-2.amazonaws.com',\n                          connection_cls=OpsWorksConnection),\n               RegionInfo(name='eu-west-1',\n                          endpoint='opsworks.eu-west-1.amazonaws.com',\n                          connection_cls=OpsWorksConnection),\n               RegionInfo(name='eu-central-1',\n                          endpoint='opsworks.eu-central-1.amazonaws.com',\n                          connection_cls=OpsWorksConnection),\n               RegionInfo(name='ap-northeast-1',\n                          endpoint='opsworks.ap-northeast-1.amazonaws.com',\n                          connection_cls=OpsWorksConnection),\n               RegionInfo(name='ap-southeast-1',\n                          endpoint='opsworks.ap-southeast-1.amazonaws.com',\n                          connection_cls=OpsWorksConnection),\n               RegionInfo(name='ap-southeast-2',\n                          endpoint='opsworks.ap-southeast-2.amazonaws.com',\n                          connection_cls=OpsWorksConnection),\n               RegionInfo(name='sa-east-1',\n                          endpoint='opsworks.sa-east-1.amazonaws.com',\n                          connection_cls=OpsWorksConnection),\n               ]\n\n    return regions\n\n", "idx": 723}
{"namespace": "boto.opsworks.connect_to_region", "completion": "    from boto.opsworks.layer1 import OpsWorksConnection\n    return OpsWorksConnection(region=region_name, **kw_params)", "idx": 724}
{"namespace": "boto.sqs.connect_to_region", "completion": "    from boto.sqs.connection import SQSConnection\n    return SQSConnection(region=region_name, **kw_params)", "idx": 725}
{"namespace": "boto.rds2.connect_to_region", "completion": "    from boto.rds2.layer1 import RDSConnection\n    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 726}
{"namespace": "boto.cloudsearch2.connect_to_region", "completion": "    from boto.cloudsearch2.layer1 import CloudSearchConnection\n    return CloudSearchConnection(region=region_name, **kw_params)", "idx": 727}
{"namespace": "boto.cloudtrail.connect_to_region", "completion": "    from boto.cloudtrail.layer1 import CloudTrailConnection\n    return CloudTrailConnection(region=region_name, **kw_params)", "idx": 728}
{"namespace": "boto.elasticache.connect_to_region", "completion": "    from boto.elasticache.layer1 import ElastiCacheConnection\n    region = RegionInfo(name=region_name, endpoint='elasticache.us-east-1.amazonaws.com', connection_cls=ElastiCacheConnection)\n    return region.connect(**kw_params)", "idx": 729}
{"namespace": "boto.ses.connect_to_region", "completion": "    if region_name not in regions():\n        return None\n    return SESConnection(**kw_params)", "idx": 730}
{"namespace": "boto.codedeploy.connect_to_region", "completion": "    from boto.codedeploy.layer1 import CodeDeployConnection\n    return CodeDeployConnection(region=RegionInfo(None, region_name, **kw_params))", "idx": 731}
{"namespace": "boto.sts.credentials.Credentials.to_dict", "completion": "        return {'access_key': self.access_key,\n                'secret_key': self.secret_key,\n                'session_token': self.session_token,\n                'expiration': self.expiration,\n                'request_id': self.request_id}\n", "idx": 732}
{"namespace": "boto.sts.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None\n\n", "idx": 733}
{"namespace": "boto.machinelearning.connect_to_region", "completion": "    from boto.machinelearning.layer1 import MachineLearningConnection\n    return MachineLearningConnection(region=RegionInfo(None, region_name, **kw_params))", "idx": 734}
{"namespace": "boto.vpc.connect_to_region", "completion": "    for region in regions(**kw_params):\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None\n\n", "idx": 735}
{"namespace": "boto.vpc.VPCConnection.get_all_vpc_peering_connections", "completion": "        params = {}\n        if vpc_peering_connection_ids:\n            self.build_list_params(params, vpc_peering_connection_ids, 'VpcPeeringConnectionId')\n        if filters:\n            self.build_filter_params(params, filters)\n        if dry_run:\n            params['DryRun'] = 'true'\n        return self.get_list('DescribeVpcPeeringConnections', params, [('item', VpcPeeringConnection)])\n", "idx": 736}
{"namespace": "boto.kinesis.connect_to_region", "completion": "    from boto.kinesis.layer1 import KinesisConnection\n    return KinesisConnection(region=RegionInfo(None, region_name, **kw_params))", "idx": 737}
{"namespace": "boto.ec2containerservice.connect_to_region", "completion": "    from boto.ec2containerservice.layer1 import EC2ContainerServiceConnection\n    return EC2ContainerServiceConnection(region=region_name, **kw_params)", "idx": 738}
{"namespace": "boto.dynamodb2.table.Table._introspect_indexes", "completion": "        return self._introspect_all_indexes(raw_indexes, {\n            'ALL': AllIndex,\n            'KEYS_ONLY': KeysOnlyIndex,\n            'INCLUDE': IncludeIndex,\n        })\n", "idx": 739}
{"namespace": "boto.dynamodb2.table.Table.describe", "completion": "        raw_data = self.connection.describe_table(self.table_name)\n        raw_schema = raw_data['Table']['KeySchema']\n        raw_attributes = raw_data['Table'].get('AttributeDefinitions', None)\n        raw_throughput = raw_data['Table'].get('ProvisionedThroughput', None)\n        raw_indexes = raw_data['Table'].get('LocalSecondaryIndexes', None)\n        raw_global_indexes = raw_data['Table'].get('GlobalSecondaryIndexes', None)\n\n        self.schema = self._introspect_schema(raw_schema, raw_attributes)\n        self.throughput = {\n            'read': raw_throughput['ReadCapacityUnits'],\n            'write': raw_throughput['WriteCapacityUnits'],\n        }\n        self.indexes = self._introspect_indexes(raw_indexes)\n        self.global_indexes = self._introspect_global_indexes(raw_global_indexes)\n\n        return raw_data\n", "idx": 740}
{"namespace": "boto.dynamodb2.table.Table.update", "completion": "        if throughput is None and global_indexes is None:\n            raise exceptions.ThroughputError(\"You must specify either throughput or global indexes to update.\")\n        if throughput is not None and global_indexes is not None:\n            raise exceptions.ThroughputError(\"You cannot specify both throughput and global indexes to update.\")\n        if throughput is not None:\n            if 'read' not in throughput or 'write' not in throughput:\n                raise exceptions.ThroughputError(\"You must specify both read and write capacity units.\")\n            if not isinstance(throughput['read'], int) or not isinstance(throughput['write'], int):\n                raise exceptions.ThroughputError(\"Throughput values must be integers.\")\n            if throughput['read'] < 1 or throughput['read'] > 10000 or throughput['write'] < 1 or throughput['write'] > 10000:\n                raise exceptions.ThroughputError(\"Throughput values must be between 1 and 10000.\")\n            self.connection.update_table(self.table_name, throughput['read'], throughput['write'])\n            return True\n        if global_indexes is not None:\n            for index_name in global_indexes:\n                if 'read' not in global_indexes[index_name] or 'write' not in global_indexes[index_name]:\n                    raise exceptions.ThroughputError(\"You must specify both read and write capacity units for each global index.\")\n                if not isinstance(global_indexes[index_name]['read'], int) or not isinstance(global_indexes[index_name]['write'], int):\n                    raise exceptions.ThroughputError(\"Throughput values must be integers.\")\n                if global_indexes[index_name]['read'] < 1 or global_indexes[index_name]['read'] > 10000 or global_indexes[index_name]['write'] < 1 or global_indexes[index_name]['write'] > 10000:\n                    raise exceptions.Throughput", "idx": 741}
{"namespace": "boto.dynamodb2.table.Table.create_global_secondary_index", "completion": "        if not issubclass(global_index.__class__, GlobalBaseIndexField):\n            msg = 'The global_index parameter must be a subclass of GlobalBaseIndexField'\n            boto.log.error(msg)\n\n            return False\n\n        gsi_data = {\n            \"Create\": {\n                \"IndexName\": global_index.name,\n                \"KeySchema\": global_index.schema(),\n                \"Projection\": global_index.projection(),\n                \"ProvisionedThroughput\": {\n                    \"ReadCapacityUnits\": int(global_index.throughput['read']),\n                    \"WriteCapacityUnits\": int(global_index.throughput['write']),\n                },\n            },\n        }\n\n        self.connection.update_table(\n            self.table_name,\n            global_secondary_index_updates=gsi_data,\n        )\n\n        self.describe()\n\n        return True\n", "idx": 742}
{"namespace": "boto.dynamodb2.table.Table.delete_global_secondary_index", "completion": "        if global_index_name:\n            gsi_data = []\n\n            gsi_data.append({\n                \"Delete\": {\n                    \"IndexName\": global_index_name\n                }\n            })\n\n            self.connection.update_table(\n                self.table_name,\n                global_secondary_index_updates=gsi_data\n            )\n\n            return True\n        else:\n            msg = 'You need to provide the global index name to delete_global_secondary_index method'\n            boto.log.error(msg)\n\n            return False\n", "idx": 743}
{"namespace": "boto.dynamodb2.table.Table.update_global_secondary_index", "completion": "        if global_indexes:\n            gsi_data = []\n\n            for gsi_name, gsi_throughput in global_indexes.items():\n                gsi_data.append({\n                    \"Update\": {\n                        \"IndexName\": gsi_name,\n                        \"ProvisionedThroughput\": {\n                            \"ReadCapacityUnits\": int(gsi_throughput['read']),\n                            \"WriteCapacityUnits\": int(gsi_throughput['write']),\n                        },\n                    },\n                })\n\n            self.connection.update_table(\n                self.table_name,\n                global_secondary_index_updates=gsi_data,\n            )\n\n            return True\n        else:\n            msg = 'You need to provide the global indexes to update_global_secondary_index method'\n            boto.log.error(msg)\n\n            return False\n", "idx": 744}
{"namespace": "boto.dynamodb2.table.Table.delete", "completion": "        try:\n            self.connection.delete_table(self.table_name)\n            return True\n        except Exception as e:\n            boto.log.error(e.message)\n            return False\n", "idx": 745}
{"namespace": "boto.dynamodb2.table.Table.get_item", "completion": "        raw_key = self._encode_keys(kwargs)\n        raw_attributes = self._encode_attributes(attributes)\n        raw_response = self.connection.get_item(\n            self.table_name,\n            raw_key,\n            attributes_to_get=raw_attributes,\n            consistent_read=consistent\n        )\n        if 'Item' not in raw_response:\n            raise exceptions.ItemNotFound\n        return Item(self, raw_response['Item'])\n", "idx": 746}
{"namespace": "boto.dynamodb2.table.Table.has_item", "completion": "        try:\n            self.get_item(**kwargs)\n            return True\n        except exceptions.ItemNotFound:\n            return False\n", "idx": 747}
{"namespace": "boto.dynamodb2.table.Table._put_item", "completion": "        if not expects:\n            expects = {}\n\n        if not isinstance(item_data, dict):\n            raise TypeError(\"item_data must be a dict\")\n\n        if not isinstance(expects, dict):\n            raise TypeError(\"expects must be a dict\")\n\n        if expects:\n            for key, value in expects.items():\n                if not isinstance(value, dict):\n                    raise TypeError(\"expects values must be a dict\")\n\n        return self.connection.put_item(\n            self.table_name,\n            item_data,\n            expected=expects\n        )\n", "idx": 748}
{"namespace": "boto.dynamodb2.table.Table.delete_item", "completion": "        raw_key = self._encode_keys(kwargs)\n        return self.connection.delete_item(self.table_name, raw_key, expected=expected, conditional_operator=conditional_operator)\n", "idx": 749}
{"namespace": "boto.dynamodb2.table.Table.get_key_fields", "completion": "        if not self.schema:\n            self.describe()\n        key_fields = []\n        for field in self.schema:\n            if field.is_hash_key or field.is_range_key:\n                key_fields.append(field.name)\n        return key_fields\n", "idx": 750}
{"namespace": "boto.dynamodb2.table.Table._build_filters", "completion": "        filters = {}\n        for key, value in filter_kwargs.items():\n            try:\n                key, operator = key.split('__')\n            except ValueError:\n                raise exceptions.QueryError(\"Table.query() only accepts query operators for parameters. The key %s is not a valid query parameter.\" % key)\n            try:\n                operator = using[operator]\n            except KeyError:\n                raise exceptions.QueryError(\"Table.query() only accepts query operators for parameters. The query operator %s is not a valid query operator.\" % operator)\n            if key not in self.schema.keys():\n                raise exceptions.QueryError(\"Table.query() only accepts query parameters that are keys of the table. The key %s is not a key of the table.\" % key)\n            if isinstance(value, list):\n                filters[key] = {operator: value}\n            else:\n                filters[key] = {operator: [value]}\n        return filters\n", "idx": 751}
{"namespace": "boto.dynamodb2.table.Table.batch_get", "completion": "        results = ResultSet(\n            max_page_size=None\n        )\n        kwargs = {\n            'consistent_read': consistent,\n            'attributes_to_get': attributes,\n        }\n        results.to_call(self._batch_get, keys, **kwargs)\n        return results\n", "idx": 752}
{"namespace": "boto.dynamodb2.table.Table.count", "completion": "        return self.connection.describe_table(self.table_name)['Table']['ItemCount']\n", "idx": 753}
{"namespace": "boto.dynamodb2.table.BatchTable.put_item", "completion": "        self._to_put.append((data, overwrite))\n\n        if len(self._to_put) >= self.table.max_batch_size:\n            self.flush()\n", "idx": 754}
{"namespace": "boto.dynamodb2.table.BatchTable.delete_item", "completion": "        self._to_delete.append(kwargs)\n\n        if self.should_flush():\n            self.flush()\n", "idx": 755}
{"namespace": "boto.dynamodb2.table.BatchTable.flush", "completion": "        if not self._to_put and not self._to_delete:\n            return\n\n        data = {}\n\n        if self._to_put:\n            data['PutRequest'] = []\n\n            for item in self._to_put:\n                data['PutRequest'].append({\n                    'Item': self.table._encode_keys(item)\n                })\n\n        if self._to_delete:\n            data['DeleteRequest'] = []\n\n            for item in self._to_delete:\n                data['DeleteRequest'].append({\n                    'Key': self.table._encode_keys(item)\n                })\n\n        response = self.table.connection.batch_write_item(data)\n        self._unprocessed.extend(response.get('UnprocessedItems', []))\n        self._to_put = []\n        self._to_delete = []\n", "idx": 756}
{"namespace": "boto.dynamodb2.table.BatchTable.resend_unprocessed", "completion": "        while self._unprocessed:\n            # Pop the next 25 items.\n            items = self._unprocessed[:25]\n            self._unprocessed = self._unprocessed[25:]\n\n            # Build the batch data.\n            batch_data = {\n                self.table.table_name: items,\n            }\n\n            # Send the batch.\n            resp = self.table.connection.batch_write_item(batch_data)\n            self.handle_unprocessed(resp)\n\n", "idx": 757}
{"namespace": "boto.dynamodb2.fields.BaseSchemaField.definition", "completion": "        return {\n            'AttributeName': self.name,\n            'AttributeType': self.data_type\n        }\n", "idx": 758}
{"namespace": "boto.dynamodb2.fields.BaseIndexField.definition", "completion": "        return [\n            {\n                'AttributeName': part.name,\n                'AttributeType': part.data_type\n            } for part in self.parts\n        ]\n", "idx": 759}
{"namespace": "boto.dynamodb2.fields.BaseIndexField.schema", "completion": "        schema = []\n\n        for part in self.parts:\n            schema.append(part.schema())\n\n        return {\n            'IndexName': self.name,\n            'KeySchema': schema,\n            'Projection': {\n                'ProjectionType': self.projection_type,\n            }\n        }\n\n", "idx": 760}
{"namespace": "boto.dynamodb2.fields.GlobalBaseIndexField.schema", "completion": "        schema_data = super(GlobalBaseIndexField, self).schema()\n        schema_data['ProvisionedThroughput'] = {\n            'ReadCapacityUnits': self.throughput['read'],\n            'WriteCapacityUnits': self.throughput['write'],\n        }\n        return schema_data\n\n", "idx": 761}
{"namespace": "boto.dynamodb2.fields.GlobalIncludeIndex.schema", "completion": "        schema_data = super(GlobalIncludeIndex, self).schema()\n        schema_data['Projection']['NonKeyAttributes'] = self.includes_fields\n        return schema_data", "idx": 762}
{"namespace": "boto.dynamodb2.items.Item.get_keys", "completion": "        key_dict = {}\n        for key in self.table.schema:\n            if key.name in self:\n                key_dict[key.name] = self[key.name]\n        return key_dict\n", "idx": 763}
{"namespace": "boto.dynamodb2.items.Item.get_raw_keys", "completion": "        key_fields = self.table.get_key_fields()\n        key_data = {}\n\n        for key, value in key_fields.items():\n            key_data[key] = self._dynamizer.encode(value)\n\n        return key_data\n", "idx": 764}
{"namespace": "boto.dynamodb2.items.Item.build_expects", "completion": "        expects = {}\n\n        if fields is None:\n            fields = self._data.keys()\n\n        for field in fields:\n            if field in self._orig_data:\n                if self._data[field] == self._orig_data[field]:\n                    # No change.\n                    continue\n                elif self._data[field] is None:\n                    # Deletion.\n                    expects[field] = {'Exists': False}\n                else:\n                    # Update.\n                    expects[field] = {'Exists': True, 'Value': self._dynamizer.encode(self._data[field])}\n            else:\n                # New.\n                expects[field] = {'Exists': False}\n\n        return expects\n", "idx": 765}
{"namespace": "boto.dynamodb2.items.Item.prepare_full", "completion": "        data = {}\n        for key, value in self.items():\n            if self._is_storable(value):\n                data[key] = self._dynamizer.encode(value)\n        return data\n", "idx": 766}
{"namespace": "boto.dynamodb2.items.Item.prepare_partial", "completion": "        # This doesn't save on its own. Rather, we prepare the datastructure\n        # and hand-off to the table to handle creation/update.\n        final_data = {}\n        altered_fields = set()\n\n        for key, value in self._data.items():\n            if not self._is_storable(value):\n                continue\n\n            if key in self._orig_data:\n                if self._orig_data[key] != value:\n                    final_data[key] = {\n                        'Action': 'PUT',\n                        'Value': self._dynamizer.encode(value),\n                    }\n                    altered_fields.add(key)\n            else:\n                final_data[key] = {\n                    'Action': 'PUT',\n                    'Value': self._dynamizer.encode(value),\n                }\n                altered_fields.add(key)\n\n        for key in self._orig_data:\n            if not key in self._data:\n                final_data[key] = {\n                    'Action': 'DELETE',\n                }\n                altered_fields.add(key)\n\n        return final_data, altered_fields\n", "idx": 767}
{"namespace": "boto.dynamodb2.items.Item.partial_save", "completion": "        if not self.needs_save():\n            return False\n\n        final_data, fields = self.prepare_partial()\n\n        self.table.update_item(self.get_keys(), final_data, fields)\n\n        self.mark_clean()\n\n        return True\n", "idx": 768}
{"namespace": "boto.dynamodb2.items.Item.save", "completion": "        if not overwrite:\n            if not self.needs_save():\n                return False\n\n        key = self.get_keys()\n        final_data = self.prepare_full()\n\n        # Remove the key(s) from the ``final_data`` if present.\n        # They should only be present if this is a new item, in which\n        # case we shouldn't be sending as part of the data to update.\n        for fieldname, value in key.items():\n            if fieldname in final_data:\n                del final_data[fieldname]\n\n        # Build expectations of all the fields.\n        expects = self.build_expects()\n        returned = self.table._put_item(key, final_data, expects=expects)\n        # Mark the object as clean.\n        self.mark_clean()\n        return returned\n", "idx": 769}
{"namespace": "boto.dynamodb2.items.Item.delete", "completion": "        key = self.get_keys()\n        return self.table._delete_item(key)\n", "idx": 770}
{"namespace": "boto.dynamodb2.connect_to_region", "completion": "    from boto.dynamodb2.layer1 import DynamoDBConnection\n    region = RegionInfo(name=region_name, endpoint=\"dynamodb.us-east-1.amazonaws.com\", connection_cls=DynamoDBConnection)\n    return region.connect(**kw_params)", "idx": 771}
{"namespace": "litecli.packages.parseutils.extract_tables", "completion": "    parsed = sqlparse.parse(sql)\n    # parsed is a list. If multiple SQL statements are given, each one is parsed into a separate entry in the list.\n    # We are interested in the first entry in the list.\n    if not parsed:\n        return []\n    # The first entry of parsed is a Statement object.\n    stmt = parsed[0]\n    stream = extract_from_part(stmt)\n    # The result is a list of tuples. Each tuple is made up of the schema, table, and alias of a table.\n    return list(extract_table_identifiers(stream))\n\n", "idx": 772}
{"namespace": "litecli.packages.parseutils.queries_start_with", "completion": "    for query in queries:\n        if query_starts_with(query, prefixes):\n            return True\n    return False\n\n", "idx": 773}
{"namespace": "litecli.packages.parseutils.is_destructive", "completion": "    destructive_queries = [\n        \"truncate\",\n        \"delete\",\n        \"drop\",\n        \"alter\",\n        \"update\",\n        \"shutdown\",\n        \"reindex\",\n        \"checkpoint\",\n        \"create or replace\",\n    ]\n\n    return queries_start_with(queries, destructive_queries)\n\n", "idx": 774}
{"namespace": "litecli.packages.completion_engine.suggest_type", "completion": "    full_text = _strip_named_query(full_text)\n    text_before_cursor = _strip_named_query(text_before_cursor)\n\n    # If we've partially typed a word then we want to complete that\n    # word. In that case, a space is typed to separate the word\n    # and the cursor is placed between the word and the space. For\n    # example typing \"tab<space>\" will populate the completion list\n    # with tables and place the cursor between \"tab\" and the space.\n    if full_text != text_before_cursor and text_before_cursor[-1].isspace():\n        full_text = text_before_cursor + full_text[len(text_before_cursor):]\n        text_before_cursor = \"\"\n\n    # If we've partially typed a word then we want to complete that\n    # word. In that case, a space is typed to separate the word\n    # and the cursor is placed between the word and the space. For\n    # example typing \"tab<space>\" will populate the completion list\n    # with tables and place the cursor between \"tab\" and the space.\n    if full_text != text_before_cursor and full_text[0].isspace():\n        full_text = full_text[0:-1] + text_before_cursor\n        text_before_cursor = \"\"\n\n    # If we've partially typed a word then we want to complete that\n    # word. In that case, a space is typed to separate the word\n    # and the cursor is placed between the word and the space. For\n    # example typing \"tab<space>\" will populate the completion list\n    # with tables and place the cursor between \"tab\" and the space.\n    if full_text != text_before_cursor and full_text[-1].isspace():\n        full_text = full_text[:-1] + text_before_cursor\n        text_before_cursor = \"\"\n\n    # If we've partially typed a word then we want to complete that\n    # word. In that case, a space is typed to separate the word\n    # and the cursor is placed between the word and the space. For\n   ", "idx": 775}
{"namespace": "datasette.plugins.get_plugins", "completion": "    plugins = []\n    for plugin_name in pm.get_plugins():\n        plugin = pm.get_plugin(plugin_name)\n        plugin_info = {\n            \"name\": plugin_name,\n            \"static_path\": plugin.get_static_path if hasattr(plugin, \"get_static_path\") else None,\n            \"templates_path\": plugin.get_templates_path if hasattr(plugin, \"get_templates_path\") else None,\n            \"hooks\": sorted([hook for hook in pm.get_hook_callers(plugin)]),\n        }\n        for attr in (\"version\", \"__version__\", \"VERSION\"):\n            if hasattr(plugin, attr):\n                plugin_info[\"version\"] = getattr(plugin, attr)\n        for attr in (\"project_name\", \"__project_name__\", \"PROJECT_NAME\"):\n            if hasattr(plugin, attr):\n                plugin_info[\"project_name\"] = getattr(plugin, attr)\n        plugins.append(plugin_info)\n    return plugins", "idx": 776}
{"namespace": "datasette.facets.ColumnFacet.suggest", "completion": "        row_count = await self.get_row_count()\n        columns = await self.get_columns(self.sql, self.params)\n        facet_size = self.get_facet_size()\n        facet_results = []\n        for column in columns:\n            # Skip columns with spaces - they cannot be used in a pivot query\n            if \" \" in column:\n                continue\n            # Skip columns with prefixes - they cannot be used in a pivot query\n            if any(column.startswith(prefix) for prefix in PIVOT_PREFIXES):\n                continue\n            # Skip columns with suffixes - they cannot be used in a pivot query\n            if any(column.endswith(suffix) for suffix in PIVOT_SUFFIXES):\n                continue\n            # Skip columns with the _ in them - they cannot be used in a pivot query\n            if \"_\" in column:\n                continue\n            # Skip columns with the . in them - they cannot be used in a pivot query\n            if \".\" in column:\n                continue\n            # Skip columns with the : in them - they cannot be used in a pivot query\n            if \":\" in column:\n                continue\n            # Skip columns with the / in them - they cannot be used in a pivot query\n            if \"/\" in column:\n                continue\n            # Skip columns with the - in them - they cannot be used in a pivot query\n            if \"-\" in column:\n                continue\n            # Skip columns with the ' in them - they cannot be used in a pivot query\n            if \"'\" in column:\n                continue\n            # Skip columns with the \" in them - they cannot be used in a pivot query\n            if '\"' in column:\n                continue\n            # Skip columns with the ( in them - they cannot be used in a pivot query\n            if \"(\" in column:\n                continue\n            # Skip columns with the ) in them - they cannot be used in a pivot query\n            if \")\" in column:\n                continue\n            # Skip columns with the , in them - they cannot be used in a pivot query\n            if \",\" in column:\n                continue\n            # Skip columns", "idx": 777}
{"namespace": "datasette.facets.ColumnFacet.facet_results", "completion": "        facet_results = []\n        facets_timed_out = []\n        facet_size = self.get_facet_size()\n        already_enabled = [c[\"config\"][\"simple\"] for c in self.get_configs()]\n        for config in self.get_configs():\n            facet_sql = \"\"\"\n                select {value} as value, count(*) as count from (\n                    {sql}\n                )\n                where {value} is not null\n                group by {value}\n                order by count desc, value asc\n            \"\"\".format(\n                value=escape_sqlite(config[\"config\"][\"simple\"]), sql=self.sql\n            )\n            try:\n                facet_rows_results = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n                facet_results_values = []\n                facet_results_timed_out = False\n                for row in facet_rows_results:\n                    if len(facet_results_values) < facet_size:\n                        facet_results_values.append(row)\n                    else:\n                        facet_results_timed_out = True\n                facet_results.append(\n                    {\n                        \"name\": config[\"config\"][\"simple\"],\n                        \"type\": self.type,\n                        \"hideable\": config[\"source\"] == \"metadata\",\n                        \"toggle_url\": path_with_removed_args(\n                            self.request, {\"_facet\": config[\"config\"][\"simple\"]}\n                        ),\n                        \"results\": [\n                            {\n                                \"value\": r[\"value\"],\n                                \"label\": r[\"value\"],\n                                \"count\": r[\"count\"],\n                                \"toggle_url\": self.ds.absolute_url(\n                                    self.request,\n                                    path_with_args(\n                                        self.request,", "idx": 778}
{"namespace": "datasette.facets.ArrayFacet.suggest", "completion": "        suggested_facets = []\n        already_enabled = [c[\"config\"][\"simple\"] for c in self.get_configs()]\n        columns = await self.get_columns(self.sql, self.params)\n        for column in columns:\n            if column in already_enabled:\n                continue\n            facet_sql = \"\"\"\n                select distinct {col} from (\n                    {sql}\n                ) where {col} is not null\n            \"\"\".format(\n                col=escape_sqlite(column), sql=self.sql\n            )\n            try:\n                facet_rows_results = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_suggest_time_limit_ms\"),\n                )\n                facet_rows = facet_rows_results.rows\n                for row in facet_rows:\n                    if self._is_json_array_of_strings(row[\"value\"]):\n                        suggested_facets.append(\n                            {\n                                \"name\": column,\n                                \"type\": self.type,\n                                \"toggle_url\": self.ds.absolute_url(\n                                    self.request,\n                                    self.ds.urls.path(\n                                        path_with_added_args(\n                                            self.request, {\"_facet\": column}\n                                        )\n                                    ),\n                                ),\n                            }\n                        )\n            except QueryInterrupted:\n                continue\n        return suggested_facets\n", "idx": 779}
{"namespace": "datasette.facets.ArrayFacet.facet_results", "completion": "        facet_results = []\n        facets_timed_out = []\n\n        qs_pairs = self.get_querystring_pairs()\n\n        facet_size = self.get_facet_size()\n        for source_and_config in self.get_configs():\n            config = source_and_config[\"config\"]\n            source = source_and_config[\"source\"]\n            column = config.get(\"column\") or config[\"simple\"]\n            facet_sql = \"\"\"\n                select j.value as value, count(*) as count from (\n                    {sql}\n                ) join json_each({col}) j\n                group by j.value order by count desc limit {limit}\n            \"\"\".format(\n                col=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            try:\n                facet_rows_results = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n                facet_results_values = []\n                facet_results.append(\n                    {\n                        \"name\": column,\n                        \"type\": self.type,\n                        \"results\": facet_results_values,\n                        \"hideable\": source != \"metadata\",\n                        \"toggle_url\": self.ds.absolute_url(\n                            self.request,\n                            self.ds.urls.path(\n                                path_with_removed_args(\n                                    self.request, {\"_facet_array\": column}\n                                )\n                            ),\n                        ),\n                        \"truncated\": len(facet_rows_results) > facet_size,\n                    }\n                )\n                facet_rows = facet_rows_results.rows[:facet_size]\n                for row in facet_rows:\n                    column_qs = \"{}_array\".format(column)\n                    if row[\"", "idx": 780}
{"namespace": "datasette.facets.DateFacet.facet_results", "completion": "        facet_results = []\n        facets_timed_out = []\n\n        facet_size = self.get_facet_size()\n        for source_and_config in self.get_configs():\n            config = source_and_config[\"config\"]\n            source = source_and_config[\"source\"]\n            column = config.get(\"column\") or config[\"simple\"]\n            # TODO: make this work for date-part columns\n            facet_sql = \"\"\"\n                select date({col}) as value, count(*) as count from (\n                    {sql}\n                )\n                where date({col}) is not null\n                group by date({col}) order by 1 limit {limit}\n            \"\"\".format(\n                col=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            try:\n                facet_rows_results = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n                facet_results_values = []\n                facet_results.append(\n                    {\n                        \"name\": column,\n                        \"type\": self.type,\n                        \"results\": facet_results_values,\n                        \"hideable\": source != \"metadata\",\n                        \"toggle_url\": self.ds.absolute_url(\n                            self.request,\n                            self.ds.urls.path(\n                                path_with_added_args(\n                                    self.request, {\"_facet_date\": column}\n                                )\n                            ),\n                        ),\n                        \"truncated\": len(facet_rows_results) > facet_size,\n                    }\n                )\n                facet_rows = facet_rows_results.rows[:facet_size]\n                pairs = self.get_querystring_pairs()\n                for row in facet_rows:\n                    value = str(row[\"value", "idx": 781}
{"namespace": "datasette.app.Datasette.invoke_startup", "completion": "        if self._startup_invoked:\n            return\n        self._startup_invoked = True\n        # Run startup functions\n        for startup_function in pm.hook.startup(datasette=self):\n            await await_me_maybe(startup_function)\n", "idx": 782}
{"namespace": "datasette.app.Datasette.get_database", "completion": "        if route:\n            return self.databases[route]\n        if name is None:\n            for dbname, db in self.databases.items():\n                if dbname != \"_internal\":\n                    return db\n        return self.databases[name]\n", "idx": 783}
{"namespace": "datasette.app.Datasette.add_database", "completion": "        # Copy the existing databases\n        databases = dict(self.databases)\n\n        # Assign a name to the new database if no name is provided\n        if name is None:\n            name = \"database\"\n\n        # Check if the name already exists\n        if name in databases:\n            # If the name already exists, append a number to make it unique\n            counter = 1\n            while name in databases:\n                name = f\"{name}{counter}\"\n                counter += 1\n\n        # Assign the name and route to the new database\n        db.name = name\n        db.route = route or name\n\n        # Add the new database to the copied databases dictionary\n        databases[name] = db\n\n        # Assign the copied dictionary back to the instance\n        self.databases = databases\n\n        # Return the added database\n        return db\n", "idx": 784}
{"namespace": "datasette.app.Datasette.ensure_permissions", "completion": "        for permission in permissions:\n            if isinstance(permission, str):\n                action = permission\n                resource = None\n            else:\n                action, resource = permission\n            if not await self.permission_allowed(\n                actor=actor, action=action, resource=resource\n            ):\n                raise Forbidden(f\"{actor} does not have {action} permission\")\n", "idx": 785}
{"namespace": "datasette.app.Datasette.check_visibility", "completion": "        assert actor is None or isinstance(actor, dict), \"actor must be None or a dict\"\n        assert (\n            action is None or isinstance(action, str)\n        ), \"action must be None or a string\"\n        assert (\n            resource is None or isinstance(resource, (str, tuple))\n        ), \"resource must be None or a string or a tuple\"\n        assert (\n            permissions is None or isinstance(permissions, (list, tuple))\n        ), \"permissions must be None or a list or a tuple\"\n\n        if permissions is None:\n            permissions = [\n                (\"view-table\", resource),\n                (\"view-database\", resource[0]),\n                (\"view-instance\", None),\n            ]\n\n        visible = False\n        private = False\n        for permission in permissions:\n            if isinstance(permission, str):\n                action = permission\n                resource = None\n            elif isinstance(permission, (tuple, list)) and len(permission) == 2:\n                action, resource = permission\n            else:\n                assert (\n                    False\n                ), \"permission should be string or tuple of two items: {}\".format(\n                    repr(permission)\n                )\n            ok = await self.permission_allowed(\n                actor,\n                action,\n                resource=resource,\n                default=None,\n            )\n            if ok is not None:\n                if ok:\n                    visible = True\n                else:\n                    private = True\n                    break\n        return visible, private\n", "idx": 786}
{"namespace": "datasette.app.Datasette.render_template", "completion": "        # Check if the startup function has been invoked\n        if not self._started:\n            raise StartupError(\"You must call datasette.client.startup() before rendering\")\n\n        # Prepare the context\n        context = context or {}\n        context[\"base_url\"] = self.setting(\"base_url\")\n        context[\"csrftoken\"] = request.scope[\"csrftoken\"] if request else \"\"\n        context[\"datasette_version\"] = __version__\n        context[\"default_page_size\"] = self.setting(\"default_page_size\")\n        context[\"hash_urls\"] = self.setting(\"hash_urls\")\n        context[\"num_sql_threads\"] = self.setting(\"num_sql_threads\")\n        context[\"sql_time_limit_ms\"] = self.setting(\"sql_time_limit_ms\")\n        context[\"sql_time_guess_tracebacks\"] = self.setting(\"sql_time_guess_tracebacks\")\n        context[\"default_facet_size\"] = self.setting(\"default_facet_size\")\n        context[\"facet_time_limit_ms\"] = self.setting(\"facet_time_limit_ms\")\n        context[\"facet_suggest_time_limit_ms\"] = self.setting(\n            \"facet_suggest_time_limit_ms\"\n        )\n        context[\"max_returned_rows\"] = self.setting(\"max_returned_rows\")\n        context[\"sql_functions_enabled\"] = self.setting(\"sql_functions_enabled\")\n        context[\"allow_download\"] = self.setting(\"allow_download\")\n        context[\"allow_facet\"] = self.setting(\"allow_facet\")\n        context[\"default_cache_ttl\"] = self.setting(\"default_cache_ttl\")\n        context[\"default_cache_ttl_hashed\"] = self.setting(\"default_cache_ttl_hashed\")\n        context[\"max_cache_size\"] = self.setting(\"max_cache_size\")\n        context[\"cache_headers\"] = self.setting(\"cache_headers\")\n        context[\"allow_csv_stream\"] = self.setting(\"", "idx": 787}
{"namespace": "datasette.app.DatasetteClient.get", "completion": "        return await self.request(\"get\", self._fix(path), **kwargs)\n", "idx": 788}
{"namespace": "datasette.utils.asgi.Request.full_path", "completion": "        if self.query_string:\n            return \"{}?{}\".format(self.path, self.query_string)\n        return self.path\n", "idx": 789}
{"namespace": "datasette.utils.asgi.Request.post_body", "completion": "        body = b''\n        more_body = True\n\n        while more_body:\n            message = await self.receive()\n            body += message.get('body', b'')\n            more_body = message.get('more_body', False)\n\n        return body\n\n", "idx": 790}
{"namespace": "datasette.utils.asgi.Request.fake", "completion": "        url = urlparse(path_with_query_string)\n        path = url.path\n        query_string = url.query\n        headers = {}\n        scope = {\n            \"http_version\": \"1.1\",\n            \"method\": method,\n            \"path\": path,\n            \"raw_path\": path.encode(\"latin-1\"),\n            \"query_string\": query_string.encode(\"latin-1\"),\n            \"scheme\": scheme,\n            \"type\": \"http\",\n            \"headers\": [(k.lower().encode(\"latin-1\"), v.encode(\"latin-1\")) for k, v in headers.items()],\n        }\n        return cls(scope, None)\n\n", "idx": 791}
{"namespace": "datasette.utils.asgi.Response.asgi_send", "completion": "        # Remove any existing content-type header\n        headers = {k: v for k, v in self.headers.items() if k.lower() != \"content-type\"}\n        headers[\"content-type\"] = self.content_type\n        for header in self._set_cookie_headers:\n            headers[\"set-cookie\"] = header\n        await send(\n            {\n                \"type\": \"http.response.start\",\n                \"status\": self.status,\n                \"headers\": [\n                    [key.encode(\"latin1\"), value.encode(\"latin1\")]\n                    for key, value in headers.items()\n                ],\n            }\n        )\n        if isinstance(self.body, str):\n            body = self.body.encode(\"utf-8\")\n        else:\n            body = self.body\n        await send({\"type\": \"http.response.body\", \"body\": body})\n", "idx": 792}
{"namespace": "datasette.utils.asgi.Response.set_cookie", "completion": "        if samesite not in SAMESITE_VALUES:\n            raise ValueError(\n                \"samesite should be one of %s\" % repr(SAMESITE_VALUES)\n            )\n\n        cookie = SimpleCookie()\n        cookie[key] = value\n        for prop_name, prop_value in [\n            (\"max_age\", max_age),\n            (\"expires\", expires),\n            (\"path\", path),\n            (\"domain\", domain),\n            (\"samesite\", samesite),\n        ]:\n            if prop_value is not None:\n                cookie[key][prop_name.replace(\"_\", \"-\")] = prop_value\n\n        for prop_name, prop_value in [(\"secure\", secure), (\"httponly\", httponly)]:\n            if prop_value:\n                cookie[key][prop_name] = True\n\n        self._set_cookie_headers.append(cookie.output(header=\"\").strip())\n\n", "idx": 793}
{"namespace": "datasette.utils.asgi.Response.html", "completion": "        return cls(\n            body=body,\n            status=status,\n            headers=headers,\n            content_type=\"text/html; charset=utf-8\",\n        )\n", "idx": 794}
{"namespace": "datasette.utils.asgi.Response.text", "completion": "        return cls(\n            body,\n            status=status,\n            headers=headers,\n            content_type=\"text/plain; charset=utf-8\",\n        )\n", "idx": 795}
{"namespace": "datasette.utils.asgi.Response.json", "completion": "        return cls(\n            json.dumps(body, default=default),\n            status=status,\n            headers=headers,\n            content_type=\"application/json; charset=utf-8\",\n        )\n", "idx": 796}
{"namespace": "datasette.utils.asgi.Response.redirect", "completion": "        headers = headers or {}\n        headers[\"Location\"] = path\n        return cls(\"\", status=status, headers=headers)\n\n", "idx": 797}
{"namespace": "datasette.utils.validate_sql_select", "completion": "    sql = re.sub(r\"--.*\\n\", \"\", sql)\n    sql = sql.lower()\n    if not any(allowed.match(sql) for allowed in allowed_sql_res):\n        raise InvalidSql(\"Invalid SQL - unsupported operation\")\n    for disallowed, error_message in disallawed_sql_res:\n        if disallowed.match(sql):\n            raise InvalidSql(error_message)\n\n", "idx": 798}
{"namespace": "datasette.utils.escape_css_string", "completion": "    return _css_re.sub(\n        lambda m: \"\\\\\" + \"000\"[m.group(0)], s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n    )\n\n", "idx": 799}
{"namespace": "datasette.utils.detect_fts", "completion": "    # Get the table information\n    table_info = table_column_details(conn, table)\n\n    # Iterate through the table columns\n    for column in table_info:\n\n        # Get the column name\n        column_name = column[\"name\"]\n\n        # If the column name ends with _fts, then it is an FTS column\n        if column_name.endswith(\"_fts\"):\n\n            # Get the FTS column name\n            fts_column_name = column_name\n\n            # Get the FTS column type\n            fts_column_type = column[\"type\"].split(\"(\")[0]\n\n            # Get the FTS table name\n            fts_table_name = fts_column_name + \"_data\"\n\n            # Return the FTS table name\n            return fts_table_name\n\n    # If no FTS column is found, return None\n    return None\n\n", "idx": 800}
{"namespace": "datasette.utils.is_url", "completion": "    if not isinstance(value, str):\n        return False\n    if not (value.startswith(\"http://\") or value.startswith(\"https://\")):\n        return False\n    if whitespace_re.search(value):\n        return False\n    return True\n\n", "idx": 801}
{"namespace": "datasette.utils.to_css_class", "completion": "    if css_class_re.match(s):\n        return s\n    s = css_invalid_chars_re.sub(\"\", s)\n    return f\"{s[:50]}-{hashlib.md5(s.encode('utf8')).hexdigest()[:6]}\"\n\n", "idx": 802}
{"namespace": "datasette.utils.escape_fts", "completion": "    if query is None:\n        return None\n    if query == \"\":\n        return \"\"\n    # Append double quote at end if missing\n    if query[-1] != '\"':\n        query += '\"'\n    # Split query into terms\n    terms = _escape_fts_re.split(query)\n    # Remove empty terms\n    terms = [term for term in terms if term != \"\" and term != \" \"]\n    # Remove duplicate terms\n    terms = list(OrderedDict.fromkeys(terms))\n    # Join terms with double quotes\n    query = '\"' + '\" \"'.join(terms) + '\"'\n    return query\n\n", "idx": 803}
{"namespace": "datasette.utils.check_connection", "completion": "    try:\n        cursor = conn.cursor()\n        cursor.execute(\"PRAGMA foreign_keys;\")\n        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n        tables = cursor.fetchall()\n        for table in tables:\n            cursor.execute(f\"PRAGMA table_info({table[0]});\")\n            cursor.fetchall()\n    except sqlite3.DatabaseError as e:\n        if e.args[0] == \"not an error\":\n            pass\n        elif e.args[0] == \"unable to open database file\":\n            raise ConnectionProblem(e)\n        elif e.args[0] == \"Error while preparing query\":\n            raise SpatialiteConnectionProblem(e)\n        else:\n            raise e\n\n", "idx": 804}
{"namespace": "datasette.utils.parse_metadata", "completion": "    if content.strip().startswith(\"{\"):\n        try:\n            return json.loads(content)\n        except json.decoder.JSONDecodeError:\n            raise BadMetadataError(\"Invalid JSON\")\n    else:\n        try:\n            return yaml.safe_load(content)\n        except yaml.scanner.ScannerError:\n            raise BadMetadataError(\"Invalid YAML\")\n\n", "idx": 805}
{"namespace": "datasette.utils.call_with_supported_arguments", "completion": "    call_with = _gather_arguments(fn, kwargs)\n    return fn(*call_with)\n\n", "idx": 806}
{"namespace": "datasette.utils.derive_named_parameters", "completion": "    # Strip any trailing semicolon from the input SQL query.\n    sql = sql.strip().rstrip(\";\")\n\n    # Find all possible named parameters in the query using a regular expression.\n    named_parameters = _re_named_parameter.findall(sql)\n\n    # Execute the \"explain\" statement on the database with a dictionary of named parameters, where the values are set to None.\n    try:\n        explain_results = await db.execute(f\"explain {sql}\", {key: None for key in named_parameters})\n    except Exception as e:\n        # If there is an error executing the \"explain\" statement, return the list of possible named parameters found in the input SQL query.\n        return named_parameters\n\n    # Return a list of named parameters identified as variables in the \"explain\" results, after removing the leading \":\" character.\n    return [parameter.lstrip(\":\") for parameter in named_parameters if parameter.lstrip(\":\") in explain_results.columns]\n\n", "idx": 807}
{"namespace": "pyramid.path.Resolver.get_package_name", "completion": "        if self.package is CALLER_PACKAGE:\n            return package_name(caller_package())\n        return package_name(self.package)\n", "idx": 808}
{"namespace": "pyramid.path.Resolver.get_package", "completion": "        if self.package is CALLER_PACKAGE:\n            package = caller_package()\n        else:\n            package = self.package\n        return package\n", "idx": 809}
{"namespace": "pyramid.path.DottedNameResolver.resolve", "completion": "        if ':' in dotted:\n            return pkg_resources.EntryPoint.parse('x=%s' % dotted).load(False)\n        if dotted.startswith('.'):\n            return self._resolve_relative(dotted)\n        return self._resolve_absolute(dotted)\n", "idx": 810}
{"namespace": "pyramid.path.DottedNameResolver.maybe_resolve", "completion": "        if isinstance(dotted, str):\n            return self.resolve(dotted)\n        else:\n            return dotted\n\n", "idx": 811}
{"namespace": "pyramid.path.PkgResourcesAssetDescriptor.abspath", "completion": "        return self.pkg_resources.resource_filename(self.pkg_name, self.path)\n", "idx": 812}
{"namespace": "pyramid.renderers.render_to_response", "completion": "    try:\n        registry = request.registry\n    except AttributeError:\n        registry = None\n    if package is None:\n        package = caller_package()\n    helper = RendererHelper(\n        name=renderer_name, package=package, registry=registry\n    )\n\n    with hide_attrs(request, 'response'):\n        result = helper.render(value, None, request=request, response=response)\n\n    return result\n\n", "idx": 813}
{"namespace": "pyramid.renderers.JSON.add_adapter", "completion": "        self.components.registerAdapter(adapter, [type_or_iface], IJSONAdapter)\n", "idx": 814}
{"namespace": "pyramid.renderers.RendererHelper.settings", "completion": "        return self.registry.settings\n", "idx": 815}
{"namespace": "pyramid.renderers.RendererHelper.render_view", "completion": "        system = {\n            'view': view,\n            'renderer_name': self.name,\n            'renderer_info': self,\n            'context': context,\n            'request': request,\n            'csrf_token': get_csrf_token(request),\n        }\n\n        result = self.renderer(info=system)\n        if result is None:\n            return response\n\n        if isinstance(result, dict):\n            result = json.dumps(result)\n\n        if isinstance(result, str):\n            response.text = result\n        else:\n            response.app_iter = result\n\n        return response\n", "idx": 816}
{"namespace": "pyramid.renderers.RendererHelper.render", "completion": "        system = system_values.copy()\n        system['renderer_name'] = self.name  # b/c\n        system['renderer_info'] = self\n        system['request'] = request\n        system['req'] = request\n        system['get_csrf_token'] = partial(get_csrf_token, request)\n        registry = self.registry\n        registry.notify(BeforeRender(renderer_name=self.name,\n                                    renderer_info=self,\n                                    system=system))\n        result = self.renderer(value, system)\n        registry.notify(AfterRender(renderer_name=self.name,\n                                   renderer_info=self,\n                                   system=system,\n                                   response=result))\n        return result\n", "idx": 817}
{"namespace": "pyramid.renderers.RendererHelper.render_to_response", "completion": "        result = self.render(value, system_values, request=request)\n        response = request.response\n        if result is None:\n            response.content_type = None\n            response.content_length = None\n        else:\n            response.app_iter = [result]\n            if response.content_type is None:\n                response.content_type = self.content_type\n            if response.content_length is None:\n                response.content_length = len(result)\n        return response\n", "idx": 818}
{"namespace": "pyramid.renderers.RendererHelper.clone", "completion": "        if name is None:\n            name = self.name\n        if package is None:\n            package = self.package\n        if registry is None:\n            registry = self.registry\n        return RendererHelper(name=name, package=package, registry=registry)\n\n", "idx": 819}
{"namespace": "pyramid.urldispatch.RoutesMapper.get_routes", "completion": "        if include_static:\n            return self.routelist\n        return [r for r in self.routelist if not r.static]\n", "idx": 820}
{"namespace": "pyramid.urldispatch.RoutesMapper.connect", "completion": "        route = Route(name, pattern, factory, predicates, pregenerator)\n        self.routes[name] = route\n\n        if static:\n            self.static_routes.append(route)\n        else:\n            self.routelist.append(route)\n\n        return route\n", "idx": 821}
{"namespace": "pyramid.testing.DummyTemplateRenderer.assert_", "completion": "        for k, v in kw.items():\n            assert self._received.get(k) == v\n        return True\n\n", "idx": 822}
{"namespace": "pyramid.testing.DummyResource.__getitem__", "completion": "        return self.subs[name]\n", "idx": 823}
{"namespace": "pyramid.testing.DummyResource.clone", "completion": "        if __name__ is _marker:\n            __name__ = self.__name__\n        if __parent__ is _marker:\n            __parent__ = self.__parent__\n        kw.update(self.kw)\n        return self.__class__(__name__=__name__, __parent__=__parent__, **kw)\n\n", "idx": 824}
{"namespace": "pyramid.testing.DummySession.get_csrf_token", "completion": "        token = self.get('_csrft_', None)\n        if token is None:\n            token = self.new_csrf_token()\n        return token\n\n", "idx": 825}
{"namespace": "pyramid.testing.DummyRequest.response", "completion": "        return self.registry.queryAdapterOrSelf(self, IResponse)\n", "idx": 826}
{"namespace": "pyramid.testing.DummyRendererFactory.add", "completion": "        self.renderers[spec] = renderer\n", "idx": 827}
{"namespace": "pyramid.authorization.ACLAuthorizationPolicy.principals_allowed_by_permission", "completion": "        return self.helper.principals_allowed_by_permission(context, permission)\n", "idx": 828}
{"namespace": "pyramid.url.URLMethodsMixin.route_url", "completion": "        registry = get_current_registry()\n        mapper = registry.getUtility(IResourceURL)\n        return mapper.generate(route_name, *elements, **kw)\n", "idx": 829}
{"namespace": "pyramid.predicates.CustomPredicate.text", "completion": "        try:\n            return self.func.__text__()\n        except AttributeError:\n            return 'custom predicate: %s' % self.func\n", "idx": 830}
{"namespace": "pyramid.threadlocal.ThreadLocalManager.pop", "completion": "        if len(self.stack) > 0:\n            return self.stack.pop()\n        return self.default\n", "idx": 831}
{"namespace": "pyramid.threadlocal.ThreadLocalManager.get", "completion": "        if self.stack:\n            return self.stack[-1]\n        else:\n            return self.default()\n", "idx": 832}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.authenticated_userid", "completion": "        identity = self._get_identity(request)\n        if identity is None:\n            return None\n        userid = identity['repoze.who.userid']\n        if userid is None:\n            return None\n        if self._clean_principal(userid) is None:\n            return None\n        if self.callback is None:\n            return userid\n        if self.callback(userid, request) is not None:\n            return userid\n", "idx": 833}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.unauthenticated_userid", "completion": "        identity = self._get_identity(request)\n        if identity is None:\n            return None\n        return identity['repoze.who.userid']\n", "idx": 834}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.forget", "completion": "        identifier = self._get_identifier(request)\n        if identifier is None:\n            return []\n        environ = request.environ\n        return identifier.forget(environ)\n\n", "idx": 835}
{"namespace": "pyramid.authentication.AuthTktAuthenticationPolicy.unauthenticated_userid", "completion": "        \"\"\"\n        This function retrieves the user ID from the auth_tkt cookie.\n        Input-Output Arguments\n        :param self: AuthTktAuthenticationPolicy. An instance of the AuthTktAuthenticationPolicy class.\n        :param request: The request object.\n        :return: The user ID extracted from the auth_tkt cookie.\n        \"\"\"\n        # Get the auth_tkt cookie from the request\n        userid = self.cookie.identify(request)\n        # If the userid is None, then the user is not authenticated\n        if userid is None:\n            return None\n        # If the userid is not None, then the user is authenticated\n        # Return the userid\n        return userid\n", "idx": 836}
{"namespace": "pyramid.authentication.SessionAuthenticationHelper.remember", "completion": "        request.session[self.userid_key] = userid\n        return []\n", "idx": 837}
{"namespace": "pyramid.authentication.SessionAuthenticationHelper.forget", "completion": "        if self.userid_key in request.session:\n            del request.session[self.userid_key]\n        return []\n", "idx": 838}
{"namespace": "pyramid.authentication.BasicAuthAuthenticationPolicy.unauthenticated_userid", "completion": "        credentials = self._get_credentials(request)\n        if credentials:\n            return credentials[0]\n", "idx": 839}
{"namespace": "pyramid.request.CallbackMethodsMixin._process_response_callbacks", "completion": "        for callback in self.response_callbacks:\n            callback(self, response)\n\n", "idx": 840}
{"namespace": "pyramid.request.CallbackMethodsMixin._process_finished_callbacks", "completion": "        callbacks = self.finished_callbacks\n        while callbacks:\n            callback = callbacks.popleft()\n            callback(self)\n\n", "idx": 841}
{"namespace": "pyramid.request.Request.session", "completion": "        session_factory = self.registry.queryUtility(IDocumentFactory)\n        if session_factory is None:\n            raise ConfigurationError(\n                'No session factory registered'\n            )\n        return session_factory(self)\n", "idx": 842}
{"namespace": "pyramid.request.RequestLocalCache.get_or_create", "completion": "        if creator is None:\n            creator = self._creator\n        if creator is None:\n            raise ValueError('creator must be provided')\n        try:\n            return self._store[request]\n        except KeyError:\n            value = self.NO_VALUE\n            self._store[request] = value\n            try:\n                value = creator(request)\n            except Exception:\n                # If the creator function raises an exception, we need to\n                # remove the entry from the cache so that the next time the\n                # creator function is called, it will be called again.\n                del self._store[request]\n                raise\n            if value is self.NO_VALUE:\n                raise ValueError('creator must return a value')\n            self._store[request] = value\n            return value\n", "idx": 843}
{"namespace": "pyramid.request.RequestLocalCache.set", "completion": "        self._store[request] = value\n        request.add_finished_callback(self._cleanup)\n", "idx": 844}
{"namespace": "pyramid.i18n.Localizer.pluralize", "completion": "        if self.pluralizer is None:\n            self.pluralizer = Pluralizer(self.translations)\n        return self.pluralizer(singular, plural, n, domain=domain, mapping=mapping)\n\n", "idx": 845}
{"namespace": "pyramid.i18n.Translations.load", "completion": "        if locales is None:\n            locales = []\n\n        if not locales:\n            return gettext.NullTranslations()\n\n        if dirname is None:\n            dirname = os.path.join(os.path.dirname(__file__), 'locale')\n\n        result = None\n        for locale in locales:\n            try:\n                catalog = gettext.translation(domain, dirname, [locale])\n            except IOError:\n                catalog = None\n            if catalog is not None:\n                result = catalog\n                break\n\n        if result is None:\n            return gettext.NullTranslations()\n\n        return result\n", "idx": 846}
{"namespace": "pyramid.i18n.Translations.add", "completion": "        if translations.domain == self.domain:\n            if merge:\n                self.merge(translations)\n            else:\n                self._catalog.update(translations._catalog)\n            self.files.extend(translations.files)\n        else:\n            self._domains[translations.domain] = translations\n        return self\n", "idx": 847}
{"namespace": "pyramid.i18n.Translations.merge", "completion": "        self.plural = translations.plural\n        self._info.update(translations._info)\n        self._catalog.update(translations._catalog)\n        self.files.extend(translations.files)\n        for domain, catalog in translations._domains.items():\n            existing = self._domains.get(domain)\n            if existing is not None:\n                existing.merge(catalog)\n            else:\n                catalog.add_fallback(self)\n                self._domains[domain] = catalog\n        return self\n", "idx": 848}
{"namespace": "pyramid.i18n.LocalizerRequestMixin.locale_name", "completion": "        locale_name = negotiate_locale_name(self)\n        return locale_name\n\n", "idx": 849}
{"namespace": "pyramid.csrf.LegacySessionCSRFStoragePolicy.check_csrf_token", "completion": "        expected_token = self.get_csrf_token(request)\n        return not strings_differ(\n            bytes_(supplied_token), bytes_(expected_token)\n        )\n\n", "idx": 850}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.new_csrf_token", "completion": "        token = self._token_factory()\n        request.session[self.key] = token\n        return token\n", "idx": 851}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.get_csrf_token", "completion": "        token = request.session.get(self.key, self._token_factory())\n        request.session[self.key] = token\n        return token\n", "idx": 852}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.check_csrf_token", "completion": "        expected_token = self.get_csrf_token(request)\n        return not strings_differ(\n            bytes_(expected_token), bytes_(supplied_token)\n        )\n\n", "idx": 853}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.new_csrf_token", "completion": "        token = self._token_factory()\n        self.cookie_profile.set_cookies(request, response=token)\n        return token\n", "idx": 854}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.get_csrf_token", "completion": "        token = request.cookies.get(self.cookie_name, None)\n        if not token:\n            token = self.new_csrf_token(request)\n        return token\n", "idx": 855}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.check_csrf_token", "completion": "        expected_token = self.get_csrf_token(request)\n        return not strings_differ(\n            bytes_(expected_token), bytes_(supplied_token)\n        )\n\n", "idx": 856}
{"namespace": "pyramid.security.PermitsResult.__repr__", "completion": "        return \"<%s instance at %s with msg %s>\" % (\n            self.__class__.__name__,\n            hex(id(self)),\n            self.msg,\n        )\n\n", "idx": 857}
{"namespace": "pyramid.util.InstancePropertyHelper.make_property", "completion": "        if name is None:\n            name = callable.__name__\n        if reify:\n            callable = reify(callable)\n        return (name, property(callable))\n", "idx": 858}
{"namespace": "pyramid.util.InstancePropertyHelper.set_property", "completion": "        name, fn = cls.make_property(callable, name, reify)\n        setattr(target.__class__, name, fn)\n", "idx": 859}
{"namespace": "pyramid.util.InstancePropertyHelper.add_property", "completion": "        name, fn = self.make_property(callable, name=name, reify=reify)\n        self.properties[name] = fn\n", "idx": 860}
{"namespace": "pyramid.util.InstancePropertyHelper.apply", "completion": "        self.apply_properties(target, self.properties)\n\n", "idx": 861}
{"namespace": "pyramid.util.InstancePropertyMixin.set_property", "completion": "        InstancePropertyHelper.set_property(self, callable, name=name, reify=reify)\n\n", "idx": 862}
{"namespace": "pyramid.util.TopologicalSorter.remove", "completion": "        self.names.remove(name)\n        del self.name2val[name]\n        after = self.name2after.pop(name, [])\n        if after:\n            self.req_after.remove(name)\n            for u in after:\n                self.req_before.remove((u, name))\n        before = self.name2before.pop(name, [])\n        if before:\n            self.req_before.remove((name,))\n            for u in before:\n                self.req_after.remove((u, name))\n", "idx": 863}
{"namespace": "pyramid.util.TopologicalSorter.add", "completion": "        if name in self.names:\n            self.remove(name)\n        self.names.append(name)\n        self.name2val[name] = val\n        if after is None:\n            if before is None:\n                after = self.default_after\n            else:\n                after = []\n        if before is None:\n            before = self.default_before\n        if after is self.first or name in after:\n            self.order.insert(0, (name, self.first))\n            self.req_before.add(name)\n            if name in self.name2after:\n                self.name2after[name].append(self.first)\n            else:\n                self.name2after[name] = [self.first]\n        elif after is not self.last:\n            for u in after:\n                self.order.append((name, u))\n                self.req_before.add(name)\n                if u in self.name2after:\n                    self.name2after[u].append(name)\n                else:\n                    self.name2after[u] = [name]\n        if before is self.last or name in before:\n            self.order.append((self.last, name))\n            self.req_after.add(name)\n            if name in self.name2before:\n                self.name2before[name].append(self.last)\n            else:\n                self.name2before[name] = [self.last]\n        elif before is not self.first:\n            for u in before:\n                self.order.append((u, name))\n                self.req_after.add(name)\n                if u in self.name2before:\n                    self.name2before[u].append(name)\n                else:\n                    self.name2before[u] = [name]\n", "idx": 864}
{"namespace": "pyramid.traversal.find_resource", "completion": "    if isinstance(path, str):\n        if path.startswith('/'):\n            path = path[1:]\n            resource = find_root(resource)\n        if not path:\n            return resource\n        path = path.split('/')\n    elif not is_nonstr_iter(path):\n        raise TypeError('path must be a str or non-string iterable')\n    for name in path:\n        resource = resource[name]\n    return resource\n\n", "idx": 865}
{"namespace": "pyramid.static.ManifestCacheBuster.manifest", "completion": "        if self.reload:\n            if not self.exists(self.manifest_path):\n                return {}\n            mtime = self.getmtime(self.manifest_path)\n            if mtime != self._mtime:\n                self._manifest = self.get_manifest()\n                self._mtime = mtime\n        return self._manifest\n", "idx": 866}
{"namespace": "pyramid.registry.Registry.registerSubscriptionAdapter", "completion": "        self.has_listeners = True\n        return super(Registry, self).registerSubscriptionAdapter(*arg, **kw)\n", "idx": 867}
{"namespace": "pyramid.registry.Registry.registerHandler", "completion": "        result = Components.registerHandler(self, *arg, **kw)\n        self.has_listeners = True\n        return result\n", "idx": 868}
{"namespace": "pyramid.registry.Registry.notify", "completion": "        if self.has_listeners:\n            for event in events:\n                self.subscribers((event,), None)\n", "idx": 869}
{"namespace": "pyramid.registry.Introspector.add", "completion": "        category = intr.category\n        discriminator = intr.discriminator\n        order = self._counter\n        self._counter += 1\n        category_intrs = self._categories.setdefault(category, {})\n        discriminator_intrs = category_intrs.setdefault(discriminator, [])\n        discriminator_intrs.append((order, intr))\n        intr.order = order\n        intr.intr = self\n        self._refs[id(intr)] = intr\n", "idx": 870}
{"namespace": "pyramid.registry.Introspector.get", "completion": "        category = self._categories.get(category_name)\n        if category is None:\n            return default\n        return category.get(discriminator, default)\n", "idx": 871}
{"namespace": "pyramid.registry.Introspector.get_category", "completion": "        category = self._categories.setdefault(category_name, {})\n        introspectables = list(category.values())\n        if sort_key is None:\n            introspectables.sort(key=operator.attrgetter('order'))\n        else:\n            introspectables.sort(key=sort_key)\n        return [{'introspectable': i, 'related': self._refs.get(i, [])} for i in introspectables]\n", "idx": 872}
{"namespace": "pyramid.registry.Introspector.categorized", "completion": "        if sort_key is None:\n            sort_key = operator.attrgetter('order')\n        result = []\n        for category_name, category in self._categories.items():\n            values = category.values()\n            values = sorted(set(values), key=sort_key)\n            result.append((category_name, [{'introspectable': intr, 'related': self.related(intr)} for intr in values]))\n        return result\n", "idx": 873}
{"namespace": "pyramid.registry.Introspector.remove", "completion": "        category = self._categories.get(category_name)\n        if category is None:\n            return\n        intr = category.pop(discriminator, None)\n        if intr is None:\n            return\n        intr.order = -1\n        for d in intr.related:\n            self.remove(category_name, d)\n", "idx": 874}
{"namespace": "pyramid.registry.Introspector.relate", "completion": "        introspectables = self._get_intrs_by_pairs(pairs)\n        for i in range(len(introspectables)):\n            for j in range(i + 1, len(introspectables)):\n                self._add_ref(introspectables[i], introspectables[j])\n                self._add_ref(introspectables[j], introspectables[i])\n", "idx": 875}
{"namespace": "pyramid.registry.Introspector.related", "completion": "        return self._refs.get(intr, [])\n\n", "idx": 876}
{"namespace": "pyramid.registry.Introspectable.discriminator_hash", "completion": "        self._assert_resolved()\n        return hash(self.discriminator)\n", "idx": 877}
{"namespace": "pyramid.registry.Introspectable.__repr__", "completion": "        return '<%s category %r, discriminator %r>' % (\n            self.type_name,\n            self.category_name,\n            self.discriminator,\n        )\n\n", "idx": 878}
{"namespace": "pyramid.scripts.proutes.PRoutesCommand._get_mapper", "completion": "        mapper = None\n        for name, item in registry.__dict__.items():\n            if isinstance(item, RoutesMapper):\n                mapper = item\n                break\n        return mapper\n", "idx": 879}
{"namespace": "pyramid.scripts.pshell.PShellCommand.make_shell", "completion": "        shells = self.find_all_shells()\n        if self.args.python_shell:\n            if self.args.python_shell in shells:\n                return shells[self.args.python_shell]\n            else:\n                raise ValueError(\n                    'could not find a shell named \"%s\"' % self.args.python_shell\n                )\n        else:\n            for shell_name in self.preferred_shells:\n                if shell_name in shells:\n                    return shells[shell_name]\n            for shell_name in shells:\n                if shell_name != 'python':\n                    return shells[shell_name]\n            return self.default_runner", "idx": 880}
{"namespace": "pyramid.config.assets.PackageOverrides.insert", "completion": "        if path.endswith('/'):\n            override = DirectoryOverride(path, source)\n        else:\n            override = FileOverride(path, source)\n        self.overrides.insert(0, override)\n        return override\n", "idx": 881}
{"namespace": "pyramid.config.assets.PackageOverrides.filtered_sources", "completion": "        for override in self.overrides:\n            for source in override.filtered_sources(resource_name):\n                yield source\n", "idx": 882}
{"namespace": "pyramid.config.assets.PackageOverrides.real_loader", "completion": "        if self._real_loader is None:\n            raise NotImplementedError(\n                \"PackageOverrides does not have a real loader\"\n            )\n        return self._real_loader\n\n", "idx": 883}
{"namespace": "pyramid.config.views.MultiView.add", "completion": "        if phash is None:\n            phash = getattr(view, '__phash__', DEFAULT_PHASH)\n\n        if accept is None:\n            self.views = sorted(\n                self._add_view(self.views, view, order, phash),\n                key=lambda x: x[0],\n            )\n        else:\n            accepts = self.media_views.setdefault(accept, [])\n            accepts = sorted(\n                self._add_view(accepts, view, order, phash),\n                key=lambda x: x[0],\n            )\n            self.media_views[accept] = accepts\n\n        if accept_order is None:\n            self.accepts = sorted(self.media_views)\n        else:\n            self.accepts = sorted(self.media_views, key=lambda x: accept_order[x])\n", "idx": 884}
{"namespace": "pyramid.config.views.MultiView.get_views", "completion": "        if request.accept and self.accepts:\n            views = []\n            for accept in self.accepts:\n                if accept in request.accept:\n                    views.extend(self.media_views[accept])\n            views.extend(self.views)\n            return views\n        return self.views\n", "idx": 885}
{"namespace": "pyramid.config.views.MultiView.match", "completion": "        for order, view, phash in self.get_views(request):\n            if not hasattr(view, '__predicated__'):\n                return view\n            pred = view.__predicated__\n            if not pred(context, request):\n                continue\n            return view\n        raise PredicateMismatch(self.name)\n", "idx": 886}
{"namespace": "pyramid.config.views.MultiView.__permitted__", "completion": "        view = self.match(context, request)\n        if not hasattr(view, '__permitted__'):\n            return True\n        return view.__permitted__(context, request)\n", "idx": 887}
{"namespace": "pyramid.config.views.MultiView.__call_permissive__", "completion": "        view = self.match(context, request)\n        if hasattr(view, '__call_permissive__'):\n            return view.__call_permissive__(context, request)\n        return view(context, request)\n", "idx": 888}
{"namespace": "pyramid.config.actions.ActionState.processSpec", "completion": "        if spec in self._seen_files:\n            return False\n        self._seen_files.add(spec)\n        return True\n", "idx": 889}
{"namespace": "pyramid.config.actions.ActionState.action", "completion": "        if kw is None:\n            kw = {}\n        action = extra\n        action.update(\n            dict(\n                discriminator=discriminator,\n                callable=callable,\n                args=args,\n                kw=kw,\n                includepath=includepath,\n                info=info,\n                order=order,\n                introspectables=introspectables,\n            )\n        )\n        self.actions.append(action)\n", "idx": 890}
{"namespace": "pyramid.config.actions.ActionInfo.__str__", "completion": "        return 'Line %s of file %s:\\n%s' % (\n            self.line,\n            self.file,\n            indent(self.src),\n        )\n\n", "idx": 891}
{"namespace": "pyramid.config.Configurator.__getattr__", "completion": "        directives = getattr(self.registry, '_directives', {})\n        try:\n            c, action_wrap = directives[name]\n        except KeyError:\n            raise AttributeError(name)\n        if action_wrap:\n            c = self.action_method(c)\n        return partial(c, self)\n", "idx": 892}
{"namespace": "pyramid.config.Configurator.with_package", "completion": "        return self.__class__(\n            registry=self.registry,\n            package=package,\n            root_package=self.root_package,\n            autocommit=self.autocommit,\n            route_prefix=self.route_prefix,\n        )\n", "idx": 893}
{"namespace": "pyramid.config.Configurator.absolute_asset_spec", "completion": "        if not isinstance(relative_spec, str):\n            return relative_spec\n\n        if relative_spec.startswith('/'):\n            return relative_spec\n\n        return self.package_name + ':' + relative_spec\n", "idx": 894}
{"namespace": "pyramid.config.Configurator.begin", "completion": "        if request is _marker:\n            request = self.registry.request\n        self.manager.push_threadlocal(self.registry, request)\n", "idx": 895}
{"namespace": "pyramid.config.Configurator.scan", "completion": "        if package is None:\n            package = caller_package()\n        scanner = self.get_predicate_domain('venusian', self.registry)\n        scanner.scan(package, categories=categories, onerror=onerror, ignore=ignore, **kw)\n", "idx": 896}
{"namespace": "pyramid.config.Configurator.make_wsgi_app", "completion": "        self.commit()\n        self.registry.registerUtility(self.registry, IRegistry)\n        self.registry.registerUtility(self.registry, IApplicationCreated)\n        return Router(self.registry)\n", "idx": 897}
{"namespace": "aiohappybase._util.camel_case_to_pep8", "completion": "    return CAPITALS.sub(lambda m: '_' + m.group(1).lower(), name)\n\n", "idx": 898}
{"namespace": "kinto.authorization._relative_object_uri", "completion": "    obj_parts = object_uri.split(\"/\")\n    parent_uri = \"\"\n    for i in range(0, len(obj_parts), 2):\n        parent_uri = obj_parts[:i + 2]\n        parent_resource_name = parent_uri[-2].rstrip(\"s\")\n        if parent_resource_name == resource_name:\n            return \"/\".join(parent_uri)\n    raise ValueError(\"No parent URI found for resource {}\".format(resource_name))\n\n", "idx": 899}
{"namespace": "kinto.core.openapi.OpenAPI.expose_authentication_method", "completion": "        cls.security_definitions[method_name] = definition\n        cls.security_roles[method_name] = definition[\"scopes\"]", "idx": 900}
{"namespace": "kinto.core.openapi.OpenAPI.generate", "completion": "        # Create the base specification\n        base_specification = {\n            \"host\": self.settings[\"http_host\"],\n            \"schemes\": [self.settings[\"http_scheme\"]],\n            \"securityDefinitions\": self.security_definitions,\n        }\n\n        # Generate the OpenAPI specification\n        return super().generate(swagger=base_specification)", "idx": 901}
{"namespace": "kinto.core.testing.get_user_headers", "completion": "    auth = \"{}:{}\".format(user, password)\n    auth = base64.b64encode(auth.encode(\"utf-8\"))\n    return {\"Authorization\": \"Basic {}\".format(auth.decode(\"utf-8\"))}\n\n", "idx": 902}
{"namespace": "kinto.core.authorization.RouteFactory.fetch_shared_objects", "completion": "        if not self.on_plural_endpoint:\n            return None\n\n        # If the permission is not a read or write permission, return None.\n        if perm not in [\"read\", \"write\"]:\n            return None\n\n        # If the permission is a read permission, set the match to the object ID match.\n        if perm == \"read\":\n            match = self._object_id_match\n        # If the permission is a write permission, set the match to the object ID match.\n        elif perm == \"write\":\n            match = self._object_id_match\n\n        # Get the objects that are readable or writable for the current principals.\n        objects = self._get_accessible_objects(principals, [match])\n\n        # Set the shared IDs to the context.\n        self.shared_ids = [obj[\"id\"] for obj in objects]\n\n        # Return the shared IDs.\n        return self.shared_ids\n", "idx": 903}
{"namespace": "kinto.core.authorization.RouteFactory.get_permission_object_id", "completion": "        if object_id is None:\n            object_id = request.path.split(\"/\")[-1]\n        if self.on_plural_endpoint:\n            plural_service = utils.current_service(request)\n            object_service = plural_service.object_service\n            object_uri = object_service.get_uri(object_id)\n        else:\n            object_uri = request.path\n        return object_uri\n", "idx": 904}
{"namespace": "kinto.core.utils.recursive_update_dict", "completion": "    for key, value in changes.items():\n        if isinstance(value, dict):\n            if key in root:\n                recursive_update_dict(root[key], value, ignores)\n            else:\n                root[key] = {}\n                recursive_update_dict(root[key], value, ignores)\n        else:\n            if key in ignores:\n                del root[key]\n            else:\n                root[key] = value\n\n", "idx": 905}
{"namespace": "kinto.core.utils.native_value", "completion": "    if isinstance(value, str):\n        try:\n            return json.loads(value)\n        except ValueError:\n            return value\n    else:\n        return value\n\n", "idx": 906}
{"namespace": "kinto.core.utils.dict_subset", "completion": "    result = {}\n    for key in keys:\n        if key in d:\n            result[key] = d[key]\n        elif \".\" in key:\n            key_list = key.split(\".\")\n            value = d\n            for k in key_list:\n                if k in value:\n                    value = value[k]\n                else:\n                    break\n            else:\n                result[key] = value\n    return result\n\n", "idx": 907}
{"namespace": "kinto.core.utils.dict_merge", "completion": "    if not isinstance(b, dict):\n        return b\n    result = deepcopy(a)\n    for k, v in b.items():\n        if k in result and isinstance(result[k], dict):\n            result[k] = dict_merge(result[k], v)\n        else:\n            result[k] = deepcopy(v)\n    return result\n\n", "idx": 908}
{"namespace": "kinto.core.utils.find_nested_value", "completion": "    if path in d:\n        return d[path]\n\n    path_parts = path.split(\".\")\n    root_keys = [\".\".join(path_parts[:i]) for i in range(len(path_parts), 0, -1)]\n    root_key = None\n\n    for key in root_keys:\n        if key in d:\n            root_key = key\n            break\n\n    if root_key is None or not isinstance(d[root_key], dict):\n        return default\n\n    subpath = path[len(root_key) + 1:]\n    return find_nested_value(d[root_key], subpath, default)\n\n", "idx": 909}
{"namespace": "kinto.core.utils.instance_uri_registry", "completion": "    fakerequest = Request.blank(path=\"/\")\n    fakerequest.registry = registry\n    return strip_uri_prefix(fakerequest.route_path(f\"{resource_name}-object\", **params))\n\n", "idx": 910}
{"namespace": "kinto.core.statsd.load_from_config", "completion": "    if statsd_module is None:\n        raise ConfigurationError(\"The statsd module is not installed.\")\n\n    settings = config.get_settings()\n    statsd_url = settings[\"statsd_url\"]\n    parsed = urlparse(statsd_url)\n    host = parsed.hostname\n    port = parsed.port\n    prefix = parsed.path\n    return Client(host, port, prefix)", "idx": 911}
{"namespace": "kinto.core.errors.http_error", "completion": "    if errno is None:\n        errno = ERRORS.UNDEFINED\n    if code is None:\n        code = httpexception.code\n    if error is None:\n        error = httpexception.title\n    if message is None:\n        message = httpexception.explanation\n\n    response = httpexception\n    response.content_type = \"application/json\"\n    response.json = ErrorSchema().deserialize(\n        {\n            \"code\": code,\n            \"errno\": errno,\n            \"error\": error,\n            \"message\": message,\n            \"info\": info,\n            \"details\": details,\n        }\n    )\n\n    return reapply_cors(response)\n\n", "idx": 912}
{"namespace": "kinto.core.resource.schema.ResourceReponses.get_and_bind", "completion": "        # Get the default response schemas for the endpoint type\n        default_schemas = getattr(self, \"default_{}_schemas\".format(endpoint_type))\n\n        # Get the default response schemas for the method\n        default_method_schemas = getattr(self, \"default_{}_schemas\".format(method.lower()))\n\n        # Get the response schemas for the endpoint type and method\n        endpoint_method_schemas = getattr(self, \"{}_{}_schemas\".format(endpoint_type, method.lower()))\n\n        # Create a dictionary of status codes mapping cloned and binded responses\n        responses = {}\n        for status_code, default_schema in default_schemas.items():\n            # Clone the default response schema\n            response_schema = default_schema()\n\n            # Bind the response schema with the method args and endpoint args\n            response_schema.bind(**kwargs)\n\n            # Bind the response schema with the method args and endpoint args\n            response_schema.bind(**kwargs)\n\n            # Add the response schema to the dictionary\n            responses[status_code] = response_schema\n\n        # Add the method-specific response schemas to the dictionary\n        for status_code, method_schema in default_method_schemas.items():\n            # Clone the method-specific response schema\n            response_schema = method_schema()\n\n            # Bind the response schema with the method args and endpoint args\n            response_schema.bind(**kwargs)\n\n            # Add the response schema to the dictionary\n            responses[status_code] = response_schema\n\n        # Add the endpoint-specific response schemas to the dictionary\n        for status_code, endpoint_schema in endpoint_method_schemas.items():\n            # Clone the endpoint-specific response schema\n            response_schema = endpoint_schema()\n\n            # Bind the response schema with the method args and endpoint args\n            response_schema.bind(**kwargs)\n\n            # Add the response schema to the dictionary\n            responses[status_code] = response_schema\n\n        return responses", "idx": 913}
{"namespace": "kinto.core.resource.Resource.timestamp", "completion": "        try:\n            return self.model.timestamp\n        except storage_exceptions.ReadOnlyError as e:\n            http_error = HTTPServiceUnavailable()\n            http_error.json = {\"message\": \"The server is currently unavailable.\"}\n            http_error.json[\"details\"] = {\"error\": str(e)}\n            raise http_error\n", "idx": 914}
{"namespace": "kinto.core.resource.Resource.plural_post", "completion": "        self._add_timestamp_header(self.request.response)\n        self._raise_304_if_not_modified()\n        self._raise_412_if_modified()\n\n        # Check if the object id is already in use.\n        if self.object_id:\n            # If it is, return the existing object.\n            obj = self.model.get_object(self.object_id)\n            if obj is not None:\n                return self.postprocess(obj)\n\n        # If it isn't, create the new object.\n        obj = self.process_object(self.request.validated)\n        self.model.create_object(obj)\n        self.request.response.status_code = 201\n        return self.postprocess(obj)\n", "idx": 915}
{"namespace": "kinto.core.resource.Resource.get", "completion": "        # Check if the object exists.\n        obj = self._get_object_or_404(self.object_id)\n\n        # Check if the object has been modified.\n        self._raise_304_if_not_modified(obj)\n\n        # Check if partial fields are requested.\n        partial_fields = self._extract_partial_fields()\n\n        # Add timestamp header and cache header to the response.\n        self._add_timestamp_header(self.request.response, obj)\n        self._add_cache_header(self.request.response)\n\n        # If partial fields are requested, extract them from the object.\n        if partial_fields:\n            obj = dict_subset(obj, partial_fields)\n\n        # Return the object.\n        return self.postprocess(obj)\n", "idx": 916}
{"namespace": "kinto.core.resource.Resource.delete", "completion": "        self._raise_400_if_invalid_id(self.object_id)\n        existing = self._get_object_or_404(self.object_id)\n        self._raise_412_if_modified(existing)\n\n        timestamp = existing[self.model.modified_field]\n        self._add_timestamp_header(self.request.response, timestamp=timestamp)\n\n        self.model.delete_object(existing)\n\n        return self.postprocess(existing, action=ACTIONS.DELETE)\n", "idx": 917}
{"namespace": "kinto.core.permission.memory.Permission.add_principal_to_ace", "completion": "        # Get the ACE for the object and permission.\n        ace = self._get_object_permission(object_id, permission)\n\n        # Add the principal to the ACE.\n        ace.add(principal)\n\n        # Update the store with the modified ACE.\n        self._store[object_id + \":\" + permission] = ace\n", "idx": 918}
{"namespace": "kinto.core.permission.memory.Permission.get_object_permission_principals", "completion": "        permission_key = f\"permission:{object_id}:{permission}\"\n        return self._store.get(permission_key, set())\n", "idx": 919}
{"namespace": "kinto.core.storage.generators.Generator.match", "completion": "        if self._regexp is None:\n            self._regexp = re.compile(self.regexp)\n\n        return self._regexp.match(object_id)\n", "idx": 920}
{"namespace": "kinto.core.storage.postgresql.migrator.MigratorMixin.create_or_migrate_schema", "completion": "        # Get the current version of the schema.\n        current_version = self.get_installed_version()\n\n        # If there is no current version, create a new schema.\n        if current_version is None:\n            logger.info(\"Creating new %s schema\", self.name)\n            if not dry_run:\n                self.client.run_sql_file(self.schema_file)\n            return\n\n        # If the current version matches the desired version, log that the schema is up-to-date.\n        if current_version == self.schema_version:\n            logger.info(\"Schema for %s is up-to-date\", self.name)\n            return\n\n        # If the current version does not match the desired version, migrate the schema.\n        logger.info(\"Migrating %s schema from version %s to %s\", self.name, current_version, self.schema_version)\n        if not dry_run:\n            self.migrate_schema(current_version)\n", "idx": 921}
{"namespace": "kinto.core.views.batch.BatchPayloadSchema.deserialize", "completion": "        # Deserialize the data\n        data = super().deserialize(cstruct)\n\n        # Merge the defaults with the requests\n        for request in data[\"requests\"]:\n            for key, value in data[\"defaults\"].items():\n                if key not in request:\n                    request[key] = value\n\n        return data\n\n", "idx": 922}
{"namespace": "kinto.plugins.accounts.utils.get_cached_reset_password", "completion": "    settings = registry.settings\n    hmac_secret = settings[\"userid_hmac_secret\"]\n    cache_key = utils.hmac_digest(hmac_secret, ACCOUNT_RESET_PASSWORD_CACHE_KEY.format(username))\n    cache = registry.cache\n    return cache.get(cache_key)\n\n", "idx": 923}
{"namespace": "kinto.plugins.accounts.utils.get_cached_validation_key", "completion": "    hmac_secret = registry.settings[\"userid_hmac_secret\"]\n    cache_key = utils.hmac_digest(hmac_secret, ACCOUNT_VALIDATION_CACHE_KEY.format(username))\n\n    cache = registry.cache\n    cache_result = cache.get(cache_key)\n    return cache_result\n\n", "idx": 924}
{"namespace": "kinto.plugins.accounts.views.validation.on_account_activated", "completion": "    settings = event.request.registry.settings\n    if not settings.get(\"account_validation.enabled\", False):\n        return\n\n    for impacted in event.impacted:\n        old_account = impacted[\"old\"]\n        new_account = impacted[\"new\"]\n\n        if old_account.get(\"validated\", False) or not new_account.get(\"validated\", False):\n            continue\n\n        Emailer(event.request, new_account).send_confirmation_email()", "idx": 925}
{"namespace": "kinto.plugins.openid.OpenIDConnectPolicy._verify_token", "completion": "        # Fetch userinfo\n        userinfo_endpoint = self.oid_config[\"userinfo_endpoint\"]\n        headers = {\"Authorization\": \"Bearer \" + access_token}\n        try:\n            response = requests.get(userinfo_endpoint, headers=headers)\n            response.raise_for_status()\n            return response.json()\n        except Exception as e:\n            request.registry.logger.debug(\"Error verifying token: %s\", e)\n            return None\n\n", "idx": 926}
{"namespace": "kinto.plugins.quotas.scripts.rebuild_quotas", "completion": "    for bucket in storage.list_all_buckets():\n        bucket_id = bucket[\"id\"]\n        bucket_size = 0\n        bucket_record_count = 0\n        bucket_collection_count = 0\n        for collection in storage.list_all_collections(bucket_id):\n            collection_id = collection[\"id\"]\n            collection_size = 0\n            collection_record_count = 0\n            for record in paginated(\n                storage.list_all_records, bucket_id, collection_id, sort=OLDEST_FIRST\n            ):\n                record_id = record[\"id\"]\n                record_size = record_size(record)\n                collection_size += record_size\n                bucket_size += record_size\n                collection_record_count += 1\n                bucket_record_count += 1\n                if not dry_run:\n                    storage.update_collection_size(\n                        bucket_id, collection_id, record_size\n                    )\n            if not dry_run:\n                storage.update_collection_record_count(\n                    bucket_id, collection_id, collection_record_count\n                )\n                storage.update_collection_size(\n                    bucket_id, collection_id, collection_size\n                )\n            bucket_collection_count += 1\n        if not dry_run:\n            storage.update_bucket_record_count(bucket_id, bucket_record_count)\n            storage.update_bucket_size(bucket_id, bucket_size)\n            storage.update_bucket_collection_count(bucket_id, bucket_collection_count)\n        logger.info(\n            \"Bucket {} has {} records, {} bytes, and {} collections.\".format(\n                bucket_id, bucket_record_count, bucket_size, bucket_collection_count\n            )\n        )\n\n", "idx": 927}
{"namespace": "kinto.config.render_template", "completion": "    logger.info(\"Rendering template: %s\", template)\n    with codecs.open(template, \"r\", encoding=\"utf-8\") as f:\n        rendered = f.read().format(**kwargs)\n    with codecs.open(destination, \"w\", encoding=\"utf-8\") as f:\n        f.write(rendered)\n\n", "idx": 928}
{"namespace": "trafilatura.sitemaps.SitemapObject.extract_sitemap_langlinks", "completion": "        # find all hreflang links\n        hreflang_links = HREFLANG_REGEX.findall(self.content)\n        LOGGER.debug('%s hreflang links found', len(hreflang_links))\n\n        # iterate through the hreflang links\n        for link in hreflang_links:\n            # check if the link matches the target language\n            if lang_filter(link, self.target_lang):\n                # handle the link\n                self.handle_link(link)\n\n        # log a debug message about the number of sitemaps and links found with hreflang for the given target language\n        LOGGER.debug('%s sitemaps and %s links with hreflang found for %s', len(self.sitemap_urls), len(self.urls), self.target_lang)\n", "idx": 929}
{"namespace": "trafilatura.sitemaps.SitemapObject.extract_sitemap_links", "completion": "        for match in LINK_REGEX.finditer(self.content):\n            self.handle_link(match[1])\n        LOGGER.debug('%s sitemaps and %s links found for %s', len(self.sitemap_urls), len(self.urls), self.sitemap_url)\n", "idx": 930}
{"namespace": "trafilatura.sitemaps.SitemapObject.process", "completion": "        # check if sitemap is plausible\n        if not self.is_sitemap_plausible():\n            return\n\n        # extract links from a TXT file\n        if self.is_txt_sitemap():\n            self.extract_sitemap_links()\n            return\n\n        # extract links from an XML sitemap\n        if self.is_xml_sitemap():\n            if self.target_lang is not None:\n                self.extract_sitemap_langlinks()\n            else:\n                self.extract_sitemap_links()\n            return\n\n        # extract links from a sitemap\n        for match in (m[1] for m in islice(LINK_REGEX.finditer(self.content), MAX_LINKS)):\n            # process middle part of the match tuple\n            self.handle_link(match)\n", "idx": 931}
{"namespace": "trafilatura.sitemaps.is_plausible_sitemap", "completion": "    # check if the URL is a sitemap\n    if not DETECT_SITEMAP_LINK.search(url):\n        return False\n\n    # check if the URL is a sitemap index\n    if '<sitemapindex' in contents:\n        return False\n\n    # check if the URL is a sitemap\n    if '<urlset' in contents:\n        return True\n\n    # check if the URL is a sitemap\n    if '<urlset ' in contents:\n        return True\n\n    # check if the URL is a sitemap\n    if '<urlset>' in contents:\n        return True\n\n    # check if the URL is a sitemap\n    if '<urlset />' in contents:\n        return True\n\n    # check if the URL is a sitemap\n    if '<urlset/>' in contents:\n        return True\n\n    # check if the URL is a sitemap\n    if '<urlset />' in contents:\n        return True\n\n    # check if the URL is a sitemap\n    if '<urlset/>' in contents:\n        return True\n\n    # check if the URL is a sitemap\n    if '<urlset />' in contents:\n        return True\n\n    # check if the URL is a sitemap\n    if '<urlset/>' in contents:\n        return True\n\n    # check if the URL is a sitemap\n    if '<urlset />' in contents:\n        return True\n\n    # check if the URL is a sitemap\n    if '<urlset/>' in contents:\n        return True\n\n    # check if the URL is a sitemap\n    if '<urlset />' in contents:\n        return True\n\n    # check if the URL is a sitemap\n    if '<urlset/>' in contents:\n        return True\n\n    # check if the URL is a sitemap\n    if '<urlset />' in contents:\n        return True\n\n    # check if the URL is a sitemap\n    if '<urlset/>' in contents:\n        return True\n\n    # check if the URL is a sitemap\n    if '<", "idx": 932}
{"namespace": "trafilatura.sitemaps.extract_robots_sitemaps", "completion": "    sitemap_urls = []\n    if robotstxt is None:\n        return sitemap_urls\n\n    for line in robotstxt.splitlines():\n        line = line.strip()\n        if line.startswith('#') or not line:\n            continue\n        if line.lower().startswith('sitemap'):\n            sitemap_urls.append(line.split()[1])\n\n    sitemap_urls = [fix_relative_urls(baseurl, s) for s in sitemap_urls]\n    return sitemap_urls", "idx": 933}
{"namespace": "trafilatura.feeds.handle_link_list", "completion": "    # Check if the link list is empty\n    if not linklist:\n        return []\n\n    # Check if the link list is a string\n    if isinstance(linklist, str):\n        linklist = [linklist]\n\n    # Check if the link list is a list\n    if not isinstance(linklist, list):\n        return []\n\n    # Check if the link list is a list of strings\n    if not all(isinstance(link, str) for link in linklist):\n        return []\n\n    # Check if the link list is a list of valid URLs\n    if not all(validate_url(link) for link in linklist):\n        return []\n\n    # Check if the link list is a list of unique URLs\n    if not len(linklist) == len(set(linklist)):\n        return []\n\n    # Check if the link list is a list of URLs that are not blacklisted\n    if any(BLACKLIST.search(link) for link in linklist):\n        return []\n\n    # Check if the link list is a list of URLs that are not already in the database\n    # if any(link in db for link in linklist):\n    #     return []\n\n    # Check if the link list is a list of URLs that are not already in the database\n    # if any(link in db for link in linklist):\n    #     return []\n\n    # Check if the link list is a list of URLs that are not already in the database\n    # if any(link in db for link in linklist):\n    #     return []\n\n    # Check if the link list is a list of URLs that are not already in the database\n    # if any(link in db for link in linklist):\n    #     return []\n\n    # Check if the link list is a list of URLs that are not already in the database\n    # if any(link in db for link in linklist):\n    #     return []\n\n    # Check if the link list is a list of URLs that are not already in the database\n    # if any(link in db for link in linklist):\n    #     return []\n\n    # Check if the link list is a list of", "idx": 934}
{"namespace": "trafilatura.feeds.find_feed_urls", "completion": "    # get domain name and base url\n    domainname, baseurl = get_hostinfo(url)\n    # get the webpage content\n    htmlstring = load_html(url)\n    # check if it is a feed\n    if htmlstring is not None and FEED_OPENING.match(htmlstring):\n        # extract feed links\n        feed_urls = extract_links(htmlstring, domainname, baseurl, url, target_lang)\n    else:\n        # determine the feed\n        feed_urls = determine_feed(htmlstring, baseurl, url)\n        # extract feed links\n        for feed_url in feed_urls:\n            feed_htmlstring = load_html(feed_url)\n            if feed_htmlstring is not None:\n                feed_urls.extend(extract_links(feed_htmlstring, domainname, baseurl, url, target_lang))\n    # filter urls\n    if target_lang is not None:\n        feed_urls = filter_urls(feed_urls, target_lang)\n    # return sorted list of unique links\n    return sorted(set(feed_urls))", "idx": 935}
{"namespace": "trafilatura.hashing.generate_hash_filename", "completion": "    # Remove any potential XML tags from the content\n    content = CLEAN_XML.sub(\"\", content)\n\n    # Generate a bag-of-word hashing of length 12\n    hash_object = blake2b(digest_size=12)\n    hash_object.update(content.encode())\n    hash_bytes = hash_object.digest()\n\n    # Encode the hash using urlsafe_b64encode and return the decoded string\n    return urlsafe_b64encode(hash_bytes).decode()\n\n", "idx": 936}
{"namespace": "trafilatura.cli_utils.download_queue_processing", "completion": "    # initialize the error list\n    errors = []\n\n    # load the download buffer\n    load_download_buffer(url_store, args, config)\n\n    # process the download queue\n    while not url_store.empty():\n\n        # get the next URL\n        url = url_store.get()\n\n        # download the URL\n        try:\n            htmlstring = download(url, args, config)\n        except Exception as e:\n            # log the error\n            errors.append((url, e))\n            # increment the error counter\n            if counter is not None:\n                counter += 1\n            # skip to the next URL\n            continue\n\n        # process the result\n        counter = process_result(htmlstring, args, url, counter, config)\n\n    # return the error list and the counter\n    return errors, counter\n\n", "idx": 937}
{"namespace": "trafilatura.utils.decode_response", "completion": "    # handle compressed file\n    response = handle_compressed_file(response)\n\n    # decode response\n    if isinstance(response, bytes):\n        # detect encoding\n        encodings = detect_encoding(response)\n        # try to decode\n        for encoding in encodings:\n            try:\n                return response.decode(encoding)\n            except UnicodeDecodeError:\n                pass\n        # try to decode as utf-8\n        try:\n            return response.decode('utf-8')\n        except UnicodeDecodeError:\n            pass\n        # try to decode as latin-1\n        try:\n            return response.decode('latin-1')\n        except UnicodeDecodeError:\n            pass\n        # try to decode as ascii\n        try:\n            return response.decode('ascii', errors='ignore')\n        except UnicodeDecodeError:\n            pass\n        # try to decode as unicode escape\n        try:\n            return response.decode('unicode-escape', errors='ignore')\n        except UnicodeDecodeError:\n            pass\n        # try to decode as raw unicode\n        try:\n            return response.decode('raw-unicode-escape', errors='ignore')\n        except UnicodeDecodeError:\n            pass\n        # try to decode as utf-8\n        try:\n            return response.decode('utf-8', errors='ignore')\n        except UnicodeDecodeError:\n            pass\n        # try to decode as latin-1\n        try:\n            return response.decode('latin-1', errors='ignore')\n        except UnicodeDecodeError:\n            pass\n        # try to decode as ascii\n        try:\n            return response.decode('ascii', errors='ignore')\n        except UnicodeDecodeError:\n            pass\n        # try to decode as unicode escape\n        try:\n            return response.decode('unicode-escape', errors='ignore')\n        except UnicodeDecodeError:\n            pass\n        # try to decode as raw unicode\n        try:\n            return response.decode('raw-unicode-escape', errors='ignore')\n        except UnicodeDecodeError:\n            pass\n        # try to decode as utf-8\n        try", "idx": 938}
{"namespace": "trafilatura.utils.txttocsv", "completion": "    # Create a list to store the data\n    data = []\n\n    # Add the metadata to the list\n    data.append(docmeta['url'])\n    data.append(docmeta['fingerprint'])\n    data.append(docmeta['hostname'])\n    data.append(docmeta['title'])\n    data.append(docmeta['image'])\n    data.append(docmeta['date'])\n    data.append(docmeta['license'])\n    data.append(docmeta['pagetype'])\n    data.append(docmeta['id'])\n\n    # Add the text and comments to the list\n    data.append(text)\n    data.append(comments)\n\n    # Convert the list to a string\n    data = '\\t'.join(data)\n\n    # Return the result\n    return data\n\n", "idx": 939}
{"namespace": "trafilatura.utils.is_similar_domain", "completion": "    # Remove the file extension from the reference domain name.\n    reference = STRIP_EXTENSION.sub('', reference)\n\n    # Remove the file extension from the new domain name.\n    new_string = STRIP_EXTENSION.sub('', new_string)\n\n    # Calculate the similarity ratio between the two strings.\n    similarity_ratio = SequenceMatcher(None, reference, new_string).ratio()\n\n    # If the similarity ratio is below the threshold, return False.\n    if similarity_ratio < threshold:\n        return False\n\n    # Otherwise, return True.\n    return True\n\n", "idx": 940}
{"namespace": "trafilatura.xml.remove_empty_elements", "completion": "    for elem in tree.iter():\n        if not elem.text and not elem.tail and not elem.attrib and not elem:\n            elem.getparent().remove(elem)\n    return tree\n\n", "idx": 941}
{"namespace": "trafilatura.xml.strip_double_tags", "completion": "    for elem in reversed(list(tree.iter())):\n        for child in elem.iter():\n            if child.tag in NESTING_WHITELIST and child in elem:\n                elem.text = (elem.text or '') + (child.text or '')\n                elem.extend(child)\n                elem.remove(child)\n    return tree\n\n", "idx": 942}
{"namespace": "trafilatura.xml.check_tei", "completion": "    # check if the document is valid\n    if not is_valid(xmldoc):\n        LOGGER.warning('Invalid XML document: %s', url)\n        return None\n\n    # remove unwanted tags\n    xmldoc = remove_unwanted_tags(xmldoc)\n\n    # remove unwanted attributes\n    xmldoc = remove_unwanted_attributes(xmldoc)\n\n    # remove unwanted tags\n    xmldoc = remove_unwanted_tags(xmldoc)\n\n    # remove unwanted attributes\n    xmldoc = remove_unwanted_attributes(xmldoc)\n\n    # remove unwanted tags\n    xmldoc = remove_unwanted_tags(xmldoc)\n\n    # remove unwanted attributes\n    xmldoc = remove_unwanted_attributes(xmldoc)\n\n    # remove unwanted tags\n    xmldoc = remove_unwanted_tags(xmldoc)\n\n    # remove unwanted attributes\n    xmldoc = remove_unwanted_attributes(xmldoc)\n\n    # remove unwanted tags\n    xmldoc = remove_unwanted_tags(xmldoc)\n\n    # remove unwanted attributes\n    xmldoc = remove_unwanted_attributes(xmldoc)\n\n    # remove unwanted tags\n    xmldoc = remove_unwanted_tags(xmldoc)\n\n    # remove unwanted attributes\n    xmldoc = remove_unwanted_attributes(xmldoc)\n\n    # remove unwanted tags\n    xmldoc = remove_unwanted_tags(xmldoc)\n\n    # remove unwanted attributes\n    xmldoc = remove_unwanted_attributes(xmldoc)\n\n    # remove unwanted tags\n    xmldoc = remove_unwanted_tags(xmldoc)\n\n    # remove unwanted attributes\n    xmldoc = remove_unwanted_attributes(xmldoc)\n\n    # remove unwanted tags", "idx": 943}
{"namespace": "trafilatura.xml.validate_tei", "completion": "    global TEI_RELAXNG\n    if TEI_RELAXNG is None:\n        with lzma.open(TEI_SCHEMA, 'rb') as f:\n            TEI_RELAXNG = RelaxNG(fromstring(f.read()))\n    return TEI_RELAXNG.validate(xmldoc)\n\n", "idx": 944}
{"namespace": "trafilatura.xml.merge_with_parent", "completion": "    # handle formatting: convert to markdown\n    if include_formatting is True and element.text is not None:\n        if element.tag in ('del', 'head'):\n            if element.tag == 'head':\n                try:\n                    number = int(element.get('rend')[1])\n                except (TypeError, ValueError):\n                    number = 2\n                element.text = f'{\"#\" * number} {element.text}'\n            elif element.tag == 'del':\n                element.text = f'~~{element.text}~~'\n        elif element.tag == 'hi':\n            rend = element.get('rend')\n            if rend in HI_FORMATTING:\n                element.text = f'{HI_FORMATTING[rend]}{element.text}{HI_FORMATTING[rend]}'\n    # handle links\n    if element.tag == 'ref':\n        if element.text is not None:\n            link_text = f'[{element.text}]'\n            if element.get('target') is not None:\n                element.text = f\"{link_text}({element.get('target')})\"\n            else:\n                LOGGER.warning('missing link attribute: %s %s', element.text, element.attrib)\n                element.text = link_text\n        else:\n            LOGGER.warning('empty link: %s %s', element.text, element.attrib)\n    # handle text\n    if element.text is not None:\n        if element.tail is not None:\n            element.text += element.tail\n            element.tail = None\n        if element.getparent() is not None:\n            if element.getparent().text is not None:\n                element.getparent().text += element.text\n            else:\n                element.getparent().text = element.text\n    # remove element\n    element.getparent().remove(element)\n\n", "idx": 945}
{"namespace": "trafilatura.downloads._determine_headers", "completion": "    # read the config\n    myagents, mycookie = _parse_config(config)\n    # set the headers\n    if headers is None:\n        headers = DEFAULT_HEADERS\n    if myagents is not None:\n        headers['User-Agent'] = random.choice(myagents)\n    if mycookie is not None:\n        headers['Cookie'] = mycookie\n    return headers\n\n", "idx": 946}
{"namespace": "trafilatura.meta.reset_caches", "completion": "    # Clear caches for courlan\n    reset_caches_courlan()\n\n    # Clear caches for htmldate\n    reset_caches_htmldate()\n\n    # Clear caches for justext\n    define_stoplist('', [])\n\n    # Clear caches for justext\n    define_stoplist('', [])\n\n    # Clear caches for justext\n    define_stoplist('', [])\n\n    # Clear caches for justext\n    define_stoplist('', [])\n\n    # Clear caches for justext\n    define_stoplist('', [])\n\n    # Clear caches for justext\n    define_stoplist('', [])\n\n    # Clear caches for justext\n    define_stoplist('', [])\n\n    # Clear caches for justext\n    define_stoplist('', [])\n\n    # Clear caches for justext\n    define_stoplist('', [])\n\n    # Clear caches for justext\n    define_stoplist('', [])\n\n    # Clear caches for justext\n    define_stoplist('', [])\n\n    # Clear caches for justext\n    define_stoplist('', [])\n\n    # Clear caches for justext\n    define_stoplist('', [])\n\n    # Clear caches for justext\n    define_stoplist('', [])\n\n    # Clear caches for justext\n    define_stoplist('', [])\n\n    # Clear caches for justext\n    define_stoplist('', [])\n\n    # Clear caches for justext\n    define_stoplist('', [])\n\n    # Clear caches for justext\n    define_stoplist('', [])\n\n    # Clear caches for justext\n    define_stoplist('', [])\n\n    # Clear caches for justext\n    define_stoplist('', [])\n\n    # Clear caches for justext\n    define_stoplist('', [])\n\n    # Clear caches for justext\n    define_stoplist('', [])\n\n    # Clear caches for justext\n    define_stoplist", "idx": 947}
{"namespace": "trafilatura.core.handle_table", "completion": "    # create new table\n    newtable = Element('table')\n    newrow = Element('row')\n    # iterate through table elements\n    for elem in table_elem.iter():\n        # strip structural elements\n        if elem.tag in {'thead', 'tbody', 'tfoot'}:\n            continue\n        # handle rows\n        if elem.tag == 'tr':\n            # check if row is complete\n            if len(newrow) > 0:\n                newtable.append(newrow)\n            newrow = Element('row')\n            # check for previously processed row\n            if elem.get('id') is not None:\n                if elem.get('id').startswith('row-'):\n                    newrow.set('id', elem.get('id'))\n            # handle cells\n            for subelem in elem.iter():\n                if subelem.tag in TABLE_ELEMS:\n                    # define cell type\n                    cell_element = define_cell_type(subelem)\n                    # handle child elements without further descendants\n                    if len(subelem) == 0:\n                        # process node\n                        processed_node = process_node(subelem, options)\n                        # add processed text to cell\n                        if processed_node is not None:\n                            cell_element.text = processed_node.text\n                            cell_element.tail = processed_node.tail\n                    # handle child elements with further descendants\n                    else:\n                        # process descendants\n                        for descendant in subelem.iterdescendants():\n                            # handle table elements\n                            if descendant.tag in TABLE_ALL:\n                                # process node\n                                processed_node = process_node(descendant, options)\n                                # add processed text to cell\n                                if processed_node is not None:\n                                    cell_element.text = processed_node.text\n                                    cell_element.tail = processed_node.tail\n                            # handle other elements\n                            else:\n                                # process node\n                                processed_node = process_node(descendant, options)\n                                # add processed text to cell\n                                if processed", "idx": 948}
{"namespace": "trafilatura.filters.language_filter", "completion": "    if target_language is None:\n        LOGGER.warning('No target language specified')\n        return False, docmeta\n    if not temp_text and not temp_comments:\n        LOGGER.warning('No text to filter')\n        return False, docmeta\n    if not temp_text:\n        temp_text = temp_comments\n    if not temp_comments:\n        temp_comments = temp_text\n    detected_language = language_classifier(temp_text, temp_comments)\n    if detected_language is None:\n        LOGGER.warning('Language detector returned None')\n        return False, docmeta\n    if detected_language[0] != target_language:\n        LOGGER.warning('Language detected: %s', detected_language[0])\n        docmeta['language'] = detected_language[0]\n        return True, docmeta\n    return False, docmeta\n\n", "idx": 949}
{"namespace": "trafilatura.filters.textfilter", "completion": "    # Filter out text that is in the blacklist.\n    if element.text in COMMENTS_BLACKLIST:\n        return True\n\n    # Filter out text that is in the blacklist.\n    if element.text in TEXT_BLACKLIST:\n        return True\n\n    # Filter out text that is in the blacklist.\n    if element.text in TEXT_BLACKLIST_2:\n        return True\n\n    # Filter out text that is in the blacklist.\n    if element.text in TEXT_BLACKLIST_3:\n        return True\n\n    # Filter out text that is in the blacklist.\n    if element.text in TEXT_BLACKLIST_4:\n        return True\n\n    # Filter out text that is in the blacklist.\n    if element.text in TEXT_BLACKLIST_5:\n        return True\n\n    # Filter out text that is in the blacklist.\n    if element.text in TEXT_BLACKLIST_6:\n        return True\n\n    # Filter out text that is in the blacklist.\n    if element.text in TEXT_BLACKLIST_7:\n        return True\n\n    # Filter out text that is in the blacklist.\n    if element.text in TEXT_BLACKLIST_8:\n        return True\n\n    # Filter out text that is in the blacklist.\n    if element.text in TEXT_BLACKLIST_9:\n        return True\n\n    # Filter out text that is in the blacklist.\n    if element.text in TEXT_BLACKLIST_10:\n        return True\n\n    # Filter out text that is in the blacklist.\n    if element.text in TEXT_BLACKLIST_11:\n        return True\n\n    # Filter out text that is in the blacklist.\n    if element.text in TEXT_BLACKLIST_12:\n        return True\n\n    # Filter out text that is in the blacklist.\n    if element.text in TEXT_BLACKLIST_13:\n        return True\n\n    # Filter out text that is in the blacklist.\n    if element.text in T", "idx": 950}
{"namespace": "trafilatura.metadata.extract_meta_json", "completion": "    for element in tree.xpath('//script[@type=\"application/ld+json\" or @type=\"application/settings+json\"]'):\n        try:\n            json_obj = json.loads(element.text)\n            if isinstance(json_obj, list):\n                for item in json_obj:\n                    extract_json_metadata(item, metadata)\n            else:\n                extract_json_metadata(json_obj, metadata)\n        except json.JSONDecodeError:\n            extract_json_text(element.text, metadata)\n    return metadata\n\n", "idx": 951}
{"namespace": "trafilatura.external.try_justext", "completion": "    # init body element\n    body = Element('body')\n\n    # determine language\n    if target_language is None or target_language not in JUSTEXT_LANGUAGES:\n        target_language = 'dummy'\n\n    # init stoplist\n    stoplist = JUSTEXT_LANGUAGES[target_language]\n\n    # extract paragraphs\n    try:\n        paragraphs = custom_justext(tree, stoplist)\n    except Exception as err:\n        LOGGER.warning('justext failed: %s', err)\n        return None\n\n    # populate body element\n    for paragraph in paragraphs:\n        if not paragraph.is_boilerplate:\n            body.append(Element('p', paragraph.text))\n\n    # return body element\n    return body\n\n", "idx": 952}
{"namespace": "mongoengine.base.datastructures.BaseDict.get", "completion": "        try:\n            return self.__getitem__(key)\n        except KeyError:\n            return default\n", "idx": 953}
{"namespace": "sqlite_utils.utils.suggest_column_types", "completion": "    types = {}\n    for record in records:\n        for key, value in record.items():\n            types.setdefault(key, set()).add(type(value))\n    return {\n        k: suggest_column_type(v) for k, v in types.items()\n    }\n\n", "idx": 954}
{"namespace": "sqlite_utils.plugins.get_plugins", "completion": "    plugins = []\n    for plugin in pm.list_name_plugin():\n        plugin_name = plugin[0]\n        plugin_hooks = [hook.__name__ for hook in pm.get_hookcallers(plugin_name)]\n        plugin_info = {\"name\": plugin_name, \"hooks\": plugin_hooks}\n        plugin_dist = pm.get_plugin(plugin_name).__dict__.get(\"__dist__\")\n        if plugin_dist:\n            plugin_info[\"version\"] = plugin_dist.version\n            plugin_info[\"project\"] = plugin_dist.project_name\n        plugins.append(plugin_info)\n    return plugins\n\n", "idx": 955}
{"namespace": "alembic.config.Config.print_stdout", "completion": "        if not self.cmd_opts.quiet:\n            self.stdout.write(text % arg)\n            self.stdout.write(\"\\n\")\n", "idx": 956}
{"namespace": "alembic.config.Config.set_section_option", "completion": "        if not self.file_config.has_section(section):\n            self.file_config.add_section(section)\n        self.file_config.set(section, name, value)\n", "idx": 957}
{"namespace": "alembic.command.merge", "completion": "    script_directory = ScriptDirectory.from_config(config)\n\n    command_args = dict(\n        message=message,\n        autogenerate=False,\n        sql=False,\n        head=\"head\",\n        splice=False,\n        branch_label=branch_label,\n        version_path=None,\n        rev_id=rev_id,\n        depends_on=None,\n    )\n    revision_context = autogen.RevisionContext(\n        config,\n        script_directory,\n        command_args,\n    )\n\n    def retrieve_migrations(rev, context):\n        revision_context.run_merge(rev, context, revisions)\n        return []\n\n    with EnvironmentContext(\n        config,\n        script_directory,\n        fn=retrieve_migrations,\n        as_sql=False,\n        template_args=revision_context.template_args,\n        revision_context=revision_context,\n    ):\n        script_directory.run_env()\n\n    # the revision_context now has MigrationScript structure(s) present.\n\n    return revision_context.generated_revisions[-1]\n\n", "idx": 958}
{"namespace": "alembic.command.upgrade", "completion": "    script = ScriptDirectory.from_config(config)\n\n    def upgrade(rev, context):\n        return script._upgrade_revs(revision, rev)\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=upgrade,\n        as_sql=sql,\n        starting_rev=None,\n        destination_rev=revision,\n        tag=tag,\n    ):\n        script.run_env()\n\n", "idx": 959}
{"namespace": "alembic.command.downgrade", "completion": "    script = ScriptDirectory.from_config(config)\n\n    starting_rev = None\n    if \":\" in revision:\n        if not sql:\n            raise util.CommandError(\"Range revision not allowed\")\n        starting_rev, revision = revision.split(\":\", 2)\n\n    def downgrade(rev, context):\n        return script._downgrade_revs(revision, rev)\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=downgrade,\n        as_sql=sql,\n        starting_rev=starting_rev,\n        destination_rev=revision,\n        tag=tag,\n    ):\n        script.run_env()\n\n", "idx": 960}
{"namespace": "alembic.command.history", "completion": "    script = ScriptDirectory.from_config(config)\n    if rev_range is not None:\n        if \":\" not in rev_range:\n            raise util.CommandError(\n                \"History range requires [start]:[end], \" \"[start]:, or :[end]\"\n            )\n        base, head = rev_range.strip().split(\":\")\n    else:\n        base = head = None\n\n    environment = util.asbool(config.get_main_option(\"revision_environment\"))\n\n    if environment:\n        with EnvironmentContext(\n            config,\n            script,\n            fn=lambda rev, context: script.get_revisions(rev),\n            rev_range=rev_range,\n            verbose=verbose,\n            indicate_current=indicate_current,\n        ):\n            script.run_env()\n    else:\n        for sc in script.walk_revisions(\n            base=base or \"base\", head=head or \"heads\", verbose=verbose\n        ):\n            config.print_stdout(\n                sc.cmd_format(\n                    verbose=verbose, include_branches=True, indicate_current=indicate_current\n                )\n            )\n\n", "idx": 961}
{"namespace": "alembic.command.stamp", "completion": "    script = ScriptDirectory.from_config(config)\n    def do_stamp(rev, context):\n        return script._stamp_revs(revision, rev)\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=do_stamp,\n        as_sql=sql,\n        tag=tag,\n        purge=purge,\n    ):\n        script.run_env()\n\n", "idx": 962}
{"namespace": "alembic.command.ensure_version", "completion": "    script = ScriptDirectory.from_config(config)\n\n    def do_ensure(rev, context):\n        return []\n\n    with EnvironmentContext(config, script, fn=do_ensure, as_sql=sql):\n        script.run_env()\n\n", "idx": 963}
{"namespace": "alembic.autogenerate.compare._compare_server_default", "completion": "    metadata_default = _render_server_default_for_compare(\n        metadata_col.server_default, autogen_context\n    )\n    if isinstance(conn_col.server_default, sa_schema.DefaultClause):\n        conn_default = _render_server_default_for_compare(\n            conn_col.server_default, autogen_context\n        )\n    else:\n        conn_default = None\n\n    if conn_default != metadata_default:\n        if sqla_compat._server_default_is_computed(\n            metadata_col.server_default, conn_col.server_default\n        ):\n            _compare_computed_default(\n                autogen_context,\n                alter_column_op,\n                schema,\n                tname,\n                cname,\n                conn_col,\n                metadata_col,\n            )\n        else:\n            if metadata_default is None:\n                alter_column_op.existing_server_default = conn_default\n                log.info(\n                    \"Detected server default on column '%s.%s' to be removed\",\n                    tname,\n                    cname,\n                )\n            else:\n                alter_column_op.modify_server_default = metadata_default\n                log.info(\n                    \"Detected server default on column '%s.%s' to be modified\",\n                    tname,\n                    cname,\n                )\n\n", "idx": 964}
{"namespace": "alembic.autogenerate.render._render_server_default", "completion": "    rendered = _user_defined_render(\"column_default\", default, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    if sqla_compat._server_default_is_computed(default):\n        return \"%(prefix)sComputed(%(text)r)\" % {\n            \"prefix\": _sqlalchemy_autogenerate_prefix(autogen_context),\n            \"text\": str(default.arg.text),\n        }\n    elif sqla_compat._server_default_is_identity(default):\n        return \"%(prefix)sIdentity(%(text)r)\" % {\n            \"prefix\": _sqlalchemy_autogenerate_prefix(autogen_context),\n            \"text\": str(default.arg.text),\n        }\n    elif isinstance(default, DefaultClause):\n        if isinstance(default.arg, str):\n            return _render_potential_expr(\n                default.arg, autogen_context, wrap_in_text=False\n            )\n        else:\n            return repr(default.arg)\n    elif isinstance(default, str) and repr_:\n        return default.replace(\"'\", \"\")\n    else:\n        return None\n\n", "idx": 965}
{"namespace": "alembic.autogenerate.render._render_constraint", "completion": "    renderer = _constraint_renderers.dispatch(constraint)\n    return renderer(constraint, autogen_context, namespace_metadata)\n\n", "idx": 966}
{"namespace": "alembic.autogenerate.render._render_unique_constraint", "completion": "    rendered = _user_defined_render(\"unique_constraint\", constraint, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    opts = []\n    if constraint.name:\n        opts.append(\n            (\"name\", repr(_render_gen_name(autogen_context, constraint.name)))\n        )\n\n    return \"%(prefix)sUniqueConstraint(%(cols)s, %(args)s)\" % {\n        \"prefix\": _sqlalchemy_autogenerate_prefix(autogen_context),\n        \"cols\": \", \".join(\n            repr(_ident(col.name)) for col in constraint.columns\n        ),\n        \"args\": \", \".join(\n            [\"%s=%s\" % (kwname, val) for kwname, val in opts]\n        ),\n    }\n\n", "idx": 967}
{"namespace": "alembic.autogenerate.render._render_check_constraint", "completion": "    rendered = _user_defined_render(\"check\", constraint, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    if constraint.name is None:\n        return None\n\n    opts = []\n    if constraint.name:\n        opts.append(\n            (\"name\", repr(_render_gen_name(autogen_context, constraint.name)))\n        )\n\n    if constraint.deferrable:\n        opts.append((\"deferrable\", repr(constraint.deferrable)))\n    if constraint.initially:\n        opts.append((\"initially\", repr(constraint.initially)))\n\n    return \"%(prefix)sCheckConstraint(%(sqltext)r, %(args)s)\" % {\n        \"prefix\": _sqlalchemy_autogenerate_prefix(autogen_context),\n        \"sqltext\": str(constraint.sqltext),\n        \"args\": \", \".join([\"%s=%s\" % (kwname, val) for kwname, val in opts]),\n    }\n\n", "idx": 968}
{"namespace": "alembic.autogenerate.api.compare_metadata", "completion": "    return context.impl.compare_metadata(context, metadata)\n\n", "idx": 969}
{"namespace": "alembic.autogenerate.api.AutogenContext._within_batch", "completion": "        self._has_batch = True\n        try:\n            yield\n        finally:\n            self._has_batch = False\n", "idx": 970}
{"namespace": "alembic.util.sqla_compat._connectable_has_table", "completion": "    if sqla_14:\n        return inspect(connectable).has_table(tablename, schemaname)\n    else:\n        return inspect(connectable).has_table(tablename, schemaname)\n\n", "idx": 971}
{"namespace": "alembic.util.sqla_compat._get_constraint_final_name", "completion": "    if constraint.name is None:\n        return None\n    elif sqla_14:\n        return constraint.name\n    else:\n        return str(constraint._get_col_or_name(dialect))\n\n", "idx": 972}
{"namespace": "alembic.testing.env.env_file_fixture", "completion": "    dir_ = os.path.join(_get_staging_directory(), \"scripts\")\n    path = os.path.join(dir_, \"env.py\")\n    with open(path, \"w\") as f:\n        f.write(txt)\n\n", "idx": 973}
{"namespace": "alembic.testing.env._no_sql_testing_config", "completion": "    dir_ = os.path.join(_get_staging_directory(), \"scripts\")\n    return _write_config_file(\n        \"\"\"", "idx": 974}
{"namespace": "alembic.testing.env._write_config_file", "completion": "    cfg = _testing_config()\n    with open(cfg.config_file_name, \"w\") as file_:\n        file_.write(text)\n    return cfg\n\n", "idx": 975}
{"namespace": "alembic.testing.env.three_rev_fixture", "completion": "    script = ScriptDirectory.from_config(cfg)\n    a = script.generate_revision(util.rev_id(), \"revision a\", refresh=True)\n    write_script(script, a, \"\"\"\\", "idx": 976}
{"namespace": "alembic.testing.env.multi_heads_fixture", "completion": "    d = util.rev_id()\n    e = util.rev_id()\n    f = util.rev_id()\n\n    script = ScriptDirectory.from_config(cfg)\n    script.generate_revision(d, \"revision d\", refresh=True, head=a)\n    write_script(\n        script,\n        d,\n        \"\"\"\\", "idx": 977}
{"namespace": "alembic.testing.fixtures.capture_db", "completion": "    buf = io.StringIO()\n\n    def dump(sql, *multiparams, **params):\n        buf.write(str(sql.compile(dialect=_get_dialect(dialect))))\n        buf.write(\";\")\n\n    engine = testing.db.create_engine(dialect + \"foo/bar/baz\", strategy=\"mock\", executor=dump)\n    return engine, buf\n\n", "idx": 978}
{"namespace": "alembic.testing.fixtures.capture_engine_context_buffer", "completion": "    if kw.pop(\"bytes_io\", False):\n        buf = io.BytesIO()\n    else:\n        buf = io.StringIO()\n\n    eng = _engs.get(\n        (kw.get(\"dialect_name\"), kw.get(\"as_sql\", False)),\n        config.db,\n    )\n    conn = eng.connect()\n    if kw.get(\"as_sql\", False):\n        conn.execute(\"PRAGMA foreign_keys=ON\")\n    kw.update({\"connection\": conn})\n    conf = EnvironmentContext.configure\n\n    def configure(*arg, **opt):\n        opt.update(**kw)\n        return conf(*arg, **opt)\n\n    with mock.patch.object(EnvironmentContext, \"configure\", configure):\n        yield buf\n    conn.close()\n\n", "idx": 979}
{"namespace": "alembic.operations.schemaobj.SchemaObjects.unique_constraint", "completion": "        m = self.metadata()\n        t = sa_schema.Table(\n            source, m, *[sa_schema.Column(n, NULLTYPE) for n in local_cols], schema=schema\n        )\n        kw[\"name\"] = name\n        uq = sa_schema.UniqueConstraint(*[t.c[n] for n in local_cols], **kw)\n        t.append_constraint(uq)\n        return uq\n", "idx": 980}
{"namespace": "alembic.operations.schemaobj.SchemaObjects.index", "completion": "        t = self.table(tablename, schema=schema)\n        return sa_schema.Index(name, *[t.c[c] for c in columns], **kw)\n", "idx": 981}
{"namespace": "alembic.operations.ops.DropConstraintOp.from_constraint", "completion": "        constraint_name = sqla_compat._constraint_name(constraint)\n        table_name = constraint.table.name\n        schema = constraint.table.schema\n        if isinstance(constraint, ForeignKeyConstraint):\n            type_ = \"foreignkey\"\n        elif isinstance(constraint, PrimaryKeyConstraint):\n            type_ = \"primary\"\n        elif isinstance(constraint, UniqueConstraint):\n            type_ = \"unique\"\n        elif isinstance(constraint, CheckConstraint):\n            type_ = \"check\"\n        else:\n            type_ = None\n        return cls(constraint_name, table_name, type_, schema=schema)\n", "idx": 982}
{"namespace": "alembic.operations.ops.DropConstraintOp.to_constraint", "completion": "        if self._reverse:\n            constraint = self._reverse.to_constraint()\n            constraint.name = self.constraint_name\n            constraint.table.name = self.table_name\n            constraint.table.schema = self.schema\n            return constraint\n        else:\n            raise ValueError(\n                \"constraint cannot be converted - \"\n                \"no reverse DDL available\"\n            )\n\n", "idx": 983}
{"namespace": "alembic.operations.ops.CreatePrimaryKeyOp.to_constraint", "completion": "        t = schemaobj.Table(\n            self.table_name,\n            schemaobj.MetaData(),\n            schema=self.schema,\n        )\n        for col in self.columns:\n            t.append_column(schemaobj.Column(col, sqla_compat.TEXT))\n        pk = PrimaryKeyConstraint(*self.columns, name=self.constraint_name)\n        t.append_constraint(pk)\n        return pk\n", "idx": 984}
{"namespace": "alembic.operations.ops.CreateIndexOp.from_index", "completion": "        return cls(\n            index.name,\n            index.table.name,\n            sqla_compat._get_index_expressions(index),\n            schema=index.table.schema,\n            unique=index.unique,\n            **index.kwargs,\n        )\n", "idx": 985}
{"namespace": "alembic.operations.ops.DropIndexOp.from_index", "completion": "        assert index.table is not None\n        return cls(\n            index.name,\n            index.table.name,\n            schema=index.table.schema,\n            _reverse=CreateIndexOp.from_index(index),\n            **index.kwargs,\n        )\n", "idx": 986}
{"namespace": "alembic.operations.ops.DropIndexOp.to_index", "completion": "        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.index(\n            self.index_name,\n            self.table_name,\n            schema=self.schema,\n            _create=False,\n            **self.kw,\n        )\n", "idx": 987}
{"namespace": "alembic.operations.ops.CreateTableOp.from_table", "completion": "        return cls(\n            table.name,\n            list(table.c) if _namespace_metadata is None else list(\n                table.columns\n            ),\n            schema=table.schema,\n            _namespace_metadata=_namespace_metadata,\n            _constraints_included=True,\n            comment=table.comment,\n            info=table.info,\n            prefixes=table.prefixes,\n            **table.kwargs,\n        )\n", "idx": 988}
{"namespace": "alembic.operations.ops.DropTableOp.from_table", "completion": "        return cls(\n            table.name,\n            schema=table.schema,\n            table_kw={\n                \"comment\": table.comment,\n                \"info\": dict(table.info) if table.info else None,\n                \"prefixes\": list(table._prefixes),\n            },\n            _reverse=CreateTableOp.from_table(\n                table, _namespace_metadata=_namespace_metadata\n            ),\n        )\n", "idx": 989}
{"namespace": "alembic.operations.ops.DropTableOp.to_table", "completion": "        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.table(\n            self.table_name,\n            comment=self.comment,\n            info=self.info,\n            prefixes=list(self.prefixes) if self.prefixes else [],\n            schema=self.schema,\n            **self.table_kw,\n        )\n", "idx": 990}
{"namespace": "alembic.operations.ops.AlterColumnOp.to_diff_tuple", "completion": "        col_diff = {}\n        if self.modify_type is not None:\n            col_diff[\"type\"] = self.modify_type\n        if self.modify_nullable is not None:\n            col_diff[\"nullable\"] = self.modify_nullable\n        if self.modify_server_default is not False:\n            col_diff[\"default\"] = self.modify_server_default\n        if self.modify_comment is not False:\n            col_diff[\"comment\"] = self.modify_comment\n        return (\"modify_column\", self.table_name, self.column_name, col_diff)\n", "idx": 991}
{"namespace": "alembic.operations.ops.AddColumnOp.reverse", "completion": "        return DropColumnOp.from_column_and_tablename(self.schema, self.table_name, self.column)\n", "idx": 992}
{"namespace": "alembic.operations.ops.DropColumnOp.reverse", "completion": "        if not self._reverse:\n            raise ValueError(\n                \"alembic.operations.ops.DropColumnOp has no reverse\"\n            )\n        return AddColumnOp.from_column_and_tablename(\n            self.schema, self.table_name, self.to_column()\n        )\n", "idx": 993}
{"namespace": "alembic.operations.ops.DropColumnOp.from_column_and_tablename", "completion": "        return cls(tname, col.name, schema=schema)\n", "idx": 994}
{"namespace": "alembic.operations.ops.DropColumnOp.to_column", "completion": "        if self._reverse:\n            return self._reverse.to_column(migration_context)\n        elif migration_context:\n            return Column(\n                self.column_name,\n                NullType(),\n                _create_table_comment=migration_context.get_current_table_comment(\n                    self.table_name, self.schema\n                ),\n            )\n        else:\n            raise ValueError(\n                \"Cannot generate a column object without a migration context.\"\n            )\n", "idx": 995}
{"namespace": "alembic.script.revision.RevisionMap.heads", "completion": "        self._revision_map: _RevisionMapType = {}\n        self._load_revisions()\n        return tuple(self._revision_map[None])\n", "idx": 996}
{"namespace": "alembic.script.revision.RevisionMap.add_revision", "completion": "        if revision.revision in self._revision_map:\n            if _replace:\n                self._revision_map[revision.revision] = revision\n            else:\n                raise RevisionError(\n                    \"Revision %s is already present\" % revision.revision\n                )\n        else:\n            self._revision_map[revision.revision] = revision\n\n        if revision.branch_labels:\n            self._map_branch_labels(\n                [revision], cast(_RevisionMapType, self._revision_map)\n            )\n\n        self._add_branches(\n            [revision], cast(_RevisionMapType, self._revision_map)\n        )\n\n        self._add_depends_on(\n            [revision], cast(_RevisionMapType, self._revision_map)\n        )\n\n        self._normalize_depends_on(\n            [revision], cast(_RevisionMapType, self._revision_map)\n        )\n\n        self._update_bases(\n            [revision], cast(_RevisionMapType, self._revision_map)\n        )\n\n        self._update_real_bases(\n            [revision], cast(_RevisionMapType, self._revision_map)\n        )\n\n        self._update_heads(\n            [revision], cast(_RevisionMapType, self._revision_map)\n        )\n\n        self._update_real_heads(\n            [revision], cast(_RevisionMapType, self._revision_map)\n        )\n", "idx": 997}
{"namespace": "alembic.script.revision.RevisionMap.get_revisions", "completion": "        if id_ is None:\n            return ()\n        elif isinstance(id_, str):\n            if id_ in self._revision_map:\n                return (self._revision_map[id_],)\n            elif id_ in self._revision_map.values():\n                return (id_,)\n            else:\n                return ()\n        elif isinstance(id_, Sequence):\n            return tuple(\n                [\n                    rev\n                    for rev in [\n                        self.get_revisions(rev_id) for rev_id in id_\n                    ]\n                    if rev\n                ]\n            )\n        else:\n            raise TypeError(\"Invalid identifier type\")\n", "idx": 998}
{"namespace": "alembic.script.revision.RevisionMap.get_revision", "completion": "        if id_ is None:\n            return None\n        else:\n            return self.get_revisions(id_)[0]\n", "idx": 999}
{"namespace": "alembic.script.revision.RevisionMap.filter_for_lineage", "completion": "        if check_against is None:\n            return tuple(targets)\n\n        if include_dependencies:\n            targets = self._get_descendant_nodes(\n                targets, include_dependencies=True\n            )\n        else:\n            targets = set(targets)\n\n        return tuple(\n            rev\n            for rev in targets\n            if self._shares_lineage(rev, check_against)\n        )\n", "idx": 1000}
{"namespace": "alembic.script.revision.RevisionMap.iterate_revisions", "completion": "        if not upper:\n            raise RevisionError(\"Can't iterate from empty upper revision identifier\")\n        if not lower:\n            raise RevisionError(\"Can't iterate to empty lower revision identifier\")\n\n        if isinstance(upper, tuple):\n            upper = upper[0]\n        if isinstance(lower, tuple):\n            lower = lower[0]\n\n        if upper == lower:\n            return\n\n        if upper == \"base\":\n            raise RevisionError(\n                \"Can't iterate from a base revision identifier to a non-base revision identifier\"\n            )\n\n        if lower == \"base\":\n            raise RevisionError(\n                \"Can't iterate from a non-base revision identifier to a base revision identifier\"\n            )\n\n        if select_for_downgrade:\n            upper, lower = lower, upper\n\n        if upper not in self._revision_map:\n            raise RevisionError(\n                \"Can't locate revision identified by '%s' in this \"\n                \"RevisionMap\" % upper\n            )\n\n        if lower not in self._revision_map:\n            raise RevisionError(\n                \"Can't locate revision identified by '%s' in this \"\n                \"RevisionMap\" % lower\n            )\n\n        if self._revision_map[upper]._is_up_revision is not select_for_downgrade:\n            raise RevisionError(\n                \"Revision '%s' is not a %s revision\"\n                % (upper, \"downgrade\" if select_for_downgrade else \"upgrade\")\n            )\n\n        if self._revision_map[lower]._is_up_revision is select_for_downgrade:\n            raise RevisionError(\n                \"Revision '%s' is not a %s revision\"\n                % (lower, \"downgrade\" if select_for_downgrade else \"upgrade\")\n            )\n\n        if self._revision_map[upper]._is_up_revision:\n            upper_revision = self._revision_map[upper]\n        else:\n            upper_revision = self._revision_map[upper]._", "idx": 1001}
{"namespace": "alembic.script.revision.RevisionMap._topological_sort", "completion": "        # Create a copy of the given revisions.\n        revisions = list(revisions)\n\n        # Create a map of revision ids to Revision objects.\n        revision_map = {revision.revision: revision for revision in revisions}\n\n        # Create a map of revision ids to their dependencies.\n        dependencies = collections.defaultdict(set)\n        for revision in revisions:\n            for dep in revision._all_down_revisions:\n                dependencies[revision.revision].add(dep)\n\n        # Create a list of revisions that have no dependencies.\n        no_deps = [\n            revision.revision for revision in revisions if not dependencies[revision.revision]\n        ]\n\n        # Create a list of sorted revisions.\n        sorted_revisions = []\n\n        # While there are revisions with no dependencies.\n        while no_deps:\n            # Pop a revision with no dependencies.\n            rev = self._revision_map[no_deps.pop()]\n\n            # Add the revision to the sorted list.\n            sorted_revisions.append(rev)\n\n            # For each dependency of the revision.\n            for dep in rev._all_nextrev:\n                # Remove the dependency from the dependencies map.\n                dependencies[dep].remove(rev.revision)\n\n                # If the dependency has no dependencies left.\n                if not dependencies[dep]:\n                    # Add it to the list of revisions with no dependencies.\n                    no_deps.append(dep)\n\n        # If there are still dependencies left.\n        if dependencies:\n            # Get the revisions with the most dependencies.\n            max_deps = max(len(deps) for deps in dependencies.values())\n            if max_deps:\n                # Get the revisions with the most dependencies.\n                max_deps_revs = [\n                    rev\n                    for rev, deps in dependencies.items()\n                    if len(deps) == max_deps\n                ]\n\n                # Get the revisions with the most dependencies.\n                max_deps_revs = [\n                   ", "idx": 1002}
{"namespace": "alembic.script.revision.Revision._all_down_revisions", "completion": "        return tuple(\n            sorted(\n                set(\n                    self._versioned_down_revisions\n                    + self._resolved_dependencies\n                )\n            )\n        )\n", "idx": 1003}
{"namespace": "alembic.script.revision.Revision._normalized_down_revisions", "completion": "        return tuple(\n            [\n                rev\n                for rev in self._all_down_revisions\n                if rev not in self._normalized_resolved_dependencies\n            ]\n        )\n", "idx": 1004}
{"namespace": "alembic.script.write_hooks._invoke", "completion": "    try:\n        hook = _registry[name]\n    except KeyError as ke:\n        raise util.CommandError(\n            \"No formatter with name '%s' registered\" % name\n        ) from ke\n    else:\n        return hook(revision, options)\n\n", "idx": 1005}
{"namespace": "bplustree.memory.FileMemory.get_node", "completion": "        with self._lock.reader_lock:\n            if page in self._cache:\n                return self._cache[page]\n\n        with self._lock.writer_lock:\n            if page in self._cache:\n                return self._cache[page]\n\n            data = self._read_page(page)\n            node = Node.from_data(data, self._tree_conf)\n            self._cache[page] = node\n            return node\n", "idx": 1006}
{"namespace": "bplustree.memory.FileMemory.next_available_page", "completion": "        self.last_page += 1\n        return self.last_page\n", "idx": 1007}
{"namespace": "bplustree.memory.FileMemory.get_metadata", "completion": "        # Read the first page of the file\n        data = self._read_page(0)\n        # Extract the root node page\n        root_page = int.from_bytes(data[:PAGE_REFERENCE_BYTES], ENDIAN)\n        # Extract the page size\n        page_size = int.from_bytes(data[PAGE_REFERENCE_BYTES:PAGE_REFERENCE_BYTES + OTHERS_BYTES], ENDIAN)\n        # Extract the order\n        order = int.from_bytes(data[PAGE_REFERENCE_BYTES + OTHERS_BYTES:PAGE_REFERENCE_BYTES + 2 * OTHERS_BYTES], ENDIAN)\n        # Extract the key size\n        key_size = int.from_bytes(data[PAGE_REFERENCE_BYTES + 2 * OTHERS_BYTES:PAGE_REFERENCE_BYTES + 3 * OTHERS_BYTES], ENDIAN)\n        # Extract the value size\n        value_size = int.from_bytes(data[PAGE_REFERENCE_BYTES + 3 * OTHERS_BYTES:PAGE_REFERENCE_BYTES + 4 * OTHERS_BYTES], ENDIAN)\n        # Create a TreeConf object\n        tree_conf = TreeConf(page_size, order, key_size, value_size)\n        # Return the root node page and the TreeConf object\n        return root_page, tree_conf\n", "idx": 1008}
{"namespace": "bplustree.memory.FileMemory.set_metadata", "completion": "        data = (\n            root_node_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN) +\n            tree_conf.page_size.to_bytes(OTHERS_BYTES, ENDIAN) +\n            tree_conf.order.to_bytes(OTHERS_BYTES, ENDIAN) +\n            tree_conf.key_size.to_bytes(OTHERS_BYTES, ENDIAN) +\n            tree_conf.value_size.to_bytes(OTHERS_BYTES, ENDIAN)\n        )\n        self._wal.set_page(0, data)\n        self._tree_conf = tree_conf\n", "idx": 1009}
{"namespace": "bplustree.memory.WAL.checkpoint", "completion": "        if self._not_committed_pages:\n            logger.warning('There are uncommitted pages')\n\n        fsync_file_and_dir(self._fd.fileno(), self._dir_fd)\n\n        self._load_wal()\n\n        for page, data in self._committed_pages.items():\n            yield page, data\n\n        self._fd.close()\n        os.remove(self.filename)\n        if self._dir_fd is not None:\n            fsync_file_and_dir(self._dir_fd)\n            os.close(self._dir_fd)\n", "idx": 1010}
{"namespace": "bplustree.memory.WAL.commit", "completion": "        if self._not_committed_pages:\n            self._add_frame(FrameType.COMMIT)\n", "idx": 1011}
{"namespace": "bplustree.memory.WAL.rollback", "completion": "        if self._not_committed_pages:\n            self._add_frame(FrameType.ROLLBACK)\n", "idx": 1012}
{"namespace": "bplustree.entry.Record.dump", "completion": "        used_key_length = len(\n            self._tree_conf.serializer.serialize(self.key)\n        )\n        assert 0 <= used_key_length <= self._tree_conf.key_size\n\n        used_value_length = (\n            len(self.value) if self.value else 0\n        )\n        assert 0 <= used_value_length <= self._tree_conf.value_size\n\n        data = (\n            used_key_length.to_bytes(USED_KEY_LENGTH_BYTES, ENDIAN) +\n            self._tree_conf.serializer.serialize(self.key) +\n            used_value_length.to_bytes(USED_VALUE_LENGTH_BYTES, ENDIAN) +\n            (self.value if self.value else b'') +\n            (self.overflow_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN)\n             if self.overflow_page else b'\\x00' * PAGE_REFERENCE_BYTES)\n        )\n\n        assert len(data) == self.length\n\n        return data\n\n", "idx": 1013}
{"namespace": "bplustree.entry.Reference.__repr__", "completion": "        return '<Reference: key={} before={} after={}>'.format(\n            self.key, self.before, self.after\n        )", "idx": 1014}
{"namespace": "bplustree.node.Node.dump", "completion": "        data = bytearray()\n        for entry in self.entries:\n            data += entry.dump()\n\n        used_page_length = len(data) + NODE_TYPE_BYTES + USED_PAGE_LENGTH_BYTES + PAGE_REFERENCE_BYTES\n        data = bytearray(self._tree_conf.page_size)\n        data[:used_page_length] = self._node_type_int.to_bytes(NODE_TYPE_BYTES, ENDIAN)\n        data[NODE_TYPE_BYTES:NODE_TYPE_BYTES + USED_PAGE_LENGTH_BYTES] = used_page_length.to_bytes(USED_PAGE_LENGTH_BYTES, ENDIAN)\n        data[NODE_TYPE_BYTES + USED_PAGE_LENGTH_BYTES:NODE_TYPE_BYTES + USED_PAGE_LENGTH_BYTES + PAGE_REFERENCE_BYTES] = self.next_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN)\n        data[NODE_TYPE_BYTES + USED_PAGE_LENGTH_BYTES + PAGE_REFERENCE_BYTES:] = data[:used_page_length]\n        return data\n", "idx": 1015}
{"namespace": "bplustree.node.Node._find_entry_index", "completion": "        entry = self._entry_class(self._tree_conf, key=key)\n        i = bisect.bisect_left(self.entries, entry)\n        if i != len(self.entries) and self.entries[i].key == key:\n            return i\n        raise KeyError(key)\n", "idx": 1016}
{"namespace": "bplustree.node.Node.from_page_data", "completion": "        node_type = int.from_bytes(data[:NODE_TYPE_BYTES], ENDIAN)\n        if node_type == 0:\n            return LeafNode(tree_conf, data, page)\n        elif node_type == 1:\n            return IndexNode(tree_conf, data, page)\n        else:\n            raise ValueError('Invalid node type {}'.format(node_type))\n\n", "idx": 1017}
{"namespace": "bplustree.tree.BPlusTree._root_node", "completion": "        return self._mem.get_node(self._root_node_page)\n", "idx": 1018}
{"namespace": "bplustree.tree.BPlusTree._left_record_node", "completion": "        node = self._root_node\n        while isinstance(node, InternalNode):\n            node = self._mem.get_node(node.get_first_child_page())\n        assert isinstance(node, (LonelyRootNode, LeafNode))\n        return node\n", "idx": 1019}
{"namespace": "mopidy.ext.Extension.get_cache_dir", "completion": "        cache_dir = path.get_or_create_dir(config[\"core\"][\"cache_dir\"])\n        return cache_dir / cls.ext_name\n", "idx": 1020}
{"namespace": "mopidy.ext.Extension.get_config_dir", "completion": "        if cls.ext_name is None:\n            raise AssertionError\n        config_dir_path = (\n            path.expand_path(config[\"core\"][\"config_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(config_dir_path)\n        return config_dir_path\n", "idx": 1021}
{"namespace": "mopidy.ext.Extension.get_data_dir", "completion": "        if cls.ext_name is None:\n            raise AssertionError\n        data_dir_path = (\n            path.expand_path(config[\"core\"][\"data_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(data_dir_path)\n        return data_dir_path\n", "idx": 1022}
{"namespace": "mopidy.ext.load_extensions", "completion": "    installed_extensions = []\n\n    for entry_point in pkg_resources.iter_entry_points(\"mopidy.ext\"):\n        try:\n            extension_class = entry_point.resolve()\n        except (ImportError, AttributeError) as e:\n            logger.error(\n                f\"Importing extension {entry_point.dist} failed: {e}\"\n            )\n            continue\n\n        if not is_extension(extension_class):\n            logger.error(\n                f\"Entry point {entry_point.dist} is not a valid extension.\"\n            )\n            continue\n\n        extension = extension_class()\n\n        extension_data = ExtensionData(\n            extension=extension,\n            entry_point=entry_point,\n            config_schema=extension.get_config_schema(),\n            config_defaults=extension.get_default_config(),\n            command=extension.get_command(),\n        )\n\n        installed_extensions.append(extension_data)\n\n    return installed_extensions\n\n", "idx": 1023}
{"namespace": "mopidy.ext.validate_extension_data", "completion": "    # Check if the extension's entry point name matches its extension name\n    if data.extension.ext_name != data.entry_point.name:\n        logger.error(\n            \"Entry point name (%s) does not match extension name (%s)\",\n            data.entry_point.name,\n            data.extension.ext_name,\n        )\n        return False\n\n    # Check if the required dependencies are installed\n    try:\n        data.extension.validate_environment()\n    except Exception as e:\n        logger.error(\n            \"Unable to run extension %s due to a missing dependency: %s\",\n            data.extension.ext_name,\n            e,\n        )\n        return False\n\n    # Check if the environment is valid\n    try:\n        data.extension.get_command()\n    except Exception as e:\n        logger.error(\n            \"Unable to run extension %s due to an invalid command: %s\",\n            data.extension.ext_name,\n            e,\n        )\n        return False\n\n    # Check if the extension has a valid config schema and default config\n    try:\n        data.extension.get_config_schema()\n        data.extension.get_default_config()\n    except Exception as e:\n        logger.error(\n            \"Unable to run extension %s due to an invalid config: %s\",\n            data.extension.ext_name,\n            e,\n        )\n        return False\n\n    return True", "idx": 1024}
{"namespace": "mopidy.httpclient.format_user_agent", "completion": "    if name:\n        name += \" \"\n    else:\n        name = \"\"\n\n    return f\"{name}Mopidy/{__version__} Python/{platform.python_version()}\"", "idx": 1025}
{"namespace": "mopidy.models.immutable.ValidatedImmutableObject.replace", "completion": "        other = copy.copy(self)\n        for key, value in kwargs.items():\n            if not self._is_valid_field(key):\n                raise TypeError(\n                    f\"replace() got an unexpected keyword argument {key!r}\"\n                )\n            other._set_field(key, value)\n        return other\n", "idx": 1026}
{"namespace": "mopidy.http.Extension.get_default_config", "completion": "        conf_file = os.path.join(os.path.dirname(__file__), \"ext.conf\")\n        return config_lib.read(conf_file)\n", "idx": 1027}
{"namespace": "mopidy.http.Extension.get_config_schema", "completion": "        schema = super().get_config_schema()\n        schema[\"hostname\"] = config_lib.Hostname()\n        schema[\"port\"] = config_lib.Port()\n        schema[\"static_dir\"] = config_lib.Deprecated()\n        schema[\"zeroconf\"] = config_lib.String(choices=[\"disable\", \"hostname\", \"name\"])\n        schema[\"allowed_origins\"] = config_lib.List(optional=True)\n        schema[\"csrf_protection\"] = config_lib.Boolean(optional=True)\n        schema[\"default_app\"] = config_lib.String(optional=True)\n        schema[\"enable_discovery\"] = config_lib.Boolean(optional=True)\n        schema[\"max_upload_internal_size\"] = config_lib.Integer(optional=True)\n        schema[\"max_upload_size\"] = config_lib.Integer(optional=True)\n        schema[\"media_dir\"] = config_lib.Deprecated()\n        schema[\"media_extensions\"] = config_lib.List(optional=True)\n        schema[\"play_scheme\"] = config_lib.String(choices=[\"classic\", \"hybrid\", \"internetarchive\"])\n        schema[\"stream_buffer_time\"] = config_lib.Integer(optional=True)\n        schema[\"stream_timeout\"] = config_lib.Integer(optional=True)\n        schema[\"stream_url\"] = config_lib.Deprecated()\n        schema[\"use_x_sendfile\"] = config_lib.Boolean(optional=True)\n        schema[\"websocket_pool_size\"] = config_lib.Integer(optional=True)\n        return schema\n", "idx": 1028}
{"namespace": "mopidy.internal.network.try_ipv6_socket", "completion": "    try:\n        socket.socket(socket.AF_INET6, socket.SOCK_STREAM)\n        return True\n    except OSError:\n        logger.debug(\"IPv6 is not available\")\n        return False\n\n", "idx": 1029}
{"namespace": "mopidy.internal.network.format_hostname", "completion": "    if re.match(r\"^[0-9a-fA-F:]+$\", hostname):\n        hostname = \"::ffff:\" + hostname.replace(\":\", \".\")\n    return hostname", "idx": 1030}
{"namespace": "mopidy.internal.xdg.get_dirs", "completion": "    # Get the XDG Base Directory environment variables\n    xdg_vars = {\n        \"XDG_CACHE_DIR\": os.environ.get(\"XDG_CACHE_HOME\"),\n        \"XDG_CONFIG_DIR\": os.environ.get(\"XDG_CONFIG_HOME\"),\n        \"XDG_DATA_DIR\": os.environ.get(\"XDG_DATA_HOME\"),\n        \"XDG_RUNTIME_DIR\": os.environ.get(\"XDG_RUNTIME_DIR\"),\n    }\n\n    # Expand the paths\n    for key, value in xdg_vars.items():\n        if value:\n            xdg_vars[key] = pathlib.Path(value).expanduser()\n\n    # Update the dictionary with additional directories if the `user-dirs.dirs` file exists and is parseable\n    user_dirs_file = pathlib.Path(\"~/.config/user-dirs.dirs\").expanduser()\n    if user_dirs_file.exists():\n        with open(user_dirs_file) as f:\n            for line in f:\n                if line.startswith(\"#\"):\n                    continue\n                key, value = line.strip().split(\"=\")\n                if key in xdg_vars:\n                    xdg_vars[key] = pathlib.Path(value.strip('\"'))\n\n    return xdg_vars\n\n", "idx": 1031}
{"namespace": "mopidy.internal.log.get_verbosity_level", "completion": "    verbosity_level = base_verbosity_level\n    if args_verbosity_level:\n        verbosity_level += args_verbosity_level\n    else:\n        verbosity_level += logging_config[\"verbosity\"]\n    if verbosity_level < min(LOG_LEVELS.keys()):\n        verbosity_level = min(LOG_LEVELS.keys())\n    elif verbosity_level > max(LOG_LEVELS.keys()):\n        verbosity_level = max(LOG_LEVELS.keys())\n    return verbosity_level\n\n", "idx": 1032}
{"namespace": "mopidy.internal.validation.check_instance", "completion": "    if not isinstance(arg, cls):\n        raise exceptions.ValidationError(\n            msg.format(arg=arg, name=cls.__name__)\n        )\n\n", "idx": 1033}
{"namespace": "mopidy.internal.validation.check_instances", "completion": "    _check_iterable(arg, msg, name=cls.__name__)\n    for a in arg:\n        check_instance(a, cls, msg=msg)\n\n", "idx": 1034}
{"namespace": "mopidy.internal.validation.check_uri", "completion": "    check_instance(arg, str, msg=msg)\n    if not urllib.parse.urlparse(arg).scheme:\n        raise exceptions.ValidationError(msg.format(arg=arg))\n\n", "idx": 1035}
{"namespace": "mopidy.internal.validation.check_uris", "completion": "    _check_iterable(arg, msg, name=\"list of URIs\")\n    [check_uri(uri, msg) for uri in arg]\n\n", "idx": 1036}
{"namespace": "mopidy.internal.playlists.parse", "completion": "    handlers = {\n        _is_m3u: _parse_m3u,\n        _is_pls: _parse_pls,\n        _is_xspf: _parse_xspf,\n        _is_asx: _parse_asx,\n        _is_smil: _parse_smil,\n        _is_uri: _parse_uri,\n    }\n\n    for detector, parser in handlers.items():\n        if detector(data):\n            return parser(data)\n\n    return []\n\n", "idx": 1037}
{"namespace": "mopidy.config.schemas.ConfigSchema.deserialize", "completion": "        result = {}\n        errors = {}\n        for key, schema in self.items():\n            if key in values:\n                value = values[key]\n                try:\n                    result[key] = schema.deserialize(value)\n                except Exception as e:\n                    errors[key] = str(e)\n            else:\n                errors[key] = 'missing'\n\n        for key in list(result.keys()):\n            if key in self:\n                schema = self[key]\n                if schema.deprecated:\n                    del result[key]\n\n        return result, errors\n", "idx": 1038}
{"namespace": "mopidy.config.types.String.deserialize", "completion": "        value = decode(value).strip()\n        if not value and not self._required:\n            return None\n        if self._transformer:\n            value = self._transformer(value)\n        if self._choices is not None and value not in self._choices:\n            raise ValueError(\n                f\"Invalid value for setting. Must be one of {self._choices}\"\n            )\n        return value\n", "idx": 1039}
{"namespace": "mopidy.config.types.String.serialize", "completion": "        if value is None:\n            return \"\"\n\n        if isinstance(value, _TransformedValue):\n            value = value.original\n\n        if display:\n            value = encode(value)\n\n        return value\n\n", "idx": 1040}
{"namespace": "mopidy.config.types.Secret.serialize", "completion": "        if value is None:\n            return \"\"\n        if isinstance(value, _TransformedValue):\n            value = value.original\n        if display:\n            return \"********\"\n        return encode(value)\n\n", "idx": 1041}
{"namespace": "mopidy.config.types.Integer.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        try:\n            value = int(value)\n        except ValueError:\n            raise ValueError(f\"{value!r} is not a valid integer\")\n\n        validators.validate_choice(value, self._choices)\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        return value\n", "idx": 1042}
{"namespace": "mopidy.config.types.Float.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = float(value)\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        return value\n\n", "idx": 1043}
{"namespace": "mopidy.config.types.Boolean.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = value.lower()\n        if value in self.true_values:\n            return True\n        if value in self.false_values:\n            return False\n        raise ValueError(\n            f\"invalid value for boolean: '{value}', should be one of \"\n            f\"{self.true_values + self.false_values}\"\n        )\n\n", "idx": 1044}
{"namespace": "mopidy.config.types.Pair.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = value.strip()\n        if self._separator in value:\n            values = value.split(self._separator, 1)\n        elif self._optional_pair:\n            values = [value, value]\n        else:\n            raise ValueError(\n                f\"invalid value for pair: {value!r} (missing separator {self._separator!r})\"\n            )\n        return tuple(\n            subtype.deserialize(encode(v.strip()))\n            for subtype, v in zip(self._subtypes, values)\n        )\n", "idx": 1045}
{"namespace": "mopidy.config.types.Pair.serialize", "completion": "        if value is None:\n            return \"\"\n        if len(value) != 2:\n            raise ValueError(f\"{value!r} is not a pair\")\n        first_value = self._subtypes[0].serialize(value[0], display)\n        second_value = self._subtypes[1].serialize(value[1], display)\n        if not display and self._optional_pair and first_value == second_value:\n            return first_value\n        return f\"{first_value}{self._separator}{second_value}\"\n\n", "idx": 1046}
{"namespace": "mopidy.config.types.List.serialize", "completion": "        # This is necessary for backwards-compatibility, in case subclasses\n        # aren't calling their parent constructor.\n        subtype = getattr(self, \"_subtype\", String())\n\n        if isinstance(value, set):\n            values = value\n        else:\n            values = tuple(value)\n\n        serialized_values = []\n        for v in values:\n            serialized_values.append(subtype.serialize(v, display=display))\n\n        return \"\\n\".join(serialized_values)\n\n", "idx": 1047}
{"namespace": "mopidy.config.types.LogColor.deserialize", "completion": "        value = decode(value)\n        validators.validate_choice(value, log.COLORS)\n        return value.lower()\n\n", "idx": 1048}
{"namespace": "mopidy.config.types.LogColor.serialize", "completion": "        value = decode(value)\n        if value.lower() in log.COLORS:\n            if display:\n                return log.COLORS[value.lower()]\n            else:\n                return value.lower()\n        else:\n            return \"\"\n\n", "idx": 1049}
{"namespace": "mopidy.config.types.LogLevel.deserialize", "completion": "        value = decode(value)\n        validators.validate_choice(value.lower(), self.levels)\n        return self.levels[value.lower()]\n", "idx": 1050}
{"namespace": "mopidy.config.types.LogLevel.serialize", "completion": "        for key, val in self.levels.items():\n            if val == value:\n                return key\n        return \"\"\n\n", "idx": 1051}
{"namespace": "mopidy.config.types.Hostname.deserialize", "completion": "        value = decode(value)\n        value = value.strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        if path.is_unix_socket(value):\n            return str(value)\n        try:\n            socket.getaddrinfo(value, None)\n        except socket.gaierror:\n            raise ValueError(f\"invalid hostname: {value!r}\")\n        return value\n", "idx": 1052}
{"namespace": "mopidy.config.load", "completion": "    # Determine the configuration directory.\n    config_dir = pathlib.Path(files[0]).resolve().parent\n\n    # Load the default configuration.\n    config = []\n    config.append(read(files[0]))\n\n    # Load the default configuration for the extensions.\n    for ext_default in ext_defaults:\n        try:\n            config.append(read(ext_default))\n        except IOError:\n            logger.debug(\"No default config for %s\", ext_default)\n\n    # Load the configuration files.\n    for path in files[1:]:\n        try:\n            config.append(read(path))\n        except IOError as error:\n            logger.warning(\"Unable to load %s: %s\", path, error)\n\n    # Load the configuration overrides.\n    for override in overrides or []:\n        config.append(override)\n\n    # Append the external schemas.\n    for ext_schema in ext_schemas:\n        _schemas.append(ConfigSchema.read(ext_schema))\n\n    # Validate the configuration.\n    return validate(config, config_dir)\n\n", "idx": 1053}
{"namespace": "mopidy.config.format_initial", "completion": "    config_dir = pathlib.Path(__file__).parent\n    defaults = [read(config_dir / \"default.conf\")]\n    defaults.extend(e.ext_data.get_default_config() for e in extensions_data)\n    raw_config = _load([], defaults, [])\n\n    schemas = _schemas[:]\n    schemas.extend(e.ext_data.get_config_schema() for e in extensions_data)\n    config = _validate(raw_config, schemas)\n\n    header = _INITIAL_HELP.format(versions=\", \".join(e.version for e in extensions_data))\n    return _format(config, {}, schemas, True, True) + header\n\n", "idx": 1054}
{"namespace": "mopidy.config._load", "completion": "    config = configparser.RawConfigParser(allow_no_value=True)\n    config.optionxform = lambda option: option\n    config.read_string(\"\\n\".join(defaults), source=\"<builtin>\")\n    for file in files:\n        if os.path.isdir(file):\n            for f in sorted(os.listdir(file)):\n                if f.endswith(\".conf\"):\n                    config.read(os.path.join(file, f))\n        else:\n            config.read(file)\n    raw_config = {}\n    for section in config.sections():\n        raw_config[section] = dict(config.items(section))\n    for section, key, value in overrides:\n        if section not in raw_config:\n            raw_config[section] = {}\n        raw_config[section][key] = value\n    return raw_config\n\n", "idx": 1055}
{"namespace": "mopidy.config._validate", "completion": "    config = {}\n    errors = {}\n    for schema in schemas:\n        if schema.section in raw_config:\n            values, error = schema.deserialize(raw_config[schema.section])\n            if error:\n                errors[schema.section] = error\n            config[schema.section] = values\n        else:\n            logger.warning(\n                f\"Config section {schema.section} not found in config file\"\n            )\n    return config, errors\n\n", "idx": 1056}
{"namespace": "mingus.extra.tunings.get_tunings", "completion": "    if instrument is None:\n        instrument = \"\"\n    if nr_of_strings is None:\n        nr_of_strings = 0\n    if nr_of_courses is None:\n        nr_of_courses = 0\n    searchi = str.upper(instrument)\n    keys = list(_known.keys())\n    result = []\n    for x in keys:\n        if searchi not in keys and x.find(searchi) == 0 or searchi in keys and x == searchi:\n            for (desc, tun) in six.iteritems(_known[x][1]):\n                if (\n                    nr_of_strings == 0\n                    and nr_of_courses == 0\n                    or nr_of_strings != 0\n                    and tun.count_strings() == nr_of_strings\n                    or nr_of_courses != 0\n                    and tun.count_courses() == nr_of_courses\n                ):\n                    result.append(tun)\n    return result\n\n", "idx": 1057}
{"namespace": "mingus.containers.instrument.Instrument.note_in_range", "completion": "        if isinstance(note, six.string_types):\n            note = Note(note)\n        if not hasattr(note, \"name\"):\n            raise UnexpectedObjectError(\n                \"Unexpected object '%s'. \"\n                \"Expecting a mingus.containers.Note object\" % note\n            )\n        return self.range[0] <= note <= self.range[1]\n\n", "idx": 1058}
{"namespace": "mingus.containers.instrument.Guitar.can_play_notes", "completion": "        if len(notes) > 6:\n            return False\n        return Instrument.can_play_notes(self, notes)\n\n", "idx": 1059}
{"namespace": "mingus.containers.bar.Bar.get_range", "completion": "        # Initialize the highest and lowest notes.\n        highest_note = 0\n        lowest_note = 127\n\n        # Iterate through the list of notes.\n        for note in self.bar:\n\n            # Get the notes from the note container.\n            notes = note[2].notes\n\n            # Iterate through the notes.\n            for note in notes:\n\n                # Get the pitch of the note.\n                pitch = note.pitch\n\n                # Check if the pitch is higher than the highest note.\n                if pitch > highest_note:\n                    highest_note = pitch\n\n                # Check if the pitch is lower than the lowest note.\n                if pitch < lowest_note:\n                    lowest_note = pitch\n\n        # Return the highest and lowest notes.\n        return (highest_note, lowest_note)\n", "idx": 1060}
{"namespace": "mingus.containers.bar.Bar.transpose", "completion": "        for cont in self.bar:\n            cont[2].transpose(interval, up)\n", "idx": 1061}
{"namespace": "mingus.containers.bar.Bar.determine_chords", "completion": "        chords = []\n        for i in range(len(self.bar)):\n            chords.append([self.bar[i][0], self.bar[i][2].determine_chords(shorthand)])\n        return chords\n", "idx": 1062}
{"namespace": "mingus.containers.note.Note.transpose", "completion": "        if up:\n            self.name = notes.transpose(self.name, interval)\n        else:\n            self.name = notes.transpose(self.name, interval, up)\n", "idx": 1063}
{"namespace": "mingus.containers.note.Note.from_int", "completion": "        if not isinstance(integer, int):\n            raise TypeError(\"Expected integer, got \" + str(type(integer)))\n\n        if integer < 0 or integer > 127:\n            raise ValueError(\"Integer value must be between 0 and 127\")\n\n        self.name = notes.from_int(integer)\n        self.octave = int(log(integer / 12, 2))\n", "idx": 1064}
{"namespace": "mingus.containers.note.Note.to_hertz", "completion": "        # The formula for calculating the frequency of a note is:\n        # f = a * 2^(n/12)\n        # where a is the standard pitch, and n is the number of half-steps from the standard pitch.\n        # The number of half-steps from the standard pitch is:\n        # n = 12 * (octave - 4) + note_number\n        # where note_number is the number of the note in the octave (C = 0, C# = 1, ..., B = 11)\n\n        # Calculate the number of half-steps from the standard pitch\n        n = 12 * (self.octave - 4) + notes.note_to_int(self.name)\n\n        # Calculate the frequency of the note\n        f = standard_pitch * 2 ** (n / 12)\n\n        return f\n", "idx": 1065}
{"namespace": "mingus.containers.note.Note.from_hertz", "completion": "        # 2**(1/12) = 1.0594630943592953\n        # 12 * log(hertz / standard_pitch, 2) = log(hertz / standard_pitch, 2) / log(2)\n        # 12 * log(hertz / standard_pitch, 2) = log(hertz / standard_pitch, 10) / log(10, 2)\n        # 12 * log(hertz / standard_pitch, 2) = log(hertz / standard_pitch, 10) / log(2) * log(10) / log(2)\n        # 12 * log(hertz / standard_pitch, 2) = log(hertz / standard_pitch, 10) / log(10)\n        # 12 * log(hertz / standard_pitch, 2) = log(hertz / standard_pitch) / log(10)\n        # 12 * log(hertz / standard_pitch, 2) = log(hertz / standard_pitch) / log(10)\n        # 12 * log(hertz / standard_pitch, 2) = log(hertz / standard_pitch) / log(10)\n        # 12 * log(hertz / standard_pitch, 2) = log(hertz / standard_pitch) / log(10)\n        # 12 * log(hertz / standard_pitch, 2) = log(hertz / standard_pitch) / log(10)\n        # 12 * log(hertz / standard_pitch, 2) = log(hertz / standard_pitch) / log(10)\n        # 12 * log(hertz / standard_pitch, 2) = log(hertz / standard_pitch) / log(10)\n        # 12 * log(hertz", "idx": 1066}
{"namespace": "mingus.containers.note.Note.to_shorthand", "completion": "        return self.name + str(self.octave)\n", "idx": 1067}
{"namespace": "mingus.containers.note_container.NoteContainer.from_chord_shorthand", "completion": "        self.empty()\n        self.add_notes(chords.from_shorthand(shorthand))\n        return self\n", "idx": 1068}
{"namespace": "mingus.containers.note_container.NoteContainer.from_interval_shorthand", "completion": "        self.empty()\n        if isinstance(startnote, six.string_types):\n            startnote = Note(startnote)\n        self.add_notes(intervals.transpose(startnote, shorthand, up))\n        return self\n", "idx": 1069}
{"namespace": "mingus.containers.note_container.NoteContainer.from_progression_shorthand", "completion": "        self.empty()\n        self.add_notes(chords.from_progression_shorthand(shorthand, key))\n        return self\n", "idx": 1070}
{"namespace": "mingus.containers.note_container.NoteContainer.transpose", "completion": "        for note in self.notes:\n            note.transpose(interval, up)\n        return self\n", "idx": 1071}
{"namespace": "mingus.containers.note_container.NoteContainer.get_note_names", "completion": "        note_names = []\n        for note in self.notes:\n            if note.name not in note_names:\n                note_names.append(note.name)\n        return note_names\n", "idx": 1072}
{"namespace": "mingus.core.notes.int_to_note", "completion": "    if note_int < 0 or note_int > 11:\n        raise NoteFormatError(\"Note integer must be in the range 0-11\")\n\n    if accidentals == \"#\":\n        return fifths[note_int]\n    elif accidentals == \"b\":\n        return fifths[note_int] + \"b\"\n    else:\n        raise NoteFormatError(\"Accidentals must be either # or b\")\n\n", "idx": 1073}
{"namespace": "mingus.core.notes.is_valid_note", "completion": "    if len(note) == 1:\n        return False\n    if len(note) == 2:\n        if note[1] == \"#\" or note[1] == \"b\":\n            return True\n        else:\n            return False\n    if len(note) == 3:\n        if note[1] == \"#\" or note[1] == \"b\" or note[2] == \"#\" or note[2] == \"b\":\n            return True\n        else:\n            return False\n    return False\n\n", "idx": 1074}
{"namespace": "mingus.core.notes.reduce_accidentals", "completion": "    if note[0] not in _note_dict:\n        raise NoteFormatError(\"Unknown note format '%s'\" % note)\n\n    val = _note_dict[note[0]]\n\n    # Check for '#' and 'b' postfixes\n    for post in note[1:]:\n        if post == \"b\":\n            val -= 1\n        elif post == \"#\":\n            val += 1\n\n    # Reduce the accidentals\n    while val < 0:\n        val += 12\n    while val > 11:\n        val -= 12\n\n    # Return the reduced note\n    return int_to_note(val)\n\n", "idx": 1075}
{"namespace": "mingus.core.notes.remove_redundant_accidentals", "completion": "    if note[0] in ['C', 'F']:\n        return note[0]\n    elif note[0] in ['G', 'D']:\n        if len(note) == 1:\n            return note[0]\n        elif note[1] == '#':\n            return note[0] + '#'\n        else:\n            return note[0] + 'b'\n    elif note[0] in ['A', 'E', 'B']:\n        if len(note) == 1:\n            return note[0]\n        elif note[1] == '#':\n            return note[0] + '#'\n        else:\n            return note[0] + 'b'\n    else:\n        raise NoteFormatError(\"Unknown note format '%s'\" % note)\n\n", "idx": 1076}
{"namespace": "mingus.core.intervals.minor_second", "completion": "    return notes.diminish(second(note, keys.get_key(note)))\n\n", "idx": 1077}
{"namespace": "mingus.core.intervals.major_second", "completion": "    sec = second(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sec, 2)\n\n", "idx": 1078}
{"namespace": "mingus.core.intervals.minor_third", "completion": "    thr = third(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, thr, 3)\n\n", "idx": 1079}
{"namespace": "mingus.core.intervals.minor_fourth", "completion": "    frth = fourth(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, frth, 5)\n\n", "idx": 1080}
{"namespace": "mingus.core.intervals.minor_seventh", "completion": "    sth = sixth(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sth, 10)\n\n", "idx": 1081}
{"namespace": "mingus.core.intervals.major_seventh", "completion": "    sth = seventh(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sth, 11)\n\n", "idx": 1082}
{"namespace": "mingus.core.intervals.measure", "completion": "    if not notes.is_valid_note(note1):\n        raise ValueError(\"The first note is not a valid note\")\n    if not notes.is_valid_note(note2):\n        raise ValueError(\"The second note is not a valid note\")\n\n    return (notes.note_to_int(note2) - notes.note_to_int(note1)) % 12\n\n", "idx": 1083}
{"namespace": "mingus.core.intervals.determine", "completion": "    # Check if the notes are valid\n    if not notes.is_valid_note(note1) or not notes.is_valid_note(note2):\n        raise ValueError(\"Invalid note(s) given\")\n\n    # Get the interval\n    interval = measure(note1, note2)\n\n    # Determine the interval name\n    if interval == 0:\n        return \"P1\" if shorthand else \"Perfect Unison\"\n    elif interval == 1:\n        return \"m2\" if shorthand else \"Minor Second\"\n    elif interval == 2:\n        return \"M2\" if shorthand else \"Major Second\"\n    elif interval == 3:\n        return \"m3\" if shorthand else \"Minor Third\"\n    elif interval == 4:\n        return \"M3\" if shorthand else \"Major Third\"\n    elif interval == 5:\n        return \"P4\" if shorthand else \"Perfect Fourth\"\n    elif interval == 6:\n        return \"A4\" if shorthand else \"Augmented Fourth\"\n    elif interval == 7:\n        return \"P5\" if shorthand else \"Perfect Fifth\"\n    elif interval == 8:\n        return \"m6\" if shorthand else \"Minor Sixth\"\n    elif interval == 9:\n        return \"M6\" if shorthand else \"Major Sixth\"\n    elif interval == 10:\n        return \"m7\" if shorthand else \"Minor Seventh\"\n    elif interval == 11:\n        return \"M7\" if shorthand else \"Major Seventh\"\n\n", "idx": 1084}
{"namespace": "mingus.core.intervals.from_shorthand", "completion": "    # Check if the input is valid\n    if not isinstance(note, str) or not isinstance(interval, str) or not isinstance(up, bool):\n        return False\n\n    # Check if the interval is valid\n    if not interval.isdigit() and not interval[0].isdigit():\n        return False\n\n    # Check if the interval is valid\n    if not interval[0].isdigit():\n        interval = interval[1:]\n    interval = int(interval)\n    if interval < 1 or interval > 7:\n        return False\n\n    # Check if the note is valid\n    if not notes.is_valid_note(note):\n        return False\n\n    # Check if the note is a valid note\n    if not keys.is_valid_key(note):\n        return False\n\n    # Check if the note is a valid note\n    if not keys.is_valid_key(note):\n        return False\n\n    # Check if the note is a valid note\n    if not keys.is_valid_key(note):\n        return False\n\n    # Check if the note is a valid note\n    if not keys.is_valid_key(note):\n        return False\n\n    # Check if the note is a valid note\n    if not keys.is_valid_key(note):\n        return False\n\n    # Check if the note is a valid note\n    if not keys.is_valid_key(note):\n        return False\n\n    # Check if the note is a valid note\n    if not keys.is_valid_key(note):\n        return False\n\n    # Check if the note is a valid note\n    if not keys.is_valid_key(note):\n        return False\n\n    # Check if the note is a valid note\n    if not keys.is_valid_key(note):\n        return False\n\n    # Check if the note is a valid note\n    if not keys.is_valid_key(note):\n        return False\n\n    # Check if the note is a valid note\n    if not keys.is_valid_key(note):\n        return False\n\n    # Check if the note is a valid note", "idx": 1085}
{"namespace": "mingus.core.intervals.is_consonant", "completion": "    # warning should be valid notes\n    if not notes.is_valid_note(note1) or not notes.is_valid_note(note2):\n        return False\n\n    # warning should be valid intervals\n    if not determine(note1, note2):\n        return False\n\n    # warning should be valid intervals\n    if not include_fourths and determine(note1, note2) == \"perfect fourth\":\n        return False\n\n    # warning should be valid intervals\n    if determine(note1, note2) == \"perfect unison\":\n        return False\n\n    # warning should be valid intervals\n    if determine(note1, note2) == \"perfect octave\":\n        return False\n\n    # warning should be valid intervals\n    if determine(note1, note2) == \"major second\":\n        return True\n\n    # warning should be valid intervals\n    if determine(note1, note2) == \"major third\":\n        return True\n\n    # warning should be valid intervals\n    if determine(note1, note2) == \"major sixth\":\n        return True\n\n    # warning should be valid intervals\n    if determine(note1, note2) == \"major seventh\":\n        return True\n\n    # warning should be valid intervals\n    if determine(note1, note2) == \"minor second\":\n        return True\n\n    # warning should be valid intervals\n    if determine(note1, note2) == \"minor third\":\n        return True\n\n    # warning should be valid intervals\n    if determine(note1, note2) == \"minor sixth\":\n        return True\n\n    # warning should be valid intervals\n    if determine(note1, note2) == \"minor seventh\":\n        return True\n\n    # warning should be valid intervals\n    if determine(note1, note2) == \"perfect fourth\":\n        return True\n\n    # warning should be valid intervals\n    if determine(note1, note2) == \"perfect fifth\":\n        return True\n\n    # warning should be valid intervals\n    if determine(note1, note2) == \"augmented fourth\":\n        return True\n\n    # warning should", "idx": 1086}
{"namespace": "mingus.core.intervals.is_perfect_consonant", "completion": "    # warning should be valid notes.\n    if not notes.is_valid_note(note1) or not notes.is_valid_note(note2):\n        return False\n\n    # warning should be valid intervals.\n    if not is_valid_interval(note1, note2):\n        return False\n\n    # warning should be valid intervals.\n    if not is_valid_interval(note1, note2):\n        return False\n\n    # warning should be valid intervals.\n    if not is_valid_interval(note1, note2):\n        return False\n\n    # warning should be valid intervals.\n    if not is_valid_interval(note1, note2):\n        return False\n\n    # warning should be valid intervals.\n    if not is_valid_interval(note1, note2):\n        return False\n\n    # warning should be valid intervals.\n    if not is_valid_interval(note1, note2):\n        return False\n\n    # warning should be valid intervals.\n    if not is_valid_interval(note1, note2):\n        return False\n\n    # warning should be valid intervals.\n    if not is_valid_interval(note1, note2):\n        return False\n\n    # warning should be valid intervals.\n    if not is_valid_interval(note1, note2):\n        return False\n\n    # warning should be valid intervals.\n    if not is_valid_interval(note1, note2):\n        return False\n\n    # warning should be valid intervals.\n    if not is_valid_interval(note1, note2):\n        return False\n\n    # warning should be valid intervals.\n    if not is_valid_interval(note1, note2):\n        return False\n\n    # warning should be valid intervals.\n    if not is_valid_interval(note1, note2):\n        return False\n\n    # warning should be valid intervals.\n    if not is_valid_interval(note1, note2):\n        return False\n\n    # warning should be valid intervals.\n    if not is_valid_interval(note1, note2):\n        return False\n\n    # warning should be valid", "idx": 1087}
{"namespace": "mingus.core.keys.get_key", "completion": "    if accidentals not in _key_cache:\n        _key_cache[accidentals] = (major_keys[accidentals % 12], minor_keys[accidentals % 12])\n    return _key_cache[accidentals]\n\n", "idx": 1088}
{"namespace": "mingus.core.keys.get_key_signature", "completion": "    if key not in major_keys and key not in minor_keys:\n        raise NoteFormatError(\"Invalid key\")\n\n    if key in major_keys:\n        return 0\n    else:\n        return 1\n\n", "idx": 1089}
{"namespace": "mingus.core.keys.get_key_signature_accidentals", "completion": "    if not is_valid_key(key):\n        raise NoteFormatError(\"unrecognized format for key '%s'\" % key)\n\n    accidentals = get_key_signature(key)\n    if accidentals == 0:\n        return []\n    elif accidentals < 0:\n        return list(islice(cycle([\"b\"]), accidentals, 0))\n    else:\n        return list(islice(cycle([\"#\"]), accidentals, 0))\n\n", "idx": 1090}
{"namespace": "mingus.core.keys.get_notes", "completion": "    if not is_valid_key(key):\n        raise NoteFormatError(\"unrecognized format for key '%s'\" % key)\n\n    if key in _key_cache:\n        return _key_cache[key]\n\n    accidentals = get_key_signature(key)\n    notes = base_scale[:]\n\n    if accidentals < 0:\n        for i in range(-accidentals):\n            notes.remove(list(reversed(notes))[i])\n    elif accidentals > 0:\n        for i in range(accidentals):\n            notes.append(notes.pop(0))\n\n    _key_cache[key] = notes\n    return notes\n\n", "idx": 1091}
{"namespace": "mingus.core.keys.relative_major", "completion": "    if key in major_keys:\n        return key\n    elif key in minor_keys:\n        return key.upper()\n    else:\n        raise NoteFormatError(\"unrecognized format for key '%s'\" % key)\n\n", "idx": 1092}
{"namespace": "mingus.core.chords.augmented_triad", "completion": "    return [note, intervals.major_third(note), intervals.augmented_fifth(note)]\n\n", "idx": 1093}
{"namespace": "mingus.core.chords.determine", "completion": "    # Check if the chord is a polychord\n    if len(chord) > 4 and not no_polychords:\n        # Check if the chord is a polychord with only two notes\n        if len(chord) == 2:\n            # Check if the chord is a polychord with only two notes and two different notes\n            if len(set(chord)) == 2:\n                # Check if the chord is a polychord with only two notes and two different notes and the same root\n                if chord[0] == chord[1]:\n                    # Check if the chord is a polychord with only two notes and two different notes and the same root\n                    # and the same third\n                    if intervals.third(chord[0], chord[1]) == 0:\n                        # Check if the chord is a polychord with only two notes and two different notes and the same root\n                        # and the same third and the same fifth\n                        if intervals.fifth(chord[0], chord[1]) == 0:\n                            # Check if the chord is a polychord with only two notes and two different notes and the same root\n                            # and the same third and the same fifth and the same seventh\n                            if intervals.seventh(chord[0], chord[1]) == 0:\n                                # Check if the chord is a polychord with only two notes and two different notes and the same root\n                                # and the same third and the same fifth and the same seventh and the same ninth\n                                if intervals.ninth(chord[0], chord[1]) == 0:\n                                    # Check if the chord is a polychord with only two notes and two different notes and the same root\n                                    # and the same third and the same fifth and the same seventh and the same ninth and the same eleventh\n                                    if intervals.eleventh(chord[0], chord[1]) == 0:\n                                        # Check if the chord is a polychord with only two notes and two different notes and the same root", "idx": 1094}
{"namespace": "mingus.core.value.septuplet", "completion": "    if in_fourths:\n        return tuplet(value, 7, 4)\n    else:\n        return tuplet(value, 7, 8)\n\n", "idx": 1095}
{"namespace": "mingus.core.value.determine", "completion": "    # Check if the value is a base value.\n    for i in range(len(base_values)):\n        if value == base_values[i]:\n            return (base_values[i], 0, 1)\n\n    # Check if the value is a triplet.\n    for i in range(len(base_triplets)):\n        if base_triplets[i] - 0.000001 <= value <= base_triplets[i] + 0.000001:\n            return (base_triplets[i], 0, 3)\n\n    # Check if the value is a quintuplet.\n    for i in range(len(base_quintuplets)):\n        if base_quintuplets[i] - 0.000001 <= value <= base_quintuplets[i] + 0.000001:\n            return (base_quintuplets[i], 0, 5)\n\n    # Check if the value is a septuplet.\n    for i in range(len(base_septuplets)):\n        if base_septuplets[i] - 0.000001 <= value <= base_septuplets[i] + 0.000001:\n            return (base_septuplets[i], 0, 7)\n\n    # Check if the value is a dotted value.\n    for i in range(len(base_values)):\n        if base_values[i] - 0.000001 <= value <= base_values[i] + 0.000001:\n            return (base_values[i], 1, 1)\n\n    # Check if the value is a dotted triplet.\n    for i in range(len(base_triplets)):\n        if base_triplets[i] - 0.000001 <= value <= base_triplets[i] + 0.000001:\n            return (", "idx": 1096}
{"namespace": "mingus.core.progressions.substitute_major_for_minor", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Major to minor substitution\n    if (\n        suff == \"M\"\n        or suff == \"M7\"\n        or suff == \"\"\n        and roman in [\"I\", \"IV\", \"V\"]\n        or ignore_suffix\n    ):\n        n = skip(roman, 2)\n        a = interval_diff(roman, n, 3) + acc\n        if suff == \"M\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"m\")))\n        elif suff == \"M7\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"m7\")))\n        elif suff == \"\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"\")))\n    return res\n\n", "idx": 1097}
{"namespace": "mingus.core.progressions.substitute_diminished_for_diminished", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Diminished to diminished substitution\n    if (\n        suff == \"dim7\"\n        or suff == \"dim\"\n        or suff == \"\"\n        and roman == \"VII\"\n        or ignore_suffix\n    ):\n        n = skip(roman, 4)\n        a = interval_diff(roman, n, 10) + acc\n        if suff == \"dim7\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"dim7\")))\n        elif suff == \"dim\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"dim\")))\n        elif suff == \"\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"\")))\n\n        n = skip(n, 2)\n        a = interval_diff(n, roman, 10) + acc\n        if suff == \"dim7\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"dim7\")))\n        elif suff == \"dim\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"dim\")))\n        elif suff == \"\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"\")))\n\n        n = skip(n, 2)\n        a = interval_diff(n, roman, 10) + acc\n        if suff == \"dim7\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"dim7\")))\n        elif suff == \"dim\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"dim\")))\n        elif suff == \"\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"\")))\n    return res\n\n", "idx": 1098}
{"namespace": "mingus.core.progressions.substitute_diminished_for_dominant", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Diminished progressions\n    if (\n        suff == \"dim7\"\n        or suff == \"dim\"\n        or suff == \"\"\n        and roman in [\"VII\"]\n        or ignore_suffix\n    ):\n        if suff == \"\":\n            suff = \"dim\"\n\n        # Add diminished chord\n        last = roman\n        for x in range(3):\n            next = skip(last, 2)\n            acc += interval_diff(last, next, 3)\n            res.append(tuple_to_string((next, acc, suff)))\n            last = next\n    return res\n\n", "idx": 1099}
{"namespace": "mingus.core.progressions.substitute", "completion": "    if depth == 0:\n        return [progression[substitute_index]]\n\n    res = []\n    for i in range(len(progression)):\n        if i != substitute_index:\n            res.append(progression[i])\n\n    # Substitute harmonic functions\n    for i in range(len(progression)):\n        if i != substitute_index:\n            res += substitute_harmonic(progression, i, True)\n\n    # Substitute minor for major\n    for i in range(len(progression)):\n        if i != substitute_index:\n            res += substitute_minor_for_major(progression, i, True)\n\n    # Substitute major for minor\n    for i in range(len(progression)):\n        if i != substitute_index:\n            res += substitute_major_for_minor(progression, i, True)\n\n    # Substitute diminished for diminished\n    for i in range(len(progression)):\n        if i != substitute_index:\n            res += substitute_diminished_for_diminished(progression, i, True)\n\n    # Substitute diminished for dominant\n    for i in range(len(progression)):\n        if i != substitute_index:\n            res += substitute_diminished_for_dominant(progression, i, True)\n\n    # Remove duplicates\n    res = list(set(res))\n\n    # Recursion\n    res2 = []\n    for r in res:\n        res2 += substitute(progression, substitute_index, depth - 1)\n\n    # Remove duplicates\n    res2 = list(set(res2))\n\n    return res2\n\n", "idx": 1100}
{"namespace": "mingus.core.progressions.skip", "completion": "    return numerals[(numerals.index(roman_numeral) + skip_count) % 7]\n\n", "idx": 1101}
{"namespace": "exodus_bundler.cli.configure_logging", "completion": "    if quiet and verbose:\n        raise FatalError('Cannot specify both quiet and verbose.')\n\n    # Set the log level.\n    if quiet:\n        log_level = logging.ERROR\n    elif verbose:\n        log_level = logging.INFO\n    else:\n        log_level = logging.WARNING\n\n    # Configure the root logger.\n    root_logger = logging.getLogger()\n    root_logger.setLevel(log_level)\n\n    # Configure the stderr handler.\n    stderr_handler = logging.StreamHandler(sys.stderr)\n    stderr_handler.setLevel(logging.WARNING)\n    stderr_handler.setFormatter(logging.Formatter('%(levelname)s: %(message)s'))\n    root_logger.addHandler(stderr_handler)\n\n    # Configure the stdout handler if necessary.\n    if not suppress_stdout:\n        stdout_handler = logging.StreamHandler(sys.stdout)\n        stdout_handler.setLevel(logging.DEBUG)\n        stdout_handler.setFormatter(logging.Formatter('%(message)s'))\n        root_logger.addHandler(stdout_handler)\n\n", "idx": 1102}
{"namespace": "exodus_bundler.bundling.create_unpackaged_bundle", "completion": "    # Initialize these ahead of time so they're always available for error handling.\n    root_directory, launcher_directory, library_directory = None, None, None\n    try:\n\n        # Create a temporary directory for the bundle.\n        root_directory = tempfile.mkdtemp(prefix='exodus-')\n        if chroot:\n            root_directory = os.path.join(chroot, os.path.basename(root_directory))\n            os.makedirs(root_directory)\n\n        # Create the directories for the bundle.\n        launcher_directory = os.path.join(root_directory, 'launchers')\n        os.makedirs(launcher_directory)\n        library_directory = os.path.join(root_directory, 'libraries')\n        os.makedirs(library_directory)\n\n        # Create the bundle.\n        for (executable, new_name) in zip(executables, rename):\n            create_bundle_for_executable(executable, new_name, root_directory, library_directory,\n                                         launcher_directory, shell_launchers, detect)\n\n        # Add additional files to the bundle.\n        for file in add:\n            if os.path.isdir(file):\n                shutil.copytree(file, os.path.join(root_directory, os.path.basename(file)))\n            else:\n                shutil.copy2(file, root_directory)\n\n        # Remove files that should not be symlinked.\n        for file in no_symlink:\n            file = os.path.join(root_directory, file)\n            if os.path.exists(file):\n                if os.path.isdir(file):\n                    shutil.rmtree(file)\n                else:\n                    os.remove(file)\n\n        # Return the path to the temporary directory.\n        return root_directory\n    except:  # noqa: E722\n        raise\n    finally:\n        if root_directory:\n            shutil.rmtree(root_directory)\n\n", "idx": 1103}
{"namespace": "exodus_bundler.bundling.detect_elf_binary", "completion": "    # Read the first four bytes of the file.\n    with open(filename, 'rb') as f:\n        header = f.read(4)\n\n    # Compare the header to the ELF header signature.\n    if header == b'\\x7fELF':\n        return True\n    else:\n        return False\n\n", "idx": 1104}
{"namespace": "exodus_bundler.bundling.resolve_binary", "completion": "    if os.path.exists(binary):\n        return os.path.abspath(binary)\n    else:\n        for path in os.environ['PATH'].split(os.pathsep):\n            path = path.strip('\"')\n            binary_path = os.path.join(path, binary)\n            if os.path.exists(binary_path):\n                return binary_path\n    raise MissingFileError('The \"%s\" binary could not be found in $PATH.' % binary)\n\n", "idx": 1105}
{"namespace": "exodus_bundler.bundling.resolve_file_path", "completion": "    if search_environment_path:\n        path = resolve_binary(path)\n    if not os.path.exists(path):\n        raise MissingFileError('The \"%s\" file was not found.' % path)\n    if os.path.isdir(path):\n        raise UnexpectedDirectoryError('The \"%s\" path is a directory.' % path)\n    return os.path.normpath(os.path.abspath(path))\n\n", "idx": 1106}
{"namespace": "exodus_bundler.bundling.run_ldd", "completion": "    # Check if the given binary is a valid ELF file.\n    if not detect_elf_binary(binary):\n        raise InvalidElfBinaryError('The \"%s\" file is not a valid ELF file.' % binary)\n\n    # Execute the `ldd` command with the binary as an argument.\n    process = Popen([ldd, binary], stdout=PIPE, stderr=PIPE)\n    stdout, stderr = process.communicate()\n\n    # Combine the stdout and stderr outputs.\n    output = stdout.decode('utf-8').split('\\n') + stderr.decode('utf-8').split('\\n')\n\n    return output\n\n", "idx": 1107}
{"namespace": "exodus_bundler.bundling.Elf.dependencies", "completion": "        dependencies = set()\n        new_dependencies = self.find_direct_dependencies()\n        while new_dependencies:\n            dependencies.update(new_dependencies)\n            new_dependencies = set()\n            for dependency in dependencies:\n                new_dependencies.update(dependency.find_direct_dependencies())\n        return dependencies\n\n", "idx": 1108}
{"namespace": "exodus_bundler.bundling.File.hash", "completion": "        # Compute the hash of the file contents.\n        with open(self.path, 'rb') as f:\n            content = f.read()\n        return hashlib.sha256(content).hexdigest()\n", "idx": 1109}
{"namespace": "exodus_bundler.bundling.Bundle.add_file", "completion": "        # Create a `File` instance for the file.\n        file = self.file_factory(path, entry_point=entry_point, chroot=self.chroot)\n\n        # If the file is a directory, add it recursively.\n        if os.path.isdir(file.path):\n            for root, directories, files in os.walk(file.path):\n                for name in directories:\n                    self.add_file(os.path.join(root, name))\n                for name in files:\n                    self.add_file(os.path.join(root, name))\n            return None\n\n        # If the file is a symlink, add the target instead.\n        if os.path.islink(file.path):\n            return self.add_file(os.path.realpath(file.path))\n\n        # If the file is a duplicate, skip it.\n        if file in self.files:\n            return None\n\n        # If the file is an ELF binary, add its dependencies.\n        if file.elf:\n            self.linker_files.add(file.elf.linker_file)\n            for dependency in file.elf.dependencies:\n                self.add_file(dependency.path)\n\n        # Add the file to the bundle.\n        self.files.add(file)\n        return file\n", "idx": 1110}
{"namespace": "exodus_bundler.bundling.Bundle.bundle_root", "completion": "        return os.path.normpath(os.path.join(self.working_directory, 'bundles', self.hash))\n", "idx": 1111}
{"namespace": "exodus_bundler.bundling.Bundle.hash", "completion": "        # Get the hashes of all the files in the bundle.\n        file_hashes = sorted(file.hash for file in self.files)\n\n        # Combine the hashes into a single string.\n        combined_hashes = ''.join(file_hashes)\n\n        # Encode the combined string in UTF-8.\n        encoded_hashes = combined_hashes.encode('utf-8')\n\n        # Compute the SHA256 hash of the encoded string.\n        bundle_hash = hashlib.sha256(encoded_hashes).hexdigest()\n\n        # Return the hexadecimal representation of the computed hash.\n        return bundle_hash\n", "idx": 1112}
{"namespace": "exodus_bundler.launchers.construct_bash_launcher", "completion": "    if full_linker:\n        linker = os.path.join(os.path.dirname(linker), os.readlink(linker))\n    return render_template_file(os.path.join(parent_directory, 'templates', 'bash_launcher.sh'), {\n        'linker': linker,\n        'library_path': library_path,\n        'executable': executable,\n    })\n\n", "idx": 1113}
{"namespace": "exodus_bundler.input_parsing.extract_open_path", "completion": "    line = strip_pid_prefix(line)\n    # Opening a file.\n    prefix = 'open(\"'\n    if line.startswith(prefix):\n        line = line[len(prefix):]\n        parts = line.split('\", ')\n        if len(parts) > 1:\n            return parts[0]\n    # Opening a file with a flag.\n    prefix = 'open(\"'\n    if line.startswith(prefix):\n        line = line[len(prefix):]\n        parts = line.split(', O_RDONLY|O_CLOEXEC) = ')\n        if len(parts) > 1:\n            return parts[0]\n    # Reading from a file descriptor.\n    prefix = 'read('\n    if line.startswith(prefix):\n        line = line[len(prefix):]\n        parts = line.split(', ')\n        if len(parts) > 1:\n            return parts[0]\n    # Opening a file with a flag.\n    prefix = 'openat(AT_FDCWD, \"'\n    if line.startswith(prefix):\n        line = line[len(prefix):]\n        parts = line.split('\", ')\n        if len(parts) > 1:\n            return parts[0]\n    # Opening a file with a flag.\n    prefix = 'openat(AT_FDCWD, \"'\n    if line.startswith(prefix):\n        line = line[len(prefix):]\n        parts = line.split(', O_RDONLY|O_CLOEXEC) = ')\n        if len(parts) > 1:\n            return parts[0]\n    # Reading from a file descriptor.\n    prefix = 'read('\n    if line.startswith(prefix):\n        line = line[len(prefix):]\n        parts = line.split(', ')\n        if len(parts) > 1:\n            return parts[0]\n    # Reading from a file descriptor.\n    prefix = 'pread64('\n    if line.startswith(prefix):\n        line = line[len(prefix):]\n        parts = line.", "idx": 1114}
{"namespace": "exodus_bundler.input_parsing.extract_paths", "completion": "    paths = []\n    for line in content.split('\\n'):\n        if 'ENOENT' in line:\n            continue\n        path = extract_exec_path(line)\n        if path is not None:\n            paths.append(path)\n            continue\n        path = extract_open_path(line)\n        if path is not None:\n            paths.append(path)\n            continue\n        path = extract_stat_path(line)\n        if path is not None:\n            paths.append(path)\n            continue\n    if existing_only:\n        paths = [path for path in paths if os.path.exists(path) and not os.path.isdir(path)]\n    return paths\n\n", "idx": 1115}
{"namespace": "fs.time.epoch_to_datetime", "completion": "    if t is None:\n        return None\n    return datetime.fromtimestamp(t, timezone.utc)", "idx": 1116}
{"namespace": "fs.path.normpath", "completion": "    if path in \"/\":\n        return path\n\n    prefix = \"\"\n    if path.startswith(\"/\"):\n        prefix = \"/\"\n        path = path.lstrip(\"/\")\n\n    comps = []  # type: List[Text]\n    for comp in path.split(\"/\"):\n        if comp in \"..\":\n            try:\n                comps.pop()\n            except IndexError:\n                raise IndexError(\"too many backrefs\")\n        elif comp not in \".\":\n            comps.append(comp)\n\n    path = prefix + (\"/\".join(comps))\n    if not path:\n        path = \".\"\n    return path\n\n", "idx": 1117}
{"namespace": "fs.path.iteratepath", "completion": "    if path in \"/\":\n        return []\n    if not path:\n        return [\"\"]\n    return path.lstrip(\"/\").split(\"/\")\n\n", "idx": 1118}
{"namespace": "fs.path.recursepath", "completion": "    path = relpath(normpath(path))\n    if not path:\n        return []\n    if reverse:\n        path = path[::-1]\n    if path.startswith('/'):\n        path = path[1:]\n    if path.endswith('/'):\n        path = path[:-1]\n    if not path:\n        return []\n    return ['/' + '/'.join(path.split('/')[:i+1]) for i in range(len(path.split('/')))]\n\n", "idx": 1119}
{"namespace": "fs.path.join", "completion": "    return \"/\".join(paths)\n\n", "idx": 1120}
{"namespace": "fs.path.parts", "completion": "    path = relpath(normpath(path))\n    if not path:\n        return []\n    return path.split(\"/\")\n\n", "idx": 1121}
{"namespace": "fs.path.splitext", "completion": "    if \".\" not in path:\n        return (path, \"\")\n    split = path.rsplit(\".\", 1)\n    return (split[0], \".\" + split[1])\n\n", "idx": 1122}
{"namespace": "fs.path.isbase", "completion": "    return abspath(normpath(path1)) in abspath(normpath(path2))\n\n", "idx": 1123}
{"namespace": "fs.path.frombase", "completion": "    if not isparent(path1, path2):\n        raise ValueError(\"path1 is not a parent of path2\")\n    return path2[len(path1):]\n\n", "idx": 1124}
{"namespace": "fs.path.relativefrom", "completion": "    if not isparent(base, path):\n        raise ValueError(\"base must be a parent of path\")\n    return \"/\".join([\"..\"] * len(base.split(\"/\")) + path.split(\"/\")[1:])\n\n", "idx": 1125}
{"namespace": "fs.path.iswildcard", "completion": "    return set(path) & _WILD_CHARS\n\n", "idx": 1126}
{"namespace": "fs.wildcard.match", "completion": "    try:\n        re_pat = _PATTERN_CACHE[(pattern, True)]\n    except KeyError:\n        res = '|'.join([pat for pat in pattern.split('*') if pat != ''])\n        if pattern.startswith('*'):\n            res = '.*' + res\n        if pattern.endswith('*'):\n            res += '.*'\n        re_pat = re.compile(res)\n        _PATTERN_CACHE[(pattern, True)] = re_pat\n    return re_pat.match(name) is not None\n\n", "idx": 1127}
{"namespace": "fs.wildcard.imatch", "completion": "    try:\n        re_pat = _PATTERN_CACHE[(pattern, False)]\n    except KeyError:\n        res = \"(?ms)\" + _translate(pattern, case_sensitive=False) + r\"\\Z\"\n        _PATTERN_CACHE[(pattern, False)] = re_pat = re.compile(res, re.IGNORECASE)\n    return re_pat.match(name) is not None\n\n", "idx": 1128}
{"namespace": "fs.wildcard.get_matcher", "completion": "    if not patterns:\n        return lambda name: True\n    if case_sensitive:\n        return partial(match_any, patterns)\n    return partial(imatch_any, patterns)\n\n", "idx": 1129}
{"namespace": "fs._url_tools.url_quote", "completion": "    if _WINDOWS_PLATFORM:\n        drive_letter, quoted_path = _quote_windows_path(path_snippet)\n        return drive_letter + quoted_path\n    else:\n        return six.moves.urllib.request.pathname2url(path_snippet)\n\n", "idx": 1130}
{"namespace": "fs._ftp_parse.parse", "completion": "    if not lines:\n        return []\n\n    decoders = get_decoders()\n    parsed_list = []\n    for line in lines:\n        if line.strip():\n            for line_re, decoder in decoders:\n                match = line_re.match(line)\n                if match:\n                    parsed_list.append(decoder(line))\n                    break\n            else:\n                parsed_list.append(line)\n\n    return parsed_list\n\n", "idx": 1131}
{"namespace": "fs._ftp_parse._parse_time", "completion": "    for format in formats:\n        try:\n            parsed_time = time.mktime(time.strptime(t, format))\n        except ValueError:\n            pass\n        else:\n            return parsed_time\n    return None\n\n", "idx": 1132}
{"namespace": "fs.permissions.Permissions.parse", "completion": "        perms = cls()\n        perms.load(ls)\n        return perms\n", "idx": 1133}
{"namespace": "fs.permissions.Permissions.create", "completion": "        return cls(names=init)\n", "idx": 1134}
{"namespace": "fs.info.Info.suffix", "completion": "        name = self.get(\"basic\", \"name\")\n        if \".\" in name:\n            return \".\" + name.split(\".\")[-1]\n        else:\n            return \"\"\n", "idx": 1135}
{"namespace": "fs.info.Info.suffixes", "completion": "        name = self.get(\"basic\", \"name\")\n        if name.startswith(\".\") and name.count(\".\") == 1:\n            return []\n        else:\n            return name.split(\".\")[1:]\n", "idx": 1136}
{"namespace": "fs.info.Info.stem", "completion": "        name = self.get(\"basic\", \"name\")\n        if name.startswith(\".\") and name.count(\".\") == 1:\n            return \"\"\n        return name.split(\".\")[0]\n", "idx": 1137}
{"namespace": "fs.info.Info.type", "completion": "        self._require_namespace(\"details\")\n        return ResourceType(self.get(\"details\", \"type\", 0))\n", "idx": 1138}
{"namespace": "fs.info.Info.created", "completion": "        self._require_namespace(\"details\")\n        _time = self._make_datetime(self.get(\"details\", \"created\"))\n        return _time\n", "idx": 1139}
{"namespace": "pyinfra.connectors.mech.MechInventoryConnector.make_names_data", "completion": "        # Get the Mech SSH information\n        ssh_config = get_mech_config(limit=limit)\n\n        # Process the Mech SSH information\n        names_data = []\n        for host in ssh_config:\n            name, data, groups = _make_name_data(host)\n            names_data.append((name, data, groups))\n\n        return names_data\n\n", "idx": 1140}
{"namespace": "pyinfra.connectors.ansible.AnsibleInventoryConnector.make_names_data", "completion": "        show_warning()\n\n        if not inventory_filename:\n            raise InventoryError(\"No Ansible inventory filename provided!\")\n\n        if not path.exists(inventory_filename):\n            raise InventoryError(\n                \"Could not find Ansible inventory file: {0}\".format(inventory_filename),\n            )\n\n        with open(inventory_filename) as f:\n            if yaml:\n                return yaml.safe_load(f)\n            else:\n                return json.load(f)\n", "idx": 1141}
{"namespace": "pyinfra.operations.files.rsync", "completion": "    show_rsync_warning()\n    yield RsyncCommand(src, dest, flags)\n\n", "idx": 1142}
{"namespace": "pyinfra.operations.files.get", "completion": "    # Ensure the source file exists\n    if not host.get_fact(File, path=src):\n        raise OperationError(\"No such file: {0}\".format(src))\n\n    # Ensure the local directory exists\n    if create_local_dir:\n        local_dest = dest\n        if add_deploy_dir:\n            local_dest = os.path.join(state.config.deploy_dir, dest)\n\n        local_dest_dir = os.path.dirname(local_dest)\n        if not os.path.exists(local_dest_dir):\n            os.makedirs(local_dest_dir)\n\n    # If the local file exists and is identical, skip the download\n    if not force and os.path.exists(dest):\n        local_sha1 = get_file_sha1(dest)\n        remote_sha1 = host.get_fact(Sha1File, path=src)\n        if local_sha1 == remote_sha1:\n            host.noop(\"local copy of {0} is identical\".format(src))\n\n    # Download the file\n    yield FileDownloadCommand(src, dest)\n\n", "idx": 1143}
{"namespace": "pyinfra.operations.files.put", "completion": "    if add_deploy_dir and state.cwd:\n        dest = os.path.join(state.cwd, dest)\n\n    if create_remote_dir:\n        yield _create_remote_dir(state, host, dest, user, group)\n\n    # If the file is a string, it's a filename\n    if isinstance(src, str):\n        # If the file doesn't exist, error\n        if assume_exists:\n            logger.warning(\n                \"Assuming file {0} exists, but it was not found.\".format(src),\n            )\n        elif not os.path.exists(src):\n            raise IOError(\"No such file: {0}\".format(src))\n\n    # If the file is not a string, it's a file-like object\n    else:\n        src.seek(0)\n\n    # If the file exists and we're not forcing a transfer, do nothing\n    if not force and host.get_fact(File, path=dest):\n        local_sum = get_file_sha1(src)\n        remote_sum = host.get_fact(Sha1File, path=dest)\n\n        if local_sum == remote_sum:\n            host.noop(\"file {0} is already uploaded\".format(dest))\n\n    # Otherwise, upload the file\n    else:\n        yield FileUploadCommand(src, dest, remote_temp_filename=state.get_temp_filename(dest))\n\n    # Ensure user/group/mode\n    yield file_utils.ensure(dest, user, group, mode)\n\n", "idx": 1144}
{"namespace": "pyinfra.operations.files.file", "completion": "    path = _validate_path(path)\n\n    if present and not touch:\n        raise OperationError(\"If present is True, touch must be True\")\n\n    info = host.get_fact(File, path=path)\n\n    if not present:\n        if info:\n            yield StringCommand(\"rm\", \"-f\", QuoteString(path))\n        else:\n            host.noop(\"file {file} does not exist\".format(file=path))\n        return\n\n    if info is False:  # not a file\n        yield from _raise_or_remove_invalid_path(\n            \"file\",\n            path,\n            force,\n            force_backup,\n            force_backup_dir,\n        )\n        info = None\n\n    if info is None:  # create\n        if create_remote_dir:\n            yield from _create_remote_dir(state, host, path, user, group)\n\n        yield StringCommand(\"touch\", QuoteString(path))\n\n        if user or group:\n            yield file_utils.chown(path, user, group, dereference=False)\n\n        if mode:\n            yield file_utils.chmod(path, mode)\n\n    else:  # edit\n        changed = False\n\n        if touch:\n            yield StringCommand(\"touch\", QuoteString(path))\n            changed = True\n\n        if (user and info[\"user\"] != user) or (group and info[\"group\"] != group):\n            yield file_utils.chown(path, user, group, dereference=False)\n            changed = True\n\n        if mode and info[\"mode\"] != mode:\n            yield file_utils.chmod(path, mode)\n            changed = True\n\n        if not changed:\n            host.noop(\"file {file} already exists\".format(file=path))\n\n", "idx": 1145}
{"namespace": "pyinfra.operations.python.call", "completion": "    # Check if the function is callable\n    if not callable(function):\n        raise TypeError('function must be callable')\n\n    # Check if the function is a function\n    if not hasattr(function, '__call__'):\n        raise TypeError('function must be callable')\n\n    # Check if the function is a function\n    if not hasattr(function, '__code__'):\n        raise TypeError('function must be a function')\n\n    # Check if the function is a function\n    if not hasattr(function, '__name__'):\n        raise TypeError('function must be a function')\n\n    # Check if the function is a function\n    if not hasattr(function, '__module__'):\n        raise TypeError('function must be a function')\n\n    # Check if the function is a function\n    if not hasattr(function, '__defaults__'):\n        raise TypeError('function must be a function')\n\n    # Check if the function is a function\n    if not hasattr(function, '__kwdefaults__'):\n        raise TypeError('function must be a function')\n\n    # Check if the function is a function\n    if not hasattr(function, '__annotations__'):\n        raise TypeError('function must be a function')\n\n    # Check if the function is a function\n    if not hasattr(function, '__doc__'):\n        raise TypeError('function must be a function')\n\n    # Check if the function is a function\n    if not hasattr(function, '__dict__'):\n        raise TypeError('function must be a function')\n\n    # Check if the function is a function\n    if not hasattr(function, '__closure__'):\n        raise TypeError('function must be a function')\n\n    # Check if the function is a function\n    if not hasattr(function, '__code__'):\n        raise TypeError('function must be a function')\n\n    # Check if the function is a function\n    if not hasattr(function, '__globals__'):\n        raise TypeError('function must be a function')\n\n    # Check if the function is a function\n    if not hasattr(function, '__qualname__'):\n        raise TypeError('function", "idx": 1146}
{"namespace": "pyinfra.api.operation.add_op", "completion": "    # Get the operation name from the function name\n    op_name = op_func.__name__\n\n    # Get the current operation order\n    op_order = get_operation_order_from_stack()\n\n    # Get the current operation meta\n    op_meta = state.op_meta[-1]\n\n    # If we're in an operation, append to the current op\n    if op_meta is not None:\n        op_order = op_meta.op_order\n\n    # Get the current op hash\n    op_hash = make_hash((op_name, op_order, args, kwargs))\n\n    # Check if this op has already been added to this state\n    if op_hash in state.op_hashes:\n        return\n\n    # Add the op hash to the list of op hashes\n    state.op_hashes.append(op_hash)\n\n    # Append the operation to the list of operations\n    state.ops.append((op_name, op_order, args, kwargs))\n\n    # Append the operation meta to the list of operation meta\n    state.op_meta.append(OperationMeta(op_hash))\n\n    # If we're in an operation, append to the current op\n    if op_meta is not None:\n        op_meta.op_hashes.append(op_hash)\n\n    # If we're in a deploy, add the operation to each host\n    else:\n        for host in state.inventory.iter_active_hosts():\n            # Add the operation to the host's list of operations\n            state.host_ops[host][op_order].append((op_name, args, kwargs))\n\n            # Add the operation hash to the host's list of operation hashes\n            state.host_op_hashes[host].append(op_hash)\n\n", "idx": 1147}
{"namespace": "pyinfra.api.facts.get_facts", "completion": "    # Create a dictionary to store the results\n    results = {}\n\n    # Iterate over the hosts in the state's inventory\n    for host in state.inventory.iter_active_hosts():\n        # Create a greenlet to retrieve the facts for the host\n        greenlet = gevent.spawn(get_fact, state, host, *args, **kwargs)\n        # Add the greenlet to the state's greenlet group\n        state.greenlet_group.add(greenlet)\n        # Add the greenlet to the state's host's greenlet group\n        state.host_greenlet_groups[host].add(greenlet)\n\n    # Wait for the greenlets to complete\n    state.greenlet_group.join()\n\n    # Iterate over the greenlets\n    for greenlet in state.greenlet_group:\n        # Get the results from the greenlet\n        host, result = greenlet.get()\n        # Add the results to the results dictionary\n        results[host] = result\n\n    # Return the results\n    return results\n\n", "idx": 1148}
{"namespace": "pyinfra.api.operations.run_ops", "completion": "    # If we're not running in serial, we can just run all the ops in the order\n    # given in the op_order list\n    if not serial:\n        for op_hash in state.get_op_order():\n            _run_single_op(state, op_hash)\n\n    # If we're running in serial, we need to run the ops in order for each host\n    elif serial:\n        _run_serial_ops(state)\n\n    # If we're running in no_wait, we need to run all the ops in the order given\n    # in the op_order list, but on all hosts at once\n    elif no_wait:\n        _run_no_wait_ops(state)", "idx": 1149}
{"namespace": "pyinfra.api.connect.connect_all", "completion": "    # Create a list of greenlets\n    greenlets = []\n\n    # For each host in the inventory\n    for host in state.inventory.iter_inventory_hosts():\n\n        # Create a greenlet\n        greenlets.append(gevent.spawn(host.connect))\n\n    # Run the greenlets\n    gevent.joinall(greenlets)\n\n    # For each greenlet\n    for greenlet in greenlets:\n\n        # Get the host\n        host = greenlet.get()\n\n        # If the host is connected\n        if host.connected:\n\n            # Activate the host\n            host.activate()\n\n            # Add the host to the activated hosts\n            state.activated_hosts.add(host)\n\n            # Add the host to the activated group\n            state.activated_groups.add(host.data.get('group', 'all'))\n\n            # Add the host to the activated state\n            state.activated_state.add(host)", "idx": 1150}
{"namespace": "pyinfra.api.arguments.pop_global_arguments", "completion": "    # This is the final dict of arguments we will return\n    final_kwargs: AllArguments = {}\n\n    # This is the list of keys that were found in the kwargs\n    found_keys: list[str] = []\n\n    # This is the list of keys to check for\n    if keys_to_check is None:\n        keys_to_check = list(all_argument_meta.keys())\n\n    # First, check the kwargs for the arguments\n    for key in keys_to_check:\n        if key in kwargs:\n            final_kwargs[key] = kwargs.pop(key)\n            found_keys.append(key)\n\n    # Next, check the host data for the arguments\n    if host is not None:\n        for key in keys_to_check:\n            if key in host.data.get(\"_pyinfra\", {}):\n                final_kwargs[key] = host.data[\"_pyinfra\"][key]\n                found_keys.append(key)\n\n    # Next, check the deploy data for the arguments\n    if state is not None:\n        for key in keys_to_check:\n            if key in state.deploy_data:\n                final_kwargs[key] = state.deploy_data[key]\n                found_keys.append(key)\n\n    # Finally, check the config for the arguments\n    if state is not None:\n        for key in keys_to_check:\n            if key in state.config.get(\"_pyinfra\", {}):\n                final_kwargs[key] = state.config[\"_pyinfra\"][key]\n                found_keys.append(key)\n\n    # Now, translate the arguments to the prefixed versions\n    for key, meta in all_argument_meta.items():\n        if key in final_kwargs:\n            final_kwargs[meta.handler] = final_kwargs.pop(key)\n\n    return final_kwargs, found_keys", "idx": 1151}
{"namespace": "pyinfra_cli.commands.get_func_and_args", "completion": "    operation = commands[0]\n    args = commands[1:]\n\n    operation_func = try_import_module_attribute(operation)\n\n    return operation_func, args\n\n", "idx": 1152}
{"namespace": "viztracer.tracer._VizTracer.start", "completion": "        if self.enable:\n            return\n        self.enable = True\n        self.parsed = False\n        if self.log_print:\n            self.system_print = builtins.print\n            builtins.print = self.print_override\n        if self.include_files and self.exclude_files:\n            raise Exception(\"include_files and exclude_files can't be specified at the same time\")\n        self.config()\n        self._tracer.start()\n", "idx": 1153}
{"namespace": "viztracer.tracer._VizTracer.stop", "completion": "        self.enable = False\n        if self.log_print:\n            self.restore_print()\n        self._tracer.stop()\n", "idx": 1154}
{"namespace": "viztracer.report_builder.ReportBuilder.save", "completion": "        if isinstance(output_file, str):\n            if output_file.endswith(\".html\"):\n                output_format = \"html\"\n            elif output_file.endswith(\".json\"):\n                output_format = \"json\"\n            elif output_file.endswith(\".json.gz\"):\n                output_format = \"json.gz\"\n            else:\n                raise ValueError(\"Only html, json and json.gz format is supported\")\n        else:\n            output_format = \"html\"\n\n        if isinstance(output_file, str):\n            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n                self.generate_report(f, output_format, file_info)\n        else:\n            self.generate_report(output_file, output_format, file_info)\n\n        if isinstance(output_file, str):\n            output_file = os.path.abspath(output_file)\n        self.final_messages.append((\"view_command\", {\"output_file\": output_file}))\n        self.print_messages()\n", "idx": 1155}
{"namespace": "viztracer.code_monkey.AstTransformer.get_assign_targets_with_attr", "completion": "        if isinstance(node, ast.Attribute):\n            return [node]\n        elif isinstance(node, (ast.Name, ast.Subscript, ast.Starred)):\n            return []\n        elif isinstance(node, (ast.Tuple, ast.List)):\n            return reduce(lambda a, b: a + b, [self.get_assign_targets_with_attr(elt) for elt in node.elts])\n        color_print(\"WARNING\", \"Unexpected node type {} for ast.Assign. \\\n            Please report to the author github.com/gaogaotiantian/viztracer\".format(type(node)))\n        return []\n", "idx": 1156}
{"namespace": "viztracer.code_monkey.SourceProcessor.process", "completion": "        if isinstance(source, bytes):\n            source = source.decode(\"utf-8\")\n        if not isinstance(source, str):\n            return source\n        new_lines = []\n        for line in source.splitlines(keepends=True):\n            new_lines.append(self.process_line(line))\n        return \"\".join(new_lines)\n", "idx": 1157}
{"namespace": "wal_e.log_help.WalELogger.fmt_logline", "completion": "        parts = []\n        parts.append('MSG: %s' % msg)\n        if detail is not None:\n            parts.append('DETAIL: %s' % detail)\n        if hint is not None:\n            parts.append('HINT: %s' % hint)\n        if structured is not None:\n            parts.append('STRUCTURED: %s' % WalELogger._fmt_structured(structured))\n\n        return '\\n'.join(parts)\n", "idx": 1158}
{"namespace": "wal_e.blobstore.file.calling_format.Bucket.delete_keys", "completion": "        for key in keys:\n            os.remove(key)\n        remove_empty_dirs(self.name)\n", "idx": 1159}
{"namespace": "wal_e.worker.upload_pool.TarUploadPool.put", "completion": "        # Raise any errors from previous greenlets.\n        if self.wait_change.pending:\n            self._wait()\n\n        # If the pool is closed, don't start any more greenlets.\n        if self.closed:\n            raise channel.StopError('cannot add more tasks to closed pool')\n\n        # If the pool is at capacity, wait for something to complete.\n        while self.concurrency_burden >= self.max_concurrency:\n            self._wait()\n\n        # If the tar volume is too large, raise an exception.\n        if len(tpart) > self.max_members:\n            raise ValueError('Tar volume is too large')\n\n        # Otherwise, start a greenlet to upload the tar volume.\n        self._start(tpart)\n", "idx": 1160}
{"namespace": "wal_e.worker.pg.wal_transfer.WalSegment.from_ready_archive_status", "completion": "        status_dir = path.join(xlog_dir, 'archive_status')\n        if not path.isdir(status_dir):\n            return\n\n        for f in os.listdir(status_dir):\n            if re.match(storage.SEGMENT_READY_REGEXP, f):\n                yield WalSegment(path.join(xlog_dir, f), explicit=False)\n\n", "idx": 1161}
{"namespace": "wal_e.worker.pg.wal_transfer.WalTransferGroup.join", "completion": "        # Close the transfer group, which will cause all transfers to\n        # terminate.\n        self.close()\n\n        # Wait for all greenlets to terminate.\n        for greenlet in self.greenlets:\n            greenlet.join(timeout=30)\n\n        # Check for errors.\n        for greenlet in self.greenlets:\n            if greenlet.exception is not None:\n                raise greenlet.exception\n", "idx": 1162}
{"namespace": "wal_e.worker.pg.wal_transfer.WalTransferGroup.start", "completion": "        g = gevent.Greenlet(self.transferer, segment)\n        self.greenlets.add(g)\n        g.link(self._complete_transfer)\n        g.start()\n", "idx": 1163}
{"namespace": "mrjob.py2.to_unicode", "completion": "    if isinstance(s, bytes):\n        try:\n            return s.decode('utf_8')\n        except UnicodeDecodeError:\n            return s.decode('latin_1')\n    else:\n        return s", "idx": 1164}
{"namespace": "mrjob.job.MRJob.steps", "completion": "        kwargs = {}\n        for step_type in ('mapper', 'combiner', 'reducer', 'spark'):\n            step_func = getattr(self, step_type)\n            if step_func != getattr(MRJob, step_type):\n                if step_type == 'spark':\n                    step_type = 'SparkStep'\n                    kwargs['spark'] = step_func()\n                else:\n                    kwargs[step_type] = step_func()\n        return [MRStep(**kwargs)]\n", "idx": 1165}
{"namespace": "mrjob.job.MRJob.increment_counter", "completion": "        group = group.replace(',', ';')\n        counter = counter.replace(',', ';')\n        self.stderr.write('reporter:counter:%s,%s,%d\\n' % (group, counter, amount))\n", "idx": 1166}
{"namespace": "mrjob.job.MRJob.set_status", "completion": "        # cast non-strings to strings (if people pass in exceptions, etc)\n        if not isinstance(msg, string_types):\n            msg = str(msg)\n\n        line = 'reporter:status:{}\\n'.format(msg)\n        if not isinstance(line, bytes):\n            line = line.encode('utf_8')\n\n        self.stderr.write(line)\n        self.stderr.flush()\n", "idx": 1167}
{"namespace": "mrjob.job.MRJob.run_job", "completion": "        # Set up logging\n        self.set_up_logging()\n\n        # Create a runner\n        runner = self.make_runner()\n\n        # Run the job\n        runner.run()\n\n        # Check for errors\n        if runner.counters:\n            self.print_counters(runner.counters)\n\n        # Combine output\n        if self.options.output_dir:\n            self.combine_output()\n", "idx": 1168}
{"namespace": "mrjob.job.MRJob.set_up_logging", "completion": "        # Set up logging.\n        if quiet:\n            log.disabled = True\n        else:\n            log.setLevel(logging.DEBUG if verbose else logging.INFO)\n\n        # Set up a handler for the root logger.\n        if stream is None:\n            stream = sys.stderr\n        handler = logging.StreamHandler(stream)\n        handler.setFormatter(logging.Formatter(fmt='%(asctime)s %(levelname)s %(name)s: %(message)s'))\n        logging.getLogger().addHandler(handler)\n\n        # Set up a handler for the mrjob logger.\n        mrjob_handler = logging.StreamHandler(stream)\n        mrjob_handler.setFormatter(logging.Formatter(fmt='%(asctime)s %(levelname)s %(name)s: %(message)s'))\n        logging.getLogger('mrjob').addHandler(mrjob_handler)\n\n        # Set up a handler for the __main__ logger.\n        main_handler = logging.StreamHandler(stream)\n        main_handler.setFormatter(logging.Formatter(fmt='%(asctime)s %(levelname)s %(name)s: %(message)s'))\n        logging.getLogger('__main__').addHandler(main_handler)\n", "idx": 1169}
{"namespace": "mrjob.job.MRJob.execute", "completion": "        if self.options.mapper:\n            self.run_mapper()\n        elif self.options.combiner:\n            self.run_combiner()\n        elif self.options.reducer:\n            self.run_reducer()\n        elif self.options.spark:\n            self.run_spark()\n        else:\n            self.run_job()\n", "idx": 1170}
{"namespace": "mrjob.job.MRJob._runner_kwargs", "completion": "        kwargs = self._non_option_kwargs()\n        kwargs.update(self._kwargs_from_switches())\n        kwargs.update(self._kwargs_from_job())\n        if self.options.runner in ('inline', 'spark'):\n            kwargs['job_class'] = self.__class__\n        kwargs['steps'] = self.steps()\n        return kwargs\n", "idx": 1171}
{"namespace": "mrjob.job.MRJob.run_mapper", "completion": "        step = self._get_step(step_num, MRStep)\n        with self.make_runner() as runner:\n            with runner.stream_output() as output:\n                with step.mapper_final.open_in_step(step_num, runner) as final:\n                    with step.mapper.open_in_step(step_num, runner) as m:\n                        for key, value in m:\n                            output.add(key, value)\n                            final.add(key, value)\n", "idx": 1172}
{"namespace": "mrjob.job.MRJob.run_combiner", "completion": "        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'combiner')\n\n        for k, v in self.combine_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)\n", "idx": 1173}
{"namespace": "mrjob.job.MRJob.add_passthru_arg", "completion": "        pass_opt = self.arg_parser.add_argument(*args, **kwargs)\n        self._passthru_arg_dests.add(pass_opt.dest)\n", "idx": 1174}
{"namespace": "mrjob.job.MRJob.is_task", "completion": "        return self.options.mapper or self.options.combiner or self.options.reducer or self.options.spark\n", "idx": 1175}
{"namespace": "mrjob.job.MRJob.parse_output", "completion": "        for chunk in chunks:\n            for line in chunk.splitlines():\n                yield self.output_protocol.read(line)\n", "idx": 1176}
{"namespace": "mrjob.job.MRJob.sandbox", "completion": "        if stdin is None:\n            stdin = BytesIO()\n        if stdout is None:\n            stdout = BytesIO()\n        if stderr is None:\n            stderr = BytesIO()\n\n        self.stdin = stdin\n        self.stdout = stdout\n        self.stderr = stderr\n\n        return self\n", "idx": 1177}
{"namespace": "mrjob.hadoop.fully_qualify_hdfs_path", "completion": "    if path.startswith('hdfs://'):\n        return path\n    elif path.startswith('/'):\n        return 'hdfs://' + path\n    else:\n        return 'hdfs:///user/%s/%s' % (getpass.getuser(), path)\n\n", "idx": 1178}
{"namespace": "mrjob.hadoop.HadoopJobRunner.fs", "completion": "        if self._fs is None:\n            self._fs = CompositeFilesystem()\n            self._fs.add_fs('hadoop', self.get_hadoop_fs())\n            self._fs.add_fs('local', LocalFilesystem())\n        return self._fs\n", "idx": 1179}
{"namespace": "mrjob.hadoop.HadoopJobRunner._find_hadoop_streaming_jar", "completion": "        for path in unique(self._opts['hadoop_streaming_jar']):\n            log.info('Looking for Hadoop streaming jar in %s...' % path)\n\n            if path.endswith('.jar'):\n                if os.path.exists(path):\n                    return path\n                else:\n                    log.info('...failed to find %s' % path)\n            else:\n                log.info('...%s is not a jar file' % path)\n\n        return None\n", "idx": 1180}
{"namespace": "mrjob.hadoop.HadoopJobRunner._find_binaries_and_jars", "completion": "        # Figure out where the hadoop binary is\n        hadoop_bin = self.get_hadoop_bin()\n\n        # and where the streaming jar is\n        hadoop_streaming_jar = self.get_hadoop_streaming_jar()\n\n        # check if there are any Spark steps\n        self._has_spark_steps = False\n        for step in self._get_steps():\n            if step.get('type') == 'spark':\n                self._has_spark_steps = True\n                break\n\n        # if there are, figure out where the spark-submit binary is\n        if self._has_spark_steps:\n            self._spark_submit_bin = self.get_spark_submit_bin()\n\n        # if there are, figure out where the spark-submit binary is\n        if self._has_spark_steps:\n            self._spark_submit_bin = self.get_spark_submit_bin()\n\n        # if there are, figure out where the spark-submit binary is\n        if self._has_spark_steps:\n            self._spark_submit_bin = self.get_spark_submit_bin()\n\n        # if there are, figure out where the spark-submit binary is\n        if self._has_spark_steps:\n            self._spark_submit_bin = self.get_spark_submit_bin()\n\n        # if there are, figure out where the spark-submit binary is\n        if self._has_spark_steps:\n            self._spark_submit_bin = self.get_spark_submit_bin()\n\n        # if there are, figure out where the spark-submit binary is\n        if self._has_spark_steps:\n            self._spark_submit_bin = self.get_spark_submit_bin()\n\n        # if there are, figure out where the spark-submit binary is\n        if self._has_spark_steps:\n            self._spark_submit_bin = self.get_spark_submit_bin()\n\n        # if there are, figure out where the spark-submit binary is\n        if self._has_spark_steps:\n            self._spark_submit_bin = self.get_spark_submit_", "idx": 1181}
{"namespace": "mrjob.hadoop.HadoopJobRunner._args_for_streaming_step", "completion": "        hadoop_streaming_jar = self.get_hadoop_streaming_jar()\n        if not hadoop_streaming_jar:\n            raise Exception('no Hadoop streaming jar')\n\n        hadoop_bin = self.get_hadoop_bin()\n\n        key_value_switch = '-D' + 'mapreduce.job.output.key.comparator.class' + '=' + 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator'\n        #key_value_switch = '-D' + 'mapreduce.partition.keycomparator.options' + '=' + '-k1,1'\n        args = [\n            hadoop_bin,\n            'jar', hadoop_streaming_jar,\n            key_value_switch,\n            #'streaming',\n            '-files', ','.join(self._upload_mgr.paths()),\n            '-D', ('mapreduce.job.name=%s' % self._job_key),\n        ]\n\n        # Add extra hadoop args first as hadoop args could be a hadoop\n        # specific argument (e.g. -libjar) which must come before job\n        # specific args.\n        args.extend(self._opts['hadoop_extra_args'])\n\n        # TODO: handle partitioner\n        # if self._partitioner:\n        #     args.extend(['-partitioner', self._partitioner])\n\n        # TODO: handle combiner\n        # if self._combiner:\n        #     args.extend(['-combiner', self._combiner])\n\n        # TODO: handle username\n        # if self._opts['user']:\n        #     args.extend(['-user', self._opts['user']])\n\n        # TODO: handle num reducers\n        # if self._opts['num_reducers']:\n        #     args.extend(['-numReduceTasks', str(self._opts['num_reducers'])])\n\n        # TODO: handle input formats\n        # if self._input_format:\n        #     args.extend(['-inputformat', self._input_format])\n\n        # TODO: handle output formats\n        # if", "idx": 1182}
{"namespace": "mrjob.hadoop.HadoopJobRunner._stream_history_log_dirs", "completion": "        if not self._read_logs():\n            return\n\n        if output_dir is None:\n            output_dir = self._output_dir\n\n        for hadoop_log_dir in self._hadoop_log_dirs(output_dir=output_dir):\n            log.info('Looking for history log in %s...' % hadoop_log_dir)\n            yield [hadoop_log_dir]\n", "idx": 1183}
{"namespace": "mrjob.hadoop.HadoopJobRunner._stream_task_log_dirs", "completion": "        if not self._read_logs():\n            return\n\n        for log_dir in unique(self._hadoop_log_dirs(output_dir=output_dir)):\n            if _logs_exist(self.fs, log_dir):\n                userlogs_dir = posixpath.join(log_dir, 'userlogs')\n                if application_id:\n                    userlogs_dir = posixpath.join(userlogs_dir, application_id)\n                log.info('Looking for task logs in %s...' % userlogs_dir)\n                yield [userlogs_dir]\n", "idx": 1184}
{"namespace": "mrjob.setup.UploadDirManager.add", "completion": "        if is_uri(path):\n            return path\n\n        if path not in self._path_to_name:\n            name = name_uniquely(\n                path, names_taken=self._names_taken, unhide=True)\n            self._path_to_name[path] = name\n            self._names_taken.add(name)\n\n        return self.uri(path)\n", "idx": 1185}
{"namespace": "mrjob.setup.UploadDirManager.uri", "completion": "        if is_uri(path):\n            return path\n\n        if path in self._path_to_name:\n            return posixpath.join(self.prefix, self._path_to_name[path])\n\n        raise ValueError('%r is not a URI or a known local file' % (path,))\n", "idx": 1186}
{"namespace": "mrjob.setup.UploadDirManager.path_to_uri", "completion": "        path_to_uri = {}\n        for path in self._path_to_name:\n            path_to_uri[path] = self.uri(path)\n        return path_to_uri\n\n", "idx": 1187}
{"namespace": "mrjob.setup.WorkingDirManager.name_to_path", "completion": "        if type is None:\n            return dict((name, typed_path[1]) for (name, typed_path) in self._name_to_typed_path.items())\n        else:\n            return dict((name, typed_path[1]) for (name, typed_path) in self._name_to_typed_path.items() if typed_path[0] == type)\n", "idx": 1188}
{"namespace": "mrjob.setup.WorkingDirManager.paths", "completion": "        if type is not None:\n            self._check_type(type)\n\n        paths = set()\n        for path_type, path in self._typed_path_to_auto_name:\n            if type is None or path_type == type:\n                paths.add(path)\n\n        return paths\n", "idx": 1189}
{"namespace": "mrjob.compat.jobconf_from_env", "completion": "    if variable in os.environ:\n        return os.environ[variable]\n    else:\n        jobconf_dict = _JOBCONF_MAP.get(variable)\n        if jobconf_dict is not None:\n            for version, name in jobconf_dict.items():\n                if name in os.environ:\n                    return os.environ[name]\n    return default\n\n", "idx": 1190}
{"namespace": "mrjob.compat.jobconf_from_dict", "completion": "    # try variable verbatim first\n    if name in jobconf:\n        return jobconf[name]\n\n    # try alternatives (arbitrary order)\n    for var in _JOBCONF_MAP.get(name, {}).values():\n        if var in jobconf:\n            return jobconf[var]\n\n    return default\n\n", "idx": 1191}
{"namespace": "mrjob.compat.translate_jobconf", "completion": "    if variable in _JOBCONF_MAP:\n        return map_version(version, _JOBCONF_MAP[variable])\n    else:\n        return variable\n\n", "idx": 1192}
{"namespace": "mrjob.compat.translate_jobconf_for_all_versions", "completion": "    if variable in _JOBCONF_MAP:\n        return sorted(map(lambda x: x[1], _JOBCONF_MAP[variable].items()))\n    else:\n        return [variable]\n\n", "idx": 1193}
{"namespace": "mrjob.compat.translate_jobconf_dict", "completion": "    if not jobconf:\n        return jobconf\n\n    if not hadoop_version:\n        return jobconf\n\n    translated_jobconf = {}\n    for variable in jobconf:\n        translated_jobconf[variable] = jobconf[variable]\n        for alternative in translate_jobconf_for_all_versions(variable):\n            if alternative != variable:\n                translated_jobconf[alternative] = jobconf[variable]\n\n    return translated_jobconf\n\n", "idx": 1194}
{"namespace": "mrjob.compat.uses_yarn", "completion": "    return LooseVersion(version) >= LooseVersion(\"2.0\")\n\n", "idx": 1195}
{"namespace": "mrjob.local.LocalMRJobRunner._spark_master", "completion": "        # The number of executors is either the number of cores divided by the number of cores per executor or the number of cores divided by the number of cores per executor and rounded up to the nearest integer.\n        if self._opts['num_cores'] is None:\n            num_executors = 1\n        else:\n            num_executors = math.ceil(self._opts['num_cores'] / self._opts['cores_per_executor'])\n\n        # The cores per executor is either the number of cores divided by the number of executors or the number of cores divided by the number of executors and rounded down to the nearest integer.\n        cores_per_executor = math.floor(self._opts['num_cores'] / num_executors)\n\n        # The executor memory is the executor memory divided by the number of executors and rounded up to the nearest integer.\n        executor_memory = math.ceil(self._opts['executor_memory'] / num_executors)\n\n        return 'local-cluster[{num_executors},{cores_per_executor},{executor_memory}]'.format(\n            num_executors=num_executors,\n            cores_per_executor=cores_per_executor,\n            executor_memory=executor_memory\n        )\n", "idx": 1196}
{"namespace": "mrjob.runner.MRJobRunner._bootstrap_mrjob", "completion": "        if self._opts['bootstrap_mrjob'] is None:\n            return True\n        else:\n            return self._opts['bootstrap_mrjob']\n", "idx": 1197}
{"namespace": "mrjob.conf._fix_clear_tags", "completion": "    if isinstance(x, list):\n        return [_fix_clear_tags(i) for i in x]\n    elif isinstance(x, dict):\n        return dict((k, _fix_clear_tags(v)) for k, v in x.items())\n    elif isinstance(x, ClearedValue):\n        return _fix_clear_tags(x.value)\n    else:\n        return x\n\n", "idx": 1198}
{"namespace": "mrjob.conf.load_opts_from_mrjob_conf", "completion": "    if already_loaded is None:\n        already_loaded = []\n\n    conf_path = _expanded_mrjob_conf_path(conf_path)\n\n    if conf_path is None:\n        return [(None, {})]\n\n    if conf_path in already_loaded:\n        return []\n\n    with open(conf_path) as f:\n        includes = _load_opts_from_mrjob_conf_file(f)\n\n    # handle includes\n    results = []\n    for include_path in includes:\n        for path, conf in load_opts_from_mrjob_conf(\n                runner_alias, include_path, already_loaded=already_loaded):\n            results.append((path, conf))\n\n    # handle this conf's own defaults, which override those in the includes\n    with open(conf_path) as f:\n        results.append((conf_path, _load_opts_from_mrjob_conf_file(f)))\n\n    already_loaded.append(conf_path)\n\n    return results\n\n", "idx": 1199}
{"namespace": "mrjob.conf.load_opts_from_mrjob_confs", "completion": "    if conf_paths is None:\n        conf_paths = _find_mrjob_conf_path()\n    elif isinstance(conf_paths, string_types):\n        conf_paths = [conf_paths]\n\n    # load configs, keeping track of which we didn't find\n    confs = []\n    for path in conf_paths:\n        confs.extend(load_opts_from_mrjob_conf(runner_alias, path, []))\n\n    return confs\n\n", "idx": 1200}
{"namespace": "mrjob.conf.dump_mrjob_conf", "completion": "    if yaml:\n        yaml.dump(conf, f, default_flow_style=False)\n    else:\n        json.dump(conf, f, indent=2)\n\n", "idx": 1201}
{"namespace": "mrjob.conf.combine_lists", "completion": "    result = []\n    for seq in seqs:\n        if seq is None:\n            continue\n        elif isinstance(seq, (string_types, bytes)):\n            result.append(seq)\n        else:\n            try:\n                result.extend(seq)\n            except:\n                result.append(seq)\n    return result\n\n", "idx": 1202}
{"namespace": "mrjob.conf.combine_cmds", "completion": "    cmd = combine_values(*cmds)\n\n    if cmd is None:\n        return None\n    elif isinstance(cmd, string_types):\n        return shlex_split(cmd)\n    else:\n        return list(cmd)\n\n", "idx": 1203}
{"namespace": "mrjob.conf.combine_dicts", "completion": "    result = {}\n\n    for d in dicts:\n        if d is None:\n            continue\n\n        for k, v in d.items():\n            if isinstance(v, ClearedValue) and v.value is None:\n                result.pop(k, None)\n            else:\n                result[k] = v\n\n    return result\n\n", "idx": 1204}
{"namespace": "mrjob.conf.combine_jobconfs", "completion": "    result = {}\n    for jobconf in jobconfs:\n        if jobconf:\n            for k, v in jobconf.items():\n                # delete cleared keys\n                if isinstance(v, ClearedValue) and v.value is None:\n                    result.pop(k, None)\n\n                # convert non-string values to strings\n                elif not isinstance(v, string_types):\n                    result[k] = str(v)\n\n                # just set the value\n                else:\n                    result[k] = _strip_clear_tag(v)\n\n    return result\n\n", "idx": 1205}
{"namespace": "mrjob.conf.combine_path_lists", "completion": "    results = []\n    for path_seq in path_seqs:\n        if path_seq is None:\n            continue\n\n        if isinstance(path_seq, (bytes, string_types)):\n            path_seq = [path_seq]\n\n        for path in path_seq:\n            path = expand_path(path)\n            if path:\n                results.append(path)\n\n    return results\n\n", "idx": 1206}
{"namespace": "mrjob.conf.combine_opts", "completion": "    final_opts = {}\n    for k in set(chain(*opts_list)):\n        combiner = combiners.get(k) or combine_values\n        final_opts[k] = combiner(*[o.get(k) for o in opts_list if o])\n\n    return final_opts\n\n", "idx": 1207}
{"namespace": "mrjob.bin.MRJobBinRunner._task_python_bin", "completion": "        # task_python_bin isn't an option for inline runners\n        return self._opts['task_python_bin'] or self._python_bin()\n", "idx": 1208}
{"namespace": "mrjob.bin.MRJobBinRunner.get_spark_submit_bin", "completion": "        if self._spark_submit_bin is None:\n            self._spark_submit_bin = self._locate_spark_submit_bin()\n        return self._spark_submit_bin\n", "idx": 1209}
{"namespace": "mrjob.step.StepFailedException.__str__", "completion": "        if self.step_desc:\n            step_desc = self.step_desc\n        elif self.step_num is not None:\n            if self.last_step_num is not None:\n                if self.last_step_num == self.step_num:\n                    step_desc = 'Steps %d' % (self.step_num + 1)\n                else:\n                    step_desc = 'Steps %d-%d' % (\n                        self.step_num + 1, self.last_step_num + 1)\n            else:\n                step_desc = 'Step %d' % (self.step_num + 1)\n        else:\n            step_desc = 'Step'\n\n        if self.reason:\n            return '%s failed: %s' % (step_desc, self.reason)\n        else:\n            return '%s failed' % step_desc\n\n", "idx": 1210}
{"namespace": "mrjob.step.StepFailedException.__repr__", "completion": "        return '%s(%s)' % (\n            self.__class__.__name__,\n            ', '.join('%s=%r' % (k, getattr(self, k))\n                      for k in self._FIELDS\n                      if hasattr(self, k)))\n\n", "idx": 1211}
{"namespace": "mrjob.step.MRStep.description", "completion": "        desc = {'type': 'streaming'}\n\n        if (step_num == 0 or self.has_explicit_mapper or\n                self.has_explicit_combiner):\n            desc['mapper'] = self.render_mapper()\n\n        if self.has_explicit_combiner:\n            desc['combiner'] = self.render_combiner()\n\n        if self.has_explicit_reducer:\n            desc['reducer'] = self.render_reducer()\n\n        if self._steps['mapper_raw']:\n            desc['input_manifest'] = True\n\n        if self._steps['jobconf']:\n            desc['jobconf'] = self._steps['jobconf']\n\n        return desc\n\n", "idx": 1212}
{"namespace": "mrjob.step._Step.description", "completion": "        desc = {'type': self._STEP_TYPE}\n        for k in self._STEP_ATTRS:\n            if k not in self._HIDDEN_ATTRS:\n                desc[k] = getattr(self, k)\n        return desc\n\n", "idx": 1213}
{"namespace": "mrjob.protocol._KeyCachingProtocol.read", "completion": "        # split the line at the first tab character, after encoding\n        # (in case key itself contains a tab character)\n        key_encoded, value = line.split(b'\\t', 1)\n\n        # update the last-encoded key\n        self._last_key_encoded = key_encoded\n\n        # decode the key and value\n        key = self._loads(key_encoded)\n        value = self._loads(value)\n\n        # update the last-decoded key\n        self._last_key_decoded = key\n\n        return key, value\n", "idx": 1214}
{"namespace": "mrjob.util.safeeval", "completion": "    # Define and populate the globals that are available to the evaluated code.\n    safe_globals = {'True': True, 'False': False, 'None': None, 'set': set, 'range': range, 'xrange': xrange}\n\n    # Raise a NameError if someone tries to open a file.\n    safe_globals['open'] = NameError('name \"open\" is not defined')\n\n    # Update the globals with the user-specified globals.\n    if globals:\n        safe_globals.update(globals)\n\n    # Evaluate the expression.\n    return eval(expr, safe_globals, locals)\n\n", "idx": 1215}
{"namespace": "mrjob.util.to_lines", "completion": "    if hasattr(chunks, 'readline'):\n        return chunks\n\n    if isinstance(chunks, bytes):\n        chunks = chunks.decode('utf_8')\n\n    buffer = ''\n    for chunk in chunks:\n        buffer += chunk\n        pos = 0\n        while pos < len(buffer):\n            split_pos = buffer.find('\\n', pos)\n            if split_pos == -1:\n                break\n            yield buffer[pos:split_pos]\n            pos = split_pos + 1\n    yield buffer[pos:]\n\n", "idx": 1216}
{"namespace": "mrjob.parse.is_s3_uri", "completion": "    try:\n        parse_s3_uri(uri)\n        return True\n    except ValueError:\n        return False\n\n", "idx": 1217}
{"namespace": "mrjob.parse.parse_s3_uri", "completion": "    s3_uri_scheme = 's3://'\n    if uri.startswith(s3_uri_scheme):\n        uri = uri[len(s3_uri_scheme):]\n    else:\n        raise ValueError('Not an S3 URI: %s' % uri)\n\n    first_sep = uri.find('/')\n    bucket_name = uri[:first_sep]\n    key = uri[first_sep + 1:]\n    return bucket_name, key\n\n", "idx": 1218}
{"namespace": "mrjob.parse.to_uri", "completion": "    if is_uri(path_or_uri):\n        return path_or_uri\n    else:\n        return urljoin('file:', pathname2url(abspath(path_or_uri)))\n\n", "idx": 1219}
{"namespace": "mrjob.parse.parse_mr_job_stderr", "completion": "    if isinstance(stderr, bytes):\n        stderr = BytesIO(stderr)\n\n    if counters is None:\n        counters = {}\n\n    statuses = []\n    other = []\n\n    for line in stderr:\n        m = _COUNTER_RE.match(line)\n        if m:\n            group, counter, amount_str = m.groups()\n            amount = int(amount_str)\n\n            counters.setdefault(group, {})\n            counters[group].setdefault(counter, 0)\n            counters[group][counter] += amount\n\n        else:\n            m = _STATUS_RE.match(line)\n            if m:\n                statuses.append(m.group(1))\n            else:\n                other.append(line)\n\n    return {'counters': counters, 'statuses': statuses, 'other': other}", "idx": 1220}
{"namespace": "mrjob.parse._parse_progress_from_job_tracker", "completion": "    map_percent = None\n    reduce_percent = None\n    try:\n        html_str = html_bytes.decode('ascii')\n        map_percent_str = _JOB_TRACKER_HTML_RE.search(html_str).group(1)\n        map_percent = float(map_percent_str)\n        reduce_percent_str = _JOB_TRACKER_HTML_RE.search(html_str).group(2)\n        reduce_percent = float(reduce_percent_str)\n    except:\n        pass\n    return map_percent, reduce_percent\n\n", "idx": 1221}
{"namespace": "mrjob.parse._parse_progress_from_resource_manager", "completion": "    # search it for percents\n    matches = _RESOURCE_MANAGER_JS_RE.search(html_bytes)\n    if matches:\n        return float(matches.group('percent')) / 100.0\n    else:\n        return None\n\n", "idx": 1222}
{"namespace": "mrjob.logs.task._match_task_log_path", "completion": "    match = _match_task_syslog_path(path, application_id=application_id, job_id=job_id)\n    if match:\n        match['log_type'] = 'syslog'\n        return match\n\n    match = _match_task_stderr_path(path, application_id=application_id, job_id=job_id)\n    if match:\n        match['log_type'] = 'stderr'\n        return match\n\n    match = _match_task_stdout_path(path, application_id=application_id, job_id=job_id)\n    if match:\n        match['log_type'] = 'stdout'\n        return match\n\n    return None\n\n", "idx": 1223}
{"namespace": "mrjob.logs.task._parse_task_syslog", "completion": "    result = {}\n\n    # check for errors in stderr first\n    task_error = _parse_task_stderr(lines)\n    if task_error:\n        result['task_error'] = task_error\n\n    # check for errors in syslog\n    hadoop_error = _parse_hadoop_error(lines)\n    if hadoop_error:\n        result['hadoop_error'] = hadoop_error\n\n    # check for split\n    split = _parse_split(lines)\n    if split:\n        result['split'] = split\n\n    return result\n\n", "idx": 1224}
{"namespace": "mrjob.logs.ids._sort_for_spark", "completion": "    return sorted(ds, key=_spark_sort_key, reverse=True)\n\n", "idx": 1225}
{"namespace": "mrjob.logs.spark._parse_spark_log", "completion": "    # this is a state machine\n    state = 'start'\n    error = None\n    error_message = None\n    error_traceback = None\n    for line in lines:\n        if state == 'start':\n            # we're looking for the application ID\n            m = _SUBMITTED_APPLICATION_RE.search(line)\n            if m:\n                record_callback(\n                    'application_id',\n                    m.group('application_id'),\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None,\n                    None", "idx": 1226}
{"namespace": "mrjob.logs.mixin.LogInterpretationMixin._pick_error", "completion": "        if self._step_type_uses_spark(step_type):\n            return None\n\n        if self._read_logs():\n            if not log_interpretation.get('errors'):\n                log.info('Scanning logs for probable cause of failure...')\n                self._interpret_step_logs(log_interpretation, step_type)\n                self._interpret_history_log(log_interpretation)\n\n        return log_interpretation.get('errors')\n", "idx": 1227}
{"namespace": "mrjob.logs.history._match_history_log_path", "completion": "    m = _HISTORY_LOG_PATH_RE.match(path)\n    if not m:\n        return None\n\n    if job_id is not None and m.group('job_id') != job_id:\n        return None\n\n    return {\n        'job_id': m.group('job_id'),\n        'path': path,\n        'yarn': m.group('suffix') == 'hadoop',\n    }\n\n", "idx": 1228}
{"namespace": "mrjob.logs.history._parse_pre_yarn_history_log", "completion": "    result = {}\n    task_to_counters = {}  # used for successful tasks in failed jobs\n\n    for record in _parse_pre_yarn_history_records(lines):\n        record_type = record.get('type')\n\n        if record_type == 'Job':\n            if 'COUNTERS' in record:\n                result['counters'] = _parse_pre_yarn_counters(\n                    record['COUNTERS'])\n\n        elif record_type == 'Task':\n            if 'COUNTERS' in record and 'TASKID' in record:\n                task_to_counters[record['TASKID']] = _parse_pre_yarn_counters(\n                    record['COUNTERS'])\n\n        elif record_type == 'MapAttempt' or record_type == 'ReduceAttempt':\n            if 'TASK_ERROR' in record:\n                error = dict(\n                    hadoop_error=dict(\n                        message=record['TASK_ERROR'],\n                        start_line=record['start_line'],\n                        num_lines=record['num_lines']),\n                    task_id=record['TASKID'],\n                    attempt_id=record['ATTEMPTID'])\n\n                result.setdefault('errors', [])\n                result['errors'].append(error)\n\n    # if job failed, patch together counters from successful tasks\n    if 'counters' not in result and task_to_counters:\n        result['counters'] = _sum_counters(*task_to_counters.values())\n\n    return result\n\n", "idx": 1229}
{"namespace": "mrjob.logs.history._parse_pre_yarn_history_records", "completion": "    # parse records\n    record = None\n    for line_num, line in enumerate(lines):\n        # skip blank lines\n        if not line.strip():\n            continue\n\n        # skip non-records\n        if not line.startswith('.'):\n            continue\n\n        # parse record\n        if record is None:\n            record = dict(fields={}, num_lines=0)\n\n        # parse record fields\n        for field in line.strip().strip('.').split(' '):\n            m = _PRE_YARN_HISTORY_KEY_PAIR.match(field)\n            if not m:\n                continue\n\n            key = _PRE_YARN_HISTORY_ESCAPE_RE.sub(r'\\1', m.group('key'))\n            value = _PRE_YARN_HISTORY_ESCAPE_RE.sub(r'\\1', m.group('escaped_value'))\n            record['fields'][key] = value\n\n        # record is done\n        record['num_lines'] += 1\n        record['start_line'] = line_num - record['num_lines']\n        record['type'] = record['fields'].get('type')\n        yield record\n\n        # reset\n        record = None\n\n    # yield last record\n    if record:\n        yield record\n\n", "idx": 1230}
{"namespace": "mrjob.logs.step._parse_step_syslog", "completion": "    result = {}\n\n    for line in lines:\n        m = _SUBMITTED_APPLICATION_RE.match(line)\n        if m:\n            result['application_id'] = m.group('application_id')\n            continue\n\n        m = _RUNNING_JOB_RE.match(line)\n        if m:\n            result['job_id'] = m.group('job_id')\n            continue\n\n        m = _OUTPUT_DIRECTORY_RE.match(line)\n        if m:\n            result['output_dir'] = m.group('output_dir')\n            continue\n\n        m = _TASK_ATTEMPT_FAILED_RE.match(line)\n        if m:\n            error = dict(\n                attempt_id=m.group('attempt_id'),\n                hadoop_error=dict(message=line.strip()))\n\n            result.setdefault('errors', []).append(error)\n            continue\n\n        m = _NOT_A_VALID_JAR_RE.match(line)\n        if m:\n            error = dict(\n                hadoop_error=dict(message=line.strip()))\n\n            result.setdefault('errors', []).append(error)\n            continue\n\n        m = _INDENTED_COUNTERS_MESSAGE_RE.match(line)\n        if m:\n            num_counters = int(m.group('num_counters'))\n            counters = _parse_indented_counters(lines, num_counters)\n            if counters:\n                result['counters'] = counters\n            continue\n\n    return result\n\n", "idx": 1231}
{"namespace": "mrjob.logs.errors._merge_and_sort_errors", "completion": "    merged_errors = {}\n    for error in errors:\n        container_id = error.get('container_id')\n        if container_id:\n            if container_id in merged_errors:\n                merged_errors[container_id].update(error)\n            else:\n                merged_errors[container_id] = error\n        else:\n            # if we don't have a container ID, just group by timestamp,\n            # so we don't merge together different error messages\n            # from different tasks\n            timestamp = error.get('timestamp')\n            if timestamp in merged_errors:\n                merged_errors[timestamp].update(error)\n            else:\n                merged_errors[timestamp] = error\n\n    def _sort_key(container_id_or_timestamp):\n        error = merged_errors[container_id_or_timestamp]\n        return (\n            _is_probably_task_error(error),\n            error.get('timestamp'),\n            error.get('hadoop_error', {}).get('timestamp'),\n            error.get('task_error', {}).get('timestamp'),\n            error.get('spark_error', {}).get('timestamp'),\n        )\n\n    return sorted(merged_errors.values(), key=_sort_key, reverse=True)\n\n", "idx": 1232}
{"namespace": "mrjob.fs.ssh.SSHFilesystem.ls", "completion": "        # The SSH filesystem is used to access the Hadoop filesystem.\n        # The Hadoop filesystem is a distributed file system that is accessed through the SSH filesystem.\n        # The SSH filesystem is a remote file system that is accessed through the SSH protocol.\n        # The SSH protocol is a remote procedure call protocol that is used to execute commands on a remote machine.\n        # The SSH filesystem is used to access the Hadoop filesystem.\n        # The Hadoop filesystem is a distributed file system that is accessed through the SSH filesystem.\n        # The SSH filesystem is a remote file system that is accessed through the SSH protocol.\n        # The SSH protocol is a remote procedure call protocol that is used to execute commands on a remote machine.\n        # The SSH filesystem is used to access the Hadoop filesystem.\n        # The Hadoop filesystem is a distributed file system that is accessed through the SSH filesystem.\n        # The SSH filesystem is a remote file system that is accessed through the SSH protocol.\n        # The SSH protocol is a remote procedure call protocol that is used to execute commands on a remote machine.\n        # The SSH filesystem is used to access the Hadoop filesystem.\n        # The Hadoop filesystem is a distributed file system that is accessed through the SSH filesystem.\n        # The SSH filesystem is a remote file system that is accessed through the SSH protocol.\n        # The SSH protocol is a remote procedure call protocol that is used to execute commands on a remote machine.\n        # The SSH filesystem is used to access the Hadoop filesystem.\n        # The Hadoop filesystem is a distributed file system that is accessed through the SSH filesystem.\n        # The SSH filesystem is a remote file system that is accessed through the SSH protocol.\n        # The SSH protocol is a remote procedure call protocol that is used to execute commands on a remote machine.\n        # The SSH filesystem is used to access the Hadoop filesystem.\n        # The Hadoop filesystem is a distributed file system that is accessed through the SSH filesystem.\n        # The SSH filesystem is a remote file system that is accessed through the SSH protocol.\n        # The SSH protocol is a remote procedure call protocol that is used to execute commands on a remote machine.\n        # The SSH filesystem is used to access the Hadoop filesystem.\n        # The Hadoop filesystem is a distributed file system that is accessed through the SSH filesystem.\n       ", "idx": 1233}
{"namespace": "mrjob.fs.ssh.SSHFilesystem._cat_file", "completion": "        m = _SSH_URI_RE.match(path)\n        addr = m.group('hostname')\n        path_to_cat = m.group('filesystem_path')\n\n        p = self._ssh_launch(\n            addr, ['cat', path_to_cat])\n\n        for chunk in self._chunk_reader(p.stdout):\n            yield chunk\n\n        self._ssh_finish_run(p)\n", "idx": 1234}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.get_hadoop_bin", "completion": "        if self._hadoop_bin is None:\n            self._hadoop_bin = which('hadoop')\n            if self._hadoop_bin is None:\n                raise Exception(\"Could not find executable 'hadoop'\")\n        return self._hadoop_bin\n", "idx": 1235}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.du", "completion": "        # Check if the path exists\n        if not self.exists(path_glob):\n            return 0\n\n        # Get the output of the Hadoop command\n        output = self.invoke_hadoop(['fs', '-du', path_glob], ok_returncodes=[0, 1, 255])\n\n        # Parse the output\n        output = output.split(b'\\n')\n        if len(output) == 1:\n            if output[0].startswith(b'Found'):\n                return int(output[0].split()[1])\n            else:\n                raise IOError('Unexpected output from Hadoop fs -du: {output!r}'.format(output=output))\n        elif len(output) == 0:\n            return 0\n        else:\n            return sum(int(line.split()[1]) for line in output if line)\n", "idx": 1236}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.mkdir", "completion": "        # create a directory in HDFS\n        mkdir_args = self.get_hadoop_bin() + ['fs', '-mkdir', path]\n        log.debug('> %s' % cmd_line(mkdir_args))\n\n        mkdir_proc = Popen(mkdir_args, stdout=PIPE, stderr=PIPE)\n\n        # this does someties happen; see #1396\n        for line in mkdir_proc.stderr:\n            log.error('STDERR: ' + to_unicode(line.rstrip(b'\\r\\n')))\n\n        mkdir_proc.stdout.close()\n        mkdir_proc.stderr.close()\n\n        returncode = mkdir_proc.wait()\n\n        if returncode != 0:\n            if returncode == 1 and self.exists(path):\n                # hadoop 2.x returns 1 if the directory already exists\n                return\n            raise IOError(\"Could not mkdir %s\" % path)\n", "idx": 1237}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.exists", "completion": "        components = urlparse(path_glob)\n        hdfs_prefix = '%s://%s' % (components.scheme, components.netloc)\n\n        # check if the path exists\n        args = self.get_hadoop_bin() + ['fs', '-ls', path_glob]\n        log.debug('> %s' % cmd_line(args))\n\n        proc = Popen(args, stdout=PIPE, stderr=PIPE)\n        stdout, stderr = proc.communicate()\n\n        if proc.returncode == 0:\n            return True\n        elif proc.returncode == -1 or proc.returncode == 255:\n            return False\n        elif stderr:\n            if _HADOOP_LS_NO_SUCH_FILE.match(stderr):\n                return False\n            else:\n                raise IOError(\"Could not check path %s: %s\" % (path_glob, stderr))\n        else:\n            raise IOError(\"Could not check path %s\" % path_glob)\n", "idx": 1238}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.rm", "completion": "        # check if path is a URI\n        if not is_uri(path_glob):\n            super(HadoopFilesystem, self).rm(path_glob)\n\n        version = self.get_hadoop_version()\n\n        # use -r on Hadoop 2 (see #991, #845)\n        if uses_yarn(version):\n            args = ['fs', '-rm', '-r', path_glob]\n        else:\n            args = ['fs', '-rmr', path_glob]\n\n        try:\n            self.invoke_hadoop(args, ok_stderr=[_HADOOP_RM_NO_SUCH_FILE])\n        except CalledProcessError:\n            raise IOError(\"Could not rm %s\" % path_glob)\n", "idx": 1239}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.touchz", "completion": "        try:\n            self.invoke_hadoop(['fs', '-touchz', path])\n        except CalledProcessError:\n            raise IOError(\"Could not touchz %s\" % path)", "idx": 1240}
{"namespace": "mrjob.fs.local.LocalFilesystem.du", "completion": "        local_path_glob = self._path_to_local(path_glob)\n        return sum(os.path.getsize(f) for f in glob.glob(local_path_glob))\n", "idx": 1241}
{"namespace": "mrjob.fs.local.LocalFilesystem.ls", "completion": "        path_glob = _from_file_uri(path_glob)\n        if os.path.isdir(path_glob):\n            for dir_path, _, file_names in os.walk(path_glob):\n                for file_name in file_names:\n                    yield _to_file_uri(os.path.join(dir_path, file_name))\n        else:\n            yield _to_file_uri(path_glob)\n", "idx": 1242}
{"namespace": "mrjob.fs.local.LocalFilesystem._cat_file", "completion": "        path = _from_file_uri(path)\n        with open(path, 'rb') as f:\n            for line in f:\n                yield line\n", "idx": 1243}
{"namespace": "mrjob.fs.local.LocalFilesystem.exists", "completion": "        path_glob = _from_file_uri(path_glob)\n        return bool(glob.glob(path_glob))\n", "idx": 1244}
{"namespace": "mrjob.fs.local.LocalFilesystem.mkdir", "completion": "        path = _from_file_uri(path)\n        if not os.path.exists(path):\n            os.makedirs(path)\n", "idx": 1245}
{"namespace": "mrjob.fs.local.LocalFilesystem.put", "completion": "        src = _from_file_uri(src)\n        path = _from_file_uri(path)\n        shutil.copyfile(src, path)\n", "idx": 1246}
{"namespace": "mrjob.fs.local.LocalFilesystem.rm", "completion": "        path_glob = _from_file_uri(path_glob)\n        for path in glob.glob(path_glob):\n            if os.path.isdir(path):\n                shutil.rmtree(path)\n            else:\n                os.remove(path)\n", "idx": 1247}
{"namespace": "mrjob.fs.local.LocalFilesystem.touchz", "completion": "        path = _from_file_uri(path)\n        if os.path.isfile(path) and os.path.getsize(path) > 0:\n            raise OSError('Non-empty file %r already exists!' % (path,))\n\n        # Create the file\n        with open(path, 'w'):\n            pass\n", "idx": 1248}
{"namespace": "mrjob.fs.local.LocalFilesystem.md5sum", "completion": "        path = _from_file_uri(path)\n        with open(path, 'rb') as f:\n            return self._md5sum_file(f)\n\n", "idx": 1249}
{"namespace": "mrjob.fs.composite.CompositeFilesystem.add_fs", "completion": "        setattr(self, name, fs)\n        self._fs_names.append(name)\n        self._disable_if[name] = disable_if\n", "idx": 1250}
{"namespace": "mrjob.fs.base.Filesystem.cat", "completion": "        raise NotImplementedError\n", "idx": 1251}
{"namespace": "mrjob.fs.base.Filesystem.join", "completion": "        raise NotImplementedError\n", "idx": 1252}
{"namespace": "mrjob.examples.mr_text_classifier.parse_doc_filename", "completion": "    # Extract the id from the filename\n    id = posixpath.splitext(posixpath.basename(input_uri))[0]\n\n    # Extract the categories from the filename\n    cats = re.split(r'[._]', id)[1:]\n\n    # Create a dictionary of categories\n    cat_dict = {}\n    for cat in cats:\n        if cat.startswith('not_'):\n            cat_dict[cat[4:]] = False\n        else:\n            cat_dict[cat] = True\n\n    return dict(id=id, cats=cat_dict)\n\n\n", "idx": 1253}
{"namespace": "prometheus_client.mmap_dict.MmapedDict.read_value", "completion": "        with self._lock:\n            if key not in self._positions:\n                self._init_value(key)\n            pos = self._positions[key]\n            return _unpack_two_doubles(self._m, pos)[0]\n", "idx": 1254}
{"namespace": "prometheus_client.mmap_dict.MmapedDict.write_value", "completion": "        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        _pack_two_doubles(self._m, pos, value, timestamp)\n", "idx": 1255}
{"namespace": "prometheus_client.multiprocess.MultiProcessCollector.merge", "completion": "        metrics = {}\n        for f in files:\n            parts = os.path.basename(f).split('_')\n            typ = parts[0]\n            d = {tuple(parts[1:]): f}\n            if typ in metrics:\n                metrics[typ].update(d)\n            else:\n                metrics[typ] = d\n        for typ, m in metrics.items():\n            if typ == 'gauge':\n                yield Gauge(m.values()[0], '', registry=None, multiprocess_mode='all').merge(m.values(), accumulate)\n            elif typ == 'counter':\n                yield Metric(m.values()[0], '', 'counter', registry=None).merge(m.values(), accumulate)\n            elif typ == 'summary':\n                yield Metric(m.values()[0], '', 'summary', registry=None).merge(m.values(), accumulate)\n            elif typ == 'histogram':\n                yield Metric(m.values()[0], '', 'histogram', registry=None).merge(m.values(), accumulate)\n            else:\n                raise ValueError('Unknown metric type: %s' % typ)\n", "idx": 1256}
{"namespace": "prometheus_client.multiprocess.MultiProcessCollector.collect", "completion": "        files = glob.glob(os.path.join(self._path, \"*.db\"))\n        return self.merge(files, accumulate=True)\n\n", "idx": 1257}
{"namespace": "prometheus_client.exposition.choose_encoder", "completion": "    if accept_header:\n        accept_header = accept_header.replace(\" \", \"\").lower()\n        for mime_type in accept_header.split(\",\"):\n            if mime_type == \"*/*\":\n                return generate_latest, CONTENT_TYPE_LATEST\n            elif mime_type == \"application/openmetrics-text\":\n                return generate_latest, CONTENT_TYPE_LATEST\n            elif mime_type == \"application/json\":\n                return generate_latest, CONTENT_TYPE_LATEST\n            elif mime_type == \"text/plain; charset=0\":\n                return generate_latest, CONTENT_TYPE_LATEST\n            elif mime_type == \"text/plain; version=0.0.4\":\n                return generate_latest, CONTENT_TYPE_LATEST\n            elif mime_type == \"text/plain\":\n                return generate_latest, CONTENT_TYPE_LATEST\n    return generate_latest, CONTENT_TYPE_LATEST\n\n", "idx": 1258}
{"namespace": "flower.command.apply_options", "completion": "    # Parse the command line to get the \"--conf\" option\n    parse_command_line([prog_name, '-c', 'dummy.py'])\n    # Parse the configuration file\n    if options.conf:\n        try:\n            parse_config_file(os.path.abspath(options.conf))\n        except IOError:\n            if options.conf != os.path.abspath(default_options['conf']):\n                raise\n    # Parse the command line again to update the options\n    parse_command_line(argv)\n\n", "idx": 1259}
{"namespace": "trackerjacker.ieee_mac_vendor_db.MacVendorDB.lookup", "completion": "        mac = mac.upper().replace(':', '')\n        for oui in self.db:\n            if mac.startswith(oui):\n                return self.db[oui]\n        return ''", "idx": 1260}
{"namespace": "trailscraper.iam.Statement.merge", "completion": "        if self.Effect != other.Effect:\n            raise ValueError(\"Trying to combine two statements with differing effects: {self.Effect} {other.Effect}\")\n\n        merged_actions = sorted(list(set(self.Action + other.Action)))\n        merged_resources = sorted(list(set(self.Resource + other.Resource)))\n\n        return Statement(Action=merged_actions, Effect=self.Effect, Resource=merged_resources)\n\n", "idx": 1261}
{"namespace": "trailscraper.iam.parse_policy_document", "completion": "    if isinstance(stream, six.string_types):\n        json_data = json.loads(stream)\n    else:\n        json_data = json.load(stream)\n\n    return PolicyDocument(\n        Statement=_parse_statements(json_data['Statement']),\n        Version=json_data['Version']\n    )\n\n", "idx": 1262}
{"namespace": "trailscraper.iam.known_iam_actions", "completion": "    known_iam_permissions = all_known_iam_permissions()\n    known_iam_actions = [iam_permission.split(':')[1] for iam_permission in known_iam_permissions]\n    known_iam_actions_grouped_by_prefix = groupbyz(lambda action: action.split(':')[0], known_iam_actions)\n    return known_iam_actions_grouped_by_prefix[prefix]\n\n", "idx": 1263}
{"namespace": "trailscraper.boto_service_definitions.service_definition_file", "completion": "    files = boto_service_definition_files()\n    files = [file for file in files if fnmatch.fnmatch(file, \"**/\" + servicename + \"/*/service-*.json\")]\n    files.sort()\n    return files[-1]\n\n", "idx": 1264}
{"namespace": "trailscraper.boto_service_definitions.operation_definition", "completion": "    service_definition_file_path = service_definition_file(servicename)\n    with open(service_definition_file_path, 'r') as service_definition_file:\n        service_definition = json.load(service_definition_file)\n\n    return service_definition['operations'][operationname]\n\n", "idx": 1265}
{"namespace": "trailscraper.cloudtrail.Record.to_statement", "completion": "        if self.event_source == \"sts.amazonaws.com\" and self.event_name == \"GetCallerIdentity\":\n            return None\n\n        if self.event_source == \"s3.amazonaws.com\":\n            return Statement(\n                Effect=\"Allow\",\n                Action=[Action(\"s3\", self._event_name_to_iam_action())],\n                Resource=self.resource_arns\n            )\n\n        if self.event_source == \"kms.amazonaws.com\":\n            return Statement(\n                Effect=\"Allow\",\n                Action=[Action(\"kms\", self._event_name_to_iam_action())],\n                Resource=self.resource_arns\n            )\n\n        if self.event_source == \"dynamodb.amazonaws.com\":\n            return Statement(\n                Effect=\"Allow\",\n                Action=[Action(\"dynamodb\", self._event_name_to_iam_action())],\n                Resource=self.resource_arns\n            )\n\n        if self.event_source == \"lambda.amazonaws.com\":\n            return Statement(\n                Effect=\"Allow\",\n                Action=[Action(\"lambda\", self._event_name_to_iam_action())],\n                Resource=self.resource_arns\n            )\n\n        if self.event_source == \"es.amazonaws.com\":\n            return Statement(\n                Effect=\"Allow\",\n                Action=[Action(\"es\", self._event_name_to_iam_action())],\n                Resource=self.resource_arns\n            )\n\n        if self.event_source == \"firehose.amazonaws.com\":\n            return Statement(\n                Effect=\"Allow\",\n                Action=[Action(\"firehose\", self._event_name_to_iam_action())],\n                Resource=self.resource_arns\n            )\n\n        if self.event_source == \"glue.amazonaws.com\":\n            return Statement(\n                Effect=\"Allow\",\n                Action=[Action(\"glue\", self._event_name_to_iam_action())],\n                Resource=self.resource_arns\n            )", "idx": 1266}
{"namespace": "trailscraper.cloudtrail.filter_records", "completion": "    return pipe(records,\n                filterz(_by_timeframe(from_date, to_date)),\n                filterz(_by_role_arns(arns_to_filter_for)))\n\n", "idx": 1267}
{"namespace": "trailscraper.record_sources.local_directory_record_source.LocalDirectoryRecordSource.load_from_dir", "completion": "        def _filter_by_date(log_file):\n            return log_file.records_within_timeframe(from_date, to_date)\n\n        return pipe(self._valid_log_files(),\n                    mapcatz(_filter_by_date),\n                    mapcatz(sortedz(key=lambda record: record.event_time)),\n                    mapcatz(list),\n                    list)\n", "idx": 1268}
{"namespace": "pyt.__main__.discover_files", "completion": "    included_files = []\n    for target in targets:\n        if os.path.isfile(target):\n            if target.endswith(\".py\"):\n                included_files.append(target)\n        else:\n            for root, dirs, files in os.walk(target):\n                for file in files:\n                    if file.endswith(\".py\"):\n                        file_path = os.path.join(root, file)\n                        if file_path not in excluded_files:\n                            included_files.append(file_path)\n                            log.debug(\"Discovered file: %s\", file_path)\n                if not recursive:\n                    break\n    return included_files\n\n", "idx": 1269}
{"namespace": "pyt.core.project_handler.get_directory_modules", "completion": "    global _local_modules\n\n    if _local_modules and os.path.dirname(_local_modules[0][1]) == directory:\n        return _local_modules\n\n    if not os.path.isdir(directory):\n        directory = os.path.dirname(directory)\n\n    for file in os.listdir(directory):\n        if file.endswith(\".py\"):\n            module_name = file[:-3]\n            module_path = os.path.join(directory, file)\n            _local_modules.append((module_name, module_path))\n\n    return _local_modules\n\n", "idx": 1270}
{"namespace": "pyt.vulnerabilities.vulnerabilities.find_triggers", "completion": "    trigger_nodes = []\n    for node in nodes:\n        if node.line_number not in nosec_lines:\n            for trigger_word in trigger_words:\n                if trigger_word.word in node.label:\n                    trigger_nodes.append(TriggerNode(trigger_word, node))\n    return trigger_nodes\n\n", "idx": 1271}
{"namespace": "pyt.vulnerabilities.vulnerabilities.label_contains", "completion": "    for trigger in triggers:\n        if trigger.word in node.label:\n            yield TriggerNode(trigger, node)\n\n", "idx": 1272}
{"namespace": "pyt.vulnerabilities.vulnerabilities.build_sanitiser_node_dict", "completion": "    sanitiser_list = extract_sanitiser_list(sinks_in_file)\n    sanitiser_nodes = find_sanitiser_nodes(cfg, sanitiser_list)\n    sanitiser_node_dict = build_sanitiser_node_dict_helper(sanitiser_nodes)\n    return sanitiser_node_dict\n\n", "idx": 1273}
{"namespace": "pyt.vulnerabilities.trigger_definitions_parser.parse", "completion": "    with open(trigger_word_file, 'r') as f:\n        data = json.load(f)\n\n    sources = [Source(trigger_word=key) for key in data['sources']]\n    sinks = [Sink.from_json(key, value) for key, value in data['sinks'].items()]\n\n    return Definitions(sources=sources, sinks=sinks)", "idx": 1274}
{"namespace": "principalmapper.querying.local_policy_simulation._statement_matches_resource", "completion": "    if 'Resource' in statement:\n        for item in _listify_string(statement['Resource']):\n            if _matches_after_expansion(resource, item, condition_keys):\n                return True\n        return False\n    elif 'NotResource' in statement:\n        for item in _listify_string(statement['NotResource']):\n            if _matches_after_expansion(resource, item, condition_keys):\n                return False\n        return True\n    else:\n        return True\n\n", "idx": 1275}
{"namespace": "principalmapper.querying.local_policy_simulation._matches_after_expansion", "completion": "    # If the second string is a regular expression, check if the first string matches the regular expression.\n    if '${' in string_to_check_against:\n        # If the second string is a regular expression, check if the first string matches the regular expression.\n        if '${' in string_to_check_against:\n            # If the second string is a regular expression, check if the first string matches the regular expression.\n            if '${' in string_to_check_against:\n                # If the second string is a regular expression, check if the first string matches the regular expression.\n                if '${' in string_to_check_against:\n                    # If the second string is a regular expression, check if the first string matches the regular expression.\n                    if '${' in string_to_check_against:\n                        # If the second string is a regular expression, check if the first string matches the regular expression.\n                        if '${' in string_to_check_against:\n                            # If the second string is a regular expression, check if the first string matches the regular expression.\n                            if '${' in string_to_check_against:\n                                # If the second string is a regular expression, check if the first string matches the regular expression.\n                                if '${' in string_to_check_against:\n                                    # If the second string is a regular expression, check if the first string matches the regular expression.\n                                    if '${' in string_to_check_against:\n                                        # If the second string is a regular expression, check if the first string matches the regular expression.\n                                        if '${' in string_to_check_against:\n                                            # If the second string is a regular expression, check if the first string matches the regular expression.\n                                            if '${' in string_to_check_against:\n                                                # If the second string is a regular expression, check if the first string matches the regular expression.\n                                                if '${' in string_to_check_against:\n                                                    # If", "idx": 1276}
{"namespace": "passpie.database.PasspieStorage.delete", "completion": "        for credential in credentials:\n            credpath = self.make_credpath(credential[\"name\"], credential[\"login\"])\n            if os.path.exists(credpath):\n                os.remove(credpath)\n                dirname = os.path.dirname(credpath)\n                if not os.listdir(dirname):\n                    shutil.rmtree(dirname)\n", "idx": 1277}
{"namespace": "passpie.database.PasspieStorage.read", "completion": "        credentials = []\n        for dirname, dirnames, filenames in os.walk(self.path):\n            for filename in filenames:\n                if filename.endswith(self.extension):\n                    credpath = os.path.join(dirname, filename)\n                    with open(credpath, 'r') as credfile:\n                        credential = yaml.load(credfile)\n                        credentials.append(credential)\n        return credentials\n", "idx": 1278}
{"namespace": "threatingestor.state.State.save_state", "completion": "        try:\n            self.cursor.execute('REPLACE INTO states (name, state) VALUES (?, ?)', (name, state))\n            self.conn.commit()\n        except sqlite3.Error:\n            raise threatingestor.exceptions.IngestorError(\"State database seems broken\")\n\n", "idx": 1279}
{"namespace": "threatingestor.state.State.get_state", "completion": "        logger.debug(f\"Retrieving state for '{name}'\")\n        self.cursor.execute('SELECT state FROM states WHERE name=?', (name,))\n        state = self.cursor.fetchone()\n        if state is None:\n            return None\n        else:\n            return state[0]", "idx": 1280}
{"namespace": "threatingestor.Ingestor.run", "completion": "        # Run as a daemon.\n        if self.config.daemon():\n            logger.info(\"Running as daemon\")\n            while True:\n                self._run_once()\n                time.sleep(self.config.daemon_interval())\n\n        # Run once.\n        else:\n            logger.info(\"Running once\")\n            self._run_once()\n", "idx": 1281}
{"namespace": "msticpy.analysis.anomalous_sequence.model.Model.compute_scores", "completion": "        if use_start_end_tokens:\n            self.sessions = [\n                [self.start_token] + ses + [self.end_token] for ses in self.sessions\n            ]\n        self.session_likelihoods = self._compute_session_likelihoods()\n        self.session_geomean_likelihoods = self._compute_session_geomean_likelihoods()\n        self.rare_windows, self.rare_window_likelihoods = self._compute_rare_windows_likelihoods(\n            window_len=2\n        )\n        self.rare_windows_geo, self.rare_window_likelihoods_geo = self._compute_rare_windows_likelihoods(\n            window_len=3\n        )\n", "idx": 1282}
{"namespace": "msticpy.analysis.anomalous_sequence.model.Model.compute_rarest_windows", "completion": "        if self.session_likelihoods is None:\n            self.compute_likelihoods_of_sessions(use_start_end_tokens=use_start_end_tokens)\n\n        if window_len > len(self.sessions[0]):\n            raise MsticpyException(\n                f\"window_len should be less than or equal to the length of the sessions. The length of the first session is {len(self.sessions[0])}\"\n            )\n\n        if window_len < 2:\n            raise MsticpyException(\n                f\"window_len should be greater than or equal to 2. The value of window_len is {window_len}\"\n            )\n\n        if self.session_type == SessionType.cmds_only:\n            rarest_windows, rarest_window_likelihoods = cmds_only.compute_rarest_windows(\n                sessions=self.sessions,\n                session_likelihoods=self.session_likelihoods,\n                window_len=window_len,\n                use_start_end_tokens=use_start_end_tokens,\n                use_geo_mean=use_geo_mean,\n            )\n        elif self.session_type == SessionType.cmds_params_only:\n            rarest_windows, rarest_window_likelihoods = cmds_params_only.compute_rarest_windows(\n                sessions=self.sessions,\n                session_likelihoods=self.session_likelihoods,\n                window_len=window_len,\n                use_start_end_tokens=use_start_end_tokens,\n                use_geo_mean=use_geo_mean,\n            )\n        else:\n            rarest_windows, rarest_window_likelihoods = cmds_params_values.compute_rarest_windows(\n                sessions=self.sessions,\n                session_likelihoods=self.session_likelihoods,\n                window_len=window_len,\n                use_start_end_tokens=use_start_end_tokens,", "idx": 1283}
{"namespace": "msticpy.analysis.anomalous_sequence.anomalous.score_sessions", "completion": "    # Checking if the input is a DataFrame\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError(\"'data' should be of type 'DataFrame'\")\n\n    # Checking if the session column is a string\n    if not isinstance(session_column, str):\n        raise TypeError(\"'session_column' should be of type 'str'\")\n\n    # Checking if the window length is an integer\n    if not isinstance(window_length, int):\n        raise TypeError(\"'window_length' should be of type 'int'\")\n\n    # Checking if the session column is in the DataFrame\n    if session_column not in data.columns:\n        raise ValueError(\"'session_column' should be one of the columns in 'data'\")\n\n    # Checking if the window length is greater than 1\n    if window_length < 1:\n        raise ValueError(\"'window_length' should be greater than 1\")\n\n    # Checking if the window length is less than the number of sessions\n    if window_length > len(data[session_column].unique()):\n        raise ValueError(\n            \"'window_length' should be less than the number of unique sessions in 'data'\"\n        )\n\n    # Checking if the DataFrame has at least one row\n    if data.shape[0] < 1:\n        raise ValueError(\"'data' should have at least one row\")\n\n    # Checking if the DataFrame has at least one column\n    if data.shape[1] < 1:\n        raise ValueError(\"'data' should have at least one column\")\n\n    # Checking if the DataFrame has at least two columns\n    if data.shape[1] < 2:\n        raise ValueError(\"'data' should have at least two columns\")\n\n    # Checking if the DataFrame has at least 'window_length' rows\n    if data.shape[0] < window_length:\n        raise ValueError(\n            \"'data' should have at least 'window_length' rows, where 'window_length' is the number of rows in the sliding window\"\n        )\n\n    # Checking if", "idx": 1284}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.laplace_smooth_counts", "completion": "    # Laplace smoothing\n    seq1_counts_sm = copy.deepcopy(seq1_counts)\n    seq2_counts_sm = copy.deepcopy(seq2_counts)\n    param_counts_sm = copy.deepcopy(param_counts)\n    cmd_param_counts_sm = copy.deepcopy(cmd_param_counts)\n\n    # Smooth the counts of individual commands\n    for cmd in seq1_counts:\n        seq1_counts_sm[cmd] += 1\n\n    # Smooth the counts of sequences of commands\n    for cmd1 in seq2_counts:\n        for cmd2 in seq2_counts[cmd1]:\n            seq2_counts_sm[cmd1][cmd2] += 1\n\n    # Smooth the counts of individual parameters\n    for param in param_counts:\n        param_counts_sm[param] += 1\n\n    # Smooth the counts of parameters conditional on commands\n    for cmd in cmd_param_counts:\n        for param in cmd_param_counts[cmd]:\n            cmd_param_counts_sm[cmd][param] += 1\n\n    # Handle unseen commands, sequences of commands, and parameters\n    # Unseen commands\n    if start_token not in seq1_counts_sm:\n        seq1_counts_sm[start_token] = 1\n\n    if end_token not in seq1_counts_sm:\n        seq1_counts_sm[end_token] = 1\n\n    if unk_token not in seq1_counts_sm:\n        seq1_counts_sm[unk_token] = 1\n\n    # Unseen sequences of commands\n    if start_token not in seq2_counts_sm:\n        seq2_counts_sm[start_token] = defaultdict(lambda: 1)\n    else:\n        if end_token not in seq2_counts_sm[start_token]:\n            seq2_counts_sm[start_token][end_token] = 1\n        if unk", "idx": 1285}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.compute_likelihood_window", "completion": "    if use_start_token and start_token is None:\n        raise MsticpyException(\n            \"use_start_token is set to True, but no start_token is provided.\"\n        )\n    if use_end_token and end_token is None:\n        raise MsticpyException(\n            \"use_end_token is set to True, but no end_token is provided.\"\n        )\n\n    if use_start_token:\n        window = [start_token] + window\n    if use_end_token:\n        window = window + [end_token]\n\n    lik: float = 1\n    for i in range(len(window) - 1):\n        cmd1 = window[i]\n        cmd2 = window[i + 1]\n        lik *= trans_probs[cmd1][cmd2]\n        if cmd1 in param_cond_cmd_probs:\n            lik *= compute_prob_setofparams_given_cmd(\n                cmd=cmd1,\n                params=window[i + 1].params,\n                param_cond_cmd_probs=param_cond_cmd_probs,\n            )\n        else:\n            lik *= 1\n    lik *= prior_probs[window[-1]]\n\n    return lik\n\n", "idx": 1286}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.compute_likelihood_windows_in_session", "completion": "    if use_start_end_tokens:\n        if start_token is None or end_token is None:\n            raise MsticpyException(\n                \"start_token and end_token should not be None, when use_start_end_tokens is True\"\n            )\n\n    session_len = len(session)\n    if session_len < window_len:\n        return [np.nan]\n\n    likelihoods = []\n    for i in range(session_len - window_len + 1):\n        window = session[i : i + window_len]\n        lik = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            use_start_token=use_start_end_tokens,\n            use_end_token=use_start_end_tokens,\n            start_token=start_token,\n            end_token=end_token,\n        )\n        if use_geo_mean:\n            lik = lik ** (1 / window_len)\n        likelihoods.append(lik)\n\n    return likelihoods\n\n", "idx": 1287}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.rarest_window_session", "completion": "    likelihoods = compute_likelihood_windows_in_session(\n        session=session,\n        prior_probs=prior_probs,\n        trans_probs=trans_probs,\n        param_cond_cmd_probs=param_cond_cmd_probs,\n        window_len=window_len,\n        use_start_end_tokens=use_start_end_tokens,\n        start_token=start_token,\n        end_token=end_token,\n        use_geo_mean=use_geo_mean,\n    )\n\n    min_lik = min(likelihoods)\n    min_lik_ind = likelihoods.index(min_lik)\n    rarest_window = session[min_lik_ind : min_lik_ind + window_len]  # noqa E203\n\n    return rarest_window, min_lik\n\n", "idx": 1288}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.compute_likelihood_window", "completion": "    if use_start_token and start_token is None:\n        raise MsticpyException(\"start_token must be set if use_start_token is set to True\")\n    if use_end_token and end_token is None:\n        raise MsticpyException(\"end_token must be set if use_end_token is set to True\")\n\n    if use_start_token:\n        window = [start_token] + window\n    if use_end_token:\n        window = window + [end_token]\n\n    if isinstance(prior_probs, dict):\n        prior_probs = StateMatrix(states=prior_probs)\n    if isinstance(trans_probs, dict):\n        trans_probs = StateMatrix(states=trans_probs)\n\n    likelihood = 1\n    for i in range(len(window) - 1):\n        likelihood *= trans_probs[window[i]][window[i + 1]]\n    likelihood *= prior_probs[window[-1]]\n\n    return likelihood\n\n", "idx": 1289}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.compute_likelihood_windows_in_session", "completion": "    if use_start_end_tokens:\n        if start_token is None or end_token is None:\n            raise MsticpyException(\n                \"start_token and end_token should not be None, when use_start_end_tokens is True\"\n            )\n\n    if window_len < 1:\n        raise MsticpyException(\"window_len should be at least 1\")\n\n    if window_len >= len(session):\n        raise MsticpyException(\"window_len should be less than the length of the session\")\n\n    likelihoods = []\n    for i in range(len(session) - window_len + 1):\n        window = session[i : i + window_len]\n        likelihood = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            use_start_token=use_start_end_tokens,\n            use_end_token=use_start_end_tokens,\n            start_token=start_token,\n            end_token=end_token,\n        )\n        if use_geo_mean:\n            likelihood = likelihood ** (1 / window_len)\n        likelihoods.append(likelihood)\n\n    return likelihoods\n\n", "idx": 1290}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.rarest_window_session", "completion": "    likelihoods = compute_likelihood_windows_in_session(\n        session=session,\n        prior_probs=prior_probs,\n        trans_probs=trans_probs,\n        window_len=window_len,\n        use_start_end_tokens=use_start_end_tokens,\n        start_token=start_token,\n        end_token=end_token,\n        use_geo_mean=use_geo_mean,\n    )\n\n    min_lik = min(likelihoods)\n    min_lik_indices = [i for i, j in enumerate(likelihoods) if j == min_lik]\n    rarest_window_ind = min_lik_indices[0]\n    rarest_window = session[rarest_window_ind : rarest_window_ind + window_len]  # noqa: E203\n\n    return rarest_window, min_lik\n\n", "idx": 1291}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.get_params_to_model_values", "completion": "    # If the input is a dict, convert it to a StateMatrix\n    if isinstance(param_counts, dict):\n        param_counts = StateMatrix(states=param_counts)\n    if isinstance(param_value_counts, dict):\n        param_value_counts = StateMatrix(states=param_value_counts)\n\n    # Get the total number of parameters\n    total_params = param_counts.get_total_count()\n\n    # Get the total number of values\n    total_values = param_value_counts.get_total_count()\n\n    # Get the total number of parameters that have at least one value\n    non_zero_params = param_counts.get_non_zero_count()\n\n    # Get the total number of values that have at least one parameter\n    non_zero_values = param_value_counts.get_non_zero_count()\n\n    # Get the total number of parameters that have at least two values\n    two_or_more_values = param_value_counts.get_two_or_more_count()\n\n    # Get the total number of parameters that have at least two values\n    two_or_more_params = param_counts.get_two_or_more_count()\n\n    # Get the total number of parameters that have at least three values\n    three_or_more_values = param_value_counts.get_three_or_more_count()\n\n    # Get the total number of parameters that have at least three values\n    three_or_more_params = param_counts.get_three_or_more_count()\n\n    # Get the total number of parameters that have at least four values\n    four_or_more_values = param_value_counts.get_four_or_more_count()\n\n    # Get the total number of parameters that have at least four values\n    four_or_more_params = param_counts.get_four_or_more_count()\n\n    # Get the total number of parameters that have at least five values\n    five_or_more_values = param_value_counts.", "idx": 1292}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_prob_setofparams_given_cmd", "completion": "    if isinstance(params_with_vals, set):\n        params_with_vals = dict.fromkeys(params_with_vals)\n\n    if isinstance(param_cond_cmd_probs, StateMatrix):\n        param_cond_cmd_probs = param_cond_cmd_probs.states\n\n    if isinstance(value_cond_param_probs, StateMatrix):\n        value_cond_param_probs = value_cond_param_probs.states\n\n    if not isinstance(modellable_params, set):\n        modellable_params = set(modellable_params)\n\n    # compute the probability of the command\n    prob_cmd = param_cond_cmd_probs[cmd].get(params_with_vals, 0)\n\n    # compute the probability of the parameters\n    prob_params = 1\n    for param, val in params_with_vals.items():\n        if param in modellable_params:\n            prob_params *= value_cond_param_probs[param].get(val, 0)\n        else:\n            prob_params *= param_cond_cmd_probs[cmd].get(param, 0)\n\n    if use_geo_mean:\n        # compute the geometric mean of the probabilities\n        num_params = len(params_with_vals) + len(modellable_params)\n        prob_cmd = np.power(prob_cmd, 1 / num_params)\n        prob_params = np.power(prob_params, 1 / num_params)\n\n    return prob_cmd * prob_params\n\n", "idx": 1293}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_likelihood_window", "completion": "    if use_start_token and start_token is None:\n        raise MsticpyException(\n            \"use_start_token is set to True but no start_token is provided.\"\n        )\n    if use_end_token and end_token is None:\n        raise MsticpyException(\n            \"use_end_token is set to True but no end_token is provided.\"\n        )\n\n    if use_start_token:\n        window = [start_token] + window\n    if use_end_token:\n        window = window + [end_token]\n\n    lik: float = 1\n    for i in range(len(window) - 1):\n        lik *= trans_probs[window[i]][window[i + 1]]\n        lik *= prior_probs[window[i + 1]]\n        if window[i + 1] in param_cond_cmd_probs:\n            lik *= compute_prob_setofparams_given_cmd(\n                window[i + 1],\n                window[i + 1].params,\n                param_cond_cmd_probs,\n                value_cond_param_probs,\n                modellable_params,\n            )\n    return lik\n\n", "idx": 1294}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_likelihood_windows_in_session", "completion": "    if use_start_end_tokens:\n        if start_token is None or end_token is None:\n            raise MsticpyException(\n                \"start_token and end_token should not be None, when use_start_end_tokens is True\"\n            )\n\n    if window_len > len(session):\n        raise MsticpyException(\n            \"window_len should be less than or equal to the length of the session.\"\n        )\n\n    likelihoods = []\n    for i in range(len(session) - window_len + 1):\n        window = session[i : i + window_len]\n        lik = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            value_cond_param_probs=value_cond_param_probs,\n            modellable_params=modellable_params,\n            use_start_token=use_start_end_tokens,\n            use_end_token=use_start_end_tokens,\n            start_token=start_token,\n            end_token=end_token,\n        )\n        if use_geo_mean:\n            lik = lik ** (1 / window_len)\n        likelihoods.append(lik)\n\n    return likelihoods\n\n", "idx": 1295}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.rarest_window_session", "completion": "    likelihoods = compute_likelihood_windows_in_session(\n        session=session,\n        prior_probs=prior_probs,\n        trans_probs=trans_probs,\n        param_cond_cmd_probs=param_cond_cmd_probs,\n        value_cond_param_probs=value_cond_param_probs,\n        modellable_params=modellable_params,\n        window_len=window_len,\n        use_start_end_tokens=use_start_end_tokens,\n        start_token=start_token,\n        end_token=end_token,\n        use_geo_mean=use_geo_mean,\n    )\n\n    min_lik = min(likelihoods)\n    min_lik_ind = likelihoods.index(min_lik)\n    rarest_window = session[min_lik_ind : min_lik_ind + window_len]\n\n    return rarest_window, min_lik\n\n", "idx": 1296}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.probabilities.compute_cmds_probs", "completion": "    # Compute the probabilities for individual commands\n    cmds_probs = defaultdict(float)\n    for cmd in seq1_counts:\n        cmds_probs[cmd] = seq1_counts[cmd] / sum(seq1_counts.values())\n    cmds_probs[unk_token] = 1 - sum(cmds_probs.values())\n\n    # Compute the probabilities for sequence commands (length 2)\n    seq_cmds_probs = defaultdict(float)\n    for cmd1 in seq2_counts:\n        for cmd2 in seq2_counts[cmd1]:\n            seq_cmds_probs[(cmd1, cmd2)] = seq2_counts[cmd1][cmd2] / sum(\n                seq2_counts[cmd1].values()\n            )\n    seq_cmds_probs[(unk_token, unk_token)] = 1 - sum(seq_cmds_probs.values())\n\n    return cmds_probs, seq_cmds_probs", "idx": 1297}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.probabilities.compute_values_probs", "completion": "    value_probs: DefaultDict[str, float] = defaultdict(lambda: 0)\n    param_value_probs: DefaultDict[str, DefaultDict[str, float]] = defaultdict(\n        lambda: defaultdict(lambda: 0)\n    )\n\n    for param, values in param_value_counts.items():\n        for value, count in values.items():\n            param_value_probs[param][value] = count / param_value_counts[param][param]\n\n    tot_value = sum(value_counts.values())\n    for value, count in value_counts.items():\n        value_probs[value] = count / tot_value\n\n    value_probs_sm = StateMatrix(states=value_probs, unk_token=unk_token)\n    param_value_probs_sm = StateMatrix(states=param_value_probs, unk_token=unk_token)\n\n    return value_probs_sm, param_value_probs_sm", "idx": 1298}
{"namespace": "msticpy.auth.msal_auth.MSALDelegatedAuth.get_token", "completion": "        # First acquire token from cache\n        self.result = self.app.acquire_token_silent(\n            scopes=self.scopes, account=self.app.get_accounts(username=self.username)\n        )\n\n        # If token is not found, authenticate\n        if not self.result:\n            if self.auth_type == \"interactive\":\n                self.result = self.app.acquire_token_interactive(\n                    scopes=self.scopes,\n                    account=self.app.get_accounts(username=self.username),\n                )\n            elif self.auth_type == \"device\":\n                self.result = self.app.acquire_token_by_device_flow(\n                    scopes=self.scopes\n                )\n            else:\n                raise Exception(\n                    \"Invalid authentication type. Please choose interactive or device.\"\n                )\n\n        # Refresh token if needed\n        if self.result.get(\"expires_in\") < 60:\n            self.result = self.app.acquire_token_silent(\n                scopes=self.scopes, account=self.app.get_accounts(username=self.username)\n            )\n", "idx": 1299}
{"namespace": "msticpy.config.query_editor.QueryParameterEditWidget.save_parameter", "completion": "        del button\n        # Get the parameter name from the widget\n        param_name = self.parameter_name_widget.value\n        # If the parameter name is empty, return\n        if not param_name:\n            return\n        # Get the parameter description from the widget\n        param_desc = self.description_widget.value\n        # Get the parameter type from the widget\n        param_type = self.type_widget.value\n        # Get the parameter default value from the widget\n        param_default = self.default_widget.value\n        # Create a QueryParameter instance with the retrieved values\n        param = QueryParameter(\n            name=param_name,\n            description=param_desc,\n            datatype=param_type,\n            default=param_default if self.default_reqd_widget.value else None,\n        )\n        # Set the parameter in the param container\n        self.param_container.parameters[param_name] = param\n        # Update the parameter dropdown options\n        self.parameter_dropdown.options = list(self.param_container.parameters.keys())\n        # Set the selected value to the newly saved parameter\n        self.parameter_dropdown.value = param_name\n        self._changed_data = True\n", "idx": 1300}
{"namespace": "msticpy.config.query_editor.QueryParameterEditWidget.delete_parameter", "completion": "        del button\n        if self.parameter_name_widget.value:\n            del self.param_container.parameters[self.parameter_name_widget.value]\n            self.parameter_dropdown.options = list(\n                self.param_container.parameters.keys()\n            )\n            self._blank_parameter()\n            self._changed_data = True\n\n", "idx": 1301}
{"namespace": "msticpy.config.query_editor.MetadataEditWidget.save_metadata", "completion": "        del button\n        self.metadata.version = self.version_widget.value\n        self.metadata.description = self.description_widget.value\n        self.metadata.data_environments = self.data_env_widget.value\n        self.metadata.data_families = self.data_families_widget.value.split(\",\")\n        self.metadata.database = self.database_widget.value\n        self.metadata.cluster = self.cluster_widget.value\n        self.metadata.clusters = self.clusters_widget.value.split(\"\\n\")\n        self.metadata.cluster_groups = self.cluster_groups_widget.value.split(\"\\n\")\n        self.metadata.tags = self.tags_widget.value.split(\",\")\n        self.metadata.data_source = self.data_source_widget.value\n        self._changed_data = True\n", "idx": 1302}
{"namespace": "msticpy.config.query_editor.QueryEditor._save_file", "completion": "        del button\n        if self.ignore_changes.value:\n            return\n        if not self.query_collection.sources:\n            return\n        if not self.query_collection.metadata:\n            return\n        if not self.query_collection.defaults:\n            return\n        if not self.query_collection.metadata.version:\n            return\n        if not self.query_collection.metadata.description:\n            return\n        if not self.query_collection.metadata.data_families:\n            return\n        if not self.query_collection.metadata.data_environments:\n            return\n        if not self.query_collection.defaults.parameters:\n            return\n        if not self.query_collection.defaults.metadata:\n            return\n        if not self.query_collection.defaults.metadata.version:\n            return\n        if not self.query_collection.defaults.metadata.description:\n            return\n        if not self.query_collection.defaults.metadata.data_families:\n            return\n        if not self.query_collection.defaults.metadata.data_environments:\n            return\n        self.query_collection.save(self.current_file)\n        self.ignore_changes.value = True\n", "idx": 1303}
{"namespace": "msticpy.config.query_editor.QueryEditor._unsaved_changes", "completion": "        return (\n            self.default_param_editor.changed_data\n            or self.metadata_editor.changed_data\n            or self.query_editor.changed_data\n        )\n\n", "idx": 1304}
{"namespace": "msticpy.config.query_editor.load_queries_from_yaml", "completion": "    # Read the YAML file\n    with open(yaml_file, \"r\", encoding=\"utf-8\") as f:\n        yaml_data = yaml.safe_load(f)\n\n    # Extract the necessary data\n    metadata = QueryMetadata(\n        version=yaml_data[\"metadata\"][\"version\"],\n        description=yaml_data[\"metadata\"][\"description\"],\n        data_families=[\n            DataFamily[family] for family in yaml_data[\"metadata\"][\"data_families\"]\n        ],\n        data_environments=[\n            DataEnvironment[environment]\n            for environment in yaml_data[\"metadata\"][\"data_environments\"]\n        ],\n    )\n    defaults = QueryDefaults(\n        metadata=yaml_data[\"defaults\"][\"metadata\"],\n        parameters={\n            param: QueryParameter(\n                name=param,\n                description=yaml_data[\"defaults\"][\"parameters\"][param][\"description\"],\n                required=yaml_data[\"defaults\"][\"parameters\"][param][\"required\"],\n                data_type=yaml_data[\"defaults\"][\"parameters\"][param][\"data_type\"],\n                default=yaml_data[\"defaults\"][\"parameters\"][param][\"default\"],\n            )\n            for param in yaml_data[\"defaults\"][\"parameters\"]\n        },\n    )\n    sources = {\n        query: Query(\n            name=query,\n            description=yaml_data[\"sources\"][query][\"description\"],\n            args=QueryArgs(\n                query=yaml_data[\"sources\"][query][\"query\"],\n                data_source=yaml_data[\"sources\"][query][\"data_source\"],\n                data_families=[\n                    DataFamily[family]\n                    for family in yaml_data[\"sources\"][query][\"data_families\"]\n                ],\n                data_environments=[\n                    DataEnvironment[environment]\n                    for environment in yaml_data[\"sources\"][query][\"data_environments\"]\n                ],\n            ),\n            metadata=yaml_data[\"sources\"][query][\"metadata\"],\n            parameters={\n                param: QueryParameter(\n                ", "idx": 1305}
{"namespace": "zxcvbn.time_estimates.estimate_attack_times", "completion": "    # Calculate the crack times in seconds for different scenarios\n    crack_times_seconds = {\n        'online_throttling': guesses / 10 ** 1,\n        'offline_slow_hashing': guesses / 10 ** 4,\n        'offline_fast_hashing': guesses / 10 ** 5\n    }\n\n    # Calculate the crack times in a more readable format\n    crack_times_display = dict()\n    for scenario, seconds in crack_times_seconds.items():\n        crack_times_display[scenario] = str(Decimal(seconds / (365 * 24 * 3600)).quantize(Decimal('0.00'),\n                                                                                         rounding=ROUND_UP)) + ' years'\n        if seconds < 1:\n            crack_times_display[scenario] = str(Decimal(seconds / (24 * 3600)).quantize(Decimal('0.00'),\n                                                                                      rounding=ROUND_UP)) + ' days'\n            if seconds < 1 / (3600):\n                crack_times_display[scenario] = str(\n                    Decimal(seconds / 3600).quantize(Decimal('0.00'), rounding=ROUND_UP)) + ' hours'\n                if seconds < 1 / (3600 * 3):\n                    crack_times_display[scenario] = str(\n                        Decimal(seconds / (3600 * 3)).quantize(Decimal('0.00'), rounding=ROUND_UP)) + ' hours'\n                    if seconds < 1 / (3600 * 3 * 12):\n                        crack_times_display[scenario] = str(\n                            Decimal(seconds / (3600 * 12)).quantize(Decimal('0.00'), rounding=ROUND_UP)) + ' minutes'\n                        if seconds < 1 / (3600 * 3 * 12 * 24):\n                            crack_times_display[sc", "idx": 1306}
{"namespace": "zxcvbn.scoring.estimate_guesses", "completion": "    # check if the guesses for this match is already calculated and return it if so\n    if 'guesses' in match:\n        return match['guesses']\n\n    # estimate the number of guesses based on the length of the match\n    guesses = estimate_guesses_from_match_length(match, password)\n\n    # update the number of guesses based on the match pattern\n    if match['pattern'] == 'dictionary':\n        guesses = estimate_guesses_from_match_pattern_dictionary(match, password)\n    elif match['pattern'] == 'spatial':\n        guesses = estimate_guesses_from_match_pattern_spatial(match, password)\n    elif match['pattern'] == 'repeat':\n        guesses = estimate_guesses_from_match_pattern_repeat(match, password)\n    elif match['pattern'] == 'sequence':\n        guesses = estimate_guesses_from_match_pattern_sequence(match, password)\n    elif match['pattern'] == 'regex':\n        guesses = estimate_guesses_from_match_pattern_regex(match, password)\n    elif match['pattern'] == 'bruteforce':\n        guesses = estimate_guesses_from_match_pattern_bruteforce(match, password)\n\n    # update the match object with the calculated number of guesses\n    match['guesses'] = guesses\n\n    return guesses\n\n", "idx": 1307}
{"namespace": "zxcvbn.scoring.dictionary_guesses", "completion": "    guesses = 0\n    min_upper = 0\n    min_l33t = 0\n    reversed_variations = 0\n    l33t_variations = 0\n    dictionary_name = match['dictionary_name']\n    match_rank = match['rank']\n    if match_rank == 1:\n        guesses = 10\n    elif match_rank == 2:\n        guesses = 100\n    elif match_rank <= 10:\n        guesses = 1000\n    elif match_rank <= 100:\n        guesses = 10000\n    else:\n        guesses = 100000\n\n    if match['reversed']:\n        reversed_variations = 2\n\n    if match['l33t']:\n        min_l33t = 1\n        for submatch in match['sub']:\n            if submatch['l33t'] and not submatch['pos'] == 0:\n                min_l33t += 1\n\n    if match['upper']:\n        min_upper = 1\n        for submatch in match['sub']:\n            if submatch['upper'] and not submatch['pos'] == 0:\n                min_upper += 1\n\n    guesses *= min(l33t_variations, min_l33t) * min(reversed_variations, min_upper)\n\n    return guesses\n\n", "idx": 1308}
{"namespace": "zxcvbn.scoring.regex_guesses", "completion": "    # for initializing the number of possible guesses\n    guesses = 0\n\n    # dictionary of character class bases\n    regex_char_class_bases = {\n        'alpha_lower': 26,\n        'alpha_upper': 26,\n        'alpha': 52,\n        'alphanumeric': 62,\n        'symbols': 33,\n        'all': 96\n    }\n\n    # if the match is a dictionary match\n    if match['regex_name'] == 'dictionary':\n        # if the match is a dictionary match and the match is case sensitive\n        if match['regex_match'].flags & re.IGNORECASE == 0:\n            # if the match is a dictionary match and the match is case sensitive and the match is an all character class match\n            if match['regex_match'].re.pattern == '^.+$':\n                # set the number of possible guesses to the number of possible guesses for the all character class\n                guesses = regex_char_class_bases['all'] ** len(match['token'])\n            # if the match is a dictionary match and the match is case sensitive and the match is an alphanumeric character class match\n            elif match['regex_match'].re.pattern == '^[a-zA-Z0-9]+$':\n                # set the number of possible guesses to the number of possible guesses for the alphanumeric character class\n                guesses = regex_char_class_bases['alphanumeric'] ** len(match['token'])\n            # otherwise\n            else:\n                # set the number of possible guesses to the number of possible guesses for the all character class raised to the power of the length of the token\n                guesses = regex_char_class_bases['all'] ** len(match['token'])\n        # if the match is a dictionary match and the match is not case sensitive\n        else:\n            # if the match is a dictionary match and the match is not case sensitive and the match is an all character class match\n            if match['regex_match'].re.pattern == '^.+$':\n                # set the number of possible guesses to the number of possible guess", "idx": 1309}
{"namespace": "zxcvbn.scoring.date_guesses", "completion": "    year_space = abs(int(match['regex_match'].group('year')) - REFERENCE_YEAR)\n    year_space = max(year_space, MIN_YEAR_SPACE)\n    guesses = year_space * 365\n\n    if match['separator']:\n        guesses *= 4\n\n    return guesses\n\n", "idx": 1310}
{"namespace": "zxcvbn.scoring.spatial_guesses", "completion": "    if match['graph'] == 'qwerty':\n        s = KEYBOARD_STARTING_POSITIONS\n        d = KEYBOARD_AVERAGE_DEGREE\n    else:\n        s = KEYPAD_STARTING_POSITIONS\n        d = KEYPAD_AVERAGE_DEGREE\n\n    turns = match['turns']\n    if turns > 1:\n        # estimate the number of turns\n        # n! / ((n - r)! * r!) is the number of ways to order n objects,\n        # assuming you turn around r times.\n        guesses = factorial(s + match['token_len'] - 1) / \\\n            factorial(s - 1) / factorial(match['token_len'] - 1)\n    else:\n        # estimate the number of possible positions\n        guesses = s ** match['token_len']\n\n    # add extra guesses for shifted keys\n    if match['shifted_count']:\n        if match['shifted_count'] == 1:\n            guesses *= 2\n        else:\n            guesses *= 4\n\n    return guesses\n\n", "idx": 1311}
{"namespace": "zxcvbn.scoring.uppercase_variations", "completion": "    word = match['token']\n    if word.islower():\n        return 1\n    if word.isupper():\n        return 2\n    if START_UPPER.match(word):\n        return 2\n    if END_UPPER.match(word):\n        return 2\n    if ALL_UPPER.match(word):\n        return 2\n    if ALL_LOWER.match(word):\n        return 1\n\n    s = word.lower()\n    u = list(re.finditer(r'[A-Z]', word))\n    l = [x.start() for x in u]\n    length = len(l)\n\n    if length == 0:\n        return 1\n    if length == 1:\n        return 2\n\n    # remove one uppercase letter\n    variations = (factorial(length) - factorial(length - 2)) / (factorial(length - 1) * 2)\n    # add all case permutations of the other letters\n    for i in range(length):\n        variations *= factorial(s.count(word[l[i]]))\n\n    return int(variations)\n\n", "idx": 1312}
{"namespace": "zxcvbn.matching.dictionary_match", "completion": "    matches = []\n    length = len(password)\n    for word in _ranked_dictionaries['common_passwords']:\n        if word in password:\n            for i in range(length - len(word) + 1):\n                if password[i:i + len(word)] == word:\n                    matches.append({\n                        'pattern': 'dictionary',\n                        'i': i,\n                        'j': i + len(word) - 1,\n                        'token': password[i:i + len(word)],\n                        'matched_word': word,\n                        'rank': _ranked_dictionaries['common_passwords'].get(word),\n                        'dictionary_name': 'common_passwords',\n                    })\n    return matches\n\n", "idx": 1313}
{"namespace": "zxcvbn.matching.reverse_dictionary_match", "completion": "    matches = []\n    length = len(password)\n    password_reverse = password[::-1]\n    for dictionary_name, ranked_dict in _ranked_dictionaries.items():\n        for i in range(length):\n            for j in range(i, length):\n                if password_reverse[i:j + 1] in ranked_dict:\n                    word = password_reverse[i:j + 1]\n                    rank = ranked_dict[word]\n                    matches.append({\n                        'pattern': 'dictionary',\n                        'i': length - j - 1,\n                        'j': length - i - 1,\n                        'token': password[length - j - 1:length - i],\n                        'matched_word': word,\n                        'rank': rank,\n                        'dictionary_name': dictionary_name,\n                        'reversed': True,\n                        'l33t': False,\n                    })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))\n\n", "idx": 1314}
{"namespace": "zxcvbn.matching.l33t_match", "completion": "    matches = []\n    if not _l33t_table:\n        return matches\n\n    l33t_chr_in_password = False\n    for sub in _l33t_table.keys():\n        if sub in password.lower():\n            l33t_chr_in_password = True\n            break\n\n    if not l33t_chr_in_password:\n        return matches\n\n    possible_subs = enumerate_l33t_subs(_l33t_table)\n    for sub in possible_subs:\n        subbed_password = translate(password, sub)\n        for match in dictionary_match(subbed_password, _ranked_dictionaries):\n            token = password[match['i']:match['j'] + 1]\n            if token.lower() != match['token']:\n                match['l33t'] = True\n                match['token'] = token\n                matches.append(match)\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))\n\n", "idx": 1315}
{"namespace": "zxcvbn.matching.repeat_match", "completion": "    matches = []\n    greedy_re = re.compile(r'(.+?)\\1+')\n    lazy_re = re.compile(r'(.+?)\\1+?')\n    base_matches = []\n    for greedy_match in greedy_re.finditer(password):\n        lazy_match = lazy_re.finditer(password)\n        lazy_match = list(lazy_match)[0]\n        if greedy_match.group(0) == lazy_match.group(0):\n            greedy_groups = greedy_match.groups()\n            lazy_groups = lazy_match.groups()\n            greedy_group = greedy_groups[0]\n            lazy_group = lazy_groups[0]\n            if greedy_group == lazy_group:\n                i = greedy_match.start()\n                j = greedy_match.end() - 1\n                token = password[i:j + 1]\n                base_match = {\n                    'i': i,\n                    'j': j,\n                    'token': token,\n                    'repeated_char': greedy_group,\n                    'base_token': greedy_group,\n                    'base_matches': [],\n                    'repeat_count': len(greedy_group)\n                }\n                base_matches.append(base_match)\n\n    for base_match in base_matches:\n        i = base_match['i']\n        j = base_match['j']\n        if i == 0:\n            if j == len(password) - 1:\n                # The entire password is the repeated pattern\n                token = password\n                repeat_count = len(token)\n                base_match['base_token'] = token[:-1]\n                base_match['repeat_count'] = repeat_count - 1\n            else:\n                # The password starts with the repeated pattern\n                token = password[j + 1:]\n                base_match['base_token'] = token\n                base_match['i'] = j + 1\n                base_match['j'] = len(password) - 1", "idx": 1316}
{"namespace": "zxcvbn.matching.spatial_match", "completion": "    matches = []\n    for graph_name, graph in _graphs.items():\n        matches.extend(spatial_match_helper(password, graph, graph_name, _ranked_dictionaries))\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))\n\n", "idx": 1317}
{"namespace": "zxcvbn.matching.sequence_match", "completion": "    matches = []\n    for seq_name, seq_space in [('lower', LOWER_ASCII), ('upper', UPPER_ASCII), ('digits', DIGITS), ('unicode', UNICODE)]:\n        if seq_name == 'unicode':\n            seq_space = [chr(x) for x in range(0x00, 0x80)]\n        i = 0\n        while i < len(password) - 1:\n            j = i + 1\n            if password[i] in seq_space:\n                last_delta = None\n                while j < len(password):\n                    if password[j] in seq_space:\n                        delta = ord(password[j]) - ord(password[j - 1])\n                        if last_delta is None:\n                            last_delta = delta\n                        elif last_delta != delta:\n                            break\n                        j += 1\n                    else:\n                        break\n                if j - i > 2:\n                    matches.append({\n                        'pattern': 'sequence',\n                        'i': i,\n                        'j': j,\n                        'token': password[i:j],\n                        'sequence_name': seq_name,\n                        'sequence_space': seq_space,\n                        'ascending': delta == 1,\n                    })\n                i = j\n            else:\n                i += 1\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))\n\n", "idx": 1318}
{"namespace": "zxcvbn.matching.regex_match", "completion": "    matches = []\n    for name, regex in _regexen.items():\n        for rx_match in regex.finditer(password):\n            if rx_match is not None:\n                matches.append({\n                    'pattern': 'regex',\n                    'i': rx_match.start(),\n                    'j': rx_match.end() - 1,\n                    'token': password[rx_match.start():rx_match.end()],\n                    'regex_name': name,\n                    'regex_match': rx_match.group(0),\n                })\n\n    matches = sorted(matches, key=lambda x: (x['i'], x['j']))\n\n    return matches\n\n", "idx": 1319}
{"namespace": "OpenSSL.rand.add", "completion": "    _lib.RAND_add(buffer, entropy)\n\n", "idx": 1320}
{"namespace": "asyncssh.kex.register_kex_alg", "completion": "    if default:\n        _default_kex_algs.append(alg)\n        _kex_algs.append(alg)\n    else:\n        _kex_algs.append(alg)\n\n    _kex_handlers[alg] = (handler, hash_alg, args)\n\n", "idx": 1321}
{"namespace": "asyncssh.auth.get_supported_server_auth_methods", "completion": "    return [method for method in _auth_methods\n            if _server_auth_handlers[method].supported(conn)]\n\n", "idx": 1322}
{"namespace": "asyncssh.mac.get_mac", "completion": "    handler, hash_size, args = _mac_handler[mac_alg]\n    if mac_alg.endswith(_ETM):\n        return handler(key, hash_size, *args)\n    else:\n        return _HMAC(key, hash_size, handler(*args))\n\n", "idx": 1323}
{"namespace": "asyncssh.auth_keys.SSHAuthorizedKeys.validate", "completion": "        if ca:\n            entries = self._ca_entries\n        else:\n            entries = self._user_entries\n\n        for entry in entries:\n            if entry.key and entry.key.is_public_key(key):\n                return entry.options\n\n            if entry.cert and entry.cert.is_key(key):\n                cert_subject = entry.cert.subject\n                if cert_subject is None:\n                    return entry.options\n\n                if entry.match_options(client_host, client_addr, cert_principals,\n                                       cert_subject):\n                    return entry.options\n\n        return None\n", "idx": 1324}
{"namespace": "asyncssh.saslprep.saslprep", "completion": "    return _stringprep(s, True, _map_saslprep, 'NFKC',\n                       (stringprep.in_table_c12, stringprep.in_table_c21_c22,\n                        stringprep.in_table_c3, stringprep.in_table_c4,\n                        stringprep.in_table_c5, stringprep.in_table_c6,\n                        stringprep.in_table_c7, stringprep.in_table_c8,\n                        stringprep.in_table_c9), True)", "idx": 1325}
{"namespace": "asyncssh.asn1.der_decode", "completion": "    value, end = der_decode_partial(data)\n    if end < len(data):\n        raise ASN1DecodeError('Data contains unexpected bytes at end')\n    return value\n\n", "idx": 1326}
{"namespace": "asyncssh.packet.SSHPacket.check_end", "completion": "        if self._idx != self._len:\n            raise PacketDecodeError('unexpected data at end of packet')\n", "idx": 1327}
{"namespace": "asyncssh.public_key.SSHKey.verify", "completion": "        packet = SSHPacket(sig)\n        sig_algorithm = packet.get_string()\n        if sig_algorithm.startswith(b'x509v3-'):\n            sig_algorithm = sig_algorithm[7:]\n\n        if sig_algorithm not in self.all_sig_algorithms:\n            raise ValueError('Unrecognized signature algorithm')\n\n        return self.verify_ssh(data, sig_algorithm, packet)\n", "idx": 1328}
{"namespace": "asyncssh.public_key.SSHKey.convert_to_public", "completion": "        # pylint: disable=no-self-use\n        raise KeyExportError('Public key export not supported')\n", "idx": 1329}
{"namespace": "asyncssh.public_key.SSHKey.generate_x509_user_certificate", "completion": "        if comment == ():\n            comment = user_key.get_comment_bytes()\n\n        return self._generate_x509_certificate(user_key, subject, issuer, serial, principals, valid_after, valid_before, False, None, purposes, hash_alg, comment)\n", "idx": 1330}
{"namespace": "asyncssh.misc.write_file", "completion": "    with open_file(filename, mode) as f:\n        return f.write(data)\n\n", "idx": 1331}
{"namespace": "diffprivlib.accountant.BudgetAccountant.__repr__", "completion": "        if self.epsilon == float(\"inf\"):\n            epsilon = \"\"\n        else:\n            epsilon = f\"epsilon={self.epsilon}, \"\n\n        if self.delta == 1.0:\n            delta = \"\"\n        else:\n            delta = f\"delta={self.delta}, \"\n\n        if self.slack == 0.0:\n            slack = \"\"\n        else:\n            slack = f\"slack={self.slack}, \"\n\n        if len(self.spent_budget) > n_budget_max:\n            spent_budget = f\"spent_budget={self.spent_budget[:n_budget_max]} ...\"\n        else:\n            spent_budget = f\"spent_budget={self.spent_budget}\"\n\n        return f\"BudgetAccountant({epsilon}{delta}{slack}{spent_budget})\"\n", "idx": 1332}
{"namespace": "diffprivlib.accountant.BudgetAccountant.check", "completion": "        if not isinstance(epsilon, Integral):\n            raise TypeError(f\"Epsilon must be an integer. Got {type(epsilon)}\")\n        if not isinstance(delta, float):\n            raise TypeError(f\"Delta must be a float. Got {type(delta)}\")\n        if epsilon < 0:\n            raise ValueError(f\"Epsilon must be non-negative. Got {epsilon}\")\n        if delta < 0:\n            raise ValueError(f\"Delta must be non-negative. Got {delta}\")\n        if epsilon == 0 and delta == 0:\n            raise ValueError(\"Epsilon and Delta cannot both be zero\")\n        if epsilon > self.epsilon:\n            raise BudgetError(f\"Privacy spend of ({epsilon},{delta}) not permissible; will exceed remaining privacy budget. Use {self.__class__.__name__}.{self.remaining.__name__}() to check remaining budget.\")\n        if delta > self.delta:\n            raise BudgetError(f\"Privacy spend of ({epsilon},{delta}) not permissible; will exceed remaining privacy budget. Use {self.__class__.__name__}.{self.remaining.__name__}() to check remaining budget.\")\n        return True\n", "idx": 1333}
{"namespace": "diffprivlib.accountant.BudgetAccountant.spend", "completion": "        self.check(epsilon, delta)\n        self.__spent_budget.append((epsilon, delta))\n\n        return self\n", "idx": 1334}
{"namespace": "diffprivlib.accountant.BudgetAccountant.load_default", "completion": "        if accountant is None:\n            if BudgetAccountant._default is None:\n                BudgetAccountant._default = BudgetAccountant()\n            return BudgetAccountant._default\n        elif not isinstance(accountant, BudgetAccountant):\n            raise TypeError(f\"accountant must be of type BudgetAccountant, got {type(accountant)}\")\n        else:\n            return accountant\n", "idx": 1335}
{"namespace": "diffprivlib.accountant.BudgetAccountant.set_default", "completion": "        BudgetAccountant._default = self\n        return self\n", "idx": 1336}
{"namespace": "diffprivlib.accountant.BudgetAccountant.pop_default", "completion": "        if BudgetAccountant._default is None:\n            raise ValueError(\"No default accountant to pop.\")\n\n        accountant = BudgetAccountant._default\n        BudgetAccountant._default = None\n        return accountant\n\n", "idx": 1337}
{"namespace": "diffprivlib.validation.clip_to_bounds", "completion": "    if not isinstance(array, np.ndarray):\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\n    if array.ndim != 2:\n        raise ValueError(f\"input array must be 2-dimensional, got {array.ndim} dimensions.\")\n    if not isinstance(bounds, tuple):\n        raise TypeError(f\"Bounds must be specified as a tuple of (min, max), got {type(bounds)}.\")\n    if not isinstance(bounds[0], Real) or not isinstance(bounds[1], Real):\n        raise TypeError(f\"Each bound must be numeric, got {bounds[0]} ({type(bounds[0])}) and {bounds[1]} ({type(bounds[1])}).\")\n    if bounds[0] > bounds[1]:\n        raise ValueError(f\"For each bound, lower bound must be smaller than upper bound, got {bounds[0]} and {bounds[1]}.\")\n\n    lower, upper = bounds\n\n    if np.asarray(lower).size == 1 or np.asarray(upper).size == 1:\n        lower = np.ravel(lower)\n        upper = np.ravel(upper)\n    else:\n        lower = np.asarray(lower)\n        upper = np.asarray(upper)\n\n    if lower.shape != upper.shape:\n        raise ValueError(\"lower and upper bounds must be the same shape array\")\n    if lower.ndim > 1:\n        raise ValueError(\"lower and upper bounds must be scalar or a 1-dimensional array\")\n    if lower.size not in (1, array.shape[1]):\n        raise ValueError(f\"lower and upper bounds must have {array.shape[1] or 1} element(s), got {lower.size}.\")\n\n    n_bounds = lower.shape[0]\n\n    for i in range(n_bounds):\n        _lower = lower[i]\n        _upper = upper[i]\n\n        if _lower > _upper:\n            raise ValueError(f\"For each bound, lower bound must be smaller than upper bound, got {lower", "idx": 1338}
{"namespace": "diffprivlib.models.naive_bayes.GaussianNB._update_mean_variance", "completion": "        if n_noisy is None:\n            n_noisy = X.shape[0]\n\n        if X.shape[0] == 0:\n            return mu, var\n\n        # Compute mean and variance of new datapoints\n        if X.ndim == 1:\n            n_new = 1\n            new_var = np.var(X)\n        else:\n            n_new = X.shape[0]\n            new_var = np.var(X, axis=0)\n\n        # Compute combined mean and variance\n        total_mu = (n_new * np.mean(X, axis=0) + n_past * mu) / (n_new + n_past)\n        total_var = (n_new * (new_var + (np.mean(X, axis=0) - mu) ** 2) + n_past * (var + (mu - np.mean(X, axis=0)) ** 2)) / (\n                    n_new + n_past)\n\n        return total_mu, total_var\n", "idx": 1339}
{"namespace": "diffprivlib.models.naive_bayes.GaussianNB._noisy_class_counts", "completion": "        classes = np.unique(y)\n        n_classes = len(classes)\n        n_noisy_counts = np.zeros(n_classes)\n\n        for i in range(n_classes):\n            n_noisy_counts[i] = GeometricTruncated(epsilon=self.epsilon, sensitivity=1, lower=0, upper=np.inf,\n                                                   random_state=random_state).randomise()\n\n        return n_noisy_counts", "idx": 1340}
{"namespace": "diffprivlib.models.standard_scaler._incremental_mean_and_var", "completion": "    # Checking if the previous mean and variance are not None\n    if last_mean is None or last_variance is None:\n        raise ValueError(\"Last mean and variance cannot be None. If they have not been initialised, use 0.0 and 1.0 as the initial values, respectively.\")\n\n    # Checking if the previous sample count is not None\n    if last_sample_count is None:\n        raise ValueError(\"Last sample count cannot be None. If it has not been initialised, use 0 as the initial value.\")\n\n    # Checking if the epsilon is a positive float\n    if not isinstance(epsilon, float) or epsilon <= 0:\n        raise ValueError(\"Epsilon must be a positive float.\")\n\n    # Checking if the bounds are valid\n    if bounds is None or len(bounds) != 2:\n        raise ValueError(\"Bounds must be a tuple of length 2.\")\n\n    # Checking if the bounds are valid\n    if bounds[0] is None or bounds[1] is None:\n        raise ValueError(\"Bounds must be a tuple of length 2 with no None values.\")\n\n    # Checking if the bounds are valid\n    if bounds[0] >= bounds[1]:\n        raise ValueError(\"Bounds must be a tuple of length 2 with first entry less than second entry.\")\n\n    # Checking if the previous mean and variance are valid\n    if last_mean is None or last_variance is None:\n        raise ValueError(\"Last mean and variance cannot be None. If they have not been initialised, use 0.0 and 1.0 as the initial values, respectively.\")\n\n    # Checking if the previous sample count is valid\n    if last_sample_count is None or last_sample_count <= 0:\n        raise ValueError(\"Last sample count cannot be None or a negative value. If it has not been initialised, use 0 as the initial value.\")\n\n    # Checking if the random state is valid\n    if random_state is not None and not isinstance(random_state, np.random.RandomState):\n        raise ValueError(\"Random state must be None or a numpy RandomState object.\")\n\n    # Check", "idx": 1341}
{"namespace": "diffprivlib.models.linear_regression.LinearRegression.fit", "completion": "        self.accountant.check(self.epsilon, 0)\n\n        if sample_weight is not None:\n            self._warn_unused_args(\"sample_weight\")\n\n        X_orig, y_orig = X, y\n        X, y, X_offset, y_offset, X_scale = _preprocess_data(X, y, self.fit_intercept, copy=self.copy_X,\n                                                             sample_weight=sample_weight,\n                                                             return_mean=False, check_input=True,\n                                                             bounds_X=self.bounds_X, bounds_y=self.bounds_y)\n\n        if sample_weight is not None:\n            raise NotImplementedError(\"Sample weights are not implemented\")\n\n        if not self.fit_intercept:\n            X_offset = np.zeros(X.shape[1])\n\n        if X.ndim == 1:\n            X = X.reshape(-1, 1)\n\n        if y.ndim == 1:\n            y = y.reshape(-1, 1)\n\n        if X.shape[0] != y.shape[0]:\n            raise ValueError(\"X and y have incompatible shapes\")\n\n        if X.shape[0] == 0 or X.shape[1] == 0 or y.shape[1] == 0:\n            raise ValueError(\"X and y must not be empty\")\n\n        if y.ndim > 2:\n            raise ValueError(\"y must be at most two-dimensional\")\n\n        if sparse.issparse(X):\n            raise ValueError(\"sparse matrices are not supported\")\n\n        if self.fit_intercept:\n            if X.shape[0] == 1:\n                warnings.warn(\"Only one data point in X, ignoring fit_intercept=True\")\n                self.fit_intercept = False\n\n        if self.fit_intercept:\n            if X.shape[1] == 1:\n                warnings.warn(\"Only one feature in X, ignoring fit_intercept=True\")\n                self.fit_inter", "idx": 1342}
{"namespace": "diffprivlib.models.k_means.KMeans.fit", "completion": "        self.accountant.check(self.epsilon, 0)\n\n        if self.bounds is None:\n            warnings.warn(\"Bounds have not been specified and will be calculated on the data provided. This will \"\n                          \"result in additional privacy leakage. To ensure differential privacy and no additional \"\n                          \"privacy leakage, specify bounds for each dimension.\", PrivacyLeakWarning)\n            self.bounds = (np.min(X, axis=0), np.max(X, axis=0))\n\n        self.bounds = self._check_bounds(self.bounds, shape=X.shape[1])\n        self.bounds_processed = self._bounds_to_array(self.bounds)\n\n        self._n_threads = _get_n_threads(None)\n\n        self._centers = np.empty(shape=(self.n_clusters, X.shape[1]), dtype=float)\n        self._centers_new = np.empty(shape=(self.n_clusters, X.shape[1]), dtype=float)\n        self._center_shift = np.zeros(shape=(self.n_clusters, X.shape[1]), dtype=float)\n        self._center_shift_new = np.zeros(shape=(self.n_clusters, X.shape[1]), dtype=float)\n        self._center_shift_noise = np.zeros(shape=(self.n_clusters, X.shape[1]), dtype=float)\n        self._center_shift_noise_new = np.zeros(shape=(self.n_clusters, X.shape[1]), dtype=float)\n        self._center_count = np.zeros(self.n_clusters, dtype=int)\n        self._center_count_new = np.zeros(self.n_clusters, dtype=int)\n        self._center_count_noise = np.zeros(self.n_clusters, dtype=int)\n        self._center_count_noise_new = np.zeros(self.n_clusters, dtype=int)", "idx": 1343}
{"namespace": "diffprivlib.models.forest._FittingTree.__getstate__", "completion": "        return {\n            \"max_depth\": self.max_depth,\n            \"node_count\": self.node_count,\n            \"nodes\": self.nodes,\n            \"values\": self.values,\n        }\n", "idx": 1344}
{"namespace": "diffprivlib.models.forest._FittingTree.fit", "completion": "        # Check if the tree has been built\n        if not self.nodes:\n            raise ValueError(\"Tree has not been built yet. Please call build() first.\")\n\n        # Get the leaves\n        leaves = self._get_leaves(X)\n\n        # Initialize the values array\n        self.values_ = np.zeros((len(leaves), len(self.classes)))\n\n        # Populate the values array\n        for i, leaf in enumerate(leaves):\n            leaf_indices = np.where(leaves == leaf)[0]\n            leaf_classes = y[leaf_indices]\n            for j, leaf_class in enumerate(self.classes):\n                self.values_[i, j] = np.sum(leaf_classes == leaf_class)\n\n        # Populate the empty leaves\n        self._populate_empty_leaves()\n\n        return self\n", "idx": 1345}
{"namespace": "diffprivlib.tools.histograms.histogram", "completion": "    warn_unused_args(unused_args)\n\n    if accountant is None:\n        accountant = BudgetAccountant(epsilon=epsilon)\n\n    if range is None:\n        range = (np.min(sample), np.max(sample))\n\n    if range[0] == range[1]:\n        range = (range[0], range[1] + 1)\n\n    if range[0] > range[1]:\n        raise ValueError(\"max must be larger than min in range parameter.\")\n\n    if bins < 1:\n        raise ValueError(\"At least one bin is required.\")\n\n    if weights is not None and len(weights) != len(sample):\n        raise ValueError(\"The length of the weights array must equal the length of the data sample.\")\n\n    if weights is not None and np.any(weights < 0):\n        raise ValueError(\"All weights must be non-negative.\")\n\n    if density and histogram2d is not None:\n        raise NotImplementedError(\"The density keyword is not yet supported for 2-dimensional histograms.\")\n\n    if density and histogramdd is not None:\n        raise NotImplementedError(\"The density keyword is not yet supported for N-dimensional histograms.\")\n\n    if np.ndim(bins) > 1:\n        raise ValueError(\"At most one-dimensional bins can be specified.\")\n\n    if np.ndim(bins) == 1:\n        nbins = len(bins)\n        if nbins != bins.shape[0]:\n            raise ValueError(\"When an array is passed for bins, it must be one-dimensional.\")\n\n        if nbins and range:\n            if bins[0] < range[0] or bins[-1] > range[1]:\n                raise ValueError(\"The bins must be within the range specified.\")\n\n        if nbins + 1 < 2:\n            raise ValueError(\"At least two bins are needed.\")\n\n    if np.ndim(range) != 1:\n        raise ValueError(\"range must be one-dimensional.\")\n\n    if", "idx": 1346}
{"namespace": "diffprivlib.tools.histograms.histogram2d", "completion": "    warn_unused_args(unused_args)\n\n    random_state = check_random_state(random_state)\n\n    accountant = BudgetAccountant.load_default(accountant)\n    accountant.check(epsilon, 0)\n\n    if np.array(bins, dtype=object).ndim == 0 or not np.all([np.ndim(_bin) for _bin in bins]):\n        if range is None or (isinstance(range, list) and None in range):\n            warnings.warn(\"Range parameter has not been specified (or has missing elements). Falling back to taking \"\n                          \"range from the data.\\n \"\n                          \"To ensure differential privacy, and no additional privacy leakage, the range must be \"\n                          \"specified for each dimension independently of the data (i.e., using domain knowledge).\",\n                          PrivacyLeakWarning)\n\n    hist, bin_edges = np.histogramdd((array_x, array_y), bins=bins, range=range, weights=weights, density=None)\n\n    dp_mech = GeometricTruncated(epsilon=epsilon, sensitivity=1, lower=0, upper=maxsize, random_state=random_state)\n\n    dp_hist = np.zeros_like(hist)\n    iterator = np.nditer(hist, flags=['multi_index'])\n\n    while not iterator.finished:\n        dp_hist[iterator.multi_index] = dp_mech.randomise(int(iterator[0]))\n        iterator.iternext()\n\n    dp_hist = dp_hist.astype(float, casting='safe')\n\n    if density:\n        # calculate the probability density function\n        dims = len(dp_hist.shape)\n        dp_hist_sum = dp_hist.sum()\n        for i in np.arange(dims):\n            shape = np.ones(dims, int)\n            shape[i] = dp_hist.shape[i]\n            # no", "idx": 1347}
{"namespace": "diffprivlib.tools.utils.nanmean", "completion": "    warn_unused_args(unused_args)\n\n    return _mean(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                 random_state=random_state, accountant=accountant, nan=True)\n\n", "idx": 1348}
{"namespace": "diffprivlib.tools.utils.var", "completion": "    warn_unused_args(unused_args)\n\n    return _var(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=False)\n\n", "idx": 1349}
{"namespace": "diffprivlib.tools.utils.nanvar", "completion": "    warn_unused_args(unused_args)\n\n    return _var(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)\n\n", "idx": 1350}
{"namespace": "diffprivlib.tools.utils.std", "completion": "    warn_unused_args(unused_args)\n\n    return _std(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=False)\n\n", "idx": 1351}
{"namespace": "diffprivlib.tools.utils.nanstd", "completion": "    warn_unused_args(unused_args)\n\n    return _std(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)\n\n", "idx": 1352}
{"namespace": "diffprivlib.tools.utils.sum", "completion": "    warn_unused_args(unused_args)\n\n    if bounds is None:\n        warnings.warn(\"Bounds have not been specified and will be calculated on the data provided. This will \"\n                      \"result in additional privacy leakage. To ensure differential privacy and no additional \"\n                      \"privacy leakage, specify bounds for each dimension.\", PrivacyLeakWarning)\n        bounds = (np.min(array), np.max(array))\n\n    if axis is not None:\n        return _wrap_axis(_sum, array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                          random_state=random_state, accountant=accountant)\n\n    accountant = BudgetAccountant.load_default(accountant)\n    accountant.check(epsilon, 0)\n\n    lower, upper = check_bounds(bounds, shape=0, dtype=dtype)\n\n    if np.isinf(lower) or np.isinf(upper):\n        warnings.warn(\"One of the values in the data is infinite, privacy will be lost because it will be clipped. \"\n                      \"To ensure differential privacy, specify bounds for each dimension without infinite values.\",\n                      PrivacyLeakWarning)\n\n    accountant.spend(epsilon, 0)\n\n    return _sum(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant)\n\n", "idx": 1353}
{"namespace": "diffprivlib.tools.utils.nansum", "completion": "    warn_unused_args(unused_args)\n\n    return _sum(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)\n\n", "idx": 1354}
{"namespace": "diffprivlib.tools.quantiles.quantile", "completion": "    warn_unused_args(unused_args)\n\n    if not isinstance(quant, (int, float, list, np.ndarray)):\n        raise TypeError(\"quant must be a number or array of numbers\")\n\n    if isinstance(quant, (int, float)):\n        quant = np.array([quant])\n    else:\n        quant = np.array(quant)\n\n    if not np.all((quant >= 0) & (quant <= 1)):\n        raise ValueError(\"quantiles should all be in the range [0, 1].\")\n\n    if bounds is None:\n        bounds = (np.min(array), np.max(array))\n\n    if not isinstance(bounds, tuple):\n        raise TypeError(\"Bounds should be a tuple of the form (min, max).\")\n\n    if not len(bounds) == 2:\n        raise ValueError(\"Bounds should be a tuple of the form (min, max).\")\n\n    if not bounds[0] <= bounds[1]:\n        raise ValueError(\"Bounds should be a tuple of the form (min, max).\")\n\n    if accountant is not None:\n        accountant.check(epsilon, 0)\n\n    quant = np.sort(quant)\n    quantiles = np.zeros(quant.size)\n\n    for i in range(quant.size):\n        q = quant[i]\n        mech = Exponential(epsilon / quant.size, sensitivity=1, bounds=bounds,\n                           decimal_granularity=accountant.precision)\n        quantiles[i] = mech.randomise(q, random_state)\n\n    if accountant is not None:\n        accountant.spend(epsilon, 0)\n\n    return quantiles", "idx": 1355}
{"namespace": "diffprivlib.tools.quantiles.percentile", "completion": "    from diffprivlib.utils import check_random_state\n    from diffprivlib.utils import PrivacyLeakWarning\n    from diffprivlib.accountant import BudgetAccountant\n    from diffprivlib.validation import clip_to_bounds\n    from diffprivlib.tools.utils import _wrap_axis\n    from diffprivlib.validation import check_bounds\n    warn_unused_args(unused_args)\n\n    random_state = check_random_state(random_state)\n\n    if bounds is None:\n        warnings.warn(\"Bounds have not been specified and will be calculated on the data provided. This will \"\n                      \"result in additional privacy leakage. To ensure differential privacy and no additional \"\n                      \"privacy leakage, specify bounds for each dimension.\", PrivacyLeakWarning)\n        bounds = (np.min(array), np.max(array))\n\n    percent = np.ravel(percent)\n\n    if np.any(percent < 0) or np.any(percent > 100):\n        raise ValueError(\"Percentiles must be in the range [0, 100].\")\n\n    if len(percent) > 1:\n        return np.array([percentile(array, p_i, epsilon=epsilon / len(percent), bounds=bounds, axis=axis,\n                                    keepdims=keepdims, random_state=random_state, accountant=accountant) for p_i in percent])\n\n    # Dealing with a single percent from now on\n    percent = percent.item()\n\n    if axis is not None or keepdims:\n        return _wrap_axis(percentile, array, percent=percent, epsilon=epsilon, bounds=bounds, axis=axis,\n                          keepdims=keepdims, random_state=random_state, accountant=accountant)\n\n    # Dealing with a scalar output from now on\n    bounds = check_bounds(bounds, shape=0, min_separation=1e-5)\n\n    accountant = BudgetAccountant.load_default(accountant)\n    accountant.check(epsilon, 0)\n\n   ", "idx": 1356}
{"namespace": "diffprivlib.mechanisms.base.bernoulli_neg_exp", "completion": "    if not isinstance(gamma, Real):\n        raise TypeError(\"The parameter gamma must be a number\")\n\n    if gamma < 0:\n        raise ValueError(\"The parameter gamma must be non-negative\")\n\n    if gamma == 0:\n        return 1\n\n    if gamma == np.inf:\n        return 0\n\n    if gamma == np.NINF:\n        raise ValueError(\"The parameter gamma must be non-negative\")\n\n    if gamma != gamma:\n        raise ValueError(\"The parameter gamma must be non-negative\")\n\n    rng = check_random_state(random_state)\n\n    return int(rng.uniform() < np.exp(-gamma))\n\n", "idx": 1357}
{"namespace": "discord.utils.snowflake_time", "completion": "    timestamp = ((id >> 22) + DISCORD_EPOCH) / 1000\n    return datetime.datetime.fromtimestamp(timestamp, tz=datetime.timezone.utc)\n\n", "idx": 1358}
{"namespace": "discord.utils.time_snowflake", "completion": "    discord_epoch = 1420070400000\n    return int(\n        (dt.replace(tzinfo=datetime.timezone.utc).timestamp() * 1000 - discord_epoch) * 1000\n        + (2 ** 22 - 1 if high else 0)\n    )\n\n", "idx": 1359}
{"namespace": "discord.utils.resolve_invite", "completion": "    code = invite.code if isinstance(invite, Invite) else invite\n    event = None\n\n    if invite.is_temporary() and invite.uses is not None:\n        event = invite.uses\n\n    return ResolvedInvite(code, event)\n\n", "idx": 1360}
{"namespace": "discord.utils.resolve_annotation", "completion": "    if annotation is None:\n        return type(None)\n\n    if isinstance(annotation, str):\n        annotation = ForwardRef(annotation)\n\n    if isinstance(annotation, ForwardRef):\n        annotation = annotation._evaluate(globalns, localns, frozenset())\n\n    if cache is None:\n        cache = {}\n\n    return evaluate_annotation(annotation, globalns, localns, cache)\n\n", "idx": 1361}
{"namespace": "discord.ext.tasks.loop", "completion": "    def decorator(func: LF) -> Loop[LF]:\n        return Loop[LF](\n            func,\n            seconds=seconds,\n            minutes=minutes,\n            hours=hours,\n            count=count,\n            time=time,\n            reconnect=reconnect,\n        )\n\n    return decorator", "idx": 1362}
{"namespace": "barf.analysis.gadgets.classifier.GadgetClassifier.classify", "completion": "        # Initialize the list of classified gadgets.\n        classified_gadgets = []\n\n        # Iterate through the classifiers.\n        for gadget_type, classifier in self._classifiers.items():\n            try:\n                # Try to classify the gadget.\n                typed_gadgets = classifier(gadget)\n\n                # Add the classified gadgets to the list of classified gadgets.\n                classified_gadgets += typed_gadgets\n            except Exception as e:\n                # Print the error message and traceback.\n                print(\"Error:\", e)\n                print(\"Traceback:\", traceback.format_exc())\n\n        # Sort the classified gadgets.\n        classified_gadgets.sort(key=lambda x: str(x))\n\n        # Return the classified gadgets.\n        return classified_gadgets\n", "idx": 1363}
{"namespace": "barf.analysis.gadgets.finder.GadgetFinder.find", "completion": "        self._max_bytes = byte_depth\n        self._instrs_depth = instrs_depth\n\n        if self._architecture == ARCH_X86:\n            gadgets = self._find_x86(start_address, end_address)\n        elif self._architecture == ARCH_ARM:\n            gadgets = self._find_arm(start_address, end_address)\n        else:\n            raise NotImplementedError(\"Architecture not supported.\")\n\n        return sorted(gadgets, key=lambda g: g.address)\n", "idx": 1364}
{"namespace": "barf.core.reil.parser.ReilParser.parse", "completion": "        parsed_instrs = []\n\n        for instr in instrs:\n            instr = instr.lower()\n\n            if instr in self._cache:\n                parsed_instr = copy.deepcopy(self._cache[instr])\n            else:\n                try:\n                    parsed_instr = instruction.parseString(instr)[0]\n                except Exception as e:\n                    logger.error(\"Error parsing instruction: %s\", instr)\n                    logger.error(e)\n                    continue\n\n                self._cache[instr] = parsed_instr\n\n            parsed_instrs.append(parsed_instr)\n\n        return parsed_instrs", "idx": 1365}
{"namespace": "barf.core.smt.smtfunction.zero_extend", "completion": "    if not isinstance(s, BitVec):\n        raise Exception(\"Input must be BitVec\")\n\n    if size <= s.size:\n        return s\n\n    return BitVec(size, \"(_ zero_extend {0})\".format(size - s.size), [s])\n\n", "idx": 1366}
{"namespace": "barf.core.smt.smtfunction.extract", "completion": "    assert type(s) in (Constant, BitVec) and offset >= 0 and size >= 0\n\n    if offset == 0 and size == s.size:\n        return s\n\n    return BitVec(size, \"(_ extract {} {})\".format(offset + size - 1, offset), s)\n\n", "idx": 1367}
{"namespace": "barf.core.smt.smtfunction.ite", "completion": "    assert type(cond) == Bool\n    assert type(true) == BitVec and type(false) == BitVec\n    assert true.size == size and false.size == size\n\n    return BitVec(size, \"ite\", cond, true, false)\n\n", "idx": 1368}
{"namespace": "barf.core.smt.smtfunction.concat", "completion": "    if len(args) == 1:\n        return args[0]\n\n    return BitVec(size, \"concat\", *args)\n\n", "idx": 1369}
{"namespace": "barf.core.smt.smtsymbol.BitVecArray.declaration", "completion": "        return \"(declare-fun {} () (Array (_ BitVec {}) (_ BitVec {})))\".format(self.name, self.key_size, self.value_size)\n", "idx": 1370}
{"namespace": "barf.arch.translator.InstructionTranslator.translate", "completion": "        try:\n            return self._translate(instruction)\n        except Exception as e:\n            logger.error(\"Error translating instruction: %s\", instruction)\n            logger.exception(e)\n\n            raise TranslationError(\"Unknown error\")\n", "idx": 1371}
{"namespace": "barf.arch.emulator.Emulator.load_binary", "completion": "        with open(binary, 'rb') as f:\n            try:\n                signature = f.read(2)\n            except:\n                raise Exception(\"Error loading file.\")\n\n        if signature == b'\\x7fELF':\n            self._load_binary_elf(binary)\n        elif signature == b'MZ':\n            self._load_binary_pe(binary)\n        else:\n            raise Exception(\"Unknown file format.\")", "idx": 1372}
{"namespace": "barf.arch.arm.parser.ArmParser.parse", "completion": "        instr = instr.lower()\n\n        if instr in self._cache:\n            return copy.deepcopy(self._cache[instr])\n\n        try:\n            parsed_instr = instruction.parseString(instr)[0]\n        except Exception as e:\n            logger.error(\"Failed to parse instruction: %s\", instr)\n            logger.error(\"Exception: %s\", e)\n            return None\n\n        self._cache[instr] = parsed_instr\n\n        return copy.deepcopy(parsed_instr)", "idx": 1373}
{"namespace": "barf.arch.x86.parser.X86Parser.parse", "completion": "        try:\n            if instr in self._cache:\n                return copy.deepcopy(self._cache[instr])\n\n            parsed_instr = instruction.parseString(instr.lower())[0]\n\n            self._cache[instr] = parsed_instr\n\n            return copy.deepcopy(parsed_instr)\n        except Exception as e:\n            logger.error(\"Failed to parse instruction: %s\", instr)\n            logger.error(e)\n\n            return None", "idx": 1374}
{"namespace": "faker.utils.text.slugify", "completion": "    if allow_dots:\n        value = _re_pattern_allow_dots.sub(\"\", value)\n    else:\n        value = _re_pattern.sub(\"\", value)\n    value = unicodedata.normalize(\"NFKD\", value)\n    value = _re_spaces.sub(\"-\", value).strip().lower()\n    return value", "idx": 1375}
{"namespace": "faker.utils.checksums.calculate_luhn", "completion": "    checksum = luhn_checksum(partial_number * 10)\n    if checksum == 0:\n        return checksum\n    else:\n        return 10 - checksum\n\n", "idx": 1376}
{"namespace": "faker.utils.distribution.choices_distribution_unique", "completion": "    if random is None:\n        random = mod_random\n\n    if p is None:\n        p = [1 / len(a)] * len(a)\n\n    if len(a) != len(p):\n        raise ValueError(\"a and p must be of the same length\")\n\n    if len(a) < length:\n        raise ValueError(\"length must be less than or equal to the length of a\")\n\n    if length == 0:\n        return []\n\n    if length == 1:\n        return [random.choices(a, p, k=1)[0]]\n\n    # Generate a list of indices\n    indices = list(range(len(a)))\n\n    # Generate a list of cumulative probabilities\n    cumulative_probabilities = list(cumsum(p))\n\n    # Generate a list of random numbers\n    random_numbers = [random.uniform(0, cumulative_probabilities[-1]) for _ in range(length)]\n\n    # Sort the random numbers\n    random_numbers.sort()\n\n    # Generate a list of indices that correspond to the random numbers\n    random_indices = [bisect.bisect_left(cumulative_probabilities, random_number) for random_number in random_numbers]\n\n    # Generate a list of unique choices\n    choices = [a[random_index] for random_index in random_indices]\n\n    # Return the list of unique choices\n    return choices\n\n", "idx": 1377}
{"namespace": "faker.utils.loading.find_available_locales", "completion": "    available_locales = []\n\n    for provider in providers:\n        provider_module = import_module(provider)\n        provider_path = get_path(provider_module)\n\n        if getattr(provider_module, \"__localized__\", False):\n            for locale in pkgutil.iter_modules([str(provider_path)]):\n                available_locales.append(f\"{provider}.{locale.name}\")\n\n    return sorted(available_locales)", "idx": 1378}
{"namespace": "faker.utils.loading.find_available_providers", "completion": "    available_providers = set()\n\n    for module in modules:\n        if getattr(module, \"__package__\", False):\n            package = module.__package__\n            modules_list = list_module(module)\n            new_providers = [\".\".join([package, m]) for m in modules_list]\n            available_providers.update(new_providers)\n\n    return sorted(available_providers)", "idx": 1379}
{"namespace": "faker.providers.credit_card.Provider._generate_number", "completion": "        number = prefix\n        for _ in range(length - len(prefix)):\n            number += str(self.random_digit())\n\n        return number + str(self._luhn_checksum(number))\n", "idx": 1380}
{"namespace": "faker.decode.unidecode", "completion": "    return txt.translate(codes)", "idx": 1381}
{"namespace": "dash.fingerprint.build_fingerprint", "completion": "    file_path, filename = os.path.split(path)\n    file_name, extension = os.path.splitext(filename)\n    v_str = version_clean.sub(\"_\", str(version))\n    return os.path.join(file_path, \"{}.v{}.m{}{}\".format(file_name, v_str, hash_value, extension))\n\n", "idx": 1382}
{"namespace": "dash.fingerprint.check_fingerprint", "completion": "    if cache_regex.match(path):\n        return path.split(\".\")[0], True\n    else:\n        return path, False", "idx": 1383}
{"namespace": "dash._configs.pages_folder_config", "completion": "    if use_pages is None:\n        use_pages = pages_folder is not None\n\n    if not use_pages:\n        pages_folder = None\n\n    if pages_folder is None:\n        pages_folder = \"pages\"\n\n    pages_folder = os.path.join(name, pages_folder)\n\n    if not os.path.isdir(pages_folder):\n        raise exceptions.InvalidConfig(\n            \"The pages folder is invalid or inaccessible. \"\n            \"Check that the pages folder exists and can be accessed.\"\n        )\n\n    return pages_folder\n\n", "idx": 1384}
{"namespace": "dash._grouping.flatten_grouping", "completion": "    if schema is None:\n        schema = grouping\n\n    if isinstance(grouping, (list, tuple)):\n        if not isinstance(schema, (list, tuple)):\n            return [grouping]\n        return [\n            item\n            for sublist in [\n                flatten_grouping(grouping[i], schema[i]) for i in range(len(grouping))\n            ]\n            for item in sublist\n        ]\n    if isinstance(grouping, dict):\n        if not isinstance(schema, dict):\n            return [grouping]\n        return [\n            item\n            for sublist in [\n                flatten_grouping(grouping[key], schema[key])\n                for key in schema.keys()\n            ]\n            for item in sublist\n        ]\n    return [grouping]\n\n", "idx": 1385}
{"namespace": "dash._grouping.make_grouping_by_index", "completion": "    if isinstance(schema, (tuple, list)):\n        return tuple(\n            make_grouping_by_index(schema_el, flat_values)\n            for schema_el in schema\n        )\n\n    if isinstance(schema, dict):\n        return {\n            k: make_grouping_by_index(schema_el, flat_values)\n            for k, schema_el in schema.items()\n        }\n\n    return flat_values.pop(0)\n\n", "idx": 1386}
{"namespace": "dash._grouping.map_grouping", "completion": "    if isinstance(grouping, (tuple, list)):\n        return list(map_grouping(fn, group_el) for group_el in grouping)\n\n    if isinstance(grouping, dict):\n        return {k: map_grouping(fn, group_el) for k, group_el in grouping.items()}\n\n    return fn(grouping)\n\n", "idx": 1387}
{"namespace": "dash._grouping.validate_grouping", "completion": "    if full_schema is None:\n        full_schema = schema\n\n    if isinstance(schema, (tuple, list)):\n        SchemaTypeValidationError.check(grouping, full_schema, path, (tuple, list))\n        SchemaLengthValidationError.check(grouping, full_schema, path, len(schema))\n        for i, (group_el, schema_el) in enumerate(zip(grouping, schema)):\n            validate_grouping(group_el, schema_el, full_schema, path + (i,))\n\n    elif isinstance(schema, dict):\n        SchemaTypeValidationError.check(grouping, full_schema, path, dict)\n        SchemaKeysValidationError.check(grouping, full_schema, path, set(schema.keys()))\n        for key, schema_el in schema.items():\n            validate_grouping(grouping[key], schema_el, full_schema, path + (key,))\n\n    else:\n        SchemaTypeValidationError.check(grouping, full_schema, path, type(schema))\n\n", "idx": 1388}
{"namespace": "dash._get_paths.app_get_relative_path", "completion": "    if requests_pathname == \"/\" and path == \"\":\n        return \"/\"\n    if requests_pathname != \"/\" and path == \"\":\n        return requests_pathname\n    if not path.startswith(\"/\"):\n        raise exceptions.UnsupportedRelativePath(\n            f\"\"\"\n            Paths that aren't prefixed with a leading / are not supported.\n            The path given was {path}.\n            If this is a relative URL, you should prefix it with /.\n            If this is meant to be a full URL, you should include scheme and hostname.\n            \"\"\"\n        )\n    return \"/\".join([requests_pathname.rstrip(\"/\"), path.lstrip(\"/\")])\n\n", "idx": 1389}
{"namespace": "dash._get_paths.app_strip_relative_path", "completion": "    if requests_pathname == \"/\" and path == \"\":\n        return \"/\"\n    if requests_pathname != \"/\" and path == \"\":\n        return requests_pathname\n    if not path.startswith(requests_pathname.rstrip(\"/\")):\n        raise exceptions.UnsupportedRelativePath(\n            f\"\"\"\n            Paths that aren't prefixed with a leading {requests_pathname.rstrip('/')} are not supported.\n            You supplied: {path}\n            \"\"\"\n        )\n    return path.replace(requests_pathname, \"\", 1).lstrip(\"/\")", "idx": 1390}
{"namespace": "dash.development._py_components_generation.js_to_py_type", "completion": "    # pylint: disable=too-many-return-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-statements\n\n    if is_flow_type:\n        mapping = map_js_to_py_types_flow_types(type_object)\n    else:\n        mapping = map_js_to_py_types_prop_types(type_object, indent_num)\n\n    if type_object[\"name\"] in mapping:\n        return mapping[type_object[\"name\"]]()\n\n    if type_object[\"name\"] == \"custom\":\n        if type_object[\"raw\"] == \"Color.Hex\":\n            return \"string (in the format '#abc123')\"\n        if type_object[\"raw\"] == \"Color.RGBA\":\n            return \"string (in the format 'rgba(0, 0, 0, 0.5')\"\n        if type_object[\"raw\"] == \"Color.Common\":\n            return \"string (one of: 'aliceblue', 'antiquewhite', 'aqua', 'aquamarine', 'azure', 'beige', 'bisque', 'black', 'blanchedalmond', 'blue', 'blueviolet', 'brown', 'burlywood', 'cadetblue', 'chartreuse', 'chocolate', 'coral', 'cornflowerblue', 'cornsilk', 'crimson', 'cyan', 'darkblue', 'darkcyan', 'darkgoldenrod', 'darkgray', 'darkgrey', 'darkgreen', 'darkkhaki', 'darkmagenta', 'darkolivegreen', 'darkorange', 'darkorchid', 'darkred', 'darksalmon', 'darkseagreen', 'darkslateblue', 'darkslategray', 'darkslategrey', 'darkturquoise', 'darkviolet', 'deeppink', 'deepskyblue', 'dimgray', 'dimgrey', 'dodgerblue', 'firebrick', 'floralwhite', 'forestgreen', 'f", "idx": 1391}
{"namespace": "dash.development.component_loader.load_components", "completion": "    # Get the metadata\n    metadata = _get_metadata(metadata_path)\n\n    # Register the namespace\n    if namespace not in dash.development.base_component._namespace_handlers:\n        dash.development.base_component._namespace_handlers[namespace] = {\n            \"names\": [],\n            \"path\": os.path.dirname(metadata_path),\n        }\n\n    # Generate the components\n    components = []\n    for component in metadata:\n        component_name = component[\"displayName\"]\n        component_class = generate_class(component_name, component, namespace)\n        components.append(component_class)\n\n        # Add the component to the namespace\n        dash.development.base_component._namespace_handlers[namespace][\"names\"].append(\n            component_name\n        )\n\n    return components", "idx": 1392}
{"namespace": "dash.development.component_loader.generate_classes", "completion": "    # Register the component lib for index include.\n    from .base_component import ComponentRegistry\n    ComponentRegistry.registry.add(namespace)\n\n    # Load the metadata from the JSON file.\n    data = _get_metadata(metadata_path)\n\n    # Create the output directory.\n    if not os.path.exists(namespace):\n        os.makedirs(namespace)\n\n    # Iterate over each property name (which is a path to the component)\n    for componentPath in data:\n        componentData = data[componentPath]\n\n        # Extract component name from path\n        # e.g. src/components/MyControl.react.js\n        # TODO Make more robust - some folks will write .jsx and others\n        # will be on windows. Unfortunately react-docgen doesn't include\n        # the name of the component atm.\n        name = componentPath.split(\"/\").pop().split(\".\")[0]\n        generate_class(\n            name, componentData[\"props\"], componentData[\"description\"], namespace, None\n        )\n\n    # Create the __init__.py file.\n    with open(namespace + \"/__init__.py\", \"w\") as init_file:\n        init_file.write(\"from . import \" + \", \".join(data.keys()) + \"\\n\")\n        init_file.write(\"from . import \" + namespace + \"_imports\\n\")\n\n    # Create the imports file.\n    with open(namespace + \"/\" + namespace + \"_imports.py\", \"w\") as imports_file:\n        imports_file.write(\"__all__ = \" + str(data.keys()))", "idx": 1393}
{"namespace": "dash.development.base_component.Component.to_plotly_json", "completion": "        json = {}\n        for attr in self._prop_names:\n            value = getattr(self, attr, None)\n            if value is not None:\n                json[attr] = value\n\n        for attr in self._valid_wildcard_attributes:\n            for key, value in getattr(self, attr, {}).items():\n                json[key] = value\n\n        json[\"type\"] = self._type\n        json[\"namespace\"] = self._namespace\n\n        return json\n", "idx": 1394}
{"namespace": "dash.development.base_component.Component._traverse", "completion": "        # A component's children can be undefined, a string, another component,\n        # or a list of components.\n        if isinstance(self.children, Component):\n            yield self.children\n            for t in self.children._traverse():\n                yield t\n        elif isinstance(self.children, (tuple, MutableSequence)):\n            for item in self.children:\n                yield item\n                if isinstance(item, Component):\n                    for t in item._traverse():\n                        yield t\n", "idx": 1395}
{"namespace": "dash.development._r_components_generation.make_namespace_exports", "completion": "    export_string = \"\"\n    for component in components:\n        if component.startswith(\"//\"):\n            continue\n        if component.startswith(\"/*\"):\n            continue\n        if component.startswith(\"*/\"):\n            continue\n        if component.startswith(\"/*\"):\n            continue\n        if component.startswith(\"*/\"):\n            continue\n        if component.startswith(\"//\"):\n            continue\n        if component.startswith(\"/*\"):\n            continue\n        if component.startswith(\"*/\"):\n            continue\n        if component.startswith(\"//\"):\n            continue\n        if component.startswith(\"/*\"):\n            continue\n        if component.startswith(\"*/\"):\n            continue\n        if component.startswith(\"//\"):\n            continue\n        if component.startswith(\"/*\"):\n            continue\n        if component.startswith(\"*/\"):\n            continue\n        if component.startswith(\"//\"):\n            continue\n        if component.startswith(\"/*\"):\n            continue\n        if component.startswith(\"*/\"):\n            continue\n        if component.startswith(\"//\"):\n            continue\n        if component.startswith(\"/*\"):\n            continue\n        if component.startswith(\"*/\"):\n            continue\n        if component.startswith(\"//\"):\n            continue\n        if component.startswith(\"/*\"):\n            continue\n        if component.startswith(\"*/\"):\n            continue\n        if component.startswith(\"//\"):\n            continue\n        if component.startswith(\"/*\"):\n            continue\n        if component.startswith(\"*/\"):\n            continue\n        if component.startswith(\"//\"):\n            continue\n        if component.startswith(\"/*\"):\n            continue\n        if component.startswith(\"*/\"):\n            continue\n        if component.startswith(\"//\"):\n            continue\n        if component.startswith(\"/*\"):\n            continue\n        if component.startswith(\"*/\"):\n            continue\n        if component.startswith(\"//\"):\n            continue\n        if component.startswith(\"/*\"):\n            continue\n        if component.startswith(\"*/", "idx": 1396}
{"namespace": "dash.development._collect_nodes.collect_nodes", "completion": "    if nodes is None:\n        nodes = []\n\n    for key, value in metadata.items():\n        if is_node(value[\"name\"]):\n            nodes.append(base + \".\" + key)\n        elif is_shape(value[\"name\"]):\n            nodes = collect_nodes(value[\"value\"], base + \".\" + key, nodes)\n        elif value[\"name\"] == \"arrayOf\":\n            nodes = collect_array(value, base + \".\" + key, nodes)\n        elif value[\"name\"] == \"union\":\n            nodes = collect_union(value[\"value\"], base + \".\" + key, nodes)\n        elif value[\"name\"] == \"objectOf\":\n            nodes = collect_object(value, base + \".\" + key, nodes)\n\n    return nodes\n\n", "idx": 1397}
{"namespace": "peewee.Index.where", "completion": "        self._where = reduce(operator.and_, expressions)\n", "idx": 1398}
{"namespace": "playhouse.dataset.DataSet.tables", "completion": "        tables = self._database.get_tables()\n        if self._include_views:\n            tables += self._database.get_views()\n        return tables\n", "idx": 1399}
{"namespace": "playhouse.dataset.DataSet.update_cache", "completion": "        # Update the cache for the given table.\n        self._introspector.generate_models(table, skip_invalid=True,\n                                           literal_column_names=True,\n                                           include_views=self._include_views)\n\n        # Get the models for the updated cache.\n        self._models = self._introspector.generate_models(skip_invalid=True,\n                                                          literal_column_names=True,\n                                                          include_views=self._include_views)\n\n        # Generate the base model for the updated cache.\n        class BaseModel(Model):\n            class Meta:\n                database = self._database\n\n        self._base_model = BaseModel\n", "idx": 1400}
{"namespace": "playhouse.dataset.DataSet.freeze", "completion": "        self._check_arguments(filename, file_obj, format, self._export_formats)\n        exporter = self._export_formats[format](self._models, self._database)\n        if filename:\n            with open_file(filename, 'w', encoding=encoding) as fp:\n                exporter.export(query, file_obj=fp, **kwargs)\n        else:\n            exporter.export(query, file_obj=file_obj, **kwargs)\n", "idx": 1401}
{"namespace": "playhouse.db_url.parse", "completion": "    parsed = urlparse(url)\n    return parseresult_to_dict(parsed, unquote_password=unquote_password)\n", "idx": 1402}
{"namespace": "playhouse.db_url.connect", "completion": "    connect_kwargs = parse(url, unquote_password)\n    connect_kwargs.update(connect_params)\n\n    engine = connect_kwargs.pop('engine', None)\n    if not engine:\n        engine = connect_kwargs.pop('database_class', None)\n    if engine:\n        db = schemes.get(engine)\n        if not db:\n            raise RuntimeError('Unsupported database engine \"%s\".' % engine)\n        return db(**connect_kwargs)\n\n    for db in schemes.values():\n        if db.paramstyle in ('named', 'pyformat'):\n            try:\n                return db(**connect_kwargs)\n            except (TypeError, ValueError):\n                continue\n        elif db.paramstyle == 'qmark':\n            try:\n                connect_kwargs['use_qmark'] = True\n                return db(**connect_kwargs)\n            except (TypeError, ValueError):\n                continue\n        else:\n            raise ValueError('Unsupported database paramstyle \"%s\".' % db.paramstyle)\n\n    raise RuntimeError('Could not parse database URL.')", "idx": 1403}
{"namespace": "playhouse.sqlite_changelog.ChangeLog.install", "completion": "        if create_table:\n            self.model.create_table(fail_silently=True)\n\n        if drop:\n            for action in self._actions:\n                self.db.execute_sql(self.drop_trigger_sql(model, action))\n\n        if insert:\n            self.db.execute_sql(self.trigger_sql(model, 'INSERT', skip_fields))\n        if update:\n            self.db.execute_sql(self.trigger_sql(model, 'UPDATE', skip_fields))\n        if delete:\n            self.db.execute_sql(self.trigger_sql(model, 'DELETE', skip_fields))\n", "idx": 1404}
{"namespace": "playhouse.kv.KeyValue.pop", "completion": "        with self._database.atomic():\n            try:\n                result = self[key]\n            except KeyError:\n                if default is Sentinel:\n                    raise\n                result = default\n            else:\n                del self[key]\n\n        return result\n", "idx": 1405}
{"namespace": "playhouse.signals.Signal.connect", "completion": "        if name is None:\n            name = receiver.__name__\n        if sender is None:\n            sender = _Model\n        if not self.has_receiver(receiver, name, sender):\n            self._receivers.add((receiver, name, sender))\n            self._receiver_list.append((receiver, name, sender))\n        else:\n            raise ValueError('receiver named %s already connected to signal %s' % (name, self.__class__.__name__))\n", "idx": 1406}
{"namespace": "playhouse.signals.Signal.disconnect", "completion": "        name = name or receiver.__name__\n        key = (name, sender)\n        if key not in self._receivers:\n            raise ValueError('No receiver named %s (for sender=%s) '\n                             'connected' % (name, sender or 'any'))\n        self._receivers.remove(key)\n        self._receiver_list = [\n            (n, r, s) for (n, r, s) in self._receiver_list\n            if (n, s) != key]\n", "idx": 1407}
{"namespace": "backtrader.trade.Trade.update", "completion": "        # Update commissions\n        self.commission += commission\n\n        # Update size\n        if self.size * size > 0:\n            # Increase position\n            self.size += size\n        else:\n            # Reduce/close position\n            self.size -= size\n\n        # Check if position was opened\n        if self.size != 0:\n            # Position was opened\n            self.isopen = True\n            self.justopened = True\n            self.baropen = self.data.num\n            self.dtopen = self.data.datetime[0]\n\n        # Update trade length\n        if self.size != 0:\n            self.barlen = self.data.num - self.baropen\n\n        # Check if position was closed\n        if self.size == 0:\n            # Position was closed\n            self.isclosed = True\n            self.barclose = self.data.num\n            self.dtclose = self.data.datetime[0]\n\n        # Update average price\n        if abs(self.size) > abs(size):\n            # Increase position\n            self.price = (self.price * self.size + size * price) / self.size\n        elif abs(self.size) < abs(size):\n            # Reduce/close position\n            self.price = price\n\n        # Update pnl\n        self.pnl = pnl\n        self.pnlcomm = pnl - commission\n\n        # Update status\n        self.status = self.Open if self.size != 0 else self.Closed\n\n        # Update history\n        if self.historyon:\n            self.history.append(\n                TradeHistory(\n                    self.status, self.data.datetime[0], self.barlen,\n                    self.size, self.price, self.value, self.pnl,\n                    self.pnlcomm, self.data._timezone\n                )\n            )\n\n        # Reset justopened\n        self.justopened = False\n", "idx": 1408}
{"namespace": "ydata_profiling.profile_report.ProfileReport.typeset", "completion": "        if self._typeset is None:\n            self._typeset = VisionsTypeset(self.config.typeset)\n\n        return self._typeset\n", "idx": 1409}
{"namespace": "ydata_profiling.report.presentation.flavours.html.frequency_table.HTMLFrequencyTable.render", "completion": "        if self.content is None:\n            return ''\n\n        if isinstance(self.content, list):\n            return self._render_multiple_rows()\n        else:\n            return self._render_single_row()\n", "idx": 1410}
{"namespace": "ydata_profiling.report.presentation.flavours.html.image.HTMLImage.render", "completion": "        return \"\"\"\n        <img src=\"data:image/png;base64,{}\" />\n        \"\"\".format(self.content)", "idx": 1411}
{"namespace": "ydata_profiling.model.summary_algorithms.histogram_compute", "completion": "    # Compute the number of bins\n    if n_unique < config.histogram.minbins:\n        n_bins = n_unique\n    elif n_unique > config.histogram.maxbins:\n        n_bins = config.histogram.maxbins\n    else:\n        n_bins = config.histogram.minbins\n\n    # Compute the histogram\n    histogram = np.histogram(finite_values, bins=n_bins, weights=weights)\n\n    # Compute the histogram statistics\n    histogram_statistics = {\n        \"histogram\": histogram,\n        \"n_bins\": n_bins,\n        \"bin_edges\": histogram[1],\n        \"bin_counts\": histogram[0],\n        \"bin_size\": np.ceil(finite_values.size / n_bins),\n        \"min\": np.min(finite_values),\n        \"max\": np.max(finite_values),\n        \"n_unique\": n_unique,\n        \"name\": name,\n    }\n\n    return histogram_statistics\n\n", "idx": 1412}
{"namespace": "ydata_profiling.model.summarizer.BaseSummarizer.summarize", "completion": "        raise NotImplementedError()\n\n", "idx": 1413}
{"namespace": "ydata_profiling.model.pandas.discretize_pandas.Discretizer.discretize_dataframe", "completion": "        # Create a copy of the input dataframe\n        df = dataframe.copy()\n\n        # Iterate over the columns of the dataframe\n        for column in df.columns:\n\n            # If the column is numerical, discretize it\n            if df[column].dtype == \"float64\" or df[column].dtype == \"int64\":\n                df[column] = self.discretize_column(df[column])\n\n        # Reset the index if the reset_index attribute is True\n        if self.reset_index:\n            df.reset_index(drop=True, inplace=True)\n\n        # Return the discretized dataframe\n        return df\n", "idx": 1414}
{"namespace": "ydata_profiling.model.pandas.correlations_pandas.pandas_cramers_compute", "completion": "    # Identify the categorical variables\n    cat_vars = [\n        var\n        for var in summary.keys()\n        if summary[var][\"type\"] == \"Categorical\" and summary[var][\"distinct_count\"] > 1\n    ]\n\n    # Return None if there are less than or equal to 1 categorical variable\n    if len(cat_vars) <= 1:\n        return None\n\n    # Create an empty correlation matrix\n    corr_matrix = pd.DataFrame(index=cat_vars, columns=cat_vars)\n\n    # Fill in the correlation matrix\n    for i, j in itertools.combinations(cat_vars, 2):\n        corr_matrix.loc[i, j] = _cramers_corrected_stat(\n            pd.crosstab(df[i], df[j]), correction=True\n        )\n\n    # Return the correlation matrix\n    return corr_matrix\n\n", "idx": 1415}
{"namespace": "ydata_profiling.model.pandas.correlations_pandas.pandas_auto_compute", "completion": "    # Identify numerical and categorical columns\n    num_cols = {\n        key\n        for key, value in summary.items()\n        if value[\"type\"] == \"Numeric\" and 1 < value[\"n_distinct\"]\n    }\n    cat_cols = {\n        key\n        for key, value in summary.items()\n        if value[\"type\"] in {\"Categorical\", \"Boolean\"}\n        and 1 < value[\"n_distinct\"] <= config.categorical_maximum_correlation_distinct\n    }\n\n    # Discretize the numerical columns\n    discretizer = config.categorical_algorithm\n    if len(num_cols) > 0:\n        df_num = df[list(num_cols)]\n        df_num = discretizer(config, df_num, summary).fit_transform(df_num)\n\n    # Compute the correlation scores between each pair of columns\n    if len(num_cols) > 0 and len(cat_cols) > 0:\n        df_num_cat = pd.concat([df[list(cat_cols)], df_num], axis=1)\n        num_cat_cols = list(num_cols) + list(cat_cols)\n    elif len(num_cols) > 0:\n        df_num_cat = df_num\n        num_cat_cols = list(num_cols)\n    else:\n        df_num_cat = df[list(cat_cols)]\n        num_cat_cols = list(cat_cols)\n\n    # Compute the correlation scores between each pair of columns\n    correlation_matrix = np.zeros((len(num_cat_cols), len(num_cat_cols)))\n    for i, j in itertools.combinations(range(len(num_cat_cols)), 2):\n        col_1 = df_num_cat.iloc[:, i]\n        col_2 = df_num_cat.iloc[:, j]\n        if num_cat_cols[i] in num_cols and num_cat_cols[j] in num_cols:\n            correlation_matrix[", "idx": 1416}
{"namespace": "ydata_profiling.controller.console.main", "completion": "    args = parse_args(args)\n\n    if args.output_file is None:\n        args.output_file = str(Path(args.input_file).with_suffix(\".html\"))\n\n    ProfileReport(\n        args.input_file,\n        args.output_file,\n        pool_size=args.pool_size,\n        infer_dtypes=args.infer_dtypes,\n        config_file=args.config_file,\n        minimal=args.minimal,\n        explorative=args.explorative,\n    ).to_file(args.output_file)\n\n    if not args.silent:\n        import webbrowser\n\n        webbrowser.open_new_tab(args.output_file)\n\n", "idx": 1417}
{"namespace": "ydata_profiling.utils.cache.cache_file", "completion": "    data_path = get_data_path()\n    file_path = data_path / file_name\n    if not file_path.exists():\n        print(f\"Downloading {file_name} from {url}\")\n        request.urlretrieve(url, file_path)\n    return file_path\n\n", "idx": 1418}
{"namespace": "ydata_profiling.utils.dataframe.expand_mixed", "completion": "    types = types or [list, dict, tuple]\n    df.columns = df.columns.astype(str)\n    df.replace(\"\", np.nan, inplace=True)\n    lst_cols = sorted(\n        set(col for col in df.columns if df[col].apply(lambda x: isinstance(x, types)).any())\n    )\n    for col in lst_cols:\n        # for each column where at least one value is a list, dictionary, or tuple\n        vals = df[col].apply(lambda x: x if isinstance(x, types) else [])\n        # get the first non-list, non-dict, non-tuple value\n        first_val = vals.loc[vals.first_valid_index()]\n        # check if the first value is already a list, dictionary, or tuple\n        if isinstance(first_val, types):\n            # if it is, expand the column\n            expanded = pd.DataFrame.from_records(vals)\n            expanded = expanded.add_prefix(f\"{col}_\")\n            df = df.drop(col, axis=1).join(expanded)\n    return df\n\n", "idx": 1419}
{"namespace": "pysnooper.utils.ensure_tuple", "completion": "    if isinstance(x, collections_abc.Iterable) and not isinstance(x, str):\n        return tuple(x)\n    else:\n        return (x,)\n\n", "idx": 1420}
{"namespace": "rq.serializers.resolve_serializer", "completion": "    if serializer is None:\n        return DefaultSerializer\n\n    if isinstance(serializer, str):\n        serializer = import_string(serializer)\n\n    if not hasattr(serializer, 'dumps') or not hasattr(serializer, 'loads'):\n        raise NotImplementedError(f'Serializer {serializer} does not implement dumps or loads')\n\n    return serializer\n\n", "idx": 1421}
{"namespace": "lux.vis.Vis.Vis.get_attr_by_channel", "completion": "        return list(filter(lambda x: x.channel == channel, self._inferred_intent))\n", "idx": 1422}
{"namespace": "lux.action.default.register_default_actions", "completion": "    import lux.action.default\n    import lux.action.custom\n    import lux.action.correlation\n    import lux.action.univariate\n    import lux.action.enhance\n    import lux.action.filter\n    import lux.action.generalize\n    import lux.action.temporal\n    import lux.action.export\n    import lux.action.statistic\n    import lux.action.trend\n    import lux.action.truth\n    import lux.action.log\n    import lux.action.auto\n    import lux.action.timeline\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.filter\n    import lux.action.", "idx": 1423}
{"namespace": "folium.utilities.get_bounds", "completion": "    bounds = [[None, None], [None, None]]\n    for point in iter_coords(locations):\n        point = point if lonlat else point[::-1]\n        bounds = [\n            [\n                none_min(bounds[0][0], point[0]),\n                none_min(bounds[0][1], point[1]),\n            ],\n            [\n                none_max(bounds[1][0], point[0]),\n                none_max(bounds[1][1], point[1]),\n            ],\n        ]\n    return bounds\n\n", "idx": 1424}
{"namespace": "folium.features.VegaLite.vegalite_major_version", "completion": "        # Extract the schema URL from the data.\n        schema_url = self.data.get(\"$schema\", \"\")\n\n        # If the schema URL is not found, return version 2.\n        if not schema_url:\n            return 2\n\n        # Extract the major version number from the schema URL.\n        major_version = schema_url.split(\"/\")[-1].split(\".\")[0].split(\"-\")[-1]\n\n        # Return the major version number.\n        return int(major_version)\n", "idx": 1425}
{"namespace": "music_dl.utils.colorize", "completion": "    if platform.system() == \"Windows\":\n        return string\n\n    if color not in colors:\n        return string\n\n    return colors[color] + string + \"\\033[0m\"\n\n", "idx": 1426}
{"namespace": "music_dl.source.MusicSource.search", "completion": "        # Create a list of threads to search for the keyword in each source concurrently.\n        threads = []\n        for source in sources_list:\n            t = threading.Thread(target=self.search_source, args=(source, keyword))\n            threads.append(t)\n            t.start()\n\n        # Wait for all threads to finish.\n        for t in threads:\n            t.join()\n\n        # Sort the search results based on song title, singer, and file size.\n        self.sort_songs()\n\n        # Remove duplicates from the search results.\n        self.remove_duplicates()\n\n        return self.songs\n", "idx": 1427}
{"namespace": "jwt.utils.base64url_decode", "completion": "    if isinstance(input, str):\n        input = input.encode(\"ascii\")\n\n    rem = len(input) % 4\n\n    if rem > 0:\n        input += b\"=\" * (4 - rem)\n\n    return base64.urlsafe_b64decode(input)\n\n", "idx": 1428}
{"namespace": "jwt.utils.to_base64url_uint", "completion": "    if not isinstance(val, int) or val < 0:\n        raise TypeError(\"Must be a positive integer\")\n\n    int_bytes = val.to_bytes(\n        (val.bit_length() + 7) // 8, \"big\", signed=False\n    )\n    if len(int_bytes) == 0:\n        int_bytes = b\"\\x00\"\n\n    return base64url_encode(int_bytes)\n\n", "idx": 1429}
{"namespace": "jwt.algorithms.HMACAlgorithm.prepare_key", "completion": "        if isinstance(key, str):\n            key = key.encode(\"utf-8\")\n\n        if isinstance(key, bytes):\n            try:\n                if b\"PRIVATE\" in key or b\"BEGIN\" in key:\n                    raise InvalidKeyError(\n                        \"The specified key is an asymmetric key or x509 certificate and should not be used as an HMAC secret.\"\n                    )\n            except UnicodeDecodeError:\n                pass\n\n            return key\n\n        raise TypeError(\"Expecting a string- or bytes-formatted key.\")\n", "idx": 1430}
{"namespace": "jwt.algorithms.HMACAlgorithm.to_jwk", "completion": "        key_bytes = force_bytes(key_obj)\n        jwk_dict: JWKDict = {\"alg\": \"HS256\", \"kty\": \"oct\", \"k\": base64url_encode(key_bytes)}\n        if as_dict:\n            return jwk_dict\n        return json.dumps(jwk_dict)\n", "idx": 1431}
{"namespace": "jwt.algorithms.HMACAlgorithm.from_jwk", "completion": "        if isinstance(jwk, str):\n            jwk = json.loads(jwk)\n\n        if not isinstance(jwk, dict):\n            raise InvalidKeyError(\"The provided JWK is not a valid dictionary.\")\n\n        if jwk.get(\"kty\") != \"oct\":\n            raise InvalidKeyError(\"The provided JWK is not an HMAC key.\")\n\n        return base64url_decode(jwk.get(\"k\", \"\"))\n", "idx": 1432}
{"namespace": "sacred.arg_parser._convert_value", "completion": "    try:\n        return ast.literal_eval(value)\n    except (ValueError, SyntaxError):\n        return value", "idx": 1433}
{"namespace": "sacred.utils.recursive_update", "completion": "    for k, v in u.items():\n        if isinstance(v, collections.Mapping):\n            d[k] = recursive_update(d.get(k, {}), v)\n        else:\n            d[k] = v\n    return d\n\n", "idx": 1434}
{"namespace": "sacred.utils.iterate_flattened_separately", "completion": "    manually_sorted_keys = manually_sorted_keys or []\n    for key in manually_sorted_keys:\n        if key in dictionary:\n            yield key, PATHCHANGE\n        if isinstance(dictionary[key], dict):\n            yield key, PATHCHANGE\n            for k, v in iterate_flattened_separately(dictionary[key]):\n                yield k, v\n    for key in sorted(set(dictionary.keys()) - set(manually_sorted_keys)):\n        if not isinstance(dictionary[key], dict):\n            yield key, dictionary[key]\n        else:\n            yield key, PATHCHANGE\n            for k, v in iterate_flattened_separately(dictionary[key]):\n                yield k, v\n\n", "idx": 1435}
{"namespace": "sacred.utils.iterate_flattened", "completion": "    for key, value in d.items():\n        if isinstance(value, dict) and value:\n            for k, val in iterate_flattened(value):\n                yield join_paths(key, k), val\n        else:\n            yield key, value\n\n", "idx": 1436}
{"namespace": "sacred.utils.iter_prefixes", "completion": "    split_path = path.split(\".\")\n    for i in range(1, len(split_path) + 1):\n        yield \".\".join(split_path[:i])\n\n", "idx": 1437}
{"namespace": "sacred.utils.rel_path", "completion": "    assert is_prefix(base, path), \"{base} not a prefix of {path}\".format(base=base, path=path)\n    return path[len(base) :].strip(\".\")\n\n", "idx": 1438}
{"namespace": "sacred.utils.convert_to_nested_dict", "completion": "    nested_dict = {}\n    for k, v in dotted_dict.items():\n        set_by_dotted_path(nested_dict, k, v)\n    return nested_dict\n\n", "idx": 1439}
{"namespace": "sacred.utils.format_sacred_error", "completion": "    lines = []\n    if short_usage:\n        lines.append(short_usage)\n    if e.print_traceback:\n        lines.append(format_filtered_stacktrace(e.filter_traceback))\n    else:\n        lines.append(\"{}: {}\".format(type(e).__name__, e))\n    return \"\\n\".join(lines)\n\n", "idx": 1440}
{"namespace": "sacred.utils.get_package_version", "completion": "    try:\n        version_string = importlib.import_module(name).__version__\n    except (ImportError, AttributeError):\n        return None\n    return parse_version(version_string)\n\n", "idx": 1441}
{"namespace": "sacred.experiment.Experiment.main", "completion": "        captured = self.command(function)\n        self.default_command = captured.__name__\n        return captured\n", "idx": 1442}
{"namespace": "sacred.experiment.Experiment.run", "completion": "        # TODO: Add a check to make sure that the command name is valid.\n        # TODO: Add a check to make sure that the config_updates is a dictionary.\n        # TODO: Add a check to make sure that the named_configs is a list.\n        # TODO: Add a check to make sure that the info is a dictionary.\n        # TODO: Add a check to make sure that the meta_info is a dictionary.\n        # TODO: Add a check to make sure that the options is a dictionary.\n        # TODO: Add a check to make sure that the config_updates is a dictionary.\n        # TODO: Add a check to make sure that the named_configs is a list.\n        # TODO: Add a check to make sure that the info is a dictionary.\n        # TODO: Add a check to make sure that the meta_info is a dictionary.\n        # TODO: Add a check to make sure that the options is a dictionary.\n        # TODO: Add a check to make sure that the config_updates is a dictionary.\n        # TODO: Add a check to make sure that the named_configs is a list.\n        # TODO: Add a check to make sure that the info is a dictionary.\n        # TODO: Add a check to make sure that the meta_info is a dictionary.\n        # TODO: Add a check to make sure that the options is a dictionary.\n        # TODO: Add a check to make sure that the config_updates is a dictionary.\n        # TODO: Add a check to make sure that the named_configs is a list.\n        # TODO: Add a check to make sure that the info is a dictionary.\n        # TODO: Add a check to make sure that the meta_info is a dictionary.\n        # TODO: Add a check to make sure that the options is a dictionary.\n        # TODO: Add a check to make sure that the config_updates is a dictionary.\n        # TODO: Add a check to make sure that the named_configs is a list.\n        # TODO: Add a check to make sure that the info is a dictionary.\n        # TODO: Add a check to make sure that the meta_info is a dictionary.\n        # TODO: Add a check to make", "idx": 1443}
{"namespace": "sacred.host_info.host_info_getter", "completion": "    if name is None:\n        name = func.__name__\n    host_info_gatherers[name] = func\n    return func\n\n", "idx": 1444}
{"namespace": "sacred.ingredient.Ingredient.command", "completion": "        if function in self.commands.values():\n            return function\n\n        captured_function = self.capture(function, prefix=prefix)\n        self.commands[function.__name__] = captured_function\n        captured_function.unobserved = unobserved\n        return captured_function\n", "idx": 1445}
{"namespace": "sacred.ingredient.Ingredient.config", "completion": "        if not callable(function):\n            raise ValueError(\"Only callable objects can be added as configs\")\n        config_scope = ConfigScope(function, self)\n        self.configurations.append(config_scope)\n        return config_scope\n", "idx": 1446}
{"namespace": "sacred.ingredient.Ingredient.named_config", "completion": "        config_scope = ConfigScope(func)\n        self.named_configs[func.__name__] = config_scope\n        return config_scope\n", "idx": 1447}
{"namespace": "sacred.ingredient.Ingredient.gather_commands", "completion": "        for cmd_name, cmd in self.commands.items():\n            yield self.post_process_name(cmd_name, self), cmd\n        for ingred in self.ingredients:\n            for cmd_name, cmd in ingred.gather_commands():\n                yield self.post_process_name(cmd_name, ingred), cmd\n", "idx": 1448}
{"namespace": "sacred.ingredient.Ingredient.gather_named_configs", "completion": "        for ingredient, _ in self.traverse_ingredients():\n            for name, conf in ingredient.named_configs.items():\n                yield join_paths(ingredient.path, name), conf\n", "idx": 1449}
{"namespace": "sacred.dependencies.Source.create", "completion": "        if not os.path.exists(filename):\n            raise ValueError(\"invalid filename or file not found {}\".format(filename))\n        main_file = get_py_file_if_possible(filename)\n        digest = get_digest(main_file)\n        repo, commit, is_dirty = get_commit_if_possible(main_file, save_git_info)\n        return Source(main_file, digest, repo, commit, is_dirty)\n", "idx": 1450}
{"namespace": "sacred.dependencies.Source.to_json", "completion": "        if base_dir is not None:\n            return (\n                os.path.relpath(self.filename, base_dir),\n                self.digest,\n                self.repo,\n                self.commit,\n                self.is_dirty,\n            )\n        return (self.filename, self.digest, self.repo, self.commit, self.is_dirty)\n", "idx": 1451}
{"namespace": "sacred.dependencies.PackageDependency.create", "completion": "        if not cls.modname_to_dist:\n            cls.modname_to_dist = {}\n            for dist in pkg_resources.working_set:\n                for modname in dist._get_metadata(\"top_level.txt\"):\n                    cls.modname_to_dist[modname] = dist\n\n        dist = cls.modname_to_dist.get(mod.__name__)\n        if dist is None:\n            return PackageDependency(mod.__name__, None)\n        else:\n            return PackageDependency(dist.project_name, dist.version)\n\n", "idx": 1452}
{"namespace": "sacred.dependencies.is_local_source", "completion": "    if not experiment_path:\n        return False\n\n    # if the module is part of the experiment, the base directory of the module should be a subdirectory of the experiment_path\n    filename = os.path.abspath(filename)\n    experiment_path = os.path.abspath(experiment_path)\n    return os.path.commonpath([filename, experiment_path]) == experiment_path\n\n", "idx": 1453}
{"namespace": "sacred.dependencies.gather_sources_and_dependencies", "completion": "    if base_dir is None:\n        base_dir = os.path.abspath(os.path.curdir)\n\n    main_file, sources = get_main_file(globs, save_git_info)\n    sources = set(sources)\n    dependencies = set()\n\n    if \"numpy\" in sys.modules:\n        dependencies.add(PackageDependency.create(sys.modules[\"numpy\"]))\n\n    for strategy in SETTINGS.SOURCE_DISCOVERY_STRATEGY:\n        sources.update(source_discovery_strategies[strategy](globs, base_dir, save_git_info))\n\n    for strategy in SETTINGS.DEPENDENCY_DISCOVERY_STRATEGY:\n        dependencies.update(dependency_discovery_strategies[strategy](globs, base_dir))\n\n    return main_file, sources, dependencies\n\n", "idx": 1454}
{"namespace": "sacred.observers.file_storage.FileStorageObserver.resource_event", "completion": "        self.run_entry[\"resources\"].append(\n            {\n                \"filename\": os.path.relpath(filename, self.basedir),\n                \"digest\": get_digest(filename),\n            }\n        )\n        self.save_json(self.run_entry, \"run.json\")\n", "idx": 1455}
{"namespace": "sacred.config.signature.Signature.get_free_parameters", "completion": "        # get the number of positional arguments\n        num_pos = len(args)\n\n        # get the number of keyword arguments\n        num_kw = len(kwargs)\n\n        # get the number of positional arguments that the function accepts\n        num_pos_accepted = len(self.positional_args)\n\n        # get the number of keyword arguments that the function accepts\n        num_kw_accepted = len(self.kwargs)\n\n        # get the number of positional arguments that the function ignores\n        num_pos_ignored = num_pos - num_pos_accepted\n\n        # get the number of keyword arguments that the function ignores\n        num_kw_ignored = num_kw - num_kw_accepted\n\n        # get the names of the positional arguments that the function ignores\n        pos_ignored = self.positional_args[num_pos_accepted:]\n\n        # get the names of the keyword arguments that the function ignores\n        kw_ignored = list(self.kwargs.keys())[num_kw_accepted:]\n\n        # get the names of the positional arguments that the function accepts\n        pos_accepted = self.positional_args[:num_pos_accepted]\n\n        # get the names of the keyword arguments that the function accepts\n        kw_accepted = list(self.kwargs.keys())[:num_kw_accepted]\n\n        # get the names of the positional arguments that the function requires\n        pos_required = self.positional_args[:max(num_pos_accepted - num_pos, 0)]\n\n        # get the names of the keyword arguments that the function requires\n        kw_required = list(self.kwargs.keys())[:max(num_kw_accepted - num_kw, 0)]\n\n        # get the names of the positional arguments that the function takes\n        pos_taken = self.positional_args[:num_pos]\n\n        # get the names of the keyword arguments that the function takes\n        kw_taken = list(kwargs.keys())[:num_kw]\n\n        # get the names of the arguments that the function takes\n        taken =", "idx": 1456}
{"namespace": "sacred.config.signature.Signature.construct_arguments", "completion": "        expected_args = self._get_expected_args(bound)\n        free_parameters = self.get_free_parameters(args, kwargs, bound)\n\n        # check for unexpected arguments\n        for arg in kwargs:\n            if arg not in expected_args:\n                raise SignatureError(\n                    \"Unexpected keyword argument '{}' for '{}'\".format(arg, self.name)\n                )\n\n        # check for unexpected positional arguments\n        for arg in args:\n            if arg not in expected_args[: len(args)]:\n                raise SignatureError(\n                    \"Unexpected positional argument '{}' for '{}'\".format(arg, self.name)\n                )\n\n        # check for conflicts\n        for arg in free_parameters:\n            if arg in kwargs:\n                raise SignatureError(\n                    \"Argument '{}' given both as positional and keyword argument for '{}'\".format(\n                        arg, self.name\n                    )\n                )\n\n        # check for missing arguments\n        missing_arguments = [arg for arg in free_parameters if arg not in options]\n        if missing_arguments:\n            raise MissingConfigError(\n                \"Missing values for required configuration parameters: {}\".format(\n                    \", \".join(missing_arguments)\n                ),\n                missing_arguments,\n            )\n\n        # construct args list and kwargs dictionary\n        args = list(args) + [options[arg] for arg in free_parameters]\n        kwargs.update({k: options[k] for k in options if k not in args})\n\n        return args, kwargs\n", "idx": 1457}
{"namespace": "sacred.config.config_files.load_config_file", "completion": "    handler = get_handler(filename)\n    with open(filename, \"r\" + handler.mode) as f:\n        return handler.load(f)\n\n", "idx": 1458}
{"namespace": "sacred.config.custom_containers.DogmaticDict.get", "completion": "        if dict.__contains__(self, k):\n            return dict.__getitem__(self, k)\n        elif k in self.fallback:\n            return self.fallback[k]\n        else:\n            return d\n", "idx": 1459}
{"namespace": "sacred.config.custom_containers.DogmaticDict.revelation", "completion": "        missing_keys = set()\n        for key in self.fixed:\n            if key not in self:\n                missing_keys.add(key)\n                self[key] = self.fixed[key]\n                if isinstance(self[key], DogmaticDict):\n                    missing_keys |= {join_paths(key, m) for m in self[key].revelation()}\n        return missing_keys\n", "idx": 1460}
{"namespace": "sacred.config.custom_containers.make_read_only", "completion": "    if isinstance(o, dict):\n        return ReadOnlyDict({k: make_read_only(v) for k, v in o.items()})\n    elif isinstance(o, list):\n        return ReadOnlyList([make_read_only(v) for v in o])\n    elif isinstance(o, tuple):\n        return tuple(make_read_only(v) for v in o)\n    else:\n        return o\n\n", "idx": 1461}
{"namespace": "sacred.config.config_scope.dedent_function_body", "completion": "    lines = body.splitlines(True)\n    # Find the first line that is not empty or a comment\n    for line in lines:\n        if not is_empty_or_comment(line):\n            break\n    else:\n        raise ValueError(\"No non-empty or non-comment line found in function body\")\n    indent = line[: re.search(r\"\\S\", line).start()]\n    dedented_lines = [dedent_line(line, indent) for line in lines]\n    dedented_body = \"\".join(dedented_lines)\n    return dedented_body\n\n", "idx": 1462}
{"namespace": "boltons.funcutils.FunctionBuilder.get_sig_str", "completion": "            if with_annotations:\n                return formatargspec(self.args,\n                                     self.varargs,\n                                     self.varkw,\n                                     [],\n                                     self.kwonlyargs,\n                                     {},\n                                     self.annotations)\n            else:\n                return formatargspec(self.args,\n                                     self.varargs,\n                                     self.varkw,\n                                     [],\n                                     self.kwonlyargs,\n                                     {},\n                                     {})\n", "idx": 1463}
{"namespace": "boltons.funcutils.FunctionBuilder.get_invocation_str", "completion": "            kwonly_pairs = None\n            formatters = {}\n            if self.kwonlyargs:\n                kwonly_pairs = dict((k, k) for k in self.kwonlyargs)\n                formatters['formatvalue'] = lambda value: '=' + value\n\n            sig = inspect_formatargspec(self.args,\n                                        self.varargs,\n                                        self.varkw,\n                                        [],\n                                        kwonly_pairs,\n                                        kwonly_pairs,\n                                        {},\n                                        **formatters)\n            sig = self._KWONLY_MARKER.sub('', sig)\n            return sig[1:-1]\n", "idx": 1464}
{"namespace": "boltons.funcutils.FunctionBuilder.from_func", "completion": "        if isinstance(func, functools.partial):\n            cur_func = func.func\n            kwargs = func.keywords.copy()\n            for arg in reversed(func.args):\n                kwargs[get_lambda_args(cur_func)[0]] = arg\n                cur_func = cur_func.__closure__[0].cell_contents\n\n            for attr in ['func', 'args', 'keywords', '__name__',\n                         '__qualname__']:\n                try:\n                    delattr(func, attr)\n                except AttributeError:\n                    pass\n\n            fb = cls.from_func(cur_func)\n            fb.name = func.__name__\n            if hasattr(func, '__qualname__'):\n                fb.qualname = func.__qualname__\n            fb.body = 'return lambda %s: _call(%s)' % (\n                fb.get_invocation_str(),\n                fb.get_invocation_str())\n            fb.args = list(kwargs.keys())\n            fb.defaults = tuple(kwargs[arg] for arg in fb.args)\n        else:\n            attrs = {'name': func.__name__,\n                     'doc': func.__doc__,\n                     'module': func.__module__,\n                     'dict': getattr(func, '__dict__', {})}\n\n            if not _IS_PY2:\n                attrs['annotations'] = getattr(func, '__annotations__', {})\n\n            fb = cls(**attrs)\n            fb.argspec = cls._argspec_to_dict(func)\n\n        return fb\n", "idx": 1465}
{"namespace": "boltons.funcutils.FunctionBuilder.get_defaults_dict", "completion": "        if self.defaults:\n            ret = dict(reversed(list(zip(reversed(self.args), reversed(self.defaults)))))\n        else:\n            ret = {}\n        if self.kwonlydefaults:\n            ret.update(self.kwonlydefaults)\n        return ret\n", "idx": 1466}
{"namespace": "boltons.funcutils.FunctionBuilder.get_arg_names", "completion": "        if only_required:\n            defaults_dict = self.get_defaults_dict()\n            args = tuple(arg for arg in self.args if arg not in defaults_dict)\n        else:\n            args = tuple(self.args)\n\n        return args\n", "idx": 1467}
{"namespace": "boltons.ioutils.SpooledIOBase.writelines", "completion": "        self._checkClosed()\n        for line in lines:\n            self.write(line)\n", "idx": 1468}
{"namespace": "boltons.ioutils.SpooledBytesIO.write", "completion": "        self._checkClosed()\n        if not isinstance(s, binary_type):\n            raise TypeError('{0} expected, got {1}'.format(\n                binary_type.__name__, type(s).__name__))\n\n        if self.tell() + len(s) >= self._max_size:\n            self.rollover()\n        self.buffer.write(s)\n", "idx": 1469}
{"namespace": "boltons.ioutils.SpooledBytesIO.seek", "completion": "        self._checkClosed()\n        return self.buffer.seek(pos, mode)\n", "idx": 1470}
{"namespace": "boltons.ioutils.SpooledBytesIO.len", "completion": "        # The current position of the file.\n        pos = self.tell()\n        # If the file has not been rolled.\n        if not self._rolled:\n            # The length of the file.\n            self._len = len(self.getvalue())\n        # If the file has been rolled.\n        else:\n            # The length of the file.\n            self._len = os.fstat(self.fileno()).st_size\n        # Move the position of the file back to where it was initially.\n        self.seek(pos)\n        # Return the length of the file.\n        return self._len\n", "idx": 1471}
{"namespace": "boltons.ioutils.SpooledStringIO.read", "completion": "        self._checkClosed()\n        ret = self.buffer.read(n)\n        self._tell = self.buffer.tell()\n        return ret\n", "idx": 1472}
{"namespace": "boltons.ioutils.SpooledStringIO.write", "completion": "        self._checkClosed()\n        if not isinstance(s, text_type):\n            raise TypeError(\"{0} expected, got {1}\".format(\n                text_type.__name__,\n                type(s).__name__\n            ))\n\n        if self.tell() + len(s.encode('utf-8')) >= self._max_size:\n            self.rollover()\n        self.buffer.write(s.encode('utf-8'))\n        self._tell = self.tell() + len(s)\n", "idx": 1473}
{"namespace": "boltons.ioutils.SpooledStringIO.seek", "completion": "        self._checkClosed()\n        if mode == os.SEEK_SET:\n            self._tell = self._traverse_codepoints(0, pos)\n        elif mode == os.SEEK_CUR:\n            self._tell = self._traverse_codepoints(self._tell, pos)\n        elif mode == os.SEEK_END:\n            self._tell = self._traverse_codepoints(self.len, pos)\n        else:\n            raise ValueError(\"Invalid whence ({}, should be 0, 1, or 2)\".format(mode))\n        return self._tell\n", "idx": 1474}
{"namespace": "boltons.ioutils.SpooledStringIO.tell", "completion": "        return self._tell\n", "idx": 1475}
{"namespace": "boltons.ioutils.SpooledStringIO.len", "completion": "        pos = self.buffer.tell()\n        self.buffer.seek(0)\n        total_codepoints = 0\n        while True:\n            ret = self.read(READ_CHUNK_SIZE)\n            total_codepoints += len(ret)\n            if not ret:\n                break\n        self.seek(pos)\n        return total_codepoints", "idx": 1476}
{"namespace": "boltons.ioutils.MultiFileReader.read", "completion": "        if amt is None:\n            return self._joiner.join(f.read() for f in self._fileobjs)\n        parts = []\n        while amt > 0 and self._index < len(self._fileobjs):\n            parts.append(self._fileobjs[self._index].read(amt))\n            got = len(parts[-1])\n            if got < amt:\n                self._index += 1\n            amt -= got\n        return self._joiner.join(parts)\n", "idx": 1477}
{"namespace": "boltons.ioutils.MultiFileReader.seek", "completion": "        if whence != os.SEEK_SET:\n            raise NotImplementedError('MultiFileReader.seek() only supports os.SEEK_SET')\n        if offset != 0:\n            raise NotImplementedError('MultiFileReader only supports seeking to start at this time')\n        for f in self._fileobjs:\n            f.seek(0)\n        self._index = 0\n\n", "idx": 1478}
{"namespace": "boltons.listutils.BarrelList.insert", "completion": "        if index < 0:\n            index += len(self)\n        if index < len(self):\n            list_idx, item_idx = self._translate_index(index)\n            self.lists[list_idx].insert(item_idx, item)\n            self._balance_list(list_idx)\n        else:\n            self.append(item)\n", "idx": 1479}
{"namespace": "boltons.listutils.BarrelList.pop", "completion": "        if len(self.lists) == 1:\n            return self.lists[0].pop(*a)\n        list_idx, rel_idx = self._translate_index(a[0])\n        if list_idx is None:\n            raise IndexError()\n        ret = self.lists[list_idx].pop(rel_idx)\n        if not self.lists[list_idx]:\n            del self.lists[list_idx]\n        return ret\n", "idx": 1480}
{"namespace": "boltons.listutils.BarrelList.sort", "completion": "        # Sort the lists\n        for l in self.lists:\n            l.sort()\n\n        # Merge the lists\n        self.lists = [self._merge(self.lists)]\n\n        # Balance the list\n        self._balance_list(0)\n", "idx": 1481}
{"namespace": "boltons.urlutils.URL.path", "completion": "        self.path_parts = tuple([unquote(p) if '%' in p else p for p in path_text.split(u'/')])\n", "idx": 1482}
{"namespace": "boltons.urlutils.URL.navigate", "completion": "        if isinstance(dest, URL):\n            dest = dest.to_text()\n\n        if dest.startswith('http'):\n            return URL(dest)\n\n        if dest.startswith('/'):\n            return URL(self.scheme + '://' + self.host + dest)\n\n        if dest.startswith('./'):\n            dest = dest[2:]\n\n        if dest.startswith('../'):\n            dest = dest[3:]\n            self.path_parts = self.path_parts[:-1]\n\n        if dest.startswith('..'):\n            dest = dest[2:]\n            self.path_parts = self.path_parts[:-1]\n\n        if dest.startswith('./'):\n            dest = dest[2:]\n\n        if dest.startswith('.'):\n            dest = dest[1:]\n\n        if dest.startswith('/'):\n            dest = dest[1:]\n\n        if dest.startswith('//'):\n            dest = dest[2:]\n\n        if dest.startswith(':'):\n            dest = dest[1:]\n\n        if dest.startswith('?'):\n            dest = dest[1:]\n\n        if dest.startswith('#'):\n            dest = dest[1:]\n\n        if dest.startswith('&'):\n            dest = dest[1:]\n\n        if dest.startswith('='):\n            dest = dest[1:]\n\n        if dest.startswith(';'):\n            dest = dest[1:]\n\n        if dest.startswith('@'):\n            dest = dest[1:]\n\n        if dest.startswith('$'):\n            dest = dest[1:]\n\n        if dest.startswith('%'):\n            dest = dest[1:]\n\n        if dest.startswith('^'):\n            dest = dest[1:]\n\n        if dest.startswith('*'):\n            dest = dest[1:]\n\n        if dest.startswith('('):\n            dest = dest[1:]\n\n        if dest.startswith(')'):\n            dest = dest[1:]\n\n        if dest.startswith('-'):\n            dest = dest[1:]", "idx": 1483}
{"namespace": "boltons.urlutils.URL.to_text", "completion": "        parts = []\n        _add = parts.append\n        if self.scheme:\n            _add(self.scheme)\n            _add(':')\n        if self.uses_netloc:\n            _add('//')\n            _add(self.get_authority(full_quote=full_quote))\n        _add(self.path)\n        if self.query_params:\n            _add('?')\n            _add(self.query_params.to_text(full_quote=full_quote))\n        if self.fragment:\n            _add('#')\n            _add(quote_fragment_part(self.fragment))\n        return u''.join(parts)\n", "idx": 1484}
{"namespace": "boltons.urlutils.QueryParamDict.to_text", "completion": "        pairs = []\n        for key, val in self.iteritems(multi=True):\n            if full_quote:\n                key = quote_plus(key)\n                val = quote_plus(val)\n            else:\n                key = quote(key)\n                val = quote(val)\n            pairs.append((key, val))\n        return urlencode(pairs)", "idx": 1485}
{"namespace": "boltons.tbutils.TracebackInfo.from_traceback", "completion": "        ret = []\n        if tb is None:\n            tb = sys.exc_info()[2]\n            if tb is None:\n                raise ValueError('no tb set and no exception being handled')\n        if limit is None:\n            limit = getattr(sys, 'tracebacklimit', 1000)\n        n = 0\n        while tb is not None and n < limit:\n            item = cls.callpoint_type.from_tb(tb)\n            ret.append(item)\n            tb = tb.tb_next\n            n += 1\n        return cls(ret)\n", "idx": 1486}
{"namespace": "boltons.tbutils.ExceptionInfo.get_formatted", "completion": "        ret = 'Traceback (most recent call last):\\n'\n        ret += self.tb_info.get_formatted()\n        ret += '%s: %s\\n' % (self.exc_type, self.exc_msg)\n        return ret\n\n", "idx": 1487}
{"namespace": "boltons.tbutils.print_exception", "completion": "    if file is None:\n        file = sys.stderr\n    if tb:\n        tbe = traceback.extract_tb(tb)\n        print('Traceback (most recent call last):', file=file)\n        for line in traceback.format_list(tbe):\n            print(line, file=file, end='')\n    lines = format_exception_only(etype, value)\n    for line in lines:\n        print(line, file=file, end='')\n\n", "idx": 1488}
{"namespace": "boltons.tbutils.ParsedException.to_string", "completion": "        ret = []\n        ret.append('Traceback (most recent call last):\\n')\n        for frame in self.frames:\n            ret.append('  File \"%s\", line %s, in %s\\n' % (frame['filepath'], frame['lineno'], frame['funcname']))\n            ret.append('    %s\\n' % frame['code'])\n        ret.append('%s: %s\\n' % (self.exc_type, self.exc_msg))\n        return ''.join(ret)\n", "idx": 1489}
{"namespace": "boltons.tbutils.ParsedException.from_string", "completion": "        # TODO: handle SyntaxError\n        # TODO: handle unicode\n        # TODO: handle unicode tracebacks\n        # TODO: handle unicode exception messages\n        # TODO: handle unicode filenames\n        # TODO: handle unicode function names\n        # TODO: handle unicode source lines\n        # TODO: handle unicode exception type\n        # TODO: handle unicode exception message\n        # TODO: handle unicode tracebacks with unicode characters\n        # TODO: handle unicode tracebacks with non-ascii characters\n        # TODO: handle unicode exception messages with unicode characters\n        # TODO: handle unicode exception messages with non-ascii characters\n        # TODO: handle unicode filenames with unicode characters\n        # TODO: handle unicode filenames with non-ascii characters\n        # TODO: handle unicode function names with unicode characters\n        # TODO: handle unicode function names with non-ascii characters\n        # TODO: handle unicode source lines with unicode characters\n        # TODO: handle unicode source lines with non-ascii characters\n        # TODO: handle unicode exception type with unicode characters\n        # TODO: handle unicode exception type with non-ascii characters\n        # TODO: handle unicode exception message with unicode characters\n        # TODO: handle unicode exception message with non-ascii characters\n        # TODO: handle unicode tracebacks with unicode characters\n        # TODO: handle unicode tracebacks with non-ascii characters\n        # TODO: handle unicode exception type with unicode characters\n        # TODO: handle unicode exception type with non-ascii characters\n        # TODO: handle unicode exception message with unicode characters\n        # TODO: handle unicode exception message with non-ascii characters\n        # TODO: handle unicode filenames with unicode characters\n        # TODO: handle unicode filenames with non-ascii characters\n        # TODO: handle unicode function names with unicode characters\n        # TODO: handle unicode function names with non-ascii characters\n        # TODO: handle unicode source lines with unicode characters\n        # TODO: handle unicode source lines with non-ascii characters\n        # TODO: handle unicode exception type with unicode characters\n        # TODO: handle unicode exception type with non-ascii characters\n        # TODO: handle unicode exception message with unicode characters\n        # TODO: handle unicode exception message with non-ascii characters\n        # TODO: handle unicode", "idx": 1490}
{"namespace": "boltons.tableutils.Table.extend", "completion": "        if not data:\n            return\n\n        # TODO: this is a bit of a hack to make sure we don't\n        # have to special case the first row\n        if not self._data:\n            self.append(data)\n            return\n\n        # TODO: this is a bit of a hack to make sure we don't\n        # have to special case the first row\n        if not self._data:\n            self.append(data)\n            return\n\n        # TODO: this is a bit of a hack to make sure we don't\n        # have to special case the first row\n        if not self._data:\n            self.append(data)\n            return\n\n        # TODO: this is a bit of a hack to make sure we don't\n        # have to special case the first row\n        if not self._data:\n            self.append(data)\n            return\n\n        # TODO: this is a bit of a hack to make sure we don't\n        # have to special case the first row\n        if not self._data:\n            self.append(data)\n            return\n\n        # TODO: this is a bit of a hack to make sure we don't\n        # have to special case the first row\n        if not self._data:\n            self.append(data)\n            return\n\n        # TODO: this is a bit of a hack to make sure we don't\n        # have to special case the first row\n        if not self._data:\n            self.append(data)\n            return\n\n        # TODO: this is a bit of a hack to make sure we don't\n        # have to special case the first row\n        if not self._data:\n            self.append(data)\n            return\n\n        # TODO: this is a bit of a hack to make sure we don't\n        # have to special case the first row\n        if not self._data:\n            self.append(data)\n            return\n\n        # TODO: this is a bit of a hack to make sure we don't\n        # have to special case the first row\n        if not self._data:\n            self.append(data)\n            return", "idx": 1491}
{"namespace": "boltons.tableutils.Table.from_object", "completion": "        return cls.from_data(data=data, headers=headers,\n                             max_depth=max_depth, _data_type=ObjectInputType(),\n                             metadata=metadata)\n", "idx": 1492}
{"namespace": "boltons.tableutils.Table.__repr__", "completion": "        if self.headers:\n            return '{type_name}(headers={headers!r}, data={data!r})'.format(\n                type_name=self.__class__.__name__,\n                headers=self.headers,\n                data=self._data\n            )\n        return '{type_name}({data!r})'.format(\n            type_name=self.__class__.__name__,\n            data=self._data\n        )\n", "idx": 1493}
{"namespace": "boltons.tableutils.Table.to_text", "completion": "        # Get the maximum length of each column\n        max_len = [0] * self._width\n        for row in self._data:\n            for i in range(self._width):\n                cell = row[i]\n                cell = to_text(cell, maxlen=maxlen)\n                max_len[i] = max(max_len[i], len(cell))\n\n        # Create the header row\n        if with_headers:\n            header_row = ' | '.join(self.headers)\n            header_row = '| ' + header_row + ' |'\n            header_row_dashes = ' | '.join(['-' * max_len[i] for i in range(self._width)])\n            header_row_dashes = '| ' + header_row_dashes + ' |'\n            header_row = ' ' + header_row + ' '\n            header_row_dashes = ' ' + header_row_dashes + ' '\n\n        # Create the data rows\n        data_rows = []\n        for row in self._data:\n            data_row = []\n            for i in range(self._width):\n                cell = row[i]\n                cell = to_text(cell, maxlen=maxlen)\n                cell = ' ' + cell + ' '\n                cell = cell.ljust(max_len[i] + 2)\n                data_row.append(cell)\n            data_row = ' | '.join(data_row)\n            data_row = '| ' + data_row + ' |'\n            data_rows.append(data_row)\n\n        # Create the table\n        table = []\n        if with_headers:\n            table.append(header_row)\n            table.append(header_row_dashes)\n        for row in data_rows:\n            table.append(row)\n        table = '\\n'.join(table)\n        return table\n", "idx": 1494}
{"namespace": "boltons.statsutils.Stats.get_histogram_counts", "completion": "        bin_digits = kw.get('bin_digits', 1)\n        if bins is None:\n            bins = self._get_bin_bounds(with_max=True)\n        elif isinstance(bins, int):\n            bins = self._get_bin_bounds(count=bins, with_max=True)\n        bins = [round(b, bin_digits) for b in bins]\n        counts = [0] * len(bins)\n        for v in self.data:\n            for i, b in enumerate(bins):\n                if v < b:\n                    counts[i] += 1\n                    break\n        return list(zip(bins, counts))\n", "idx": 1495}
{"namespace": "boltons.setutils.IndexedSet.add", "completion": "        if item not in self.item_index_map:\n            self.item_index_map[item] = len(self.item_list)\n            self.item_list.append(item)\n", "idx": 1496}
{"namespace": "boltons.setutils.IndexedSet.pop", "completion": "        if index is None:\n            index = -1\n        if index < 0:\n            index += len(self)\n        real_index = self._get_real_index(index)\n        item = self.item_list[real_index]\n        self.item_list[real_index] = _MISSING\n        self._add_dead(real_index)\n        self._cull()\n        return item\n", "idx": 1497}
{"namespace": "boltons.setutils.IndexedSet.index", "completion": "        try:\n            return self._get_apparent_index(self.item_index_map[val])\n        except KeyError:\n            raise ValueError(f'{val!r} is not in {type(self).__name__}')\n", "idx": 1498}
{"namespace": "boltons.setutils.complement", "completion": "    class _ComplementSet(object):\n\n        \"\"\"\n        This class is a private class which is used to create a complement set. It is not meant to be used outside of the complement function.\n        \"\"\"\n\n        def __init__(self, wrapped):\n            self.wrapped = wrapped\n\n        def __contains__(self, item):\n            return item not in self.wrapped\n\n        def __iter__(self):\n            return iter(self.wrapped)\n\n        def __len__(self):\n            return len(self.wrapped)\n\n        def __repr__(self):\n            return 'complement(%s)' % repr(self.wrapped)\n\n        def __str__(self):\n            return 'complement(%s)' % str(self.wrapped)\n\n    return _ComplementSet(wrapped)", "idx": 1499}
{"namespace": "boltons.strutils.strip_ansi", "completion": "    if isinstance(text, (str, bytes)):\n        return ANSI_SEQUENCES.sub('', text)\n    elif isinstance(text, bytearray):\n        return bytearray(ANSI_SEQUENCES.sub('', text.decode()).encode())\n    else:\n        raise TypeError('Argument must be a string, bytes, or bytearray')\n\n", "idx": 1500}
{"namespace": "boltons.strutils.asciify", "completion": "    # If the text is already an ASCII bytestring, we're done.\n    if isinstance(text, bytes):\n        return text\n\n    # If the text is a unicode string, we need to convert it to a bytestring.\n    if isinstance(text, unicode):\n        # If the text is all ASCII characters, we can return it as a bytestring.\n        try:\n            return text.encode('ascii')\n        # If not, we need to deaccent and retry.\n        except UnicodeEncodeError:\n            pass\n\n    # If we're here, the text must be a unicode string with non-ASCII characters.\n    # We need to deaccent before converting.\n    deaccented = unicodedata.normalize('NFD', text)\n    ascii_text = deaccented.encode('ascii', 'ignore' if ignore else 'replace')\n    return ascii_text\n\n", "idx": 1501}
{"namespace": "boltons.strutils.indent", "completion": "    return newline.join(\n        margin + line if key(line) else line\n        for line in iter_splitlines(text)\n    )\n\n", "idx": 1502}
{"namespace": "boltons.strutils.multi_replace", "completion": "    mr = MultiReplace(sub_map, **kwargs)\n    return mr.sub(text)\n\n", "idx": 1503}
{"namespace": "boltons.cacheutils.LRI._get_flattened_ll", "completion": "        flattened_ll = []\n        curr_node = self._anchor\n        while curr_node[PREV] != self._anchor:\n            flattened_ll.append((curr_node[KEY], curr_node[VALUE]))\n            curr_node = curr_node[PREV]\n        return flattened_ll\n", "idx": 1504}
{"namespace": "boltons.cacheutils.LRI.pop", "completion": "        with self._lock:\n            try:\n                ret = super(LRI, self).pop(key)\n            except KeyError:\n                if default is _MISSING:\n                    raise\n                ret = default\n            else:\n                self._remove_from_ll(key)\n            return ret\n", "idx": 1505}
{"namespace": "boltons.cacheutils.LRI.popitem", "completion": "        with self._lock:\n            try:\n                item = super(LRI, self).popitem()\n            except KeyError:\n                raise KeyError('LRI is empty')\n            else:\n                self._remove_from_ll(item[0])\n                return item\n", "idx": 1506}
{"namespace": "boltons.cacheutils.LRI.clear", "completion": "        with self._lock:\n            super(LRI, self).clear()\n            self._init_ll()\n", "idx": 1507}
{"namespace": "boltons.cacheutils.LRI.setdefault", "completion": "        with self._lock:\n            try:\n                return self[key]\n            except KeyError:\n                self.soft_miss_count += 1\n                self[key] = default\n                return default\n", "idx": 1508}
{"namespace": "boltons.cacheutils.LRI.update", "completion": "        with self._lock:\n            if hasattr(E, 'keys'):\n                for k in E.keys():\n                    self[k] = E[k]\n            else:\n                for k, v in E:\n                    self[k] = v\n            for k in F:\n                self[k] = F[k]\n", "idx": 1509}
{"namespace": "boltons.cacheutils.LRI.__repr__", "completion": "        return \"%s(max_size=%r, on_miss=%r, values=%r)\" % (\n            self.__class__.__name__, self.max_size, self.on_miss, self.values())\n\n", "idx": 1510}
{"namespace": "boltons.cacheutils.CachedFunction.__repr__", "completion": "        return '{name}(func={func!r}, scoped={scoped!r}, typed={typed!r})'.format(\n            name=self.__class__.__name__,\n            func=self.func,\n            scoped=self.scoped,\n            typed=self.typed\n        )\n\n", "idx": 1511}
{"namespace": "boltons.cacheutils.ThresholdCounter.elements", "completion": "        for k, v in self._count_map.items():\n            for i in range(v[0]):\n                yield k\n", "idx": 1512}
{"namespace": "boltons.cacheutils.ThresholdCounter.most_common", "completion": "        if n is None:\n            return sorted(self.iteritems(), key=lambda x: (-x[1], x[0]))\n        return heapq.nlargest(n, self.iteritems(), key=lambda x: (-x[1], x[0]))\n", "idx": 1513}
{"namespace": "boltons.cacheutils.ThresholdCounter.update", "completion": "        if iterable is not None:\n            if hasattr(iterable, 'iteritems'):\n                for key, count in iterable.iteritems():\n                    for i in range(count):\n                        self.add(key)\n            else:\n                for key in iterable:\n                    self.add(key)\n        if len(kwargs) > 0:\n            self.update(kwargs)\n", "idx": 1514}
{"namespace": "boltons.cacheutils.MinIDMap.get", "completion": "        if a in self.mapping:\n            return self.mapping[a]\n        if self.free:\n            i = self.free.pop()\n        else:\n            i = len(self.ref_map)\n        self.ref_map[i] = a\n        self.mapping[a] = i\n        return i\n", "idx": 1515}
{"namespace": "boltons.iterutils.chunked", "completion": "    if not is_iterable(src):\n        raise TypeError('expected an iterable')\n\n    if not isinstance(size, int):\n        raise TypeError('expected an integer')\n\n    if count is not None and not isinstance(count, int):\n        raise TypeError('expected an integer')\n\n    if 'fill' in kw:\n        fill = kw['fill']\n    else:\n        fill = None\n\n    if count is None:\n        return list(chunked_iter(src, size, fill))\n    else:\n        return list(itertools.islice(chunked_iter(src, size, fill), count))\n\n", "idx": 1516}
{"namespace": "boltons.iterutils.chunk_ranges", "completion": "    # Check input arguments\n    if not isinstance(input_size, int) or input_size < 1:\n        raise ValueError('Input size must be a positive integer.')\n    if not isinstance(chunk_size, int) or chunk_size < 1:\n        raise ValueError('Chunk size must be a positive integer.')\n    if not isinstance(input_offset, int) or input_offset < 0:\n        raise ValueError('Input offset must be a non-negative integer.')\n    if not isinstance(overlap_size, int) or overlap_size < 0:\n        raise ValueError('Overlap size must be a non-negative integer.')\n    if overlap_size >= chunk_size:\n        raise ValueError('Overlap size must be smaller than chunk size.')\n\n    # Generate chunk ranges\n    if input_size <= chunk_size:\n        yield input_offset, input_offset + input_size\n    else:\n        if align:\n            chunk_start = input_offset + (chunk_size - overlap_size)\n        else:\n            chunk_start = input_offset\n        while True:\n            chunk_end = min(chunk_start + chunk_size, input_offset + input_size)\n            yield chunk_start, chunk_end\n            if chunk_end == input_offset + input_size:\n                break\n            chunk_start += (chunk_size - overlap_size)\n\n", "idx": 1517}
{"namespace": "boltons.iterutils.remap", "completion": "    reraise_visit = kwargs.pop('reraise_visit', True)\n    if kwargs:\n        raise TypeError('unexpected keyword arguments: %r' % kwargs.keys())\n\n    def visit_wrapper(path, key, value):\n        try:\n            return visit(path, key, value)\n        except Exception:\n            if reraise_visit:\n                raise\n            else:\n                return True\n\n    def enter_wrapper(path, key, value):\n        try:\n            return enter(path, key, value)\n        except Exception:\n            if reraise_visit:\n                raise\n            else:\n                return False\n\n    def exit_wrapper(path, key, old_parent, new_parent, new_items):\n        try:\n            return exit(path, key, old_parent, new_parent, new_items)\n        except Exception:\n            if reraise_visit:\n                raise\n            else:\n                return new_parent\n\n    return _remap(root, visit_wrapper, enter_wrapper, exit_wrapper)\n\n", "idx": 1518}
{"namespace": "boltons.iterutils.get_path", "completion": "    try:\n        for seg in path:\n            root = root[seg]\n        return root\n    except (KeyError, IndexError, TypeError) as e:\n        if default is _UNSET:\n            raise PathAccessError(e, seg, path)\n        return default\n\n", "idx": 1519}
{"namespace": "boltons.iterutils.research", "completion": "    # TODO: add a `max_depth` argument\n\n    # TODO: add a `max_results` argument\n\n    # TODO: add a `path_type` argument\n\n    # TODO: add a `path_filter` argument\n\n    # TODO: add a `path_formatter` argument\n\n    # TODO: add a `value_filter` argument\n\n    # TODO: add a `value_formatter` argument\n\n    # TODO: add a `error_formatter` argument\n\n    # TODO: add a `error_handler` argument\n\n    # TODO: add a `error_filter` argument\n\n    # TODO: add a `error_handler` argument\n\n    # TODO: add a `error_filter` argument\n\n    # TODO: add a `error_handler` argument\n\n    # TODO: add a `error_filter` argument\n\n    # TODO: add a `error_handler` argument\n\n    # TODO: add a `error_filter` argument\n\n    # TODO: add a `error_handler` argument\n\n    # TODO: add a `error_filter` argument\n\n    # TODO: add a `error_handler` argument\n\n    # TODO: add a `error_filter` argument\n\n    # TODO: add a `error_handler` argument\n\n    # TODO: add a `error_filter` argument\n\n    # TODO: add a `error_handler` argument\n\n    # TODO: add a `error_filter` argument\n\n    # TODO: add a `error_handler` argument\n\n    # TODO: add a `error_filter` argument\n\n    # TODO: add a `error_handler` argument\n\n    # TODO: add a `error_filter` argument\n\n    # TODO: add a `error_handler` argument\n\n    # TODO: add a `error_filter` argument\n\n    # TODO: add a `error_handler` argument\n\n    # TODO: add a `error_filter` argument\n\n    # TODO: add a `error_handler` argument\n\n    # TODO: add a `error_filter` argument\n\n    # TODO: add a `error_handler` argument\n\n    # TODO: add", "idx": 1520}
{"namespace": "boltons.socketutils.BufferedSocket.getrecvbuffer", "completion": "        return self.rbuf\n", "idx": 1521}
{"namespace": "boltons.socketutils.BufferedSocket.getsendbuffer", "completion": "        with self._send_lock:\n            return self.sbuf[:]\n", "idx": 1522}
{"namespace": "boltons.socketutils.BufferedSocket.recv", "completion": "        if flags != 0:\n            raise ValueError('non-zero flags not supported: {!r}'.format(flags))\n\n        if timeout is _UNSET:\n            timeout = self.timeout\n\n        if size <= 0:\n            return b''\n\n        with self._recv_lock:\n            if len(self.rbuf) >= size:\n                data = self.rbuf[:size]\n                self.rbuf = self.rbuf[size:]\n                return data\n            elif self.rbuf:\n                data = self.rbuf\n                self.rbuf = b''\n                return data\n\n        if timeout is None:\n            data = self.sock.recv(size)\n        else:\n            data = self._recv_timed(size, timeout)\n\n        if data:\n            with self._recv_lock:\n                self.rbuf += data\n\n        if len(self.rbuf) > self.maxsize:\n            raise MessageTooLong(\n                'received too much data, buffer full')\n\n        with self._recv_lock:\n            if len(self.rbuf) >= size:\n                data = self.rbuf[:size]\n                self.rbuf = self.rbuf[size:]\n                return data\n            elif self.rbuf:\n                data = self.rbuf\n                self.rbuf = b''\n                return data\n\n        return b''\n", "idx": 1523}
{"namespace": "boltons.socketutils.BufferedSocket.recv_close", "completion": "        if timeout is _UNSET:\n            timeout = self.timeout\n        if maxsize is _UNSET:\n            maxsize = self.maxsize\n        with self._recv_lock:\n            self.sock.settimeout(timeout)\n            try:\n                data = self.sock.recv(self._recvsize)\n            except socket.timeout:\n                raise Timeout(timeout)\n            except socket.error:\n                raise ConnectionClosed()\n            if len(data) > maxsize:\n                raise MessageTooLong(maxsize)\n            while True:\n                try:\n                    data += self.sock.recv(self._recvsize)\n                except socket.timeout:\n                    break\n                except socket.error:\n                    break\n                if len(data) > maxsize:\n                    raise MessageTooLong(maxsize)\n        return data\n", "idx": 1524}
{"namespace": "boltons.socketutils.BufferedSocket.flush", "completion": "        with self._send_lock:\n            self.send(b'')\n", "idx": 1525}
{"namespace": "boltons.socketutils.BufferedSocket.buffer", "completion": "        with self._send_lock:\n            self.sbuf.append(data)\n", "idx": 1526}
{"namespace": "boltons.socketutils.BufferedSocket.close", "completion": "        with self._send_lock:\n            with self._recv_lock:\n                self.sock.close()\n                self.rbuf = b''\n                self.sbuf = []\n", "idx": 1527}
{"namespace": "boltons.socketutils.NetstringSocket.setmaxsize", "completion": "        self.maxsize = maxsize\n        self._msgsize_maxsize = len(str(maxsize)) + 1\n", "idx": 1528}
{"namespace": "boltons.socketutils.NetstringSocket.write_ns", "completion": "        if len(payload) > self.maxsize:\n            raise NetstringMessageTooLong(len(payload), self.maxsize)\n        self.bsock.send(str(len(payload)).encode('ascii') + b':' + payload + b',')\n\n", "idx": 1529}
{"namespace": "boltons.fileutils.FilePerms.__repr__", "completion": "        return 'FilePerms(user=%r, group=%r, other=%r)' % (self.user, self.group, self.other)\n", "idx": 1530}
{"namespace": "boltons.mathutils.Bits.as_hex", "completion": "        return '{0:#0{1}x}'.format(self.val, (self.len + 3) // 4)\n", "idx": 1531}
{"namespace": "boltons.mathutils.Bits.from_hex", "completion": "        if type(hex) is bytes:\n            hex = hex.decode('ascii')\n        if not hex.startswith('0x'):\n            hex = '0x' + hex\n        return cls(hex)\n", "idx": 1532}
{"namespace": "boltons.formatutils.split_format_str", "completion": "    # TODO: this is a bit of a hack\n    if fstr.startswith('{'):\n        return [('', fstr)]\n\n    # TODO: this is a bit of a hack\n    if fstr.endswith('}'):\n        return [(fstr, '')]\n\n    def _iter_fields():\n        last = 0\n        for m in _pos_farg_re.finditer(fstr):\n            start, end = m.start(), m.end()\n            if start != end:\n                yield fstr[last:start]\n                yield fstr[start:end]\n                last = end\n        yield fstr[last:]\n    return list(zip(_iter_fields(), _iter_fields()))[1::2]\n\n", "idx": 1533}
{"namespace": "boltons.formatutils.infer_positional_format_args", "completion": "    ret = []\n    for lit, fname in split_format_str(fstr):\n        if fname is None:\n            ret.append(lit)\n            continue\n        ret.append(lit)\n        ret.append('{%d%s}' % (len(ret) - 1, fname[1:]))\n    return ''.join(ret)\n\n", "idx": 1534}
{"namespace": "boltons.formatutils.tokenize_format_str", "completion": "    # TODO: memoize\n    if resolve_pos:\n        fstr = infer_positional_format_args(fstr)\n    formatter = Formatter()\n    ret = []\n    for lit, fname, fspec, conv in formatter.parse(fstr):\n        if lit:\n            ret.append(lit)\n        if fname is None:\n            continue\n        ret.append(BaseFormatField(fname, fspec, conv))\n    return ret\n\n", "idx": 1535}
{"namespace": "boltons.dictutils.OneToOne.clear", "completion": "        dict.clear(self)\n        dict.clear(self.inv)\n", "idx": 1536}
{"namespace": "boltons.dictutils.OneToOne.pop", "completion": "        if default is _MISSING:\n            val = dict.__getitem__(self, key)\n            dict.__delitem__(self.inv, val)\n        else:\n            val = dict.pop(self, key, default)\n        dict.__delitem__(self, key)\n        return val\n", "idx": 1537}
{"namespace": "boltons.dictutils.OneToOne.popitem", "completion": "        try:\n            key = next(iter(self))\n        except StopIteration:\n            raise KeyError()\n        val = self[key]\n        del self[key]\n        return key, val\n", "idx": 1538}
{"namespace": "boltons.dictutils.ManyToMany.update", "completion": "        if type(iterable) is type(self):\n            for key, vals in iterable.data.items():\n                self.data.setdefault(key, set()).update(vals)\n            for val, keys in iterable.inv.data.items():\n                self.inv.data.setdefault(val, set()).update(keys)\n        else:\n            try:\n                for key, val in iterable.items():\n                    self.data.setdefault(key, set()).update(val)\n                    for val_ in val:\n                        self.inv.data.setdefault(val_, set()).add(key)\n            except AttributeError:\n                for key, val in iterable or ():\n                    self.data.setdefault(key, set()).add(val)\n                    self.inv.data.setdefault(val, set()).add(key)\n", "idx": 1539}
{"namespace": "boltons.dictutils.ManyToMany.add", "completion": "        if key not in self.data:\n            self.data[key] = set()\n        self.data[key].add(val)\n        if val not in self.inv.data:\n            self.inv.data[val] = set()\n        self.inv.data[val].add(key)\n", "idx": 1540}
{"namespace": "boltons.dictutils.ManyToMany.remove", "completion": "        self.data[key].remove(val)\n        if not self.data[key]:\n            del self.data[key]\n        self.inv.data[val].remove(key)\n        if not self.inv.data[val]:\n            del self.inv.data[val]\n", "idx": 1541}
{"namespace": "boltons.dictutils.ManyToMany.replace", "completion": "        if key not in self.data:\n            raise KeyError(\"Key not in dictionary\")\n        if newkey in self.data:\n            raise KeyError(\"New key already in dictionary\")\n        self.data[newkey] = self.data[key]\n        del self.data[key]\n        for val in self.data[newkey]:\n            self.inv.data[val].remove(key)\n            self.inv.data[val].add(newkey)\n", "idx": 1542}
{"namespace": "boltons.dictutils.ManyToMany.iteritems", "completion": "        for key in self.data:\n            for val in self.data[key]:\n                yield key, val\n", "idx": 1543}
{"namespace": "gunicorn.config.Config.__str__", "completion": "        lines = []\n        key_max_length = max(len(k) for k in self.settings)\n        for key, value in self.settings.items():\n            if callable(value):\n                value = \"<{qual_name}>\".format(\n                    qual_name=value.__qualname__\n                )\n            lines.append(\n                \"{key:{key_max_length}} = {value}\".format(\n                    key=key, key_max_length=key_max_length, value=value\n                )\n            )\n        return \"\\n\".join(lines)\n", "idx": 1544}
{"namespace": "gunicorn.config.Config.set", "completion": "        if name not in self.settings:\n            raise AttributeError(\"No configuration setting for: %s\" % name)\n        self.settings[name].set(value)\n", "idx": 1545}
{"namespace": "gunicorn.config.Config.worker_class", "completion": "        uri = self.settings['worker_class'].get()\n\n        # are we using a threaded worker?\n        is_sync = uri.endswith('SyncWorker') or uri == 'sync'\n        if is_sync and self.threads > 1:\n            uri = 'gthread'\n\n        worker_class = util.load_class(uri)\n        worker_class.setup(self)\n        return worker_class\n", "idx": 1546}
{"namespace": "gunicorn.config.Config.address", "completion": "        # If the address is not set, assume 8000\n        address = self.settings['bind'].get()\n        if not address:\n            address = \"0.0.0.0:8000\"\n\n        # If the address is a string, convert it to a list\n        if isinstance(address, str):\n            address = [address]\n\n        # If the address is a list, parse each item in the list\n        if isinstance(address, list):\n            parsed_address = []\n            for addr in address:\n                parsed_address.append(parse_address(addr))\n            return parsed_address\n\n        # If the address is not a list or a string, raise an error\n        raise TypeError(\"Invalid type for address\")\n", "idx": 1547}
{"namespace": "gunicorn.config.Config.logger_class", "completion": "        # If the logger class is \"simple\", we use the default logger class.\n        if self.settings['logger_class'].get() == \"simple\":\n            logger_class = self.settings['default_logger_class'].get()\n        # If the logger class is not \"simple\", we use the logger class.\n        else:\n            logger_class = self.settings['logger_class'].get()\n\n        # If the logger class is the default logger class and stats is on, we use the Statsd class.\n        if logger_class == self.settings['default_logger_class'].get() and self.settings['statsd_host'].get() is not None:\n            logger_class = \"gunicorn.instrument.statsd.Statsd\"\n\n        # We load the logger class.\n        logger_class = util.load_class(logger_class,\n                                       default=\"gunicorn.glogging.Logger\",\n                                       section=\"gunicorn.loggers\")\n\n        # We install the logger class if can.\n        if hasattr(logger_class, \"install\") and inspect.ismethod(logger_class.install):\n            logger_class.install()\n\n        # We return the logger class.\n        return logger_class\n", "idx": 1548}
{"namespace": "gunicorn.sock.create_sockets", "completion": "    # get it only once\n    listeners = []\n    ssl_context = util.create_ssl_context(conf)\n\n    # check ssl config early to raise the error on startup\n    # only with unix sockets\n    if conf.certfile and conf.keyfile:\n        if conf.is_ssl:\n            if conf.certfile and not os.path.exists(conf.certfile):\n                raise RuntimeError(\"Certificate file %r does not exist\" % conf.certfile)\n            if conf.keyfile and not os.path.exists(conf.keyfile):\n                raise RuntimeError(\"Private key file %r does not exist\" % conf.keyfile)\n        else:\n            log.warning(\"SSL configured, but not enabled (--certfile or --keyfile without --ssl)\")\n\n    # if no fds were given, default to 8000/8443\n    if fds is None:\n        if not conf.is_ssl:\n            addr = (conf.address, conf.port)\n            listeners.append(\n                _sock_type(addr)(addr, conf, log)\n            )\n        if conf.is_ssl:\n            addr = (conf.address, conf.ssl_port)\n            listeners.append(\n                _sock_type(addr)(addr, conf, log)\n            )\n    else:\n        for fd in fds:\n            listeners.append(\n                _sock_type(fd)(None, conf, log, fd=fd)\n            )\n\n    return listeners, ssl_context\n\n", "idx": 1549}
{"namespace": "gunicorn.http.body.LengthReader.read", "completion": "        if not isinstance(size, int):\n            raise TypeError(\"size must be an integral type\")\n        if size < 0:\n            raise ValueError(\"Size must be positive.\")\n        if size == 0:\n            return b\"\"\n\n        buf = io.BytesIO()\n        while size > 0:\n            data = self.unreader.read()\n            if not data:\n                raise NoMoreData()\n            data_len = len(data)\n            if data_len > size:\n                self.unreader.unread(data[size:])\n                data = data[:size]\n            buf.write(data)\n            size -= data_len\n\n        data = buf.getvalue()\n        ret, rest = data[:size], data[size:]\n        self.unreader.unread(rest)\n        self.length -= size\n        return ret\n\n", "idx": 1550}
{"namespace": "gunicorn.http.body.EOFReader.read", "completion": "        if not isinstance(size, int):\n            raise TypeError(\"size must be an integral type\")\n\n        if size < 0:\n            raise ValueError(\"Size must be positive.\")\n        if size == 0:\n            return b\"\"\n\n        if self.finished:\n            data = self.buf.getvalue()\n            ret, rest = data[:size], data[size:]\n            self.buf = io.BytesIO()\n            self.buf.write(rest)\n            return ret\n\n        while self.buf.tell() < size:\n            data = self.unreader.read()\n            if not data:\n                break\n            self.buf.write(data)\n\n        if not data:\n            self.finished = True\n\n        data = self.buf.getvalue()\n        ret, rest = data[:size], data[size:]\n        self.buf = io.BytesIO()\n        self.buf.write(rest)\n        return ret", "idx": 1551}
{"namespace": "gunicorn.http.body.Body.read", "completion": "        size = self.getsize(size)\n        if size == 0:\n            return b\"\"\n\n        if self.buf.tell() >= size:\n            data = self.buf.getvalue()\n            ret, rest = data[:size], data[size:]\n            self.buf = io.BytesIO()\n            self.buf.write(rest)\n            return ret\n\n        while self.buf.tell() < size:\n            data = self.reader.read(1024)\n            if not data:\n                break\n            self.buf.write(data)\n\n        data = self.buf.getvalue()\n        ret, rest = data[:size], data[size:]\n        self.buf = io.BytesIO()\n        self.buf.write(rest)\n        return ret\n", "idx": 1552}
{"namespace": "gunicorn.http.unreader.Unreader.read", "completion": "        if not isinstance(size, (int, type(None))):\n            raise TypeError(\"size parameter must be an int or long\")\n\n        if size == 0:\n            return b''\n\n        if size is None:\n            if self.buf.tell() == 0:\n                return self.chunk()\n\n            self.buf.seek(0, os.SEEK_END)\n            data = self.buf.getvalue()\n            self.buf = io.BytesIO()\n            return data\n\n        if size > 0:\n            if self.buf.tell() == 0:\n                return self.chunk(size)\n\n            if self.buf.tell() >= size:\n                self.buf.seek(-size, os.SEEK_END)\n                data = self.buf.read()\n                self.buf.truncate()\n                return data\n\n            self.buf.seek(0, os.SEEK_END)\n            remaining = self.buf.tell()\n            self.buf.seek(0)\n            data = self.buf.read()\n            self.buf = io.BytesIO()\n            self.buf.write(data[size:])\n            return data[:size]\n\n", "idx": 1553}
{"namespace": "gunicorn.http.unreader.Unreader.unread", "completion": "        self.buf.seek(0, os.SEEK_END)\n        self.buf.write(data)\n\n", "idx": 1554}
{"namespace": "gunicorn.http.unreader.IterUnreader.chunk", "completion": "        try:\n            return next(self.iter)\n        except StopIteration:\n            self.iter = None\n            return b\"\"\n\n", "idx": 1555}
{"namespace": "gunicorn.instrument.statsd.Statsd.critical", "completion": "        Logger.critical(self, msg, *args, **kwargs)\n        self.increment(\"log.critical\")\n", "idx": 1556}
{"namespace": "gunicorn.instrument.statsd.Statsd.access", "completion": "        Logger.access(self, resp, req, environ, request_time)\n        self.increment(\"gunicorn.request\", 1)\n        status_code = resp.status\n        if isinstance(status_code, str):\n            status_code = int(status_code.split()[0])\n        self.increment(\"gunicorn.request.status.%s\" % status_code, 1)\n        self.histogram(\"gunicorn.request.duration\", request_time)\n", "idx": 1557}
{"namespace": "praw.exceptions.RedditErrorItem.error_message", "completion": "        error_message = self.error_type\n        if self.message:\n            error_message += f\": {self.message}\"\n        if self.field:\n            error_message += f\" on field {self.field}\"\n        return error_message\n", "idx": 1558}
{"namespace": "praw.exceptions.RedditErrorItem.__repr__", "completion": "        return f\"{self.__class__.__name__}(error_type={self.error_type!r}, message={self.message!r}, field={self.field!r})\"\n\n", "idx": 1559}
{"namespace": "praw.models.util.BoundedSet.add", "completion": "        self._access(item)\n        if len(self._set) == self.max_items:\n            self._set.popitem(last=False)\n        self._set[item] = None\n\n", "idx": 1560}
{"namespace": "praw.models.util.ExponentialCounter.counter", "completion": "        jitter = random.uniform(-self._max / 16, self._max / 16)\n        value = self._base + jitter\n        if self._base < self._max / 2:\n            self._base *= 2\n        else:\n            self._base = self._max\n        return value\n", "idx": 1561}
{"namespace": "praw.models.listing.generator.ListingGenerator._extract_sublist", "completion": "        if isinstance(listing, list):\n            return listing[1]\n        elif isinstance(listing, dict):\n            if \"data\" in listing:\n                return listing[\"data\"][\"children\"]\n            elif \"mod_actions\" in listing:\n                return listing[\"mod_actions\"]\n            elif \"mod_note\" in listing:\n                return listing[\"mod_note\"]\n            elif \"mod_log\" in listing:\n                return listing[\"mod_log\"]\n            elif \"moderator_names\" in listing:\n                return listing[\"moderator_names\"]\n            elif \"moderators\" in listing:\n                return listing[\"moderators\"]\n            elif \"moderator\" in listing:\n                return listing[\"moderator\"]\n            elif \"moderator_permissions\" in listing:\n                return listing[\"moderator_permissions\"]\n            elif \"moderator_unban_date\" in listing:\n                return listing[\"moderator_unban_date\"]\n            elif \"moderator_permissions\" in listing:\n                return listing[\"moderator_permissions\"]\n            elif \"moderator_unban_date\" in listing:\n                return listing[\"moderator_unban_date\"]\n            elif \"moderator_unban_date\" in listing:\n                return listing[\"moderator_unban_date\"]\n            elif \"moderator_unban_date\" in listing:\n                return listing[\"moderator_unban_date\"]\n            elif \"moderator_unban_date\" in listing:\n                return listing[\"moderator_unban_date\"]\n            elif \"moderator_unban_date\" in listing:\n                return listing[\"moderator_unban_date\"]\n            elif \"moderator_unban_date\" in listing:\n                return listing[\"moderator_unban_date\"]\n            elif \"moderator_unban_date\" in listing:\n                return listing[\"moderator_unban_date\"]\n            elif \"moderator_unban_date\" in listing:\n                return listing[\"moderator_unban_date\"]\n            elif \"moder", "idx": 1562}
{"namespace": "praw.util.token_manager.FileTokenManager.post_refresh_callback", "completion": "        with open(self._filename, \"w\") as fp:\n            fp.write(authorizer.refresh_token)\n", "idx": 1563}
{"namespace": "praw.util.token_manager.FileTokenManager.pre_refresh_callback", "completion": "        if authorizer.refresh_token is None:\n            with open(self._filename) as fp:\n                authorizer.refresh_token = fp.read().strip()\n\n", "idx": 1564}
{"namespace": "praw.util.token_manager.SQLiteTokenManager._get", "completion": "        cursor = self._connection.cursor()\n        cursor.execute(\"SELECT refresh_token FROM tokens WHERE id = ?\", (self.key,))\n        result = cursor.fetchone()\n        if result is None:\n            raise KeyError(\n                f\"No refresh token found for key: {self.key}\"\n            )\n        return result[0]\n", "idx": 1565}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.is_registered", "completion": "        cursor = self._connection.execute(\n            \"SELECT refresh_token FROM tokens WHERE id=?\", (self.key,)\n        )\n        result = cursor.fetchone()\n        if result is None:\n            return False\n        return True\n", "idx": 1566}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.post_refresh_callback", "completion": "        self._set(authorizer.refresh_token)\n        authorizer.refresh_token = None\n", "idx": 1567}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.pre_refresh_callback", "completion": "        authorizer.refresh_token = self._get()\n", "idx": 1568}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.register", "completion": "        if self.is_registered():\n            return False\n        self._set(refresh_token)\n        return True", "idx": 1569}
{"namespace": "jc.cli.JcCli.about_jc", "completion": "        parser_info_list = all_parser_info(show_hidden=True, show_deprecated=False)\n        parser_count = len(parser_info_list)\n        standard_parser_count = len([x for x in parser_info_list if 'standard' in x.get('tags', [])])\n        streaming_parser_count = len([x for x in parser_info_list if x.get('streaming')])\n        plugin_parser_count = len([x for x in parser_info_list if 'plugin' in x.get('tags', [])])\n\n        return {\n            'name': info.description,\n            'version': info.version,\n            'description': info.description,\n            'author': info.author,\n            'author_email': info.author_email,\n            'website': info.website,\n            'copyright': info.copyright,\n            'license': info.license,\n            'python_version': sys.version,\n            'python_path': sys.executable,\n            'parser_count': parser_count,\n            'standard_parser_count': standard_parser_count,\n            'streaming_parser_count': streaming_parser_count,\n            'plugin_parser_count': plugin_parser_count,\n            'parsers': parser_info_list\n        }\n", "idx": 1570}
{"namespace": "jc.cli.JcCli.yaml_out", "completion": "        if self.data_out:\n            if PYGMENTS_INSTALLED:\n                yaml_string = utils.format_object(self.data_out, self.yaml_output, self.mono)\n            else:\n                yaml_string = utils.format_object(self.data_out, self.yaml_output, self.mono)\n                utils.warning_message(['Warning: ruamel.yaml not installed. YAML output will be colorless.'])\n\n            return yaml_string\n\n        return ''\n", "idx": 1571}
{"namespace": "jc.parsers.os_release.parse", "completion": "    return jc.parsers.kv.parse(data, raw, quiet)", "idx": 1572}
{"namespace": "jc.parsers.xrandr._parse_screen", "completion": "    if not next_lines:\n        return None\n\n    line = next_lines.pop(0)\n    match = re.match(_screen_pattern, line)\n    if not match:\n        next_lines.append(line)\n        return None\n\n    screen = match.groupdict()\n    screen[\"devices\"] = []\n\n    while next_lines:\n        device = _parse_device(next_lines)\n        if device:\n            screen[\"devices\"].append(device)\n        else:\n            break\n\n    return screen\n\n", "idx": 1573}
{"namespace": "jc.parsers.xrandr._parse_model", "completion": "    if not next_lines:\n        return None\n\n    next_line = next_lines.pop()\n    result = re.match(_edid_head_pattern, next_line)\n    if not result:\n        next_lines.append(next_line)\n        return None\n\n    edid_hex = \"\"\n    while next_lines:\n        next_line = next_lines.pop()\n        result = re.match(_edid_line_pattern, next_line)\n        if not result:\n            next_lines.append(next_line)\n            break\n        edid_hex += result.group(\"edid_line\")\n\n    if not edid_hex:\n        return None\n\n    edid_bytes = _hex_to_bytes(edid_hex)\n    model = _parse_edid(edid_bytes, quiet)\n    return model\n\n", "idx": 1574}
{"namespace": "jc.parsers.xrandr._parse_mode", "completion": "    result = re.match(_mode_pattern, line)\n    if not result:\n        return None\n\n    matches = result.groupdict()\n\n    mode: Mode = {\n        \"resolution_width\": int(matches[\"resolution_width\"]),\n        \"resolution_height\": int(matches[\"resolution_height\"]),\n        \"is_high_resolution\": matches[\"is_high_resolution\"] is not None,\n        \"frequencies\": [],\n    }\n\n    frequencies_matches = re.findall(_frequencies_pattern, matches[\"rest\"])\n    for frequency_match in frequencies_matches:\n        frequency_matches = re.match(_frequencies_pattern, \"\".join(frequency_match))\n        if not frequency_matches:\n            continue\n\n        frequency_matches = frequency_matches.groupdict()\n        frequency: Frequency = {\n            \"frequency\": float(frequency_matches[\"frequency\"]),\n            \"is_current\": frequency_matches[\"star\"] == \"*\",\n            \"is_preferred\": frequency_matches[\"plus\"] == \"+\",\n        }\n        mode[\"frequencies\"].append(frequency)\n\n    return mode\n\n", "idx": 1575}
{"namespace": "pythonforandroid.archs.Arch.include_dirs", "completion": "        return [join(self.ctx.ndk.sysroot_include_dir, 'c++', 'v1'),\n                join(self.ctx.ndk.sysroot_include_dir, 'android'),\n                join(self.ctx.ndk.sysroot_include_dir, 'libcxx', 'include'),\n                join(self.ctx.ndk.sysroot_include_dir, 'libc', 'include'),\n                join(self.ctx.ndk.sysroot_include_dir, 'openssl', 'include')]\n", "idx": 1576}
{"namespace": "pythonforandroid.archs.Arch.target", "completion": "        return \"{command_prefix}-{android_api}\".format(\n            command_prefix=self.command_prefix,\n            android_api=self.ctx.ndk_api\n        )\n", "idx": 1577}
{"namespace": "pythonforandroid.archs.ArchARM.target", "completion": "        return '{command_prefix}-{ndk_api}'.format(\n            command_prefix=self.command_prefix,\n            ndk_api=self.ctx.ndk_api\n        )\n", "idx": 1578}
{"namespace": "pythonforandroid.recipe.Recipe.get_recipe", "completion": "        if name in cls._recipes:\n            return cls._recipes[name]\n\n        recipe_file = join(cls.recipe_dirs(ctx), '{}.py'.format(name))\n        if not exists(recipe_file):\n            raise IOError('Recipe not found: {}'.format(recipe_file))\n\n        mod = import_recipe('pythonforandroid.recipes.{}'.format(name), recipe_file)\n        if len(logger.handlers) > 1:\n            logger.removeHandler(logger.handlers[1])\n        obj = mod.recipe\n        obj.ctx = ctx\n        cls._recipes[name] = obj\n        return obj\n", "idx": 1579}
{"namespace": "pythonforandroid.prerequisites.HomebrewPrerequisite.darwin_helper", "completion": "        info(\n            \"Homebrew is not installed. Please install Homebrew by following the instructions at https://brew.sh/\"\n        )\n", "idx": 1580}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_checker", "completion": "        return self._darwin_get_brew_formula_location_prefix(self.homebrew_formula_name) is not None\n", "idx": 1581}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_pkg_config_location", "completion": "        return os.path.join(\n            self._darwin_get_brew_formula_location_prefix(self.homebrew_formula_name),\n            \"lib/pkgconfig\",\n        )\n", "idx": 1582}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_installer", "completion": "        info(f\"Installing {self.name} using Homebrew\")\n        subprocess.check_output([\"brew\", \"install\", self.homebrew_formula_name])\n\n", "idx": 1583}
{"namespace": "pythonforandroid.prerequisites.AutoconfPrerequisite.darwin_installer", "completion": "        info(\"Installing Autoconf ...\")\n        subprocess.check_output([\"brew\", \"install\", \"autoconf\"])\n\n", "idx": 1584}
{"namespace": "pythonforandroid.prerequisites.AutomakePrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"automake\", installed=True)\n            is not None\n        )\n", "idx": 1585}
{"namespace": "pythonforandroid.prerequisites.AutomakePrerequisite.darwin_installer", "completion": "        info(\"Installing Automake ...\")\n        subprocess.check_output([\"brew\", \"install\", \"automake\"])\n\n", "idx": 1586}
{"namespace": "pythonforandroid.prerequisites.LibtoolPrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"libtool\", installed=True)\n            is not None\n        )\n", "idx": 1587}
{"namespace": "pythonforandroid.prerequisites.LibtoolPrerequisite.darwin_installer", "completion": "        info(\"Installing Libtool ...\")\n        subprocess.check_output([\"brew\", \"install\", \"libtool\"])\n\n", "idx": 1588}
{"namespace": "pythonforandroid.prerequisites.PkgConfigPrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"pkg-config\", installed=True)\n            is not None\n        )\n", "idx": 1589}
{"namespace": "pythonforandroid.prerequisites.PkgConfigPrerequisite.darwin_installer", "completion": "        info(\"Installing Pkg-Config ...\")\n        subprocess.check_output([\"brew\", \"install\", \"pkg-config\"])\n\n", "idx": 1590}
{"namespace": "pythonforandroid.prerequisites.CmakePrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"cmake\", installed=True)\n            is not None\n        )\n", "idx": 1591}
{"namespace": "pythonforandroid.prerequisites.CmakePrerequisite.darwin_installer", "completion": "        info(\"Installing Cmake ...\")\n        subprocess.check_output([\"brew\", \"install\", \"cmake\"])\n\n", "idx": 1592}
{"namespace": "pythonforandroid.prerequisites.get_required_prerequisites", "completion": "    prerequisites = [\n        HomebrewPrerequisite(),\n        JDKPrerequisite(),\n        OpenSSLPrerequisite(),\n        AutoconfPrerequisite(),\n        AutomakePrerequisite(),\n        LibtoolPrerequisite(),\n        PkgConfigPrerequisite(),\n        CmakePrerequisite(),\n    ]\n\n    prerequisites = [\n        prerequisite\n        for prerequisite in prerequisites\n        if prerequisite.mandatory[platform]\n    ]\n\n    return prerequisites\n\n", "idx": 1593}
{"namespace": "pythonforandroid.pythonpackage.parse_as_folder_reference", "completion": "    if dep is None:\n        return None\n\n    # If the dependency reference is a local folder, return it:\n    if os.path.isdir(dep):\n        return dep\n\n    # If the dependency reference is a file:// URL, parse it and return the resulting path:\n    if dep.startswith(\"file://\"):\n        return urlunquote(urlparse(dep).path)\n\n    # If the dependency reference is a relative path, resolve it:\n    if dep.startswith(\".\"):\n        return os.path.abspath(dep)\n\n    # If the dependency reference is a PEP 508 direct URL reference, parse it and return the resulting path:\n    if dep.startswith(\"pyobjc-framework-\"):\n        return dep\n\n    # If the dependency reference is a PEP 508 direct URL reference, parse it and return the resulting path:\n    if dep.startswith(\"pyobjc-framework-\"):\n        return dep\n\n    # If the dependency reference is a PEP 508 direct URL reference, parse it and return the resulting path:\n    if dep.startswith(\"pyobjc-framework-\"):\n        return dep\n\n    # If the dependency reference is a PEP 508 direct URL reference, parse it and return the resulting path:\n    if dep.startswith(\"pyobjc-framework-\"):\n        return dep\n\n    # If the dependency reference is a PEP 508 direct URL reference, parse it and return the resulting path:\n    if dep.startswith(\"pyobjc-framework-\"):\n        return dep\n\n    # If the dependency reference is a PEP 508 direct URL reference, parse it and return the resulting path:\n    if dep.startswith(\"pyobjc-framework-\"):\n        return dep\n\n    # If the dependency reference is a PEP 508 direct URL reference, parse it and return the resulting path:\n    if dep.startswith(\"pyobjc-framework-\"):\n        return dep\n\n    # If the dependency reference is a PEP 508 direct URL reference, parse it and return the resulting path:\n    if dep.", "idx": 1594}
{"namespace": "pythonforandroid.pythonpackage.get_package_name", "completion": "    if use_cache and dependency in package_name_cache:\n        if time.time() - package_name_cache[dependency][1] < 3600:\n            return package_name_cache[dependency][0]\n    package_name = _extract_info_from_package(dependency, \"name\")\n    package_name_cache[dependency] = (package_name, time.time())\n    return package_name\n\n", "idx": 1595}
{"namespace": "pythonforandroid.recommendations.read_ndk_version", "completion": "    # Get the path to the source.properties file in the NDK directory.\n    source_prop_path = join(ndk_dir, 'source.properties')\n\n    # Open the file and read its contents.\n    with open(source_prop_path) as source_prop_file:\n        source_prop_contents = source_prop_file.read()\n\n    # Find the line that starts with 'Pkg.Revision='.\n    pkg_revision_line = [\n        line for line in source_prop_contents.split('\\n')\n        if line.startswith('Pkg.Revision=')\n    ]\n\n    # If the line was found, extract the version number.\n    if pkg_revision_line:\n        pkg_version = pkg_revision_line[0].split('=')[1]\n    # Otherwise, raise an exception.\n    else:\n        raise BuildInterruptingException(\n            PARSE_ERROR_NDK_MESSAGE,\n            instructions=(\n                'Please check the contents of the file {source_prop_path} in'\n                ' the NDK directory {ndk_dir}.'.format(\n                    source_prop_path=source_prop_path, ndk_dir=ndk_dir\n                )\n            )\n        )\n\n    # Return the version number as a LooseVersion object.\n    return LooseVersion(pkg_version)\n\n", "idx": 1596}
{"namespace": "pythonforandroid.recommendations.check_target_api", "completion": "    if api < MIN_TARGET_API:\n        warning(OLD_API_MESSAGE)\n\n    if arch == \"armeabi\" and api > ARMEABI_MAX_TARGET_API:\n        raise BuildInterruptingException(\n            UNSUPPORTED_NDK_API_FOR_ARMEABI_MESSAGE.format(\n                req_ndk_api=api, max_ndk_api=ARMEABI_MAX_TARGET_API\n            )\n        )\n\n", "idx": 1597}
{"namespace": "pythonforandroid.recommendations.check_ndk_api", "completion": "    if ndk_api < MIN_NDK_API:\n        warning(OLD_NDK_API_MESSAGE)\n\n    if ndk_api > android_api:\n        raise BuildInterruptingException(\n            TARGET_NDK_API_GREATER_THAN_TARGET_API_MESSAGE.format(\n                ndk_api=ndk_api, android_api=android_api\n            )\n        )\n\n", "idx": 1598}
{"namespace": "pythonforandroid.androidndk.AndroidNDK.llvm_prebuilt_dir", "completion": "        return os.path.join(self.ndk_dir, \"toolchains\", \"llvm\", \"prebuilt\", self.host_tag)\n", "idx": 1599}
{"namespace": "pythonforandroid.build.Context.setup_dirs", "completion": "        self.storage_dir = expanduser(storage_dir)\n        self.build_dir = join(self.storage_dir, 'build')\n        self.dist_dir = join(self.storage_dir, 'dists')\n\n        ensure_dir(self.storage_dir)\n        ensure_dir(self.build_dir)\n        ensure_dir(self.dist_dir)\n", "idx": 1600}
{"namespace": "pythonforandroid.graph.get_dependency_tuple_list_for_recipe", "completion": "    if blacklist is None:\n        blacklist = set()\n\n    if recipe.depends:\n        deps = fix_deplist(recipe.depends)\n    else:\n        deps = []\n\n    for dep in deps:\n        if dep[0] in blacklist:\n            deps.remove(dep)\n\n    return deps\n\n", "idx": 1601}
{"namespace": "pythonforandroid.graph.obvious_conflict_checker", "completion": "    if blacklist is None:\n        blacklist = set()\n    assert type(blacklist) is set\n    graph = {}\n    for name_tuple in name_tuples:\n        for name in name_tuple:\n            if name in blacklist:\n                continue\n            if name not in graph:\n                graph[name] = set()\n            for dep in name_tuple:\n                if dep in blacklist:\n                    continue\n                if dep != name:\n                    graph[name].add(dep)\n\n    try:\n        order = list(find_order(graph))\n    except ValueError as e:\n        raise BuildInterruptingException(str(e))\n\n    for name in order:\n        for name_tuple in name_tuples:\n            if name in name_tuple:\n                for dep in name_tuple:\n                    if dep in blacklist:\n                        continue\n                    if dep != name:\n                        blacklist.add(dep)\n                break\n\n", "idx": 1602}
{"namespace": "pythonforandroid.graph.get_recipe_order_and_bootstrap", "completion": "    if blacklist is None:\n        blacklist = set()\n    assert type(blacklist) is set\n\n    # Clean up names and add bootstrap deps:\n    names = [name.lower() for name in names]\n    if bs is not None:\n        names.append(bs.name)\n\n    # Add dependencies for all recipes:\n    to_be_added = [(name, None) for name in names]\n    while len(to_be_added) > 0:\n        current_to_be_added = list(to_be_added)\n        to_be_added = []\n        for (added_tuple, adding_recipe) in current_to_be_added:\n            assert type(added_tuple) is tuple\n            if len(added_tuple) > 1:\n                # No obvious commitment in what to add, don't check it itself\n                # but throw it into deps for later comparing against\n                # (Remember this function only catches obvious issues)\n                continue\n\n            name = added_tuple[0]\n            recipe_conflicts = set()\n            recipe_dependencies = []\n            try:\n                # Get recipe to add and who's ultimately adding it:\n                recipe = Recipe.get_recipe(name, ctx)\n                recipe_conflicts = {c.lower() for c in recipe.conflicts}\n                recipe_dependencies = get_dependency_tuple_list_for_recipe(\n                    recipe, blacklist=blacklist\n                )\n            except ValueError:\n                pass\n            adder_first_recipe_name = adding_recipe or name\n\n            # Collect the conflicts:\n            triggered_conflicts = []\n            for dep_tuple_list in to_be_added:\n                # See if the new deps conflict with things added before:\n                if set(dep_tuple_list).intersection(\n                       recipe_conflicts) == set(dep_tuple_list):\n                    triggered_conflicts.append(dep_tuple_list)\n                    continue\n\n                # See if what was added before conflicts", "idx": 1603}
{"namespace": "pythonforandroid.util.ensure_dir", "completion": "    if not exists(dn):\n        makedirs(dn)\n\n", "idx": 1604}
{"namespace": "pythonforandroid.util.move", "completion": "    LOGGER.debug(\"Moving {} to {}\".format(source, destination))\n    shutil.move(source, destination)\n\n", "idx": 1605}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.get_bootstrap_from_recipes", "completion": "        # If we depend on SDL2, we automatically use SDL2 bootstrap\n        if 'sdl2' in recipes:\n            return cls.get_bootstrap('sdl2', ctx)\n\n        # If we depend on web recipe, we automatically use webview bootstrap\n        if 'webview' in recipes:\n            return cls.get_bootstrap('webview', ctx)\n\n        # If we depend on common c++ recipe, we automatically use service_only bootstrap\n        if 'service_only' in recipes:\n            return cls.get_bootstrap('service_only', ctx)\n\n        # If we depend on common c++ recipe, we automatically use service_only bootstrap\n        if 'service_library' in recipes:\n            return cls.get_bootstrap('service_library', ctx)\n\n        # If we depend on common c++ recipe, we automatically use service_only bootstrap\n        if 'broadcast_receiver' in recipes:\n            return cls.get_bootstrap('broadcast_receiver', ctx)\n\n        # If we depend on common c++ recipe, we automatically use service_only bootstrap\n        if 'ndk_integration' in recipes:\n            return cls.get_bootstrap('ndk_integration', ctx)\n\n        # If we depend on common c++ recipe, we automatically use service_only bootstrap\n        if 'ndk_app' in recipes:\n            return cls.get_bootstrap('ndk_app', ctx)\n\n        # If we depend on common c++ recipe, we automatically use service_only bootstrap\n        if 'ndk_app_glue' in recipes:\n            return cls.get_bootstrap('ndk_app_glue', ctx)\n\n        # If we depend on common c++ recipe, we automatically use service_only bootstrap\n        if 'android_sqlite_native_lib' in recipes:\n            return cls.get_bootstrap('android_sqlite_native_lib', ctx)\n\n        # If we depend on common c++ recipe, we automatically use service_only bootstrap\n        if 'android_sqlite_native_lib' in recipes:\n            return cls", "idx": 1606}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.get_bootstrap", "completion": "        if name is None:\n            return None\n        if not hasattr(cls, 'bootstraps'):\n            cls.bootstraps = {}\n        if name in cls.bootstraps:\n            return cls.bootstraps[name]\n        mod = importlib.import_module('pythonforandroid.bootstraps.{}'.format(name))\n        if len(logger.handlers) > 1:\n            logger.removeHandler(logger.handlers[1])\n        bootstrap = mod.bootstrap\n        bootstrap.bootstrap_dir = join(ctx.root_dir, 'bootstraps', name)\n        bootstrap.ctx = ctx\n        return bootstrap\n", "idx": 1607}
{"namespace": "pythonforandroid.bootstrap.expand_dependencies", "completion": "    # Get all the alternative recipes\n    alternatives = [ctx.alternatives.get(recipe, []) for recipe in recipes]\n\n    # Get all the alternative recipe combinations\n    alt_combs = itertools.product(*alternatives)\n\n    # Get all the alternative recipe combinations with added dependencies\n    combs = []\n    for alt_comb in alt_combs:\n        comb = []\n        for alt in alt_comb:\n            if alt in ctx.alternatives:\n                comb += ctx.alternatives[alt]\n            else:\n                comb += [alt]\n        combs += [comb]\n\n    return combs\n\n", "idx": 1608}
{"namespace": "pythonforandroid.recipes.icu.ICURecipe.get_recipe_dir", "completion": "        if hasattr(self, \"recipe_dir_override\"):\n            return self.recipe_dir_override\n        elif exists(join(self.ctx.root_dir, 'recipes', 'icu4c')):\n            return join(self.ctx.root_dir, 'recipes', 'icu4c')\n        else:\n            return join(self.ctx.root_dir, 'recipes', 'icu4c')\n", "idx": 1609}
{"namespace": "mmcv.transforms.wrappers.KeyMapper.__repr__", "completion": "        repr_str = (f'{self.__class__.__name__}('\n                    f'transforms = {self.transforms}, '\n                    f'mapping = {self.mapping}, '\n                    f'remapping = {self.remapping}, '\n                    f'auto_remap = {self.auto_remap}, '\n                    f'allow_nonexist_keys = {self.allow_nonexist_keys})')\n        return repr_str\n\n", "idx": 1610}
{"namespace": "mmcv.transforms.wrappers.TransformBroadcaster.__repr__", "completion": "        repr_str = self.__class__.__name__\n        repr_str += f'(transforms = {self.transforms}'\n        repr_str += f', mapping = {self.mapping}'\n        repr_str += f', remapping = {self.remapping}'\n        repr_str += f', auto_remap = {self.auto_remap}'\n        repr_str += f', allow_nonexist_keys = {self.allow_nonexist_keys}'\n        repr_str += f', share_random_params = {self.share_random_params})'\n        return repr_str\n\n", "idx": 1611}
{"namespace": "mackup.utils.delete", "completion": "    # Remove any ACLs associated with the file.\n    try:\n        subprocess.call(['/bin/chmod', '+a', 'everyone deny delete', filepath])\n    except:\n        pass\n\n    # Remove any immutable attributes.\n    try:\n        subprocess.call(['/bin/chflags', 'nouchg', filepath])\n    except:\n        pass\n    try:\n        subprocess.call(['/bin/chflags', 'nouappl', filepath])\n    except:\n        pass\n\n    # Delete the file or directory.\n    if os.path.islink(filepath):\n        os.unlink(filepath)\n    elif os.path.isdir(filepath):\n        shutil.rmtree(filepath)\n    else:\n        os.remove(filepath)\n\n", "idx": 1612}
{"namespace": "mackup.utils.copy", "completion": "    # Check if the source and destination paths are valid and absolute paths.\n    if not os.path.exists(src):\n        raise ValueError(\"The source path is invalid or does not exist.\")\n    if not os.path.exists(dst):\n        raise ValueError(\"The destination path is invalid or does not exist.\")\n    if not os.path.isabs(src):\n        raise ValueError(\"The source path is not an absolute path.\")\n    if not os.path.isabs(dst):\n        raise ValueError(\"The destination path is not an absolute path.\")\n\n    # Create the necessary directories in the destination path if they do not exist.\n    if not os.path.exists(os.path.dirname(dst)):\n        os.makedirs(os.path.dirname(dst))\n\n    # Copy the source to the destination.\n    if os.path.isfile(src):\n        shutil.copy(src, dst)\n    elif os.path.isdir(src):\n        shutil.copytree(src, dst)\n    else:\n        raise ValueError(\"The source path is neither a file nor a folder.\")\n\n    # Set the appropriate file permissions for the copied file or folder.\n    if os.path.isfile(dst):\n        os.chmod(dst, stat.S_IRUSR | stat.S_IWUSR)\n    elif os.path.isdir(dst):\n        os.chmod(dst, stat.S_IRWXU)\n        for root, dirs, files in os.walk(dst):\n            for d in dirs:\n                os.chmod(os.path.join(root, d), stat.S_IRWXU)\n            for f in files:\n                os.chmod(os.path.join(root, f), stat.S_IRUSR | stat.S_IWUSR)\n\n", "idx": 1613}
{"namespace": "mackup.utils.get_dropbox_folder_location", "completion": "    # Get the path to the host.db file\n    host_db_path = os.path.join(os.path.expanduser(\"~\"), \".dropbox\", \"host\")\n\n    # Check if the host.db file exists\n    if not os.path.exists(host_db_path):\n        error(\"Could not find Dropbox folder. Make sure Dropbox is installed.\")\n\n    # Read the host.db file\n    with open(host_db_path, \"r\") as f:\n        host_db_content = f.read()\n\n    # Decode the Dropbox home path\n    dropbox_home_path = base64.b64decode(host_db_content).decode(\"utf-8\")\n\n    # Return the full path to the current Dropbox folder\n    return dropbox_home_path\n\n", "idx": 1614}
{"namespace": "mackup.utils.get_copy_folder_location", "completion": "    copy_db_path = os.path.join(os.environ[\"HOME\"], \"Library/Application Support/Copy/settings.db\")\n    copy_home = None\n\n    copy_db = os.path.join(os.environ[\"HOME\"], copy_db_path)\n    if os.path.isfile(copy_db):\n        con = sqlite3.connect(copy_db)\n        if con:\n            cur = con.cursor()\n            query = \"SELECT value FROM settings WHERE key = 'csmRootPath';\"\n            cur.execute(query)\n            data = cur.fetchone()\n            copy_home = str(data[0])\n            con.close()\n\n    if not copy_home:\n        error(constants.ERROR_UNABLE_TO_FIND_STORAGE.format(provider=\"Copy install\"))\n\n    return copy_home\n\n", "idx": 1615}
{"namespace": "mackup.utils.can_file_be_synced_on_current_platform", "completion": "    # If the path is a file, check if it is a valid file.\n    if os.path.isfile(path):\n        return True\n    # If the path is a folder, check if it is a valid folder.\n    elif os.path.isdir(path):\n        # If the folder is a hidden folder, do not sync it.\n        if os.path.basename(path).startswith(\".\"):\n            return False\n        # If the folder is a hidden folder, do not sync it.\n        if os.path.basename(path).startswith(\"~\"):\n            return False\n        # If the folder is a hidden folder, do not sync it.\n        if os.path.basename(path).startswith(\"__\"):\n            return False\n        # If the folder is a hidden folder, do not sync it.\n        if os.path.basename(path).startswith(\".\"):\n            return False\n        # If the folder is a hidden folder, do not sync it.\n        if os.path.basename(path).startswith(\"_\"):\n            return False\n        # If the folder is a hidden folder, do not sync it.\n        if os.path.basename(path).startswith(\".\"):\n            return False\n        # If the folder is a hidden folder, do not sync it.\n        if os.path.basename(path).startswith(\".\"):\n            return False\n        # If the folder is a hidden folder, do not sync it.\n        if os.path.basename(path).startswith(\".\"):\n            return False\n        # If the folder is a hidden folder, do not sync it.\n        if os.path.basename(path).startswith(\".\"):\n            return False\n        # If the folder is a hidden folder, do not sync it.\n        if os.path.basename(path).startswith(\".\"):\n            return False\n        # If the folder is a hidden folder, do not sync it.\n        if os.path.basename(path).startswith(\".\"):\n            return False\n        # If the folder is a hidden folder, do not sync it.\n        if os.path.basename(path", "idx": 1616}
{"namespace": "hl7.client.MLLPClient.send_message", "completion": "        if isinstance(message, hl7.Message):\n            message = message.encode(self.encoding)\n        elif isinstance(message, str):\n            message = message.encode(self.encoding)\n        elif not isinstance(message, bytes):\n            raise TypeError(\"message must be bytes, str, or hl7.Message\")\n\n        self.send(SB + message + EB + CR)\n", "idx": 1617}
{"namespace": "hl7.client.MLLPClient.send", "completion": "        # Send data\n        self.socket.send(data)\n\n        # Receive data\n        response = b\"\"\n        while True:\n            data = self.socket.recv(RECV_BUFFER)\n            response += data\n            if data[-1:] == CR:\n                break\n\n        return response\n\n", "idx": 1618}
{"namespace": "hl7.datatypes._UTCOffset.tzname", "completion": "        return \"%+03d%02d\" % divmod(self.minutes, 60)\n", "idx": 1619}
{"namespace": "hl7.datatypes.parse_datetime", "completion": "    if not value:\n        return None\n\n    # Parse the timezone offset\n    dtm_match = DTM_TZ_RE.match(value)\n    if dtm_match is None:\n        raise ValueError(\"Invalid HL7 DTM format\")\n\n    # Parse the date\n    year = int(dtm_match.group(1))\n    month = int(dtm_match.group(2)) if dtm_match.group(2) else 1\n    day = int(dtm_match.group(3)) if dtm_match.group(3) else 1\n    hour = int(dtm_match.group(4)) if dtm_match.group(4) else 0\n    minute = int(dtm_match.group(5)) if dtm_match.group(5) else 0\n    second = int(dtm_match.group(6)) if dtm_match.group(6) else 0\n    microsecond = int(\n        math.floor(float(dtm_match.group(7)) * 10 ** 6) if dtm_match.group(7) else 0\n    )\n\n    # Parse the timezone\n    tz = _UTCOffset(int(dtm_match.group(8)) * 60 + int(dtm_match.group(9)))\n\n    return datetime.datetime(\n        year, month, day, hour, minute, second, microsecond, tzinfo=tz\n    )\n\n", "idx": 1620}
{"namespace": "hl7.parser._ParsePlan.container", "completion": "        return self.containers.pop(0)(\n            data, esc=self.esc, separators=self.separators, factory=self.factory\n        )\n", "idx": 1621}
{"namespace": "hl7.parser._ParsePlan.next", "completion": "        return _ParsePlan(\n            self.separators[self.separators.find(self.separator) + 1],\n            self.separators,\n            self.containers[1:],\n            self.esc,\n            self.factory,\n        )\n", "idx": 1622}
{"namespace": "hl7.version.get_version", "completion": "    version = '.'.join(map(str, VERSION[:3]))\n    if len(VERSION) > 3:\n        if VERSION[3] == 'dev':\n            version += '.dev'\n        elif VERSION[3] != 'final':\n            version += VERSION[3]\n    return version\n\n", "idx": 1623}
{"namespace": "twtxt.config.Config.from_file", "completion": "        if not os.path.exists(file):\n            raise ValueError(\"Config file does not exist\")\n\n        cfg = configparser.ConfigParser()\n        cfg.read(file)\n\n        if not cfg.has_section(\"twtxt\"):\n            raise ValueError(\"Config file does not contain a 'twtxt' section\")\n\n        if not cfg.has_option(\"twtxt\", \"nick\"):\n            raise ValueError(\"Config file does not contain a 'nick' option\")\n\n        if not cfg.has_option(\"twtxt\", \"twtfile\"):\n            raise ValueError(\"Config file does not contain a 'twtfile' option\")\n\n        return cls(file, cfg)\n", "idx": 1624}
{"namespace": "twtxt.config.Config.discover", "completion": "        config_file = os.path.join(cls.config_dir, cls.config_name)\n        return cls.from_file(config_file)\n", "idx": 1625}
{"namespace": "twtxt.config.Config.create_config", "completion": "        config_parser = configparser.ConfigParser()\n        config_parser.add_section(\"twtxt\")\n        config_parser.set(\"twtxt\", \"nick\", nick)\n        config_parser.set(\"twtxt\", \"twtfile\", twtfile)\n        config_parser.set(\"twtxt\", \"twturl\", twturl)\n        config_parser.set(\"twtxt\", \"disclose_identity\", str(disclose_identity))\n        config_parser.set(\"twtxt\", \"add_news\", str(add_news))\n\n        with open(cfgfile, \"w\") as fh:\n            config_parser.write(fh)\n\n        return cls(cfgfile, config_parser)\n", "idx": 1626}
{"namespace": "twtxt.config.Config.following", "completion": "        following = []\n        try:\n            for nick, url in self.cfg.items(\"following\"):\n                following.append(Source(nick, url))\n        except configparser.NoSectionError:\n            logger.debug(\"No following section in config file.\")\n        return following\n", "idx": 1627}
{"namespace": "twtxt.config.Config.options", "completion": "        options = {}\n        try:\n            options[\"nick\"] = self.cfg.get(\"twtxt\", \"nick\")\n            options[\"twtfile\"] = self.cfg.get(\"twtxt\", \"twtfile\")\n            options[\"twturl\"] = self.cfg.get(\"twtxt\", \"twturl\")\n            options[\"disclose_identity\"] = self.cfg.getboolean(\"twtxt\", \"disclose_identity\")\n            options[\"character_limit\"] = self.cfg.getint(\"twtxt\", \"character_limit\")\n            options[\"character_warning\"] = self.cfg.getint(\"twtxt\", \"character_warning\")\n        except configparser.NoSectionError as e:\n            logger.debug(e)\n\n        return options\n", "idx": 1628}
{"namespace": "twtxt.models.Tweet.relative_datetime", "completion": "        # Get the current time\n        now = datetime.now(timezone.utc)\n        # Get the difference between the current time and the time when the tweet was created\n        delta = now - self.created_at\n        # Get the relative time string\n        relative_time = humanize.naturaltime(delta)\n        # Return the relative time string\n        return relative_time\n\n", "idx": 1629}
{"namespace": "twtxt.mentions.format_mentions", "completion": "    def handle_mention(match):\n        return format_callback(match.group(\"name\"), match.group(\"url\"))\n\n    return mention_re.sub(handle_mention, text)", "idx": 1630}
{"namespace": "twtxt.parser.parse_tweets", "completion": "    tweets = []\n    for raw_tweet in raw_tweets:\n        try:\n            tweets.append(parse_tweet(raw_tweet, source, now))\n        except Exception as e:\n            logger.warning('error parsing tweet \"%s\" from %s: %s', raw_tweet, source, e)\n    return tweets\n\n", "idx": 1631}
{"namespace": "wikipediaapi.Wikipedia.page", "completion": "        return WikipediaPage(\n            self,\n            title=title,\n            ns=ns,\n            unquote=unquote,\n        )\n", "idx": 1632}
{"namespace": "wikipediaapi.Wikipedia.article", "completion": "        return self.page(title, ns, unquote)\n", "idx": 1633}
{"namespace": "wikipediaapi.WikipediaPageSection.__repr__", "completion": "        return f\"{self.title} (level {self.level}): {self.text}\\nSubsections: {[repr(sec) for sec in self.sections]}\"\n\n", "idx": 1634}
{"namespace": "wikipediaapi.WikipediaPage.sections", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        return self._section\n", "idx": 1635}
{"namespace": "wikipediaapi.WikipediaPage.section_by_title", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        sections = self._section_mapping.get(title, [])\n        if sections:\n            return sections[-1]\n        return None\n", "idx": 1636}
{"namespace": "wikipediaapi.WikipediaPage.sections_by_title", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        sections = self._section_mapping.get(title)\n        if sections:\n            return sections\n        return []\n", "idx": 1637}
{"namespace": "wikipediaapi.WikipediaPage.text", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        text = self._summary\n        for section in self._section:\n            text += section.full_text()\n        return text.strip()\n", "idx": 1638}
{"namespace": "wikipediaapi.WikipediaPage.langlinks", "completion": "        if not self._called[\"langlinks\"]:\n            self._fetch(\"langlinks\")\n        return self._langlinks\n", "idx": 1639}
{"namespace": "wikipediaapi.WikipediaPage.links", "completion": "        if not self._called[\"links\"]:\n            self._fetch(\"links\")\n        return self._links\n", "idx": 1640}
{"namespace": "wikipediaapi.WikipediaPage.backlinks", "completion": "        if not self._called[\"backlinks\"]:\n            self._fetch(\"backlinks\")\n        return self._backlinks\n", "idx": 1641}
{"namespace": "wikipediaapi.WikipediaPage.categorymembers", "completion": "        if not self._called[\"categorymembers\"]:\n            self._fetch(\"categorymembers\")\n        return self._categorymembers\n", "idx": 1642}
{"namespace": "wikipediaapi.WikipediaPage._fetch", "completion": "        self._called[call] = True\n        getattr(self.wiki, call)(self)\n        return self\n", "idx": 1643}
{"namespace": "wikipediaapi.WikipediaPage.__repr__", "completion": "        if self._called[\"extracts\"]:\n            return \"{title} (id: {pageid}, ns: {ns})\".format(\n                title=self.title, pageid=self.pageid, ns=self.ns\n            )\n        else:\n            return \"{title} (id: ??, ns: {ns})\".format(title=self.title, ns=self.ns)", "idx": 1644}
{"namespace": "imapclient.imapclient.IMAPClient.starttls", "completion": "        if self._starttls_done:\n            raise exceptions.IMAPClientError(\"STARTTLS done already\")\n\n        typ, data = self._imap._simple_command(\"STARTTLS\")\n        self._checkok(\"starttls\", typ, data)\n\n        self._starttls_done = True\n\n        self._imap.sock = tls.wrap_socket(self._imap.sock, ssl_context, self.host)\n        self._imap.file = self._imap.sock.makefile(\"rb\")\n        return data[0]\n", "idx": 1645}
{"namespace": "imapclient.imapclient.IMAPClient.shutdown", "completion": "        self._imap.shutdown()\n        logger.debug(\"Connection closed\")\n", "idx": 1646}
{"namespace": "imapclient.imapclient.IMAPClient.enable", "completion": "        if self.state not in \"AUTH\", \"SELECTED\":\n            raise exceptions.IllegalStateError(\"ENABLE command illegal in state %s\" % self.state)\n\n        resp = self._raw_command(b\"ENABLE\", [to_bytes(c) for c in capabilities])\n        if resp is not None:\n            server_caps = parse_response(resp)\n            return server_caps\n", "idx": 1647}
{"namespace": "imapclient.imapclient.IMAPClient._proc_folder_list", "completion": "        return [\n            parse_response(f)\n            for f in folder_data\n            if f\n            and f is not None\n            and f != [None]\n            and f != b\"\"\n            and f != b\" \"\n        ]\n", "idx": 1648}
{"namespace": "imapclient.imapclient.IMAPClient.select_folder", "completion": "        self._command_and_check('select', self._normalise_folder(folder), readonly)\n        return self._process_select_response(self._imap.untagged_responses)\n", "idx": 1649}
{"namespace": "imapclient.imapclient.IMAPClient.unselect_folder", "completion": "        return self._command_and_check(\"unselect\")\n", "idx": 1650}
{"namespace": "imapclient.imapclient.IMAPClient.noop", "completion": "        return self._imap.noop()\n", "idx": 1651}
{"namespace": "imapclient.imapclient.IMAPClient.idle", "completion": "        self._idle_tag = self._imap._command(\"IDLE\")\n        return\n", "idx": 1652}
{"namespace": "imapclient.imapclient.IMAPClient.idle_check", "completion": "        # Use the newer poll() function if it's available, otherwise fall back to select.poll()\n        poll_socket = self._poll_socket if POLL_SUPPORT else self._select_poll_socket\n        # Poll the socket until we get an IDLE response or a timeout is reached\n        sock = self._imap.sock\n        if poll_socket(sock, timeout):\n            # We have an IDLE response, so read it\n            line = self._imap._get_line()\n            if not line.startswith(b\"*\"):\n                raise exceptions.IMAPClientError(\"Unexpected response while checking for IDLE: %s\" % line)\n            return parse_response(line)\n        else:\n            # We have timed out, so return an empty list\n            return []\n", "idx": 1653}
{"namespace": "imapclient.imapclient.IMAPClient.idle_done", "completion": "        logger.debug(\"< DONE\")\n        _, data = self._imap.send(imaplib.IMAP4.done, self._idle_tag)\n        self._idle_tag = None\n        return data\n", "idx": 1654}
{"namespace": "imapclient.imapclient.IMAPClient.folder_status", "completion": "        if what is None:\n            what = ['MESSAGES', 'RECENT', 'UIDNEXT', 'UIDVALIDITY', 'UNSEEN']\n        else:\n            what = [item.upper() for item in what]\n        status = {}\n        resp = self._command_and_check('status', self._normalise_folder(folder), '({})'.format(' '.join(what)), uid=True)\n        if resp is not None:\n            # Get the response code from the server\n            code = resp[0]\n            # Get the response text from the server\n            text = resp[1]\n            # Split the response text into a list\n            text_list = text.decode().split(' ')\n            # Iterate through the list and assign the status items to a dictionary\n            for i in range(0, len(text_list), 2):\n                status[text_list[i]] = text_list[i + 1]\n        return status\n", "idx": 1655}
{"namespace": "imapclient.imapclient.IMAPClient.sort", "completion": "        args = []\n        if charset:\n            args.extend([b\"CHARSET\", to_bytes(charset)])\n        args.extend(_normalise_sort_criteria(sort_criteria, charset))\n        args.extend(_normalise_search_criteria(criteria, charset))\n\n        try:\n            data = self._raw_command_untagged(b\"SORT\", args)\n        except imaplib.IMAP4.error as e:\n            # Make BAD IMAP responses easier to understand to the user, with a link to the docs\n            m = re.match(r\"SORT command error: BAD \\[(.+)\\]\", str(e))\n            if m:\n                raise exceptions.InvalidCriteriaError(\n                    \"{original_msg}\\n\\n\"\n                    \"This error may have been caused by a syntax error in the criteria: \"\n                    \"{criteria}\\nPlease refer to the documentation for more information \"\n                    \"about search criteria syntax..\\n\"\n                    \"https://imapclient.readthedocs.io/en/master/#imapclient.IMAPClient.search\".format(\n                        original_msg=m.group(1),\n                        criteria='\"%s\"' % criteria\n                        if not isinstance(criteria, list)\n                        else criteria,\n                    )\n                )\n\n            # If the exception is not from a BAD IMAP response, re-raise as-is\n            raise\n\n        return parse_message_list(data)\n", "idx": 1656}
{"namespace": "imapclient.imapclient.IMAPClient.thread", "completion": "        args = [algorithm, to_bytes(charset)]\n        args.extend(_normalise_search_criteria(criteria, charset))\n        data = self._raw_command_untagged(b\"THREAD\", args, unpack=True)\n        return parse_response(data)\n", "idx": 1657}
{"namespace": "imapclient.imapclient.IMAPClient.get_flags", "completion": "        response = self._raw_command_untagged(b\"FETCH\", messages, b\"(FLAGS)\", uid=True)\n        return self._process_fetch_response(response)\n", "idx": 1658}
{"namespace": "imapclient.imapclient.IMAPClient.get_gmail_labels", "completion": "        response = self.fetch(messages, [\"X-GM-LABELS\"])\n        return self._filter_fetch_dict(response, b\"X-GM-LABELS\")\n", "idx": 1659}
{"namespace": "imapclient.imapclient.IMAPClient.append", "completion": "        if msg_time:\n            time_val = '\"%s\"' % datetime_to_INTERNALDATE(msg_time)\n            if PY3:\n                if isinstance(msg, str):\n                    msg = msg.encode(\"utf-8\")\n            else:\n                if isinstance(msg, unicode):\n                    msg = msg.encode(\"utf-8\")\n            if isinstance(flags, (tuple, list)):\n                flags = seq_to_parentheses(flags)\n            return self._command_and_check(\n                \"append\",\n                self._normalise_folder(folder),\n                flags,\n                time_val,\n                to_bytes(msg),\n                unpack=True,\n            )\n        else:\n            return self._command_and_check(\n                \"append\",\n                self._normalise_folder(folder),\n                seq_to_parentheses(flags),\n                to_bytes(msg),\n                unpack=True,\n            )\n", "idx": 1660}
{"namespace": "imapclient.imapclient.IMAPClient.multiappend", "completion": "        if not msgs:\n            return\n\n        # Build up the IMAP command string\n        command = b'APPEND %s' % self._normalise_folder(folder)\n        for msg in msgs:\n            if isinstance(msg, dict):\n                flags = seq_to_parenstr(msg['flags'])\n                date = datetime_to_INTERNALDATE(msg['date'])\n                command += b' (%s %s)' % (flags, date)\n                command += b' {%s}' % len(msg['msg'])\n            else:\n                command += b' NIL NIL {%s}' % len(msg)\n        command += b' \\r\\n'\n\n        # Add the message data to the command string\n        for msg in msgs:\n            if isinstance(msg, dict):\n                command += msg['msg']\n            else:\n                command += msg\n\n        # Send the command to the server\n        tag = self._imap._command(command)\n        typ, data = self._imap._command_complete('APPEND', tag)\n        self._checkok('append', typ, data)\n        return data[0]\n", "idx": 1661}
{"namespace": "imapclient.imapclient.IMAPClient.expunge", "completion": "        if messages:\n            if self.use_uid:\n                messages = [\"UID %s\" % msg for msg in messages]\n            else:\n                messages = [\"%s\" % msg for msg in messages]\n            self._raw_command_untagged(\n                b\"EXPUNGE\",\n                b\" \".join(messages),\n                uid=self.use_uid,\n            )\n            return None\n        tag = self._imap._command(\"EXPUNGE\")\n        return self._consume_until_tagged_response(tag, \"EXPUNGE\")\n", "idx": 1662}
{"namespace": "imapclient.imapclient.IMAPClient.getacl", "completion": "        return self._command_and_check(\"getacl\", self._normalise_folder(folder), uid=True)\n", "idx": 1663}
{"namespace": "imapclient.imapclient.IMAPClient.setacl", "completion": "        return self._command_and_check(\"setacl\", self._normalise_folder(folder), who, what, unpack=True)\n", "idx": 1664}
{"namespace": "imapclient.imapclient.IMAPClient.get_quota_root", "completion": "        return _parse_quota_root(self._command_and_check(\"getquotaroot\", _quote(mailbox)))\n", "idx": 1665}
{"namespace": "imapclient.imapclient.IMAPClient.set_quota", "completion": "        quota_args = []\n        for quota in quotas:\n            quota_args.append(quota.quota_root)\n            quota_args.append(quota.resource)\n            quota_args.append(str(quota.limit))\n        return self._command_and_check(\"setquota\", *quota_args, unpack=True)\n", "idx": 1666}
{"namespace": "imapclient.imapclient.IMAPClient._consume_until_tagged_response", "completion": "        untagged_responses = []\n        while True:\n            line = self._imap.response(tag)\n            if line[0] is TAGGED:\n                tagged_response = line[1]\n                if tagged_response[0] == tag:\n                    break\n                else:\n                    untagged_responses.append(line)\n            else:\n                untagged_responses.append(line)\n        if tagged_response[0] != tag:\n            raise exceptions.IMAPClientError(\n                \"Unexpected tagged response while waiting for %s: %s\"\n                % (tag, to_unicode(tagged_response))\n            )\n        if tagged_response[1] != b\"OK\":\n            raise exceptions.IMAPClientError(\n                \"%s failed: %s\" % (command, to_unicode(tagged_response[2]))\n            )\n        return tagged_response, untagged_responses\n", "idx": 1667}
{"namespace": "imapclient.imapclient._normalise_search_criteria", "completion": "    if not criteria:\n        raise exceptions.InvalidCriteriaError(\"no criteria specified\")\n\n    if not charset:\n        charset = \"us-ascii\"\n\n    if isinstance(criteria, (str, bytes)):\n        return [criteria]\n\n    if isinstance(criteria, (int, datetime.date)):\n        return [imap_utf7.encode(str(criteria), charset)]\n\n    if isinstance(criteria, (list, tuple)):\n        return [_normalise_search_criteria(item, charset) for item in criteria]\n\n    raise exceptions.InvalidCriteriaError(\n        \"criteria not a string, integer, nor date\"\n    )\n\n", "idx": 1668}
{"namespace": "imapclient.response_lexer.TokenSource.current_literal", "completion": "        return self.lex.current_literal\n", "idx": 1669}
{"namespace": "imapclient.imap_utf7.decode", "completion": "    if not isinstance(s, (str, bytes)):\n        return s\n\n    if isinstance(s, bytes):\n        s = s.decode(\"ascii\")\n\n    res = []\n    # Store base64 substring that will be decoded once the shift character & is found\n    b64_buffer = []\n    for c in s:\n        # Shift character & is also a special case in modified BASE64 encoding\n        # so we have to take care of it\n        if c == \"&\":\n            # & char is a shift from ASCII to base64 encoding\n            # - char is a shift back from base64 to ASCII\n            if not b64_buffer:\n                b64_buffer.append(\"&\")\n            else:\n                b64_buffer.append(\"-\")\n                # Once the shift back to ASCII is found, we join the buffer\n                # and decode it with the imap UTF-7 codec\n                res.append(base64_utf7_decode(\"\".join(b64_buffer)).decode(\"utf-7\"))\n                # Then we clear the buffer for the next base64 substring\n                b64_buffer = []\n        else:\n            # Store the char in the buffer until the next shift character\n            b64_buffer.append(c)\n\n    # If the string finish with a base64 substring, we decode it\n    if b64_buffer:\n        if b64_buffer[0] == \"&\":\n            # If the first char is a shift, it means that the string finishes with a base64 substring\n            # that must be decoded\n            res.append(base64_utf7_decode(\"\".join(b64_buffer)).decode(\"utf-7\"))\n        else:\n            # If the first char is a -, it means that the string finishes with a modified base64\n            # substring that must be decoded\n            res.append(base64_utf7_decode(\"-\" + \"\".join(b64_buffer)).decode(\"utf-7\"))\n\n    return \"\".join(res)\n\n", "idx": 1670}
{"namespace": "imapclient.fixed_offset.FixedOffset.for_system", "completion": "        offset = time.timezone\n        if time.daylight:\n            offset += time.altzone\n        return cls(offset)", "idx": 1671}
{"namespace": "imapclient.datetime_util.parse_to_datetime", "completion": "    # Split the string into date and time parts\n    parts = timestamp.split(b\"T\")\n    if len(parts) != 2:\n        raise ValueError(\"Invalid datetime string: %r\" % timestamp)\n\n    # Parse the date part\n    date_parts = parts[0].split(b\"-\")\n    if len(date_parts) != 3:\n        raise ValueError(\"Invalid date: %r\" % parts[0])\n\n    # Parse the time part\n    time_parts = parts[1].split(b\":\")\n    if len(time_parts) != 3:\n        raise ValueError(\"Invalid time: %r\" % parts[1])\n\n    # Convert the date parts to integers\n    year = int(date_parts[0])\n    month = int(date_parts[1])\n    day = int(date_parts[2])\n\n    # Convert the time parts to integers\n    hour = int(time_parts[0])\n    minute = int(time_parts[1])\n    second = int(time_parts[2])\n\n    # Create a datetime object from the date and time parts\n    dt = datetime(year, month, day, hour, minute, second)\n\n    # Parse the timezone part\n    if normalise:\n        # If we're normalising the datetime, ignore the timezone\n        return dt\n\n    if len(timestamp) < 21:\n        # No timezone information, assume UTC\n        return dt.replace(tzinfo=FixedOffset(0))\n\n    # Parse the timezone part\n    tz_parts = timestamp[20:]\n    if tz_parts[0] != b\"+\":\n        raise ValueError(\"Invalid timezone: %r\" % tz_parts)\n\n    # Parse the timezone offset\n    tz_parts = tz_parts[1:]\n    tz_parts = tz_parts.split(b\":\")\n    if len(tz_parts) == 2:\n        # Offset from UTC specified as +/-HH:MM\n        tz_hour = int(tz_parts[0])\n        tz_minute = int(tz_parts[1", "idx": 1672}
{"namespace": "imapclient.datetime_util.datetime_to_INTERNALDATE", "completion": "    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=FixedOffset.for_system())\n\n    return dt.strftime(\"%d-%b-%Y %H:%M:%S %z\")\n\n", "idx": 1673}
{"namespace": "imapclient.datetime_util.format_criteria_date", "completion": "    if isinstance(dt, datetime):\n        dt = dt.date()\n\n    return dt.strftime(\"%d-%b-%Y\").encode(\"ascii\")", "idx": 1674}
{"namespace": "imapclient.util.assert_imap_protocol", "completion": "    if not condition:\n        raise IMAP4ProtocolError(message)\n\n", "idx": 1675}
{"namespace": "ehforwarderbot.utils.get_config_path", "completion": "    if module_id is None:\n        module_id = coordinator.profile\n    data_path = get_data_path(module_id)\n    config_path = data_path / f\"config.{ext}\"\n    if not config_path.exists():\n        config_path.touch()\n    return config_path\n\n", "idx": 1676}
{"namespace": "ehforwarderbot.utils.get_custom_modules_path", "completion": "    base_path = get_base_path()\n    channel_path = base_path / \"modules\"\n    if not channel_path.exists():\n        channel_path.mkdir(parents=True)\n    return channel_path\n\n", "idx": 1677}
{"namespace": "ehforwarderbot.chat.Chat.add_member", "completion": "        if id:\n            warnings.warn(\"`id` argument is deprecated, use `uid` instead.\", DeprecationWarning)\n        uid = uid or id\n        member = ChatMember(self, name=name, alias=alias, uid=uid,\n                            vendor_specific=vendor_specific, description=description,\n                            middleware=middleware)\n        self.members.append(member)\n        return member\n", "idx": 1678}
{"namespace": "ehforwarderbot.chat.Chat.add_system_member", "completion": "        member = self.make_system_member(name=name, alias=alias, id=id, uid=uid,\n                                         vendor_specific=vendor_specific, description=description,\n                                         middleware=middleware)\n        self.members.append(member)\n        return member\n", "idx": 1679}
{"namespace": "ehforwarderbot.chat.PrivateChat.verify", "completion": "        super().verify()\n        for member in self.members:\n            assert isinstance(member, ChatMember)\n", "idx": 1680}
{"namespace": "telethon.utils.get_inner_text", "completion": "    if not entities:\n        return [text]\n\n    result = []\n    current_entity = entities[0]\n    current_index = 0\n\n    while current_index < len(text):\n        if current_index == current_entity['offset']:\n            result.append(text[current_index:current_entity['offset'] + current_entity['length']])\n            current_index = current_entity['offset'] + current_entity['length']\n            entities.pop(0)\n            if entities:\n                current_entity = entities[0]\n        else:\n            if text[current_index] == ' ':\n                result.append(' ')\n            current_index += 1\n\n    return ''.join(result).split(' ')\n\n", "idx": 1681}
{"namespace": "telethon.extensions.html.parse", "completion": "    if not html:\n        return '', []\n\n    parser = HTMLToTelegramParser()\n    parser.feed(html)\n    text = del_surrogate(parser.text)\n    entities = parser.entities\n\n    return text, entities\n\n", "idx": 1682}
{"namespace": "telethon.extensions.html.unparse", "completion": "    if not text or not entities:\n        return text\n\n    entities = sorted(entities, key=lambda e: e.offset)\n    if entities[0].offset != 0:\n        entities.insert(0, MessageEntity(type=MessageEntityUnknown, offset=0, length=entities[0].offset))\n\n    result = []\n    offset = 0\n    for entity in entities:\n        if entity.offset > offset:\n            result.append(escape(text[offset:entity.offset]))\n        elif entity.offset < offset:\n            continue\n\n        entity_type = type(entity)\n        if entity_type in ENTITY_TO_FORMATTER:\n            formatter = ENTITY_TO_FORMATTER[entity_type]\n            if isinstance(formatter, str):\n                result.append(formatter)\n            else:\n                result.extend(formatter(entity, text))\n        elif entity_type == MessageEntityMention:\n            result.append('<a href=\"tg://user?id={}\">'.format(entity.user_id))\n        elif entity_type == MessageEntityHashtag:\n            result.append('<a href=\"https://t.me/hashtag/{0}\">#{0}</a>'.format(\n                text[entity.offset:entity.offset + entity.length]))\n        elif entity_type == MessageEntityCashtag:\n            result.append('<a href=\"https://t.me/s/{0}\">$ {0}</a>'.format(\n                text[entity.offset:entity.offset + entity.length]))\n        elif entity_type == MessageEntityBotCommand:\n            result.append('<a href=\"tg://bot_command/{0}\">'.format(\n                text[entity.offset:entity.offset + entity.length]))\n        elif entity_type == MessageEntityUrl:\n            result.append('<a href=\"{0}\">'.format(text[entity.offset:entity.offset + entity.length]))\n        elif entity_type == MessageEntityMentionName:\n            result.append('<a href=\"tg://user?id={0}\">", "idx": 1683}
{"namespace": "telethon.crypto.rsa.encrypt", "completion": "    global _server_keys\n    key = _server_keys.get(fingerprint)\n    if not key:\n        return None\n\n    key, old = key\n    if use_old:\n        key = rsa.PublicKey.load_pkcs1(old)\n\n    # Append the SHA1 hash of the data, the data itself, and padding to the data\n    data = sha1(data).digest() + data + os.urandom(256 - (len(data) + 20))\n\n    # Encrypt the data using RSA encryption\n    return rsa.encrypt(data, key)\n\n", "idx": 1684}
{"namespace": "hbmqtt.codecs.encode_string", "completion": "    data = string.encode(encoding='utf-8')\n    return int_to_bytes(len(data), 2) + data\n\n", "idx": 1685}
{"namespace": "hbmqtt.plugins.manager.PluginManager.get_plugin", "completion": "        for plugin in self._plugins:\n            if plugin.name == name:\n                return plugin\n        return None\n", "idx": 1686}
{"namespace": "pysimplesoap.simplexml.SimpleXMLElement.add_child", "completion": "        if ns is True:\n            ns = self.__ns\n        if ns is not False:\n            prefix = self.__ns_rx.match(name).group(1)\n            name = name.replace(prefix + ':', '')\n            name = '%s:%s' % (self.__namespaces_map[prefix], name)\n        if text is None:\n            element = self.__document.createElement(name)\n        else:\n            element = self.__document.createElement(name)\n            if isinstance(text, CDATA):\n                element.appendChild(self.__document.createCDATASection(text))\n            else:\n                element.appendChild(self.__document.createTextNode(text))\n        self.__elements[0].appendChild(element)\n        return SimpleXMLElement(elements=[element], document=self.__document,\n                                namespace=ns, prefix=prefix, namespaces_map=self.__namespaces_map)\n", "idx": 1687}
{"namespace": "pysimplesoap.simplexml.SimpleXMLElement.as_xml", "completion": "        xml_string = self.__document.toxml('utf-8')\n        if filename is not None:\n            with open(filename, 'w') as f:\n                f.write(xml_string)\n        if pretty:\n            return xml.dom.minidom.parseString(xml_string).toprettyxml()\n        else:\n            return xml_string\n", "idx": 1688}
{"namespace": "twilio.base.deserialize.iso8601_date", "completion": "    try:\n        return datetime.datetime.strptime(s, ISO8601_DATE_FORMAT).date()\n    except ValueError:\n        return s\n\n", "idx": 1689}
{"namespace": "twilio.base.deserialize.iso8601_datetime", "completion": "    try:\n        return datetime.datetime.strptime(s, ISO8601_DATETIME_FORMAT).replace(\n            tzinfo=datetime.timezone.utc\n        )\n    except (TypeError, ValueError):\n        return s\n\n", "idx": 1690}
{"namespace": "twilio.base.serialize.iso8601_date", "completion": "    if isinstance(d, str):\n        return d\n    elif isinstance(d, datetime.datetime):\n        return d.date().isoformat()\n    elif isinstance(d, datetime.date):\n        return d.isoformat()\n    else:\n        return None\n\n", "idx": 1691}
{"namespace": "twilio.base.serialize.iso8601_datetime", "completion": "    if d == values.unset:\n        return d\n    elif isinstance(d, datetime.datetime):\n        return str(d.strftime('%Y-%m-%dT%H:%M:%SZ'))\n    elif isinstance(d, datetime.date):\n        return str(d.strftime('%Y-%m-%dT%H:%M:%SZ'))\n    elif isinstance(d, str):\n        return d\n\n", "idx": 1692}
{"namespace": "twilio.base.serialize.prefixed_collapsible_map", "completion": "    return {\n        \"{prefix}{k}\".format(prefix=prefix, k=k): v\n        for k, v in m.items()\n    }\n\n", "idx": 1693}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.dial", "completion": "        return self.nest(Dial(\n            number=number,\n            action=action,\n            method=method,\n            timeout=timeout,\n            hangup_on_star=hangup_on_star,\n            time_limit=time_limit,\n            caller_id=caller_id,\n            record=record,\n            trim=trim,\n            recording_status_callback=recording_status_callback,\n            recording_status_callback_method=recording_status_callback_method,\n            recording_status_callback_event=recording_status_callback_event,\n            answer_on_bridge=answer_on_bridge,\n            ring_tone=ring_tone,\n            recording_track=recording_track,\n            sequential=sequential,\n            refer_url=refer_url,\n            refer_method=refer_method,\n            **kwargs\n        ))\n", "idx": 1694}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.enqueue", "completion": "        return self.nest(Enqueue(\n            name=name,\n            action=action,\n            max_queue_size=max_queue_size,\n            method=method,\n            wait_url=wait_url,\n            wait_url_method=wait_url_method,\n            workflow_sid=workflow_sid,\n            **kwargs\n        ))\n", "idx": 1695}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.gather", "completion": "        return self.nest(\n            Gather(\n                input=input,\n                action=action,\n                method=method,\n                timeout=timeout,\n                speech_timeout=speech_timeout,\n                max_speech_time=max_speech_time,\n                profanity_filter=profanity_filter,\n                finish_on_key=finish_on_key,\n                num_digits=num_digits,\n                partial_result_callback=partial_result_callback,\n                partial_result_callback_method=partial_result_callback_method,\n                language=language,\n                hints=hints,\n                barge_in=barge_in,\n                debug=debug,\n                action_on_empty_result=action_on_empty_result,\n                speech_model=speech_model,\n                enhanced=enhanced,\n                **kwargs\n            )\n        )\n", "idx": 1696}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.say", "completion": "        return self.nest(Say(message=message, voice=voice, loop=loop, language=language, **kwargs))\n", "idx": 1697}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.sms", "completion": "        return self.nest(\n            Sms(\n                message,\n                to=to,\n                from_=from_,\n                action=action,\n                method=method,\n                status_callback=status_callback,\n                **kwargs\n            )\n        )\n", "idx": 1698}
{"namespace": "twilio.twiml.voice_response.Gather.say", "completion": "        return self.nest(Say(message=message, voice=voice, loop=loop, language=language, **kwargs))\n", "idx": 1699}
{"namespace": "twilio.twiml.voice_response.Dial.client", "completion": "        return self.nest(\n            Client(\n                identity=identity,\n                url=url,\n                method=method,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                **kwargs\n            )\n        )\n", "idx": 1700}
{"namespace": "twilio.twiml.voice_response.Dial.conference", "completion": "        return self.nest(\n            Conference(\n                name,\n                muted=muted,\n                beep=beep,\n                start_conference_on_enter=start_conference_on_enter,\n                end_conference_on_exit=end_conference_on_exit,\n                wait_url=wait_url,\n                wait_method=wait_method,\n                max_participants=max_participants,\n                record=record,\n                region=region,\n                coach=coach,\n                trim=trim,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                recording_status_callback=recording_status_callback,\n                recording_status_callback_method=recording_status_callback_method,\n                recording_status_callback_event=recording_status_callback_event,\n                event_callback_url=event_callback_url,\n                jitter_buffer_size=jitter_buffer_size,\n                participant_label=participant_label,\n                **kwargs\n            )\n        )\n", "idx": 1701}
{"namespace": "twilio.twiml.voice_response.Dial.queue", "completion": "        return self.nest(\n            Queue(\n                name,\n                url=url,\n                method=method,\n                reservation_sid=reservation_sid,\n                post_work_activity_sid=post_work_activity_sid,\n                **kwargs\n            )\n        )\n", "idx": 1702}
{"namespace": "twilio.twiml.voice_response.Dial.sip", "completion": "        return self.nest(\n            Sip(\n                sip_url,\n                username=username,\n                password=password,\n                url=url,\n                method=method,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                machine_detection=machine_detection,\n                amd_status_callback_method=amd_status_callback_method,\n                amd_status_callback=amd_status_callback,\n                machine_detection_timeout=machine_detection_timeout,\n                machine_detection_speech_threshold=machine_detection_speech_threshold,\n                machine_detection_speech_end_threshold=machine_detection_speech_end_threshold,\n                machine_detection_silence_timeout=machine_detection_silence_timeout,\n                **kwargs\n            )\n        )\n", "idx": 1703}
{"namespace": "twilio.twiml.messaging_response.MessagingResponse.message", "completion": "        return self.nest(Message(\n            body=body,\n            to=to,\n            from_=from_,\n            action=action,\n            method=method,\n            status_callback=status_callback,\n            **kwargs\n        ))\n\n", "idx": 1704}
{"namespace": "twilio.twiml.TwiML.append", "completion": "        self.nest(verb)\n        return self\n", "idx": 1705}
{"namespace": "twilio.jwt.Jwt.to_jwt", "completion": "        if ttl:\n            self.ttl = ttl\n\n        if not self.secret_key:\n            raise ValueError(\"No secret key configured for JWT\")\n\n        headers = self.headers.copy()\n        payload = self.payload.copy()\n\n        return jwt_lib.encode(payload, self.secret_key, algorithm=self.algorithm, headers=headers)\n", "idx": 1706}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_client_outgoing", "completion": "        self.capabilities[\"outgoing\"] = {\n            \"application_sid\": application_sid,\n            \"params\": kwargs,\n        }\n", "idx": 1707}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_client_incoming", "completion": "        self.client_name = client_name\n        self.capabilities[\"incoming\"] = ScopeURI(\"client\", \"incoming\", {\"clientName\": client_name})\n", "idx": 1708}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_event_stream", "completion": "        scope = ScopeURI(\"stream\", \"subscribe\", kwargs)\n        self.capabilities[\"stream\"] = scope\n", "idx": 1709}
{"namespace": "twilio.jwt.client.ClientCapabilityToken._generate_payload", "completion": "        if \"outgoing\" in self.capabilities and self.client_name is not None:\n            self.capabilities[\"outgoing\"].add_param(\"clientName\", self.client_name)\n\n        return {\"scope\": \" \".join([str(scope) for scope in self.capabilities.values()])}\n\n", "idx": 1710}
{"namespace": "twilio.jwt.client.ScopeURI.to_payload", "completion": "        if self.params:\n            params = sorted(self.params.items(), key=lambda x: x[0])\n            param_string = \"?\" + urlencode(params, doseq=True)\n        else:\n            param_string = \"\"\n\n        return \"scope:{0}:{1}{2}\".format(self.service, self.privilege, param_string)", "idx": 1711}
{"namespace": "twilio.jwt.access_token.AccessToken.add_grant", "completion": "        if not isinstance(grant, AccessTokenGrant):\n            raise ValueError(\"Grant must be an instance of AccessTokenGrant.\")\n        self.grants.append(grant)\n", "idx": 1712}
{"namespace": "twilio.jwt.taskrouter.capabilities.WorkerCapabilityToken.allow_update_activities", "completion": "        self._make_policy(self.workspace_url + \"/Activities\", \"POST\", {\"ActivitySid\": {\"required\": True}})\n", "idx": 1713}
{"namespace": "zulipterminal.platform_code.successful_GUI_return_code", "completion": "    if PLATFORM == \"Linux\":\n        return 1\n    else:\n        return 0", "idx": 1714}
{"namespace": "zulipterminal.platform_code.normalized_file_path", "completion": "    if PLATFORM == \"WSL\":\n        return path.replace(\"/\", \"\\\\\")\n    return path", "idx": 1715}
{"namespace": "zulipterminal.helper.canonicalize_color", "completion": "    # Check if the color string is in the format '#xxxxxx'\n    if re.match(\"^#[A-Fa-f0-9]{6}$\", color):\n        return color[0:7].lower()\n\n    # Check if the color string is in the format '#xxx'\n    elif re.match(\"^#[A-Fa-f0-9]{3}$\", color):\n        return color[0:4].lower()\n\n    # Return the original string if the color string is not in the format '#xxx' or '#xxxxxx'\n    return color", "idx": 1716}
{"namespace": "zulipterminal.helper.get_unused_fence", "completion": "    max_length = 0\n    for match in re.findall(r\"^`+\", content, re.MULTILINE):\n        if len(match) > max_length:\n            max_length = len(match)\n    return \"`\" * (max_length + 1)\n\n", "idx": 1717}
{"namespace": "zulipterminal.helper.open_media", "completion": "    command = [tool, media_path]\n    try:\n        subprocess.run(command, check=True)\n    except subprocess.CalledProcessError as e:\n        controller.report_error(\n            [\"Error while opening media file\", f\"{e.returncode} {e.stderr}\"],\n        )", "idx": 1718}
{"namespace": "zulipterminal.server_url.encode_stream", "completion": "    return hash_util_encode(stream_name.replace(\" \", \"-\") + \":\" + str(stream_id))\n\n", "idx": 1719}
{"namespace": "zulipterminal.server_url.near_message_url", "completion": "    if message[\"type\"] == \"stream\":\n        return near_stream_message_url(server_url, message)\n    else:\n        return near_pm_message_url(server_url, message)", "idx": 1720}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.update_recipients", "completion": "        # FIXME: Refactor this function\n        # TODO: Handle stream names and mentioning users\n        # TODO: Handle \"To:\" and \"CC:\" separately\n        # TODO: Handle empty input\n\n        # Get the input text from the write_box\n        write_box_text = write_box.edit_text\n\n        # If the input text is empty, set the recipients to empty list\n        if not write_box_text:\n            self._set_regular_and_typing_recipient_user_ids(None)\n            return\n\n        # If the input text is not empty, parse the recipients from the text\n        # and set the recipients in the WriteBox instance\n        # The recipients are stored as a list of user IDs\n        # The recipient emails are stored in the WriteBox instance\n        # as well\n        write_box_text_list = write_box_text.split(\",\")\n        recipient_emails = []\n        recipient_user_ids = []\n        for recipient_email in write_box_text_list:\n            recipient_email = recipient_email.strip()\n            if recipient_email in self.model.user_id_email_dict:\n                recipient_user_ids.append(\n                    self.model.user_id_email_dict[recipient_email]\n                )\n                recipient_emails.append(recipient_email)\n        self.recipient_emails = recipient_emails\n        self._set_regular_and_typing_recipient_user_ids(recipient_user_ids)\n", "idx": 1721}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.stream_box_view", "completion": "        self._setup_common_stream_compose(stream_id, caption, title)\n        self.stream_write_box.edit_text = caption\n        self.stream_write_box.edit_pos = len(caption)\n        self.stream_write_box.set_caption(caption)\n        self.model.stream_id_write_box_id[stream_id] = id(self.stream_write_box)\n        self.model.stream_write_box_id_info[id(self.stream_write_box)] = {\n            \"stream_id\": stream_id,\n            \"captions\": [caption],\n            \"plain_stream_id\": stream_id,\n        }\n        self.stream_write_box.connect_to_new_message_signal(\n            self.model.controller.narrow_to_topic\n        )\n        # FIXME: Refactor after feature freeze\n        self.stream_write_box.connect_to_autocomplete(\n            self.view.users,\n            \"sender_stream\",\n            primary_key_for_command(\"AUTOCOMPLETE\"),\n            primary_key_for_command(\"AUTOCOMPLETE_REVERSE\"),\n        )\n        self.stream_write_box.callback_func = self.set_stream_write_box_style\n        self.stream_write_box.stream_id = stream_id\n", "idx": 1722}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.stream_box_edit_view", "completion": "        self.stream_write_box = ReadlineEdit(\n            edit_text=caption, max_char=self.model.max_stream_name_length\n        )\n        self.stream_write_box.enable_autocomplete(\n            func=self._stream_box_autocomplete,\n            key=primary_key_for_command(\"AUTOCOMPLETE\"),\n            key_reverse=primary_key_for_command(\"AUTOCOMPLETE_REVERSE\"),\n        )\n        self.stream_write_box.set_completer_delims(\"\")\n        self._setup_common_stream_compose(stream_id, caption, title)\n\n        # Use and set a callback to set the stream marker\n        self._set_stream_write_box_style(None, caption)\n        urwid.connect_signal(\n            self.stream_write_box, \"change\", self._set_stream_write_box_style\n        )\n\n        # Add edit mode button to header write box\n        self.header_write_box.contents[0][0].contents.append((EditModeButton(), self.options()))\n", "idx": 1723}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._set_stream_write_box_style", "completion": "        if new_text == \"\":\n            self.header_write_box.widget_list[0] = urwid.Text((\"default\", \"?\"))\n            self.header_write_box.widget_list[2] = urwid.Text(\"\")\n            return\n\n        # Set the color of the stream marker\n        color = self.model.stream_dict[new_text][\"color\"]\n        self.header_write_box.widget_list[0] = urwid.Text((\"stream_marker\", \" #  \") + (color, new_text))\n\n        # Set the color of the stream name\n        self.header_write_box.widget_list[2] = urwid.Text(color + new_text)\n", "idx": 1724}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._to_box_autocomplete", "completion": "        # Get the list of users from the view.\n        users = self.view.users\n\n        # Split the text by comma and get the most recent recipient for autocomplete.\n        recipients = text.rsplit(\",\", 1)\n        latest_recipient = recipients[-1]\n\n        # Find the users that match the latest text.\n        matching_users = [\n            user\n            for user in users\n            if user.full_name.lower().startswith(latest_recipient.lstrip())\n        ]\n\n        # Append the autocompleted recipients to the string containing the previous recipients.\n        if matching_users:\n            matching_recipients = [\n                recipients[-1] + user.full_name[len(latest_recipient) :]\n                for user in matching_users\n            ]\n            recipients = recipients[:-1] + matching_recipients\n\n        # Get the full names of the matching users.\n        matching_user_names = [user.full_name for user in matching_users]\n\n        # Process the typeaheads using the updated recipients, state, and user names.\n        typeaheads = self._process_typeaheads(recipients, state, matching_user_names)\n\n        return typeaheads\n", "idx": 1725}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._topic_box_autocomplete", "completion": "        topic_names = self.model.topics_in_stream(self.stream_id)\n        topic_typeaheads = match_topics(topic_names, text)\n        return self._process_typeaheads(topic_typeaheads, state)\n", "idx": 1726}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._stream_box_autocomplete", "completion": "        streams_list = self.view.pinned_streams + self.view.unpinned_streams\n        stream_typeaheads = match_stream(streams_list, text, self.view.pinned_streams)\n\n        # Process typeaheads and return the result.\n        return self._process_typeaheads(stream_typeaheads, state, stream_typeaheads)\n", "idx": 1727}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.generic_autocomplete", "completion": "        # Split text at the last space to get the function to autocomplete.\n        # For example, for \"/mention <stream> <topic>\", if \"<stream> <topic>\" is\n        # the text, then get \"<stream>\".\n        try:\n            # FIXME: See #2126 for a clean solution to this\n            prefix, _ = text.rsplit(\" \", 1)\n        except ValueError:\n            prefix = text\n\n        # Process the autocomplete only if the prefix is not empty.\n        if prefix != \"\":\n            # Check for each prefix and call the corresponding autocomplete function.\n            if prefix[0] == \"/\":\n                return self.autocomplete_slash(text, state)\n            elif prefix[0] == \"#\":\n                return self.autocomplete_topic_or_stream(text, state)\n            elif prefix[0] == \"@\":\n                return self.autocomplete_mention(text, state)\n            elif prefix[0] == \"&\":\n                return self.autocomplete_user_group(text, state)\n            elif prefix[0] == \":\":\n                return self.autocomplete_emoji(text, state)\n            elif prefix[0] == \">\":\n                return self.autocomplete_citation(text, state)\n            elif prefix[0] == \"!_\":\n                return self.autocomplete_stream(text, state)\n            elif prefix[0] == \"!\":\n                return self.autocomplete_not_command(text, state)\n            else:\n                return self.autocomplete_nonexistent_stream(text, state)\n", "idx": 1728}
{"namespace": "zulipterminal.ui_tools.boxes.PanelSearchBox.reset_search_text", "completion": "        self.set_caption(self.search_text)\n        self.set_edit_text(\"\")\n", "idx": 1729}
{"namespace": "zulipterminal.ui_tools.boxes.PanelSearchBox.valid_char", "completion": "        if self.edit_text:\n            return super().valid_char(ch)\n        else:\n            return ch.isprintable() and not unicodedata.category(ch).startswith(\"C\")\n", "idx": 1730}
{"namespace": "zulipterminal.ui_tools.utils.is_muted", "completion": "    if msg[\"type\"] == \"private\":\n        return False\n    if model.is_search_narrow():\n        return False\n    if len(model.narrow) == 2 and model.narrow[0][0] == \"stream\":\n        stream_id = model.stream_id\n        topic = msg[\"subject\"]\n        return is_topic_muted(stream_id, topic, model)\n    return is_stream_muted(msg[\"stream_id\"], model)\n\n", "idx": 1731}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.update_count", "completion": "        self.count = count\n        if text_color is None:\n            if count == 0:\n                text_color = self.original_color\n            else:\n                text_color = \"task_focused\"\n        self.count_style = text_color\n\n        count_text = f\" {count}\" if count > 0 else \"\"\n        self.button_suffix.set_text(count_text)\n", "idx": 1732}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.update_widget", "completion": "        if isinstance(self.prefix_character, tuple):\n            self.button_prefix.set_text((\"footer_contrast\", self.prefix_character[0]))\n            self.button_suffix.set_text((\"footer_contrast\", self.prefix_character[1]))\n        else:\n            self.button_prefix.set_text((\"footer_contrast\", self.prefix_character))\n            self.button_suffix.set_text(\"\")\n\n        self._label.set_text(count_text)\n\n        self.set_label(\"footer_contrast\", self._caption)\n\n        if text_color:\n            self._w.set_attr_map({None: text_color})\n", "idx": 1733}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.keypress", "completion": "        if is_command_key(\"ENTER\", key):\n            self.activate(key)\n            return None\n        return super().keypress(size, key)\n\n", "idx": 1734}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._parse_narrow_link", "completion": "        # NOTE: The order of the regex patterns is important.\n        # We put the most specific regex at the top.\n        # Otherwise, the shorter regex patterns will also match the longer ones.\n        # For example, the link '/#narrow/stream/1-test-stream/topic/hello' will match the following regex:\n        # '(?P<stream_id>\\d+)-(?P<stream_name>.*)'\n        # and will return the following dict:\n        # {'stream_id': '1', 'stream_name': 'test-stream/topic/hello'}\n        # which is not correct.\n        # The order of the regex below is therefore important.\n        regexes = {\n            r\"(?P<narrow>#narrow)\\/(?P<narrow_stream>stream)\\/(?P<stream_data>[\\d+-]*)(?P<stream_name>[\\S]+)\\/(?P<topic_narrow>topic)\\/(?P<topic_name>[\\S]*)\"\n            r\"(?P<message_id>[\\d]*)$\":\n                {\n                    \"narrow\": \"stream\",\n                    \"stream\": cls._decode_stream_data,\n                    \"topic_name\": hash_util_decode\n                },\n            r\"(?P<narrow>#narrow)\\/(?P<narrow_stream>stream)\\/(?P<stream_data>[\\d+-]*)(?P<stream_name>[\\S]+)\\/(?P<topic_narrow>topic)\\/(?P<topic_name>[\\S]*)\"\n            r\"\\/(?P<near_narrow>near)\\/(?P<message_id>[\\d]*)$\":\n                {\n                    \"narrow\": \"stream\",\n                    \"stream\": cls._decode_stream_data,\n                    \"topic_name\": hash_util_decode,\n                    \"message_id\": cls._decode_message_id\n                },\n            r\"(?P<narrow>#narrow)\\/(?P<narrow_stream>stream)\\/(?P<stream_data>[\\d+-]*)(?P<stream_name", "idx": 1735}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._validate_and_patch_stream_data", "completion": "        stream_id = parsed_link[\"stream\"][\"stream_id\"]\n        stream_name = parsed_link[\"stream\"][\"stream_name\"]\n        if stream_id is None and stream_name is None:\n            return \"The link does not contain a valid stream ID or name.\"\n        if stream_id is not None and stream_name is not None:\n            return \"The link contains both a valid stream ID and name.\"\n        if stream_id is not None:\n            if not self.model.is_user_subscribed_to_stream(stream_id):\n                return \"The stream to which the message belongs is not in your home view.\"\n            stream_name = self.model.stream_dict[stream_id][\"name\"]\n            parsed_link[\"stream\"][\"stream_name\"] = stream_name\n        elif stream_name is not None:\n            if not self.model.is_valid_stream(stream_name):\n                return \"The stream to which the message belongs is not subscribed to.\"\n            stream_id = self.model.stream_id_from_name(stream_name)\n            parsed_link[\"stream\"][\"stream_id\"] = stream_id\n        return \"\"\n", "idx": 1736}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._validate_narrow_link", "completion": "        if parsed_link[\"narrow\"] == \"stream:near\":\n            if not parsed_link[\"message_id\"]:\n                return \"The message ID is missing\"\n            elif not parsed_link[\"stream\"][\"stream_id\"] and not parsed_link[\"stream\"][\"stream_name\"]:\n                return \"The stream ID or stream name is missing\"\n            else:\n                return self._validate_and_patch_stream_data(parsed_link)\n\n        elif parsed_link[\"narrow\"] == \"stream:topic:near\":\n            if not parsed_link[\"message_id\"]:\n                return \"The message ID is missing\"\n            elif not parsed_link[\"stream\"][\"stream_id\"] and not parsed_link[\"stream\"][\"stream_name\"]:\n                return \"The stream ID or stream name is missing\"\n            elif not parsed_link[\"topic_name\"]:\n                return \"The topic name is missing\"\n            else:\n                return self._validate_and_patch_stream_data(parsed_link)\n\n        elif parsed_link[\"narrow\"] == \"stream\":\n            if not parsed_link[\"stream\"][\"stream_id\"] and not parsed_link[\"stream\"][\"stream_name\"]:\n                return \"The stream ID or stream name is missing\"\n            else:\n                return self._validate_and_patch_stream_data(parsed_link)\n\n        elif parsed_link[\"narrow\"] == \"stream:topic\":\n            if not parsed_link[\"stream\"][\"stream_id\"] and not parsed_link[\"stream\"][\"stream_name\"]:\n                return \"The stream ID or stream name is missing\"\n            elif not parsed_link[\"topic_name\"]:\n                return \"The topic name is missing\"\n            else:\n                return self._validate_and_patch_stream_data(parsed_link)\n\n        return \"\"\n", "idx": 1737}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._switch_narrow_to", "completion": "        narrow: str = parsed_link[\"narrow\"]\n        if narrow == \"stream\":\n            self.controller.narrow_to_stream(\n                stream_name=parsed_link[\"stream\"][\"stream_name\"],\n                contextual_message_id=parsed_link.get(\"message_id\"),\n            )\n        elif narrow == \"stream:near\":\n            self.controller.narrow_to_stream(\n                stream_name=parsed_link[\"stream\"][\"stream_name\"],\n                contextual_message_id=parsed_link.get(\"message_id\"),\n                contextual_earlier_than=True,\n            )\n        elif narrow == \"stream:topic\":\n            self.controller.narrow_to_topic(\n                stream_name=parsed_link[\"stream\"][\"stream_name\"],\n                topic_name=parsed_link[\"topic_name\"],\n                contextual_message_id=parsed_link.get(\"message_id\"),\n            )\n        elif narrow == \"stream:topic:near\":\n            self.controller.narrow_to_topic(\n                stream_name=parsed_link[\"stream\"][\"stream_name\"],\n                topic_name=parsed_link[\"topic_name\"],\n                contextual_message_id=parsed_link.get(\"message_id\"),\n                contextual_earlier_than=True,\n            )\n", "idx": 1738}
{"namespace": "zulipterminal.config.themes.complete_and_incomplete_themes", "completion": "    complete_themes = []\n    incomplete_themes = []\n    for theme in THEMES.keys():\n        theme_styles = THEMES[theme][\"styles\"]\n        theme_meta = THEMES[theme][\"meta\"]\n        if (\n            theme_styles.keys() == REQUIRED_STYLES.keys()\n            and theme_meta.keys() == REQUIRED_META.keys()\n        ):\n            complete_themes.append(theme)\n        else:\n            incomplete_themes.append(theme)\n    return sorted(complete_themes), sorted(incomplete_themes)\n\n", "idx": 1739}
{"namespace": "zulipterminal.config.themes.validate_colors", "completion": "    theme_styles = THEMES[theme_name].STYLES\n    invalid_colors = []\n    for style_name, style_conf in theme_styles.items():\n        if len(style_conf) == 3:\n            fg, bg, attr = style_conf\n        elif len(style_conf) == 4:\n            fg, bg, attr, meta = style_conf\n        else:\n            continue\n\n        if fg is not None and fg not in valid_16_color_codes:\n            invalid_colors.append(fg)\n        if bg is not None and bg not in valid_16_color_codes:\n            invalid_colors.append(bg)\n\n    if invalid_colors:\n        raise InvalidThemeColorCode(\n            \"Invalid color(s) in theme '{theme_name}': {color_codes}\".format(\n                theme_name=theme_name, color_codes=\", \".join(invalid_colors)\n            )\n        )\n\n", "idx": 1740}
{"namespace": "zulipterminal.config.themes.parse_themefile", "completion": "    theme_spec = []\n    for style_name, (fg, bg) in theme_styles.items():\n        fg_code = fg.value.split()[0]\n        bg_code = bg.value.split()[0]\n        fg_code = convert_color_code(fg_code, color_depth)\n        bg_code = convert_color_code(bg_code, color_depth)\n        attr = []\n        if fg.value.split()[1] == \"bold\":\n            attr.append(\"bold\")\n        if fg.value.split()[1] == \"italic\":\n            attr.append(\"italics\")\n        if fg.value.split()[1] == \"underline\":\n            attr.append(\"underline\")\n        if fg.value.split()[1] == \"strikethrough\":\n            attr.append(\"strikethrough\")\n        if fg.value.split()[1] == \"reverse\":\n            attr.append(\"reverse\")\n        if bg.value.split()[1] == \"bold\":\n            attr.append(\"bold\")\n        if bg.value.split()[1] == \"italic\":\n            attr.append(\"italics\")\n        if bg.value.split()[1] == \"underline\":\n            attr.append(\"underline\")\n        if bg.value.split()[1] == \"strikethrough\":\n            attr.append(\"strikethrough\")\n        if bg.value.split()[1] == \"reverse\":\n            attr.append(\"reverse\")\n        if style_name is None:\n            theme_spec.append((color_depth, \"\", \"\", \"\"))\n        else:\n            theme_spec.append((color_depth, style_name, \" \".join(attr), fg_code, bg_code))\n    return theme_spec\n\n", "idx": 1741}
{"namespace": "zulipterminal.config.themes.add_pygments_style", "completion": "    pygments_styles = theme_meta[\"pygments\"][\"styles\"]\n    pygments_bg = theme_meta[\"pygments\"][\"background\"]\n    pygments_overrides = theme_meta[\"pygments\"][\"overrides\"]\n\n    if pygments_styles is not None:\n        pygments_styles = pygments_styles.split(\",\")\n        for style in pygments_styles:\n            style = style.strip()\n            if style not in STANDARD_TYPES:\n                raise Exception(f\"Invalid Pygments style '{style}'\")\n\n    if pygments_bg is not None:\n        if pygments_bg not in (\"light\", \"dark\"):\n            raise Exception(f\"Invalid Pygments background '{pygments_bg}'\")\n\n    if pygments_overrides is not None:\n        try:\n            pygments_overrides = dict(\n                [\n                    override.split(\"=\")\n                    for override in pygments_overrides.split(\",\")\n                    if \"=\" in override\n                ]\n            )\n        except Exception:\n            raise Exception(f\"Invalid Pygments overrides '{pygments_overrides}'\")\n\n    if pygments_styles is not None:\n        for style in pygments_styles:\n            if style == \"default\":\n                continue\n            elif style == \"monokai\":\n                style_meta = {\n                    \"default\": \"other\",\n                    \"comment\": \"comment\",\n                    \"string\": \"string\",\n                    \"number\": \"number\",\n                    \"keyword\": \"keyword\",\n                    \"operator\": \"keyword\",\n                    \"punctuation\": \"other\",\n                    \"namespace\": \"keyword\",\n                    \"boolean\": \"number\",\n                    \"class\": \"keyword\",\n                    \"function\": \"keyword\",\n                    \"method\": \"keyword\",\n                    \"rule\": \"other\",\n                    \"exception\": \"keyword\",\n                    \"constant\": \"number\",\n                    \"parameter\": \"other\",\n                    \"attribute\": \"other\",\n                    \"statement\": \"keyword\",\n                    \"from\": \"keyword\",\n                ", "idx": 1742}
{"namespace": "zulipterminal.config.keys.is_command_key", "completion": "    if command in KEY_BINDINGS:\n        keys = KEY_BINDINGS[command]['keys']\n        if key in keys:\n            return True\n        else:\n            return False\n    else:\n        raise InvalidCommand(command)\n\n", "idx": 1743}
{"namespace": "zulipterminal.config.keys.keys_for_command", "completion": "    try:\n        return KEY_BINDINGS[command][\"keys\"]\n    except KeyError as exception:\n        raise InvalidCommand(command)\n\n", "idx": 1744}
{"namespace": "zulipterminal.config.keys.commands_for_random_tips", "completion": "    commands = []\n    for command in KEY_BINDINGS.values():\n        if not command.get(\"excluded_from_random_tips\", False):\n            commands.append(command)\n    return commands\n\n", "idx": 1745}
{"namespace": "hypertools.datageometry.DataGeometry.transform", "completion": "        # if no data is passed, return the xform_data\n        if data is None:\n            return self.xform_data\n\n        # if data is passed, transform it\n        else:\n\n            # convert to numpy array if text\n            if isinstance(data, list):\n                data = list(map(convert_text, data))\n\n            # if data is a list, format it\n            if isinstance(data, list):\n                data = format_data(data)\n\n            # if data is a numpy array, format it\n            elif isinstance(data, np.ndarray):\n                data = format_data([data])\n\n            # if data is a pandas dataframe, format it\n            elif isinstance(data, pd.DataFrame):\n                data = format_data([data])\n\n            # if data is a numpy array, format it\n            elif isinstance(data, pd.Series):\n                data = format_data([data])\n\n            # if data is a numpy array, format it\n            elif isinstance(data, pd.Panel):\n                data = format_data([data])\n\n            # if data is a numpy array, format it\n            elif isinstance(data, pd.Panel4D):\n                data = format_data([data])\n\n            # if data is a numpy array, format it\n            elif isinstance(data, pd.PanelND):\n                data = format_data([data])\n\n            # if data is a numpy array, format it\n            elif isinstance(data, pd.SparseDataFrame):\n                data = format_data([data])\n\n            # if data is a numpy array, format it\n            elif isinstance(data, pd.SparseSeries):\n                data = format_data([data])\n\n            # if data is a numpy array, format it\n            elif isinstance(data, pd.SparsePanel):\n                data = format_data([data])\n\n            # if data is a numpy array, format it\n            elif isinstance(data, pd.SparsePanel4D):\n                data = format_data([data])\n\n            # if data is a numpy array, format it\n            elif isinstance(data, pd.", "idx": 1746}
{"namespace": "hypertools.datageometry.DataGeometry.plot", "completion": "        # if no new data passed,\n        if data is None:\n            data = self.xform_data\n        else:\n            data = self.transform(data)\n\n        # if no kwargs passed,\n        if kwargs is None:\n            kwargs = self.kwargs\n        else:\n            kwargs = self.kwargs.update(kwargs)\n\n        # if no fig and ax passed,\n        if self.fig is None and self.ax is None:\n            from .plot import plot\n            fig, ax = plot(data, **kwargs)\n        else:\n            fig = self.fig\n            ax = self.ax\n\n        # if no line_ani passed,\n        if self.line_ani is None:\n            from .plot import plot\n            line_ani = plot(data, fig=fig, ax=ax, **kwargs)\n        else:\n            line_ani = self.line_ani\n\n        # return a new DataGeometry object\n        return DataGeometry(fig=fig, ax=ax, line_ani=line_ani, data=data,\n                            xform_data=data, reduce=self.reduce,\n                            align=self.align, normalize=self.normalize,\n                            semantic=self.semantic, vectorizer=self.vectorizer,\n                            corpus=self.corpus, kwargs=kwargs,\n                            version=self.version, dtype=self.dtype)\n", "idx": 1747}
{"namespace": "awesome_autodl.autodl_topic2papers", "completion": "    topic2path = autodl_topic2path()\n    topic2papers = OrderedDict()\n    for topic, path in topic2path.items():\n        topic2papers[topic] = load_yaml(path)\n    return topic2papers\n\n", "idx": 1748}
{"namespace": "awesome_autodl.get_bib_abbrv_obj", "completion": "    from awesome_autodl.bib_utils import BibAbbreviations\n    bib_abbrv_obj = BibAbbreviations(get_bib_abbrv_file())\n    return bib_abbrv_obj\n\n", "idx": 1749}
{"namespace": "mssqlcli.localized_strings.translation", "completion": "    global LANGUAGES\n    if not languages:\n        if not LANGUAGES:\n            LANGUAGES = ['en']\n        languages = LANGUAGES\n    return gettext.translation(domain, localedir, languages=languages, fallback=True)\n\n", "idx": 1750}
{"namespace": "mssqlcli.mssqlbuffer._is_query_executable", "completion": "    sql = sql.strip()\n    if sql[-2:].upper() == 'GO':\n        return True\n    if sql.count('\\'') % 2 == 0:\n        return False\n    return False\n\n", "idx": 1751}
{"namespace": "mssqlcli.telemetry.conclude", "completion": "    _session.end_time = datetime.now()\n    payload = _session.generate_payload()\n    if payload:\n        _output_payload(payload)\n        if separate_process:\n            return _upload_payload_separate_process(payload, service_endpoint_uri)\n        else:\n            return _upload_payload(payload, service_endpoint_uri)\n\n", "idx": 1752}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.start", "completion": "        self.request_thread = threading.Thread(target=self.request_thread_handler, name=self.REQUEST_THREAD_NAME)\n        self.request_thread.daemon = True\n        self.request_thread.start()\n\n        self.response_thread = threading.Thread(target=self.response_thread_handler, name=self.RESPONSE_THREAD_NAME)\n        self.response_thread.daemon = True\n        self.response_thread.start()\n", "idx": 1753}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.submit_request", "completion": "        if method is None or params is None:\n            raise ValueError(u'Method and params must be specified.')\n\n        request = {\n            u'method': method,\n            u'params': params,\n            u'jsonrpc': u'2.0',\n            u'id': request_id\n        }\n\n        self.request_queue.put(request)\n", "idx": 1754}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.get_response", "completion": "        if request_id in self.response_map:\n            if not self.response_map[request_id].empty():\n                return self.response_map[request_id].get()\n\n        if owner_uri in self.response_map:\n            if not self.response_map[owner_uri].empty():\n                return self.response_map[owner_uri].get()\n\n        if not self.exception_queue.empty():\n            raise self.exception_queue.get()\n\n        return None\n", "idx": 1755}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.shutdown", "completion": "        self.cancel = True\n        self.request_queue.put(None)\n        self.request_thread.join()\n        self.writer.close()\n        logger.debug(u'Json Rpc client shutdown.')\n\n", "idx": 1756}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcWriter.send_request", "completion": "        content = {\n            u'jsonrpc': u'2.0',\n            u'method': method,\n            u'params': params,\n            u'id': request_id\n        }\n\n        content_body = json.dumps(content, sort_keys=True, check_circular=False)\n        content_header = self.HEADER.format(len(content_body))\n        content_header = content_header.encode(self.encoding)\n        content_body = content_body.encode(self.encoding)\n\n        try:\n            self.stream.write(content_header)\n            self.stream.write(content_body)\n            self.stream.flush()\n        except ValueError:\n            # Stream was closed.\n            raise\n", "idx": 1757}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.read_response", "completion": "        while self.needs_more_data:\n            try:\n                self._read_from_stream()\n            except ValueError as ex:\n                logger.debug(u'Read Response encountered exception %s', ex)\n                raise\n\n        # Reset the read state machine.\n        self.read_state = ReadState.Header\n        self.needs_more_data = True\n\n        # Trim the buffer.\n        self._trim_buffer()\n\n        # Parse the content as JSON.\n        content = self.buffer[self.read_offset:self.buffer_end_offset].decode(self.encoding)\n        return json.loads(content)\n", "idx": 1758}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.read_next_chunk", "completion": "        # Check if the buffer needs to be resized.\n        if self.buffer_end_offset > (len(self.buffer) * self.BUFFER_RESIZE_TRIGGER):\n            # Resize the buffer.\n            new_buffer = bytearray(len(self.buffer) * 2)\n            # Copy the data from the old buffer to the new buffer.\n            new_buffer[:self.buffer_end_offset] = self.buffer[:self.buffer_end_offset]\n            # Set the buffer to the new buffer.\n            self.buffer = new_buffer\n\n        # Read data from the stream into the buffer.\n        bytes_read = self.stream.readinto(self.buffer[self.buffer_end_offset:])\n        # If the stream is closed or empty, raise an exception.\n        if bytes_read == 0:\n            raise ValueError(u'Stream is closed or empty.')\n\n        # Update the buffer offset.\n        self.buffer_end_offset += bytes_read\n        return True\n", "idx": 1759}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.try_read_headers", "completion": "        # Find the end of the header.\n        header_end_offset = self.buffer.find(\n            b'\\r\\n\\r\\n', self.read_offset)\n        if header_end_offset == -1:\n            # Header end was not found.\n            return False\n\n        # Extract the header.\n        header_bytes = self.buffer[self.read_offset:header_end_offset]\n        header_str = header_bytes.decode(self.encoding)\n        header_lines = header_str.split(u'\\r\\n')\n\n        # Extract the headers.\n        for header_line in header_lines:\n            # Ignore the first line.\n            if header_line:\n                key_value = header_line.split(u':', 1)\n                if len(key_value) == 2:\n                    self.headers[key_value[0]] = key_value[1].strip()\n\n        # Check if the content length header is present.\n        if u'Content-Length' not in self.headers:\n            raise EOFError(u'Content-Length header not found.')\n\n        # Extract the content length.\n        self.expected_content_length = int(self.headers[u'Content-Length'])\n\n        # Update the read offset.\n        self.read_offset = header_end_offset + len(u'\\r\\n\\r\\n')\n\n        # Update the read state.\n        self.read_state = ReadState.Content\n\n        return True\n", "idx": 1760}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.close", "completion": "        try:\n            self.stream.close()\n        except AttributeError:\n            raise", "idx": 1761}
{"namespace": "mssqlcli.packages.prioritization.PrevalenceCounter.update", "completion": "        for keyword in keywords:\n            self.keyword_counts[keyword] += len(keyword_regexs[keyword].findall(text))\n\n        for t in sqlparse.parse(text)[0].flatten():\n            if t.ttype is Name:\n                self.name_counts[t.value] += 1\n\n", "idx": 1762}
{"namespace": "mssqlcli.packages.sqlcompletion.suggest_type", "completion": "    # If the previous token is \\i, then the input is likely a path to a SQL\n    # file.\n    if text_before_cursor.strip().endswith('\\\\i '):\n        return Path(), []\n\n    # Completion for \\copy can be a path or a table name\n    if full_text.startswith('\\copy ') and \\\n            previous_token_starts_with(text_before_cursor, ' ') and \\\n            not text_before_cursor.strip().endswith('\\\\i '):\n        return (Path() if text_before_cursor.split()[-1].endswith(os.path.sep)\n                else FromClauseItem()), []\n\n    statement = SqlStatement(full_text, text_before_cursor)\n\n    # Handle special commands\n    if statement.parsed:\n        # \\d\n        if statement.full_text.startswith('\\d'):\n            return (Table(None, statement.get_tables('before')), [])\n\n        # \\s\n        if statement.full_text.startswith('\\s'):\n            return (View(None, statement.get_tables('before')), [])\n\n        # \\?\n        if statement.full_text.startswith('\\?'):\n            return (NamedQuery(None), [])\n\n        # \\dn\n        if statement.full_text.startswith('\\dn '):\n            return (Schema(), [])\n\n        # \\df\n        if statement.full_text.startswith('\\df '):\n            # Return all functions in the search path\n            return (Function(None, statement.get_tables('before'), usage='special'), [])\n\n        # \\dT\n        if statement.full_text.startswith('\\dT'):\n            # Return all data types\n            return (Datatype(), [])\n\n        # \\x\n        if statement.full_text.startswith('\\x'):\n            return (Keyword(statement.last_token), [])\n\n        if statement.full_text.startswith('SELECT'):\n            return (Column(None, None, statement.get_tables('before'),\n                           qualifiable=True, context='select'),\n                   ", "idx": 1763}
{"namespace": "mssqlcli.packages.parseutils.ctes.extract_ctes", "completion": "    parsed = parse(sql)\n    tokens = parsed[0].flatten()\n\n    if not tokens or tokens[0].ttype != Keyword.CTE:\n        return [], sql\n\n    idx = 1\n    while tokens[idx].ttype is CTE:\n        idx += 1\n\n    if tokens[idx].ttype is not Keyword.DML:\n        return [], sql\n\n    # Strip the CTEs from the query\n    idx += 1\n    remaining_sql = \"\".join(str(token) for token in tokens[idx:])\n\n    # Extract the CTEs\n    ctes = []\n    while tokens[idx].ttype is CTE:\n        # Strip the CTE keyword\n        idx += 1\n\n        # Strip the alias\n        alias = tokens[idx]\n        idx += 1\n\n        # Strip the column list\n        while tokens[idx].ttype is not Keyword.DML:\n            idx += 1\n\n        # Strip the DML keyword\n        idx += 1\n\n        # Strip the CTE body\n        paren_depth = 0\n        start = idx\n        for idx in range(idx, len(tokens)):\n            if tokens[idx].value == \"(\":\n                paren_depth += 1\n            elif tokens[idx].value == \")\":\n                paren_depth -= 1\n                if paren_depth == 0:\n                    break\n            elif tokens[idx].value == \",\" and paren_depth == 1:\n                # Ignore commas within parentheses (e.g., \"1, 2 UNION ALL 3, 4\")\n                pass\n            elif tokens[idx].ttype is Keyword:\n                # Keywords not within parentheses may also terminate the CTE\n                break\n\n        columns = tokens[start:idx]\n        if columns[-1].value == \",\":\n            columns = columns[:-1]\n        columns = [col.value for col in columns]\n\n        ctes.append(TableExpression(alias.value, columns, alias.value_start_idx, tokens[idx].value_end_idx", "idx": 1764}
{"namespace": "mssqlcli.packages.parseutils.tables.extract_tables", "completion": "    parsed = sqlparse.parse(sql)\n    if not parsed:\n        return ()\n    # INSERT statements must stop looking for tables at the sign of first\n    # Punctuation. eg: INSERT INTO abc (col1, col2) VALUES (1, 2)\n    # abc is the table name, but if we don't stop at the first lparen, then\n    # we'll identify abc, col1 and col2 as table names.\n    insert_stmt = parsed[0].token_first().value.lower() == 'insert'\n    stream = extract_from_part(parsed[0], stop_at_punctuation=insert_stmt)\n    # skip first item if it is insert statement\n    if insert_stmt:\n        stream = stream[1:]\n    return tuple(extract_table_identifiers(stream, allow_functions=False))\n\n", "idx": 1765}
{"namespace": "googleapiclient.channel.Channel.body", "completion": "        body = {\n            \"id\": self.id,\n            \"token\": self.token,\n            \"type\": self.type,\n            \"address\": self.address,\n        }\n        if self.params is not None:\n            body[\"params\"] = self.params\n        if self.resource_id is not None:\n            body[\"resourceId\"] = self.resource_id\n        if self.resource_uri is not None:\n            body[\"resourceUri\"] = self.resource_uri\n        if self.expiration is not None:\n            body[\"expiration\"] = self.expiration\n        return body\n", "idx": 1766}
{"namespace": "googleapiclient.channel.Channel.update", "completion": "        for key, value in CHANNEL_PARAMS.items():\n            if key in resp:\n                setattr(self, value, resp[key])\n\n", "idx": 1767}
{"namespace": "googleapiclient.channel.notification_from_headers", "completion": "    # Convert the headers to all upper case.\n    headers = _upper_header_keys(headers)\n\n    # Validate the notification.\n    if X_GOOG_MESSAGE_NUMBER not in headers:\n        raise InvalidNotificationError(\"Missing Message Number.\")\n    if X_GOOG_RESOURCE_STATE not in headers:\n        raise InvalidNotificationError(\"Missing Resource State.\")\n    if X_GOOG_RESOURCE_URI not in headers:\n        raise InvalidNotificationError(\"Missing Resource URI.\")\n    if X_GOOG_RESOURCE_ID not in headers:\n        raise InvalidNotificationError(\"Missing Resource ID.\")\n    if X_GOOG_CHANNEL_ID not in headers:\n        raise InvalidNotificationError(\"Missing Channel ID.\")\n\n    # Validate the channel id.\n    if channel.id != headers[X_GOOG_CHANNEL_ID]:\n        raise InvalidNotificationError(\"Channel ID does not match.\")\n\n    # Parse the notification.\n    message_number = int(headers[X_GOOG_MESSAGE_NUMBER])\n    state = headers[X_GOOG_RESOURCE_STATE]\n    resource_uri = headers[X_GOOG_RESOURCE_URI]\n    resource_id = headers[X_GOOG_RESOURCE_ID]\n\n    return Notification(message_number, state, resource_uri, resource_id)\n\n", "idx": 1768}
{"namespace": "googleapiclient.channel.new_webhook_channel", "completion": "    if expiration is not None:\n        expiration = int((expiration - EPOCH).total_seconds() * 1000)\n    return Channel(\"web_hook\", str(uuid.uuid4()), token, url, expiration, params)\n\n", "idx": 1769}
{"namespace": "googleapiclient.model.BaseModel._build_query", "completion": "        alt_param = self.alt_param\n        if alt_param is not None:\n            params[\"alt\"] = alt_param\n        query_params = []\n        for param, value in params.items():\n            if value is None:\n                continue\n            if isinstance(value, list):\n                for element in value:\n                    query_params.append((param, str(element)))\n            else:\n                query_params.append((param, str(value)))\n        return urllib.parse.urlencode(query_params)\n", "idx": 1770}
{"namespace": "googleapiclient.model.BaseModel.response", "completion": "        self._log_response(resp, content)\n        if resp.status < 300:\n            if resp.status == 204:\n                # A 204 response is a special case because the response body is empty.\n                return self.no_content_response\n            return self.deserialize(content)\n        else:\n            LOGGER.error(\"Http error %s\", resp)\n            raise HttpError(resp, content, uri=self.uri)\n", "idx": 1771}
{"namespace": "googleapiclient.model.makepatch", "completion": "    patch = {}\n    for key in original:\n        if key in modified:\n            if original[key] != modified[key]:\n                patch[key] = modified[key]\n        else:\n            patch[key] = None\n    for key in modified:\n        if key not in original:\n            patch[key] = modified[key]\n    return patch", "idx": 1772}
{"namespace": "googleapiclient._helpers.update_query_params", "completion": "    parsed_uri = urllib.parse.urlparse(uri)\n    query_params = parse_unique_urlencoded(parsed_uri.query)\n    query_params.update(params)\n    new_query_params = urllib.parse.urlencode(query_params)\n    new_uri = urllib.parse.urlunparse(\n        urllib.parse.ParseResult(\n            parsed_uri.scheme,\n            parsed_uri.netloc,\n            parsed_uri.path,\n            parsed_uri.params,\n            new_query_params,\n            parsed_uri.fragment,\n        )\n    )\n    return new_uri", "idx": 1773}
{"namespace": "googleapiclient._helpers._add_query_parameter", "completion": "    if value is None:\n        return url\n\n    scheme, netloc, path, query_params, query, fragment = urllib.parse.urlparse(url)\n    query_params = urllib.parse.parse_qs(query_params)\n    query_params[name] = [value]\n    query = urllib.parse.urlencode(query_params, doseq=True)\n\n    return urllib.parse.urlunparse((scheme, netloc, path, query_params, query, fragment))\n\n", "idx": 1774}
{"namespace": "gif_for_cli.display.display_txt_frames", "completion": "    try:\n        for i in range(num_loops):\n            for txt_frame in txt_frames:\n                stdout.write('\\r')\n                stdout.write(txt_frame)\n                stdout.flush()\n                time.sleep(seconds_per_frame)\n    except KeyboardInterrupt:\n        stdout.write('\\r')\n        stdout.write(STORED_CELL_CHAR)\n        stdout.flush()\n        raise\n\n", "idx": 1775}
{"namespace": "pyVmomi.SoapAdapter.SoapResponseDeserializer.Deserialize", "completion": "        self.result = None\n        self.fault = None\n        self.deser.resultType = resultType\n        self.parser = ParserCreate(namespace_separator=NS_SEP)\n        self.origHandlers = GetHandlers(self.parser)\n        SetHandlers(self.parser, GetHandlers(self))\n        if not nsMap:\n            nsMap = {}\n        self.nsMap = nsMap\n        try:\n            self.parser.ParseFile(response)\n        except ExpatError as e:\n            raise\n        except socket.timeout as e:\n            raise\n        except Exception as e:\n            raise\n        if self.fault:\n            raise self.fault\n        return self.result\n", "idx": 1776}
{"namespace": "pyVmomi.VmomiSupport.GetRequestContext", "completion": "    try:\n        return _threadLocalContext.requestContext\n    except AttributeError:\n        _threadLocalContext.requestContext = StringDict()\n        return _threadLocalContext.requestContext\n\n", "idx": 1777}
{"namespace": "pycoin.bloomfilter.filter_size_required", "completion": "    filter_size = (-1 / pow(LOG_2, 2) * element_count * math.log(false_positive_probability)) / 8\n    if filter_size > 36000:\n        filter_size = 36000\n    return int(filter_size)\n\n", "idx": 1778}
{"namespace": "pycoin.bloomfilter.BloomFilter.add_spendable", "completion": "        self.add_item(spendable.tx_hash + struct.pack(\">I\", spendable.tx_out_index))\n", "idx": 1779}
{"namespace": "pycoin.bloomfilter.murmur3", "completion": "    c1 = 0xcc9e2d51\n    c2 = 0x1b873593\n\n    length = len(data)\n    h1 = seed & 0xffffffff\n    roundedEnd = (length & 0xfffffffc)\n    for i in range(0, roundedEnd, 4):\n        k1 = (data[i] & 0xff) | ((data[i + 1] & 0xff) << 8) | \\\n             ((data[i + 2] & 0xff) << 16) | ((data[i + 3] & 0xff) << 24)\n        k1 *= c1\n        k1 = (k1 << 15) | ((k1 & 0xffffffff) >> 17)\n        k1 *= c2\n\n        h1 ^= k1\n        h1 = (h1 << 13) | ((h1 & 0xffffffff) >> 19)\n        h1 = h1 * 5 + 0xe6546b64\n\n    if length & 0x03 > 0:\n        k1 = 0\n        if length & 0x02 > 0:\n            k1 = (data[roundedEnd + 2] & 0xff) << 16\n        if length & 0x01 > 0:\n            k1 |= (data[roundedEnd + 1] & 0xff) << 8\n        k1 *= c1\n        k1 = (k1 << 15) | ((k1 & 0xffffffff) >> 17)\n        k1 *= c2\n        h1 ^= k1\n\n    h1 ^= length\n    h1 ^= (h1 >> 16)\n    h1 *= 0x85ebca6b\n    h1 ^= (h1 >> 13)\n    h1 *= 0xc2b2ae35\n    h1 ^= (h1 >> 16)", "idx": 1780}
{"namespace": "pycoin.networks.registry.network_for_netcode", "completion": "    for prefix in search_prefixes():\n        try:\n            module = importlib.import_module(prefix + \".\" + symbol)\n            if getattr(module, \"symbol\", None) == symbol:\n                return module\n        except ImportError:\n            pass\n    raise ValueError(\"no network with symbol %s found\" % symbol)\n\n", "idx": 1781}
{"namespace": "pycoin.satoshi.IntStreamer.IntStreamer.int_from_script_bytes", "completion": "        if len(s) == 0:\n            return 0\n\n        v = s[-1] & 0x7f\n        if require_minimal and v == 0 and (s[-1] & 0x80) != 0:\n            raise ScriptError(\"non-minimally encoded\")\n\n        for c in s[-2::-1]:\n            v <<= 8\n            v |= c\n\n        if v & 0x80000000:\n            v = -((v & 0x7fffffff) + 1)\n\n        return v\n", "idx": 1782}
{"namespace": "pycoin.satoshi.stackops.do_OP_RIPEMD160", "completion": "    v = stack.pop()\n    stack.append(hashlib.new('ripemd160', v).digest())\n\n", "idx": 1783}
{"namespace": "pycoin.satoshi.stackops.do_OP_HASH160", "completion": "    stack.append(hash160(stack.pop()))\n\n", "idx": 1784}
{"namespace": "pycoin.satoshi.stackops.do_OP_HASH256", "completion": "    stack.append(hashlib.sha256(stack.pop()).digest())\n\n", "idx": 1785}
{"namespace": "pycoin.services.providers.providers_for_config_string", "completion": "    providers = []\n    for descriptor in config_string.split():\n        provider = provider_for_descriptor_and_netcode(descriptor, netcode)\n        if provider:\n            providers.append(provider)\n        else:\n            warnings.warn(\"could not parse descriptor %s\" % descriptor)\n    return providers\n\n", "idx": 1786}
{"namespace": "pycoin.services.providers.get_default_providers_for_netcode", "completion": "    if netcode is None:\n        netcode = get_current_netcode()\n    if not hasattr(THREAD_LOCALS, \"default_providers\"):\n        THREAD_LOCALS.default_providers = {}\n    if netcode not in THREAD_LOCALS.default_providers:\n        THREAD_LOCALS.default_providers[netcode] = providers_for_netcode_from_env(netcode)\n    return THREAD_LOCALS.default_providers[netcode]", "idx": 1787}
{"namespace": "pycoin.services.providers.set_default_providers_for_netcode", "completion": "    if not hasattr(THREAD_LOCALS, \"providers\"):\n        THREAD_LOCALS.providers = {}\n    THREAD_LOCALS.providers[netcode] = provider_list\n\n", "idx": 1788}
{"namespace": "pycoin.blockchain.BlockChain.BlockChain.tuple_for_index", "completion": "        if index < 0:\n            index = len(self) + index\n        if index < len(self._locked_chain):\n            return self._locked_chain[index]\n        if index < len(self._locked_chain) + len(self._longest_local_block_chain()):\n            return self._longest_local_block_chain()[index - len(self._locked_chain)]\n        return self._longest_chain_cache[index - len(self._locked_chain) - len(self._longest_local_block_chain())]\n", "idx": 1789}
{"namespace": "pycoin.blockchain.ChainFinder.ChainFinder.find_ancestral_path", "completion": "        # if the two hashes are the same, return an empty list\n        if h1 == h2:\n            return [], []\n\n        # if the first hash is a parent of the second hash, return the path from the first hash to the second hash\n        if h2 in self.parent_lookup.get(h1, []):\n            return self.maximum_path(h1, path_cache), []\n\n        # if the second hash is a parent of the first hash, return the path from the second hash to the first hash\n        if h1 in self.parent_lookup.get(h2, []):\n            return [], self.maximum_path(h2, path_cache)\n\n        # if the first hash is not a parent of the second hash, find the common ancestor\n        common_ancestor = None\n        for ancestor in self.parent_lookup.get(h1, []):\n            if ancestor in self.parent_lookup.get(h2, []):\n                common_ancestor = ancestor\n                break\n\n        # if there is no common ancestor, return None\n        if common_ancestor is None:\n            return None\n\n        # if there is a common ancestor, return the path from the first hash to the common ancestor and the path from the second hash to the common ancestor\n        return self.maximum_path(h1, path_cache), self.maximum_path(h2, path_cache)\n", "idx": 1790}
{"namespace": "pycoin.contrib.bech32m.bech32_encode", "completion": "    # Check if the HRP is valid.\n    if not hrp:\n        raise ValueError(\"Invalid HRP\")\n\n    # Check if the data is valid.\n    if not data:\n        raise ValueError(\"Invalid data\")\n\n    # Check if the spec is valid.\n    if spec not in [Encoding.BECH32, Encoding.BECH32M]:\n        raise ValueError(\"Invalid spec\")\n\n    # Check if the data is valid.\n    if any(x < 0 or x > 31 for x in data):\n        raise ValueError(\"Invalid data\")\n\n    # Check if the data is not too long.\n    if len(data) > 93:\n        raise ValueError(\"Invalid data (too long)\")\n\n    # Create the checksum.\n    checksum = bech32_create_checksum(hrp, data, spec)\n\n    # Create the Bech32 string.\n    bech32_str = hrp + \"\".join([CHARSET[d] for d in data + checksum])\n\n    return bech32_str\n\n", "idx": 1791}
{"namespace": "pycoin.contrib.bech32m.decode", "completion": "    # Decode the address.\n    hrp_expanded = bech32_hrp_expand(hrp)\n    decoded = convertbits(bech32_decode(addr)[1], 5, 8, False)\n\n    # Check if the decoding was successful.\n    if decoded is None:\n        return (None, None)\n\n    # Check if the decoded data is 20 bytes long.\n    if len(decoded) != 40:\n        return (None, None)\n\n    # Check if the decoded data starts with the correct flag.\n    if decoded[0] != 0x00:\n        return (None, None)\n\n    # Check if the checksum is correct.\n    checksum = decoded[-6:]\n    decoded_check = decoded[:-6]\n    if bech32_verify_checksum(hrp, decoded_check) is None:\n        return (None, None)\n\n    # Return the version byte and the decoded data.\n    return (decoded[1], bytes(decoded_check))\n\n", "idx": 1792}
{"namespace": "pycoin.crack.bip32.crack_bip32", "completion": "    for child in path.split('/'):\n        if child[-1] == 'h':\n            child = int(child[:-1]) + 0x80000000\n        else:\n            child = int(child)\n        secret_exponent = ascend_bip32(bip32_pub_node, secret_exponent, child)\n    return bip32_pub_node.subkey_by_secret_exponent(secret_exponent)", "idx": 1793}
{"namespace": "pycoin.message.PeerAddress.ip_bin_to_ip4_addr", "completion": "    return \".\".join(\"%d\" % v for v in struct.unpack(\">BBBB\", ip_bin[-4:]))\n\n", "idx": 1794}
{"namespace": "pycoin.message.PeerAddress.PeerAddress.host", "completion": "        if self.ip_bin.startswith(IP4_HEADER):\n            return ip_bin_to_ip4_addr(self.ip_bin[-4:])\n        return ip_bin_to_ip6_addr(self.ip_bin)\n", "idx": 1795}
{"namespace": "oletools.msodde.field_is_blacklisted", "completion": "    # split into words\n    words = FIELD_WORD_REGEX.findall(contents)\n    # check if first word is in blacklist\n    if words[0].lower() not in FIELD_BLACKLIST_CMDS:\n        return False\n    # check number of arguments\n    if len(words) != FIELD_BLACKLIST[FIELD_BLACKLIST_CMDS.index(words[0].lower())][1]:\n        return False\n    # check number of switches\n    if len(words) != FIELD_BLACKLIST[FIELD_BLACKLIST_CMDS.index(words[0].lower())][2]:\n        return False\n    # check switches\n    for word in words:\n        if FIELD_SWITCH_REGEX.match(word):\n            return False\n    return True\n\n", "idx": 1796}
{"namespace": "oletools.ppt_record_parser.is_ppt", "completion": "    try:\n        with open(filename, 'rb') as f:\n            data = f.read()\n    except:\n        data = filename\n\n    try:\n        f = io.BytesIO(data)\n        ole = olefile.OleFileIO(f)\n        if ole.exists('PowerPoint Document'):\n            stream = ole.openstream('PowerPoint Document')\n            if stream.read(8) == b'\\xE0\\x3F\\x00\\x00\\xF3\\x00\\x00\\xF4':\n                return True\n        return False\n    except:\n        return False\n\n", "idx": 1797}
{"namespace": "oletools.rtfobj.is_rtf", "completion": "    if treat_str_as_data:\n        data = arg\n    else:\n        with open(arg, 'rb') as f:\n            data = f.read()\n\n    if data[:4] == b'\\x7b\\x5c\\x72\\x74':\n        return True\n    else:\n        return False\n\n", "idx": 1798}
{"namespace": "oletools.oleobj.get_sane_embedded_filenames", "completion": "    # generate sane filenames\n    sane_filenames = []\n    # add filename with suffix\n    sane_filenames.append(sanitize_filename(filename, max_len=max_len))\n    # add filename without suffix\n    sane_filenames.append(sanitize_filename(os.path.splitext(filename)[0], max_len=max_len))\n    # add random filename with suffix\n    sane_filenames.append(sanitize_filename(filename, replacement=str(random.randint(100000, 999999)), max_len=max_len))\n    # add random filename without suffix\n    sane_filenames.append(sanitize_filename(os.path.splitext(filename)[0], replacement=str(random.randint(100000, 999999)), max_len=max_len))\n    # add filename with index\n    sane_filenames.append(sanitize_filename(filename, replacement=str(noname_index), max_len=max_len))\n    # return sane filenames\n    return sane_filenames\n\n", "idx": 1799}
{"namespace": "oletools.ooxml.get_type", "completion": "    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO: check if this is still needed\n    # TODO", "idx": 1800}
{"namespace": "oletools.ooxml.ZipSubFile.read", "completion": "        if self.handle is None:\n            raise IOError('read on closed file')\n        if self.pos >= self.size:\n            return b''\n        if size == -1:\n            size = self.size - self.pos\n        data = self.handle.read(size)\n        self.pos += len(data)\n        return data\n", "idx": 1801}
{"namespace": "oletools.ooxml.ZipSubFile.seek", "completion": "        if self.handle is None:\n            raise IOError('seek on closed handle')\n        if offset == io.SEEK_SET:\n            new_pos = pos\n        elif offset == io.SEEK_CUR:\n            new_pos = self.pos + pos\n        elif offset == io.SEEK_END:\n            new_pos = self.size + pos\n        else:\n            raise ValueError('invalid offset {0}'.format(offset))\n        if new_pos < 0:\n            raise ValueError('new position {0} is negative'.format(new_pos))\n        if new_pos > self.size:\n            # print('ZipSubFile: seek fake to end')\n            self.pos = self.size\n            self.handle.read()\n        else:\n            # print('ZipSubFile: seek to {0}'.format(new_pos))\n            self.handle.seek(new_pos)\n            self.pos = new_pos\n", "idx": 1802}
{"namespace": "oletools.ooxml.XmlParser.iter_xml", "completion": "        # Iterate through the subfiles\n        for subfile, handle in self.iter_files(subfiles):\n            # Parse the XML\n            try:\n                # Parse the XML\n                tree = ET.parse(handle)\n                root = tree.getroot()\n                # Iterate through the elements in the XML\n                for elem in self.iter_xml_elements(root, need_children, tags):\n                    # Yield the subfile name, element, and depth\n                    yield subfile, elem, self.get_xml_depth(elem)\n            except ET.ParseError:\n                # If the XML is not well-formed, add the subfile to the list of subfiles without XML\n                self.subfiles_no_xml.add(subfile)\n", "idx": 1803}
{"namespace": "oletools.oleid.OleID.check", "completion": "        # === FILE TYPE ===\n        # check if file is OLE\n        if self.ole is None:\n            self.ole = olefile.OleFileIO(self.data_bytesio)\n        # check if file is OLE2\n        if self.ole.olestruct.SectorSize > 0x1000:\n            self.add_indicator(\"ole_sector_size\", self.ole.olestruct.SectorSize,\n                               _type=int, name=\"OLE Sector Size\",\n                               description=\"The size of the OLE sectors in bytes. This is usually 512, but can be larger.\",\n                               risk=RISK.INFO)\n        # check if file is OLE2\n        if self.ole.olestruct.MinorVersion == 0x003E:\n            self.add_indicator(\"ole_version\", self.ole.olestruct.MinorVersion,\n                               _type=int, name=\"OLE Version\",\n                               description=\"The version of the OLE format. This is usually 62 (0x3E hex).\",\n                               risk=RISK.INFO)\n        # check if file is OLE2\n        if self.ole.olestruct.ByteOrder == 0xFFFE:\n            self.add_indicator(\"ole_byte_order\", self.ole.olestruct.ByteOrder,\n                               _type=int, name=\"OLE Byte Order\",\n                               description=\"The byte order of the OLE file. This is usually 65234 (0xFFFE hex).\",\n                               risk=RISK.INFO)\n        # check if file is OLE2\n        if self.ole.olestruct.LogToFatSize == 0xFFFF:\n            self.add_indicator(\"ole_log_to_fat\", self.ole.olestruct.LogToFatSize,\n                               _type=int, name=\"OLE Log To FAT Size\",\n                               description=\"The size of the OLE Log To FAT array. This is usually 65535 (0x", "idx": 1804}
{"namespace": "tools.cgrep.is_valid_ip", "completion": "  try:\n    ip = nacaddr.IP(arg)\n  except ValueError:\n    raise argparse.ArgumentTypeError('%s is not a valid IP address' % arg)\n  return ip\n\n", "idx": 1805}
{"namespace": "tools.cgrep.group_diff", "completion": "  # get the network objects for the first and second group\n  first_obj, first_nets = get_nets([options.gmp[0]], db)[0]\n  second_obj, second_nets = get_nets([options.gmp[1]], db)[0]\n\n  # get the network objects for the first and second group\n  first_svc, first_ports = get_ports([options.gmp[0]], db)[0]\n  second_svc, second_ports = get_ports([options.gmp[1]], db)[0]\n\n  # get the common network objects\n  common_nets = list(set(first_nets).intersection(set(second_nets)))\n\n  # get the network objects for the first group that are not in the second group\n  diff1_nets = list(set(first_nets).difference(set(second_nets)))\n\n  # get the network objects for the second group that are not in the first group\n  diff2_nets = list(set(second_nets).difference(set(first_nets)))\n\n  # get the common service objects\n  common_ports = list(set(first_ports).intersection(set(second_ports)))\n\n  # get the service objects for the first group that are not in the second group\n  diff1_ports = list(set(first_ports).difference(set(second_ports)))\n\n  # get the service objects for the second group that are not in the first group\n  diff2_ports = list(set(second_ports).difference(set(first_ports)))\n\n  # return the common, diff1, and diff2\n  return (common_nets, diff1_nets, diff2_nets), (common_ports, diff1_ports, diff2_ports)\n\n", "idx": 1806}
{"namespace": "tools.cgrep.compare_tokens", "completion": "  # get the network objects from the database\n  first_obj = db.GetNet(options.cmp[0])\n  second_obj = db.GetNet(options.cmp[1])\n  # get the union of the two objects\n  union = first_obj + second_obj\n  # get the differences between the two objects\n  diff = first_obj.difference(second_obj)\n  # return the meta information and the differences\n  return (options.cmp[0], options.cmp[1], union), diff\n\n", "idx": 1807}
{"namespace": "capirca.aclgen.EntryPoint", "completion": "  app.run(main)\n\n", "idx": 1808}
{"namespace": "capirca.lib.nacaddr.IP", "completion": "  if isinstance(ip, ipaddress._BaseNetwork):\n    return IPv4(ip, comment, token) if ip.version == 4 else IPv6(ip, comment, token)\n  ip = ipaddress.ip_network(ip, strict)\n  return IPv4(ip, comment, token) if ip.version == 4 else IPv6(ip, comment, token)\n\n", "idx": 1809}
{"namespace": "csvkit.cli.CSVKitUtility.run", "completion": "        if 'f' not in self.override_flags:\n            with self._open_input_file(self.args.input_path) as f:\n                with warnings.catch_warnings():\n                    if self.args.no_header_row:\n                        warnings.filterwarnings(action='ignore', category=UserWarning, module='agate')\n\n                    self.main(f)\n        else:\n            with warnings.catch_warnings():\n                if self.args.no_header_row:\n                    warnings.filterwarnings(action='ignore', category=UserWarning, module='agate')\n\n                self.main(self.args.input_path)\n", "idx": 1810}
{"namespace": "csvkit.convert.fixed.fixed2csv", "completion": "    # Read the schema file\n    with open(schema, 'r') as schema_file:\n        schema_reader = agate.csv.reader(schema_file)\n        schema_header = next(schema_reader)\n        schema_rows = [r for r in schema_reader]\n\n    # Create a named tuple to store the schema information\n    Schema = namedtuple('Schema', schema_header)\n\n    # Create a list of Schema objects\n    schema_list = []\n    for r in schema_rows:\n        schema_list.append(Schema(*r))\n\n    # Read the fixed-width file\n    f.seek(0)\n    f_lines = f.readlines()\n\n    # Skip the specified number of lines\n    f_lines = f_lines[skip_lines:]\n\n    # Create a list of column names\n    column_names = [s.name for s in schema_list]\n\n    # Create a list of column indices\n    column_indices = [int(s.start) for s in schema_list]\n\n    # Create a list of column lengths\n    column_lengths = [int(s.length) for s in schema_list]\n\n    # Create a list of column widths\n    column_widths = [int(s.length) for s in schema_list]\n\n    # Create a list of column types\n    column_types = [s.type for s in schema_list]\n\n    # Create a list of column formats\n    column_formats = [s.format for s in schema_list]\n\n    # Create a list of column nulls\n    column_nulls = [s.null for s in schema_list]\n\n    # Create a list of column defaults\n    column_defaults = [s.default for s in schema_list]\n\n    # Create a list of column values\n    column_values = []\n    for i in range(len(column_names)):\n        column_values.append([])\n\n    # Parse the fixed-width file\n    for line in f_lines:\n        for i in range(len(column_names)):\n            value = line[column_", "idx": 1811}
{"namespace": "check_dummies.find_backend", "completion": "    backends = []\n    for match in _re_backend.finditer(line):\n        backends.append(match.group(1))\n    if len(backends) > 0:\n        return \"_and_\".join(backends)\n    else:\n        return None\n\n", "idx": 1812}
{"namespace": "check_dummies.create_dummy_object", "completion": "    if name[0].isupper():\n        return DUMMY_CLASS.format(name, backend_name)\n    else:\n        return DUMMY_FUNCTION.format(name, backend_name)\n\n", "idx": 1813}
{"namespace": "pycorrector.en_spell.EnSpell.check_init", "completion": "        if len(self.word_freq_dict) == 0:\n            self._init()\n", "idx": 1814}
{"namespace": "pycorrector.en_spell.EnSpell.candidates", "completion": "        self.check_init()\n        # if the word is in the dictionary, return it\n        if word in self.word_freq_dict:\n            return {word}\n        # if the word is not in the dictionary, check if it is in the set of edit1 words\n        # if it is in the set of edit1 words, return the set of edit1 words\n        elif word in self.known(self.edits1(word)):\n            return self.known(self.edits1(word))\n        # if the word is not in the dictionary and not in the set of edit1 words, check if it is in the set of edit2 words\n        # if it is in the set of edit2 words, return the set of edit2 words\n        elif word in self.known(self.edits2(word)):\n            return self.known(self.edits2(word))\n        # if the word is not in the dictionary and not in the set of edit1 and edit2 words, return the empty set\n        else:\n            return set()\n", "idx": 1815}
{"namespace": "pycorrector.en_spell.EnSpell.correct_word", "completion": "        self.check_init()\n        candidates = self.candidates(word)\n        if candidates:\n            return max(candidates, key=self.probability)\n        else:\n            return word\n", "idx": 1816}
{"namespace": "pycorrector.en_spell.EnSpell.correct", "completion": "        self.check_init()\n        if not text:\n            return text, []\n        if not include_symbol:\n            text = re.sub(r'[^a-zA-Z0-9]', ' ', text)\n        text = text.split()\n        details = []\n        for i, word in enumerate(text):\n            if len(word) > 1 and word.isalpha():\n                if word in self.custom_confusion:\n                    correct_word = self.custom_confusion[word]\n                else:\n                    correct_word = self.correct_word(word)\n                if correct_word != word:\n                    start_idx = text.index(word)\n                    end_idx = start_idx + len(word)\n                    details.append([word, correct_word, start_idx, end_idx])\n                    text[i] = correct_word\n        details.sort(key=lambda x: x[2])\n        return ' '.join(text), details\n\n", "idx": 1817}
{"namespace": "whereami.predict.crossval", "completion": "    if path is not None:\n        X, y = get_train_data(path)\n\n    if len(X) < folds:\n        raise ValueError(f\"There are not enough samples ({len(X)}). Need at least {folds} samples.\")\n\n    if clf is None:\n        clf = get_model(path)\n\n    print(f\"KFold folds={folds}, running {n} times\")\n    scores = []\n    for i in range(n):\n        scores.append(cross_val_score(clf, X, y, folds=folds))\n        print(f\"{i+1}/{n}: {scores[i]}\")\n    print(\"-------- total --------\")\n    total = sum(scores) / len(scores)\n    print(total)\n    return total\n\n", "idx": 1818}
{"namespace": "stellar.models.Table.get_table_name", "completion": "        if not self.snapshot:\n            raise Exception('Table name requires snapshot')\n        if not self.snapshot.hash:\n            raise Exception('Snapshot hash is empty')\n        if old:\n            return 'stellar_{}{}{}'.format(\n                self.table_name,\n                self.snapshot.hash,\n                postfix\n            )\n        return 'stellar_{}{}'.format(\n            hashlib.md5(\n                '{}|{}|{}'.format(\n                    self.table_name,\n                    self.snapshot.hash,\n                    postfix\n                ).encode('utf-8')\n            ).hexdigest()[:16],\n            postfix\n        )\n", "idx": 1819}
{"namespace": "chatette.utils.Singleton.reset_instance", "completion": "        if cls._instance is not None:\n            cls._instance = None\n        cls._instance = cls(*args, **kwargs)\n        return cls._instance\n\n", "idx": 1820}
{"namespace": "chatette.utils.cast_to_unicode", "completion": "    if sys.version_info[0] < 3:\n        if isinstance(anything, str):\n            return unicode(anything, \"utf-8\")\n        elif isinstance(anything, list):\n            return [cast_to_unicode(element) for element in anything]\n        elif isinstance(anything, dict):\n            return {\n                cast_to_unicode(key): cast_to_unicode(value)\n                for key, value in anything.items()\n            }\n        else:\n            return anything\n    else:\n        return anything\n\n", "idx": 1821}
{"namespace": "chatette.cli.terminal_writer.TerminalWriter.write", "completion": "        if self._file_mode == 'quiet':\n            return\n        if self.redirection_file_path is None:\n            print(text, end='')\n        else:\n            if self.buffered_text is None:\n                self.buffered_text = text\n            else:\n                self.buffered_text += text\n", "idx": 1822}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.find_redirection_file_path", "completion": "        if REDIRECTION_SYM in tokens:\n            return (RedirectionType.overwrite, tokens[tokens.index(REDIRECTION_SYM) + 1])\n        if REDIRECTION_APPEND_SYM in tokens:\n            return (RedirectionType.append, tokens[tokens.index(REDIRECTION_APPEND_SYM) + 1])\n        return None\n", "idx": 1823}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.get_unit_type_from_str", "completion": "        if unit_type_str == \"alias\":\n            return UnitType.alias\n        elif unit_type_str == \"slot\":\n            return UnitType.slot\n        elif unit_type_str == \"intent\":\n            return UnitType.intent\n        else:\n            return None\n", "idx": 1824}
{"namespace": "chatette.cli.interactive_commands.unhide_command.UnhideCommand.execute", "completion": "        if len(self._command_tokens) < 3:\n            self.print_wrapper.error_log(\"Missing some arguments\\nUsage: \" + \\\n                                         'unhide <unit-type> \"<template>\"')\n            return\n\n        unit_type = CommandStrategy.get_unit_type_from_str(self._command_tokens[1])\n        if unit_type is None:\n            self.print_wrapper.error_log(\"Unknown unit type: '\" + \\\n                                         self._command_tokens[1] + \"'.\")\n            return\n\n        template = CommandStrategy.get_template_from_tokens(self._command_tokens[2:])\n        if template is None:\n            self.print_wrapper.error_log(\"Could not interpret the template.\")\n            return\n\n        if unit_type == \"alias\":\n            if not AST.get_or_create()[unit_type]._str_matches_unit_key(template):\n                self.print_wrapper.write(\"No alias was hidden with the key '\" + \\\n                                         template + \"'.\")\n                return\n        elif unit_type == \"slot\":\n            if not AST.get_or_create()[unit_type]._str_matches_unit_key(template):\n                self.print_wrapper.write(\"No slot was hidden with the key '\" + \\\n                                         template + \"'.\")\n                return\n        elif unit_type == \"intent\":\n            if not AST.get_or_create()[unit_type]._str_matches_unit_key(template):\n                self.print_wrapper.write(\"No intent was hidden with the key '\" + \\\n                                         template + \"'.\")\n                return\n        else:\n            raise ValueError(\"Unexpected unit type: '\" + unit_type + \"'\")\n\n        if unit_type == \"alias\":\n            AST.get_or_create()[unit_type].unhide(template)\n        elif unit_type == \"slot\":\n            AST.get_or_create()[unit_type].unhide(template)\n        elif unit_type", "idx": 1825}
{"namespace": "chatette.adapters.factory.create_adapter", "completion": "    if adapter_name == \"rasa\":\n        from chatette.adapters.rasa import RasaAdapter\n        return RasaAdapter(base_filepath)\n    elif adapter_name == \"rasa-md\" or adapter_name == \"rasamd\":\n        from chatette.adapters.rasa_md import RasaMDAdapter\n        return RasaMDAdapter(base_filepath)\n    elif adapter_name == \"jsonl\":\n        from chatette.adapters.jsonl import JsonlAdapter\n        return JsonlAdapter(base_filepath)\n    else:\n        raise ValueError(\"Unknown adapter name: '\" + adapter_name + \"'.\")", "idx": 1826}
{"namespace": "chatette.parsing.ChoiceBuilder.create_concrete", "completion": "        self._check_information()\n        modifiers = self._build_modifiers_repr()\n        return Choice(self.leading_space, modifiers, self.rules)\n", "idx": 1827}
{"namespace": "chatette.parsing.UnitRefBuilder._build_modifiers_repr", "completion": "        modifiers = super(UnitRefBuilder, self)._build_modifiers_repr()\n        modifiers.argument_value = self.arg_value\n        modifiers.variation_name = self.variation\n        return modifiers\n", "idx": 1828}
{"namespace": "chatette.parsing.UnitRefBuilder.create_concrete", "completion": "        from chatette.units.modifiable.unit_reference import UnitReference\n        self._check_information()\n        return UnitReference(\n            self.leading_space, self._build_modifiers_repr(),\n            self.type, self.identifier, self.variation\n        )\n", "idx": 1829}
{"namespace": "chatette.parsing.UnitDefBuilder._build_modifiers_repr", "completion": "        modifiers = super(UnitDefBuilder, self)._build_modifiers_repr()\n        modifiers.argument_name = self.arg_name\n        return modifiers\n", "idx": 1830}
{"namespace": "chatette.parsing.AliasDefBuilder.create_concrete", "completion": "        from chatette.units.modifiable.definitions.alias import AliasDefinition\n        self._check_information()\n        if self.variation is not None:\n            if self.identifier in AST.get_or_create()[UnitType.alias]:\n                return AST.get_or_create()[UnitType.alias][self.identifier].variations[self.variation]\n        return AliasDefinition(\n            self.identifier,\n            self.leading_space, self._build_modifiers_repr()\n        )\n", "idx": 1831}
{"namespace": "chatette.parsing.SlotDefBuilder.create_concrete", "completion": "        from chatette.units.modifiable.definitions.slot import SlotDefinition\n        self._check_information()\n        if self.variation is not None:\n            definitions = AST.get_or_create()[UnitType.slot]\n            if self.identifier in definitions:\n                return definitions[self.identifier]\n        return SlotDefinition(self.identifier, self._build_modifiers_repr())\n", "idx": 1832}
{"namespace": "chatette.parsing.IntentDefBuilder.create_concrete", "completion": "        from chatette.units.modifiable.definitions.intent import IntentDefinition\n        self._check_information()\n        if self.variation is not None:\n            definitions = AST.get_or_create()[UnitType.intent]\n            if self.identifier in definitions:\n                return definitions[self.identifier]\n        return IntentDefinition(\n            self.identifier, self._build_modifiers_repr(),\n            self.nb_training_ex, self.nb_testing_ex\n        )\n", "idx": 1833}
{"namespace": "bentoml._internal.resource.get_resource", "completion": "    if resource_kind not in _RESOURCE_REGISTRY:\n        raise BentoMLConfigException(\n            f\"The resource kind '{resource_kind}' is not registered in the resource registry.\"\n        )\n\n    resource_class = _RESOURCE_REGISTRY[resource_kind]\n\n    if resource_kind not in resources:\n        return None\n\n    if resources[resource_kind] == \"system\":\n        resource = resource_class.from_system()\n    else:\n        resource = resource_class.from_spec(resources[resource_kind])\n\n    if validate:\n        resource.validate()\n\n    return resource\n\n", "idx": 1834}
{"namespace": "bentoml._internal.resource.system_resources", "completion": "    return {\n        resource_kind: resource.from_system()\n        for resource_kind, resource in _RESOURCE_REGISTRY.items()\n    }\n\n", "idx": 1835}
{"namespace": "bentoml._internal.resource.CpuResource.from_spec", "completion": "        if isinstance(spec, (float, int)):\n            return float(spec)\n        elif isinstance(spec, str):\n            if spec.endswith(\"m\"):\n                return float(spec[:-1]) / 1000\n            else:\n                return float(spec)\n        else:\n            raise ValueError(f\"Invalid specification for CpuResource: {spec}\")\n", "idx": 1836}
{"namespace": "bentoml._internal.resource.CpuResource.from_system", "completion": "        if psutil.WINDOWS:\n            return float(psutil.cpu_count(logical=False))\n        else:\n            return float(psutil.cpu_count(logical=False) or psutil.cpu_count())\n", "idx": 1837}
{"namespace": "bentoml._internal.resource.CpuResource.validate", "completion": "        if val < 0:\n            raise BentoMLConfigException(f\"Invalid CPU resource limit '{val}'. \")\n\n        if val > query_os_cpu_count():\n            raise BentoMLConfigException(\n                f\"Insufficient CPU resources: Requesting {val} but only \"\n                f\"{query_os_cpu_count()} CPUs are available.\"\n            )\n\n", "idx": 1838}
{"namespace": "bentoml._internal.types.LazyType.get_class", "completion": "        if self._runtime_class is not None:\n            return self._runtime_class\n\n        if self.qualname == \"*\":\n            return t.Any\n\n        if self.module not in sys.modules and import_module:\n            importlib.import_module(self.module)\n\n        module = sys.modules[self.module]\n        self._runtime_class = getattr(module, self.qualname)\n        return self._runtime_class\n", "idx": 1839}
{"namespace": "bentoml._internal.models.model.Model.create", "completion": "        if isinstance(name, str):\n            name = Tag(name, None)\n\n        if name.version is None:\n            name = Tag(name.name, f\"v{get_model_info_version()}\")\n\n        if labels is not None:\n            labels = normalize_labels_value(labels)\n\n        if metadata is not None:\n            metadata = normalize_metadata_value(metadata)\n\n        model_info = ModelInfo(\n            name=name.name,\n            version=name.version,\n            module=module,\n            api_version=api_version,\n            context=context,\n            signatures=signatures,\n            labels=labels,\n            options=options,\n            metadata=metadata,\n        )\n\n        model_fs = fs.open_fs(f\"temp://{model_info.tag}/\")\n        model_fs.makedirs(\"\", recreate=True)\n\n        with model_fs.open(MODEL_YAML_FILENAME, \"w\") as model_yaml_file:\n            yaml.dump(\n                model_info.dict(),\n                model_yaml_file,\n                default_flow_style=False,\n                sort_keys=False,\n            )\n\n        if custom_objects is not None:\n            with model_fs.open(CUSTOM_OBJECTS_FILENAME, \"wb\") as custom_objects_file:\n                cloudpickle.dump(custom_objects, custom_objects_file)\n\n        return cls(name, model_fs, model_info, custom_objects, _internal=True)\n", "idx": 1840}
{"namespace": "bentoml._internal.models.model.Model.from_fs", "completion": "        info = ModelInfo.from_yaml(item_fs)\n        return cls(\n            info.tag,\n            item_fs,\n            info,\n            _internal=True,\n        )\n", "idx": 1841}
{"namespace": "bentoml._internal.utils.metrics.linear_buckets", "completion": "    assert start < end\n    assert step > 0.0\n\n    buckets: list[float] = []\n    bound = start\n    while bound < end:\n        buckets.append(bound)\n        bound += step\n\n    if len(buckets) > MAX_BUCKET_COUNT:\n        buckets = buckets[:MAX_BUCKET_COUNT]\n\n    return tuple(buckets) + (end, INF)\n\n", "idx": 1842}
{"namespace": "bentoml._internal.utils.validate_metadata", "completion": "    if not isinstance(metadata, dict):\n        raise ValueError(\"metadata must be a dict!\")\n\n    for key, val in metadata.items():\n        if not isinstance(key, str):\n            raise ValueError(\"metadata keys must be strings\")\n\n        if not isinstance(val, str):\n            raise ValueError(\"metadata values must be strings\")\n\n", "idx": 1843}
{"namespace": "bentoml._internal.utils.analytics.usage_stats.get_serve_info", "completion": "    return ServeInfo(\n        serve_id=secrets.token_urlsafe(16),\n        serve_started_timestamp=datetime.now(timezone.utc),\n    )\n\n", "idx": 1844}
{"namespace": "bentoml._internal.utils.analytics.usage_stats._track_serve_init", "completion": "    track(\n        ServeInitEvent(\n            production=production,\n            serve_id=serve_info.serve_id,\n            serve_kind=serve_kind,\n            serve_started_timestamp=serve_info.serve_started_timestamp,\n            models=svc.num_models,\n            runners=svc.num_runners,\n            apis=svc.num_apis,\n            model_types=svc.model_types,\n            runner_types=svc.runner_types,\n            api_input_types=svc.api_input_types,\n            api_output_types=svc.api_output_types,\n            from_server_api=from_server_api,\n        )\n    )\n\n", "idx": 1845}
{"namespace": "bentoml._internal.service.service.get_valid_service_name", "completion": "    if not user_provided_svc_name.islower():\n        logger.warning(\n            f\"Service name '{user_provided_svc_name}' is not lowercase. Converting it to lowercase.\"\n        )\n        return Tag(user_provided_svc_name.lower()).name\n    return Tag(user_provided_svc_name).name\n\n", "idx": 1846}
{"namespace": "bentoml._internal.configuration.helpers.flatten_dict", "completion": "    for k, v in d.items():\n        new_key = parent + sep + k if parent else k\n        if isinstance(v, dict):\n            yield from flatten_dict(v, new_key, sep=sep)\n        else:\n            if any(x in punctuation for x in new_key):\n                new_key = '\"' + new_key + '\"'\n            yield new_key, v\n\n", "idx": 1847}
{"namespace": "bentoml._internal.configuration.helpers.load_config_file", "completion": "    if not os.path.exists(path):\n        raise BentoMLConfigException(f\"Config file {path} not found.\")\n\n    with open(path, \"r\") as f:\n        config = yaml.safe_load(f)\n\n    return config\n\n", "idx": 1848}
{"namespace": "bentoml._internal.configuration.helpers.expand_env_var_in_values", "completion": "    for k, v in d.items():\n        if isinstance(v, dict):\n            expand_env_var_in_values(v)\n        elif isinstance(v, str):\n            d[k] = expand_env_var(v)\n        elif isinstance(v, t.MutableSequence):\n            d[k] = [expand_env_var(i) for i in v]\n\n", "idx": 1849}
{"namespace": "bentoml._internal.runner.strategy.DefaultStrategy.get_worker_count", "completion": "        if resource_request is None:\n            resource_request = {}\n\n        if runnable_class.required_resources is None:\n            raise ValueError(\n                f\"{runnable_class.__name__} does not support any resource types.\"\n            )\n\n        if \"GPU\" in runnable_class.required_resources:\n            if \"GPU\" not in resource_request:\n                raise ValueError(\n                    f\"{runnable_class.__name__} requires GPU resources, but no GPU resource \"\n                    f\"request is given.\"\n                )\n            if \"GPU\" not in system_resources:\n                raise ValueError(\n                    f\"{runnable_class.__name__} requires GPU resources, but no GPU is available.\"\n                )\n            return math.ceil(\n                system_resources[\"GPU\"][\"total\"] / resource_request[\"GPU\"] / workers_per_resource\n            )\n\n        if \"CPU\" in runnable_class.required_resources:\n            if \"CPU\" not in system_resources:\n                raise ValueError(\n                    f\"{runnable_class.__name__} requires CPU resources, but no CPU is available.\"\n                )\n            return math.ceil(\n                system_resources[\"CPU\"][\"total\"] / resource_request[\"CPU\"] / workers_per_resource\n            )\n\n        raise ValueError(\n            f\"{runnable_class.__name__} requires {runnable_class.required_resources}, but no \"\n            f\"supported resource is available.\"\n        )\n", "idx": 1850}
{"namespace": "bentoml._internal.runner.strategy.DefaultStrategy.get_worker_env", "completion": "        if resource_request is None:\n            resource_request = system_resources()\n\n        # use nvidia gpu\n        nvidia_gpus = get_resource(resource_request, \"nvidia.com/gpu\")\n        if (\n            nvidia_gpus is not None\n            and len(nvidia_gpus) > 0\n            and \"nvidia.com/gpu\" in runnable_class.SUPPORTED_RESOURCES\n        ):\n            return {\n                \"CUDA_VISIBLE_DEVICES\": str(\n                    nvidia_gpus[\n                        math.floor(worker_index / workers_per_resource)\n                    ].device_id\n                )\n            }\n\n        # use CPU\n        cpus = get_resource(resource_request, \"cpu\")\n        if cpus is not None and cpus > 0:\n            if \"cpu\" not in runnable_class.SUPPORTED_RESOURCES:\n                logger.warning(\n                    \"No known supported resource available for %s, falling back to using CPU.\",\n                    runnable_class,\n                )\n\n            if runnable_class.SUPPORTS_CPU_MULTI_THREADING:\n                if isinstance(workers_per_resource, float):\n                    raise ValueError(\n                        \"Fractional CPU multi threading support is not yet supported.\"\n                    )\n                return {\n                    env_name: str(\n                        math.ceil(cpus * workers_per_resource / len(THREAD_ENVS))\n                    )\n                    for env_name in THREAD_ENVS\n                }\n\n            return {}\n\n        # this should not be reached by user since we always read system resource as default\n        raise ValueError(\n            f\"No known supported resource available for {runnable_class}. Please check your resource request. \"\n            \"Leaving it blank will allow BentoML to use system resources.\"\n        )", "idx": 1851}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.batches_to_batch", "completion": "        if len(batches) == 1:\n            return batches[0], [batches[0].shape[batch_dim]]\n\n        batches = [np.ascontiguousarray(batch) for batch in batches]\n        batch_sizes = [batch.shape[batch_dim] for batch in batches]\n        batch_shape = list(batches[0].shape)\n        batch_shape[batch_dim] = sum(batch_sizes)\n        batch = np.zeros(batch_shape, dtype=batches[0].dtype)\n\n        indices = []\n        idx = 0\n        for batch_size in batch_sizes:\n            indices.append(idx)\n            idx += batch_size\n\n        batch_dim_slice = slice(None)\n        batch_dim_ind = batch_dim\n        if batch_dim < 0:\n            batch_dim_ind = len(batch.shape) + batch_dim\n        if batch_dim_ind < len(batch.shape) - 1:\n            batch_dim_slice = batch_dim_slice,\n\n        for batch_idx, batch_item in enumerate(batches):\n            batch_slice = [\n                slice(None) if i == batch_dim_ind else None for i in range(len(batch.shape))\n            ]\n            batch_slice = tuple(batch_slice)\n            batch[batch_slice] = batch_item\n\n        return batch, indices\n", "idx": 1852}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.to_payload", "completion": "        if batch.ndim == 0:\n            return cls.create_payload(\n                data=pickle.dumps(batch),\n                batch_size=1,\n            )\n        else:\n            if not batch.flags.c_contiguous and not batch.flags.f_contiguous:\n                batch = np.ascontiguousarray(batch)\n            return cls.create_payload(\n                data=base64.b64encode(batch.data),\n                batch_size=batch.shape[batch_dim],\n            )\n", "idx": 1853}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.from_payload", "completion": "        if payload.meta[\"format\"] == \"pickle5\":\n            bs_str = payload.meta[\"pickle_bytes_str\"]\n            bs = base64.b64decode(bs_str)\n            indices = payload.meta[\"indices\"]\n            return pep574_loads(bs, indices)\n        else:\n            return pickle.loads(payload.data)\n", "idx": 1854}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.batch_to_payloads", "completion": "        subbatches = cls.batch_to_batches(batch, indices, batch_dim)\n        return [cls.to_payload(subbatch, batch_dim) for subbatch in subbatches]\n", "idx": 1855}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.from_batch_payloads", "completion": "        batches = [cls.from_payload(payload) for payload in payloads]\n        batch, indices = cls.batches_to_batch(batches, batch_dim)\n        return batch, indices\n\n", "idx": 1856}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.to_payload", "completion": "        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n        if isinstance(batch, pd.Series):\n            batch = batch.to_frame()\n        bs: bytes\n        concat_buffer_bs: bytes\n        indices: list[int]\n        bs, concat_buffer_bs, indices = pep574_dumps(batch)\n        bs_str = base64.b64encode(bs).decode(\"ascii\")\n        meta = {\"format\": \"pickle5\", \"pickle_bytes_str\": bs_str}\n        if indices:\n            meta[\"with_buffer\"] = True\n            meta[\"indices\"] = indices\n            meta[\"concat_buffer_bs\"] = base64.b64encode(concat_buffer_bs).decode(\"ascii\")\n        else:\n            meta[\"with_buffer\"] = False\n        return cls.create_payload(bs, batch.shape[batch_dim], meta)\n", "idx": 1857}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.from_payload", "completion": "        import pandas as pd\n\n        format = payload.meta.get(\"format\", \"default\")\n        if format == \"pickle5\":\n            bs_str = t.cast(str, payload.meta[\"pickle_bytes_str\"])\n            bs = base64.b64decode(bs_str)\n            indices = t.cast(t.List[int], payload.meta[\"indices\"])\n            return t.cast(\n                ext.PdDataFrame,\n                pep574_loads(bs, payload.data, indices),\n            )\n\n        return pd.read_pickle(io.BytesIO(payload.data))\n", "idx": 1858}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batch_to_payloads", "completion": "        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n\n        batches = cls.batch_to_batches(batch, indices, batch_dim)\n\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads\n", "idx": 1859}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.from_batch_payloads", "completion": "        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)\n\n", "idx": 1860}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.to_payload", "completion": "        assert (\n            batch_dim == 0\n        ), \"Default Runner DataContainer does not support batch_dim other than 0\"\n\n        if isinstance(batch, t.Iterator):\n            batch = list(batch)\n\n        bs: bytes = pickle.dumps(batch)\n        return cls.create_payload(bs, batch_size=len(batch))\n", "idx": 1861}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batch_to_payloads", "completion": "        assert (\n            batch_dim == 0\n        ), \"Default Runner DataContainer does not support batch_dim other than 0\"\n        batches = cls.batch_to_batches(batch, indices, batch_dim)\n        return [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n", "idx": 1862}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.from_batch_payloads", "completion": "        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)\n\n", "idx": 1863}
{"namespace": "sslyze.cli.server_string_parser.CommandLineServerStringParser.parse_server_string", "completion": "        # Extract ip from target\n        if '{' in server_str:\n            # IPv6 address\n            host, ip = cls._parse_ipv6_server_string(server_str)\n        elif '[' in server_str:\n            # IPv6 address\n            host, ip = cls._parse_ipv6_address(server_str)\n        elif ':' in server_str:\n            # IPv4 address\n            host, ip = cls._parse_ipv4_server_string(server_str)\n        else:\n            # Hostname\n            host = server_str\n            ip = None\n\n        # Extract port from target\n        if ':' in host:\n            host, port = host.split(':')\n            try:\n                port = int(port)\n            except ValueError:\n                raise InvalidServerStringError(server_str, cls.SERVER_STRING_ERROR_BAD_PORT)\n        else:\n            port = None\n\n        return host, ip, port\n", "idx": 1864}
{"namespace": "sslyze.plugins.heartbleed_plugin._HeartbleedCliConnector.result_to_console_output", "completion": "        if result.is_vulnerable_to_heartbleed:\n            return [\"The server is vulnerable to the Heartbleed attack\"]\n        else:\n            return [\"The server is not vulnerable to the Heartbleed attack\"]\n\n", "idx": 1865}
{"namespace": "sslyze.plugins.http_headers_plugin._HttpHeadersCliConnector.result_to_console_output", "completion": "        # TODO(AD): Remove this function as it is deprecated\n        # TODO(AD): Remove the expect_ct_header field from the result object\n        # TODO(AD): Remove the expect_ct_header field from the JSON output\n        # TODO(AD): Remove the expect_ct_header field from the JSON schema\n        # TODO(AD): Remove the expect_ct_header field from the JSON schema documentation\n        # TODO(AD): Remove the expect_ct_header field from the scan result object\n        # TODO(AD): Remove the expect_ct_header field from the scan result object documentation\n        # TODO(AD): Remove the expect_ct_header field from the scan result object constructor\n        # TODO(AD): Remove the expect_ct_header field from the scan result object constructor documentation\n        # TODO(AD): Remove the expect_ct_header field from the scan result object constructor tests\n        # TODO(AD): Remove the expect_ct_header field from the scan result object tests\n        # TODO(AD): Remove the expect_ct_header field from the scan result object tests documentation\n        # TODO(AD): Remove the expect_ct_header field from the scan result object tests documentation\n        # TODO(AD): Remove the expect_ct_header field from the scan result object tests documentation\n        # TODO(AD): Remove the expect_ct_header field from the scan result object tests documentation\n        # TODO(AD): Remove the expect_ct_header field from the scan result object tests documentation\n        # TODO(AD): Remove the expect_ct_header field from the scan result object tests documentation\n        # TODO(AD): Remove the expect_ct_header field from the scan result object tests documentation\n        # TODO(AD): Remove the expect_ct_header field from the scan result object tests documentation\n        # TODO(AD): Remove the expect_ct_header field from the scan result object tests documentation\n        # TODO(AD): Remove the expect_ct_header field from the scan result object tests documentation\n        # TODO(AD): Remove the expect_ct_header field from the scan result object tests documentation\n        # TODO(AD): Remove the expect_ct_header field from the scan result object tests documentation\n        # TODO(AD): Remove the expect_ct_header field from the", "idx": 1866}
{"namespace": "sslyze.plugins.http_headers_plugin._detect_http_redirection", "completion": "    # Check if the response is a redirection\n    if http_response.status_code in [301, 302, 303, 307, 308]:\n        # Check if the redirection is to the same server\n        if http_response.headers.get(\"Location\", None):\n            redirected_location = urlsplit(http_response.headers[\"Location\"])\n            if redirected_location.hostname == server_host_name and redirected_location.port == server_port:\n                return redirected_location.path\n\n    return None\n\n", "idx": 1867}
{"namespace": "sslyze.plugins.session_renegotiation_plugin._SessionRenegotiationCliConnector.result_to_console_output", "completion": "        result_txt: List[str] = []\n        if result.is_vulnerable_to_client_renegotiation_dos:\n            result_txt.append(\"The server is vulnerable to client-initiated renegotiation DoS.\")\n        else:\n            result_txt.append(\"The server is not vulnerable to client-initiated renegotiation DoS.\")\n\n        if result.supports_secure_renegotiation:\n            result_txt.append(\"The server supports secure renegotiation.\")\n        else:\n            result_txt.append(\"The server does not support secure renegotiation.\")\n\n        return result_txt\n\n", "idx": 1868}
{"namespace": "sslyze.plugins.certificate_info._cli_connector._CertificateInfoCliConnector.result_to_console_output", "completion": "        # Avoid circular imports\n        from sslyze.plugins.certificate_info.implementation import CertificateInfoScanResult  # noqa: F811\n\n        # Create the list of strings that will be returned\n        console_output = []\n\n        # Add the hostname and number of certificates\n        console_output.append(f\"Hostname sent for SNI: {result.hostname_sent_for_sni}\")\n        console_output.append(f\"Number of certificates detected: {len(result.certificate_deployments)}\")\n\n        # Iterate through each certificate deployment and add the formatted information to the result list\n        for cert_deployment in result.certificate_deployments:\n            console_output.append(cls._format_certificate_deployment(cert_deployment))\n\n        return console_output\n", "idx": 1869}
{"namespace": "sslyze.plugins.certificate_info._cli_connector._get_name_as_short_text", "completion": "    # Check if there is a common name (CN) in the name field\n    for attribute in name_field:\n        if attribute.oid == x509.NameOID.COMMON_NAME:\n            return attribute.value\n\n    # If there is no CN, return the entire name field as a string\n    return str(name_field)", "idx": 1870}
{"namespace": "sslyze.plugins.certificate_info._symantec.SymantecDistructTester.get_distrust_timeline", "completion": "        # Check if the chain contains any blacklisted certificates\n        for cert in verified_certificate_chain:\n            if cert.key_hash in cls._CA_KEYS_BLACKLIST:\n                return SymantecDistrustTimelineEnum.MARCH_2018\n\n        # Check if the chain contains any whitelisted certificates\n        for cert in verified_certificate_chain:\n            if cert.key_hash in cls._CA_KEYS_WHITELIST:\n                return SymantecDistrustTimelineEnum.SEPTEMBER_2018\n\n        return None\n", "idx": 1871}
{"namespace": "sslyze.plugins.certificate_info._certificate_utils.parse_subject_alternative_name_extension", "completion": "    try:\n        san_extension = certificate.extensions.get_extension_for_oid(ExtensionOID.SUBJECT_ALTERNATIVE_NAME)\n    except ExtensionNotFound:\n        return SubjectAlternativeNameExtension(dns_names=[], ip_addresses=[])\n\n    san_extension_value = cast(SubjectAlternativeName, san_extension.value)\n    dns_names = [\n        san_name.value\n        for san_name in san_extension_value\n        if isinstance(san_name, DNSName)\n    ]\n    ip_addresses = [\n        ip_address.value\n        for ip_address in san_extension_value\n        if isinstance(ip_address, IPAddress)\n    ]\n\n    return SubjectAlternativeNameExtension(dns_names=dns_names, ip_addresses=ip_addresses)\n\n", "idx": 1872}
{"namespace": "sslyze.plugins.certificate_info._cert_chain_analyzer._certificate_matches_hostname", "completion": "    try:\n        # Get the names from the certificate\n        names = [name.value for name in certificate.subject.get_attributes_for_oid(NameOID.COMMON_NAME)]\n        names.extend(name.value for name in certificate.subject.get_attributes_for_oid(NameOID.DNS_NAME))\n        names.extend(name.value for name in certificate.extensions.get_extension_for_class(SubjectAlternativeName).value)\n\n        # Create a dictionary with the properly formatted names\n        names_dict = {}\n        for name in names:\n            if name.startswith(\"*.\"):\n                # Wildcard name\n                names_dict[name[2:].lower()] = True\n            else:\n                # Normal name\n                names_dict[name.lower()] = False\n\n        # Check if the server_hostname matches any of the names in the certificate\n        for name in names_dict:\n            if server_hostname.lower().endswith(name) and (not names_dict[name] or server_hostname.lower().startswith(\".\")):\n                return True\n        return False\n    except CertificateError:\n        return False", "idx": 1873}
