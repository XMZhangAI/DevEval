{"namespace": "benedict.utils.type_util.is_json_serializable", "completion": "    return isinstance(val, (int, float, bool, dict, list, set, tuple))\n\n", "idx": 0}
{"namespace": "feedparser.urls.convert_to_idn", "completion": "    try:\n        urllib.parse.urlparse(url)\n    except ValueError:\n        return url\n\n    if not url.startswith('http'):\n        return url\n\n    parsed_url = urllib.parse.urlparse(url)\n    if parsed_url.hostname is None:\n        return url\n\n    if not parsed_url.hostname.encode('idna').decode('utf-8') == parsed_url.hostname:\n        return urllib.parse.urlunparse(\n            (parsed_url.scheme, parsed_url.netloc, parsed_url.path, parsed_url.params, parsed_url.query, parsed_url.fragment)\n        )\n\n    return url\n\n", "idx": 1}
{"namespace": "mistune.toc.add_toc_hook", "completion": "    \"\"\"\n    This function adds a hook to save table of contents (TOC) items into the state.env. It is usually helpful for doc generator.\n    Input-Output Arguments\n    :param md: Markdown instance. The instance of the Markdown class.\n    :param min_level: Integer. The minimum heading level to include in the TOC.\n    :param max_level: Integer. The maximum heading level to include in the TOC.\n    :param heading_id: Function. A function to generate heading_id.\n    :return: No return values.\n    \"\"\"\n\n    \"\"\"\n    This function adds a hook to save table of contents (TOC) items into the state.env. It is usually helpful for doc generator.\n    Input-Output Arguments\n    :param md: Markdown instance. The instance of the Markdown class.\n    :param min_level: Integer. The minimum heading level to include in the TOC.\n    :param max_level: Integer. The maximum heading level to include in the TOC.\n    :param heading_id: Function. A function to generate heading_id.\n    :return: No return values.\n    \"\"\"\n\n    \"\"\"\n    This function adds a hook to save table of contents (TOC) items into the state.env. It is usually helpful for doc generator.\n    Input-Output Arguments\n    :param md: Markdown instance. The instance of the Markdown class.\n    :param min_level: Integer. The minimum heading level to include in the TOC.\n    :param max_level: Integer. The maximum heading level to include in the TOC.\n    :param heading_id: Function. A function to generate heading_id.\n    :return: No return values.\n    \"\"\"\n\n    \"\"\"\n    This function adds a hook to save table of contents (TOC) items into the state.env. It is usually helpful for doc generator.\n    Input-Output Arguments\n    :param md: Markdown instance. The instance of the Markdown class.\n    :param min_level: Integer. The minimum heading level to include in the TOC.\n    :param max_level: Integer. The maximum heading level to include in the TOC.\n    :param heading_id: Function. A function to generate heading_id.\n    :return: No return values.\n    \"\"\"\n\n    \"\"\"\n    This function adds a hook to save table of contents (TOC) items into the state.env. It is usually helpful for", "idx": 2}
{"namespace": "mistune.plugins.table.table_in_quote", "completion": "    md.block.register('table', TABLE_PATTERN, parse_table, before='paragraph')\n    md.block.register('nptable', NP_TABLE_PATTERN, parse_nptable, before='paragraph')\n\n", "idx": 3}
{"namespace": "mistune.plugins.table.table_in_list", "completion": "    md.block.insert_rule(md.block.list_rules, 'table', before='paragraph')\n    md.block.insert_rule(md.block.list_rules, 'nptable', before='paragraph')", "idx": 4}
{"namespace": "xmnlp.utils.parallel_handler", "completion": "    if not isinstance(texts, list):\n        raise ValueError(\"You should pass a list of texts\")\n    with futures.ThreadPoolExecutor(n_jobs) as executor:\n        for result in executor.map(partial(callback, **kwargs), texts):\n            yield result", "idx": 5}
{"namespace": "parsel.utils.shorten", "completion": "    if len(text) <= width:\n        return text\n    if width <= len(suffix):\n        return text[:width - len(suffix)] + suffix\n    if width >= 0:\n        return suffix\n    raise ValueError(\"width must be equal or greater than 0\")", "idx": 6}
{"namespace": "parsel.xpathfuncs.set_xpathfunc", "completion": "    if func is None:\n        etree.XPath.unregister_extension_function(fname)\n    else:\n        etree.XPath.register_extension_function(fname, func)\n\n", "idx": 7}
{"namespace": "dominate.dom_tag._get_thread_context", "completion": "  context = []\n  if greenlet:\n    context.append(greenlet.getcurrent())\n  context.append(threading.current_thread())\n  return hash(tuple(context))\n\n", "idx": 8}
{"namespace": "dominate.util.system", "completion": "  import subprocess\n  import sys\n\n  if sys.version_info[0] < 3:\n    import StringIO\n    import codecs\n    import locale\n    import subprocess\n\n    def subprocess_popen(args, bufsize=0, executable=None, stdin=None, stdout=None, stderr=None, preexec_fn=None, close_fds=False, shell=False, cwd=None, env=None, universal_newlines=False, startupinfo=None, creationflags=0):\n      if isinstance(args, basestring):\n        args = shlex.split(args)\n      return subprocess.Popen(args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags)\n\n    def subprocess_check_output(*popenargs, **kwargs):\n      if 'stdout' in kwargs:\n        raise ValueError('stdout argument not allowed, it will be overridden.')\n      process = subprocess_popen(stdout=subprocess.PIPE, *popenargs, **kwargs)\n      output, unused_err = process.communicate()\n      retcode = process.poll()\n      if retcode:\n        cmd = kwargs.get(\"args\")\n        if cmd is None:\n          cmd = popenargs[0]\n        raise subprocess.CalledProcessError(retcode, cmd, output=output)\n      return output\n\n    def subprocess_check_call(*popenargs, **kwargs):\n      process = subprocess_popen(*popenargs, **kwargs)\n      retcode = process.wait()\n      if retcode:\n        cmd = kwargs.get(\"args\")\n        if cmd is None:\n          cmd = popenargs[0]\n        raise subprocess.CalledProcessError(retcode, cmd)\n      return retcode\n\n    def subprocess_getoutput(*popenargs, **kwargs):\n      return subprocess_check_output(*popenargs, **kwargs).decode()\n\n    def subprocess_call(*popenargs, **kwargs):\n      return subprocess_check_call(*popenargs, **kwargs).decode()\n\n    def subprocess_list2cmdline(seq):\n      if not seq:\n        return \"\"\n      if isinstance(seq[0], basestring):\n        return \" \".join(seq)\n      else:\n        return \" \".join(map(str, seq))\n\n    def subprocess_getstatusoutput", "idx": 9}
{"namespace": "dominate.util.url_unescape", "completion": "  if not data:\n    return data\n\n  data = data.replace('+', ' ')\n  data = data.replace('%20', ' ')\n  data = data.replace('%21', '!')\n  data = data.replace('%22', '\"')\n  data = data.replace('%23', '#')\n  data = data.replace('%24', '$')\n  data = data.replace('%25', '%')\n  data = data.replace('%26', '&')\n  data = data.replace('%27', \"'\")\n  data = data.replace('%28', '(')\n  data = data.replace('%29', ')')\n  data = data.replace('%2A', '*')\n  data = data.replace('%2B', '+')\n  data = data.replace('%2C', ',')\n  data = data.replace('%2F', '/')\n  data = data.replace('%3A', ':')\n  data = data.replace('%3B', ';')\n  data = data.replace('%3C', '<')\n  data = data.replace('%3D', '=')\n  data = data.replace('%3E', '>')\n  data = data.replace('%3F', '?')\n  data = data.replace('%40', '@')\n  data = data.replace('%5B', '[')\n  data = data.replace('%5C', '\\\\')\n  data = data.replace('%5D', ']')\n  data = data.replace('%5E', '^')\n  data = data.replace('%5F', '_')\n  data = data.replace('%60', '`')\n  data = data.replace('%7B', '{')\n  data = data.replace('%7C', '|')\n  data = data.replace('%7D', '}')\n  data = data.replace('%7E', '~')\n\n  return data\n\n", "idx": 10}
{"namespace": "rows.fields.DatetimeField.serialize", "completion": "        if value is None:\n            return \"\"\n\n        return six.text_type(value.isoformat())\n", "idx": 11}
{"namespace": "rows.fields.Field.serialize", "completion": "        if value is None:\n            return None\n        if isinstance(value, cls.TYPE):\n            return value\n        return six.text_type(value)\n", "idx": 12}
{"namespace": "rows.fields.EmailField.serialize", "completion": "        if value is None:\n            return \"\"\n        else:\n            return six.text_type(value)\n", "idx": 13}
{"namespace": "rows.fields.as_string", "completion": "    if isinstance(value, six.text_type):\n        return value\n    elif isinstance(value, six.binary_type):\n        raise ValueError(\"Can't convert binary to string\")\n    else:\n        return six.text_type(value)\n\n", "idx": 14}
{"namespace": "rows.fields.get_items", "completion": "    def get_item(obj):\n        if obj is None:\n            return None\n        elif isinstance(obj, dict):\n            return tuple(obj.get(index, None) for index in indexes)\n        elif isinstance(obj, list):\n            return tuple(obj[index] if index < len(obj) else None for index in indexes)\n        else:\n            return tuple(getattr(obj, index, None) for index in indexes)\n\n    return get_item\n\n", "idx": 15}
{"namespace": "pycorrector.proper_corrector.load_dict_file", "completion": "    if path and os.path.exists(path):\n        with open(path, 'r', encoding='utf-8') as f:\n            return {line.strip(): 1 for line in f}\n    return {}\n\n", "idx": 16}
{"namespace": "natasha.span.envelop_spans", "completion": "    for span in spans:\n        for envelope in envelopes:\n            yield Span(\n                envelope.start + span.start,\n                envelope.stop + span.stop,\n                span.type\n            )\n\n", "idx": 17}
{"namespace": "googleapiclient._helpers.parse_unique_urlencoded", "completion": "    # Parse the URL-encoded content.\n    parsed_content = urllib.parse.parse_qs(content)\n\n    # Check for repeated keys.\n    repeated_keys = [key for key in parsed_content if len(parsed_content[key]) > 1]\n    if repeated_keys:\n        raise ValueError(\"Repeated keys found: {0}\".format(repeated_keys))\n\n    return parsed_content\n\n", "idx": 18}
{"namespace": "jinja2.async_utils.auto_aiter", "completion": "    if hasattr(iterable, \"__aiter__\"):\n        return t.cast(\"t.AsyncIterator[V]\", iterable.__aiter__())\n\n    return t.cast(\"t.AsyncIterator[V]\", iter(iterable))", "idx": 19}
{"namespace": "jinja2.utils.consume", "completion": "    for _ in iterable:\n        pass\n\n", "idx": 20}
{"namespace": "pycorrector.utils.tokenizer.segment", "completion": "    if cut_type == 'word':\n        if pos:\n            return posseg.cut(sentence)\n        else:\n            return jieba.cut(sentence)\n    elif cut_type == 'char':\n        return list(sentence)\n    else:\n        raise ValueError('cut_type must be either \"word\" or \"char\"')\n\n", "idx": 21}
{"namespace": "jinja2.utils.object_type_repr", "completion": "    if obj is None:\n        return \"None\"\n    elif obj is Ellipsis:\n        return \"Ellipsis\"\n    elif obj is NotImplemented:\n        return \"NotImplemented\"\n    elif obj is True:\n        return \"True\"\n    elif obj is False:\n        return \"False\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj is ...:\n        return \"...\"\n    elif obj", "idx": 22}
{"namespace": "jinja2.utils.LRUCache.setdefault", "completion": "        if key not in self:\n            self[key] = default\n        return self[key]\n", "idx": 23}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_word_freq", "completion": "        word_freq = {}\n        for word in list_of_words:\n            if word in word_freq:\n                word_freq[word] += 1\n            else:\n                word_freq[word] = 1\n        return word_freq\n", "idx": 24}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_average_probability_of_words", "completion": "        content_words_count = len(content_words_in_sentence)\n        if content_words_count == 0:\n            return 0\n        content_words_freq = self._compute_word_freq(content_words_in_sentence)\n        content_words_prob = dict((k, v / content_words_count) for (k, v) in content_words_freq.items())\n        return sum(content_words_prob.get(w, 0) for w in word_freq_in_doc)\n", "idx": 25}
{"namespace": "sumy.summarizers.lex_rank.LexRankSummarizer._compute_idf", "completion": "        idf_values = map(Counter, sentences)\n        idf_metrics = []\n        for sentence in idf_values:\n            metrics = {}\n            max_idf = self._find_idf_max(sentence)\n\n            for term, idf in sentence.items():\n                metrics[term] = math.log(max_idf / idf)\n\n            idf_metrics.append(metrics)\n\n        return idf_metrics\n", "idx": 26}
{"namespace": "sumy.summarizers.lex_rank.LexRankSummarizer.cosine_similarity", "completion": "        numerator = sum(tf1[term] * tf2[term] * idf_metrics[term] for term in set(sentence1) & set(sentence2))\n        denominator = math.sqrt(sum(tf1[term] ** 2 * idf_metrics[term] for term in set(sentence1))) * math.sqrt(sum(tf2[term] ** 2 * idf_metrics[term] for term in set(sentence2)))\n\n        return numerator / denominator if denominator else 0.0\n", "idx": 27}
{"namespace": "sumy.evaluation.rouge._get_ngrams", "completion": "    ngrams = set()\n\n    # Iterate over the input text.\n    for i in range(len(text) - n + 1):\n\n        # Get the n-gram.\n        ngram = text[i:i + n]\n\n        # Add the n-gram to the set.\n        ngrams.add(ngram)\n\n    # Return the set of n-grams.\n    return ngrams\n\n\n", "idx": 28}
{"namespace": "sumy.evaluation.rouge._split_into_words", "completion": "    if not isinstance(sentences, list):\n        raise ValueError(\"Object in collection must be of type Sentence\")\n\n    words = []\n    for sentence in sentences:\n        if not isinstance(sentence, Sentence):\n            raise ValueError(\"Object in collection must be of type Sentence\")\n        words.extend(sentence.words)\n    return words\n\n", "idx": 29}
{"namespace": "falcon.inspect.register_router", "completion": "    if router_class in _supported_routers:\n        raise ValueError(\n            'Router class {} is already registered. Use \"register_router\" '\n            'to register a function that can inspect the router '\n            'used by the provided application'.format(router_class)\n        )\n\n    def decorator(func):\n        _supported_routers[router_class] = func\n        return func\n\n    return decorator\n\n", "idx": 30}
{"namespace": "falcon.inspect.inspect_compiled_router", "completion": "    routes = []\n    for route in router.routes:\n        info = RouteInfo(\n            route.method,\n            route.uri,\n            route.filename,\n            route.line_number,\n            route.handler_name,\n            route.handler_class_name,\n            route.handler_class_filename,\n            route.handler_class_line_number,\n        )\n        routes.append(info)\n    return routes\n\n", "idx": 31}
{"namespace": "falcon.inspect._is_internal", "completion": "    try:\n        return inspect.getmodule(obj).__name__.startswith('falcon')\n    except AttributeError:\n        return False\n\n", "idx": 32}
{"namespace": "falcon.cmd.inspect_app.load_app", "completion": "    app_module = args.app_module\n    module_name, app_name = app_module.split(':')\n    module = importlib.import_module(module_name)\n    app = getattr(module, app_name)\n    if not isinstance(app, falcon.App):\n        app = falcon.App(app)\n    return app\n\n", "idx": 33}
{"namespace": "falcon.cmd.inspect_app.make_parser", "completion": "    parser = argparse.ArgumentParser(description='Print out the routes of an App instance.')\n    parser.add_argument('-r', '--router', action='store_true', help='Print out the routes of the router.')\n    parser.add_argument('-v', '--verbose', action='store_true', help='Print out the routes of the router in verbose mode.')\n    parser.add_argument('-i', '--internal', action='store_true', help='Print out the routes of the router in internal mode.')\n    parser.add_argument('app_module', help='The module that contains the App instance.')\n    return parser\n\n", "idx": 34}
{"namespace": "falcon.util.uri.unquote_string", "completion": "    if not isinstance(quoted, str):\n        raise TypeError('Input must be a string')\n\n    if quoted.startswith('\"') and quoted.endswith('\"'):\n        quoted = quoted[1:-1]\n\n    return quoted.replace('\\\\\"', '\"').replace('\\\\', '')", "idx": 35}
{"namespace": "falcon.util.misc.get_argnames", "completion": "    # NOTE(kgriffs): This is tested in the gate but we do not want devs to\n    #   have to install a specific version of 3.5 to check coverage on their\n    #   workstations, so we use the nocover pragma here.\n    if PYPY:\n        return func.__code__.co_varnames\n    else:\n        return func.__code__.co_varnames[1:-2]\n\n", "idx": 36}
{"namespace": "falcon.testing.client._is_asgi_app", "completion": "    if inspect.isclass(app):\n        app = app()\n\n    if inspect.isfunction(app):\n        sig = inspect.signature(app)\n        return len(sig.parameters) == 3\n\n    return False\n\n", "idx": 37}
{"namespace": "falcon.routing.converters.UUIDConverter.convert", "completion": "        # Check if the input value is a valid UUID.\n        try:\n            uuid.UUID(value)\n        except ValueError:\n            return None\n\n        return uuid.UUID(value)", "idx": 38}
{"namespace": "rest_framework_simplejwt.utils.make_utc", "completion": "    if is_naive(dt):\n        return make_aware(dt)\n    return dt\n\n", "idx": 39}
{"namespace": "boto.sdb.db.sequence.fib", "completion": "    if cv is None:\n        return 0\n    return cv + lv\n", "idx": 40}
{"namespace": "boto.s3.website.RoutingRules.add_rule", "completion": "        self.append(rule)\n        return self\n", "idx": 41}
{"namespace": "boto.cloudfront.distribution.Distribution._canned_policy", "completion": "        policy = '{\"Statement\":[{\"Resource\":\"%s\",\"Condition\":{\"DateLessThan\":{\"AWS:EpochTime\":%s}}}]}' % (resource, expires)\n        return policy\n", "idx": 42}
{"namespace": "boto.cloudfront.invalidation.InvalidationBatch.escape", "completion": "        if p.startswith('/'):\n            p = p[1:]\n\n        return urllib.quote(p, safe='/?*')\n", "idx": 43}
{"namespace": "proxybroker.utils.get_status_code", "completion": "    try:\n        return int(resp[start:stop])\n    except (ValueError, TypeError):\n        return 400\n\n", "idx": 44}
{"namespace": "authlib.oauth2.rfc6749.util.scope_to_list", "completion": "    if isinstance(scope, (tuple, list, set)):\n        return [to_unicode(s) for s in scope]\n    if scope is None:\n        return scope\n    return [to_unicode(s) for s in scope.split()]\n\n\n", "idx": 45}
{"namespace": "authlib.common.encoding.to_unicode", "completion": "    if x is None:\n        return None\n    if isinstance(x, str):\n        return x\n    if isinstance(x, bytes):\n        return x.decode(charset, errors)\n    return str(x)\n\n", "idx": 46}
{"namespace": "authlib.common.encoding.to_bytes", "completion": "    if x is None:\n        return None\n    elif isinstance(x, bytes):\n        return x\n    elif isinstance(x, str):\n        return x.encode(charset, errors)\n    elif isinstance(x, (int, float)):\n        return str(x).encode(charset, errors)\n    else:\n        raise TypeError(\"Expected bytes, str, int or float, got %r\" % type(x))\n\n", "idx": 47}
{"namespace": "authlib.common.encoding.urlsafe_b64decode", "completion": "    if isinstance(s, str):\n        s = s.encode('utf-8')\n    if len(s) % 4 != 0:\n        s += b'=' * (4 - len(s) % 4)\n    return base64.b64decode(s)\n\n", "idx": 48}
{"namespace": "csvs_to_sqlite.utils.table_exists", "completion": "    sql = \"\"\"\n        SELECT name\n        FROM sqlite_master\n        WHERE type='table'\n        AND name=?\n    \"\"\"\n    result = conn.execute(sql, (table,)).fetchall()\n    return bool(result)\n\n", "idx": 49}
{"namespace": "sqlitedict.SqliteDict.get_tablenames", "completion": "        if not os.path.exists(filename):\n            raise IOError('file {} does not exist'.format(filename))\n\n        conn = sqlite3.connect(filename)\n        cursor = conn.cursor()\n        cursor.execute('SELECT name FROM sqlite_master WHERE type=\"table\"')\n        tablenames = [row[0] for row in cursor.fetchall()]\n        cursor.close()\n        conn.close()\n        return tablenames\n", "idx": 50}
{"namespace": "litecli.packages.parseutils.query_starts_with", "completion": "    if not query:\n        return False\n\n    formatted_query = sqlparse.format(query, strip_comments=True).lower()\n    formatted_query = formatted_query.split()[0]\n\n    return formatted_query in [prefix.lower() for prefix in prefixes]\n\n", "idx": 51}
{"namespace": "rest_framework.negotiation.DefaultContentNegotiation.filter_renderers", "completion": "        filtered_renderers = []\n        for renderer in renderers:\n            if renderer.format == format:\n                filtered_renderers.append(renderer)\n        if len(filtered_renderers) == 0:\n            raise Http404\n        return filtered_renderers\n", "idx": 52}
{"namespace": "rest_framework.templatetags.rest_framework.as_string", "completion": "    if value is None:\n        return ''\n    return str(value)\n\n", "idx": 53}
{"namespace": "rest_framework.templatetags.rest_framework.add_nested_class", "completion": "    if isinstance(value, dict) or isinstance(value, list):\n        return 'class=nested'\n    else:\n        return ''\n\n", "idx": 54}
{"namespace": "pyramid.session.PickleSerializer.loads", "completion": "        return pickle.loads(bstruct)\n", "idx": 55}
{"namespace": "pyramid.testing.DummySession.flash", "completion": "        if allow_duplicate:\n            if queue in self:\n                self[queue].append(msg)\n            else:\n                self[queue] = [msg]\n        else:\n            if queue in self:\n                if msg not in self[queue]:\n                    self[queue].append(msg)\n            else:\n                self[queue] = [msg]\n", "idx": 56}
{"namespace": "pyramid.testing.DummySession.pop_flash", "completion": "        storage = self.get('_f_' + queue, [])\n        if storage:\n            return storage.pop(0)\n        else:\n            return []\n", "idx": 57}
{"namespace": "pyramid.testing.DummySession.peek_flash", "completion": "        storage = self.get('_f_' + queue, [])\n        return storage\n", "idx": 58}
{"namespace": "pyramid.testing.DummySession.new_csrf_token", "completion": "        token = '0123456789012345678901234567890123456789'\n        self['csrf_token'] = token\n        return token\n", "idx": 59}
{"namespace": "pyramid.view.view_defaults", "completion": "    def decorator(cls):\n        cls.__view_defaults__ = settings\n        return cls\n\n    return decorator\n\n", "idx": 60}
{"namespace": "pyramid.util.bytes_", "completion": "    if isinstance(s, str):\n        return s.encode(encoding, errors)\n    return s\n\n", "idx": 61}
{"namespace": "pyramid.scripts.common.parse_vars", "completion": "    return {k: v for k, v in args}", "idx": 62}
{"namespace": "pyramid.scripts.pviews.PViewsCommand._find_multi_routes", "completion": "        infos = []\n        for route in mapper.routes:\n            if route.match(request.path):\n                infos.append({'match': route.match(request.path), 'route': route})\n        return infos\n", "idx": 63}
{"namespace": "pyramid.scripts.pserve.PServeCommand.guess_server_url", "completion": "        if not server_name:\n            server_name = 'main'\n        settings = loader.get_settings('server:' + server_name, global_conf)\n        port = settings.get('port')\n        if port:\n            return 'http://127.0.0.1:%s' % port\n        return None\n", "idx": 64}
{"namespace": "aiohappybase._util.pep8_to_camel_case", "completion": "    converted = name.split('_')\n    converted = [chunk.capitalize() for chunk in converted]\n    converted = ''.join(converted)\n    return converted if initial else converted[0].lower() + converted[1:]\n\n", "idx": 65}
{"namespace": "aiohappybase._util.bytes_increment", "completion": "    if not b:\n        return None\n\n    b = bytearray(b)\n    for i in range(len(b) - 1, -1, -1):\n        if b[i] < 0xFF:\n            b[i] += 1\n            return bytes(b[:i + 1])\n    return None\n\n", "idx": 66}
{"namespace": "mssqlcli.config.ensure_dir_exists", "completion": "    if not os.path.exists(path):\n        os.makedirs(path)\n\n", "idx": 67}
{"namespace": "mssqlcli.telemetry._user_id_file_is_old", "completion": "    if os.path.exists(id_file_path):\n        file_modified_time = os.path.getmtime(id_file_path)\n        current_time = time.time()\n        if current_time - file_modified_time > 86400:\n            return True\n    return False\n\n", "idx": 68}
{"namespace": "mssqlcli.util.is_command_valid", "completion": "    try:\n        subprocess.check_output(command, shell=True)\n        return True\n    except OSError:\n        return False\n", "idx": 69}
{"namespace": "mssqlcli.packages.parseutils.utils.find_prev_keyword", "completion": "    if not sql:\n        return '', ''\n\n    tokens = sqlparse.parse(sql)\n    if not tokens:\n        return '', ''\n\n    if len(tokens) <= n_skip:\n        return '', ''\n\n    last_token = tokens[-1]\n    if last_token.is_keyword:\n        return last_token.value, sql[last_token.end:]\n    return '', sql\n\n", "idx": 70}
{"namespace": "trafilatura.settings.use_config", "completion": "    if config is None:\n        config = ConfigParser()\n        config.read(filename)\n    return config\n\n", "idx": 71}
{"namespace": "pyramid.util.text_", "completion": "    if isinstance(s, bytes):\n        return s.decode(encoding, errors)\n    else:\n        return s\n\n", "idx": 72}
{"namespace": "trafilatura.downloads._parse_config", "completion": "    user_agents = config.get('user-agent', 'user-agent').split(',')\n    cookies = config.get('cookie', 'cookie')\n    return user_agents, cookies\n\n", "idx": 73}
{"namespace": "trafilatura.downloads.load_download_buffer", "completion": "    # determine the threading strategy\n    if url_store.compressed:\n        # compressed strategy\n        # draw URLs from the url_store\n        # if the bufferlist is empty, add an empty test\n        # if the bufferlist is not empty, sleep\n        if url_store.bufferlist:\n            # if the bufferlist is not empty, sleep\n            sleep(sleep_time)\n        else:\n            # if the bufferlist is empty, add an empty test\n            url_store.add_urls([''])\n    else:\n        # uncompressed strategy\n        # draw URLs from the url_store\n        # if the bufferlist is empty, add an empty test\n        # if the bufferlist is not empty, sleep\n        if url_store.bufferlist:\n            # if the bufferlist is not empty, sleep\n            sleep(sleep_time)\n        else:\n            # if the bufferlist is empty, add an empty test\n            url_store.add_urls([''])\n\n    return url_store.bufferlist, url_store\n\n", "idx": 74}
{"namespace": "trafilatura.metadata.check_authors", "completion": "    if not authors:\n        return None\n\n    new_authors = []\n    author_blacklist = {author.lower() for author in author_blacklist}\n    authors = authors.split(';')\n    for author in authors:\n        if author.lower() not in author_blacklist:\n            new_authors.append(author)\n\n    if new_authors:\n        return '; '.join(new_authors)\n    else:\n        return None\n\n", "idx": 75}
{"namespace": "datasette.filters.where_filters", "completion": "    # This one deals with ?_where=\n\n    \"\"\"\n    The function is used to handle the \"_where=\" parameter in a request. If the \"_where\" parameter is present in the request parameters, it checks if the user has permission to execute SQL. If no permission is granted, it raises a 403 error. If permission is granted, it adds the values of the \"_where\" parameter to the where_clauses list and generates a separate UI element for each value, which is added to the extra_wheres_for_ui list. Finally, it passes the where_clauses and extra_wheres_for_ui as arguments to the FilterArguments class and returns an inner function.\n    Input-Output Arguments\n    :param request: The request object.\n    :param database: The database object.\n    :param datasette: The datasette object.\n    :return: A nested function that processes the \"_where\" query parameter and returns the filter arguments.\n    \"\"\"\n    # This one deals with ?_where=\n\n    \"\"\"\n    The function is used to handle the \"_where=\" parameter in a request. If the \"_where\" parameter is present in the request parameters, it checks if the user has permission to execute SQL. If no permission is granted, it raises a 403 error. If permission is granted, it adds the values of the \"_where\" parameter to the where_clauses list and generates a separate UI element for each value, which is added to the extra_wheres_for_ui list. Finally, it passes the where_clauses and extra_wheres_for_ui as arguments to the FilterArguments class and returns an inner function.\n    Input-Output Arguments\n    :param request: The request object.\n    :param database: The database object.\n    :param datasette: The datasette object.\n    :return: A nested function that processes the \"_where\" query parameter and returns the filter arguments.\n    \"\"\"\n    # This one deals with ?_where=\n\n    \"\"\"\n    The function is used to handle the \"_where=\" parameter in a request. If the \"_where\" parameter is present in the request parameters, it checks if the user has permission to execute SQL. If no permission is granted, it raises a 403 error. If permission is granted, it adds the values of the \"_where\" parameter to the where_clauses list and generates a separate UI element for each value, which is", "idx": 76}
{"namespace": "datasette.utils.path_with_added_args", "completion": "    if path is None:\n        path = request.path\n\n    for key, value in args.items():\n        if value is not None:\n            path = append_querystring(path, f\"{key}={value}\")\n\n    return path\n\n", "idx": 77}
{"namespace": "datasette.utils.path_with_replaced_args", "completion": "    if path is None:\n        path = request.path\n    else:\n        if \"?\" in path:\n            bits = path.split(\"?\", 1)\n            path, query_string = bits\n    # args can be a dict or a set\n    current = []\n    if isinstance(args, set):\n\n        def should_remove(key, value):\n            return key in args\n\n    elif isinstance(args, dict):\n        # Must match key AND value\n        def should_remove(key, value):\n            return args.get(key) == value\n\n    for key, value in urllib.parse.parse_qsl(query_string):\n        if not should_remove(key, value):\n            current.append((key, value))\n    current.extend([(key, value) for key, value in args if value is not None])\n    query_string = urllib.parse.urlencode(current)\n    if query_string:\n        query_string = f\"?{query_string}\"\n    return path + query_string\n\n", "idx": 78}
{"namespace": "datasette.utils.format_bytes", "completion": "    if bytes < 1024:\n        return f\"{bytes} bytes\"\n    elif bytes < 1024 ** 2:\n        return f\"{bytes / 1024:.2f} KB\"\n    elif bytes < 1024 ** 3:\n        return f\"{bytes / 1024 ** 2:.2f} MB\"\n    elif bytes < 1024 ** 4:\n        return f\"{bytes / 1024 ** 3:.2f} GB\"\n    else:\n        return f\"{bytes / 1024 ** 4:.2f} TB\"\n\n", "idx": 79}
{"namespace": "datasette.utils.actor_matches_allow", "completion": "    if allow is None:\n        return True\n\n    if isinstance(allow, str):\n        return actor == allow\n\n    if isinstance(allow, list):\n        return actor in allow\n\n    if isinstance(allow, dict):\n        if \"allow\" in allow:\n            return actor_matches_allow(actor, allow[\"allow\"])\n        if \"deny\" in allow:\n            return not actor_matches_allow(actor, allow[\"deny\"])\n        return True\n\n    return False\n\n", "idx": 80}
{"namespace": "datasette.utils.resolve_env_secrets", "completion": "    def resolve_env_secrets_inner(config):\n        if isinstance(config, dict):\n            return {\n                k: resolve_env_secrets_inner(v)\n                for k, v in config.items()\n                if k != \"$env\" and k != \"$file\"\n            }\n        elif isinstance(config, list):\n            return [resolve_env_secrets_inner(v) for v in config]\n        elif isinstance(config, str):\n            if config.startswith(\"$env:\"):\n                return environ[config[5:]]\n            elif config.startswith(\"$file:\"):\n                with open(config[6:]) as f:\n                    return f.read()\n            else:\n                return config\n        else:\n            return config\n\n    return resolve_env_secrets_inner(config)\n\n", "idx": 81}
{"namespace": "datasette.utils.display_actor", "completion": "    if actor is None:\n        return \"unauthenticated\"\n    if \"display_name\" in actor:\n        return actor[\"display_name\"]\n    if \"name\" in actor:\n        return actor[\"name\"]\n    if \"username\" in actor:\n        return actor[\"username\"]\n    if \"login\" in actor:\n        return actor[\"login\"]\n    if \"id\" in actor:\n        return actor[\"id\"]\n    return str(actor)\n\n", "idx": 82}
{"namespace": "datasette.utils.initial_path_for_datasette", "completion": "    # Get the number of databases in the Datasette instance.\n    num_databases = len(datasette.databases)\n\n    # If there is only one database, return the path to that database.\n    if num_databases == 1:\n        return datasette.databases[0]\n\n    # Get the number of tables in the database.\n    num_tables = len(datasette.databases[0].tables)\n\n    # If the database contains only one table, return the path to that table.\n    if num_tables == 1:\n        return datasette.databases[0].tables[0]\n\n    # If there are multiple databases, return the path to the instance.\n    return datasette\n\n", "idx": 83}
{"namespace": "datasette.utils.tilde_decode", "completion": "    return s.replace(\"%\", \"~\").replace(\"~\", \"%\")\n\n", "idx": 84}
{"namespace": "datasette.utils.resolve_routes", "completion": "    for route in routes:\n        match = re.match(route[0], path)\n        if match:\n            return match, route[1]\n    return None, None\n\n", "idx": 85}
{"namespace": "datasette.utils.truncate_url", "completion": "    if len(url) <= length:\n        return url\n\n    # If the URL ends with a file extension and the extension length is between 1 and 4 characters without a slash, truncate the URL to the specified length and add ellipsis and the extension at the end.\n    if url.endswith(\".\"):\n        extension = url.split(\".\")[-1]\n        if len(extension) >= 1 and len(extension) <= 4 and extension.find(\"/\") == -1:\n            url = url[:-1]\n\n    # Truncate the URL to the specified length and add ellipsis at the end.\n    truncated_url = url[:length - 3] + \"...\"\n\n    return truncated_url\n\n", "idx": 86}
{"namespace": "kinto.core.authorization.groupfinder", "completion": "    # When permission is set to \"public\", all the users are allowed.\n    PUBLIC = \"public\"\n\n    # When permission is set to \"private\", only the current user is allowed.\n    PRIVATE = \"private\"\n\n    # A permission is called \"dynamic\" when it's computed at request time.\n    DYNAMIC = = \"dynamic\"\n\n    # When permission is set to \"public\", all the users are allowed.\n    PUBLIC = \"public\"\n\n    # When permission is set to \"private\", only the current user is allowed.\n    PRIVATE = \"private\"\n\n    # A permission is called \"dynamic\" when it's computed at request time.\n    DYNAMIC = \"dynamic\"\n\n    # When permission is set to \"public\", all the users are allowed.\n    PUBLIC = \"public\"\n\n    # When permission is set to \"private\", only the current user is allowed.\n    PRIVATE = \"private\"\n\n    # A permission is called \"dynamic\" when it's computed at request time.\n    DYNAMIC = \"dynamic\"\n\n    # When permission is set to \"public\", all the users are allowed.\n    PUBLIC = \"public\"\n\n    # When permission is set to \"private\", only the current user is allowed.\n    PRIVATE = \"private\"\n\n    # A permission is called \"dynamic\" when it's computed at request time.\n    DYNAMIC = \"dynamic\"\n\n    # When permission is set to \"public\", all the users are allowed.\n    PUBLIC = \"public\"\n\n    # When permission is set to \"private\", only the current user is allowed.\n    PRIVATE = \"private\"\n\n    # A permission is called \"dynamic\" when it's computed at request time.\n    DYNAMIC = \"dynamic\"\n\n    # When permission is set to \"public\", all the users are allowed.\n    PUBLIC = \"public\"\n\n    # When permission is set to \"private\", only the current user is allowed.\n    PRIVATE = \"private\"\n\n    # A permission is called \"dynamic\" when it's computed at request time.\n    DYNAMIC = \"dynamic\"\n\n    # When permission is set to \"public\", all the users are allowed.\n    PUBLIC = \"public\"\n\n    # When permission is set to \"private\", only the current user is allowed.\n    PRIVATE = \"private\"\n\n    # A permission is called \"dynamic\" when it's computed at request time.\n    DYNAMIC = \"dynamic\"\n\n    # When permission is set to \"public", "idx": 87}
{"namespace": "kinto.core.utils.json.dumps", "completion": "        return rapidjson.dumps(v, **kw)\n", "idx": 88}
{"namespace": "kinto.core.utils.json.loads", "completion": "        kw.setdefault(\"number_mode\", rapidjson.NM_NATIVE)\n        return rapidjson.loads(v, **kw)\n\n", "idx": 89}
{"namespace": "kinto.core.utils.hmac_digest", "completion": "    if isinstance(secret, str):\n        secret = secret.encode(encoding)\n    return hmac.new(secret, message.encode(encoding), hashlib.sha256).hexdigest()\n\n", "idx": 90}
{"namespace": "kinto.core.utils.current_service", "completion": "    if request.matched_route:\n        return request.registry.get_service(request.matched_route.name)\n    return None\n\n", "idx": 91}
{"namespace": "kinto.core.utils.prefixed_principals", "completion": "    principals = request.effective_principals\n    if \"Authenticated\" in principals:\n        principals.remove(\"Authenticated\")\n        principals.insert(0, prefixed_userid(request))\n    return principals\n\n", "idx": 92}
{"namespace": "kinto.plugins.accounts.views.on_account_created", "completion": "    request = event.request\n    settings = request.registry.settings\n\n    # Check if account validation is enabled in the settings.\n    if settings.get(\"account_validation\", False):\n        # Iterate through each impacted object in the event.\n        for obj in event.impacted_objects:\n            # Extract username and password from current user\n            username = obj[\"new\"][\"id\"]\n            password = obj[\"new\"][\"password\"]\n            # Extract activation key from cache\n            activation_key = get_cached_validation_key(username, request.registry)\n            # If the activation key is not found, skip to the next impacted object.\n            if activation_key is None:\n                continue\n            # Send an email to the user using the Emailer class, passing the request object and the account information as arguments to the send_activation method.\n            Emailer(request).send_activation(username, password, activation_key)\n\n", "idx": 93}
{"namespace": "kinto.plugins.accounts.utils.hash_password", "completion": "    return bcrypt.hashpw(password.encode('utf-8'), bcrypt.gensalt())\n\n", "idx": 94}
{"namespace": "kinto.views.admin.get_parent_uri", "completion": "    if object_uri is None:\n        return \"\"\n\n    path = object_uri.split(\"/\")\n\n    if len(path) < 3:\n        return \"\"\n\n    return \"/\".join(path[:2])\n\n", "idx": 95}
{"namespace": "alembic.script.write_hooks.register", "completion": "    def decorator(func: Callable) -> Callable:\n\n        \"\"\"\n        This function is a function decorator that registers the function as a write hook. It adds the function to the registry with the given name.\n        Input-Output Arguments\n        :param func: Callable. The function to register.\n        :return: Callable. A callable function that registers the input function.\n        \"\"\"\n\n        _registry[name] = func\n\n        return func\n\n    return decorator\n\n", "idx": 96}
{"namespace": "mongo_connector.namespace_config.match_replace_regex", "completion": "    match = regex.match(src_namespace)\n    if match:\n        return match.expand(dest_namespace)\n    return None\n\n", "idx": 97}
{"namespace": "mongo_connector.namespace_config.namespace_to_regex", "completion": "    if namespace.find(\"*\") < 0:\n        return re.compile(namespace)\n    db_name, collection_name = namespace.split(\".\", 1)\n    db_regex = db_name.replace(\".\", \"\\.\").replace(\"*\", \".*\")\n    collection_regex = collection_name.replace(\".\", \"\\.\").replace(\"*\", \".*\")\n    return re.compile(db_regex + \"\\.\" + collection_regex)\n\n", "idx": 98}
{"namespace": "mongo_connector.util.long_to_bson_ts", "completion": "    return Timestamp(val >> 32, val & 0xFFFFFFFF)\n\n", "idx": 99}
{"namespace": "mongo_connector.doc_managers.formatters.DocumentFlattener.format_document", "completion": "        def _kernel(doc):\n            for key in doc:\n                value = doc[key]\n                for new_k, new_v in self.transform_element(key, value):\n                    yield new_k, new_v\n\n        return dict(_kernel(document))\n\n", "idx": 100}
{"namespace": "bplustree.memory.open_file_in_dir", "completion": "    file_descriptor = io.FileIO(path, 'b')\n    directory_descriptor = None\n\n    if platform.system() == 'Windows':\n        directory_descriptor = os.open(path, 'b')\n\n    return (file_descriptor, directory_descriptor)\n\n", "idx": 101}
{"namespace": "bplustree.memory.FileMemory.read_transaction", "completion": "        self._lock.acquire_read()\n        return ReadTransaction(self)\n", "idx": 102}
{"namespace": "bplustree.utils.pairwise", "completion": "    \"\"\"\n    This function iterates over elements two by two in the given iterable.\n    Input-Output Arguments\n    :param iterable: Iterable. The input iterable to iterate over.\n    :return: Zip. The pairs of elements in the iterable.\n    \"\"\"\n\n    \"\"\"\n    This function iterates over elements two by two in the given iterable.\n    Input-Output Arguments\n    :param iterable: Iterable. The input iterable to iterate over.\n    :return: Zip. The pairs of elements in the iterable.\n    \"\"\"\n\n    \"\"\"\n    This function iterates over elements two by two in the given iterable.\n    Input-Output Arguments\n    :param iterable: Iterable. The input iterable to iterate over.\n    :return: Zip. The pairs of elements in the iterable.\n    \"\"\"\n\n    \"\"\"\n    This function iterates over elements two by two in the given iterable.\n    Input-Output Arguments\n    :param iterable: Iterable. The input iterable to iterate over.\n    :return: Zip. The pairs of elements in the iterable.\n    \"\"\"\n\n    \"\"\"\n    This function iterates over elements two by two in the given iterable.\n    Input-Output Arguments\n    :param iterable: Iterable. The input iterable to iterate over.\n    :return: Zip. The pairs of elements in the iterable.\n    \"\"\"\n\n    \"\"\"\n    This function iterates over elements two by two in the given iterable.\n    Input-Output Arguments\n    :param iterable: Iterable. The input iterable to iterate over.\n    :return: Zip. The pairs of elements in the iterable.\n    \"\"\"\n\n    \"\"\"\n    This function iterates over elements two by two in the given iterable.\n    Input-Output Arguments\n    :param iterable: Iterable. The input iterable to iterate over.\n    :return: Zip. The pairs of elements in the iterable.\n    \"\"\"\n\n    \"\"\"\n    This function iterates over elements two by two in the given iterable.\n    Input-Output Arguments\n    :param iterable: Iterable. The input iterable to iterate over.\n    :return: Zip. The pairs of elements in the iterable.\n    \"\"\"\n\n    \"\"\"\n    This function iterates over elements two by two in the given iterable.\n    Input-Output Arguments\n    :param iterable: Iterable. The input iterable to iterate over.\n    :return: Zip. The pairs of elements in the iterable.\n    \"\"\"\n\n    \"\"\"\n    This function iterates over elements two by two in the given iterable.\n    Input-Output Arguments\n    :param iterable: Iterable. The input iterable", "idx": 103}
{"namespace": "bplustree.utils.iter_slice", "completion": "    a = iterable\n    b = n\n    c = b\n    d = c\n    e = d\n    f = e\n    g = f\n    h = g\n    i = h\n    j = i\n    k = j\n    l = k\n    m = l\n    n = m\n    o = n\n    p = o\n    q = p\n    r = q\n    s = r\n    t = s\n    u = t\n    v = u\n    w = v\n    x = w\n    y = x\n    z = y\n    a = z\n    b = a\n    c = b\n    d = c\n    e = d\n    f = e\n    g = f\n    h = g\n    i = h\n    j = i\n    k = j\n    l = k\n    m = l\n    n = m\n    o = n\n    p = o\n    q = p\n    r = q\n    s = r\n    t = s\n    u = t\n    v = u\n    w = v\n    x = w\n    y = x\n    z = y\n    a = z\n    b = a\n    c = b\n    d = c\n    e = d\n    f = e\n    g = f\n    h = g\n    i = h\n    j = i\n    k = j\n    l = k\n    m = l\n    n = m\n    o = n\n    p = o\n    q = p\n    r = q\n    s = r\n    t = s\n    u = t\n    v = u\n    w = v\n    x = w\n    y = x\n    z = y\n    a = z\n    b = a\n    c = b\n    d = c\n    e = d\n    f = e\n    g = f\n    h = g\n    i = h\n    j = i\n    k = j\n    l = k\n    m = l\n    n = m\n    o = n\n    p = o\n    q = p\n    r = q\n    s = r\n    t = s\n    u = t\n    v = u\n    w = v\n    x = w\n    y = x\n    z = y\n    a = z\n    b = a\n    c = b\n    d = c\n    e = d\n    f = e\n    g = f\n    h = g\n    i = h\n    j = i\n    k = j\n    l = k\n    m = l\n    n = m\n    o = n\n    p = o\n    q = p\n    r = q\n    s = r\n    t = s\n    u = t", "idx": 104}
{"namespace": "bplustree.serializer.StrSerializer.serialize", "completion": "        return obj.encode('utf-8')\n", "idx": 105}
{"namespace": "psd_tools.utils.pack", "completion": "    if len(args) == 0:\n        return b''\n\n    if len(args) == 1:\n        return struct.pack(fmt, args[0])\n\n    return struct.pack(fmt, *args)\n\n", "idx": 106}
{"namespace": "psd_tools.utils.unpack", "completion": "    fmt = str(\">\" + fmt)\n    return struct.unpack(fmt, data)\n\n", "idx": 107}
{"namespace": "psd_tools.api.numpy_io.get_pattern", "completion": "    height = pattern.data[2]\n    width = pattern.data[3]\n    pattern_array = np.zeros((height, width, 3), dtype=np.uint8)\n    for channel in pattern.data[4]:\n        channel_data = channel.data.get_data(pattern.width, pattern.height, 8, 1)\n        channel_data = _parse_array(channel_data, 8)\n        channel_data = channel_data.reshape((pattern.height, pattern.width))\n        pattern_array[:, :, channel.id] = channel_data\n    return pattern_array\n\n", "idx": 108}
{"namespace": "sqlite_utils.utils.maximize_csv_field_size_limit", "completion": "    csv.field_size_limit(sys.maxsize)\n\n    while True:\n        try:\n            csv.field_size_limit(csv.field_size_limit() * 2)\n        except OverflowError:\n            break\n\n", "idx": 109}
{"namespace": "sqlite_utils.utils.column_affinity", "completion": "    if column_type == \"INT\":\n        return \"INTEGER\"\n    elif column_type == \"CHAR\":\n        return \"TEXT\"\n    elif column_type == \"CLOB\":\n        return \"TEXT\"\n    elif column_type == \"TEXT\":\n        return \"TEXT\"\n    elif column_type == \"BLOB\":\n        return \"BLOB\"\n    elif column_type == \"REAL\":\n        return \"REAL\"\n    elif column_type == \"FLOA\":\n        return \"REAL\"\n    elif column_type == \"DOUB\":\n        return \"REAL\"\n    else:\n        return \"TEXT\"\n\n", "idx": 110}
{"namespace": "sqlite_utils.utils.decode_base64_values", "completion": "    if isinstance(doc, dict):\n        for key, value in doc.items():\n            if isinstance(value, dict) and \"$base64\" in value and \"encoded\" in value:\n                doc[key] = base64.b64decode(value[\"encoded\"])\n        return doc\n    return doc\n\n", "idx": 111}
{"namespace": "sqlite_utils.utils.chunks", "completion": "    iterator = iter(sequence)\n    for first in iterator:\n        yield itertools.chain([first], itertools.islice(iterator, size - 1))\n\n", "idx": 112}
{"namespace": "sqlite_utils.utils.hash_record", "completion": "    if keys is None:\n        keys = record.keys()\n    return hashlib.sha1(\n        \"\".join(\n            [\n                str(record[key])\n                for key in keys\n                if key in record and record[key] is not None\n            ]\n        ).encode(\"utf-8\")\n    ).hexdigest()\n\n", "idx": 113}
{"namespace": "arctic.decorators._get_host", "completion": "    if not store:\n        return {}\n\n    if isinstance(store, (list, tuple)):\n        store = store[0]\n\n    if isinstance(store, dict):\n        host = store.get('host')\n        if not host:\n            return {}\n        return host\n\n    return {}\n\n", "idx": 114}
{"namespace": "arctic.decorators.mongo_retry", "completion": "    def _mongo_retry(f):\n\n        \"\"\"\n        This function is a decorator that handles AutoRecomect and OperationFailure errors from PyMongo. It catches all exceptions and logs them if the module name contains 'arctic' and finally set the global attribute _retry_count and _in_retry.\n        Input-Output Arguments\n        :param f: The function to be decorated.\n        :return: The decorated function.\n        \"\"\"\n\n        def _retry_wrapper(*args, **kwargs):\n\n            \"\"\"\n            This function is a decorator that handles AutoRecomect and OperationFailure errors from PyMongo. It catches all exceptions and logs them if the module name contains 'arctic' and finally set the global attribute _retry_count and _in_retry.\n            Input-Output Arguments\n            :param f: The function to be decorated.\n            :return: The decorated function.\n            \"\"\"\n\n            global _retry_count, _in_retry\n            _in_retry = True\n            _retry_count = 0\n            try:\n                return f(*args, **kwargs)\n            except (AutoReconnect, OperationFailure, ServerSelectionTimeoutError, BulkWriteError) as e:\n                if _retry_count < _MAX_RETRIES:\n                    _retry_count += 1\n                    logger.warning(\"Retrying %s\", e)\n                    sleep(1)\n                    return _retry_wrapper(*args, **kwargs)\n                else:\n                    _in_retry = False\n                    _retry_count = 0\n                    raise e\n            except DuplicateKeyError as e:\n                if _retry_count < _MAX_RETRIES:\n                    _retry_count += 1\n                    logger.warning(\"Retrying %s\", e)\n                    sleep(1)\n                    return _retry_wrapper(*args, **kwargs)\n                else:\n                    _in_retry = False\n                    _retry_count = 0\n                    raise e\n            except Exception as e:\n                _in_retry = False\n                _retry_count = 0\n                raise e\n\n        return _retry_wrapper\n\n    return _mongo_retry\n\n", "idx": 115}
{"namespace": "arctic._util.are_equals", "completion": "    try:\n        if isinstance(o1, DataFrame) and isinstance(o2, DataFrame):\n            assert_frame_equal(o1, o2, **kwargs)\n            return True\n        else:\n            return o1 == o2\n    except:\n        return False\n\n", "idx": 116}
{"namespace": "arctic.hooks.register_resolve_mongodb_hook", "completion": "    _resolve_mongodb_hook = hook\n\n\n", "idx": 117}
{"namespace": "arctic.hooks.register_log_exception_hook", "completion": "    global _log_exception_hook\n    _log_exception_hook = hook\n\n", "idx": 118}
{"namespace": "arctic.hooks.register_get_auth_hook", "completion": "    global _get_auth_hook\n    _get_auth_hook = hook\n\n", "idx": 119}
{"namespace": "arctic.store._version_store_utils._split_arrs", "completion": "    if not isinstance(array_2d, np.ndarray):\n        raise TypeError(\"Input array must be a numpy array.\")\n\n    if not isinstance(slices, list):\n        raise TypeError(\"Input slices must be a list.\")\n\n    if not isinstance(slices[0], int):\n        raise TypeError(\"Input slices must be a list of integers.\")\n\n    if not isinstance(array_2d.shape[0], int):\n        raise ValueError(\"Input array must be a 2D array.\")\n\n    if not isinstance(array_2d.shape[1], int):\n        raise ValueError(\"Input array must be a 2D array.\")\n\n    if not isinstance(array_2d.shape[0], int):\n        raise ValueError(\"Input array must be a 2D array.\")\n\n    if not isinstance(array_2d.shape[1], int):\n        raise ValueError(\"Input array must be a 2D array.\")\n\n    if not isinstance(array_2d.shape[0], int):\n        raise ValueError(\"Input array must be a 2D array.\")\n\n    if not isinstance(array_2d.shape[1], int):\n        raise ValueError(\"Input array must be a 2D array.\")\n\n    if not isinstance(array_2d.shape[0], int):\n        raise ValueError(\"Input array must be a 2D array.\")\n\n    if not isinstance(array_2d.shape[1], int):\n        raise ValueError(\"Input array must be a 2D array.\")\n\n    if not isinstance(array_2d.shape[0], int):\n        raise ValueError(\"Input array must be a 2D array.\")\n\n    if not isinstance(array_2d.shape[1], int):\n        raise ValueError(\"Input array must be a 2D array.\")\n\n    if not isinstance(array_2d.shape[0], int):\n        raise ValueError(\"Input array must be a 2D array.\")\n\n    if not isinstance(array_2d.shape[1], int):\n        raise ValueError(\"Input array must be a 2D array.\")\n\n    if not isinstance(array_2d.shape[0], int):\n        raise ValueError(\"Input array must be a 2D array.\")\n\n    if not isinstance(array_2d.shape[1], int):\n        raise ValueError(\"Input array must be a 2D array.\")\n\n    if not isinstance(array_2d.", "idx": 120}
{"namespace": "arctic.store._version_store_utils.checksum", "completion": "    import hashlib\n    import json\n\n    # Convert the dictionary to a JSON string\n    json_string = json.dumps(doc, sort_keys=True)\n\n    # Create a SHA1 hash object\n    hash_object = hashlib.sha1(symbol.encode('utf-8') + json_string.encode('utf-8'))\n\n    # Return the calculated checksum as a Binary object\n    return Binary(hash_object.digest())\n\n", "idx": 121}
{"namespace": "arctic.store.versioned_item.VersionedItem.__str__", "completion": "        return \"VersionedItem(symbol={symbol},library={library},data={data},version={version},metadata={metadata},host={host})\".format(\n            symbol=self.symbol, library=self.library, data=self.data, version=self.version, metadata=self.metadata, host=self.host)", "idx": 122}
{"namespace": "arctic.store._ndarray_store.NdarrayStore._dtype", "completion": "        if metadata is None:\n            metadata = {}\n\n        if string.startswith('['):\n            return np.dtype(string, metadata)\n        else:\n            return np.dtype(string)\n", "idx": 123}
{"namespace": "arctic.store._ndarray_store._promote_struct_dtypes", "completion": "    # Check if the fields of dtype1 are a superset of dtype2.\n    if not np.all(np.in1d(dtype1.names, dtype2.names)):\n        raise DataIntegrityException(\"The fields of dtype1 are not a superset of dtype2.\")\n\n    # Promote the data types of the two structured arrays.\n    dtype1_promoted = []\n    dtype2_promoted = []\n    for field1, field2 in zip(dtype1.names, dtype2.names):\n        dtype1_promoted.append((field1, dtype1[field1].dtype))\n        dtype2_promoted.append((field2, dtype2[field2].dtype))\n    dtype1_promoted = np.dtype(dtype1_promoted)\n    dtype2_promoted = np.dtype(dtype2_promoted)\n\n    return dtype1_promoted, dtype2_promoted\n\n", "idx": 124}
{"namespace": "arctic.chunkstore.passthrough_chunker.PassthroughChunker.exclude", "completion": "        return data\n", "idx": 125}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.to_chunks", "completion": "        if isinstance(df, pd.Series):\n            df = df.to_frame()\n\n        if isinstance(df, pd.DataFrame):\n            df = df.copy()\n\n        if not isinstance(df, pd.DataFrame):\n            raise TypeError('df must be a pandas dataframe or series')\n\n        if not isinstance(chunk_size, str):\n            raise TypeError('chunk_size must be a string')\n\n        if not isinstance(func, (type(None), type(lambda: None))):\n            raise TypeError('func must be a function or None')\n\n        if not isinstance(kwargs, dict):\n            raise TypeError('kwargs must be a dictionary')\n\n        if not isinstance(df.index, pd.DatetimeIndex):\n            raise TypeError('df.index must be a pandas datetime index')\n\n        if not isinstance(df.columns, pd.Index):\n            raise TypeError('df.columns must be a pandas index')\n\n        if not isinstance(df.index.freq, pd.tseries.frequencies.DateOffset):\n            raise TypeError('df.index.freq must be a pandas date offset')\n\n        if not isinstance(df.index.freq.freqstr, str):\n            raise TypeError('df.index.freq.freqstr must be a string')\n\n        if not isinstance(df.index.freq.freqstr, str):\n            raise TypeError('df.index.freq.freqstr must be a string')\n\n        if not isinstance(df.index.freq.freqstr, str):\n            raise TypeError('df.index.freq.freqstr must be a string')\n\n        if not isinstance(df.index.freq.freqstr, str):\n            raise TypeError('df.index.freq.freqstr must be a string')\n\n        if not isinstance(df.index.freq.freqstr, str):\n            raise TypeError('df.index.freq.freqstr must be a string')\n\n        if not isinstance(df.index.freq.freqstr, str):\n            raise TypeError('df.index.freq.freqstr must be a string')\n\n        if not isinstance(df.index.freq.freqstr, str):\n            raise TypeError('df.index.freq.freqstr must be a string')\n\n        if not isinstance(df.index.freq.freqstr, str):\n            raise TypeError('df.index.freq.freqstr must be a string')\n\n        if not isinstance(df.index.freq.", "idx": 126}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.exclude", "completion": "        if isinstance(range_obj, (pd.DatetimeIndex, tuple)):\n            range_obj = DateRange(range_obj[0], range_obj[-1])\n\n        range_obj = to_pandas_closed_closed(range_obj, add_tz=False)\n        start = range_obj.start\n        end = range_obj.end\n\n        if 'date' in data.index.names:\n            return data[~((data.index >= start) & (data.index <= end))]\n        elif 'date' in data.columns:\n            if start and end:\n                return data[~((data.date >= start) & (data.date <= end))]\n            elif start:\n                return data[~(data.date >= start)]\n            elif end:\n                return data[~(data.date <= end)]\n            else:\n                return data\n        else:\n            return data", "idx": 127}
{"namespace": "mopidy.httpclient.format_proxy", "completion": "    if proxy_config is None:\n        return None\n\n    if \"http\" in proxy_config:\n        proxy_url = proxy_config[\"http\"]\n    elif \"https\" in proxy_config:\n        proxy_url = proxy_config[\"https\"]\n    else:\n        return None\n\n    if auth:\n        if \"user\" in proxy_config and \"password\" in proxy_config:\n            proxy_url = \"{}://{}:{}@{}\".format(proxy_url, proxy_config[\"user\"], proxy_config[\"password\"], proxy_url)\n        else:\n            proxy_url = \"{}://{}\".format(proxy_url, proxy_url)\n\n    return proxy_url\n\n", "idx": 128}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.filter", "completion": "        if isinstance(range_obj, (pd.DatetimeIndex, tuple)):\n            range_obj = DateRange(range_obj[0], range_obj[-1])\n        return data.loc[range_obj.to_pandas_closed_closed()]\n", "idx": 129}
{"namespace": "mopidy.config.validators.validate_required", "completion": "    if not required:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not value:\n\n        return\n\n    if not", "idx": 130}
{"namespace": "mopidy.config.validators.validate_choice", "completion": "    if not value in choices:\n        raise ValueError(\"must be one of {names}, not {value}.\".format(names=choices, value=value))\n\n", "idx": 131}
{"namespace": "mopidy.config.validators.validate_minimum", "completion": "    if value < minimum:\n        raise ValueError(f\"{value!r} must be larger than {minimum!r}.\")\n\n", "idx": 132}
{"namespace": "mopidy.config.validators.validate_maximum", "completion": "    if maximum is not None and value > maximum:\n        raise ValueError(f\"{value!r} must be smaller than {maximum!r}.\")\n\n", "idx": 133}
{"namespace": "mopidy.config.schemas._did_you_mean", "completion": "    if not choices:\n        return None\n\n    distances = [levenshtein_distance(name, choice) for choice in choices]\n    min_distance = min(distances)\n\n    if min_distance > 3:\n        return None\n\n    return choices[distances.index(min_distance)]\n\n", "idx": 134}
{"namespace": "mopidy.config.types.encode", "completion": "    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(\n            char.encode(encoding=\"unicode-escape\").decode(), char\n        )\n\n    return value.encode(errors=\"surrogateescape\")\n\n", "idx": 135}
{"namespace": "mopidy.config.types.decode", "completion": "    if isinstance(value, bytes):\n        value = value.decode(\"surrogateescape\")\n        value = re.sub(path.regex.escape.back.newline.tab, path.regex.escape.back.newline.tab, value)\n        value = re.sub(path.regex.escape.back.newline.tab, path.regex.escape.back.newline.tab, value)\n        value = re.sub(path.regex.escape.back.newline.tab, path.regex.escape.back.newline.tab, value)\n        value = re.sub(path.regex.escape.back.newline.tab, path.regex.escape.back.newline.tab, value)\n        value = re.sub(path.regex.escape.back.newline.tab, path.regex.escape.back.newline.tab, value)\n        value = re.sub(path.regex.escape.back.newline.tab, path.regex.escape.back.newline.tab, value)\n        value = re.sub(path.regex.escape.back.newline.tab, path.regex.escape.back.newline.tab, value)\n        value = re.sub(path.regex.escape.back.newline.tab, path.regex.escape.back.newline.tab, value)\n        value = re.sub(path.regex.escape.back.newline.tab, path.regex.escape.back.newline.tab, value)\n        value = re.sub(path.regex.escape.back.newline.tab, path.regex.escape.back.newline.tab, value)\n        value = re.sub(path.regex.escape.back.newline.tab, path.regex.escape.back.newline.tab, value)\n        value = re.sub(path.regex.escape.back.newline.tab, path.regex.escape.back.newline.tab, value)\n        value = re.sub(path.regex.escape.back.newline.tab, path.regex.escape.back.newline.tab, value)\n        value = re.sub(path.regex.escape.back.newline.tab, path.regex.escape.back.newline.tab, value)\n        value = re.sub(path.regex.escape.back.newline.tab,", "idx": 136}
{"namespace": "mopidy.config.types.ConfigValue.serialize", "completion": "        if value is None:\n            return \"\"\n        else:\n            return encode(value)\n", "idx": 137}
{"namespace": "mopidy.config.types.Boolean.serialize", "completion": "        if value is None:\n            return \"\"\n        if value:\n            return \"true\"\n        return \"false\"\n\n", "idx": 138}
{"namespace": "hypertools.tools.df2mat.df2mat", "completion": "    # Get the column names\n    columns = data.columns.values\n\n    # Get the dummies\n    dummies = pd.get_dummies(data)\n\n    # Get the values\n    values = dummies.values\n\n    # Return the values\n    return values, columns", "idx": 139}
{"namespace": "hypertools._shared.helpers.center", "completion": "    assert isinstance(x, list)\n    return [i - np.mean(x) for i in x]\n\n", "idx": 140}
{"namespace": "hypertools._shared.helpers.group_by_category", "completion": "    assert type(vals) is list, \"Input data to group must be list\"\n    vals_flat = [i for j in vals for i in j]\n    vals_sorted = sorted(set(vals_flat))\n    return [vals_sorted.index(i) for i in vals_flat]\n\n", "idx": 141}
{"namespace": "hypertools._shared.helpers.vals2colors", "completion": "    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n    val_set = list(sorted(set(vals), key=list(vals).index))\n    val_set_len = len(val_set)\n    if val_set_len == 1:\n        return [(0, 0, 0)]\n    val_set_min = min(val_set)\n    val_set_max = max(val_set)\n    val_set_range = val_set_max - val_set_min\n    val_set_range_norm = val_set_range / val_set_len\n    val_set_range_norm_res = val_set_range_norm / res\n    val_set_range_norm_res_min = val_set_min - val_set_range_norm_res\n    val_set_range_norm_res_max = val_set_min + val_set_range_norm_res\n    val_set_range_norm_res_min_norm = (val_set_range_norm_res_min - val_set_min) / val_set_range\n    val_set_range_norm_res_max_norm = (val_set_range_norm_res_max - val_set_min) / val_set_range\n    val_set_range_norm_res_min_norm_res = val_set_range_norm_res_min_norm / res\n    val_set_range_norm_res_max_norm_res = val_set_range_norm_res_max_norm / res\n    val_set_range_norm_res_min_norm_res_min = val_set_range_norm_res_min_norm_res - val_set_range_norm_res_min_norm\n    val_set_range_norm_res_max_norm_res_max = val_set_range_norm_res_max_norm_res - val_set_range_norm_res_max_norm\n    val_set_range_norm_res_min_norm_res_min_norm = val_set_range_norm_res_min_norm_res /", "idx": 142}
{"namespace": "hypertools._shared.helpers.vals2bins", "completion": "    # flatten if list of lists\n    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n\n    # get palette from seaborn\n    ranks = np.digitize(vals, np.linspace(np.min(vals), np.max(vals)+1, res+1)) - 1\n    return ranks\n\n", "idx": 143}
{"namespace": "hypertools._shared.helpers.interp_array", "completion": "    # check if array is a list\n    if not isinstance(arr,list):\n        raise TypeError(\"Input array must be a list\")\n\n    # check if array is a list of lists\n    if any(isinstance(el,list) for el in arr):\n        raise TypeError(\"Input array must be a list of lists\")\n\n    # check if array is a list of lists of lists\n    if any(isinstance(el,list) for el in arr) and any(isinstance(el,list) for el in arr[0]):\n        raise TypeError(\"Input array must be a list of lists of lists\")\n\n    # check if array is a list of lists of lists of lists\n    if any(isinstance(el,list) for el in arr) and any(isinstance(el,list) for el in arr[0]) and any(isinstance(el,list) for el in arr[0][0]):\n        raise TypeError(\"Input array must be a list of lists of lists of lists\")\n\n    # check if array is a list of lists of lists of lists of lists\n    if any(isinstance(el,list) for el in arr) and any(isinstance(el,list) for el in arr[0]) and any(isinstance(el,list) for el in arr[0][0]) and any(isinstance(el,list) for el in arr[0][0][0]):\n        raise TypeError(\"Input array must be a list of lists of lists of lists of lists\")\n\n    # check if array is a list of lists of lists of lists of lists of lists\n    if any(isinstance(el,list) for el in arr) and any(isinstance(el,list) for el in arr[0]) and any(isinstance(el,list) for el in arr[0][0]) and any(isinstance(el,list) for el in arr[0][0][0]) and any(isinstance(el,list) for el in arr[0][0][0][0]):\n        raise TypeError(\"Input array must be a list of lists of lists of lists of lists of lists\")\n\n    # check if array is a list of lists of lists of lists of lists of lists of lists\n    if any(isinstance(el,list) for el in arr) and any(isinstance(el,list) for el in arr[0]) and any(isinstance(el,list) for el", "idx": 144}
{"namespace": "hypertools._shared.helpers.parse_args", "completion": "    if len(x) != len(args):\n        print(\"The length of x and args must be the same\")\n        sys.exit()\n    else:\n        return [tuple(i) for i in zip(x, args)]\n\n", "idx": 145}
{"namespace": "hypertools._shared.helpers.parse_kwargs", "completion": "    # create list of dictionaries\n    out = []\n    for i, item in enumerate(x):\n        tmp = {}\n        for key, val in kwargs.items():\n            if isinstance(val, (tuple, list)):\n                if len(val) == len(x):\n                    tmp[key] = val[i]\n                else:\n                    print('Error: arguments must be a list of the same length as x')\n                    sys.exit(1)\n            else:\n                tmp[key] = val\n        out.append(tmp)\n    return out\n\n", "idx": 146}
{"namespace": "gif_for_cli.utils._get_default_display_mode", "completion": "    if \"truecolor\" in environ.get(\"TERM\", \"\") or \"truecolor\" in environ.get(\"COLORTERM\", \"\"):\n        return \"truecolor\"\n    elif \"256\" in environ.get(\"TERM\", \"\") or \"256\" in environ.get(\"COLORTERM\", \"\"):\n        return \"256fgbg\"\n    else:\n        return \"nocolor\"\n\n", "idx": 147}
{"namespace": "gif_for_cli.utils._pool_type", "completion": "    try:\n        val = int(val)\n    except ValueError:\n        raise argparse.ArgumentTypeError(\"Pool type must be an integer.\")\n    if val <= 0:\n        raise argparse.ArgumentTypeError(\"Pool type must be greater than 0.\")\n    return val\n\n", "idx": 148}
{"namespace": "gif_for_cli.generate.utils.get_avg_for_em", "completion": "    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area is a rectangle.\n    # The area", "idx": 149}
{"namespace": "gif_for_cli.generate.utils.process_input_source", "completion": "    if input_source.startswith('https://tenor.com/view/'):\n        gif_id = input_source.split('/')[-1]\n        gif_url = 'https://tenor.com/view/gifurl/{}'.format(gif_id)\n    else:\n        gif_url = 'https://api.tenor.com/v1/search?q={}&key={}'.format(\n            input_source, api_key)\n\n    return gif_url\n\n", "idx": 150}
{"namespace": "hypertools._shared.helpers.reshape_data", "completion": "    # check if the input data is a list of lists\n    if any(isinstance(el, list) for el in x):\n        x = list(itertools.chain(*x))\n\n    # check if the input data is a list of lists\n    if any(isinstance(el, list) for el in labels):\n        labels = list(itertools.chain(*labels))\n\n    # check if the input data is a list of lists\n    if any(isinstance(el, list) for el in hue):\n        hue = list(itertools.chain(*hue))\n\n    # check if the input data is a list of lists\n    if any(isinstance(el, list) for el in labels):\n        labels = list(itertools.chain(*labels))\n\n    # check if the input data is a list of lists\n    if any(isinstance(el, list) for el in hue):\n        hue = list(itertools.chain(*hue))\n\n    # check if the input data is a list of lists\n    if any(isinstance(el, list) for el in labels):\n        labels = list(itertools.chain(*labels))\n\n    # check if the input data is a list of lists\n    if any(isinstance(el, list) for el in hue):\n        hue = list(itertools.chain(*hue))\n\n    # check if the input data is a list of lists\n    if any(isinstance(el, list) for el in labels):\n        labels = list(itertools.chain(*labels))\n\n    # check if the input data is a list of lists\n    if any(isinstance(el, list) for el in hue):\n        hue = list(itertools.chain(*hue))\n\n    # check if the input data is a list of lists\n    if any(isinstance(el, list) for el in labels):\n        labels = list(itertools.chain(*labels))\n\n    # check if the input data is a list of lists\n    if any(isinstance(el, list) for el in hue):\n        hue = list(itertools.chain(*hue))\n\n    # check if the input data is a list of lists\n    if any(isinstance(el, list) for el in labels):\n        labels = list(itertools.chain(*labels))\n\n    # check if the input data is a list of lists\n    if any(isinstance(el, list) for el in hue):\n        hue = list(", "idx": 151}
{"namespace": "mingus.extra.lilypond.from_Note", "completion": "    if not isinstance(note, Note):\n        raise TypeError(\"Input must be a Note object.\")\n\n    if not process_octaves:\n        return \"\\\\note %s\" % note.name\n\n    if standalone:\n        return \"\\\\relative c' \\\\note %s\" % note.name\n    else:\n        return \"\\\\relative c' \\\\note %s\" % note.name\n\n", "idx": 152}
{"namespace": "mingus.extra.tablature._get_qsize", "completion": "    # Calculate the quarter note size\n    qsize = 0\n    for string in range(tuning.num_strings):\n        for fret in range(tuning.frets[string]):\n            note = tuning.get_Note(string, fret)\n            if note is not None:\n                if qsize < len(str(note)):\n                    qsize = len(str(note))\n    qsize = int(width / qsize)\n    return qsize\n\n", "idx": 153}
{"namespace": "mingus.core.notes.augment", "completion": "    if note[-1] == \"b\":\n        return note[:-1] + \"#\"\n    else:\n        return note + \"#\"\n\n", "idx": 154}
{"namespace": "mingus.core.meter.valid_beat_duration", "completion": "    import math\n    return math.log(duration, 2).is_integer()\n\n", "idx": 155}
{"namespace": "mingus.core.notes.diminish", "completion": "    if note[-1] != \"#\":\n        return note + \"b\"\n    else:\n        return note[:-1]\n\n", "idx": 156}
{"namespace": "mingus.core.intervals.invert", "completion": "    return interval[::-1]\n\n", "idx": 157}
{"namespace": "mingus.core.progressions.parse_string", "completion": "    # Initialize the output variables\n    roman_numeral = \"\"\n    accidentals = 0\n    suffix = \"\"\n\n    # Check for accidentals\n    if progression[0] == \"#\":\n        accidentals = 1\n        progression = progression[1:]\n    elif progression[0] == \"b\":\n        accidentals = -1\n        progression = progression[1:]\n\n    # Check for suffix\n    if progression[-1] in [\"7\", \"m\", \"M\", \"dim\", \"aug\", \"sus\", \"add\", \"9\", \"11\"]:\n        suffix = progression[-1]\n        progression = progression[:-1]\n\n    # Check for roman numeral\n    for i in range(len(numerals)):\n        if progression[: len(numerals[i])] == numerals[i]:\n            roman_numeral = numerals[i]\n            break\n\n    return (roman_numeral, accidentals, suffix)", "idx": 158}
{"namespace": "exodus_bundler.bundling.bytes_to_int", "completion": "    return struct.unpack('>I', bytes.decode('utf-8').encode('utf-8'))[0]\n\n\n", "idx": 159}
{"namespace": "exodus_bundler.templating.render_template", "completion": "    for key, value in context.items():\n        string = string.replace('{{' + key + '}}', value)\n\n    return string\n\n", "idx": 160}
{"namespace": "exodus_bundler.input_parsing.strip_pid_prefix", "completion": "    if line.startswith('[pid '):\n        return line[len('[pid '):]\n    return line\n\n", "idx": 161}
{"namespace": "fs.path.abspath", "completion": "    if isabs(path):\n        return path\n    return \"/\" + path\n\n", "idx": 162}
{"namespace": "fs.path.combine", "completion": "    if path1.endswith(\"/\"):\n        return path1 + path2\n    return path1 + \"/\" + path2\n\n", "idx": 163}
{"namespace": "fs.path.split", "completion": "    head, tail = path.rsplit(\"/\", 1)\n    return head, tail\n\n", "idx": 164}
{"namespace": "fs.path.isparent", "completion": "    _path1 = forcedir(abspath(path1))\n    _path2 = forcedir(abspath(path2))\n    return _path1.startswith(_path2)  # longer one is child\n\n", "idx": 165}
{"namespace": "fs.path.forcedir", "completion": "    if path.endswith(\"/\"):\n        return path\n    return path + \"/\"\n\n", "idx": 166}
{"namespace": "fs.wildcard.match_any", "completion": "    return any(match(pattern, name) for pattern in patterns)\n\n", "idx": 167}
{"namespace": "fs.wildcard.imatch_any", "completion": "    if not patterns:\n        return True\n    return any(imatch(pattern, name) for pattern in patterns)\n\n", "idx": 168}
{"namespace": "wal_e.cmd.parse_boolean_envvar", "completion": "    if val.lower() == 'false':\n        return False\n    elif val.lower() == '0':\n        return False\n    elif val.lower() == 'true':\n        return True\n    elif val.lower() == '1':\n        return True\n    else:\n        raise UserException('Invalid boolean value for environment variable',\n                            'The value of the environment variable must be either \"false\", \"0\", \"true\" or \"1\".')\n\n", "idx": 169}
{"namespace": "wal_e.log_help.get_log_destinations", "completion": "    log_destinations = os.environ.get('WALE_LOG_DESTINATION', 'stderr,syslog')\n    log_destinations = log_destinations.split(',')\n    return log_destinations\n\n", "idx": 170}
{"namespace": "wal_e.log_help.WalELogger._fmt_structured", "completion": "        # Sort the dictionary\n        d = sorted(d.items())\n\n        # Format the dictionary\n        d = ', '.join(['{0}={1}'.format(k, v) for k, v in d])\n\n        # Format the time\n        time = datetime.datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%S.%f-00')\n\n        # Format the pid\n        pid = os.getpid()\n\n        # Format the output\n        output = 'time={0} pid={1} {2}'.format(time, pid, d)\n\n        return output\n", "idx": 171}
{"namespace": "wal_e.tar_partition._fsync_files", "completion": "    for filename in filenames:\n        with open(filename, 'rb') as f:\n            f.flush()\n            os.fsync(f.fileno())\n        os.fsync(os.path.dirname(filename))\n\n", "idx": 172}
{"namespace": "wal_e.blobstore.file.calling_format.Bucket.list", "completion": "        path = os.path.join(\"/\", prefix.strip(\"/\"))\n        file_paths = [os.path.join(path, f) for f in os.listdir(path)]\n        return [FileKey(bucket=self, name=os.path.relpath(f, path)) for f in file_paths]", "idx": 173}
{"namespace": "pyinfra.operations.util.files.unix_path_join", "completion": "    return \"/\".join(path_parts)\n\n", "idx": 174}
{"namespace": "pyinfra.operations.server.shell", "completion": "    if isinstance(commands, str):\n        commands = [commands]\n\n    for command in commands:\n        yield command\n\n", "idx": 175}
{"namespace": "pyinfra.api.util.try_int", "completion": "    try:\n        return int(value)\n    except ValueError:\n        return value\n\n", "idx": 176}
{"namespace": "mrjob.job.MRJob.mr_job_script", "completion": "        # TODO: This is a hack. We should be able to get the path of the script\n        # containing the MRJob class.\n        # TODO: This is a hack. We should be able to get the path of the script\n        # containing the MRJob class.\n        # TODO: This is a hack. We should be able to get the path of the script\n        # containing the MRJob class.\n        # TODO: This is a hack. We should be able to get the path of the script\n        # containing the MRJob class.\n        # TODO: This is a hack. We should be able to get the path of the script\n        # containing the MRJob class.\n        # TODO: This is a hack. We should be able to get the path of the script\n        # containing the MRJob class.\n        # TODO: This is a hack. We should be able to get the path of the script\n        # containing the MRJob class.\n        # TODO: This is a hack. We should be able to get the path of the script\n        # containing the MRJob class.\n        # TODO: This is a hack. We should be able to get the path of the script\n        # containing the MRJob class.\n        # TODO: This is a hack. We should be able to get the path of the script\n        # containing the MRJob class.\n        # TODO: This is a hack. We should be able to get the path of the script\n        # containing the MRJob class.\n        # TODO: This is a hack. We should be able to get the path of the script\n        # containing the MRJob class.\n        # TODO: This is a hack. We should be able to get the path of the script\n        # containing the MRJob class.\n        # TODO: This is a hack. We should be able to get the path of the script\n        # containing the MRJob class.\n        # TODO: This is a hack. We should be able to get the path of the script\n        # containing the MRJob class.\n        # TODO: This is a hack. We should be able to get the path of the script\n        # containing the MRJob class.\n        # TODO: This is a hack. We should be able to get the path of the script\n        # containing the MRJob class.\n        # TODO: This is a hack. We should be able to get the path of the script\n        # containing the", "idx": 177}
{"namespace": "mrjob.compat.map_version", "completion": "    if isinstance(version_map, dict):\n        version_map = version_map.items()\n    version_map = sorted(version_map, key=lambda x: x[0])\n    for version_tuple in version_map:\n        if LooseVersion(version) < version_tuple[0]:\n            return version_map[0][1]\n    return version_map[-1][1]\n\n", "idx": 178}
{"namespace": "mrjob.conf.combine_values", "completion": "    for value in values:\n        if value is not None:\n            return value\n    return None\n\n", "idx": 179}
{"namespace": "mrjob.protocol.BytesProtocol.read", "completion": "        if b'\\t' in line:\n            key, value = line.split(b'\\t', 1)\n        else:\n            key, value = line, None\n        return (key, value)\n", "idx": 180}
{"namespace": "mrjob.protocol.TextProtocol.write", "completion": "        return u'\\t'.join(x for x in (key, value) if x is not None).encode('utf_8')\n\n", "idx": 181}
{"namespace": "mrjob.protocol.TextProtocol.read", "completion": "        try:\n            key_value = line.decode('utf_8').split(b'\\t', 1)\n        except UnicodeDecodeError:\n            key_value = line.decode('latin_1').split(b'\\t', 1)\n\n        if len(key_value) == 1:\n            key_value.append(None)\n\n        return tuple(key_value)\n", "idx": 182}
{"namespace": "mrjob.protocol.TextValueProtocol.read", "completion": "        try:\n            line = line.decode('utf_8')\n        except UnicodeDecodeError:\n            line = line.decode('latin_1')\n\n        return (None, line)\n", "idx": 183}
{"namespace": "mrjob.util.file_ext", "completion": "    filename = filename.lstrip(\".\")\n    return filename[filename.find(\".\"):]\n\n", "idx": 184}
{"namespace": "mrjob.util.cmd_line", "completion": "    return ' '.join(pipes.quote(str(arg)) for arg in args)\n\n", "idx": 185}
{"namespace": "mrjob.util.save_cwd", "completion": "    original_cwd = os.getcwd()\n\n    try:\n        yield\n\n    finally:\n        os.chdir(original_cwd)\n\n", "idx": 186}
{"namespace": "mrjob.util.save_sys_std", "completion": "    original_stdin = sys.stdin\n    original_stdout = sys.stdout\n    original_stderr = sys.stderr\n\n    try:\n        yield\n\n    finally:\n        sys.stdin = original_stdin\n        sys.stdout = original_stdout\n        sys.stderr = original_stderr\n\n", "idx": 187}
{"namespace": "mrjob.util.unarchive", "completion": "    if not os.path.exists(dest):\n        os.makedirs(dest)\n\n    if is_zipfile(archive_path):\n        with ZipFile(archive_path, 'r') as zip_ref:\n            zip_ref.extractall(dest)\n    else:\n        with tarfile.open(archive_path, 'r') as tar_ref:\n            tar_ref.extractall(dest)\n\n", "idx": 188}
{"namespace": "mrjob.util.unique", "completion": "    seen = set()\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            yield item\n\n", "idx": 189}
{"namespace": "mrjob.parse.urlparse", "completion": "    # If the URL has a fragment, split it into a path and a fragment\n    if '#' in urlstring:\n        urlstring, fragment = urlstring.split('#', 1)\n\n    # Parse the URL\n    result = urlparse_buggy(urlstring, scheme, allow_fragments, *args, **kwargs)\n\n    # If the URL had a fragment, add it to the result\n    if fragment:\n        result = result._replace(fragment=fragment)\n\n    return result\n\n", "idx": 190}
{"namespace": "mrjob.util.which", "completion": "    if path is None:\n        path = os.environ['PATH']\n\n    for p in path.split(os.pathsep):\n        p = os.path.join(p, cmd)\n        if os.path.exists(p) and os.access(p, os.X_OK):\n            return p\n\n    return None\n\n", "idx": 191}
{"namespace": "sshuttle.ssh.parse_hostport", "completion": "    if not rhostport:\n        return None\n\n    if rhostport.startswith(\"[\"):\n        rhostport = rhostport[1:-1]\n\n    if rhostport.startswith(\"@\"):\n        rhostport = rhostport[1:]\n\n    if rhostport.startswith(\"~\"):\n        rhostport = rhostport[1:]\n\n    if rhostport.startswith(\"~\"):\n        rhostport = rhostport[1:]\n\n    if rhostport.startswith(\"~\"):\n        rhostport = rhostport[1:]\n\n    if rhostport.startswith(\"~\"):\n        rhostport = rhostport[1:]\n\n    if rhostport.startswith(\"~\"):\n        rhostport = rhostport[1:]\n\n    if rhostport.startswith(\"~\"):\n        rhostport = rhostport[1:]\n\n    if rhostport.startswith(\"~\"):\n        rhostport = rhostport[1:]\n\n    if rhostport.startswith(\"~\"):\n        rhostport = rhostport[1:]\n\n    if rhostport.startswith(\"~\"):\n        rhostport = rhostport[1:]\n\n    if rhostport.startswith(\"~\"):\n        rhostport = rhostport[1:]\n\n    if rhostport.startswith(\"~\"):\n        rhostport = rhostport[1:]\n\n    if rhostport.startswith(\"~\"):\n        rhostport = rhostport[1:]\n\n    if rhostport.startswith(\"~\"):\n        rhostport = rhostport[1:]\n\n    if rhostport.startswith(\"~\"):\n        rhostport = rhostport[1:]\n\n    if rhostport.startswith(\"~\"):\n        rhostport = rhostport[1:]\n\n    if rhostport.startswith(\"~\"):\n        rhostport = rhostport[1:]\n\n    if rhostport.startswith(\"~\"):\n        rhostport = rhostport[1:]\n\n    if rhostport.startswith(\"~\"):\n        rhostport = rhostport[1:]\n\n    if rhostport.startswith(\"~\"):\n        rhostport = rhostport[1:]\n\n    if rhostport.startswith(\"~\"):\n        rhostport = rhostport[1:]\n\n    if rhostport.startswith(\"~\"):\n        rhostport = rhostport[1:]\n\n    if rhostport.startswith", "idx": 192}
{"namespace": "flower.utils.search.stringified_dict_contains_value", "completion": "    if not str_dict:\n        return False\n    try:\n        dict_ = eval(str_dict)\n    except (SyntaxError, ValueError):\n        return False\n    return key in dict_ and dict_[key] == value\n\n", "idx": 193}
{"namespace": "flower.utils.abs_path", "completion": "    if os.path.isabs(path):\n        return path\n    else:\n        return os.path.abspath(path)\n\n", "idx": 194}
{"namespace": "flower.utils.strtobool", "completion": "    val = val.lower()\n    if val in ('y', 'yes', 't', 'true', 'on', '1'):\n        return 1\n    elif val in ('n', 'no', 'f', 'false', 'off', '0'):\n        return 0\n    else:\n        raise ValueError('Not a valid boolean value: %s' % val)", "idx": 195}
{"namespace": "sshuttle.methods.get_method", "completion": "    try:\n        module = __import__(\"sshuttle.methods.%s\" % method_name)\n    except ImportError:\n        raise Fatal(\"fw: Unknown method %s.\" % method_name)\n    return getattr(module, \"Method\")", "idx": 196}
{"namespace": "trailscraper.iam.all_known_iam_permissions", "completion": "    return set(os.path.join(os.path.dirname(os.path.realpath(__file__)), \"known-iam-actions.txt\"))\n\n", "idx": 197}
{"namespace": "trailscraper.cloudtrail.parse_records", "completion": "    return filterz(lambda x: x is not None, map(_parse_record, json_records))", "idx": 198}
{"namespace": "pycoin.satoshi.IntStreamer.IntStreamer.int_to_script_bytes", "completion": "        if v == 0:\n            return b''\n        if v < 0:\n            v = -v\n        s = bytearray()\n        while v > 0:\n            s.append(v & 0xff)\n            v >>= 8\n        return bytes(s)", "idx": 199}
{"namespace": "pycoin.satoshi.stackops.do_OP_2DROP", "completion": "    stack.pop()\n    stack.pop()\n\n", "idx": 200}
{"namespace": "pycoin.satoshi.stackops.do_OP_2DUP", "completion": "    stack.append(stack[-1])\n    stack.append(stack[-1])\n\n", "idx": 201}
{"namespace": "pycoin.satoshi.stackops.do_OP_3DUP", "completion": "    stack.append(stack[-3])\n    stack.append(stack[-3])\n    stack.append(stack[-3])\n\n", "idx": 202}
{"namespace": "trailscraper.s3_download._s3_key_prefixes", "completion": "    delta = to_date - from_date\n    dates = [from_date + datetime.timedelta(days=i) for i in range(delta.days + 1)]\n    prefixes = []\n    for date in dates:\n        for org_id in org_ids:\n            for account_id in account_ids:\n                for region in regions:\n                    prefixes.append(_s3_key_prefix(prefix, date, account_id, region))\n    return prefixes\n\n", "idx": 203}
{"namespace": "pycoin.satoshi.stackops.do_OP_2OVER", "completion": "    stack.append(stack[-4])\n    stack.append(stack[-4])\n\n", "idx": 204}
{"namespace": "pycoin.satoshi.stackops.do_OP_2SWAP", "completion": "    #  (x1 x2 x3 x4 -- x3 x4 x1 x2)\n    stack.append(stack.pop(-4))\n    stack.append(stack.pop(-4))\n    stack.append(stack.pop(-4))\n    stack.append(stack.pop(-4))\n\n", "idx": 205}
{"namespace": "pycoin.satoshi.stackops.do_OP_IFDUP", "completion": "    if stack[-1] != 0:\n        stack.append(stack[-1])\n\n", "idx": 206}
{"namespace": "pycoin.satoshi.stackops.do_OP_NIP", "completion": "    stack.append(stack.pop(-2))\n\n", "idx": 207}
{"namespace": "pycoin.satoshi.stackops.do_OP_TUCK", "completion": "    v1 = stack.pop()\n    v2 = stack.pop()\n    stack.append(v1)\n    stack.append(v2)\n    stack.append(v1)\n\n", "idx": 208}
{"namespace": "pycoin.satoshi.stackops.do_OP_CAT", "completion": "    v1 = stack.pop()\n    v2 = stack.pop()\n    stack.append(v1 + v2)\n\n", "idx": 209}
{"namespace": "pycoin.crack.ecdsa.crack_secret_exponent_from_k", "completion": "    \"\"\"\n    Given a signature of a signed value and a known k, this function returns the secret exponent for RSA.\n    Input-Output Arguments\n    :param generator: The generator.\n    :param signed_value: The signed value.\n    :param sig: The signature.\n    :param k: The known k value.\n    :return: The secret exponent.\n    \"\"\"\n\n    \"\"\"\n    Given a signature of a signed value and a known k, this function returns the secret exponent for RSA.\n    Input-Output Arguments\n    :param generator: The generator.\n    :param signed_value: The signed value.\n    :param sig: The signature.\n    :param k: The known k value.\n    :return: The secret exponent.\n    \"\"\"\n\n    \"\"\"\n    Given a signature of a signed value and a known k, this function returns the secret exponent for RSA.\n    Input-Output Arguments\n    :param generator: The generator.\n    :param signed_value: The signed value.\n    :param sig: The signature.\n    :param k: The known k value.\n    :return: The secret exponent.\n    \"\"\"\n\n    \"\"\"\n    Given a signature of a signed value and a known k, this function returns the secret exponent for RSA.\n    Input-Output Arguments\n    :param generator: The generator.\n    :param signed_value: The signed value.\n    :param sig: The signature.\n    :param k: The known k value.\n    :return: The secret exponent.\n    \"\"\"\n\n    \"\"\"\n    Given a signature of a signed value and a known k, this function returns the secret exponent for RSA.\n    Input-Output Arguments\n    :param generator: The generator.\n    :param signed_value: The signed value.\n    :param sig: The signature.\n    :param k: The known k value.\n    :return: The secret exponent.\n    \"\"\"\n\n    \"\"\"\n    Given a signature of a signed value and a known k, this function returns the secret exponent for RSA.\n    Input-Output Arguments\n    :param generator: The generator.\n    :param signed_value: The signed value.\n    :param sig: The signature.\n    :param k: The known k value.\n    :return: The secret exponent.\n    \"\"\"\n\n    \"\"\"\n    Given a signature of a signed value and a known k, this function returns the secret exponent for RSA.\n    Input-Output Arguments\n    :param generator: The generator.\n    :param signed_value:", "idx": 210}
{"namespace": "pycoin.crack.ecdsa.crack_k_from_sigs", "completion": "    return ((val1 * sig2.inverse(sig1)) * generator.inverse(sig2)) % generator.order()\n\n\n", "idx": 211}
{"namespace": "pycoin.message.make_parser_and_packer.standard_streamer", "completion": "    streamer = Streamer(parsing_functions, parse_satoshi_int)\n    for name, (parse_func, stream_func) in parsing_functions:\n        streamer.register_parsing_function(name, parse_func, stream_func)\n    return streamer\n\n", "idx": 212}
{"namespace": "pycoin.key.subpaths.subpaths_for_path_range", "completion": "    \"\"\"\n    This function returns an iterator of paths based on the given path range. It processes the input path range like the format \"xx/xx/x-x\" and returns an iterator of paths based on the given conditions \"xx/xx/x1, xx/xx/x2\" and so on.\n    Input-Output Arguments\n    :param path_range: String. The input path range.\n    :param hardening_chars: String. The characters that indicate hardening. Defaults to \"'pH\".\n    :return: Iterator. An iterator of paths based on the given path range.\n    \"\"\"\n\n    \"\"\"\n    This function returns an iterator of paths based on the given path range. It processes the input path range like the format \"xx/xx/x-x\" and returns an iterator of paths based on the given conditions \"xx/xx/x1, xx/xx/x2\" and so on.\n    Input-Output Arguments\n    :param path_range: String. The input path range.\n    :param hardening_chars: String. The characters that indicate hardening. Defaults to \"'pH\".\n    :return: Iterator. An iterator of paths based on the given path range.\n    \"\"\"\n\n    \"\"\"\n    This function returns an iterator of paths based on the given path range. It processes the input path range like the format \"xx/xx/x-x\" and returns an iterator of paths based on the given conditions \"xx/xx/x1, xx/xx/x2\" and so on.\n    Input-Output Arguments\n    :param path_range: String. The input path range.\n    :param hardening_chars: String. The characters that indicate hardening. Defaults to \"'pH\".\n    :return: Iterator. An iterator of paths based on the given path range.\n    \"\"\"\n\n    \"\"\"\n    This function returns an iterator of paths based on the given path range. It processes the input path range like the format \"xx/xx/x-x\" and returns an iterator of paths based on the given conditions \"xx/xx/x1, xx/xx/x2\" and so on.\n    Input-Output Arguments\n    :param path_range: String. The input path range.\n    :param hardening_chars: String. The characters that indicate hardening. Defaults to \"'pH\".\n    :return: Iterator. An iterator of paths based on the given path", "idx": 213}
{"namespace": "pyt.core.project_handler._is_python_file", "completion": "    return os.path.splitext(path)[1] == '.py'", "idx": 214}
{"namespace": "pycoin.encoding.hexbytes.h2b", "completion": "    return binascii.unhexlify(h)", "idx": 215}
{"namespace": "zxcvbn.scoring.calc_average_degree", "completion": "    \"\"\"\n    Calculate the average degree of a graph. It iterates through the graph and calculates the average degree based on the number of neighbors for each node.\n    Input-Output Arguments\n    :param graph: Dictionary. The input graph represented as a dictionary where keys are nodes and values are lists of neighboring nodes.\n    :return: Float. The average degree of the graph.\n    \"\"\"\n\n    \"\"\"\n    Calculate the average degree of a graph. It iterates through the graph and calculates the average degree based on the number of neighbors for each node.\n    Input-Output Arguments\n    :param graph: Dictionary. The input graph represented as a dictionary where keys are nodes and values are lists of neighboring nodes.\n    :return: Float. The average degree of the graph.\n    \"\"\"\n\n    \"\"\"\n    Calculate the average degree of a graph. It iterates through the graph and calculates the average degree based on the number of neighbors for each node.\n    Input-Output Arguments\n    :param graph: Dictionary. The input graph represented as a dictionary where keys are nodes and values are lists of neighboring nodes.\n    :return: Float. The average degree of the graph.\n    \"\"\"\n\n    \"\"\"\n    Calculate the average degree of a graph. It iterates through the graph and calculates the average degree based on the number of neighbors for each node.\n    Input-Output Arguments\n    :param graph: Dictionary. The input graph represented as a dictionary where keys are nodes and values are lists of neighboring nodes.\n    :return: Float. The average degree of the graph.\n    \"\"\"\n\n    \"\"\"\n    Calculate the average degree of a graph. It iterates through the graph and calculates the average degree based on the number of neighbors for each node.\n    Input-Output Arguments\n    :param graph: Dictionary. The input graph represented as a dictionary where keys are nodes and values are lists of neighboring nodes.\n    :return: Float. The average degree of the graph.\n    \"\"\"\n\n    \"\"\"\n    Calculate the average degree of a graph. It iterates through the graph and calculates the average degree based on the number of neighbors for each node.\n    Input-Output Arguments\n    :param graph: Dictionary. The input graph represented as a dictionary where keys are nodes and values are lists of neighboring nodes.\n    :return: Float. The average degree of the graph.\n    \"\"\"\n\n    \"\"\"\n    Calculate the average degree of a graph. It iterates through the graph and calculates the", "idx": 216}
{"namespace": "zxcvbn.scoring.nCk", "completion": "    if k > n:\n        return 0\n\n    if k == 0:\n        return 1\n\n    if k == n:\n        return 1\n\n    if k == 1:\n        return n\n\n    return n * nCk(n - 1, k - 1) + nCk(n - 1, k)\n\n", "idx": 217}
{"namespace": "zxcvbn.matching.relevant_l33t_subtable", "completion": "    subtable = {}\n    for char in password:\n        if char in table:\n            subtable[char] = table[char]\n\n    return subtable\n\n", "idx": 218}
{"namespace": "zxcvbn.matching.translate", "completion": "    translated_string = \"\"\n    for char in string:\n        if char in chr_map:\n            translated_string += chr_map[char]\n        else:\n            translated_string += char\n\n    return translated_string\n\n", "idx": 219}
{"namespace": "tools.cgrep.get_nets", "completion": "  results = []\n  for obj in objects:\n    try:\n      nets = db.GetNetParents(obj)\n    except naming.UndefinedAddressError:\n      logging.info('%s is an invalid object', obj)\n    else:\n      results.append((obj, nets))\n  return results\n\n", "idx": 220}
{"namespace": "tools.cgrep.get_ports", "completion": "  results = []\n  for svc in svc_group:\n    svc_obj = db.GetService(svc)\n    for port in svc_obj.ports:\n      results.append((svc, '%s/%s' % (port, svc_obj.protocol)))\n  return results\n\n", "idx": 221}
{"namespace": "tools.cgrep.compare_ip_token", "completion": "  token = options.token\n  ip = options.ip\n  ip = nacaddr.IP(ip)\n  net = db.GetNet(token)\n  results = []\n  for el in net:\n    if ip.version == el.version:\n      if ip.subnet_of(el):\n        results.append(str(el))\n  if results:\n    return 'IP %s is in %s' % (ip, token)\n  else:\n    return 'IP %s is not in %s' % (ip, token)\n\n", "idx": 222}
{"namespace": "tools.cgrep.get_services", "completion": "  port = options.port[0]\n  protocol = options.port[1]\n  results = []\n  for svc in db.GetServiceParents(port, protocol):\n    results.append(svc)\n  return port, protocol, results\n\n", "idx": 223}
{"namespace": "asyncssh.packet.String", "completion": "    if isinstance(value, str):\n        value = value.encode('utf-8')\n\n    return UInt32(len(value)) + value\n\n", "idx": 224}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_cmd_counts", "completion": "    # Laplace smoothing\n    # Add 1 to each of the counts, including the unk_token, to handle unseen commands\n    # Laplace smoothing is used for smoothing individual command counts (seq1_counts) and sequence command counts of length 2 (seq2_counts)\n    # Laplace smoothing is used to smooth the counts of the commands in the training data\n    # Laplace smoothing is used to smooth the counts of the commands in the test data\n    # Laplace smoothing is used to smooth the counts of the commands in the validation data\n    # Laplace smoothing is used to smooth the counts of the commands in the test data\n    # Laplace smoothing is used to smooth the counts of the commands in the validation data\n    # Laplace smoothing is used to smooth the counts of the commands in the test data\n    # Laplace smoothing is used to smooth the counts of the commands in the validation data\n    # Laplace smoothing is used to smooth the counts of the commands in the test data\n    # Laplace smoothing is used to smooth the counts of the commands in the validation data\n    # Laplace smoothing is used to smooth the counts of the commands in the test data\n    # Laplace smoothing is used to smooth the counts of the commands in the validation data\n    # Laplace smoothing is used to smooth the counts of the commands in the test data\n    # Laplace smoothing is used to smooth the counts of the commands in the validation data\n    # Laplace smoothing is used to smooth the counts of the commands in the test data\n    # Laplace smoothing is used to smooth the counts of the commands in the validation data\n    # Laplace smoothing is used to smooth the counts of the commands in the test data\n    # Laplace smoothing is used to smooth the counts of the commands in the validation data\n    # Laplace smoothing is used to smooth the counts of the commands in the test data\n    # Laplace smoothing is used to smooth the counts of the commands in the validation data\n    # Laplace smoothing is used to smooth the counts of the commands in the test data\n    # Laplace smoothing is used to smooth the counts of the commands in the validation data\n    # Laplace smoothing is used to smooth the counts of the commands in the test data\n    # Laplace smoothing is used to smooth the counts of the commands in the validation data\n    # Laplace smoothing is used to smooth the counts of the commands in the test data\n    # Laplace smoothing is used to", "idx": 225}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_param_counts", "completion": "    param_counts_ls = copy.deepcopy(param_counts)\n    cmd_param_counts_ls = copy.deepcopy(cmd_param_counts)\n\n    for cmd in cmds:\n        for param in cmds:\n            if param != unk_token:\n                param_counts_ls[param] += 1\n                cmd_param_counts_ls[cmd][param] += 1\n\n    return param_counts_ls, cmd_param_counts_ls", "idx": 226}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_value_counts", "completion": "    value_counts_ls = copy.deepcopy(value_counts)\n    param_value_counts_ls = copy.deepcopy(param_value_counts)\n\n    values: List[str] = list(value_counts.keys()) + [unk_token]\n    for param in params:\n        for value in values:\n            if value in param_value_counts_ls[param] or value == unk_token:\n                value_counts_ls[value] += 1\n                param_value_counts_ls[param][value] += 1\n\n    return value_counts_ls, param_value_counts_ls", "idx": 227}
{"namespace": "diffprivlib.validation.check_epsilon_delta", "completion": "    if not isinstance(epsilon, Real):\n        raise TypeError(\"Epsilon must be numeric\")\n    if not isinstance(delta, Real):\n        raise TypeError(\"Delta must be numeric\")\n    if epsilon < 0:\n        raise ValueError(\"Epsilon must be non-negative\")\n    if delta < 0 or delta > 1:\n        raise ValueError(\"Delta must be in [0, 1]\")\n    if epsilon == 0 and delta == 0 and not allow_zero:\n        raise ValueError(\"Epsilon and Delta cannot both be zero\")\n    if epsilon == 0 and delta == 0 and allow_zero:\n        warn_unused_args(epsilon=epsilon, delta=delta)\n\n", "idx": 228}
{"namespace": "diffprivlib.utils.check_random_state", "completion": "    if seed is None and not secure:\n        return np.random.RandomState()\n    elif seed is None and secure:\n        return secrets.SystemRandom()\n    elif isinstance(seed, int):\n        return np.random.RandomState(seed)\n    elif isinstance(seed, np.random.RandomState):\n        return seed\n    elif isinstance(seed, secrets.SystemRandom):\n        return seed\n    else:\n        raise ValueError(\"Invalid seed. Seed must be None, int, or instance of RandomState.\")\n\n", "idx": 229}
{"namespace": "diffprivlib.validation.clip_to_norm", "completion": "    if not isinstance(array, np.ndarray):\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\n    if array.ndim != 2:\n        raise ValueError(f\"Input array must be 2-dimensional, got {array.ndim} dimensions.\")\n    if not isinstance(clip, Real):\n        raise TypeError(f\"Clip value must be numeric, got {type(clip)}.\")\n    if clip <= 0:\n        raise ValueError(f\"Clip value must be strictly positive, got {clip}.\")\n\n    norms = np.linalg.norm(array, axis=1)\n    norms = np.clip(norms, 0, clip)\n    norms = np.expand_dims(norms, axis=1)\n    return array / norms\n\n", "idx": 230}
{"namespace": "diffprivlib.models.pca.PCA.fit_transform", "completion": "        self.fit(X, y)\n        return self.transform(X)\n", "idx": 231}
{"namespace": "discord.utils.get_slots", "completion": "    for base in cls.__mro__:\n        for name in base.__slots__:\n            yield name\n\n", "idx": 232}
{"namespace": "discord.utils.is_inside_class", "completion": "    if not hasattr(func, '__qualname__'):\n        return False\n    return '.' in func.__qualname__\n\n", "idx": 233}
{"namespace": "faker.utils.decorators.slugify", "completion": "    @wraps(fn)\n\n    def slugify(fn: Callable) -> Callable:\n        @wraps(fn)\n\n        def slugify(fn: Callable) -> Callable:\n            @wraps(fn)\n\n            def slugify(fn: Callable) -> Callable:\n                @wraps(fn)\n\n                def slugify(fn: Callable) -> Callable:\n                    @wraps(fn)\n\n                    def slugify(fn: Callable) -> Callable:\n                        @wraps(fn)\n\n                        def slugify(fn: Callable) -> Callable:\n                            @wraps(fn)\n\n                            def slugify(fn: Callable) -> Callable:\n                                @wraps(fn)\n\n                                def slugify(fn: Callable) -> Callable:\n                                    @wraps(fn)\n\n                                    def slugify(fn: Callable) -> Callable:\n                                        @wraps(fn)\n\n                                        def slugify(fn: Callable) -> Callable:\n                                            @wraps(fn)\n\n                                            def slugify(fn: Callable) -> Callable:\n                                                @wraps(fn)\n\n                                                def slugify(fn: Callable) -> Callable:\n                                                    @wraps(fn)\n\n                                                    def slugify(fn: Callable) -> Callable:\n                                                        @wraps(fn)\n\n                                                        def slugify(fn: Callable) -> Callable:\n                                                            @wraps(fn)\n\n                                                            def slugify(fn: Callable) -> Callable:\n                                                                @wraps(fn)\n\n                                                                def slugify(fn: Callable) -> Callable:\n                                                                    @wraps(fn)\n\n                                                                    def slugify(fn: Callable) -> Callable:\n                                                                        @wraps(fn)\n\n                                                                        def slugify(fn: Callable) -> Callable:\n                                                                            @wraps(fn)\n\n                                                                            def slugify(fn: Callable) -> Callable:\n                                                                                @wraps(fn)\n\n                                                                                def slugify(fn: Callable) -> Callable:\n                                                                                    @wraps(fn)\n\n                                                                                    def slugify(fn: Callable) -> Callable:\n                                                                                        @wraps(fn)\n\n                                                                                        def slugify(fn: Callable) -> Callable:\n                                                                                            @wraps(fn)\n\n                                                                                            def slugify(fn: Callable) -> Callable:\n                                                                                                @wraps(fn)\n\n                                                                                                def slugify(fn: Callable) -> Callable:\n                                                                                                    @wraps(fn)\n\n                                                                                                    def slugify(fn: Callable) -> Callable:\n                                                                                                       ", "idx": 234}
{"namespace": "faker.utils.decorators.slugify_domain", "completion": "    @wraps(fn)\n    def wrapper(*args: Tuple[T, ...], **kwargs: Dict[str, T]) -> str:\n        return text.slugify(fn(*args, **kwargs), allow_dots=True)\n\n    return wrapper", "idx": 235}
{"namespace": "faker.utils.decorators.slugify_unicode", "completion": "    def wrapper(*args: Tuple[T, ...], **kwargs: Dict[str, T]) -> str:\n        return text.slugify(fn(*args, **kwargs), allow_dots=True)\n\n    return wrapper\n\n", "idx": 236}
{"namespace": "faker.utils.loading.get_path", "completion": "    \"\"\"\n    Get the path of the given module. It first checks if the system is frozen. If it is, it checks if it is frozen by PyInstaller or others and then returns the path accordingly. If the system is not frozen, it returns the path of the module. If the file is None, it raises RuntimeError(f\"Can't find path from module `{module}.\").\n    Input-Output Arguments\n    :param module: ModuleType. The module for which the path is to be found.\n    :return: str. The path of the given module.\n    \"\"\"\n\n    \"\"\"\n    Get the path of the given module. It first checks if the system is frozen. If it is, it checks if it is frozen by PyInstaller or others and then returns the path accordingly. If the system is not frozen, it returns the path of the module. If the file is None, it raises RuntimeError(f\"Can't find path from module `{module}.\").\n    Input-Output Arguments\n    :param module: ModuleType. The module for which the path is to be found.\n    :return: str. The path of the given module.\n    \"\"\"\n\n    \"\"\"\n    Get the path of the given module. It first checks if the system is frozen. If it is, it checks if it is frozen by PyInstaller or others and then returns the path accordingly. If the system is not frozen, it returns the path of the module. If the file is None, it raises RuntimeError(f\"Can't find path from module `{module}.\").\n    Input-Output Arguments\n    :param module: ModuleType. The module for which the path is to be found.\n    :return: str. The path of the given module.\n    \"\"\"\n\n    \"\"\"\n    Get the path of the given module. It first checks if the system is frozen. If it is, it checks if it is frozen by PyInstaller or others and then returns the path accordingly. If the system is not frozen, it returns the path of the module. If the file is None, it raises RuntimeError(f\"Can't find path from module `{module}.\").\n    Input-Output Arguments\n    :param module: ModuleType. The module for which the path is to be found.\n    :return: str. The path of the given module.\n    \"\"\"\n\n    \"\"\"\n    Get the path of the given module. It first checks if the system is frozen. If it is, it checks if it", "idx": 237}
{"namespace": "faker.utils.checksums.luhn_checksum", "completion": "    \"\"\"\n    Calculate the Luhn checksum for the given number. The Luhn algorithm is used to validate a variety of identification numbers, such as credit card numbers, IMEI numbers, National Provider Identifier numbers in the United The Luhn algorithm is used to validate a variety of identification numbers, such as credit card numbers, IMEI numbers, National Provider Identifier numbers in the United States, and Canadian Social Insurance Numbers.\n    Input-Output Arguments\n    :param number: float. The number for which the Luhn checksum needs to be calculated.\n    :return: int. The Luhn checksum for the given number.\n    \"\"\"\n\n    \"\"\"\n    Calculate the Luhn checksum for the given number. The Luhn algorithm is used to validate a variety of identification numbers, such as credit card numbers, IMEI numbers, National Provider Identifier numbers in the United States, and Canadian Social Insurance Numbers.\n    Input-Output Arguments\n    :param number: float. The number for which the Luhn checksum needs to be calculated.\n    :return: int. The Luhn checksum for the given number.\n    \"\"\"\n\n    \"\"\"\n    Calculate the Luhn checksum for the given number. The Luhn algorithm is used to validate a variety of identification numbers, such as credit card numbers, IMEI numbers, National Provider Identifier numbers in the United States, and Canadian Social Insurance Numbers.\n    Input-Output Arguments\n    :param number: float. The number for which the Luhn checksum needs to be calculated.\n    :return: int. The Luhn checksum for the given number.\n    \"\"\"\n\n    \"\"\"\n    Calculate the Luhn checksum for the given number. The Luhn algorithm is used to validate a variety of identification numbers, such as credit card numbers, IMEI numbers, National Provider Identifier numbers in the United States, and Canadian Social Insurance Numbers.\n    Input-Output Arguments\n    :param number: float. The number for which the Luhn checksum needs to be calculated.\n    :return: int. The Luhn checksum for the given number.\n    \"\"\"\n\n    \"\"\"\n    Calculate the Luhn checksum for the given number. The Luhn algorithm is used to validate a variety of identification numbers, such as credit card numbers, IMEI numbers, National Provider Identifier numbers in the United States, and Canadian Social Insurance Numbers.\n    Input-Output Arguments\n    :param number: float. The number for", "idx": 238}
{"namespace": "faker.utils.datasets.add_ordereddicts", "completion": "    \"\"\"\n    This function takes multiple ordered dictionaries and combines them into a single ordered dictionary. It first extracts the items from each input ordered dictionary and then combines them into a single ordered dictionary.\n    Input-Output Arguments\n    :param odicts: OrderedDictType. Multiple ordered dictionaries to be combined.\n    :return: OrderedDictType. The combined ordered dictionary.\n    \"\"\"\n\n    \"\"\"\n    This function takes multiple ordered dictionaries and combines them into a single ordered dictionary. It first extracts the items from each input ordered dictionary and then combines them into a single ordered dictionary.\n    Input-Output Arguments\n    :param odicts: OrderedDictType. Multiple ordered dictionaries to be combined.\n    :return: OrderedDictType. The combined ordered dictionary.\n    \"\"\"\n\n    \"\"\"\n    This function takes multiple ordered dictionaries and combines them into a single ordered dictionary. It first extracts the items from each input ordered dictionary and then combines them into a single ordered dictionary.\n    Input-Output Arguments\n    :param odicts: OrderedDictType. Multiple ordered dictionaries to be combined.\n    :return: OrderedDictType. The combined ordered dictionary.\n    \"\"\"\n\n    \"\"\"\n    This function takes multiple ordered dictionaries and combines them into a single ordered dictionary. It first extracts the items from each input ordered dictionary and then combines them into a single ordered dictionary.\n    Input-Output Arguments\n    :param odicts: OrderedDictType. Multiple ordered dictionaries to be combined.\n    :return: OrderedDictType. The combined ordered dictionary.\n    \"\"\"\n\n    \"\"\"\n    This function takes multiple ordered dictionaries and combines them into a single ordered dictionary. It first extracts the items from each input ordered dictionary and then combines them into a single ordered dictionary.\n    Input-Output Arguments\n    :param odicts: OrderedDictType. Multiple ordered dictionaries to be combined.\n    :return: OrderedDictType. The combined ordered dictionary.\n    \"\"\"\n\n    \"\"\"\n    This function takes multiple ordered dictionaries and combines them into a single ordered dictionary. It first extracts the items from each input ordered dictionary and then combines them into a single ordered dictionary.\n    Input-Output Arguments\n    :param odicts: OrderedDictType. Multiple ordered dictionaries to be combined.\n    :return: OrderedDictType. The combined ordered dictionary.\n    \"\"\"\n\n    \"\"\"\n    This function takes multiple ordered dictionaries and combines them into a single ordered dictionary. It first extracts the items from each input ordered dictionary and then combines them into a single ordered dictionary.\n    Input-", "idx": 239}
{"namespace": "faker.providers.person.pl_PL.checksum_identity_card_number", "completion": "    # Calculate the control digit\n    control_digit = 0\n    for i, character in enumerate(characters):\n        if isinstance(character, str):\n            control_digit += int(character) * (7 - i % 8)\n        else:\n            control_digit += character * (7 - i % 8)\n\n    # Return the control digit\n    return control_digit % 10\n\n", "idx": 240}
{"namespace": "faker.providers.company.pl_PL.regon_checksum", "completion": "    # Calculate the control digit.\n    control_digit = 0\n    for i in range(len(digits)):\n        control_digit += digits[i] * (i + 1)\n\n    # Calculate the remainder.\n    remainder = control_digit % 11\n\n    # Calculate the control digit.\n    if remainder == 10:\n        control_digit = 0\n    else:\n        control_digit = 11 - remainder\n\n    return control_digit", "idx": 241}
{"namespace": "faker.providers.company.ru_RU.calculate_checksum", "completion": "    # Convert the input value to a list of integers.\n    value_list = [int(x) for x in str(value)]\n\n    # Calculate the checksum value.\n    checksum_value = 0\n    for i in range(len(value_list)):\n        checksum_value += value_list[i] * CompanyProvider.factors[i]\n\n    # Return the calculated checksum value.\n    return str(checksum_value)", "idx": 242}
{"namespace": "faker.providers.company.pl_PL.local_regon_checksum", "completion": "    weights_for_check_digit = [2, 4, 8, 5, 0, 9, 7, 3, 6, 1, 2, 4, 8]\n    check_digit = 0\n\n    for i in range(0, 12):\n        check_digit += weights_for_check_digit[i] * digits[i]\n\n    check_digit %= 11\n\n    if check_digit == 10:\n        check_digit = 0\n\n    return check_digit", "idx": 243}
{"namespace": "faker.providers.company.pl_PL.company_vat_checksum", "completion": "    weights_for_check_digit = [6, 5, 7, 2, 3, 4, 5, 6, 7]\n    check_digit = 0\n\n    for i in range(0, 9):\n        check_digit += weights_for_check_digit[i] * digits[i]\n\n    check_digit %= 11\n\n    if check_digit == 10:\n        check_digit = 0\n\n    return check_digit\n\n", "idx": 244}
{"namespace": "faker.providers.company.pt_BR.company_id_checksum", "completion": "    # Calculate the checksum based on the weights and digits.\n    checksum = 0\n    for i in range(len(digits)):\n        checksum += digits[i] * (i + 1)\n\n    # Calculate the checksum digits.\n    checksum_digits = []\n    while checksum > 0:\n        checksum_digits.append(checksum % 10)\n        checksum = checksum // 10\n\n    # Return the calculated checksum digits.\n    return checksum_digits", "idx": 245}
{"namespace": "faker.providers.misc.Provider.binary", "completion": "        return self.generator.random.getrandbits(length).to_bytes(length, byteorder=\"big\")\n", "idx": 246}
{"namespace": "faker.providers.python.Provider.pystr", "completion": "        if min_chars is None:\n            min_chars = 0\n        if min_chars < 0:\n            raise ValueError(\"Invalid `min_chars` value: must be greater than or equal to 0\")\n        if max_chars < min_chars:\n            raise ValueError(\"Invalid `max_chars` value: must be greater than or equal to `min_chars`\")\n\n        return prefix + self.pystr_upper_lower_case(min_chars, max_chars) + suffix\n", "idx": 247}
{"namespace": "dash._utils.AttributeDict.set_read_only", "completion": "        for name in names:\n            if name in self._read_only:\n                self._read_only[name] = msg\n            else:\n                self._read_only[name] = msg\n", "idx": 248}
{"namespace": "dash._utils.AttributeDict.first", "completion": "        for name in names:\n            if name in self:\n                return self[name]\n        return next(iter(self.values()))\n\n", "idx": 249}
{"namespace": "dash._get_paths.app_get_asset_url", "completion": "    if config.assets_external_path:\n        return config.assets_external_path + path\n    else:\n        return config.requests_pathname_prefix + path\n\n", "idx": 250}
{"namespace": "peewee.sort_models", "completion": "    # Create a dictionary of models and their dependencies.\n    model_dependencies = {}\n    for model in models:\n        model_dependencies[model] = set(model.dependencies(search_nullable=True))\n\n    # Create a list of sorted models.\n    sorted_models = []\n\n    # Perform a depth-first search to sort the models based on their dependencies.\n    while model_dependencies:\n        # Find a model with no dependencies.\n        for model in model_dependencies:\n            if not model_dependencies[model]:\n                break\n        else:\n            raise ValueError('Circular dependency detected in models.')\n\n        # Remove the model from the list of models with dependencies.\n        model_dependencies.pop(model)\n\n        # Add the model to the list of sorted models.\n        sorted_models.append(model)\n\n        # Remove the model from the dependencies of other models.\n        for other_model in model_dependencies:\n            model_dependencies[other_model].discard(model)\n\n    return sorted_models\n\n", "idx": 251}
{"namespace": "dash._grouping.grouping_len", "completion": "    return len(flatten_grouping(grouping))\n\n", "idx": 252}
{"namespace": "playhouse.kv.KeyValue.get", "completion": "        if key in self:\n            return self[key]\n        return default\n", "idx": 253}
{"namespace": "playhouse.kv.KeyValue.setdefault", "completion": "        if key not in self:\n            self[key] = default\n        return self[key]\n", "idx": 254}
{"namespace": "sslyze.plugins.certificate_info._certificate_utils.get_public_key_sha256", "completion": "    return sha256(certificate.public_key().public_bytes(Encoding.DER, PublicFormat.SubjectPublicKeyInfo)).digest()  # type: ignore\n\n", "idx": 255}
{"namespace": "ydata_profiling.compare_reports._compare_title", "completion": "    if len(set(titles)) == 1:\n        return titles[0]\n    else:\n        return f\"{titles[0]} vs {titles[1]}\"\n\n", "idx": 256}
{"namespace": "ydata_profiling.report.formatters.fmt_bytesize", "completion": "    for unit in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\", \"E\", \"Z\"]:\n        if abs(num) < 1024.0:\n            return f\"{num:3.1f}{unit}{suffix}\"\n        num /= 1024.0\n    return f\"{num:.1f}Y{suffix}\"\n\n", "idx": 257}
{"namespace": "ydata_profiling.report.formatters.fmt_percent", "completion": "    if edge_cases:\n        if value == 0:\n            return \"0%\"\n        elif value == 1:\n            return \"100%\"\n        elif value == -1:\n            return \"-100%\"\n\n    return f\"{value * 100:.1f}%\"\n\n", "idx": 258}
{"namespace": "ydata_profiling.report.formatters.fmt_numeric", "completion": "    if isinstance(value, (int, float)):\n        return f\"{value:.{precision}f}\"\n    else:\n        return str(value)\n\n", "idx": 259}
{"namespace": "ydata_profiling.report.formatters.fmt_array", "completion": "    if value.size == 0:\n        return \"[]\"\n    if value.size == 1:\n        return f\"[{value[0]}]\"\n    if value.size > 1:\n        if np.isnan(threshold):\n            return f\"[{', '.join(map(str, value))}]\"\n        else:\n            if np.any(value > threshold):\n                return f\"[{', '.join(map(str, value[:3]))}, ...]\"\n            else:\n                return f\"[{', '.join(map(str, value))}]\"\n\n", "idx": 260}
{"namespace": "ydata_profiling.report.formatters.fmt_monotonic", "completion": "    if value == 0:\n        return \"0\"\n    elif value > 0:\n        return f\"+{value}\"\n    else:\n        return f\"{value}\"\n\n", "idx": 261}
{"namespace": "ydata_profiling.visualisation.plot._plot_pie_chart", "completion": "    # Use the pd.Series indices as category names\n    labels = data.index.values.astype(str)\n\n    # Plot\n    _, ax = plt.subplots(figsize=(7, 2))\n    ax.axis(\"off\")\n\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0.4, 1.6)\n\n    # Plot the pie chart\n    ax.pie(\n        data,\n        labels=labels,\n        colors=colors,\n        startangle=90,\n        counterclock=False,\n        wedgeprops={\"edgecolor\": \"black\"},\n    )\n\n    legend = None\n    if not hide_legend:\n        legend = ax.legend(\n            ncol=1, bbox_to_anchor=(0, 0), fontsize=\"xx-large\", loc=\"upper left\"\n        )\n\n    return ax, legend\n\n", "idx": 262}
{"namespace": "ydata_profiling.visualisation.plot._prepare_heatmap_data", "completion": "    # Sort the dataframe by the specified column(s)\n    if sortby is not None:\n        if isinstance(sortby, str):\n            dataframe = dataframe.sort_values(by=sortby)\n        else:\n            dataframe = dataframe.sort_values(by=sortby, ascending=False)\n\n    # Select the top entities\n    if selected_entities is not None:\n        dataframe = dataframe[dataframe[entity_column].isin(selected_entities)]\n    else:\n        dataframe = dataframe.head(max_entities)\n\n    # Reset the index\n    dataframe = dataframe.reset_index(drop=True)\n\n    return dataframe\n\n", "idx": 263}
{"namespace": "ydata_profiling.visualisation.plot._create_timeseries_heatmap", "completion": "    # Create the heatmap\n    heatmap = sns.heatmap(\n        df,\n        cmap=color,\n        linewidths=0.5,\n        linecolor=\"white\",\n        cbar=False,\n        square=True,\n        xticklabels=True,\n        yticklabels=True,\n    )\n\n    # Set the figure size\n    heatmap.figure.set_size_inches(figsize)\n\n    # Set the title\n    heatmap.set_title(\"Timeseries Heatmap\", fontsize=16)\n\n    # Set the x-axis label\n    heatmap.set_xlabel(\"Time\", fontsize=14)\n\n    # Set the y-axis label\n    heatmap.set_ylabel(\"Entity\", fontsize=14)\n\n    # Set the font size of the x-axis tick labels\n    heatmap.xaxis.set_tick_params(labelsize=12)\n\n    # Set the font size of the y-axis tick labels\n    heatmap.yaxis.set_tick_params(labelsize=12)\n\n    # Set the font size of the x-axis label\n    heatmap.xaxis.label.set_size(14)\n\n    # Set the font size of the y-axis label\n    heatmap.yaxis.label.set_size(14)\n\n    # Set the font size of the title\n    heatmap.title.set_size(16)\n\n    # Set the font size of the colorbar\n    heatmap.cax.set_fontsize(12)\n\n    # Set the font size of the colorbar label\n    heatmap.cax.set_ylabel(\"Count\", fontsize=14)\n\n    # Set the font size of the colorbar tick labels\n    heatmap.cax.tick_params(labelsize=12)\n\n    # Set the font size of the colorbar label\n    heatmap.cax.label.set_size(14)\n\n    # Set the font size of the colorbar title\n    heatmap.cax.title.set_size(16)\n\n    # Set the font size of the colorbar tick labels\n    heatmap.cax.tick_params(labelsize=12)\n\n    # Set the font size of the colorbar tick labels\n    heatmap.cax.tick_params(labelsize=12)\n\n    # Set the font size of the colorbar tick labels\n    heatmap.", "idx": 264}
{"namespace": "ydata_profiling.model.expectation_algorithms.generic_expectations", "completion": "    \"\"\"\n    This function defines generic expectations for a given column in a batch. It checks if the column exists, if there are any missing values, and if all values are unique.\n    Input-Output Arguments\n    :param name: str. The name of the column to be checked.\n    :param summary: dict. A summary of the column statistics.\n    :param batch: Any. The batch of data to be checked.\n    :param *args: Additional arguments.\n    :return: Tuple. A tuple containing the name of the column, its summary, and the batch.\n    \"\"\"\n\n    \"\"\"\n    This function defines generic expectations for a given column in a batch. It checks if the column exists, if there are any missing values, and if all values are unique.\n    Input-Output Arguments\n    :param name: str. The name of the column to be checked.\n    :param summary: dict. A summary of the column statistics.\n    :param batch: Any. The batch of data to be checked.\n    :param *args: Additional arguments.\n    :return: Tuple. A tuple containing the name of the column, its summary, and the batch.\n    \"\"\"\n\n    \"\"\"\n    This function defines generic expectations for a given column in a batch. It checks if the column exists, if there are any missing values, and if all values are unique.\n    Input-Output Arguments\n    :param name: str. The name of the column to be checked.\n    :param summary: dict. A summary of the column statistics.\n    :param batch: Any. The batch of data to be checked.\n    :param *args: Additional arguments.\n    :return: Tuple. A tuple containing the name of the column, its summary, and the batch.\n    \"\"\"\n\n    \"\"\"\n    This function defines generic expectations for a given column in a batch. It checks if the column exists, if there are any missing values, and if all values are unique.\n    Input-Output Arguments\n    :param name: str. The name of the column to be checked.\n    :param summary: dict. A summary of the column statistics.\n    :param batch: Any. The batch of data to be checked.\n    :param *args: Additional arguments.\n    :return: Tuple. A tuple containing the name of the column, its summary, and the batch.\n    \"\"\"\n\n    \"\"\"\n    This function defines generic expectations for a given column in a batch. It checks if the", "idx": 265}
{"namespace": "ydata_profiling.model.expectation_algorithms.numeric_expectations", "completion": "    if summary[\"p_missing\"] > 0.0:\n        batch.expect_column_values_to_not_be_null(name)\n\n    if summary[\"p_unique\"] < 1.0:\n        batch.expect_column_values_to_be_unique(name)\n\n    if summary[\"p_zero\"] > 0.0:\n        batch.expect_column_values_to_not_be_zero(name)\n\n    if summary[\"p_inf\"] > 0.0:\n        batch.expect_column_values_to_not_be_infinite(name)\n\n    if summary[\"p_nan\"] > 0.0:\n        batch.expect_column_values_to_not_be_nan(name)\n\n    if summary[\"p_gt\"] > 0.0:\n        batch.expect_column_values_to_be_greater_than(name, summary[\"p_gt\"])\n\n    if summary[\"p_lt\"] > 0.0:\n        batch.expect_column_values_to_be_less_than(name, summary[\"p_lt\"])\n\n    if summary[\"p_ge\"] > 0.0:\n        batch.expect_column_values_to_be_greater_than_or_equal_to(name, summary[\"p_ge\"])\n\n    if summary[\"p_le\"] > 0.0:\n        batch.expect_column_values_to_be_less_than_or_equal_to(name, summary[\"p_le\"])\n\n    if summary[\"p_eq\"] > 0.0:\n        batch.expect_column_values_to_be_equal_to(name, summary[\"p_eq\"])\n\n    if summary[\"p_ne\"] > 0.0:\n        batch.expect_column_values_to_not_be_equal_to(name, summary[\"p_ne\"])\n\n    if summary[\"p_between\"] > 0.0:\n        batch.expect_column_values_to_be_between(name, summary[\"p_between\"])\n\n    if summary[\"p_not_between\"] > 0.0:\n        batch.expect_column_values_to_not_be_between(name, summary[\"p_not_between\"])\n\n    if summary[\"p_in_set", "idx": 266}
{"namespace": "ydata_profiling.model.expectation_algorithms.categorical_expectations", "completion": "    if summary[\"n_distinct\"] > 1000:\n        batch.expect_column_values_to_be_in_set(\n            name,\n            set(summary[\"value_counts\"].keys()),\n            mostly=summary[\"p_distinct\"],\n        )\n\n    return name, summary, batch\n\n", "idx": 267}
{"namespace": "ydata_profiling.model.expectation_algorithms.datetime_expectations", "completion": "    # Check if the \"min\" and \"max\" keys are present in the summary\n    if \"min\" in summary and \"max\" in summary:\n        # Set the expectations for the datetime values in the batch\n        batch.expect_column_values_to_be_between(\n            name, min_value=summary[\"min\"], max_value=summary[\"max\"]\n        )\n\n    return name, summary, batch\n\n", "idx": 268}
{"namespace": "ydata_profiling.model.expectation_algorithms.file_expectations", "completion": "    return name, summary, batch\n\n", "idx": 269}
{"namespace": "ydata_profiling.model.pandas.describe_categorical_pandas.word_summary_vc", "completion": "    # TODO: add stop words\n    # TODO: add n_words_total\n    # TODO: add n_words_distinct\n    # TODO: add n_words_with_digits\n    # TODO: add n_words_with_punctuation\n    # TODO: add n_words_with_symbols\n    # TODO: add n_words_with_whitespace\n    # TODO: add n_words_with_non_ascii\n    # TODO: add n_words_with_non_ascii_punctuation\n    # TODO: add n_words_with_non_ascii_symbols\n    # TODO: add n_words_with_non_ascii_whitespace\n    # TODO: add n_words_with_non_ascii_punctuation_symbols\n    # TODO: add n_words_with_non_ascii_punctuation_whitespace\n    # TODO: add n_words_with_non_ascii_symbols_whitespace\n    # TODO: add n_words_with_non_ascii_punctuation_symbols_whitespace\n    # TODO: add n_words_with_non_ascii_punctuation_symbols_whitespace\n    # TODO: add n_words_with_non_ascii_punctuation_symbols_whitespace\n    # TODO: add n_words_with_non_ascii_punctuation_symbols_whitespace\n    # TODO: add n_words_with_non_ascii_punctuation_symbols_whitespace\n    # TODO: add n_words_with_non_ascii_punctuation_symbols_whitespace\n    # TODO: add n_words_with_non_ascii_punctuation_symbols_whitespace\n    # TODO: add n_words_with_non_ascii_punctuation_symbols_whitespace\n    # TODO: add n_words_with_non_ascii_punctuation_symbols_whitespace\n    # TODO: add n_words_with_non_ascii_punctuation_symbols_whitespace\n    # TODO: add n_words_with_non_ascii_punctuation_symbols_whitespace\n    # TODO: add n_words_with_non_ascii_punctuation_symbols_whitespace\n    # TODO: add n_words_with_non_ascii_punctuation_symbols_whitespace\n    # TODO: add n_words_with_non_ascii_punctuation_symbols_whitespace\n    # TODO: add n_words_with_", "idx": 270}
{"namespace": "ydata_profiling.model.pandas.imbalance_pandas.column_imbalance_score", "completion": "    # Calculate entropy\n    entropy_value = entropy(value_counts)\n\n    # Calculate score\n    score = 1 - entropy_value / log2(n_classes)\n\n    return score", "idx": 271}
{"namespace": "django.core.exceptions.ValidationError.messages", "completion": "        if hasattr(self, \"error_dict\"):\n            return sum(self.error_dict.values(), [])\n        else:\n            return self.error_list\n", "idx": 272}
{"namespace": "django.utils.module_loading.module_has_submodule", "completion": "    try:\n        importlib.import_module(package)\n    except ImportError:\n        return False\n    try:\n        importlib.import_module(package + \".\" + module_name)\n    except ImportError:\n        return False\n    return True\n\n", "idx": 273}
{"namespace": "django.utils.timezone.get_fixed_timezone", "completion": "    if isinstance(offset, timedelta):\n        offset = offset.total_seconds() / 60\n\n    return timezone(timedelta(minutes=offset), \"UTC\")\n\n", "idx": 274}
{"namespace": "django.utils.encoding.filepath_to_uri", "completion": "    # The list of safe characters here is constructed from the \"reserved\" and\n    # \"unreserved\" characters specified in RFC 3986 Sections 2.2 and 2.3:\n    #     reserved    = gen-delims / sub-delims\n    #     gen-delims  = \":\" / \"/\" / \"?\" / \"#\" / \"[\" / \"]\" / \"@\"\n    #     sub-delims  = \"!\" / \"$\" / \"&\" / \"'\" / \"(\" / \")\"\n    #                   / \"*\" / \"+\" / \",\" / \";\" / \"=\"\n    #     unreserved  = ALPHA / DIGIT / \"-\" / \".\" / \"_\" / \"~\"\n    # Of the unreserved characters, urllib.parse.quote() already considers all\n    # but the ~ safe.\n    # The % character is also added to the list of safe characters here, as the\n    # end of RFC 3987 Section 3.1 specifically mentions that % must not be\n    # converted.\n    if path is None:\n        return path\n    elif isinstance(path, Promise):\n        path = str(path)\n    return quote(path, safe=\"/#%[]=:;$&()+,!?*@'~\")\n\n", "idx": 275}
{"namespace": "django.utils._os.to_path", "completion": "    if isinstance(value, Path):\n        return value\n    elif isinstance(value, str):\n        return Path(value)\n    else:\n        raise TypeError(\n            \"The value must be a string or a pathlib.Path instance, not {}.\".format(\n                type(value)\n            )\n        )", "idx": 276}
{"namespace": "django.utils.lorem_ipsum.sentence", "completion": "    sentence = \"\"\n    sentence += WORDS[random.randint(0, len(WORDS) - 1)].capitalize()\n    sentence += \" \"\n    sentence += WORDS[random.randint(0, len(WORDS) - 1)]\n    sentence += \" \"\n    sentence += WORDS[random.randint(0, len(WORDS) - 1)]\n    sentence += \" \"\n    sentence += WORDS[random.randint(0, len(WORDS) - 1)]\n    sentence += \" \"\n    sentence += WORDS[random.randint(0, len(WORDS) - 1)]\n    sentence += \" \"\n    sentence += WORDS[random.randint(0, len(WORDS) - 1)]\n    sentence += \" \"\n    sentence += WORDS[random.randint(0, len(WORDS) - 1)]\n    sentence += \" \"\n    sentence += WORDS[random.randint(0, len(WORDS) - 1)]\n    sentence += \" \"\n    sentence += WORDS[random.randint(0, len(WORDS) - 1)]\n    sentence += \" \"\n    sentence += WORDS[random.randint(0, len(WORDS) - 1)]\n    sentence += \" \"\n    sentence += WORDS[random.randint(0, len(WORDS) - 1)]\n    sentence += \" \"\n    sentence += WORDS[random.randint(0, len(WORDS) - 1)]\n    sentence += \" \"\n    sentence += WORDS[random.randint(0, len(WORDS) - 1)]\n    sentence += \" \"\n    sentence += WORDS[random.randint(0, len(WORDS) - 1)]\n    sentence += \" \"\n    sentence += WORDS[random.randint(0, len(WORDS) - 1)]\n    sentence += \" \"\n    sentence += WORDS[random.randint(0, len(WORDS) - 1)]\n    sentence += \" \"\n    sentence += WORDS[random.randint(0, len(WORDS) - 1)]\n    sentence += \" \"\n    sentence += WORDS[random.randint(0, len(WORDS) - 1)]\n    sentence += \" \"\n    sentence += WORDS[random.randint(0, len(WORDS) - 1)]\n    sentence += \" \"\n    sentence += WORDS[random.randint(0, len(WORDS) - 1", "idx": 277}
{"namespace": "ydata_profiling.utils.dataframe.sort_column_names", "completion": "    if sort is None:\n        return dct\n\n    if sort == \"ascending\":\n        return {k: v for k, v in sorted(dct.items(), key=lambda item: item[0])}\n    elif sort == \"descending\":\n        return {k: v for k, v in sorted(dct.items(), key=lambda item: item[0], reverse=True)}\n    else:\n        raise ValueError(f\"Invalid sort value: {sort}. Valid values are 'ascending', 'descending' or None.\")", "idx": 278}
{"namespace": "django.utils.ipv6.is_valid_ipv6_address", "completion": "    try:\n        ipaddress.IPv6Address(ip_str)\n        return True\n    except ValueError:\n        return False", "idx": 279}
{"namespace": "django.utils.http.urlsafe_base64_decode", "completion": "    if isinstance(s, bytes):\n        s = s.decode(\"ascii\")\n    if s[-2:] == \"==\":\n        s = s + \"==\"\n    return base64.urlsafe_b64decode(s)\n\n", "idx": 280}
{"namespace": "django.utils.http.parse_etags", "completion": "    if etag_str == \"*\":\n        return [\"*\"]\n    elif etag_str == \"\":\n        return []\n    else:\n        return [\n            quote(etag, safe=RFC3986_GENDELIMS + RFC3986_SUBDELIMS)\n            for etag in etag_str.split(\",\")\n        ]\n\n", "idx": 281}
{"namespace": "django.utils.http.is_same_domain", "completion": "    if pattern == \"*\":\n        return True\n    if pattern.startswith(\"*.\"):\n        return host.endswith(pattern[1:])\n    if pattern.endswith(\".*\"):\n        return host.startswith(pattern[:-1])\n    return host == pattern\n\n", "idx": 282}
{"namespace": "django.utils.http.content_disposition_header", "completion": "    if as_attachment:\n        disposition = \"attachment\"\n    else:\n        disposition = \"inline\"\n\n    if filename:\n        filename = quote(filename)\n        return f\"{disposition}; filename={filename}\"\n    else:\n        return disposition\n\n", "idx": 283}
{"namespace": "pysnooper.utils.truncate", "completion": "    if len(string) <= max_length:\n        return string\n    else:\n        return '...' + string[-max_length + 3:]\n\n", "idx": 284}
{"namespace": "pysnooper.variables.needs_parentheses", "completion": "    bytecode = utils.get_bytecode(source)\n    bytecode_with_parentheses = utils.get_bytecode(source, with_parentheses=True)\n    return bytecode != bytecode_with_parentheses\n", "idx": 285}
{"namespace": "django.test.utils.extend_sys_path", "completion": "    original_sys_path = sys.path[:]\n    try:\n        sys.path.extend(paths)\n        yield\n    finally:\n        sys.path[:] = original_sys_path\n\n", "idx": 286}
{"namespace": "albumentations.augmentations.functional.normalize_cv2", "completion": "    mean = np.array(mean, dtype=img.dtype)\n    denominator = np.array(denominator, dtype=img.dtype)\n\n    if mean.ndim == 1:\n        mean = mean[None, ...]\n    if denominator.ndim == 1:\n        denominator = denominator[None, ...]\n\n    mean = np.broadcast_to(mean, img.shape)\n    denominator = np.broadcast_to(denominator, img.shape)\n\n    return (img - mean) / denominator\n\n", "idx": 287}
{"namespace": "albumentations.augmentations.functional.normalize_numpy", "completion": "    img = img.astype(\"float32\")\n    img -= mean\n    img *= denominator\n    return img\n\n", "idx": 288}
{"namespace": "albumentations.augmentations.functional.gamma_transform", "completion": "    # Check the data type of the input image\n    if img.dtype == np.uint8:\n        # Convert the image to float32 for gamma correction\n        img = img.astype(np.float32)\n\n        # Apply gamma correction\n        img = np.power(img / 255, gamma)\n\n        # Convert the image back to uint8\n        img = np.clip(img * 255, 0, 255).astype(np.uint8)\n\n    elif img.dtype == np.float32:\n        # Apply gamma correction\n        img = np.power(img, gamma)\n\n    else:\n        raise TypeError(\"Unsupported data type for gamma correction. Only uint8 and float32 are supported.\")\n\n    return img\n\n", "idx": 289}
{"namespace": "albumentations.augmentations.functional.swap_tiles_on_image", "completion": "    # Create a copy of the input image\n    output_image = image.copy()\n\n    # Iterate over the tiles\n    for tile in tiles:\n        # Extract the current and old left-up corner coordinates, height, and width of the tile\n        current_left_up_x, current_left_up_y, old_left_up_x, old_left_up_y, height, width = tile\n\n        # Get the current tile from the input image\n        current_tile = output_image[current_left_up_y : current_left_up_y + height, current_left_up_x : current_left_up_x + width]\n\n        # Get the old tile from the input image\n        old_tile = output_image[old_left_up_y : old_left_up_y + height, old_left_up_x : old_left_up_x + width]\n\n        # Swap the current and old tiles in the output image\n        output_image[current_left_up_y : current_left_up_y + height, current_left_up_x : current_left_up_x + width] = old_tile\n        output_image[old_left_up_y : old_left_up_y + height, old_left_up_x : old_left_up_x + width] = current_tile\n\n    return output_image\n\n", "idx": 290}
{"namespace": "albumentations.augmentations.geometric.functional.keypoint_rotate", "completion": "    x, y, angle, scale = keypoint[:4]\n    angle = np.deg2rad(angle)\n    x_t = (np.cos(angle) * x * scale + np.sin(angle) * y) / scale\n    y_t = -np.sin(angle) * x * scale + np.cos(angle) * y\n    x_t = x_t + 0.5\n    y_t = y_t + 0.5\n\n    return x_t, y_t, angle, scale\n\n", "idx": 291}
{"namespace": "albumentations.augmentations.geometric.functional.keypoint_shift_scale_rotate", "completion": "    x, y, a, s = keypoint[:4]\n    center = (cols - 1) * 0.5, (rows - 1) * 0.5\n    matrix = cv2.getRotationMatrix2D(center, angle, scale)\n    matrix[0, 2] += dx * cols\n    matrix[1, 2] += dy * rows\n    x, y = cv2.transform(np.array([[[x, y]]]), matrix).squeeze()\n    return x, y, a + math.radians(angle), s\n\n", "idx": 292}
{"namespace": "albumentations.core.keypoints_utils.angle_to_2pi_range", "completion": "    return (angle + math.pi) % (2 * math.pi)\n\n", "idx": 293}
{"namespace": "albumentations.augmentations.geometric.functional.rot90", "completion": "    return np.rot90(img, factor)\n\n", "idx": 294}
{"namespace": "albumentations.core.keypoints_utils.convert_keypoints_to_albumentations", "completion": "    if source_format not in keypoint_formats:\n        raise ValueError(\"Unknown source_format {}. Supported formats are: {}\".format(source_format, keypoint_formats))\n\n    if source_format == \"xy\":\n        return [convert_keypoint_to_albumentations(kp, source_format, rows, cols, check_validity, angle_in_degrees) for kp in keypoints]\n    elif source_format == \"yx\":\n        return [convert_keypoint_to_albumentations(kp, source_format, rows, cols, check_validity, angle_in_degrees) for kp in keypoints]\n    elif source_format == \"xya\":\n        return [convert_keypoint_to_albumentations(kp, source_format, rows, cols, check_validity, angle_in_degrees) for kp in keypoints]\n    elif source_format == \"xys\":\n        return [convert_keypoint_to_albumentations(kp, source_format, rows, cols, check_validity, angle_in_degrees) for kp in keypoints]\n    elif source_format == \"xyas\":\n        return [convert_keypoint_to_albumentations(kp, source_format, rows, cols, check_validity, angle_in_degrees) for kp in keypoints]\n    elif source_format == \"xysa\":\n        return [convert_keypoint_to_albumentations(kp, source_format, rows, cols, check_validity, angle_in_degrees) for kp in keypoints]\n    else:\n        raise ValueError(f\"Invalid source format. Got: {source_format}\")\n\n", "idx": 295}
{"namespace": "albumentations.core.keypoints_utils.convert_keypoints_from_albumentations", "completion": "    return [\n        convert_keypoint_from_albumentations(kp, target_format, rows, cols, check_validity, angle_in_degrees)\n        for kp in keypoints\n    ]", "idx": 296}
{"namespace": "albumentations.core.transforms_interface.to_tuple", "completion": "    if isinstance(param, (tuple, list)):\n        if len(param) == 2:\n            return param\n        else:\n            return tuple(param)\n    else:\n        if low is None:\n            low = 0.0\n        if bias is None:\n            bias = 0.0\n        return (param + bias - low, param + bias + low)\n\n", "idx": 297}
{"namespace": "albumentations.core.composition.ReplayCompose.replay", "completion": "        replay_transforms = []\n        for transform in saved_augmentations:\n            replay_transforms.append(instantiate_nonserializable(transform))\n\n        replay_transforms = Compose(replay_transforms)\n        return replay_transforms(**kwargs)\n", "idx": 298}
{"namespace": "albumentations.core.serialization.shorten_class_name", "completion": "    if class_fullname.startswith(\"albumentations.\"):\n        return class_fullname.split(\".\")[-1]\n    else:\n        return class_fullname.split(\".\")[-1]\n\n", "idx": 299}
{"namespace": "wandb.util.to_forward_slash_path", "completion": "    if platform.system() == \"Windows\":\n        return path.replace(\"\\\\\", \"/\")\n    return path\n\n", "idx": 300}
{"namespace": "wandb.util.make_artifact_name_safe", "completion": "    name = re.sub(r\"[^a-zA-Z0-9_.-]\", \"_\", name)\n    if len(name) > 128:\n        name = re.sub(\n            r\"^((\\w+){1,128})$\", r\"\\1...\", name\n        )  # type: ignore[arg-type]\n    return name\n\n", "idx": 301}
{"namespace": "wandb.sdk.wandb_settings._redact_dict", "completion": "    if not isinstance(d, dict):\n        raise TypeError(f\"Expected a dict, got {type(d)}\")\n\n    if not isinstance(unsafe_keys, set):\n        raise TypeError(f\"Expected a set, got {type(unsafe_keys)}\")\n\n    if not isinstance(redact_str, str):\n        raise TypeError(f\"Expected a str, got {type(redact_str)}\")\n\n    return {\n        key: redact_str if key in unsafe_keys else val\n        for key, val in d.items()\n    }\n\n", "idx": 302}
{"namespace": "wandb.sdk.launch.builder.build.get_current_python_version", "completion": "    import sys\n\n    return (\n        sys.version.replace(\"\\n\", \"\"),\n        \".\".join(sys.version.split()[0].split(\".\")[:2]),\n    )\n\n", "idx": 303}
{"namespace": "wandb.sdk.artifacts.storage_policy.StoragePolicy.lookup_by_name", "completion": "        for subclass in cls.__subclasses__():\n            if subclass.__name__ == name:\n                return subclass\n        raise NotImplementedError(f\"No storage policy with name {name} found.\")\n", "idx": 304}
{"namespace": "wandb.sdk.lib.runid.generate_id", "completion": "    # Generate a random string of the specified length.\n    random_string = ''.join(secrets.choice(string.ascii_lowercase + string.digits) for _ in range(length))\n\n    # Return the generated random string.\n    return random_string", "idx": 305}
{"namespace": "wandb.sdk.internal.file_stream.CRDedupeFilePolicy.get_consecutive_offsets", "completion": "        if not console:\n            return []\n\n        # Sort the keys of the dictionary\n        sorted_keys = sorted(console.keys())\n\n        # Initialize the list of intervals\n        intervals = []\n\n        # Iterate over the sorted keys\n        for i in range(len(sorted_keys) - 1):\n            # Get the current and next key\n            current_key = sorted_keys[i]\n            next_key = sorted_keys[i + 1]\n\n            # Check if the current key is consecutive to the next key\n            if next_key - current_key == 1:\n                # If so, add the current key to the interval\n                intervals.append([current_key, next_key])\n            else:\n                # If not, add the current key as a separate interval\n                intervals.append([current_key, current_key])\n\n        return intervals\n", "idx": 306}
{"namespace": "wandb.sdk.internal.system.assets.ipu.IPUStats.sample", "completion": "        try:\n            devices = self._gc_ipu_info.getDevices()\n            for device in devices:\n                if device[\"pid\"] == self._pid:\n                    device_name = device[\"name\"]\n                    if device_name not in self._devices_called:\n                        self._devices_called.add(device_name)\n                        metrics = device[\"metrics\"]\n                        for metric in metrics:\n                            key = metric[\"key\"]\n                            value = metric[\"value\"]\n                            if key in self.variable_metric_keys:\n                                metric_key, metric_value = self.parse_metric(key, value)\n                                if metric_key is not None:\n                                    wandb.log({metric_key: metric_value}, step=wandb.run.step)\n        except Exception as e:\n            raise e\n", "idx": 307}
{"namespace": "csvkit.cleanup.join_rows", "completion": "    if not isinstance(rows, list):\n        raise CSVTestException(\"The 'rows' argument must be a list.\")\n\n    if not isinstance(joiner, str):\n        raise CSVTestException(\"The 'joiner' argument must be a string.\")\n\n    if len(rows) == 0:\n        raise CSVTestException(\"The 'rows' argument must not be empty.\")\n\n    if len(rows) == 1:\n        return rows[0]\n\n    if len(rows) == 2:\n        return joiner.join(rows)\n\n    if len(rows) > 2:\n        return joiner.join(rows)", "idx": 308}
{"namespace": "csvkit.convert.guess_format", "completion": "    # Get the extension of the file\n    extension = filename.split('.')[-1]\n\n    # Check the extension and return the corresponding format\n    if extension in ['csv', 'dbf', 'fixed', 'xls', 'xlsx', 'json']:\n        return extension\n    elif extension == 'js':\n        return 'json'\n    else:\n        return None", "idx": 309}
{"namespace": "folium.utilities.normalize", "completion": "    return rendered.replace(\" \", \"\").replace(\"\\n\", \"\")\n\n", "idx": 310}
{"namespace": "tpot.gp_deap.initialize_stats_dict", "completion": "    individual.stats = defaultdict(list)\n    individual.stats['generation'] = []\n    individual.stats['mutation_count'] = []\n    individual.stats['crossover_count'] = []\n    individual.stats['predecessor'] = []\n\n", "idx": 311}
{"namespace": "bentoml_cli.env_manager.remove_env_arg", "completion": "    if \"--env\" in cmd_args:\n        cmd_args.remove(\"--env\")\n    if \"--env=\" in cmd_args:\n        cmd_args.remove(\"--env=\")\n\n    return cmd_args\n\n", "idx": 312}
{"namespace": "bentoml._internal.utils.uri.path_to_uri", "completion": "    \"\"\"\n    Convert a given path to a URI. It first converts the path to an absolute path and then uses the appropriate method based on the operating system to convert it to a URI.\n    Input-Output Arguments\n    :param path: String. The path to be converted to a URI.\n    :return: String. The URI string (quoted, absolute).\n    \"\"\"\n\n    \"\"\"\n    Convert a given path to a URI. It first converts the path to an absolute path and then uses the appropriate method based on the operating system to convert it to a URI.\n    Input-Output Arguments\n    :param path: String. The path to be converted to a URI.\n    :return: String. The URI string (quoted, absolute).\n    \"\"\"\n\n    \"\"\"\n    Convert a given path to a URI. It first converts the path to an absolute path and then uses the appropriate method based on the operating system to convert it to a URI.\n    Input-Output Arguments\n    :param path: String. The path to be converted to a URI.\n    :return: String. The URI string (quoted, absolute).\n    \"\"\"\n\n    \"\"\"\n    Convert a given path to a URI. It first converts the path to an absolute path and then uses the appropriate method based on the operating system to convert it to a URI.\n    Input-Output Arguments\n    :param path: String. The path to be converted to a URI.\n    :return: String. The URI string (quoted, absolute).\n    \"\"\"\n\n    \"\"\"\n    Convert a given path to a URI. It first converts the path to an absolute path and then uses the appropriate method based on the operating system to convert it to a URI.\n    Input-Output Arguments\n    :param path: String. The path to be converted to a URI.\n    :return: String. The URI string (quoted, absolute).\n    \"\"\"\n\n    \"\"\"\n    Convert a given path to a URI. It first converts the path to an absolute path and then uses the appropriate method based on the operating system to convert it to a URI.\n    Input-Output Arguments\n    :param path: String. The path to be converted to a URI.\n    :return: String. The URI string (quoted, absolute).\n    \"\"\"\n\n    \"\"\"\n    Convert a given path to a URI. It first converts the path to an absolute path and then uses the appropriate method based on the operating system to convert it to a URI.\n    Input-Output Arguments\n    :param path: String.", "idx": 313}
{"namespace": "bentoml._internal.utils.uri.uri_to_path", "completion": "    parsed_uri = urlparse(uri)\n    if parsed_uri.scheme == \"file\":\n        return url2pathname(uri)\n    raise ValueError(\"Unsupported scheme\")\n\n", "idx": 314}
{"namespace": "bentoml._internal.utils.validate_labels", "completion": "    if not isinstance(labels, dict):\n        raise ValueError(\"Labels must be a dictionary.\")\n    if not all(isinstance(key, str) for key in labels.keys()):\n        raise ValueError(\"Labels keys must be strings.\")\n    if not all(isinstance(value, str) for value in labels.values()):\n        raise ValueError(\"Labels values must be strings.\")\n\n", "idx": 315}
{"namespace": "bentoml._internal.configuration.helpers.is_valid_ip_address", "completion": "    try:\n        ipaddress.ip_address(addr)\n        return True\n    except ValueError:\n        return False", "idx": 316}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batches_to_batch", "completion": "        # pandas.concat may consume lots of memory, need optimization later\n        batch: ext.PdDataFrame = pd.concat(batches, axis=batch_dim)\n        indices = list(\n            itertools.accumulate(subbatch.shape[batch_dim] for subbatch in batches)\n        )\n        indices = [0] + indices\n        return batch, indices\n", "idx": 317}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batch_to_batches", "completion": "        import pandas as pd\n\n        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n        return pd.split(batch, indices[1:-1], ignore_index=True)  # type: ignore (incomplete panadas types)\n", "idx": 318}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batches_to_batch", "completion": "        batch: list[t.Any] = []\n        indices: list[int] = []\n        for i, subbatch in enumerate(batches):\n            batch.extend(subbatch)\n            indices.append(len(batch))\n        return batch, indices\n", "idx": 319}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batch_to_batches", "completion": "        assert (\n            batch_dim == 0\n        ), \"Default Runner DataContainer does not support batch_dim other than 0\"\n        batches = []\n        for i in range(len(indices) - 1):\n            batches.append(batch[indices[i] : indices[i + 1]])\n        return batches\n", "idx": 320}
{"namespace": "jwt.utils.force_bytes", "completion": "    if isinstance(value, bytes):\n        return value\n    elif isinstance(value, str):\n        return value.encode(\"utf-8\")\n    else:\n        raise TypeError(\"Input value must be bytes or str.\")\n\n", "idx": 321}
{"namespace": "pytube.cli.display_progress_bar", "completion": "    if filesize == 0:\n        return\n    scale = max(0.0, min(1.0, scale))\n    progress = int(bytes_received * scale / filesize)\n    print(\n        f\"\\r{ch * progress}{' ' * (20 - progress)} {bytes_received} / {filesize}\",\n        end=\"\",\n    )\n\n", "idx": 322}
{"namespace": "pytube.cli._download", "completion": "    filesize = stream.filesize / 1000000\n    print(f\"Downloading {filename} ({filesize:.2f} MB)\")\n    stream.download(target, filename, on_progress)\n\n", "idx": 323}
{"namespace": "pytube.cli.display_streams", "completion": "    print(f\"Available formats for {youtube.title}\")\n    for stream in youtube.streams.filter(progressive=True):\n        print(f\"itag: {stream.itag} | {stream.resolution} | {stream.mime_type}\")\n    for stream in youtube.streams.filter(progressive=False):\n        print(f\"itag: {stream.itag} | {stream.resolution} | {stream.mime_type}\")\n\n", "idx": 324}
{"namespace": "pytube.cli._unique_name", "completion": "    filename = f\"{base}.{subtype}\"\n    file_path = os.path.join(target, filename)\n    if os.path.exists(file_path):\n        i = 1\n        while os.path.exists(file_path):\n            filename = f\"{base}-{i}.{subtype}\"\n            file_path = os.path.join(target, filename)\n            i += 1\n    return filename\n\n", "idx": 325}
{"namespace": "pytube.cli._print_available_captions", "completion": "    for caption in captions:\n        print(caption.code)\n\n", "idx": 326}
{"namespace": "pytube.cipher.throttling_reverse", "completion": "    arr.reverse()\n\n", "idx": 327}
{"namespace": "pytube.helpers.setup_logger", "completion": "    # Create a logger\n    logger = logging.getLogger(__name__)\n    logger.setLevel(level)\n\n    # Create a stream handler\n    stream_handler = logging.StreamHandler()\n    stream_handler.setLevel(level)\n\n    # Create a formatter\n    formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n\n    # Add the formatter to the stream handler\n    stream_handler.setFormatter(formatter)\n\n    # Add the stream handler to the logger\n    logger.addHandler(stream_handler)\n\n    # If a log filename is provided, create a file handler\n    if log_filename:\n        file_handler = logging.FileHandler(log_filename)\n        file_handler.setLevel(level)\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n\n", "idx": 328}
{"namespace": "pytube.helpers.deprecated", "completion": "    def decorator(func: Callable[..., GenericType]) -> GenericType:\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            warnings.simplefilter(\"always\", DeprecationWarning)  # turn off filter\n            warnings.warn(\n                \"Call to deprecated function {}. {}\".format(func.__name__, reason),\n                category=DeprecationWarning,\n                stacklevel=2,\n            )\n            warnings.simplefilter(\"default\", DeprecationWarning)  # reset filter\n            return func(*args, **kwargs)\n\n        return wrapper  # type: ignore\n\n    return decorator\n\n", "idx": 329}
{"namespace": "pytube.helpers.uniqueify", "completion": "    seen = set()\n    seen_add = seen.add\n    return [x for x in duped_list if not (x in seen or seen_add(x))]\n\n", "idx": 330}
{"namespace": "pytube.helpers.target_directory", "completion": "    if output_path is None:\n        return os.getcwd()\n    elif os.path.isabs(output_path):\n        return output_path\n    else:\n        return os.path.abspath(output_path)\n\n", "idx": 331}
{"namespace": "pytube.extract.is_private", "completion": "    private_strings = [\n        'This live stream recording is not available.'\n    ]\n    for string in private_strings:\n        if string in watch_html:\n            return True\n    return False\n\n", "idx": 332}
{"namespace": "pymc.math.cartesian", "completion": "    arrays = [np.asarray(x) for x in arrays]\n    shape = (len(x) for x in arrays)\n\n    ix = np.indices(shape, dtype=int)\n    ix = ix.reshape(len(arrays), -1).T\n\n    for n, arr in enumerate(arrays):\n        ix[:, n] = arrays[n][ix[:, n]]\n\n    return ix\n\n", "idx": 333}
{"namespace": "pymc.math.log1mexp", "completion": "    if negative_input:\n        return pt.log1p(-pt.exp(x))\n    else:\n        return pt.log1p(pt.exp(-x))\n\n", "idx": 334}
{"namespace": "pymc.math.log1mexp_numpy", "completion": "    if not negative_input:\n        warnings.warn(\n            \"pymc.math.log1mexp_numpy will expect a negative input in a future \"\n            \"version of PyMC.\\n To suppress this warning set `negative_input=True`\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        x = -x\n\n    return np.log1p(-np.exp(x))\n\n", "idx": 335}
{"namespace": "pymc.util.drop_warning_stat", "completion": "    # Get the sample stats groups\n    sample_stats_groups = idata.sample_stats.groups\n\n    # Create a new InferenceData object\n    new_idata = arviz.InferenceData()\n\n    # Iterate over the sample stats groups\n    for group in sample_stats_groups:\n\n        # Get the sample stats for the group\n        sample_stats = idata.sample_stats[group]\n\n        # Remove the \"warning\" stat from the sample stats\n        sample_stats = sample_stats.drop(\"warning\")\n\n        # Add the sample stats to the new InferenceData object\n        new_idata.add_groups(group, sample_stats)\n\n    # Return the new InferenceData object\n    return new_idata\n\n", "idx": 336}
{"namespace": "pymc.pytensorf.walk_model", "completion": "    if stop_at_vars is None:\n        stop_at_vars = set()\n\n    for graph in graphs:\n        for node in walk(graph):\n            if node in stop_at_vars:\n                break\n            yield node\n            for var in expand_fn(node):\n                if var in stop_at_vars:\n                    break\n                yield var\n\n", "idx": 337}
{"namespace": "pymc.testing.select_by_precision", "completion": "    if floatX == \"float64\":\n        return float64\n    else:\n        return float32\n\n", "idx": 338}
{"namespace": "pymc.gp.cov.handle_args", "completion": "    def wrapper(*args, **kwargs):\n        if len(args) == 1:\n            return func(*args, **kwargs)\n        else:\n            return func(*args[0], *args[1])\n\n    return wrapper", "idx": 339}
{"namespace": "pymc.gp.util.kmeans_inducing_points", "completion": "    # Check if the input is a numpy array\n    if not isinstance(X, np.ndarray):\n        raise TypeError(\"The input `X` must be a numpy array.\")\n\n    # Check if the number of inducing points is a positive integer\n    if not isinstance(n_inducing, int) or n_inducing <= 0:\n        raise ValueError(\"The number of inducing points `n_inducing` must be a positive integer.\")\n\n    # Check if the number of inducing points is less than the number of data points\n    if n_inducing > X.shape[0]:\n        raise ValueError(\n            \"The number of inducing points `n_inducing` must be less than or equal to the number of data points.\"\n        )\n\n    # Initialize the inducing points\n    fu, _ = kmeans(X, n_inducing, **kmeans_kwargs)\n\n    # Scale the inducing points\n    fu = fu * np.sqrt(X.shape[0] / n_inducing)\n\n    return fu\n\n", "idx": 340}
{"namespace": "pymc.pytensorf.floatX", "completion": "    return pt.cast(X, pytensor.config.floatX)\n\n", "idx": 341}
{"namespace": "pymc.distributions.multivariate.posdef", "completion": "    try:\n        L = pt.cholesky(AA)\n        return True\n    except pt.PyTensorException:\n        return False\n\n", "idx": 342}
{"namespace": "pymc.distributions.dist_math.multigammaln", "completion": "    if p == 1:\n        return np.sum(gammaln(a), axis=-1)\n    else:\n        return gammaln(np.sum(a, axis=-1)) - np.sum(gammaln(a), axis=-1)\n\n", "idx": 343}
{"namespace": "pymc.distributions.dist_math.incomplete_beta", "completion": "    return pt.betainc(a, b, value)\n\n", "idx": 344}
{"namespace": "pymc.sampling.forward.observed_dependent_deterministics", "completion": "    # Get the deterministics, observed random variables, and basic random variables from the model.\n    deterministics = model.deterministics\n    observed_rvs = model.observed_RVs\n    basic_rvs = model.basic_RVs\n\n    # Initialize an empty list to store the dependent deterministics.\n    dependent_deterministics = []\n\n    # Iterate over the deterministics and check if they depend directly on observed variables.\n    for deterministic in deterministics:\n        # Get the dependencies of the deterministic.\n        dependencies = ancestors(deterministic)\n\n        # Check if any of the dependencies are observed random variables.\n        if any(rv in observed_rvs for rv in dependencies):\n            # If the deterministic depends directly on an observed random variable, add it to the list.\n            dependent_deterministics.append(deterministic)\n\n    # Return the list of dependent deterministics.\n    return dependent_deterministics\n\n", "idx": 345}
{"namespace": "pymc.smc.kernels.systematic_resampling", "completion": "    # Normalize the weights\n    normalized_weights = weights / weights.sum()\n\n    # Generate a vector of indices\n    new_indices = np.arange(len(normalized_weights))\n\n    # Sort the indices based on the normalized weights\n    sorted_indices = np.argsort(normalized_weights)\n\n    # Generate a vector of cumulative sum of the normalized weights\n    cumulative_sum = np.cumsum(normalized_weights[sorted_indices])\n\n    # Generate a vector of uniform random numbers\n    uniform_random_numbers = rng.uniform(size=len(normalized_weights))\n\n    # Generate a vector of indices based on the uniform random numbers\n    new_indices = np.searchsorted(cumulative_sum, uniform_random_numbers)\n\n    # Return the new indices\n    return sorted_indices[new_indices]\n\n", "idx": 346}
{"namespace": "pymc.backends.base._squeeze_cat", "completion": "    if combine:\n        results = np.concatenate(results)\n    if squeeze and len(results) == 1:\n        results = results[0]\n    return results\n\n", "idx": 347}
{"namespace": "pymc.logprob.transforms.SimplexTransform.forward", "completion": "        log_value = pt.log(value)\n        sum_log_value = pt.sum(log_value)\n        transformed_value = log_value - sum_log_value\n        return transformed_value\n", "idx": 348}
{"namespace": "pymc.logprob.transforms.SimplexTransform.backward", "completion": "        value = pt.as_tensor(value)\n        N = value.shape[-1].astype(value.dtype)\n        shift = pt.sum(value, -1, keepdims=True) / N\n        return pt.exp(value[..., :-1] + shift)\n", "idx": 349}
{"namespace": "pymc.logprob.utils.walk_model", "completion": "    if stop_at_vars is None:\n        stop_at_vars = set()\n    if not isinstance(stop_at_vars, set):\n        stop_at_vars = set(stop_at_vars)\n    if not isinstance(walk_past_rvs, bool):\n        raise TypeError(f\"walk_past_rvs must be a bool, not {type(walk_past_rvs)}\")\n\n    for graph in graphs:\n        for node in walk(graph, stop_at_vars=stop_at_vars):\n            if isinstance(node, MeasurableVariable):\n                if walk_past_rvs:\n                    yield node\n            elif isinstance(node, Variable):\n                yield node\n            elif isinstance(node, Op):\n                if isinstance(node.op, HasInnerGraph):\n                    for inner_node in walk_model(\n                        node.op.inner_graphs,\n                        walk_past_rvs=walk_past_rvs,\n                        stop_at_vars=stop_at_vars,\n                        expand_fn=expand_fn,\n                    ):\n                        yield inner_node\n                else:\n                    for inner_node in expand_fn(node):\n                        yield inner_node\n            else:\n                raise TypeError(\n                    f\"walk_model only accepts TensorVariable, Variable, and Op as input, not {type(node)}\"\n                )\n\n", "idx": 350}
{"namespace": "sacred.metrics_logger.linearize_metrics", "completion": "    metrics = {}\n    for metric in logged_metrics:\n        if metric.name not in metrics:\n            metrics[metric.name] = {\"steps\": [], \"values\": [], \"timestamps\": []}\n        metrics[metric.name][\"steps\"].append(metric.step)\n        metrics[metric.name][\"values\"].append(metric.value)\n        metrics[metric.name][\"timestamps\"].append(metric.timestamp)\n    return metrics", "idx": 351}
{"namespace": "sacred.utils.set_by_dotted_path", "completion": "    if not isinstance(path, str):\n        raise TypeError(\"path must be a string\")\n    if not PYTHON_IDENTIFIER.match(path):\n        raise ValueError(\"path must be a valid python identifier\")\n    if not isinstance(value, (dict, list, tuple, str, int, float, bool, type(None))):\n        raise TypeError(\"value must be a valid python object\")\n\n    keys = path.split(\".\")\n    for key in keys[:-1]:\n        if key not in d:\n            d[key] = {}\n        d = d[key]\n    d[keys[-1]] = value\n\n", "idx": 352}
{"namespace": "sacred.utils.get_by_dotted_path", "completion": "    split_path = path.split(\".\")\n    current_option = d\n    for p in split_path:\n        if p not in current_option:\n            return default\n        current_option = current_option[p]\n    return current_option\n\n", "idx": 353}
{"namespace": "pymc.logprob.scan.construct_scan", "completion": "    # Get the inner-graph outputs\n    inner_outs = [\n        getattr(scan_args, inner_out_type)[oo_info.index]\n        for oo_info in scan_args.outer_outputs\n        for inner_out_type in [\"inner_out_mit_sot\", \"inner_out_sit_sot\", \"inner_out_nit_sot\"]\n    ]\n\n    # Get the inner-graph inputs\n    inner_ins = [\n        getattr(scan_args, inner_in_type)[oo_info.index]\n        for oo_info in scan_args.outer_inputs\n        for inner_in_type in [\"inner_in_mit_sot\", \"inner_in_sit_sot\"]\n    ]\n\n    # Get the inner-graph updates\n    inner_updates = [\n        getattr(scan_args, inner_up_type)[oo_info.index]\n        for oo_info in scan_args.outer_updates\n        for inner_up_type in [\"inner_up_mit_sot\", \"inner_up_sit_sot\"]\n    ]\n\n    # Get the inner-graph outputs\n    inner_outs = [\n        getattr(scan_args, inner_out_type)[oo_info.index]\n        for oo_info in scan_args.outer_outputs\n        for inner_out_type in [\"inner_out_mit_sot\", \"inner_out_sit_sot\", \"inner_out_nit_sot\"]\n    ]\n\n    # Get the inner-graph inputs\n    inner_ins = [\n        getattr(scan_args, inner_in_type)[oo_info.index]\n        for oo_info in scan_args.outer_inputs\n        for inner_in_type in [\"inner_in_mit_sot\", \"inner_in_sit_sot\"]\n    ]\n\n    # Get the inner-graph updates\n    inner_updates = [\n        getattr(scan_args, inner_up_type)[oo_info.index]\n        for oo_info in scan_args.outer_updates\n        for inner_up_type in [\"inner_up_mit_sot\", \"inner_up_sit_sot\"]\n    ]\n\n    # Get the inner-graph outputs\n    inner_outs = [\n        getattr(scan", "idx": 354}
{"namespace": "sacred.utils.is_prefix", "completion": "    if pre_path == path:\n        return True\n    if not pre_path or not path:\n        return False\n    if pre_path.endswith(\".\"):\n        pre_path = pre_path[:-1]\n    if path.endswith(\".\"):\n        path = path[:-1]\n    return pre_path in path\n\n", "idx": 355}
{"namespace": "sacred.utils.get_inheritors", "completion": "    subclasses = set()\n    for subclass in cls.__subclasses__():\n        subclasses.add(subclass)\n        subclasses.update(get_inheritors(subclass))\n    return subclasses\n\n", "idx": 356}
{"namespace": "sacred.utils.convert_camel_case_to_snake_case", "completion": "    s1 = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1_\\2\", name)\n    return re.sub(\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", s1).lower()\n\n", "idx": 357}
{"namespace": "sacred.utils.module_exists", "completion": "    try:\n        pkgutil.find_loader(modname)\n        return True\n    except ImportError:\n        return False\n\n", "idx": 358}
{"namespace": "sacred.utils.apply_backspaces_and_linefeeds", "completion": "    # Remove backspace and linefeed characters\n    text = text.replace(\"\\b\", \"\")\n    text = text.replace(\"\\n\", \"\")\n\n    # Apply backspace and linefeed characters line by line\n    lines = text.split(\"\\r\")\n    interpreted_text = \"\"\n    for line in lines:\n        interpreted_text += line\n        interpreted_text += \"\\n\"\n\n    return interpreted_text\n\n", "idx": 359}
{"namespace": "sacred.commands.help_for_command", "completion": "    help_text = pydoc.getdoc(command)\n    help_text = help_text.replace(\"\\b\", \"\")\n    return help_text\n\n", "idx": 360}
{"namespace": "sacred.optional.optional_import", "completion": "    for package_name in package_names:\n        try:\n            package = importlib.import_module(package_name)\n            return True, package\n        except ImportError:\n            pass\n    return False, None\n\n", "idx": 361}
{"namespace": "sacred.dependencies.get_py_file_if_possible", "completion": "    if pyc_name.endswith(\".py\"):\n        return pyc_name\n    elif pyc_name.endswith(\".so\"):\n        return pyc_name\n    elif pyc_name.endswith(\".pyd\"):\n        return pyc_name\n    elif pyc_name.endswith(\".ipynb\"):\n        return pyc_name\n    elif pyc_name.endswith(\".py\"):\n        return pyc_name\n    else:\n        py_name = pyc_name[:-1]\n        if os.path.exists(py_name):\n            return py_name\n        else:\n            return pyc_name\n\n", "idx": 362}
{"namespace": "sacred.config.custom_containers.DogmaticDict.update", "completion": "        if iterable is not None:\n            if isinstance(iterable, dict):\n                for k, v in iterable.items():\n                    self[k] = v\n            else:\n                for k, v in iterable:\n                    self[k] = v\n\n        for k, v in kwargs.items():\n            self[k] = v\n", "idx": 363}
{"namespace": "sacred.config.config_scope.is_empty_or_comment", "completion": "    line = line.strip()\n    return line == \"\" or line.startswith(\"#\")\n\n", "idx": 364}
{"namespace": "boltons.funcutils.copy_function", "completion": "    # TODO: handle descriptors?\n    # TODO: handle slots?\n    # TODO: handle __wrapped__?\n    # TODO: handle __code__?\n    # TODO: handle __closure__?\n    # TODO: handle __dict__?\n    # TODO: handle __defaults__?\n    # TODO: handle __kwdefaults__?\n    # TODO: handle __globals__?\n    # TODO: handle __module__?\n    # TODO: handle __name__?\n    # TODO: handle __qualname__?\n    # TODO: handle __annotations__?\n    # TODO: handle __doc__?\n    # TODO: handle __dict__?\n    # TODO: handle __func__?\n    # TODO: handle __self__?\n    # TODO: handle __class__?\n    # TODO: handle __code__?\n    # TODO: handle __defaults__?\n    # TODO: handle __kwdefaults__?\n    # TODO: handle __globals__?\n    # TODO: handle __module__?\n    # TODO: handle __name__?\n    # TODO: handle __qualname__?\n    # TODO: handle __annotations__?\n    # TODO: handle __doc__?\n    # TODO: handle __dict__?\n    # TODO: handle __func__?\n    # TODO: handle __self__?\n    # TODO: handle __class__?\n    # TODO: handle __code__?\n    # TODO: handle __defaults__?\n    # TODO: handle __kwdefaults__?\n    # TODO: handle __globals__?\n    # TODO: handle __module__?\n    # TODO: handle __name__?\n    # TODO: handle __qualname__?\n    # TODO: handle __annotations__?\n    # TODO: handle __doc__?\n    # TODO: handle __dict__?\n    # TODO: handle __func__?\n    # TODO: handle __self__?\n    # TODO: handle __class__?\n    # TODO: handle __code__?\n    # TODO: handle __defaults__?\n    # TODO: handle __kwdefaults__?\n    # TODO: handle __globals__?\n    # TODO: handle __module__?\n    # TODO: handle __name__?\n    # TODO: handle __qualname__?\n    # TODO: handle __annotations__?\n    # TODO: handle __doc__?\n    # TODO: handle __dict__?\n    # TODO: handle __func__?\n   ", "idx": 365}
{"namespace": "sacred.config.config_scope.dedent_line", "completion": "    # Remove the common leading whitespace\n    return line[len(indent) :]\n\n", "idx": 366}
{"namespace": "boltons.funcutils.format_invocation", "completion": "    if kwargs is None:\n        kwargs = kw\n    if args:\n        args = ', '.join(str(arg) for arg in args)\n    if kwargs:\n        kwargs = ', '.join(str(k) + '=' + str(v) for k, v in kwargs.items())\n    if args and kwargs:\n        return name + '(' + args + ', ' + kwargs + ')'\n    elif args:\n        return name + '(' + args + ')'\n    elif kwargs:\n        return name + '(' + kwargs + ')'\n    else:\n        return name + '()'\n\n", "idx": 367}
{"namespace": "boltons.listutils.SplayList.shift", "completion": "        if item_index == dest_index:\n            return\n\n        if item_index < 0:\n            item_index += len(self)\n\n        if dest_index < 0:\n            dest_index += len(self)\n\n        if item_index < 0 or item_index >= len(self):\n            raise IndexError(\"Index out of range\")\n\n        if dest_index < 0 or dest_index >= len(self):\n            raise IndexError(\"Index out of range\")\n\n        item = self.pop(item_index)\n        self.insert(dest_index, item)\n", "idx": 368}
{"namespace": "boltons.strutils.gzip_bytes", "completion": "    return zlib.compress(bytestring, level)\n\n", "idx": 369}
{"namespace": "boltons.strutils.is_uuid", "completion": "    if isinstance(obj, uuid.UUID):\n        return obj.version == version\n    elif isinstance(obj, string_types):\n        try:\n            uuid.UUID(obj, version=version)\n            return True\n        except ValueError:\n            return False\n    else:\n        return False\n\n", "idx": 370}
{"namespace": "boltons.strutils.parse_int_list", "completion": "    # Split the range string into a list of integers and ranges.\n    range_list = range_string.split(delim)\n\n    # Create an empty list to store the integers.\n    int_list = []\n\n    # Iterate over the range list.\n    for range_str in range_list:\n        # Check if the range string contains a range delimiter.\n        if range_delim in range_str:\n            # Split the range string into a list of start and end integers.\n            start, end = range_str.split(range_delim)\n            # Convert the start and end integers to integers.\n            start = int(start)\n            end = int(end)\n            # Check if the start integer is greater than the end integer.\n            if start > end:\n                # Swap the start and end integers.\n                start, end = end, start\n            # Append the integers in the range to the integer list.\n            int_list.extend(range(start, end + 1))\n        else:\n            # Convert the integer string to an integer.\n            int_list.append(int(range_str))\n\n    # Sort the integer list.\n    int_list.sort()\n\n    # Return the sorted integer list.\n    return int_list\n\n", "idx": 371}
{"namespace": "boltons.cacheutils.ThresholdCounter.get", "completion": "        try:\n            return self._count_map[key][0]\n        except KeyError:\n            return default\n", "idx": 372}
{"namespace": "boltons.iterutils.backoff_iter", "completion": "    if not start:\n        raise ValueError('start must be a positive number')\n    if not stop:\n        raise ValueError('stop must be a positive number')\n    if not factor:\n        raise ValueError('factor must be a positive number')\n    if not jitter:\n        jitter = 0.0\n    if jitter:\n        if jitter < -1.0 or jitter > 1.0:\n            raise ValueError('jitter must be between -1.0 and 1.0')\n        if jitter > 0.0:\n            jitter = 1.0\n        else:\n            jitter = -1.0\n    if count == 'repeat':\n        count = None\n    if count is not None:\n        count = int(count)\n    if count is not None and count < 0:\n        raise ValueError('count must be a positive integer or None')\n    if start > stop:\n        raise ValueError('start must be less than stop')\n    if factor <= 1.0:\n        raise ValueError('factor must be greater than 1.0')\n    if jitter:\n        if jitter > 0.0:\n            factor = factor * (1.0 + jitter)\n        else:\n            factor = factor * (1.0 - jitter)\n    cur = start\n    while True:\n        if count is not None:\n            if count == 0:\n                break\n            count -= 1\n        yield cur\n        cur *= factor\n        if cur > stop:\n            cur = stop\n        if cur == stop:\n            break\n\n", "idx": 373}
{"namespace": "boltons.cacheutils.cached", "completion": "    def decorator(func):\n        return CachedFunction(func, cache, scoped=scoped, typed=typed, key=key)\n    return decorator\n\n", "idx": 374}
{"namespace": "boltons.timeutils.total_seconds", "completion": "    return (td.microseconds + (td.seconds + td.days * 24 * 3600) * 10 ** 6) / 10 ** 6\n\n", "idx": 375}
{"namespace": "boltons.gcutils.get_all", "completion": "    if include_subtypes:\n        return [obj for obj in gc.get_objects() if isinstance(obj, type_obj)]\n    else:\n        return [obj for obj in gc.get_objects() if type(obj) is type_obj]\n\n", "idx": 376}
{"namespace": "boltons.timeutils.daterange", "completion": "    if not isinstance(start, date):\n        raise TypeError('start must be a datetime.date object')\n    if not isinstance(stop, date):\n        raise TypeError('stop must be a datetime.date object')\n    if not isinstance(step, (int, timedelta, tuple)):\n        raise TypeError('step must be an int, datetime.timedelta, or tuple')\n    if not isinstance(inclusive, bool):\n        raise TypeError('inclusive must be a bool')\n\n    if isinstance(step, timedelta):\n        step = (step.days, 0, 0)\n    elif isinstance(step, tuple):\n        if len(step) != 3:\n            raise ValueError('step must be a tuple of length 3')\n        step = (step[0], step[1], step[2])\n\n    if step[0] == 0 and step[1] == 0 and step[2] == 0:\n        raise ValueError('step must not be zero')\n\n    if step[0] < 0:\n        start, stop = stop, start\n\n    if step[0] > 0:\n        while start <= stop:\n            yield start\n            start += timedelta(days=step[0],\n                               hours=step[1],\n                               minutes=step[2])\n    else:\n        while start >= stop:\n            yield start\n            start += timedelta(days=step[0],\n                               hours=step[1],\n                               minutes=step[2])\n\n    if inclusive and start == stop:\n        yield start\n\n", "idx": 377}
{"namespace": "boltons.mathutils.clamp", "completion": "    return max(lower, min(upper, x))\n\n", "idx": 378}
{"namespace": "boltons.mathutils.ceil", "completion": "    if options is None:\n        return _ceil(x)\n    else:\n        return _ceil(x) if x in options else min(options, key=lambda x: abs(x - x))\n\n", "idx": 379}
{"namespace": "boltons.formatutils.get_format_args", "completion": "    # TODO: memoize\n    # TODO: handle named arguments\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments with default values\n    # TODO: handle named arguments", "idx": 380}
{"namespace": "boltons.mathutils.floor", "completion": "    if options is None:\n        return _floor(x)\n    options = sorted(options)\n    i = bisect.bisect_right(options, x)\n    if i == 0:\n        raise ValueError(\"no floor options less than or equal to: %r\" % x)\n    return options[i - 1]\n\n", "idx": 381}
{"namespace": "boltons.dictutils.OneToOne.setdefault", "completion": "        if key not in self:\n            self[key] = default\n        return self[key]\n", "idx": 382}
{"namespace": "boltons.dictutils.OneToOne.update", "completion": "        if isinstance(dict_or_iterable, dict):\n            for k, v in dict_or_iterable.items():\n                self[k] = v\n        elif hasattr(dict_or_iterable, 'keys'):\n            for k in dict_or_iterable.keys():\n                self[k] = dict_or_iterable[k]\n        else:\n            for k, v in dict_or_iterable:\n                self[k] = v\n\n        for k, v in kw.items():\n            self[k] = v\n", "idx": 383}
{"namespace": "boltons.dictutils.ManyToMany.get", "completion": "        return self.data.get(key, default)\n", "idx": 384}
{"namespace": "boltons.dictutils.FrozenDict.updated", "completion": "        if not a and not kw:\n            return self\n\n        new_dict = self.copy()\n        new_dict.update(*a, **kw)\n        return new_dict\n", "idx": 385}
{"namespace": "boltons.dictutils.subdict", "completion": "    if keep is None:\n        keep = d.keys()\n    if drop is None:\n        drop = []\n\n    return {k: d[k] for k in keep if k not in drop}\n\n", "idx": 386}
{"namespace": "boltons.dictutils.FrozenDict.__repr__", "completion": "        class_name = self.__class__.__name__\n        dict_repr = dict.__repr__(self)\n        return f'{class_name}({dict_repr})'\n", "idx": 387}
{"namespace": "gunicorn.config.validate_callable", "completion": "    if not isinstance(arity, int):\n        raise TypeError(\"Invalid arity: %s\" % arity)\n\n    if arity == -1:\n        return arity\n\n    def validate(val):\n        if not callable(val):\n            raise TypeError(\"Not a callable: %s\" % val)\n        if not hasattr(val, \"__call__\"):\n            raise TypeError(\"Not a callable: %s\" % val)\n        if not hasattr(val, \"__code__\"):\n            raise TypeError(\"Not a callable: %s\" % val)\n        if not hasattr(val, \"__name__\"):\n            raise TypeError(\"Not a callable: %s\" % val)\n        if not hasattr(val, \"__module__\"):\n            raise TypeError(\"Not a callable: %s\" % val)\n        if not hasattr(val, \"__qualname__\"):\n            raise TypeError(\"Not a callable: %s\" % val)\n        if not hasattr(val, \"__doc__\"):\n            raise TypeError(\"Not a callable: %s\" % val)\n        if not hasattr(val, \"__annotations__\"):\n            raise TypeError(\"Not a callable: %s\" % val)\n        if not hasattr(val, \"__dict__\"):\n            raise TypeError(\"Not a callable: %s\" % val)\n        if not hasattr(val, \"__kwdefaults__\"):\n            raise TypeError(\"Not a callable: %s\" % val)\n        if not hasattr(val, \"__defaults__\"):\n            raise TypeError(\"Not a callable: %s\" % val)\n        if not hasattr(val, \"__closure__\"):\n            raise TypeError(\"Not a callable: %s\" % val)\n        if not hasattr(val, \"__globals__\"):\n            raise TypeError(\"Not a callable: %s\" % val)\n        if not hasattr(val, \"__code__\"):\n            raise TypeError(\"Not a callable: %s\" % val)\n        if not hasattr(val, \"__self__\"):\n            raise TypeError(\"Not a callable: %s\" % val)\n        if not hasattr(val, \"__class__\"):\n            raise TypeError(\"Not a callable: %s\" % val)\n        if not hasattr(val, \"__name__\"):\n            raise TypeError(\"Not a callable: %s\" % val)\n        if not hasattr(val, \"__module__\"):\n            raise TypeError(\"Not a callable: %s\" % val)\n        if not hasattr(val, \"__qualname", "idx": 388}
{"namespace": "gunicorn.config.get_default_config_file", "completion": "    # Create the path by joining the current working directory with the file name 'gunicorn.conf.py'.\n    default_config_file_path = os.path.join(util.getcwd(), 'gunicorn.conf.py')\n\n    # Check if the file exists.\n    if os.path.exists(default_config_file_path):\n        return default_config_file_path\n    else:\n        return None\n\n", "idx": 389}
{"namespace": "gunicorn.util.is_ipv6", "completion": "    try:\n        socket.inet_pton(socket.AF_INET6, addr)\n    except socket.error:\n        return False\n    return True\n\n", "idx": 390}
{"namespace": "gunicorn.systemd.listen_fds", "completion": "    if unset_environment:\n        os.unsetenv('LISTEN_PID')\n        os.unsetenv('LISTEN_FDS')\n\n    if 'LISTEN_PID' not in os.environ:\n        return 0\n\n    if 'LISTEN_FDS' not in os.environ:\n        return 0\n\n    return int(os.environ['LISTEN_FDS'])\n\n", "idx": 391}
{"namespace": "gunicorn.util.http_date", "completion": "    if timestamp is None:\n        timestamp = time.time()\n    return email.utils.formatdate(timestamp, usegmt=True)\n\n", "idx": 392}
{"namespace": "gunicorn.util.parse_address", "completion": "    if netloc.startswith('unix:'):\n        return netloc[5:], None\n\n    if netloc.startswith('fd:'):\n        return None, int(netloc[3:])\n\n    if ':' in netloc:\n        host, port = netloc.split(':', 1)\n    else:\n        host = netloc\n        port = default_port\n\n    return host, port\n\n", "idx": 393}
{"namespace": "gunicorn.util.to_bytestring", "completion": "    if isinstance(value, str):\n        return value.encode(encoding)\n    return value\n\n", "idx": 394}
{"namespace": "gunicorn.util.warn", "completion": "    sys.stderr.write(\"WARNING: %s\\n\" % msg)\n\n", "idx": 395}
{"namespace": "gunicorn.util.split_request_uri", "completion": "    if uri.startswith(\"//\"):\n        uri = \".\" + uri\n\n    return urllib.parse.urlsplit(uri)\n\n", "idx": 396}
{"namespace": "praw.models.listing.listing.ModNoteListing.after", "completion": "        return getattr(self, \"next\", None) if getattr(self, \"has_next_page\", False) else getattr(self, \"end_cursor\", None)", "idx": 397}
{"namespace": "praw.models.util.permissions_string", "completion": "    if permissions is None:\n        return \"+all\"\n\n    return \",\".join(permissions)\n\n", "idx": 398}
{"namespace": "jc.cli.JcCli.json_out", "completion": "        if self.pretty:\n            return json.dumps(self.data_out, indent=self.json_indent, separators=self.json_separators)\n\n        if self.ascii_only:\n            return json.dumps(self.data_out, separators=self.json_separators)\n\n        if not self.mono:\n            class JcStyle(Style):\n                styles: CustomColorType = self.custom_colors\n\n            return str(highlight(json.dumps(self.data_out, separators=self.json_separators), JsonLexer(), Terminal256Formatter(style=JcStyle))[0:-1])\n\n        return json.dumps(self.data_out, separators=self.json_separators)\n", "idx": 399}
{"namespace": "pythonforandroid.pythonpackage.transform_dep_for_pip", "completion": "    if \"@\" in dependency and \"://\" in dependency:\n        return dependency.replace(\"@\", \"%40\").replace(\"://\", \"%3A%2F%2F\")\n    else:\n        return dependency\n\n", "idx": 400}
{"namespace": "pythonforandroid.graph.fix_deplist", "completion": "    deps = [d for d in deps]\n\n    return deps\n\n", "idx": 401}
{"namespace": "pythonforandroid.util.walk_valid_filens", "completion": "    for root, dirs, files in walk(base_dir):\n        for dir_name in invalid_dir_names:\n            if fnmatch(root, dir_name):\n                dirs[:] = []\n        for file_name in invalid_file_patterns:\n            if fnmatch(root, file_name):\n                files[:] = []\n        for file in files:\n            yield join(root, file)\n\n", "idx": 402}
{"namespace": "pythonforandroid.bootstrap._cmp_bootstraps_by_priority", "completion": "    if a.priority == b.priority:\n        return cmp(a.name, b.name)\n    else:\n        return cmp(a.priority, b.priority)\n\n", "idx": 403}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.all_bootstraps", "completion": "        # Get the bootstraps directory:\n        bootstrap_dirs = cls.get_bootstrap_dirs()\n\n        # Get all the available bootstraps:\n        bootstraps = []\n        for bootstrap_dir in bootstrap_dirs:\n            for filename in os.listdir(bootstrap_dir):\n                if filename.endswith('.py'):\n                    bootstrap_name = filename[:-3]\n                    bootstraps.append(getattr(cls, bootstrap_name))\n        return bootstraps\n", "idx": 404}
{"namespace": "mmcv.image.colorspace._convert_input_type_range", "completion": "    if img.dtype == np.uint8:\n        img = img.astype(np.float32) / 255.0\n    elif img.dtype != np.float32:\n        raise TypeError(f'Input image must be of type np.uint8 or np.float32, but got {img.dtype}')\n    return img\n\n", "idx": 405}
{"namespace": "mackup.utils.error", "completion": "    print(message)\n    sys.exit(1)\n\n", "idx": 406}
{"namespace": "mmcv.image.colorspace._convert_output_type_range", "completion": "    img_type = img.dtype\n    if img_type == np.float32:\n        if dst_type == np.uint8:\n            img *= 255.\n        elif dst_type == np.float32:\n            pass\n        else:\n            raise TypeError('The dst_type should be np.uint8 or np.float32, '\n                            f'but got {dst_type}')\n    elif img_type == np.uint8:\n        if dst_type == np.uint8:\n            pass\n        elif dst_type == np.float32:\n            img /= 255.\n        else:\n            raise TypeError('The dst_type should be np.uint8 or np.float32, '\n                            f'but got {dst_type}')\n    else:\n        raise TypeError('The img type should be np.float32 or np.uint8, '\n                        f'but got {img_type}')\n    return img.astype(dst_type)\n\n", "idx": 407}
{"namespace": "mackup.utils.is_process_running", "completion": "    # Check if the process is running\n    process_running = subprocess.Popen(\n        [\"pgrep\", \"-f\", process_name], stdout=subprocess.PIPE\n    )\n    output, error = process_running.communicate()\n\n    if output:\n        return True\n    else:\n        return False\n\n", "idx": 408}
{"namespace": "stellar.operations._get_pid_column", "completion": "    server_version = raw_conn.server_version\n    version_number = server_version.split('.')[0]\n    if version_number == '10':\n        return 'pid'\n    elif version_number == '9':\n        return 'procpid'\n    elif version_number == '8':\n        return 'procpid'\n    elif version_number == '7':\n        return 'procpid'\n    elif version_number == '6':\n        return 'procpid'\n    elif version_number == '5':\n        return 'procpid'\n    elif version_number == '4':\n        return 'procpid'\n    elif version_number == '3':\n        return 'procpid'\n    elif version_number == '2':\n        return 'procpid'\n    elif version_number == '1':\n        return 'procpid'\n    else:\n        return 'procpid'\n\n", "idx": 409}
{"namespace": "imapclient.imap_utf7.encode", "completion": "    if isinstance(s, str):\n        s = s.encode('utf-8')\n\n    if len(s) == 0:\n        return b''\n\n    # The first byte is the shift character, which is & in this modified UTF-7 convention.\n    # The rest of the bytes are the base64 encoding of the input string.\n    # The base64 encoding is done using the standard Python base64 module.\n    # The base64 encoding is done in the ASCII context, which is the default.\n    # The ASCII context is used to encode the input string, which is in the UTF-8 context.\n    # The ASCII context is used to encode the base64 encoding, which is in the UTF-8 context.\n    # The ASCII context is used to encode the base64 encoding, which is in the UTF-8 context.\n    # The ASCII context is used to encode the base64 encoding, which is in the UTF-8 context.\n    # The ASCII context is used to encode the base64 encoding, which is in the UTF-8 context.\n    # The ASCII context is used to encode the base64 encoding, which is in the UTF-8 context.\n    # The ASCII context is used to encode the base64 encoding, which is in the UTF-8 context.\n    # The ASCII context is used to encode the base64 encoding, which is in the UTF-8 context.\n    # The ASCII context is used to encode the base64 encoding, which is in the UTF-8 context.\n    # The ASCII context is used to encode the base64 encoding, which is in the UTF-8 context.\n    # The ASCII context is used to encode the base64 encoding, which is in the UTF-8 context.\n    # The ASCII context is used to encode the base64 encoding, which is in the UTF-8 context.\n    # The ASCII context is used to encode the base64 encoding, which is in the UTF-8 context.\n    # The ASCII context is used to encode the base64 encoding, which is in the UTF-8 context.\n    # The ASCII context is used to encode the base64 encoding, which is in the UTF-8 context.\n    # The ASCII context is used to encode the base64 encoding, which is in the UTF-8 context.\n    # The ASCII context is used to encode", "idx": 410}
{"namespace": "imapclient.version._imapclient_version_string", "completion": "    major, minor, micro, releaselevel = vinfo\n    return f\"{major}.{minor}.{micro}.{releaselevel}\"", "idx": 411}
{"namespace": "telethon.helpers.generate_key_data_from_nonce", "completion": "    server_nonce_bytes = server_nonce.to_bytes(8, 'big')\n    new_nonce_bytes = new_nonce.to_bytes(8, 'big')\n\n    hash1 = sha1(server_nonce_bytes).digest()\n    hash2 = sha1(new_nonce_bytes).digest()\n    hash3 = sha1(hash2).digest()\n\n    key = hash1[:12] + hash2[:12]\n    iv = hash2[12:] + hash3[:4] + new_nonce_bytes[:4]\n\n    return key, iv\n\n", "idx": 412}
{"namespace": "hbmqtt.codecs.bytes_to_int", "completion": "    return unpack('>I', data)[0]\n\n", "idx": 413}
{"namespace": "zulipterminal.helper.display_error_if_present", "completion": "    if \"msg\" in response and controller.view:\n        controller.view.error_message(response[\"msg\"])\n\n", "idx": 414}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._decode_message_id", "completion": "        from zulipterminal.config.regexes import REGEX_MESSAGE_ID\n\n        if re.match(REGEX_MESSAGE_ID, message_id):\n            return int(message_id)\n        else:\n            return None\n", "idx": 415}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton.handle_narrow_link", "completion": "        parsed_link = self._parse_narrow_link(self.link)\n        error = self._validate_narrow_link(parsed_link)\n        if error:\n            self.view.footer.update_footer(error)\n        else:\n            self._switch_narrow_to(parsed_link)\n\n", "idx": 416}
{"namespace": "zulipterminal.config.color.color_properties", "completion": "    if not isinstance(colors, Enum):\n        raise TypeError(\"Colors must be an Enum.\")\n\n    if not isinstance(prop, tuple):\n        raise TypeError(\"Properties must be a tuple.\")\n\n    if not all(isinstance(p, str) for p in prop):\n        raise TypeError(\"Properties must be strings.\")\n\n    if not all(p.isupper() for p in prop):\n        raise TypeError(\"Properties must be uppercase.\")\n\n    if not all(p in DefaultColor for p in prop):\n        raise TypeError(\"Properties must be in DefaultColor.\")\n\n    if not all(p in DefaultColor.__members__ for p in prop):\n        raise TypeError(\"Properties must be in DefaultColor.\")\n\n    if not all(p in DefaultColor.__members__.values() for p in prop):\n        raise TypeError(\"Properties must be in DefaultColor.\")\n\n    if not all(p in DefaultColor.__members__.values() for p in prop):\n        raise TypeError(\"Properties must be in DefaultColor.\")\n\n    if not all(p in DefaultColor.__members__.values() for p in prop):\n        raise TypeError(\"Properties must be in DefaultColor.\")\n\n    if not all(p in DefaultColor.__members__.values() for p in prop):\n        raise TypeError(\"Properties must be in DefaultColor.\")\n\n    if not all(p in DefaultColor.__members__.values() for p in prop):\n        raise TypeError(\"Properties must be in DefaultColor.\")\n\n    if not all(p in DefaultColor.__members__.values() for p in prop):\n        raise TypeError(\"Properties must be in DefaultColor.\")\n\n    if not all(p in DefaultColor.__members__.values() for p in prop):\n        raise TypeError(\"Properties must be in DefaultColor.\")\n\n    if not all(p in DefaultColor.__members__.values() for p in prop):\n        raise TypeError(\"Properties must be in DefaultColor.\")\n\n    if not all(p in DefaultColor.__members__.values() for p in prop):\n        raise TypeError(\"Properties must be in DefaultColor.\")\n\n    if not all(p in DefaultColor.__members__.values() for p in prop):\n        raise TypeError(\"Properties must be in DefaultColor.\")\n\n    if not all(p in DefaultColor.__members__.values() for p in prop):\n        raise TypeError(\"Properties must be in DefaultColor.\")\n\n    if not all(p in DefaultColor.__members__.values() for p in prop):\n        raise TypeError(\"Properties must be in DefaultColor.\")", "idx": 417}
{"namespace": "twilio.base.deserialize.decimal", "completion": "    if d is None:\n        return d\n    if d == \"\":\n        return d\n    return BasicContext(prec=100).create_decimal(d)", "idx": 418}
{"namespace": "twilio.base.deserialize.integer", "completion": "    try:\n        return int(i)\n    except ValueError:\n        return i", "idx": 419}
{"namespace": "twilio.base.serialize.object", "completion": "    if isinstance(obj, dict):\n        return json.dumps(obj)\n    elif isinstance(obj, list):\n        return json.dumps(obj)\n    elif isinstance(obj, str):\n        return obj\n    else:\n        return str(obj)", "idx": 420}
{"namespace": "twilio.base.serialize.map", "completion": "    return [serialize_func(x) for x in lst]", "idx": 421}
{"namespace": "twilio.base.obsolete.deprecated_method", "completion": "    def deprecated_method_wrapper(func):\n        @functools.wraps(func)\n        def new_func(*args, **kwargs):\n            warnings.warn(\n                \"{} has been deprecated from this version of the library. \"\n                \"Please refer to current documentation for guidance.\".format(func.__name__)\n            )\n            return func(*args, **kwargs)\n\n        return new_func\n\n    return deprecated_method_wrapper", "idx": 422}
{"namespace": "chatette.utils.sample_indulgent", "completion": "    if nb_items > len(array):\n        return deepcopy(array)\n    return sample(array, nb_items)\n\n", "idx": 423}
{"namespace": "chatette.utils.rchop", "completion": "    if string.endswith(ending):\n        return string[:-len(ending)]\n    return string\n\n", "idx": 424}
{"namespace": "chatette.utils.str_to_bool", "completion": "    if text.lower() == \"true\":\n        return True\n    elif text.lower() == \"false\":\n        return False\n    else:\n        raise ValueError(\"The string {} is not a boolean.\".format(text))\n\n", "idx": 425}
{"namespace": "chatette.utils.min_if_exist", "completion": "    if n1 is None and n2 is None:\n        return None\n    if n1 is None:\n        return n2\n    if n2 is None:\n        return n1\n    return min(n1, n2)\n\n", "idx": 426}
{"namespace": "chatette.utils.append_to_list_in_dict", "completion": "    if key not in dict_of_lists:\n        dict_of_lists[key] = [value]\n    else:\n        dict_of_lists[key].append(value)\n\n", "idx": 427}
{"namespace": "chatette.utils.extend_list_in_dict", "completion": "    if key not in dict_of_lists:\n        dict_of_lists[key] = values\n    else:\n        dict_of_lists[key].extend(values)\n\n", "idx": 428}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy._is_end_regex", "completion": "        if len(word) < 2:\n            return False\n        if word[-2] != '\\\\':\n            return False\n        if word[-1] not in ['g', 'i', 'G', 'I']:\n            return False\n        return True\n", "idx": 429}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.execute", "completion": "        pass", "idx": 430}
{"namespace": "aioxmpp.network.group_and_order_srv_records", "completion": "    if rng is None:\n        rng = random.Random()\n\n    # Sort the records by priority\n    all_records.sort(key=lambda x: x[0])\n\n    # Group the records by priority\n    grouped_records = itertools.groupby(all_records, key=lambda x: x[0])\n\n    # Order the records by weight\n    ordered_records = []\n    for _, group in grouped_records:\n        group = list(group)\n        group.sort(key=lambda x: x[1])\n        ordered_records.extend(group)\n\n    # Order the records by port\n    ordered_records.sort(key=lambda x: x[2])\n\n    # Return the ordered records\n    return ordered_records\n\n", "idx": 431}
{"namespace": "aioxmpp.nonza.StreamFeatures.get_feature", "completion": "        try:\n            return self.features[feature_cls.TAG][0]\n        except IndexError:\n            return default\n", "idx": 432}
{"namespace": "aioxmpp.connector.XMPPOverTLSConnector._context_factory_factory", "completion": "        def context_factory():\n            ssl_context = metadata.ssl_context_factory()\n            ssl_context.set_alpn_protocols([\"xmpp-client\"])\n            verifier.setup_context(ssl_context, None)\n            return ssl_context\n\n        return context_factory\n", "idx": 433}
{"namespace": "aioxmpp.xmltestutils.element_path", "completion": "    path = \"\"\n    while el is not None:\n        if el.tag == \"{%s}text\" % el.nsmap[\"xsi\"]:\n            path = \"text\" + path\n        else:\n            path = el.tag + \"[\" + str(el.index) + \"]\" + path\n        el = el.getparent()\n        if el is upto:\n            break\n    return path\n\n", "idx": 434}
{"namespace": "aioxmpp.structs.JID.fromstr", "completion": "        if not s:\n            raise ValueError(\"JID must not be empty\")\n\n        if strict:\n            s = s.strip()\n\n        if not s:\n            raise ValueError(\"JID must not be empty\")\n\n        if strict:\n            if s.count(\"@\") > 1:\n                raise ValueError(\"JID must not contain more than one @\")\n            if s.count(\"/\") > 1:\n                raise ValueError(\"JID must not contain more than one /\")\n\n        if strict:\n            if s.count(\"@\") == 1:\n                localpart, domain = s.split(\"@\", 1)\n                if s.count(\"/\") == 1:\n                    localpart, resource = localpart.split(\"/\", 1)\n                else:\n                    resource = None\n            else:\n                localpart = None\n                domain, resource = s.split(\"/\", 1)\n        else:\n            if s.count(\"@\") == 1:\n                localpart, domain = s.split(\"@\", 1)\n                if s.count(\"/\") == 1:\n                    localpart, resource = localpart.split(\"/\", 1)\n                else:\n                    resource = None\n            else:\n                localpart = None\n                domain, resource = s.split(\"/\", 1)\n\n        return cls(localpart, domain, resource, strict=strict)\n\n", "idx": 435}
{"namespace": "aioxmpp.security_layer.extract_python_dict_from_x509", "completion": "    result = {}\n    result['subject'] = x509.get_subject().get_components()\n    result['subjectAltName'] = x509.get_subject_alt_name()\n    return result\n\n", "idx": 436}
{"namespace": "aioxmpp.security_layer.extract_blob", "completion": "    # Get the ASN.1 blob from the X509 certificate\n    der_cert = OpenSSL.crypto.dump_certificate(OpenSSL.crypto.FILETYPE_ASN1, x509)\n\n    # Decode the ASN.1 blob\n    der_cert = pyasn1.codec.der.decoder.decode(der_cert, asn1Spec=pyasn1_modules.rfc2459.Certificate())[0]\n\n    # Get the ASN.1 blob from the certificate\n    der_cert = der_cert.getComponentByName(\"tbsCertificate\").asOctets()\n\n    return der_cert\n\n", "idx": 437}
{"namespace": "aioxmpp.security_layer.blob_to_pyasn1", "completion": "    return pyasn1.codec.der.decoder.decode(blob, asn1Spec=pyasn1_modules.rfc2459.Certificate())[0]\n\n", "idx": 438}
{"namespace": "aioxmpp.security_layer.extract_pk_blob_from_pyasn1", "completion": "    return pyasn1.codec.der.encoder.encode(\n        pyasn1_struct.getComponentByName(\"tbsCertificate\").getComponentByName(\"subjectPublicKeyInfo\").getComponentByName(\"publicKey\").getComponentByName(\"key\")\n    )\n\n", "idx": 439}
{"namespace": "aioxmpp.callbacks.AdHocSignal.ASYNC_WITH_LOOP", "completion": "        if not hasattr(f, \"__call__\"):\n            raise TypeError(\"must be callable, got {!r}\".format(f))\n        return functools.partial(cls._async_wrapper, loop)\n", "idx": 440}
{"namespace": "aioxmpp.callbacks.AdHocSignal.SPAWN_WITH_LOOP", "completion": "        if loop is None:\n            loop = asyncio.get_event_loop()\n\n        def create_wrapper(f):\n            if not hasattr(f, \"__call__\"):\n                raise TypeError(\"must be callable, got {!r}\".format(f))\n            if not asyncio.iscoroutinefunction(f):\n                raise TypeError(\"must be a coroutine function, got {!r}\".format(f))\n            return functools.partial(cls._spawn_wrapper,\n                                     f,\n                                     loop)\n\n        return create_wrapper\n", "idx": 441}
{"namespace": "aioxmpp.callbacks.first_signal", "completion": "    dispatcher = SignalDispatcher()\n    for signal in signals:\n        dispatcher.add_callback(signal, OneshotTagListener(dispatcher.unicast))\n    return dispatcher.future()", "idx": 442}
{"namespace": "aioxmpp.tasks.TaskPool.spawn", "completion": "        coro = __coro_fun(*args, **kwargs)\n        return self.add(__groups, coro)", "idx": 443}
{"namespace": "aioxmpp.protocol.send_and_wait_for", "completion": "    if timeout is None:\n        timeout = float('inf')\n    if cb is not None:\n        xmlstream.on_message.connect(cb)\n    xmlstream.send_xso(send)\n    try:\n        response = await asyncio.wait_for(wait_for, timeout)\n    except asyncio.TimeoutError:\n        raise TimeoutError(\"Timeout while waiting for response\")\n    finally:\n        xmlstream.on_message.disconnect(cb)\n    return response\n", "idx": 444}
{"namespace": "aioxmpp.testutils.run_coroutine_with_peer", "completion": "    if not loop:\n        loop = asyncio.get_event_loop()\n    local_future = asyncio.Future(loop=loop)\n    peer_future = asyncio.Future(loop=loop)\n    local_future.set_result(None)\n    peer_future.set_result(None)\n    loop.create_task(coroutine)\n    loop.create_task(peer_coroutine)\n    loop.create_task(asyncio.wait_for(asyncio.gather(local_future, peer_future), timeout=timeout))\n    return local_future.result()\n\n", "idx": 445}
{"namespace": "aioxmpp.testutils.make_listener", "completion": "    listener = unittest.mock.Mock()\n    for signal in instance.signals:\n        listener.__setattr__(signal.name, unittest.mock.Mock())\n    return listener\n\n", "idx": 446}
{"namespace": "aioxmpp.vcard.service.VCardService.set_vcard", "completion": "        iq = aioxmpp.IQ(\n            type_=aioxmpp.IQType.SET,\n            to=jid,\n            payload=vcard,\n        )\n\n        await self.client.send(iq)", "idx": 447}
{"namespace": "aioxmpp.rsm.xso.ResultSetMetadata.limit", "completion": "        result = copy.deepcopy(self)\n        result.max_ = max_\n        return result\n", "idx": 448}
{"namespace": "aioxmpp.muc.service.Room.features", "completion": "        return self._service.features\n", "idx": 449}
{"namespace": "aioxmpp.xso.query.EvaluationContext.eval_bool", "completion": "        return bool(expr.eval(self))\n\n", "idx": 450}
{"namespace": "aioxmpp.xso.query._BoolOpMixin.eval", "completion": "        result = self.expr.eval(ec)\n        iterator = iter(result)\n        try:\n            next(iterator)\n        except StopIteration:\n            return False\n        else:\n            return True\n        finally:\n            if hasattr(iterator, \"close\"):\n                iterator.close()\n\n", "idx": 451}
{"namespace": "aioxmpp.xso.model.drop_handler", "completion": "    depth = 0\n    while True:\n        ev_type, *ev_args = yield\n        if ev_type == \"start\":\n            depth += 1\n        elif ev_type == \"end\":\n            depth -= 1\n            if depth == 0:\n                break\n\n", "idx": 452}
{"namespace": "aioxmpp.xso.model.guard", "completion": "    depth = 1\n    while depth:\n        ev = yield\n        if ev[0] == \"start\":\n            depth += 1\n        elif ev[0] == \"end\":\n            depth -= 1\n        dest.send(ev)\n    try:\n        return dest.send(None)\n    except StopIteration as err:\n        return err.value\n    except:  # NOQA\n        raise\n\n", "idx": 453}
{"namespace": "aioxmpp.xso.model.capture_events", "completion": "    try:\n        while True:\n            ev = yield\n            dest.append(ev)\n    except GeneratorExit:\n        dest.clear()\n    except:  # NOQA\n        dest.clear()\n        raise\n\n", "idx": 454}
{"namespace": "aioxmpp.xso.model.events_to_sax", "completion": "    for ev_type, *ev_args in events:\n        if ev_type == \"start\":\n            dest.startElementNS(ev_args[0], ev_args[1], ev_args[2])\n        elif ev_type == \"end\":\n            dest.endElementNS(ev_args[0], ev_args[1])\n        elif ev_type == \"text\":\n            dest.characters(ev_args[0])\n        else:\n            raise ValueError(\"unknown event type {!r}\".format(ev_type))\n\n", "idx": 455}
{"namespace": "aioxmpp.adhoc.service.AdHocClient.get_command_info", "completion": "        disco = self.dependencies[aioxmpp.disco.DiscoClient]\n        response = await disco.query_items(\n            peer_jid,\n            node=namespaces.xep0050_commands,\n            node_type=namespaces.xep0050_command,\n            node_name=command_name,\n        )\n        return response.items[0]\n", "idx": 456}
{"namespace": "aioxmpp.entitycaps.caps115.build_identities_string", "completion": "    identities_string = \"\"\n    for identity in identities:\n        identities_string += identity.to_string() + \"<\"\n\n    identities_string = identities_string.encode(\"utf-8\")\n    identities_string = identities_string.replace(b\"<\", b\"<\")\n    identities_string = identities_string.replace(b\">\", b\">\")\n    identities_string = identities_string.replace(b\"<\", b\"<\")\n    identities_string = identities_string.replace(b\">\", b\">\")\n    identities_string = identities_string.replace(b\"<\", b\"<\")\n    identities_string = identities_string.replace(b\">\", b\">\")\n    identities_string = identities_string.replace(b\"<\", b\"<\")\n    identities_string = identities_string.replace(b\">\", b\">\")\n    identities_string = identities_string.replace(b\"<\", b\"<\")\n    identities_string = identities_string.replace(b\">\", b\">\")\n    identities_string = identities_string.replace(b\"<\", b\"<\")\n    identities_string = identities_string.replace(b\">\", b\">\")\n    identities_string = identities_string.replace(b\"<\", b\"<\")\n    identities_string = identities_string.replace(b\">\", b\">\")\n    identities_string = identities_string.replace(b\"<\", b\"<\")\n    identities_string = identities_string.replace(b\">\", b\">\")\n    identities_string = identities_string.replace(b\"<\", b\"<\")\n    identities_string = identities_string.replace(b\">\", b\">\")\n    identities_string = identities_string.replace(b\"<\", b\"<\")\n    identities_string = identities_string.replace(b\">\", b\">\")\n    identities_string = identities_string.replace(b\"<\", b\"<\")\n    identities_string = identities_string.replace(b\">\", b\">\")\n    identities_string = identities_string.replace(b\"<\", b\"<\")\n    identities_string = identities_string.replace(b\">\", b", "idx": 457}
{"namespace": "aioxmpp.entitycaps.caps115.build_features_string", "completion": "    features = [\n        escape(feature).encode(\"utf-8\")\n        for feature in features\n    ]\n\n    if len(set(features)) != len(features):\n        raise ValueError(\"duplicate feature\")\n\n    features.sort()\n    features.append(b\"\")\n    return b\"<\".join(features)\n\n\n", "idx": 458}
{"namespace": "aioxmpp.entitycaps.caps115.build_forms_string", "completion": "    forms = [\n        b\"/\".join([\n            escape(form.type_).encode(\"utf-8\"),\n            escape(form.lang or \"\").encode(\"utf-8\"),\n            escape(form.action or \"\").encode(\"utf-8\"),\n            escape(form.method or \"\").encode(\"utf-8\"),\n            escape(form.accept or \"\").encode(\"utf-8\"),\n            escape(form.accept_charset or \"\").encode(\"utf-8\"),\n            escape(form.enctype or \"\").encode(\"utf-8\"),\n            escape(form.encoding or \"\").encode(\"utf-8\"),\n            escape(form.name or \"\").encode(\"utf-8\"),\n            escape(form.target or \"\").encode(\"utf-8\"),\n        ])\n        for form in forms\n    ]\n\n    if len(set(forms)) != len(forms):\n        raise ValueError(\"duplicate form\")\n\n    forms.sort()\n    forms.append(b\"\")\n    return b\"<\".join(forms)\n\n", "idx": 459}
{"namespace": "aioxmpp.entitycaps.caps115.Key.path", "completion": "        return pathlib.Path(\n            urllib.parse.quote(self.node, safe=\"\")\n        ).joinpath(\n            \"hashes\",\n            \"{}-{}\".format(self.algo, self.node)\n        )\n\n", "idx": 460}
{"namespace": "aioxmpp.entitycaps.caps390._process_features", "completion": "    features_bytes = b''.join(map(lambda feature: feature.encode('utf-8'), features))\n    return base64.b64encode(features_bytes)\n\n", "idx": 461}
{"namespace": "aioxmpp.entitycaps.caps390._process_identities", "completion": "    parts = [\n        _process_identity(identity)\n        for identity in identities\n    ]\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1d\"\n\n", "idx": 462}
{"namespace": "aioxmpp.entitycaps.caps390._process_extensions", "completion": "    parts = [\n        _process_form(form)\n        for form in exts\n    ]\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1c\"\n\n", "idx": 463}
{"namespace": "aioxmpp.entitycaps.caps390._calculate_hash", "completion": "    if algo == \"sha-1\":\n        return aioxmpp.hashes.sha1(hash_input)\n    elif algo == \"sha-256\":\n        return aioxmpp.hashes.sha256(hash_input)\n    elif algo == \"sha-512\":\n        return aioxmpp.hashes.sha512(hash_input)\n    else:\n        raise ValueError(\"Unknown hash algorithm: {}\".format(algo))\n\n", "idx": 464}
{"namespace": "aioxmpp.entitycaps.caps390.Key.node", "completion": "        return base64.urlsafe_b64encode(self.digest).decode(\"utf-8\")\n", "idx": 465}
{"namespace": "aioxmpp.entitycaps.caps390.Key.path", "completion": "        return \"{}.{}/{}\".format(\n            base64.b64encode(self.digest).decode(\"ascii\"),\n            self.algo,\n            \"xml\"\n        )\n", "idx": 466}
{"namespace": "aioxmpp.entitycaps.caps390.Implementation.extract_keys", "completion": "        if presence.xep0390_caps is not None:\n            for key in presence.xep0390_caps.keys:\n                yield Key(key.algo, key.digest)\n        else:\n            return\n", "idx": 467}
{"namespace": "aioxmpp.roster.service.RosterClient.approve", "completion": "        self.client.send(\n            stanza.Presence(\n                to=peer_jid,\n                type=structs.PresenceType.SUBSCRIBED\n            )\n        )\n", "idx": 468}
{"namespace": "aioxmpp.roster.service.RosterClient.subscribe", "completion": "        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.SUBSCRIBE,\n                            to=peer_jid)\n        )\n", "idx": 469}
{"namespace": "aioxmpp.roster.service.RosterClient.unsubscribe", "completion": "        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.UNSUBSCRIBE,\n                            to=peer_jid)\n        )\n\n", "idx": 470}
{"namespace": "aioxmpp.forms.fields.BoundSingleValueField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass\n", "idx": 471}
{"namespace": "aioxmpp.forms.fields.BoundMultiValueField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass\n", "idx": 472}
{"namespace": "aioxmpp.forms.fields.BoundOptionsField.options", "completion": "        try:\n            del self._options\n        except AttributeError:\n            pass\n", "idx": 473}
{"namespace": "aioxmpp.forms.fields.BoundSelectField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass\n", "idx": 474}
{"namespace": "aioxmpp.forms.fields.BoundMultiSelectField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass\n", "idx": 475}
{"namespace": "cupy.random._generator.reset_states", "completion": "    global _random_states\n    _random_states = {}\n\n", "idx": 476}
{"namespace": "cupy.random._generator._check_and_get_dtype", "completion": "    if isinstance(dtype, numpy.dtype):\n        dtype = dtype.char\n    if dtype == 'f':\n        return numpy.float32\n    elif dtype == 'd':\n        return numpy.float64\n    elif dtype == 'i':\n        return numpy.int32\n    elif dtype == 'l':\n        return numpy.int64\n    elif dtype == 'u':\n        return numpy.uint32\n    elif dtype == 'L':\n        return numpy.uint64\n    elif dtype == 'b':\n        return numpy.bool_\n    elif dtype == 'c':\n        return numpy.complex64\n    elif dtype == 'C':\n        return numpy.complex128\n    else:\n        raise ValueError('dtype must be one of f, d, i, l, u, L, b, c, C')", "idx": 477}
{"namespace": "cupy_builder._command.filter_files_by_extension", "completion": "    \"\"\"\n    Filter the files in the given list based on the file extension. It separates the files with the given extension from the rest of the files.\n    Input-Output Arguments\n    :param sources: List of strings. The list of file paths to be filtered.\n    :param extension: String. The file extension to be used for filtering.\n    :return: Two lists of strings. The first list contains the files with the given extension, and the second list contains the rest of the files.\n    \"\"\"\n\n    \"\"\"\n    Filter the files in the given list based on the file extension. It separates the files with the given extension from the rest of the files.\n    Input-Output Arguments\n    :param sources: List of strings. The list of file paths to be filtered.\n    :param extension: String. The file extension to be used for filtering.\n    :return: Two lists of strings. The first list contains the files with the given extension, and the second list contains the rest of the files.\n    \"\"\"\n\n    \"\"\"\n    Filter the files in the given list based on the file extension. It separates the files with the given extension from the rest of the files.\n    Input-Output Arguments\n    :param sources: List of strings. The list of file paths to be filtered.\n    :param extension: String. The file extension to be used for filtering.\n    :return: Two lists of strings. The first list contains the files with the given extension, and the second list contains the rest of the files.\n    \"\"\"\n\n    \"\"\"\n    Filter the files in the given list based on the file extension. It separates the files with the given extension from the rest of the files.\n    Input-Output Arguments\n    :param sources: List of strings. The list of file paths to be filtered.\n    :param extension: String. The file extension to be used for filtering.\n    :return: Two lists of strings. The first list contains the files with the given extension, and the second list contains the rest of the files.\n    \"\"\"\n\n    \"\"\"\n    Filter the files in the given list based on the file extension. It separates the files with the given extension from the rest of the files.\n    Input-Output Arguments\n    :param sources: List of strings. The list of file paths to be filtered.\n    :param extension: String. The file extension to be used for filtering.\n    :return: Two lists of strings. The first list contains", "idx": 478}
{"namespace": "datasets.table._in_memory_arrow_table_from_file", "completion": "    # Read the Arrow file from the given filename.\n    arrow_file = pa.open_file(filename)\n\n    # Read the Arrow table from the Arrow file.\n    arrow_table = arrow_file.read_all()\n\n    # Return the in-memory Arrow table.\n    return arrow_table\n\n", "idx": 479}
{"namespace": "datasets.table._in_memory_arrow_table_from_buffer", "completion": "    buffer_reader = pa.ipc.BufferReader(buffer)\n    opened_stream = pa.ipc.open_stream(buffer_reader)\n    pa_table = opened_stream.read_all()\n    return pa_table\n\n", "idx": 480}
{"namespace": "datasets.table._interpolation_search", "completion": "    if not arr:\n        raise IndexError(\"The array is empty.\")\n\n    if x < arr[0]:\n        raise IndexError(\"The query is outside the array values.\")\n\n    if x >= arr[-1]:\n        raise IndexError(\"The query is outside the array values.\")\n\n    i = 0\n    j = len(arr) - 1\n\n    while i < j:\n        mid = (i + j) // 2\n        if arr[mid] <= x:\n            i = mid + 1\n        else:\n            j = mid\n\n    return i\n\n", "idx": 481}
{"namespace": "datasets.data_files._is_inside_unrequested_special_dir", "completion": "    if matched_rel_path.startswith((\".git\", \".hg\", \".svn\", \".idea\", \".vscode\", \".DS_Store\")):\n        return True\n    if matched_rel_path.startswith((\"__pycache__\", \".ipynb_checkpoints\")):\n        return True\n    if matched_rel_path.startswith((\"data\", \"code\", \"src\", \"srcs\", \"srcs_\")):\n        return True\n    if matched_rel_path.startswith((\"data\", \"code\", \"src\", \"srcs\", \"srcs_\")):\n        return True\n    if matched_rel_path.startswith((\"data\", \"code\", \"src\", \"srcs\", \"srcs_\")):\n        return True\n    if matched_rel_path.startswith((\"data\", \"code\", \"src\", \"srcs\", \"srcs_\")):\n        return True\n    if matched_rel_path.startswith((\"data\", \"code\", \"src\", \"srcs\", \"srcs_\")):\n        return True\n    if matched_rel_path.startswith((\"data\", \"code\", \"src\", \"srcs\", \"srcs_\")):\n        return True\n    if matched_rel_path.startswith((\"data\", \"code\", \"src\", \"srcs\", \"srcs_\")):\n        return True\n    if matched_rel_path.startswith((\"data\", \"code\", \"src\", \"srcs\", \"srcs_\")):\n        return True\n    if matched_rel_path.startswith((\"data\", \"code\", \"src\", \"srcs\", \"srcs_\")):\n        return True\n    if matched_rel_path.startswith((\"data\", \"code\", \"src\", \"srcs\", \"srcs_\")):\n        return True\n    if matched_rel_path.startswith((\"data\", \"code\", \"src\", \"srcs\", \"srcs_\")):\n        return True\n    if matched_rel_path.startswith((\"data\", \"code\", \"src\", \"srcs\", \"srcs_\")):\n        return True\n    if matched_rel_path.startswith((\"data\", \"code\", \"src\", \"srcs\", \"srcs_\")):\n        return True\n    if matched_rel_path.startswith((\"data\", \"code\", \"src\", \"srcs\", \"srcs_\")):\n        return True\n    if matched_rel_path.", "idx": 482}
{"namespace": "datasets.data_files._is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir", "completion": "    # We just need to check if every special directories from the path is present explicly in the pattern.\n    # Since we assume that the path matches the pattern, it's equivalent to counting that both\n    # the parent path and the parent pattern have the same number of special directories.\n    data_dirs_to_ignore_in_path = [part for part in PurePath(matched_rel_path).parent.parts if part.startswith(\".\")]\n    data_dirs_to_ignore_in_pattern = [part for part in PurePath(pattern).parent.parts if part.startswith(\".\")]\n    return len(data_dirs_to_ignore_in_path) != len(data_dirs_to_ignore_in_pattern)\n\n", "idx": 483}
{"namespace": "datasets.iterable_dataset._batch_to_examples", "completion": "    return [dict(zip(batch, example)) for example in zip(*batch.values())]\n\n", "idx": 484}
{"namespace": "datasets.iterable_dataset._examples_to_batch", "completion": "    columns = set(example.keys() for example in examples)\n    return {\n        column: [example[column] for example in examples]\n        for column in columns\n    }\n\n", "idx": 485}
{"namespace": "datasets.iterable_dataset.RandomlyCyclingMultiSourcesExamplesIterable._iter_random_indices", "completion": "        if p is None:\n            p = [1 / num_sources] * num_sources\n        while True:\n            yield rng.choice(num_sources, size=random_batch_size, p=p)\n", "idx": 486}
{"namespace": "datasets.iterable_dataset.BufferShuffledExamplesIterable._iter_random_indices", "completion": "        while True:\n            yield from (int(i) for i in rng.integers(0, buffer_size, size=random_batch_size))\n", "idx": 487}
{"namespace": "datasets.iterable_dataset.IterableDataset.remove_columns", "completion": "        if isinstance(column_names, str):\n            column_names = [column_names]\n        return self.map(partial(_remove_columns_fn, column_names=column_names), remove_columns=column_names)\n", "idx": 488}
{"namespace": "datasets.dataset_dict.DatasetDict.with_format", "completion": "        self._check_values_type()\n        return DatasetDict(\n            {k: dataset.with_format(type=type, columns=columns, output_all_columns=output_all_columns, **format_kwargs) for k, dataset in self.items()}\n        )\n", "idx": 489}
{"namespace": "datasets.dataset_dict.DatasetDict.with_transform", "completion": "        dataset = copy.deepcopy(self)\n        dataset.set_format(\"custom\", columns=columns, output_all_columns=output_all_columns, transform=transform)\n        return dataset\n", "idx": 490}
{"namespace": "datasets.dataset_dict.DatasetDict.align_labels_with_mapping", "completion": "        self._check_values_type()\n        return DatasetDict({k: dataset.align_labels_with_mapping(label2id, label_column) for k, dataset in self.items()})\n", "idx": 491}
{"namespace": "datasets.dataset_dict.IterableDatasetDict.map", "completion": "        return IterableDatasetDict(\n            {k: dataset.map(function, with_indices, input_columns, batched, batch_size, drop_last_batch, remove_columns, fn_kwargs) for k, dataset in self.items()}\n        )\n", "idx": 492}
{"namespace": "datasets.dataset_dict.IterableDatasetDict.filter", "completion": "        return IterableDatasetDict(\n            {\n                k: dataset.filter(\n                    function=function,\n                    with_indices=with_indices,\n                    input_columns=input_columns,\n                    batched=batched,\n                    batch_size=batch_size,\n                    fn_kwargs=fn_kwargs,\n                )\n                for k, dataset in self.items()\n            }\n        )\n", "idx": 493}
{"namespace": "datasets.arrow_dataset.Dataset.num_rows", "completion": "        if self._indices is not None:\n            return self._indices.num_rows\n        else:\n            return self._data.num_rows\n", "idx": 494}
{"namespace": "datasets.filesystems.extract_path_from_uri", "completion": "    if dataset_path.startswith(\"s3://\"):\n        dataset_path = dataset_path[5:]\n    return dataset_path\n\n", "idx": 495}
{"namespace": "datasets.filesystems.is_remote_filesystem", "completion": "    return fs.protocol in fsspec.registry", "idx": 496}
{"namespace": "datasets.utils.file_utils.hash_url_to_filename", "completion": "    if etag is not None:\n        url = url + \".\" + etag\n    h = sha256(url.encode(\"utf-8\")).hexdigest()\n    return h + \".h5\" if url.endswith(\".h5\") else h\n\n", "idx": 497}
{"namespace": "datasets.utils.hub.hf_hub_url", "completion": "    if version.parse(hfh.__version__) < version.parse(\"0.11.0\"):\n        path = quote(path)\n\n    return f\"https://huggingface.co/{repo_id}/blob/{revision}/{path}\"", "idx": 498}
{"namespace": "datasets.utils.sharding._number_of_shards_in_gen_kwargs", "completion": "    if len(gen_kwargs['num_shards_per_dim']) != len(gen_kwargs['num_shards_per_dim'][0]):\n        raise ValueError('The lengths of the lists in the input dictionary are different.')\n\n    return np.prod(gen_kwargs['num_shards_per_dim'])\n\n", "idx": 499}
{"namespace": "datasets.utils.sharding._distribute_shards", "completion": "    # If the number of shards is less than the maximum number of jobs, then each job is given a range of one shard.\n    # The order of shard indices is preserved, and all the jobs are given approximately the same number of shards.\n    if num_shards <= max_num_jobs:\n        return [range(num_shards)]\n\n    # Otherwise, the shards are distributed among the jobs.\n    # The order of shard indices is preserved, and all the jobs are given approximately the same number of shards.\n    num_shards_per_job = int(np.ceil(num_shards / max_num_jobs))\n    return [range(i * num_shards_per_job, (i + 1) * num_shards_per_job) for i in range(max_num_jobs)]", "idx": 500}
{"namespace": "datasets.utils.py_utils.temporary_assignment", "completion": "    original_value = getattr(obj, attr)\n    setattr(obj, attr, value)\n    try:\n        yield\n    finally:\n        setattr(obj, attr, original_value)\n\n", "idx": 501}
{"namespace": "datasets.utils.extract.TarExtractor.extract", "completion": "        # Create the output directory if it does not exist\n        if not os.path.exists(output_path):\n            os.makedirs(output_path)\n\n        # Open the tar file\n        with tarfile.open(input_path, \"r\") as tar:\n            # Extract all the contents to the output path\n            tar.extractall(path=output_path, members=TarExtractor.safemembers(tar, output_path))\n\n", "idx": 502}
{"namespace": "datasets.utils.extract.Extractor.infer_extractor_format", "completion": "        magic_number_length = cls._get_magic_number_max_length()\n        magic_number = cls._read_magic_number(path, magic_number_length=magic_number_length)\n        for extractor_format, extractor in cls.extractors.items():\n            if issubclass(extractor, MagicNumberBaseExtractor) and extractor.is_extractable(path, magic_number=magic_number):\n                return extractor_format\n        return \"\"\n", "idx": 503}
{"namespace": "datasets.utils.py_utils.asdict", "completion": "    if is_dataclass(obj):\n        return {k.name: v for k, v in zip(fields(obj), obj)}\n    elif isinstance(obj, (list, tuple)):\n        return [asdict(o) for o in obj]\n    elif isinstance(obj, dict):\n        return {k: asdict(v) for k, v in obj.items()}\n    else:\n        return obj\n\n", "idx": 504}
{"namespace": "datasets.utils.metadata.MetadataConfigs.from_dataset_card_data", "completion": "        if cls.FIELD_NAME in dataset_card_data:\n            metadata_configs = dataset_card_data[cls.FIELD_NAME]\n            cls._raise_if_data_files_field_not_valid(metadata_configs)\n            return cls(metadata_configs)\n        else:\n            return cls()\n", "idx": 505}
{"namespace": "pymorphy2.analyzer.lang_dict_path", "completion": "    lang_paths = _lang_dict_paths()\n    if lang in lang_paths:\n        return lang_paths[lang]\n    else:\n        raise ValueError(\"Language not found. Please check the spelling of the language.\")\n\n", "idx": 506}
{"namespace": "pymorphy2.dawg.assert_can_create", "completion": "    if not EXTENSION_AVAILABLE:\n        raise NotImplementedError(\"The extension is not available.\")\n\n", "idx": 507}
{"namespace": "pymorphy2.opencorpora_dict.compile._to_paradigm", "completion": "    stem = \"\"\n    paradigm = []\n\n    for word_form, tag in lexeme:\n        prefix = \"\"\n        for pref in paradigm_prefixes:\n            if word_form.startswith(pref):\n                prefix = pref\n                break\n        if prefix:\n            stem = word_form[len(prefix):]\n        else:\n            stem = \"\"\n        paradigm.append((word_form[len(stem):], tag, prefix))\n\n    return stem, paradigm\n\n", "idx": 508}
{"namespace": "pymorphy2.units.by_analogy.KnownPrefixAnalyzer.tag", "completion": "        result = []\n        for prefix, unprefixed_word in self.possible_splits(word_lower):\n            method = (self, prefix)\n\n            tags = self.morph.tag(unprefixed_word)\n            for fixed_word, tag, normal_form, score, methods_stack in tags:\n\n                if not tag.is_productive():\n                    continue\n\n                tag = (\n                    prefix + fixed_word,\n                    tag,\n                    prefix + normal_form,\n                    score * self.score_multiplier,\n                    methods_stack + (method,)\n                )\n\n                add_tag_if_not_seen(tag, result, seen_tags)\n\n        return result\n", "idx": 509}
{"namespace": "pymorphy2.units.by_analogy.UnknownPrefixAnalyzer.tag", "completion": "        result = []\n        for prefix, unprefixed_word in word_splits(word_lower):\n            for tag in self.morph.tag(unprefixed_word):\n                if not tag.is_productive():\n                    continue\n                add_tag_if_not_seen(tag, result, seen_tags)\n        return result\n\n", "idx": 510}
{"namespace": "benedict.dicts.keylist.keylist_util.get_item", "completion": "    if not type_util.is_dict(d):\n        raise TypeError(f\"Invalid dictionary: {d!r}\")\n    if not type_util.is_list(keys):\n        raise TypeError(f\"Invalid keys: {keys!r}\")\n\n    for key in reversed(keys):\n        if type_util.is_dict(d):\n            d = d[key]\n        elif type_util.is_list_or_tuple(d):\n            d = d[key]\n        else:\n            return None\n    return d\n\n", "idx": 511}
{"namespace": "benedict.dicts.keylist.keylist_util.set_item", "completion": "    item = d\n    for key in keys[:-1]:\n        try:\n            item_key, item_value = _get_item_key_and_value(item, key)\n            item = item_value\n        except (IndexError, KeyError):\n            item[key] = {}\n            item = item[key]\n    item[keys[-1]] = value\n\n", "idx": 512}
{"namespace": "benedict.dicts.keypath.keypath_util._split_key_indexes", "completion": "    if not type_util.is_string(key):\n        return [key]\n    if not key.count(\"[\") or not key.endswith(\"]\"):\n        return [key]\n    return [key[0:key.index(\"[\")], key[key.index(\"[\") + 1:key.index(\"]\")]]\n\n", "idx": 513}
{"namespace": "feedparser.urls.make_safe_absolute_uri", "completion": "    if not ACCEPTABLE_URI_SCHEMES:\n        return ''\n\n    if not base:\n        return rel or ''\n\n    if not rel:\n        return base\n\n    if rel.startswith('//'):\n        return base.split('//', 1)[0] + rel\n\n    if rel.startswith('/'):\n        return base.split('/', 3)[0] + rel\n\n    if rel.startswith('?'):\n        return base.split('?', 1)[0] + rel\n\n    if rel.startswith('#'):\n        return base.split('#', 1)[0] + rel\n\n    if rel.startswith('mailto:'):\n        return rel\n\n    if rel.startswith('feed:'):\n        return rel\n\n    if rel.startswith('git:'):\n        return rel\n\n    if rel.startswith('http:'):\n        return rel\n\n    if rel.startswith('https:'):\n        return rel\n\n    if rel.startswith('irc:'):\n        return rel\n\n    if rel.startswith('ircs:'):\n        return rel\n\n    if rel.startswith('itms:'):\n        return rel\n\n    if rel.startswith('magnet:'):\n        return rel\n\n    if rel.startswith('mms:'):\n        return rel\n\n    if rel.startswith('news:'):\n        return rel\n\n    if rel.startswith('nntp:'):\n        return rel\n\n    if rel.startswith('prospero:'):\n        return rel\n\n    if rel.startswith('rsync:'):\n        return rel\n\n    if rel.startswith('rtsp:'):\n        return rel\n\n    if rel.startswith('rtspu:'):\n        return rel\n\n    if rel.startswith('sftp:'):\n        return rel\n\n    if rel.startswith('shttp:'):\n        return rel\n\n    if rel.startswith('sip:'):\n        return rel\n\n    if rel.startswith('sips:'):\n        return rel\n\n    if rel.startswith('snews:'):\n        return rel\n\n    if rel.startswith('svn:'):\n        return rel\n\n    if rel.startswith('svn+ssh:'):\n        return rel\n\n    if rel.startswith('telnet:'):\n        return rel\n\n    if rel.startswith('wais:'):\n        return rel\n\n    if rel.startswith('aim:'):\n        return rel\n\n    if rel.startswith('callto:'):\n        return rel\n\n    if rel.startswith('cvs:'):\n        return rel\n\n    if rel.startswith('facetime:'):\n        return rel\n\n    if rel.startswith('git:'):\n       ", "idx": 514}
{"namespace": "feedparser.api._open_resource", "completion": "    if isinstance(url_file_stream_or_string, str):\n        if url_file_stream_or_string.startswith('http'):\n            # It's a URL.\n            if handlers is None:\n                handlers = []\n            if request_headers is None:\n                request_headers = {}\n            opener = urllib.request.build_opener(*handlers)\n            if agent is not None:\n                request_headers['User-Agent'] = agent\n            if referrer is not None:\n                request_headers['Referer'] = referrer\n            if etag is not None:\n                request_headers['If-None-Match'] = etag\n            if modified is not None:\n                request_headers['If-Modified-Since'] = modified\n            request = urllib.request.Request(url_file_stream_or_string, headers=request_headers)\n            try:\n                response = opener.open(request)\n            except urllib.error.HTTPError as e:\n                if e.code == 304:\n                    # Not modified.\n                    result.status = 304\n                    return None\n                else:\n                    raise\n            result.status = response.code\n            result.headers = response.headers\n            result.url = response.geturl()\n            return response\n        else:\n            # It's a filename.\n            return open(url_file_stream_or_string, 'rb')\n    else:\n        # It's a stream.\n        return url_file_stream_or_string\n\n", "idx": 515}
{"namespace": "feedparser.http._build_urllib2_request", "completion": "    request = urllib.request.Request(url)\n    request.add_header('User-Agent', agent)\n    request.add_header('Accept', accept_header)\n    request.add_header('If-None-Match', etag)\n    request.add_header('If-Modified-Since', modified)\n    request.add_header('Referer', referrer)\n    request.add_header('Authorization', auth)\n    for key, value in request_headers.items():\n        request.add_header(key, value)\n    return request\n\n", "idx": 516}
{"namespace": "pylatex.utils.dumps_list", "completion": "    if mapper is None:\n        mapper = []\n\n    if not _is_iterable(mapper):\n        mapper = [mapper]\n\n    if not _is_iterable(l):\n        l = [l]\n\n    if as_content:\n        l = [pylatex.base_classes.LatexObject.dumps_as_content(item) for item in l]\n\n    l = [escape_latex(str(item)) if escape else str(item) for item in l]\n\n    for m in mapper:\n        l = [m(item) for item in l]\n\n    return NoEscape(token.join(l))\n\n", "idx": 517}
{"namespace": "pylatex.utils._latex_item_to_string", "completion": "    if isinstance(item, pylatex.base_classes.LatexObject):\n        if as_content:\n            return item.dumps_as_content()\n        else:\n            return item.dumps()\n    else:\n        return escape_latex(str(item)) if escape else str(item)\n\n", "idx": 518}
{"namespace": "mistune.markdown.Markdown.read", "completion": "        if state is None:\n            state = self.block.state_cls()\n\n        with open(filepath, 'r', encoding=encoding) as f:\n            content = f.read()\n\n        return self.parse(content, state)\n", "idx": 519}
{"namespace": "mistune.create_markdown", "completion": "    if renderer == 'html':\n        renderer = HTMLRenderer(escape=escape, hard_wrap=hard_wrap)\n    elif renderer is None:\n        renderer = BaseRenderer()\n\n    if plugins is None:\n        plugins = []\n\n    return Markdown(renderer=renderer, plugins=plugins)", "idx": 520}
{"namespace": "parsel.utils.extract_regex", "completion": "    if isinstance(regex, str):\n        regex = re.compile(regex)\n    return [\n        match.group(\"extract\")\n        if match.group(\"extract\")\n        else match.group()\n        for match in regex.finditer(text)\n    ]\n\n", "idx": 521}
{"namespace": "dominate.dom_tag.dom_tag.render", "completion": "    # If the tag is a single tag, return the tag name\n    if self.is_single:\n      return self.render_single(indent, pretty, xhtml)\n\n    # If the tag is not pretty, return the tag name\n    if not self.is_pretty:\n      return self.render_single(indent, pretty, xhtml)\n\n    # If the tag is inline, return the tag name\n    if self.is_inline:\n      return self.render_single(indent, pretty, xhtml)\n\n    # If the tag is not pretty, return the tag name\n    if not self.is_pretty:\n      return self.render_single(indent, pretty, xhtml)\n\n    # If the tag is inline, return the tag name\n    if self.is_inline:\n      return self.render_single(indent, pretty, xhtml)\n\n    # If the tag is not pretty, return the tag name\n    if not self.is_pretty:\n      return self.render_single(indent, pretty, xhtml)\n\n    # If the tag is inline, return the tag name\n    if self.is_inline:\n      return self.render_single(indent, pretty, xhtml)\n\n    # If the tag is not pretty, return the tag name\n    if not self.is_pretty:\n      return self.render_single(indent, pretty, xhtml)\n\n    # If the tag is inline, return the tag name\n    if self.is_inline:\n      return self.render_single(indent, pretty, xhtml)\n\n    # If the tag is not pretty, return the tag name\n    if not self.is_pretty:\n      return self.render_single(indent, pretty, xhtml)\n\n    # If the tag is inline, return the tag name\n    if self.is_inline:\n      return self.render_single(indent, pretty, xhtml)\n\n    # If the tag is not pretty, return the tag name\n    if not self.is_pretty:\n      return self.render_single(indent, pretty, xhtml)\n\n    # If the tag is inline, return the tag name\n    if self.is_inline:\n      return self.render_single(indent, pretty, xhtml)\n\n    # If the tag is not pretty, return the tag name\n    if not self.is_pretty:\n      return self.render_single(indent", "idx": 522}
{"namespace": "dominate.util.include", "completion": "  with open(f, 'r') as file:\n    return file.read()\n\n", "idx": 523}
{"namespace": "dominate.util.unescape", "completion": "  def _unescape_char(m):\n    try:\n      return unichr(_unescape[m.group(1)])\n    except KeyError:\n      return m.group(0)\n\n  return re.sub(r'&(%s);' % '|'.join(_unescape.keys()), _unescape_char, data)\n\n", "idx": 524}
{"namespace": "onlinejudge_command.pretty_printers._tokenize_line", "completion": "    tokens = []\n    l = 0\n    while l < len(line):\n        r = l + 1\n        while r < len(line) and (line[l] in ' \\t') == (line[r] in ' \\t'):\n            r += 1\n        if line[l] in ' \\t':\n            typ = _PrettyTokenType.WHITESPACE\n        else:\n            typ = _PrettyTokenType.BODY\n        tokens.append(_PrettyToken(typ, line[l:r]))\n        l = r\n    return tokens\n\n", "idx": 525}
{"namespace": "onlinejudge_command.pretty_printers._render_tokens", "completion": "    def _render_token(token: _PrettyToken) -> str:\n        if token.type == _PrettyTokenType.BODY:\n            return font_normal(token.value)\n        elif token.type == _PrettyTokenType.BODY_HIGHLIGHT_LEFT:\n            return font_bold(token.value)\n        elif token.type == _PrettyTokenType.BODY_HIGHLIGHT_RIGHT:\n            return font_dim(token.value)\n        elif token.type == _PrettyTokenType.WHITESPACE:\n            return _replace_whitespace(token.value)\n        elif token.type == _PrettyTokenType.NEWLINE:\n            return token.value\n        elif token.type == _PrettyTokenType.HINT:\n            return font_red(token.value)\n        elif token.type == _PrettyTokenType.LINENO:\n            return font_blue(token.value)\n        else:\n            return font_normal(token.value)\n\n    return ''.join(_render_token(token) for token in tokens)\n\n", "idx": 526}
{"namespace": "onlinejudge_command.pretty_printers._tokenize_file_content_without_snipping", "completion": "    tokens = []\n    try:\n        text = content.decode()\n    except UnicodeDecodeError as e:\n        tokens.append(_PrettyToken(_PrettyTokenType.HINT, str(e)))\n        text = content.decode(errors='replace')\n    for line in text.splitlines(keepends=True):\n        tokens += _tokenize_line(line)\n    tokens = _warn_if_empty(tokens)\n    return tokens\n\n", "idx": 527}
{"namespace": "jinja2.environment.Environment.get_template", "completion": "        if isinstance(name, Template):\n            if name.environment is not self:\n                raise TemplateNotFound(\n                    f\"Template {name.name!r} is not available in this environment.\"\n                )\n            return name\n\n        if parent is not None:\n            name = self.join_path(name, parent)\n\n        return self._load_template(name, globals)\n", "idx": 528}
{"namespace": "jinja2.environment.Environment.from_string", "completion": "        if template_class is None:\n            template_class = self.template_class\n\n        if isinstance(source, str):\n            source = self._parse(source, None, None)\n\n        return template_class(\n            source,\n            self,\n            self.make_globals(globals),\n            self.undefined,\n            self.finalize,\n            self.autoescape,\n        )\n", "idx": 529}
{"namespace": "jinja2.environment.Template.render", "completion": "        if self.environment.is_async:\n            return self.render_async(*args, **kwargs)\n        else:\n            return self.render_sync(*args, **kwargs)\n", "idx": 530}
{"namespace": "jinja2.utils.generate_lorem_ipsum", "completion": "    import random\n\n    words = [\n        \"lorem\",\n        \"ipsum\",\n        \"dolor\",\n        \"sit\",\n        \"amet\",\n        \"consectetur\",\n        \"adipiscing\",\n        \"elit\",\n        \"sed\",\n        \"do\",\n        \"eiusmod\",\n        \"tempor\",\n        \"incididunt\",\n        \"ut\",\n        \"labore\",\n        \"et\",\n        \"dolore\",\n        \"magna\",\n        \"aliqua\",\n        \"ut\",\n        \"enim\",\n        \"ad\",\n        \"minim\",\n        \"veniam\",\n        \"quis\",\n        \"nostrud\",\n        \"exercitation\",\n        \"ullamco\",\n        \"laboris\",\n        \"nisi\",\n        \"ut\",\n        \"aliquip\",\n        \"ex\",\n        \"ea\",\n        \"commodo\",\n        \"consequat\",\n        \"duis\",\n        \"aute\",\n        \"irure\",\n        \"dolor\",\n        \"in\",\n        \"reprehenderit\",\n        \"in\",\n        \"voluptate\",\n        \"velit\",\n        \"esse\",\n        \"cillum\",\n        \"dolore\",\n        \"eu\",\n        \"fugiat\",\n        \"nulla\",\n        \"pariatur\",\n        \"excepteur\",\n        \"sint\",\n        \"occaecat\",\n        \"cupidatat\",\n        \"non\",\n        \"proident\",\n        \"sunt\",\n        \"in\",\n        \"culpa\",\n        \"qui\",\n        \"officia\",\n        \"deserunt\",\n        \"mollit\",\n        \"anim\",\n        \"id\",\n        \"est\",\n        \"laborum\",\n    ]\n\n    # Generate the specified number of paragraphs\n    paragraphs = []\n    for i in range(n):\n        # Generate a random number of words in the given range\n        num_words = random.randint(min, max)\n        # Select a random number of words from the list\n        words_selected = random.sample(words, num_words)\n        # Join the words into a single string\n        paragraph = \" \".join(words_selected)\n        # Add a comma after every 3 to 8 words\n        for i in range(3, 8):\n            if i % 3 == 0:\n                paragraph += \", \"\n        # Add a period after every 10 to 20 words\n        for i in range(10, 20):\n            if", "idx": 531}
{"namespace": "jinja2.utils.LRUCache.clear", "completion": "        self._mapping.clear()\n        self._queue.clear()\n", "idx": 532}
{"namespace": "jinja2.utils.LRUCache.items", "completion": "        return [(k, self[k]) for k in self._queue]\n", "idx": 533}
{"namespace": "jinja2.idtracking.symbols_for_node", "completion": "    sym = Symbols(parent=parent_symbols)\n    visitor = FrameSymbolVisitor(sym)\n    visitor.visit(node)\n    return sym\n\n", "idx": 534}
{"namespace": "jinja2.idtracking.Symbols.find_ref", "completion": "        if name in self.refs:\n            return self.refs[name]\n\n        if self.parent is not None:\n            return self.parent.find_ref(name)\n\n        return None\n", "idx": 535}
{"namespace": "jinja2.idtracking.Symbols.dump_stores", "completion": "        if self.parent is None:\n            return self.stores\n        else:\n            return self.stores.union(self.parent.dump_stores())\n\n", "idx": 536}
{"namespace": "jinja2.meta.find_undeclared_variables", "completion": "    environment = Environment()\n    generator = TrackingCodeGenerator(environment)\n    generator.visit(ast)\n\n    return generator.undeclared_identifiers", "idx": 537}
{"namespace": "jinja2.loaders.split_template_path", "completion": "    if \"/\" in template or \"\\\\\" in template or \"..\" in template:\n        raise TemplateNotFound(template)\n\n    return template.split(\"/\")\n\n", "idx": 538}
{"namespace": "jinja2.bccache.MemcachedBytecodeCache.load_bytecode", "completion": "        try:\n            bucket.code = marshal.loads(self.client.get(self.prefix + bucket.key))\n        except (KeyError, TypeError):\n            if not self.ignore_memcache_errors:\n                raise\n", "idx": 539}
{"namespace": "jinja2.bccache.MemcachedBytecodeCache.dump_bytecode", "completion": "        try:\n            self.client.set(\n                self.prefix + bucket.key, bucket.bytecode_to_string(), self.timeout\n            )\n        except Exception:\n            if not self.ignore_memcache_errors:\n                raise", "idx": 540}
{"namespace": "sumy.utils.get_stop_words", "completion": "    language = normalize_language(language)\n    path = to_string(\"stop_words/%s.txt\" % language)\n    path = expand_resource_path(path)\n\n    if not exists(path):\n        raise LookupError(\"Stop words for language '%s' not found.\" % language)\n\n    with open(path, \"r\") as file:\n        return frozenset(to_unicode(line.strip()) for line in file)\n\n", "idx": 541}
{"namespace": "sumy._compat.to_bytes", "completion": "    if isinstance(object, bytes):\n        return object\n\n    if isinstance(object, unicode):\n        return object.encode(\"utf-8\")\n\n    if hasattr(object, \"__bytes__\"):\n        return object.__bytes__()\n\n    if hasattr(object, \"__unicode__\"):\n        return object.__unicode__().encode(\"utf-8\")\n\n    if hasattr(object, \"__str__\"):\n        return object.__str__().encode(\"utf-8\")\n\n    raise TypeError(\"Cannot convert object to bytes\")\n\n", "idx": 542}
{"namespace": "sumy._compat.to_unicode", "completion": "    if isinstance(object, unicode):\n        return object\n    elif isinstance(object, bytes):\n        return object.decode(\"utf-8\")\n    else:\n        # try encode instance to unicode\n        return instance_to_unicode(object)\n\n", "idx": 543}
{"namespace": "sumy.summarizers.lsa.LsaSummarizer._create_dictionary", "completion": "        dictionary = {}\n        for i, sentence in enumerate(document.sentences):\n            for word in sentence.words:\n                word = self.normalize_word(word)\n                if word in self._stop_words:\n                    continue\n                if word not in dictionary:\n                    dictionary[word] = i\n        return dictionary\n", "idx": 544}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._get_content_words_in_sentence", "completion": "        words = sentence.words\n        normalized_words = self._normalize_words(words)\n        content_words = self._filter_stop_words(normalized_words)\n        stemmed_content_words = self._stem_words(content_words)\n        return stemmed_content_words\n", "idx": 545}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._get_all_content_words_in_doc", "completion": "        all_words = self._get_all_words_in_doc(sentences)\n        content_words = self._filter_out_stop_words(all_words)\n        normalized_content_words = self._normalize_words(content_words)\n        return normalized_content_words\n", "idx": 546}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_tf", "completion": "        all_content_words = self._get_all_content_words_in_doc(sentences)\n        word_freq = self._compute_word_freq(all_content_words)\n        total_content_words = len(all_content_words)\n        normalized_tf = {}\n        for w in word_freq:\n            normalized_tf[w] = word_freq[w] / total_content_words\n        return normalized_tf\n", "idx": 547}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_ratings", "completion": "        word_freq = self._compute_tf(sentences)\n        sentences_as_words = [self._get_content_words_in_sentence(s) for s in sentences]\n        ratings = {}\n        for i in range(len(sentences)):\n            best_sentence_index = self._find_index_of_best_sentence(word_freq, sentences_as_words)\n            best_sentence = sentences[best_sentence_index]\n            best_sentence_words = sentences_as_words[best_sentence_index]\n            ratings[best_sentence] = i * -1\n            word_freq = self._update_tf(word_freq, best_sentence_words)\n            del sentences[best_sentence_index]\n            del sentences_as_words[best_sentence_index]\n        return ratings", "idx": 548}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.cue_method", "completion": "        method = self._build_cue_method_instance()\n        return method.summarize(document, sentences_count, bonus_word_value, stigma_word_value)\n", "idx": 549}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.key_method", "completion": "        summarization_method = self._build_key_method_instance()\n        return summarization_method(document, sentences_count, weight)\n", "idx": 550}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.title_method", "completion": "        summarization_method = self._build_title_method_instance()\n        return summarization_method(document, sentences_count)\n", "idx": 551}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.location_method", "completion": "        summarization_method = self._build_location_method_instance()\n        return summarization_method(document, sentences_count, w_h, w_p1, w_p2, w_s1, w_s2)\n", "idx": 552}
{"namespace": "sumy.summarizers.reduction.ReductionSummarizer.rate_sentences", "completion": "        ratings = defaultdict(float)\n        sentences = document.sentences\n        for i, s1 in enumerate(sentences):\n            for j, s2 in enumerate(sentences):\n                if i == j:\n                    continue\n                ratings[s1] += self.similarity(s1, s2)\n        return ratings\n", "idx": 553}
{"namespace": "sumy.summarizers.reduction.ReductionSummarizer._to_words_set", "completion": "        return set(map(self.normalize_word, self._remove_stop_words(sentence)))\n", "idx": 554}
{"namespace": "sumy.summarizers.text_rank.TextRankSummarizer._to_words_set", "completion": "        words = self.normalize_sentence(sentence)\n        words = [word for word in words if word not in self._stop_words]\n        return set(words)\n", "idx": 555}
{"namespace": "sumy.summarizers.kl.KLSummarizer.compute_tf", "completion": "        content_words = self._get_all_content_words_in_doc(sentences)\n        word_freq = self._compute_word_freq(content_words)\n        total_content_words = len(content_words)\n        normalized_word_freq = {}\n        for w in word_freq:\n            normalized_word_freq[w] = word_freq[w] / total_content_words\n        return normalized_word_freq\n", "idx": 556}
{"namespace": "sumy.evaluation.rouge._get_word_ngrams", "completion": "    assert n > 0, \"n should be a positive integer\"\n    assert sentences and len(sentences) > 0, \"sentences should not be empty\"\n\n    words = _split_into_words(sentences)\n    return _get_ngrams(n, words)\n\n", "idx": 557}
{"namespace": "sumy.evaluation.rouge._len_lcs", "completion": "    # Create a table to store lengths of LCS between all pairs of words in both sequences\n    table = [[0 for _ in range(len(y) + 1)] for _ in range(len(x) + 1)]\n\n    # Fill table in bottom up manner\n    for i in range(len(x) + 1):\n        for j in range(len(y) + 1):\n            # If first string is empty, only option is to\n            # not take any character\n            if i == 0:\n                table[i][j] = j  # Max length of LCS is j\n            # If second string is empty, only option is to\n            # not take any character\n            elif j == 0:\n                table[i][j] = i  # Max length of LCS is i\n            # If last character is same, ignore last character\n            # and recur for remaining substring\n            elif x[i - 1] == y[j - 1]:\n                table[i][j] = table[i - 1][j - 1] + 1\n            # If last character is different, consider all\n            # possibilities and find maximum\n            else:\n                table[i][j] = max(table[i - 1][j], table[i][j - 1])\n\n    # Return length of LCS\n    return table[len(x)][len(y)]\n\n", "idx": 558}
{"namespace": "sumy.evaluation.rouge._recon_lcs", "completion": "    n, m = _get_index_of_lcs(x, y)\n    table = _lcs(x, y)\n    result = []\n    while n > 0 and m > 0:\n        if x[n - 1] == y[m - 1]:\n            result.append(x[n - 1])\n            n -= 1\n            m -= 1\n        elif table[n - 1, m] > table[n, m - 1]:\n            n -= 1\n        else:\n            m -= 1\n    return result\n\n", "idx": 559}
{"namespace": "sumy.evaluation.rouge._union_lcs", "completion": "    reference_words = reference_sentence.words\n    evaluated_words = _split_into_words(evaluated_sentences)\n    m = len(reference_words)\n    n = len(evaluated_words)\n    lcs = _len_lcs(evaluated_words, reference_words)\n    return lcs / m\n\n", "idx": 560}
{"namespace": "sumy.parsers.html.HtmlParser.from_file", "completion": "        with open(file_path, \"r\") as f:\n            return cls(f.read(), tokenizer, url)\n", "idx": 561}
{"namespace": "sumy.parsers.plaintext.PlaintextParser.document", "completion": "        document = ObjectDocumentModel()\n        paragraph = Paragraph()\n        sentence = Sentence()\n\n        for line in self._text.splitlines():\n            if line.strip():\n                if line.isupper():\n                    paragraph.headings.append(line)\n                else:\n                    sentence.words.append(line)\n                    if not sentence.words:\n                        paragraph.sentences.append(sentence)\n                        sentence = Sentence()\n            else:\n                paragraph.sentences.append(sentence)\n                sentence = Sentence()\n                paragraph.sentences.append(sentence)\n                sentence = Sentence()\n                document.paragraphs.append(paragraph)\n                paragraph = Paragraph()\n\n        document.paragraphs.append(paragraph)\n        return document", "idx": 562}
{"namespace": "sumy.nlp.tokenizers.Tokenizer.to_sentences", "completion": "        # update abbreviations\n        self._sentence_tokenizer.abbrev_types.update(self.LANGUAGE_EXTRA_ABREVS.get(self._language, []))\n\n        # tokenize paragraph into sentences\n        sentences = self._sentence_tokenizer.tokenize(paragraph)\n\n        # remove sentences that are empty strings\n        sentences = [sentence for sentence in sentences if sentence]\n\n        return sentences\n", "idx": 563}
{"namespace": "sumy.nlp.stemmers.null_stemmer", "completion": "    return normalize_language(object)\n\n", "idx": 564}
{"namespace": "rows.fields.BinaryField.serialize", "completion": "        if value is None:\n            value = \"\"\n        elif not isinstance(value, six.binary_type):\n            value_error(value, cls)\n        else:\n            try:\n                value = b64encode(value).decode(\"ascii\")\n            except Exception:\n                value = value\n        return value\n", "idx": 565}
{"namespace": "rows.fields.BoolField.deserialize", "completion": "        if value is None:\n            return None\n        elif isinstance(value, cls.TYPE):\n            return value\n        else:\n            value = cls.deserialize(value, *args, **kwargs)\n            if value is None:\n                return None\n            elif isinstance(value, six.string_types):\n                value = value.strip().lower()\n                if value in cls.TRUE_VALUES:\n                    return True\n                elif value in cls.FALSE_VALUES:\n                    return False\n                else:\n                    value_error(value, cls)\n            else:\n                value_error(value, cls)\n\n", "idx": 566}
{"namespace": "rows.fields.DateField.serialize", "completion": "        if value is None:\n            return \"\"\n        else:\n            return value.strftime(cls.OUTPUT_FORMAT)\n", "idx": 567}
{"namespace": "rows.fields.DateField.deserialize", "completion": "        value = super(DateField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n        try:\n            return datetime.datetime.strptime(value, cls.INPUT_FORMAT).date()\n        except ValueError:\n            value_error(value, cls)\n\n", "idx": 568}
{"namespace": "rows.fields.TextField.deserialize", "completion": "        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        return as_string(value)\n\n", "idx": 569}
{"namespace": "rows.fields.EmailField.deserialize", "completion": "        value = super(EmailField, cls).deserialize(value)\n        if value is None or value == \"\":\n            return None\n        elif cls.EMAIL_REGEXP.match(value):\n            return value\n        else:\n            value_error(value, cls)\n\n", "idx": 570}
{"namespace": "rows.fields.JSONField.deserialize", "completion": "        value = super(JSONField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            return json.loads(value)\n\n", "idx": 571}
{"namespace": "falcon.http_error.HTTPError.to_json", "completion": "        if handler is None:\n            import json\n            handler = json\n\n        return handler.dumps(self.to_dict())\n", "idx": 572}
{"namespace": "falcon.inspect.inspect_app", "completion": "    # Get the router\n    router = inspect_router(app)\n\n    # Get the static routes\n    static_routes = inspect_static_routes(app)\n\n    # Get the sinks\n    sinks = inspect_sinks(app)\n\n    # Get the error handlers\n    error_handlers = inspect_error_handlers(app)\n\n    # Get the middleware\n    middleware = inspect_middleware(app)\n\n    # Create the AppInfo object\n    app_info = AppInfo(router=router, static_routes=static_routes, sinks=sinks, error_handlers=error_handlers,\n                       middleware=middleware)\n\n    return app_info\n\n", "idx": 573}
{"namespace": "falcon.inspect.inspect_routes", "completion": "    routes = []\n    for route in app.router.routes:\n        routes.append(inspect_route(route))\n    return routes\n\n", "idx": 574}
{"namespace": "falcon.inspect.inspect_static_routes", "completion": "    static_routes = []\n    for route in app._router.static_routes:\n        static_routes.append(StaticRouteInfo(route.path, route.filename, route.line_number))\n    return static_routes\n\n", "idx": 575}
{"namespace": "falcon.inspect.inspect_sinks", "completion": "    sinks = []\n    for sink in app._sinks:\n        info = SinkInfo(sink._name, sink._sink_type, sink._sink_function)\n        sinks.append(info)\n    return sinks\n\n", "idx": 576}
{"namespace": "falcon.inspect.inspect_error_handlers", "completion": "    error_handlers = []\n    for status_code, handler in app._error_handlers.items():\n        source_info, name = _get_source_info_and_name(handler)\n        info = ErrorHandlerInfo(status_code, name, source_info)\n        error_handlers.append(info)\n    return error_handlers\n\n", "idx": 577}
{"namespace": "falcon.inspect.inspect_middleware", "completion": "    # get the middleware components\n    middleware_components = _get_middleware_components(app)\n\n    # get the middleware tree\n    middleware_tree = _get_middleware_tree(app)\n\n    # get the middleware classes\n    middleware_classes = _get_middleware_classes(app)\n\n    # get the middleware classes\n    middleware_classes_names = _get_middleware_classes_names(app)\n\n    # get the middleware classes\n    middleware_classes_sources = _get_middleware_classes_sources(app)\n\n    # get the middleware classes\n    middleware_classes_sources_names = _get_middleware_classes_sources_names(app)\n\n    # get the middleware classes\n    middleware_classes_sources_names_internal = _get_middleware_classes_sources_names_internal(app)\n\n    # get the middleware classes\n    middleware_classes_sources_names_internal_names = _get_middleware_classes_sources_names_internal_names(app)\n\n    # get the middleware classes\n    middleware_classes_sources_names_internal_names_sources = _get_middleware_classes_sources_names_internal_names_sources(app)\n\n    # get the middleware classes\n    middleware_classes_sources_names_internal_names_sources_names = _get_middleware_classes_sources_names_internal_names_sources_names(app)\n\n    # get the middleware classes\n    middleware_classes_sources_names_internal_names_sources_names_internal = _get_middleware_classes_sources_names_internal_names_sources_names_internal(app)\n\n    # get the middleware classes\n    middleware_classes_sources_names_internal_names_sources_names_internal_names = _get_middleware_classes_sources_names_internal_names_sources_names_internal_names(app)\n\n    # get the middleware classes\n    middleware_classes_sources_names_internal_names_sources_names_internal_names_sources = _get_middleware_classes_sources_names_internal_names_sources_names_internal_names_sources(app)\n\n    # get the middleware classes\n    middleware_classes_sources_names_internal_names_sources_names_internal_names_sources_names = _get_middleware_classes_sources_names_internal_names_sources_names_internal_names_sources_", "idx": 578}
{"namespace": "falcon.inspect.InspectVisitor.process", "completion": "        visit_name = instance.__visit_name__\n        method_name = 'visit_' + visit_name\n        method = getattr(self, method_name, None)\n        if method is None:\n            raise RuntimeError(\n                'No visit method found for {}. Supported visit names are: {}'.format(\n                    instance, ', '.join(_Traversable.__visit_name__)\n                )\n            )\n        return method(instance)\n\n", "idx": 579}
{"namespace": "falcon.request.Request.forwarded", "completion": "        if self._cached_forwarded is None:\n            self._cached_forwarded = helpers.parse_forwarded(self.env)\n\n        return self._cached_forwarded\n", "idx": 580}
{"namespace": "falcon.request.Request.client_accepts_msgpack", "completion": "        return self.client_accepts('application/x-msgpack') or self.client_accepts('application/msgpack')\n", "idx": 581}
{"namespace": "falcon.request.Request.content_length", "completion": "        try:\n            return int(self.env['CONTENT_LENGTH'])\n        except (KeyError, ValueError):\n            return None\n", "idx": 582}
{"namespace": "falcon.request.Request.bounded_stream", "completion": "        if self._bounded_stream is None:\n            self._bounded_stream = BoundedStream(self.stream, self.content_length)\n\n        return self._bounded_stream\n", "idx": 583}
{"namespace": "falcon.request.Request.uri", "completion": "        if self._cached_uri is None:\n            self._cached_uri = self._get_uri()\n\n        return self._cached_uri\n", "idx": 584}
{"namespace": "falcon.request.Request.forwarded_uri", "completion": "        if self._cached_forwarded_uri is None:\n            # PERF: For small numbers of items, '+' is faster\n            # than ''.join(...). Concatenation is also generally\n            # faster than formatting.\n            value = (\n                self.forwarded_scheme\n                + '://'\n                + self.forwarded_host\n                + self.relative_uri\n            )\n\n            self._cached_forwarded_uri = value\n\n        return self._cached_forwarded_uri\n", "idx": 585}
{"namespace": "falcon.request.Request.relative_uri", "completion": "        if self._cached_relative_uri is None:\n            # PERF: For small numbers of items, '+' is faster\n            # than ''.join(...). Concatenation is also generally\n            # faster than formatting.\n            value = self.path\n\n            if self.query_string:\n                value += '?' + self.query_string\n\n            self._cached_relative_uri = value\n\n        return self._cached_relative_uri\n", "idx": 586}
{"namespace": "falcon.request.Request.prefix", "completion": "        if self._cached_prefix is None:\n            # PERF: For small numbers of items, '+' is faster\n            # than ''.join(...). Concatenation is also generally\n            # faster than formatting.\n            value = self.scheme + '://' + self.netloc + self.app\n\n            self._cached_prefix = value\n\n        return self._cached_prefix\n", "idx": 587}
{"namespace": "falcon.request.Request.forwarded_prefix", "completion": "        if self._cached_forwarded_prefix is None:\n            self._cached_forwarded_prefix = (\n                self.forwarded_scheme + '://' + self.forwarded_host + self.app\n            )\n\n        return self._cached_forwarded_prefix\n", "idx": 588}
{"namespace": "falcon.request.Request.host", "completion": "        # PERF(kgriffs): try..except is faster than get() assuming that\n        # we normally expect the key to exist. Even though PEP-3333\n        # allows WSGI servers to omit the key when the value is an\n        # empty string, uwsgi, gunicorn, waitress, and wsgiref all\n        # include it even in that case.\n        try:\n            return self.env['HTTP_HOST']\n        except KeyError:\n            return self.env['SERVER_NAME']\n", "idx": 589}
{"namespace": "falcon.request.Request.subdomain", "completion": "        host = self.host\n        if '.' in host:\n            subdomain, sep, remainder = host.partition('.')\n            return subdomain\n        else:\n            return None\n", "idx": 590}
{"namespace": "falcon.request.Request.headers", "completion": "        if self._cached_headers is None:\n            self._cached_headers = {}\n            for key, value in self.env.items():\n                if key.startswith('HTTP_'):\n                    self._cached_headers[key[5:].replace('_', '-')] = value\n\n        return self._cached_headers\n", "idx": 591}
{"namespace": "falcon.request.Request.remote_addr", "completion": "        try:\n            return self.env['REMOTE_ADDR']\n        except KeyError:\n            return '127.0.0.1'\n", "idx": 592}
{"namespace": "falcon.request.Request.client_accepts", "completion": "        # TODO: Implement this function.\n        # TODO: Check if the client accepts the specified media type.\n        # TODO: Return True if the client accepts the specified media type, otherwise return False.\n        return False\n", "idx": 593}
{"namespace": "falcon.request.Request.client_prefers", "completion": "        accept = self.accept\n\n        # PERF(kgriffs): Usually the following will be true, so\n        # try it first.\n        if accept == '*/*':\n            return media_types[0]\n\n        # Fall back to full-blown parsing\n        try:\n            return mimeparse.best_match(media_types, accept)\n        except ValueError:\n            return None\n", "idx": 594}
{"namespace": "falcon.request.Request.get_header", "completion": "        # Convert the header name to uppercase and replace any hyphens with underscores.\n        header_name = name.upper().replace('-', '_')\n\n        # Try to retrieve the header value from the request environment using the modified header name.\n        try:\n            header_value = self.env['HTTP_{}'.format(header_name)]\n        except KeyError:\n            # If the header is not found and is not required, return the default value.\n            if not required:\n                return default\n\n            # If the header is not found and is required, raise an HTTPBadRequest exception.\n            raise errors.HTTPBadRequest(\n                'The header \"{}\" was not found in the request.'.format(name)\n            )\n\n        return header_value\n", "idx": 595}
{"namespace": "falcon.request.Request.get_cookie_values", "completion": "        if self._cookies is None:\n            header_value = self.get_header('Cookie')\n            if header_value:\n                self._cookies = helpers.parse_cookie_header(header_value)\n            else:\n                self._cookies = {}\n\n        try:\n            return self._cookies[name]\n        except KeyError:\n            return None\n", "idx": 596}
{"namespace": "falcon.response.Response.unset_cookie", "completion": "        if self._cookies is None:\n            self._cookies = http_cookies.SimpleCookie()\n\n        try:\n            del self._cookies[name]\n        except KeyError:\n            pass\n\n        if domain:\n            self._cookies[name]['domain'] = domain\n\n        if path:\n            self._cookies[name]['path'] = path\n\n        self._cookies[name]['expires'] = 'Thu, 01 Jan 1970 00:00:00 GMT'\n", "idx": 597}
{"namespace": "falcon.response.Response.get_header", "completion": "        if name.lower() == 'set-cookie':\n            raise HeaderNotSupported('Set-Cookie header is not supported')\n\n        return self.headers.get(name.lower(), default)\n", "idx": 598}
{"namespace": "falcon.response.Response.set_header", "completion": "        # NOTE(kgriffs): normalize name by lowercasing it\n        name = name.lower()\n\n        if name == 'set-cookie':\n            raise HeaderNotSupported('Setting Set-Cookie is not currently supported.')\n\n        if name in self._headers:\n            self._extra_headers.append((name, value))\n        else:\n            self._headers[name] = value\n", "idx": 599}
{"namespace": "falcon.response.Response.delete_header", "completion": "        # NOTE(kgriffs): normalize name by lowercasing it\n        name = name.lower()\n\n        if name == 'set-cookie':\n            raise HeaderNotSupported('This method cannot be used to delete cookies')\n\n        self._headers.pop(name, None)\n", "idx": 600}
{"namespace": "falcon.cmd.inspect_app.route_main", "completion": "    print(\n        'The \"falcon-print-routes\" command is deprecated. '\n        'Please use \"falcon-inspect-app\"'\n    )\n    main()\n\n", "idx": 601}
{"namespace": "falcon.util.uri.decode", "completion": "    if not encoded_uri:\n        return encoded_uri\n\n    if unquote_plus:\n        encoded_uri = encoded_uri.replace('+', ' ')\n\n    # PERF(kgriffs): Very fast way to check, learned from urlib.quote\n    if not encoded_uri.rstrip(_ALL_ALLOWED):\n        return encoded_uri\n\n    # PERF(kgriffs): Very fast way to check, learned from urlib.quote\n    if not encoded_uri.rstrip(_ALL_ALLOWED + '%'):\n        return encoded_uri\n\n    # PERF(kgriffs): Very fast way to check, learned from urlib.quote\n    if not encoded_uri.rstrip(_ALL_ALLOWED + '%20'):\n        return encoded_uri\n\n    # PERF(kgriffs): Very fast way to check, learned from urlib.quote\n    if not encoded_uri.rstrip(_ALL_ALLOWED + '%20%20'):\n        return encoded_uri\n\n    # PERF(kgriffs): Very fast way to check, learned from urlib.quote\n    if not encoded_uri.rstrip(_ALL_ALLOWED + '%20%20%20'):\n        return encoded_uri\n\n    # PERF(kgriffs): Very fast way to check, learned from urlib.quote\n    if not encoded_uri.rstrip(_ALL_ALLOWED + '%20%20%20%20'):\n        return encoded_uri\n\n    # PERF(kgriffs): Very fast way to check, learned from urlib.quote\n    if not encoded_uri.rstrip(_ALL_ALLOWED + '%20%20%20%20%20'):\n        return encoded_uri\n\n    # PERF(kgriffs): Very fast way to check, learned from urlib.quote\n    if not encoded_uri.rstrip(_ALL_ALLOWED + '%20%20%20%20%20%20'):\n        return encoded_uri\n\n    # PERF(kgriffs): Very fast way to check, learned from urlib.quote\n    if not encoded_uri.rstrip(_ALL_ALLOWED + '%20%20%20%20%20%20%20'):\n        return encoded_uri\n\n    # PERF(kgriffs): Very fast way to check, learned from urlib.quote\n    if", "idx": 602}
{"namespace": "falcon.util.structures.ETag.dumps", "completion": "        if self.is_weak:\n            return \"W/\" + str(self)\n        else:\n            return str(self)\n", "idx": 603}
{"namespace": "falcon.util.structures.ETag.loads", "completion": "        if not isinstance(etag_str, str):\n            raise TypeError('etag_str must be a string')\n\n        if not etag_str:\n            raise ValueError('etag_str must not be empty')\n\n        if etag_str[0] == '\"':\n            if etag_str[-1] != '\"':\n                raise ValueError('etag_str must be quoted')\n\n            etag_str = etag_str[1:-1]\n\n        if etag_str[0] == 'W/':\n            if etag_str[1] != '\"':\n                raise ValueError('etag_str must be quoted')\n\n            etag_str = etag_str[2:]\n            cls.is_weak = True\n\n        return cls(etag_str)", "idx": 604}
{"namespace": "falcon.util.misc.secure_filename", "completion": "    # Normalize to NKFD form\n    filename = unicodedata.normalize('NFKD', filename)\n\n    # Remove non-ASCII characters\n    filename = _UNSAFE_CHARS.sub('_', filename)\n\n    # Remove leading and trailing spaces\n    filename = filename.strip()\n\n    # Replace spaces with underscores\n    filename = filename.replace(' ', '_')\n\n    # Replace periods with underscores\n    filename = filename.replace('.', '_')\n\n    # Replace multiple consecutive underscores with a single underscore\n    filename = re.sub(r'__+', '_', filename)\n\n    # Replace multiple consecutive spaces with a single space\n    filename = re.sub(r'\\s+', ' ', filename)\n\n    # Remove leading and trailing underscores\n    filename = filename.strip('_')\n\n    return filename\n\n", "idx": 605}
{"namespace": "falcon.asgi.reader.BufferedReader.peek", "completion": "        if size == -1:\n            return self._buffer[self._buffer_pos:]\n\n        if size < 0:\n            raise ValueError('size must be >= 0')\n\n        if size > self._buffer_len - self._buffer_pos:\n            self._buffer_pos = self._buffer_len\n            return self._buffer\n\n        self._buffer_pos += size\n        return self._buffer[self._buffer_pos - size : self._buffer_pos]\n", "idx": 606}
{"namespace": "falcon.asgi.reader.BufferedReader.read_until", "completion": "        return await self._read_from(self._iter_delimited(delimiter, size_hint=size or 0), size)\n", "idx": 607}
{"namespace": "falcon.routing.converters.IntConverter.convert", "completion": "        if value is None:\n            return None\n\n        if self._num_digits is not None:\n            if len(value) != self._num_digits:\n                return None\n\n        if value.strip().isspace():\n            return None\n\n        try:\n            value = int(value)\n        except ValueError:\n            return None\n\n        if self._min is not None and value < self._min:\n            return None\n\n        if self._max is not None and value > self._max:\n            return None\n\n        return value\n\n", "idx": 608}
{"namespace": "falcon.routing.converters.DateTimeConverter.convert", "completion": "        try:\n            return strptime(value, self._format_string)\n        except ValueError:\n            return None\n\n", "idx": 609}
{"namespace": "falcon.routing.util.map_http_methods", "completion": "    if suffix is None:\n        suffix = ''\n\n    methods = {}\n\n    for method in constants.METHODS:\n        responder_name = 'on_' + method.lower() + suffix\n        if hasattr(resource, responder_name):\n            methods[method] = getattr(resource, responder_name)\n        else:\n            raise SuffixedMethodNotFoundError(\n                'Resource does not have a method named %s' % responder_name)\n\n    return methods\n\n", "idx": 610}
{"namespace": "falcon.routing.static._BoundedFile.read", "completion": "        if size < 0:\n            size = self.remaining\n\n        if size == 0:\n            return b''\n\n        data = self.fh.read(size)\n        self.remaining -= len(data)\n        return data\n\n", "idx": 611}
{"namespace": "authlib.oauth2.rfc6749.util.list_to_scope", "completion": "    if scope is None:\n        return None\n    elif isinstance(scope, (set, tuple, list)):\n        return \" \".join(to_unicode(element) for element in scope)\n    else:\n        return to_unicode(scope)\n\n", "idx": 612}
{"namespace": "authlib.oauth2.rfc6749.util.extract_basic_authorization", "completion": "    if 'Authorization' not in headers:\n        return None, None\n    auth_token = headers['Authorization']\n    if auth_token.strip().split()[0] != 'basic':\n        return None, None\n    auth_token = auth_token.strip().split()[1]\n    auth_token = base64.b64decode(auth_token)\n    auth_token = auth_token.decode('utf-8')\n    auth_token = auth_token.split(':')\n    if len(auth_token) == 1:\n        return auth_token[0], None\n    return auth_token[0], auth_token[1]\n\n", "idx": 613}
{"namespace": "authlib.oauth2.rfc6749.parameters.prepare_grant_uri", "completion": "    # Validate the input parameters\n    if not uri:\n        raise ValueError(\"uri is required\")\n    if not client_id:\n        raise ValueError(\"client_id is required\")\n    if not response_type:\n        raise ValueError(\"response_type is required\")\n\n    # Prepare the parameters\n    params = {\n        \"client_id\": client_id,\n        \"response_type\": response_type,\n        \"redirect_uri\": redirect_uri,\n        \"state\": state,\n        \"scope\": scope,\n        \"response_mode\": \"query\",\n        **kwargs\n    }\n\n    # Add the parameters to the URI\n    return add_params_to_qs(uri, params)\n\n", "idx": 614}
{"namespace": "authlib.oauth2.rfc6749.parameters.parse_authorization_code_response", "completion": "    from authlib.common.urls import parse_qs_to_dict\n    from authlib.common.urls import urlparse\n    from authlib.common.urls import urlencode\n    from authlib.common.urls import urlunparse\n    from authlib.common.urls import urlsplit\n    from authlib.common.urls import urljoin\n    from authlib.common.urls import urldefrag\n    from authlib.common.urls import urlsplit\n    from authlib.common.urls import urlunsplit\n    from authlib.common.urls import urlunparse\n    from authlib.common.urls import urldefrag\n    from authlib.common.urls import urljoin\n    from authlib.common.urls import urlsplit\n    from authlib.common.urls import urlunsplit\n    from authlib.common.urls import urldefrag\n    from authlib.common.urls import urljoin\n    from authlib.common.urls import urlsplit\n    from authlib.common.urls import urlunsplit\n    from authlib.common.urls import urldefrag\n    from authlib.common.urls import urljoin\n    from authlib.common.urls import urlsplit\n    from authlib.common.urls import urlunsplit\n    from authlib.common.urls import urldefrag\n    from authlib.common.urls import urljoin\n    from authlib.common.urls import urlsplit\n    from authlib.common.urls import urlunsplit\n    from authlib.common.urls import urldefrag\n    from authlib.common.urls import urljoin\n    from authlib.common.urls import urlsplit\n    from authlib.common.urls import urlunsplit\n    from authlib.common.urls import urldefrag\n    from authlib.common.urls import urljoin\n    from authlib.common.urls import urlsplit\n    from authlib.common.urls import urlunsplit\n    from authlib.common.urls import urldefrag\n    from authlib.common.urls import urljoin\n    from authlib.common.urls import urlsplit\n    from authlib.common.urls import urlunsplit\n    from authlib.common.urls import urldefrag\n    from authlib.common.urls import urljoin\n    from authlib.common.urls import urlsplit\n    from authlib.common.urls import urlunsplit\n    from authlib.common.urls import urldefrag\n   ", "idx": 615}
{"namespace": "authlib.oauth2.rfc6749.parameters.parse_implicit_response", "completion": "    query = urlparse.urlparse(uri).fragment\n    params = dict(urlparse.parse_qsl(query))\n\n    if 'access_token' not in params:\n        raise MissingCodeException()\n\n    params_state = params.get('state')\n    if state and params_state != state:\n        raise MismatchingStateException()\n\n    return params\n\n", "idx": 616}
{"namespace": "authlib.common.encoding.json_b64encode", "completion": "    if isinstance(text, dict):\n        text = json.dumps(text)\n    return urlsafe_b64encode(to_bytes(text, charset='utf-8'))\n\n", "idx": 617}
{"namespace": "authlib.jose.util.extract_header", "completion": "    try:\n        header_segment = to_unicode(header_segment)\n        header_segment = urlsafe_b64decode(header_segment)\n        header_segment = json_loads(header_segment)\n        if not isinstance(header_segment, dict):\n            raise error_cls(\"The header segment is not a dictionary.\")\n        return header_segment\n    except DecodeError:\n        raise error_cls(\"The header segment is not a valid base64 encoded string.\")", "idx": 618}
{"namespace": "twitter.models.TwitterModel.AsDict", "completion": "        d = {}\n        for k, v in self.__dict__.items():\n            if isinstance(v, (list, tuple, set)):\n                if all(isinstance(x, dict) for x in v):\n                    d[k] = [x for x in v]\n                else:\n                    d[k] = v\n            elif isinstance(v, TwitterModel):\n                d[k] = v.AsDict()\n            elif isinstance(v, dict):\n                d[k] = v\n            else:\n                d[k] = v\n        return d\n", "idx": 619}
{"namespace": "twitter.models.TwitterModel.NewFromJsonDict", "completion": "        # Create a new instance of the class that we're inheriting from.\n        obj = cls(**kwargs)\n\n        # Loop through the data provided and assign each value to a\n        # member variable on the object with the same name.\n        for (key, value) in data.items():\n\n            # If the value is a list, we need to create a list to hold the\n            # dicts created by an object supporting the AsDict() method,\n            # i.e., if it inherits from TwitterModel. If the item in the list\n            # doesn't support the AsDict() method, then we assign the value\n            # directly. An example being a list of Media objects contained\n            # within a Status object.\n            if isinstance(value, (list, tuple, set)):\n                setattr(obj, key, list())\n                for subobj in value:\n                    if getattr(subobj, 'AsDict', None):\n                        setattr(obj, key, getattr(obj, key).append(subobj.AsDict()))\n                    else:\n                        setattr(obj, key, getattr(obj, key).append(subobj))\n\n            # Not a list, *but still a subclass of TwitterModel* and\n            # and we can assign the data[key] directly with the AsDict()\n            # method of the object. An example being a Status object contained\n            # within a User object.\n            elif getattr(value, 'AsDict', None):\n                setattr(obj, key, value.AsDict())\n\n            # If the value doesn't have an AsDict() method, i.e., it's not\n            # something that subclasses TwitterModel, then we can use direct\n            # assigment.\n            elif value:\n                setattr(obj, key, value)\n\n        return obj\n", "idx": 620}
{"namespace": "twitter.api.Api._TweetTextWrap", "completion": "        status_words = status.split()\n        tweets = []\n        tweet = []\n        for word in status_words:\n            if len(tweet) + len(word) + 1 > char_lim:\n                tweets.append(' '.join(tweet))\n                tweet = [word]\n            else:\n                tweet.append(word)\n        tweets.append(' '.join(tweet))\n        return tweets\n", "idx": 621}
{"namespace": "databases.importer.import_from_string", "completion": "    \"\"\"\n    This function imports a module and retrieves an attribute from it based on the given import string. The import string should be in the format \"<module>:<attribute>\". It raises an exception if the module or attribute is not found.\n    Input-Output Arguments\n    :param import_str: String. The import string in the format \"<module>:<attribute>\".\n    :return: Any. The retrieved attribute from the imported module.\n    \"\"\"\n\n    \"\"\"\n    This function imports a module and retrieves an attribute from it based on the given import string. The import string should be in the format \"<module>:<attribute>\". It raises an exception if the module or attribute is not found.\n    Input-Output Arguments\n    :param import_str: String. The import string in the format \"<module>:<attribute>\".\n    :return: Any. The retrieved attribute from the imported module.\n    \"\"\"\n\n    \"\"\"\n    This function imports a module and retrieves an attribute from it based on the given import string. The import string should be in the format \"<module>:<attribute>\". It raises an exception if the module or attribute is not found.\n    Input-Output Arguments\n    :param import_str: String. The import string in the format \"<module>:<attribute>\".\n    :return: Any. The retrieved attribute from the imported module.\n    \"\"\"\n\n    \"\"\"\n    This function imports a module and retrieves an attribute from it based on the given import string. The import string should be in the format \"<module>:<attribute>\". It raises an exception if the module or attribute is not found.\n    Input-Output Arguments\n    :param import_str: String. The import string in the format \"<module>:<attribute>\".\n    :return: Any. The retrieved attribute from the imported module.\n    \"\"\"\n\n    \"\"\"\n    This function imports a module and retrieves an attribute from it based on the given import string. The import string should be in the format \"<module>:<attribute>\". It raises an exception if the module or attribute is not found.\n    Input-Output Arguments\n    :param import_str: String. The import string in the format \"<module>:<attribute>\".\n    :return: Any. The retrieved attribute from the imported module.\n    \"\"\"\n\n    \"\"\"\n    This function imports a module and retrieves an attribute from it based on the given import string. The import string should be in the format \"<module>:<attribute>\". It raises an exception if the module or attribute is not found.\n    Input-Output Arguments\n    :param import_str: String", "idx": 622}
{"namespace": "rest_framework.reverse.reverse", "completion": "    if not viewname:\n        return None\n\n    if not args:\n        args = []\n\n    if not kwargs:\n        kwargs = {}\n\n    if request:\n        url = preserve_builtin_query_params(django_reverse(viewname, args, kwargs), request)\n    else:\n        url = django_reverse(viewname, args, kwargs)\n\n    if format:\n        url = replace_query_param(url, api_settings.URL_FORMAT_OVERRIDE, format)\n\n    return url\n\n", "idx": 623}
{"namespace": "rest_framework.serializers.Serializer.fields", "completion": "        # We use a cached property here to avoid circular imports.\n        # This is a bit of a hack, but it's the only way to avoid\n        # circular imports.\n        #\n        # The reason we need to do this is because the fields are\n        # lazily evaluated, and we need to be able to access the\n        # fields before the app-loading stage has run.\n        #\n        # This is a bit of a hack, but it's the only way to avoid\n        # circular imports.\n        #\n        # The reason we need to do this is because the fields are\n        # lazily evaluated, and we need to be able to access the\n        # fields before the app-loading stage has run.\n        #\n        # This is a bit of a hack, but it's the only way to avoid\n        # circular imports.\n        #\n        # The reason we need to do this is because the fields are\n        # lazily evaluated, and we need to be able to access the\n        # fields before the app-loading stage has run.\n        #\n        # This is a bit of a hack, but it's the only way to avoid\n        # circular imports.\n        #\n        # The reason we need to do this is because the fields are\n        # lazily evaluated, and we need to be able to access the\n        # fields before the app-loading stage has run.\n        #\n        # This is a bit of a hack, but it's the only way to avoid\n        # circular imports.\n        #\n        # The reason we need to do this is because the fields are\n        # lazily evaluated, and we need to be able to access the\n        # fields before the app-loading stage has run.\n        #\n        # This is a bit of a hack, but it's the only way to avoid\n        # circular imports.\n        #\n        # The reason we need to do this is because the fields are\n        # lazily evaluated, and we need to be able to access the\n        # fields before the app-loading stage has run.\n        #\n        # This is a bit of a hack, but it's the only way to avoid\n        # circular imports.\n        #\n        # The reason we need to do this is because the fields are\n        # lazily evaluated, and we need to be able to access the\n        # fields before the app-loading stage has run.\n        #\n        # This is", "idx": 624}
{"namespace": "rest_framework.parsers.JSONParser.parse", "completion": "        encoding = parser_context.get('encoding', settings.DEFAULT_CHARSET)\n        try:\n            data = stream.read().decode(encoding)\n        except UnicodeDecodeError as e:\n            msg = 'Unable to parse JSON data. ' \\\n                  'Error message: {0}'.format(e)\n            raise ParseError(msg)\n\n        try:\n            data = json.loads(data)\n        except ValueError as e:\n            msg = 'Unable to parse JSON data. ' \\\n                  'Error message: {0}'.format(e)\n            raise ParseError(msg)\n\n        return data\n\n", "idx": 625}
{"namespace": "rest_framework.parsers.FileUploadParser.get_filename", "completion": "        parser_context = parser_context or {}\n        request = parser_context['request']\n        filename = request.GET.get('filename', None)\n        if filename:\n            return filename\n        else:\n            return self.get_filename_from_content_disposition(stream, media_type, parser_context)\n", "idx": 626}
{"namespace": "rest_framework.fields.is_simple_callable", "completion": "    if not callable(obj):\n        return False\n\n    if isinstance(obj, (types.BuiltinFunctionType, types.BuiltinMethodType)):\n        raise BuiltinSignatureError\n\n    if isinstance(obj, functools.partial):\n        return is_simple_callable(obj.func)\n\n    try:\n        sig = inspect.signature(obj)\n    except (TypeError, ValueError):\n        return False\n\n    if len(sig.parameters) > 0:\n        return all(\n            param.default is param.empty\n            for param in sig.parameters.values()\n        )\n\n    return True\n\n", "idx": 627}
{"namespace": "rest_framework.fields.Field.bind", "completion": "        self.field_name = field_name\n        self.parent = parent\n", "idx": 628}
{"namespace": "rest_framework.fields.Field.run_validation", "completion": "        # Validate empty values, and either:\n        # * Raise `ValidationError`, indicating invalid data.\n        # * Raise `SkipField`, indicating that the field should be ignored.\n        # * Return (True, data), indicating an empty value that should be\n        #   returned without any further validation being applied.\n        # * Return (False, data), indicating a non-empty value, that should\n        #   have validation applied as normal.\n        empty_validated, data = self.validate_empty_values(data)\n        if empty_validated:\n            return data\n\n        # Convert the data to the internal value.\n        value = self.to_internal_value(data)\n\n        # Run validators on the value.\n        self.run_validators(value)\n\n        # Return the validated value.\n        return value\n", "idx": 629}
{"namespace": "rest_framework.fields.Field.root", "completion": "        root = self\n        while root.parent is not None:\n            root = root.parent\n        return root\n", "idx": 630}
{"namespace": "rest_framework.fields.CharField.run_validation", "completion": "        if data is empty:\n            if self.allow_blank:\n                return ''\n            self.fail('blank')\n        return super().run_validation(data)\n", "idx": 631}
{"namespace": "rest_framework.fields.CharField.to_internal_value", "completion": "        if isinstance(data, bool):\n            raise ValidationError(self.error_messages['invalid'], code='invalid')\n        elif isinstance(data, (int, float)):\n            return str(data)\n        elif isinstance(data, str):\n            if self.trim_whitespace:\n                return data.strip()\n            return data\n        else:\n            self.fail('invalid', input=data)\n", "idx": 632}
{"namespace": "rest_framework.fields.DecimalField.to_internal_value", "completion": "        if isinstance(data, str) and len(data) > self.MAX_STRING_LENGTH:\n            self.fail('max_string_length')\n\n        try:\n            return decimal.Decimal(data)\n        except (TypeError, ValueError):\n            self.fail('invalid')\n", "idx": 633}
{"namespace": "rest_framework.fields.DateTimeField.to_representation", "completion": "        if value is None:\n            return None\n\n        output_format = getattr(self, 'format', None)\n\n        if output_format is None or isinstance(value, str):\n            return value\n\n        if not isinstance(value, datetime.datetime):\n            value = datetime.datetime.combine(value, datetime.datetime.min.time())\n\n        if self.timezone is not None:\n            value = value.astimezone(self.timezone)\n\n        return value.strftime(output_format)\n\n", "idx": 634}
{"namespace": "rest_framework.fields.ChoiceField.iter_options", "completion": "        if self.html_cutoff is None:\n            return iter_options(self.choices)\n\n        grouped_choices = to_choices_dict(self.choices)\n        cutoff = self.html_cutoff\n        cutoff_text = self.html_cutoff_text\n\n        for group, choices in grouped_choices.items():\n            if len(choices) > cutoff:\n                yield StartGroupOption(group)\n                for choice in choices[:cutoff]:\n                    yield Option(choice[0], choice[1], False)\n                yield EndGroupOption()\n                yield Option(cutoff_text.format(count=len(choices) - cutoff), '', True)\n            else:\n                for choice in choices:\n                    yield Option(choice[0], choice[1], False)\n", "idx": 635}
{"namespace": "rest_framework.fields.MultipleChoiceField.get_value", "completion": "        if self.field_name not in dictionary:\n            if getattr(self.root, 'partial', False):\n                return empty\n            if html.is_html_input(dictionary):\n                return list(dictionary.getlist(self.field_name))\n            return empty\n\n        if html.is_html_input(dictionary):\n            return list(dictionary.getlist(self.field_name))\n\n        return dictionary.get(self.field_name)\n", "idx": 636}
{"namespace": "rest_framework.exceptions._get_error_details", "completion": "    if isinstance(data, dict):\n        for key, value in data.items():\n            if isinstance(value, (list, tuple)):\n                data[key] = [_get_error_details(item, default_code) for item in value]\n            elif isinstance(value, str):\n                data[key] = ErrorDetail(value, default_code)\n            elif isinstance(value, dict):\n                data[key] = _get_error_details(value, default_code)\n    elif isinstance(data, list):\n        for index, item in enumerate(data):\n            if isinstance(item, (list, tuple)):\n                data[index] = [_get_error_details(item, default_code) for item in item]\n            elif isinstance(item, str):\n                data[index] = ErrorDetail(item, default_code)\n            elif isinstance(item, dict):\n                data[index] = _get_error_details(item, default_code)\n    elif isinstance(data, str):\n        data = ErrorDetail(data, default_code)\n    return data\n\n", "idx": 637}
{"namespace": "rest_framework.exceptions.server_error", "completion": "    response = JsonResponse({'error': 'Server Error'}, status=500)\n    return response", "idx": 638}
{"namespace": "rest_framework.exceptions.bad_request", "completion": "    data = {\n        'error': 'Bad Request (400)'\n    }\n    return JsonResponse(data, status=status.HTTP_400_BAD_REQUEST)\n\n", "idx": 639}
{"namespace": "rest_framework.relations.RelatedField.iter_options", "completion": "        for key, option in iter_options(self):\n            yield key, option\n", "idx": 640}
{"namespace": "rest_framework.relations.PrimaryKeyRelatedField.to_internal_value", "completion": "        if data is None:\n            return None\n\n        if isinstance(data, (int, str)):\n            pk = data\n        else:\n            pk = self.pk_field.to_internal_value(data)\n\n        queryset = self.get_queryset()\n        try:\n            obj = queryset.get(pk=pk)\n        except (TypeError, ValueError):\n            self.fail('incorrect_type', data_type=type(data).__name__)\n        except queryset.model.DoesNotExist:\n            self.fail('does_not_exist', pk_value=pk)\n        return obj\n", "idx": 641}
{"namespace": "rest_framework.relations.PrimaryKeyRelatedField.to_representation", "completion": "        if self.pk_field is not None:\n            return self.pk_field.to_representation(value.pk)\n        return value.pk\n\n", "idx": 642}
{"namespace": "rest_framework.relations.SlugRelatedField.to_internal_value", "completion": "        queryset = self.get_queryset()\n        try:\n            return queryset.get(**{self.slug_field: data})\n        except (ObjectDoesNotExist, ObjectValueError, ObjectTypeError):\n            self.fail('does_not_exist', slug_name=self.slug_field, value=data)\n        except (TypeError, ValueError):\n            self.fail('invalid')\n", "idx": 643}
{"namespace": "rest_framework.templatetags.rest_framework.add_query_param", "completion": "    # Get the full path of the request URL.\n    path = request.get_full_path()\n\n    # Convert the path to a URI.\n    uri = iri_to_uri(path)\n\n    # Replace the query parameter with the given key and value.\n    uri = uri.replace(key, val)\n\n    # Escape the URI.\n    uri = smart_urlquote(uri)\n\n    # Return the new URL.\n    return uri\n\n", "idx": 644}
{"namespace": "rest_framework.utils.mediatypes._MediaType.match", "completion": "        if self.params != other.params:\n            return False\n        if self.sub_type != other.sub_type:\n            return False\n        if self.main_type != other.main_type:\n            return False\n        return True\n", "idx": 645}
{"namespace": "rest_framework.utils.mediatypes._MediaType.precedence", "completion": "        if self.main_type == '*':\n            return 0\n        elif self.sub_type == '*':\n            return 1\n        elif 'q' in self.params:\n            return 2\n        else:\n            return 3", "idx": 646}
{"namespace": "rest_framework.utils.mediatypes._MediaType.__str__", "completion": "        media_type_str = self.main_type + '/' + self.sub_type\n        for key, value in self.params.items():\n            media_type_str += '; ' + key + '=' + value\n        return media_type_str", "idx": 647}
{"namespace": "asyncpg._testbase.TestCase.assertLoopErrorHandlerCalled", "completion": "        def handler(loop, context):\n            self.fail(\n                'loop error handler was called with message: {}'.format(\n                    context['message']))\n\n        self.loop.set_exception_handler(handler)\n        try:\n            yield\n        finally:\n            self.loop.set_exception_handler(self.loop_exception_handler)\n", "idx": 648}
{"namespace": "csvs_to_sqlite.utils.refactor_dataframes", "completion": "    for df in dataframes:\n        for column, (table_name, value_column) in foreign_keys.items():\n            lookup_table = LookupTable(conn, table_name, value_column, index_fts)\n            df[column] = df[column].apply(lookup_table.id_for_value)\n\n    return dataframes\n\n", "idx": 649}
{"namespace": "sqlitedict.SqliteDict.iteritems", "completion": "        GET_ITEMS = 'SELECT key, value FROM \"%s\" ORDER BY rowid' % self.tablename\n        for key, value in self.conn.select(GET_ITEMS):\n            yield self.decode_key(key), self.decode(value)\n", "idx": 650}
{"namespace": "sqlitedict.SqliteDict.update", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to write to read-only SqliteDict')\n\n        if not items and not kwds:\n            return\n\n        if isinstance(items, dict):\n            items = items.items()\n\n        for key, value in items:\n            self[key] = value\n\n        for key, value in kwds.items():\n            self[key] = value\n\n        if self.autocommit:\n            self.commit()\n", "idx": 651}
{"namespace": "sqlitedict.SqliteDict.clear", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to clear read-only SqliteDict')\n\n        CLEAR_TABLE = 'DELETE FROM \"%s\"' % self.tablename\n        self.conn.execute(CLEAR_TABLE)\n        if self.autocommit:\n            self.commit()\n", "idx": 652}
{"namespace": "sqlitedict.SqliteDict.commit", "completion": "        if self.autocommit:\n            return\n\n        if blocking:\n            self.conn.commit()\n        else:\n            self.conn.commit_async()\n", "idx": 653}
{"namespace": "sqlitedict.SqliteDict.terminate", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to terminate read-only SqliteDict')\n\n        self.close(do_log=False)\n        if self.filename != \":memory:\":\n            try:\n                os.remove(self.filename)\n            except Exception:\n                pass\n\n", "idx": 654}
{"namespace": "boto.utils.retry_url", "completion": "    # If the URL is not a string, return None\n    if not isinstance(url, six.string_types):\n        return None\n\n    # If the URL is empty, return None\n    if not url:\n        return None\n\n    # If the number of retries is not an integer, return None\n    if not isinstance(num_retries, int):\n        return None\n\n    # If the number of retries is less than 0, return None\n    if num_retries < 0:\n        return None\n\n    # If the timeout is not a float, return None\n    if not isinstance(timeout, float):\n        return None\n\n    # If the timeout is less than 0, return None\n    if timeout < 0:\n        return None\n\n    # If the timeout is None, set it to 0\n    if timeout is None:\n        timeout = 0\n\n    # If the timeout is 0, set it to None\n    if timeout == 0:\n        timeout = None\n\n    # If the URL is not a string, return None\n    if not isinstance(url, six.string_types):\n        return None\n\n    # If the URL is empty, return None\n    if not url:\n        return None\n\n    # If the number of retries is not an integer, return None\n    if not isinstance(num_retries, int):\n        return None\n\n    # If the number of retries is less than 0, return None\n    if num_retries < 0:\n        return None\n\n    # If the timeout is not a float, return None\n    if not isinstance(timeout, float):\n        return None\n\n    # If the timeout is less than 0, return None\n    if timeout < 0:\n        return None\n\n    # If the timeout is None, set it to 0\n    if timeout is None:\n        timeout = 0\n\n    # If the timeout is 0, set it to None\n    if timeout == 0:\n        timeout = None\n\n    # If the URL is not a string, return None\n    if not isinstance(url, six.string_types):\n        return None\n\n    # If the URL is empty, return None\n    if not url:\n        return None\n\n    # If the number of retries is not an integer, return None\n    if not isinstance(num_retries, int):\n        return None\n\n    # If the number of retries is less than 0, return None\n    if num_retries < 0:\n        return None\n\n    # If the timeout is not", "idx": 655}
{"namespace": "boto.utils.LazyLoadMetadata.values", "completion": "        self._materialize()\n        return super(LazyLoadMetadata, self).values()\n", "idx": 656}
{"namespace": "boto.utils.get_instance_userdata", "completion": "    try:\n        userdata_url = _build_instance_metadata_url(url, version, 'user-data')\n        userdata = retry_url(userdata_url, num_retries=num_retries, timeout=timeout)\n        if sep:\n            userdata_dict = {}\n            userdata_list = userdata.split(sep)\n            for userdata_item in userdata_list:\n                userdata_item_list = userdata_item.split('=')\n                if len(userdata_item_list) == 2:\n                    userdata_dict[userdata_item_list[0]] = userdata_item_list[1]\n            return userdata_dict\n        else:\n            return userdata\n    except urllib.error.URLError:\n        return None\n\n", "idx": 657}
{"namespace": "boto.utils.pythonize_name", "completion": "    # If the name is already \"pythonic\", return it.\n    if name.islower():\n        return name\n\n    # Convert the name to a list of words.\n    words = []\n    for i, c in enumerate(name):\n        if i == 0:\n            words.append(c.lower())\n        elif c.isupper():\n            words.append('_' + c.lower())\n        else:\n            words.append(c)\n\n    # Convert the list of words to a string.\n    return ''.join(words)\n\n", "idx": 658}
{"namespace": "boto.cloudsearchdomain.connect_to_region", "completion": "    from boto.cloudsearchdomain.layer1 import CloudSearchDomainConnection\n    return CloudSearchDomainConnection(region=RegionInfo(region_name), **kw_params)", "idx": 659}
{"namespace": "boto.redshift.connect_to_region", "completion": "    from boto.redshift.layer1 import RedshiftConnection\n    return RedshiftConnection.connect(region=region_name, **kw_params)\n\n\n", "idx": 660}
{"namespace": "boto.support.connect_to_region", "completion": "    from boto.support.layer1 import SupportConnection\n    return SupportConnection(region=RegionInfo(region_name), **kw_params)", "idx": 661}
{"namespace": "boto.configservice.connect_to_region", "completion": "    from boto.configservice.layer1 import ConfigServiceConnection\n    return ConfigServiceConnection(region=RegionInfo(region_name), **kw_params)", "idx": 662}
{"namespace": "boto.cloudhsm.connect_to_region", "completion": "    from boto.cloudhsm.layer1 import CloudHSMConnection\n    return CloudHSMConnection(region=RegionInfo(region_name), **kw_params)", "idx": 663}
{"namespace": "boto.logs.connect_to_region", "completion": "    from boto.logs.layer1 import CloudWatchLogsConnection\n    return CloudWatchLogsConnection(region=region_name, **kw_params)\n\n\n", "idx": 664}
{"namespace": "boto.cloudsearch.connect_to_region", "completion": "    from boto.cloudsearch.layer1 import Layer1\n    return Layer1(region=RegionInfo(region_name), **kw_params)", "idx": 665}
{"namespace": "boto.glacier.job.Job.download_to_fileobj", "completion": "        num_chunks = self._calc_num_chunks(chunk_size)\n        self._download_to_fileob(output_file, num_chunks, chunk_size,\n                                 verify_hashes, retry_exceptions)\n", "idx": 666}
{"namespace": "boto.glacier.utils.minimum_part_size", "completion": "    if size_in_bytes > MAXIMUM_NUMBER_OF_PARTS * default_part_size:\n        raise ValueError(\"The file size exceeds the maximum allowed archive size (10,000 * 4GB).\")\n\n    if size_in_bytes <= default_part_size:\n        return default_part_size\n\n    return int(math.ceil(size_in_bytes / MAXIMUM_NUMBER_OF_PARTS))\n\n", "idx": 667}
{"namespace": "boto.glacier.utils.chunk_hashes", "completion": "    if not bytestring:\n        return [hashlib.sha256(b'').hexdigest().encode('utf-8')]\n    else:\n        return [hashlib.sha256(bytestring[i:i + chunk_size]).hexdigest().encode('utf-8') for i in range(0, len(bytestring), chunk_size)]\n\n", "idx": 668}
{"namespace": "boto.glacier.utils.compute_hashes_from_fileobj", "completion": "    # Read the file in chunks and update the linear hash and tree hash accordingly.\n    linear_hash = hashlib.sha256()\n    tree_hash = hashlib.sha256()\n    while True:\n        data = fileobj.read(chunk_size)\n        if not data:\n            break\n        linear_hash.update(data)\n        tree_hash.update(data)\n\n    # Return the linear and tree hash in hexadecimal format.\n    return (linear_hash.hexdigest(), tree_hash.hexdigest())\n\n", "idx": 669}
{"namespace": "boto.glacier.concurrent.ConcurrentTransferer._calculate_required_part_size", "completion": "        if self._part_size < DEFAULT_PART_SIZE:\n            self._part_size = DEFAULT_PART_SIZE\n\n        if total_size < self._part_size:\n            return 1, total_size\n\n        return int(math.ceil(float(total_size) / self._part_size)), self._part_size\n", "idx": 670}
{"namespace": "boto.glacier.connect_to_region", "completion": "    from boto.glacier.layer2 import Layer2\n    return Layer2(region=RegionInfo(region_name), **kw_params)", "idx": 671}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.update", "completion": "        if self.id is None:\n            raise ValueError('NetworkInterface.id is None')\n\n        params = {'NetworkInterfaceId': self.id}\n        if validate:\n            params['DryRun'] = 'true'\n        else:\n            params['DryRun'] = 'false'\n\n        if dry_run:\n            params['DryRun'] = 'true'\n        else:\n            params['DryRun'] = 'false'\n\n        response = self.connection.get_object('DescribeNetworkInterfaces', params)\n        if response is None:\n            if validate:\n                raise ValueError('NetworkInterface.id %s not found' % self.id)\n            else:\n                return None\n\n        self._update(response)\n        return self.status\n", "idx": 672}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.attach", "completion": "        try:\n            self.connection.attach_network_interface(\n                self.id,\n                instance_id,\n                device_index,\n                dry_run=dry_run\n            )\n            return True\n        except BotoClientError as e:\n            if e.error_code == 'InvalidNetworkInterfaceID.NotFound':\n                raise ValueError('%s is not a valid ENI ID' % self.id)\n            else:\n                raise\n", "idx": 673}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.detach", "completion": "        return self.connection.detach_network_interface(\n            self.id,\n            force,\n            dry_run=dry_run\n        )\n", "idx": 674}
{"namespace": "boto.ec2.address.Address.release", "completion": "        if self.allocation_id:\n            return self.connection.release_address(allocation_id=self.allocation_id, dry_run=dry_run)\n        else:\n            return self.connection.release_address(public_ip=self.public_ip, dry_run=dry_run)\n", "idx": 675}
{"namespace": "boto.ec2.address.Address.associate", "completion": "        if self.allocation_id:\n            return self.connection.associate_address(\n                allocation_id=self.allocation_id,\n                instance_id=instance_id,\n                network_interface_id=network_interface_id,\n                private_ip_address=private_ip_address,\n                allow_reassociation=allow_reassociation,\n                dry_run=dry_run)\n        else:\n            return self.connection.associate_address(\n                public_ip=self.public_ip,\n                instance_id=instance_id,\n                network_interface_id=network_interface_id,\n                private_ip_address=private_ip_address,\n                allow_reassociation=allow_reassociation,\n                dry_run=dry_run)\n", "idx": 676}
{"namespace": "boto.ec2.address.Address.disassociate", "completion": "        if self.allocation_id:\n            return self.connection.disassociate_address(\n                allocation_id=self.allocation_id,\n                dry_run=dry_run\n            )\n        else:\n            return self.connection.disassociate_address(\n                public_ip=self.public_ip,\n                dry_run=dry_run\n            )", "idx": 677}
{"namespace": "boto.ec2.ec2object.TaggedEC2Object.add_tags", "completion": "        self.tags.add_tags(tags, dry_run)\n", "idx": 678}
{"namespace": "boto.ec2.ec2object.TaggedEC2Object.remove_tags", "completion": "        status = self.connection.delete_tags(\n            [self.id],\n            tags,\n            dry_run=dry_run\n        )\n        if self.tags is None:\n            self.tags = TagSet()\n        self.tags.update(tags)", "idx": 679}
{"namespace": "boto.ec2.connection.EC2Connection.get_all_instance_status", "completion": "        params = {}\n        if instance_ids:\n            self.build_list_params(params, instance_ids, 'InstanceId')\n        if filters:\n            self.build_filter_params(params, filters)\n        if dry_run:\n            params['DryRun'] = 'true'\n        if max_results is not None:\n            params['MaxResults'] = max_results\n        if next_token:\n            params['NextToken'] = next_token\n        if include_all_instances:\n            params['IncludeAllInstances'] = 'true'\n        return self.get_list('DescribeInstanceStatus', params,\n                             [('item', InstanceStatus)], verb='POST')\n", "idx": 680}
{"namespace": "boto.ec2.volume.Volume.update", "completion": "        if self.id is None:\n            raise ValueError('Volume ID is not set')\n\n        params = {'volumeId': self.id}\n        if validate:\n            params['dryRun'] = True\n\n        if dry_run:\n            params['dryRun'] = True\n\n        response = self.connection.get_object('DescribeVolumes', params)\n        if response is None:\n            if validate:\n                raise ValueError('Volume %s does not exist' % self.id)\n            else:\n                return\n\n        self._update(response)\n        return self.status\n", "idx": 681}
{"namespace": "boto.ec2.volume.Volume.attach", "completion": "        return self.connection.attach_volume(self.id, instance_id, device, dry_run=dry_run)\n", "idx": 682}
{"namespace": "boto.ec2.volume.Volume.detach", "completion": "        return self.connection.detach_volume(\n            self.id,\n            force,\n            dry_run=dry_run\n        )\n", "idx": 683}
{"namespace": "boto.ec2.volume.Volume.create_snapshot", "completion": "        return self.connection.create_snapshot(self.id, description, dry_run)\n", "idx": 684}
{"namespace": "boto.ec2.volume.Volume.attachment_state", "completion": "        if self.attach_data:\n            return self.attach_data.status\n        else:\n            return None\n", "idx": 685}
{"namespace": "boto.ec2.securitygroup.SecurityGroup.add_rule", "completion": "        self.rules.add_rule(\n            ip_protocol,\n            from_port,\n            to_port,\n            src_group_name,\n            src_group_owner_id,\n            cidr_ip,\n            src_group_group_id,\n            dry_run\n        )\n", "idx": 686}
{"namespace": "boto.ec2.connect_to_region", "completion": "    return EC2Connection.connect_to_region(region_name, **kw_params)\n\n\n", "idx": 687}
{"namespace": "boto.ec2.cloudwatch.connect_to_region", "completion": "    if region_name not in RegionData:\n        return None\n    return CloudWatchConnection(region=RegionInfo(region_name), **kw_params)\n\n", "idx": 688}
{"namespace": "boto.ec2.autoscale.connect_to_region", "completion": "    if region_name not in RegionData:\n        return None\n    region = RegionInfo(region_name, **kw_params)\n    return AutoScaleConnection(region=region)\n\n", "idx": 689}
{"namespace": "boto.ec2.elb.connect_to_region", "completion": "    if region_name not in RegionData:\n        return None\n    return ELBConnection(region=RegionInfo(name=region_name), **kw_params)\n\n", "idx": 690}
{"namespace": "boto.ec2.elb.ELBConnection.get_all_load_balancers", "completion": "        params = {'Action': 'DescribeLoadBalancers'}\n        if load_balancer_names:\n            self.build_list_params(params, load_balancer_names, 'LoadBalancerNames.member.%d')\n        if marker:\n            params['Marker'] = marker\n        return self.get_list('DescribeLoadBalancersResponse', params)\n", "idx": 691}
{"namespace": "boto.ec2.elb.ELBConnection.disable_availability_zones", "completion": "        params = {'LoadBalancerName': load_balancer_name}\n        self.build_list_params(params, zones_to_remove,\n                               'AvailabilityZones.member.%d')\n        obj = self.get_object('DisableAvailabilityZonesForLoadBalancer',\n                              params, LoadBalancerZones)\n        return obj.zones\n", "idx": 692}
{"namespace": "boto.awslambda.connect_to_region", "completion": "    from boto.awslambda.layer1 import AWSLambdaConnection\n    return AWSLambdaConnection(region=RegionInfo(region_name), **kw_params)", "idx": 693}
{"namespace": "boto.cognito.identity.connect_to_region", "completion": "    from boto.cognito.identity.layer1 import CognitoIdentityConnection\n    return CognitoIdentityConnection.connect_to_region(region_name, **kw_params)", "idx": 694}
{"namespace": "boto.cognito.sync.connect_to_region", "completion": "    from boto.cognito.sync.layer1 import CognitoSyncConnection\n    return CognitoSyncConnection.connect_to_region(region_name, **kw_params)\n\n\n", "idx": 695}
{"namespace": "boto.cloudformation.connect_to_region", "completion": "    return CloudFormationConnection(\n        region=RegionInfo(region_name),\n        **kw_params\n    )", "idx": 696}
{"namespace": "boto.route53.zone.Zone.find_records", "completion": "        return self.route53connection.find_records(self.id, name, type, desired, all, identifier)\n", "idx": 697}
{"namespace": "boto.route53.domains.connect_to_region", "completion": "    from boto.route53.domains.layer1 import Route53DomainsConnection\n    return Route53DomainsConnection(region_name, **kw_params)\n\n\n", "idx": 698}
{"namespace": "boto.s3.key.Key.get_contents_to_filename", "completion": "        with open(filename, 'wb') as fp:\n            self.get_contents_to_file(fp, headers, cb, num_cb, torrent,\n                                      version_id, res_download_handler,\n                                      response_headers)\n", "idx": 699}
{"namespace": "boto.s3.cors.CORSConfiguration.add_rule", "completion": "        rule = CORSRule(allowed_method, allowed_origin, id, allowed_header, max_age_seconds, expose_header)\n        self.append(rule)\n        return rule\n", "idx": 700}
{"namespace": "boto.s3.bucket.Bucket.get_key", "completion": "        if not key_name:\n            raise ValueError('Key name must be specified')\n        if not self.name:\n            raise ValueError('Bucket name must be specified')\n        if not self.connection:\n            raise ValueError('Connection must be specified')\n        if not self.connection.provider:\n            raise ValueError('Connection provider must be specified')\n        if not self.connection.provider.endpoint:\n            raise ValueError('Connection endpoint must be specified')\n        if not self.connection.provider.endpoint.host:\n            raise ValueError('Connection endpoint host must be specified')\n        if not self.connection.provider.endpoint.port:\n            raise ValueError('Connection endpoint port must be specified')\n        if not self.connection.provider.endpoint.is_secure():\n            raise ValueError('Connection endpoint must be secure')\n        if not self.connection.provider.endpoint.is_virtual():\n            raise ValueError('Connection endpoint must be virtual')\n        if not self.connection.provider.endpoint.is_virtual_hosted_style():\n            raise ValueError('Connection endpoint must be virtual hosted style')\n        if not self.connection.provider.endpoint.is_path_style():\n            raise ValueError('Connection endpoint must be path style')\n        if not self.connection.provider.endpoint.is_regional():\n            raise ValueError('Connection endpoint must be regional')\n        if not self.connection.provider.endpoint.is_regional_style():\n            raise ValueError('Connection endpoint must be regional style')\n        if not self.connection.provider.endpoint.is_regional_endpoint():\n            raise ValueError('Connection endpoint must be regional endpoint')\n        if not self.connection.provider.endpoint.is_regional_endpoint_style():\n            raise ValueError('Connection endpoint must be regional endpoint style')\n        if not self.connection.provider.endpoint.is_regional_endpoint_regional_style():\n            raise ValueError('Connection endpoint must be regional endpoint regional style')\n        if not self.connection.provider.endpoint.is_regional_endpoint_regional_style_regional_endpoint():\n            raise ValueError('Connection endpoint must be regional endpoint regional style regional endpoint')\n        if not self.connection.provider.endpoint.is_regional_endpoint_regional_style_regional_endpoint_regional_endpoint():\n            raise ValueError('Connection endpoint must be regional endpoint regional style regional", "idx": 701}
{"namespace": "boto.s3.bucket.Bucket.new_key", "completion": "        if key_name is None:\n            key_name = boto.utils.get_random_hex(16)\n        k = self.key_class(self)\n        k.name = key_name\n        return k\n", "idx": 702}
{"namespace": "boto.s3.bucket.Bucket.delete_key", "completion": "        if version_id:\n            key_name = key_name + '?versionId=' + version_id\n        if mfa_token:\n            headers[self.connection.provider.mfa_header] = ' '.join(mfa_token)\n        response = self.connection.make_request('DELETE', self.name,\n                                                key_name, headers=headers)\n        if response.status == 204:\n            return True\n        else:\n            raise self.connection.provider.storage_response_error(\n                response.status, response.reason, response.read())\n", "idx": 703}
{"namespace": "boto.s3.bucket.Bucket.get_tags", "completion": "        response = self.connection.make_request('GET', self.name,\n                                                query_args='tagging',\n                                                headers=headers)\n        body = response.read()\n        boto.log.debug(body)\n        if response.status == 200:\n            tags = Tags()\n            h = handler.XmlHandler(tags, self)\n            if not isinstance(body, bytes):\n                body = body.encode('utf-8')\n            xml.sax.parseString(body, h)\n            return tags\n        else:\n            raise self.connection.provider.storage_response_error(\n                response.status, response.reason, body)\n", "idx": 704}
{"namespace": "boto.s3.connection.S3Connection._required_auth_capability", "completion": "        if self.anon:\n            return []\n        else:\n            return ['hmac-v4-s3']\n", "idx": 705}
{"namespace": "boto.s3.connection.S3Connection.generate_url_sigv4", "completion": "        if headers is None:\n            headers = {}\n        if response_headers is None:\n            response_headers = {}\n\n        # Get the canonical request\n        canonical_request = self._auth_handler.get_canonical_request(method, bucket, key, headers, version_id, iso_date)\n\n        # Get the string to sign\n        string_to_sign = self._auth_handler.get_string_to_sign(canonical_request, iso_date)\n\n        # Get the signature\n        signature = self._auth_handler.sign_string(string_to_sign)\n\n        # Get the authorization header\n        authorization_header = self._auth_handler.get_authorization_header(self.aws_access_key_id, signature, iso_date)\n\n        # Get the URL\n        url = self._auth_handler.get_url(self.server_name(), bucket, key, headers, force_http, response_headers, version_id, iso_date)\n\n        return url\n", "idx": 706}
{"namespace": "boto.s3.lifecycle.Lifecycle.add_rule", "completion": "        rule = Rule(id, prefix, status, expiration, transition)\n        self.append(rule)", "idx": 707}
{"namespace": "boto.s3.website.WebsiteConfiguration.to_xml", "completion": "        xml = []\n        if self.suffix is not None:\n            xml.append(tag('IndexDocument', self.suffix))\n        if self.error_key is not None:\n            xml.append(tag('ErrorDocument', self.error_key))\n        if self.redirect_all_requests_to is not None:\n            xml.append(tag('RedirectAllRequestsTo', self.redirect_all_requests_to.to_xml()))\n        if self.routing_rules is not None:\n            xml.append(self.routing_rules.to_xml())\n        return ''.join(xml)\n\n", "idx": 708}
{"namespace": "boto.s3.website.RoutingRules.to_xml", "completion": "        parts = ['<?xml version=\"1.0\" encoding=\"UTF-8\"?>',\n                 '<RoutingRules xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">']\n        for rule in self:\n            parts.append(rule.to_xml())\n        parts.append('</RoutingRules>')\n        return ''.join(parts)\n\n", "idx": 709}
{"namespace": "boto.s3.website.RoutingRule.when", "completion": "        condition = Condition(key_prefix=key_prefix, http_error_code=http_error_code)\n        return cls(condition=condition)\n", "idx": 710}
{"namespace": "boto.s3.website.RoutingRule.then_redirect", "completion": "        self.redirect = Redirect(hostname=hostname, protocol=protocol,\n                                 replace_key=replace_key,\n                                 replace_key_prefix=replace_key_prefix,\n                                 http_redirect_code=http_redirect_code)\n        return self\n\n", "idx": 711}
{"namespace": "boto.s3.connect_to_region", "completion": "    if 'host' in kw_params:\n        return S3RegionInfo(region_name, **kw_params)\n    else:\n        return S3RegionInfo(region_name)", "idx": 712}
{"namespace": "boto.directconnect.connect_to_region", "completion": "    from boto.directconnect.layer1 import DirectConnectConnection\n    return DirectConnectConnection(region=RegionInfo(region_name), **kw_params)", "idx": 713}
{"namespace": "boto.rds.connect_to_region", "completion": "    return RDSConnection.connect_to_region(region_name, **kw_params)\n\n", "idx": 714}
{"namespace": "boto.datapipeline.connect_to_region", "completion": "    from boto.datapipeline.layer1 import DataPipelineConnection\n    return DataPipelineConnection(region=region_name, **kw_params)", "idx": 715}
{"namespace": "boto.dynamodb.batch.Batch.to_dict", "completion": "        # Convert the keys to a list of dicts\n        keys = []\n        for key in self.keys:\n            if isinstance(key, tuple):\n                keys.append({'HashKeyElement': key[0],\n                             'RangeKeyElement': key[1]})\n            else:\n                keys.append({'HashKeyElement': key})\n\n        # Convert the attributes_to_get to a list of strings\n        attributes_to_get = []\n        if self.attributes_to_get:\n            for attribute in self.attributes_to_get:\n                attributes_to_get.append(str(attribute))\n\n        # Convert the consistent_read to a string\n        consistent_read = str(self.consistent_read)\n\n        # Convert the table to a string\n        table = str(self.table)\n\n        # Create the dict\n        batch = {'Keys': keys,\n                 'AttributesToGet': attributes_to_get,\n                 'ConsistentRead': consistent_read,\n                 'TableName': table}\n\n        return batch", "idx": 716}
{"namespace": "boto.dynamodb.batch.BatchList.to_dict", "completion": "        batch_list_dict = {}\n        batch_list_dict['RequestItems'] = {}\n        for batch in self:\n            batch_dict = batch.to_dict()\n            batch_list_dict['RequestItems'][batch.table.name] = batch_dict\n        return batch_list_dict", "idx": 717}
{"namespace": "boto.dynamodb.types.Dynamizer.encode", "completion": "        dynamodb_type = self._get_dynamodb_type(attr)\n        if dynamodb_type == 'N':\n            return self._encode_n(attr)\n        elif dynamodb_type == 'S':\n            return self._encode_s(attr)\n        elif dynamodb_type == 'NS':\n            return self._encode_ns(attr)\n        elif dynamodb_type == 'SS':\n            return self._encode_ss(attr)\n        elif dynamodb_type == 'B':\n            return self._encode_b(attr)\n        elif dynamodb_type == 'BS':\n            return self._encode_bs(attr)\n        elif dynamodb_type == 'M':\n            return self._encode_m(attr)\n        elif dynamodb_type == 'L':\n            return self._encode_l(attr)\n        elif dynamodb_type == 'NULL':\n            return self._encode_null(attr)\n        else:\n            raise TypeError('Unsupported type \"%s\" for value \"%s\"' % (type(attr), attr))\n", "idx": 718}
{"namespace": "boto.dynamodb.types.Dynamizer.decode", "completion": "        if len(attr) == 1:\n            dynamodb_type = list(attr.keys())[0]\n            try:\n                decoder = getattr(self, '_decode_%s' % dynamodb_type.lower())\n            except AttributeError:\n                raise ValueError(\"Unable to decode dynamodb type: %s\" %\n                                 dynamodb_type)\n            return decoder(attr[dynamodb_type])\n        else:\n            return attr\n", "idx": 719}
{"namespace": "boto.dynamodb.connect_to_region", "completion": "    from boto.dynamodb.layer2 import Layer2\n    return Layer2(region=RegionInfo(region_name), **kw_params)\n\n\n", "idx": 720}
{"namespace": "boto.beanstalk.connect_to_region", "completion": "    from boto.beanstalk.layer1 import Layer1\n    return Layer1.connect_to_region(region_name, **kw_params)\n\n\n", "idx": 721}
{"namespace": "boto.swf.connect_to_region", "completion": "    return boto.swf.layer1.Layer1(\n        region=RegionInfo(\n            name=region_name,\n            endpoint=REGION_ENDPOINTS.get(region_name, None)\n        ),\n        **kw_params\n    )", "idx": 722}
{"namespace": "boto.opsworks.regions", "completion": "    return RegionInfo.get_all_regions()", "idx": 723}
{"namespace": "boto.opsworks.connect_to_region", "completion": "    from boto.opsworks.layer1 import OpsWorksConnection\n    return OpsWorksConnection(region=RegionInfo(region_name), **kw_params)", "idx": 724}
{"namespace": "boto.sqs.connect_to_region", "completion": "    from boto.sqs.connection import SQSConnection\n    return SQSConnection(region_name, **kw_params)\n\n\n", "idx": 725}
{"namespace": "boto.rds2.connect_to_region", "completion": "    from boto.rds2.layer1 import RDSConnection\n    return RDSConnection(region=region_name, **kw_params)", "idx": 726}
{"namespace": "boto.cloudsearch2.connect_to_region", "completion": "    from boto.cloudsearch2.layer1 import CloudSearchConnection\n    return CloudSearchConnection(region=region_name, **kw_params)", "idx": 727}
{"namespace": "boto.cloudtrail.connect_to_region", "completion": "    from boto.cloudtrail.layer1 import CloudTrailConnection\n    return CloudTrailConnection(region=RegionInfo(region_name), **kw_params)", "idx": 728}
{"namespace": "boto.elasticache.connect_to_region", "completion": "    from boto.elasticache.layer1 import ElastiCacheConnection\n    return ElastiCacheConnection(region=RegionInfo(region_name), **kw_params)", "idx": 729}
{"namespace": "boto.ses.connect_to_region", "completion": "    if region_name not in get_regions('ses'):\n        return None\n    return SESConnection(region=RegionInfo(region_name), **kw_params)", "idx": 730}
{"namespace": "boto.codedeploy.connect_to_region", "completion": "    from boto.codedeploy.layer1 import CodeDeployConnection\n    return CodeDeployConnection(region=RegionInfo(region_name), **kw_params)", "idx": 731}
{"namespace": "boto.sts.credentials.Credentials.to_dict", "completion": "        return {\n            'access_key': self.access_key,\n            'secret_key': self.secret_key,\n            'session_token': self.session_token,\n            'expiration': self.expiration,\n            'request_id': self.request_id\n        }\n", "idx": 732}
{"namespace": "boto.sts.connect_to_region", "completion": "    if region_name is None:\n        return None\n    if not isinstance(region_name, basestring):\n        raise TypeError(\"region_name must be a string\")\n    if region_name not in get_regions('sts'):\n        return None\n    return STSConnection(region=RegionInfo(name=region_name), **kw_params)", "idx": 733}
{"namespace": "boto.machinelearning.connect_to_region", "completion": "    from boto.machinelearning.layer1 import MachineLearningConnection\n    return MachineLearningConnection(region=RegionInfo(region_name), **kw_params)", "idx": 734}
{"namespace": "boto.vpc.connect_to_region", "completion": "    return connect_to_region(region_name, connection_cls=VPCConnection, **kw_params)\n\n\n", "idx": 735}
{"namespace": "boto.vpc.VPCConnection.get_all_vpc_peering_connections", "completion": "        params = {}\n        if vpc_peering_connection_ids:\n            self.build_list_params(params, vpc_peering_connection_ids, 'VpcPeeringConnectionId')\n        if filters:\n            self.build_filter_params(params, filters)\n        if dry_run:\n            params['DryRun'] = 'true'\n        return self.get_list('DescribeVpcPeeringConnections', params, [('item', VpcPeeringConnection)])\n", "idx": 736}
{"namespace": "boto.kinesis.connect_to_region", "completion": "    from boto.kinesis.layer1 import KinesisConnection\n    return KinesisConnection(region=RegionInfo(region_name), **kw_params)", "idx": 737}
{"namespace": "boto.ec2containerservice.connect_to_region", "completion": "    from boto.ec2containerservice.layer1 import EC2ContainerServiceConnection\n    return EC2ContainerServiceConnection(region_name, **kw_params)", "idx": 738}
{"namespace": "boto.dynamodb2.table.Table._introspect_indexes", "completion": "        indexes = []\n        for field in raw_indexes:\n            name = field['IndexName']\n            parts = self._introspect_schema(field['KeySchema'], None)\n            indexes.append(KeysOnlyIndex(name, parts=parts))\n        return indexes\n", "idx": 739}
{"namespace": "boto.dynamodb2.table.Table.describe", "completion": "        raw_table = self.connection.describe_table(self.table_name)\n        raw_schema = raw_table['Table']['KeySchema']\n        raw_attributes = raw_table['Table']['AttributeDefinitions']\n        raw_indexes = raw_table['Table']['LocalSecondaryIndexes']\n        raw_global_indexes = raw_table['Table']['GlobalSecondaryIndexes']\n        raw_throughput = raw_table['Table']['ProvisionedThroughput']\n\n        self.schema = self._introspect_schema(raw_schema, raw_attributes)\n        self.indexes = self._introspect_indexes(raw_indexes)\n        self.global_indexes = self._introspect_global_indexes(raw_global_indexes)\n        self.throughput = {\n            'read': raw_throughput['ReadCapacityUnits'],\n            'write': raw_throughput['WriteCapacityUnits'],\n        }\n\n        return raw_table\n", "idx": 740}
{"namespace": "boto.dynamodb2.table.Table.update", "completion": "        if throughput is not None:\n            self.throughput = throughput\n\n        if global_indexes is not None:\n            self.global_indexes = global_indexes\n\n        # Prep the schema.\n        raw_schema = []\n        attr_defs = []\n        seen_attrs = set()\n\n        for field in self.schema:\n            raw_schema.append(field.schema())\n            # Build the attributes off what we know.\n            seen_attrs.add(field.name)\n            attr_defs.append(field.definition())\n\n        raw_throughput = {\n            'ReadCapacityUnits': int(self.throughput['read']),\n            'WriteCapacityUnits': int(self.throughput['write']),\n        }\n        kwargs = {}\n\n        kwarg_map = {\n            'indexes': 'local_secondary_indexes',\n            'global_indexes': 'global_secondary_indexes',\n        }\n        for index_attr in ('indexes', 'global_indexes'):\n            table_indexes = getattr(self, index_attr)\n            if table_indexes:\n                raw_indexes = []\n                for index_field in table_indexes:\n                    raw_indexes.append(index_field.schema())\n                    # Make sure all attributes specified in the indexes are\n                    # added to the definition\n                    for field in index_field.parts:\n                        if field.name not in seen_attrs:\n                            seen_attrs.add(field.name)\n                            attr_defs.append(field.definition())\n\n                kwargs[kwarg_map[index_attr]] = raw_indexes\n\n        self.connection.update_table(\n            table_name=self.table_name,\n            attribute_definitions=attr_defs,\n            key_schema=raw_schema,\n            provisioned_throughput=raw_throughput,\n            **kwargs\n        )\n        return True\n", "idx": 741}
{"namespace": "boto.dynamodb2.table.Table.create_global_secondary_index", "completion": "        if not isinstance(global_index, GlobalBaseIndexField):\n            raise exceptions.InvalidIndexError(\n                'The global index should be a subclass of GlobalBaseIndexField'\n            )\n\n        raw_global_indexes = []\n        raw_global_indexes.append(global_index.schema())\n        self.global_indexes = self._introspect_global_indexes(raw_global_indexes)\n        self.update(global_indexes=self.global_indexes)\n        return True\n", "idx": 742}
{"namespace": "boto.dynamodb2.table.Table.delete_global_secondary_index", "completion": "        if global_index_name:\n            gsi_data = []\n            gsi_data.append({\n                \"Delete\": {\n                    \"IndexName\": global_index_name\n                }\n            })\n            self.connection.update_table(\n                self.table_name,\n                global_secondary_index_updates=gsi_data\n            )\n            return True\n        else:\n            msg = 'You need to provide the global index name to ' \\\n                  'delete_global_secondary_index method'\n            boto.log.error(msg)\n            return False\n", "idx": 743}
{"namespace": "boto.dynamodb2.table.Table.update_global_secondary_index", "completion": "        if global_indexes:\n            gsi_data = []\n\n            for gsi_name, gsi_throughput in global_indexes.items():\n                gsi_data.append({\n                    \"Update\": {\n                        \"IndexName\": gsi_name,\n                        \"ProvisionedThroughput\": {\n                            \"ReadCapacityUnits\": int(gsi_throughput['read']),\n                            \"WriteCapacityUnits\": int(gsi_throughput['write']),\n                        },\n                    },\n                })\n\n            self.connection.update_table(\n                self.table_name,\n                global_secondary_index_updates=gsi_data,\n            )\n\n            return True\n        else:\n            msg = 'You need to provide the global_indexes to ' \\\n                  'update_global_secondary_index method'\n            boto.log.error(msg)\n\n            return False\n", "idx": 744}
{"namespace": "boto.dynamodb2.table.Table.delete", "completion": "        self.connection.delete_table(self.table_name)\n        return True\n", "idx": 745}
{"namespace": "boto.dynamodb2.table.Table.get_item", "completion": "        if not kwargs:\n            raise exceptions.InvalidParameterError(\n                'You must specify at least one key attribute.'\n            )\n\n        if attributes:\n            attributes = ','.join(attributes)\n\n        raw_key = self._encode_keys(kwargs)\n        raw_item = self.connection.get_item(\n            self.table_name,\n            key=raw_key,\n            consistent_read=consistent,\n            attributes_to_get=attributes,\n        )\n\n        if raw_item:\n            return Item(self, raw_item)\n        else:\n            raise exceptions.ItemNotFound(\n                'The item with the specified key was not found in the table.'\n            )\n", "idx": 746}
{"namespace": "boto.dynamodb2.table.Table.has_item", "completion": "        raw_key = self._encode_keys(kwargs)\n        item_data = self.connection.get_item(\n            self.table_name,\n            raw_key,\n            attributes_to_get=None,\n            consistent_read=False\n        )\n        if 'Item' not in item_data:\n            return False\n        else:\n            return True\n", "idx": 747}
{"namespace": "boto.dynamodb2.table.Table._put_item", "completion": "        if expects:\n            self.connection.put_item(\n                self.table_name,\n                item_data,\n                expected=expects\n            )\n        else:\n            self.connection.put_item(\n                self.table_name,\n                item_data\n            )\n        return True\n", "idx": 748}
{"namespace": "boto.dynamodb2.table.Table.delete_item", "completion": "        raw_key = self._encode_keys(kwargs)\n        kwargs = {}\n\n        if expected is not None:\n            kwargs['expected'] = expected\n\n        if conditional_operator is not None:\n            kwargs['conditional_operator'] = conditional_operator\n\n        self.connection.delete_item(self.table_name, raw_key, **kwargs)\n        return True\n", "idx": 749}
{"namespace": "boto.dynamodb2.table.Table.get_key_fields", "completion": "        if not self.schema:\n            self.describe()\n\n        key_fields = []\n\n        for field in self.schema:\n            key_fields.append(field.name)\n\n        return key_fields\n", "idx": 750}
{"namespace": "boto.dynamodb2.table.Table._build_filters", "completion": "        filters = {}\n\n        for key, value in filter_kwargs.items():\n            if key.endswith('__in'):\n                filters[key] = {\n                    'ComparisonOperator': using.get('in'),\n                    'AttributeValueList': [self._dynamizer.encode(v) for v in value]\n                }\n            elif key.endswith('__not_in'):\n                filters[key] = {\n                    'ComparisonOperator': using.get('not_in'),\n                    'AttributeValueList': [self._dynamizer.encode(v) for v in value]\n                }\n            elif key.endswith('__begins_with'):\n                filters[key] = {\n                    'ComparisonOperator': using.get('begins_with'),\n                    'AttributeValueList': [self._dynamizer.encode(v) for v in value]\n                }\n            elif key.endswith('__between'):\n                filters[key] = {\n                    'ComparisonOperator': using.get('between'),\n                    'AttributeValueList': [self._dynamizer.encode(v) for v in value]\n                }\n            elif key.endswith('__gt'):\n                filters[key] = {\n                    'ComparisonOperator': using.get('gt'),\n                    'AttributeValueList': [self._dynamizer.encode(v)]\n                }\n            elif key.endswith('__gte'):\n                filters[key] = {\n                    'ComparisonOperator': using.get('gte'),\n                    'AttributeValueList': [self._dynamizer.encode(v)]\n                }\n            elif key.endswith('__lt'):\n                filters[key] = {\n                    'ComparisonOperator': using.get('lt'),\n                    'AttributeValueList': [self._dynamizer.encode(v)]\n                }\n            elif key.endswith('__lte'):\n                filters[key] = {\n                    'ComparisonOperator': using.get('lte'),\n                    'AttributeValueList': [self._dynamizer.encode(v)]\n                }\n            elif key.endswith('__eq'):\n                filters[key] = {\n                    'ComparisonOperator': using.get('eq'),\n                    'AttributeValueList': [self._dynamizer.encode(v)]\n                }\n            elif key.endswith('__ne'):\n                filters[key] = {\n                    'ComparisonOperator': using.get('ne'),\n                    'AttributeValueList': [self._dynamizer.encode(v)]\n                }\n            else:\n               ", "idx": 751}
{"namespace": "boto.dynamodb2.table.Table.batch_get", "completion": "        if not keys:\n            raise exceptions.BatchGetItemError(\"No keys specified for batch get\")\n\n        if len(keys) > self.max_batch_get:\n            raise exceptions.BatchGetItemError(\n                \"Number of keys specified for batch get exceeds the maximum allowed batch size of %s\" % self.max_batch_get\n            )\n\n        if not self.schema:\n            raise exceptions.BatchGetItemError(\n                \"Table schema is not populated. Please call Table.describe() to populate the schema.\"\n            )\n\n        if len(self.schema) != len(keys[0]):\n            raise exceptions.BatchGetItemError(\n                \"Number of keys specified for batch get does not match the number of fields in the schema.\"\n            )\n\n        for key in keys:\n            for field in key:\n                if field not in self.schema:\n                    raise exceptions.BatchGetItemError(\n                        \"Field %s specified in the keys is not present in the schema.\" % field\n                    )\n\n        kwargs = {\n            'consistent_read': consistent,\n            'attributes_to_get': attributes,\n        }\n\n        results = ResultSet()\n        results.to_call(self._batch_get, keys, **kwargs)\n        return results\n", "idx": 752}
{"namespace": "boto.dynamodb2.table.Table.count", "completion": "        return self.connection.count(self.table_name)\n", "idx": 753}
{"namespace": "boto.dynamodb2.table.BatchTable.put_item", "completion": "        if overwrite:\n            self._to_put.append(data)\n        else:\n            self._to_put.append(data)\n", "idx": 754}
{"namespace": "boto.dynamodb2.table.BatchTable.delete_item", "completion": "        self._to_delete.append(kwargs)\n\n        if self.should_flush():\n            self.flush()\n", "idx": 755}
{"namespace": "boto.dynamodb2.table.BatchTable.flush", "completion": "        if not self._to_put and not self._to_delete:\n            return False\n\n        # Prepare the data to be inserted or deleted.\n        if self._to_put:\n            prepared_data = []\n            for data in self._to_put:\n                prepared_data.append(self.table._encode_keys(data))\n            self._to_put = []\n            self._unprocessed = self.table._put_item(prepared_data)\n\n        if self._to_delete:\n            prepared_data = []\n            for data in self._to_delete:\n                prepared_data.append(self.table._encode_keys(data))\n            self._to_delete = []\n            self._unprocessed = self.table._delete_item(prepared_data)\n\n        return True\n", "idx": 756}
{"namespace": "boto.dynamodb2.table.BatchTable.resend_unprocessed", "completion": "        while self._unprocessed:\n            # We'll insert data here shortly.\n            batch_data = {\n                self.table.table_name: [\n                    # We'll insert data here shortly.\n                ],\n            }\n\n            for unprocessed in self._unprocessed:\n                batch_data[self.table.table_name].append(unprocessed)\n\n            resp = self.table.connection.batch_write_item(batch_data)\n            self.handle_unprocessed(resp)", "idx": 757}
{"namespace": "boto.dynamodb2.fields.BaseSchemaField.definition", "completion": "        return {\n            'AttributeName': self.name,\n            'AttributeType': self.data_type\n        }", "idx": 758}
{"namespace": "boto.dynamodb2.fields.BaseIndexField.definition", "completion": "        return [\n            {\n                'AttributeName': part.name,\n                'AttributeType': part.data_type,\n            }\n            for part in self.parts\n        ]\n", "idx": 759}
{"namespace": "boto.dynamodb2.fields.BaseIndexField.schema", "completion": "        schema = {\n            'IndexName': self.name,\n            'KeySchema': self.definition(),\n            'Projection': {\n                'ProjectionType': self.projection_type,\n            },\n        }\n\n        return schema\n\n", "idx": 760}
{"namespace": "boto.dynamodb2.fields.GlobalBaseIndexField.schema", "completion": "        schema_data = super(GlobalBaseIndexField, self).schema()\n        schema_data['ProvisionedThroughput'] = {\n            'ReadCapacityUnits': self.throughput['read'],\n            'WriteCapacityUnits': self.throughput['write'],\n        }\n        return schema_data\n\n", "idx": 761}
{"namespace": "boto.dynamodb2.fields.GlobalIncludeIndex.schema", "completion": "        schema_data = super(GlobalIncludeIndex, self).schema()\n        schema_data['ProvisionedThroughput'] = {\n            'ReadCapacityUnits': int(self.throughput['read']),\n            'WriteCapacityUnits': int(self.throughput['write']),\n        }\n        return schema_data", "idx": 762}
{"namespace": "boto.dynamodb2.items.Item.get_keys", "completion": "        keys = {}\n        for key in self.table.key_schema:\n            keys[key] = self[key]\n        return keys\n", "idx": 763}
{"namespace": "boto.dynamodb2.items.Item.get_raw_keys", "completion": "        key_fields = self.table.get_key_fields()\n        key_data = {}\n\n        for key in key_fields:\n            key_data[key] = self._dynamizer.encode(self[key])\n\n        return key_data\n", "idx": 764}
{"namespace": "boto.dynamodb2.items.Item.build_expects", "completion": "        expects = {}\n\n        if fields is None:\n            fields = self.keys()\n\n        for field in fields:\n            if field in self._data:\n                if self._data[field] is None:\n                    expects[field] = {\n                        'Exists': False,\n                    }\n                else:\n                    expects[field] = {\n                        'Exists': True,\n                        'Value': self._dynamizer.encode(self._data[field]),\n                    }\n            else:\n                expects[field] = {\n                    'Exists': False,\n                }\n\n        return expects\n", "idx": 765}
{"namespace": "boto.dynamodb2.items.Item.prepare_full", "completion": "        encoded_data = {}\n        for key, value in self._data.items():\n            encoded_data[key] = self._dynamizer.encode(value)\n        return encoded_data\n", "idx": 766}
{"namespace": "boto.dynamodb2.items.Item.prepare_partial", "completion": "        # This doesn't save on its own. Rather, we prepare the datastructure\n        # and hand-off to the table to handle creation/update.\n        final_data = {}\n        altered_fields = set()\n\n        for key, value in self._data.items():\n            if not self._is_storable(value):\n                continue\n\n            if key in self._orig_data:\n                if self._data[key] != self._orig_data[key]:\n                    final_data[key] = self._dynamizer.encode(value)\n                    altered_fields.add(key)\n            else:\n                final_data[key] = self._dynamizer.encode(value)\n                altered_fields.add(key)\n\n        return final_data, altered_fields\n", "idx": 767}
{"namespace": "boto.dynamodb2.items.Item.partial_save", "completion": "        # This doesn't save on its own. Rather, we prepare the datastructure\n        # and hand-off to the table to handle creation/update.\n        final_data = {}\n        fields = set()\n        alterations = self._determine_alterations()\n\n        for key, value in alterations['adds'].items():\n            final_data[key] = {\n                'Action': 'PUT',\n                'Value': self._dynamizer.encode(self._data[key])\n            }\n            fields.add(key)\n\n        for key, value in alterations['changes'].items():\n            final_data[key] = {\n                'Action': 'PUT',\n                'Value': self._dynamizer.encode(self._data[key])\n            }\n            fields.add(key)\n\n        for key in alterations['deletes']:\n            final_data[key] = {\n                'Action': 'DELETE',\n            }\n            fields.add(key)\n\n        return self.table.partial_save(self, final_data, fields)\n", "idx": 768}
{"namespace": "boto.dynamodb2.items.Item.save", "completion": "        key = self.get_keys()\n        final_data = self.prepare_full()\n\n        if not final_data:\n            return False\n\n        # Build expectations of all fields.\n        expects = self.build_expects()\n        returned = self.table._put_item(key, final_data, expects=expects, overwrite=overwrite)\n        # Mark the object as clean.\n        self.mark_clean()\n        return returned\n", "idx": 769}
{"namespace": "boto.dynamodb2.items.Item.delete", "completion": "        key = self.get_keys()\n        returned = self.table._delete_item(key)\n        return returned\n", "idx": 770}
{"namespace": "boto.dynamodb2.connect_to_region", "completion": "    from boto.dynamodb2.layer1 import DynamoDBConnection\n    return DynamoDBConnection(region=RegionInfo(region_name), **kw_params)\n\n\n", "idx": 771}
{"namespace": "litecli.packages.parseutils.extract_tables", "completion": "    parsed = sqlparse.parse(sql)[0]\n    return list(extract_table_identifiers(extract_from_part(parsed)))", "idx": 772}
{"namespace": "litecli.packages.parseutils.queries_start_with", "completion": "    return any(query_starts_with(query, prefixes) for query in queries)", "idx": 773}
{"namespace": "litecli.packages.parseutils.is_destructive", "completion": "    destructive_queries = [\n        \"drop\",\n        \"delete\",\n        \"truncate\",\n        \"alter\",\n        \"create\",\n        \"insert\",\n        \"update\",\n        \"rename\",\n        \"grant\",\n        \"revoke\",\n        \"revoke all\",\n        \"revoke grant\",\n        \"revoke all grant\",\n        \"revoke grant all\",\n        \"revoke grant all privileges\",\n        \"revoke all privileges\",\n        \"revoke privileges\",\n        \"revoke all privileges on\",\n        \"revoke privileges on\",\n        \"revoke all privileges on all tables\",\n        \"revoke privileges on all tables\",\n        \"revoke all privileges on all sequences\",\n        \"revoke privileges on all sequences\",\n        \"revoke all privileges on all functions\",\n        \"revoke privileges on all functions\",\n        \"revoke all privileges on all procedures\",\n        \"revoke privileges on all procedures\",\n        \"revoke all privileges on all views\",\n        \"revoke privileges on all views\",\n        \"revoke all privileges on all types\",\n        \"revoke privileges on all types\",\n        \"revoke all privileges on all domains\",\n        \"revoke privileges on all domains\",\n        \"revoke all privileges on all operators\",\n        \"revoke privileges on all operators\",\n        \"revoke all privileges on all aggregates\",\n        \"revoke privileges on all aggregates\",\n        \"revoke all privileges on all languages\",\n        \"revoke privileges on all languages\",\n        \"revoke all privileges on all casts\",\n        \"revoke privileges on all casts\",\n        \"revoke all privileges on all conversions\",\n        \"revoke privileges on all conversions\",\n        \"revoke all privileges on all collations\",\n        \"revoke privileges on all collations\",\n        \"revoke all privileges on all conversions\",\n        \"revoke privileges on all conversions\",\n        \"revoke all privileges on all domains\",\n        \"revoke privileges on all domains\",\n        \"revoke all privileges on all event triggers\",\n        \"revoke privileges on all event triggers\",\n        \"revoke all privileges on all functions\",\n        \"revoke privileges on all functions\",\n        \"revoke all privileges on all languages\",\n        \"revoke privileges on all languages\",\n        \"revoke all privileges on all operators\",\n        \"revoke privileges on all operators\",\n        \"revoke all privileges on all procedures\",\n        \"revoke privileges on all procedures\",\n        \"revoke all privileges on all sequences\",\n        \"revoke privileges on all sequences\",\n        \"revoke all privileges on all tablespaces\",\n        \"revoke privileges on all tablespaces\",\n        \"revoke all privileges on all tablespaces\",\n        \"revoke privileges on all", "idx": 774}
{"namespace": "litecli.packages.completion_engine.suggest_type", "completion": "    # Get the last word typed so far.\n    last_word_typed = last_word(full_text)\n\n    # Get the tables that have been typed so far.\n    tables_typed = extract_tables(full_text)\n\n    # Get the previous keyword typed so far.\n    prev_keyword_typed = find_prev_keyword(full_text)\n\n    # Get the special command typed so far.\n    special_command_typed = parse_special_command(full_text)\n\n    # Get the type of entity that has been typed so far.\n    type_of_entity = suggest_type_of_entity(last_word_typed, tables_typed, prev_keyword_typed, special_command_typed)\n\n    # Get the scope of the entity that has been typed so far.\n    scope_of_entity = suggest_scope_of_entity(last_word_typed, tables_typed, prev_keyword_typed, special_command_typed)\n\n    # Get the type of entity that has been typed so far.\n    type_of_entity_before_cursor = suggest_type_of_entity(text_before_cursor)\n\n    # Get the scope of the entity that has been typed so far.\n    scope_of_entity_before_cursor = suggest_scope_of_entity(text_before_cursor)\n\n    # Get the type of entity that has been typed so far.\n    type_of_entity_before_cursor_before_cursor = suggest_type_of_entity(text_before_cursor, text_before_cursor)\n\n    # Get the scope of the entity that has been typed so far.\n    scope_of_entity_before_cursor_before_cursor = suggest_scope_of_entity(text_before_cursor, text_before_cursor)\n\n    # Get the type of entity that has been typed so far.\n    type_of_entity_before_cursor_before_cursor_before_cursor = suggest_type_of_entity(text_before_cursor, text_before_cursor, text_before_cursor)\n\n    # Get the scope of the entity that has been typed so far.\n    scope_of_entity_before_cursor_before_cursor_before_cursor = suggest_scope_of_entity(text_before_cursor, text_before_cursor, text_", "idx": 775}
{"namespace": "datasette.plugins.get_plugins", "completion": "    plugins = []\n    for plugin in pm.list_name_plugins():\n        plugin_info = {}\n        plugin_info[\"name\"] = plugin\n        plugin_info[\"static_path\"] = pm.get_plugin(plugin).static_path\n        plugin_info[\"templates_path\"] = pm.get_plugin(plugin).templates_path\n        plugin_info[\"hooks\"] = pm.get_plugin(plugin).hooks\n        plugin_info[\"version\"] = pm.get_plugin(plugin).version\n        plugin_info[\"project_name\"] = pm.get_plugin(plugin).project_name\n        plugins.append(plugin_info)\n    return plugins", "idx": 776}
{"namespace": "datasette.facets.ColumnFacet.suggest", "completion": "        # Get the row count and columns from the dataset\n        row_count = await self.get_row_count()\n        columns = await self.get_columns(self.sql, self.params)\n\n        # Determine the facet size\n        facet_size = self.get_facet_size()\n\n        # Iterate through each column\n        suggested_facets = []\n        for column in columns:\n            # Construct a SQL query to retrieve distinct values and their counts\n            sql = f\"select {column}, count(*) from ({self.sql}) group by {column}\"\n\n            # Execute the query and get the results\n            results = await self.ds.execute(self.database, sql, self.params)\n\n            # Check if the number of distinct values is between 1 and the row count\n            if 1 < len(results.rows) < row_count:\n                # Check if the number of distinct values is less than or equal to the facet size\n                if len(results.rows) <= facet_size:\n                    # Check if at least one distinct value has a count greater than 1\n                    if any(count > 1 for _, count in results.rows):\n                        # Add the column as a suggested facet\n                        suggested_facets.append(\n                            {\n                                \"name\": column,\n                                \"toggle_url\": self.ds.url_for(\n                                    \"facet\",\n                                    table=self.table,\n                                    column=column,\n                                    toggle=True,\n                                ),\n                            }\n                        )\n\n        return suggested_facets\n", "idx": 777}
{"namespace": "datasette.facets.ColumnFacet.facet_results", "completion": "        facet_size = self.get_facet_size()\n        facet_results = []\n        facets_timed_out = []\n        # Get the facet values and their corresponding counts\n        try:\n            facet_values = await self.ds.execute(\n                self.database,\n                f\"select value, count(*) as n from ({self.sql}) group by value limit {facet_size + 1}\",\n                self.params,\n                truncate=False,\n                custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n            )\n        except QueryInterrupted:\n            facets_timed_out.append(self.type)\n            return facet_results, facets_timed_out\n        # Get the column label if available\n        column_label = None\n        if self.table:\n            table_metadata = self.ds.metadata(\"tables\", database=self.database) or {}\n            table_metadata = table_metadata.get(self.table) or {}\n            column_label = table_metadata.get(\"columns\", {}).get(self.type)\n        # Format the facet results\n        for row in facet_values:\n            value = row[\"value\"]\n            count = row[\"n\"]\n            # Check if the value is a JSON object\n            if isinstance(value, str) and value.startswith(\"{\"):\n                value = json.loads(value)\n            # Check if the value is a list\n            if isinstance(value, list):\n                value = \", \".join(str(item) for item in value)\n            # Check if the value is a dictionary\n            if isinstance(value, dict):\n                value = \", \".join(f\"{key}: {value[key]}\" for key in value)\n            # Check if the value is a datetime object\n            if isinstance(value, datetime.datetime):\n                value = value.strftime(\"%Y-%m-%d %H:%M:%S\")\n            # Check if the value is a date object\n            if isinstance(value, datetime.date):\n                value = value.strftime(\"%Y-%m-%d\")\n            # Check if the value is a time object\n            if isinstance(value, datetime.time):\n                value = value.strftime(\"%H:%M:%S\")\n            # Check if the value is a timedelta object\n            if isinstance(value, datetime.timedelta):\n                value = str(value)\n            # Check if the value is", "idx": 778}
{"namespace": "datasette.facets.ArrayFacet.suggest", "completion": "        row_count = await self.get_row_count()\n        columns = await self.get_columns(self.sql, self.params)\n        facet_size = self.get_facet_size()\n        suggested_facets = []\n        already_enabled = [c[\"config\"][\"simple\"] for c in self.get_configs()]\n        for column in columns:\n            if column in already_enabled:\n                continue\n            suggested_facet_sql = \"\"\"\n                select {column} as value, count(*) as n from (\n                    {sql}\n                ) where value is not null\n                group by value\n                limit {limit}\n            \"\"\".format(\n                column=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            distinct_values = None\n            try:\n                distinct_values = await self.ds.execute(\n                    self.database,\n                    suggested_facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_suggest_time_limit_ms\"),\n                )\n                num_distinct_values = len(distinct_values)\n                if (\n                    1 < num_distinct_values < row_count\n                    and num_distinct_values <= facet_size\n                    # And at least one has n > 1\n                    and any(r[\"n\"] > 1 for r in distinct_values)\n                ):\n                    suggested_facets.append(\n                        {\n                            \"name\": column,\n                            \"toggle_url\": self.ds.absolute_url(\n                                self.request,\n                                self.ds.urls.path(\n                                    path_with_added_args(\n                                        self.request, {\"_facet\": column}\n                                    )\n                                ),\n                            ),\n                        }\n                    )\n            except QueryInterrupted:\n                continue\n        return suggested_facets\n", "idx": 779}
{"namespace": "datasette.facets.ArrayFacet.facet_results", "completion": "        facet_results = []\n        facets_timed_out = []\n\n        qs_pairs = self.get_querystring_pairs()\n\n        facet_size = self.get_facet_size()\n        for source_and_config in self.get_configs():\n            config = source_and_config[\"config\"]\n            source = source_and_config[\"source\"]\n            column = config.get(\"column\") or config[\"simple\"]\n            facet_sql = \"\"\"\n                select {col} as value, count(*) as count from (\n                    {sql}\n                )\n                where {col} is not null\n                group by {col} order by count desc, value limit {limit}\n            \"\"\".format(\n                col=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            try:\n                facet_rows_results = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n                facet_results_values = []\n                facet_results.append(\n                    {\n                        \"name\": column,\n                        \"type\": self.type,\n                        \"hideable\": source != \"metadata\",\n                        \"toggle_url\": self.ds.urls.path(\n                            path_with_removed_args(self.request, {\"_facet_array\": column})\n                        ),\n                        \"results\": facet_results_values,\n                        \"truncated\": len(facet_rows_results) > facet_size,\n                    }\n                )\n                facet_rows = facet_rows_results.rows[:facet_size]\n                if self.table:\n                    # Attempt to expand foreign keys into labels\n                    values = [row[\"value\"] for row in facet_rows]\n                    expanded = await self.ds.expand_foreign_keys(\n                        self.database, self.table, column, values\n                    )\n                else:\n                    expanded = {}\n                for row in facet_rows:\n                    column_qs = column\n                    if column.startswith(\"_\"):\n                        column_qs = \"{}__exact\".format(column)\n                    selected = (column_qs, str(row[\"value\"])) in qs_pairs\n                    if selected:\n                        toggle_path = path_with_removed_args(\n                            self.request, {column", "idx": 780}
{"namespace": "datasette.facets.DateFacet.facet_results", "completion": "        facet_results = []\n        facets_timed_out = []\n\n        facet_size = self.get_facet_size()\n        for source_and_config in self.get_configs():\n            config = source_and_config[\"config\"]\n            source = source_and_config[\"source\"]\n            column = config.get(\"column\") or config[\"simple\"]\n            facet_sql = \"\"\"\n                select\n                    date({column}) as value,\n                    count(*) as count\n                from (\n                    {sql}\n                )\n                where {column} is not null\n                group by value\n                order by count desc, value limit {limit}\n            \"\"\".format(\n                column=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            try:\n                facet_rows_results = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n                facet_results_values = []\n                facet_results.append(\n                    {\n                        \"name\": column,\n                        \"type\": self.type,\n                        \"hideable\": source != \"metadata\",\n                        \"toggle_url\": self.ds.urls.path(\n                            path_with_removed_args(\n                                self.request, {\"_facet_date\": column}\n                            )\n                        ),\n                        \"results\": facet_results_values,\n                        \"truncated\": len(facet_rows_results) > facet_size,\n                    }\n                )\n                facet_rows = facet_rows_results.rows[:facet_size]\n                pairs = self.get_querystring_pairs()\n                for row in facet_rows:\n                    value = str(row[\"value\"])\n                    selected = (f\"{column}__date\", value) in pairs\n                    if selected:\n                        toggle_path = path_with_removed_args(\n                            self.request, {f\"{column}__date\": value}\n                        )\n                    else:\n                        toggle_path = path_with_added_args(\n                            self.request, {f\"{column}__date\": value}\n                        )\n                    facet_results_values.append(\n                        {\n                            \"value\": value,\n                            \"label\": value,\n                            \"count\": row[\"count\"],\n                            \"toggle_url\": self", "idx": 781}
{"namespace": "datasette.app.Datasette.invoke_startup", "completion": "        if self._startup_invoked:\n            return\n        self._startup_invoked = True\n        # Ensure that the internal database is created\n        await self.refresh_schemas()\n        # Execute plugins\n        for plugin in get_plugins():\n            if plugin[\"startup\"]:\n                await plugin[\"startup\"](self)\n", "idx": 782}
{"namespace": "datasette.app.Datasette.get_database", "completion": "        if route:\n            for db in self.databases.values():\n                if db.route == route:\n                    return db\n        if name:\n            return self.databases[name]\n        for db in self.databases.values():\n            if db.name != \"_internal\":\n                return db\n", "idx": 783}
{"namespace": "datasette.app.Datasette.add_database", "completion": "        if name is None:\n            name = db.name\n        if name in self.databases:\n            name = \"{}_{}\".format(name, len(self.databases) + 1)\n        if route is None:\n            route = name\n        self.databases[name] = db\n        db.name = name\n        db.route = route\n        return db\n", "idx": 784}
{"namespace": "datasette.app.Datasette.ensure_permissions", "completion": "        for permission in permissions:\n            if isinstance(permission, str):\n                action = permission\n                resource = None\n            else:\n                action, resource = permission\n            if not await self.permission_allowed(actor, action, resource):\n                raise Forbidden(\n                    \"Permission denied for actor {} to {} {}\".format(\n                        actor, action, resource\n                    )\n                )\n", "idx": 785}
{"namespace": "datasette.app.Datasette.check_visibility", "completion": "        assert actor is None or isinstance(\n            actor, dict\n        ), \"actor must be None or a dict\"\n        if permissions is None:\n            permissions = []\n        if resource is None:\n            resource = None\n        if action is None:\n            action = None\n        if resource is not None:\n            if isinstance(resource, tuple):\n                database, table = resource\n            else:\n                database = None\n                table = resource\n        else:\n            database = None\n            table = None\n        if action is not None:\n            permissions.append((action, resource))\n        if database is not None:\n            permissions.append((\"view-database\", database))\n        if table is not None:\n            permissions.append((\"view-table\", (database, table)))\n        if actor is not None:\n            permissions.append((\"view-actor\", actor))\n        visible = await self.permission_allowed(\n            actor,\n            \"view-instance\",\n            default=True,\n        )\n        private = False\n        for permission in permissions:\n            if isinstance(permission, str):\n                action = permission\n                resource = None\n            elif isinstance(permission, (tuple, list)) and len(permission) == 2:\n                action, resource = permission\n            else:\n                assert (\n                    False\n                ), \"permission should be string or tuple of two items: {}\".format(\n                    repr(permission)\n                )\n            ok = await self.permission_allowed(\n                actor,\n                action,\n                resource=resource,\n                default=None,\n            )\n            if ok is not None:\n                if ok:\n                    visible = True\n                else:\n                    private = True\n        return visible, private\n", "idx": 786}
{"namespace": "datasette.app.Datasette.render_template", "completion": "        if not self.startup_invoked:\n            raise StartupError(\"Datasette startup has not been invoked\")\n        if context is None:\n            context = {}\n        if isinstance(templates, Template):\n            template = templates\n        else:\n            template = self.jinja_env.get_template(templates)\n        # Add some variables to the context\n        context.update(\n            {\n                \"datasette\": self,\n                \"urls\": self.urls,\n                \"version\": __version__,\n                \"version_note\": self.version_note,\n                \"setting\": self.setting,\n                \"settings_dict\": self.settings_dict(),\n                \"static_url\": self.static_url,\n                \"static_mounts\": self.static_mounts,\n                \"plugin_config\": self.plugin_config,\n                \"plugin_urls\": self.plugin_urls,\n                \"plugin_templates\": self.plugin_templates,\n                \"plugin_static\": self.plugin_static,\n                \"plugin_hooks\": self.plugin_hooks,\n                \"plugin_hooks_dict\": self.plugin_hooks_dict,\n                \"plugin_hooks_list\": self.plugin_hooks_list,\n                \"plugin_hooks_list_dict\": self.plugin_hooks_list_dict,\n                \"plugin_hooks_list_dict_flat\": self.plugin_hooks_list_dict_flat,\n                \"plugin_hooks_list_dict_flat_dict\": self.plugin_hooks_list_dict_flat_dict,\n                \"plugin_hooks_list_dict_flat_dict_flat\": self.plugin_hooks_list_dict_flat_dict_flat,\n                \"plugin_hooks_list_dict_flat_dict_flat_dict\": self.plugin_hooks_list_dict_flat_dict_flat_dict,\n                \"plugin_hooks_list_dict_flat_dict_flat_dict_flat\": self.plugin_hooks_list_dict_flat_dict_flat_dict_flat,\n                \"plugin_hooks_list_dict_flat_dict_flat_dict_flat_dict\": self.plugin_hooks_list_dict_flat_dict_flat_dict_flat_dict,\n                \"plugin_hooks_list_dict_flat_dict_flat_dict_flat_dict_flat\": self.", "idx": 787}
{"namespace": "datasette.app.DatasetteClient.get", "completion": "        path = self._fix(path)\n        async with httpx.AsyncClient() as client:\n            response = await client.get(path, **kwargs)\n        return response\n", "idx": 788}
{"namespace": "datasette.utils.asgi.Request.full_path", "completion": "        if self.query_string:\n            return \"{}?{}\".format(self.path, self.query_string)\n        else:\n            return self.path\n", "idx": 789}
{"namespace": "datasette.utils.asgi.Request.post_body", "completion": "        body = b''\n        while True:\n            message = await self.receive()\n            if message[\"type\"] == \"http.request\":\n                body += message.get(\"body\", b'')\n            elif message[\"type\"] == \"http.request.body\":\n                continue\n            elif message[\"type\"] == \"http.disconnect\":\n                break\n        return body\n", "idx": 790}
{"namespace": "datasette.utils.asgi.Request.fake", "completion": "        if url_vars is None:\n            url_vars = {}\n        path, query_string = path_with_query_string.split(\"?\", 1)\n        return cls(\n            {\n                \"type\": \"http\",\n                \"http_version\": \"1.1\",\n                \"method\": method,\n                \"scheme\": scheme,\n                \"path\": path,\n                \"query_string\": query_string.encode(\"utf-8\"),\n                \"raw_path\": path.encode(\"utf-8\"),\n                \"headers\": [],\n                \"client\": (\"127.0.0.1\", 80),\n                \"server\": (\"127.0.0.1\", 80),\n                \"root_path\": \"\",\n                \"path_info\": path,\n                \"actor\": None,\n                \"url_route\": {\"name\": \"test\", \"kwargs\": url_vars},\n            },\n            lambda: None,\n        )\n\n", "idx": 791}
{"namespace": "datasette.utils.asgi.Response.asgi_send", "completion": "        # Remove any existing content-type header\n        headers = {k: v for k, v in self.headers.items() if k.lower() != \"content-type\"}\n        headers[\"content-type\"] = self.content_type\n        await send(\n            {\n                \"type\": \"http.response.start\",\n                \"status\": self.status,\n                \"headers\": [\n                    [key.encode(\"utf-8\"), value.encode(\"utf-8\")]\n                    for key, value in headers.items()\n                ],\n            }\n        )\n        w = AsgiWriter(send)\n        await w.write(self.body)\n        for header in self._set_cookie_headers:\n            await send(\n                {\n                    \"type\": \"http.response.headers\",\n                    \"headers\": [[b\"set-cookie\", header.encode(\"utf-8\")]],\n                }\n            )\n", "idx": 792}
{"namespace": "datasette.utils.asgi.Response.set_cookie", "completion": "        if samesite not in SAMESITE_VALUES:\n            raise ValueError(\n                \"samesite must be one of {}\".format(SAMESITE_VALUES)\n            )\n\n        cookie = Morsel()\n        cookie.set(key, value)\n        if max_age:\n            cookie[\"max-age\"] = max_age\n        if expires:\n            cookie[\"expires\"] = expires\n        if path:\n            cookie[\"path\"] = path\n        if domain:\n            cookie[\"domain\"] = domain\n        if secure:\n            cookie[\"secure\"] = True\n        if httponly:\n            cookie[\"httponly\"] = True\n        if samesite:\n            cookie[\"samesite\"] = samesite\n        self._set_cookie_headers.append(cookie.OutputString())\n\n", "idx": 793}
{"namespace": "datasette.utils.asgi.Response.html", "completion": "        return cls(body, status, headers, content_type=\"text/html; charset=utf-8\")\n", "idx": 794}
{"namespace": "datasette.utils.asgi.Response.text", "completion": "        return cls(\n            body,\n            status=status,\n            headers=headers,\n            content_type=\"text/plain; charset=utf-8\",\n        )\n", "idx": 795}
{"namespace": "datasette.utils.asgi.Response.json", "completion": "        return cls(\n            json.dumps(body, default=default),\n            status=status,\n            headers=headers,\n            content_type=\"application/json; charset=utf-8\",\n        )\n", "idx": 796}
{"namespace": "datasette.utils.asgi.Response.redirect", "completion": "        return cls(\n            \"\",\n            status=status,\n            headers=headers or {},\n            content_type=\"text/html; charset=utf-8\",\n        )\n", "idx": 797}
{"namespace": "datasette.utils.validate_sql_select", "completion": "    # Remove comments from the SQL statement\n    sql = re.sub(comment_re, \"\", sql)\n\n    # Convert the SQL statement to lowercase\n    sql = sql.lower()\n\n    # Check if the SQL statement matches any of the allowed patterns\n    for allowed_pattern in allowed_sql_res:\n        if allowed_pattern.match(sql):\n            return\n\n    # Check if the SQL statement matches any of the disallowed patterns\n    for disallowed_pattern, error_message in disallawed_sql_res:\n        if disallowed_pattern.match(sql):\n            raise InvalidSql(error_message)\n\n    # If the SQL statement does not match any of the allowed or disallowed patterns, raise an invalid sql exception\n    raise InvalidSql(\"Statement contained a disallowed SQL keyword.\")\n\n", "idx": 798}
{"namespace": "datasette.utils.escape_css_string", "completion": "    return _css_re.sub(lambda m: f\"\\\\{ord(m.group(0)):x}\", s)\n\n", "idx": 799}
{"namespace": "datasette.utils.detect_fts", "completion": "    # Check if the table has a corresponding FTS virtual table\n    fts_table = conn.execute(\n        f\"SELECT name FROM sqlite_master WHERE type='table' AND name LIKE '{table}_fts'\"\n    ).fetchone()\n    if fts_table:\n        return fts_table[0]\n    else:\n        return None\n\n", "idx": 800}
{"namespace": "datasette.utils.is_url", "completion": "    if not isinstance(value, str):\n        return False\n    if not value.startswith((\"http://\", \"https://\")):\n        return False\n    if whitespace_re.search(value):\n        return False\n    return True\n\n", "idx": 801}
{"namespace": "datasette.utils.to_css_class", "completion": "    if css_class_re.match(s):\n        return s\n    else:\n        s = css_invalid_chars_re.sub(\"\", s)\n        return f\"{s[:20]}{hashlib.md5(s.encode('utf-8')).hexdigest()[:6]}\"\n\n", "idx": 802}
{"namespace": "datasette.utils.escape_fts", "completion": "    terms = _escape_fts_re.split(query)\n    terms = [term for term in terms if term]\n    terms = list(set(terms))\n    return '\"{}\"'.format('\" \"'.join(terms))\n\n", "idx": 803}
{"namespace": "datasette.utils.check_connection", "completion": "    try:\n        conn.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n    except sqlite3.OperationalError as e:\n        raise ConnectionProblem(\n            \"The connection to the SQLite database failed. Please check the connection string and try again.\"\n        ) from e\n\n    for table in conn.execute(\"SELECT name FROM sqlite_master WHERE type='table'\"):\n        try:\n            conn.execute(f\"PRAGMA table_info({escape_sqlite(table[0])})\")\n        except sqlite3.OperationalError as e:\n            raise ConnectionProblem(\n                f\"The connection to the SQLite database failed. Please check the connection string and try again.\"\n            ) from e\n\n", "idx": 804}
{"namespace": "datasette.utils.parse_metadata", "completion": "    try:\n        return json.loads(content)\n    except json.decoder.JSONDecodeError:\n        try:\n            return yaml.safe_load(content)\n        except yaml.YAMLError:\n            raise BadMetadataError(\n                \"The metadata is not in JSON or YAML format. Please check the format and try again.\"\n            )\n\n", "idx": 805}
{"namespace": "datasette.utils.call_with_supported_arguments", "completion": "    call_with = _gather_arguments(fn, kwargs)\n    return fn(*call_with)\n\n", "idx": 806}
{"namespace": "datasette.utils.derive_named_parameters", "completion": "    # Strip any trailing semicolon from the input SQL query\n    sql = sql.rstrip(\";\")\n    # Find all possible named parameters in the input SQL query\n    named_parameters = _re_named_parameter.findall(sql)\n    # Execute the \"explain\" statement on the database with a dictionary of named parameters, where the values are set to None\n    try:\n        results = await db.execute(f\"explain {sql}\", named_parameters)\n    except sqlite3.OperationalError as e:\n        # If there is an error executing the \"explain\" statement, return the list of possible named parameters found in the input SQL query\n        return named_parameters\n    # Return a list of named parameters that are identified as variables in the \"explain\" results, after removing the leading \":\" character\n    return [parameter[1:] for parameter in named_parameters if parameter[1:] in results[0]]\n\n", "idx": 807}
{"namespace": "pyramid.path.Resolver.get_package_name", "completion": "        if self.package is CALLER_PACKAGE:\n            return package_name(caller_module())\n        else:\n            return package_name(self.package)\n", "idx": 808}
{"namespace": "pyramid.path.Resolver.get_package", "completion": "        return self.package\n", "idx": 809}
{"namespace": "pyramid.path.DottedNameResolver.resolve", "completion": "        if ':' in dotted:\n            return pkg_resources.working_set.resolve(dotted)\n        else:\n            if self.package is CALLER_PACKAGE:\n                package_name = caller_package().__name__\n            else:\n                package_name = getattr(self.package, '__name__', None)\n            if package_name is None:\n                raise ValueError(\n                    'relative dotted name %r irresolveable without package' % (dotted,)\n                )\n            return pkg_resources.working_set.resolve(package_name + '.' + dotted)\n\n", "idx": 810}
{"namespace": "pyramid.path.DottedNameResolver.maybe_resolve", "completion": "        if isinstance(dotted, str):\n            return self.resolve(dotted)\n        else:\n            return dotted\n\n", "idx": 811}
{"namespace": "pyramid.path.PkgResourcesAssetDescriptor.abspath", "completion": "        return self.pkg_resources.resource_filename(self.pkg_name, self.path)\n", "idx": 812}
{"namespace": "pyramid.renderers.render_to_response", "completion": "    try:\n        registry = request.registry\n    except AttributeError:\n        registry = None\n    if package is None:\n        package = caller_package()\n    helper = RendererHelper(\n        name=renderer_name, package=package, registry=registry\n    )\n\n    with hide_attrs(request, 'response'):\n        result = helper.render(value, response, request=request)\n\n    return result\n\n", "idx": 813}
{"namespace": "pyramid.renderers.JSON.add_adapter", "completion": "        self.components.register(adapter, type_or_iface)\n", "idx": 814}
{"namespace": "pyramid.renderers.RendererHelper.settings", "completion": "        try:\n            settings = self.registry.settings\n        except AttributeError:\n            settings = {}\n        return settings\n", "idx": 815}
{"namespace": "pyramid.renderers.RendererHelper.render_view", "completion": "        # Create the system dictionary\n        system = {\n            'view': view,\n            'renderer_name': self.name,\n            'renderer_info': self,\n            'context': context,\n            'request': request,\n            'csrf_token': get_csrf_token(request),\n        }\n\n        # Add renderer globals to the system dictionary\n        renderer_globals = self.settings.get('renderer_globals', {})\n        system.update(renderer_globals)\n\n        # Render the view\n        result = view(context, request)\n\n        # Render the result\n        return self.render(result, system, request=request, response=response)\n", "idx": 816}
{"namespace": "pyramid.renderers.RendererHelper.render", "completion": "        if system_values is None:\n            system_values = {}\n        if request is not None:\n            system_values['request'] = request\n        system_values['renderer_name'] = self.name\n        system_values['renderer_info'] = self\n        system_values['view'] = None\n        system_values['context'] = None\n        system_values['req'] = request\n        system_values['get_csrf_token'] = partial(get_csrf_token, request)\n        self.registry.notify(SystemValuesProduced(system_values))\n        return self.renderer(value, system_values)\n", "idx": 817}
{"namespace": "pyramid.renderers.RendererHelper.render_to_response", "completion": "        from pyramid.events import BeforeRender\n        renderer = self.renderer\n        if system_values is None:\n            system_values = {\n                'view': None,\n                'renderer_name': self.name,  # b/c\n                'renderer_info': self,\n                'context': getattr(request, 'context', None),\n                'request': request,\n                'req': request,\n                'get_csrf_token': partial(get_csrf_token, request),\n            }\n\n        system_values = BeforeRender(system_values, value)\n\n        registry = self.registry\n        registry.notify(system_values)\n        result = renderer(value, system_values)\n        response = request.response\n        response.body = result\n        return response\n\n", "idx": 818}
{"namespace": "pyramid.renderers.RendererHelper.clone", "completion": "        if name is None:\n            name = self.name\n        if package is None:\n            package = self.package\n        if registry is None:\n            registry = self.registry\n\n        return RendererHelper(name, package, registry)", "idx": 819}
{"namespace": "pyramid.urldispatch.RoutesMapper.get_routes", "completion": "        if include_static:\n            return self.routelist + self.static_routes\n        else:\n            return self.routelist\n", "idx": 820}
{"namespace": "pyramid.urldispatch.RoutesMapper.connect", "completion": "        route = Route(\n            name, pattern, factory, predicates, pregenerator\n        )  # create a new route\n        self.routes[name] = route  # add the route to the routes dictionary\n\n        if static is True:\n            self.static_routes.append(route)\n        else:\n            self.routelist.append(route)\n\n        return route\n\n", "idx": 821}
{"namespace": "pyramid.testing.DummyTemplateRenderer.assert_", "completion": "        for k, v in kw.items():\n            if k not in self._received:\n                raise AssertionError(\n                    \"Expected key %s not found in received data\" % k\n                )\n            if self._received[k] != v:\n                raise AssertionError(\n                    \"Expected value %s for key %s, but got %s\"\n                    % (v, k, self._received[k])\n                )\n        return True\n\n", "idx": 822}
{"namespace": "pyramid.testing.DummyResource.__getitem__", "completion": "        return self.subs[name]\n\n", "idx": 823}
{"namespace": "pyramid.testing.DummyResource.clone", "completion": "        if __name__ is _marker:\n            __name__ = self.__name__\n        if __parent__ is _marker:\n            __parent__ = self.__parent__\n        return DummyResource(\n            __name__, __parent__, self.__provides__, **self.kw, **kw\n        )\n\n", "idx": 824}
{"namespace": "pyramid.testing.DummySession.get_csrf_token", "completion": "        if '_csrft_' in self:\n            return self['_csrft_']\n        else:\n            return self.new_csrf_token()\n\n", "idx": 825}
{"namespace": "pyramid.testing.DummyRequest.response", "completion": "        return self.registry.response_factory(self)\n", "idx": 826}
{"namespace": "pyramid.testing.DummyRendererFactory.add", "completion": "        self.renderers[spec] = renderer\n", "idx": 827}
{"namespace": "pyramid.authorization.ACLAuthorizationPolicy.principals_allowed_by_permission", "completion": "        return self.helper.principals_allowed_by_permission(context, permission)\n", "idx": 828}
{"namespace": "pyramid.url.URLMethodsMixin.route_url", "completion": "        request = self\n        registry = get_current_registry()\n        route = registry.getRoute(route_name)\n        if route is None:\n            raise KeyError(route_name)\n        return route.generate(request, *elements, **kw)\n", "idx": 829}
{"namespace": "pyramid.predicates.CustomPredicate.text", "completion": "        if hasattr(self.func, '__text__'):\n            return self.func.__text__()\n        else:\n            return 'custom predicate %s' % self.func.__name__\n", "idx": 830}
{"namespace": "pyramid.threadlocal.ThreadLocalManager.pop", "completion": "        if self.stack:\n            return self.stack.pop()\n        else:\n            return self.default\n", "idx": 831}
{"namespace": "pyramid.threadlocal.ThreadLocalManager.get", "completion": "        if self.stack:\n            return self.stack[-1]\n        else:\n            return self.default()\n", "idx": 832}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.authenticated_userid", "completion": "        identity = self._get_identity(request)\n        if identity is None:\n            return None\n        userid = identity.get('repoze.who.userid')\n        if userid is None:\n            return None\n        if self._clean_principal(userid) is None:\n            return None\n        if self.callback is None:\n            return userid\n        callback_ok = self.callback(userid, request)\n        if callback_ok is not None:  # is not None!\n            return userid\n        return None\n", "idx": 833}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.unauthenticated_userid", "completion": "        identity = self._get_identity(request)\n\n        if identity is None:\n            self.debug and self._log(\n                'repoze.who identity is None, returning None',\n                'unauthenticated_userid',\n                request,\n            )\n            return None\n\n        userid = identity['repoze.who.userid']\n\n        if userid is None:\n            self.debug and self._log(\n                'repoze.who.userid is None, returning None' % userid,\n                'unauthenticated_userid',\n                request,\n            )\n            return None\n\n        return userid\n", "idx": 834}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.forget", "completion": "        identifier = self._get_identifier(request)\n        if identifier is None:\n            return []\n        return identifier.forget(request.environ)\n\n", "idx": 835}
{"namespace": "pyramid.authentication.AuthTktAuthenticationPolicy.unauthenticated_userid", "completion": "        cookie = self.cookie.get_cookie(request)\n        if cookie is None:\n            return None\n        return cookie.get_userid()\n", "idx": 836}
{"namespace": "pyramid.authentication.SessionAuthenticationHelper.remember", "completion": "        request.session[self.userid_key] = userid\n        return []\n", "idx": 837}
{"namespace": "pyramid.authentication.SessionAuthenticationHelper.forget", "completion": "        request.session.pop(self.userid_key, None)\n        return []\n", "idx": 838}
{"namespace": "pyramid.authentication.BasicAuthAuthenticationPolicy.unauthenticated_userid", "completion": "        auth_header = request.headers.get('Authorization')\n        if auth_header is None:\n            return None\n        auth_header = auth_header.strip()\n        if not auth_header.lower().startswith('basic '):\n            return None\n        auth_header = auth_header[6:]\n        try:\n            auth_header = base64.b64decode(auth_header).decode('utf-8')\n        except Exception:\n            return None\n        username, password = auth_header.split(':', 1)\n        return username\n", "idx": 839}
{"namespace": "pyramid.request.CallbackMethodsMixin._process_response_callbacks", "completion": "        for callback in self.response_callbacks:\n            callback(self, response)\n\n", "idx": 840}
{"namespace": "pyramid.request.CallbackMethodsMixin._process_finished_callbacks", "completion": "        callbacks = self.finished_callbacks\n        while callbacks:\n            callback = callbacks.popleft()\n            callback(self)\n\n", "idx": 841}
{"namespace": "pyramid.request.Request.session", "completion": "        from pyramid.interfaces import ISessionFactory\n\n        session_factory = self.registry.queryUtility(ISessionFactory)\n        if session_factory is None:\n            raise ConfigurationError(\n                \"No session factory registered.  \"\n                \"Please register a session factory.\"\n            )\n        return session_factory(self)\n", "idx": 842}
{"namespace": "pyramid.request.RequestLocalCache.get_or_create", "completion": "        if creator is None:\n            creator = self._creator\n        if creator is None:\n            raise TypeError(\n                'No creator function provided and no creator function bound to the cache'\n            )\n        if request not in self._store:\n            self._store[request] = creator(request)\n        return self._store[request]\n", "idx": 843}
{"namespace": "pyramid.request.RequestLocalCache.set", "completion": "        self._store[request] = value\n        request.add_finished_callback(self.remove)\n", "idx": 844}
{"namespace": "pyramid.i18n.Localizer.pluralize", "completion": "        if self.pluralizer is None:\n            self.pluralizer = Pluralizer(self.translations)\n        return self.pluralizer(singular, plural, n, domain=domain, mapping=mapping)\n\n", "idx": 845}
{"namespace": "pyramid.i18n.Translations.load", "completion": "        if locales is None:\n            locales = []\n        if dirname is None:\n            return gettext.NullTranslations()\n        if not os.path.isdir(dirname):\n            return gettext.NullTranslations()\n        if not locales:\n            locales = ['en']\n        for locale in locales:\n            if isinstance(locale, str):\n                locale = Locale.parse(locale)\n            if locale.language in ('en', ''):\n                continue\n            for lang in locale.languages:\n                if lang in ('en', ''):\n                    continue\n                for territory in locale.territories:\n                    if territory in ('US', ''):\n                        continue\n                    mofile = os.path.join(dirname, lang + '_' + territory + '.mo')\n                    if os.path.isfile(mofile):\n                        with open(mofile, 'rb') as mofp:\n                            return cls(mofp, domain)\n        return gettext.NullTranslations()\n", "idx": 846}
{"namespace": "pyramid.i18n.Translations.add", "completion": "        if translations.domain != self.domain:\n            if translations.domain not in self._domains:\n                self._domains[translations.domain] = Translations(\n                    domain=translations.domain\n                )\n            self._domains[translations.domain].add(translations, merge)\n            return self\n\n        if merge:\n            self._catalog.update(translations._catalog)\n        else:\n            self._catalog = translations._catalog\n\n        return self\n", "idx": 847}
{"namespace": "pyramid.i18n.Translations.merge", "completion": "        for key in translations._catalog:\n            if key in self._catalog:\n                self._catalog[key] = translations._catalog[key]\n            else:\n                self._catalog[key] = translations._catalog[key]\n\n        self.files.extend(translations.files)\n\n        return self\n", "idx": 848}
{"namespace": "pyramid.i18n.LocalizerRequestMixin.locale_name", "completion": "        return negotiate_locale_name(self)", "idx": 849}
{"namespace": "pyramid.csrf.LegacySessionCSRFStoragePolicy.check_csrf_token", "completion": "        if not request.session.get_csrf_token():\n            return False\n\n        return bytes(request.session.get_csrf_token()) == bytes(supplied_token)\n\n", "idx": 850}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.new_csrf_token", "completion": "        token = self._token_factory()\n        request.session[self.key] = token\n        return token\n", "idx": 851}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.get_csrf_token", "completion": "        if self.key in request.session:\n            return request.session[self.key]\n        else:\n            return self.new_csrf_token(request)\n", "idx": 852}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.check_csrf_token", "completion": "        expected_token = self.get_csrf_token(request)\n        return not strings_differ(\n            bytes_(expected_token), bytes_(supplied_token)\n        )\n\n", "idx": 853}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.new_csrf_token", "completion": "        token = self._token_factory()\n        self.cookie_profile.set_cookie(request, token)\n        request.add_response_callback(self.set_csrf_token, token)\n        return token\n", "idx": 854}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.get_csrf_token", "completion": "        token = request.cookies.get(self.cookie_name, None)\n        if not token:\n            token = self.new_csrf_token(request)\n        return token\n", "idx": 855}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.check_csrf_token", "completion": "        expected_token = self.get_csrf_token(request)\n        return not strings_differ(\n            bytes_(expected_token), bytes_(supplied_token)\n        )\n\n", "idx": 856}
{"namespace": "pyramid.security.PermitsResult.__repr__", "completion": "        return \"<{0} instance at {1} with msg {2}>\".format(\n            self.__class__.__name__, id(self), self.msg\n        )\n\n", "idx": 857}
{"namespace": "pyramid.util.InstancePropertyHelper.make_property", "completion": "        if name is None:\n            name = callable.__name__\n\n        if reify:\n            property_ = SettableProperty(callable)\n        else:\n            property_ = property(callable)\n\n        return name, property_\n", "idx": 858}
{"namespace": "pyramid.util.InstancePropertyHelper.set_property", "completion": "        name, fn = cls.make_property(callable, name, reify)\n        setattr(target, name, fn)\n", "idx": 859}
{"namespace": "pyramid.util.InstancePropertyHelper.add_property", "completion": "        prop = self.make_property(callable, name=name, reify=reify)\n        self.properties[prop[0]] = prop[1]\n", "idx": 860}
{"namespace": "pyramid.util.InstancePropertyHelper.apply", "completion": "        self.apply_properties(target, self.properties)\n\n", "idx": 861}
{"namespace": "pyramid.util.InstancePropertyMixin.set_property", "completion": "        InstancePropertyHelper.set_property(self, callable, name=name, reify=reify)\n", "idx": 862}
{"namespace": "pyramid.util.TopologicalSorter.remove", "completion": "        if name in self.name2val:\n            del self.name2val[name]\n            self.names.remove(name)\n            for before in self.name2before[name]:\n                self.req_after.remove((name, before))\n            for after in self.name2after[name]:\n                self.req_before.remove((after, name))\n            del self.name2before[name]\n            del self.name2after[name]\n", "idx": 863}
{"namespace": "pyramid.util.TopologicalSorter.add", "completion": "        if name in self.name2val:\n            raise ValueError(\n                'name %r already in topological sorter' % name\n            )\n        self.names.append(name)\n        self.name2val[name] = val\n        if after is None:\n            after = self.default_after\n        if before is None:\n            before = self.default_before\n        if after is not None:\n            if after is FIRST:\n                self.req_before.add(name)\n            elif after is LAST:\n                self.req_after.add(name)\n            else:\n                self.req_after.add(name)\n                for u in as_sorted_tuple(after):\n                    self.req_after.add(u)\n                    self.order.append((u, name))\n                    self.name2after.setdefault(u, []).append(name)\n        if before is not None:\n            if before is LAST:\n                self.req_before.add(name)\n            elif before is FIRST:\n                self.req_before.add(name)\n            else:\n                self.req_before.add(name)\n                for u in as_sorted_tuple(before):\n                    self.req_before.add(u)\n                    self.order.append((name, u))\n                    self.name2before.setdefault(u, []).append(name)\n", "idx": 864}
{"namespace": "pyramid.traversal.find_resource", "completion": "    if isinstance(path, tuple):\n        path = path[0]\n    if isinstance(path, str):\n        path = path.split('/')\n    if path[0] == '':\n        path = path[1:]\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[0] == '':\n        return resource\n    if path[", "idx": 865}
{"namespace": "pyramid.static.ManifestCacheBuster.manifest", "completion": "        if self.reload:\n            if self.exists(self.manifest_path):\n                mtime = self.getmtime(self.manifest_path)\n                if mtime > self._mtime:\n                    self._manifest = self.get_manifest()\n                    self._mtime = mtime\n        return self._manifest\n", "idx": 866}
{"namespace": "pyramid.registry.Registry.registerSubscriptionAdapter", "completion": "        self.has_listeners = True\n        return Components.registerSubscriptionAdapter(self, *arg, **kw)\n", "idx": 867}
{"namespace": "pyramid.registry.Registry.registerHandler", "completion": "        result = Components.registerHandler(self, *arg, **kw)\n        self.has_listeners = True\n        return result\n", "idx": 868}
{"namespace": "pyramid.registry.Registry.notify", "completion": "        if not self.has_listeners:\n            return\n\n        for event in events:\n            self.subscribers(event)\n", "idx": 869}
{"namespace": "pyramid.registry.Introspector.add", "completion": "        if intr.category not in self._categories:\n            self._categories[intr.category] = {}\n\n        if intr.discriminator not in self._categories[intr.category]:\n            self._categories[intr.category][intr.discriminator] = []\n\n        self._categories[intr.category][intr.discriminator].append(\n            (self._counter, intr)\n        )\n        self._counter += 1\n        self._refs[intr.name] = intr\n", "idx": 870}
{"namespace": "pyramid.registry.Introspector.get", "completion": "        category = self._categories.get(category_name, {})\n        if discriminator in category:\n            return category[discriminator]\n        if discriminator in category:\n            return category[discriminator]\n        if discriminator in category:\n            return category[discriminator]\n        return default\n", "idx": 871}
{"namespace": "pyramid.registry.Introspector.get_category", "completion": "        category = self._categories.get(category_name, default)\n        if sort_key is None:\n            return [\n                {'introspectable': intr, 'related': self._refs[intr.discriminator]}\n                for intr in sorted(category.values(), key=lambda x: x.order)\n            ]\n        else:\n            return [\n                {'introspectable': intr, 'related': self._refs[intr.discriminator]}\n                for intr in sorted(category.values(), key=sort_key)\n            ]\n", "idx": 872}
{"namespace": "pyramid.registry.Introspector.categorized", "completion": "        if sort_key is None:\n            sort_key = operator.attrgetter('order')\n        categories = self._categories.items()\n        categories = sorted(categories, key=sort_key)\n        return [\n            (category_name, self.get_category(category_name, sort_key=sort_key))\n            for category_name, _ in categories\n        ]\n", "idx": 873}
{"namespace": "pyramid.registry.Introspector.remove", "completion": "        category = self._categories.get(category_name)\n        if category is None:\n            return\n        intr = category.get(discriminator)\n        if intr is None:\n            return\n        del category[discriminator]\n        del category[intr.discriminator_hash]\n        for ref in self._refs.values():\n            if ref.introspectable is intr:\n                del self._refs[ref.ref_hash]\n", "idx": 874}
{"namespace": "pyramid.registry.Introspector.relate", "completion": "        introspectables = self._get_intrs_by_pairs(pairs)\n        for intr in introspectables:\n            for other in introspectables:\n                if intr is other:\n                    continue\n                L = self._refs.setdefault(other, [])\n                if intr not in L:\n                    L.append(intr)\n", "idx": 875}
{"namespace": "pyramid.registry.Introspector.related", "completion": "        category_name = intr.category_name\n        discriminator = intr.discriminator\n        related = self._categories.get(category_name, {}).get(discriminator)\n        if related is None:\n            raise KeyError((category_name, discriminator))\n        return self._refs.get(related, [])", "idx": 876}
{"namespace": "pyramid.registry.Introspectable.discriminator_hash", "completion": "        self._assert_resolved()\n        return hash(self.discriminator)\n", "idx": 877}
{"namespace": "pyramid.registry.Introspectable.__repr__", "completion": "        return '<%s category %r, discriminator %r>' % (\n            self.type_name,\n            self.category_name,\n            self.discriminator,\n        )\n", "idx": 878}
{"namespace": "pyramid.scripts.proutes.PRoutesCommand._get_mapper", "completion": "        # Get the routes mapper object associated with the given registry.\n        mapper = registry.queryUtility(IRouteRequest)\n\n        # Return the routes mapper object associated with the given registry.\n        return mapper\n", "idx": 879}
{"namespace": "pyramid.scripts.pshell.PShellCommand.make_shell", "completion": "        shells = self.find_all_shells()\n        if self.args.python_shell:\n            shell_name = self.args.python_shell.lower()\n            if shell_name in shells:\n                return shells[shell_name]\n            else:\n                raise ValueError(\n                    'could not find a shell named \"%s\"' % (shell_name,)\n                )\n        elif self.preferred_shells:\n            for shell_name in self.preferred_shells:\n                if shell_name in shells:\n                    return shells[shell_name]\n        elif shells:\n            return shells[list(shells.keys())[0]]\n        else:\n            return self.default_runner", "idx": 880}
{"namespace": "pyramid.config.assets.PackageOverrides.insert", "completion": "        if path.endswith('/'):\n            override = DirectoryOverride(path, source)\n        else:\n            override = FileOverride(path, source)\n        self.overrides.insert(0, override)\n        return override\n", "idx": 881}
{"namespace": "pyramid.config.assets.PackageOverrides.filtered_sources", "completion": "        for override in self.overrides:\n            if override.is_resource_name_match(resource_name):\n                yield override.source\n", "idx": 882}
{"namespace": "pyramid.config.assets.PackageOverrides.real_loader", "completion": "        if self._real_loader is None:\n            raise NotImplementedError(\n                \"The real loader of the PackageOverrides instance is not set.\"\n            )\n        return self._real_loader\n\n", "idx": 883}
{"namespace": "pyramid.config.views.MultiView.add", "completion": "        if phash is None:\n            phash = DEFAULT_PHASH\n        if accept is None:\n            self.views.append((order, view))\n        else:\n            if accept not in self.media_views:\n                self.media_views[accept] = []\n                self.accepts.append((accept_order, accept))\n            self.media_views[accept].append((order, view))\n        self.views.sort()\n        self.accepts.sort()\n", "idx": 884}
{"namespace": "pyramid.config.views.MultiView.get_views", "completion": "        if request.accept is not None and self.accepts:\n            views = []\n            for accept in request.accept:\n                if accept in self.media_views:\n                    views.extend(\n                        [v for _, v, _ in self.media_views[accept]]\n                    )\n            views.extend([v for _, v, _ in self.views])\n            return views\n        else:\n            return [v for _, v, _ in self.views]\n", "idx": 885}
{"namespace": "pyramid.config.views.MultiView.match", "completion": "        for order, view, phash in self.get_views(request):\n            if phash is not None:\n                if phash(context, request):\n                    return view\n            else:\n                return view\n        raise PredicateMismatch(self.name)\n\n", "idx": 886}
{"namespace": "pyramid.config.views.MultiView.__permitted__", "completion": "        view = self.match(context, request)\n        if hasattr(view, '__permitted__'):\n            return view.__permitted__(context, request)\n        return True\n\n", "idx": 887}
{"namespace": "pyramid.config.views.MultiView.__call_permissive__", "completion": "        view = self.match(context, request)\n        if hasattr(view, '__call_permissive__'):\n            return view.__call_permissive__(context, request)\n        return view(context, request)\n", "idx": 888}
{"namespace": "pyramid.config.actions.ActionState.processSpec", "completion": "        if spec in self._seen_files:\n            return False\n        else:\n            self._seen_files.add(spec)\n            return True\n", "idx": 889}
{"namespace": "pyramid.config.actions.ActionState.action", "completion": "        if kw is None:\n            kw = {}\n        action = dict(\n            discriminator=discriminator,\n            callable=callable,\n            args=args,\n            kw=kw,\n            order=order,\n            includepath=includepath,\n            info=info,\n            introspectables=introspectables,\n        )\n        action.update(extra)\n        self.actions.append(action)\n", "idx": 890}
{"namespace": "pyramid.config.actions.ActionInfo.__str__", "completion": "        return (\n            'Line {} of file {}:\\n{}'.format(self.line, self.file, self.src)\n            if self.file\n            else self.src\n        )\n", "idx": 891}
{"namespace": "pyramid.config.Configurator.__getattr__", "completion": "        if name in self.registry._directives:\n            c, action_wrap = self.registry._directives[name]\n            if action_wrap:\n                c = self.action_wrap(c)\n            return c\n        raise AttributeError(name)\n", "idx": 892}
{"namespace": "pyramid.config.Configurator.with_package", "completion": "        return self.__class__(\n            registry=self.registry,\n            package=package,\n            root_package=self.root_package,\n            autocommit=self.autocommit,\n            route_prefix=self.route_prefix,\n        )\n", "idx": 893}
{"namespace": "pyramid.config.Configurator.absolute_asset_spec", "completion": "        if isinstance(relative_spec, str):\n            if relative_spec.startswith('.'):\n                return '%s:%s' % (self.package_name, relative_spec)\n            else:\n                return relative_spec\n        else:\n            return relative_spec\n", "idx": 894}
{"namespace": "pyramid.config.Configurator.begin", "completion": "        if request is _marker:\n            request = self.registry.request\n        self.registry.push(request)\n", "idx": 895}
{"namespace": "pyramid.config.Configurator.scan", "completion": "        if package is None:\n            package = caller_package()\n        venusian = self.venusian\n        scanner = venusian.Scanner(\n            venusian.attach,\n            venusian.scan,\n            venusian.scan_module,\n            venusian.scan_package,\n            venusian.scan_global,\n            venusian.scan_module_global,\n            venusian.scan_package_global,\n            venusian.scan_class,\n            venusian.scan_class_global,\n            venusian.scan_instance,\n            venusian.scan_instance_global,\n            venusian.scan_method,\n            venusian.scan_method_global,\n            venusian.scan_function,\n            venusian.scan_function_global,\n            venusian.scan_attribute,\n            venusian.scan_attribute_global,\n            venusian.scan_class_attribute,\n            venusian.scan_class_attribute_global,\n            venusian.scan_instance_attribute,\n            venusian.scan_instance_attribute_global,\n            venusian.scan_method_attribute,\n            venusian.scan_method_attribute_global,\n            venusian.scan_function_attribute,\n            venusian.scan_function_attribute_global,\n            venusian.scan_class_method,\n            venusian.scan_class_method_global,\n            venusian.scan_instance_method,\n            venusian.scan_instance_method_global,\n            venusian.scan_method_method,\n            venusian.scan_method_method_global,\n            venusian.scan_function_method,\n            venusian.scan_function_method_global,\n            venusian.scan_class_function,\n            venusian.scan_class_function_global,\n            venusian.scan_instance_function,\n            venusian.scan_instance_function_global,\n            venusian.scan_method_function,\n            venusian.scan_method_function_global,\n            venusian.scan_function_function,\n            venusian.scan_function_function_global,\n            venusian.scan_class_attribute_function,\n            venusian", "idx": 896}
{"namespace": "pyramid.config.Configurator.make_wsgi_app", "completion": "        self.commit()\n        self.registry.notify(ApplicationCreated(self.registry))\n        return self.registry.wsgi_app\n", "idx": 897}
{"namespace": "aiohappybase._util.camel_case_to_pep8", "completion": "    return CAPITALS.sub(r'_\\1', name).lower()", "idx": 898}
{"namespace": "kinto.authorization._relative_object_uri", "completion": "    obj_parts = object_uri.split(\"/\")\n    for i in range(len(obj_parts) - 1, 0, -1):\n        if obj_parts[i] == resource_name:\n            return \"/\".join(obj_parts[:i])\n    raise ValueError(\n        \"The object URI does not match the resource name. The resource name is: {}. The object URI is: {}.\".format(\n            resource_name, object_uri\n        )\n    )\n\n", "idx": 899}
{"namespace": "kinto.core.openapi.OpenAPI.expose_authentication_method", "completion": "        cls.security_definitions[method_name] = definition\n        cls.security_roles[method_name] = definition.get(\"scopes\", [])\n", "idx": 900}
{"namespace": "kinto.core.openapi.OpenAPI.generate", "completion": "        # Create the base specification dictionary\n        base_spec = {\n            \"swagger\": \"2.0\",\n            \"info\": {\n                \"title\": self.api_title,\n                \"version\": self.api_version,\n            },\n            \"host\": self.request.host,\n            \"schemes\": self.request.scheme,\n            \"securityDefinitions\": self.security_definitions,\n        }\n\n        # Call the generate method of the parent class, passing the base specification as the \"swagger\" parameter\n        return super().generate(swagger=base_spec)", "idx": 901}
{"namespace": "kinto.core.testing.get_user_headers", "completion": "    auth = f\"{user}:{password}\"\n    encoded_auth = base64.b64encode(auth.encode(\"utf-8\")).decode(\"utf-8\")\n    return {\"Authorization\": f\"Basic {encoded_auth}\"}", "idx": 902}
{"namespace": "kinto.core.authorization.RouteFactory.fetch_shared_objects", "completion": "        # Get the object IDs that are readable or writable for the current principals.\n        object_ids = self._get_accessible_objects(perm, principals, get_bound_permissions)\n\n        # Set shared ids to the context.\n        self.shared_ids = object_ids\n\n        # Return the object IDs.\n        return object_ids\n", "idx": 903}
{"namespace": "kinto.core.authorization.RouteFactory.get_permission_object_id", "completion": "        # If the request is on a plural endpoint, and object_id is provided, it finds the object URI by inspecting the \"plural\" service and its sibling \"object\" service.\n        if self.on_plural_endpoint and object_id is not None:\n            # Get the \"plural\" service.\n            plural_service = utils.current_service(request)\n            # Get the \"object\" service.\n            object_service = utils.current_service(request, self.resource_name)\n            # Get the \"plural\" service's object id.\n            plural_object_id = plural_service.object_id\n            # Get the \"object\" service's object id.\n            object_object_id = object_service.object_id\n            # If the \"object\" service's object id is not None, then the object URI is the \"plural\" service's object id with the \"object\" service's object id appended.\n            if object_object_id is not None:\n                object_id = f\"{plural_object_id}/{object_object_id}\"\n            # If the \"object\" service's object id is None, then the object URI is the \"plural\" service's object id.\n            else:\n                object_id = plural_object_id\n\n        return object_id\n", "idx": 904}
{"namespace": "kinto.core.utils.recursive_update_dict", "completion": "    for key, value in changes.items():\n        if key in root:\n            if isinstance(value, dict):\n                if key in root:\n                    recursive_update_dict(root[key], value, ignores)\n                else:\n                    root[key] = value\n            else:\n                if key in ignores:\n                    root.pop(key)\n                else:\n                    root[key] = value\n        else:\n            root[key] = value\n\n", "idx": 905}
{"namespace": "kinto.core.utils.native_value", "completion": "    if isinstance(value, str):\n        try:\n            return json.loads(value)\n        except ValueError:\n            return value\n    else:\n        return value\n\n", "idx": 906}
{"namespace": "kinto.core.utils.dict_subset", "completion": "    return {k: d[k] for k in keys if k in d}\n\n", "idx": 907}
{"namespace": "kinto.core.utils.dict_merge", "completion": "    if not isinstance(a, dict) or not isinstance(b, dict):\n        raise TypeError(\"Both input arguments must be dictionaries.\")\n\n    result = a.copy()\n    for key, value in b.items():\n        if key in result and isinstance(result[key], dict) and isinstance(value, dict):\n            result[key] = dict_merge(result[key], value)\n        else:\n            result[key] = value\n\n    return result\n\n", "idx": 908}
{"namespace": "kinto.core.utils.find_nested_value", "completion": "    if path in d:\n        return d[path]\n    parts = path.split(\".\")\n    for i in range(len(parts), 0, -1):\n        root = \".\".join(parts[:i])\n        if root in d and isinstance(d[root], dict):\n            return find_nested_value(d[root], \".\".join(parts[i:]), default)\n    return default\n\n", "idx": 909}
{"namespace": "kinto.core.utils.instance_uri_registry", "completion": "    api_prefix = f\"/{registry.route_prefix}\"\n    path = api_prefix + \"/\" + resource_name\n\n    q = registry.queryUtility\n    routes_mapper = q(IRoutesMapper)\n\n    fakerequest = Request.blank(path=path)\n    fakerequest.registry = registry\n    info = routes_mapper(fakerequest)\n    matchdict, route = info[\"match\"], info[\"route\"]\n    if route is None:\n        raise ValueError(\"URI has no route\")\n\n    return strip_uri_prefix(request.route_path(f\"{resource_name}-object\", **params))\n\n", "idx": 910}
{"namespace": "kinto.core.statsd.load_from_config", "completion": "    if not statsd_module:\n        raise ConfigurationError(\"The statsd module is not installed. Please install it to use the StatsD client.\")\n\n    statsd_url = config.get(\"statsd_url\")\n    if not statsd_url:\n        raise ConfigurationError(\"The statsd_url setting is not defined. Please specify a statsd_url setting to use the StatsD client.\")\n\n    parsed_url = urlparse(statsd_url)\n    if not parsed_url.hostname:\n        raise ConfigurationError(\"The statsd_url setting is not valid. Please specify a valid statsd_url setting to use the StatsD client.\")\n\n    return Client(parsed_url.hostname, parsed_url.port, parsed_url.path)", "idx": 911}
{"namespace": "kinto.core.errors.http_error", "completion": "    if errno is None:\n        errno = ERRORS.UNDEFINED\n\n    if code is None:\n        code = httpexception.code\n\n    if error is None:\n        error = httpexception.title\n\n    if message is None:\n        message = None\n\n    if info is None:\n        info = None\n\n    if details is None:\n        details = colander.drop\n\n    response = ErrorSchema().deserialize(\n        {\n            \"code\": code,\n            \"errno\": errno,\n            \"error\": error,\n            \"message\": message,\n            \"info\": info,\n            \"details\": details,\n        }\n    )\n\n    response.status_code = code\n    response.headers[\"Content-Type\"] = \"application/json\"\n\n    return response\n\n", "idx": 912}
{"namespace": "kinto.core.resource.schema.ResourceReponses.get_and_bind", "completion": "        # Get default schemas\n        default_schemas = self.default_schemas.copy()\n        default_object_schemas = self.default_object_schemas.copy()\n        default_plural_schemas = self.default_plural_schemas.copy()\n        default_get_schemas = self.default_get_schemas.copy()\n        default_post_schemas = self.default_post_schemas.copy()\n        default_put_schemas = self.default_put_schemas.copy()\n        default_patch_schemas = self.default_patch_schemas.copy()\n        default_delete_schemas = self.default_delete_schemas.copy()\n\n        # Get endpoint-specific schemas\n        if endpoint_type in self.object_get_schemas:\n            default_object_schemas.update(self.object_get_schemas[endpoint_type])\n        if endpoint_type in self.object_patch_schemas:\n            default_object_schemas.update(self.object_patch_schemas[endpoint_type])\n        if endpoint_type in self.object_delete_schemas:\n            default_object_schemas.update(self.object_delete_schemas[endpoint_type])\n\n        # Get method-specific schemas\n        if method in self.default_object_schemas:\n            default_object_schemas.update(self.default_object_schemas[method])\n        if method in self.default_plural_schemas:\n            default_plural_schemas.update(self.default_plural_schemas[method])\n        if method in self.default_get_schemas:\n            default_get_schemas.update(self.default_get_schemas[method])\n        if method in self.default_post_schemas:\n            default_post_schemas.update(self.default_post_schemas[method])\n        if method in self.default_put_schemas:\n            default_put_schemas.update(self.default_put_schemas[method])\n        if method in self.default_patch_schemas:\n            default_patch_schemas.update(self.default_patch_schemas[method])\n        if method in self.default_delete_schemas:\n            default_delete_schemas.update(self.default_delete_schemas[method])\n\n        # Get endpoint-specific schemas\n        if endpoint_type in self.default_object_schemas:\n            default_object_schemas", "idx": 913}
{"namespace": "kinto.core.resource.Resource.timestamp", "completion": "        try:\n            return self.model.timestamp\n        except storage_exceptions.ReadOnlyError as e:\n            http_error(self.request, e)\n", "idx": 914}
{"namespace": "kinto.core.resource.Resource.plural_post", "completion": "        self._raise_400_if_invalid_id(self.object_id)\n        self._raise_409_if_conflict(self.object_id)\n        self._raise_412_if_modified(obj={})\n\n        # The object id is added to the posted body.\n        if self.object_id:\n            self.request.json[\"data\"][self.model.id_field] = self.object_id\n\n        # The object is processed, created, and returned.\n        return self._create_object()\n", "idx": 915}
{"namespace": "kinto.core.resource.Resource.get", "completion": "        self._add_timestamp_header(self.request.response)\n        self._add_cache_header(self.request.response)\n        self._raise_304_if_not_modified()\n        # Plural endpoints are considered resources that always exist\n        self._raise_412_if_modified(obj={})\n\n        obj = self._get_object_or_404(self.object_id)\n\n        partial_fields = self._extract_partial_fields()\n\n        if partial_fields:\n            obj = dict_subset(obj, partial_fields)\n\n        return self.postprocess(obj)\n", "idx": 916}
{"namespace": "kinto.core.resource.Resource.delete", "completion": "        self._raise_400_if_invalid_id(self.object_id)\n        obj = self._get_object_or_404(self.object_id)\n        self._raise_412_if_modified(obj)\n\n        self.model.delete_object(self.object_id)\n        timestamp = obj[self.model.modified_field]\n        self._add_timestamp_header(self.request.response, timestamp=timestamp)\n\n        return self.postprocess(obj, action=ACTIONS.DELETE)\n", "idx": 917}
{"namespace": "kinto.core.permission.memory.Permission.add_principal_to_ace", "completion": "        # Fetch the set of principals associated with the given object and permission.\n        object_permission_key = f\"object:{object_id}:{permission}\"\n        object_permission_principals = self._store.get(object_permission_key, set())\n\n        # Add the new principal to the set.\n        object_permission_principals.add(principal)\n\n        # Update the store with the modified set.\n        self._store[object_permission_key] = object_permission_principals\n", "idx": 918}
{"namespace": "kinto.core.permission.memory.Permission.get_object_permission_principals", "completion": "        permission_key = f\"permission:{object_id}:{permission}\"\n        object_permission_principals = self._store.get(permission_key, set())\n        return object_permission_principals\n", "idx": 919}
{"namespace": "kinto.core.storage.generators.Generator.match", "completion": "        if self._regexp is None:\n            self._regexp = re.compile(self.regexp)\n\n        return self._regexp.match(object_id) is not None\n", "idx": 920}
{"namespace": "kinto.core.storage.postgresql.migrator.MigratorMixin.create_or_migrate_schema", "completion": "        # Get the current version of the schema\n        current_version = self.get_installed_version()\n\n        # If there is no current version, create a new schema\n        if current_version is None:\n            self.create_schema(dry_run)\n\n        # If the current version matches the desired version, log that the schema is up-to-date\n        elif current_version == self.schema_version:\n            logger.info(f\"Schema is up-to-date: {self.name}\")\n\n        # Otherwise, migrate the schema to the desired version\n        else:\n            self.migrate_schema(current_version, dry_run)\n", "idx": 921}
{"namespace": "kinto.core.views.batch.BatchPayloadSchema.deserialize", "completion": "        # Deserialize the data.\n        deserialized_data = super().deserialize(cstruct)\n\n        # Merge the defaults with the requests.\n        for request in deserialized_data[\"requests\"]:\n            request.update(deserialized_data[\"defaults\"])\n\n        return deserialized_data\n\n", "idx": 922}
{"namespace": "kinto.plugins.accounts.utils.get_cached_reset_password", "completion": "    settings = registry.settings\n    hmac_secret = settings[\"userid_hmac_secret\"]\n    cache_key = utils.hmac_digest(hmac_secret, ACCOUNT_RESET_PASSWORD_CACHE_KEY.format(username))\n    cache = registry.cache\n    reset_password = cache.get(cache_key)\n    return reset_password\n\n", "idx": 923}
{"namespace": "kinto.plugins.accounts.utils.get_cached_validation_key", "completion": "    hmac_secret = registry.settings[\"userid_hmac_secret\"]\n    cache_key = utils.hmac_digest(hmac_secret, ACCOUNT_VALIDATION_CACHE_KEY.format(username))\n\n    cache = registry.cache\n    cache_result = cache.get(cache_key)\n    return cache_result\n\n", "idx": 924}
{"namespace": "kinto.plugins.accounts.views.validation.on_account_activated", "completion": "    settings = event.request.registry.settings\n    if settings.get(\"account_validation.enabled\", False):\n        for impacted_object in event.data[\"impacted_objects\"]:\n            if impacted_object[\"action\"] == ACTIONS.UPDATE:\n                if impacted_object[\"old\"][\"validated\"] is True:\n                    continue\n                if impacted_object[\"new\"][\"validated\"] is False:\n                    continue\n                Emailer(event.request, impacted_object[\"new\"]).send_confirmation_email()", "idx": 925}
{"namespace": "kinto.plugins.openid.OpenIDConnectPolicy._verify_token", "completion": "        # Fetch user profile information from the userinfo endpoint.\n        userinfo_endpoint = self.oid_config.get(\"userinfo_endpoint\")\n        if userinfo_endpoint is None:\n            return None\n        headers = {\"Authorization\": f\"{self.header_type} {access_token}\"}\n        try:\n            response = requests.get(userinfo_endpoint, headers=headers)\n            response.raise_for_status()\n            return response.json()\n        except requests.exceptions.RequestException as e:\n            request.logger.debug(f\"Failed to fetch user profile: {e}\")\n            return None\n", "idx": 926}
{"namespace": "kinto.plugins.quotas.scripts.rebuild_quotas", "completion": "    for bucket in storage.buckets:\n\n        # Calculate the total record count, storage size, and collection count for each bucket.\n        total_record_count = record_size(bucket)\n        total_storage_size = bucket.storage_size\n        total_collection_count = bucket.collection_count\n\n        # Update the quota information for each bucket in the storage.\n        quota = COLLECTION_QUOTA_OBJECT_ID.get(bucket.id)\n        quota.total_record_count = total_record_count\n        quota.total_storage_size = total_storage_size\n        quota.total_collection_count = total_collection_count\n        quota.save()\n\n        # Log the final size of each bucket.\n        logger.info(f\"Bucket {bucket.id} has {total_record_count} records, {total_storage_size} storage, and {total_collection_count} collections.\")\n", "idx": 927}
{"namespace": "kinto.config.render_template", "completion": "    with open(template, 'r') as template_file:\n        template_content = template_file.read()\n\n    for key, value in kwargs.items():\n        template_content = template_content.replace('{{' + key + '}}', value)\n\n    with open(destination, 'w') as destination_file:\n        destination_file.write(template_content)\n\n", "idx": 928}
{"namespace": "trafilatura.sitemaps.SitemapObject.extract_sitemap_langlinks", "completion": "        # extract links corresponding to a given target language from the content of a SitemapObject\n        # search for the presence of 'hreflang=' in the content and then use a regular expression to extract the links\n        # iterate through the extracted attributes and check if they match the target language\n        # if a match is found, it handles the link\n        # finally, log a debug message '%s sitemaps and %s links with hreflang found for %s' about the number of sitemaps and links found with hreflang for the given target language.\n\n        # extract links corresponding to a given target language from the content of a SitemapObject\n        # search for the presence of 'hreflang=' in the content and then use a regular expression to extract the links\n        # iterate through the extracted attributes and check if they match the target language\n        # if a match is found, it handles the link\n        # finally, log a debug message '%s sitemaps and %s links with hreflang found for %s' about the number of sitemaps and links found with hreflang for the given target language.\n\n        # extract links corresponding to a given target language from the content of a SitemapObject\n        # search for the presence of 'hreflang=' in the content and then use a regular expression to extract the links\n        # iterate through the extracted attributes and check if they match the target language\n        # if a match is found, it handles the link\n        # finally, log a debug message '%s sitemaps and %s links with hreflang found for %s' about the number of sitemaps and links found with hreflang for the given target language.\n\n        # extract links corresponding to a given target language from the content of a SitemapObject\n        # search for the presence of 'hreflang=' in the content and then use a regular expression to extract the links\n        # iterate through the extracted attributes and check if they match the target language\n        # if a match is found, it handles the link\n        # finally, log a debug message '%s sitemaps and %s links with hreflang found for %s' about the number of sitemaps and links found with hreflang for the given target language.\n\n        # extract links corresponding to a given target language from the content of a SitemapObject\n        # search for the presence of 'hreflang=' in the content and then use a regular expression to extract the links\n        # iterate through the extracted", "idx": 929}
{"namespace": "trafilatura.sitemaps.SitemapObject.extract_sitemap_links", "completion": "        # compile regex here for modularity and efficiency\n        link_regex = re.compile(LINK_REGEX, re.DOTALL)\n        # extract\n        for link in (m[1] for m in islice(link_regex.finditer(self.content), MAX_LINKS)):\n            self.handle_link(link)\n        LOGGER.debug('%s sitemaps and %s links found for %s', len(self.sitemap_urls), len(self.urls), self.sitemap_url)\n", "idx": 930}
{"namespace": "trafilatura.sitemaps.SitemapObject.process", "completion": "        # check if sitemap is plausible\n        if not SITEMAP_FORMAT.search(self.content):\n            LOGGER.warning('sitemap format not recognized: %s', self.sitemap_url)\n            return\n\n        # extract links from txt file\n        if self.content.startswith('urlset'):\n            for match in (m[1] for m in islice(DETECT_LINKS.finditer(self.content), MAX_LINKS)):\n                self.handle_link(match)\n            LOGGER.debug('%s sitemaps and %s links found for %s', len(self.sitemap_urls), len(self.urls), self.sitemap_url)\n            return\n\n        # extract links from xml sitemap\n        if self.target_lang:\n            self.extract_sitemap_langlinks()\n        else:\n            self.extract_sitemap_links()\n\n        if self.sitemap_urls or self.urls:\n            return\n\n        # extract links from sitemap\n        for match in (m[1] for m in islice(DETECT_LINKS.finditer(self.content), MAX_LINKS)):\n            self.handle_link(match)\n        LOGGER.debug('%s sitemaps and %s links found for %s', len(self.sitemap_urls), len(self.urls), self.sitemap_url)\n\n", "idx": 931}
{"namespace": "trafilatura.sitemaps.is_plausible_sitemap", "completion": "    # Check if the URL is valid.\n    if not url.endswith(('.gz', 'sitemap', '.xml')):\n        return False\n\n    # Check if the contents of the sitemap are empty.\n    if contents is None or len(contents) == 0:\n        return False\n\n    # Check if the contents of the sitemap are in the expected format.\n    if not SITEMAP_FORMAT.match(contents):\n        return False\n\n    return True\n\n", "idx": 932}
{"namespace": "trafilatura.sitemaps.extract_robots_sitemaps", "completion": "    # Split the robots.txt file into lines\n    lines = robotstxt.splitlines()\n\n    # Remove comments and empty lines\n    lines = [line for line in lines if not line.startswith('#') and line.strip() != '']\n\n    # Initialize the list of sitemap URLs\n    sitemap_urls = []\n\n    # Iterate through the lines\n    for line in lines:\n        # Check if the line contains the sitemap directive\n        if line.startswith('Sitemap:'):\n            # Extract the sitemap URL from the line\n            sitemap_url = line.split(':', 1)[1].strip()\n            # Resolve the sitemap URL relative to the base URL\n            sitemap_url = fix_relative_urls(baseurl, sitemap_url)\n            # Append the sitemap URL to the list\n            sitemap_urls.append(sitemap_url)\n\n    return sitemap_urls", "idx": 933}
{"namespace": "trafilatura.feeds.handle_link_list", "completion": "    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if the link is valid\n    # check if", "idx": 934}
{"namespace": "trafilatura.feeds.find_feed_urls", "completion": "    # check if it's a feed\n    if validate_url(url)[0] is False:\n        LOGGER.debug('Invalid URL: %s', url)\n        return []\n    # get domain name and base URL\n    domainname, baseurl = get_hostinfo(url)\n    # fetch the webpage content\n    htmlstring = load_html(url)\n    # check if it's a feed\n    if htmlstring is None:\n        LOGGER.debug('Invalid HTML/Feed page: %s', url)\n        return []\n    # check if it's a feed\n    if FEED_OPENING.match(htmlstring):\n        feed_urls = extract_links(htmlstring, domainname, baseurl, url, target_lang)\n    # check if it's a web page\n    else:\n        feed_urls = determine_feed(htmlstring, baseurl, url)\n    # filter URLs based on target language\n    if target_lang is not None:\n        feed_urls = filter_urls(feed_urls, target_lang)\n    # log result\n    LOGGER.debug('Feed URLs found: %s of which %s valid', len(feed_urls), len(feed_urls))\n    return feed_urls", "idx": 935}
{"namespace": "trafilatura.hashing.generate_hash_filename", "completion": "    # Remove any potential XML tags from the content\n    content = CLEAN_XML.sub(\"\", content)\n\n    # Generate a bag-of-word hashing of length 12\n    hash_bytes = generate_bow_hash(content)\n\n    # Encode the hash using urlsafe_b64encode and return as a decoded string\n    return urlsafe_b64encode(hash_bytes).decode(\"utf-8\")\n\n", "idx": 936}
{"namespace": "trafilatura.cli_utils.download_queue_processing", "completion": "    # load the download buffer\n    download_buffer = load_download_buffer(args)\n    # process the downloads\n    errors = []\n    while not url_store.empty():\n        # get the next URL from the queue\n        url = url_store.get()\n        # process the download\n        try:\n            htmlstring = buffered_downloads(url, args, download_buffer)\n        except Exception as e:\n            # handle exceptions\n            errors.append(e)\n            continue\n        # process the result\n        counter = process_result(htmlstring, args, url, counter, config)\n    # return the errors and the updated counter\n    return errors, counter\n\n", "idx": 937}
{"namespace": "trafilatura.utils.decode_response", "completion": "    # if response is bytes\n    if isinstance(response, bytes):\n        # try to decode it\n        response = handle_compressed_file(response)\n        # if it is bytes\n        if isinstance(response, bytes):\n            # try to guess the encoding\n            guesses = detect_encoding(response)\n            # if it is not empty\n            if len(guesses) > 0:\n                # try to decode it\n                try:\n                    response = response.decode(guesses[0])\n                except UnicodeDecodeError:\n                    pass  # logging.debug('cannot decode response')\n    # if response is a urllib3 response\n    elif isinstance(response, HTTPResponse):\n        # try to decode it\n        response = handle_compressed_file(response.data)\n        # if it is bytes\n        if isinstance(response, bytes):\n            # try to guess the encoding\n            guesses = detect_encoding(response)\n            # if it is not empty\n            if len(guesses) > 0:\n                # try to decode it\n                try:\n                    response = response.decode(guesses[0])\n                except UnicodeDecodeError:\n                    pass  # logging.debug('cannot decode response')\n    # if response is not bytes or a urllib3 response\n    else:\n        pass  # logging.debug('cannot decode response')\n    return response\n\n", "idx": 938}
{"namespace": "trafilatura.utils.txttocsv", "completion": "    # Initialize the result string\n    result = \"\"\n\n    # Add the URL, fingerprint, hostname, title, image, date, license, and pagetype fields\n    result += docmeta[\"url\"] + \"\\t\" + docmeta[\"fingerprint\"] + \"\\t\" + docmeta[\"hostname\"] + \"\\t\" + docmeta[\"title\"] + \"\\t\" + docmeta[\"image\"] + \"\\t\" + docmeta[\"date\"] + \"\\t\" + docmeta[\"license\"] + \"\\t\" + docmeta[\"pagetype\"] + \"\\t\" + docmeta[\"id\"] + \"\\n\"\n\n    # Add the text and comments fields\n    result += text + \"\\t\" + comments\n\n    # Return the result string\n    return result\n\n", "idx": 939}
{"namespace": "trafilatura.utils.is_similar_domain", "completion": "    # Remove the file extension from both strings\n    reference = reference.split('.')[0]\n    new_string = new_string.split('.')[0]\n\n    # Calculate the similarity ratio\n    ratio = SequenceMatcher(None, reference, new_string).ratio()\n\n    # Return True if the similarity ratio is above the threshold, False otherwise\n    return ratio >= threshold\n\n", "idx": 940}
{"namespace": "trafilatura.xml.remove_empty_elements", "completion": "    for elem in tree.iter('*'):\n        if elem.text is None and elem.tail is None and not elem.getchildren():\n            elem.getparent().remove(elem)\n\n    return tree\n\n", "idx": 941}
{"namespace": "trafilatura.xml.strip_double_tags", "completion": "    # Define the list of tags to check for nested tags.\n    allowed_tags = ['cell', 'figure', 'item', 'note', 'quote']\n\n    # Iterate through the tree in reverse order.\n    for element in reversed(list(tree.iter())):\n        # Check if the current element is a tag.\n        if element.tag in allowed_tags:\n            # Check if the current element has a parent.\n            if element.getparent() is not None:\n                # Check if the parent element is a tag.\n                if element.getparent().tag in allowed_tags:\n                    # Merge the current element with its parent element.\n                    element.getparent().append(element)\n                    # Remove the current element from the tree.\n                    element.getparent().remove(element)\n\n    # Return the modified tree.\n    return tree\n\n", "idx": 942}
{"namespace": "trafilatura.xml.check_tei", "completion": "    # Check if the XML document is valid.\n    if not validate_tei(xmldoc):\n        raise ValueError(\"The XML document is not valid.\")\n\n    # Remove the <teiHeader> element.\n    xmldoc.remove(xmldoc.find(\"teiHeader\"))\n\n    # Remove the <teiCorpus> element.\n    xmldoc.remove(xmldoc.find(\"teiCorpus\"))\n\n    # Remove the <teiHeader> element.\n    xmldoc.remove(xmldoc.find(\"teiHeader\"))\n\n    # Remove the <teiHeader> element.\n    xmldoc.remove(xmldoc.find(\"teiHeader\"))\n\n    # Remove the <teiHeader> element.\n    xmldoc.remove(xmldoc.find(\"teiHeader\"))\n\n    # Remove the <teiHeader> element.\n    xmldoc.remove(xmldoc.find(\"teiHeader\"))\n\n    # Remove the <teiHeader> element.\n    xmldoc.remove(xmldoc.find(\"teiHeader\"))\n\n    # Remove the <teiHeader> element.\n    xmldoc.remove(xmldoc.find(\"teiHeader\"))\n\n    # Remove the <teiHeader> element.\n    xmldoc.remove(xmldoc.find(\"teiHeader\"))\n\n    # Remove the <teiHeader> element.\n    xmldoc.remove(xmldoc.find(\"teiHeader\"))\n\n    # Remove the <teiHeader> element.\n    xmldoc.remove(xmldoc.find(\"teiHeader\"))\n\n    # Remove the <teiHeader> element.\n    xmldoc.remove(xmldoc.find(\"teiHeader\"))\n\n    # Remove the <teiHeader> element.\n    xmldoc.remove(xmldoc.find(\"teiHeader\"))\n\n    # Remove the <teiHeader> element.\n    xmldoc.remove(xmldoc.find(\"teiHeader\"))\n\n    # Remove the <teiHeader> element.\n    xmldoc.remove(xmldoc.find(\"teiHeader\"))\n\n    # Remove the <teiHeader> element.\n    xmldoc.remove(xmldoc.find(\"teiHeader\"))\n\n    # Remove the <teiHeader> element.\n    xmldoc.remove(xmldoc.find(\"teiHeader\"))\n\n    # Remove the <teiHeader> element.\n    xmldoc.remove(xmldoc.find(\"tei", "idx": 943}
{"namespace": "trafilatura.xml.validate_tei", "completion": "    # load TEI validator if it is not already loaded\n    if TEI_RELAXNG is None:\n        with lzma.open(TEI_SCHEMA, 'rb') as f:\n            TEI_RELAXNG = RelaxNG(fromstring(f.read()))\n    # validate XML document\n    return TEI_RELAXNG.validate(xmldoc)\n\n", "idx": 944}
{"namespace": "trafilatura.xml.merge_with_parent", "completion": "    parent = element.getparent()\n    if parent is None:\n        return\n    # handle formatting\n    if include_formatting is True:\n        if element.tag in ('del', 'head'):\n            if element.tag == 'head':\n                try:\n                    number = int(element.get('rend')[1])\n                except (TypeError, ValueError):\n                    number = 2\n                element.text = f'{\"#\" * number} {element.text}'\n            elif element.tag == 'del':\n                element.text = f'~~{element.text}~~'\n        elif element.tag == 'hi':\n            rend = element.get('rend')\n            if rend in HI_FORMATTING:\n                element.text = f'{HI_FORMATTING[rend]}{element.text}{HI_FORMATTING[rend]}'\n    # handle links\n    if element.tag == 'ref':\n        if element.text is not None:\n            link_text = f'[{element.text}]'\n            if element.get('target') is not None:\n                element.text = f\"{link_text}({element.get('target')})\"\n            else:\n                LOGGER.warning('missing link attribute: %s %s', element.text, element.attrib)\n                element.text = link_text\n        else:\n            LOGGER.warning('empty link: %s %s', element.text, element.attrib)\n    # handle text\n    parent.text = (parent.text or '') + replace_element_text(element, include_formatting)\n    # remove element\n    parent.remove(element)\n\n", "idx": 945}
{"namespace": "trafilatura.downloads._determine_headers", "completion": "    # load a series of user-agents\n    myagents, mycookie = _parse_config(config)\n    if myagents is not None:\n        myagent = random.choice(myagents)\n    else:\n        myagent = None\n    if mycookie is not None:\n        mycookie = mycookie.strip()\n    if headers is None:\n        headers = DEFAULT_HEADERS.copy()\n    if myagent is not None:\n        headers['User-Agent'] = myagent\n    if mycookie is not None:\n        headers['Cookie'] = mycookie\n    return headers\n\n", "idx": 946}
{"namespace": "trafilatura.meta.reset_caches", "completion": "    reset_caches_courlan()\n    reset_caches_htmldate()\n    define_stoplist()\n    gc.collect()", "idx": 947}
{"namespace": "trafilatura.core.handle_table", "completion": "    # define new table element\n    newtable = Element('table')\n    # define new row element\n    newrow = Element('row')\n    # iterate through table elements\n    for child in table_elem.iter('*'):\n        # check for thead, tbody, tfoot\n        if child.tag in {'thead', 'tbody', 'tfoot'}:\n            # check if there are any rows in the table\n            if len(newrow) > 0:\n                # append row to table\n                newtable.append(newrow)\n                # reset row\n                newrow = Element('row')\n            # append child to table\n            newtable.append(child)\n            # skip child\n            continue\n        # check for tr\n        if child.tag == 'tr':\n            # check if there are any rows in the table\n            if len(newrow) > 0:\n                # append row to table\n                newtable.append(newrow)\n                # reset row\n                newrow = Element('row')\n            # append child to table\n            newtable.append(child)\n            # skip child\n            continue\n        # check for td or th\n        if child.tag in {'td', 'th'}:\n            # define cell element\n            cell_element = define_cell_type(child)\n            # iterate through child elements\n            for subchild in child.iter('*'):\n                # check for nested table\n                if subchild.tag == 'table':\n                    # skip child\n                    continue\n                # check for nested tr\n                if subchild.tag == 'tr':\n                    # check if there are any rows in the table\n                    if len(newrow) > 0:\n                        # append row to table\n                        newtable.append(newrow)\n                        # reset row\n                        newrow = Element('row')\n                    # append child to table\n                    newtable.append(subchild)\n                    # skip child\n                    continue\n                # check for nested td or th\n                if subchild.tag in {'td', 'th'}:\n                    # define cell element\n                    cell_element = define_cell_type(subchild)\n                    # iterate through subchild elements\n                    for subsubchild in subchild.iter('*'):\n                        # check for nested table\n                        if subsubchild.tag == 'table':\n                            # skip child\n                            continue\n                        # check for nested tr\n                        if subsubchild.tag == 'tr':\n                            # check if there are any rows in the table\n                            if len(", "idx": 948}
{"namespace": "trafilatura.filters.language_filter", "completion": "    # Check if the target language is specified\n    if target_language is None:\n        LOGGER.warning('Target language not specified, skipping language filter')\n        return False, docmeta\n\n    # Detect the language of the text\n    detected_language = language_classifier(temp_text, temp_comments)\n\n    # Check if the detected language is different from the target language\n    if detected_language is not None and detected_language != target_language:\n        LOGGER.warning('Detected language %s is different from the target language %s', detected_language, target_language)\n        return True, docmeta\n\n    return False, docmeta\n\n", "idx": 949}
{"namespace": "trafilatura.filters.textfilter", "completion": "    # Filter out text that matches certain conditions.\n    # This function is called for each element in the HTML tree.\n    # The function returns True if the text should be filtered out, False otherwise.\n\n    # Filter out text that matches certain conditions.\n    # This function is called for each element in the HTML tree.\n    # The function returns True if the text should be filtered out, False otherwise.\n\n    # Filter out text that matches certain conditions.\n    # This function is called for each element in the HTML tree.\n    # The function returns True if the text should be filtered out, False otherwise.\n\n    # Filter out text that matches certain conditions.\n    # This function is called for each element in the HTML tree.\n    # The function returns True if the text should be filtered out, False otherwise.\n\n    # Filter out text that matches certain conditions.\n    # This function is called for each element in the HTML tree.\n    # The function returns True if the text should be filtered out, False otherwise.\n\n    # Filter out text that matches certain conditions.\n    # This function is called for each element in the HTML tree.\n    # The function returns True if the text should be filtered out, False otherwise.\n\n    # Filter out text that matches certain conditions.\n    # This function is called for each element in the HTML tree.\n    # The function returns True if the text should be filtered out, False otherwise.\n\n    # Filter out text that matches certain conditions.\n    # This function is called for each element in the HTML tree.\n    # The function returns True if the text should be filtered out, False otherwise.\n\n    # Filter out text that matches certain conditions.\n    # This function is called for each element in the HTML tree.\n    # The function returns True if the text should be filtered out, False otherwise.\n\n    # Filter out text that matches certain conditions.\n    # This function is called for each element in the HTML tree.\n    # The function returns True if the text should be filtered out, False otherwise.\n\n    # Filter out text that matches certain conditions.\n    # This function is called for each element in the HTML tree.\n    # The function returns True if the text should be filtered out, False otherwise.\n\n    # Filter out text that matches certain conditions.\n    # This function is called for each element in the HTML tree.\n    # The function returns True if the text should be filtered out, False otherwise.\n\n    # Filter out text that matches certain", "idx": 950}
{"namespace": "trafilatura.metadata.extract_meta_json", "completion": "    for element in tree.iter():\n        if element.tag == 'script' and element.get('type') in ('application/ld+json', 'application/settings+json'):\n            try:\n                json_text = element.text.strip()\n                json_text = JSON_MINIFY.sub(r'\\1', json_text)\n                json_object = json.loads(json_text)\n                for key, value in json_object.items():\n                    if key == 'author':\n                        if isinstance(value, list):\n                            value = '; '.join(value)\n                        metadata.author = value\n                    elif key == 'description':\n                        metadata.description = value\n                    elif key == 'image':\n                        metadata.image = value\n                    elif key == 'keywords':\n                        metadata.tags = value\n                    elif key == 'publisher':\n                        metadata.publisher = value\n                    elif key == 'title':\n                        metadata.title = value\n            except json.JSONDecodeError:\n                for key, value in json.loads(json_text).items():\n                    if key == 'author':\n                        metadata.author = value\n                    elif key == 'description':\n                        metadata.description = value\n                    elif key == 'image':\n                        metadata.image = value\n                    elif key == 'keywords':\n                        metadata.tags = value\n                    elif key == 'publisher':\n                        metadata.publisher = value\n                    elif key == 'title':\n                        metadata.title = value\n    return metadata\n\n", "idx": 951}
{"namespace": "trafilatura.external.try_justext", "completion": "    # init body element\n    body = Element('body')\n\n    # determine language\n    if target_language is None:\n        target_language = 'en'\n    if target_language not in jt_stoplist_init():\n        target_language = 'en'\n\n    # init stoplist\n    stoplist = jt_stoplist_init()[target_language]\n\n    # extract paragraphs\n    paragraphs = custom_justext(tree, stoplist)\n\n    # populate body element\n    for paragraph in paragraphs:\n        if paragraph.text:\n            body.append(Element('p', fromstring_bytes(paragraph.text)))\n\n    return body\n\n", "idx": 952}
{"namespace": "mongoengine.base.datastructures.BaseDict.get", "completion": "        if key in self:\n            return self[key]\n        return default\n", "idx": 953}
{"namespace": "sqlite_utils.utils.suggest_column_types", "completion": "    column_types = {}\n    for record in records:\n        for key, value in record.items():\n            column_types[key] = suggest_column_type(value)\n    return column_types\n\n", "idx": 954}
{"namespace": "sqlite_utils.plugins.get_plugins", "completion": "    \"\"\"\n    TThis function retrieves information about the installed plugins. It retrieves the plugins, iterates over them and creates a dictionary for each plugin containing its name and the names of the hooks it implements. It also checks if there is corresponding distribution information for the plugin and includes the version and project name in the dictionary if available. Finally, it appends each plugin dictionary to a list and returns the list.\n    Input-Output Arguments\n    :param: No input parameters.\n    :return: List of dictionaries. Each dictionary contains information about a plugin, including its name, hooks, version (if available), and project name (if available).\n    \"\"\"\n\n    \"\"\"\n    TThis function retrieves information about the installed plugins. It retrieves the plugins, iterates over them and creates a dictionary for each plugin containing its name and the names of the hooks it implements. It also checks if there is corresponding distribution information for the plugin and includes the version and project name in the dictionary if available. Finally, it appends each plugin dictionary to a list and returns the list.\n    Input-Output Arguments\n    :param: No input parameters.\n    :return: List of dictionaries. Each dictionary contains information about a plugin, including its name, hooks, version (if available), and project name (if available).\n    \"\"\"\n\n    \"\"\"\n    TThis function retrieves information about the installed plugins. It retrieves the plugins, iterates over them and creates a dictionary for each plugin containing its name and the names of the hooks it implements. It also checks if there is corresponding distribution information for the plugin and includes the version and project name in the dictionary if available. Finally, it appends each plugin dictionary to a list and returns the list.\n    Input-Output Arguments\n    :param: No input parameters.\n    :return: List of dictionaries. Each dictionary contains information about a plugin, including its name, hooks, version (if available), and project name (if available).\n    \"\"\"\n\n    \"\"\"\n    TThis function retrieves information about the installed plugins. It retrieves the plugins, iterates over them and creates a dictionary for each plugin containing its name and the names of the hooks it implements. It also checks if there is corresponding distribution information for the plugin and includes the version and project name in the dictionary if available. Finally, it appends each plugin dictionary to a list and returns the list.\n    Input-Output Arguments\n    :param: No input parameters.\n    :return: List of dictionaries. Each dictionary contains information", "idx": 955}
{"namespace": "alembic.config.Config.print_stdout", "completion": "        if self.cmd_opts and self.cmd_opts.quiet:\n            return\n        self.stdout.write(text.format(*arg) + \"\\n\")\n", "idx": 956}
{"namespace": "alembic.config.Config.set_section_option", "completion": "        if not self.file_config.has_section(section):\n            self.file_config.add_section(section)\n        self.file_config.set(section, name, value)\n", "idx": 957}
{"namespace": "alembic.command.merge", "completion": "    script_directory = ScriptDirectory.from_config(config)\n\n    command_args = dict(\n        message=message,\n        autogenerate=False,\n        sql=False,\n        head=revisions,\n        splice=False,\n        branch_label=branch_label,\n        version_path=None,\n        rev_id=rev_id,\n        depends_on=None,\n    )\n    revision_context = autogen.RevisionContext(\n        config,\n        script_directory,\n        command_args,\n    )\n\n    def retrieve_migrations(rev, context):\n        revision_context.run_no_autogenerate(rev, context)\n        return []\n\n    with EnvironmentContext(\n        config,\n        script_directory,\n        fn=retrieve_migrations,\n        as_sql=False,\n        template_args=revision_context.template_args,\n        revision_context=revision_context,\n    ):\n        script_directory.run_env()\n\n    # the revision_context now has MigrationScript structure(s) present.\n    # these could theoretically be further processed / rewritten *here*,\n    # in addition to the hooks present within each run_migrations() call,\n    # or at the end of env.py run_migrations_online().\n\n    scripts = [script for script in revision_context.generate_scripts()]\n    if len(scripts) == 1:\n        return scripts[0]\n    else:\n        return scripts", "idx": 958}
{"namespace": "alembic.command.upgrade", "completion": "    script_directory = ScriptDirectory.from_config(config)\n\n    command_args = dict(\n        revision=revision,\n        sql=sql,\n        tag=tag,\n    )\n    upgrade_context = autogen.UpgradeContext(\n        config,\n        script_directory,\n        command_args,\n    )\n\n    def retrieve_migrations(rev, context):\n        upgrade_context.run_upgrade(rev, context)\n        return []\n\n    with EnvironmentContext(\n        config,\n        script_directory,\n        fn=retrieve_migrations,\n        as_sql=sql,\n        template_args=upgrade_context.template_args,\n        upgrade_context=upgrade_context,\n    ):\n        script_directory.run_env()\n\n    # the upgrade_context now has MigrationScript structure(s) present.\n    # these could theoretically be further processed / rewritten *here*,\n    # in addition to the hooks present within each run_migrations() call,\n    # or at the end of env.py run_migrations_online().\n\n", "idx": 959}
{"namespace": "alembic.command.downgrade", "completion": "    script = ScriptDirectory.from_config(config)\n\n    starting_rev = None\n    if \":\" in revision:\n        if not sql:\n            raise util.CommandError(\"Range revision not allowed\")\n        starting_rev, revision = revision.split(\":\", 2)\n\n    def downgrade(rev, context):\n        return script._downgrade_revs(revision, rev)\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=downgrade,\n        as_sql=sql,\n        starting_rev=starting_rev,\n        destination_rev=revision,\n        tag=tag,\n    ):\n        script.run_env()\n\n", "idx": 960}
{"namespace": "alembic.command.history", "completion": "    script = ScriptDirectory.from_config(config)\n\n    def history(rev, context):\n        return script.get_revisions(rev_range)\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=history,\n        as_sql=False,\n        verbose=verbose,\n        indicate_current=indicate_current,\n    ):\n        script.run_env()\n\n", "idx": 961}
{"namespace": "alembic.command.stamp", "completion": "    script = ScriptDirectory.from_config(config)\n\n    def stamp(rev, context):\n        return script._stamp_revs(revision, rev, purge)\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=stamp,\n        as_sql=sql,\n        tag=tag,\n    ):\n        script.run_env()\n\n", "idx": 962}
{"namespace": "alembic.command.ensure_version", "completion": "    script = ScriptDirectory.from_config(config)\n\n    def ensure_version_table(rev, context):\n        return script._ensure_version_table(rev)\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=ensure_version_table,\n        as_sql=sql,\n    ):\n        script.run_env()", "idx": 963}
{"namespace": "alembic.autogenerate.compare._compare_server_default", "completion": "    metadata_default = _render_server_default_for_compare(\n        metadata_col.server_default, autogen_context\n    )\n    conn_default = _render_server_default_for_compare(\n        conn_col.server_default, autogen_context\n    )\n\n    if metadata_default != conn_default:\n        alter_column_op.modify_server_default = metadata_default\n        log.info(\n            \"Detected server default change from %r to %r on '%s.%s'\",\n            conn_default,\n            metadata_default,\n            tname,\n            cname,\n        )\n        return True\n    else:\n        return False\n\n", "idx": 964}
{"namespace": "alembic.autogenerate.render._render_server_default", "completion": "    rendered = _user_defined_render(\"server_default\", default, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    if isinstance(default, FetchedValue):\n        return \"FetchedValue()\"\n\n    if isinstance(default, str):\n        return repr(default)\n\n    if isinstance(default, TextClause):\n        return repr(default)\n\n    if isinstance(default, ColumnElement):\n        return repr(default)\n\n    if sqla_compat._server_default_is_computed(default):\n        return \"Computed(%r)\" % default\n\n    if sqla_compat._server_default_is_identity(default):\n        return \"Identity(%r)\" % default\n\n    if isinstance(default, str):\n        return repr(default)\n\n    return None\n\n", "idx": 965}
{"namespace": "alembic.autogenerate.render._render_constraint", "completion": "    renderer = _constraint_renderers.dispatch(constraint)\n    if renderer is not None:\n        return renderer(constraint, autogen_context, namespace_metadata)\n    else:\n        return \"Unknown constraint: %r\" % constraint\n\n", "idx": 966}
{"namespace": "alembic.autogenerate.render._render_unique_constraint", "completion": "    rendered = _user_defined_render(\"unique_constraint\", constraint, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    opts = []\n    if constraint.name:\n        opts.append(\n            (\"name\", repr(_render_gen_name(autogen_context, constraint.name)))\n        )\n\n    return \"%(prefix)sUniqueConstraint(%(args)s)\" % {\n        \"prefix\": _sqlalchemy_autogenerate_prefix(autogen_context),\n        \"args\": \", \".join(\n            [repr(c.name) for c in constraint.columns]\n            + [\"%s=%s\" % (kwname, val) for kwname, val in opts]\n        ),\n    }\n\n", "idx": 967}
{"namespace": "alembic.autogenerate.render._render_check_constraint", "completion": "    rendered = _user_defined_render(\"check\", constraint, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    if constraint.parent is not None and constraint.parent.table is not None:\n        return None\n\n    opts = []\n    if constraint.name:\n        opts.append(\n            (\"name\", repr(_render_gen_name(autogen_context, constraint.name)))\n        )\n    return \"%(prefix)sCheckConstraint(%(args)s)\" % {\n        \"prefix\": _sqlalchemy_autogenerate_prefix(autogen_context),\n        \"args\": \", \".join(\n            [repr(c.name) for c in constraint.elements]\n            + [\"%s=%s\" % (kwname, val) for kwname, val in opts]\n        ),\n    }\n\n", "idx": 968}
{"namespace": "alembic.autogenerate.api.compare_metadata", "completion": "    return context.compare_metadata(metadata)\n\n", "idx": 969}
{"namespace": "alembic.autogenerate.api.AutogenContext._within_batch", "completion": "        self._has_batch = True\n        yield\n        self._has_batch = False\n", "idx": 970}
{"namespace": "alembic.util.sqla_compat._connectable_has_table", "completion": "    if sqla_14:\n        return connectable.dialect.has_table(connectable, tablename, schemaname)\n    else:\n        return connectable.dialect.has_table(connectable, tablename)\n\n", "idx": 971}
{"namespace": "alembic.util.sqla_compat._get_constraint_final_name", "completion": "    if constraint.name is not None:\n        return constraint.name\n\n    if sqla_14:\n        return constraint.dialect_name(dialect)\n\n    if dialect is None:\n        return None\n\n    if dialect.name == \"sqlite\":\n        return constraint.name\n\n    if dialect.name == \"postgresql\":\n        return dialect.identifier_preparer.format_constraint_name(\n            constraint.name\n        )\n\n    if dialect.name == \"mysql\":\n        return dialect.identifier_preparer.format_constraint_name(\n            constraint.name\n        )\n\n    if dialect.name == \"mssql\":\n        return dialect.identifier_preparer.format_constraint_name(\n            constraint.name\n        )\n\n    if dialect.name == \"oracle\":\n        return dialect.identifier_preparer.format_constraint_name(\n            constraint.name\n        )\n\n    if dialect.name == \"mssql\":\n        return dialect.identifier_preparer.format_constraint_name(\n            constraint.name\n        )\n\n    if dialect.name == \"firebird\":\n        return dialect.identifier_preparer.format_constraint_name(\n            constraint.name\n        )\n\n    if dialect.name == \"sybase\":\n        return dialect.identifier_preparer.format_constraint_name(\n            constraint.name\n        )\n\n    if dialect.name == \"ibm_db_sa\":\n        return dialect.identifier_preparer.format_constraint_name(\n            constraint.name\n        )\n\n    if dialect.name == \"informix\":\n        return dialect.identifier_preparer.format_constraint_name(\n            constraint.name\n        )\n\n    if dialect.name == \"maxdb\":\n        return dialect.identifier_preparer.format_constraint_name(\n            constraint.name\n        )\n\n    if dialect.name == \"greenplum\":\n        return dialect.identifier_preparer.format_constraint_name(\n            constraint.name\n        )\n\n    if dialect.name == \"redshift\":\n        return dialect.identifier_preparer.format_constraint_name(\n            constraint.name\n        )\n\n    if dialect.name == \"snowflake\":\n        return dialect.identifier_preparer.format_constraint_name(\n            constraint.name\n        )\n\n    if dialect.name == \"exasol\":\n        return dialect.identifier_preparer.format_constraint_name(\n           ", "idx": 972}
{"namespace": "alembic.testing.env.env_file_fixture", "completion": "    dir_ = os.path.join(_get_staging_directory(), \"scripts\")\n    path = os.path.join(dir_, \"env.py\")\n    with open(path, \"w\") as f:\n        f.write(txt)\n\n", "idx": 973}
{"namespace": "alembic.testing.env._no_sql_testing_config", "completion": "    dir_ = os.path.join(_get_staging_directory(), \"scripts\")\n    url = \"postgresql:///%s/foo\" % dir_\n\n    return _write_config_file(\n        \"\"\"", "idx": 974}
{"namespace": "alembic.testing.env._write_config_file", "completion": "    cfg = _testing_config()\n    path = os.path.join(_get_staging_directory(), \"scripts\", \"alembic.ini\")\n    with open(path, \"w\") as f:\n        f.write(text)\n    return cfg\n\n", "idx": 975}
{"namespace": "alembic.testing.env.three_rev_fixture", "completion": "    scriptdir = ScriptDirectory.from_config(cfg)\n    a = scriptdir.generate_revision(\n        \"a\",\n        \"create table foo (id integer primary key)\",\n        \"drop table foo\",\n    )\n    b = scriptdir.generate_revision(\n        \"b\",\n        \"create table bar (id integer primary key)\",\n        \"drop table bar\",\n    )\n    c = scriptdir.generate_revision(\n        \"c\",\n        \"create table baz (id integer primary key)\",\n        \"drop table baz\",\n    )\n    return a, b, c\n\n", "idx": 976}
{"namespace": "alembic.testing.env.multi_heads_fixture", "completion": "    d = util.rev_id()\n    e = util.rev_id()\n    f = util.rev_id()\n\n    script = ScriptDirectory.from_config(cfg)\n    script.generate_revision(d, \"revision d\", refresh=True, head=a)\n    write_script(\n        script,\n        d,\n        \"\"\"\\", "idx": 977}
{"namespace": "alembic.testing.fixtures.capture_db", "completion": "    engine = mock.create_mock_engine(dialect)\n    buffer = io.StringIO()\n    with engine.connect() as conn:\n        conn.execution_options(stream_results=True)\n        conn.execute_string = buffer.write\n        return engine, buffer\n\n", "idx": 978}
{"namespace": "alembic.testing.fixtures.capture_engine_context_buffer", "completion": "    if kw.pop(\"bytes_io\", False):\n        buf = io.BytesIO()\n    else:\n        buf = io.StringIO()\n\n    kw.update({\"dialect_name\": \"sqlite\", \"output_buffer\": buf})\n    conf = EnvironmentContext.configure\n\n    def configure(*arg, **opt):\n        opt.update(**kw)\n        return conf(*arg, **opt)\n\n    with mock.patch.object(EnvironmentContext, \"configure\", configure):\n        yield buf\n\n", "idx": 979}
{"namespace": "alembic.operations.schemaobj.SchemaObjects.unique_constraint", "completion": "        m = self.metadata()\n        t = sa_schema.Table(\n            source,\n            m,\n            *[sa_schema.Column(n, NULLTYPE) for n in local_cols],\n            schema=schema,\n        )\n        u = sa_schema.UniqueConstraint(\n            *[t.c[n] for n in local_cols], name=name, **kw\n        )\n        t.append_constraint(u)\n        return u\n", "idx": 980}
{"namespace": "alembic.operations.schemaobj.SchemaObjects.index", "completion": "        m = self.metadata()\n        t = sa_schema.Table(tablename, m, schema=schema)\n        i = sa_schema.Index(name, *[t.c[c] for c in columns], **kw)\n        t.append_constraint(i)\n        return i\n", "idx": 981}
{"namespace": "alembic.operations.ops.DropConstraintOp.from_constraint", "completion": "        if isinstance(constraint, ForeignKeyConstraint):\n            return cls(\n                constraint.name,\n                constraint.table.name,\n                \"foreignkey\",\n                schema=constraint.table.schema,\n            )\n        else:\n            return cls(\n                constraint.name,\n                constraint.table.name,\n                schema=constraint.table.schema,\n            )\n", "idx": 982}
{"namespace": "alembic.operations.ops.DropConstraintOp.to_constraint", "completion": "        if self._reverse is not None:\n            constraint = self._reverse.to_constraint()\n            constraint.name = self.constraint_name\n            constraint.table = sqla_compat._table_for_constraint(constraint)\n            constraint.schema = self.schema\n            return constraint\n        else:\n            raise ValueError(\"No reverse operation found\")\n\n", "idx": 983}
{"namespace": "alembic.operations.ops.CreatePrimaryKeyOp.to_constraint", "completion": "        if migration_context is None:\n            migration_context = MigrationContext()\n\n        schema_obj = migration_context.get_schema_obj(self.schema)\n        table_obj = schema_obj.get_table_obj(self.table_name)\n        return PrimaryKeyConstraint(\n            *self.columns,\n            name=self.constraint_name,\n            schema=schema_obj,\n            table=table_obj,\n            **self.kw,\n        )\n", "idx": 984}
{"namespace": "alembic.operations.ops.CreateIndexOp.from_index", "completion": "        index_name = sqla_compat.index_name_or_none(index.name)\n        table_name = index.table.name\n        columns = [\n            sqla_compat.column_name_or_none(c.name)\n            for c in index.columns\n            if c.name is not None\n        ]\n        schema = index.table.schema\n        unique = index.unique\n        if_not_exists = index.if_not_exists\n        kw = index.dialect_kwargs\n        return cls(\n            index_name,\n            table_name,\n            columns,\n            schema=schema,\n            unique=unique,\n            if_not_exists=if_not_exists,\n            **kw,\n        )\n", "idx": 985}
{"namespace": "alembic.operations.ops.DropIndexOp.from_index", "completion": "        assert index.table is not None\n        return cls(\n            index.name,  # type: ignore[arg-type]\n            index.table.name,\n            schema=index.table.schema,\n            if_exists=index.if_exists,\n            _reverse=CreateIndexOp.from_index(index),\n        )\n", "idx": 986}
{"namespace": "alembic.operations.ops.DropIndexOp.to_index", "completion": "        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.index(\n            self.index_name,\n            self.table_name,\n            [],\n            schema=self.schema,\n            **self.kw,\n        )\n", "idx": 987}
{"namespace": "alembic.operations.ops.CreateTableOp.from_table", "completion": "        return cls(\n            table.name,\n            table.columns,\n            schema=table.schema,\n            _namespace_metadata=_namespace_metadata,\n            **table.dialect_kwargs,\n        )\n", "idx": 988}
{"namespace": "alembic.operations.ops.DropTableOp.from_table", "completion": "        if _namespace_metadata is None:\n            _namespace_metadata = table.metadata\n\n        return cls(\n            table.name,\n            schema=table.schema,\n            table_kw=table.kwargs,\n            _reverse=CreateTableOp.from_table(table, _namespace_metadata=_namespace_metadata),\n            comment=table.comment,\n            info=dict(table.info),\n            prefixes=list(table._prefixes),\n        )\n", "idx": 989}
{"namespace": "alembic.operations.ops.DropTableOp.to_table", "completion": "        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.table(\n            self.table_name,\n            schema=self.schema,\n            comment=self.comment,\n            info=self.info,\n            prefixes=list(self.prefixes) if self.prefixes else [],\n            **self.table_kw,\n        )\n", "idx": 990}
{"namespace": "alembic.operations.ops.AlterColumnOp.to_diff_tuple", "completion": "        return (\n            \"alter_column\",\n            self.to_table(),\n            self.column_name,\n            self.existing_type,\n            self.existing_server_default,\n            self.existing_nullable,\n            self.existing_comment,\n            self.modify_nullable,\n            self.modify_comment,\n            self.modify_server_default,\n            self.modify_name,\n            self.modify_type,\n        )\n", "idx": 991}
{"namespace": "alembic.operations.ops.AddColumnOp.reverse", "completion": "        return DropColumnOp(\n            self.table_name,\n            self.column.name,\n            schema=self.schema,\n            existing_type=self.column.type,\n            existing_server_default=self.column.server_default,\n            existing_nullable=self.column.nullable,\n            existing_comment=self.column.comment,\n        )\n", "idx": 992}
{"namespace": "alembic.operations.ops.DropColumnOp.reverse", "completion": "        if self._reverse is None:\n            raise ValueError(\n                \"Cannot reverse operation, \"\n                \"please provide a reverse operation\"\n            )\n        return self._reverse\n", "idx": 993}
{"namespace": "alembic.operations.ops.DropColumnOp.from_column_and_tablename", "completion": "        return cls(tname, col.name, schema=schema, _reverse=col)\n", "idx": 994}
{"namespace": "alembic.operations.ops.DropColumnOp.to_column", "completion": "        if self._reverse is not None:\n            return self._reverse.column\n\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.column(\n            self.column_name,\n            schema=self.schema,\n            type_=sqla_compat._NULLTYPE,\n        )\n", "idx": 995}
{"namespace": "alembic.script.revision.RevisionMap.heads", "completion": "        self.init()\n        return tuple(self._heads)\n", "idx": 996}
{"namespace": "alembic.script.revision.RevisionMap.add_revision", "completion": "        if revision.revision in self._revision_map:\n            if _replace:\n                self._revision_map[revision.revision] = revision\n            else:\n                raise RevisionError(\n                    \"Revision %s is present more than once\" % revision.revision\n                )\n        else:\n            self._revision_map[revision.revision] = revision\n\n        if revision.branch_labels:\n            assert revision._orig_branch_labels is not None\n            for branch_label in revision._orig_branch_labels:\n                if branch_label in self._revision_map:\n                    raise RevisionError(\n                        \"Branch name '%s' in revision %s already \"\n                        \"used by revision %s\"\n                        % (\n                            branch_label,\n                            revision.revision,\n                            self._revision_map[branch_label].revision,\n                        )\n                    )\n                self._revision_map[branch_label] = revision\n\n        if revision.dependencies:\n            deps = [\n                self._revision_map[dep] for dep in util.to_tuple(revision.dependencies)\n            ]\n            revision._resolved_dependencies = tuple(\n                [d.revision for d in deps if d is not None]\n            )\n        else:\n            revision._resolved_dependencies = ()\n\n        if revision._resolved_dependencies:\n            normalized_resolved = set(revision._resolved_dependencies)\n            for rev in self._get_ancestor_nodes(\n                [revision],\n                include_dependencies=False,\n                map_=cast(_RevisionMapType, self._revision_map),\n            ):\n                if rev is revision:\n                    continue\n                elif rev._resolved_dependencies:\n                    normalized_resolved.difference_update(\n                        rev._resolved_dependencies\n                    )\n\n            revision._normalized_resolved_dependencies = tuple(normalized_resolved)\n        else:\n            revision._normalized_resolved_dependencies = ()\n\n        if revision.down_revision:\n            down_revision = self._revision_map[revision.down_revision]\n            down_revision.add_nextrev(revision)\n            if revision.revision in down_revision._versioned_down_revisions:\n                self.heads.remove(revision.revision)\n            self._real_heads.remove(revision.revision)\n\n        if revision.is_base:\n            self.bases += (revision,)\n        if revision._is_real_base:\n            self._real_bases += (revision,)\n\n        self._add_depends_on", "idx": 997}
{"namespace": "alembic.script.revision.RevisionMap.get_revisions", "completion": "        if id_ is None:\n            return ()\n        elif isinstance(id_, str):\n            return self.get_revision(id_)\n        elif isinstance(id_, Iterable):\n            return tuple(self.get_revision(id_) for id_ in id_)\n        else:\n            raise TypeError(\n                \"id_ must be a string, a sequence of strings, or None\"\n            )\n", "idx": 998}
{"namespace": "alembic.script.revision.RevisionMap.get_revision", "completion": "        if id_ is None:\n            return None\n        else:\n            resolved_id, branch_label = self._resolve_revision_number(id_)\n            if len(resolved_id) == 1:\n                try:\n                    rint = int(resolved_id[0])\n                    if rint < 0:\n                        # branch@-n -> walk down from heads\n                        select_heads = self.get_revisions(\"heads\")\n                        if branch_label is not None:\n                            select_heads = tuple(\n                                head\n                                for head in select_heads\n                                if branch_label\n                                in is_revision(head).branch_labels\n                            )\n                        return self._walk(select_heads[0], steps=rint)\n                except ValueError:\n                    # couldn't resolve as integer\n                    pass\n            if len(resolved_id) > 1:\n                raise MultipleHeads(resolved_id, id_)\n            return self._revision_for_ident(resolved_id[0], branch_label)\n", "idx": 999}
{"namespace": "alembic.script.revision.RevisionMap.filter_for_lineage", "completion": "        targets = set(targets)\n        if check_against:\n            check_against = self._resolve_revision_number(check_against)[0]\n        else:\n            check_against = ()\n\n        if include_dependencies:\n            targets = targets.union(\n                self._get_descendant_nodes(targets, include_dependencies=True)\n            )\n\n        if check_against:\n            targets = targets.intersection(\n                self._get_ancestor_nodes(targets, include_dependencies=False)\n            )\n\n        return tuple(targets)\n", "idx": 1000}
{"namespace": "alembic.script.revision.RevisionMap.iterate_revisions", "completion": "        if inclusive:\n            inclusive = True\n        else:\n            inclusive = False\n\n        if implicit_base:\n            implicit_base = True\n        else:\n            implicit_base = False\n\n        if assert_relative_length:\n            assert_relative_length = True\n        else:\n            assert_relative_length = False\n\n        if select_for_downgrade:\n            select_for_downgrade = True\n        else:\n            select_for_downgrade = False\n\n        if not isinstance(upper, (tuple, list)):\n            upper = (upper,)\n\n        if not isinstance(lower, (tuple, list)):\n            lower = (lower,)\n\n        if not upper:\n            upper = (\"base\",)\n\n        if not lower:\n            lower = (\"base\",)\n\n        if not isinstance(upper, (tuple, list)):\n            upper = (upper,)\n\n        if not isinstance(lower, (tuple, list)):\n            lower = (lower,)\n\n        if not upper:\n            upper = (\"base\",)\n\n        if not lower:\n            lower = (\"base\",)\n\n        if not isinstance(upper, (tuple, list)):\n            upper = (upper,)\n\n        if not isinstance(lower, (tuple, list)):\n            lower = (lower,)\n\n        if not upper:\n            upper = (\"base\",)\n\n        if not lower:\n            lower = (\"base\",)\n\n        if not isinstance(upper, (tuple, list)):\n            upper = (upper,)\n\n        if not isinstance(lower, (tuple, list)):\n            lower = (lower,)\n\n        if not upper:\n            upper = (\"base\",)\n\n        if not lower:\n            lower = (\"base\",)\n\n        if not isinstance(upper, (tuple, list)):\n            upper = (upper,)\n\n        if not isinstance(lower, (tuple, list)):\n            lower = (lower,)\n\n        if not upper:\n            upper = (\"base\",)\n\n        if not lower:\n            lower = (\"base\",)\n\n        if not isinstance(upper, (tuple, list)):\n            upper = (upper,)\n\n        if not isinstance(lower, (tuple, list)):\n            lower = (lower,)\n\n        if not upper:\n            upper = (\"base\",)\n\n        if not lower:\n            lower = (\"base\",)\n\n        if not isinstance(upper, (tuple, list)):\n            upper = (upper,)\n\n        if not isinstance(lower, (tuple, list)):\n            lower = (lower,)\n\n        if not upper:\n            upper = (\"base\",", "idx": 1001}
{"namespace": "alembic.script.revision.RevisionMap._topological_sort", "completion": "        # Create a dictionary to store the in-degree of each revision.\n        in_degree = {}\n        for revision in revisions:\n            in_degree[revision.revision] = 0\n\n        # Calculate the in-degree of each revision.\n        for revision in revisions:\n            for dependency in revision._normalized_resolved_dependencies:\n                in_degree[dependency] += 1\n\n        # Create a queue to store the revisions with in-degree 0.\n        queue = collections.deque()\n        for revision in revisions:\n            if in_degree[revision.revision] == 0:\n                queue.append(revision.revision)\n\n        # Perform the topological sort.\n        sorted_order = []\n        while queue:\n            revision = queue.popleft()\n            sorted_order.append(revision)\n            for next_revision in self._revision_map[revision].nextrev:\n                in_degree[next_revision] -= 1\n                if in_degree[next_revision] == 0:\n                    queue.append(next_revision)\n\n        # Check for cycles.\n        if len(sorted_order) != len(revisions):\n            raise CycleDetected(sorted_order)\n\n        return sorted_order\n", "idx": 1002}
{"namespace": "alembic.script.revision.Revision._all_down_revisions", "completion": "        return tuple(\n            set(\n                self._versioned_down_revisions\n                + self._resolved_dependencies\n                + self._normalized_resolved_dependencies\n            )\n        )\n", "idx": 1003}
{"namespace": "alembic.script.revision.Revision._normalized_down_revisions", "completion": "        return util.dedupe_tuple(\n            util.to_tuple(self.down_revision, default=())\n            + self._normalized_resolved_dependencies\n        )\n", "idx": 1004}
{"namespace": "alembic.script.write_hooks._invoke", "completion": "    if name not in _registry:\n        raise util.command.CommandError(\n            f\"No formatter with name '{name}' registered\"\n        )\n\n    return _registry[name](revision, options)\n\n", "idx": 1005}
{"namespace": "bplustree.memory.FileMemory.get_node", "completion": "        # Check if the node is present in the cache\n        if page in self._cache:\n            return self._cache[page]\n\n        # Retrieve the data from the storage\n        data = read_from_file(self._fd, page * self._tree_conf.page_size,\n                              (page + 1) * self._tree_conf.page_size)\n\n        # Create a Node object using the data\n        node = Node(data, self._tree_conf)\n\n        # Add the node to the cache\n        self._cache[page] = node\n\n        return node\n", "idx": 1006}
{"namespace": "bplustree.memory.FileMemory.next_available_page", "completion": "        self._fd.seek(0, io.SEEK_END)\n        last_byte = self._fd.tell()\n        self.last_page = int(last_byte / self._tree_conf.page_size)\n        return self.last_page\n", "idx": 1007}
{"namespace": "bplustree.memory.FileMemory.get_metadata", "completion": "        # Read the first page of the file\n        data = self._read_page(0)\n\n        # Extract the root node page, page size, order, key size, and value size\n        root_node_page = int.from_bytes(data[0:4], byteorder='little')\n        page_size = int.from_bytes(data[4:8], byteorder='little')\n        order = int.from_bytes(data[8:12], byteorder='little')\n        key_size = int.from_bytes(data[12:16], byteorder='little')\n        value_size = int.from_bytes(data[16:20], byteorder='little')\n\n        # Create a TreeConf object with the extracted values\n        tree_conf = TreeConf(page_size, order, key_size, value_size)\n\n        # Return the root node page and the TreeConf object\n        return root_node_page, tree_conf\n", "idx": 1008}
{"namespace": "bplustree.memory.FileMemory.set_metadata", "completion": "        data = bytes()\n        data += root_node_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN)\n        data += tree_conf.page_size.to_bytes(OTHERS_BYTES, ENDIAN)\n        data += tree_conf.order.to_bytes(OTHERS_BYTES, ENDIAN)\n        data += tree_conf.key_size.to_bytes(OTHERS_BYTES, ENDIAN)\n        data += tree_conf.value_size.to_bytes(OTHERS_BYTES, ENDIAN)\n        self._write_page(0, data)\n        self._tree_conf = tree_conf\n", "idx": 1009}
{"namespace": "bplustree.memory.WAL.checkpoint", "completion": "        if self._not_committed_pages:\n            logger.warning('There are uncommitted pages, '\n                           'the B+Tree was not closed properly')\n\n        fsync_file_and_dir(self._fd.fileno(), self._dir_fd)\n\n        for page, data in self._committed_pages.items():\n            yield page, data\n\n        self._fd.close()\n        os.remove(self.filename)\n        if self._dir_fd is not None:\n            fsync_file_and_dir(self._dir_fd)\n", "idx": 1010}
{"namespace": "bplustree.memory.WAL.commit", "completion": "        self._add_frame(FrameType.COMMIT)\n", "idx": 1011}
{"namespace": "bplustree.memory.WAL.rollback", "completion": "        if self._not_committed_pages:\n            self._add_frame(FrameType.ROLLBACK)", "idx": 1012}
{"namespace": "bplustree.entry.Record.dump", "completion": "        key = self._tree_conf.serializer.serialize(self.key)\n        used_key_length = len(key)\n        end_used_key_length = USED_KEY_LENGTH_BYTES\n        used_key_length_bytes = used_key_length.to_bytes(\n            end_used_key_length, ENDIAN\n        )\n\n        used_value_length = 0\n        end_used_value_length = end_used_key_length + used_key_length\n        if self.value:\n            used_value_length = len(self.value)\n            end_used_value_length += used_value_length\n            used_value_length_bytes = used_value_length.to_bytes(\n                USED_VALUE_LENGTH_BYTES, ENDIAN\n            )\n        else:\n            used_value_length_bytes = b'\\x00' * USED_VALUE_LENGTH_BYTES\n\n        overflow_page = 0\n        end_overflow = end_used_value_length + self._tree_conf.value_size\n        if self.overflow_page:\n            overflow_page = self.overflow_page\n            end_overflow += PAGE_REFERENCE_BYTES\n            overflow_page_bytes = overflow_page.to_bytes(\n                PAGE_REFERENCE_BYTES, ENDIAN\n            )\n        else:\n            overflow_page_bytes = b'\\x00' * PAGE_REFERENCE_BYTES\n\n        return (\n            used_key_length_bytes + key +\n            used_value_length_bytes +\n            (b'\\x00' * (self._tree_conf.value_size - used_value_length)) +\n            overflow_page_bytes\n        )", "idx": 1013}
{"namespace": "bplustree.entry.Reference.__repr__", "completion": "        return '<Reference: key={} before={} after={}>'.format(\n            self.key, self.before, self.after\n        )", "idx": 1014}
{"namespace": "bplustree.node.Node.dump", "completion": "        data = bytearray(self._tree_conf.page_size)\n        end_used_page_length = NODE_TYPE_BYTES + USED_PAGE_LENGTH_BYTES\n        used_page_length = 0\n        for entry in self.entries:\n            entry_data = entry.dump()\n            data[used_page_length:used_page_length+len(entry_data)] = entry_data\n            used_page_length += len(entry_data)\n\n        data[NODE_TYPE_BYTES:end_used_page_length] = int(self._node_type_int).to_bytes(\n            NODE_TYPE_BYTES, ENDIAN\n        )\n        data[end_used_page_length:end_used_page_length+USED_PAGE_LENGTH_BYTES] = used_page_length.to_bytes(\n            USED_PAGE_LENGTH_BYTES, ENDIAN\n        )\n        if self.next_page:\n            data[end_used_page_length+USED_PAGE_LENGTH_BYTES:end_used_page_length+PAGE_REFERENCE_BYTES] = self.next_page.to_bytes(\n                PAGE_REFERENCE_BYTES, ENDIAN\n            )\n        else:\n            data[end_used_page_length+USED_PAGE_LENGTH_BYTES:end_used_page_length+PAGE_REFERENCE_BYTES] = 0.to_bytes(\n                PAGE_REFERENCE_BYTES, ENDIAN\n            )\n\n        return data\n", "idx": 1015}
{"namespace": "bplustree.node.Node._find_entry_index", "completion": "        entry = self._entry_class(self._tree_conf, key=key)\n        return bisect.bisect_left(self.entries, entry)\n\n", "idx": 1016}
{"namespace": "bplustree.node.Node.from_page_data", "completion": "        node_type = int.from_bytes(data[0:1], ENDIAN)\n        if node_type == 0:\n            return cls(tree_conf, data=data, page=page)\n        elif node_type == 1:\n            return cls(tree_conf, data=data, page=page)\n        elif node_type == 2:\n            return cls(tree_conf, data=data, page=page)\n        else:\n            raise ValueError('Unknown node type {}'.format(node_type))\n", "idx": 1017}
{"namespace": "bplustree.tree.BPlusTree._root_node", "completion": "        with self._mem.read_transaction:\n            return self._mem.get_node(self._root_node_page)\n", "idx": 1018}
{"namespace": "bplustree.tree.BPlusTree._left_record_node", "completion": "        node = self._root_node\n        while True:\n            if isinstance(node, LonelyRootNode):\n                return node\n            elif isinstance(node, LeafNode):\n                return node\n            else:\n                node = self._mem.get_node(node.children[0])\n", "idx": 1019}
{"namespace": "mopidy.ext.Extension.get_cache_dir", "completion": "        cache_dir = config[\"paths\"][\"cache_dir\"]\n        cache_dir = path.Path(cache_dir)\n        cache_dir.mkdir(parents=True, exist_ok=True)\n        return cache_dir\n", "idx": 1020}
{"namespace": "mopidy.ext.Extension.get_config_dir", "completion": "        if cls.ext_name is None:\n            raise AssertionError\n        config_dir_path = (\n            path.expand_path(config[\"core\"][\"config_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(config_dir_path)\n        return config_dir_path\n", "idx": 1021}
{"namespace": "mopidy.ext.Extension.get_data_dir", "completion": "        if cls.ext_name is None:\n            raise AssertionError\n        data_dir_path = (\n            path.expand_path(config[\"core\"][\"data_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(data_dir_path)\n        return data_dir_path\n", "idx": 1022}
{"namespace": "mopidy.ext.load_extensions", "completion": "    installed_extensions = []\n    for entry_point in pkg_resources.iter_entry_points(\"mopidy.ext\"):\n        try:\n            extension_class = entry_point.load()\n        except Exception as e:\n            logger.warning(\n                \"Failed to load extension %s: %s\", entry_point.name, e\n            )\n            continue\n        if not issubclass(extension_class, Extension):\n            logger.warning(\n                \"Extension %s does not inherit from Extension\",\n                extension_class.__name__,\n            )\n            continue\n        extension_data = ExtensionData(\n            extension=extension_class,\n            entry_point=entry_point,\n            config_schema=extension_class.get_config_schema(),\n            config_defaults=extension_class.get_default_config(),\n            command=extension_class.get_command(),\n        )\n        installed_extensions.append(extension_data)\n    return installed_extensions", "idx": 1023}
{"namespace": "mopidy.ext.validate_extension_data", "completion": "    # Check if the entry point name matches the extension name.\n    if data.entry_point.name != data.extension.ext_name:\n        logger.error(\n            \"Extension %s has an invalid entry point name: %s\",\n            data.extension.ext_name,\n            data.entry_point.name,\n        )\n        return False\n\n    # Check if the required dependencies are installed.\n    if data.extension.dist_name:\n        try:\n            pkg_resources.get_distribution(data.extension.dist_name)\n        except pkg_resources.DistributionNotFound:\n            logger.error(\n                \"Extension %s requires %s, but it is not installed\",\n                data.extension.ext_name,\n                data.extension.dist_name,\n            )\n            return False\n\n    # Check if the environment is valid.\n    try:\n        data.extension.validate_environment()\n    except Exception as e:\n        logger.error(\n            \"Extension %s failed to validate environment: %s\",\n            data.extension.ext_name,\n            e,\n        )\n        return False\n\n    # Check if the extension has a valid config schema and default config.\n    try:\n        data.config_schema.validate(data.config_defaults)\n    except Exception as e:\n        logger.error(\n            \"Extension %s has an invalid config schema: %s\",\n            data.extension.ext_name,\n            e,\n        )\n        return False\n\n    return True", "idx": 1024}
{"namespace": "mopidy.httpclient.format_user_agent", "completion": "    import mopidy\n    import platform\n    import sys\n\n    if name is None:\n        name = \"Mopidy\"\n\n    return f\"{name}/{mopidy.__version__} Python/{sys.version} {platform.system()}/{platform.release()}\"", "idx": 1025}
{"namespace": "mopidy.models.immutable.ValidatedImmutableObject.replace", "completion": "        other = copy.copy(self)\n        for key, value in kwargs.items():\n            if not self._is_valid_field(key):\n                raise TypeError(\n                    f\"replace() got an unexpected keyword argument {key!r}\"\n                )\n            other._set_field(key, value)\n        return other\n", "idx": 1026}
{"namespace": "mopidy.http.Extension.get_default_config", "completion": "        config_data = {}\n        config_file = os.path.join(os.path.dirname(__file__), \"ext.conf\")\n        if os.path.exists(config_file):\n            with open(config_file) as f:\n                config_data = config_lib.parse_config(f)\n        return config_data\n", "idx": 1027}
{"namespace": "mopidy.http.Extension.get_config_schema", "completion": "        schema = super(Extension, self).get_config_schema()\n        schema[\"http_port\"] = config_lib.Integer(minimum=1, maximum=65535, default=8000, title=\"HTTP Port\")\n        schema[\"http_host\"] = config_lib.String(default=\"0.0.0.0\", title=\"HTTP Host\")\n        schema[\"http_ssl\"] = config_lib.Boolean(default=False, title=\"Use SSL\")\n        schema[\"http_cert\"] = config_lib.String(default=\"\", title=\"SSL Certificate\")\n        schema[\"http_key\"] = config_lib.String(default=\"\", title=\"SSL Key\")\n        schema[\"http_ca\"] = config_lib.String(default=\"\", title=\"SSL CA\")\n        schema[\"http_cert_path\"] = config_lib.String(default=\"\", title=\"SSL Certificate Path\")\n        schema[\"http_key_path\"] = config_lib.String(default=\"\", title=\"SSL Key Path\")\n        schema[\"http_ca_path\"] = config_lib.String(default=\"\", title=\"SSL CA Path\")\n        schema[\"http_cert_password\"] = config_lib.String(default=\"\", title=\"SSL Certificate Password\")\n        schema[\"http_key_password\"] = config_lib.String(default=\"\", title=\"SSL Key Password\")\n        schema[\"http_ca_password\"] = config_lib.String(default=\"\", title=\"SSL CA Password\")\n        return schema\n", "idx": 1028}
{"namespace": "mopidy.internal.network.try_ipv6_socket", "completion": "    \"\"\"\n    This function checks if the system supports IPv6 by attempting to create a socket with the AF_INET6 address family. If the socket creation is successful, it returns True. Otherwise, it returns False after logging a debug message.\n    Input-Output Arguments\n    :param: No input parameters.\n    :return: Bool. True if the system supports IPv6, False otherwise.\n    \"\"\"\n\n    \"\"\"\n    This function checks if the system supports IPv6 by attempting to create a socket with the AF_INET6 address family. If the socket creation is successful, it returns True. Otherwise, it returns False after logging a debug message.\n    Input-Output Arguments\n    :param: No input parameters.\n    :return: Bool. True if the system supports IPv6, False otherwise.\n    \"\"\"\n\n    \"\"\"\n    This function checks if the system supports IPv6 by attempting to create a socket with the AF_INET6 address family. If the socket creation is successful, it returns True. Otherwise, it returns False after logging a debug message.\n    Input-Output Arguments\n    :param: No input parameters.\n    :return: Bool. True if the system supports IPv6, False otherwise.\n    \"\"\"\n\n    \"\"\"\n    This function checks if the system supports IPv6 by attempting to create a socket with the AF_INET6 address family. If the socket creation is successful, it returns True. Otherwise, it returns False after logging a debug message.\n    Input-Output Arguments\n    :param: No input parameters.\n    :return: Bool. True if the system supports IPv6, False otherwise.\n    \"\"\"\n\n    \"\"\"\n    This function checks if the system supports IPv6 by attempting to create a socket with the AF_INET6 address family. If the socket creation is successful, it returns True. Otherwise, it returns False after logging a debug message.\n    Input-Output Arguments\n    :param: No input parameters.\n    :return: Bool. True if the system supports IPv6, False otherwise.\n    \"\"\"\n\n    \"\"\"\n    This function checks if the system supports IPv6 by attempting to create a socket with the AF_INET6 address family. If the socket creation is successful, it returns True. Otherwise, it returns False after logging a debug message.\n    Input-Output Arguments\n    :param: No input parameters.\n    :return: Bool. True if the system supports IPv6, False otherwise.\n    \"\"\"\n\n    \"\"\"\n    This function checks if the system supports IPv6 by attempting", "idx": 1029}
{"namespace": "mopidy.internal.network.format_hostname", "completion": "    if hostname.startswith(\"x:x:x:x:x:x:x:x\"):\n        return \"::ffff:x.x.x.x\"\n    return hostname\n\n", "idx": 1030}
{"namespace": "mopidy.internal.xdg.get_dirs", "completion": "    dirs = {}\n    for key in os.environ:\n        if key.startswith(\"XDG_\"):\n            dirs[key] = pathlib.Path(os.environ[key]).expanduser()\n    if \"XDG_USER_DIRS\" in os.environ:\n        user_dirs = os.environ[\"XDG_USER_DIRS\"].split(\":\")\n        for user_dir in user_dirs:\n            dirs[user_dir] = pathlib.Path(user_dir).expanduser()\n    return dirs", "idx": 1031}
{"namespace": "mopidy.internal.log.get_verbosity_level", "completion": "    if args_verbosity_level is not None:\n        verbosity_level = base_verbosity_level + args_verbosity_level\n    else:\n        verbosity_level = base_verbosity_level + logging_config[\"verbosity\"]\n\n    if verbosity_level < min(LOG_LEVELS.keys()):\n        verbosity_level = min(LOG_LEVELS.keys())\n    elif verbosity_level > max(LOG_LEVELS.keys()):\n        verbosity_level = max(LOG_LEVELS.keys())\n\n    return verbosity_level\n\n", "idx": 1032}
{"namespace": "mopidy.internal.validation.check_instance", "completion": "    if not isinstance(arg, cls):\n        raise exceptions.ValidationError(msg.format(arg=arg, name=cls.__name__))\n\n", "idx": 1033}
{"namespace": "mopidy.internal.validation.check_instances", "completion": "    if not isinstance(arg, list):\n        raise exceptions.ValidationError(msg.format(arg=arg, name=cls.__name__))\n    elif not all(isinstance(a, cls) for a in arg):\n        raise exceptions.ValidationError(msg.format(arg=arg, name=cls.__name__))\n\n", "idx": 1034}
{"namespace": "mopidy.internal.validation.check_uri", "completion": "    if not isinstance(arg, str):\n        raise exceptions.ValidationError(msg.format(arg=arg))\n    elif not arg.strip():\n        raise exceptions.ValidationError(msg.format(arg=arg))\n    elif not arg.startswith(\"http\"):\n        raise exceptions.ValidationError(msg.format(arg=arg))\n\n", "idx": 1035}
{"namespace": "mopidy.internal.validation.check_uris", "completion": "    _check_iterable(arg, msg, arg=arg)\n    if not all(check_uri(uri) for uri in arg):\n        raise exceptions.ValidationError(msg.format(arg=arg))\n\n", "idx": 1036}
{"namespace": "mopidy.internal.playlists.parse", "completion": "    \"\"\"\n    This function parses the given data and returns a list of parsed items. It uses a dictionary of handlers, where each handler is associated with a specific detector function. It iterates through the handlers and checks if the detector function returns True for the given data. If a match is found, it calls the corresponding parser function and returns the parsed items as a list. If no match is found, it parses the result as uris and returns the parsed items as a list.\n    Input-Output Arguments\n    :param data: The data to be parsed.\n    :return: List. The list of parsed items.\n    \"\"\"\n\n    \"\"\"\n    This function parses the given data and returns a list of parsed items. It uses a dictionary of handlers, where each handler is associated with a specific detector function. It iterates through the handlers and checks if the detector function returns True for the given data. If a match is found, it calls the corresponding parser function and returns the parsed items as a list. If no match is found, it parses the result as uris and returns the parsed items as a list.\n    Input-Output Arguments\n    :param data: The data to be parsed.\n    :return: List. The list of parsed items.\n    \"\"\"\n\n    \"\"\"\n    This function parses the given data and returns a list of parsed items. It uses a dictionary of handlers, where each handler is associated with a specific detector function. It iterates through the handlers and checks if the detector function returns True for the given data. If a match is found, it calls the corresponding parser function and returns the parsed items as a list. If no match is found, it parses the result as uris and returns the parsed items as a list.\n    Input-Output Arguments\n    :param data: The data to be parsed.\n    :return: List. The list of parsed items.\n    \"\"\"\n\n    \"\"\"\n    This function parses the given data and returns a list of parsed items. It uses a dictionary of handlers, where each handler is associated with a specific detector function. It iterates through the handlers and checks if the detector function returns True for the given data. If a match is found, it calls the corresponding parser function and returns the parsed items as a list. If no match is found, it parses the result as uris and returns the parsed items as a list.\n    Input-Output Arguments\n    :param data: The data to be parsed.\n    :return: List.", "idx": 1037}
{"namespace": "mopidy.config.schemas.ConfigSchema.deserialize", "completion": "        result = {}\n        errors = {}\n        for key, value in values.items():\n            if key in self:\n                try:\n                    result[key] = self[key].deserialize(value)\n                except Exception as e:\n                    errors[key] = str(e)\n                    result[key] = None\n            else:\n                errors[key] = \"Unknown key\"\n                result[key] = None\n        for key in self.keys():\n            if key not in values:\n                result[key] = None\n        return result, errors\n", "idx": 1038}
{"namespace": "mopidy.config.types.String.deserialize", "completion": "        value = decode(value)\n        value = value.strip()\n        if not value:\n            if self._required:\n                raise ValueError(\"Value is required\")\n            return None\n        if self._transformer:\n            value = self._transformer(value)\n        if self._choices:\n            if value not in self._choices:\n                raise ValueError(\n                    \"Value must be one of: {}\".format(\", \".join(self._choices))\n                )\n        return value\n", "idx": 1039}
{"namespace": "mopidy.config.types.String.serialize", "completion": "        if value is None:\n            return \"\"\n        if isinstance(value, _TransformedValue):\n            value = value.original\n        return encode(value)\n\n", "idx": 1040}
{"namespace": "mopidy.config.types.Secret.serialize", "completion": "        if value is None:\n            return \"\"\n        if display:\n            return \"********\"\n        return super().serialize(value, display)\n\n", "idx": 1041}
{"namespace": "mopidy.config.types.Integer.deserialize", "completion": "        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        value = int(value)\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        validators.validate_choice(value, self._choices)\n        return value\n", "idx": 1042}
{"namespace": "mopidy.config.types.Float.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = float(value)\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        return value\n\n", "idx": 1043}
{"namespace": "mopidy.config.types.Boolean.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        if value.lower() in self.true_values:\n            return True\n        if value.lower() in self.false_values:\n            return False\n        raise ValueError(\n            \"Invalid boolean value: %s. Valid values are: %s\"\n            % (value, \", \".join(self.true_values + self.false_values))\n        )\n\n", "idx": 1044}
{"namespace": "mopidy.config.types.Pair.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        if self._separator in value:\n            left, right = value.split(self._separator, 1)\n        else:\n            if self._optional_pair:\n                left = value\n                right = value\n            else:\n                raise ValueError(\n                    f\"invalid value for pair: {value!r} (missing separator)\"\n                )\n        return (\n            self._subtypes[0].deserialize(left),\n            self._subtypes[1].deserialize(right),\n        )\n", "idx": 1045}
{"namespace": "mopidy.config.types.Pair.serialize", "completion": "        if value is None:\n            return \"\"\n\n        serialized_values = [\n            self._subtypes[0].serialize(value[0], display),\n            self._subtypes[1].serialize(value[1], display),\n        ]\n\n        if self._optional_pair and serialized_values[0] == serialized_values[1]:\n            return serialized_values[0]\n\n        return self._separator.join(serialized_values)\n\n", "idx": 1046}
{"namespace": "mopidy.config.types.List.serialize", "completion": "        if value is None:\n            return \"\"\n\n        serialized_values = []\n        for item in value:\n            serialized_values.append(self._subtype.serialize(item, display))\n\n        return \"\\n\".join(serialized_values)\n\n", "idx": 1047}
{"namespace": "mopidy.config.types.LogColor.deserialize", "completion": "        value = decode(value)\n        validators.validate_choice(value, log.colors)\n        return value.lower()\n", "idx": 1048}
{"namespace": "mopidy.config.types.LogColor.serialize", "completion": "        if value in log.COLORS:\n            return value\n        else:\n            return \"\"\n\n", "idx": 1049}
{"namespace": "mopidy.config.types.LogLevel.deserialize", "completion": "        value = decode(value)\n        validators.validate_choice(value.lower(), self.levels)\n        return self.levels[value.lower()]\n", "idx": 1050}
{"namespace": "mopidy.config.types.LogLevel.serialize", "completion": "        if value is None:\n            return \"\"\n        for key, val in self.levels.items():\n            if val == value:\n                return key\n        return \"\"\n\n", "idx": 1051}
{"namespace": "mopidy.config.types.Hostname.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        if path.is_unix_socket(value):\n            return str(value)\n        try:\n            socket.gethostbyname(value)\n        except socket.gaierror:\n            raise ValueError(f\"Invalid hostname or IP address: {value!r}\")\n        return value\n", "idx": 1052}
{"namespace": "mopidy.config.load", "completion": "    # Determine the configuration directory based on the current file path.\n    config_dir = pathlib.Path(files[0]).parent\n\n    # Read the default configuration file and append it to an empty list.\n    default_config = read(config_dir / \"defaults.conf\")\n    config = [default_config]\n\n    # Extend the list using ext_defaults.\n    config.extend(read(pathlib.Path(ext_defaults[0])) for ext_defaults in ext_defaults)\n\n    # Load the configuration files, combine them with the default configurations and any overrides, and store the result in the variable \"raw_config\".\n    raw_config = \"\\n\".join(read(pathlib.Path(file)) for file in files)\n    raw_config += \"\\n\".join(read(pathlib.Path(override)) for override in overrides)\n\n    # Append the external schemas to the list of schemas and validate the \"raw_config\" against the schemas.\n    schemas = _schemas\n    schemas.extend(read(pathlib.Path(schema)) for schema in ext_schemas)\n    config = validate(raw_config, schemas)\n\n    return config\n\n", "idx": 1053}
{"namespace": "mopidy.config.format_initial", "completion": "    # Read the default configuration file.\n    config_dir = pathlib.Path(__file__).parent\n    default_config = read(config_dir / \"default.conf\")\n\n    # Get the default configuration for each extension.\n    default_configs = []\n    for extension_data in extensions_data:\n        default_config_file = extension_data.get(\"default_config_file\")\n        if default_config_file:\n            default_config_file = pathlib.Path(default_config_file)\n            default_config_file = read(default_config_file)\n            default_configs.append(default_config_file)\n        else:\n            default_configs.append(\"\")\n\n    # Load the raw configuration.\n    raw_config = _load(\n        [default_config] + default_configs,\n        [default_config] + default_configs,\n        [],\n    )\n\n    # Validate the configuration against the schemas.\n    schemas = _schemas[:]\n    for extension_data in extensions_data:\n        schemas.extend(extension_data.get(\"schemas\", []))\n\n    config = _validate(raw_config, schemas)\n\n    # Create a header with version information for each extension.\n    header = \"\"\n    for extension_data in extensions_data:\n        extension_name = extension_data.get(\"extension_name\")\n        if extension_name:\n            header += f\"# {extension_name}\\n\"\n        version = extension_data.get(\"version\")\n        if version:\n            header += f\"#   {version}\\n\"\n        header += \"\\n\"\n\n    # Format the configuration.\n    formatted_config = _format(config, {}, schemas, display=False)\n\n    # Return the formatted initial configuration.\n    return header + formatted_config\n\n", "idx": 1054}
{"namespace": "mopidy.config._load", "completion": "    parser = configparser.RawConfigParser(inline_comment_prefixes=(\"#\", \";\"))\n    parser.read_string(\"\\n\".join(defaults))\n\n    for file in files:\n        if os.path.isdir(file):\n            for f in os.listdir(file):\n                if f.endswith(\".conf\"):\n                    parser.read(os.path.join(file, f))\n        else:\n            parser.read(file)\n\n    raw_config = {}\n    for section in parser.sections():\n        raw_config[section] = dict(parser.items(section))\n\n    for section, key, value in overrides:\n        if section not in raw_config:\n            raw_config[section] = {}\n        raw_config[section][key] = value\n\n    return raw_config\n\n", "idx": 1055}
{"namespace": "mopidy.config._validate", "completion": "    validated_config = {}\n    errors = {}\n    for schema in schemas:\n        section = raw_config.get(schema.name)\n        if section is None:\n            continue\n        try:\n            validated_config[schema.name] = schema.deserialize(section)\n        except ValueError as e:\n            errors[schema.name] = e\n\n    return validated_config, errors\n\n", "idx": 1056}
{"namespace": "mingus.extra.tunings.get_tunings", "completion": "    result = []\n    if instrument is not None:\n        searchi = str.upper(instrument)\n        keys = list(_known.keys())\n        for x in keys:\n            if (\n                searchi not in keys\n                and x.find(searchi) == 0\n                or searchi in keys\n                and x == searchi\n            ):\n                for (desc, tun) in six.iteritems(_known[x][1]):\n                    result.append(tun)\n    else:\n        for (desc, tun) in six.iteritems(_known[x][1]):\n            result.append(tun)\n    if nr_of_strings is not None:\n        result = [x for x in result if x.count_strings() == nr_of_strings]\n    if nr_of_courses is not None:\n        result = [x for x in result if x.count_courses() == nr_of_courses]\n    return result\n\n", "idx": 1057}
{"namespace": "mingus.containers.instrument.Instrument.note_in_range", "completion": "        if isinstance(note, six.string_types):\n            note = Note(note)\n        if not hasattr(note, \"name\"):\n            raise UnexpectedObjectError(\n                \"Unexpected object '%s'. \"\n                \"Expecting a mingus.containers.Note object\" % note\n            )\n        if note < self.range[0] or note > self.range[1]:\n            return False\n        else:\n            return True\n", "idx": 1058}
{"namespace": "mingus.containers.instrument.Guitar.can_play_notes", "completion": "        if len(notes) > 6:\n            return False\n        else:\n            return Instrument.can_play_notes(self, notes)\n\n", "idx": 1059}
{"namespace": "mingus.containers.bar.Bar.get_range", "completion": "        # warning should check types\n        if len(self.bar) == 0:\n            return (0, 0)\n        else:\n            highest = 0\n            lowest = 0\n            for x in self.bar:\n                if x[2].highest_note() > highest:\n                    highest = x[2].highest_note()\n                if x[2].lowest_note() < lowest:\n                    lowest = x[2].lowest_note()\n            return (highest, lowest)\n", "idx": 1060}
{"namespace": "mingus.containers.bar.Bar.transpose", "completion": "        if up:\n            for cont in self.bar:\n                cont[2].transpose(interval)\n        else:\n            for cont in self.bar:\n                cont[2].transpose(interval, up=False)\n", "idx": 1061}
{"namespace": "mingus.containers.bar.Bar.determine_chords", "completion": "        # TODO: This function is not implemented yet.\n        # TODO: This function should be able to take a list of notes and determine the chords for those notes.\n        # TODO: This function should be able to take a list of notes and determine the chords for those notes.\n        # TODO: This function should be able to take a list of notes and determine the chords for those notes.\n        # TODO: This function should be able to take a list of notes and determine the chords for those notes.\n        # TODO: This function should be able to take a list of notes and determine the chords for those notes.\n        # TODO: This function should be able to take a list of notes and determine the chords for those notes.\n        # TODO: This function should be able to take a list of notes and determine the chords for those notes.\n        # TODO: This function should be able to take a list of notes and determine the chords for those notes.\n        # TODO: This function should be able to take a list of notes and determine the chords for those notes.\n        # TODO: This function should be able to take a list of notes and determine the chords for those notes.\n        # TODO: This function should be able to take a list of notes and determine the chords for those notes.\n        # TODO: This function should be able to take a list of notes and determine the chords for those notes.\n        # TODO: This function should be able to take a list of notes and determine the chords for those notes.\n        # TODO: This function should be able to take a list of notes and determine the chords for those notes.\n        # TODO: This function should be able to take a list of notes and determine the chords for those notes.\n        # TODO: This function should be able to take a list of notes and determine the chords for those notes.\n        # TODO: This function should be able to take a list of notes and determine the chords for those notes.\n        # TODO: This function should be able to take a list of notes and determine the chords for those notes.\n        # TODO: This function should be able to take a list of notes and determine the chords for those notes.\n        # TODO: This function should be able to take a list of notes and determine the chords for those notes.\n        # TODO: This function should be able", "idx": 1062}
{"namespace": "mingus.containers.note.Note.transpose", "completion": "        if up:\n            self.name = notes.transpose_up(self.name, interval)\n        else:\n            self.name = notes.transpose_down(self.name, interval)\n", "idx": 1063}
{"namespace": "mingus.containers.note.Note.from_int", "completion": "        if integer < 0 or integer > 127:\n            raise ValueError(\"MIDI integer must be 0-127\")\n\n        self.name = notes.from_int(integer)\n        self.octave = integer // 12\n", "idx": 1064}
{"namespace": "mingus.containers.note.Note.to_hertz", "completion": "        return notes.note_to_hertz(self.name, self.octave, standard_pitch)\n", "idx": 1065}
{"namespace": "mingus.containers.note.Note.from_hertz", "completion": "        # int(Note(\"A\")) == 57\n        diff = log(hertz / standard_pitch, 2) * 12\n        self.name = notes.int_to_note(int(diff) % 12)\n        self.octave = int(diff) // 12\n        return self\n", "idx": 1066}
{"namespace": "mingus.containers.note.Note.to_shorthand", "completion": "        return notes.to_shorthand(self.name, self.octave)\n", "idx": 1067}
{"namespace": "mingus.containers.note_container.NoteContainer.from_chord_shorthand", "completion": "        self.empty()\n        for note in chords.from_shorthand(shorthand):\n            self.add_note(note)\n        return self\n", "idx": 1068}
{"namespace": "mingus.containers.note_container.NoteContainer.from_interval_shorthand", "completion": "        self.empty()\n        if isinstance(startnote, six.string_types):\n            startnote = Note(startnote)\n        self.add_note(startnote)\n        self.add_notes(intervals.from_shorthand(shorthand, up))\n        return self\n", "idx": 1069}
{"namespace": "mingus.containers.note_container.NoteContainer.from_progression_shorthand", "completion": "        self.empty()\n        if isinstance(key, six.string_types):\n            key = Note(key)\n        for note in chords.from_shorthand(shorthand, key):\n            self.add_note(note)\n        return self\n", "idx": 1070}
{"namespace": "mingus.containers.note_container.NoteContainer.transpose", "completion": "        for note in self.notes:\n            note.transpose(interval, up)\n        return self\n", "idx": 1071}
{"namespace": "mingus.containers.note_container.NoteContainer.get_note_names", "completion": "        return list(set([x.name for x in self.notes]))\n", "idx": 1072}
{"namespace": "mingus.core.notes.int_to_note", "completion": "    if note_int < 0 or note_int > 11:\n        raise ValueError(\"Note integer out of range\")\n    if accidentals == \"#\":\n        note_list = [\"C\", \"D\", \"E\", \"F\", \"G\", \"A\", \"B\"]\n    elif accidentals == \"b\":\n        note_list = [\"C\", \"Db\", \"Eb\", \"F\", \"Gb\", \"Ab\", \"Bb\"]\n    elif accidentals == \"bb\":\n        note_list = [\"C\", \"Dbb\", \"Ebb\", \"F\", \"Gbb\", \"Abb\", \"Bbb\"]\n    else:\n        raise ValueError(\"Invalid accidental type\")\n    return note_list[note_int]\n\n", "idx": 1073}
{"namespace": "mingus.core.notes.is_valid_note", "completion": "    if note in _note_dict:\n        return True\n    else:\n        return False\n\n", "idx": 1074}
{"namespace": "mingus.core.notes.reduce_accidentals", "completion": "    if note[0] == \"C\":\n        if note[1] == \"#\":\n            return \"C#\"\n        elif note[1] == \"b\":\n            return \"Db\"\n        else:\n            return \"C\"\n    elif note[0] == \"D\":\n        if note[1] == \"#\":\n            return \"D#\"\n        elif note[1] == \"b\":\n            return \"Eb\"\n        else:\n            return \"D\"\n    elif note[0] == \"E\":\n        if note[1] == \"#\":\n            return \"E#\"\n        elif note[1] == \"b\":\n            return \"Fb\"\n        else:\n            return \"E\"\n    elif note[0] == \"F\":\n        if note[1] == \"#\":\n            return \"F#\"\n        elif note[1] == \"b\":\n            return \"Gb\"\n        else:\n            return \"F\"\n    elif note[0] == \"G\":\n        if note[1] == \"#\":\n            return \"G#\"\n        elif note[1] == \"b\":\n            return \"Ab\"\n        else:\n            return \"G\"\n    elif note[0] == \"A\":\n        if note[1] == \"#\":\n            return \"A#\"\n        elif note[1] == \"b\":\n            return \"Bb\"\n        else:\n            return \"A\"\n    elif note[0] == \"B\":\n        if note[1] == \"#\":\n            return \"B#\"\n        elif note[1] == \"b\":\n            return \"Cb\"\n        else:\n            return \"B\"\n    else:\n        return \"C\"\n\n", "idx": 1075}
{"namespace": "mingus.core.notes.remove_redundant_accidentals", "completion": "    if note[0] == \"C\":\n        return note\n    else:\n        val = note_to_int(note[0])\n        for token in note[1:]:\n            if token == \"b\":\n                val -= 1\n            elif token == \"#\":\n                val += 1\n            else:\n                raise NoteFormatError(\"Unknown note format '%s'\" % note)\n        if val >= note_to_int(note[0]):\n            return int_to_note(val % 12)\n        else:\n            return int_to_note(val % 12, \"b\")\n\n", "idx": 1076}
{"namespace": "mingus.core.intervals.minor_second", "completion": "    return notes.diminish(note)\n\n", "idx": 1077}
{"namespace": "mingus.core.intervals.major_second", "completion": "    sec = second(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sec, 2)\n\n", "idx": 1078}
{"namespace": "mingus.core.intervals.minor_third", "completion": "    third = third(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, third, 1)\n\n", "idx": 1079}
{"namespace": "mingus.core.intervals.minor_fourth", "completion": "    fth = fourth(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, fth, 5)\n\n", "idx": 1080}
{"namespace": "mingus.core.intervals.minor_seventh", "completion": "    sev = seventh(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sev, 10)\n\n", "idx": 1081}
{"namespace": "mingus.core.intervals.major_seventh", "completion": "    sth = seventh(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sth, 11)\n\n", "idx": 1082}
{"namespace": "mingus.core.intervals.measure", "completion": "    # Convert the notes to integers\n    note1_int = notes.note_to_int(note1)\n    note2_int = notes.note_to_int(note2)\n\n    # Calculate the number of half-note steps between the notes\n    steps = note2_int - note1_int\n\n    # Return the number of half-note steps\n    return steps\n\n", "idx": 1083}
{"namespace": "mingus.core.intervals.determine", "completion": "    # Check if the notes are valid\n    if not notes.is_valid_note(note1):\n        raise KeyError(\"The first note '%s' is not a valid note\" % note1)\n    if not notes.is_valid_note(note2):\n        raise KeyError(\"The second note '%s' is not a valid note\" % note2)\n\n    # Determine the interval\n    interval = measure(note1, note2)\n\n    # Determine the name of the interval\n    if interval == 0:\n        name = \"unison\"\n    elif interval == 1:\n        name = \"minor second\"\n    elif interval == 2:\n        name = \"major second\"\n    elif interval == 3:\n        name = \"minor third\"\n    elif interval == 4:\n        name = \"major third\"\n    elif interval == 5:\n        name = \"perfect fourth\"\n    elif interval == 6:\n        name = \"tritone\"\n    elif interval == 7:\n        name = \"perfect fifth\"\n    elif interval == 8:\n        name = \"minor sixth\"\n    elif interval == 9:\n        name = \"major sixth\"\n    elif interval == 10:\n        name = \"minor seventh\"\n    elif interval == 11:\n        name = \"major seventh\"\n    else:\n        raise KeyError(\"The interval '%s' is not a valid interval\" % interval)\n\n    # Return the name of the interval\n    if shorthand:\n        return name[0] + name[1:].replace(\" \", \"\")\n    else:\n        return name\n\n", "idx": 1084}
{"namespace": "mingus.core.intervals.from_shorthand", "completion": "    # Check if the input is valid\n    if not notes.is_valid_note(note):\n        return False\n\n    # Check if the interval is valid\n    if not interval.isdigit():\n        return False\n    interval = int(interval)\n    if interval < 1 or interval > 7:\n        return False\n\n    # Check if the interval is sharp or flat\n    if interval == 1:\n        interval_type = \"\"\n    elif interval == 2:\n        interval_type = \"b\"\n    elif interval == 3:\n        interval_type = \"#\"\n    elif interval == 4:\n        interval_type = \"bb\"\n    elif interval == 5:\n        interval_type = \"b#\"\n    elif interval == 6:\n        interval_type = \"#b\"\n    elif interval == 7:\n        interval_type = \"#\"\n\n    # Get the starting note's index in the list of notes\n    note_index = notes.fifths.index(note[0])\n\n    # Move up or down by the specified interval\n    if up:\n        note_index += interval\n    else:\n        note_index -= interval\n\n    # Check if the resulting note is valid\n    if note_index < 0:\n        note_index += 7\n    elif note_index > 6:\n        note_index -= 7\n\n    # Return the resulting note\n    return notes.fifths[note_index] + interval_type + note[1:]\n\n", "idx": 1085}
{"namespace": "mingus.core.intervals.is_consonant", "completion": "    # Corner case for unisons\n    if note1[0] == note2[0]:\n        return True\n\n    # Get the interval between the two notes\n    interval = determine(note1, note2)\n\n    # Corner case for perfect fourths\n    if interval == \"perfect fourth\" and not include_fourths:\n        return False\n\n    # Corner case for perfect fifths\n    if interval == \"perfect fifth\":\n        return False\n\n    # Corner case for major sevenths\n    if interval == \"major seventh\":\n        return False\n\n    # Corner case for minor sevenths\n    if interval == \"minor seventh\":\n        return False\n\n    # Corner case for major sixths\n    if interval == \"major sixth\":\n        return False\n\n    # Corner case for minor sixths\n    if interval == \"minor sixth\":\n        return False\n\n    # Corner case for major thirds\n    if interval == \"major third\":\n        return False\n\n    # Corner case for minor thirds\n    if interval == \"minor third\":\n        return False\n\n    # Corner case for major seconds\n    if interval == \"major second\":\n        return False\n\n    # Corner case for minor seconds\n    if interval == \"minor second\":\n        return False\n\n    # Corner case for major fourths\n    if interval == \"major fourth\":\n        return False\n\n    # Corner case for minor fourths\n    if interval == \"minor fourth\":\n        return False\n\n    # Corner case for major fifths\n    if interval == \"major fifth\":\n        return False\n\n    # Corner case for minor fifths\n    if interval == \"minor fifth\":\n        return False\n\n    # Corner case for major unisons\n    if interval == \"major unison\":\n        return False\n\n    # Corner case for minor unisons\n    if interval == \"minor unison\":\n        return False\n\n    # Corner case for augmented unisons\n    if interval == \"augmented unison\":\n        return False\n\n    # Corner case for diminished unisons\n    if interval == \"diminished unison\":\n        return False\n\n    # Corner case for augmented thirds\n    if interval == \"augmented third\":\n        return False\n\n    # Corner case for diminished thirds\n    if interval == \"diminished third\":\n        return False\n\n    # Corner case for augmented seconds\n    if interval", "idx": 1086}
{"namespace": "mingus.core.intervals.is_perfect_consonant", "completion": "    if note1[0] == note2[0]:\n        return True\n    elif include_fourths and note1[0] == notes.diminish(note2[0]):\n        return True\n    elif note1[0] == notes.augment(note2[0]):\n        return True\n    else:\n        return False\n\n", "idx": 1087}
{"namespace": "mingus.core.keys.get_key", "completion": "    if accidentals not in range(-7, 8):\n        raise ValueError(\"Accidentals must be in the range [-7, 7]\")\n\n    if accidentals in _key_cache:\n        return _key_cache[accidentals]\n\n    major_key = major_keys[accidentals + 7]\n    minor_key = minor_keys[accidentals + 7]\n\n    _key_cache[accidentals] = (major_key, minor_key)\n\n    return (major_key, minor_key)\n\n", "idx": 1088}
{"namespace": "mingus.core.keys.get_key_signature", "completion": "    if not is_valid_key(key):\n        raise NoteFormatError(\"key not in recognized format.\")\n    return keys.index(key) - 7\n\n", "idx": 1089}
{"namespace": "mingus.core.keys.get_key_signature_accidentals", "completion": "    if not is_valid_key(key):\n        raise NoteFormatError(\"unrecognized format for key '%s'\" % key)\n\n    for couple in keys:\n        if key in couple:\n            accidentals = keys.index(couple) - 7\n            return accidentals\n\n    if accidentals == 0:\n        return [0]\n    elif accidentals == 1:\n        return [1, -1]\n    elif accidentals == 2:\n        return [2, -1, 1]\n    elif accidentals == 3:\n        return [3, -1, 1, -2]\n    elif accidentals == 4:\n        return [4, -1, 1, -2, 2]\n    elif accidentals == 5:\n        return [5, -1, 1, -2, 2, -3]\n    elif accidentals == 6:\n        return [6, -1, 1, -2, 2, -3, 3]\n    elif accidentals == 7:\n        return [7, -1, 1, -2, 2, -3, 3, -4]\n    elif accidentals == 8:\n        return [8, -1, 1, -2, 2, -3, 3, -4, 4]\n    elif accidentals == 9:\n        return [9, -1, 1, -2, 2, -3, 3, -4, 4, -5]\n    elif accidentals == 10:\n        return [10, -1, 1, -2, 2, -3, 3, -4, 4, -5, 5]\n    elif accidentals == 11:\n        return [11, -1, 1, -2, 2, -3, 3, -4, 4, -5, 5, -6]\n    elif accidentals == 12:\n        return [12, -1, 1, -2, 2, -3, 3, -4, 4, -5, 5, -6, 6]\n\n", "idx": 1090}
{"namespace": "mingus.core.keys.get_notes", "completion": "    from mingus.core import notes\n    if not is_valid_key(key):\n        raise NoteFormatError(\"unrecognized format for key '%s'\" % key)\n\n    for couple in keys:\n        if key in couple:\n            key_index = keys.index(couple)\n            break\n\n    notes_list = []\n    for i in range(7):\n        notes_list.append(notes.fifths[(key_index + i) % 7])\n\n    return notes_list\n\n", "idx": 1091}
{"namespace": "mingus.core.keys.relative_major", "completion": "    if not is_valid_key(key):\n        raise NoteFormatError(\"unrecognized format for key '%s'\" % key)\n\n    for couple in keys:\n        if key in couple:\n            return couple[0]\n\n", "idx": 1092}
{"namespace": "mingus.core.chords.augmented_triad", "completion": "    return [note, intervals.major_third(note), intervals.augmented_fifth(note)]\n\n", "idx": 1093}
{"namespace": "mingus.core.chords.determine", "completion": "    # Determine the number of notes in the chord\n    num_notes = len(chord)\n\n    # Determine the chord name\n    if num_notes == 3:\n        if chord == triad(chord[0], \"C\"):\n            if shorthand:\n                return \"I\"\n            else:\n                return \"Tonic\"\n        elif chord == triad(chord[0], \"C#\"):\n            if shorthand:\n                return \"ii\"\n            else:\n                return \"Supertonic\"\n        elif chord == triad(chord[0], \"D\"):\n            if shorthand:\n                return \"iii\"\n            else:\n                return \"Mediant\"\n        elif chord == triad(chord[0], \"D#\"):\n            if shorthand:\n                return \"IV\"\n            else:\n                return \"Subdominant\"\n        elif chord == triad(chord[0], \"E\"):\n            if shorthand:\n                return \"V\"\n            else:\n                return \"Dominant\"\n        elif chord == triad(chord[0], \"F\"):\n            if shorthand:\n                return \"vi\"\n            else:\n                return \"Submediant\"\n        elif chord == triad(chord[0], \"F#\"):\n            if shorthand:\n                return \"VII\"\n            else:\n                return \"Subtonic\"\n        elif chord == triad(chord[0], \"G\"):\n            if shorthand:\n                return \"I\"\n            else:\n                return \"Tonic\"\n        elif chord == triad(chord[0], \"G#\"):\n            if shorthand:\n                return \"ii\"\n            else:\n                return \"Supertonic\"\n        elif chord == triad(chord[0], \"A\"):\n            if shorthand:\n                return \"iii\"\n            else:\n                return \"Mediant\"\n        elif chord == triad(chord[0], \"A#\"):\n            if shorthand:\n                return \"IV\"\n            else:\n                return \"Subdominant\"\n        elif chord == triad(chord[0], \"B\"):\n            if shorthand:\n                return \"V\"\n            else:\n                return \"Dominant\"\n        elif chord == triad(chord[0], \"C\"):\n            if shorthand:\n                return \"vi\"\n            else:\n                return \"Sub", "idx": 1094}
{"namespace": "mingus.core.value.septuplet", "completion": "    if in_fourths:\n        return tuplet(value, 7, 4)\n    else:\n        return tuplet(value, 7, 8)\n\n", "idx": 1095}
{"namespace": "mingus.core.value.determine", "completion": "    if value in base_values:\n        return (value, 0, 1)\n    elif value in base_triplets:\n        return (value, 0, 3)\n    elif value in base_quintuplets:\n        return (value, 0, 5)\n    elif value in base_septuplets:\n        return (value, 0, 7)\n    else:\n        for i in range(1, 5):\n            if value == dots(value, i):\n                return (value, i, 1)\n        for i in range(1, 5):\n            if value == triplet(value, i):\n                return (value, i, 3)\n        for i in range(1, 5):\n            if value == quintuplet(value, i):\n                return (value, i, 5)\n        for i in range(1, 5):\n            if value == septuplet(value, i):\n                return (value, i, 7)\n        return (value, 0, 1)\n\n", "idx": 1096}
{"namespace": "mingus.core.progressions.substitute_major_for_minor", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Major to minor substitution\n    if (\n        suff == \"M\"\n        or suff == \"M7\"\n        or suff == \"\"\n        and roman in [\"I\", \"IV\", \"V\"]\n        or ignore_suffix\n    ):\n        n = skip(roman, 2)\n        a = interval_diff(roman, n, 3) + acc\n        if suff == \"M\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"m\")))\n        elif suff == \"M7\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"m7\")))\n        elif suff == \"\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"\")))\n    return res\n\n", "idx": 1097}
{"namespace": "mingus.core.progressions.substitute_diminished_for_diminished", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Diminished to diminished substitution\n    if (\n        suff == \"dim7\"\n        or suff == \"dim\"\n        or suff == \"\"\n        and roman == \"VII\"\n        or ignore_suffix\n    ):\n        for i in range(3):\n            n = skip(roman, 2)\n            a = interval_diff(roman, n, 3) + acc\n            res.append(tuple_to_string((n, a, \"dim\")))\n    return res\n\n", "idx": 1098}
{"namespace": "mingus.core.progressions.substitute_diminished_for_dominant", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Diminished progressions\n    if (\n        suff == \"dim7\"\n        or suff == \"dim\"\n        or suff == \"\"\n        and roman in [\"VII\"]\n        or ignore_suffix\n    ):\n        if suff == \"\":\n            suff = \"dim\"\n\n        # Add diminished chord\n        last = roman\n        for x in range(4):\n            next = skip(last, 2)\n            acc += interval_diff(last, next, 3)\n            res.append(tuple_to_string((next, acc, suff)))\n            last = next\n    return res\n\n", "idx": 1099}
{"namespace": "mingus.core.progressions.substitute", "completion": "    substitutions = []\n    substitutions.extend(substitute_harmonic(progression, substitute_index))\n    substitutions.extend(substitute_minor_for_major(progression, substitute_index))\n    substitutions.extend(substitute_major_for_minor(progression, substitute_index))\n    substitutions.extend(substitute_diminished_for_diminished(progression, substitute_index))\n    substitutions.extend(substitute_diminished_for_dominant(progression, substitute_index))\n\n    if depth > 0:\n        for i in range(len(progression)):\n            if i != substitute_index:\n                substitutions.extend(substitute(progression, i, depth - 1))\n\n    return substitutions\n\n", "idx": 1100}
{"namespace": "mingus.core.progressions.skip", "completion": "    index = numeral_intervals[numerals.index(roman_numeral)]\n    index += skip_count\n    index = index % 7\n    return numerals[index]", "idx": 1101}
{"namespace": "exodus_bundler.cli.configure_logging", "completion": "    if quiet:\n        logger.setLevel(logging.ERROR)\n    elif verbose:\n        logger.setLevel(logging.DEBUG)\n    else:\n        logger.setLevel(logging.INFO)\n\n    # Add a stderr handler.\n\n    stderr_handler = logging.StreamHandler(sys.stderr)\n    stderr_handler.setLevel(logging.WARNING)\n    stderr_handler.setFormatter(logging.Formatter(\n        '%(levelname)s: %(message)s'\n    ))\n    logger.addHandler(stderr_handler)\n\n    # Add an optional stdout handler.\n\n    if not suppress_stdout:\n        stdout_handler = logging.StreamHandler(sys.stdout)\n        stdout_handler.setLevel(logging.DEBUG)\n        stdout_handler.setFormatter(logging.Formatter(\n            '%(levelname)s: %(message)s'\n        ))\n        logger.addHandler(stdout_handler)\n\n", "idx": 1102}
{"namespace": "exodus_bundler.bundling.create_unpackaged_bundle", "completion": "    # Initialize the temporary directory.\n    root_directory = tempfile.mkdtemp()\n\n    # Create the main executable files.\n    for executable in executables:\n        shutil.copy(executable, os.path.join(root_directory, os.path.basename(executable)))\n\n    # Create the shell launchers.\n    if shell_launchers:\n        for executable in executables:\n            launcher_path = os.path.join(root_directory, os.path.basename(executable))\n            with open(launcher_path, 'w') as launcher_file:\n                launcher_file.write(construct_bash_launcher(executable))\n                os.chmod(launcher_path, stat.S_IEXEC)\n\n    # Create the additional files.\n    for file_path in add:\n        shutil.copy(file_path, os.path.join(root_directory, os.path.basename(file_path)))\n\n    # Create the chroot directory.\n    if chroot:\n        chroot_directory = os.path.join(root_directory, 'chroot')\n        os.mkdir(chroot_directory)\n        os.chmod(chroot_directory, stat.S_IRWXU)\n\n    # Detect dependencies.\n    if detect:\n        for executable in executables:\n            dependencies = detect_dependencies(executable)\n            for dependency in dependencies:\n                shutil.copy(dependency, os.path.join(root_directory, os.path.basename(dependency)))\n\n    # Create the symlinks.\n    for executable in executables:\n        symlink_path = os.path.join(root_directory, os.path.basename(executable))\n        if symlink_path not in no_symlink:\n            os.symlink(os.path.basename(executable), symlink_path)\n\n    # Rename the executables.\n    for executable, new_name in zip(executables, rename):\n        os.rename(os.path.join(root_directory, os.path.basename(executable)),\n                  os.path.join(root_directory, new_name))\n\n    # Return the path to the temporary directory.\n    return root_directory\n\n", "idx": 1103}
{"namespace": "exodus_bundler.bundling.detect_elf_binary", "completion": "    with open(filename, 'rb') as f:\n        header = f.read(4)\n        if header == b'\\x7fELF':\n            return True\n        else:\n            return False\n\n", "idx": 1104}
{"namespace": "exodus_bundler.bundling.resolve_binary", "completion": "    if os.path.exists(binary):\n        return binary\n    else:\n        for path in os.environ['PATH'].split(os.pathsep):\n            path = path.strip('\"')\n            binary_path = os.path.join(path, binary)\n            if os.path.exists(binary_path):\n                return binary_path\n        raise MissingFileError('The \"%s\" binary could not be found in $PATH.' % binary)\n\n", "idx": 1105}
{"namespace": "exodus_bundler.bundling.resolve_file_path", "completion": "    if search_environment_path:\n        path = resolve_binary(path)\n    else:\n        path = os.path.normpath(os.path.abspath(path))\n\n    if not os.path.exists(path):\n        raise MissingFileError('The \"%s\" file was not found.' % path)\n    if os.path.isdir(path):\n        raise UnexpectedDirectoryError('The \"%s\" file is a directory.' % path)\n\n    return path\n\n", "idx": 1106}
{"namespace": "exodus_bundler.bundling.run_ldd", "completion": "    if not detect_elf_binary(binary):\n        raise InvalidElfBinaryError('The \"%s\" file is not an ELF binary.' % binary)\n\n    # Run the `ldd` command with the binary as an argument.\n    ldd_process = Popen([ldd, binary], stdout=PIPE, stderr=PIPE)\n    # Capture the stdout and stderr outputs.\n    stdout, stderr = ldd_process.communicate()\n    # Return the combined stdout and stderr outputs as a list of lines.\n    return stdout.decode('utf-8').split('\\n') + stderr.decode('utf-8').split('\\n')\n\n", "idx": 1107}
{"namespace": "exodus_bundler.bundling.Elf.dependencies", "completion": "        # Find the direct dependencies.\n        direct_dependencies = self.find_direct_dependencies()\n        # Find the dependencies of the direct dependencies.\n        dependencies = direct_dependencies.copy()\n        while True:\n            # Find the dependencies of the dependencies.\n            new_dependencies = set()\n            for dependency in dependencies:\n                new_dependencies.update(dependency.find_direct_dependencies())\n            # If there are no new dependencies, we're done.\n            if not new_dependencies:\n                break\n            # Otherwise, add the new dependencies to the set of dependencies.\n            dependencies.update(new_dependencies)\n        return dependencies\n\n", "idx": 1108}
{"namespace": "exodus_bundler.bundling.File.hash", "completion": "        hash_object = hashlib.sha256()\n        with open(self.path, 'rb') as file:\n            while True:\n                data = file.read(1024)\n                if not data:\n                    break\n                hash_object.update(data)\n        return hash_object.hexdigest()\n", "idx": 1109}
{"namespace": "exodus_bundler.bundling.Bundle.add_file", "completion": "        # If the file is a directory, add all of its contents recursively.\n        if os.path.isdir(path):\n            for root, directories, filenames in os.walk(path):\n                for filename in filenames:\n                    self.add_file(os.path.join(root, filename), entry_point=entry_point)\n            return None\n\n        # Otherwise, add the file.\n        file = File(path, entry_point=entry_point, chroot=self.chroot, file_factory=self.file_factory)\n        self.files.add(file)\n        return file\n", "idx": 1110}
{"namespace": "exodus_bundler.bundling.Bundle.bundle_root", "completion": "        return os.path.join(self.working_directory, 'bundles', self.hash)\n", "idx": 1111}
{"namespace": "exodus_bundler.bundling.Bundle.hash", "completion": "        # First, we'll retrieve the hashes of all the files in the bundle.\n        file_hashes = []\n        for file in self.files:\n            file_hashes.append(file.hash)\n\n        # Next, we'll sort the hashes.\n        file_hashes.sort()\n\n        # Now we'll combine the hashes into a single string.\n        combined_string = ''\n        for file_hash in file_hashes:\n            combined_string += file_hash\n\n        # Finally, we'll compute the SHA256 hash of the combined string.\n        hash_object = hashlib.sha256(combined_string.encode('utf-8'))\n        hash_value = hash_object.hexdigest()\n\n        return hash_value", "idx": 1112}
{"namespace": "exodus_bundler.launchers.construct_bash_launcher", "completion": "    # Render the template file.\n    template_file = os.path.join(parent_directory, 'templates', 'bash_launcher.sh')\n    bash_launcher = render_template_file(template_file, linker=linker, library_path=library_path, executable=executable, full_linker=full_linker)\n\n    return bash_launcher\n\n", "idx": 1113}
{"namespace": "exodus_bundler.input_parsing.extract_open_path", "completion": "    line = strip_pid_prefix(line)\n    if line.startswith('open(\"'):\n        line = line[len('open(\"'):]\n        parts = line.split('\", ')\n        if len(parts) > 1:\n            return parts[0]\n    return None\n\n", "idx": 1114}
{"namespace": "exodus_bundler.input_parsing.extract_paths", "completion": "    paths = []\n    for line in content:\n        if line.startswith('openat(AT_FDCWD, \"'):\n            path = extract_open_path(line)\n            if path is not None:\n                paths.append(path)\n        elif line.startswith('stat(\"'):\n            path = extract_stat_path(line)\n            if path is not None:\n                paths.append(path)\n        elif line.startswith('execve(\"'):\n            path = extract_exec_path(line)\n            if path is not None:\n                paths.append(path)\n\n    if existing_only:\n        paths = [path for path in paths if os.path.exists(path) and not os.path.isdir(path)]\n\n    return paths\n\n", "idx": 1115}
{"namespace": "fs.time.epoch_to_datetime", "completion": "    if t is None:\n        return None\n    return datetime.fromtimestamp(t, timezone.utc)", "idx": 1116}
{"namespace": "fs.path.normpath", "completion": "    if not path:\n        return path\n\n    if _requires_normalization(path):\n        raise IndexError(\"Illegal back reference in path: %r\" % path)\n\n    return re.sub(r\"(?<!/)\\.\\.\", \"/\", path)\n\n", "idx": 1117}
{"namespace": "fs.path.iteratepath", "completion": "    return path.split(\"/\")\n\n", "idx": 1118}
{"namespace": "fs.path.recursepath", "completion": "    path = relpath(normpath(path))\n    if not path:\n        return []\n    if reverse:\n        return [path] + [\"/\".join(path.split(\"/\")[:-1]) for path in path.split(\"/\")[:-1]]\n    else:\n        return [\"/\".join(path.split(\"/\")[:-1]) for path in path.split(\"/\")[:-1]] + [path]\n\n", "idx": 1119}
{"namespace": "fs.path.join", "completion": "    return \"/\".join(paths)\n\n", "idx": 1120}
{"namespace": "fs.path.parts", "completion": "    path = relpath(normpath(path))\n    if not path:\n        return []\n    return path.split(\"/\")\n\n", "idx": 1121}
{"namespace": "fs.path.splitext", "completion": "    if \".\" not in path:\n        return path, \"\"\n    split = path.rsplit(\".\", 1)\n    return (split[0], split[1])\n\n", "idx": 1122}
{"namespace": "fs.path.isbase", "completion": "    return path1 == path2[:len(path1)]\n\n", "idx": 1123}
{"namespace": "fs.path.frombase", "completion": "    if not isparent(path1, path2):\n        raise ValueError(\"path1 is not a parent directory of path2\")\n    return path2[len(path1):]\n\n", "idx": 1124}
{"namespace": "fs.path.relativefrom", "completion": "    if not isbase(base, path):\n        raise ValueError(\"path must be a child of base\")\n    return frombase(base, path)\n\n", "idx": 1125}
{"namespace": "fs.path.iswildcard", "completion": "    return path.endswith(tuple(_WILD_CHARS))", "idx": 1126}
{"namespace": "fs.wildcard.match", "completion": "    if not pattern:\n        return True\n\n    if not name:\n        return False\n\n    if pattern == name:\n        return True\n\n    if pattern == '*':\n        return True\n\n    if pattern == '**':\n        return True\n\n    if pattern == '**/*':\n        return True\n\n    if pattern == '**/**':\n        return True\n\n    if pattern == '**/**/*':\n        return True\n\n    if pattern == '**/**/**':\n        return True\n\n    if pattern == '**/**/**/*':\n        return True\n\n    if pattern == '**/**/**/**':\n        return True\n\n    if pattern == '**/**/**/**/*':\n        return True\n\n    if pattern == '**/**/**/**/**':\n        return True\n\n    if pattern == '**/**/**/**/**/*':\n        return True\n\n    if pattern == '**/**/**/**/**/**':\n        return True\n\n    if pattern == '**/**/**/**/**/**/*':\n        return True\n\n    if pattern == '**/**/**/**/**/**/**':\n        return True\n\n    if pattern == '**/**/**/**/**/**/**/*':\n        return True\n\n    if pattern == '**/**/**/**/**/**/**/**':\n        return True\n\n    if pattern == '**/**/**/**/**/**/**/**/*':\n        return True\n\n    if pattern == '**/**/**/**/**/**/**/**/**':\n        return True\n\n    if pattern == '**/**/**/**/**/**/**/**/**/*':\n        return True\n\n    if pattern == '**/**/**/**/**/**/**/**/**/**':\n        return True\n\n    if pattern == '**/**/**/**/**/**/**/**/**/*/**':\n        return True\n\n    if pattern == '**/**/**/**/**/**/**/**/**/*/**/*':\n        return True\n\n    if pattern == '**/**/**/**/**/**/**/**/**/*/**/*/**':\n        return True\n\n    if pattern == '**/**/**/**/**/**/**/**/**/*/**/*/**/*':\n        return True\n\n    if pattern == '**/**/**/**/**/**/**/**/**/*/**/*/**/*/**':\n        return True\n\n    if pattern == '**/**/**/**/**/**/**/**/**/*/**/*/**/*/**/*':\n        return True\n\n    if pattern == '**/**/**/**/**/**/**/**/**/*/**/*/**/*/**/*/**':\n        return True\n\n    if pattern == '**/**/**/**/**/**/**/**/**/*/**/*/**/*/**/*/**':\n        return", "idx": 1127}
{"namespace": "fs.wildcard.imatch", "completion": "    try:\n        re_pat = _PATTERN_CACHE[(pattern, False)]\n    except KeyError:\n        res = \"(?mi)\" + _translate(pattern) + r\"\\Z\"\n        _PATTERN_CACHE[(pattern, False)] = re_pat = re.compile(res)\n    return re_pat.match(name) is not None\n\n", "idx": 1128}
{"namespace": "fs.wildcard.get_matcher", "completion": "    if not patterns:\n        return lambda name: True\n    if case_sensitive:\n        return partial(match_any, patterns)\n    else:\n        return partial(imatch_any, patterns)\n\n", "idx": 1129}
{"namespace": "fs._url_tools.url_quote", "completion": "    if _WINDOWS_PLATFORM:\n        drive_letter, windows_path = re.split(r\"(?=)\", path_snippet)\n        windows_path = windows_path.replace(\"\\\\\", \"/\")\n        return \"file:///\" + windows_path\n    else:\n        return urllib.pathname2url(path_snippet)", "idx": 1130}
{"namespace": "fs._ftp_parse.parse", "completion": "    parsed_lines = []\n    for line in lines:\n        if line.strip():\n            parsed_lines.append(line)\n    return parsed_lines\n\n", "idx": 1131}
{"namespace": "fs._ftp_parse._parse_time", "completion": "    for format in formats:\n        try:\n            return time.mktime(datetime.strptime(t, format).timetuple())\n        except ValueError:\n            pass\n    return None\n\n", "idx": 1132}
{"namespace": "fs.permissions.Permissions.parse", "completion": "        if ls is None:\n            return cls()\n        if isinstance(ls, Permissions):\n            return ls\n        if isinstance(ls, int):\n            return cls(mode=ls)\n        if isinstance(ls, str):\n            return cls(names=ls.split(\",\"))\n        raise TypeError(\"Invalid type for ls: {}\".format(type(ls)))\n", "idx": 1133}
{"namespace": "fs.permissions.Permissions.create", "completion": "        return cls(init)\n", "idx": 1134}
{"namespace": "fs.info.Info.suffix", "completion": "        return cast(Text, self.get(\"basic\", \"suffix\"))\n", "idx": 1135}
{"namespace": "fs.info.Info.suffixes", "completion": "        name = self.get(\"basic\", \"name\")\n        if name.startswith(\".\") and name.count(\".\") == 1:\n            return []\n        else:\n            return name.split(\".\")\n", "idx": 1136}
{"namespace": "fs.info.Info.stem", "completion": "        name = self.get(\"basic\", \"name\")\n        if name.startswith(\".\") and name.count(\".\") == 1:\n            return \"\"\n        basename, dot, ext = name.rpartition(\".\")\n        return basename if dot else name\n", "idx": 1137}
{"namespace": "fs.info.Info.type", "completion": "        self._require_namespace(\"details\")\n        return ResourceType(self.get(\"details\", \"type\"))\n", "idx": 1138}
{"namespace": "fs.info.Info.created", "completion": "        self._require_namespace(\"details\")\n        _time = self._make_datetime(self.get(\"details\", \"created\"))\n        return _time\n", "idx": 1139}
{"namespace": "pyinfra.connectors.mech.MechInventoryConnector.make_names_data", "completion": "        # Get Mech SSH information\n        mech_ssh_info = get_mech_config(limit)\n\n        # Process Mech SSH information to create a list of host names and their data\n        names_data = []\n        for host in mech_ssh_info:\n            name, data, groups = _make_name_data(host)\n            names_data.append((name, data, groups))\n\n        return names_data\n", "idx": 1140}
{"namespace": "pyinfra.connectors.ansible.AnsibleInventoryConnector.make_names_data", "completion": "        if inventory_filename is None:\n            raise InventoryError(\"No Ansible inventory filename provided!\")\n\n        if not path.exists(inventory_filename):\n            raise InventoryError(\n                \"Could not find Ansible inventory file: {0}\".format(\n                    inventory_filename\n                )\n            )\n\n        with open(inventory_filename, \"r\") as f:\n            data = yaml.safe_load(f)\n\n        return data\n", "idx": 1141}
{"namespace": "pyinfra.operations.files.rsync", "completion": "    show_rsync_warning()\n\n    # Ensure the source directory exists\n    if not os.path.isdir(src):\n        raise IOError(\"No such directory: {0}\".format(src))\n\n    # Ensure the destination directory exists\n    dest_to_ensure = dest\n    dest_link_info = host.get_fact(Link, path=dest)\n    if dest_link_info:\n        dest_to_ensure = dest_link_info[\"link_target\"]\n\n    yield from directory(\n        path=dest_to_ensure,\n        user=None,\n        group=None,\n        mode=None,\n    )\n\n    # Create the rsync command\n    rsync_command = RsyncCommand(\n        src,\n        dest,\n        flags=flags,\n    )\n\n    # Yield the rsync command\n    yield rsync_command\n\n", "idx": 1142}
{"namespace": "pyinfra.operations.files.get", "completion": "    # Add deploy directory?\n    if add_deploy_dir and state.cwd:\n        src = os.path.join(state.cwd, src)\n\n    # Ensure the destination directory exists\n    if create_local_dir:\n        dest_dir = os.path.dirname(dest)\n        if not os.path.isdir(dest_dir):\n            os.makedirs(dest_dir)\n\n    # Get the file's SHA1 hash\n    sha1_hash = host.get_fact(Sha1File, path=src)\n\n    # Get the file's SHA1 hash\n    sha256_hash = host.get_fact(Sha256File, path=src)\n\n    # Get the file's MD5 hash\n    md5_hash = host.get_fact(Md5File, path=src)\n\n    # Get the file's size\n    file_size = host.get_fact(File, path=src)[\"size\"]\n\n    # Get the file's modification time\n    file_mtime = host.get_fact(File, path=src)[\"mtime\"]\n\n    # Get the file's permissions\n    file_mode = host.get_fact(File, path=src)[\"mode\"]\n\n    # Get the file's owner\n    file_owner = host.get_fact(File, path=src)[\"owner\"]\n\n    # Get the file's group\n    file_group = host.get_fact(File, path=src)[\"group\"]\n\n    # Get the file's SHA1 hash\n    file_sha1_hash = host.get_fact(Sha1File, path=src)\n\n    # Get the file's SHA256 hash\n    file_sha256_hash = host.get_fact(Sha256File, path=src)\n\n    # Get the file's MD5 hash\n    file_md5_hash = host.get_fact(Md5File, path=src)\n\n    # Get the file's size\n    file_size = host.get_fact(File, path=src)[\"size\"]\n\n    # Get the file's modification time\n    file_mtime = host.get_fact(File, path=src)[\"mtime\"]\n\n    # Get the file's permissions\n    file_mode = host.get_fact(File, path=src)[\"mode\"]\n\n    # Get the file's owner\n    file_owner = host.get_fact", "idx": 1143}
{"namespace": "pyinfra.operations.files.put", "completion": "    if add_deploy_dir and state.cwd:\n        dest = os.path.join(state.cwd, dest)\n\n    if create_remote_dir:\n        remote_dirname = posixpath.dirname(dest)\n        if remote_dirname:\n            yield from directory(\n                path=remote_dirname,\n                user=user,\n                group=group,\n                mode=mode,\n                _no_check_owner_mode=True,  # don't check existing user/mode\n                _no_fail_on_link=True,  # don't fail if the path is a link\n            )\n\n    # If we're uploading a file-like object, we need to create a temporary file\n    # to upload.\n    if hasattr(src, \"read\"):\n        temp_filename = state.get_temp_filename(dest)\n        with open(temp_filename, \"wb\") as temp_file:\n            shutil.copyfileobj(src, temp_file)\n        src = temp_filename\n\n    # If we're uploading a file, we need to check if it exists.\n    if not os.path.exists(src):\n        if assume_exists:\n            host.noop(\"file {0} does not exist\".format(src))\n        else:\n            raise IOError(\"file {0} does not exist\".format(src))\n\n    # If we're uploading a file, we need to check if it's a directory.\n    if os.path.isdir(src):\n        raise IOError(\"file {0} is a directory\".format(src))\n\n    # If we're uploading a file, we need to check if it's a symlink.\n    if os.path.islink(src):\n        raise IOError(\"file {0} is a symlink\".format(src))\n\n    # If we're uploading a file, we need to check if it's a socket.\n    if os.path.issocket(src):\n        raise IOError(\"file {0} is a socket\".format(src))\n\n    # If we're uploading a file, we need to check if it's a block device.\n    if os.path.isblockdev(src):\n        raise IOError(\"file {0} is a block device\".format(src))\n\n    # If we're uploading a file, we need to check if it's a character device.\n    if os.path.ischardev(src):\n        raise IOError", "idx": 1144}
{"namespace": "pyinfra.operations.files.file", "completion": "    path = _validate_path(path)\n\n    if present and not touch:\n        raise OperationError(\"If present is True touch must be provided\")\n\n    info = host.get_fact(File, path=path)\n\n    if info is False:  # not a file\n        yield from _raise_or_remove_invalid_path(\n            \"file\",\n            path,\n            force,\n            force_backup,\n            force_backup_dir,\n        )\n        info = None\n\n    if not present:\n        if info:\n            yield StringCommand(\"rm\", \"-rf\", QuoteString(path))\n        else:\n            host.noop(\"file {0} does not exist\".format(path))\n        return\n\n    if info is None:  # create\n        if create_remote_dir:\n            yield from _create_remote_dir(state, host, path, user, group)\n\n        yield StringCommand(\"touch\", QuoteString(path))\n\n        if user or group:\n            yield file_utils.chown(path, user, group)\n\n        if mode:\n            yield file_utils.chmod(path, mode)\n\n    else:  # edit\n        changed = False\n\n        # If the target is wrong, remove & recreate the file\n        if not info or info[\"path\"] != path:\n            changed = True\n            yield StringCommand(\"rm\", \"-rf\", QuoteString(path))\n            yield StringCommand(\"touch\", QuoteString(path))\n\n        # Check user/group\n        if (user and info[\"user\"] != user) or (group and info[\"group\"] != group):\n            yield file_utils.chown(path, user, group)\n            changed = True\n\n        # Check mode\n        if mode and info[\"mode\"] != mode:\n            yield file_utils.chmod(path, mode)\n            changed = True\n\n        if not changed:\n            host.noop(\"file {0} already exists\".format(path))\n\n", "idx": 1145}
{"namespace": "pyinfra.operations.python.call", "completion": "    return FunctionCommand(function, *args, **kwargs)\n\n", "idx": 1146}
{"namespace": "pyinfra.api.operation.add_op", "completion": "    # get the operation name\n    op_name = op_func.__name__\n\n    # get the operation meta\n    op_meta = state.get_op_meta(op_name)\n\n    # get the operation arguments\n    op_args = state.get_op_args(op_name)\n\n    # get the operation kwargs\n    op_kwargs = state.get_op_kwargs(op_name)\n\n    # get the operation order\n    op_order = state.get_op_order(op_name)\n\n    # get the operation hash\n    op_hash = state.get_op_hash(op_name)\n\n    # get the operation meta\n    op_meta = state.get_op_meta(op_name)\n\n    # get the operation arguments\n    op_args = state.get_op_args(op_name)\n\n    # get the operation kwargs\n    op_kwargs = state.get_op_kwargs(op_name)\n\n    # get the operation order\n    op_order = state.get_op_order(op_name)\n\n    # get the operation hash\n    op_hash = state.get_op_hash(op_name)\n\n    # get the operation meta\n    op_meta = state.get_op_meta(op_name)\n\n    # get the operation arguments\n    op_args = state.get_op_args(op_name)\n\n    # get the operation kwargs\n    op_kwargs = state.get_op_kwargs(op_name)\n\n    # get the operation order\n    op_order = state.get_op_order(op_name)\n\n    # get the operation hash\n    op_hash = state.get_op_hash(op_name)\n\n    # get the operation meta\n    op_meta = state.get_op_meta(op_name)\n\n    # get the operation arguments\n    op_args = state.get_op_args(op_name)\n\n    # get the operation kwargs\n    op_kwargs = state.get_op_kwargs(op_name)\n\n    # get the operation order\n    op_order = state.get_op_order(op_name)\n\n    # get the operation hash\n    op_hash = state.get_op_hash(op_name)\n\n    # get the operation meta\n    op_meta = state.get_op_meta(op_name)", "idx": 1147}
{"namespace": "pyinfra.api.facts.get_facts", "completion": "    # TODO: this is here to avoid popping stuff accidentally, this is horrible! Change the\n    # pop function to return the clean kwargs to avoid the indirect mutation.\n    kwargs = kwargs.copy()\n\n    # Get the defaults *and* overrides by popping from kwargs, executor kwargs passed\n    # into get_fact override everything else (applied below).\n    override_kwargs, override_kwarg_keys = pop_global_arguments(\n        kwargs,\n        state=state,\n        keys_to_check=get_connector_argument_keys(),\n    )\n\n    executor_kwargs = _get_executor_kwargs(\n        state,\n        override_kwargs=override_kwargs,  # type: ignore[arg-type]\n        override_kwarg_keys=override_kwarg_keys,\n    )\n\n    # Get the facts for each host in the state's inventory\n    facts = {}\n    for host in state.inventory.iter_active_hosts():\n        facts[host] = get_fact(state, host, *args, **executor_kwargs, **kwargs)\n\n    return facts\n\n", "idx": 1148}
{"namespace": "pyinfra.api.operations.run_ops", "completion": "    if serial:\n        _run_serial_ops(state)\n    elif no_wait:\n        _run_no_wait_ops(state)\n    else:\n        for op_hash in state.get_op_order():\n            _run_single_op(state, op_hash)", "idx": 1149}
{"namespace": "pyinfra.api.connect.connect_all", "completion": "    \"\"\"\n    This function connects to all the configured servers in parallel. It reads and writes the inventory of the input State instance. It activates the hosts that are initially connected to and updates the state accordingly.\n    Input-Output Arguments\n    :param state: State. The state object containing the inventory to connect to.\n    :return: No return values.\n    \"\"\"\n\n    \"\"\"\n    This function connects to all the configured servers in parallel. It reads and writes the inventory of the input State instance. It activates the hosts that are initially connected to and updates the state accordingly.\n    Input-Output Arguments\n    :param state: State. The state object containing the inventory to connect to.\n    :return: No return values.\n    \"\"\"\n\n    \"\"\"\n    This function connects to all the configured servers in parallel. It reads and writes the inventory of the input State instance. It activates the hosts that are initially connected to and updates the state accordingly.\n    Input-Output Arguments\n    :param state: State. The state object containing the inventory to connect to.\n    :return: No return values.\n    \"\"\"\n\n    \"\"\"\n    This function connects to all the configured servers in parallel. It reads and writes the inventory of the input State instance. It activates the hosts that are initially connected to and updates the state accordingly.\n    Input-Output Arguments\n    :param state: State. The state object containing the inventory to connect to.\n    :return: No return values.\n    \"\"\"\n\n    \"\"\"\n    This function connects to all the configured servers in parallel. It reads and writes the inventory of the input State instance. It activates the hosts that are initially connected to and updates the state accordingly.\n    Input-Output Arguments\n    :param state: State. The state object containing the inventory to connect to.\n    :return: No return values.\n    \"\"\"\n\n    \"\"\"\n    This function connects to all the configured servers in parallel. It reads and writes the inventory of the input State instance. It activates the hosts that are initially connected to and updates the state accordingly.\n    Input-Output Arguments\n    :param state: State. The state object containing the inventory to connect to.\n    :return: No return values.\n    \"\"\"\n\n    \"\"\"\n    This function connects to all the configured servers in parallel. It reads and writes the inventory of the input State instance. It activates the hosts that are initially connected to and updates the state accordingly.\n    Input-Output Arguments\n    :param state: State. The state", "idx": 1150}
{"namespace": "pyinfra.api.arguments.pop_global_arguments", "completion": "    # Get the keys to check\n    if keys_to_check is None:\n        keys_to_check = get_connector_argument_keys() + get_execution_kwarg_keys()\n\n    # Get the arguments from the current context\n    current_context_arguments = state.context.get(\"arguments\", {}) if state else {}\n\n    # Get the arguments from the deploy context\n    deploy_context_arguments = state.deploy_context.get(\"arguments\", {}) if state else {}\n\n    # Get the arguments from the host data variables\n    host_data_arguments = host.data.get(\"arguments\", {}) if host else {}\n\n    # Get the arguments from the config variables\n    config_arguments = state.config.get(\"arguments\", {}) if state else {}\n\n    # Get the arguments from the kwargs\n    kwargs_arguments = {k: v for k, v in kwargs.items() if k in keys_to_check}\n\n    # Merge the arguments\n    arguments = {\n        **current_context_arguments,\n        **deploy_context_arguments,\n        **host_data_arguments,\n        **config_arguments,\n        **kwargs_arguments,\n    }\n\n    # Pop the arguments\n    popped_arguments = {k: arguments.pop(k) for k in keys_to_check if k in arguments}\n\n    return popped_arguments, list(popped_arguments.keys())\n\n", "idx": 1151}
{"namespace": "pyinfra_cli.commands.get_func_and_args", "completion": "    # Extract the operation name from the commands list.\n    operation_name = commands[0]\n\n    # Import the corresponding module attribute.\n    module_attribute = try_import_module_attribute(operation_name)\n\n    # Parse the arguments.\n    args = commands[1:]\n\n    # Return the operation function and its arguments.\n    return module_attribute, args\n\n", "idx": 1152}
{"namespace": "viztracer.tracer._VizTracer.start", "completion": "        if not self.initialized:\n            return\n\n        if self.enable:\n            return\n\n        if self.log_print:\n            self.system_print = builtins.print\n            builtins.print = self.print_log\n\n        if self.include_files and self.exclude_files:\n            raise ValueError(\"include_files and exclude_files cannot be specified at the same time.\")\n\n        self.enable = True\n        self.parsed = False\n        self.config()\n        self._tracer.start()\n", "idx": 1153}
{"namespace": "viztracer.tracer._VizTracer.stop", "completion": "        self.enable = False\n        if self.log_print:\n            self.restore_print()\n        self._tracer.stop()\n", "idx": 1154}
{"namespace": "viztracer.report_builder.ReportBuilder.save", "completion": "        if isinstance(output_file, str):\n            if output_file.endswith(\".html\"):\n                output_format = \"html\"\n            elif output_file.endswith(\".json\"):\n                output_format = \"json\"\n            elif output_file.endswith(\".gz\"):\n                output_format = \"gz\"\n            else:\n                raise ValueError(f\"Invalid file extension for {output_file}\")\n            if output_format == \"gz\":\n                with gzip.open(output_file, \"wt\", encoding=\"utf-8\") as f:\n                    self.generate_report(f, output_format, file_info)\n            else:\n                with open(output_file, \"w\", encoding=\"utf-8\") as f:\n                    self.generate_report(f, output_format, file_info)\n        elif isinstance(output_file, TextIO):\n            self.generate_report(output_file, \"html\", file_info)\n        else:\n            raise TypeError(\"Invalid output_file type\")\n        if self.verbose > 0:\n            for msg in self.final_messages:\n                if msg[0] == \"view_command\":\n                    color_print(f\"To view the report, run: {msg[1]['view_command']}\", \"green\")\n                else:\n                    color_print(f\"Message: {msg[0]}\", \"yellow\")\n                    if msg[1]:\n                        color_print(f\"Data: {msg[1]}\", \"yellow\")", "idx": 1155}
{"namespace": "viztracer.code_monkey.AstTransformer.get_assign_targets_with_attr", "completion": "        if isinstance(node, ast.Attribute):\n            return [node]\n        elif isinstance(node, (ast.Name, ast.Subscript, ast.Starred)):\n            return []\n        elif isinstance(node, (ast.Tuple, ast.List)):\n            return reduce(lambda a, b: a + b, [self.get_assign_targets_with_attr(elt) for elt in node.elts])\n        color_print(\"WARNING\", \"Unexpected node type {} for ast.Assign. \\\n            Please report to the author github.com/gaogaotiantian/viztracer\".format(type(node)))\n        return []\n", "idx": 1156}
{"namespace": "viztracer.code_monkey.SourceProcessor.process", "completion": "        if isinstance(source, bytes):\n            source = source.decode(\"utf-8\")\n        if not isinstance(source, str):\n            return source\n        lines = source.split(\"\\n\")\n        new_lines = []\n        for line in lines:\n            if self.is_comment(line):\n                new_lines.append(self.transform_comment(line))\n            else:\n                new_lines.append(line)\n        return \"\\n\".join(new_lines)\n", "idx": 1157}
{"namespace": "wal_e.log_help.WalELogger.fmt_logline", "completion": "        log_line = []\n        if msg:\n            log_line.append(\"MSG: {0}\".format(msg))\n        if detail:\n            log_line.append(\"DETAIL: {0}\".format(detail))\n        if hint:\n            log_line.append(\"HINT: {0}\".format(hint))\n        if structured:\n            log_line.append(\"STRUCTURED: {0}\".format(WalELogger._fmt_structured(structured)))\n\n        return '\\n'.join(log_line)\n", "idx": 1158}
{"namespace": "wal_e.blobstore.file.calling_format.Bucket.delete_keys", "completion": "        for key in keys:\n            key_path = os.path.join(self.name, key.strip(\"/\"))\n            if os.path.isfile(key_path):\n                os.remove(key_path)\n            elif os.path.isdir(key_path):\n                shutil.rmtree(key_path)\n        remove_empty_dirs(self.name)\n\n", "idx": 1159}
{"namespace": "wal_e.worker.upload_pool.TarUploadPool.put", "completion": "        # Check if there is too much work already\n        if self.member_burden >= self.max_members:\n            raise tar_partition.TooMuchWorkError(\n                \"Too much work already, please wait for some to finish.\")\n\n        # Check if there is too much concurrency already\n        if self.concurrency_burden >= self.max_concurrency:\n            raise tar_partition.TooMuchConcurrencyError(\n                \"Too much concurrency already, please wait for some to finish.\")\n\n        # Start the upload\n        self._start(tpart)\n\n        # Wait for the upload to finish\n        self._wait()\n", "idx": 1160}
{"namespace": "wal_e.worker.pg.wal_transfer.WalSegment.from_ready_archive_status", "completion": "        # Get the archive_status directory path\n        status_dir = path.join(xlog_dir, 'archive_status')\n\n        # Iterate through the files in the archive_status directory\n        for filename in os.listdir(status_dir):\n            # Filter out non-segment files\n            if not re.match(storage.SEGMENT_REGEXP, filename):\n                continue\n\n            # Create a WalSegment instance for each segment file\n            segment_path = path.join(status_dir, filename)\n            yield WalSegment(segment_path)\n", "idx": 1161}
{"namespace": "wal_e.worker.pg.wal_transfer.WalTransferGroup.join", "completion": "        # Wait for the transfer to exit.\n        self.wait_change.get()\n\n        # Close the input WalTransferGroup instance.\n        self.closed = True\n\n        # Wait for all running greenlets to exit.\n        for greenlet in self.greenlets:\n            greenlet.kill()\n\n        # Wait for all running greenlets to exit.\n        gevent.joinall(self.greenlets)\n", "idx": 1162}
{"namespace": "wal_e.worker.pg.wal_transfer.WalTransferGroup.start", "completion": "        # Create a greenlet to execute the transferer function with the given segment as an argument.\n        g = gevent.spawn(self.transferer, segment)\n\n        # Add the greenlet to the set of greenlets.\n        self.greenlets.add(g)\n\n        # Start the execution.\n        g.start()\n", "idx": 1163}
{"namespace": "mrjob.py2.to_unicode", "completion": "    if isinstance(s, str):\n        return s\n    else:\n        try:\n            return s.decode('utf-8')\n        except UnicodeDecodeError:\n            return s.decode('latin-1')", "idx": 1164}
{"namespace": "mrjob.job.MRJob.steps", "completion": "        # create a dictionary of redefined methods\n        redefined_methods = {}\n        for method in dir(self):\n            if not method.startswith('_') and method != 'steps':\n                redefined_methods[method] = getattr(self, method)\n\n        # create a dictionary of redefined methods that are not redefined\n        redefined_methods_not_redefined = {}\n        for method in dir(MRJob):\n            if not method.startswith('_') and method != 'steps':\n                if method not in redefined_methods:\n                    redefined_methods_not_redefined[method] = getattr(MRJob, method)\n\n        # create a dictionary of redefined methods that are not redefined and are not in the list of methods that are redefined\n        redefined_methods_not_redefined_not_redefined = {}\n        for method in dir(MRJob):\n            if not method.startswith('_') and method != 'steps':\n                if method not in redefined_methods and method not in redefined_methods_not_redefined:\n                    redefined_methods_not_redefined_not_redefined[method] = getattr(MRJob, method)\n\n        # create a dictionary of redefined methods that are not redefined and are not in the list of methods that are redefined and are not in the list of methods that are not redefined\n        redefined_methods_not_redefined_not_redefined_not_redefined = {}\n        for method in dir(MRJob):\n            if not method.startswith('_') and method != 'steps':\n                if method not in redefined_methods and method not in redefined_methods_not_redefined and method not in redefined_methods_not_redefined_not_redefined:\n                    redefined_methods_not_redefined_not_redefined_not_redefined[method] = getattr(MRJob, method)\n\n        # create a dictionary of redefined methods that are not redefined and are not in the list of methods that are redefined and are not in the list of methods that are not redefined and are not in the list of methods that are not redefined\n        redefined_methods_not_redefined_not_redefined_not_redefined_not_redefined = {}\n        for method in dir(MRJob):\n            if not method.startswith('_')", "idx": 1165}
{"namespace": "mrjob.job.MRJob.increment_counter", "completion": "        # Replace commas in group and counter with semicolons to avoid confusion with Hadoop streaming\n        group = group.replace(',', ';')\n        counter = counter.replace(',', ';')\n\n        # Construct the line to be outputted\n        line = \"reporter:counter:{group},{counter},{amount}\\n\".format(group=group, counter=counter, amount=amount)\n\n        # Output the line to the standard error stream\n        self.stderr.write(line)\n", "idx": 1166}
{"namespace": "mrjob.job.MRJob.set_status", "completion": "        # cast non-strings to strings (if people pass in exceptions, etc)\n        if not isinstance(msg, string_types):\n            msg = str(msg)\n\n        line = 'reporter:status:%s\\n' % msg\n        if not isinstance(line, bytes):\n            line = line.encode('utf_8')\n\n        self.stderr.write(line)\n        self.stderr.flush()\n", "idx": 1167}
{"namespace": "mrjob.job.MRJob.run_job", "completion": "        # set up logging\n        log.info('setting up job from %s', self)\n\n        # create a runner\n        runner = self._runner_class()()\n\n        # run the job\n        runner.run(self)\n", "idx": 1168}
{"namespace": "mrjob.job.MRJob.set_up_logging", "completion": "        # set up logging\n        if quiet:\n            log.disabled = True\n        elif verbose:\n            log.setLevel(logging.DEBUG)\n        else:\n            log.setLevel(logging.INFO)\n\n        # set up logging for the main module\n        if stream is None:\n            stream = sys.stderr\n        log_stream = codecs.getwriter('utf_8')(stream)\n        logging.basicConfig(stream=log_stream, level=log.level)\n", "idx": 1169}
{"namespace": "mrjob.job.MRJob.execute", "completion": "        if self.options.runner:\n            self._run_from_runner()\n        elif self.options.mapper:\n            self.run_mapper()\n        elif self.options.combiner:\n            self.run_combiner()\n        elif self.options.reducer:\n            self.run_reducer()\n        elif self.options.spark:\n            self.run_spark()\n        else:\n            self.run_job()\n", "idx": 1170}
{"namespace": "mrjob.job.MRJob._runner_kwargs", "completion": "        # non-option keyword arguments\n        kwargs = self._non_option_kwargs()\n\n        # keyword arguments from switches\n        kwargs.update(self._kwargs_from_switches())\n\n        # keyword arguments from the job\n        kwargs.update(self._job_kwargs())\n\n        # include the MRJob class in the keyword arguments if the runner class is \"inline\" or \"spark\"\n        if self._runner_class() in (MRJobRunner, SparkStepRunner):\n            kwargs['job_class'] = self.__class__\n\n        # include the steps description in the keyword arguments\n        kwargs['steps'] = self.steps()\n\n        return kwargs\n", "idx": 1171}
{"namespace": "mrjob.job.MRJob.run_mapper", "completion": "        step = self._get_step(step_num, MRStep)\n\n        # get input and output protocols\n        input_protocol = step.input_protocol\n        output_protocol = step.output_protocol\n\n        # get input and output paths\n        input_path = step.input_path\n        output_path = step.output_path\n\n        # get input and output uris\n        input_uri = step.input_uri\n        output_uri = step.output_uri\n\n        # get input and output files\n        input_file = step.input_file\n        output_file = step.output_file\n\n        # get input and output streams\n        input_stream = step.input_stream\n        output_stream = step.output_stream\n\n        # get input and output paths\n        input_paths = step.input_paths\n        output_paths = step.output_paths\n\n        # get input and output uris\n        input_uris = step.input_uris\n        output_uris = step.output_uris\n\n        # get input and output files\n        input_files = step.input_files\n        output_files = step.output_files\n\n        # get input and output streams\n        input_streams = step.input_streams\n        output_streams = step.output_streams\n\n        # get input and output paths\n        input_paths = step.input_paths\n        output_paths = step.output_paths\n\n        # get input and output uris\n        input_uris = step.input_uris\n        output_uris = step.output_uris\n\n        # get input and output files\n        input_files = step.input_files\n        output_files = step.output_files\n\n        # get input and output streams\n        input_streams = step.input_streams\n        output_streams = step.output_streams\n\n        # get input and output paths\n        input_paths = step.input_paths\n        output_paths = step.output_paths\n\n        # get input and output uris\n        input_uris = step.input_uris\n        output_uris = step.output_uris\n\n        # get input and output files\n        input_files = step.input_files\n        output_files = step.output_files\n\n        # get input and output streams\n        input_streams = step.input_streams\n        output_streams = step.output_streams\n\n        # get input and output paths\n        input_paths = step.input_paths\n        output_paths", "idx": 1172}
{"namespace": "mrjob.job.MRJob.run_combiner", "completion": "        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'combiner')\n\n        for k, v in self.combine_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)\n", "idx": 1173}
{"namespace": "mrjob.job.MRJob.add_passthru_arg", "completion": "        pass_opt = self.arg_parser.add_argument(*args, **kwargs)\n\n        self._passthru_arg_dests.add(pass_opt.dest)\n", "idx": 1174}
{"namespace": "mrjob.job.MRJob.is_task", "completion": "        if self.options.run_mapper:\n            return True\n        elif self.options.run_combiner:\n            return True\n        elif self.options.run_reducer:\n            return True\n        elif self.options.run_spark:\n            return True\n        else:\n            return False\n", "idx": 1175}
{"namespace": "mrjob.job.MRJob.parse_output", "completion": "        for chunk in chunks:\n            for line in to_lines(chunk):\n                key, value = self.output_protocol().read(line.rstrip(b'\\r\\n'))\n                yield key, value\n", "idx": 1176}
{"namespace": "mrjob.job.MRJob.sandbox", "completion": "        if stdin is None:\n            stdin = BytesIO()\n        if stdout is None:\n            stdout = BytesIO()\n        if stderr is None:\n            stderr = BytesIO()\n\n        self.stdin = stdin\n        self.stdout = stdout\n        self.stderr = stderr\n\n        return self\n", "idx": 1177}
{"namespace": "mrjob.hadoop.fully_qualify_hdfs_path", "completion": "    if path.startswith('hdfs://'):\n        return path\n\n    if path.startswith('/'):\n        return 'hdfs://' + path\n\n    return 'hdfs:///user/' + getpass.getuser() + '/' + path\n\n", "idx": 1178}
{"namespace": "mrjob.hadoop.HadoopJobRunner.fs", "completion": "        if not hasattr(self, '_fs'):\n            self._fs = CompositeFilesystem(\n                HadoopFilesystem(self._hadoop_tmp_dir),\n                LocalFilesystem())\n        return self._fs\n", "idx": 1179}
{"namespace": "mrjob.hadoop.HadoopJobRunner._find_hadoop_streaming_jar", "completion": "        # If the user has specified the path of the Hadoop streaming jar file, return it.\n        if self._opts['hadoop_streaming_jar']:\n            return self._opts['hadoop_streaming_jar']\n\n        # If the user has specified the path of the Hadoop streaming jar file, return it.\n        if self._opts['hadoop_streaming_jar']:\n            return self._opts['hadoop_streaming_jar']\n\n        # If the user has specified the path of the Hadoop streaming jar file, return it.\n        if self._opts['hadoop_streaming_jar']:\n            return self._opts['hadoop_streaming_jar']\n\n        # If the user has specified the path of the Hadoop streaming jar file, return it.\n        if self._opts['hadoop_streaming_jar']:\n            return self._opts['hadoop_streaming_jar']\n\n        # If the user has specified the path of the Hadoop streaming jar file, return it.\n        if self._opts['hadoop_streaming_jar']:\n            return self._opts['hadoop_streaming_jar']\n\n        # If the user has specified the path of the Hadoop streaming jar file, return it.\n        if self._opts['hadoop_streaming_jar']:\n            return self._opts['hadoop_streaming_jar']\n\n        # If the user has specified the path of the Hadoop streaming jar file, return it.\n        if self._opts['hadoop_streaming_jar']:\n            return self._opts['hadoop_streaming_jar']\n\n        # If the user has specified the path of the Hadoop streaming jar file, return it.\n        if self._opts['hadoop_streaming_jar']:\n            return self._opts['hadoop_streaming_jar']\n\n        # If the user has specified the path of the Hadoop streaming jar file, return it.\n        if self._opts['hadoop_streaming_jar']:\n            return self._opts['hadoop_streaming_jar']\n\n        # If the user has specified the path of the Hadoop streaming jar file, return it.\n        if self._opts['hadoop_streaming_jar']:\n            return self._opts['hadoop_streaming_jar']\n\n        # If the user has specified the path of the Hadoop streaming jar file, return it.\n        if self._opts['hadoop_streaming_jar']:\n            return self._opts['hadoop_streaming_jar']\n\n        # If the user has specified the path of the Hadoop streaming jar file, return it.\n        if self._opts['hadoop_", "idx": 1180}
{"namespace": "mrjob.hadoop.HadoopJobRunner._find_binaries_and_jars", "completion": "        # Find the Hadoop binary\n        hadoop_bin = self.get_hadoop_bin()\n        if hadoop_bin is None:\n            raise RuntimeError('Could not find Hadoop binary. '\n                               'Please set HADOOP_HOME or HADOOP_PREFIX '\n                               'or HADOOP_INSTALL or HADOOP_MAPRED_HOME '\n                               'or HADOOP_STREAMING_JAR or HADOOP_BIN.')\n\n        # Find the Hadoop streaming jar\n        hadoop_streaming_jar = self.get_hadoop_streaming_jar()\n        if hadoop_streaming_jar is None:\n            raise RuntimeError('Could not find Hadoop streaming jar. '\n                               'Please set HADOOP_STREAMING_JAR.')\n\n        # Find the Spark submit binary\n        spark_submit_bin = self._find_spark_submit_bin()\n        if spark_submit_bin is None:\n            raise RuntimeError('Could not find Spark submit binary. '\n                               'Please set SPARK_HOME or SPARK_BIN.')\n\n        # Find the Spark submit jar\n        spark_submit_jar = self._find_spark_submit_jar()\n        if spark_submit_jar is None:\n            raise RuntimeError('Could not find Spark submit jar. '\n                               'Please set SPARK_HOME or SPARK_BIN.')\n\n        # Find the Spark submit driver class\n        spark_submit_driver_class = self._find_spark_submit_driver_class()\n        if spark_submit_driver_class is None:\n            raise RuntimeError('Could not find Spark submit driver class. '\n                               'Please set SPARK_HOME or SPARK_BIN.')\n\n        # Find the Spark submit driver args\n        spark_submit_driver_args = self._find_spark_submit_driver_args()\n        if spark_submit_driver_args is None:\n            raise RuntimeError('Could not find Spark submit driver args. '\n                               'Please set SPARK_HOME or SPARK_BIN.')\n\n        # Find the Spark submit executor class\n        spark_submit_executor_class = self._find_spark_submit_executor_class()\n        if spark_submit_executor_class is None:\n            raise RuntimeError('Could not find Spark submit executor class. '\n                               'Please set SPARK_HOME or SPARK_BIN.')\n\n        # Find the Spark submit executor args\n        spark_submit_executor_args = self._find_spark_", "idx": 1181}
{"namespace": "mrjob.hadoop.HadoopJobRunner._args_for_streaming_step", "completion": "        # check if the Hadoop streaming jar is available\n        if not self.get_hadoop_streaming_jar():\n            raise Exception('no Hadoop streaming jar')\n\n        # construct the command line arguments for the Hadoop streaming step\n        hadoop_bin = self.get_hadoop_bin()\n        hadoop_streaming_jar = self.get_hadoop_streaming_jar()\n        step_args = [hadoop_bin[0], 'jar', hadoop_streaming_jar] + self._args_for_step(step_num)\n\n        return step_args\n", "idx": 1182}
{"namespace": "mrjob.hadoop.HadoopJobRunner._stream_history_log_dirs", "completion": "        if self._opts['read_logs']:\n            for log_dir in self._hadoop_log_dirs(output_dir):\n                yield [log_dir]\n", "idx": 1183}
{"namespace": "mrjob.hadoop.HadoopJobRunner._stream_task_log_dirs", "completion": "        if not self._read_logs():\n            return\n\n        for log_dir in unique(self._hadoop_log_dirs(output_dir=output_dir)):\n            if _logs_exist(self.fs, log_dir):\n                log.info('Looking for task logs in %s...' % log_dir)\n                # logs aren't always in a subdir named history/\n                yield [log_dir]\n", "idx": 1184}
{"namespace": "mrjob.setup.UploadDirManager.add", "completion": "        if is_uri(path):\n            return path\n        else:\n            path = expand_path(path)\n            name = name_uniquely(path, self._names_taken, unhide=True)\n            self._path_to_name[path] = name\n            self._names_taken.add(name)\n            return self.uri(path)\n", "idx": 1185}
{"namespace": "mrjob.setup.UploadDirManager.uri", "completion": "        if is_uri(path):\n            return path\n        elif path in self._path_to_name:\n            return posixpath.join(self.prefix, self._path_to_name[path])\n        else:\n            raise ValueError('%r is not a URI or a known local file' % path)\n", "idx": 1186}
{"namespace": "mrjob.setup.UploadDirManager.path_to_uri", "completion": "        return self._path_to_name\n\n", "idx": 1187}
{"namespace": "mrjob.setup.WorkingDirManager.name_to_path", "completion": "        if type is None:\n            return self._name_to_typed_path\n        else:\n            return {name: path for name, (path_type, path) in self._name_to_typed_path.items() if path_type == type}\n", "idx": 1188}
{"namespace": "mrjob.setup.WorkingDirManager.paths", "completion": "        if type is not None:\n            self._check_type(type)\n\n        paths = set()\n        for path_type, path in self._typed_path_to_auto_name:\n            if type is None or path_type == type:\n                paths.add(path)\n\n        return paths\n", "idx": 1189}
{"namespace": "mrjob.compat.jobconf_from_env", "completion": "    if variable in os.environ:\n        return os.environ[variable]\n    else:\n        for jobconf_dict in _JOBCONF_DICT_LIST:\n            if variable in jobconf_dict:\n                return os.environ.get(jobconf_dict[variable], default)\n    return default\n\n", "idx": 1190}
{"namespace": "mrjob.compat.jobconf_from_dict", "completion": "    # try variable verbatim first\n    if name in jobconf:\n        return jobconf[name]\n\n    # try alternatives (arbitrary order)\n    for var in _JOBCONF_MAP.get(name, {}).values():\n        if var in jobconf:\n            return jobconf[var]\n\n    return default\n\n", "idx": 1191}
{"namespace": "mrjob.compat.translate_jobconf", "completion": "    if variable in _JOBCONF_MAP:\n        return map_version(version, _JOBCONF_MAP[variable])\n    else:\n        return variable\n\n", "idx": 1192}
{"namespace": "mrjob.compat.translate_jobconf_for_all_versions", "completion": "    if variable in _JOBCONF_MAP:\n        return sorted(_JOBCONF_MAP[variable].values())\n    else:\n        return [variable]\n\n", "idx": 1193}
{"namespace": "mrjob.compat.translate_jobconf_dict", "completion": "    if hadoop_version is None:\n        raise TypeError\n\n    translated_jobconf = {}\n    for key, value in jobconf.items():\n        translated_jobconf[translate_jobconf(key, hadoop_version)] = value\n\n    # print warning message if any configuration property names do not match the name in the hadoop version\n    untranslated_jobconf = []\n    for key in jobconf.keys():\n        if key not in translated_jobconf.keys():\n            untranslated_jobconf.append(key)\n\n    if untranslated_jobconf:\n        log.warning(\"Detected hadoop configuration property names that do not match version {hadoop version}:\\nThe have been translated to the following names:\\n{translated names}\".format(\n            hadoop_version=hadoop_version,\n            translated_names='\\n'.join(sorted(untranslated_jobconf))\n        ))\n\n    return {**jobconf, **translated_jobconf}\n\n", "idx": 1194}
{"namespace": "mrjob.compat.uses_yarn", "completion": "    if version is None:\n        raise TypeError\n\n    if not version:\n        raise ValueError\n\n    return LooseVersion(version) >= LooseVersion('2.0.0')\n\n", "idx": 1195}
{"namespace": "mrjob.local.LocalMRJobRunner._spark_master", "completion": "        num_executors = self._opts['num_executors']\n        cores_per_executor = self._opts['cores_per_executor']\n        executor_memory = self._opts['executor_memory']\n\n        # If the user has specified a custom executor memory, we use that. Otherwise, we calculate the executor memory based on the number of executors, cores per executor, and the default executor memory.\n        if executor_memory:\n            executor_memory_mb = int(executor_memory.split('g')[0]) * 1024\n        else:\n            executor_memory_mb = int(_DEFAULT_EXECUTOR_MEMORY.split('g')[0]) * 1024\n\n        # Round up the executor memory to the nearest multiple of 1024 MB.\n        executor_memory_mb = int(math.ceil(executor_memory_mb / 1024.0)) * 1024\n\n        return 'local-cluster[{num_executors},{cores_per_executor},{executor_memory_mb}]'.format(num_executors=num_executors, cores_per_executor=cores_per_executor, executor_memory_mb=executor_memory_mb)\n", "idx": 1196}
{"namespace": "mrjob.runner.MRJobRunner._bootstrap_mrjob", "completion": "        if self._opts['bootstrap_mrjob'] is None:\n            return True\n        else:\n            return self._opts['bootstrap_mrjob']\n", "idx": 1197}
{"namespace": "mrjob.conf._fix_clear_tags", "completion": "    if isinstance(x, list):\n        for i in range(len(x)):\n            x[i] = _fix_clear_tags(x[i])\n    elif isinstance(x, dict):\n        for k in x.keys():\n            if isinstance(k, ClearedValue):\n                x[k.value] = _fix_clear_tags(x[k])\n                del x[k]\n            else:\n                x[k] = _fix_clear_tags(x[k])\n    elif isinstance(x, ClearedValue):\n        return x.value\n    return x\n\n", "idx": 1198}
{"namespace": "mrjob.conf.load_opts_from_mrjob_conf", "completion": "    if already_loaded is None:\n        already_loaded = []\n\n    conf_path = _expanded_mrjob_conf_path(conf_path)\n\n    if conf_path is None:\n        return [(None, {})]\n\n    if conf_path in already_loaded:\n        return []\n\n    already_loaded.append(conf_path)\n\n    conf_object = _conf_object_at_path(conf_path)\n\n    if conf_object is None:\n        return [(None, {})]\n\n    if 'runners' not in conf_object:\n        return [(None, {})]\n\n    if runner_alias not in conf_object['runners']:\n        return [(None, {})]\n\n    runner_conf = conf_object['runners'][runner_alias]\n\n    if 'includes' in runner_conf:\n        for include in runner_conf['includes']:\n            if include not in already_loaded:\n                already_loaded.extend(load_opts_from_mrjob_conf(runner_alias, include, already_loaded))\n\n    return [(conf_path, runner_conf)]\n\n", "idx": 1199}
{"namespace": "mrjob.conf.load_opts_from_mrjob_confs", "completion": "    if conf_paths is None:\n        conf_paths = [find_mrjob_conf()]\n\n    if not conf_paths:\n        log.warning('No config specified for %s runner', runner_alias)\n        return [(None, {})]\n\n    # load all the config files\n    conf_paths = [expand_path(conf_path) for conf_path in conf_paths]\n    confs = []\n    for conf_path in conf_paths:\n        confs += _load_opts_from_mrjob_conf(runner_alias, conf_path)\n\n    return confs\n\n", "idx": 1200}
{"namespace": "mrjob.conf.dump_mrjob_conf", "completion": "    if yaml:\n        _dump_yaml_with_clear_tags(conf, f)\n    else:\n        json.dump(conf, f, indent=2)\n\n", "idx": 1201}
{"namespace": "mrjob.conf.combine_lists", "completion": "    return [item for seq in seqs for item in seq if item is not None]\n\n", "idx": 1202}
{"namespace": "mrjob.conf.combine_cmds", "completion": "    result = []\n\n    for cmd in cmds:\n        if cmd is None:\n            continue\n\n        if isinstance(cmd, (bytes, string_types)):\n            result.append(shlex_split(cmd))\n        else:\n            result.append(cmd)\n\n    return result\n\n", "idx": 1203}
{"namespace": "mrjob.conf.combine_dicts", "completion": "    result = {}\n\n    for d in dicts:\n        if d is None:\n            continue\n\n        for k, v in d.items():\n            if isinstance(v, ClearedValue) and v.value is None:\n                del result[k]\n            else:\n                result[k] = v\n\n    return result\n\n", "idx": 1204}
{"namespace": "mrjob.conf.combine_jobconfs", "completion": "    result = {}\n    for jobconf in jobconfs:\n        if jobconf:\n            for k, v in jobconf.items():\n                if isinstance(v, string_types):\n                    result[k] = v\n                elif v is None:\n                    result.pop(k, None)\n                else:\n                    result[k] = str(v)\n    return result\n\n", "idx": 1205}
{"namespace": "mrjob.conf.combine_path_lists", "completion": "    result = []\n\n    for path_seq in path_seqs:\n        if path_seq is None:\n            continue\n\n        if isinstance(path_seq, string_types):\n            result.append(expand_path(path_seq))\n        else:\n            result.extend(expand_path(p) for p in path_seq)\n\n    return result\n\n", "idx": 1206}
{"namespace": "mrjob.conf.combine_opts", "completion": "    # collect all the keys from the dictionaries that are not wrapped in ClearedValue\n    keys = set()\n    for opts in opts_list:\n        for key in opts.keys():\n            if not isinstance(opts[key], ClearedValue):\n                keys.add(key)\n\n    # create a new dictionary to store the combined options\n    combined_opts = {}\n\n    # iterate through each key and use the sub-combiner specified in the combiners map for that key, or defaults to a function\n    for key in keys:\n        if key in combiners:\n            combined_opts[key] = combiners[key](*[_strip_clear_tag(opts[key]) for opts in opts_list])\n        else:\n            combined_opts[key] = combine_values(*[_strip_clear_tag(opts[key]) for opts in opts_list])\n\n    return combined_opts\n\n", "idx": 1207}
{"namespace": "mrjob.bin.MRJobBinRunner._task_python_bin", "completion": "        return self._opts['task_python_bin'] or self._default_python_bin()\n", "idx": 1208}
{"namespace": "mrjob.bin.MRJobBinRunner.get_spark_submit_bin", "completion": "        if self._spark_submit_bin is None:\n            self._spark_submit_bin = which(self._opts['spark_submit_bin'])\n\n        return self._spark_submit_bin\n", "idx": 1209}
{"namespace": "mrjob.step.StepFailedException.__str__", "completion": "        if self.reason:\n            return '%s failed: %s' % (self.step_desc, self.reason)\n        else:\n            return '%s failed' % self.step_desc\n", "idx": 1210}
{"namespace": "mrjob.step.StepFailedException.__repr__", "completion": "        return '{class name}({\", \"-separated list of fields: {field name}={field value}})'.format(\n            class name=self.__class__.__name__,\n            \", \"-separated list of fields: {field name}={field value}\".format(\n                \", \".join(\n                    \"{field name}={field value}\".format(\n                        field name=field,\n                        field value=getattr(self, field)\n                    ) for field in self._FIELDS\n                )\n            )\n        )\n\n", "idx": 1211}
{"namespace": "mrjob.step.MRStep.description", "completion": "        desc = {'type': 'streaming'}\n        if self.has_explicit_mapper or step_num == 0 or self.has_explicit_combiner:\n            desc['mapper'] = self.render_mapper()\n        if self.has_explicit_combiner:\n            desc['combiner'] = self.render_combiner()\n        if self.has_explicit_reducer:\n            desc['reducer'] = self.render_reducer()\n        if self.has_explicit_mapper and self._steps['mapper_raw']:\n            desc['input_manifest'] = True\n        if self._steps['jobconf']:\n            desc['jobconf'] = self._steps['jobconf']\n        return desc\n\n", "idx": 1212}
{"namespace": "mrjob.step._Step.description", "completion": "        desc = {'type': self._STEP_TYPE}\n        for k in self._STEP_ATTRS:\n            if k not in self._HIDDEN_ATTRS:\n                desc[k] = getattr(self, k)\n        return desc\n\n", "idx": 1213}
{"namespace": "mrjob.protocol._KeyCachingProtocol.read", "completion": "        key, value = line.split('\\t', 1)\n        self._last_key_encoded = key\n        self._last_key_decoded = self._loads(key)\n        return self._last_key_decoded, self._loads(value)\n", "idx": 1214}
{"namespace": "mrjob.util.safeeval", "completion": "    if globals is None:\n        globals = {}\n    if locals is None:\n        locals = {}\n\n    # Add some safe builtins\n    safe_builtins = {\n        'True': True,\n        'False': False,\n        'None': None,\n        'set': set,\n        'range': range,\n        'xrange': xrange,\n        'open': open,\n    }\n    globals.update(safe_builtins)\n\n    # Add some safe functions\n    safe_functions = {\n        'len': len,\n        'abs': abs,\n        'sum': sum,\n        'min': min,\n        'max': max,\n        'round': round,\n        'pow': pow,\n        'range': range,\n        'xrange': xrange,\n        'open': open,\n    }\n    locals.update(safe_functions)\n\n    # Add some safe classes\n    safe_classes = {\n        'str': str,\n        'int': int,\n        'float': float,\n        'list': list,\n        'tuple': tuple,\n        'dict': dict,\n        'set': set,\n        'frozenset': frozenset,\n        'object': object,\n    }\n    locals.update(safe_classes)\n\n    # Add some safe modules\n    safe_modules = {\n        'os': os,\n        'sys': sys,\n        'math': math,\n        'random': random,\n        're': re,\n        'datetime': datetime,\n        'time': time,\n        'calendar': calendar,\n        'itertools': itertools,\n        'functools': functools,\n        'operator': operator,\n        'collections': collections,\n        'heapq': heapq,\n        'bisect': bisect,\n        'array': array,\n        'struct': struct,\n        'json': json,\n        'zlib': zlib,\n        'gzip': gzip,\n        'bz2': bz2,\n        'lzma': lzma,\n        'uuid': uuid,\n        'base64': base64,\n        'binascii': binascii,\n        'hashlib': hashlib,\n        'hmac': hmac,\n        'md5': md5,\n        'sha': sha,\n        'sha224': sha224,\n        'sha256': sha256,\n        'sha384': sha384,\n        'sha512': sha512,\n       ", "idx": 1215}
{"namespace": "mrjob.util.to_lines", "completion": "    if hasattr(chunks, 'readline'):\n        return chunks\n\n    for chunk in chunks:\n        for line in chunk.splitlines():\n            yield line\n\n", "idx": 1216}
{"namespace": "mrjob.parse.is_s3_uri", "completion": "    try:\n        parsed_uri = urlparse(uri)\n        return parsed_uri.scheme == 's3'\n    except ValueError:\n        return False\n\n", "idx": 1217}
{"namespace": "mrjob.parse.parse_s3_uri", "completion": "    if not is_s3_uri(uri):\n        raise ValueError('Not an S3 URI: %s' % uri)\n\n    parsed = urlparse(uri)\n    if parsed.scheme != 's3':\n        raise ValueError('Not an S3 URI: %s' % uri)\n\n    if parsed.netloc == '':\n        raise ValueError('No bucket name in S3 URI: %s' % uri)\n\n    if parsed.path == '':\n        raise ValueError('No key in S3 URI: %s' % uri)\n\n    return parsed.netloc, parsed.path[1:]\n\n", "idx": 1218}
{"namespace": "mrjob.parse.to_uri", "completion": "    if is_uri(path_or_uri):\n        return path_or_uri\n    else:\n        return pathname2url(abspath(path_or_uri))\n\n", "idx": 1219}
{"namespace": "mrjob.parse.parse_mr_job_stderr", "completion": "    if isinstance(stderr, BytesIO):\n        stderr = stderr.getvalue()\n\n    if isinstance(stderr, list):\n        stderr = b'\\n'.join(stderr)\n\n    if not isinstance(stderr, bytes):\n        raise TypeError('Expected bytes, got %s' % type(stderr))\n\n    if counters is None:\n        counters = {}\n\n    statuses = []\n    other = []\n\n    for line in stderr.split(b'\\n'):\n        if line.startswith(b'reporter:counter:'):\n            match = _COUNTER_RE.match(line)\n            if match:\n                group, name, count = match.groups()\n                group = group.decode('utf-8')\n                name = name.decode('utf-8')\n                count = int(count)\n                if group not in counters:\n                    counters[group] = {}\n                counters[group][name] = counters[group].get(name, 0) + count\n            else:\n                other.append(line.decode('utf-8'))\n        elif line.startswith(b'reporter:status:'):\n            match = _STATUS_RE.match(line)\n            if match:\n                statuses.append(match.group(1).decode('utf-8'))\n            else:\n                other.append(line.decode('utf-8'))\n        else:\n            other.append(line.decode('utf-8'))\n\n    return {'counters': counters, 'statuses': statuses, 'other': other}", "idx": 1220}
{"namespace": "mrjob.parse._parse_progress_from_job_tracker", "completion": "    # The HTML content is searched between 'Running Jobs' and 'Jobs'\n    html_bytes = html_bytes[html_bytes.find(b'Running Jobs'):html_bytes.find(b'Jobs')]\n\n    # The map_percent and reduce_percent values are extracted from the HTML content\n    map_percent = _JOB_TRACKER_HTML_RE.search(html_bytes)\n    if map_percent:\n        map_percent = float(map_percent.group(1))\n    else:\n        map_percent = None\n\n    reduce_percent = _JOB_TRACKER_HTML_RE.search(html_bytes)\n    if reduce_percent:\n        reduce_percent = float(reduce_percent.group(1))\n    else:\n        reduce_percent = None\n\n    return map_percent, reduce_percent\n\n", "idx": 1221}
{"namespace": "mrjob.parse._parse_progress_from_resource_manager", "completion": "    # snip out the Running Jobs section (ignore the header)\n    start = html_bytes.rfind(b'Running Jobs')\n    if start == -1:\n        return None\n    end = html_bytes.find(b'Jobs', start + len(b'Running Jobs'))\n    if end == -1:\n        end = None\n\n    html_bytes = html_bytes[start:end]\n\n    # search it for percents\n    matches = _RESOURCE_MANAGER_JS_RE.search(html_bytes)\n    if matches:\n        return float(matches.group('percent'))\n    else:\n        return None\n\n", "idx": 1222}
{"namespace": "mrjob.logs.task._match_task_log_path", "completion": "    if not path:\n        return None\n\n    # YARN\n    if _YARN_TASK_LOG_PATH_RE.match(path):\n        match = _YARN_TASK_LOG_PATH_RE.match(path)\n        if match:\n            if application_id and match.group('application_id') != application_id:\n                return None\n            if job_id and match.group('application_id') != job_id:\n                return None\n            return {\n                'application_id': match.group('application_id'),\n                'container_id': match.group('container_id'),\n                'log_type': match.group('log_type'),\n            }\n\n    # pre-YARN\n    if _PRE_YARN_TASK_LOG_PATH_RE.match(path):\n        match = _PRE_YARN_TASK_LOG_PATH_RE.match(path)\n        if match:\n            if application_id and match.group('application_id') != application_id:\n                return None\n            if job_id and match.group('application_id') != job_id:\n                return None\n            return {\n                'attempt_id': match.group('attempt_id'),\n                'log_type': match.group('log_type'),\n            }\n\n    return None\n\n", "idx": 1223}
{"namespace": "mrjob.logs.task._parse_task_syslog", "completion": "    result = {}\n\n    # check for errors in stdout\n    check_stdout = _parse_task_stdout(lines)\n    if check_stdout:\n        result['check_stdout'] = check_stdout\n\n    # check for errors in stderr\n    hadoop_error = _parse_task_stderr(lines)\n    if hadoop_error:\n        result['hadoop_error'] = hadoop_error\n\n    # check for input splits\n    split = _parse_task_input_split(lines)\n    if split:\n        result['split'] = split\n\n    return result\n\n", "idx": 1224}
{"namespace": "mrjob.logs.ids._sort_for_spark", "completion": "    # Sort by time\n    ds = sorted(ds, key=_time_sort_key, reverse=True)\n\n    # Sort by ID\n    ds = sorted(ds, key=_id_sort_key, reverse=True)\n\n    # Sort by error\n    ds = sorted(ds, key=_error_sort_key, reverse=True)\n\n    # Sort by log\n    ds = sorted(ds, key=_log_sort_key, reverse=True)\n\n    return ds\n\n", "idx": 1225}
{"namespace": "mrjob.logs.spark._parse_spark_log", "completion": "    # if the log is a step log, it will have a line like\n    # \"Application ID: application_1541888110140_0001\"\n    # if it's a task log, it will have a line like\n    # \"Application ID: application_1541888110140_0001_000001\"\n    # we can use this to extract the application ID\n    application_id = _SUBMITTED_APPLICATION_RE.search(lines[0])\n    if application_id:\n        application_id = application_id.group(1)\n    else:\n        application_id = None\n\n    # if the log is a step log, it will have a line like\n    # \"Log: /tmp/spark-12345678901234567890/spark-yarn-app-user-1541888110140-0001.out\"\n    # if it's a task log, it will have a line like\n    # \"Log: /tmp/spark-12345678901234567890/spark-yarn-app-user-1541888110140-0001-000001.out\"\n    # we can use this to extract the log file\n    log_file = None\n    for line in lines:\n        if line.startswith('Log: '):\n            log_file = line[5:]\n            break\n\n    # if the log is a step log, it will have a line like\n    # \"Driver log: /tmp/spark-12345678901234567890/spark-yarn-app-user-1541888110140-0001-driver.out\"\n    # if it's a task log, it will have a line like\n    # \"Driver log: /tmp/spark-12345678901234567890/spark-yarn-app-user-1541888110140-", "idx": 1226}
{"namespace": "mrjob.logs.mixin.LogInterpretationMixin._pick_error", "completion": "        if self._step_type_uses_spark(step_type):\n            return None\n\n        if self._read_logs():\n            if not self._pick_error_from_logs(log_interpretation, step_type):\n                self._interpret_history_log(log_interpretation)\n                self._pick_error_from_logs(log_interpretation, step_type)\n\n        return None\n", "idx": 1227}
{"namespace": "mrjob.logs.history._match_history_log_path", "completion": "    match = _HISTORY_LOG_PATH_RE.match(path)\n    if match:\n        if job_id is not None and match.group('job_id') != job_id:\n            return None\n        else:\n            return {'job_id': match.group('job_id'), 'yarn': True}\n    else:\n        return None\n\n", "idx": 1228}
{"namespace": "mrjob.logs.history._parse_pre_yarn_history_log", "completion": "    result = {}\n    task_to_counters = {}  # used for successful tasks in failed jobs\n\n    for line_num, line in enumerate(lines):\n        # empty space or \"Avro-Json\" header\n        if not line.startswith('TASK'):\n            continue\n\n        # parse the line\n        m = _PRE_YARN_HISTORY_RECORD.match(line)\n        if not m:\n            continue\n\n        # extract events. Looks like there's just one per record\n        events = m.group('key_pairs').split()\n\n        # update container_id -> attempt_id mapping\n        for event in events:\n            if 'TASKID' in event:\n                result.setdefault('attempt_to_container_id', {})\n                result['attempt_to_container_id'][\n                    event.split('=')[1]] = event.split('=')[1]\n\n        if 'FAILED' in events:\n            for event in events:\n                if 'ERROR' in event:\n                    err_msg = event.split('=')[1]\n                    error = dict(\n                        hadoop_error=dict(\n                            message=err_msg,\n                            start_line=line_num,\n                            num_lines=1))\n\n                    if 'TASKID' in event:\n                        error['task_id'] = event.split('=')[1]\n\n                    if 'ATTEMPTID' in event:\n                        error['attempt_id'] = event.split('=')[1]\n\n                    result.setdefault('errors', [])\n                    result['errors'].append(error)\n\n        elif 'COUNTERS' in events:\n            for event in events:\n                if 'TASKID' in event:\n                    task_id = event.split('=')[1]\n                    counters_record = event.split('=')[1]\n                    task_to_counters[task_id] = _extract_pre_yarn_counters(\n                        counters_record)\n\n        elif 'JOB' in events:\n            for event in events:\n                if 'COUNTERS' in event:\n                    counters_record = event.split('=')[1]\n                    result['counters'] = _extract_pre_yarn_counters(\n                        counters_record)\n\n    # if job failed, patch together counters from successful tasks\n    if 'counters' not in result and task_to_counters:\n        result['counters'] = _sum_counters(*task_to_counters.values())\n\n    return result", "idx": 1229}
{"namespace": "mrjob.logs.history._parse_pre_yarn_history_records", "completion": "    # The regex is used to match the beginning of a record.\n    # The regex is used to match the end of a record.\n    # The regex is used to match the fields of a record.\n    # The regex is used to match the type of a record.\n    # The regex is used to match the fields of a record.\n    # The regex is used to match the fields of a record.\n    # The regex is used to match the fields of a record.\n    # The regex is used to match the fields of a record.\n    # The regex is used to match the fields of a record.\n    # The regex is used to match the fields of a record.\n    # The regex is used to match the fields of a record.\n    # The regex is used to match the fields of a record.\n    # The regex is used to match the fields of a record.\n    # The regex is used to match the fields of a record.\n    # The regex is used to match the fields of a record.\n    # The regex is used to match the fields of a record.\n    # The regex is used to match the fields of a record.\n    # The regex is used to match the fields of a record.\n    # The regex is used to match the fields of a record.\n    # The regex is used to match the fields of a record.\n    # The regex is used to match the fields of a record.\n    # The regex is used to match the fields of a record.\n    # The regex is used to match the fields of a record.\n    # The regex is used to match the fields of a record.\n    # The regex is used to match the fields of a record.\n    # The regex is used to match the fields of a record.\n    # The regex is used to match the fields of a record.\n    # The regex is used to match the fields of a record.\n    # The regex is used to match the fields of a record.\n    # The regex is used to match the fields of a record.\n    # The regex is used to match the fields of a record.\n    # The regex is used to match the fields of a record.\n    # The regex is used to match the fields of a record.\n    # The regex is used to match the fields of a record.\n    # The regex is used to match the fields of a record.\n    # The regex is used to match the fields", "idx": 1230}
{"namespace": "mrjob.logs.step._parse_step_syslog", "completion": "    result = {}\n\n    # hadoop streaming always prints \"packageJobJar...\" to stdout,\n    # and prints Streaming Command Failed! to stderr on failure\n    for line in lines:\n        if _HADOOP_STREAMING_NON_LOG4J_LINE_RE.match(line):\n            result['errors'] = [dict(hadoop_error=dict(message=line))]\n            return result\n\n    # first line of counters message\n    m = _INDENTED_COUNTERS_MESSAGE_RE.match(lines[0])\n    if m:\n        result['counters'] = dict(num_counters=int(m.group('num_counters')))\n        return result\n\n    # header for a group of counters\n    m = _INDENTED_COUNTER_GROUP_RE.match(lines[0])\n    if m:\n        result['counters'] = dict(group=m.group('group'))\n        return result\n\n    # line for a counter\n    m = _INDENTED_COUNTER_RE.match(lines[0])\n    if m:\n        result['counters'] = dict(\n            counter=m.group('counter'),\n            amount=int(m.group('amount')))\n        return result\n\n    # line specifying output directory\n    m = _OUTPUT_DIRECTORY_RE.match(lines[0])\n    if m:\n        result['output_dir'] = m.group('output_dir')\n        return result\n\n    # how to get application_id from YARN\n    m = _SUBMITTED_APPLICATION_RE.match(lines[0])\n    if m:\n        result['application_id'] = m.group('application_id')\n        return result\n\n    # how to get job_id (all versions)\n    m = _RUNNING_JOB_RE.match(lines[0])\n    if m:\n        result['job_id'] = m.group('job_id')\n        return result\n\n    # job progress (YARN)\n    # no need to make this work for pre-YARN, only Dataproc runner uses it\n    m = _JOB_PROGRESS_RE.match(lines[0])\n    if m:\n        result['job_progress'] = dict(\n            map=int(m.group('map')),\n            reduce=int(m.group('reduce')))\n        return result\n\n    # if you specify a bad jar, this is all you get\n    m = _NOT_A_", "idx": 1231}
{"namespace": "mrjob.logs.errors._merge_and_sort_errors", "completion": "    # Initialize a dictionary to save errors\n    merged_errors = {}\n\n    # Iterate through each error in the given list of errors and merge them by container id.\n    for error in errors:\n        if error.get('container_id'):\n            if error.get('container_id') in merged_errors:\n                merged_errors[error.get('container_id')].append(error)\n            else:\n                merged_errors[error.get('container_id')] = [error]\n        else:\n            if error.get('time') in merged_errors:\n                merged_errors[error.get('time')].append(error)\n            else:\n                merged_errors[error.get('time')] = [error]\n\n    # Use a custom key sort function to prioritize task errors and sort the errors based on their keys.\n    def custom_key_sort(error):\n        if error.get('container_id'):\n            return error.get('container_id')\n        else:\n            return error.get('time')\n\n    return sorted(merged_errors.values(), key=custom_key_sort, reverse=True)\n\n", "idx": 1232}
{"namespace": "mrjob.fs.ssh.SSHFilesystem.ls", "completion": "        # get the path of the file system\n        path = self._get_path(path_glob)\n\n        # get the hostname of the file system\n        hostname = self._get_hostname(path)\n\n        # get the path of the file system\n        filesystem_path = self._get_filesystem_path(path)\n\n        # get the command to execute\n        command = ['find', filesystem_path, '-type', 'f']\n\n        # execute the command\n        p = self._ssh_launch(hostname, command)\n\n        # read the output of the command\n        stdout, stderr = p.communicate()\n\n        # close the file handles\n        self._ssh_finish_run(p)\n\n        # return the output of the command\n        return stdout.splitlines()\n", "idx": 1233}
{"namespace": "mrjob.fs.ssh.SSHFilesystem._cat_file", "completion": "        m = _SSH_URI_RE.match(path)\n        addr = m.group('hostname')\n        path_to_cat = m.group('filesystem_path')\n\n        p = self._ssh_launch(addr, ['zcat', path_to_cat])\n\n        for line in p.stdout:\n            yield line\n\n        self._ssh_finish_run(p)\n", "idx": 1234}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.get_hadoop_bin", "completion": "        if self._hadoop_bin is None:\n            self._hadoop_bin = which('hadoop')\n\n        return self._hadoop_bin\n", "idx": 1235}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.du", "completion": "        hadoop_bin = self.get_hadoop_bin()\n        hadoop_bin.append('fs')\n        hadoop_bin.append('-du')\n        hadoop_bin.append(path_glob)\n\n        output = self.invoke_hadoop(hadoop_bin, return_stdout=True)\n\n        if output == b'0\\n':\n            return 0\n\n        elif output == b'1\\n':\n            return 1\n\n        elif output == b'255\\n':\n            raise IOError('Unexpected output from Hadoop fs -du: {output!r}')\n\n        else:\n            output = output.split(b'\\n')\n            output = output[0].split(b'\\t')\n            return int(output[0])\n", "idx": 1236}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.mkdir", "completion": "        # Hadoop 'fs -mkdir' command (additionally with '-p' option on Hadoop 2)\n        args = self.get_hadoop_bin() + ['fs', '-mkdir', '-p', path]\n        log.debug('> %s' % cmd_line(args))\n\n        proc = Popen(args, stdout=PIPE, stderr=PIPE)\n        stdout, stderr = proc.communicate()\n\n        # check if STDERR is okay\n        stderr_is_ok = False\n        if stderr:\n            for stderr_re in [_HADOOP_FILE_EXISTS_RE]:\n                if stderr_re.match(stderr):\n                    stderr_is_ok = True\n                    break\n\n        if not stderr_is_ok:\n            for line in BytesIO(stderr):\n                log.error('STDERR: ' + to_unicode(line.rstrip(b'\\r\\n')))\n\n        if proc.returncode != 0:\n            raise IOError(\"Could not mkdir %s\" % path)\n", "idx": 1237}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.exists", "completion": "        components = urlparse(path_glob)\n        hdfs_prefix = '%s://%s' % (components.scheme, components.netloc)\n\n        version = self.get_hadoop_version()\n\n        # use -d on Hadoop 2 (see #1152)\n        if uses_yarn(version):\n            args = ['fs', '-ls', '-d', path_glob]\n        else:\n            args = ['fs', '-lsr', path_glob]\n\n        try:\n            self.invoke_hadoop(args, ok_returncodes=[0, 1, 255],\n                               ok_stderr=[_HADOOP_LS_NO_SUCH_FILE])\n        except CalledProcessError:\n            raise IOError(\"Could not check path %s\" % path_glob)\n\n        return True\n", "idx": 1238}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.rm", "completion": "        # check if path is a URI\n        if is_uri(path_glob):\n            # if it is, then we need to remove the file or directory\n            self.invoke_hadoop(['fs', '-rm', '-r', path_glob])\n        else:\n            # if it is not, then we need to remove the file or directory using the superclass\n            super(HadoopFilesystem, self).rm(path_glob)\n", "idx": 1239}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.touchz", "completion": "        self.invoke_hadoop(['fs', '-touchz', path])\n", "idx": 1240}
{"namespace": "mrjob.fs.local.LocalFilesystem.du", "completion": "        # Convert the input path to a local file path format.\n        path_glob = path_glob.replace('file:///', '')\n\n        # Iterate through all the files in the given path and get the file size.\n        total_size = 0\n        for file in glob.glob(path_glob):\n            total_size += os.path.getsize(file)\n\n        return total_size\n", "idx": 1241}
{"namespace": "mrjob.fs.local.LocalFilesystem.ls", "completion": "        path_glob = _from_file_uri(path_glob)\n        if os.path.isdir(path_glob):\n            for root, dirs, files in os.walk(path_glob):\n                for file in files:\n                    yield os.path.join(root, file)\n        else:\n            yield path_glob\n", "idx": 1242}
{"namespace": "mrjob.fs.local.LocalFilesystem._cat_file", "completion": "        path = _from_file_uri(path)\n        with open(path, 'rb') as f:\n            while True:\n                chunk = f.read(1024)\n                if not chunk:\n                    break\n                yield chunk\n", "idx": 1243}
{"namespace": "mrjob.fs.local.LocalFilesystem.exists", "completion": "        path_glob = _from_file_uri(path_glob)\n        return any(os.path.exists(path) for path in glob.glob(path_glob))\n", "idx": 1244}
{"namespace": "mrjob.fs.local.LocalFilesystem.mkdir", "completion": "        path = _from_file_uri(path)\n        if not os.path.exists(path):\n            os.makedirs(path)\n", "idx": 1245}
{"namespace": "mrjob.fs.local.LocalFilesystem.put", "completion": "        src = _from_file_uri(src)\n        path = _from_file_uri(path)\n        shutil.copy(src, path)\n", "idx": 1246}
{"namespace": "mrjob.fs.local.LocalFilesystem.rm", "completion": "        path_glob = _from_file_uri(path_glob)\n        for path in glob.glob(path_glob):\n            if os.path.isdir(path):\n                shutil.rmtree(path)\n            else:\n                os.remove(path)\n", "idx": 1247}
{"namespace": "mrjob.fs.local.LocalFilesystem.touchz", "completion": "        path = _from_file_uri(path)\n        if os.path.exists(path):\n            if os.path.getsize(path) > 0:\n                raise OSError('File already exists and is not empty')\n        else:\n            open(path, 'a').close()\n", "idx": 1248}
{"namespace": "mrjob.fs.local.LocalFilesystem.md5sum", "completion": "        path = _from_file_uri(path)\n        with open(path, 'rb') as f:\n            return self._md5sum_file(f)\n\n", "idx": 1249}
{"namespace": "mrjob.fs.composite.CompositeFilesystem.add_fs", "completion": "        setattr(self, name, fs)\n        self._fs_names.append(name)\n        if disable_if:\n            self._disable_if[name] = disable_if\n", "idx": 1250}
{"namespace": "mrjob.fs.base.Filesystem.cat", "completion": "        raise NotImplementedError\n", "idx": 1251}
{"namespace": "mrjob.fs.base.Filesystem.join", "completion": "        if path.startswith('s3://'):\n            return posixpath.join(path, *paths)\n        else:\n            return posixpath.join(path, *paths)\n", "idx": 1252}
{"namespace": "mrjob.examples.mr_text_classifier.parse_doc_filename", "completion": "    # Extract the id and categories from the filename\n    id, cats = re.match(r'(.*)-([^-]+)-([^-]+)-([^-]+)\\.txt', input_uri).groups()\n    cats = dict(zip(cats.split('-'), [True, False, False]))\n\n    # Return the parsed information\n    return dict(id=id, cats=cats)\n\n", "idx": 1253}
{"namespace": "prometheus_client.mmap_dict.MmapedDict.read_value", "completion": "        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        encoded_len = _unpack_integer(self._m, pos)[0]\n        # check we are not reading beyond bounds\n        if encoded_len + pos > self._used:\n            raise RuntimeError('Read beyond file size detected, file is corrupted.')\n        pos += 4\n        encoded_key = self._m[pos:pos + encoded_len]\n        padded_len = encoded_len + (8 - (encoded_len + 4) % 8)\n        pos += padded_len\n        value, timestamp = _unpack_two_doubles(self._m, pos)\n        return value, timestamp\n", "idx": 1254}
{"namespace": "prometheus_client.mmap_dict.MmapedDict.write_value", "completion": "        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        _pack_two_doubles(self._m, pos, value, timestamp)\n", "idx": 1255}
{"namespace": "prometheus_client.multiprocess.MultiProcessCollector.merge", "completion": "        if not files:\n            return {}\n\n        merged = {}\n        for file in files:\n            with open(file, 'rb') as f:\n                metrics = pickle.load(f)\n                for name, metric in metrics.items():\n                    if name not in merged:\n                        merged[name] = metric\n                    else:\n                        if accumulate:\n                            merged[name].add(metric)\n                        else:\n                            merged[name].set(metric)\n        return merged\n", "idx": 1256}
{"namespace": "prometheus_client.multiprocess.MultiProcessCollector.collect", "completion": "        files = glob.glob(os.path.join(self._path, '*.db'))\n        return self.merge(files)", "idx": 1257}
{"namespace": "prometheus_client.exposition.choose_encoder", "completion": "    if accept_header is None:\n        return generate_latest, CONTENT_TYPE_LATEST\n\n    accept_header = accept_header.lower()\n    if 'application/openmetrics-text' in accept_header:\n        return generate_latest, CONTENT_TYPE_LATEST\n    else:\n        return generate_latest, CONTENT_TYPE_LATEST\n\n", "idx": 1258}
{"namespace": "flower.command.apply_options", "completion": "    # pylint: disable=protected-access\n    conf_file_name = options._options['conf'].default\n    if conf_file_name == 'celeryconfig.py':\n        conf_file_name = 'celeryconfig.py'\n    else:\n        conf_file_name = 'flowerconfig.py'\n    try:\n        parse_config_file(conf_file_name)\n    except IOError:\n        pass\n    parse_command_line(argv)\n    parse_config_file(conf_file_name)\n\n", "idx": 1259}
{"namespace": "trackerjacker.ieee_mac_vendor_db.MacVendorDB.lookup", "completion": "        mac = mac.upper().replace(':', '')\n        for prefix, vendor in self.db.items():\n            if mac.startswith(prefix):\n                return vendor\n        return ''", "idx": 1260}
{"namespace": "trailscraper.iam.Statement.merge", "completion": "        if self.Effect != other.Effect:\n            raise ValueError(\"Trying to combine two statements with differing effects: {self.Effect} {other.Effect}\")\n\n        merged_actions = sorted(set(self.Action + other.Action))\n        merged_resources = sorted(set(self.Resource + other.Resource))\n\n        return Statement(Action=merged_actions, Effect=self.Effect, Resource=merged_resources)\n", "idx": 1261}
{"namespace": "trailscraper.iam.parse_policy_document", "completion": "    if isinstance(stream, str):\n        json_data = json.loads(stream)\n    else:\n        json_data = json.load(stream)\n\n    return PolicyDocument(Statement=_parse_statements(json_data['Statement']), Version=json_data['Version'])\n\n", "idx": 1262}
{"namespace": "trailscraper.iam.known_iam_actions", "completion": "    known_actions = all_known_iam_permissions()\n    actions = [Action(prefix=prefix, action=action) for action in known_actions]\n    actions_by_prefix = groupbyz(lambda action: action.prefix)(actions)\n    return actions_by_prefix.get(prefix, [])\n\n", "idx": 1263}
{"namespace": "trailscraper.boto_service_definitions.service_definition_file", "completion": "    files = boto_service_definition_files()\n    files = [file for file in files if fnmatch.fnmatch(file, '**/' + servicename + '/*/service-*.json')]\n    files = sorted(files, key=lambda x: x.split('/')[-1])\n    return files[-1]\n\n\n", "idx": 1264}
{"namespace": "trailscraper.boto_service_definitions.operation_definition", "completion": "    service_definition_file = service_definition_file(servicename)\n    with open(service_definition_file, 'r') as f:\n        service_definition = json.load(f)\n\n    return service_definition['operations'][operationname]\n\n", "idx": 1265}
{"namespace": "trailscraper.cloudtrail.Record.to_statement", "completion": "        if self.event_source == \"sts.amazonaws.com\" and self.event_name == \"GetCallerIdentity\":\n            return None\n\n        if self.event_source == \"s3.amazonaws.com\":\n            return self._to_api_gateway_statement()\n\n        iam_prefix = self._source_to_iam_prefix()\n        iam_action = self._event_name_to_iam_action()\n\n        return Statement(\n            Effect=\"Allow\",\n            Action=[Action(iam_prefix, iam_action)],\n            Resource=[f\"arn:aws:s3:::{self.event_source}\"]\n        )\n\n", "idx": 1266}
{"namespace": "trailscraper.cloudtrail.filter_records", "completion": "    return filterz(_by_timeframe(from_date, to_date),\n                   _by_role_arns(arns_to_filter_for),\n                   records)", "idx": 1267}
{"namespace": "trailscraper.record_sources.local_directory_record_source.LocalDirectoryRecordSource.load_from_dir", "completion": "        return pipe(self._valid_log_files(),\n                    mapz(lambda log_file: log_file.load_from_file(from_date, to_date)),\n                    sortedz(key=lambda record: record.event_time),\n                    list)", "idx": 1268}
{"namespace": "pyt.__main__.discover_files", "completion": "    discovered_files = []\n    for target in targets:\n        if os.path.isdir(target):\n            for root, _, files in os.walk(target):\n                for file in files:\n                    if file.endswith(\".py\"):\n                        discovered_files.append(os.path.join(root, file))\n        else:\n            discovered_files.append(target)\n\n    log.debug(\"Discovered files: %s\", \", \".join(discovered_files))\n    return discovered_files\n\n", "idx": 1269}
{"namespace": "pyt.core.project_handler.get_directory_modules", "completion": "    if _local_modules:\n        if directory == _directory_of_first_module:\n            return _local_modules\n        else:\n            directory = os.path.dirname(directory)\n    if not os.path.isdir(directory):\n        return list()\n    _local_modules.clear()\n    for file in os.listdir(directory):\n        if file.endswith('.py'):\n            _local_modules.append((file[:-3], os.path.join(directory, file)))\n    return _local_modules\n\n", "idx": 1270}
{"namespace": "pyt.vulnerabilities.vulnerabilities.find_triggers", "completion": "    trigger_nodes = list()\n\n    for node in nodes:\n        if node.line_number not in nosec_lines:\n            for trigger_word in trigger_words:\n                if trigger_word.label in node.label:\n                    trigger_nodes.append(TriggerNode(trigger_word, node))\n\n    return trigger_nodes\n\n", "idx": 1271}
{"namespace": "pyt.vulnerabilities.vulnerabilities.label_contains", "completion": "    for trigger in triggers:\n        if trigger.label in node.label:\n            yield TriggerNode(trigger, node)\n\n", "idx": 1272}
{"namespace": "pyt.vulnerabilities.vulnerabilities.build_sanitiser_node_dict", "completion": "    sanitiser_node_dict = dict()\n    sanitisers = extract_sanitisers(sinks_in_file)\n    for sanitiser in sanitisers:\n        sanitiser_node_dict[sanitiser] = find_sanitiser_nodes(cfg, sanitiser)\n    return sanitiser_node_dict\n\n", "idx": 1273}
{"namespace": "pyt.vulnerabilities.trigger_definitions_parser.parse", "completion": "    with open(trigger_word_file, 'r') as f:\n        data = json.load(f)\n\n    sources = [Source(trigger) for trigger in data['sources']]\n    sinks = [Sink.from_json(key, value) for key, value in data['sinks'].items()]\n\n    return Definitions(sources, sinks)", "idx": 1274}
{"namespace": "principalmapper.querying.local_policy_simulation._statement_matches_resource", "completion": "    if 'Resource' in statement:\n        for item in _listify_string(statement['Resource']):\n            if _matches_after_expansion(resource, item, condition_keys):\n                return True\n        return False\n    elif 'NotResource' in statement:\n        result = True\n        for item in _listify_string(statement['NotResource']):\n            if _matches_after_expansion(resource, item, condition_keys):\n                result = False\n                break\n        return result\n    else:\n        return True\n\n", "idx": 1275}
{"namespace": "principalmapper.querying.local_policy_simulation._matches_after_expansion", "completion": "    # If the second string is a regular expression, use re.match to check if the first string matches the second string.\n    if re.match(r\"^/(.*)/$\", string_to_check_against):\n        return re.match(string_to_check, string_to_check_against) is not None\n\n    # If the second string is a variable, use the condition_keys dictionary to substitute the variable with its corresponding value.\n    if string_to_check_against.startswith(\"${\") and string_to_check_against.endswith(\"}\"):\n        variable_name = string_to_check_against[2:-1]\n        if condition_keys is not None and variable_name in condition_keys:\n            return _matches_after_expansion(string_to_check, condition_keys[variable_name])\n\n    # If the second string is a wildcard, use the _compose_pattern function to compile the wildcard into a regular expression.\n    if string_to_check_against.startswith(\"*\") and string_to_check_against.endswith(\"*\"):\n        return _compose_pattern(string_to_check_against[1:-1]).match(string_to_check) is not None\n\n    # If the second string is a wildcard, use the _compose_pattern function to compile the wildcard into a regular expression.\n    if string_to_check_against.startswith(\"*\") and not string_to_check_against.endswith(\"*\"):\n        return _compose_pattern(string_to_check_against[1:]).match(string_to_check) is not None\n\n    # If the second string is a wildcard, use the _compose_pattern function to compile the wildcard into a regular expression.\n    if not string_to_check_against.startswith(\"*\") and string_to_check_against.endswith(\"*\"):\n        return _compose_pattern(string_to_check_against[:-1]).match(string_to_check) is not None\n\n    # If the second string is a wildcard, use the _compose_pattern function to compile the wildcard into a regular expression.\n    if string_to_check_against.startswith(\"*\") and string_to_check_against.endswith(\"*\"):\n        return _compose_pattern(string_to_check_", "idx": 1276}
{"namespace": "passpie.database.PasspieStorage.delete", "completion": "        for cred in credentials:\n            credpath = self.make_credpath(cred[\"name\"], cred[\"login\"])\n            os.remove(credpath)\n            dirname = os.path.dirname(credpath)\n            if not os.listdir(dirname):\n                os.rmdir(dirname)\n", "idx": 1277}
{"namespace": "passpie.database.PasspieStorage.read", "completion": "        data = {}\n        for dirname, dirnames, filenames in os.walk(self.path):\n            for filename in filenames:\n                if filename.endswith(self.extension):\n                    with open(os.path.join(dirname, filename), \"r\") as f:\n                        data[filename] = yaml.load(f)\n        return data\n", "idx": 1278}
{"namespace": "threatingestor.state.State.save_state", "completion": "        try:\n            self.cursor.execute('INSERT OR REPLACE INTO states VALUES (?, ?)', (name, state))\n            self.conn.commit()\n        except sqlite3.Error:\n            raise threatingestor.exceptions.IngestorError(\"State database seems broken\")\n\n", "idx": 1279}
{"namespace": "threatingestor.state.State.get_state", "completion": "        logger.debug(f\"Retrieving state for '{name}'\")\n        self.cursor.execute('SELECT state FROM states WHERE name = ?', (name,))\n        state = self.cursor.fetchone()\n        if state:\n            return state[0]\n        else:\n            return None\n\n", "idx": 1280}
{"namespace": "threatingestor.Ingestor.run", "completion": "        # Run once or forever.\n        if self.config.run_once():\n            self.run_once()\n        else:\n            self.run_forever()\n", "idx": 1281}
{"namespace": "msticpy.analysis.anomalous_sequence.model.Model.compute_scores", "completion": "        self._compute_session_likelihoods(use_start_end_tokens)\n        self._compute_session_geomean_likelihoods(use_start_end_tokens)\n        self._compute_rare_windows(use_start_end_tokens)\n        self._compute_rare_window_likelihoods(use_start_end_tokens)\n        self._compute_rare_window_geomean_likelihoods(use_start_end_tokens)\n", "idx": 1282}
{"namespace": "msticpy.analysis.anomalous_sequence.model.Model.compute_rarest_windows", "completion": "        if self.session_likelihoods is None:\n            self.compute_likelihoods_of_sessions(use_start_end_tokens=use_start_end_tokens)\n\n        if use_geo_mean:\n            self.session_likelihoods = [\n                self.session_likelihoods[idx] ** (1 / window_len)\n                for idx, session in enumerate(self.sessions)\n            ]\n\n        result = []\n        for sess in self.session_likelihoods:\n            tmp = []\n            for i in range(len(sess) - window_len + 1):\n                tmp.append(np.prod(sess[i : i + window_len]))\n            result.append(tmp)\n\n        self.rare_windows[window_len] = result\n", "idx": 1283}
{"namespace": "msticpy.analysis.anomalous_sequence.anomalous.score_sessions", "completion": "    # Get the sessions\n    sessions = data[session_column].unique()\n\n    # Get the number of sessions\n    num_sessions = len(sessions)\n\n    # Get the number of rows in the DataFrame\n    num_rows = len(data)\n\n    # Get the number of columns in the DataFrame\n    num_cols = len(data.columns)\n\n    # Get the number of sessions in the DataFrame\n    num_sessions_df = len(data[session_column].unique())\n\n    # Get the number of rows in the DataFrame\n    num_rows_df = len(data)\n\n    # Get the number of columns in the DataFrame\n    num_cols_df = len(data.columns)\n\n    # Get the number of sessions in the DataFrame\n    num_sessions_df = len(data[session_column].unique())\n\n    # Get the number of rows in the DataFrame\n    num_rows_df = len(data)\n\n    # Get the number of columns in the DataFrame\n    num_cols_df = len(data.columns)\n\n    # Get the number of sessions in the DataFrame\n    num_sessions_df = len(data[session_column].unique())\n\n    # Get the number of rows in the DataFrame\n    num_rows_df = len(data)\n\n    # Get the number of columns in the DataFrame\n    num_cols_df = len(data.columns)\n\n    # Get the number of sessions in the DataFrame\n    num_sessions_df = len(data[session_column].unique())\n\n    # Get the number of rows in the DataFrame\n    num_rows_df = len(data)\n\n    # Get the number of columns in the DataFrame\n    num_cols_df = len(data.columns)\n\n    # Get the number of sessions in the DataFrame\n    num_sessions_df = len(data[session_column].unique())\n\n    # Get the number of rows in the DataFrame\n    num_rows_df = len(data)\n\n    # Get the number of columns in the DataFrame\n    num_cols_df = len(data.columns)\n\n    # Get the number of sessions in the DataFrame\n    num_sessions_df = len(data[session_column].unique())\n\n    # Get the number of rows in the DataFrame\n    num_rows_df = len(data)\n\n    # Get the number of columns in the DataFrame\n    num_cols_df = len(data.columns)\n\n    #", "idx": 1284}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.laplace_smooth_counts", "completion": "    seq1_counts_sm = StateMatrix(seq1_counts)\n    seq2_counts_sm = StateMatrix(seq2_counts)\n    param_counts_sm = StateMatrix(param_counts)\n    cmd_param_counts_sm = StateMatrix(cmd_param_counts)\n\n    for cmd in seq1_counts_sm.keys():\n        seq1_counts_sm[cmd] += 1\n        seq2_counts_sm[start_token][cmd] += 1\n        seq2_counts_sm[cmd][end_token] += 1\n\n    for cmd in seq2_counts_sm.keys():\n        for cmd2 in seq2_counts_sm[cmd].keys():\n            seq2_counts_sm[cmd][cmd2] += 1\n\n    for par in param_counts_sm.keys():\n        param_counts_sm[par] += 1\n\n    for cmd in cmd_param_counts_sm.keys():\n        for par in cmd_param_counts_sm[cmd].keys():\n            cmd_param_counts_sm[cmd][par] += 1\n\n    return seq1_counts_sm, seq2_counts_sm, param_counts_sm, cmd_param_counts_sm\n\n", "idx": 1285}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.compute_likelihood_window", "completion": "    if use_start_token:\n        window = [Cmd(name=start_token)] + window\n    if use_end_token:\n        window = window + [Cmd(name=end_token)]\n\n    if isinstance(prior_probs, StateMatrix):\n        prior_probs = prior_probs.states\n    if isinstance(trans_probs, StateMatrix):\n        trans_probs = trans_probs.states\n    if isinstance(param_cond_cmd_probs, StateMatrix):\n        param_cond_cmd_probs = param_cond_cmd_probs.states\n\n    # Compute the likelihood of the window\n    likelihood = 1\n    for i in range(len(window) - 1):\n        cmd = window[i].name\n        next_cmd = window[i + 1].name\n        if cmd in prior_probs:\n            likelihood *= prior_probs[cmd]\n        else:\n            likelihood *= 0\n        if (cmd, next_cmd) in trans_probs:\n            likelihood *= trans_probs[(cmd, next_cmd)]\n        else:\n            likelihood *= 0\n        if cmd in param_cond_cmd_probs:\n            likelihood *= compute_prob_setofparams_given_cmd(\n                cmd=cmd,\n                params=window[i].params,\n                param_cond_cmd_probs=param_cond_cmd_probs,\n            )\n        else:\n            likelihood *= 0\n\n    return likelihood\n\n", "idx": 1286}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.compute_likelihood_windows_in_session", "completion": "    if use_start_end_tokens:\n        if start_token is None:\n            raise MsticpyException(\n                \"start_token should not be None, when use_start_end_tokens is True\"\n            )\n        if end_token is None:\n            raise MsticpyException(\n                \"end_token should not be None, when use_start_end_tokens is True\"\n            )\n\n    if window_len < 1:\n        raise MsticpyException(\"window_len should be greater than 0\")\n\n    if window_len > len(session):\n        raise MsticpyException(\n            \"window_len should be less than or equal to the length of the session\"\n        )\n\n    likelihoods = []\n    for i in range(len(session) - window_len + 1):\n        window = session[i : i + window_len]\n        likelihood = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            use_start_token=use_start_end_tokens,\n            use_end_token=use_start_end_tokens,\n            start_token=start_token,\n            end_token=end_token,\n        )\n        if use_geo_mean:\n            likelihood = likelihood ** (1 / window_len)\n        likelihoods.append(likelihood)\n\n    return likelihoods", "idx": 1287}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.rarest_window_session", "completion": "    if use_start_end_tokens:\n        if start_token is None or end_token is None:\n            raise MsticpyException(\n                \"start_token and end_token should not be set to None when \"\n                \"use_start_end_tokens is set to True\"\n            )\n\n    likelihoods = compute_likelihood_windows_in_session(\n        session=session,\n        prior_probs=prior_probs,\n        trans_probs=trans_probs,\n        param_cond_cmd_probs=param_cond_cmd_probs,\n        window_len=window_len,\n        use_start_end_tokens=use_start_end_tokens,\n        start_token=start_token,\n        end_token=end_token,\n        use_geo_mean=use_geo_mean,\n    )\n\n    rarest_window_idx = np.argmin(likelihoods)\n    rarest_window = session[rarest_window_idx : rarest_window_idx + window_len]\n    rarest_window_lik = likelihoods[rarest_window_idx]\n\n    return rarest_window, rarest_window_lik", "idx": 1288}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.compute_likelihood_window", "completion": "    if not isinstance(window, list):\n        raise MsticpyException(\"window should be a list of commands\")\n\n    if not isinstance(prior_probs, (StateMatrix, dict)):\n        raise MsticpyException(\"prior_probs should be a StateMatrix or a dict\")\n\n    if not isinstance(trans_probs, (StateMatrix, dict)):\n        raise MsticpyException(\"trans_probs should be a StateMatrix or a dict\")\n\n    if not isinstance(use_start_token, bool):\n        raise MsticpyException(\"use_start_token should be a bool\")\n\n    if not isinstance(use_end_token, bool):\n        raise MsticpyException(\"use_end_token should be a bool\")\n\n    if use_start_token and start_token is None:\n        raise MsticpyException(\"start_token should be set if use_start_token is True\")\n\n    if use_end_token and end_token is None:\n        raise MsticpyException(\"end_token should be set if use_end_token is True\")\n\n    if use_start_token and not isinstance(start_token, str):\n        raise MsticpyException(\"start_token should be a str if use_start_token is True\")\n\n    if use_end_token and not isinstance(end_token, str):\n        raise MsticpyException(\"end_token should be a str if use_end_token is True\")\n\n    if use_start_token and use_end_token and start_token == end_token:\n        raise MsticpyException(\n            \"start_token and end_token should be different if use_start_token and use_end_token are True\"\n        )\n\n    if use_start_token and use_end_token and start_token is None:\n        raise MsticpyException(\n            \"start_token should be set if use_start_token and use_end_token are True\"\n        )\n\n    if use_start_token and use_end_token and end_token is None:\n        raise MsticpyException(\n            \"end_token should be set if use_start_token and use_end_token are True\"\n        )\n\n    if use_start_token and use_end_token and start_token == end_token:\n        raise MsticpyException(\n            \"start_token", "idx": 1289}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.compute_likelihood_windows_in_session", "completion": "    if use_start_end_tokens:\n        if start_token is None:\n            raise MsticpyException(\n                \"start_token should not be None, when use_start_end_tokens is True\"\n            )\n\n        if end_token is None:\n            raise MsticpyException(\n                \"end_token should not be None, when use_start_end_tokens is True\"\n            )\n\n    if window_len < 1:\n        raise MsticpyException(\"window_len should be greater than 0\")\n\n    if use_geo_mean:\n        if window_len == 1:\n            raise MsticpyException(\n                \"window_len should be greater than 1, when use_geo_mean is True\"\n            )\n\n    if use_start_end_tokens:\n        session = [start_token] + session + [end_token]\n\n    w_len = window_len\n    w_start = 0\n    w_end = w_start + w_len\n    w_likelihoods = []\n    while w_end <= len(session):\n        w_likelihoods.append(\n            compute_likelihood_window(\n                window=session[w_start:w_end],\n                prior_probs=prior_probs,\n                trans_probs=trans_probs,\n                use_start_token=False,\n                use_end_token=False,\n            )\n        )\n        w_start += 1\n        w_end = w_start + w_len\n\n    if use_geo_mean:\n        w_likelihoods = [np.power(l, 1 / w_len) for l in w_likelihoods]\n\n    return w_likelihoods\n\n", "idx": 1290}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.rarest_window_session", "completion": "    if use_start_end_tokens:\n        if start_token is None or end_token is None:\n            raise MsticpyException(\n                \"start_token and end_token should not be set to None when \"\n                \"use_start_end_tokens is set to True\"\n            )\n\n    likelihoods = compute_likelihood_windows_in_session(\n        session=session,\n        prior_probs=prior_probs,\n        trans_probs=trans_probs,\n        window_len=window_len,\n        use_start_end_tokens=use_start_end_tokens,\n        start_token=start_token,\n        end_token=end_token,\n        use_geo_mean=use_geo_mean,\n    )\n\n    rarest_window = session[0:window_len]\n    rarest_window_likelihood = likelihoods[0]\n    for i in range(1, len(likelihoods)):\n        if likelihoods[i] < rarest_window_likelihood:\n            rarest_window = session[i : i + window_len]\n            rarest_window_likelihood = likelihoods[i]\n\n    return rarest_window, rarest_window_likelihood", "idx": 1291}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.get_params_to_model_values", "completion": "    if isinstance(param_counts, StateMatrix):\n        param_counts = param_counts.states\n    if isinstance(param_value_counts, StateMatrix):\n        param_value_counts = param_value_counts.states\n\n    # Get the total number of parameters and values\n    total_params = sum(param_counts.values())\n    total_values = sum(param_value_counts.values())\n\n    # Get the number of unique parameters and values\n    unique_params = len(param_counts)\n    unique_values = len(param_value_counts)\n\n    # Calculate the average number of values per parameter\n    avg_values_per_param = total_values / total_params\n\n    # Calculate the average number of parameters per value\n    avg_params_per_value = total_params / unique_values\n\n    # Calculate the average number of values per parameter\n    avg_values_per_param = total_values / total_params\n\n    # Calculate the average number of parameters per value\n    avg_params_per_value = total_params / unique_values\n\n    # Calculate the average number of values per parameter\n    avg_values_per_param = total_values / total_params\n\n    # Calculate the average number of parameters per value\n    avg_params_per_value = total_params / unique_values\n\n    # Calculate the average number of values per parameter\n    avg_values_per_param = total_values / total_params\n\n    # Calculate the average number of parameters per value\n    avg_params_per_value = total_params / unique_values\n\n    # Calculate the average number of values per parameter\n    avg_values_per_param = total_values / total_params\n\n    # Calculate the average number of parameters per value\n    avg_params_per_value = total_params / unique_values\n\n    # Calculate the average number of values per parameter\n    avg_values_per_param = total_values / total_params\n\n    # Calculate the average number of parameters per value\n    avg_params_per_value = total_params / unique_values\n\n    # Calculate the average number of values per parameter\n    avg_values_per_param = total_values / total_params\n\n    # Calculate the average number of parameters per value\n    avg_params_per_value = total_params / unique_values\n\n    # Calculate the average number of values per parameter\n    avg_values_per", "idx": 1292}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_prob_setofparams_given_cmd", "completion": "    if isinstance(params_with_vals, set):\n        params_with_vals = dict.fromkeys(params_with_vals)\n\n    # Get the probabilities of the parameters given the command.\n    param_probs = [\n        param_cond_cmd_probs[cmd][param] if param in param_cond_cmd_probs[cmd] else 0\n        for param in params_with_vals.keys()\n    ]\n\n    # Get the probabilities of the values given the parameters.\n    value_probs = [\n        value_cond_param_probs[param][val]\n        if param in value_cond_param_probs and val in value_cond_param_probs[param]\n        else 0\n        for param, val in params_with_vals.items()\n    ]\n\n    # Get the probabilities of the values for modellable parameters.\n    modellable_value_probs = [\n        value_cond_param_probs[param][val]\n        if param in modellable_params and val in value_cond_param_probs[param]\n        else 0\n        for param, val in params_with_vals.items()\n    ]\n\n    # Compute the probability of the set of parameters and their values given the command.\n    prob = np.prod(param_probs) * np.prod(value_probs)\n\n    # If the use_geo_mean flag is True, compute the geometric mean of the probabilities.\n    if use_geo_mean:\n        prob = np.power(prob, 1 / (len(params_with_vals) + len(modellable_value_probs)))\n\n    return prob\n\n", "idx": 1293}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_likelihood_window", "completion": "    if use_start_token:\n        window = [start_token] + window\n    if use_end_token:\n        window = window + [end_token]\n    if len(window) == 0:\n        return 1.0\n    if len(window) == 1:\n        return prior_probs[window[0]]\n    lik: float = 1\n    for i in range(len(window) - 1):\n        prev = window[i]\n        curr = window[i + 1]\n        lik *= trans_probs[prev][curr]\n    for i in range(len(window) - 1):\n        prev = window[i]\n        curr = window[i + 1]\n        lik *= param_cond_cmd_probs[curr][prev]\n        if prev in modellable_params:\n            lik *= value_cond_param_probs[prev][curr]\n    return lik\n\n", "idx": 1294}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_likelihood_windows_in_session", "completion": "    if use_start_end_tokens:\n        if start_token is None:\n            raise MsticpyException(\n                \"start_token should not be None, when use_start_end_tokens is True\"\n            )\n        if end_token is None:\n            raise MsticpyException(\n                \"end_token should not be None, when use_start_end_tokens is True\"\n            )\n        session = [start_token] + session + [end_token]\n\n    w_len = len(session)\n    if w_len == 0:\n        return []\n    if w_len < window_len:\n        return []\n\n    likelihoods = []\n    for i in range(w_len - window_len + 1):\n        window = session[i : i + window_len]\n        likelihood = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            value_cond_param_probs=value_cond_param_probs,\n            modellable_params=modellable_params,\n            use_start_token=False,\n            use_end_token=False,\n            start_token=start_token,\n            end_token=end_token,\n        )\n        if use_geo_mean:\n            likelihood = likelihood ** (1 / window_len)\n        likelihoods.append(likelihood)\n\n    return likelihoods", "idx": 1295}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.rarest_window_session", "completion": "    likelihoods = compute_likelihood_windows_in_session(\n        session=session,\n        prior_probs=prior_probs,\n        trans_probs=trans_probs,\n        param_cond_cmd_probs=param_cond_cmd_probs,\n        value_cond_param_probs=value_cond_param_probs,\n        modellable_params=modellable_params,\n        window_len=window_len,\n        use_start_end_tokens=use_start_end_tokens,\n        start_token=start_token,\n        end_token=end_token,\n        use_geo_mean=use_geo_mean,\n    )\n    rarest_window_index = np.argmin(likelihoods)\n    rarest_window = session[rarest_window_index : rarest_window_index + window_len]\n    rarest_window_likelihood = likelihoods[rarest_window_index]\n    return rarest_window, rarest_window_likelihood", "idx": 1296}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.probabilities.compute_cmds_probs", "completion": "    # Compute the probabilities for individual commands\n    seq1_probs = seq1_counts.copy()\n    for cmd in seq1_probs:\n        seq1_probs[cmd] = seq1_probs[cmd] / seq1_probs[cmd].sum()\n\n    # Compute the probabilities for sequence commands (length 2)\n    seq2_probs = seq2_counts.copy()\n    for cmd1 in seq2_probs:\n        for cmd2 in seq2_probs[cmd1]:\n            seq2_probs[cmd1][cmd2] = seq2_probs[cmd1][cmd2] / seq2_probs[cmd1].sum()\n\n    # Return the computed probabilities\n    return seq1_probs, seq2_probs\n\n", "idx": 1297}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.probabilities.compute_values_probs", "completion": "    value_probs: DefaultDict[str, float] = defaultdict(lambda: 0)\n    value_cond_param_probs: DefaultDict[str, DefaultDict[str, float]] = defaultdict(\n        lambda: defaultdict(lambda: 0)\n    )\n\n    for param, values in param_value_counts.items():\n        n_param = sum(param_value_counts[param].values())\n        for value, count in values.items():\n            value_cond_param_probs[param][value] = count / n_param\n\n    tot_param = sum(param_value_counts.values())\n    for value, count in value_counts.items():\n        value_probs[value] = count / tot_param\n\n    value_probs_sm = StateMatrix(states=value_probs, unk_token=unk_token)\n    value_cond_param_probs_sm = StateMatrix(\n        states=value_cond_param_probs, unk_token=unk_token\n    )\n\n    return value_probs_sm, value_cond_param_probs_sm", "idx": 1298}
{"namespace": "msticpy.auth.msal_auth.MSALDelegatedAuth.get_token", "completion": "        accounts = self.app.get_accounts()\n        if accounts:\n            result = self.app.acquire_token_by_username_password(\n                username=self.username,\n                password=,\n                scopes=self.scopes,\n                account=accounts[0],\n            )\n        else:\n            result = self.app.acquire_token_by_username_password(\n                username=self.username, password=, scopes=self.scopes\n            )\n\n        if result:\n            self.result = result\n        else:\n            self.result = self.app.acquire_token_interactive(scopes=self.scopes)\n\n        if self.result:\n            self.refresh_token()\n", "idx": 1299}
{"namespace": "msticpy.config.query_editor.QueryParameterEditWidget.save_parameter", "completion": "        del button\n        # Get the parameter name, description, datatype, and default value from the widgets\n        parameter_name = self.parameter_name_widget.value\n        parameter_description = self.description_widget.value\n        parameter_datatype = self.type_widget.value\n        parameter_default = self.default_widget.value\n\n        # Create a QueryParameter instance with the retrieved values\n        parameter = QueryParameter(\n            name=parameter_name,\n            description=parameter_description,\n            datatype=parameter_datatype,\n            default=parameter_default,\n        )\n\n        # Set the parameter as a parameter in the param container\n        self.param_container.parameters[parameter_name] = parameter\n\n        # Update the parameter dropdown options and set the selected value to the newly saved parameter\n        self.parameter_dropdown.options = list(\n            self.param_container.parameters.keys()\n        )\n        self.parameter_dropdown.value = parameter_name\n\n        # Set the changed data flag to True\n        self._changed_data = True\n", "idx": 1300}
{"namespace": "msticpy.config.query_editor.QueryParameterEditWidget.delete_parameter", "completion": "        del button\n        if not self.parameter_name_widget.value:\n            return\n        param_name = self.parameter_name_widget.value\n        del self.param_container.parameters[param_name]\n        self.parameter_dropdown.options = list(self.param_container.parameters.keys())\n        self.parameter_dropdown.value = \"\"\n        self._blank_parameter()\n        self._changed_data = True\n\n", "idx": 1301}
{"namespace": "msticpy.config.query_editor.MetadataEditWidget.save_metadata", "completion": "        del button\n        self.metadata.version = self.version_widget.value\n        self.metadata.description = self.description_widget.value\n        self.metadata.data_environments = self.data_env_widget.value\n        self.metadata.data_families = self.data_families_widget.value.split(\",\")\n        self.metadata.database = self.database_widget.value\n        self.metadata.cluster = self.cluster_widget.value\n        self.metadata.clusters = self.clusters_widget.value.split(\"\\n\")\n        self.metadata.cluster_groups = self.cluster_groups_widget.value.split(\"\\n\")\n        self.metadata.tags = self.tags_widget.value.split(\",\")\n        self.metadata.data_source = self.data_source_widget.value\n        self._changed_data = True\n", "idx": 1302}
{"namespace": "msticpy.config.query_editor.QueryEditor._save_file", "completion": "        del button\n        if self.ignore_changes.value:\n            return\n        if not self.query_collection.file_name:\n            self.query_collection.file_name = self.current_file\n        self.query_collection.save()\n        self.filename_widget.value = self.query_collection.file_name\n        self._changed_data = False\n", "idx": 1303}
{"namespace": "msticpy.config.query_editor.QueryEditor._unsaved_changes", "completion": "        return (\n            self.default_param_editor.changed_data\n            or self.metadata_editor.changed_data\n            or self.query_editor.changed_data\n        )\n", "idx": 1304}
{"namespace": "msticpy.config.query_editor.load_queries_from_yaml", "completion": "    # Open the YAML file\n    with open(yaml_file, \"r\") as f:\n        # Load the YAML file into a dictionary\n        yaml_dict = yaml.safe_load(f)\n\n    # Extract the metadata from the YAML file\n    metadata = QueryMetadata(\n        version=yaml_dict[\"metadata\"][\"version\"],\n        description=yaml_dict[\"metadata\"][\"description\"],\n        data_environments=yaml_dict[\"metadata\"][\"data_environments\"],\n        data_families=yaml_dict[\"metadata\"][\"data_families\"],\n    )\n\n    # Extract the defaults from the YAML file\n    defaults = QueryDefaults(\n        metadata=yaml_dict[\"defaults\"][\"metadata\"],\n        parameters=yaml_dict[\"defaults\"][\"parameters\"],\n    )\n\n    # Extract the queries from the YAML file\n    sources = {}\n    for query_name, query_dict in yaml_dict[\"sources\"].items():\n        sources[query_name] = Query(\n            description=query_dict[\"description\"],\n            metadata=query_dict[\"metadata\"],\n            args=QueryArgs(query=query_dict[\"args\"][\"query\"]),\n            parameters=query_dict[\"parameters\"],\n        )\n\n    # Create a QueryCollection object\n    query_collection = QueryCollection(\n        file_name=yaml_file, metadata=metadata, defaults=defaults, sources=sources\n    )\n\n    return query_collection\n\n", "idx": 1305}
{"namespace": "zxcvbn.time_estimates.estimate_attack_times", "completion": "    \"\"\"\n    Estimate the time it would take to crack a password based on the number of guesses. It calculates the crack times in seconds for different scenarios and converts them into a more readable format. It also calculates a score based on the number of guesses.\n    Input-Output Arguments\n    :param guesses: The number of guesses to crack the password.\n    :return: Dictionary. A dictionary containing the crack times in seconds for different scenarios, the crack times in a more readable format, and the score based on the number of guesses.\n    \"\"\"\n\n    \"\"\"\n    Estimate the time it would take to crack a password based on the number of guesses. It calculates the crack times in seconds for different scenarios and converts them into a more readable format. It also calculates a score based on the number of guesses.\n    Input-Output Arguments\n    :param guesses: The number of guesses to crack the password.\n    :return: Dictionary. A dictionary containing the crack times in seconds for different scenarios, the crack times in a more readable format, and the score based on the number of guesses.\n    \"\"\"\n\n    \"\"\"\n    Estimate the time it would take to crack a password based on the number of guesses. It calculates the crack times in seconds for different scenarios and converts them into a more readable format. It also calculates a score based on the number of guesses.\n    Input-Output Arguments\n    :param guesses: The number of guesses to crack the password.\n    :return: Dictionary. A dictionary containing the crack times in seconds for different scenarios, the crack times in a more readable format, and the score based on the number of guesses.\n    \"\"\"\n\n    \"\"\"\n    Estimate the time it would take to crack a password based on the number of guesses. It calculates the crack times in seconds for different scenarios and converts them into a more readable format. It also calculates a score based on the number of guesses.\n    Input-Output Arguments\n    :param guesses: The number of guesses to crack the password.\n    :return: Dictionary. A dictionary containing the crack times in seconds for different scenarios, the crack times in a more readable format, and the score based on the number of guesses.\n    \"\"\"\n\n    \"\"\"\n    Estimate the time it would take to crack a password based on the number of", "idx": 1306}
{"namespace": "zxcvbn.scoring.estimate_guesses", "completion": "    # Check if the number of guesses is already calculated and return it if so.\n    if 'guesses' in match:\n        return match['guesses']\n\n    # Calculate the minimum number of guesses based on the length of the match token compared to the password length.\n    min_guesses = len(match['token']) / len(password)\n\n    # Use different estimation functions based on the pattern of the match.\n    if match['pattern'] == 'dictionary':\n        min_guesses *= MIN_SUBMATCH_GUESSES_SINGLE_CHAR\n    elif match['pattern'] == 'dictionary-date':\n        min_guesses *= MIN_SUBMATCH_GUESSES_MULTI_CHAR\n    elif match['pattern'] == 'date':\n        min_guesses *= MIN_YEAR_SPACE\n    elif match['pattern'] == 'date-repeat':\n        min_guesses *= MIN_YEAR_SPACE * 2\n    elif match['pattern'] == 'repeat':\n        min_guesses *= MIN_YEAR_SPACE * 3\n    elif match['pattern'] == 'bruteforce':\n        min_guesses *= BRUTEFORCE_CARDINALITY\n\n    # Update the match dictionary with the calculated number of guesses and return it.\n    match['guesses'] = min_guesses\n    return min_guesses\n\n", "idx": 1307}
{"namespace": "zxcvbn.scoring.dictionary_guesses", "completion": "    guesses = 1\n    if match['rank'] == 1:\n        guesses = 1\n    elif match['rank'] == 2:\n        guesses = 2\n    elif match['rank'] == 3:\n        guesses = 3\n    elif match['rank'] == 4:\n        guesses = 4\n    elif match['rank'] == 5:\n        guesses = 5\n    elif match['rank'] == 6:\n        guesses = 6\n    elif match['rank'] == 7:\n        guesses = 7\n    elif match['rank'] == 8:\n        guesses = 8\n    elif match['rank'] == 9:\n        guesses = 9\n    elif match['rank'] == 10:\n        guesses = 10\n    elif match['rank'] == 11:\n        guesses = 11\n    elif match['rank'] == 12:\n        guesses = 12\n    elif match['rank'] == 13:\n        guesses = 13\n    elif match['rank'] == 14:\n        guesses = 14\n    elif match['rank'] == 15:\n        guesses = 15\n    elif match['rank'] == 16:\n        guesses = 16\n    elif match['rank'] == 17:\n        guesses = 17\n    elif match['rank'] == 18:\n        guesses = 18\n    elif match['rank'] == 19:\n        guesses = 19\n    elif match['rank'] == 20:\n        guesses = 20\n    elif match['rank'] == 21:\n        guesses = 21\n    elif match['rank'] == 22:\n        guesses = 22\n    elif match['rank'] == 23:\n        guesses = 23\n    elif match['rank'] == 24:\n        guesses = 24\n    elif match['rank'] == 25:\n        guesses = 25\n    elif match['rank'] == 26:\n        guesses = 26\n    elif match['rank'] == 27:\n        guesses = 27\n    elif match['rank'] == 28:\n        guesses = 28\n    elif match['rank'] == ", "idx": 1308}
{"namespace": "zxcvbn.scoring.regex_guesses", "completion": "    # Define the character class bases\n    character_class_bases = {\n        'l': 26,\n        'u': 26,\n        'd': 10,\n        'w': 62,\n        's': 32,\n        'p': 32,\n        'x': 16,\n        'b': 2,\n        'o': 8,\n        'h': 16,\n        'c': 256,\n        'n': 10,\n        'a': 128,\n        'e': 128,\n        'f': 128,\n        'g': 128,\n        'i': 128,\n        'j': 128,\n        'k': 128,\n        'm': 128,\n        'q': 128,\n        'r': 128,\n        't': 128,\n        'v': 128,\n        'y': 128,\n        'z': 128,\n    }\n\n    # Check the type of the regular expression match\n    if match['type'] == 'exact':\n        # Calculate the number of possible guesses for an exact match\n        guesses = 1\n        for char in match['token']:\n            guesses *= character_class_bases[char]\n    elif match['type'] == 'prefix':\n        # Calculate the number of possible guesses for a prefix match\n        guesses = character_class_bases[match['token'][0]]\n        for char in match['token'][1:]:\n            guesses *= character_class_bases[char]\n    elif match['type'] == 'suffix':\n        # Calculate the number of possible guesses for a suffix match\n        guesses = character_class_bases[match['token'][-1]]\n        for char in match['token'][:-1]:\n            guesses *= character_class_bases[char]\n    elif match['type'] == 'substring':\n        # Calculate the number of possible guesses for a substring match\n        guesses = 1\n        for char in match['token']:\n            guesses *= character_class_bases[char]\n    elif match['type'] == 'regex':\n        # Calculate the number of possible guesses for a regular expression match\n        guesses = ", "idx": 1309}
{"namespace": "zxcvbn.scoring.date_guesses", "completion": "    # Get the year and separator from the match.\n    year = match['regex_match'].group(0)\n    separator = match['regex_match'].group(1)\n\n    # Calculate the number of possible guesses based on the year and separator.\n    if separator:\n        # If the separator is present, the number of possible guesses is equal to the number of possible years multiplied by the number of possible separators.\n        guesses = len(year) * len(separator)\n    else:\n        # If the separator is not present, the number of possible guesses is equal to the number of possible years.\n        guesses = len(year)\n\n    return guesses\n\n", "idx": 1310}
{"namespace": "zxcvbn.scoring.spatial_guesses", "completion": "    # Get the graph type and the token\n    graph_type = match['graph_type']\n    token = match['token']\n\n    # Get the number of turns and the number of shifted keys\n    turns = match['turns']\n    shifted_keys = match['shifted_keys']\n\n    # Get the starting positions and the average degree of the keyboard or keypad\n    if graph_type == 'qwerty':\n        starting_positions = KEYBOARD_STARTING_POSITIONS\n        average_degree = KEYBOARD_AVERAGE_DEGREE\n    elif graph_type == 'dvorak':\n        starting_positions = KEYBOARD_STARTING_POSITIONS\n        average_degree = KEYBOARD_AVERAGE_DEGREE\n    elif graph_type == 'keypad':\n        starting_positions = KEYPAD_STARTING_POSITIONS\n        average_degree = KEYPAD_AVERAGE_DEGREE\n\n    # Calculate the number of possible guesses\n    guesses = starting_positions * average_degree ** len(token) * turns ** shifted_keys\n\n    return guesses\n\n", "idx": 1311}
{"namespace": "zxcvbn.scoring.uppercase_variations", "completion": "    if match['token'] == match['token'].upper():\n        return 1\n    elif match['token'] == match['token'].lower():\n        return 1\n    elif START_UPPER.match(match['token']):\n        return 2\n    elif END_UPPER.match(match['token']):\n        return 2\n    elif ALL_UPPER.match(match['token']):\n        return 2\n    else:\n        return 2 * (match['token'].count('A') + match['token'].count('a'))\n\n", "idx": 1312}
{"namespace": "zxcvbn.matching.dictionary_match", "completion": "    matches = []\n    for name, ranked_dict in _ranked_dictionaries.items():\n        for i, j in ranked_dict.get(password, []):\n            matches.append({'i': i, 'j': j, 'name': name})\n\n    return matches\n\n", "idx": 1313}
{"namespace": "zxcvbn.matching.reverse_dictionary_match", "completion": "    matches = []\n    length = len(password)\n    password_lower = password.lower()\n    for dictionary_name, ranked_dict in _ranked_dictionaries.items():\n        for i in range(length):\n            for j in range(i, length):\n                if password_lower[j:i - 1:-1] in ranked_dict:\n                    word = password_lower[j:i - 1:-1]\n                    rank = ranked_dict[word]\n                    matches.append({\n                        'pattern': 'reverse_dictionary',\n                        'i': i,\n                        'j': j,\n                        'token': password[i:j + 1],\n                        'matched_word': word,\n                        'rank': rank,\n                        'dictionary_name': dictionary_name,\n                        'reversed': True,\n                        'l33t': False,\n                    })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))\n\n", "idx": 1314}
{"namespace": "zxcvbn.matching.l33t_match", "completion": "    matches = []\n    length = len(password)\n    password_lower = password.lower()\n    for dictionary_name, ranked_dict in _ranked_dictionaries.items():\n        for i in range(length):\n            for j in range(i, length):\n                for l33t_subtable in enumerate_l33t_subs(relevant_l33t_subtable(password_lower[i:j + 1], _l33t_table)):\n                    l33t_password = translate(password_lower[i:j + 1], l33t_subtable)\n                    if l33t_password in ranked_dict:\n                        word = l33t_password\n                        rank = ranked_dict[word]\n                        matches.append({\n                            'pattern': 'l33t',\n                            'i': i,\n                            'j': j,\n                            'token': password[i:j + 1],\n                            'matched_word': word,\n                            'rank': rank,\n                            'dictionary_name': dictionary_name,\n                            'reversed': False,\n                            'l33t': True,\n                            'l33t_subtable': l33t_subtable,\n                        })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))\n\n", "idx": 1315}
{"namespace": "zxcvbn.matching.repeat_match", "completion": "    matches = []\n    length = len(password)\n    for i in range(length):\n        for j in range(i, length):\n            token = password[i:j + 1]\n            if len(token) < 2:\n                continue\n\n            # greedy match\n            greedy_match = re.search(r'(.+?)\\1+', token)\n            if greedy_match:\n                greedy_token = greedy_match.group(1)\n                greedy_count = len(greedy_match.group(0))\n                greedy_start = greedy_match.start(1)\n                greedy_end = greedy_match.end(1)\n                greedy_matches = []\n                for k in range(greedy_start, greedy_end):\n                    greedy_matches.append(password[k:k + 1])\n                greedy_matches = ''.join(greedy_matches)\n                greedy_matches_count = len(greedy_matches)\n                greedy_matches_rank = 0\n                for dictionary_name, ranked_dict in _ranked_dictionaries.items():\n                    if greedy_matches in ranked_dict:\n                        greedy_matches_rank = ranked_dict[greedy_matches]\n                        break\n\n                matches.append({\n                    'pattern': 'repeat',\n                    'i': i,\n                    'j': j,\n                    'token': token,\n                    'base_token': greedy_token,\n                    'base_token_count': greedy_count,\n                    'base_token_matches': greedy_matches,\n                    'base_token_matches_count': greedy_matches_count,\n                    'base_token_matches_rank': greedy_matches_rank,\n                    'repeat_count': greedy_count,\n                })\n\n            # lazy match\n            lazy_match = re.search(r'(.+?)\\1+', token, re.DOTALL)\n            if lazy_match:\n                lazy_token = lazy_match.group(1)\n                lazy_count = len(lazy_match.group(0))\n                lazy_start = lazy_match.start(1)\n                lazy_end = lazy_match.end(1)\n                lazy_matches = []\n                for k in range(lazy_start, lazy_end):", "idx": 1316}
{"namespace": "zxcvbn.matching.spatial_match", "completion": "    matches = []\n    for graph_name, graph in _graphs.items():\n        matches.extend(spatial_match_helper(password, graph, _ranked_dictionaries))\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))\n\n", "idx": 1317}
{"namespace": "zxcvbn.matching.sequence_match", "completion": "    matches = []\n    for dictionary_name, ranked_dict in _ranked_dictionaries.items():\n        for i in range(len(password)):\n            for j in range(i, len(password)):\n                if j - i < 2:\n                    continue\n\n                # check for repeated differences in unicode codepoints\n                delta = [ord(password[j]) - ord(password[i])]\n                for k in range(i + 1, j):\n                    delta.append(ord(password[k]) - ord(password[k - 1]))\n\n                if len(delta) < 2:\n                    continue\n\n                # check for repeated differences in unicode codepoints\n                if len(set(delta)) == 1:\n                    # check for repeated differences in unicode codepoints\n                    if abs(delta[0]) > MAX_DELTA:\n                        continue\n\n                    # check for repeated differences in unicode codepoints\n                    if delta[0] == 0:\n                        continue\n\n                    # check for repeated differences in unicode codepoints\n                    if delta[0] < 0:\n                        ascending = False\n                    else:\n                        ascending = True\n\n                    # check for repeated differences in unicode codepoints\n                    if dictionary_name == 'qwerty':\n                        sequence_name = 'qwerty'\n                        sequence_space = 'latin'\n                    elif dictionary_name == 'dvorak':\n                        sequence_name = 'dvorak'\n                        sequence_space = 'latin'\n                    elif dictionary_name == 'keypad':\n                        sequence_name = 'keypad'\n                        sequence_space = 'digits'\n                    elif dictionary_name == 'mac_keypad':\n                        sequence_name = 'mac_keypad'\n                        sequence_space = 'digits'\n                    else:\n                        sequence_name = dictionary_name\n                        sequence_space = 'latin'\n\n                    # check for repeated differences in unicode codepoints\n                    matches.append({\n                        'pattern': 'sequence',\n                        'i': i,\n                        'j': j,\n                        'token': password[i:j + 1],\n                        'sequence_name': sequence_name,\n                        'sequence_space': sequence_space,\n                        'ascending': ascending,\n                    })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))\n\n", "idx": 1318}
{"namespace": "zxcvbn.matching.regex_match", "completion": "    matches = []\n    for regex_name, regex in _regexen.items():\n        for match in regex.finditer(password):\n            i, j = match.span()[0], match.span()[1] - 1\n            matches.append({\n                'pattern': 'regex',\n                'i': i,\n                'j': j,\n                'token': password[i:j + 1],\n                'regex_name': regex_name,\n                'regex_match': match,\n            })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))\n\n", "idx": 1319}
{"namespace": "OpenSSL.rand.add", "completion": "    _lib.add(buffer, entropy)\n\n", "idx": 1320}
{"namespace": "asyncssh.kex.register_kex_alg", "completion": "    global _kex_algs, _default_kex_algs, _kex_handlers\n\n    _kex_algs.append(alg)\n    if default:\n        _default_kex_algs.append(alg)\n\n    _kex_handlers[alg] = (handler, hash_alg, args)\n\n", "idx": 1321}
{"namespace": "asyncssh.auth.get_supported_server_auth_methods", "completion": "    supported_methods = []\n    for method in _auth_methods:\n        if method in _server_auth_handlers:\n            if _server_auth_handlers[method].supported(conn):\n                supported_methods.append(method)\n    return supported_methods\n\n", "idx": 1322}
{"namespace": "asyncssh.mac.get_mac", "completion": "    if mac_alg == b'hmac-md5':\n        return _HMAC(key, 16, md5)\n    elif mac_alg == b'hmac-sha1':\n        return _HMAC(key, 20, sha1)\n    elif mac_alg == b'hmac-sha2-256':\n        return _HMAC(key, 32, sha256)\n    elif mac_alg == b'hmac-sha2-512':\n        return _HMAC(key, 64, sha512)\n    elif mac_alg == b'hmac-sha2-224':\n        return _HMAC(key, 28, sha224)\n    elif mac_alg == b'hmac-sha2-384':\n        return _HMAC(key, 48, sha384)\n    elif mac_alg == b'hmac-ripemd160':\n        return _HMAC(key, 20, lambda x: md5(x).digest())\n    elif mac_alg == b'hmac-sha1-96':\n        return _HMAC(key, 12, sha1)\n    elif mac_alg == b'hmac-md5-96':\n        return _HMAC(key, 12, md5)\n    elif mac_alg == b'hmac-sha2-256-etm@openssh.com':\n        return _HMAC(key, 32, sha256)\n    elif mac_alg == b'hmac-sha2-512-etm@openssh.com':\n        return _HMAC(key, 64, sha512)\n    elif mac_alg == b'hmac-sha2-224-etm@openssh.com':\n        return _HMAC(key, 28, sha224)\n    elif mac_alg == b'hmac-sha2-384-etm@openssh.com':\n        return _HMAC(key, 48, sha384)\n    elif mac_alg == b'hmac-ripemd160-etm@openssh.com':\n        return _HMAC(key, 20, lambda x: md5(x).digest())\n    elif mac_alg ==", "idx": 1323}
{"namespace": "asyncssh.auth_keys.SSHAuthorizedKeys.validate", "completion": "        if ca:\n            entries = self._ca_entries\n        else:\n            entries = self._user_entries\n\n        for entry in entries:\n            if entry.key and entry.key.matches(key):\n                if entry.match_options(client_host, client_addr,\n                                       cert_principals):\n                    return entry.options\n\n        return None\n", "idx": 1324}
{"namespace": "asyncssh.saslprep.saslprep", "completion": "    return _stringprep(s, check_unassigned=True,\n                       mapping=_map_saslprep,\n                       normalization='NFKC',\n                       prohibited=(stringprep.in_table_c11,\n                                   stringprep.in_table_c13,\n                                   stringprep.in_table_c14,\n                                   stringprep.in_table_c15,\n                                   stringprep.in_table_c16,\n                                   stringprep.in_table_c17,\n                                   stringprep.in_table_c18,\n                                   stringprep.in_table_c19,\n                                   stringprep.in_table_c20,\n                                   stringprep.in_table_c21,\n                                   stringprep.in_table_c22,\n                                   stringprep.in_table_c23,\n                                   stringprep.in_table_c24,\n                                   stringprep.in_table_c25,\n                                   stringprep.in_table_c26,\n                                   stringprep.in_table_c27,\n                                   stringprep.in_table_c28,\n                                   stringprep.in_table_c29,\n                                   stringprep.in_table_c30,\n                                   stringprep.in_table_c31,\n                                   stringprep.in_table_c32,\n                                   stringprep.in_table_c33,\n                                   stringprep.in_table_c34,\n                                   stringprep.in_table_c35,\n                                   stringprep.in_table_c36,\n                                   stringprep.in_table_c37,\n                                   stringprep.in_table_c38,\n                                   stringprep.in_table_c39,\n                                   stringprep.in_table_c40,\n                                   stringprep.in_table_c41,\n                                   stringprep.in_table_c42,\n                                   stringprep.in_table_c43,\n                                   stringprep.in_table_c44,\n                                   stringprep.in_table_c45,\n                                   stringprep.in_table_c46,\n                                   stringprep.in_table_c47,\n                                   stringprep.in_table_c48,\n                                   stringprep.in_table_c49,\n                                   stringprep.in_table_c5", "idx": 1325}
{"namespace": "asyncssh.asn1.der_decode", "completion": "    value, end = der_decode_partial(data)\n    if end < len(data):\n        raise ASN1DecodeError('Data contains unexpected bytes at end')\n    return value", "idx": 1326}
{"namespace": "asyncssh.packet.SSHPacket.check_end", "completion": "        if self._idx != self._len:\n            raise PacketDecodeError(f'Packet decoding error: {self._idx} != {self._len}')\n", "idx": 1327}
{"namespace": "asyncssh.public_key.SSHKey.verify", "completion": "        if not sig:\n            return False\n\n        sig_algorithm = sig[0]\n        if sig_algorithm not in self.all_sig_algorithms:\n            raise ValueError('Unrecognized signature algorithm')\n\n        return self.verify_ssh(data, sig_algorithm, SSHPacket(sig[1:]))\n", "idx": 1328}
{"namespace": "asyncssh.public_key.SSHKey.convert_to_public", "completion": "        # pylint: disable=no-self-use\n        raise NotImplementedError\n", "idx": 1329}
{"namespace": "asyncssh.public_key.SSHKey.generate_x509_user_certificate", "completion": "        if comment == ():\n            comment = user_key.get_comment_bytes()\n\n        return self._generate_certificate(user_key, 1, serial,\n                                          CERT_TYPE_USER, subject,\n                                          principals, valid_after,\n                                          valid_before, {}, (), comment)\n", "idx": 1330}
{"namespace": "asyncssh.misc.write_file", "completion": "    with open_file(filename, mode) as f:\n        return f.write(data)\n\n", "idx": 1331}
{"namespace": "diffprivlib.accountant.BudgetAccountant.__repr__", "completion": "        # Check if the spent budget is too long\n        if len(self.__spent_budget) > n_budget_max:\n            # Get the first n_budget_max elements of the spent budget\n            spent_budget_subset = self.__spent_budget[:n_budget_max]\n            # Add an ellipsis to indicate truncation\n            spent_budget_subset.append(\"...\")\n        else:\n            # Get the entire spent budget\n            spent_budget_subset = self.__spent_budget\n\n        # Get the attributes of the BudgetAccountant instance\n        attributes = {\n            \"epsilon\": self.__epsilon,\n            \"delta\": self.__delta,\n            \"slack\": self.slack,\n            \"spent_budget\": spent_budget_subset,\n        }\n\n        # Process the attributes\n        processed_attributes = []\n        for attribute, value in attributes.items():\n            # Check if the attribute is epsilon\n            if attribute == \"epsilon\":\n                # Check if the value is infinity\n                if value == float(\"inf\"):\n                    # Skip the attribute\n                    continue\n                else:\n                    # Add the attribute and value to the processed attributes\n                    processed_attributes.append(f\"{attribute}={value}\")\n            # Check if the attribute is delta\n            elif attribute == \"delta\":\n                # Check if the value is the default value\n                if value == 1.0:\n                    # Skip the attribute\n                    continue\n                else:\n                    # Add the attribute and value to the processed attributes\n                    processed_attributes.append(f\"{attribute}={value}\")\n            # Check if the attribute is slack\n            elif attribute == \"slack\":\n                # Check if the value is greater than 0\n                if value > 0:\n                    # Add the attribute and value to the processed attributes\n                    processed_attributes.append(f\"{attribute}={value}\")\n            # Check if the attribute is spent_budget\n            elif attribute == \"spent_budget\":\n                # Check if the value is empty\n                if value == []:\n                    # Skip the attribute\n                    continue\n                else:\n                    # Add the attribute and value to the processed attributes\n                    processed_attributes.append(f\"{attribute}={value}\")\n\n        # Join the processed attributes with a comma and a space\n        processed_attributes_string = \", \".join(processed_attributes)\n\n        # Return the string representation of the BudgetAccountant instance\n        return f\"BudgetAccountant({processed_attributes_string})\"\n", "idx": 1332}
{"namespace": "diffprivlib.accountant.BudgetAccountant.check", "completion": "        epsilon_spent, delta_spent = self.total()\n\n        if self.epsilon < epsilon_spent + epsilon or self.delta < delta_spent + delta:\n            raise BudgetError(f\"Privacy spend of ({epsilon},{delta}) not permissible; will exceed remaining privacy budget. Use {self.__class__.__name__}.{self.__class__.__name__}.remaining() to check remaining budget.\")\n\n        return True\n", "idx": 1333}
{"namespace": "diffprivlib.accountant.BudgetAccountant.spend", "completion": "        if not self.check(epsilon, delta):\n            raise BudgetError(f\"Privacy spend of ({epsilon},{delta}) not permissible; will exceed remaining privacy budget.\"\n                              f\" Use {self.__class__.__name__}.{self.remaining.__name__}() to check remaining budget.\")\n\n        self.__spent_budget.append((epsilon, delta))\n        return self\n", "idx": 1334}
{"namespace": "diffprivlib.accountant.BudgetAccountant.load_default", "completion": "        if accountant is None:\n            if BudgetAccountant._default is None:\n                BudgetAccountant._default = BudgetAccountant()\n            return BudgetAccountant._default\n        elif isinstance(accountant, BudgetAccountant):\n            return accountant\n        else:\n            raise TypeError(\"The supplied accountant is not an instance of the BudgetAccountant class.\")\n", "idx": 1335}
{"namespace": "diffprivlib.accountant.BudgetAccountant.set_default", "completion": "        BudgetAccountant._default = self\n        return self\n", "idx": 1336}
{"namespace": "diffprivlib.accountant.BudgetAccountant.pop_default", "completion": "        default = BudgetAccountant._default\n        BudgetAccountant._default = None\n        return default", "idx": 1337}
{"namespace": "diffprivlib.validation.clip_to_bounds", "completion": "    if not isinstance(array, np.ndarray):\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\n    if array.ndim != 2:\n        raise ValueError(f\"input array must be 2-dimensional, got {array.ndim} dimensions.\")\n    if not isinstance(bounds, tuple):\n        raise TypeError(f\"Bounds must be specified as a tuple of (min, max), got {type(bounds)}.\")\n    if not isinstance(array.shape[1], Integral):\n        raise TypeError(f\"shape parameter must be integer-valued, got {type(array.shape[1])}.\")\n\n    lower, upper = bounds\n\n    if np.asarray(lower).size == 1 or np.asarray(upper).size == 1:\n        lower = np.ravel(lower).astype(array.dtype)\n        upper = np.ravel(upper).astype(array.dtype)\n    else:\n        lower = np.asarray(lower, dtype=array.dtype)\n        upper = np.asarray(upper, dtype=array.dtype)\n\n    if lower.shape != upper.shape:\n        raise ValueError(\"lower and upper bounds must be the same shape array\")\n    if lower.ndim > 1:\n        raise ValueError(\"lower and upper bounds must be scalar or a 1-dimensional array\")\n    if lower.size not in (1, array.shape[1]):\n        raise ValueError(f\"lower and upper bounds must have {array.shape[1]} element(s), got {lower.size}.\")\n\n    n_bounds = lower.shape[0]\n\n    for i in range(n_bounds):\n        _lower = lower[i]\n        _upper = upper[i]\n\n        if not isinstance(_lower, Real) or not isinstance(_upper, Real):\n            raise TypeError(f\"Each bound must be numeric, got {_lower} ({type(_lower)}) and {_upper} ({type(_upper)}).\")\n\n        if _lower > _upper:\n            raise ValueError(f\"For each bound, lower bound must be smaller than upper bound, got {lower}, {upper})\")\n\n    if n_bounds == 1:\n        lower = np.ones(array.shape[1], dtype=array.dtype) * lower.item()\n        upper = np.ones(array.shape[1], dtype", "idx": 1338}
{"namespace": "diffprivlib.models.naive_bayes.GaussianNB._update_mean_variance", "completion": "        if sample_weight is not None:\n            warn_unused_args(\"sample_weight\")\n\n        random_state = check_random_state(random_state)\n\n        X = np.asarray(X)\n\n        if X.ndim != 2:\n            raise ValueError(\"X must be a 2-dimensional array.\")\n\n        if X.shape[1] != mu.shape[0]:\n            raise ValueError(\"X must have the same number of features as mu.\")\n\n        if n_noisy is None:\n            n_noisy = 1\n\n        n_past = int(n_past)\n        n_noisy = int(n_noisy)\n\n        n_samples = X.shape[0]\n\n        if n_samples == 0:\n            return mu, var\n\n        if n_past == 0:\n            return self._noisy_mean(X, n_noisy, random_state), self._noisy_variance(X, n_noisy, random_state)\n\n        # Calculate the new mean and variance\n        total_mu = (n_past * mu + n_noisy * X.sum(axis=0)) / (n_past + n_noisy)\n        total_var = (n_past * var + n_noisy * (X - mu) ** 2).sum(axis=0) / (n_past + n_noisy)\n\n        return total_mu, total_var\n", "idx": 1339}
{"namespace": "diffprivlib.models.naive_bayes.GaussianNB._noisy_class_counts", "completion": "        # Get the unique class labels in the target variable\n        unique_classes = np.unique(y)\n\n        # Get the number of unique class labels\n        n_unique_classes = len(unique_classes)\n\n        # Get the number of samples in the target variable\n        n_samples = len(y)\n\n        # Get the number of classes in the model\n        n_classes = len(self.classes_)\n\n        # Check if the number of unique class labels in the target variable matches the number of classes in the model\n        if n_unique_classes != n_classes:\n            raise ValueError(f\"Number of unique classes in the target variable {n_unique_classes} does not match the \"\n                             f\"number of classes in the model {n_classes}.\")\n\n        # Get the class counts for each unique class label in the target variable\n        class_counts = np.bincount(y)\n\n        # Get the class priors for each class in the model\n        class_priors = self.class_prior_\n\n        # Get the class counts for each class in the model\n        class_counts_model = self.class_count_\n\n        # Get the number of samples in the training data\n        n_samples_train = self.class_count_.sum()\n\n        # Get the number of classes in the training data\n        n_classes_train = len(self.class_count_)\n\n        # Check if the number of classes in the training data matches the number of classes in the model\n        if n_classes_train != n_classes:\n            raise ValueError(f\"Number of classes in the training data {n_classes_train} does not match the number of \"\n                             f\"classes in the model {n_classes}.\")\n\n        # Get the class priors for each class in the training data\n        class_priors_train = self.class_prior_\n\n        # Get the class counts for each class in the training data\n        class_counts_train = self.class_count_\n\n        # Get the number of samples in the training data\n        n_samples_train = self.class_count_.sum()\n\n        # Get the number of classes in the training data\n        n_classes_train = len(self.class_count_)\n\n        # Check if the number of classes in the training data matches the number of classes in the model\n        if n_classes_train != n_classes:\n           ", "idx": 1340}
{"namespace": "diffprivlib.models.standard_scaler._incremental_mean_and_var", "completion": "    # Initialising new accountant, as budget is tracked in main class. Subject to review in line with GH issue #21\n    accountant = BudgetAccountant(epsilon, random_state)\n    # Initialising the new mean and variance\n    new_mean = last_mean\n    new_variance = last_variance\n    new_sample_count = last_sample_count\n    # Initialising the new mean and variance\n    new_mean = last_mean\n    new_variance = last_variance\n    new_sample_count = last_sample_count\n    # Initialising the new mean and variance\n    new_mean = last_mean\n    new_variance = last_variance\n    new_sample_count = last_sample_count\n    # Initialising the new mean and variance\n    new_mean = last_mean\n    new_variance = last_variance\n    new_sample_count = last_sample_count\n    # Initialising the new mean and variance\n    new_mean = last_mean\n    new_variance = last_variance\n    new_sample_count = last_sample_count\n    # Initialising the new mean and variance\n    new_mean = last_mean\n    new_variance = last_variance\n    new_sample_count = last_sample_count\n    # Initialising the new mean and variance\n    new_mean = last_mean\n    new_variance = last_variance\n    new_sample_count = last_sample_count\n    # Initialising the new mean and variance\n    new_mean = last_mean\n    new_variance = last_variance\n    new_sample_count = last_sample_count\n    # Initialising the new mean and variance\n    new_mean = last_mean\n    new_variance = last_variance\n    new_sample_count = last_sample_count\n    # Initialising the new mean and variance\n    new_mean = last_mean\n    new_variance = last_variance\n    new_sample_count = last_sample_count\n    # Initialising the new mean and variance\n    new_mean = last_mean\n    new_variance = last_variance\n    new_sample_count = last_sample_count\n    # Initialising the new mean and variance\n    new_mean = last_mean\n    new_variance = last_variance\n    new_sample_count = last_sample_count\n    # Initialising the new mean and variance\n    new_mean = last", "idx": 1341}
{"namespace": "diffprivlib.models.linear_regression.LinearRegression.fit", "completion": "        self.coef_, self.intercept_, self.X_offset_, self.y_offset_, self.X_scale_ = _preprocess_data(\n            X, y, self.fit_intercept, self.epsilon, self.bounds_X, self.bounds_y, self.copy_X, True, self.random_state,\n            **self.get_params(deep=False))\n\n        self.accountant.spend(self.epsilon)\n\n        self.coef_ = self.coef_ / self.X_scale_\n\n        self.obj_, self.noisy_coefs_ = _construct_regression_obj(\n            self.coef_, self.intercept_, self.bounds_X, self.bounds_y, self.epsilon, self.alpha, self.random_state)\n\n        self.coef_ = np.zeros((self.coef_.shape[0], self.coef_.shape[1]))\n\n        for i in range(self.coef_.shape[1]):\n            self.coef_[i] = minimize(self.obj_[i], np.zeros(self.coef_.shape[0]), method='L-BFGS-B', jac=True,\n                                     bounds=((0, None),) * self.coef_.shape[0], options={'maxiter': 1000000})['x']\n\n        self.coef_ = self.coef_ * self.X_scale_\n\n        return self\n", "idx": 1342}
{"namespace": "diffprivlib.models.k_means.KMeans.fit", "completion": "        # Check if the data is valid\n        self._validate_data(X, y, sample_weight)\n\n        # Check if the bounds are valid\n        self._validate_bounds(X)\n\n        # Check if the epsilon is valid\n        self._validate_epsilon(self.epsilon)\n\n        # Check if the accountant is valid\n        self._validate_accountant(self.accountant)\n\n        # Check if the number of clusters is valid\n        self._validate_n_clusters(self.n_clusters)\n\n        # Check if the number of threads is valid\n        self._validate_n_threads(self._n_threads)\n\n        # Check if the number of clusters is valid\n        self._validate_n_clusters(self.n_clusters)\n\n        # Check if the number of threads is valid\n        self._validate_n_threads(self._n_threads)\n\n        # Check if the number of clusters is valid\n        self._validate_n_clusters(self.n_clusters)\n\n        # Check if the number of threads is valid\n        self._validate_n_threads(self._n_threads)\n\n        # Check if the number of clusters is valid\n        self._validate_n_clusters(self.n_clusters)\n\n        # Check if the number of threads is valid\n        self._validate_n_threads(self._n_threads)\n\n        # Check if the number of clusters is valid\n        self._validate_n_clusters(self.n_clusters)\n\n        # Check if the number of threads is valid\n        self._validate_n_threads(self._n_threads)\n\n        # Check if the number of clusters is valid\n        self._validate_n_clusters(self.n_clusters)\n\n        # Check if the number of threads is valid\n        self._validate_n_threads(self._n_threads)\n\n        # Check if the number of clusters is valid\n        self._validate_n_clusters(self.n_clusters)\n\n        # Check if the number of threads is valid\n        self._validate_n_threads(self._n_threads)\n\n        # Check if the number of clusters is valid\n        self._validate_n_clusters(self.n_clusters)\n\n        # Check if the number of threads is valid\n        self._validate_n_threads(self._n_threads)\n\n        # Check if the number of clusters is valid\n        self._validate_n_clusters(", "idx": 1343}
{"namespace": "diffprivlib.models.forest._FittingTree.__getstate__", "completion": "        # Get the state of the Fitting Tree object\n        state = {\n            \"max_depth\": self.max_depth,\n            \"node_count\": self.node_count,\n            \"nodes\": self.nodes,\n            \"values\": self.values,\n            \"classes\": self.classes,\n            \"epsilon\": self.epsilon,\n            \"bounds\": self.bounds,\n            \"random_state\": self.random_state\n        }\n\n        return state\n", "idx": 1344}
{"namespace": "diffprivlib.models.forest._FittingTree.fit", "completion": "        if not hasattr(self, \"nodes\"):\n            raise ValueError(\"Tree has not been built. Call build() first.\")\n\n        # Check if we have a leaf node, then add it\n        leaves = []\n        for node in self.nodes:\n            if node.left_child == self._TREE_LEAF and node.right_child == self._TREE_LEAF:\n                leaves.append(node.node_id)\n\n        # Calculate the unique leaves\n        unique_leaves = np.unique(leaves)\n\n        # Initialize an array to store the values for each leaf\n        self.values_ = np.zeros(len(unique_leaves))\n\n        # Populate the values for the real leaves based on the target vector\n        for i, leaf in enumerate(unique_leaves):\n            self.values_[i] = y[leaves == leaf][0]\n\n        # Populate the values for the empty leaves\n        for i, leaf in enumerate(unique_leaves):\n            if self.values_[i] == 0:\n                self.values_[i] = self.random_state.randint(self.classes.shape[0])\n\n        # Assign the calculated values to the tree\n        for i, leaf in enumerate(unique_leaves):\n            self.nodes[leaf].value = self.values_[i]\n\n        return self\n", "idx": 1345}
{"namespace": "diffprivlib.tools.histograms.histogram", "completion": "    # Check for unused arguments\n    warn_unused_args(unused_args)\n\n    # Check for valid epsilon\n    if epsilon <= 0:\n        raise ValueError(\"Epsilon must be greater than 0.\")\n\n    # Check for valid bins\n    if isinstance(bins, int):\n        if bins <= 0:\n            raise ValueError(\"Number of bins must be greater than 0.\")\n    elif isinstance(bins, str):\n        if bins != 'auto':\n            raise ValueError(\"Invalid value for bins. Valid values are 'auto' or an integer.\")\n    else:\n        raise ValueError(\"Invalid value for bins. Valid values are 'auto' or an integer.\")\n\n    # Check for valid range\n    if range is not None:\n        if not isinstance(range, tuple) or len(range) != 2:\n            raise ValueError(\"Invalid value for range. Valid values are a tuple of length 2.\")\n        if range[0] >= range[1]:\n            raise ValueError(\"Invalid value for range. The lower bound must be less than the upper bound.\")\n\n    # Check for valid weights\n    if weights is not None:\n        if not isinstance(weights, np.ndarray):\n            raise ValueError(\"Invalid value for weights. Valid values are a numpy array.\")\n        if len(weights) != len(sample):\n            raise ValueError(\"Invalid value for weights. The length of weights must be equal to the length of sample.\")\n\n    # Check for valid density\n    if density is not None:\n        if not isinstance(density, bool):\n            raise ValueError(\"Invalid value for density. Valid values are True or False.\")\n\n    # Check for valid random_state\n    random_state = check_random_state(random_state)\n\n    # Check for valid accountant\n    if accountant is not None:\n        if not isinstance(accountant, BudgetAccountant):\n            raise ValueError(\"Invalid value for accountant. Valid values are BudgetAccountant.\")\n\n    # Check for valid sample\n    if not isinstance(sample, np.ndarray):\n        raise ValueError(\"Invalid value for sample. Valid values are a numpy array.\")\n\n    # Check for valid sample\n    if len(sample) == 0:\n        raise ValueError(\"Invalid value for sample. The length of sample must be greater than 0.\")\n\n    # Check for valid sample\n    if len(sample) == 1:\n        warnings.warn(\"The input sample has only one element. The histogram will be a single point.\", PrivacyLeak", "idx": 1346}
{"namespace": "diffprivlib.tools.histograms.histogram2d", "completion": "    warn_unused_args(unused_args)\n\n    random_state = check_random_state(random_state)\n\n    accountant = BudgetAccountant.load_default(accountant)\n    accountant.check(epsilon, 0)\n\n    if range is None:\n        warnings.warn(\"Range parameter has not been specified. Falling back to taking range from the data.\\n\"\n                      \"To ensure differential privacy, and no additional privacy leakage, the range must be \"\n                      \"specified independently of the data (i.e., using domain knowledge).\", PrivacyLeakWarning)\n\n    hist, xedges, yedges = np.histogram2d(array_x, array_y, bins=bins, range=range, weights=weights, density=None)\n\n    dp_mech = GeometricTruncated(epsilon=epsilon, sensitivity=1, lower=0, upper=maxsize, random_state=random_state)\n\n    dp_hist = np.zeros_like(hist)\n    iterator = np.nditer(hist, flags=['multi_index'])\n\n    while not iterator.finished:\n        dp_hist[iterator.multi_index] = dp_mech.randomise(int(iterator[0]))\n        iterator.iternext()\n\n    dp_hist = dp_hist.astype(float, casting='safe')\n\n    if density:\n        # calculate the probability density function\n        dp_hist_sum = dp_hist.sum()\n        for i in np.arange(2):\n            shape = np.ones(2, int)\n            shape[i] = dp_hist.shape[i]\n            # noinspection PyUnresolvedReferences\n            dp_hist = dp_hist / np.diff(xedges[i]).reshape(shape)\n\n        if dp_hist_sum > 0:\n            dp_hist /= dp_hist_sum\n\n    accountant.spend(epsilon, 0)\n\n    return dp_hist, xedges, yedges", "idx": 1347}
{"namespace": "diffprivlib.tools.utils.nanmean", "completion": "    warn_unused_args(unused_args)\n\n    return _mean(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                 random_state=random_state, accountant=accountant, nan=True)\n\n", "idx": 1348}
{"namespace": "diffprivlib.tools.utils.var", "completion": "    warn_unused_args(unused_args)\n\n    return _var(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant)\n\n", "idx": 1349}
{"namespace": "diffprivlib.tools.utils.nanvar", "completion": "    warn_unused_args(unused_args)\n\n    return _var(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)\n\n", "idx": 1350}
{"namespace": "diffprivlib.tools.utils.std", "completion": "    warn_unused_args(unused_args)\n\n    return _std(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=False)\n\n", "idx": 1351}
{"namespace": "diffprivlib.tools.utils.nanstd", "completion": "    warn_unused_args(unused_args)\n\n    return _std(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)\n\n", "idx": 1352}
{"namespace": "diffprivlib.tools.utils.sum", "completion": "    warn_unused_args(unused_args)\n\n    return _sum(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant)\n\n", "idx": 1353}
{"namespace": "diffprivlib.tools.utils.nansum", "completion": "    warn_unused_args(unused_args)\n\n    return _sum(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)\n\n", "idx": 1354}
{"namespace": "diffprivlib.tools.quantiles.quantile", "completion": "    # Check random state\n    random_state = check_random_state(random_state)\n\n    # Check quantile\n    quant = check_quantile(quant)\n\n    # Check bounds\n    bounds = check_bounds(array, bounds)\n\n    # Check axis\n    axis = check_axis(array, axis)\n\n    # Check keepdims\n    keepdims = check_keepdims(keepdims)\n\n    # Check epsilon\n    epsilon = check_epsilon(epsilon)\n\n    # Check accountant\n    accountant = check_accountant(accountant)\n\n    # Check array\n    array = check_array(array, bounds)\n\n    # Check array of quantiles\n    quant = check_array_of_quantiles(quant, array)\n\n    # Check array of quantiles\n    quant = check_array_of_quantiles(quant, array)\n\n    # Check array of quantiles\n    quant = check_array_of_quantiles(quant, array)\n\n    # Check array of quantiles\n    quant = check_array_of_quantiles(quant, array)\n\n    # Check array of quantiles\n    quant = check_array_of_quantiles(quant, array)\n\n    # Check array of quantiles\n    quant = check_array_of_quantiles(quant, array)\n\n    # Check array of quantiles\n    quant = check_array_of_quantiles(quant, array)\n\n    # Check array of quantiles\n    quant = check_array_of_quantiles(quant, array)\n\n    # Check array of quantiles\n    quant = check_array_of_quantiles(quant, array)\n\n    # Check array of quantiles\n    quant = check_array_of_quantiles(quant, array)\n\n    # Check array of quantiles\n    quant = check_array_of_quantiles(quant, array)\n\n    # Check array of quantiles\n    quant = check_array_of_quantiles(quant, array)\n\n    # Check array of quantiles\n    quant = check_array_of_quantiles(quant, array)\n\n    # Check array of quantiles\n    quant = check_array_of_quantiles(quant, array)\n\n    # Check array of quantiles\n    quant = check_array_of_quantiles(quant, array)\n\n    # Check array of quantiles\n    quant = check_array_of_quantiles(quant, array)\n\n    # Check array of", "idx": 1355}
{"namespace": "diffprivlib.tools.quantiles.percentile", "completion": "    from diffprivlib.utils import check_random_state\n    from diffprivlib.utils import PrivacyLeakWarning\n    from diffprivlib.accountant import BudgetAccountant\n    from diffprivlib.validation import clip_to_bounds\n    from diffprivlib.tools.utils import _wrap_axis\n    from diffprivlib.validation import check_bounds\n    warn_unused_args(unused_args)\n\n    random_state = check_random_state(random_state)\n\n    if bounds is None:\n        warnings.warn(\"Bounds have not been specified and will be calculated on the data provided. This will \"\n                      \"result in additional privacy leakage. To ensure differential privacy and no additional \"\n                      \"privacy leakage, specify bounds for each dimension.\", PrivacyLeakWarning)\n        bounds = (np.min(array), np.max(array))\n\n    percent = np.ravel(percent)\n\n    if np.any(percent < 0) or np.any(percent > 100):\n        raise ValueError(\"Percentiles must be in the unit interval [0, 100].\")\n\n    if len(percent) > 1:\n        return np.array([percentile(array, p_i, epsilon=epsilon / len(percent), bounds=bounds, axis=axis, keepdims=keepdims,\n                                    accountant=accountant, random_state=random_state) for p_i in percent])\n\n    # Dealing with a single percent from now on\n    percent = percent.item()\n\n    if axis is not None or keepdims:\n        return _wrap_axis(percentile, array, percent=percent, epsilon=epsilon, bounds=bounds, axis=axis, keepdims=keepdims,\n                          random_state=random_state, accountant=accountant)\n\n    # Dealing with a scalar output from now on\n    bounds = check_bounds(bounds, shape=0, min_separation=1e-5)\n\n    accountant = BudgetAccountant.load_default(accountant)\n    accountant.check(epsilon, 0)\n\n    # Let's ravel array to be single-dimensional\n    array = clip_to_bounds(np.ravel(array), bounds)\n\n    k = array.size\n    array = np.append(array, list(bounds))\n    array.sort()\n\n    interval_sizes = np", "idx": 1356}
{"namespace": "diffprivlib.mechanisms.base.bernoulli_neg_exp", "completion": "    if not isinstance(gamma, Real) or gamma < 0:\n        raise ValueError(\"Gamma must be non-negative\")\n\n    if gamma == 0:\n        return 0\n\n    return int(np.random.random() < np.exp(-gamma))\n\n", "idx": 1357}
{"namespace": "discord.utils.snowflake_time", "completion": "    timestamp = (id >> 22) + DISCORD_EPOCH\n    return datetime.datetime.utcfromtimestamp(timestamp)\n\n", "idx": 1358}
{"namespace": "discord.utils.time_snowflake", "completion": "    if not dt.tzinfo:\n        dt = dt.replace(tzinfo=datetime.timezone.utc)\n    timestamp = (dt - DISCORD_EPOCH).total_seconds() * 1000\n    if high:\n        return int(timestamp) << 22\n    return int(timestamp) >> 12\n\n", "idx": 1359}
{"namespace": "discord.utils.resolve_invite", "completion": "    if isinstance(invite, Invite):\n        return ResolvedInvite(invite.code, invite.event_id)\n    elif isinstance(invite, str):\n        if invite.startswith('https://discord.gg/'):\n            return ResolvedInvite(invite[19:], None)\n        elif invite.startswith('https://discord.com/invite/'):\n            return ResolvedInvite(invite[26:], None)\n        elif invite.startswith('https://discordapp.com/invite/'):\n            return ResolvedInvite(invite[28:], None)\n        elif invite.startswith('https://discord.me/'):\n            return ResolvedInvite(invite[17:], None)\n        elif invite.startswith('https://discord.io/'):\n            return ResolvedInvite(invite[17:], None)\n        elif invite.startswith('https://discord.us/'):\n            return ResolvedInvite(invite[17:], None)\n        elif invite.startswith('https://discord.gg/'):\n            return ResolvedInvite(invite[17:], None)\n        elif invite.startswith('https://discord.com/'):\n            return ResolvedInvite(invite[17:], None)\n        elif invite.startswith('https://discord.com/'):\n            return ResolvedInvite(invite[17:], None)\n        elif invite.startswith('https://discord.com/'):\n            return ResolvedInvite(invite[17:], None)\n        elif invite.startswith('https://discord.com/'):\n            return ResolvedInvite(invite[17:], None)\n        elif invite.startswith('https://discord.com/'):\n            return ResolvedInvite(invite[17:], None)\n        elif invite.startswith('https://discord.com/'):\n            return ResolvedInvite(invite[17:], None)\n        elif invite.startswith('https://discord.com/'):\n            return ResolvedInvite(invite[17:], None)\n        elif invite.startswith('https://discord.com/'):\n            return ResolvedInvite(invite[17:], None)\n        elif invite.startswith('https://discord.com/'):\n            return ResolvedInvite(invite[17:], None)\n        elif invite.startswith('https://discord.com/'):\n            return ResolvedInvite(invite[17:], None)\n        elif invite.startswith('https://discord.com/'):\n            return ResolvedInvite(invite[17:], None)\n        elif invite.startswith('https://discord.com/'):", "idx": 1360}
{"namespace": "discord.utils.resolve_annotation", "completion": "    if annotation is None:\n        return type(None)\n\n    if isinstance(annotation, str):\n        annotation = ForwardRef(annotation)\n\n    if localns is None:\n        localns = {}\n\n    if cache is None:\n        cache = {}\n\n    return evaluate_annotation(annotation, globalns, localns, cache)\n\n", "idx": 1361}
{"namespace": "discord.ext.tasks.loop", "completion": "    def decorator(coro: LF) -> Loop[LF]:\n        return Loop(\n            coro,\n            seconds=seconds,\n            minutes=minutes,\n            hours=hours,\n            time=time,\n            count=count,\n            reconnect=reconnect,\n        )\n\n    return decorator", "idx": 1362}
{"namespace": "barf.analysis.gadgets.classifier.GadgetClassifier.classify", "completion": "        # Classified gadgets.\n        classified_gadgets = []\n\n        # Iterate through the classifiers.\n        for gadget_type, classifier in self._classifiers.items():\n\n            # Try to classify the gadget.\n            try:\n\n                # Classify the gadget.\n                classified_gadgets.append(classifier(gadget))\n\n            # If an error occurs during classification, print the error message and traceback.\n            except Exception as e:\n\n                print(\"Error during classification: %s\" % e)\n                print(e.__traceback__)\n\n        # Sort the classified gadgets.\n        classified_gadgets.sort(key=lambda g: g.to_string())\n\n        return classified_gadgets\n", "idx": 1363}
{"namespace": "barf.analysis.gadgets.finder.GadgetFinder.find", "completion": "        # Set the maximum number of bytes and the depth of instructions to be considered.\n        self._max_bytes = byte_depth\n        self._instrs_depth = instrs_depth\n\n        # Call the appropriate method based on the architecture to find the candidates.\n        if self._architecture == ARCH_ARM:\n            return self._find_arm(start_address, end_address)\n        elif self._architecture == ARCH_X86:\n            return self._find_x86(start_address, end_address)\n        else:\n            raise NotImplementedError(\"Architecture not supported.\")\n", "idx": 1364}
{"namespace": "barf.core.reil.parser.ReilParser.parse", "completion": "        parsed_instrs = []\n\n        for instr in instrs:\n            instr = instr.lower()\n\n            if instr in self._cache:\n                parsed_instrs.append(copy.deepcopy(self._cache[instr]))\n            else:\n                try:\n                    parsed_instr = instruction.parseString(instr)[0]\n\n                    self._cache[instr] = parsed_instr\n\n                    parsed_instrs.append(parsed_instr)\n                except Exception as e:\n                    logger.error(\"Error parsing instruction: %s\", instr)\n                    logger.error(e)\n\n        return parsed_instrs", "idx": 1365}
{"namespace": "barf.core.smt.smtfunction.zero_extend", "completion": "    if isinstance(s, Constant):\n        if s.size() == size:\n            return s\n        else:\n            return BitVec(size, s.value())\n    elif isinstance(s, BitVec):\n        if s.size() == size:\n            return s\n        else:\n            return BitVec(size, s.value())\n    else:\n        raise TypeError(\"The input value is not of a valid type.\")", "idx": 1366}
{"namespace": "barf.core.smt.smtfunction.extract", "completion": "    assert type(s) in (Constant, BitVec) and offset >= 0 and size >= 0\n\n    if offset == 0 and size == s.size:\n        return s\n\n    return BitVec(size, \"(_ extract {})\".format(size), s, offset)\n\n", "idx": 1367}
{"namespace": "barf.core.smt.smtfunction.ite", "completion": "    assert type(cond) is bool\n    assert type(true) in (Constant, BitVec) and true.size == size\n    assert type(false) in (Constant, BitVec) and false.size == size\n\n    return BitVec(size, \"(_ ite {})\".format(cond), true, false)\n\n", "idx": 1368}
{"namespace": "barf.core.smt.smtfunction.concat", "completion": "    if len(args) == 1:\n        return args[0]\n\n    return BitVec(size, \"(_ concat {})\".format(size), *args)\n\n", "idx": 1369}
{"namespace": "barf.core.smt.smtsymbol.BitVecArray.declaration", "completion": "        return \"(declare-fun {} () (Array (_ BitVec {}) (_ BitVec {})))\".format(self.name, self.key_size, self.value_size)\n", "idx": 1370}
{"namespace": "barf.arch.translator.InstructionTranslator.translate", "completion": "        # The REIL representation of the instruction.\n        reil_instruction = None\n\n        # The REIL builder.\n        reil_builder = ReilBuilder()\n\n        # The REIL mnemonic.\n        reil_mnemonic = None\n\n        # The REIL operands.\n        reil_operands = []\n\n        # The REIL immediate operand.\n        reil_immediate_operand = None\n\n        # The REIL label.\n        reil_label = None\n\n        # The REIL register operand.\n        reil_register_operand = None\n\n        # The REIL immediate operand.\n        reil_immediate_operand = None\n\n        # The REIL label.\n        reil_label = None\n\n        # The REIL register operand.\n        reil_register_operand = None\n\n        # The REIL immediate operand.\n        reil_immediate_operand = None\n\n        # The REIL label.\n        reil_label = None\n\n        # The REIL register operand.\n        reil_register_operand = None\n\n        # The REIL immediate operand.\n        reil_immediate_operand = None\n\n        # The REIL label.\n        reil_label = None\n\n        # The REIL register operand.\n        reil_register_operand = None\n\n        # The REIL immediate operand.\n        reil_immediate_operand = None\n\n        # The REIL label.\n        reil_label = None\n\n        # The REIL register operand.\n        reil_register_operand = None\n\n        # The REIL immediate operand.\n        reil_immediate_operand = None\n\n        # The REIL label.\n        reil_label = None\n\n        # The REIL register operand.\n        reil_register_operand = None\n\n        # The REIL immediate operand.\n        reil_immediate_operand = None\n\n        # The REIL label.\n        reil_label = None\n\n        # The REIL register operand.\n        reil_register_operand = None\n\n        # The REIL immediate operand.\n        reil_immediate_operand = None\n\n        # The REIL label.\n        reil_label = None\n\n        # The REIL register operand.\n        reil_register_operand = None\n\n        # The REIL immediate operand.\n        reil_immediate_operand = None\n\n        # The REIL label.\n        reil_label = None\n\n        # The REIL register operand.\n        reil_register_operand = None\n\n        # The REIL", "idx": 1371}
{"namespace": "barf.arch.emulator.Emulator.load_binary", "completion": "        if binary.startswith(b'\\x7fELF'):\n            self._load_binary_elf(binary)\n        elif binary.startswith(b'MZ'):\n            self._load_binary_pe(binary)\n        else:\n            raise Exception(\"Unknown file format.\")", "idx": 1372}
{"namespace": "barf.arch.arm.parser.ArmParser.parse", "completion": "        instr = instr.lower()\n\n        if instr in self._cache:\n            return copy.deepcopy(self._cache[instr])\n\n        try:\n            parsed_instr = instruction.parseString(instr)\n        except Exception as e:\n            logger.error(\"Failed to parse instruction: %s\", instr)\n            logger.error(e)\n            return None\n\n        self._cache[instr] = parsed_instr\n\n        return copy.deepcopy(parsed_instr)", "idx": 1373}
{"namespace": "barf.arch.x86.parser.X86Parser.parse", "completion": "        instr = instr.lower()\n\n        if instr in self._cache:\n            return copy.deepcopy(self._cache[instr])\n\n        try:\n            parsed_instr = instruction.parseString(instr)[0]\n        except Exception as e:\n            logger.error(\"Failed to parse instruction: %s\", instr)\n            logger.error(e)\n            return None\n\n        self._cache[instr] = parsed_instr\n\n        return copy.deepcopy(parsed_instr)", "idx": 1374}
{"namespace": "faker.utils.text.slugify", "completion": "    if allow_unicode:\n        value = unicodedata.normalize(\"NFKD\", value)\n        value = _re_spaces.sub(\"-\", value)\n    else:\n        value = _re_pattern.sub(\"\", value)\n        value = _re_spaces.sub(\"-\", value)\n\n    if allow_dots:\n        value = _re_pattern_allow_dots.sub(\"\", value)\n\n    return value.lower()", "idx": 1375}
{"namespace": "faker.utils.checksums.calculate_luhn", "completion": "    partial_number = partial_number * 10\n    checksum = luhn_checksum(partial_number)\n    if checksum == 0:\n        return checksum\n    else:\n        return 10 - checksum\n\n", "idx": 1376}
{"namespace": "faker.utils.distribution.choices_distribution_unique", "completion": "    if random is None:\n        random = mod_random\n    if p is None:\n        p = [1.0] * len(a)\n    if len(a) != len(p):\n        raise ValueError(\"The length of the input sequence and the probabilities must be the same.\")\n    if length > len(a):\n        raise ValueError(\"The number of unique choices to generate cannot be greater than the length of the input sequence.\")\n    if length < 1:\n        raise ValueError(\"The number of unique choices to generate must be greater than or equal to 1.\")\n    if not all(0 <= x <= 1 for x in p):\n        raise ValueError(\"The probabilities must be between 0 and 1.\")\n    if sum(p) != 1:\n        raise ValueError(\"The sum of the probabilities must be equal to 1.\")\n    cdf = list(cumsum(p))\n    choices = []\n    for _ in range(length):\n        r = random.random()\n        i = bisect.bisect_left(cdf, r)\n        choices.append(a[i])\n        cdf[i] = 1.0\n    return choices\n\n", "idx": 1377}
{"namespace": "faker.utils.loading.find_available_locales", "completion": "    available_locales = []\n\n    for provider in providers:\n        provider_module = import_module(provider)\n        if getattr(provider_module, \"is_localized\", False):\n            languages = getattr(provider_module, \"languages\", [])\n            available_locales.extend(languages)\n\n    return sorted(available_locales)", "idx": 1378}
{"namespace": "faker.utils.loading.find_available_providers", "completion": "    available_providers = set()\n\n    for module in modules:\n        if hasattr(module, \"__package__\"):\n            providers = [f\"{module.__package__}.{module.__name__}\" for module in list_module(module)]\n            available_providers.update(providers)\n    return sorted(available_providers)", "idx": 1379}
{"namespace": "faker.providers.credit_card.Provider._generate_number", "completion": "        number = prefix\n        for _ in range(length - len(prefix)):\n            number += str(self.generator.random.randint(0, 9))\n        number += str(self.luhn_check_digit(number))\n        return number\n", "idx": 1380}
{"namespace": "faker.decode.unidecode", "completion": "    \"\"\"\n    This function takes a string as input and returns a new string with all non-ASCII characters replaced by their closest ASCII equivalents. It iterates over each character in the input string, checks its codepoint, and replaces it with the corresponding ASCII character if available.\n    Input-Output Arguments\n    :param txt: String. The input text to be processed.\n    :return: String. The processed text with non-ASCII characters replaced by their closest ASCII equivalents.\n    \"\"\"\n\n    \"\"\"\n    This function takes a string as input and returns a new string with all non-ASCII characters replaced by their closest ASCII equivalents. It iterates over each character in the input string, checks its codepoint, and replaces it with the corresponding ASCII character if available.\n    Input-Output Arguments\n    :param txt: String. The input text to be processed.\n    :return: String. The processed text with non-ASCII characters replaced by their closest ASCII equivalents.\n    \"\"\"\n\n    \"\"\"\n    This function takes a string as input and returns a new string with all non-ASCII characters replaced by their closest ASCII equivalents. It iterates over each character in the input string, checks its codepoint, and replaces it with the corresponding ASCII character if available.\n    Input-Output Arguments\n    :param txt: String. The input text to be processed.\n    :return: String. The processed text with non-ASCII characters replaced by their closest ASCII equivalents.\n    \"\"\"\n\n    \"\"\"\n    This function takes a string as input and returns a new string with all non-ASCII characters replaced by their closest ASCII equivalents. It iterates over each character in the input string, checks its codepoint, and replaces it with the corresponding ASCII character if available.\n    Input-Output Arguments\n    :param txt: String. The input text to be processed.\n    :return: String. The processed text with non-ASCII characters replaced by their closest ASCII equivalents.\n    \"\"\"\n\n    \"\"\"\n    This function takes a string as input and returns a new string with all non-ASCII characters replaced by their closest ASCII equivalents. It iterates over each character in the input string, checks its codepoint, and replaces it with the corresponding ASCII character if available.\n    Input-Output Arguments\n    :param txt: String. The input text to be processed.\n    :return: String. The processed text with non-ASCII characters replaced by their closest ASCII equivalents.\n    \"\"\"\n\n    \"\"\"\n    This function", "idx": 1381}
{"namespace": "dash.fingerprint.build_fingerprint", "completion": "    \"\"\"\n    This function builds a fingerprint for a file based on the given path, version, and hash value. It extracts the filename and extension from the path, constructs a file path without the filename, replaces the version with underscores, and concatenates all the parts to form the fingerprint. The format of a fingerprint is \"{file_path}.v{v_str}m{hash_value}.{extension}\".\n    Input-Output Arguments\n    :param path: String. The path of the file.\n    :param version: Any data type. The version of the file.\n    :param hash_value: Any data type. The hash value of the file.\n    :return: String. The fingerprint of the file.\n    \"\"\"\n\n    \"\"\"\n    This function builds a fingerprint for a file based on the given path, version, and hash value. It extracts the filename and extension from the path, constructs a file path without the filename, replaces the version with underscores, and concatenates all the parts to form the fingerprint. The format of a fingerprint is \"{file_path}.v{v_str}m{hash_value}.{extension}\".\n    Input-Output Arguments\n    :param path: String. The path of the file.\n    :param version: Any data type. The version of the file.\n    :param hash_value: Any data type. The hash value of the file.\n    :return: String. The fingerprint of the file.\n    \"\"\"\n\n    \"\"\"\n    This function builds a fingerprint for a file based on the given path, version, and hash value. It extracts the filename and extension from the path, constructs a file path without the filename, replaces the version with underscores, and concatenates all the parts to form the fingerprint. The format of a fingerprint is \"{file_path}.v{v_str}m{hash_value}.{extension}\".\n    Input-Output Arguments\n    :param path: String. The path of the file.\n    :param version: Any data type. The version of the file.\n    :param hash_value: Any data type. The hash value of the file.\n    :return: String. The fingerprint of the file.\n    \"\"\"\n\n    \"\"\"\n    This function builds a fingerprint for a file based on the given path, version, and hash value. It extracts the filename and extension from the path, constructs a file path without the filename, replaces the version with underscores, and", "idx": 1382}
{"namespace": "dash.fingerprint.check_fingerprint", "completion": "    if cache_regex.match(path):\n        return path.split(\".\")[0], True\n    else:\n        return path, False", "idx": 1383}
{"namespace": "dash._configs.pages_folder_config", "completion": "    if use_pages:\n        if not os.path.exists(pages_folder):\n            raise exceptions.InvalidConfig(\n                f\"The pages folder {pages_folder} does not exist.\"\n            )\n        return pages_folder\n    else:\n        return None\n\n", "idx": 1384}
{"namespace": "dash._grouping.flatten_grouping", "completion": "    if schema is None:\n        schema = grouping\n\n    if isinstance(schema, AttributeDict):\n        schema = stringify_id(schema)\n\n    if isinstance(grouping, (list, tuple)):\n        return [flatten_grouping(item, schema) for item in grouping]\n    elif isinstance(grouping, dict):\n        return [flatten_grouping(key, schema) for key in grouping]\n    elif isinstance(grouping, (str, int, float)):\n        return [grouping]\n    else:\n        raise InvalidCallbackReturnValue(\n            \"The input grouping value is not a valid grouping value. \"\n            \"A grouping value is either a list, tuple, dict, str, int, or float.\"\n        )\n\n", "idx": 1385}
{"namespace": "dash._grouping.make_grouping_by_index", "completion": "    if isinstance(schema, (tuple, list)):\n        return tuple(\n            make_grouping_by_index(schema_el, flat_values)\n            for schema_el, flat_values in zip(schema, flat_values)\n        )\n\n    if isinstance(schema, dict):\n        return AttributeDict(\n            {k: make_grouping_by_index(schema[k], flat_values) for k in schema}\n        )\n\n    return flat_values[0]\n\n", "idx": 1386}
{"namespace": "dash._grouping.map_grouping", "completion": "    if isinstance(grouping, (tuple, list)):\n        return tuple(map_grouping(fn, el) for el in grouping)\n\n    if isinstance(grouping, dict):\n        return {k: map_grouping(fn, v) for k, v in grouping.items()}\n\n    return fn(grouping)\n\n", "idx": 1387}
{"namespace": "dash._grouping.validate_grouping", "completion": "    if full_schema is None:\n        full_schema = schema\n\n    if isinstance(schema, (tuple, list)):\n        SchemaLengthValidationError.check(\n            grouping, full_schema, path, len(schema)\n        )\n        for i, el in enumerate(schema):\n            validate_grouping(grouping[i], el, full_schema, path + (i,))\n\n    elif isinstance(schema, dict):\n        SchemaKeysValidationError.check(\n            grouping, full_schema, path, schema.keys()\n        )\n        for k, v in schema.items():\n            validate_grouping(grouping[k], v, full_schema, path + (k,))\n\n    else:\n        SchemaTypeValidationError.check(\n            grouping, full_schema, path, type(schema)\n        )\n\n", "idx": 1388}
{"namespace": "dash._get_paths.app_get_relative_path", "completion": "    if requests_pathname == \"/\" and path == \"\":\n        return \"/\"\n    elif requests_pathname != \"/\" and path == \"\":\n        return requests_pathname\n    elif path.startswith(\"/\"):\n        raise exceptions.InvalidPathException(\n            \"The path should not start with '/'.\"\n        )\n    else:\n        return \"/\".join(\n            [\n                requests_pathname.rstrip(\"/\"),\n                path.lstrip(\"/\"),\n            ]\n        )\n\n", "idx": 1389}
{"namespace": "dash._get_paths.app_strip_relative_path", "completion": "    if requests_pathname == \"/\" and path == \"\":\n        return \"\"\n    if requests_pathname != \"/\" and path == \"\":\n        return requests_pathname\n    if not path.startswith(\"/\"):\n        raise exceptions.UnsupportedRelativePath(\n            f\"\"\"\n            Paths that aren't prefixed with a leading / are not supported.\n            You supplied: {path}\n            \"\"\"\n        )\n    if requests_pathname.endswith(\"/\"):\n        return \"/\".join([requests_pathname.rstrip(\"/\"), path.lstrip(\"/\")])\n    else:\n        return \"/\".join([requests_pathname, path.lstrip(\"/\")])", "idx": 1390}
{"namespace": "dash.development._py_components_generation.js_to_py_type", "completion": "    if is_flow_type:\n        type_map = map_js_to_py_types_flow_types(type_object)\n    else:\n        type_map = map_js_to_py_types_prop_types(type_object, indent_num)\n\n    if \"name\" in type_object:\n        return type_map[type_object[\"name\"]]()\n    if \"elements\" in type_object:\n        return \"list of \" + js_to_py_type(type_object[\"elements\"])\n    if \"signature\" in type_object:\n        return \"dict with keys:\\n\" + \"\\n\".join(\n            create_prop_docstring(\n                prop_name=prop[\"key\"],\n                type_object=prop[\"value\"],\n                required=prop[\"value\"][\"required\"],\n                description=prop[\"value\"].get(\"description\", \"\"),\n                default=prop.get(\"defaultValue\"),\n                indent_num=indent_num + 2,\n                is_flow_type=True,\n            )\n            for prop in type_object[\"signature\"][\"properties\"]\n        )\n    return \"\"", "idx": 1391}
{"namespace": "dash.development.component_loader.load_components", "completion": "    # Load metadata\n    metadata = _get_metadata(metadata_path)\n\n    # Register component library\n    generate_class(\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary\",\n        \"ComponentLibrary", "idx": 1392}
{"namespace": "dash.development.component_loader.generate_classes", "completion": "    # Load the metadata\n    data = _get_metadata(metadata_path)\n\n    # Iterate over each property name (which is a path to the component)\n    for componentPath in data:\n        componentData = data[componentPath]\n\n        # Extract component name from path\n        # e.g. src/components/MyControl.react.js\n        # TODO Make more robust - some folks will write .jsx and others\n        # will be on windows. Unfortunately react-docgen doesn't include\n        # the name of the component atm.\n        name = componentPath.split(\"/\").pop().split(\".\")[0]\n        component = generate_class(\n            name, componentData[\"props\"], componentData[\"description\"], namespace, None\n        )\n\n        # Write the component to a file\n        component.write_to_file(os.path.join(namespace, name + \".py\"))\n\n    # Write the imports file\n    with open(os.path.join(namespace, \"__init__.py\"), \"w\") as f:\n        f.write(\"__all__ = [\")\n        for componentPath in data:\n            componentData = data[componentPath]\n            name = componentPath.split(\"/\").pop().split(\".\")[0]\n            f.write(name + \", \")\n        f.write(\"]\")", "idx": 1393}
{"namespace": "dash.development.base_component.Component.to_plotly_json", "completion": "        # pylint: disable=no-member\n        json = {\n            \"type\": self._type,\n            \"namespace\": self._namespace,\n            \"props\": {\n                k: v\n                for k, v in self.__dict__.items()\n                if k in self._prop_names\n            },\n        }\n\n        # Add wildcard properties\n        for k, v in self.__dict__.items():\n            if k.startswith(\"data-\") or k.startswith(\"aria-\"):\n                json[\"props\"][k] = v\n\n        return json\n", "idx": 1394}
{"namespace": "dash.development.base_component.Component._traverse", "completion": "        # pylint: disable=access-member-before-definition,\n        # pylint: disable=attribute-defined-outside-init\n        if isinstance(self.children, Component):\n            yield self.children\n            return\n\n        # if children is like a list\n        if isinstance(self.children, (tuple, MutableSequence)):\n            for item in self.children:\n                # If the item itself is the one we're looking for\n                if isinstance(item, Component):\n                    yield item\n                # Otherwise, recursively dig into that item's subtree\n                # Make sure it's not like a string\n                elif isinstance(item, (tuple, MutableSequence)):\n                    for subitem in item:\n                        if isinstance(subitem, Component):\n                            yield subitem\n", "idx": 1395}
{"namespace": "dash.development._r_components_generation.make_namespace_exports", "completion": "    export_string = \"\"\n    for component in components:\n        if component.startswith(prefix):\n            export_string += \"export({prefix, {component}})\\n\".format(\n                prefix=prefix, component=component\n            )\n    return export_string\n\n", "idx": 1396}
{"namespace": "dash.development._collect_nodes.collect_nodes", "completion": "    if nodes is None:\n        nodes = []\n\n    for key, value in metadata.items():\n        if is_node(key):\n            nodes.append(base + key)\n        elif is_shape(key):\n            nodes = collect_nodes(value, base + key, nodes)\n        elif key == \"arrayOf\":\n            nodes = collect_array(value, base + key, nodes)\n        elif key == \"union\":\n            nodes = collect_union(value, base + key, nodes)\n        elif key == \"objectOf\":\n            nodes = collect_object(value, base + key, nodes)\n    return nodes\n\n", "idx": 1397}
{"namespace": "peewee.Index.where", "completion": "        if self._where is not None:\n            expressions = (self._where,) + expressions\n        self._where = reduce(operator.and_, expressions)\n", "idx": 1398}
{"namespace": "playhouse.dataset.DataSet.tables", "completion": "        tables = self._database.get_tables()\n        if self._include_views:\n            views = self._database.get_views()\n            tables.extend(views)\n        return tables\n", "idx": 1399}
{"namespace": "playhouse.dataset.DataSet.update_cache", "completion": "        if table is None:\n            self._models = self._introspector.generate_models(\n                skip_invalid=True,\n                literal_column_names=True,\n                include_views=self._include_views)\n        else:\n            self._models = self._introspector.generate_models(\n                skip_invalid=True,\n                literal_column_names=True,\n                include_views=self._include_views,\n                tables=[table])\n\n        # Update the cache for the table.\n        self._models[table] = self._introspector.generate_model(\n            table,\n            skip_invalid=True,\n            literal_column_names=True,\n            include_views=self._include_views)\n\n        # Update the cache for the related tables.\n        for table in self._models[table].get_related_tables():\n            self._models[table] = self._introspector.generate_model(\n                table,\n                skip_invalid=True,\n                literal_column_names=True,\n                include_views=self._include_views)\n", "idx": 1400}
{"namespace": "playhouse.dataset.DataSet.freeze", "completion": "        self._check_arguments(filename, file_obj, format, self._export_formats)\n        if filename:\n            file_obj = open_file(filename, 'w', encoding=encoding)\n        exporter = self._export_formats[format](self, file_obj, **kwargs)\n        exporter.export(query)\n        if filename:\n            file_obj.close()\n", "idx": 1401}
{"namespace": "playhouse.db_url.parse", "completion": "    parsed = urlparse(url)\n    return parseresult_to_dict(parsed, unquote_password)", "idx": 1402}
{"namespace": "playhouse.db_url.connect", "completion": "    connect_params.update(parse(url, unquote_password))\n    return create_engine(**connect_params)\n", "idx": 1403}
{"namespace": "playhouse.sqlite_changelog.ChangeLog.install", "completion": "        if create_table:\n            self.model.create_table()\n\n        if drop:\n            for action in self._actions:\n                if action == 'INSERT' and not insert:\n                    continue\n                if action == 'UPDATE' and not update:\n                    continue\n                if action == 'DELETE' and not delete:\n                    continue\n                self.db.execute_sql(self.drop_trigger_sql(model, action))\n\n        for action in self._actions:\n            if action == 'INSERT' and not insert:\n                continue\n            if action == 'UPDATE' and not update:\n                continue\n            if action == 'DELETE' and not delete:\n                continue\n            self.db.execute_sql(self.trigger_sql(model, action, skip_fields))", "idx": 1404}
{"namespace": "playhouse.kv.KeyValue.pop", "completion": "        if default is Sentinel:\n            return self.model.get(self.key == key).delete_instance()\n        else:\n            return self.model.get(self.key == key).delete_instance() or default\n", "idx": 1405}
{"namespace": "playhouse.signals.Signal.connect", "completion": "        if name is None:\n            name = receiver.__name__\n\n        if sender is not None:\n            if (name, sender) in self._receivers:\n                raise ValueError(\"Receiver with name %s and sender %s already exists\" % (name, sender))\n\n        self._receivers.add((name, sender))\n        self._receiver_list.append((receiver, name, sender))\n", "idx": 1406}
{"namespace": "playhouse.signals.Signal.disconnect", "completion": "        if receiver is None:\n            if name is None:\n                raise ValueError('receiver or name must be provided')\n            else:\n                receiver = self._get_receiver(name, sender)\n        if receiver is None:\n            raise ValueError('receiver named %s (for sender=%s) not '\n                             'connected' % (name, sender or 'any'))\n        self._receivers.remove((name, sender))\n        self._receiver_list.remove((name, receiver, sender))\n", "idx": 1407}
{"namespace": "backtrader.trade.Trade.update", "completion": "        # Update commissions\n        self.commission += commission\n\n        # Update size\n        self.size += size\n\n        # Update value\n        self.value += value\n\n        # Update pnl\n        self.pnl += pnl\n\n        # Update pnlcomm\n        self.pnlcomm += pnl - commission\n\n        # Update history\n        if self.historyon:\n            self.history.append(\n                TradeHistory(\n                    self.status,\n                    order.dt,\n                    order.bar,\n                    self.size,\n                    self.price,\n                    self.value,\n                    self.pnl,\n                    self.pnlcomm,\n                    order.tz\n                )\n            )\n\n        # Update status\n        if self.size == 0:\n            self.status = self.Closed\n            self.isclosed = True\n        elif self.size != 0:\n            self.status = self.Open\n            self.isopen = True\n\n        # Update justopened\n        if self.size != 0 and self.justopened is False:\n            self.justopened = True\n\n        # Update baropen\n        if self.size != 0 and self.baropen == 0:\n            self.baropen = order.bar\n\n        # Update dtopen\n        if self.size != 0 and self.dtopen == 0.0:\n            self.dtopen = order.dt\n\n        # Update barclose\n        if self.size == 0 and self.barclose == 0:\n            self.barclose = order.bar\n\n        # Update dtclose\n        if self.size == 0 and self.dtclose == 0.0:\n            self.dtclose = order.dt\n\n        # Update barlen\n        if self.size != 0:\n            self.barlen = order.bar - self.baropen\n\n        # Update price\n        if self.size != 0 and abs(self.size) > abs(self.size):\n            self.price = (self.price * abs(self.size) + price * size) / \\\n                         abs(self.size)\n        elif self.size != 0 and abs(self.size) <= abs(self.size):\n            self.price = price\n\n        # Update size\n        if self.size != 0 and self.size < 0:\n            self.size = 0\n        elif self.size != 0 and self.size > 0:\n            self.size = size\n\n        # Update", "idx": 1408}
{"namespace": "ydata_profiling.profile_report.ProfileReport.typeset", "completion": "        if self._typeset is None:\n            self._typeset = VisionsTypeset(\n                self.config, self._type_schema, self.df\n            )  # type: ignore\n\n        return self._typeset\n", "idx": 1409}
{"namespace": "ydata_profiling.report.presentation.flavours.html.frequency_table.HTMLFrequencyTable.render", "completion": "        if isinstance(self.content, list):\n            rows = self.content\n        else:\n            rows = [self.content]\n\n        html = \"\"\n        for row in rows:\n            html += f\"<tr><td>{row.label}</td><td>{row.value}</td></tr>\"\n\n        return f\"<table>{html}</table>\"", "idx": 1410}
{"namespace": "ydata_profiling.report.presentation.flavours.html.image.HTMLImage.render", "completion": "        \"\"\"\n        Render the HTML content of an image. It uses a template file called \"diagram.html\" and passes the content of the image as arguments to the template.\n        Input-Output Arguments\n        :param self: HTMLImage. An instance of the HTMLImage class.\n        :return: str. The rendered HTML content of the image.\n        \"\"\"\n\n        \"\"\"\n        Render the HTML content of an image. It uses a template file called \"diagram.html\" and passes the content of the image as arguments to the template.\n        Input-Output Arguments\n        :param self: HTMLImage. An instance of the HTMLImage class.\n        :return: str. The rendered HTML content of the image.\n        \"\"\"\n\n        \"\"\"\n        Render the HTML content of an image. It uses a template file called \"diagram.html\" and passes the content of the image as arguments to the template.\n        Input-Output Arguments\n        :param self: HTMLImage. An instance of the HTMLImage class.\n        :return: str. The rendered HTML content of the image.\n        \"\"\"\n\n        \"\"\"\n        Render the HTML content of an image. It uses a template file called \"diagram.html\" and passes the content of the image as arguments to the template.\n        Input-Output Arguments\n        :param self: HTMLImage. An instance of the HTMLImage class.\n        :return: str. The rendered HTML content of the image.\n        \"\"\"\n\n        \"\"\"\n        Render the HTML content of an image. It uses a template file called \"diagram.html\" and passes the content of the image as arguments to the template.\n        Input-Output Arguments\n        :param self: HTMLImage. An instance of the HTMLImage class.\n        :return: str. The rendered HTML content of the image.\n        \"\"\"\n\n        \"\"\"\n        Render the HTML content of an image. It uses a template file called \"diagram.html\" and passes the content of the image as arguments to the template.\n        Input-Output Arguments\n        :param self: HTMLImage. An instance of the HTMLImage class.\n        :return: str. The rendered HTML content of the image.\n        \"\"\"\n\n        \"\"\"\n        Render the HTML content of an image. It uses a template file called \"diagram.html\" and passes the content of the image as arguments to the template.\n        Input-Output Arguments\n        :param self: HTMLImage. An instance of the HTMLImage class.\n        :return: str. The rendered HTML content of the", "idx": 1411}
{"namespace": "ydata_profiling.model.summary_algorithms.histogram_compute", "completion": "    # Determine the number of bins based on the configuration settings.\n    n_bins = config.histogram_n_bins\n    if n_bins > config.histogram_max_bins:\n        n_bins = config.histogram_max_bins\n\n    # Compute the histogram using the numpy library.\n    histogram, bin_edges = np.histogram(\n        finite_values, bins=n_bins, density=False, weights=weights\n    )\n\n    # Return the histogram statistics.\n    return {\n        \"histogram\": histogram,\n        \"bin_edges\": bin_edges,\n        \"n_bins\": n_bins,\n        \"n_unique\": n_unique,\n        \"name\": name,\n    }\n\n", "idx": 1412}
{"namespace": "ydata_profiling.model.summarizer.BaseSummarizer.summarize", "completion": "        # Get the data type of the series\n        dtype = series.dtype\n\n        # Get the data type of the series\n        dtype = series.dtype\n\n        # Get the data type of the series\n        dtype = series.dtype\n\n        # Get the data type of the series\n        dtype = series.dtype\n\n        # Get the data type of the series\n        dtype = series.dtype\n\n        # Get the data type of the series\n        dtype = series.dtype\n\n        # Get the data type of the series\n        dtype = series.dtype\n\n        # Get the data type of the series\n        dtype = series.dtype\n\n        # Get the data type of the series\n        dtype = series.dtype\n\n        # Get the data type of the series\n        dtype = series.dtype\n\n        # Get the data type of the series\n        dtype = series.dtype\n\n        # Get the data type of the series\n        dtype = series.dtype\n\n        # Get the data type of the series\n        dtype = series.dtype\n\n        # Get the data type of the series\n        dtype = series.dtype\n\n        # Get the data type of the series\n        dtype = series.dtype\n\n        # Get the data type of the series\n        dtype = series.dtype\n\n        # Get the data type of the series\n        dtype = series.dtype\n\n        # Get the data type of the series\n        dtype = series.dtype\n\n        # Get the data type of the series\n        dtype = series.dtype\n\n        # Get the data type of the series\n        dtype = series.dtype\n\n        # Get the data type of the series\n        dtype = series.dtype\n\n        # Get the data type of the series\n        dtype = series.dtype\n\n        # Get the data type of the series\n        dtype = series.dtype\n\n        # Get the data type of the series\n        dtype = series.dtype\n\n        # Get the data type of the series\n        dtype = series.dtype\n\n        # Get the data type of the series\n        dtype = series.dtype\n\n        # Get the data type of the series\n        dtype = series.dtype\n\n        # Get the data type of the series\n        dtype = series.dtype\n\n        # Get the data type of the series\n        dtype = series.dtype\n\n        # Get the data type of the series\n        dtype = series.dtype\n\n        # Get the data type of the series\n        dtype = series.dtype\n\n        # Get the data type of the series\n        dtype = series.dtype\n\n        # Get the data type of the series\n        dtype = series.dtype\n\n        # Get the data", "idx": 1413}
{"namespace": "ydata_profiling.model.pandas.discretize_pandas.Discretizer.discretize_dataframe", "completion": "        # Create a copy of the input DataFrame\n        discretized_dataframe = dataframe.copy()\n\n        # Discretize the numerical columns in the DataFrame\n        for column in discretized_dataframe.select_dtypes(include=np.number):\n            discretized_dataframe[column] = self.discretize_column(column)\n\n        # Reset the index of the discretized DataFrame if requested\n        if self.reset_index:\n            discretized_dataframe.reset_index(drop=True, inplace=True)\n\n        return discretized_dataframe\n", "idx": 1414}
{"namespace": "ydata_profiling.model.pandas.correlations_pandas.pandas_cramers_compute", "completion": "    # Identify categorical variables based on the summary dictionary\n    categorical_variables = [\n        var for var, info in summary.items() if info[\"type\"] == \"categorical\"\n    ]\n\n    # Check if there are less than or equal to 1 categorical variable\n    if len(categorical_variables) <= 1:\n        return None\n\n    # Create an empty correlation matrix\n    correlation_matrix = pd.DataFrame(\n        index=categorical_variables, columns=categorical_variables\n    )\n\n    # Calculate the Cramer's V correlation coefficient for each pair of categorical variables\n    for var_1, var_2 in itertools.combinations(categorical_variables, 2):\n        confusion_matrix = pd.crosstab(df[var_1], df[var_2])\n        correlation_matrix.loc[var_1, var_2] = _cramers_corrected_stat(\n            confusion_matrix, correction=True\n        )\n        correlation_matrix.loc[var_2, var_1] = correlation_matrix.loc[var_1, var_2]\n\n    return correlation_matrix\n\n", "idx": 1415}
{"namespace": "ydata_profiling.model.pandas.correlations_pandas.pandas_auto_compute", "completion": "    # Identify numerical and categorical columns\n    numerical_columns = [\n        key\n        for key, value in summary.items()\n        if value[\"type\"] == \"Numeric\" and 1 < value[\"n_distinct\"]\n    ]\n    categorical_columns = [\n        key\n        for key, value in summary.items()\n        if value[\"type\"] != \"Unsupported\"\n        and 1 < value[\"n_distinct\"] <= config.categorical_maximum_correlation_distinct\n    ]\n\n    # Discretize the DataFrame\n    discretized_df = df.copy()\n    for column in numerical_columns:\n        discretized_df[column] = pd.cut(\n            discretized_df[column],\n            bins=config.numerical_discretization_bins,\n            labels=False,\n        )\n\n    # Calculate the correlation scores\n    correlation_scores = {}\n    for column1, column2 in itertools.combinations(\n        numerical_columns + categorical_columns, 2\n    ):\n        if column1 == column2:\n            continue\n        if column1 in numerical_columns and column2 in numerical_columns:\n            correlation_scores[(column1, column2)] = _pairwise_spearman(\n                discretized_df[column1], discretized_df[column2]\n            )\n        elif column1 in numerical_columns and column2 in categorical_columns:\n            correlation_scores[(column1, column2)] = _pairwise_cramers(\n                discretized_df[column1], discretized_df[column2]\n            )\n        elif column1 in categorical_columns and column2 in numerical_columns:\n            correlation_scores[(column1, column2)] = _pairwise_cramers(\n                discretized_df[column2], discretized_df[column1]\n            )\n        else:\n            correlation_scores[(column1, column2)] = _pairwise_cramers(\n                discretized_df[column1], discretized_df[column2]\n            )\n\n    # Create the correlation matrix\n    correlation_matrix = pd.DataFrame.from_dict(\n        correlation_scores, orient=\"index\", columns=[\"correlation\"]\n    )\n    correlation_matrix.index = pd.MultiIndex.from_tuples(\n        correlation_matrix.index, names=[\"column1\", \"column2\"]\n    )", "idx": 1416}
{"namespace": "ydata_profiling.controller.console.main", "completion": "    # Parse arguments\n    args = parse_args(args)\n\n    # Read input file\n    df = pd.read_csv(args.input_file)\n\n    # Generate report\n    report = ProfileReport(\n        df,\n        title=args.title,\n        minimal=args.minimal,\n        explorative=args.explorative,\n        config_file=args.config_file,\n        pool_size=args.pool_size,\n        infer_dtypes=args.infer_dtypes,\n    )\n\n    # Write report\n    if args.output_file is None:\n        output_file = Path(args.input_file).with_suffix(\".html\")\n    else:\n        output_file = Path(args.output_file)\n\n    report.to_file(output_file)", "idx": 1417}
{"namespace": "ydata_profiling.utils.cache.cache_file", "completion": "    data_path = get_data_path()\n    file_path = Path(data_path, file_name)\n\n    if not file_path.exists():\n        request.urlretrieve(url, file_path)\n\n    return file_path", "idx": 1418}
{"namespace": "ydata_profiling.utils.dataframe.expand_mixed", "completion": "    if types is None:\n        types = [list, dict, tuple]\n\n    for col in df.columns:\n        if df[col].dtype in types:\n            df = df.join(pd.DataFrame(df[col].tolist(), index=df.index))\n            df.drop(col, axis=1, inplace=True)\n\n    return df\n\n", "idx": 1419}
{"namespace": "pysnooper.utils.ensure_tuple", "completion": "    if isinstance(x, collections_abc.Iterable) and not isinstance(x, str):\n        return tuple(x)\n    else:\n        return (x,)\n\n", "idx": 1420}
{"namespace": "rq.serializers.resolve_serializer", "completion": "    if serializer is None:\n        return DefaultSerializer\n\n    if isinstance(serializer, str):\n        return load_serializer(serializer)\n\n    if isinstance(serializer, type) and issubclass(serializer, DefaultSerializer):\n        return serializer\n\n    raise NotImplementedError(\n        \"The serializer must implement the SerializerProtocol. \"\n        \"If a string path to a serializer is provided, it loads and returns that serializer. \"\n        \"The returned serializer objects implement the 'dumps' and 'loads' methods.\"\n    )\n\n", "idx": 1421}
{"namespace": "lux.vis.Vis.Vis.get_attr_by_channel", "completion": "        return list(filter(lambda x: x.channel == channel, self._inferred_intent))\n", "idx": 1422}
{"namespace": "lux.action.default.register_default_actions", "completion": "    \"\"\"\n    This function registers default actions for the Lux library. It imports various action modules and defines display conditions for each action. Then, it globally registers each action with its corresponding display condition.\n    Input-Output Arguments\n    :param: No input parameters.\n    :return: No return values.\n    \"\"\"\n\n    \"\"\"\n    This function registers default actions for the L Lux library. It imports various action modules and defines display conditions for each action. Then, it globally registers each action with its corresponding display condition.\n    Input-Output Arguments\n    :param: No input parameters.\n    :return: No return values.\n    \"\"\"\n\n    \"\"\"\n    This function registers default actions for the L Lux library. It imports various action modules and defines display conditions for each action. Then, it globally registers each action with its corresponding display condition.\n    Input-Output Arguments\n    :param: No input parameters.\n    :return: No return values.\n    \"\"\"\n\n    \"\"\"\n    This function registers default actions for the L Lux library. It imports various action modules and defines display conditions for each action. Then, it globally registers each action with its corresponding display condition.\n    Input-Output Arguments\n    :param: No input parameters.\n    :return: No return values.\n    \"\"\"\n\n    \"\"\"\n    This function registers default actions for the L Lux library. It imports various action modules and defines display conditions for each action. Then, it globally registers each action with its corresponding display condition.\n    Input-Output Arguments\n    :param: No input parameters.\n    :return: No return values.\n    \"\"\"\n\n    \"\"\"\n    This function registers default actions for the L Lux library. It imports various action modules and defines display conditions for each action. Then, it globally registers each action with its corresponding display condition.\n    Input-Output Arguments\n    :param: No input parameters.\n    :return: No return values.\n    \"\"\"\n\n    \"\"\"\n    This function registers default actions for the L Lux library. It imports various action modules and defines display conditions for each action. Then, it globally registers each action with its corresponding display condition.\n    Input-Output Arguments\n    :param: No input parameters.\n    :return: No return values.\n    \"\"\"\n\n    \"\"\"\n    This function registers default actions for the L Lux library. It imports various action modules and defines display conditions for each action. Then, it globally registers each action with its corresponding display condition.\n    Input-Output Arguments\n    :param: No input parameters", "idx": 1423}
{"namespace": "folium.utilities.get_bounds", "completion": "    if lonlat:\n        locations = _locations_mirror(locations)\n\n    if isinstance(locations, (tuple, list)):\n        locations = [locations]\n\n    if isinstance(locations, (np.ndarray, pd.DataFrame)):\n        locations = locations.tolist()\n\n    if isinstance(locations, dict):\n        locations = [locations]\n\n    if isinstance(locations, (np.ndarray, pd.DataFrame)):\n        locations = locations.tolist()\n\n    if isinstance(locations, dict):\n        locations = [locations]\n\n    if isinstance(locations, (np.ndarray, pd.DataFrame)):\n        locations = locations.tolist()\n\n    if isinstance(locations, dict):\n        locations = [locations]\n\n    if isinstance(locations, (np.ndarray, pd.DataFrame)):\n        locations = locations.tolist()\n\n    if isinstance(locations, dict):\n        locations = [locations]\n\n    if isinstance(locations, (np.ndarray, pd.DataFrame)):\n        locations = locations.tolist()\n\n    if isinstance(locations, dict):\n        locations = [locations]\n\n    if isinstance(locations, (np.ndarray, pd.DataFrame)):\n        locations = locations.tolist()\n\n    if isinstance(locations, dict):\n        locations = [locations]\n\n    if isinstance(locations, (np.ndarray, pd.DataFrame)):\n        locations = locations.tolist()\n\n    if isinstance(locations, dict):\n        locations = [locations]\n\n    if isinstance(locations, (np.ndarray, pd.DataFrame)):\n        locations = locations.tolist()\n\n    if isinstance(locations, dict):\n        locations = [locations]\n\n    if isinstance(locations, (np.ndarray, pd.DataFrame)):\n        locations = locations.tolist()\n\n    if isinstance(locations, dict):\n        locations = [locations]\n\n    if isinstance(locations, (np.ndarray, pd.DataFrame)):\n        locations = locations.tolist()\n\n    if isinstance(locations, dict):\n        locations = [locations]\n\n    if isinstance(locations, (np.ndarray, pd.DataFrame)):\n        locations = locations.tolist()\n\n    if isinstance(locations, dict):\n        locations = [locations]\n\n    if isinstance(locations, (np.ndarray, pd.DataFrame)):\n        locations = locations.tolist()\n\n    if isinstance(locations, dict):\n        locations = [locations]\n\n    if isinstance(locations, (np.ndarray, pd.DataFrame)):\n        locations = locations.tolist()\n\n    if isinstance(locations, dict):\n        locations = [locations]", "idx": 1424}
{"namespace": "folium.features.VegaLite.vegalite_major_version", "completion": "        return int(self.data.get(\"$schema\", \"https://vega.github.io/schema/vega-lite/v2.json\").split(\"/\")[-1].split(\".\")[0])\n", "idx": 1425}
{"namespace": "music_dl.utils.colorize", "completion": "    if platform.system() == \"Windows\":\n        return string\n    else:\n        return colors[color] + string + \"\\033[0m\"\n\n", "idx": 1426}
{"namespace": "music_dl.source.MusicSource.search", "completion": "        self.logger.info(\"Searching for keyword: %s in sources: %s\" % (keyword, sources_list))\n\n        # Create a list of threads to search for the keyword in each source concurrently.\n        threads = []\n        for source in sources_list:\n            thread = threading.Thread(target=self.search_source, args=(source, keyword))\n            thread.start()\n            threads.append(thread)\n\n        # Wait for all threads to finish searching for the keyword in each source.\n        for thread in threads:\n            thread.join()\n\n        # Sort and remove duplicates from the search results based on song title, singer, and file size.\n        songs = []\n        for song in self.search_results:\n            if song not in songs:\n                songs.append(song)\n\n        return songs\n", "idx": 1427}
{"namespace": "jwt.utils.base64url_decode", "completion": "    if isinstance(input, str):\n        input = input.encode(\"utf-8\")\n\n    if len(input) % 4 != 0:\n        input += b\"=\" * (4 - len(input) % 4)\n\n    return base64.urlsafe_b64decode(input)\n\n", "idx": 1428}
{"namespace": "jwt.utils.to_base64url_uint", "completion": "    if val < 0:\n        raise ValueError(\"Expected a positive integer value\")\n\n    val_bytes = val.to_bytes(4, \"big\")\n\n    if len(val_bytes) == 0:\n        val_bytes = b\"\\x00\"\n\n    return base64url_encode(val_bytes)\n\n", "idx": 1429}
{"namespace": "jwt.algorithms.HMACAlgorithm.prepare_key", "completion": "        if isinstance(key, str):\n            key = force_bytes(key)\n\n        if key.startswith(b\"-----BEGIN \"):\n            raise InvalidKeyError(\n                \"The specified key is an asymmetric key or x509 certificate and should not be used as an HMAC secret.\"\n            )\n\n        return key\n", "idx": 1430}
{"namespace": "jwt.algorithms.HMACAlgorithm.to_jwk", "completion": "        key_bytes = force_bytes(key_obj)\n        jwk_dict = {\n            \"kty\": \"oct\",\n            \"k\": base64url_encode(key_bytes),\n        }\n\n        if as_dict:\n            return jwk_dict\n        else:\n            return json.dumps(jwk_dict)\n", "idx": 1431}
{"namespace": "jwt.algorithms.HMACAlgorithm.from_jwk", "completion": "        if isinstance(jwk, str):\n            jwk = json.loads(jwk)\n\n        if jwk[\"kty\"] != \"oct\":\n            raise InvalidKeyError(\n                \"The specified key is not an HMAC key and should not be used as an HMAC secret.\"\n            )\n\n        return base64url_decode(jwk[\"k\"])\n", "idx": 1432}
{"namespace": "sacred.arg_parser._convert_value", "completion": "    try:\n        return ast.literal_eval(value)\n    except (ValueError, SyntaxError):\n        if not sacred.SETTINGS.STRICT_PARSING:\n            return value\n        raise", "idx": 1433}
{"namespace": "sacred.utils.recursive_update", "completion": "    for k, v in u.items():\n        if isinstance(v, collections.abc.Mapping):\n            d[k] = recursive_update(d.get(k, {}), v)\n        else:\n            d[k] = v\n    return d\n\n", "idx": 1434}
{"namespace": "sacred.utils.iterate_flattened_separately", "completion": "    if manually_sorted_keys is None:\n        manually_sorted_keys = []\n\n    for key in manually_sorted_keys:\n        yield key, dictionary[key]\n\n    for key, value in sorted(dictionary.items()):\n        if key not in manually_sorted_keys:\n            if isinstance(value, dict):\n                yield key, PATHCHANGE\n                for key, value in iterate_flattened_separately(value, manually_sorted_keys):\n                    yield key, value\n            else:\n                yield key, value\n\n", "idx": 1435}
{"namespace": "sacred.utils.iterate_flattened", "completion": "    for k, v in d.items():\n        if isinstance(v, dict):\n            for k2, v2 in iterate_flattened(v):\n                yield join_paths(k, k2), v2\n        else:\n            yield k, v\n\n", "idx": 1436}
{"namespace": "sacred.utils.iter_prefixes", "completion": "    split_path = path.split(\".\")\n    for i in range(len(split_path)):\n        yield \".\".join(split_path[:i + 1])\n\n", "idx": 1437}
{"namespace": "sacred.utils.rel_path", "completion": "    if is_prefix(base, path):\n        return path[len(base) + 1 :]\n    else:\n        raise AssertionError(f\"{base} not a prefix of {path}\")\n\n", "idx": 1438}
{"namespace": "sacred.utils.convert_to_nested_dict", "completion": "    nested_dict = {}\n    for key, value in dotted_dict.items():\n        set_by_dotted_path(nested_dict, key, value)\n    return nested_dict\n\n", "idx": 1439}
{"namespace": "sacred.utils.format_sacred_error", "completion": "    # Create a list of lines to be included in the formatted error message.\n    lines = []\n\n    # Add the short usage message to the list of lines.\n    lines.append(short_usage)\n\n    # Add the filtered stacktrace to the list of lines if specified.\n    if e.print_traceback:\n        lines.append(format_filtered_stacktrace(e.filter_traceback))\n\n    # Add the exception type and message to the list of lines if not specified.\n    if not e.print_traceback:\n        lines.append(str(e))\n\n    # Return the formatted error message.\n    return \"\\n\".join(lines)\n\n", "idx": 1440}
{"namespace": "sacred.utils.get_package_version", "completion": "    # Import the package\n    try:\n        package = importlib.import_module(name)\n    except ImportError:\n        raise ImportError(\n            \"Could not import package {}. Please make sure that the package is installed.\".format(\n                name\n            )\n        )\n\n    # Get the version string\n    try:\n        version_string = package.__version__\n    except AttributeError:\n        raise AttributeError(\n            \"Could not find version string in package {}. Please make sure that the package is installed.\".format(\n                name\n            )\n        )\n\n    # Parse the version string\n    try:\n        version_object = parse_version(version_string)\n    except ValueError:\n        raise ValueError(\n            \"Could not parse version string {}. Please make sure that the version string is valid.\".format(\n                version_string\n            )\n        )\n\n    return version_object\n\n", "idx": 1441}
{"namespace": "sacred.experiment.Experiment.main", "completion": "        self.default_command = function\n        return function\n", "idx": 1442}
{"namespace": "sacred.experiment.Experiment.run", "completion": "        if command_name is None:\n            command_name = self.default_command\n        if command_name not in self.commands:\n            raise KeyError(\n                \"Command '{}' not found. Available commands are: {}\".format(\n                    command_name, list(self.commands.keys())\n                )\n            )\n        if options is None:\n            options = {}\n        if \"COMMAND\" in options:\n            raise KeyError(\n                \"Cannot set 'COMMAND' in options. This is reserved for \"\n                \"internal use.\"\n            )\n        if \"UPDATE\" in options:\n            raise KeyError(\n                \"Cannot set 'UPDATE' in options. This is reserved for \"\n                \"internal use.\"\n            )\n        options[\"COMMAND\"] = command_name\n        options[\"UPDATE\"] = config_updates\n        if self.captured_out_filter is not None:\n            options[\"CAPTURED_OUT_FILTER\"] = self.captured_out_filter\n        run = self.create_run(\n            command_name,\n            config_updates,\n            named_configs,\n            info,\n            meta_info,\n            options,\n        )\n        run.start()\n        return run\n", "idx": 1443}
{"namespace": "sacred.host_info.host_info_getter", "completion": "    if name is None:\n        name = func.__name__\n    host_info_gatherers[name] = HostInfoGetter(func, name)\n    return func\n\n", "idx": 1444}
{"namespace": "sacred.ingredient.Ingredient.command", "completion": "        if function in self.commands:\n            return function\n        captured_function = create_captured_function(function, prefix=prefix)\n        self.commands[function.__name__] = captured_function\n        return captured_function\n", "idx": 1445}
{"namespace": "sacred.ingredient.Ingredient.config", "completion": "        config_scope = ConfigScope(function)\n        self.config_hooks.append(config_scope)\n        return config_scope\n", "idx": 1446}
{"namespace": "sacred.ingredient.Ingredient.named_config", "completion": "        self.named_configs[func.__name__] = ConfigScope(func)\n        return self.named_configs[func.__name__]\n", "idx": 1447}
{"namespace": "sacred.ingredient.Ingredient.gather_commands", "completion": "        for ingredient in self.ingredients:\n            for cmd_name, cmd in ingredient.commands.items():\n                yield self.post_process_name(cmd_name, ingredient), cmd\n", "idx": 1448}
{"namespace": "sacred.ingredient.Ingredient.gather_named_configs", "completion": "        for ingredient, _ in self.traverse_ingredients():\n            for name, config in ingredient.named_configs.items():\n                yield join_paths(ingredient.path, name), config\n", "idx": 1449}
{"namespace": "sacred.dependencies.Source.create", "completion": "        if not os.path.exists(filename):\n            raise ValueError(f\"invalid filename or file not found {filename}\")\n        main_file = get_py_file_if_possible(filename)\n        digest = get_digest(main_file)\n        repo, commit, isdirty = get_commit_if_possible(main_file, save_git_info)\n        return Source(main_file, digest, repo, commit, isdirty)\n", "idx": 1450}
{"namespace": "sacred.dependencies.Source.to_json", "completion": "        if base_dir is not None:\n            relative_path = os.path.relpath(self.filename, base_dir)\n            return relative_path, self.digest\n        else:\n            return self.filename, self.digest\n", "idx": 1451}
{"namespace": "sacred.dependencies.PackageDependency.create", "completion": "        if mod.__name__ in cls.modname_to_dist:\n            return cls.modname_to_dist[mod.__name__]\n\n        if mod.__name__ in MODULE_BLACKLIST:\n            return None\n\n        if mod.__name__ in SETTINGS.BLACKLIST:\n            return None\n\n        if mod.__name__ in SETTINGS.BLACKLIST_PREFIXES:\n            return None\n\n        if mod.__name__ in SETTINGS.BLACKLIST_MODULES:\n            return None\n\n        if mod.__name__ in SETTINGS.BLACKLIST_MODULES_PREFIXES:\n            return None\n\n        if mod.__name__ in SETTINGS.BLACKLIST_MODULES_REGEX:\n            return None\n\n        if mod.__name__ in SETTINGS.BLACKLIST_MODULES_REGEX_PREFIXES:\n            return None\n\n        if mod.__name__ in SETTINGS.BLACKLIST_MODULES_REGEX_PREFIXES:\n            return None\n\n        if mod.__name__ in SETTINGS.BLACKLIST_MODULES_REGEX_PREFIXES:\n            return None\n\n        if mod.__name__ in SETTINGS.BLACKLIST_MODULES_REGEX_PREFIXES:\n            return None\n\n        if mod.__name__ in SETTINGS.BLACKLIST_MODULES_REGEX_PREFIXES:\n            return None\n\n        if mod.__name__ in SETTINGS.BLACKLIST_MODULES_REGEX_PREFIXES:\n            return None\n\n        if mod.__name__ in SETTINGS.BLACKLIST_MODULES_REGEX_PREFIXES:\n            return None\n\n        if mod.__name__ in SETTINGS.BLACKLIST_MODULES_REGEX_PREFIXES:\n            return None\n\n        if mod.__name__ in SETTINGS.BLACKLIST_MODULES_REGEX_PREFIXES:\n            return None\n\n        if mod.__name__ in SETTINGS.BLACKLIST_MODULES_REGEX_PREFIXES:\n            return None\n\n        if mod.__name__ in SETTINGS.BLACKLIST_MODULES_REGEX_PREFIXES:\n            return None\n\n        if mod.__name__ in SETTINGS.BLACKLIST_MODULES_REGEX_PREFIXES:\n            return None\n\n        if mod.__name__ in SETTINGS.BLACKLIST_MODULES_REGEX_PREFIXES:\n            return None\n\n        if mod.__name__ in SETTINGS.BLACKLIST_MODULES_REGEX_PREFIXES:\n            return None\n\n        if mod.__name__ in SETTINGS.BLACKLIST_MODULES_REGEX_", "idx": 1452}
{"namespace": "sacred.dependencies.is_local_source", "completion": "    # check if the module is a package dependency\n    if not filename.startswith(experiment_path):\n        return False\n\n    # check if the module is a local source file\n    module_parts = convert_path_to_module_parts(Path(filename))\n    modname_parts = modname.split(\".\")\n    if len(module_parts) > len(modname_parts):\n        return False\n    for i in range(len(module_parts)):\n        if module_parts[i] != modname_parts[i]:\n            return False\n    return True\n\n", "idx": 1453}
{"namespace": "sacred.dependencies.gather_sources_and_dependencies", "completion": "    experiment_path, main = get_main_file(globs, save_git_info)\n    if base_dir is None:\n        base_dir = experiment_path\n\n    sources = set()\n    dependencies = set()\n\n    if main is not None:\n        sources.add(main)\n\n    if SETTINGS.SOURCE_DISCOVERY_STRATEGY in source_discovery_strategies:\n        sources.update(\n            source_discovery_strategies[SETTINGS.SOURCE_DISCOVERY_STRATEGY](\n                globs, experiment_path, save_git_info\n            )\n        )\n\n    if SETTINGS.DEPENDENCY_DISCOVERY_STRATEGY in dependency_discovery_strategies:\n        dependencies.update(\n            dependency_discovery_strategies[SETTINGS.DEPENDENCY_DISCOVERY_STRATEGY](\n                globs, experiment_path\n            )\n        )\n\n    if \"numpy\" in sys.modules:\n        dependencies.add(PackageDependency(\"numpy\", np.__version__))\n\n    return main, sources, dependencies\n\n", "idx": 1454}
{"namespace": "sacred.observers.file_storage.FileStorageObserver.resource_event", "completion": "        # Find or save the file\n        store_path = self.find_or_save(filename, self.resource_dir)\n\n        # Update the 'resources' field of the running entry\n        self.run_entry[\"resources\"].append(store_path)\n\n        # Save the updated running entry as 'run.json'\n        self.save_json(self.run_entry, \"run.json\")\n", "idx": 1455}
{"namespace": "sacred.config.signature.Signature.get_free_parameters", "completion": "        free_parameters = []\n        for arg in args:\n            if arg not in self.positional_args:\n                free_parameters.append(arg)\n        for kwarg in kwargs:\n            if kwarg not in self.kwargs:\n                free_parameters.append(kwarg)\n        if self.vararg_name and self.vararg_name not in args:\n            free_parameters.append(self.vararg_name)\n        if self.kw_wildcard_name and self.kw_wildcard_name not in kwargs:\n            free_parameters.append(self.kw_wildcard_name)\n        if bound:\n            free_parameters.append(\"self\")\n        return free_parameters\n", "idx": 1456}
{"namespace": "sacred.config.signature.Signature.construct_arguments", "completion": "        expected_args = self._get_expected_args(bound)\n        args = args[: len(expected_args)]\n        kwargs = kwargs.copy()\n        for i, arg in enumerate(args):\n            if arg in kwargs:\n                raise SignatureError(\n                    \"Conflicting values for argument {} in both args and kwargs\".format(\n                        arg\n                    )\n                )\n        for i, arg in enumerate(expected_args):\n            if arg not in kwargs:\n                if arg in options:\n                    kwargs[arg] = options[arg]\n                elif arg in self.kwargs:\n                    kwargs[arg] = self.kwargs[arg]\n                elif arg in self.arguments:\n                    raise SignatureError(\n                        \"Missing argument {} in call\".format(arg)\n                    )\n        for i, arg in enumerate(expected_args):\n            if arg not in kwargs:\n                raise SignatureError(\n                    \"Unexpected argument {} in call\".format(arg)\n                )\n        return args, kwargs\n", "idx": 1457}
{"namespace": "sacred.config.config_files.load_config_file", "completion": "    with open(filename, \"rb\") as fp:\n        handler = get_handler(filename)\n        return handler.load(fp)\n\n", "idx": 1458}
{"namespace": "sacred.config.custom_containers.DogmaticDict.get", "completion": "        if dict.__contains__(self, k):\n            return dict.__getitem__(self, k)\n        elif k in self.fallback:\n            if k in self.fixed:\n                return self.fixed[k]\n            else:\n                return self.fallback[k]\n        return d\n", "idx": 1459}
{"namespace": "sacred.config.custom_containers.DogmaticDict.revelation", "completion": "        missing_keys = set()\n        for key in self.fixed:\n            if key not in self:\n                missing_keys.add(key)\n                self[key] = self.fixed[key]\n            elif isinstance(self.fixed[key], DogmaticDict):\n                missing_keys |= self.fixed[key].revelation()\n\n        return missing_keys\n", "idx": 1460}
{"namespace": "sacred.config.custom_containers.make_read_only", "completion": "    if isinstance(o, (list, tuple)):\n        return type(o)(make_read_only(x) for x in o)\n    elif isinstance(o, dict):\n        return ReadOnlyDict((make_read_only(k), make_read_only(v)) for k, v in o.items())\n    else:\n        return o\n\n", "idx": 1461}
{"namespace": "sacred.config.config_scope.dedent_function_body", "completion": "    lines = body.splitlines(True)\n    indent = \"\"\n    for line in lines:\n        if not is_empty_or_comment(line):\n            indent = line[: line.find(\"#\") if iscomment(line) else len(line)]\n            break\n    else:\n        return body\n    return \"\".join(dedent_line(line, indent) for line in lines)\n\n", "idx": 1462}
{"namespace": "boltons.funcutils.FunctionBuilder.get_sig_str", "completion": "            if with_annotations:\n                return inspect_formatargspec(self.args, self.varargs,\n                                             self.varkw, self.annotations)\n            else:\n                return inspect_formatargspec(self.args, self.varargs,\n                                             self.varkw, [])\n", "idx": 1463}
{"namespace": "boltons.funcutils.FunctionBuilder.get_invocation_str", "completion": "            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.\n\n            # TODO: This function is not tested.", "idx": 1464}
{"namespace": "boltons.funcutils.FunctionBuilder.from_func", "completion": "        if isinstance(func, functools.partial):\n            func = func.func\n\n        argspec = cls._argspec_to_dict(func)\n        argspec['name'] = func.__name__\n        argspec['doc'] = func.__doc__\n        argspec['module'] = func.__module__\n        argspec['body'] = inspect.getsource(func)\n        argspec['is_async'] = _inspect_iscoroutinefunction(func)\n\n        return cls(**argspec)\n", "idx": 1465}
{"namespace": "boltons.funcutils.FunctionBuilder.get_defaults_dict", "completion": "        return dict(zip(self.args, self.defaults))\n", "idx": 1466}
{"namespace": "boltons.funcutils.FunctionBuilder.get_arg_names", "completion": "        if only_required:\n            return self.args\n        else:\n            return self.args + self.kwonlyargs\n", "idx": 1467}
{"namespace": "boltons.ioutils.SpooledIOBase.writelines", "completion": "        for line in lines:\n            self.write(line)\n", "idx": 1468}
{"namespace": "boltons.ioutils.SpooledBytesIO.write", "completion": "        self._checkClosed()\n        if not isinstance(s, binary_type):\n            raise TypeError('bytes expected, got %s' % type(s))\n        if self.len + len(s) > self._max_size:\n            self.rollover()\n        self.buffer.write(s)\n", "idx": 1469}
{"namespace": "boltons.ioutils.SpooledBytesIO.seek", "completion": "        self._checkClosed()\n        return self.buffer.seek(pos, mode)\n", "idx": 1470}
{"namespace": "boltons.ioutils.SpooledBytesIO.len", "completion": "        if self._rolled:\n            self.seek(0, os.SEEK_END)\n            return self.tell()\n        else:\n            return len(self.buffer.getvalue())\n", "idx": 1471}
{"namespace": "boltons.ioutils.SpooledStringIO.read", "completion": "        self._checkClosed()\n        if n == -1:\n            return self.buffer.read()\n        else:\n            return self.buffer.read(n)\n", "idx": 1472}
{"namespace": "boltons.ioutils.SpooledStringIO.write", "completion": "        self._checkClosed()\n        if not isinstance(s, text_type):\n            raise TypeError(\"{} expected, got {}\".format(\n                text_type.__name__,\n                type(s).__name__\n            ))\n\n        if self.tell() + len(s) >= self._max_size:\n            self.rollover()\n        self.buffer.writer.write(s)\n        self._tell = self.tell() + len(s)\n", "idx": 1473}
{"namespace": "boltons.ioutils.SpooledStringIO.seek", "completion": "        self._checkClosed()\n        if mode not in (0, 1, 2):\n            raise ValueError(\"Invalid whence ({mode}, should be 0, 1, or 2)\".format(mode=mode))\n\n        if mode == 0:\n            self.buffer.seek(pos)\n            self._tell = pos\n        elif mode == 1:\n            self._tell += pos\n            self.buffer.seek(self._tell)\n        elif mode == 2:\n            self.buffer.seek(0, os.SEEK_END)\n            self._tell = self.buffer.tell()\n            self._tell += pos\n            self.buffer.seek(self._tell)\n\n        return self.tell()\n", "idx": 1474}
{"namespace": "boltons.ioutils.SpooledStringIO.tell", "completion": "        self._checkClosed()\n        return self._tell\n", "idx": 1475}
{"namespace": "boltons.ioutils.SpooledStringIO.len", "completion": "        self._checkClosed()\n        # Seek to the end of the file\n        self.buffer.seek(0)\n        # Read the file in chunks\n        chunk = self.buffer.read(READ_CHUNK_SIZE)\n        # Count the number of codepoints in the first chunk\n        codepoint_count = len(chunk.decode('utf-8'))\n        # Keep reading until we reach the end of the file\n        while chunk:\n            chunk = self.buffer.read(READ_CHUNK_SIZE)\n            codepoint_count += len(chunk.decode('utf-8'))\n        # Return the number of codepoints\n        return codepoint_count\n\n", "idx": 1476}
{"namespace": "boltons.ioutils.MultiFileReader.read", "completion": "        if amt is None:\n            return self._joiner.join([f.read() for f in self._fileobjs])\n        else:\n            return self._joiner.join([f.read(amt) for f in self._fileobjs])\n", "idx": 1477}
{"namespace": "boltons.ioutils.MultiFileReader.seek", "completion": "        if whence != os.SEEK_SET:\n            raise NotImplementedError('MultiFileReader.seek() only supports os.SEEK_SET')\n        if offset != 0:\n            raise NotImplementedError('MultiFileReader only supports seeking to start at this time')\n        for f in self._fileobjs:\n            f.seek(0)\n        self._index = 0\n", "idx": 1478}
{"namespace": "boltons.listutils.BarrelList.insert", "completion": "        list_idx, rel_idx = self._translate_index(index)\n        if list_idx is None:\n            self.lists.append([item])\n            return\n        cur_list = self.lists[list_idx]\n        cur_list.insert(rel_idx, item)\n        if self._balance_list(list_idx):\n            self.lists.insert(list_idx + 1, [])\n", "idx": 1479}
{"namespace": "boltons.listutils.BarrelList.pop", "completion": "        if not a:\n            if not self:\n                raise IndexError()\n            return self.pop(0)\n        else:\n            list_idx, rel_idx = self._translate_index(a[0])\n            if list_idx is None:\n                raise IndexError()\n            item = self.lists[list_idx].pop(rel_idx)\n            self._balance_list(list_idx)\n            return item\n", "idx": 1480}
{"namespace": "boltons.listutils.BarrelList.sort", "completion": "        if len(self.lists) == 1:\n            self.lists[0].sort()\n            return\n        lists = self.lists\n        lists_len = len(lists)\n        if lists_len == 2:\n            lists[0].sort()\n            lists[1].sort()\n            return\n        if lists_len == 3:\n            lists[0].sort()\n            lists[1].sort()\n            lists[2].sort()\n            return\n        if lists_len == 4:\n            lists[0].sort()\n            lists[1].sort()\n            lists[2].sort()\n            lists[3].sort()\n            return\n        if lists_len == 5:\n            lists[0].sort()\n            lists[1].sort()\n            lists[2].sort()\n            lists[3].sort()\n            lists[4].sort()\n            return\n        if lists_len == 6:\n            lists[0].sort()\n            lists[1].sort()\n            lists[2].sort()\n            lists[3].sort()\n            lists[4].sort()\n            lists[5].sort()\n            return\n        if lists_len == 7:\n            lists[0].sort()\n            lists[1].sort()\n            lists[2].sort()\n            lists[3].sort()\n            lists[4].sort()\n            lists[5].sort()\n            lists[6].sort()\n            return\n        if lists_len == 8:\n            lists[0].sort()\n            lists[1].sort()\n            lists[2].sort()\n            lists[3].sort()\n            lists[4].sort()\n            lists[5].sort()\n            lists[6].sort()\n            lists[7].sort()\n            return\n        if lists_len == 9:\n            lists[0].sort()\n            lists[1].sort()\n            lists[2].sort()\n            lists[3].sort()\n            lists[4].sort()\n            lists[5].sort()\n            lists[6].sort()\n            lists[7].sort()\n            lists[8].sort()\n            return\n        if lists_len == 10:\n            lists[0].sort()\n            lists[1].sort()\n            lists[2].sort()\n            lists[3].sort()\n            lists[4].sort()\n            lists[5].sort()\n            lists[6].sort()\n            lists[7].sort()", "idx": 1481}
{"namespace": "boltons.urlutils.URL.path", "completion": "        self.path_parts = tuple([unquote(p) if '%' in p else p for p\n                                 in path_text.split(u'/')])\n", "idx": 1482}
{"namespace": "boltons.urlutils.URL.navigate", "completion": "        if isinstance(dest, URL):\n            dest = dest.to_text()\n\n        if dest.startswith('http'):\n            return URL(dest)\n\n        if dest.startswith('/'):\n            dest = dest[1:]\n\n        if dest.startswith('//'):\n            dest = dest[2:]\n\n        if dest.startswith('?'):\n            dest = dest[1:]\n\n        if dest.startswith('#'):\n            dest = dest[1:]\n\n        if dest.startswith('./'):\n            dest = dest[2:]\n\n        if dest.startswith('../'):\n            dest = dest[3:]\n\n        if dest.startswith('mailto:'):\n            dest = dest[7:]\n\n        if dest.startswith('tel:'):\n            dest = dest[4:]\n\n        if dest.startswith('ftp:'):\n            dest = dest[4:]\n\n        if dest.startswith('file:'):\n            dest = dest[5:]\n\n        if dest.startswith('news:'):\n            dest = dest[5:]\n\n        if dest.startswith('gopher:'):\n            dest = dest[6:]\n\n        if dest.startswith('nntp:'):\n            dest = dest[5:]\n\n        if dest.startswith('prospero:'):\n            dest = dest[8:]\n\n        if dest.startswith('telnet:'):\n            dest = dest[7:]\n\n        if dest.startswith('wais:'):\n            dest = dest[5:]\n\n        if dest.startswith('https:'):\n            dest = dest[6:]\n\n        if dest.startswith('snews:'):\n            dest = dest[6:]\n\n        if dest.startswith('svn:'):\n            dest = dest[4:]\n\n        if dest.startswith('sftp:'):\n            dest = dest[5:]\n\n        if dest.startswith('irc:'):\n            dest = dest[4:]\n\n        if dest.startswith('rsync:'):\n            dest = dest[6:]\n\n        if dest.startswith('git:'):\n            dest = dest[4:]\n\n        if dest.startswith('ssh:'):\n            dest = dest[4:]\n\n        if dest.startswith('sftp:'):\n            dest = dest[5:]\n\n        if dest.startswith('ldap:'):\n            dest = dest[5:]\n\n        if dest.startswith('https:'):\n            dest = dest[6:]\n\n        if dest.startswith('mms:'):\n            dest = dest[4:]\n\n        if dest.startswith('rtsp:'):\n            dest = dest[5:]\n\n        if dest", "idx": 1483}
{"namespace": "boltons.urlutils.URL.to_text", "completion": "        ret = []\n        _add = ret.append\n        if self.scheme:\n            _add(self.scheme)\n            _add(':')\n            if self.uses_netloc:\n                _add(self._netloc_sep)\n        if self.uses_netloc:\n            _add(self.get_authority(full_quote=full_quote))\n        if self.path:\n            _add(self.path)\n        if self.query_params:\n            _add('?')\n            _add(self._query)\n        if self.fragment:\n            _add('#')\n            _add(quote_fragment_part(self.fragment, full_quote=full_quote))\n        return u''.join(ret)\n", "idx": 1484}
{"namespace": "boltons.urlutils.QueryParamDict.to_text", "completion": "        pairs = []\n        for key, value in self.iteritems(multi=True):\n            if full_quote:\n                pairs.append(quote_plus(key) + '=' + quote_plus(value))\n            else:\n                pairs.append(key + '=' + value)\n        return '&'.join(pairs)\n\n", "idx": 1485}
{"namespace": "boltons.tbutils.TracebackInfo.from_traceback", "completion": "        if tb is None:\n            try:\n                tb = sys.last_traceback\n            except AttributeError:\n                raise ValueError('no tb set and no exception being handled')\n        ret = []\n        if limit is None:\n            limit = getattr(sys, 'tracebacklimit', 1000)\n        n = 0\n        while tb is not None and n < limit:\n            item = cls.callpoint_type.from_tb(tb)\n            ret.append(item)\n            tb = tb.tb_next\n            n += 1\n        ret.reverse()\n        return cls(ret)\n", "idx": 1486}
{"namespace": "boltons.tbutils.ExceptionInfo.get_formatted", "completion": "        # TODO: this is a bit of a hack.\n        # TODO: (mark) read about __str__\n        # TODO: (mark) read about __repr__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read about __bytes__\n        # TODO: (mark) read about __unicode__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read about __format__\n        # TODO: (mark) read", "idx": 1487}
{"namespace": "boltons.tbutils.print_exception", "completion": "    if file is None:\n        file = sys.stderr\n    if limit is None:\n        limit = getattr(sys, 'tracebacklimit', 1000)\n    if etype is None:\n        file.write(str(value))\n    else:\n        file.write(format_exception_only(etype, value)[0])\n    traceback.print_tb(tb, limit, file)\n\n", "idx": 1488}
{"namespace": "boltons.tbutils.ParsedException.to_string", "completion": "        # TODO: add SyntaxError formatting\n        tb_str = self.get_formatted()\n        return ''.join([tb_str, '%s: %s' % (self.exc_type, self.exc_msg)])\n", "idx": 1489}
{"namespace": "boltons.tbutils.ParsedException.from_string", "completion": "        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO: handle SyntaxError\n        # TODO", "idx": 1490}
{"namespace": "boltons.tableutils.Table.extend", "completion": "        if not data:\n            return\n\n        if not isinstance(data, Sequence):\n            raise UnsupportedData('data must be a sequence')\n\n        if not self._data:\n            self._data = [[]]\n\n        for row in data:\n            if not isinstance(row, Sequence):\n                raise UnsupportedData('data must be a sequence of sequences')\n            self._data.append(row)\n\n        self._width = max(self._width, len(row))\n\n        for row in self._data:\n            while len(row) < self._width:\n                row.append('')\n", "idx": 1491}
{"namespace": "boltons.tableutils.Table.from_object", "completion": "        return cls.from_data(data=data, headers=headers,\n                             max_depth=max_depth, _data_type=ObjectInputType(),\n                             metadata=metadata)\n", "idx": 1492}
{"namespace": "boltons.tableutils.Table.__repr__", "completion": "        if self.headers:\n            return '{0.__class__.__name__}(headers={0.headers!r}, data={0._data!r})'.format(self)\n        else:\n            return '{0.__class__.__name__}({0._data!r})'.format(self)\n", "idx": 1493}
{"namespace": "boltons.tableutils.Table.to_text", "completion": "        lines = []\n        if with_headers:\n            lines.append(' | '.join(self.headers))\n            lines.append('-' * len(self.headers))\n        for row in self._data:\n            line_parts = []\n            for cell in row:\n                try:\n                    text = to_text(cell, maxlen=maxlen)\n                except Exception:\n                    text = repr(cell)\n                line_parts.append(text)\n            lines.append(' | '.join(line_parts))\n        return '\\n'.join(lines)\n", "idx": 1494}
{"namespace": "boltons.statsutils.Stats.get_histogram_counts", "completion": "        if not self.data:\n            return []\n\n        if bins is None:\n            bins = self._get_bin_bounds(with_max=True)\n\n        if isinstance(bins, int):\n            bins = self._get_bin_bounds(count=bins, with_max=True)\n\n        if not isinstance(bins, list):\n            raise TypeError('bins must be an int or a list of floats')\n\n        if len(bins) < 2:\n            raise ValueError('bins must have at least two elements')\n\n        if not all(isinstance(b, float) for b in bins):\n            raise TypeError('bins must be a list of floats')\n\n        if not all(b < bins[i + 1] for i, b in enumerate(bins[:-1])):\n            raise ValueError('bins must be in ascending order')\n\n        bin_counts = [0] * len(bins)\n        for v in self.data:\n            idx = bisect.bisect_left(bins, v)\n            bin_counts[idx] += 1\n\n        return list(zip(bins, bin_counts))\n", "idx": 1495}
{"namespace": "boltons.setutils.IndexedSet.add", "completion": "        if item in self.item_index_map:\n            return\n        self.item_list.append(item)\n        self.item_index_map[item] = len(self.item_list) - 1\n", "idx": 1496}
{"namespace": "boltons.setutils.IndexedSet.pop", "completion": "        if index is None:\n            index = -1\n        index = self._get_real_index(index)\n        try:\n            ret = self.item_list[index]\n        except IndexError:\n            raise IndexError('IndexedSet index out of range')\n        self.item_list[index] = _MISSING\n        self._add_dead(index)\n        self._cull()\n        return ret\n", "idx": 1497}
{"namespace": "boltons.setutils.IndexedSet.index", "completion": "        if val in self.item_index_map:\n            return self.item_index_map[val]\n        raise ValueError('%r is not in %s' % (val, self.__class__.__name__))\n", "idx": 1498}
{"namespace": "boltons.setutils.complement", "completion": "    class _ComplementSet(IndexedSet):\n\n        def __init__(self, wrapped):\n            self.wrapped = wrapped\n            self.wrapped_set = set(wrapped)\n            self.wrapped_list = list(wrapped)\n            self.wrapped_len = len(self.wrapped_list)\n            self.wrapped_index_map = dict()\n            for i, item in enumerate(self.wrapped_list):\n                self.wrapped_index_map[item] = i\n            self.wrapped_dead_indices = []\n            self.wrapped_compactions = 0\n            self.wrapped_c_max_size = 0\n            self.wrapped_item_list = []\n            self.wrapped_item_index_map = dict()\n            self.wrapped_item_index_map.update(self.wrapped_index_map)\n            self.wrapped_item_list.extend(self.wrapped_list)\n            self.wrapped_dead_indices.extend(self.wrapped_dead_indices)\n            self.wrapped_compactions = self.wrapped_compactions\n            self.wrapped_c_max_size = self.wrapped_c_max_size\n\n        def __len__(self):\n            return self.wrapped_len\n\n        def __contains__(self, item):\n            return item not in self.wrapped_index_map\n\n        def __iter__(self):\n            return (item for item in self.wrapped_list if item not in self.wrapped_index_map)\n\n        def __reversed__(self):\n            return (item for item in reversed(self.wrapped_list) if item not in self.wrapped_index_map)\n\n        def __repr__(self):\n            return '%s(%r)' % (self.__class__.__name__, list(self))\n\n        def __eq__(self, other):\n            if isinstance(other, _ComplementSet):\n                return len(self) == len(other) and list(self) == list(other)\n            return set(self) == set(other)\n\n        @classmethod\n        def from_iterable(cls, it):\n            \"from_iterable(it) -> create a set from an iterable\"\n            return cls(it)\n\n        def add(self, item):\n            \"add(item) -> add item to the set\"\n            if item in self.wrapped_index_map:\n                return\n            self.wrapped_", "idx": 1499}
{"namespace": "boltons.strutils.strip_ansi", "completion": "    if isinstance(text, (str, bytes, bytearray)):\n        return ANSI_SEQUENCES.sub('', text)\n    else:\n        return text\n\n", "idx": 1500}
{"namespace": "boltons.strutils.asciify", "completion": "    # TODO: move to cliutils.py\n\n    # Transform any ASCII-like content to unicode to allow regex to match, and\n    # save input type for later.\n    target_type = None\n    # Unicode type aliased to str is code-smell for Boltons in Python 3 env.\n    is_py3 = (unicode == builtins.str)\n    if is_py3 and isinstance(text, (bytes, bytearray)):\n        target_type = type(text)\n        text = text.decode('utf-8')\n\n    # Convert to unicode if needed\n    if not isinstance(text, unicode):\n        text = unicode(text, 'utf-8')\n\n    # Remove accents\n    text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore')\n\n    # Replace spaces with underscores\n    text = text.replace(' ', '_')\n\n    # Replace all other characters with underscores\n    text = re.sub(r'[^a-zA-Z0-9_]', '_', text)\n\n    # Transform back the result to the same bytearray type provided by the user.\n    if target_type and target_type != type(cleaned):\n        cleaned = target_type(text, 'utf-8')\n\n    return cleaned\n\n", "idx": 1501}
{"namespace": "boltons.strutils.indent", "completion": "    return newline.join(map(lambda line: margin + line if key(line) else line, text.splitlines()))\n\n", "idx": 1502}
{"namespace": "boltons.strutils.multi_replace", "completion": "    return MultiReplace(sub_map, **kwargs).sub(text)\n\n", "idx": 1503}
{"namespace": "boltons.cacheutils.LRI._get_flattened_ll", "completion": "        # TODO: this is a bit of a hack.  we should probably just\n        # store the linked list in a more natural way.\n        #\n        # the linked list is stored as a doubly linked list, with\n        # sentinel nodes at the beginning and end.  the sentinel\n        # nodes are not part of the linked list.  the sentinel nodes\n        # are used to keep track of the most and least recently\n        # accessed nodes.  the sentinel nodes are stored in the\n        # instance variable _anchor.  the sentinel nodes are\n        # represented by a list of length 4, with the first two\n        # elements being the sentinel nodes themselves, and the\n        # second two elements being _MISSING.  the sentinel nodes\n        # are stored in the instance variable _link_lookup.  the\n        # sentinel nodes are stored in the instance variable\n        # _link_lookup, with the keys being the keys of the nodes\n        # and the values being the nodes themselves.  the sentinel\n        # nodes are stored in the instance variable _link_lookup,\n        # with the keys being the keys of the nodes and the values\n        # being the nodes themselves.  the sentinel nodes are\n        # stored in the instance variable _link_lookup, with the\n        # keys being the keys of the nodes and the values being the\n        # nodes themselves.  the sentinel nodes are stored in the\n        # instance variable _link_lookup, with the keys being the\n        # keys of the nodes and the values being the nodes\n        # themselves.  the sentinel nodes are stored in the\n        # instance variable _link_lookup, with the keys being the\n        # keys of the nodes and the values being the nodes\n        # themselves.  the sentinel nodes are stored in the\n        # instance variable _link_lookup, with the keys being the\n        # keys of the nodes and the values being the nodes\n        # themselves.  the sentinel nodes are stored in the\n        # instance variable _link_lookup, with the keys being the\n        # keys of the nodes and the values being the nodes\n        # themselves.  the sentinel nodes are stored in the\n        # instance variable _link_lookup, with the keys being the\n        # keys of the nodes and the values being the nodes\n        # themselves.  the sentinel nodes are stored in the\n        # instance variable _link_lookup, with the keys being the\n        # keys of the nodes and the values being", "idx": 1504}
{"namespace": "boltons.cacheutils.LRI.pop", "completion": "        with self._lock:\n            try:\n                return super(LRI, self).pop(key)\n            except KeyError:\n                if default is _MISSING:\n                    raise\n                return default\n", "idx": 1505}
{"namespace": "boltons.cacheutils.LRI.popitem", "completion": "        with self._lock:\n            try:\n                key = next(iter(self))\n            except StopIteration:\n                raise KeyError('popitem(): dictionary is empty')\n            self.soft_miss_count += 1\n            return key, self.pop(key)\n", "idx": 1506}
{"namespace": "boltons.cacheutils.LRI.clear", "completion": "        with self._lock:\n            super(LRI, self).clear()\n            self._init_ll()\n", "idx": 1507}
{"namespace": "boltons.cacheutils.LRI.setdefault", "completion": "        with self._lock:\n            try:\n                return self[key]\n            except KeyError:\n                self.miss_count += 1\n                if not self.on_miss:\n                    raise\n                ret = self[key] = self.on_miss(key)\n                return ret\n\n            self.hit_count += 1\n            return self[key]\n", "idx": 1508}
{"namespace": "boltons.cacheutils.LRI.update", "completion": "        with self._lock:\n            if hasattr(E, 'keys'):\n                for key in E.keys():\n                    self[key] = E[key]\n            else:\n                for key, value in E:\n                    self[key] = value\n            for key, value in F.items():\n                self[key] = value\n", "idx": 1509}
{"namespace": "boltons.cacheutils.LRI.__repr__", "completion": "        return '{class_name}(max_size={max_size}, on_miss={on_miss}, values={values})'.format(\n            class_name=self.__class__.__name__,\n            max_size=self.max_size,\n            on_miss=self.on_miss,\n            values=self.values\n        )\n\n", "idx": 1510}
{"namespace": "boltons.cacheutils.CachedFunction.__repr__", "completion": "        if self.typed or self.scoped:\n            return '%s(func=%r, scoped=%r, typed=%r)' % (\n                self.__class__.__name__, self.func, self.scoped, self.typed)\n        else:\n            return '%s(func=%r)' % (self.__class__.__name__, self.func)\n\n", "idx": 1511}
{"namespace": "boltons.cacheutils.ThresholdCounter.elements", "completion": "        for key, (count, bucket) in self._count_map.items():\n            if count >= bucket:\n                for _ in xrange(count):\n                    yield key\n", "idx": 1512}
{"namespace": "boltons.cacheutils.ThresholdCounter.most_common", "completion": "        if n is None:\n            return self.iteritems()\n        else:\n            return heapq.nlargest(n, self.iteritems(), key=attrgetter(1))\n", "idx": 1513}
{"namespace": "boltons.cacheutils.ThresholdCounter.update", "completion": "        if isinstance(iterable, Mapping):\n            for k, v in iterable.items():\n                self.add(k)\n        else:\n            for k in iterable:\n                self.add(k)\n\n        for k, v in kwargs.items():\n            self.add(k)\n", "idx": 1514}
{"namespace": "boltons.cacheutils.MinIDMap.get", "completion": "        if a in self.mapping:\n            return self.mapping[a]\n        else:\n            if self.free:\n                id = self.free.pop()\n            else:\n                id = len(self.mapping)\n            self.mapping[a] = id\n            self.ref_map[id] = a\n            return id\n", "idx": 1515}
{"namespace": "boltons.iterutils.chunked", "completion": "    if count is None:\n        count = math.ceil(len(src) / size)\n\n    if 'fill' in kw:\n        fill = kw['fill']\n    else:\n        fill = None\n\n    if fill is None:\n        return list(itertools.islice(itertools.zip_longest(*[iter(src)] * size), count))\n    else:\n        return list(itertools.islice(itertools.zip_longest(*[iter(src)] * size, fillvalue=fill), count))\n\n", "idx": 1516}
{"namespace": "boltons.iterutils.chunk_ranges", "completion": "    if input_size < 0:\n        raise ValueError('input_size must be a positive integer')\n    if chunk_size < 0:\n        raise ValueError('chunk_size must be a positive integer')\n    if overlap_size < 0:\n        raise ValueError('overlap_size must be a positive integer')\n    if input_offset < 0:\n        raise ValueError('input_offset must be a positive integer')\n    if input_size < input_offset:\n        raise ValueError('input_offset must be less than or equal to input_size')\n    if chunk_size < overlap_size:\n        raise ValueError('chunk_size must be greater than or equal to overlap_size')\n\n    if align:\n        input_offset = input_offset + (chunk_size - overlap_size)\n\n    for i in range(0, input_size - chunk_size + 1, chunk_size - overlap_size):\n        yield (i + input_offset, i + input_offset + chunk_size)\n\n", "idx": 1517}
{"namespace": "boltons.iterutils.remap", "completion": "    if visit is default_visit:\n        visit = lambda path, key, value: (key, value)\n    if enter is default_visit:\n        enter = lambda path, key, value: (value, False)\n    if exit is default_visit:\n        exit = lambda path, key, old_parent, new_parent, new_items: new_parent\n    if 'reraise_visit' not in kwargs:\n        kwargs['reraise_visit'] = True\n    if 'reraise_enter' not in kwargs:\n        kwargs['reraise_enter'] = True\n    if 'reraise_exit' not in kwargs:\n        kwargs['reraise_exit'] = True\n\n    def _remap(root, path=()):\n        if isinstance(root, Mapping):\n            new_parent = root.__class__()\n            for key, value in root.items():\n                new_key, new_value = visit(path, key, value)\n                if new_key is True:\n                    new_key = key\n                if new_key is False:\n                    continue\n                if isinstance(new_value, Iterable):\n                    new_value, new_items = _remap(new_value, path + (key,))\n                else:\n                    new_items = []\n                new_parent[new_key] = new_value\n                if new_items:\n                    new_parent = exit(path, key, root, new_parent, new_items)\n            return new_parent, new_parent.items()\n        elif isinstance(root, Sequence):\n            new_parent = root.__class__()\n            for i, value in enumerate(root):\n                new_value = visit(path, i, value)\n                if new_value is True:\n                    new_value = value\n                if new_value is False:\n                    continue\n                if isinstance(new_value, Iterable):\n                    new_value, new_items = _remap(new_value, path + (i,))\n                else:\n                    new_items = []\n                new_parent.append(new_value)\n                if new_items:\n                    new_parent = exit(path, i, root, new_parent, new_items)\n            return new_parent, enumerate(new_parent)\n        elif isinstance(root, Set):\n            new_parent = root.__class__()\n            for i, value in enumerate(root):", "idx": 1518}
{"namespace": "boltons.iterutils.get_path", "completion": "    if not isinstance(path, tuple):\n        raise TypeError('expected tuple, not: %r' % path)\n    if not isinstance(root, Mapping):\n        raise TypeError('expected Mapping, not: %r' % root)\n    if not path:\n        return root\n    try:\n        return get_path(root[path[0]], path[1:], default)\n    except (KeyError, IndexError, TypeError) as exc:\n        raise PathAccessError(exc, path[0], path)\n    except Exception as exc:\n        raise PathAccessError(exc, path[0], path)\n    if default is not _UNSET:\n        return default\n    raise PathAccessError(exc, path[0], path)\n\n", "idx": 1519}
{"namespace": "boltons.iterutils.research", "completion": "    if not callable(query):\n        raise TypeError('query expected callable, not: %r' % query)\n    reraise_visit = not reraise\n    path, registry, stack = (), {}, [(None, root)]\n    new_items_stack = []\n    while stack:\n        key, value = stack.pop()\n        id_value = id(value)\n        if key is _REMAP_EXIT:\n            key, new_parent, old_parent = value\n            id_value = id(old_parent)\n            path, new_items = new_items_stack.pop()\n            value = exit(path, key, old_parent, new_parent, new_items)\n            registry[id_value] = value\n            if not new_items_stack:\n                continue\n        elif id_value in registry:\n            value = registry[id_value]\n        else:\n            res = enter(path, key, value)\n            try:\n                new_parent, new_items = res\n            except TypeError:\n                # TODO: handle False?\n                raise TypeError('enter should return a tuple of (new_parent,'\n                                ' items_iterator), not: %r' % res)\n            if new_items is not False:\n                # traverse unless False is explicitly passed\n                registry[id_value] = new_parent\n                new_items_stack.append((path, []))\n                if value is not root:\n                    path += (key,)\n                stack.append((_REMAP_EXIT, (key, new_parent, value)))\n                if new_items:\n                    stack.extend(reversed(list(new_items)))\n                continue\n        if query is _orig_default_visit:\n            # avoid function call overhead by inlining identity operation\n            visited_item = (key, value)\n        else:\n            try:\n                visited_item = query(path, key, value)\n            except Exception:\n                if reraise_visit:\n                    raise\n                visited_item = True\n            if visited_item is False:\n                continue  # drop\n            elif visited_item is True:\n                visited_item = (key, value)\n            # TODO: typecheck?\n            #    raise TypeError('expected (key, value) from visit(),'\n            #                    ' not: %r' % visited_item)\n        try:\n            new_items_stack[-1][1].append", "idx": 1520}
{"namespace": "boltons.socketutils.BufferedSocket.getrecvbuffer", "completion": "        return self.rbuf\n", "idx": 1521}
{"namespace": "boltons.socketutils.BufferedSocket.getsendbuffer", "completion": "        with self._recv_lock:\n            return self.sbuf\n", "idx": 1522}
{"namespace": "boltons.socketutils.BufferedSocket.recv", "completion": "        if size < 0:\n            raise ValueError('size must be non-negative')\n\n        if flags != 0:\n            raise ValueError('non-zero flags not supported: {flags!r}')\n\n        if timeout is _UNSET:\n            timeout = self.timeout\n\n        if timeout is None:\n            timeout = 0.0\n\n        if timeout < 0:\n            raise ValueError('timeout must be non-negative')\n\n        with self._recv_lock:\n            if len(self.rbuf) >= size:\n                return self.rbuf[:size]\n\n            if len(self.rbuf) > 0:\n                return self.rbuf + self.recv(size - len(self.rbuf), flags, timeout)\n\n            if timeout == 0.0:\n                try:\n                    return self.sock.recv(size, flags)\n                except socket.error as e:\n                    if e.errno == socket.errno.EWOULDBLOCK:\n                        raise Timeout('recv timed out')\n                    else:\n                        raise\n\n            start = time.time()\n            while True:\n                try:\n                    data = self.sock.recv(size, flags)\n                except socket.error as e:\n                    if e.errno == socket.errno.EWOULDBLOCK:\n                        if time.time() - start >= timeout:\n                            raise Timeout('recv timed out')\n                        else:\n                            time.sleep(0.01)\n                            continue\n                    else:\n                        raise\n                else:\n                    break\n\n            if len(data) == 0:\n                raise ConnectionClosed('recv returned 0 bytes')\n\n            self.rbuf += data\n            return self.rbuf[:size]\n", "idx": 1523}
{"namespace": "boltons.socketutils.BufferedSocket.recv_close", "completion": "        if maxsize is _UNSET:\n            maxsize = self.maxsize\n\n        if timeout is _UNSET:\n            timeout = self.timeout\n\n        if timeout is None:\n            timeout = 0\n\n        if maxsize is None:\n            maxsize = _RECV_LARGE_MAXSIZE\n\n        with self._recv_lock:\n            self.sock.settimeout(timeout)\n            data = b''\n            while True:\n                try:\n                    data += self.sock.recv(self._recvsize)\n                except socket.timeout:\n                    raise Timeout(timeout)\n                except socket.error as e:\n                    if e.errno == socket.errno.EWOULDBLOCK:\n                        if len(data) >= maxsize:\n                            raise MessageTooLong(maxsize)\n                        else:\n                            continue\n                    else:\n                        raise e\n                if len(data) >= maxsize:\n                    raise MessageTooLong(maxsize)\n                if not data:\n                    break\n            return data\n", "idx": 1524}
{"namespace": "boltons.socketutils.BufferedSocket.flush", "completion": "        with self._send_lock:\n            self.send(b'')\n", "idx": 1525}
{"namespace": "boltons.socketutils.BufferedSocket.buffer", "completion": "        with self._send_lock:\n            self.sbuf.append(data)\n", "idx": 1526}
{"namespace": "boltons.socketutils.BufferedSocket.close", "completion": "        self.sock.close()\n        self.rbuf = b''\n        self.sbuf = []\n", "idx": 1527}
{"namespace": "boltons.socketutils.NetstringSocket.setmaxsize", "completion": "        self.maxsize = maxsize\n        self._msgsize_maxsize = len(str(maxsize)) + 1  # len(str()) == log10\n", "idx": 1528}
{"namespace": "boltons.socketutils.NetstringSocket.write_ns", "completion": "        if len(payload) > self.maxsize:\n            raise NetstringMessageTooLong(len(payload), self.maxsize)\n\n        size = str(len(payload))\n        self.bsock.sendall(size + ':' + payload + ',')\n", "idx": 1529}
{"namespace": "boltons.fileutils.FilePerms.__repr__", "completion": "        return '%s(user=%r, group=%r, other=%r)' % (self.__class__.__name__, self.user, self.group, self.other)\n", "idx": 1530}
{"namespace": "boltons.mathutils.Bits.as_hex", "completion": "        return '{{0:0{0}x}}'.format(self.len // 4).format(self.val)\n", "idx": 1531}
{"namespace": "boltons.mathutils.Bits.from_hex", "completion": "        if type(hex) is bytes:\n            hex = hex.decode('ascii')\n        if not hex.startswith('0x'):\n            hex = '0x' + hex\n        return cls(int(hex, 16))\n", "idx": 1532}
{"namespace": "boltons.formatutils.split_format_str", "completion": "    ret = []\n    pos = 0\n    while True:\n        m = _pos_farg_re.search(fstr, pos)\n        if m is None:\n            ret.append((fstr[pos:],))\n            break\n        ret.append((fstr[pos:m.start()],))\n        ret.append((fstr[m.start():m.end()],))\n        pos = m.end()\n    return ret\n\n", "idx": 1533}
{"namespace": "boltons.formatutils.infer_positional_format_args", "completion": "    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is a hack, but it works.\n    # TODO: this is", "idx": 1534}
{"namespace": "boltons.formatutils.tokenize_format_str", "completion": "    # TODO: memoize\n    ret = []\n    formatter = Formatter()\n    for lit, fname, fspec, conv in formatter.parse(fstr):\n        if fname is None:\n            ret.append(lit)\n            continue\n        if resolve_pos:\n            fname = infer_positional_format_args(fname)\n        ret.append(BaseFormatField(fname, fspec, conv))\n    return ret\n\n", "idx": 1535}
{"namespace": "boltons.dictutils.OneToOne.clear", "completion": "        dict.clear(self)\n        dict.clear(self.inv)\n", "idx": 1536}
{"namespace": "boltons.dictutils.OneToOne.pop", "completion": "        if key in self:\n            val = self[key]\n            del self[key]\n            del self.inv[val]\n            return val\n        if default is _MISSING:\n            raise KeyError(key)\n        return default\n", "idx": 1537}
{"namespace": "boltons.dictutils.OneToOne.popitem", "completion": "        key, value = dict.popitem(self)\n        dict.__delitem__(self.inv, value)\n        return key, value\n", "idx": 1538}
{"namespace": "boltons.dictutils.ManyToMany.update", "completion": "        if isinstance(iterable, ManyToMany):\n            self.data.update(iterable.data)\n            self.inv.data.update(iterable.inv.data)\n        elif isinstance(iterable, dict):\n            for key, val in iterable.items():\n                self.add(key, val)\n        elif isinstance(iterable, list):\n            for key, val in iterable:\n                self.add(key, val)\n        else:\n            raise TypeError(\"The input argument is not of type ManyToMany, dict, or list.\")\n", "idx": 1539}
{"namespace": "boltons.dictutils.ManyToMany.add", "completion": "        if key not in self.data:\n            self.data[key] = set()\n        self.data[key].add(val)\n        if val not in self.inv.data:\n            self.inv.data[val] = set()\n        self.inv.data[val].add(key)\n        return\n", "idx": 1540}
{"namespace": "boltons.dictutils.ManyToMany.remove", "completion": "        self.data[key].remove(val)\n        if not self.data[key]:\n            del self.data[key]\n        self.inv.data[val].remove(key)\n        if not self.inv.data[val]:\n            del self.inv.data[val]\n", "idx": 1541}
{"namespace": "boltons.dictutils.ManyToMany.replace", "completion": "        # Replace the key with the new key in the data dictionary.\n        self.data[newkey] = self.data[key]\n        del self.data[key]\n\n        # Update the corresponding sets in the forward and inverse dictionaries.\n        for val in self.data[newkey]:\n            self.inv.data[val].remove(key)\n            self.inv.data[val].add(newkey)\n\n        for val in self.inv.data[newkey]:\n            self.data[val].remove(key)\n            self.data[val].add(newkey)\n\n        # Remove the old key from the forward and inverse dictionaries.\n        del self.inv.data[key]\n        del self.data[newkey]\n", "idx": 1542}
{"namespace": "boltons.dictutils.ManyToMany.iteritems", "completion": "        for key in self.data:\n            for val in self.data[key]:\n                yield key, val\n", "idx": 1543}
{"namespace": "gunicorn.config.Config.__str__", "completion": "        lines = []\n        for setting in self.settings.values():\n            if callable(setting.value):\n                value = \"<{qual_name}>\".format(qual_name=setting.value.__qualname__)\n            else:\n                value = setting.value\n            lines.append(\"{key:{key_max_length}} = {value}\".format(\n                key=setting.name,\n                key_max_length=max(len(setting.name), 10),\n                value=value,\n            ))\n        return \"\\n\".join(lines)\n", "idx": 1544}
{"namespace": "gunicorn.config.Config.set", "completion": "        if name not in self.settings:\n            raise AttributeError(\"No configuration setting for: %s\" % name)\n        self.settings[name].set(value)\n", "idx": 1545}
{"namespace": "gunicorn.config.Config.worker_class", "completion": "        uri = self.settings['worker_class'].get()\n\n        # are we using a threaded worker?\n        is_sync = uri.endswith('SyncWorker') or uri == 'sync'\n        if is_sync and self.threads > 1:\n            uri = uri.replace('SyncWorker', 'Thread')\n\n        try:\n            worker_class = util.import_object(uri)\n        except ImportError as e:\n            raise ConfigError(\"Failed to import worker class: %s\" % e)\n\n        worker_class.setup(self)\n        return worker_class\n", "idx": 1546}
{"namespace": "gunicorn.config.Config.address", "completion": "        # get the bind address from settings\n        bind = self.settings['bind'].get()\n\n        # parse the bind address\n        parsed_bind = util.parse_addresses(bind)\n\n        # return the parsed bind address\n        return parsed_bind\n", "idx": 1547}
{"namespace": "gunicorn.config.Config.logger_class", "completion": "        # check if the logger_class is \"simple\"\n        if self.settings['logger_class'].get() == \"simple\":\n            # if statsd is on, switch to the gunicorn.instrument.statsd.Statsd class\n            if self.settings['statsd'].get():\n                return util.load_class(\"gunicorn.instrument.statsd.Statsd\")\n            # load the default logger class\n            return util.load_class(\"gunicorn.glogging.Logger\")\n\n        # load the logger class\n        return util.load_class(self.settings['logger_class'].get())\n", "idx": 1548}
{"namespace": "gunicorn.sock.create_sockets", "completion": "    sockets = []\n    for addr in conf.bind:\n        sock_type = _sock_type(addr)\n        if fds is None:\n            sock = sock_type(addr, conf, log)\n        else:\n            sock = sock_type(addr, conf, log, fds.pop(0))\n        sockets.append(sock)\n\n    if conf.is_ssl:\n        if conf.certfile is None:\n            raise ValueError(\"SSL certificate file is not set\")\n        if conf.keyfile is None:\n            raise ValueError(\"SSL key file is not set\")\n\n        if conf.certfile == conf.keyfile:\n            raise ValueError(\"SSL certificate and key file are the same\")\n\n        if not os.path.exists(conf.certfile):\n            raise ValueError(\"SSL certificate file does not exist\")\n\n        if not os.path.exists(conf.keyfile):\n            raise ValueError(\"SSL key file does not exist\")\n\n        if conf.certfile.endswith(\".pem\"):\n            ssl_context = ssl.SSLContext(ssl.PROTOCOL_SSLv23)\n            ssl_context.load_cert_chain(conf.certfile, conf.keyfile)\n        else:\n            ssl_context = ssl.SSLContext(ssl.PROTOCOL_SSLv23)\n            ssl_context.load_cert_chain(conf.certfile, conf.keyfile)\n            ssl_context.verify_mode = ssl.CERT_REQUIRED\n            ssl_context.check_hostname = False\n\n        for sock in sockets:\n            sock.sock = ssl_context.wrap_socket(sock.sock)\n\n    return sockets", "idx": 1549}
{"namespace": "gunicorn.http.body.LengthReader.read", "completion": "        if not isinstance(size, int):\n            raise TypeError(\"size must be an integer type\")\n        if size < 0:\n            raise ValueError(\"Size must be positive.\")\n        if size == 0:\n            return b\"\"\n\n        buf = io.BytesIO()\n        while buf.tell() < size:\n            buf.write(self.unreader.read())\n\n        data = buf.getvalue()\n        ret, rest = data[:size], data[size:]\n        self.unreader.unread(rest)\n        self.length -= size\n        return ret", "idx": 1550}
{"namespace": "gunicorn.http.body.EOFReader.read", "completion": "        if not isinstance(size, int):\n            raise TypeError(\"size must be an integral type\")\n\n        if size < 0:\n            raise ValueError(\"Size must be positive.\")\n        if size == 0:\n            return b\"\"\n\n        if self.finished:\n            return b\"\"\n\n        buf = io.BytesIO()\n        data = self.unreader.read()\n        while data:\n            buf.write(data)\n            if buf.tell() >= size:\n                break\n            data = self.unreader.read()\n\n        buf = buf.getvalue()\n        ret, rest = buf[:size], buf[size:]\n        self.unreader.unread(rest)\n        if not data:\n            self.finished = True\n        return ret", "idx": 1551}
{"namespace": "gunicorn.http.body.Body.read", "completion": "        if size is None:\n            size = sys.maxsize\n        elif not isinstance(size, int):\n            raise TypeError(\"size must be an integral type\")\n        elif size < 0:\n            return b\"\"\n\n        if size == 0:\n            return b\"\"\n\n        if self.buf.tell() >= size:\n            ret, rest = self.buf.getvalue()[:size], self.buf.getvalue()[size:]\n            self.buf = io.BytesIO()\n            self.buf.write(rest)\n            return ret\n\n        data = self.reader.read(size)\n        while data:\n            self.buf.write(data)\n            if self.buf.tell() >= size:\n                break\n            data = self.reader.read(size)\n\n        data = self.buf.getvalue()\n        ret, rest = data[:size], data[size:]\n        self.buf = io.BytesIO()\n        self.buf.write(rest)\n        return ret\n", "idx": 1552}
{"namespace": "gunicorn.http.unreader.Unreader.read", "completion": "        if not isinstance(size, int) and not isinstance(size, long):\n            raise TypeError(\"size parameter must be an int or long.\")\n\n        if size == 0:\n            return b\"\"\n\n        if size is None:\n            if self.buf.tell() > 0:\n                data = self.buf.read()\n                self.buf.seek(0)\n                self.buf.truncate()\n                return data\n            else:\n                return self.chunk()\n\n        while self.buf.tell() < size:\n            data = self.chunk()\n            if data:\n                self.buf.write(data)\n            else:\n                data = self.buf.read()\n                self.buf.seek(0)\n                self.buf.truncate()\n                return data\n\n        data = self.buf.read(size)\n        self.buf.seek(0)\n        self.buf.truncate()\n        return data\n", "idx": 1553}
{"namespace": "gunicorn.http.unreader.Unreader.unread", "completion": "        self.buf.seek(0, os.SEEK_END)\n        self.buf.write(data)\n", "idx": 1554}
{"namespace": "gunicorn.http.unreader.IterUnreader.chunk", "completion": "        try:\n            return next(self.iter)\n        except StopIteration:\n            self.iter = None\n            return b\"\"\n\n", "idx": 1555}
{"namespace": "gunicorn.instrument.statsd.Statsd.critical", "completion": "        Logger.critical(self, msg, *args, **kwargs)\n        self.incr(self.prefix + \"gunicorn.log.critical\")\n", "idx": 1556}
{"namespace": "gunicorn.instrument.statsd.Statsd.access", "completion": "        try:\n            status_code = resp.status_code\n            if isinstance(status_code, str):\n                status_code = int(status_code.split(None, 1)[0])\n            self.histogram(\"gunicorn.http.duration\", request_time.total_seconds() * 1000)\n            self.increment(\"gunicorn.http.requests\")\n            self.increment(\"gunicorn.http.requests.%d\" % status_code)\n        except Exception:\n            Logger.warning(self, \"Failed to log to statsd\", exc_info=True)\n", "idx": 1557}
{"namespace": "praw.exceptions.RedditErrorItem.error_message", "completion": "        return f\"{self.error_type}: {self.message} on field {self.field}\"\n", "idx": 1558}
{"namespace": "praw.exceptions.RedditErrorItem.__repr__", "completion": "        return f\"{self.__class__.__name__}(error_type={self.error_type}, message={self.message}, field={self.field})\"\n\n", "idx": 1559}
{"namespace": "praw.models.util.BoundedSet.add", "completion": "        self._access(item)\n        if len(self._set) > self.max_items:\n            self._set.popitem(last=False)\n\n", "idx": 1560}
{"namespace": "praw.models.util.ExponentialCounter.counter", "completion": "        jitter = random.uniform(-0.5 * self._max, 0.5 * self._max)\n        self._base = self._base * 2 if self._base < self._max / 2 else self._max\n        return self._base + jitter\n", "idx": 1561}
{"namespace": "praw.models.listing.generator.ListingGenerator._extract_sublist", "completion": "        if isinstance(listing, list):\n            if len(listing) == 2:\n                return listing[1]\n            else:\n                raise ValueError(\n                    \"The generator returned a list PRAW didn't recognize. File a bug report at PRAW.\"\n                )\n        elif isinstance(listing, dict):\n            if \"data\" in listing:\n                if \"children\" in listing[\"data\"]:\n                    return listing[\"data\"][\"children\"]\n                elif \"mod_notes\" in listing[\"data\"]:\n                    return listing[\"data\"][\"mod_notes\"]\n                elif \"mod_note_replies\" in listing[\"data\"]:\n                    return listing[\"data\"][\"mod_note_replies\"]\n                elif \"mod_reports\" in listing[\"data\"]:\n                    return listing[\"data\"][\"mod_reports\"]\n                elif \"mod_log\" in listing[\"data\"]:\n                    return listing[\"data\"][\"mod_log\"]\n                elif \"mod_contributors\" in listing[\"data\"]:\n                    return listing[\"data\"][\"mod_contributors\"]\n                elif \"mod_contributors_history\" in listing[\"data\"]:\n                    return listing[\"data\"][\"mod_contributors_history\"]\n                elif \"mod_contributors_history_replies\" in listing[\"data\"]:\n                    return listing[\"data\"][\"mod_contributors_history_replies\"]\n                elif \"mod_reports_history\" in listing[\"data\"]:\n                    return listing[\"data\"][\"mod_reports_history\"]\n                elif \"mod_reports_history_replies\" in listing[\"data\"]:\n                    return listing[\"data\"][\"mod_reports_history_replies\"]\n                elif \"mod_reports_replies\" in listing[\"data\"]:\n                    return listing[\"data\"][\"mod_reports_replies\"]\n                elif \"mod_reports_replies_history\" in listing[\"data\"]:\n                    return listing[\"data\"][\"mod_reports_replies_history\"]\n                elif \"mod_reports_replies_history_replies\" in listing[\"data\"]:\n                    return listing[\"data\"][\"mod_reports_replies_history_replies\"]\n                elif \"mod_reports_replies_history_replies_history\" in listing[\"data\"]:\n                    return listing[\"data\"][\"mod_reports_replies_history_replies_history\"]\n                elif \"mod_reports_replies_history_replies_history_replies\" in listing[\"data\"]:\n                    return listing[\"data\"][\"mod_reports_replies_history_replies_history_replies\"]\n                elif \"mod_reports_replies_history_replies_history_replies", "idx": 1562}
{"namespace": "praw.util.token_manager.FileTokenManager.post_refresh_callback", "completion": "        with open(self._filename, \"w\") as f:\n            f.write(authorizer.refresh_token)\n", "idx": 1563}
{"namespace": "praw.util.token_manager.FileTokenManager.pre_refresh_callback", "completion": "        with open(self._filename, \"r\") as fp:\n            authorizer.refresh_token = fp.read()\n\n", "idx": 1564}
{"namespace": "praw.util.token_manager.SQLiteTokenManager._get", "completion": "        cursor = self._connection.execute(\n            \"SELECT refresh_token FROM tokens WHERE id = ?\", (self.key,)\n        )\n        result = cursor.fetchone()\n        if result is None:\n            raise KeyError(\n                \"No refresh token found for key '{}'\".format(self.key)\n            )\n        return result[0]\n", "idx": 1565}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.is_registered", "completion": "        cursor = self._connection.execute(\n            \"SELECT id FROM tokens WHERE id=?\", (self.key,)\n        )\n        result = cursor.fetchone()\n        if result is None:\n            return False\n        return True\n", "idx": 1566}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.post_refresh_callback", "completion": "        self._set(authorizer.refresh_token)\n        authorizer.refresh_token = None\n", "idx": 1567}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.pre_refresh_callback", "completion": "        authorizer.refresh_token = self._get()\n", "idx": 1568}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.register", "completion": "        if self.is_registered():\n            return False\n        else:\n            self._set(refresh_token)\n            return True\n", "idx": 1569}
{"namespace": "jc.cli.JcCli.about_jc", "completion": "        about_jc: JSONDictType = {\n            'library': {\n                'name': info.name,\n                'version': info.version,\n                'description': info.description,\n                'author': info.author,\n                'author_email': info.author_email,\n                'website': info.website,\n                'copyright': info.copyright,\n                'license': info.license,\n                'python_version': sys.version,\n                'python_path': sys.executable,\n                'parser_count': len(all_parser_info(show_hidden=True, show_deprecated=False)),\n                'standard_parser_count': len(all_parser_info(show_hidden=True, show_deprecated=False, standard=True)),\n                'streaming_parser_count': len(all_parser_info(show_hidden=True, show_deprecated=False, streaming=True)),\n                'plugin_parser_count': len(all_parser_info(show_hidden=True, show_deprecated=False, plugin=True)),\n            },\n            'parsers': all_parser_info(show_hidden=True, show_deprecated=False)\n        }\n\n        return about_jc\n", "idx": 1570}
{"namespace": "jc.cli.JcCli.yaml_out", "completion": "        if self.data_out is None:\n            return ''\n\n        if self.yaml_output:\n            try:\n                import ruamel.yaml as yaml\n            except ImportError:\n                utils.warning_message(['ruamel.yaml library not installed. Falling back to JSON formatting.'])\n                return self.json_out()\n\n            yaml_out = yaml.YAML()\n            yaml_out.indent(mapping=2, sequence=4, offset=2)\n            yaml_out.width = 1000000\n            yaml_out.allow_unicode = True\n            yaml_out.encoding = 'utf-8'\n            yaml_out.explicit_start = True\n            yaml_out.explicit_end = True\n            yaml_out.default_flow_style = False\n            yaml_out.preserve_quotes = True\n            yaml_out.sort_keys = False\n            yaml_out.line_break = '\\n'\n            yaml_out.block_seq_indent = 4\n            yaml_out.block_map_indent = 4\n            yaml_out.block_item_indent = 4\n            yaml_out.line_break = '\\n'\n            yaml_out.indent(mapping=2, sequence=4, offset=2)\n            yaml_out.width = 1000000\n            yaml_out.allow_unicode = True\n            yaml_out.encoding = 'utf-8'\n            yaml_out.explicit_start = True\n            yaml_out.explicit_end = True\n            yaml_out.default_flow_style = False\n            yaml_out.preserve_quotes = True\n            yaml_out.sort_keys = False\n            yaml_out.line_break = '\\n'\n            yaml_out.block_seq_indent = 4\n            yaml_out.block_map_indent = 4\n            yaml_out.block_item_indent = 4\n            yaml_out.line_break = '\\n'\n            yaml_out.indent(mapping=2, sequence=4, offset=2)\n            yaml_out.width = 1000000\n            yaml_out.allow_unicode = True\n            yaml_out.encoding = 'utf-8'\n            yaml_out.explicit_start = True\n            yaml_out.explicit_end = True\n            yaml_out.default", "idx": 1571}
{"namespace": "jc.parsers.os_release.parse", "completion": "    jc.utils.compatibility(__name__, info.compatible, quiet)\n    jc.utils.input_type_check(data)\n\n    raw_output: JSONDictType = {}\n\n    if jc.utils.has_data(data):\n\n        # remove leading and trailing whitespace\n        data = data.strip()\n\n        # remove leading and trailing whitespace\n        data = data.strip()\n\n        # remove leading and trailing whitespace\n        data = data.strip()\n\n        # remove leading and trailing whitespace\n        data = data.strip()\n\n        # remove leading and trailing whitespace\n        data = data.strip()\n\n        # remove leading and trailing whitespace\n        data = data.strip()\n\n        # remove leading and trailing whitespace\n        data = data.strip()\n\n        # remove leading and trailing whitespace\n        data = data.strip()\n\n        # remove leading and trailing whitespace\n        data = data.strip()\n\n        # remove leading and trailing whitespace\n        data = data.strip()\n\n        # remove leading and trailing whitespace\n        data = data.strip()\n\n        # remove leading and trailing whitespace\n        data = data.strip()\n\n        # remove leading and trailing whitespace\n        data = data.strip()\n\n        # remove leading and trailing whitespace\n        data = data.strip()\n\n        # remove leading and trailing whitespace\n        data = data.strip()\n\n        # remove leading and trailing whitespace\n        data = data.strip()\n\n        # remove leading and trailing whitespace\n        data = data.strip()\n\n        # remove leading and trailing whitespace\n        data = data.strip()\n\n        # remove leading and trailing whitespace\n        data = data.strip()\n\n        # remove leading and trailing whitespace\n        data = data.strip()\n\n        # remove leading and trailing whitespace\n        data = data.strip()\n\n        # remove leading and trailing whitespace\n        data = data.strip()\n\n        # remove leading and trailing whitespace\n        data = data.strip()\n\n        # remove leading and trailing whitespace\n        data = data.strip()\n\n        # remove leading and trailing whitespace\n        data = data.strip()\n\n        # remove leading and trailing whitespace\n        data = data.strip()\n\n        # remove leading and trailing whitespace\n        data = data.strip()\n\n        # remove leading and trailing whitespace\n        data = data.strip()\n\n        # remove leading and trailing whitespace\n        data = data.strip()\n\n        # remove leading and trailing whitespace\n        data = data.strip()\n\n        # remove leading and trailing whitespace\n        data = data.strip()\n\n        # remove leading and trailing whitespace\n        data = data.strip()\n\n        #", "idx": 1572}
{"namespace": "jc.parsers.xrandr._parse_screen", "completion": "    screen_line = next_lines.pop(0)\n    screen_match = re.match(_screen_pattern, screen_line)\n    if screen_match is None:\n        next_lines.insert(0, screen_line)\n        return None\n\n    screen_dict: Screen = {\n        \"screen_number\": int(screen_match.group(\"screen_number\")),\n        \"minimum_width\": int(screen_match.group(\"minimum_width\")),\n        \"minimum_height\": int(screen_match.group(\"minimum_height\")),\n        \"current_width\": int(screen_match.group(\"current_width\")),\n        \"current_height\": int(screen_match.group(\"current_height\")),\n        \"maximum_width\": int(screen_match.group(\"maximum_width\")),\n        \"maximum_height\": int(screen_match.group(\"maximum_height\")),\n        \"devices\": [],\n    }\n\n    while len(next_lines) > 0:\n        device = _parse_device(next_lines)\n        if device is not None:\n            screen_dict[\"devices\"].append(device)\n        else:\n            break\n\n    return screen_dict\n\n", "idx": 1573}
{"namespace": "jc.parsers.xrandr._parse_model", "completion": "    if not next_lines:\n        return None\n\n    next_line = next_lines.pop()\n    if not re.match(_edid_head_pattern, next_line):\n        next_lines.append(next_line)\n        return None\n\n    model: Model = {\"name\": \"\", \"product_id\": \"\", \"serial_number\": \"\"}\n\n    while next_lines:\n        next_line = next_lines.pop()\n        result = re.match(_edid_line_pattern, next_line)\n        if not result:\n            next_lines.append(next_line)\n            break\n        else:\n            model[\"name\"] += result.group(\"edid_line\")\n\n    while next_lines:\n        next_line = next_lines.pop()\n        result = re.match(_edid_line_pattern, next_line)\n        if not result:\n            next_lines.append(next_line)\n            break\n        else:\n            model[\"product_id\"] += result.group(\"edid_line\")\n\n    while next_lines:\n        next_line = next_lines.pop()\n        result = re.match(_edid_line_pattern, next_line)\n        if not result:\n            next_lines.append(next_line)\n            break\n        else:\n            model[\"serial_number\"] += result.group(\"edid_line\")\n\n    return model\n\n", "idx": 1574}
{"namespace": "jc.parsers.xrandr._parse_mode", "completion": "    result = re.match(_mode_pattern, line)\n    if not result:\n        return None\n\n    matches = result.groupdict()\n\n    mode: Mode = {\n        \"resolution_width\": int(matches[\"resolution_width\"]),\n        \"resolution_height\": int(matches[\"resolution_height\"]),\n        \"is_high_resolution\": matches[\"is_high_resolution\"] is not None,\n        \"frequencies\": [],\n    }\n\n    frequencies = re.findall(_frequencies_pattern, matches[\"rest\"])\n    for frequency in frequencies:\n        frequency_dict: Frequency = {\n            \"frequency\": float(frequency[0]),\n            \"is_current\": frequency[1] == \"*\",\n            \"is_preferred\": frequency[2] == \"+\",\n        }\n        mode[\"frequencies\"].append(frequency_dict)\n\n    return mode\n\n", "idx": 1575}
{"namespace": "pythonforandroid.archs.Arch.include_dirs", "completion": "        return [\n            join(self.ctx.ndk.sysroot_include_dir, self.command_prefix),\n            join(self.ctx.ndk.sysroot_include_dir, self.arch),\n            join(self.ctx.ndk.sysroot_include_dir, self.arch, 'c++', 'v1'),\n            join(self.ctx.ndk.sysroot_include_dir, self.arch, 'c++', 'v1', 'include'),\n            join(self.ctx.ndk.sysroot_include_dir, self.arch, 'c++', 'v1', 'backward'),\n            join(self.ctx.ndk.sysroot_include_dir, self.arch, 'c++', 'v1', 'include', 'c++'),\n            join(self.ctx.ndk.sysroot_include_dir, self.arch, 'c++', 'v1', 'include', 'c++', 'v1'),\n            join(self.ctx.ndk.sysroot_include_dir, self.arch, 'c++', 'v1', 'include', 'c++', 'v1', 'bits'),\n            join(self.ctx.ndk.sysroot_include_dir, self.arch, 'c++', 'v1', 'include', 'c++', 'v1', 'bits', 'c++config'),\n            join(self.ctx.ndk.sysroot_include_dir, self.arch, 'c++', 'v1', 'include', 'c++', 'v1', 'bits', 'os_defines'),\n            join(self.ctx.ndk.sysroot_include_dir, self.arch, 'c++', 'v1', 'include', 'c++', 'v1', 'bits', 'cpu_defines'),\n            join(self.ctx.ndk.sysroot_include_dir, self.arch, 'c++', 'v1', 'include', 'c++', 'v1', 'bits', 'invoke.h'),\n            join(self.ctx.ndk.sysroot_include_dir, self.arch, 'c++', 'v1', 'include', 'c++', 'v1', 'bits', 'c++abi.h", "idx": 1576}
{"namespace": "pythonforandroid.archs.Arch.target", "completion": "        return \"{}-{}\".format(self.command_prefix, self.ctx.ndk_api)\n", "idx": 1577}
{"namespace": "pythonforandroid.archs.ArchARM.target", "completion": "        return '{triplet}{ndk_api}'.format(\n            triplet=self.command_prefix, ndk_api=self.ctx.ndk_api\n        )\n\n", "idx": 1578}
{"namespace": "pythonforandroid.recipe.Recipe.get_recipe", "completion": "        if name in cls.recipe_cache:\n            return cls.recipe_cache[name]\n\n        for recipes_dir in cls.recipe_dirs(ctx):\n            if recipes_dir and exists(recipes_dir):\n                for name in listdir(recipes_dir):\n                    if name in forbidden_dirs:\n                        continue\n                    fn = join(recipes_dir, name)\n                    if isdir(fn):\n                        try:\n                            mod = import_module(\n                                'recipes.{}'.format(name))\n                            recipe = mod.Recipe\n                            if recipe.__name__ == name:\n                                cls.recipe_cache[name] = recipe\n                                return recipe\n                        except ImportError:\n                            pass\n        raise ImportError('No recipe named {} found'.format(name))\n", "idx": 1579}
{"namespace": "pythonforandroid.prerequisites.HomebrewPrerequisite.darwin_helper", "completion": "        info(\n            \"The installer for Homebrew is not supported on macOS. Please follow the instructions at https://docs.brew.sh/Installation to install Homebrew.\"\n        )\n", "idx": 1580}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_checker", "completion": "        # Check if the Homebrew formula for OpenSSL is installed.\n        if self._darwin_get_brew_formula_location_prefix(self.homebrew_formula_name):\n            return True\n        else:\n            return False\n", "idx": 1581}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_pkg_config_location", "completion": "        prefix_location = self._darwin_get_brew_formula_location_prefix(\n            self.homebrew_formula_name, installed=True\n        )\n        return os.path.join(prefix_location, \"lib/pkgconfig\")\n", "idx": 1582}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_installer", "completion": "        # Check if Homebrew is installed\n        if shutil.which(\"brew\") is None:\n            error(\"Homebrew is not installed. Please install Homebrew and try again.\")\n            return\n\n        # Check if OpenSSL is already installed\n        if self._darwin_get_brew_formula_location_prefix(self.homebrew_formula_name):\n            info(\"OpenSSL is already installed. Skipping installation.\")\n            return\n\n        # Install OpenSSL using Homebrew\n        info(\"Installing OpenSSL using Homebrew...\")\n        subprocess.check_call([\"brew\", \"install\", self.homebrew_formula_name])\n\n        # Check if OpenSSL is installed\n        if self._darwin_get_brew_formula_location_prefix(self.homebrew_formula_name):\n            info(\"OpenSSL installation successful.\")\n        else:\n            error(\"OpenSSL installation failed. Please try again.\")\n\n", "idx": 1583}
{"namespace": "pythonforandroid.prerequisites.AutoconfPrerequisite.darwin_installer", "completion": "        info(\"Installing Autoconf ...\")\n        subprocess.check_output([\"brew\", \"install\", \"autoconf\"])\n\n", "idx": 1584}
{"namespace": "pythonforandroid.prerequisites.AutomakePrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"automake\", installed=True)\n            is not None\n        )\n", "idx": 1585}
{"namespace": "pythonforandroid.prerequisites.AutomakePrerequisite.darwin_installer", "completion": "        info(\"Installing Automake ...\")\n        subprocess.check_output([\"brew\", \"install\", \"automake\"])\n\n", "idx": 1586}
{"namespace": "pythonforandroid.prerequisites.LibtoolPrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"libtool\", installed=True)\n            is not None\n        )\n", "idx": 1587}
{"namespace": "pythonforandroid.prerequisites.LibtoolPrerequisite.darwin_installer", "completion": "        info(\"Installing Libtool ...\")\n        subprocess.check_output([\"brew\", \"install\", \"libtool\"])\n\n", "idx": 1588}
{"namespace": "pythonforandroid.prerequisites.PkgConfigPrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"pkg-config\", installed=True)\n            is not None\n        )\n", "idx": 1589}
{"namespace": "pythonforandroid.prerequisites.PkgConfigPrerequisite.darwin_installer", "completion": "        info(\"Installing Pkg-Config ...\")\n        subprocess.check_output([\"brew\", \"install\", \"pkg-config\"])\n\n", "idx": 1590}
{"namespace": "pythonforandroid.prerequisites.CmakePrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"cmake\", installed=True)\n            is not None\n        )\n", "idx": 1591}
{"namespace": "pythonforandroid.prerequisites.CmakePrerequisite.darwin_installer", "completion": "        info(\"Installing CMake ...\")\n        subprocess.check_output([\"brew\", \"install\", \"cmake\"])\n\n", "idx": 1592}
{"namespace": "pythonforandroid.prerequisites.get_required_prerequisites", "completion": "    prerequisites = [\n        HomebrewPrerequisite(),\n        JDKPrerequisite(),\n        OpenSSLPrerequisite(),\n        AutoconfPrerequisite(),\n        AutomakePrerequisite(),\n        LibtoolPrerequisite(),\n        PkgConfigPrerequisite(),\n        CmakePrerequisite(),\n    ]\n\n    return [pr for pr in prerequisites if pr.mandatory[platform]]", "idx": 1593}
{"namespace": "pythonforandroid.pythonpackage.parse_as_folder_reference", "completion": "    # Check if the dependency reference is a folder path:\n    if os.path.isdir(dep):\n        return dep\n\n    # Check if the dependency reference is a file:// URL:\n    if dep.startswith(\"file://\"):\n        # Parse the file:// URL:\n        parsed_url = urlparse(dep)\n\n        # Check if the parsed URL is a file URL:\n        if parsed_url.scheme == \"file\":\n            # Get the file path from the parsed URL:\n            file_path = urlunquote(parsed_url.path)\n\n            # Check if the file path exists:\n            if os.path.exists(file_path):\n                # Return the file path:\n                return file_path\n\n    # If the dependency reference is not a folder path or a file:// URL, return None:\n    return None\n\n", "idx": 1594}
{"namespace": "pythonforandroid.pythonpackage.get_package_name", "completion": "    if use_cache and dependency in package_name_cache:\n        if time.time() - package_name_cache[dependency][1] < 60 * 60:\n            return package_name_cache[dependency][0]\n\n    package_name = _extract_info_from_package(dependency, \"name\")\n    package_name_cache[dependency] = (package_name, time.time())\n    return package_name\n\n", "idx": 1595}
{"namespace": "pythonforandroid.recommendations.read_ndk_version", "completion": "    try:\n        with open(join(ndk_dir, 'source.properties'), 'r') as f:\n            for line in f:\n                if line.startswith('Pkg.Revision'):\n                    return LooseVersion(line.split('=')[1])\n    except IOError:\n        warning(PARSE_ERROR_NDK_MESSAGE)\n        return None", "idx": 1596}
{"namespace": "pythonforandroid.recommendations.check_target_api", "completion": "    if api < MIN_TARGET_API:\n        warning(OLD_API_MESSAGE)\n\n    if api < ARMEABI_MAX_TARGET_API and arch == 'armeabi':\n        raise BuildInterruptingException(\n            UNSUPPORTED_NDK_API_FOR_ARMEABI_MESSAGE.format(\n                req_ndk_api=MIN_TARGET_API, max_ndk_api=ARMEABI_MAX_TARGET_API\n            ),\n            instructions=(\n                'Please, go to the android NDK page ({ndk_url}) and download a'\n                ' supported version.\\n*** The currently recommended NDK'\n                ' version is {rec_version} ***'.format(\n                    ndk_url=NDK_DOWNLOAD_URL,\n                    rec_version=RECOMMENDED_NDK_VERSION,\n                )\n            ),\n        )", "idx": 1597}
{"namespace": "pythonforandroid.recommendations.check_ndk_api", "completion": "    if ndk_api < MIN_NDK_API:\n        warning(OLD_NDK_API_MESSAGE)\n\n    if ndk_api > android_api:\n        raise BuildInterruptingException(\n            TARGET_NDK_API_GREATER_THAN_TARGET_API_MESSAGE.format(\n                ndk_api=ndk_api, android_api=android_api\n            ),\n            instructions='You probably want to build with --ndk-api={android_api}'.format(\n                android_api=android_api\n            ),\n        )", "idx": 1598}
{"namespace": "pythonforandroid.androidndk.AndroidNDK.llvm_prebuilt_dir", "completion": "        return os.path.join(self.ndk_dir, \"toolchains\", \"llvm\", \"prebuilt\", self.host_tag)\n", "idx": 1599}
{"namespace": "pythonforandroid.build.Context.setup_dirs", "completion": "        # Set the storage directory\n        self.storage_dir = storage_dir\n\n        # Set the build directory\n        self.build_dir = join(self.storage_dir, 'build')\n\n        # Set the distribution directory\n        self.distribution.setup_dirs(self.build_dir)\n", "idx": 1600}
{"namespace": "pythonforandroid.graph.get_dependency_tuple_list_for_recipe", "completion": "    # Get the dependencies of the recipe\n    dependencies = recipe.depends\n\n    # Convert the dependencies into tuples\n    dependencies = [tuple(dep.lower()) for dep in dependencies]\n\n    # Filter out the blacklisted items\n    if blacklist is not None:\n        dependencies = [dep for dep in dependencies if dep not in blacklist]\n\n    return dependencies\n\n", "idx": 1601}
{"namespace": "pythonforandroid.graph.obvious_conflict_checker", "completion": "    if blacklist is None:\n        blacklist = set()\n    assert type(blacklist) is set\n    # Get all recipes and their dependencies\n    all_recipes = Recipe.get_all_recipes(ctx)\n    all_inputs = [r.name for r in all_recipes]\n    all_inputs.sort()\n    all_inputs = [r.lower() for r in all_inputs]\n    all_inputs = [r for r in all_inputs if r not in blacklist]\n\n    # Get all dependencies for all recipes\n    all_dependencies = []\n    for recipe in all_recipes:\n        dependencies = get_dependency_tuple_list_for_recipe(\n            recipe, blacklist=blacklist\n        )\n        all_dependencies.extend(dependencies)\n\n    # Get all conflicts for all recipes\n    all_conflicts = []\n    for recipe in all_recipes:\n        conflicts = [dep.lower() for dep in recipe.conflicts]\n        all_conflicts.extend(conflicts)\n\n    # Get all conflicts for all recipes\n    all_opt_depends = []\n    for recipe in all_recipes:\n        opt_depends = recipe.get_opt_depends_in_list(all_inputs)\n        all_opt_depends.extend(opt_depends)\n\n    # Get all conflicts for all recipes\n    all_opt_conflicts = []\n    for recipe in all_recipes:\n        opt_conflicts = recipe.get_opt_conflicts_in_list(all_inputs)\n        all_opt_conflicts.extend(opt_conflicts)\n\n    # Get all conflicts for all recipes\n    all_opt_conflicts_tuples = []\n    for recipe in all_recipes:\n        opt_conflicts_tuples = recipe.get_opt_conflicts_tuples_in_list(all_inputs)\n        all_opt_conflicts_tuples.extend(opt_conflicts_tuples)\n\n    # Get all conflicts for all recipes\n    all_opt_conflicts_tuples_tuples = []\n    for recipe in all_recipes:\n        opt_conflicts_tuples_tuples = recipe.get_opt_conflicts_tuples_tuples_in_list(all_inputs)\n        all_opt_conflicts_tuples_tuples.extend(opt_conflicts_tuples_tuples)\n\n    # Get all conflicts for all recipes\n    all_opt_conflicts_tuples_", "idx": 1602}
{"namespace": "pythonforandroid.graph.get_recipe_order_and_bootstrap", "completion": "    if blacklist is None:\n        blacklist = set()\n    assert type(blacklist) is set\n    if bs is None:\n        bs = Bootstrap.get_bootstrap(ctx)\n    assert isinstance(bs, Bootstrap)\n\n    # Clean up names:\n    names = [name.lower() for name in names]\n    names = [name for name in names if name not in blacklist]\n\n    # Add bootstrap dependencies:\n    names = bs.add_bootstrap_dependencies(names)\n\n    # Check for obvious conflicts:\n    obvious_conflict_checker(ctx, names, blacklist=blacklist)\n\n    # Get all possible orders:\n    orders = recursively_collect_orders(\n        names[0], ctx, names, blacklist=blacklist\n    )\n\n    # Convert to linear list:\n    orders = [list(order) for order in orders]\n\n    # Sort by preference:\n    orders.sort(key=lambda order: order.count(names[0]))\n\n    # Return the first order:\n    return orders[0], [Recipe.get_recipe(name, ctx) for name in orders[0]], bs.get_python_modules(), bs", "idx": 1603}
{"namespace": "pythonforandroid.util.ensure_dir", "completion": "    if not exists(dn):\n        LOGGER.debug(\"Create directory {}\".format(dn))\n        makedirs(dn)\n\n", "idx": 1604}
{"namespace": "pythonforandroid.util.move", "completion": "    LOGGER.debug(\"Moving {0} to {1}\".format(source, destination))\n    shutil.move(source, destination)\n\n", "idx": 1605}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.get_bootstrap_from_recipes", "completion": "        # Get all bootstraps that are usable for the given recipes:\n        bootstraps = cls.get_usable_bootstraps_for_recipes(recipes, ctx)\n\n        # If there are no usable bootstraps, return None:\n        if len(bootstraps) == 0:\n            return None\n\n        # If there is only one usable bootstrap, return it:\n        if len(bootstraps) == 1:\n            return bootstraps.pop()\n\n        # If there are multiple usable bootstraps, sort them by priority:\n        bootstraps = sorted(bootstraps, key=_cmp_bootstraps_by_priority)\n\n        # Return the first bootstrap in the sorted list:\n        return bootstraps[0]\n", "idx": 1606}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.get_bootstrap", "completion": "        # Get the bootstrap directory:\n        bootstrap_dirs = cls.get_bootstrap_dirs()\n        bootstrap_dir = None\n        for bootstrap_dir in bootstrap_dirs:\n            if name in listdir(bootstrap_dir):\n                break\n        if bootstrap_dir is None:\n            raise BuildInterruptingException(\n                'Could not find bootstrap directory for {}. '\n                'Available bootstraps: {}'.format(name, cls.all_bootstraps()))\n\n        # Get the bootstrap class:\n        bootstrap_class = getattr(\n            importlib.import_module(\n                bootstrap_dir.replace(ctx.root_dir, '') + '.bootstrap'),\n            name.title()\n        )\n\n        # Set the bootstrap directory:\n        bootstrap_class.bootstrap_dir = bootstrap_dir\n\n        # Set the context:\n        bootstrap_class.ctx = ctx\n\n        return bootstrap_class\n\n", "idx": 1607}
{"namespace": "pythonforandroid.bootstrap.expand_dependencies", "completion": "    # Get all recipes:\n    recipes_list = [Recipe.get_recipe(recipe, ctx) for recipe in recipes]\n\n    # Get all recipes that have alternatives:\n    recipes_with_alternatives = [recipe for recipe in recipes_list if recipe.alternatives]\n\n    # Get all recipes that do not have alternatives:\n    recipes_without_alternatives = [recipe for recipe in recipes_list if not recipe.alternatives]\n\n    # Get all recipes that have alternatives:\n    recipes_with_alternatives_lists = [recipe.alternatives for recipe in recipes_with_alternatives]\n\n    # Get all recipes that do not have alternatives:\n    recipes_without_alternatives_lists = [recipe.recipe_depends for recipe in recipes_without_alternatives]\n\n    # Get all recipes that have alternatives:\n    recipes_with_alternatives_lists_expanded = [expand_dependencies(recipe, ctx) for recipe in recipes_with_alternatives_lists]\n\n    # Get all recipes that do not have alternatives:\n    recipes_without_alternatives_lists_expanded = [recipes_without_alternatives_lists]\n\n    # Combine all recipes:\n    recipes_lists_expanded = recipes_with_alternatives_lists_expanded + recipes_without_alternatives_lists_expanded\n\n    # Return the combined recipes:\n    return recipes_lists_expanded", "idx": 1608}
{"namespace": "pythonforandroid.recipes.icu.ICURecipe.get_recipe_dir", "completion": "        if self.local_recipes_dir is not None:\n            if os.path.exists(os.path.join(self.local_recipes_dir, 'icu4c')):\n                return os.path.join(self.local_recipes_dir, 'icu4c')\n        return os.path.join(self.root_dir, 'icu4c')\n", "idx": 1609}
{"namespace": "mmcv.transforms.wrappers.KeyMapper.__repr__", "completion": "        # Get the string representation of the transforms\n        transform_str = self.transforms.__repr__()\n\n        # Get the string representation of the mapping\n        mapping_str = str(self.mapping)\n\n        # Get the string representation of the remapping\n        remapping_str = str(self.remapping)\n\n        # Get the string representation of the auto_remap\n        auto_remap_str = str(self.auto_remap)\n\n        # Get the string representation of the allow_nonexist_keys\n        allow_nonexist_keys_str = str(self.allow_nonexist_keys)\n\n        # Return the string representation of the KeyMapper instance\n        return f'{self.__class__.__name__}(transforms = {transform_str}, mapping = {mapping_str}, remapping = {remapping_str}, auto_remap = {auto_remap_str}, allow_nonexist_keys = {allow_nonexist_keys_str})'\n\n", "idx": 1610}
{"namespace": "mmcv.transforms.wrappers.TransformBroadcaster.__repr__", "completion": "        repr_str = self.__class__.__name__\n        repr_str += f'(transforms = {self.transforms}'\n        repr_str += f', mapping = {self.mapping}'\n        repr_str += f', remapping = {self.remapping}'\n        repr_str += f', auto_remap = {self.auto_remap}'\n        repr_str += f', allow_nonexist_keys = {self.allow_nonexist_keys})'\n        return repr_str\n\n", "idx": 1611}
{"namespace": "mackup.utils.delete", "completion": "    if os.path.islink(filepath):\n        os.unlink(filepath)\n    elif os.path.isdir(filepath):\n        shutil.rmtree(filepath)\n    else:\n        os.remove(filepath)\n\n", "idx": 1612}
{"namespace": "mackup.utils.copy", "completion": "    # Check if the source and destination paths are valid and absolute paths.\n    if not os.path.isabs(src) or not os.path.isabs(dst):\n        raise ValueError(\"Source and destination paths must be absolute paths.\")\n\n    # Create the necessary directories in the destination path if they do not exist.\n    if not os.path.exists(os.path.dirname(dst)):\n        os.makedirs(os.path.dirname(dst))\n\n    # Copy the file to the destination.\n    if os.path.isfile(src):\n        shutil.copyfile(src, dst)\n    # Copy the folder to the destination.\n    elif os.path.isdir(src):\n        shutil.copytree(src, dst)\n    # Raise a ValueError if the source is neither a file nor a folder.\n    else:\n        raise ValueError(\"Source must be a file or a folder.\")\n\n    # Set the appropriate file permissions for the copied file or folder.\n    os.chmod(dst, 0o755)\n\n", "idx": 1613}
{"namespace": "mackup.utils.get_dropbox_folder_location", "completion": "    # Get the home directory\n    home = os.path.expanduser(\"~\")\n\n    # Get the host.db file path\n    host_db_path = os.path.join(home, \".dropbox\", \"host\")\n\n    # Read the host.db file\n    with open(host_db_path, \"r\") as host_db:\n        host_db_content = host_db.read()\n\n    # Decode the Dropbox home path\n    dropbox_home_path = base64.b64decode(host_db_content)\n\n    # Return the full path to the current Dropbox folder\n    return dropbox_home_path\n\n", "idx": 1614}
{"namespace": "mackup.utils.get_copy_folder_location", "completion": "    copy_db_path = \"Library/Application Support/Copy/Copy.db\"\n    yosemite_copy_db_path = (\n        \"Library/Application Support/Copy/user_default/Copy.db\"\n    )\n    yosemite_copy_db = os.path.join(os.environ[\"HOME\"], yosemite_copy_db_path)\n    if os.path.isfile(yosemite_copy_db):\n        copy_db_path = yosemite_copy_db\n\n    copy_home = None\n\n    copy_db = os.path.join(os.environ[\"HOME\"], copy_db_path)\n    if os.path.isfile(copy_db):\n        con = sqlite3.connect(copy_db)\n        if con:\n            cur = con.cursor()\n            query = (\n                \"SELECT data_value \"\n                \"FROM data \"\n                \"WHERE entry_key = 'csmRootPath';\"\n            )\n            cur.execute(query)\n            data = cur.fetchone()\n            copy_home = str(data[0])\n            con.close()\n\n    if not copy_home:\n        error(constants.ERROR_UNABLE_TO_FIND_STORAGE.format(provider=\"Copy install\"))\n\n    return copy_home\n\n", "idx": 1615}
{"namespace": "mackup.utils.can_file_be_synced_on_current_platform", "completion": "    # Check if the file or folder exists\n    if not os.path.exists(path):\n        return False\n\n    # Check if the file is a symbolic link\n    if os.path.islink(path):\n        return False\n\n    # Check if the file is a hidden file\n    if path.startswith(\".\"):\n        return False\n\n    # Check if the file is a Dropbox file\n    if path.startswith(get_dropbox_folder_location()):\n        return False\n\n    # Check if the file is a Google Drive file\n    if path.startswith(get_google_drive_folder_location()):\n        return False\n\n    # Check if the file is a Copy file\n    if path.startswith(get_copy_folder_location()):\n        return False\n\n    # Check if the file is a iCloud Drive file\n    if path.startswith(get_icloud_folder_location()):\n        return False\n\n    # Check if the file is a Sublime Text file\n    if is_process_running(\"Sublime Text\"):\n        if path.startswith(os.path.expanduser(\"~/Library/Application Support/Sublime Text 3/Packages/User/\")):\n            return False\n\n    # Check if the file is a TextMate file\n    if is_process_running(\"TextMate\"):\n        if path.startswith(os.path.expanduser(\"~/Library/Application Support/TextMate/\")):\n            return False\n\n    # Check if the file is a Vim file\n    if is_process_running(\"vim\"):\n        if path.startswith(os.path.expanduser(\"~/.vim/\")):\n            return False\n\n    # Check if the file is a Emacs file\n    if is_process_running(\"Emacs\"):\n        if path.startswith(os.path.expanduser(\"~/.emacs.d/\")):\n            return False\n\n    # Check if the file is a Atom file\n    if is_process_running(\"Atom\"):\n        if path.startswith(os.path.expanduser(\"~/.atom/\")):\n            return False\n\n    # Check if the file is a Visual Studio Code file\n    if is_process_running(\"Code\"):\n        if path.startswith(os.path.expanduser(\"~/Library/Application Support/Code/User/\")):\n            return False\n\n    # Check if the file is a PyCharm file\n    if is_process_running(\"PyCharm\"):\n        if path.startswith(os", "idx": 1616}
{"namespace": "hl7.client.MLLPClient.send_message", "completion": "        if isinstance(message, hl7.Message):\n            message = message.encode(self.encoding)\n        elif isinstance(message, str):\n            message = message.encode(self.encoding)\n        elif isinstance(message, bytes):\n            pass\n        else:\n            raise TypeError(\"Message must be a string, bytes, or hl7.Message object\")\n\n        message = SB + message + EB + CR\n\n        self.send(message)\n", "idx": 1617}
{"namespace": "hl7.client.MLLPClient.send", "completion": "        # Send the data to the server\n        self.socket.send(data)\n\n        # Wait for the server to return a response\n        response = self.socket.recv(RECV_BUFFER)\n\n        return response\n\n", "idx": 1618}
{"namespace": "hl7.datatypes._UTCOffset.tzname", "completion": "        # Calculate the UTC offset in minutes\n        utc_offset = self.minutes\n\n        # Format the UTC offset in the format \"+/-HHMM\"\n        utc_offset_str = \"+%02d%02d\" % (math.floor(utc_offset / 60), utc_offset % 60)\n\n        return utc_offset_str\n", "idx": 1619}
{"namespace": "hl7.datatypes.parse_datetime", "completion": "    if not value:\n        return None\n\n    if len(value) < 4:\n        return None\n\n    if len(value) > 26:\n        return None\n\n    if value[0] != \"2\" and value[0] != \"1\":\n        return None\n\n    if value[1] != \"0\" and value[1] != \"1\":\n        return None\n\n    if value[2] != \"0\" and value[2] != \"1\":\n        return None\n\n    if value[3] != \"0\" and value[3] != \"1\":\n        return None\n\n    if len(value) > 4 and value[4] != \"0\" and value[4] != \"1\":\n        return None\n\n    if len(value) > 6 and value[5] != \"0\" and value[5] != \"1\":\n        return None\n\n    if len(value) > 8 and value[6] != \"0\" and value[6] != \"1\":\n        return None\n\n    if len(value) > 10 and value[7] != \"0\" and value[7] != \"1\":\n        return None\n\n    if len(value) > 12 and value[8] != \"0\" and value[8] != \"1\":\n        return None\n\n    if len(value) > 14 and value[9] != \"0\" and value[9] != \"1\":\n        return None\n\n    if len(value) > 16 and value[10] != \"0\" and value[10] != \"1\":\n        return None\n\n    if len(value) > 18 and value[11] != \"0\" and value[11] != \"1\":\n        return None\n\n    if len(value) > 20 and value[12] != \"0\" and value[12] != \"1\":\n        return None\n\n    if len(value) > 22 and value[13] != \"0\" and value[13] != \"1\":\n        return None\n\n    if len(value) > 24 and value[14] != \"0\" and value[14] != \"1\":\n        return None\n\n    if len(value) > 26 and value[15] != \"0\" and value[15] != \"1", "idx": 1620}
{"namespace": "hl7.parser._ParsePlan.container", "completion": "        return self.containers[len(data) - 1](data, esc=self.esc, separators=self.separators, factory=self.factory)\n", "idx": 1621}
{"namespace": "hl7.parser._ParsePlan.next", "completion": "        return _ParsePlan(\n            self.separators[self.separators.find(self.separator) + 1],\n            self.separators,\n            self.containers[1:],\n            self.esc,\n            self.factory,\n        )\n", "idx": 1622}
{"namespace": "hl7.version.get_version", "completion": "    version = VERSION\n    if len(version) < 4:\n        return str(version[0])\n    if version[3] == \"final\":\n        return str(version[0])\n    if version[3] == \"dev\":\n        return str(version[0]) + \".dev\"\n    return str(version[0]) + version[3]", "idx": 1623}
{"namespace": "twtxt.config.Config.from_file", "completion": "        if not os.path.exists(file):\n            logger.error(\"Config file does not exist: %s\", file)\n            return None\n\n        cfg = configparser.ConfigParser()\n        cfg.read(file)\n\n        if not cfg.has_section(\"twtxt\"):\n            logger.error(\"Config file is not a valid twtxt config file: %s\", file)\n            return None\n\n        if not cfg.has_option(\"twtxt\", \"sources\"):\n            logger.error(\"Config file is not a valid twtxt config file: %s\", file)\n            return None\n\n        sources = cfg.get(\"twtxt\", \"sources\").split(\",\")\n        sources = [Source(s) for s in sources]\n\n        return cls(file, cfg)\n", "idx": 1624}
{"namespace": "twtxt.config.Config.discover", "completion": "        config_file = os.path.join(cls.config_dir, cls.config_name)\n        return cls.from_file(config_file)\n", "idx": 1625}
{"namespace": "twtxt.config.Config.create_config", "completion": "        config_parser = configparser.ConfigParser()\n        config_parser.add_section(\"twtxt\")\n        config_parser[\"twtxt\"][\"nick\"] = nick\n        config_parser[\"twtxt\"][\"twtfile\"] = twtfile\n        config_parser[\"twtxt\"][\"twturl\"] = twturl\n        config_parser[\"twtxt\"][\"disclose_identity\"] = disclose_identity\n        config_parser[\"twtxt\"][\"add_news\"] = add_news\n        config_parser.add_section(\"sources\")\n        config_parser.add_section(\"news\")\n        config_parser.add_section(\"news_sources\")\n        config_parser.add_section(\"news_sources_sources\")\n        config_parser.add_section(\"news_sources_sources_sources\")\n        config_parser.add_section(\"news_sources_sources_sources_sources\")\n        config_parser.add_section(\"news_sources_sources_sources_sources_sources\")\n        config_parser.add_section(\"news_sources_sources_sources_sources_sources_sources\")\n        config_parser.add_section(\"news_sources_sources_sources_sources_sources_sources_sources\")\n        config_parser.add_section(\"news_sources_sources_sources_sources_sources_sources_sources_sources\")\n        config_parser.add_section(\"news_sources_sources_sources_sources_sources_sources_sources_sources_sources\")\n        config_parser.add_section(\"news_sources_sources_sources_sources_sources_sources_sources_sources_sources_sources\")\n        config_parser.add_section(\"news_sources_sources_sources_sources_sources_sources_sources_sources_sources_sources_sources\")\n        config_parser.add_section(\"news_sources_sources_sources_sources_sources_sources_sources_sources_sources_sources_sources_sources\")\n        config_parser.add_section(\"news_sources_sources_sources_sources_sources_sources_sources_sources_sources_sources_sources_sources_sources\")\n        config_parser.add_section(\"news_sources_sources_sources_sources_sources_sources_sources_sources_sources_sources_sources_sources_sources_sources\")\n        config_parser.add_section(\"news_sources_sources", "idx": 1626}
{"namespace": "twtxt.config.Config.following", "completion": "        following = []\n        if \"following\" in self.cfg:\n            for section in self.cfg[\"following\"]:\n                if section == \"twtxt\":\n                    following.append(Source(section, self.cfg[\"following\"][section]))\n                else:\n                    logger.debug(\"Unknown section in following: %s\", section)\n        else:\n            logger.debug(\"No following section in config file.\")\n        return following\n", "idx": 1627}
{"namespace": "twtxt.config.Config.options", "completion": "        options = {}\n        try:\n            for (option, value) in self.cfg.items(\"twtxt\"):\n                options[option] = value\n        except configparser.NoSectionError as e:\n            logger.debug(e)\n\n        return options\n", "idx": 1628}
{"namespace": "twtxt.models.Tweet.relative_datetime", "completion": "        delta = datetime.now(tzlocal()) - self.created_at\n        return humanize.naturaldelta(delta)\n", "idx": 1629}
{"namespace": "twtxt.mentions.format_mentions", "completion": "    def handle_mention(match):\n        return format_callback(match.group(1), match.group(2))\n\n    return mention_re.sub(handle_mention, text)", "idx": 1630}
{"namespace": "twtxt.parser.parse_tweets", "completion": "    tweets = []\n\n    for raw_tweet in raw_tweets:\n\n        try:\n            tweet = parse_tweet(raw_tweet, source, now)\n            tweets.append(tweet)\n        except Exception as e:\n            logger.error(e)\n\n    return tweets\n\n", "idx": 1631}
{"namespace": "wikipediaapi.Wikipedia.page", "completion": "        if unquote:\n            title = parse.unquote(title)\n\n        return WikipediaPage(\n            self,\n            title,\n            namespace2int(ns),\n        )\n", "idx": 1632}
{"namespace": "wikipediaapi.Wikipedia.article", "completion": "        return self.page(title, ns, unquote)\n", "idx": 1633}
{"namespace": "wikipediaapi.WikipediaPageSection.__repr__", "completion": "        return (\n            \"WikipediaPageSection(title=\"\n            + self.title\n            + \", level=\"\n            + str(self.level)\n            + \", text=\"\n            + self.text\n            + \", sections=\"\n            + str(len(self.sections))\n            + \")\"\n        )\n\n", "idx": 1634}
{"namespace": "wikipediaapi.WikipediaPage.sections", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        return self._section\n", "idx": 1635}
{"namespace": "wikipediaapi.WikipediaPage.section_by_title", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        sections = self._section_mapping.get(title, [])\n        if sections:\n            return sections[-1]\n        return None\n", "idx": 1636}
{"namespace": "wikipediaapi.WikipediaPage.sections_by_title", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        sections = self._section_mapping.get(title)\n        if sections:\n            return sections\n        return []\n", "idx": 1637}
{"namespace": "wikipediaapi.WikipediaPage.text", "completion": "        text = self.summary\n        for section in self.sections:\n            text += section.full_text()\n        return text\n", "idx": 1638}
{"namespace": "wikipediaapi.WikipediaPage.langlinks", "completion": "        if not self._called[\"langlinks\"]:\n            self._fetch(\"langlinks\")\n        return self._langlinks\n", "idx": 1639}
{"namespace": "wikipediaapi.WikipediaPage.links", "completion": "        if not self._called[\"links\"]:\n            self._fetch(\"links\")\n        return self._links\n", "idx": 1640}
{"namespace": "wikipediaapi.WikipediaPage.backlinks", "completion": "        if not self._called[\"backlinks\"]:\n            self._fetch(\"backlinks\")\n        return self._backlinks\n", "idx": 1641}
{"namespace": "wikipediaapi.WikipediaPage.categorymembers", "completion": "        if not self._called[\"categorymembers\"]:\n            self._fetch(\"categorymembers\")\n        return self._categorymembers\n", "idx": 1642}
{"namespace": "wikipediaapi.WikipediaPage._fetch", "completion": "        if call not in self._called:\n            raise ValueError(f\"Unknown call: {call}\")\n\n        if self._called[call]:\n            return self\n\n        getattr(self.wiki, call)(self)\n        self._called[call] = True\n        return self\n", "idx": 1643}
{"namespace": "wikipediaapi.WikipediaPage.__repr__", "completion": "        if self.pageid != -1:\n            return \"{} (id: {}, ns: {})\".format(\n                self.title, self.pageid, self.ns\n            )\n        else:\n            return \"{} (id: ??, ns: {})\".format(self.title, self.ns)", "idx": 1644}
{"namespace": "imapclient.imapclient.IMAPClient.starttls", "completion": "        if self._starttls_done:\n            raise exceptions.IMAPClientError(\"STARTTLS already done\")\n\n        if self.ssl:\n            raise exceptions.IMAPClientError(\"Already using SSL\")\n\n        if self.stream:\n            raise exceptions.IMAPClientError(\"Can't use STARTTLS with stream\")\n\n        if ssl_context is None:\n            ssl_context = ssl_lib.create_default_context()\n\n        self._imap.starttls(ssl_context)\n        self._starttls_done = True\n        self._imap.debug = 5\n        self._imap._mesg = IMAPlibLoggerAdapter(getLogger(\"imapclient.imaplib\"), {})\n        self._set_read_timeout()\n        self._imap.select()\n", "idx": 1645}
{"namespace": "imapclient.imapclient.IMAPClient.shutdown", "completion": "        self._imap.shutdown()\n        logger.debug(\"Connection closed\")\n", "idx": 1646}
{"namespace": "imapclient.imapclient.IMAPClient.enable", "completion": "        typ, data = self._command_and_check(\"enable\", *capabilities)\n        self._check_resp(\"OK\", \"enable\", typ, data)\n        return data[0]\n", "idx": 1647}
{"namespace": "imapclient.imapclient.IMAPClient._proc_folder_list", "completion": "        folder_data = [x for x in folder_data if x]\n        folder_list = []\n        for folder in folder_data:\n            folder = folder.split(b\" \")\n            flags = []\n            for flag in folder[0].split(b\"\\\\\"):\n                if flag:\n                    flags.append(flag)\n            delimiter = folder[1]\n            name = folder[2]\n            if name.isdigit():\n                name = str(name)\n            if self.folder_encode:\n                name = decode_utf7(name)\n            folder_list.append((flags, delimiter, name))\n        return folder_list\n", "idx": 1648}
{"namespace": "imapclient.imapclient.IMAPClient.select_folder", "completion": "        folder = self._normalise_folder(folder)\n        typ, data = self._imap._simple_command(\"SELECT\", folder, readonly)\n        self._checkok(\"select\", typ, data)\n        typ, data = self._imap._untagged_response(typ, data, \"SELECT\")\n        return parse_response(data)\n", "idx": 1649}
{"namespace": "imapclient.imapclient.IMAPClient.unselect_folder", "completion": "        self._command_and_check(\"unselect\")\n        return self._process_select_response(self._imap.untagged_responses)\n", "idx": 1650}
{"namespace": "imapclient.imapclient.IMAPClient.noop", "completion": "        typ, data = self._imap.noop()\n        self._checkok(\"NOOP\", typ, data)\n        typ, data = self._imap._untagged_response(typ, data, \"NOOP\")\n        return data\n", "idx": 1651}
{"namespace": "imapclient.imapclient.IMAPClient.idle", "completion": "        tag = self._imap._command(\"IDLE\")\n        return self._consume_until_tagged_response(tag, \"IDLE\")\n", "idx": 1652}
{"namespace": "imapclient.imapclient.IMAPClient.idle_check", "completion": "        if self._idle_tag is None:\n            raise exceptions.IMAPClientError(\"IDLE mode not started\")\n\n        if POLL_SUPPORT:\n            poll_socket = self._poll_socket\n        else:\n            poll_socket = self._select_poll_socket\n\n        while True:\n            try:\n                ready = poll_socket(self._imap.sock, timeout)\n            except socket.error as e:\n                if e.errno == errno.EINTR:\n                    continue\n                raise\n\n            if ready:\n                break\n\n            # If we get here, we timed out\n            return []\n\n        # We got a response, so we can stop polling\n        self._idle_tag = None\n\n        # Read the response\n        resp = self._imap._get_response()\n        if resp is None:\n            raise exceptions.IMAPClientError(\"Unexpected IDLE response: %s\" % resp)\n\n        # Parse the response\n        return parse_response(resp)\n", "idx": 1653}
{"namespace": "imapclient.imapclient.IMAPClient.idle_done", "completion": "        self._command_and_check(\"DONE\")\n        return self._consume_until_tagged_response(self._idle_tag, \"DONE\")\n", "idx": 1654}
{"namespace": "imapclient.imapclient.IMAPClient.folder_status", "completion": "        if what is None:\n            what = ['MESSAGES', 'RECENT', 'UIDNEXT', 'UIDVALIDITY', 'UNSEEN']\n\n        folder = self._normalise_folder(folder)\n        typ, data = self._imap._simple_command(\"STATUS\", folder, what)\n        self._checkok(\"status\", typ, data)\n        typ, data = self._imap._untagged_response(typ, data, \"STATUS\")\n        return self._proc_status_response(data)\n", "idx": 1655}
{"namespace": "imapclient.imapclient.IMAPClient.sort", "completion": "        args = []\n        if charset:\n            args.extend([b\"CHARSET\", to_bytes(charset)])\n        args.extend(_normalise_search_criteria(criteria, charset))\n\n        try:\n            data = self._raw_command_untagged(b\"SORT\", args, sort_criteria)\n        except imaplib.IMAP4.error as e:\n            # Make BAD IMAP responses easier to understand to the user, with a link to the docs\n            m = re.match(r\"SORT command error: BAD \\[(.+)\\]\", str(e))\n            if m:\n                raise exceptions.InvalidCriteriaError(\n                    \"{original_msg}\\n\\n\"\n                    \"This error may have been caused by a syntax error in the criteria: \"\n                    \"{criteria}\\nPlease refer to the documentation for more information \"\n                    \"about search criteria syntax..\\n\"\n                    \"https://imapclient.readthedocs.io/en/master/#imapclient.IMAPClient.sort\".format(\n                        original_msg=m.group(1),\n                        criteria='\"%s\"' % criteria\n                        if not isinstance(criteria, list)\n                        else criteria,\n                    )\n                )\n\n            # If the exception is not from a BAD IMAP response, re-raise as-is\n            raise\n\n        return parse_message_list(data)\n", "idx": 1656}
{"namespace": "imapclient.imapclient.IMAPClient.thread", "completion": "        args = [\n            to_bytes(algorithm),\n            to_bytes(charset),\n        ]\n        args.extend(_normalise_search_criteria(criteria, charset))\n        ids = self._raw_command_untagged(b\"THREAD\", args, unpack=True)\n        return [tuple(int(i) for i in thread.split()) for thread in ids.split()]\n", "idx": 1657}
{"namespace": "imapclient.imapclient.IMAPClient.get_flags", "completion": "        # Convert the input parameter to a list of strings\n        messages = [str(message) for message in messages]\n\n        # Create a list of strings that will be used to retrieve the flags for each message\n        messages_str = \",\".join(messages)\n\n        # Retrieve the flags for each message\n        flags = self._command_and_check(\"fetch\", messages_str, \"(FLAGS)\")\n\n        # Parse the response\n        flags = parse_response(flags)\n\n        # Create a dictionary that contains the flags set for each message\n        flags_dict = {}\n        for message in flags:\n            flags_dict[message[0]] = message[1][0][1:-1].split()\n\n        return flags_dict\n", "idx": 1658}
{"namespace": "imapclient.imapclient.IMAPClient.get_gmail_labels", "completion": "        response = self.fetch(messages, [\"X-GM-LABELS\"])\n        return self._filter_fetch_dict(response, b\"X-GM-LABELS\")\n", "idx": 1659}
{"namespace": "imapclient.imapclient.IMAPClient.append", "completion": "        if not isinstance(folder, bytes):\n            folder = self._normalise_folder(folder)\n\n        if not isinstance(flags, tuple):\n            flags = tuple(flags)\n\n        if not isinstance(msg_time, datetime):\n            msg_time = None\n\n        if msg_time is not None:\n            msg_time = msg_time.astimezone(datetime.timezone.utc)\n\n        if msg_time is not None:\n            msg_time = msg_time.strftime(\"%d-%b-%Y %H:%M:%S %z\")\n\n        if self.use_uid:\n            tag = self._imap._command(\"UID\", \"APPEND\", folder, msg_time, flags, b\"(\")\n        else:\n            tag = self._imap._command(\"APPEND\", folder, msg_time, flags, b\"(\")\n\n        self._imap._send(b\"\\r\\n\" + msg + b\"\\r\\n)\")\n        typ, data = self._imap._command_complete(\"APPEND\", tag)\n        self._checkok(\"append\", typ, data)\n        return data\n", "idx": 1660}
{"namespace": "imapclient.imapclient.IMAPClient.multiappend", "completion": "        if not msgs:\n            return []\n\n        args = [\n            \"MULTIAPPEND\",\n            self._normalise_folder(folder),\n            seq_to_parenstr(msgs),\n        ]\n        if self.use_uid:\n            args.insert(0, \"UID\")\n        tag = self._imap._command(*args)\n        typ, data = self._imap._command_complete(\"MULTIAPPEND\", tag)\n        self._checkok(\"multiappend\", typ, data)\n        typ, data = self._imap._untagged_response(typ, data, \"MULTIAPPEND\")\n        return parse_response(data)\n", "idx": 1661}
{"namespace": "imapclient.imapclient.IMAPClient.expunge", "completion": "        if messages is None:\n            return self._command_and_check(\"expunge\", uid=True, unpack=True)\n        else:\n            return self._command_and_check(\n                \"expunge\",\n                join_message_ids(messages),\n                uid=True,\n                unpack=True,\n            )\n", "idx": 1662}
{"namespace": "imapclient.imapclient.IMAPClient.getacl", "completion": "        return self._command_and_check(\"GETACL\", self._normalise_folder(folder), unpack=True)\n", "idx": 1663}
{"namespace": "imapclient.imapclient.IMAPClient.setacl", "completion": "        return self._command_and_check(\n            \"setacl\", self._normalise_folder(folder), who, what\n        )\n", "idx": 1664}
{"namespace": "imapclient.imapclient.IMAPClient.get_quota_root", "completion": "        return _parse_quota_root(self._command_and_check(\"getquotaroot\", _quote(mailbox)))\n", "idx": 1665}
{"namespace": "imapclient.imapclient.IMAPClient.set_quota", "completion": "        quota_args = []\n        for quota in quotas:\n            quota_args.append(quota.resource)\n            quota_args.append(quota.usage)\n        self._command_and_check(\"setquota\", *quota_args)\n", "idx": 1666}
{"namespace": "imapclient.imapclient.IMAPClient._consume_until_tagged_response", "completion": "        untagged_responses = []\n        while True:\n            typ, data = self._imap._get_response()\n            if typ == \"untagged\":\n                untagged_responses.append(data)\n            elif typ == \"tagged\":\n                if data[0] == tag:\n                    return data, untagged_responses\n                else:\n                    raise exceptions.IMAPClientError(\n                        \"Unexpected tagged response: %s\" % to_unicode(data[0])\n                    )\n            else:\n                raise exceptions.IMAPClientError(\n                    \"Unexpected response: %s\" % to_unicode(data[0])\n                )\n", "idx": 1667}
{"namespace": "imapclient.imapclient._normalise_search_criteria", "completion": "    if not criteria:\n        raise exceptions.InvalidCriteriaError(\"No search criteria specified\")\n\n    if isinstance(criteria, (str, bytes)):\n        criteria = [criteria]\n\n    if isinstance(criteria, (int, datetime, date)):\n        criteria = [criteria]\n\n    if isinstance(criteria, (list, tuple)):\n        criteria = list(criteria)\n\n    if isinstance(criteria, bytes):\n        criteria = [criteria]\n\n    if charset is None:\n        charset = \"us-ascii\"\n\n    criteria = [to_bytes(c, charset) for c in criteria]\n\n    return criteria\n\n", "idx": 1668}
{"namespace": "imapclient.response_lexer.TokenSource.current_literal", "completion": "        if self.lex.current_source is not None:\n            return self.lex.current_source.literal\n        return None\n", "idx": 1669}
{"namespace": "imapclient.imap_utf7.decode", "completion": "    if not isinstance(s, (bytes, str)):\n        return s\n\n    # The result string\n    res = \"\"\n\n    # The buffer for the base64 characters\n    b64_buffer = []\n\n    # The current state of the decoder\n    state = \"ASCII\"\n\n    # The current character\n    c = None\n\n    # The current character's ordinal value\n    o = None\n\n    # The current character's base64 value\n    b64_val = None\n\n    # The current character's base64 value\n    b64_val = None\n\n    # The current character's base64 value\n    b64_val = None\n\n    # The current character's base64 value\n    b64_val = None\n\n    # The current character's base64 value\n    b64_val = None\n\n    # The current character's base64 value\n    b64_val = None\n\n    # The current character's base64 value\n    b64_val = None\n\n    # The current character's base64 value\n    b64_val = None\n\n    # The current character's base64 value\n    b64_val = None\n\n    # The current character's base64 value\n    b64_val = None\n\n    # The current character's base64 value\n    b64_val = None\n\n    # The current character's base64 value\n    b64_val = None\n\n    # The current character's base64 value\n    b64_val = None\n\n    # The current character's base64 value\n    b64_val = None\n\n    # The current character's base64 value\n    b64_val = None\n\n    # The current character's base64 value\n    b64_val = None\n\n    # The current character's base64 value\n    b64_val = None\n\n    # The current character's base64 value\n    b64_val = None\n\n    # The current character's base64 value\n    b64_val = None\n\n    # The current character's base64 value\n    b64_val = None\n\n    # The current character's base64 value\n    b64_val = None\n\n    # The current character's base64 value\n    b64_val = None\n\n    # The current character's base64 value\n    b64_val = None\n\n    # The current character's base64", "idx": 1670}
{"namespace": "imapclient.fixed_offset.FixedOffset.for_system", "completion": "        # Get the current time zone\n        current_tz = time.tzname(0)\n\n        # Get the current time zone offset\n        current_tz_offset = time.timezone\n\n        # Get the current DST offset\n        current_dst_offset = time.altzone\n\n        # Get the current DST status\n        current_dst_status = time.daylight\n\n        # Get the current DST status\n        current_dst_status = time.daylight\n\n        # Get the current DST status\n        current_dst_status = time.daylight\n\n        # Get the current DST status\n        current_dst_status = time.daylight\n\n        # Get the current DST status\n        current_dst_status = time.daylight\n\n        # Get the current DST status\n        current_dst_status = time.daylight\n\n        # Get the current DST status\n        current_dst_status = time.daylight\n\n        # Get the current DST status\n        current_dst_status = time.daylight\n\n        # Get the current DST status\n        current_dst_status = time.daylight\n\n        # Get the current DST status\n        current_dst_status = time.daylight\n\n        # Get the current DST status\n        current_dst_status = time.daylight\n\n        # Get the current DST status\n        current_dst_status = time.daylight\n\n        # Get the current DST status\n        current_dst_status = time.daylight\n\n        # Get the current DST status\n        current_dst_status = time.daylight\n\n        # Get the current DST status\n        current_dst_status = time.daylight\n\n        # Get the current DST status\n        current_dst_status = time.daylight\n\n        # Get the current DST status\n        current_dst_status = time.daylight\n\n        # Get the current DST status\n        current_dst_status = time.daylight\n\n        # Get the current DST status\n        current_dst_status = time.daylight\n\n        # Get the current DST status\n        current_dst_status = time.daylight\n\n        # Get the current DST status\n        current_dst_status = time.daylight\n\n        # Get the current DST status\n        current_dst_status = time.daylight\n\n        # Get the current DST status\n        current_dst_status = time.daylight\n\n        # Get the current DST", "idx": 1671}
{"namespace": "imapclient.datetime_util.parse_to_datetime", "completion": "    # Parse the date\n    date = parsedate_tz(timestamp)\n\n    # Convert to a datetime object\n    dt = datetime(date[0], date[1], date[2], date[3], date[4], date[5], date[6])\n\n    # Adjust to the local time\n    if normalise:\n        dt = dt.replace(tzinfo=FixedOffset(0))\n\n    return dt\n\n", "idx": 1672}
{"namespace": "imapclient.datetime_util.datetime_to_INTERNALDATE", "completion": "    if dt.tzinfo is None:\n        dt = datetime_to_native(dt)\n\n    return dt.strftime(\"%Y%m%d%H%M%S\")\n\n", "idx": 1673}
{"namespace": "imapclient.datetime_util.format_criteria_date", "completion": "    if isinstance(dt, datetime):\n        dt = dt.date()\n\n    return dt.strftime(\"%d-%b-%Y\").encode(\"utf-8\")", "idx": 1674}
{"namespace": "imapclient.util.assert_imap_protocol", "completion": "    if not condition:\n        if message is None:\n            message = b\"Server replied with a response that violates the IMAP protocol\"\n        raise ProtocolError(message)\n\n", "idx": 1675}
{"namespace": "ehforwarderbot.utils.get_config_path", "completion": "    profile = coordinator.profile\n    if module_id is None:\n        module_id = profile\n    config_path = get_base_path() / 'profiles' / profile / module_id / 'config.' + ext\n    if not config_path.exists():\n        config_path.mkdir(parents=True)\n    return config_path\n\n", "idx": 1676}
{"namespace": "ehforwarderbot.utils.get_custom_modules_path", "completion": "    custom_modules_path = get_base_path() / 'modules'\n    if not custom_modules_path.exists():\n        custom_modules_path.mkdir(parents=True)\n    return custom_modules_path\n\n", "idx": 1677}
{"namespace": "ehforwarderbot.chat.Chat.add_member", "completion": "        if id:\n            warnings.warn(\"`id` argument is deprecated, use `uid` instead.\", DeprecationWarning)\n            uid = id\n        m = ChatMember(chat=self, name=name, alias=alias, uid=uid, id=id,\n                       vendor_specific=vendor_specific, description=description,\n                       middleware=middleware)\n        self.members.append(m)\n        return m\n", "idx": 1678}
{"namespace": "ehforwarderbot.chat.Chat.add_system_member", "completion": "        if id:\n            warnings.warn(\"`id` argument is deprecated, use `uid` instead.\", DeprecationWarning)\n            uid = uid or id\n        member = SystemChatMember(self, name=name, alias=alias, id=id, uid=uid,\n                                  vendor_specific=vendor_specific, description=description,\n                                  middleware=middleware)\n        self.members.append(member)\n        return member\n", "idx": 1679}
{"namespace": "ehforwarderbot.chat.PrivateChat.verify", "completion": "        super().verify()\n        assert len(self.members) == 2, \"PrivateChat must have exactly 2 members\"\n        assert isinstance(self.members[0], ChatMember), \"First member of PrivateChat must be a ChatMember\"\n        assert isinstance(self.members[1], ChatMember), \"Second member of PrivateChat must be a ChatMember\"\n", "idx": 1680}
{"namespace": "telethon.utils.get_inner_text", "completion": "    result = []\n    i = 0\n    for entity in entities:\n        if entity.type == 'bold':\n            result.append(text[i:entity.offset])\n            result.append('**')\n            i = entity.offset + entity.length\n        elif entity.type == 'italic':\n            result.append(text[i:entity.offset])\n            result.append('*')\n            i = entity.offset + entity.length\n        elif entity.type == 'code':\n            result.append(text[i:entity.offset])\n            result.append('`')\n            i = entity.offset + entity.length\n        elif entity.type == 'pre':\n            result.append(text[i:entity.offset])\n            result.append('```')\n            i = entity.offset + entity.length\n        elif entity.type == 'text_link':\n            result.append(text[i:entity.offset])\n            result.append('[')\n            i = entity.offset + entity.length\n        elif entity.type == 'text_mention':\n            result.append(text[i:entity.offset])\n            result.append('@')\n            i = entity.offset + entity.length\n        elif entity.type == 'text_mention_name':\n            result.append(text[i:entity.offset])\n            result.append('@')\n            i = entity.offset + entity.length\n        elif entity.type == 'text_phone_number':\n            result.append(text[i:entity.offset])\n            result.append('(')\n            i = entity.offset + entity.length\n        elif entity.type == 'text_email':\n            result.append(text[i:entity.offset])\n            result.append('(')\n            i = entity.offset + entity.length\n        elif entity.type == 'text_custom_emoji':\n            result.append(text[i:entity.offset])\n            result.append('(')\n            i = entity.offset + entity.length\n        elif entity.type == 'text_hashtag':\n            result.append(text[i:entity.offset])\n            result.append('#')\n            i = entity.offset + entity.length\n        elif entity.type == 'text_bold':\n            result.append(text[i:entity.offset])\n            result.append('**')\n            i = entity.offset + entity.", "idx": 1681}
{"namespace": "telethon.extensions.html.parse", "completion": "    parser = HTMLToTelegramParser()\n    parser.feed(html)\n    parser.close()\n\n    text = parser.text\n    entities = parser.entities\n\n    # Strip the text\n    text = escape(text)\n    text = add_surrogate(text)\n\n    # Strip the entities\n    for entity in entities:\n        entity.offset = add_surrogate(text[:entity.offset])\n        entity.length = add_surrogate(text[entity.offset:entity.offset + entity.length])\n\n    return text, entities", "idx": 1682}
{"namespace": "telethon.extensions.html.unparse", "completion": "    if not text:\n        return text\n\n    if not entities:\n        return escape(text)\n\n    text = add_surrogate(text)\n    entities = sorted(entities, key=lambda e: e.offset)\n\n    html = ''\n    for entity in entities:\n        html += escape(text[entity.offset:entity.offset + entity.length])\n        try:\n            formatter = ENTITY_TO_FORMATTER[type(entity)]\n        except KeyError:\n            continue\n        html += formatter(entity, entity.text)\n\n    return del_surrogate(html)", "idx": 1683}
{"namespace": "telethon.crypto.rsa.encrypt", "completion": "    global _server_keys\n    if fingerprint not in _server_keys:\n        return None\n    key, old = _server_keys[fingerprint]\n    if use_old and old:\n        key = old\n    return rsa.core.encrypt(data, key)", "idx": 1684}
{"namespace": "hbmqtt.codecs.encode_string", "completion": "    if not string:\n        return b''\n    encoded_string = string.encode(encoding='utf-8')\n    return pack(\"!H\", len(encoded_string)) + encoded_string\n\n", "idx": 1685}
{"namespace": "hbmqtt.plugins.manager.PluginManager.get_plugin", "completion": "        for plugin in self._plugins:\n            if plugin.name == name:\n                return plugin\n        return None\n", "idx": 1686}
{"namespace": "pysimplesoap.simplexml.SimpleXMLElement.add_child", "completion": "        if ns is True:\n            ns = self.__ns\n        elif ns is False:\n            ns = None\n\n        if ns is not None:\n            name = \"%s:%s\" % (ns, name)\n\n        if text is not None:\n            if isinstance(text, basestring):\n                text = xml.dom.minidom.CDATASection(text)\n            child = self.__document.createElementNS(ns, name)\n            child.appendChild(text)\n        else:\n            child = self.__document.createElementNS(ns, name)\n\n        self.__elements[-1].appendChild(child)\n        return SimpleXMLElement(elements=self.__elements, document=self.__document,\n                                namespace=ns, prefix=self.__prefix,\n                                namespaces_map=self.__namespaces_map, jetty=self.__jetty)\n", "idx": 1687}
{"namespace": "pysimplesoap.simplexml.SimpleXMLElement.as_xml", "completion": "        if filename is not None:\n            with open(filename, \"w\") as f:\n                f.write(self.as_xml(pretty=pretty))\n        if pretty:\n            return self.__document.toprettyxml(indent=\"  \")\n        else:\n            return self.__document.toxml()\n", "idx": 1688}
{"namespace": "twilio.base.deserialize.iso8601_date", "completion": "    try:\n        return datetime.datetime.strptime(s, ISO8601_DATE_FORMAT).date()\n    except ValueError:\n        return s\n\n", "idx": 1689}
{"namespace": "twilio.base.deserialize.iso8601_datetime", "completion": "    try:\n        return (\n            datetime.datetime.strptime(s, ISO8601_DATETIME_FORMAT)\n            .replace(tzinfo=datetime.timezone.utc)\n        )\n    except (TypeError, ValueError):\n        return s\n\n", "idx": 1690}
{"namespace": "twilio.base.serialize.iso8601_date", "completion": "    if isinstance(d, str):\n\n        return d\n\n    elif isinstance(d, datetime.datetime):\n\n        return d.strftime(\"%Y-%m-%d\")\n\n    elif isinstance(d, datetime.date):\n\n        return d.strftime(\"%Y-%m-%d\")\n\n    else:\n\n        return None\n", "idx": 1691}
{"namespace": "twilio.base.serialize.iso8601_datetime", "completion": "    if d == values.unset:\n        return d\n    elif isinstance(d, datetime.datetime):\n        return d.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n    elif isinstance(d, datetime.date):\n        return d.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n    elif isinstance(d, str):\n        return d\n\n\n", "idx": 1692}
{"namespace": "twilio.base.serialize.prefixed_collapsible_map", "completion": "    if m == values.unset:\n        return m\n    elif isinstance(m, dict):\n        return {prefix + k: v for k, v in m.items()}\n    else:\n        return m\n\n", "idx": 1693}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.dial", "completion": "        return self.nest(\n            Dial(\n                number=number,\n                action=action,\n                method=method,\n                timeout=timeout,\n                hangup_on_star=hangup_on_star,\n                time_limit=time_limit,\n                caller_id=caller_id,\n                record=record,\n                trim=trim,\n                recording_status_callback=recording_status_callback,\n                recording_status_callback_method=recording_status_callback_method,\n                recording_status_callback_event=recording_status_callback_event,\n                answer_on_bridge=answer_on_bridge,\n                ring_tone=ring_tone,\n                recording_track=recording_track,\n                sequential=sequential,\n                refer_url=refer_url,\n                refer_method=refer_method,\n                **kwargs\n            )\n        )\n", "idx": 1694}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.enqueue", "completion": "        return self.nest(\n            Enqueue(\n                name=name,\n                action=action,\n                max_queue_size=max_queue_size,\n                method=method,\n                wait_url=wait_url,\n                wait_url_method=wait_url_method,\n                workflow_sid=workflow_sid,\n                **kwargs\n            )\n        )\n", "idx": 1695}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.gather", "completion": "        return self.nest(\n            Gather(\n                input=input,\n                action=action,\n                method=method,\n                timeout=timeout,\n                speech_timeout=speech_timeout,\n                max_speech_time=max_speech_time,\n                profanity_filter=profanity_filter,\n                finish_on_key=finish_on_key,\n                num_digits=num_digits,\n                partial_result_callback=partial_result_callback,\n                partial_result_callback_method=partial_result_callback_method,\n                language=language,\n                hints=hints,\n                barge_in=barge_in,\n                debug=debug,\n                action_on_empty_result=action_on_empty_result,\n                speech_model=speech_model,\n                enhanced=enhanced,\n                **kwargs\n            )\n        )\n", "idx": 1696}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.say", "completion": "        return self.nest(Say(message=message, voice=voice, loop=loop, language=language, **kwargs))\n", "idx": 1697}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.sms", "completion": "        return self.nest(\n            Sms(\n                message,\n                to=to,\n                from_=from_,\n                action=action,\n                method=method,\n                status_callback=status_callback,\n                **kwargs\n            )\n        )\n", "idx": 1698}
{"namespace": "twilio.twiml.voice_response.Gather.say", "completion": "        return self.nest(\n            Say(message=message, voice=voice, loop=loop, language=language, **kwargs)\n        )\n", "idx": 1699}
{"namespace": "twilio.twiml.voice_response.Dial.client", "completion": "        return self.nest(\n            Client(\n                identity=identity,\n                url=url,\n                method=method,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                **kwargs\n            )\n        )\n", "idx": 1700}
{"namespace": "twilio.twiml.voice_response.Dial.conference", "completion": "        return self.nest(\n            Conference(\n                name,\n                muted=muted,\n                beep=beep,\n                start_conference_on_enter=start_conference_on_enter,\n                end_conference_on_exit=end_conference_on_exit,\n                wait_url=wait_url,\n                wait_method=wait_method,\n                max_participants=max_participants,\n                record=record,\n                region=region,\n                coach=coach,\n                trim=trim,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                recording_status_callback=recording_status_callback,\n                recording_status_callback_method=recording_status_callback_method,\n                recording_status_callback_event=recording_status_callback_event,\n                event_callback_url=event_callback_url,\n                jitter_buffer_size=jitter_buffer_size,\n                participant_label=participant_label,\n                **kwargs\n            )\n        )\n", "idx": 1701}
{"namespace": "twilio.twiml.voice_response.Dial.queue", "completion": "        return self.nest(\n            Queue(\n                name,\n                url=url,\n                method=method,\n                reservation_sid=reservation_sid,\n                post_work_activity_sid=post_work_activity_sid,\n                **kwargs\n            )\n        )\n", "idx": 1702}
{"namespace": "twilio.twiml.voice_response.Dial.sip", "completion": "        return self.nest(\n            Sip(\n                sip_url,\n                username=username,\n                password=,\n                url=url,\n                method=method,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                machine_detection=machine_detection,\n                amd_status_callback_method=amd_status_callback_method,\n                amd_status_callback=amd_status_callback,\n                machine_detection_timeout=machine_detection_timeout,\n                machine_detection_speech_threshold=machine_detection_speech_threshold,\n                machine_detection_speech_end_threshold=machine_detection_speech_end_threshold,\n                machine_detection_silence_timeout=machine_detection_silence_timeout,\n                **kwargs\n            )\n        )\n", "idx": 1703}
{"namespace": "twilio.twiml.messaging_response.MessagingResponse.message", "completion": "        message = Message(\n            body=body,\n            to=to,\n            from_=from_,\n            action=action,\n            method=method,\n            status_callback=status_callback,\n            **kwargs\n        )\n\n        self.append(message)\n\n        return message\n\n", "idx": 1704}
{"namespace": "twilio.twiml.TwiML.append", "completion": "        if not isinstance(verb, TwiML):\n            raise TwiMLException(\"Verb must be an instance of TwiML.\")\n\n        self.verbs.append(verb)\n        return self\n", "idx": 1705}
{"namespace": "twilio.jwt.Jwt.to_jwt", "completion": "        if ttl is None:\n            ttl = self.ttl\n\n        payload = self.payload.copy()\n        payload[\"exp\"] = int(time.time()) + ttl\n\n        return jwt_lib.encode(payload, self.secret_key, self.algorithm, self.headers)\n", "idx": 1706}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_client_outgoing", "completion": "        self.capabilities[application_sid] = {\n            \"client_outgoing\": {\n                \"application_sid\": application_sid,\n                \"parameters\": kwargs,\n            }\n        }\n", "idx": 1707}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_client_incoming", "completion": "        self.client_name = client_name\n        self.capabilities[\"incoming\"] = ScopeURI(\"client\", \"incoming\", {\"clientName\": client_name})\n", "idx": 1708}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_event_stream", "completion": "        scope = ScopeURI(\"client\", \"events\", {})\n        if kwargs:\n            scope.add_param(\"appParams\", urlencode(kwargs, doseq=True))\n\n        self.capabilities[\"events\"] = scope\n", "idx": 1709}
{"namespace": "twilio.jwt.client.ClientCapabilityToken._generate_payload", "completion": "        if \"outgoing\" in self.capabilities and self.client_name is not None:\n            self.capabilities[\"outgoing\"].add_param(\"clientName\", self.client_name)\n\n        payload = []\n        for capability in self.capabilities:\n            payload.append(str(self.capabilities[capability]))\n\n        return {\"scope\": \" \".join(payload)}", "idx": 1710}
{"namespace": "twilio.jwt.client.ScopeURI.to_payload", "completion": "        if self.params:\n            params = \"?\" + urlencode(sorted(self.params.items()), doseq=True)\n        else:\n            params = \"\"\n\n        return \"scope:{}:{}{}\".format(self.service, self.privilege, params)", "idx": 1711}
{"namespace": "twilio.jwt.access_token.AccessToken.add_grant", "completion": "        if not isinstance(grant, AccessTokenGrant):\n            raise ValueError(\"Grant must be an instance of AccessTokenGrant.\")\n\n        self.grants.append(grant)\n", "idx": 1712}
{"namespace": "twilio.jwt.taskrouter.capabilities.WorkerCapabilityToken.allow_update_activities", "completion": "        self._make_policy(self.workspace_url + \"/Activities\", \"POST\", True,\n                          post_filter={\"ActivitySid\": {\"required\": True}})\n", "idx": 1713}
{"namespace": "zulipterminal.platform_code.successful_GUI_return_code", "completion": "    if PLATFORM == \"WSL\":\n        return 1\n    else:\n        return 0", "idx": 1714}
{"namespace": "zulipterminal.platform_code.normalized_file_path", "completion": "    if PLATFORM == \"WSL\":\n        return path.replace(\"/\", \"\\\\\")\n    return path", "idx": 1715}
{"namespace": "zulipterminal.helper.canonicalize_color", "completion": "    if color.startswith(\"#\"):\n        color = color[1:]\n\n    if len(color) == 3:\n        color = \"\".join([c * 2 for c in color])\n\n    if len(color) == 6:\n        color = color.lower()\n\n    return color\n\n", "idx": 1716}
{"namespace": "zulipterminal.helper.get_unused_fence", "completion": "    pattern = r\"^(\\s*)`{3,}\"\n    matches = findall(pattern, content)\n    if matches:\n        max_length = max(len(match) for match in matches)\n        return \"`\" * (max_length + 1)\n    else:\n        return \"`\"\n\n", "idx": 1717}
{"namespace": "zulipterminal.helper.open_media", "completion": "    command = [tool, media_path]\n    try:\n        subprocess.run(command, check=True)\n    except subprocess.CalledProcessError as e:\n        controller.report_error(\n            [\n                \"Error opening media file. Please check if the media file is supported by the tool you are using.\"\n            ]\n        )\n        return\n    controller.report_success([\" Opened \", (\"bold\", media_path)])\n\n", "idx": 1718}
{"namespace": "zulipterminal.server_url.encode_stream", "completion": "    # `safe` has a default value of \"/\", but we want those encoded, too.\n    return urllib.parse.quote(stream_name, safe=b\"\").replace(\".\", \"%2E\").replace(\"%\", \".\")\n\n", "idx": 1719}
{"namespace": "zulipterminal.server_url.near_message_url", "completion": "    if message[\"type\"] == \"stream\":\n        return near_stream_message_url(server_url, message)\n    else:\n        return near_pm_message_url(server_url, message)", "idx": 1720}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.update_recipients", "completion": "        # Extract the recipient emails from the input text\n        recipient_emails = [\n            match_user_name_and_email(email) for email in write_box.edit_text.split()\n        ]\n\n        # Set the recipient user IDs in the WriteBox instance\n        self.recipient_user_ids = [\n            self.model.email_user_id_dict[email] for email in recipient_emails\n        ]\n\n        # Update the typing recipient user IDs\n        self.typing_recipient_user_ids = [\n            user_id\n            for user_id in self.recipient_user_ids\n            if user_id != self.model.user_id\n        ]\n", "idx": 1721}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.stream_box_view", "completion": "        self._setup_common_stream_compose(stream_id, caption, title)\n\n        self.stream_write_box = ReadlineEdit(\n            edit_text=caption, max_char=self.model.max_stream_name_length\n        )\n        self.stream_write_box.enable_autocomplete(\n            func=self._stream_box_autocomplete,\n            key=primary_key_for_command(\"AUTOCOMPLETE\"),\n            key_reverse=primary_key_for_command(\"AUTOCOMPLETE_REVERSE\"),\n        )\n        self.stream_write_box.set_completer_delims(\"\")\n\n        urwid.connect_signal(self.stream_write_box, \"change\", self._update_stream_marker)\n", "idx": 1722}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.stream_box_edit_view", "completion": "        self.stream_write_box = ReadlineEdit(\n            edit_text=caption, max_char=self.model.max_stream_name_length\n        )\n        self.stream_write_box.enable_autocomplete(\n            func=self._stream_box_autocomplete,\n            key=primary_key_for_command(\"AUTOCOMPLETE\"),\n            key_reverse=primary_key_for_command(\"AUTOCOMPLETE_REVERSE\"),\n        )\n        self.stream_write_box.set_completer_delims(\"\")\n        self._setup_common_stream_compose(stream_id, caption, title)\n\n        # Use and set a callback to set the stream marker\n        self._set_stream_write_box_style(None, caption)\n        urwid.connect_signal(\n            self.stream_write_box, \"change\", self._set_stream_write_box_style\n        )\n\n        # Add an edit mode button to the header write box\n        edit_mode_button = EditModeButton(\n            self.view.controller,\n            self.view.controller.edit_mode_button_callback,\n            self.view.controller.edit_mode_button_callback_reverse,\n        )\n        self.header_write_box.contents[0] = (\n            (\"pack\", urwid.Text((\"default\", \"?\"))),\n            self.stream_write_box,\n            (\"pack\", urwid.Text(STREAM_TOPIC_SEPARATOR)),\n            self.title_write_box,\n            (\"pack\", edit_mode_button),\n        )\n", "idx": 1723}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._set_stream_write_box_style", "completion": "        if new_text:\n            stream_info = self.model.get_stream_info_by_name(new_text)\n            if stream_info:\n                stream_id = stream_info[\"stream_id\"]\n                stream_color = stream_info[\"color\"]\n                stream_marker = stream_info[\"stream_marker\"]\n                self.header_write_box.widget_list[0] = urwid.Text(stream_marker)\n                self.header_write_box.widget_list[1] = urwid.Text(\n                    new_text,\n                    (\"stream_color\", stream_color),\n                )\n            else:\n                self.header_write_box.widget_list[0] = urwid.Text(\n                    INVALID_MARKER, (\"invalid_marker\", \"invalid_marker\")\n                )\n                self.header_write_box.widget_list[1] = urwid.Text(\n                    new_text, (\"invalid_marker\", \"invalid_marker\")\n                )\n        else:\n            self.header_write_box.widget_list[0] = urwid.Text(\n                INVALID_MARKER, (\"invalid_marker\", \"invalid_marker\")\n            )\n            self.header_write_box.widget_list[1] = urwid.Text(\n                INVALID_MARKER, (\"invalid_marker\", \"invalid_marker\")\n            )\n", "idx": 1724}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._to_box_autocomplete", "completion": "        # Get the list of users from the view.\n        users = self.view.model.user_dict\n\n        # Split the text by comma and get the most recent recipient for autocomplete.\n        recipients = text.split(\",\")\n        latest_recipient = recipients[-1].strip()\n\n        # Find the users that match the latest text.\n        matching_users = [\n            user\n            for user in users.values()\n            if latest_recipient in user[\"full_name\"]\n        ]\n\n        # Append the autocompleted recipients to the string containing the previous recipients.\n        autocompleted_recipients = \", \".join(\n            [\n                f\"{user['full_name']} <{user['email']}>\"\n                for user in matching_users\n            ]\n        )\n        recipients.append(autocompleted_recipients)\n\n        # Get the full names of the matching users.\n        full_names = [user[\"full_name\"] for user in matching_users]\n\n        # Process the typeaheads using the updated recipients, state, and user names.\n        return self.view.controller.process_typeahead(\n            recipients, state, full_names\n        )\n", "idx": 1725}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._topic_box_autocomplete", "completion": "        # Get the list of topics in the stream\n        topics = self.model.get_topics_in_stream(self.stream_id)\n\n        # Match the input text with the list of topics\n        matching_topics = [\n            topic for topic in topics if match_topics(topic, text)\n        ]\n\n        # Generate typeaheads for the matching topics\n        typeaheads = [\n            f\"{topic}{get_unused_fence(topic)}\" for topic in matching_topics\n        ]\n\n        # Process the typeaheads and return them as suggestions\n        return self._process_typeaheads(typeaheads, state)\n", "idx": 1726}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._stream_box_autocomplete", "completion": "        stream_names = self.model.streams_in_narrow\n        stream_typeaheads = match_stream(stream_names, text)\n\n        # Typeaheads and suggestions are the same.\n        return self._process_typeaheads(stream_typeaheads, state, stream_typeaheads)\n", "idx": 1727}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.generic_autocomplete", "completion": "        # Check for specific prefixes in the text.\n        if text.startswith(\"stream:\"):\n            return self._stream_box_autocomplete(text[7:], state)\n        elif text.startswith(\"topic:\"):\n            return self._topic_box_autocomplete(text[6:], state)\n        elif text.startswith(\"to:\"):\n            return self._to_box_autocomplete(text[3:], state)\n        else:\n            return None\n", "idx": 1728}
{"namespace": "zulipterminal.ui_tools.boxes.PanelSearchBox.reset_search_text", "completion": "        self.set_caption(self.search_text)\n        self.set_edit_text(\"\")\n", "idx": 1729}
{"namespace": "zulipterminal.ui_tools.boxes.PanelSearchBox.valid_char", "completion": "        if self.edit_text:\n            return super().valid_char(ch)\n        else:\n            return (\n                ch.isprintable()\n                and not ch.isspace()\n                and not ch.isspace()\n                and not ch.isspace()\n            )\n", "idx": 1730}
{"namespace": "zulipterminal.ui_tools.utils.is_muted", "completion": "    if msg.get(\"type\") == \"private\":\n        return False\n    if model.narrow:\n        return False\n    if msg.get(\"stream_id\") in model.muted_streams:\n        return True\n    if msg.get(\"topic_id\") in model.muted_topics:\n        return True\n    return False\n\n\n", "idx": 1731}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.update_count", "completion": "        if text_color is None:\n            if count > 0:\n                text_color = \"light red\"\n            else:\n                text_color = \"dark gray\"\n\n        count_text = str(count)\n        if self.count_style == \"count_only\":\n            count_text = f\"({count_text})\"\n        elif self.count_style == \"count_prefix\":\n            count_text = f\"{self.prefix_character}{count_text}\"\n        elif self.count_style == \"count_suffix\":\n            count_text = f\"{count_text}{self.prefix_character}\"\n        elif self.count_style == \"count_both\":\n            count_text = f\"{self.prefix_character}{count_text}{self.prefix_character}\"\n\n        self.count = count\n        self.button_prefix.set_text(count_text)\n        self.button_prefix.set_text_color(text_color)\n        self.button_suffix.set_text(count_text)\n        self.button_suffix.set_text_color(text_color)\n", "idx": 1732}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.update_widget", "completion": "        if self.prefix_character is not None:\n            self.button_prefix.set_text(self.prefix_character)\n            self.button_prefix.set_text_color(text_color)\n\n        self._label.set_text(count_text[1])\n        self._label.set_text_color(text_color)\n\n        if self.prefix_character is not None:\n            self.button_suffix.set_text(self.prefix_character)\n            self.button_suffix.set_text_color(text_color)\n\n        self._w.set_attr_map({None: text_color})\n\n        return self._w\n", "idx": 1733}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.keypress", "completion": "        if key == \"enter\":\n            self.activate(key)\n            return None\n        else:\n            return super().keypress(size, key)\n\n", "idx": 1734}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._parse_narrow_link", "completion": "        from zulipterminal.config.regexes import (\n            REGEX_INTERNAL_LINK_NARROW_STREAM,\n            REGEX_INTERNAL_LINK_NARROW_STREAM_TOPIC,\n        )\n\n        parsed_link = ParsedNarrowLink()\n        parsed_link[\"narrow\"] = \"stream\"\n        parsed_link[\"stream\"] = cls._decode_stream_data(link.split(\"/\")[-1])\n        parsed_link[\"message_id\"] = cls._decode_message_id(link.split(\"/\")[-1])\n\n        if re.match(REGEX_INTERNAL_LINK_NARROW_STREAM, link):\n            parsed_link[\"topic_name\"] = None\n        elif re.match(REGEX_INTERNAL_LINK_NARROW_STREAM_TOPIC, link):\n            parsed_link[\"topic_name\"] = link.split(\"/\")[-1]\n        else:\n            parsed_link[\"topic_name\"] = None\n\n        return parsed_link\n", "idx": 1735}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._validate_and_patch_stream_data", "completion": "        stream_data = parsed_link[\"stream\"]\n        stream_id = stream_data.get(\"stream_id\")\n        stream_name = stream_data.get(\"stream_name\")\n        if stream_id is not None:\n            if not self.model.is_subscribed_to_stream(stream_id):\n                return f\"Stream {stream_name} is not subscribed to by you.\"\n        else:\n            stream_id = self.model.get_stream_id_by_name(stream_name)\n            if stream_id is None:\n                return f\"Stream {stream_name} does not exist.\"\n            if not self.model.is_subscribed_to_stream(stream_id):\n                return f\"Stream {stream_name} is not subscribed to by you.\"\n            stream_data[\"stream_id\"] = stream_id\n\n        return \"\"\n", "idx": 1736}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._validate_narrow_link", "completion": "        model = self.model\n        # Validate stream data.\n        validation_error = self._validate_and_patch_stream_data(parsed_link)\n        if validation_error:\n            return validation_error\n\n        # Validate message ID.\n        if parsed_link[\"message_id\"] is not None:\n            if not model.is_valid_message_id(parsed_link[\"message_id\"]):\n                return \"The message ID seems to be invalid\"\n\n        # Validate topic name.\n        if parsed_link[\"topic_name\"] and not model.is_valid_topic_name(\n            parsed_link[\"topic_name\"]\n        ):\n            return \"The topic name seems to be invalid\"\n\n        return \"\"\n", "idx": 1737}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._switch_narrow_to", "completion": "        if parsed_link[\"narrow\"] == \"stream\":\n            self.controller.narrow_to_stream(\n                stream_name=parsed_link[\"stream\"][\"stream_name\"],\n            )\n        elif parsed_link[\"narrow\"] == \"stream:topic\":\n            self.controller.narrow_to_topic(\n                stream_name=parsed_link[\"stream\"][\"stream_name\"],\n                topic_name=parsed_link[\"topic_name\"],\n            )\n        elif parsed_link[\"narrow\"] == \"stream:near\":\n            self.controller.narrow_to_message(\n                stream_name=parsed_link[\"stream\"][\"stream_name\"],\n                message_id=parsed_link[\"message_id\"],\n            )\n        elif parsed_link[\"narrow\"] == \"stream:topic:near\":\n            self.controller.narrow_to_topic_message(\n                stream_name=parsed_link[\"stream\"][\"stream_name\"],\n                topic_name=parsed_link[\"topic_name\"],\n                message_id=parsed_link[\"message_id\"],\n            )\n", "idx": 1738}
{"namespace": "zulipterminal.config.themes.complete_and_incomplete_themes", "completion": "    complete_themes = []\n    incomplete_themes = []\n\n    for theme_name, theme_info in THEMES.items():\n        if theme_info[\"meta\"] == REQUIRED_META:\n            complete_themes.append(theme_name)\n        else:\n            incomplete_themes.append(theme_name)\n\n    complete_themes.sort()\n    incomplete_themes.sort()\n\n    return complete_themes, incomplete_themes\n\n", "idx": 1739}
{"namespace": "zulipterminal.config.themes.validate_colors", "completion": "    if color_depth != 16:\n        raise InvalidThemeColorCode(\n            f\"Color depth {color_depth} is not supported. Only 16-color depth is supported.\"\n        )\n\n    theme_styles = THEMES[theme_name].STYLES\n    for style_name, style_value in theme_styles.items():\n        if style_value in STANDARD_TYPES:\n            continue\n        if style_value not in valid_16_color_codes:\n            raise InvalidThemeColorCode(\n                f\"Color code {style_value} for style {style_name} in theme {theme_name} is not valid.\"\n            )\n\n", "idx": 1740}
{"namespace": "zulipterminal.config.themes.parse_themefile", "completion": "    urwid_theme = []\n    for style_name, (fg, bg) in theme_styles.items():\n        if style_name is None:\n            style_name = \"\"\n        if isinstance(fg, str):\n            fg = STANDARD_TYPES[fg]\n        if isinstance(bg, str):\n            bg = STANDARD_TYPES[bg]\n        if color_depth == 1:\n            fg = fg.value.split()[0]\n            bg = bg.value.split()[0]\n        elif color_depth == 16:\n            fg = fg.value.split()[0]\n            bg = bg.value.split()[0]\n        elif color_depth == 256:\n            fg = fg.value.split()[0]\n            bg = bg.value.split()[0]\n        elif color_depth == 2 ** 24:\n            fg = fg.value.split()[0]\n            bg = bg.value.split()[0]\n        else:\n            raise ValueError(\n                f\"Invalid color depth: {color_depth}. Valid color depths are 1, 16, 256, and 2^24.\"\n            )\n        urwid_theme.append((style_name, fg, bg))\n    return urwid_theme\n\n", "idx": 1741}
{"namespace": "zulipterminal.config.themes.add_pygments_style", "completion": "    # Get the Pygments styles from the theme metadata\n    pygments_styles = theme_meta.get(\"pygments\", {}).get(\"styles\", {})\n\n    # Get the background color from the theme metadata\n    background_color = theme_meta.get(\"pygments\", {}).get(\"background\", \"\")\n\n    # Get the overrides from the theme metadata\n    overrides = theme_meta.get(\"pygments\", {}).get(\"overrides\", {})\n\n    # Add the Pygments styles to the Urwid theme\n    for style_name, style_value in pygments_styles.items():\n        urwid_theme.append((style_name, style_value))\n\n    # Add the background color to the Urwid theme\n    urwid_theme.append((\"background\", background_color))\n\n    # Add the overrides to the Urwid theme\n    urwid_theme.append((\"overrides\", overrides))", "idx": 1742}
{"namespace": "zulipterminal.config.keys.is_command_key", "completion": "    if command in KEY_BINDINGS:\n        return key in KEY_BINDINGS[command][\"keys\"]\n    else:\n        return False\n\n", "idx": 1743}
{"namespace": "zulipterminal.config.keys.keys_for_command", "completion": "    try:\n        return KEY_BINDINGS[command][\"keys\"]\n    except KeyError as exception:\n        raise InvalidCommand(command)\n\n", "idx": 1744}
{"namespace": "zulipterminal.config.keys.commands_for_random_tips", "completion": "    return [\n        key_binding\n        for key_binding in KEY_BINDINGS.values()\n        if not key_binding[\"excluded_from_random_tips\"]\n    ]\n\n", "idx": 1745}
{"namespace": "hypertools.datageometry.DataGeometry.transform", "completion": "        if data is None:\n            return self.xform_data\n\n        if isinstance(data, list):\n            data = list(map(convert_text, data))\n\n        if isinstance(data, pd.DataFrame):\n            data = data.values\n\n        if isinstance(data, np.ndarray):\n            data = [data]\n\n        if isinstance(data[0], np.ndarray):\n            data = [data]\n\n        if isinstance(data[0], list):\n            data = data[0]\n\n        if isinstance(data[0], pd.DataFrame):\n            data = [d.values for d in data]\n\n        if isinstance(data[0], np.ndarray):\n            data = [data]\n\n        if isinstance(data[0], list):\n            data = data[0]\n\n        if isinstance(data[0], pd.DataFrame):\n            data = [d.values for d in data]\n\n        if isinstance(data[0], np.ndarray):\n            data = [data]\n\n        if isinstance(data[0], list):\n            data = data[0]\n\n        if isinstance(data[0], pd.DataFrame):\n            data = [d.values for d in data]\n\n        if isinstance(data[0], np.ndarray):\n            data = [data]\n\n        if isinstance(data[0], list):\n            data = data[0]\n\n        if isinstance(data[0], pd.DataFrame):\n            data = [d.values for d in data]\n\n        if isinstance(data[0], np.ndarray):\n            data = [data]\n\n        if isinstance(data[0], list):\n            data = data[0]\n\n        if isinstance(data[0], pd.DataFrame):\n            data = [d.values for d in data]\n\n        if isinstance(data[0], np.ndarray):\n            data = [data]\n\n        if isinstance(data[0], list):\n            data = data[0]\n\n        if isinstance(data[0], pd.DataFrame):\n            data = [d.values for d in data]\n\n        if isinstance(data[0], np.ndarray):\n            data = [data]\n\n        if isinstance(data[0], list):\n            data = data[0]\n\n        if isinstance(data[0], pd.DataFrame):\n            data = [d.values for d in data]\n\n        if isinstance(data[0], np.ndarray):\n            data = [data]\n\n        if isinstance", "idx": 1746}
{"namespace": "hypertools.datageometry.DataGeometry.plot", "completion": "        # if no new data passed,\n        from .tools.align import align as aligner\n        from .tools.normalize import normalize as normalizer\n        from .tools.reduce import reduce as reducer\n        if data is None:\n            data = self.xform_data\n        else:\n            formatted = format_data(\n                data,\n                semantic=self.semantic,\n                vectorizer=self.vectorizer,\n                corpus=self.corpus,\n                ppca=True)\n            norm = normalizer(formatted, normalize=self.normalize)\n            reduction = reducer(\n                norm,\n                reduce=self.reduce,\n                ndims=self.reduce['params']['n_components'])\n            data = aligner(reduction, align=self.align)\n\n        # plot the data\n        from .plot import plot\n        return plot(data, **kwargs)\n", "idx": 1747}
{"namespace": "awesome_autodl.autodl_topic2papers", "completion": "    from autodl.paper import AutoDLpaper\n    from autodl.paper import AutoDLpaper_from_yaml\n    from autodl.paper import AutoDLpaper_from_yaml_file\n    from autodl.paper import AutoDLpaper_from_yaml_file_list\n    from autodl.paper import AutoDLpaper_from_yaml_file_list_with_topic\n    from autodl.paper import AutoDLpaper_from_yaml_file_list_with_topic_and_year\n    from autodl.paper import AutoDLpaper_from_yaml_file_list_with_topic_and_year_and_month\n    from autodl.paper import AutoDLpaper_from_yaml_file_list_with_topic_and_year_and_month_and_day\n    from autodl.paper import AutoDLpaper_from_yaml_file_list_with_topic_and_year_and_month_and_day_and_hour\n    from autodl.paper import AutoDLpaper_from_yaml_file_list_with_topic_and_year_and_month_and_day_and_hour_and_minute\n    from autodl.paper import AutoDLpaper_from_yaml_file_list_with_topic_and_year_and_month_and_day_and_hour_and_minute_and_second\n    from autodl.paper import AutoDLpaper_from_yaml_file_list_with_topic_and_year_and_month_and_day_and_hour_and_minute_and_second_and_microsecond\n    from autodl.paper import AutoDLpaper_from_yaml_file_list_with_topic_and_year_and_month_and_day_and_hour_and_minute_and_second_and_microsecond_and_tzinfo\n    from autodl.paper import AutoDLpaper_from_yaml_file_list_with_topic_and_year_and_month_and_day_and_hour_and_minute_and_second_and_microsecond_and_tzinfo_and_offset\n    from autodl.paper import AutoDLpaper_from_yaml_file_list_with_topic", "idx": 1748}
{"namespace": "awesome_autodl.get_bib_abbrv_obj", "completion": "    from awesome_autodl.utils import load_yaml\n    from awesome_autodl.data_cls import BibAbbreviations\n\n    abbrv_file = get_bib_abbrv_file()\n    if not abbrv_file.exists():\n        ValueError(f\"Can not find {abbrv_file} at {abbrv_file}.\")\n    abbrv_obj = BibAbbreviations(abbrv_file)\n    return abbrv_obj\n\n", "idx": 1749}
{"namespace": "mssqlcli.localized_strings.translation", "completion": "    if languages is None:\n        languages = LANGUAGES\n\n    return gettext.translation(domain, localedir, languages)\n\n", "idx": 1750}
{"namespace": "mssqlcli.mssqlbuffer._is_query_executable", "completion": "    if sql.strip().endswith('GO'):\n        return True\n    if sql.strip().startswith('GO'):\n        return False\n    if sql.strip().startswith('/*'):\n        return False\n    if sql.strip().endswith('*/'):\n        return False\n    if sql.strip().startswith('/*'):\n        return False\n    if sql.strip().endswith('*/'):\n        return False\n    if sql.strip().startswith('/*'):\n        return False\n    if sql.strip().endswith('*/'):\n        return False\n    if sql.strip().startswith('/*'):\n        return False\n    if sql.strip().endswith('*/'):\n        return False\n    if sql.strip().startswith('/*'):\n        return False\n    if sql.strip().endswith('*/'):\n        return False\n    if sql.strip().startswith('/*'):\n        return False\n    if sql.strip().endswith('*/'):\n        return False\n    if sql.strip().startswith('/*'):\n        return False\n    if sql.strip().endswith('*/'):\n        return False\n    if sql.strip().startswith('/*'):\n        return False\n    if sql.strip().endswith('*/'):\n        return False\n    if sql.strip().startswith('/*'):\n        return False\n    if sql.strip().endswith('*/'):\n        return False\n    if sql.strip().startswith('/*'):\n        return False\n    if sql.strip().endswith('*/'):\n        return False\n    if sql.strip().startswith('/*'):\n        return False\n    if sql.strip().endswith('*/'):\n        return False\n    if sql.strip().startswith('/*'):\n        return False\n    if sql.strip().endswith('*/'):\n        return False\n    if sql.strip().startswith('/*'):\n        return False\n    if sql.strip().endswith('*/'):\n        return False\n    if sql.strip().startswith('/*'):\n        return False\n    if sql.strip().endswith('*/'):\n        return False\n    if sql.strip().startswith('/*'):\n        return False\n    if sql.strip().endswith('*/'):\n        return False\n    if sql.strip().startswith('/*'):\n        return False\n    if sql.strip().endswith('*/'):\n        return False\n    if sql.strip().startswith('/*'):\n        return False\n    if sql.strip().endswith('*/'):\n        return False\n    if sql.strip().startswith('/*'):\n        return False\n    if sql.strip().endswith('*/'):\n        return False\n    if sql.strip().", "idx": 1751}
{"namespace": "mssqlcli.telemetry.conclude", "completion": "    _session.end_time = datetime.now()\n    payload = _session.generate_payload()\n    telemetry_core.output_telemetry_payload(payload)\n    return telemetry_core.upload_telemetry_payload(payload, service_endpoint_uri, separate_process)\n\n", "idx": 1752}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.start", "completion": "        self.request_thread = threading.Thread(name=self.REQUEST_THREAD_NAME, target=self.request_thread_func)\n        self.request_thread.start()\n\n        self.response_thread = threading.Thread(name=self.RESPONSE_THREAD_NAME, target=self.response_thread_func)\n        self.response_thread.start()\n", "idx": 1753}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.submit_request", "completion": "        if method is None or params is None:\n            raise ValueError('Method and params cannot be None.')\n\n        request = {\n            u'method': method,\n            u'params': params,\n            u'id': request_id\n        }\n\n        self.request_queue.put(request)\n", "idx": 1754}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.get_response", "completion": "        if request_id in self.response_map:\n            return self.response_map[request_id].get()\n        elif owner_uri in self.response_map:\n            return self.response_map[owner_uri].get()\n        elif self.exception_queue.qsize() > 0:\n            return self.exception_queue.get()\n        else:\n            return None\n", "idx": 1755}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.shutdown", "completion": "        self.cancel = True\n        self.request_queue.put(None)\n        self.request_thread.join()\n        self.writer.close()\n        logger.debug('Json Rpc client shutdown.')\n\n", "idx": 1756}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcWriter.send_request", "completion": "        if self.stream.closed:\n            raise ValueError(u'Stream was closed.')\n\n        content = {u'method': method, u'params': params, u'id': request_id}\n        content_str = json.dumps(content, ensure_ascii=False)\n        content_bytes = content_str.encode(self.encoding)\n        header = self.HEADER.format(len(content_bytes))\n        header_bytes = header.encode(self.encoding)\n        self.stream.write(header_bytes)\n        self.stream.write(content_bytes)\n        self.stream.flush()\n", "idx": 1757}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.read_response", "completion": "        while self.needs_more_data:\n            try:\n                if self.read_state == ReadState.Header:\n                    self.read_header()\n                elif self.read_state == ReadState.Content:\n                    self.read_content()\n                else:\n                    raise ValueError(u'Invalid read state.')\n\n            except ValueError as ex:\n                logger.debug(u'Read Response encountered exception %s', ex)\n                raise\n\n        # Trim the buffer.\n        self.buffer = self.buffer[self.read_offset:]\n        self.buffer_end_offset = self.buffer_end_offset - self.read_offset\n        self.read_offset = 0\n\n        # Parse the content as JSON.\n        content = self.buffer.decode(self.encoding)\n        try:\n            return json.loads(content)\n        except ValueError as ex:\n            logger.debug(u'Read Response encountered exception %s', ex)\n            raise\n", "idx": 1758}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.read_next_chunk", "completion": "        try:\n            # Read data from the stream into the buffer.\n            self.buffer[self.buffer_end_offset:self.buffer_end_offset] = self.stream.read(\n                self.DEFAULT_BUFFER_SIZE)\n            # Update the buffer offset.\n            self.buffer_end_offset += len(self.buffer[self.buffer_end_offset:])\n            # Check if the buffer needs to be resized.\n            if self.buffer_end_offset >= self.buffer_size * self.BUFFER_RESIZE_TRIGGER:\n                self.resize_buffer()\n            return True\n        except ValueError as ex:\n            logger.debug(u'JSON RPC Reader on read_next_chunk() encountered exception: %s', ex)\n            raise\n", "idx": 1759}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.try_read_headers", "completion": "        # Find the last header containing '\\r\\n\\r\\n'\n        last_header_end = self.buffer.rfind(self.CR, self.read_offset, self.buffer_end_offset)\n        if last_header_end == -1:\n            return False\n\n        # Split the headers by new line\n        headers = self.buffer[self.read_offset:last_header_end].split(self.LF)\n\n        # Extract the key-value pairs and store them in the headers dictionary\n        for header in headers:\n            if header:\n                key, value = header.split(b': ', 1)\n                self.headers[key.decode(self.encoding)] = value.decode(self.encoding)\n\n        # Check if the 'content-length' header is present and store its value in the expected content length\n        if 'content-length' in self.headers:\n            self.expected_content_length = int(self.headers['content-length'])\n\n        # Update the read offset to the end of the last header\n        self.read_offset = last_header_end + 2\n\n        # Update the read state to Content\n        self.read_state = ReadState.Content\n\n        return True\n", "idx": 1760}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.close", "completion": "        try:\n            self.stream.close()\n        except AttributeError:\n            pass", "idx": 1761}
{"namespace": "mssqlcli.packages.prioritization.PrevalenceCounter.update", "completion": "        # Update the keyword counts\n        for keyword in keywords:\n            self.keyword_counts[keyword] += len(keyword_regexs[keyword].findall(text))\n\n        # Update the name counts\n        for name in Name.findall(text):\n            self.name_counts[name.value] += 1\n\n", "idx": 1762}
{"namespace": "mssqlcli.packages.sqlcompletion.suggest_type", "completion": "    if full_text.startswith('\\\\i '):\n        return Path, None\n\n    stmt = SqlStatement(full_text, text_before_cursor)\n\n    if stmt.parsed:\n        if stmt.parsed.token_first().value.lower() == 'select':\n            return Column, stmt.get_tables()\n        elif stmt.parsed.token_first().value.lower() == 'insert':\n            return Column, stmt.get_tables(scope='insert')\n        elif stmt.parsed.token_first().value.lower() == 'create':\n            return Column, stmt.get_tables()\n        elif stmt.parsed.token_first().value.lower() == 'alter':\n            return Column, stmt.get_tables()\n        elif stmt.parsed.token_first().value.lower() == 'drop':\n            return Column, stmt.get_tables()\n        elif stmt.parsed.token_first().value.lower() == 'delete':\n            return Column, stmt.get_tables()\n        elif stmt.parsed.token_first().value.lower() == 'update':\n            return Column, stmt.get_tables()\n        elif stmt.parsed.token_first().value.lower() == 'with':\n            return Column, stmt.get_tables()\n        elif stmt.parsed.token_first().value.lower() == 'set':\n            return Column, stmt.get_tables()\n        elif stmt.parsed.token_first().value.lower() == 'from':\n            return FromClauseItem, stmt.get_tables()\n        elif stmt.parsed.token_first().value.lower() == 'join':\n            return Join, stmt.get_tables()\n        elif stmt.parsed.token_first().value.lower() == 'on':\n            return JoinCondition, stmt.get_tables()\n        elif stmt.parsed.token_first().value.lower() == 'where':\n            return Column, stmt.get_tables()\n        elif stmt.parsed.token_first().value.lower() == 'group':\n            return Column, stmt.get_tables()\n        elif stmt.parsed.token_first().value.lower() == 'order':\n            return Column, stmt.get_tables()\n        elif stmt.parsed.token_first().value.lower() == 'having':\n            return Column, stmt.get_tables()\n        elif stmt.parsed.token", "idx": 1763}
{"namespace": "mssqlcli.packages.parseutils.ctes.extract_ctes", "completion": "    # Parse the query using a parser\n    parsed = parse(sql)\n\n    # Check if the first meaningful token is \"WITH\"\n    if not parsed.tokens:\n        return sql, tuple()\n\n    # Extract the CTEs from the query\n    ctes = []\n    for token in parsed.tokens:\n        if token.ttype is Keyword and token.value == 'WITH':\n            ctes.append(extract_cte(parsed, token))\n\n    # Return the remaining SQL text after the CTEs have been stripped\n    return sql[ctes[-1].stop:], tuple(ctes)\n\n", "idx": 1764}
{"namespace": "mssqlcli.packages.parseutils.tables.extract_tables", "completion": "    parsed = sqlparse.parse(sql)[0]\n    return extract_table_identifiers(extract_from_part(parsed))", "idx": 1765}
{"namespace": "googleapiclient.channel.Channel.body", "completion": "        body = {\n            \"id\": self.id,\n            \"token\": self.token,\n            \"type\": self.type,\n            \"address\": self.address,\n        }\n        if self.params:\n            body[\"params\"] = self.params\n        if self.resource_id:\n            body[\"resourceId\"] = self.resource_id\n        if self.resource_uri:\n            body[\"resourceUri\"] = self.resource_uri\n        if self.expiration:\n            body[\"expiration\"] = self.expiration\n        return body\n", "idx": 1766}
{"namespace": "googleapiclient.channel.Channel.update", "completion": "        for key in resp:\n            if key in CHANNEL_PARAMS:\n                setattr(self, CHANNEL_PARAMS[key], resp[key])\n\n", "idx": 1767}
{"namespace": "googleapiclient.channel.notification_from_headers", "completion": "    # Validate the notification.\n    if not _validate_notification(channel, headers):\n        raise InvalidNotificationError(\"Invalid notification\")\n\n    # Parse the notification.\n    message_number = int(headers[X_GOOG_MESSAGE_NUMBER])\n    state = headers[X_GOOG_RESOURCE_STATE]\n    resource_uri = headers[X_GOOG_RESOURCE_URI]\n    resource_id = headers[X_GOOG_RESOURCE_ID]\n\n    return Notification(message_number, state, resource_uri, resource_id)\n\n", "idx": 1768}
{"namespace": "googleapiclient.channel.new_webhook_channel", "completion": "    if expiration is None:\n        expiration = datetime.datetime.utcnow() + datetime.timedelta(days=365)\n    expiration = int(expiration.strftime(\"%s\")) * 1000\n    id = str(uuid.uuid4())\n    return Channel(\n        \"web_hook\",\n        id,\n        token,\n        url,\n        expiration,\n        params,\n    )", "idx": 1769}
{"namespace": "googleapiclient.model.BaseModel._build_query", "completion": "        if self.alt_param:\n            params[self.alt_param] = self.alt_param\n        return urllib.urlencode(\n            [(k, v) for k, v in params.items() if v is not None]\n        )\n", "idx": 1770}
{"namespace": "googleapiclient.model.BaseModel.response", "completion": "        self._log_response(resp, content)\n        if resp.status == 204:\n            return self.no_content_response\n        if resp.status < 200 or resp.status >= 300:\n            raise HttpError(resp, content)\n        return self.deserialize(content)\n", "idx": 1771}
{"namespace": "googleapiclient.model.makepatch", "completion": "    patch = {}\n    for key in modified:\n        if key in original:\n            if isinstance(original[key], dict) and isinstance(modified[key], dict):\n                patch[key] = makepatch(original[key], modified[key])\n            elif original[key] != modified[key]:\n                patch[key] = modified[key]\n        else:\n            patch[key] = modified[key]\n    return patch", "idx": 1772}
{"namespace": "googleapiclient._helpers.update_query_params", "completion": "    if not uri.startswith(\"http\"):\n        raise ValueError(\"URI must start with http\")\n\n    if \"?\" not in uri:\n        uri += \"?\"\n\n    uri_params = urllib.parse.parse_qs(uri[uri.index(\"?\") + 1 :])\n\n    for key, value in params.items():\n        if key in uri_params:\n            uri_params[key] = value\n        else:\n            uri_params[key] = [value]\n\n    return uri + urllib.parse.urlencode(uri_params, doseq=True)", "idx": 1773}
{"namespace": "googleapiclient._helpers._add_query_parameter", "completion": "    if value is None:\n        return url\n\n    # Parse the URL into its components\n    parsed_url = urllib.parse.urlparse(url)\n\n    # Parse the query string into a dictionary\n    query_params = urllib.parse.parse_qs(parsed_url.query)\n\n    # Add the new query parameter to the dictionary\n    query_params[name] = value\n\n    # Convert the dictionary back to a query string\n    new_query = urllib.parse.urlencode(query_params)\n\n    # Create a new URL with the updated query string\n    new_url = urllib.parse.urlunparse(\n        (parsed_url.scheme, parsed_url.netloc, parsed_url.path, parsed_url.params, new_query, parsed_url.fragment)\n    )\n\n    return new_url\n\n", "idx": 1774}
{"namespace": "gif_for_cli.display.display_txt_frames", "completion": "    try:\n        for i in range(num_loops):\n            for txt_frame in txt_frames:\n                stdout.write(txt_frame)\n                stdout.write(STORED_CELL_CHAR)\n                stdout.flush()\n                time.sleep(seconds_per_frame)\n    except KeyboardInterrupt:\n        raise KeyboardInterrupt\n\n", "idx": 1775}
{"namespace": "pyVmomi.SoapAdapter.SoapResponseDeserializer.Deserialize", "completion": "        self.deser.Deserialize(response, resultType, nsMap=nsMap)\n        self.deser.Deserialize(response, resultType, nsMap=nsMap)\n        self.deser.Deserialize(response, resultType, nsMap=nsMap)\n        self.deser.Deserialize(response, resultType, nsMap=nsMap)\n        self.deser.Deserialize(response, resultType, nsMap=nsMap)\n        self.deser.Deserialize(response, resultType, nsMap=nsMap)\n        self.deser.Deserialize(response, resultType, nsMap=nsMap)\n        self.deser.Deserialize(response, resultType, nsMap=nsMap)\n        self.deser.Deserialize(response, resultType, nsMap=nsMap)\n        self.deser.Deserialize(response, resultType, nsMap=nsMap)\n        self.deser.Deserialize(response, resultType, nsMap=nsMap)\n        self.deser.Deserialize(response, resultType, nsMap=nsMap)\n        self.deser.Deserialize(response, resultType, nsMap=nsMap)\n        self.deser.Deserialize(response, resultType, nsMap=nsMap)\n        self.deser.Deserialize(response, resultType, nsMap=nsMap)\n        self.deser.Deserialize(response, resultType, nsMap=nsMap)\n        self.deser.Deserialize(response, resultType, nsMap=nsMap)\n        self.deser.Deserialize(response, resultType, nsMap=nsMap)\n        self.deser.Deserialize(response, resultType, nsMap=nsMap)\n        self.deser.Deserialize(response, resultType, nsMap=nsMap)\n        self.deser.Deserialize(response, resultType, nsMap=nsMap)\n        self.deser.Deserialize(response, resultType, nsMap=nsMap)\n        self.deser.Deserialize(response, resultType, nsMap=nsMap)\n        self.deser.Deserialize(response, resultType, nsMap=nsMap)\n        self.deser.Deserialize(response, resultType, nsMap=nsMap)\n        self.deser.Deserialize(response, resultType, nsMap=nsMap)\n        self.deser.", "idx": 1776}
{"namespace": "pyVmomi.VmomiSupport.GetRequestContext", "completion": "    if not hasattr(_threadLocalContext, 'context'):\n        _threadLocalContext.context = StringDict()\n    return _threadLocalContext.context\n\n", "idx": 1777}
{"namespace": "pycoin.bloomfilter.filter_size_required", "completion": "    # Calculate the size in bytes\n    size = (-1 / (LOG_2 ** 2) * element_count * math.log(false_positive_probability)) / 8\n\n    # Ensure it does not exceed the maximum size\n    if size > 36000:\n        size = 36000\n\n    # Return the size in bytes\n    return int(size)\n\n", "idx": 1778}
{"namespace": "pycoin.bloomfilter.BloomFilter.add_spendable", "completion": "        self.add_item(spendable.serialize())\n", "idx": 1779}
{"namespace": "pycoin.bloomfilter.murmur3", "completion": "    # The hash function is based on the MurmurHash3 algorithm.\n    # See http://code.google.com/p/smhasher/wiki/MurmurHash3 for details.\n\n    # The hash function is implemented in C, and the following code is a Python translation.\n\n    # The hash function is based on the MurmurHash3 algorithm.\n    # See http://code.google.com/p/smhasher/wiki/MurmurHash3 for details.\n\n    # The hash function is implemented in C, and the following code is a Python translation.\n\n    # The hash function is based on the MurmurHash3 algorithm.\n    # See http://code.google.com/p/smhasher/wiki/MurmurHash3 for details.\n\n    # The hash function is implemented in C, and the following code is a Python translation.\n\n    # The hash function is based on the MurmurHash3 algorithm.\n    # See http://code.google.com/p/smhasher/wiki/MurmurHash3 for details.\n\n    # The hash function is implemented in C, and the following code is a Python translation.\n\n    # The hash function is based on the MurmurHash3 algorithm.\n    # See http://code.google.com/p/smhasher/wiki/MurmurHash3 for details.\n\n    # The hash function is implemented in C, and the following code is a Python translation.\n\n    # The hash function is based on the MurmurHash3 algorithm.\n    # See http://code.google.com/p/smhasher/wiki/MurmurHash3 for details.\n\n    # The hash function is implemented in C, and the following code is a Python translation.\n\n    # The hash function is based on the MurmurHash3 algorithm.\n    # See http://code.google.com/p/smhasher/wiki/MurmurHash3 for details.\n\n    # The hash function is implemented in C, and the following code is a Python translation.\n\n    # The hash function is based on the MurmurHash3 algorithm.\n    # See http://code.google.com/p/smhasher/wiki/MurmurHash3 for details.\n\n    # The hash function is implemented in C, and the following code is a Python translation.\n\n    # The hash", "idx": 1780}
{"namespace": "pycoin.networks.registry.network_for_netcode", "completion": "    try:\n        symbol = int(symbol)\n    except Exception:\n        pass\n    try:\n        symbol = int(symbol)\n    except Exception:\n        pass\n    try:\n        symbol = int(symbol)\n    except Exception:\n        pass\n    try:\n        symbol = int(symbol)\n    except Exception:\n        pass\n    try:\n        symbol = int(symbol)\n    except Exception:\n        pass\n    try:\n        symbol = int(symbol)\n    except Exception:\n        pass\n    try:\n        symbol = int(symbol)\n    except Exception:\n        pass\n    try:\n        symbol = int(symbol)\n    except Exception:\n        pass\n    try:\n        symbol = int(symbol)\n    except Exception:\n        pass\n    try:\n        symbol = int(symbol)\n    except Exception:\n        pass\n    try:\n        symbol = int(symbol)\n    except Exception:\n        pass\n    try:\n        symbol = int(symbol)\n    except Exception:\n        pass\n    try:\n        symbol = int(symbol)\n    except Exception:\n        pass\n    try:\n        symbol = int(symbol)\n    except Exception:\n        pass\n    try:\n        symbol = int(symbol)\n    except Exception:\n        pass\n    try:\n        symbol = int(symbol)\n    except Exception:\n        pass\n    try:\n        symbol = int(symbol)\n    except Exception:\n        pass\n    try:\n        symbol = int(symbol)\n    except Exception:\n        pass\n    try:\n        symbol = int(symbol)\n    except Exception:\n        pass\n    try:\n        symbol = int(symbol)\n    except Exception:\n        pass\n    try:\n        symbol = int(symbol)\n    except Exception:\n        pass\n    try:\n        symbol = int(symbol)\n    except Exception:\n        pass\n    try:\n        symbol = int(symbol)\n    except Exception:\n        pass\n    try:\n        symbol = int(symbol)\n    except Exception:\n        pass\n    try:\n        symbol = int(symbol)\n    except Exception:\n        pass\n    try:\n        symbol = int(symbol)\n    except Exception:\n        pass\n    try:\n        symbol = int(symbol)\n    except Exception:\n        pass\n    try:\n        symbol = int(symbol)\n    except Exception:\n        pass\n    try:\n        symbol = int(symbol)\n    except Exception:\n        pass\n    try:\n        symbol = int(symbol)\n    except Exception:\n        pass\n    try:\n        symbol = int(symbol)\n    except Exception:\n        pass\n    try:\n       ", "idx": 1781}
{"namespace": "pycoin.satoshi.IntStreamer.IntStreamer.int_from_script_bytes", "completion": "        if not s:\n            return 0\n\n        s = s[::-1]\n        value = s[0] & 0x7f\n        if require_minimal and value == 0 and len(s) > 1:\n            raise ScriptError(\"Non-minimal encoding\")\n        if s[0] & 0x80:\n            value = -value\n        for b in s[1:]:\n            value = (value << 8) + b\n        return value\n", "idx": 1782}
{"namespace": "pycoin.satoshi.stackops.do_OP_RIPEMD160", "completion": "    #  (x -- x')\n    v = stack.pop()\n    stack.append(hashlib.new('ripemd160', v).digest())\n\n", "idx": 1783}
{"namespace": "pycoin.satoshi.stackops.do_OP_HASH160", "completion": "    stack.append(hashlib.sha256(stack.pop()).digest())\n\n", "idx": 1784}
{"namespace": "pycoin.satoshi.stackops.do_OP_HASH256", "completion": "    stack.append(hashlib.sha256(stack.pop()).digest())\n\n", "idx": 1785}
{"namespace": "pycoin.services.providers.providers_for_config_string", "completion": "    providers = []\n    for descriptor in config_string.split(\",\"):\n        provider = provider_for_descriptor_and_netcode(descriptor, netcode)\n        if provider:\n            providers.append(provider)\n        else:\n            warnings.warn(\"Could not parse descriptor %s\" % descriptor)\n    return providers\n\n", "idx": 1786}
{"namespace": "pycoin.services.providers.get_default_providers_for_netcode", "completion": "    if netcode is None:\n        netcode = get_current_netcode()\n\n    if not hasattr(THREAD_LOCALS, \"default_providers\"):\n        THREAD_LOCALS.default_providers = {}\n\n    if netcode not in THREAD_LOCALS.default_providers:\n        THREAD_LOCALS.default_providers[netcode] = providers_for_netcode_from_env(netcode)\n\n    return THREAD_LOCALS.default_providers[netcode]", "idx": 1787}
{"namespace": "pycoin.services.providers.set_default_providers_for_netcode", "completion": "    if not hasattr(THREAD_LOCALS, \"providers\"):\n        THREAD_LOCALS.providers = {}\n    THREAD_LOCALS.providers[netcode] = provider_list\n\n", "idx": 1788}
{"namespace": "pycoin.blockchain.BlockChain.BlockChain.tuple_for_index", "completion": "        if index < 0:\n            index = self.length() + index\n\n        if index < len(self._locked_chain):\n            return self._locked_chain[index]\n\n        if self._longest_chain_cache is None:\n            self._longest_chain_cache = self._longest_local_block_chain()\n\n        return self._longest_chain_cache[index - len(self._locked_chain)]\n", "idx": 1789}
{"namespace": "pycoin.blockchain.ChainFinder.ChainFinder.find_ancestral_path", "completion": "        # first, find the common ancestor\n        common_ancestor = None\n        for h in self.trees_from_bottom.keys():\n            if h1 in self.trees_from_bottom[h] and h2 in self.trees_from_bottom[h]:\n                common_ancestor = h\n                break\n\n        # now, find the ancestral path from h1 to the common ancestor\n        h1_path = self.maximum_path(h1, path_cache)\n        h1_path_index = h1_path.index(common_ancestor)\n        h1_path = h1_path[h1_path_index:]\n\n        # now, find the ancestral path from h2 to the common ancestor\n        h2_path = self.maximum_path(h2, path_cache)\n        h2_path_index = h2_path.index(common_ancestor)\n        h2_path = h2_path[h2_path_index:]\n\n        return h1_path, h2_path\n", "idx": 1790}
{"namespace": "pycoin.contrib.bech32m.bech32_encode", "completion": "    # Convert the data to a list of integers.\n    data = [ord(x) for x in data]\n\n    # Compute the checksum values.\n    chk = bech32_create_checksum(hrp, data, spec)\n\n    # Convert the data to a list of characters.\n    data = [CHARSET[b] for b in data]\n\n    # Convert the checksum values to characters.\n    chk = [CHARSET[b] for b in chk]\n\n    # Assemble the Bech32 string.\n    return hrp + '1' + ''.join(data) + ''.join(chk)\n\n", "idx": 1791}
{"namespace": "pycoin.contrib.bech32m.decode", "completion": "    # Decode the address.\n    hrp, data, spec = bech32_decode(addr)\n    if hrp is None or data is None or spec is None:\n        return (None, None)\n\n    # Check the version byte.\n    if data[0] not in [0, 0x50]:\n        return (None, None)\n\n    # Check the data length.\n    if len(data) not in [20, 32]:\n        return (None, None)\n\n    # Check the checksum.\n    if spec == Encoding.BECH32:\n        if bech32_verify_checksum(hrp, data) != Encoding.BECH32:\n            return (None, None)\n    elif spec == Encoding.BECH32M:\n        if bech32_verify_checksum(hrp, data) != Encoding.BECH32M:\n            return (None, None)\n\n    # Convert the data to a byte array.\n    data = convertbits(data, 5, 8, False)\n    if data is None:\n        return (None, None)\n\n    # Return the version byte and the decoded data.\n    return (data[0], data[1:])\n\n", "idx": 1792}
{"namespace": "pycoin.crack.bip32.crack_bip32", "completion": "    path_list = path.split(\"/\")\n    for child in path_list:\n        secret_exponent = ascend_bip32(bip32_pub_node, secret_exponent, int(child))\n    return bip32_pub_node.from_secret_exponent(secret_exponent)", "idx": 1793}
{"namespace": "pycoin.message.PeerAddress.ip_bin_to_ip4_addr", "completion": "    return \".\".join(\"%d\" % v for v in struct.unpack(\">HHHH\", ip_bin))\n\n\n", "idx": 1794}
{"namespace": "pycoin.message.PeerAddress.PeerAddress.host", "completion": "        if self.ip_bin.startswith(IP4_HEADER):\n            return ip_bin_to_ip4_addr(self.ip_bin)\n        else:\n            return ip_bin_to_ip6_addr(self.ip_bin)\n", "idx": 1795}
{"namespace": "oletools.msodde.field_is_blacklisted", "completion": "    # split into components\n    parts = FIELD_WORD_REGEX.findall(contents)\n    if len(parts) < 2:\n        return False\n    # check if first part is a command\n    if parts[0].lower() not in FIELD_BLACKLIST_CMDS:\n        return False\n    # check if number of arguments is correct\n    if len(parts) - 1 not in FIELD_BLACKLIST[FIELD_BLACKLIST_CMDS.index(parts[0].lower())][0]:\n        return False\n    # check if switches are correct\n    for part in parts[1:]:\n        if not FIELD_SWITCH_REGEX.match(part):\n            return False\n    return True\n\n", "idx": 1796}
{"namespace": "oletools.ppt_record_parser.is_ppt", "completion": "    # check if file is a ppt file\n    try:\n        ppt = PptFile(filename)\n    except Exception:\n        return False\n\n    # check if file is a ppt file\n    if ppt.is_ppt():\n        return True\n    else:\n        return False\n\n", "idx": 1797}
{"namespace": "oletools.rtfobj.is_rtf", "completion": "    if treat_str_as_data:\n        data = arg\n    else:\n        data = open(arg, 'rb').read()\n\n    if data[:4] == b'{\\rtf':\n        return True\n    else:\n        return False\n\n", "idx": 1798}
{"namespace": "oletools.oleobj.get_sane_embedded_filenames", "completion": "    # Extract the filename from the input paths\n    filename = os.path.basename(filename)\n    src_path = os.path.basename(src_path)\n    tmp_path = os.path.basename(tmp_path)\n\n    # Sanitize the filename\n    sane_filename = sanitize_filename(filename, max_len=max_len)\n\n    # Generate the sane filenames\n    sane_filenames = [\n        sane_filename,\n        os.path.join(src_path, sane_filename),\n        os.path.join(tmp_path, sane_filename),\n        os.path.join(src_path, sane_filename),\n        os.path.join(tmp_path, sane_filename),\n        os.path.join(src_path, sane_filename),\n        os.path.join(tmp_path, sane_filename),\n        os.path.join(src_path, sane_filename),\n        os.path.join(tmp_path, sane_filename),\n        os.path.join(src_path, sane_filename),\n        os.path.join(tmp_path, sane_filename),\n        os.path.join(src_path, sane_filename),\n        os.path.join(tmp_path, sane_filename),\n        sane_filename,\n        os.path.join(src_path, sane_filename),\n        os.path.join(tmp_path, sane_filename),\n        os.path.join(src_path, sane_filename),\n        os.path.join(tmp_path, sane_filename),\n        os.path.join(src_path, sane_filename),\n        os.path.join(tmp_path, sane_filename),\n        os.path.join(src_path, sane_filename),\n        os.path.join(tmp_path, sane_filename),\n        os.path.join(src_path, sane_filename),\n        os.path.join(tmp_path, sane_filename),\n        sane_filename,\n        os.path.join(src_path, sane_filename),\n        os.path.join(tmp_path, sane_filename),\n        os.path.join(src", "idx": 1799}
{"namespace": "oletools.ooxml.get_type", "completion": "    # check if file is a zip file\n    if not is_zipfile(filename):\n        return DOCTYPE_NONE\n\n    # open the file\n    with ZipFile(filename) as zf:\n\n        # check if the file contains the [Content_Types].xml file\n        if FILE_CONTENT_TYPES not in zf.namelist():\n            return DOCTYPE_NONE\n\n        # check if the file contains the _rels/.rels file\n        if FILE_RELATIONSHIPS not in zf.namelist():\n            return DOCTYPE_NONE\n\n        # check if the file contains a single-xml file\n        if len(zf.namelist()) > 3:\n            return DOCTYPE_MIXED\n\n        # check if the file contains a single-xml file\n        if len(zf.namelist()) == 3:\n            return DOCTYPE_NONE\n\n        # check if the file contains a single-xml file\n        if len(zf.namelist()) == 2:\n            return DOCTYPE_NONE\n\n        # check if the file contains a single-xml file\n        if len(zf.namelist()) == 1:\n            return DOCTYPE_NONE\n\n    return DOCTYPE_NONE\n\n", "idx": 1800}
{"namespace": "oletools.ooxml.ZipSubFile.read", "completion": "        if self.closed:\n            raise IOError('file is closed')\n        if self.pos >= self.size:\n            return b''\n        if size == -1:\n            size = self.size - self.pos\n        data = self.handle.read(size)\n        self.pos += len(data)\n        return data\n", "idx": 1801}
{"namespace": "oletools.ooxml.ZipSubFile.seek", "completion": "        if self.handle is None:\n            raise IOError('seek on closed handle')\n        if offset == io.SEEK_SET:\n            new_pos = pos\n        elif offset == io.SEEK_CUR:\n            new_pos = self.pos + pos\n        elif offset == io.SEEK_END:\n            new_pos = self.size + pos\n        else:\n            raise ValueError('invalid offset')\n        if new_pos < 0:\n            raise ValueError('negative position')\n        if new_pos > self.size:\n            raise ValueError('position beyond end of file')\n        self.pos = new_pos\n        self.handle.seek(new_pos)\n        # print('ZipSubFile: seek to {} (pos now {})'.format(pos, self.pos))\n", "idx": 1802}
{"namespace": "oletools.ooxml.XmlParser.iter_xml", "completion": "        if self.did_iter_all:\n            raise BadOOXML(self.filename, 'already iterated over all files')\n\n        for subfile, handle in self.iter_files(subfiles):\n            if subfile in self.subfiles_no_xml:\n                continue\n            try:\n                for event, elem in ET.iterparse(handle, events=('start', 'end')):\n                    if event == 'start':\n                        if need_children:\n                            elem.set('children', len(elem))\n                        if tags is not None:\n                            if elem.tag not in tags:\n                                continue\n                        yield subfile, elem, elem.get('depth')\n                    elif event == 'end':\n                        if need_children:\n                            elem.set('children', len(elem))\n                        if tags is not None:\n                            if elem.tag not in tags:\n                                continue\n                        yield subfile, elem, elem.get('depth')\n            except ET.ParseError as err:\n                self.subfiles_no_xml.add(subfile)\n                logger.warning('XML parse error in {0}: {1}'.format(subfile, err))\n            except ET.XMLSyntaxError as err:\n                self.subfiles_no_xml.add(subfile)\n                logger.warning('XML syntax error in {0}: {1}'.format(subfile, err))\n            except ET.XMLTreeBuilderError as err:\n                self.subfiles_no_xml.add(subfile)\n                logger.warning('XML tree builder error in {0}: {1}'.format(subfile, err))\n            except ET.ParseError as err:\n                self.subfiles_no_xml.add(subfile)\n                logger.warning('XML parse error in {0}: {1}'.format(subfile, err))\n            except ET.XMLSyntaxError as err:\n                self.subfiles_no_xml.add(subfile)\n                logger.warning('XML syntax error in {0}: {1}'.format(subfile, err))\n            except ET.XMLTreeBuilderError as err:\n                self.subfiles_no_xml.add(subfile)\n                logger.warning('XML tree builder error in {0}: {1}'.format(subfile, err))\n            except ET.ParseError as err:\n                self.subfiles_no_xml.add(subfile)\n                logger.warning", "idx": 1803}
{"namespace": "oletools.oleid.OleID.check", "completion": "        #TODO: add a check for the file size\n        #TODO: add a check for the file type\n        #TODO: add a check for the file version\n        #TODO: add a check for the file date\n        #TODO: add a check for the file size\n        #TODO: add a check for the file type\n        #TODO: add a check for the file version\n        #TODO: add a check for the file date\n        #TODO: add a check for the file size\n        #TODO: add a check for the file type\n        #TODO: add a check for the file version\n        #TODO: add a check for the file date\n        #TODO: add a check for the file size\n        #TODO: add a check for the file type\n        #TODO: add a check for the file version\n        #TODO: add a check for the file date\n        #TODO: add a check for the file size\n        #TODO: add a check for the file type\n        #TODO: add a check for the file version\n        #TODO: add a check for the file date\n        #TODO: add a check for the file size\n        #TODO: add a check for the file type\n        #TODO: add a check for the file version\n        #TODO: add a check for the file date\n        #TODO: add a check for the file size\n        #TODO: add a check for the file type\n        #TODO: add a check for the file version\n        #TODO: add a check for the file date\n        #TODO: add a check for the file size\n        #TODO: add a check for the file type\n        #TODO: add a check for the file version\n        #TODO: add a check for the file date\n        #TODO: add a check for the file size\n        #TODO: add a check for the file type\n        #TODO: add a check for the file version\n        #TODO: add a check for the file date\n        #TODO: add a check for the file size\n        #TODO: add a check for the file type\n        #TODO: add a check for the file version\n        #TODO: add a check for the file date\n        #TODO: add a check for the file size\n        #TODO: add a check for the file type\n        #TODO: add a check for the file version\n        #TODO: add a check for the file date\n        #TODO: add a check for the file size\n        #TODO: add", "idx": 1804}
{"namespace": "tools.cgrep.is_valid_ip", "completion": "  try:\n    nacaddr.IP(arg)\n  except nacaddr.InvalidIP:\n    raise argparse.ArgumentTypeError('Invalid IP address: %s' % arg)\n  return arg\n\n", "idx": 1805}
{"namespace": "tools.cgrep.group_diff", "completion": "  first_obj = options.gmp[0]\n  second_obj = options.gmp[1]\n  first_name, first_nets = get_nets([first_obj], db)[0]\n  second_name, second_nets = get_nets([second_obj], db)[0]\n  common = []\n  diff1 = []\n  diff2 = []\n  for first_net in first_nets:\n    for second_net in second_nets:\n      if first_net.version == second_net.version:\n        if first_net.subnet_of(second_net):\n          common.append(first_net)\n        else:\n          diff1.append(first_net)\n          diff2.append(second_net)\n  return common, diff1, diff2\n\n", "idx": 1806}
{"namespace": "tools.cgrep.compare_tokens", "completion": "  first_obj = options.cmp[0]\n  second_obj = options.cmp[1]\n  first_net = get_nets([first_obj], db)[0][1]\n  second_net = get_nets([second_obj], db)[0][1]\n  first_svc = get_ports([first_obj], db)[0][1]\n  second_svc = get_ports([second_obj], db)[0][1]\n  first_name = first_obj\n  second_name = second_obj\n  union = first_net | second_net\n  results = []\n  for net in union:\n    if net not in first_net:\n      results.append(net)\n    if net not in second_net:\n      results.append(net)\n  return first_name, second_name, union, results\n\n", "idx": 1807}
{"namespace": "capirca.aclgen.EntryPoint", "completion": "  SetupFlags()\n  app.run(main)\n\n", "idx": 1808}
{"namespace": "capirca.lib.nacaddr.IP", "completion": "  if isinstance(ip, ipaddress._BaseNetwork):\n    if ip.version == 4:\n      return IPv4(ip, comment, token)\n    elif ip.version == 6:\n      return IPv6(ip, comment, token)\n    else:\n      raise ValueError('IP address version not supported.')\n  else:\n    try:\n      ip = ipaddress.ip_network(ip, strict=strict)\n    except ValueError:\n      raise ValueError('Invalid IP address.')\n    if ip.version == 4:\n      return IPv4(ip, comment, token)\n    elif ip.version == 6:\n      return IPv6(ip, comment, token)\n    else:\n      raise ValueError('IP address version not supported.')\n\n", "idx": 1809}
{"namespace": "csvkit.cli.CSVKitUtility.run", "completion": "        if 'f' not in self.args.override_flags:\n            self.args.input_file = self._open_input_file(self.args.input_file)\n        self.main_loop()\n        if 'f' not in self.args.override_flags:\n            self.args.input_file.close()\n", "idx": 1810}
{"namespace": "csvkit.convert.fixed.fixed2csv", "completion": "    # Read the fixed-width file.\n    data = f.read()\n\n    # Parse the fixed-width file based on the provided schema.\n    parsed = parse_fixed(data, schema, skip_lines=skip_lines, **kwargs)\n\n    # Write the parsed data to a CSV file.\n    if output is not None:\n        output.write(parsed)\n        return None\n    else:\n        return parsed\n\n", "idx": 1811}
{"namespace": "check_dummies.find_backend", "completion": "    # Search for the backend\n    match = _re_backend.search(line)\n    if match:\n        return match.group(1)\n    return None\n\n", "idx": 1812}
{"namespace": "check_dummies.create_dummy_object", "completion": "    if name.isupper():\n        return DUMMY_CONSTANT.format(name)\n    elif name.islower():\n        return DUMMY_CLASS.format(name, backend_name)\n    else:\n        return DUMMY_FUNCTION.format(name, backend_name)\n\n", "idx": 1813}
{"namespace": "pycorrector.en_spell.EnSpell.check_init", "completion": "        if not self.word_freq_dict:\n            self._init()\n", "idx": 1814}
{"namespace": "pycorrector.en_spell.EnSpell.candidates", "completion": "        self.check_init()\n        if word in self.word_freq_dict:\n            return [word]\n        else:\n            return (self.known([word]) or self.known(self.edits1(word)) or self.known(self.edits2(word)) or [word])\n", "idx": 1815}
{"namespace": "pycorrector.en_spell.EnSpell.correct_word", "completion": "        self.check_init()\n        candidates = self.candidates(word)\n        if len(candidates) == 0:\n            return word\n        else:\n            return max(candidates, key=self.probability)\n", "idx": 1816}
{"namespace": "pycorrector.en_spell.EnSpell.correct", "completion": "        self.check_init()\n        if not self.custom_confusion:\n            self.set_en_custom_confusion_dict(config.en_custom_confusion_path)\n        details = []\n        if include_symbol:\n            text = text.replace(' ', '')\n        else:\n            text = text.replace(' ', '').replace('.', '').replace(',', '').replace('?', '').replace('!', '')\n        words = text.split()\n        for i, word in enumerate(words):\n            if len(word) > 1 and word.isalpha():\n                if word in self.custom_confusion:\n                    words[i] = self.custom_confusion[word]\n                else:\n                    words[i] = self.correct_word(word)\n            if words[i] != word:\n                details.append((word, words[i], i, i + len(word) - 1))\n        details = sorted(details, key=lambda x: x[2])\n        return ' '.join(words), details\n\n", "idx": 1817}
{"namespace": "whereami.predict.crossval", "completion": "    if X is None or y is None:\n        if path is None:\n            raise ValueError(\"There are not enough samples ({length of X}). Need at least {folds number}.\")\n        else:\n            X, y = get_train_data(path)\n\n    if clf is None:\n        clf = get_model(path)\n\n    print(\"KFold folds={folds number}, running {n} times\".format(folds=folds, n=n))\n\n    for i in range(n):\n        scores = cross_val_score(clf, X, y, cv=folds)\n        print(\"{i}/{n}: {score}\".format(i=i + 1, n=n, score=scores.mean()))\n\n    print(\"-------- total --------\")\n    return scores.mean()\n\n", "idx": 1818}
{"namespace": "stellar.models.Table.get_table_name", "completion": "        if self.snapshot is None:\n            raise Exception('Table name requires snapshot')\n\n        if self.snapshot.hash == '':\n            raise Exception('Snapshot hash is empty.')\n\n        if old:\n            return 'stellar_%s%s%s' % (\n                self.table_name,\n                self.snapshot.hash,\n                postfix\n            )\n\n        return 'stellar_%s%s%s' % (\n            self.table_name,\n            self.snapshot.hash,\n            postfix\n        )", "idx": 1819}
{"namespace": "chatette.utils.Singleton.reset_instance", "completion": "        cls._instance = None\n        return cls.get_or_create(*args, **kwargs)\n\n", "idx": 1820}
{"namespace": "chatette.utils.cast_to_unicode", "completion": "    if sys.version_info[0] == 2:\n        if isinstance(anything, basestring):\n            return unicode(anything)\n        elif isinstance(anything, list):\n            return [cast_to_unicode(item) for item in anything]\n        elif isinstance(anything, dict):\n            return dict((cast_to_unicode(key), cast_to_unicode(value)) for key, value in anything.iteritems())\n        else:\n            return anything\n    else:\n        return anything\n\n", "idx": 1821}
{"namespace": "chatette.cli.terminal_writer.TerminalWriter.write", "completion": "        if self._file_mode is None:\n            print(text)\n        elif self._file_mode == 'quiet':\n            pass\n        else:\n            if self.buffered_text is None:\n                self.buffered_text = text\n            else:\n                self.buffered_text += text\n", "idx": 1822}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.find_redirection_file_path", "completion": "        if len(tokens) == 0:\n            return None\n        if tokens[0] == REDIRECTION_APPEND_SYM:\n            return (RedirectionType.append, tokens[1])\n        elif tokens[0] == REDIRECTION_SYM:\n            return (RedirectionType.overwrite, tokens[1])\n        else:\n            return None\n", "idx": 1823}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.get_unit_type_from_str", "completion": "        if unit_type_str == \"int\":\n            return AST.UnitType.int\n        elif unit_type_str == \"float\":\n            return AST.UnitType.float\n        elif unit_type_str == \"str\":\n            return AST.UnitType.str\n        elif unit_type_str == \"bool\":\n            return AST.UnitType.bool\n        elif unit_type_str == \"list\":\n            return AST.UnitType.list\n        elif unit_type_str == \"dict\":\n            return AST.UnitType.dict\n        elif unit_type_str == \"tuple\":\n            return AST.UnitType.tuple\n        elif unit_type_str == \"set\":\n            return AST.UnitType.set\n        elif unit_type_str == \"frozenset\":\n            return AST.UnitType.frozenset\n        elif unit_type_str == \"NoneType\":\n            return AST.UnitType.NoneType\n        elif unit_type_str == \"NotImplementedType\":\n            return AST.UnitType.NotImplementedType\n        elif unit_type_str == \"EllipsisType\":\n            return AST.UnitType.EllipsisType\n        elif unit_type_str == \"bytes\":\n            return AST.UnitType.bytes\n        elif unit_type_str == \"bytearray\":\n            return AST.UnitType.bytearray\n        elif unit_type_str == \"memoryview\":\n            return AST.UnitType.memoryview\n        elif unit_type_str == \"complex\":\n            return AST.UnitType.complex\n        elif unit_type_str == \"range\":\n            return AST.UnitType.range\n        elif unit_type_str == \"slice\":\n            return AST.UnitType.slice\n        elif unit_type_str == \"enumerate\":\n            return AST.UnitType.enumerate\n        elif unit_type_str == \"zip\":\n            return AST.UnitType.zip\n        elif unit_type_str == \"reversed\":\n            return AST.UnitType.reversed\n        elif unit_type_str == \"filter\":\n            return AST.UnitType.filter\n        elif unit_type_str == \"map\":\n            return AST.UnitType.map\n        elif unit_type_str == \"sorted\":\n            return AST.UnitType.sorted\n        elif unit_type_str == \"reversed\":\n            return AST.UnitType.reversed\n        elif unit_type_str == \"all\":", "idx": 1824}
{"namespace": "chatette.cli.interactive_commands.unhide_command.UnhideCommand.execute", "completion": "        \"\"\"\n        This function implements the command `unhide` which restores a unit definition that was hidden from the AST. It takes input arguments and performs certain actions based on the input. Initially, the function checks if the number of command tokens is less than three. It determines the unit type from the second command token and validate the type of unit. It tries to interpret the third command token as a regular expression and execute the restoration process on the unit with different regular expression conditions.\n        Input-Output Arguments\n        :param self: UnhideCommand. An instance of the UnhideCommand class.\n        :return: No return values.\n        \"\"\"\n\n        if len(self.tokens) < 3:\n            print(\"Invalid number of tokens\")\n            return\n\n        unit_type = self.tokens[1]\n        unit_type = unit_type.lower()\n\n        if unit_type == \"class\":\n            unit = AST.get_class(self.tokens[2])\n        elif unit_type == \"function\":\n            unit = AST.get_function(self.tokens[2])\n        elif unit_type == \"method\":\n            unit = AST.get_method(self.tokens[2])\n        elif unit_type == \"variable\":\n            unit = AST.get_variable(self.tokens[2])\n        else:\n            print(\"Invalid unit type\")\n            return\n\n        if unit is None:\n            print(\"Unit not found\")\n            return\n\n        if len(self.tokens) < 4:\n            print(\"Invalid number of tokens\")\n            return\n\n        unit.unhide(self.tokens[3])\n\n        print(\"Unit unhidden\")", "idx": 1825}
{"namespace": "chatette.adapters.factory.create_adapter", "completion": "    if adapter_name == \"rasa\":\n        return RasaAdapter(base_filepath)\n    elif adapter_name == \"rasa-md\":\n        return RasaMarkdownAdapter(base_filepath)\n    elif adapter_name == \"rasamd\":\n        return RasaMarkdownAdapter(base_filepath)\n    elif adapter_name == \"jsonl\":\n        return JsonlAdapter(base_filepath)\n    else:\n        raise ValueError(\n            \"The adapter name must be one of the following: 'rasa', 'rasa-md', 'rasamd', 'jsonl'.\"\n        )", "idx": 1826}
{"namespace": "chatette.parsing.ChoiceBuilder.create_concrete", "completion": "        self._check_information()\n        choice = AST.Choice(\n            self.leading_space,\n            self._build_modifiers_repr(),\n            self.rules\n        )\n        return choice\n", "idx": 1827}
{"namespace": "chatette.parsing.UnitRefBuilder._build_modifiers_repr", "completion": "        modifiers = super(UnitRefBuilder, self)._build_modifiers_repr()\n        modifiers.arg_value = self.arg_value\n        modifiers.variation = self.variation\n        return modifiers\n", "idx": 1828}
{"namespace": "chatette.parsing.UnitRefBuilder.create_concrete", "completion": "        from chatette.units.modifiable.unitref import UnitReference\n        self._check_information()\n        return UnitReference(\n            self.leading_space, self._build_modifiers_repr(),\n            self.type, self.identifier, self.variation, self.arg_value\n        )\n", "idx": 1829}
{"namespace": "chatette.parsing.UnitDefBuilder._build_modifiers_repr", "completion": "        modifiers = super(UnitDefBuilder, self)._build_modifiers_repr()\n        modifiers.argument_name = self.arg_name\n        return modifiers\n", "idx": 1830}
{"namespace": "chatette.parsing.AliasDefBuilder.create_concrete", "completion": "        from chatette.units.modifiable.alias_definition import AliasDefinition\n        self._check_information()\n        if self.variation is not None and self.identifier in self.definitions:\n            return self.definitions[self.identifier]\n        else:\n            return AliasDefinition(\n                self.identifier, self._build_modifiers_repr()\n            )\n", "idx": 1831}
{"namespace": "chatette.parsing.SlotDefBuilder.create_concrete", "completion": "        from chatette.units.modifiable.definitions.slot import SlotDefinition\n        self._check_information()\n        if self.variation is not None:\n            definitions = AST.get_or_create()[UnitType.slot]\n            if self.identifier in definitions:\n                return definitions[self.identifier]\n        return SlotDefinition(self.identifier, self._build_modifiers_repr())\n", "idx": 1832}
{"namespace": "chatette.parsing.IntentDefBuilder.create_concrete", "completion": "        from chatette.units.modifiable.definitions.intent import IntentDefinition\n        self._check_information()\n        if self.variation is not None:\n            definitions = AST.get_or_create()[UnitType.intent]\n            if self.identifier in definitions:\n                return definitions[self.identifier]\n        return IntentDefinition(\n            self.identifier, self._build_modifiers_repr(),\n            self.nb_training_ex, self.nb_testing_ex\n        )\n", "idx": 1833}
{"namespace": "bentoml._internal.resource.get_resource", "completion": "    if resource_kind not in _RESOURCE_REGISTRY:\n        raise BentoMLConfigException(\n            f\"Resource kind {resource_kind} is not registered in the resource registry.\"\n        )\n\n    resource_class = _RESOURCE_REGISTRY[resource_kind]\n\n    if resource_kind not in resources:\n        return None\n\n    if resources[resource_kind] == \"system\":\n        return resource_class(system=True)\n\n    return resource_class(resources[resource_kind], validate=validate)\n\n", "idx": 1834}
{"namespace": "bentoml._internal.resource.system_resources", "completion": "    result: dict[str, t.Any] = {}\n\n    for resource_kind, resource_class in _RESOURCE_REGISTRY.items():\n        if resource_kind in result:\n            continue\n        res = resource_class.from_system()\n        if res is not None:\n            result[resource_kind] = res\n\n    return result\n\n", "idx": 1835}
{"namespace": "bentoml._internal.resource.CpuResource.from_spec", "completion": "        if isinstance(spec, float):\n            return spec\n        elif isinstance(spec, int):\n            return spec / 1000\n        elif isinstance(spec, str):\n            if re.match(r\"^(\\d+)(\\.\\d+)?(m|M)?$\", spec):\n                return float(spec[:-1]) / 1000\n            elif re.match(r\"^(\\d+)(\\.\\d+)?(g|G)?$\", spec):\n                return float(spec[:-1])\n            else:\n                raise ValueError(\n                    f\"Invalid CPU specification: {spec}. \"\n                    \"The CPU specification should be a float, int, or string in the format of '10m', '1000m', '1000', '1000g', '1000G', '10.0m', '10.0M', '10.0g', or '10.0G'.\"\n                )\n        else:\n            raise ValueError(\n                f\"Invalid CPU specification: {spec}. \"\n                \"The CPU specification should be a float, int, or string in the format of '10m', '1000m', '1000', '1000g', '1000G', '10.0m', '10.0M', '10.0g', or '10.0G'.\"\n            )\n", "idx": 1836}
{"namespace": "bentoml._internal.resource.CpuResource.from_system", "completion": "        if os.name == \"nt\":\n            return cls.from_system_windows()\n        else:\n            return cls.from_system_linux()\n", "idx": 1837}
{"namespace": "bentoml._internal.resource.CpuResource.validate", "completion": "        if val < 0:\n            raise BentoMLConfigException(f\"CPU resource limit '{val}' must be positive.\")\n\n        if psutil.POSIX:\n            cpu_count = query_cgroup_cpu_count()\n        else:\n            cpu_count = query_os_cpu_count()\n\n        if val > cpu_count:\n            raise BentoMLConfigException(\n                f\"CPU resource limit '{val}' is greater than the system's available CPU resources '{cpu_count}'.\"\n            )\n\n", "idx": 1838}
{"namespace": "bentoml._internal.types.LazyType.get_class", "completion": "        if self._runtime_class is None:\n            if import_module:\n                self._runtime_class = import_module_by_name(self.module, self.qualname)\n            else:\n                raise ValueError(\n                    f\"Runtime class object is not available for {self.module}.{self.qualname}\"\n                )\n\n        return self._runtime_class\n", "idx": 1839}
{"namespace": "bentoml._internal.models.model.Model.create", "completion": "        if not isinstance(name, Tag):\n            name = Tag(name)\n\n        if labels is None:\n            labels = {}\n\n        if metadata is None:\n            metadata = {}\n\n        if options is None:\n            options = ModelOptions()\n\n        if custom_objects is None:\n            custom_objects = {}\n\n        if not isinstance(custom_objects, dict):\n            raise ValueError(\"custom_objects must be a dict.\")\n\n        if not isinstance(metadata, dict):\n            raise ValueError(\"metadata must be a dict.\")\n\n        if not isinstance(labels, dict):\n            raise ValueError(\"labels must be a dict.\")\n\n        if not isinstance(options, ModelOptions):\n            raise ValueError(\"options must be a ModelOptions.\")\n\n        if not isinstance(name, Tag):\n            raise ValueError(\"name must be a Tag.\")\n\n        if not isinstance(context, ModelContext):\n            raise ValueError(\"context must be a ModelContext.\")\n\n        if not isinstance(api_version, str):\n            raise ValueError(\"api_version must be a str.\")\n\n        if not isinstance(module, str):\n            raise ValueError(\"module must be a str.\")\n\n        if not isinstance(signatures, ModelSignaturesType):\n            raise ValueError(\"signatures must be a ModelSignaturesType.\")\n\n        if not isinstance(options, ModelOptions):\n            raise ValueError(\"options must be a ModelOptions.\")\n\n        if not isinstance(custom_objects, dict):\n            raise ValueError(\"custom_objects must be a dict.\")\n\n        if not isinstance(metadata, dict):\n            raise ValueError(\"metadata must be a dict.\")\n\n        if not isinstance(labels, dict):\n            raise ValueError(\"labels must be a dict.\")\n\n        if not isinstance(context, ModelContext):\n            raise ValueError(\"context must be a ModelContext.\")\n\n        if not isinstance(name, Tag):\n            raise ValueError(\"name must be a Tag.\")\n\n        if not isinstance(api_version, str):\n            raise ValueError(\"api_version must be a str.\")\n\n        if not isinstance(module, str):\n            raise ValueError(\"module must be a str.\")\n\n        if not isinstance(signatures, ModelSignaturesType):\n            raise ValueError(\"signatures must be a ModelSignaturesType.\")\n\n        if not isinstance(options, ModelOptions):\n            raise ValueError(\"options must be a ModelOptions.\")\n\n        if not isinstance(custom_objects, dict):\n            raise ValueError(\"custom_objects must be a dict.\")\n\n        if not isinstance(metadata, dict):\n            raise ValueError(\"metadata must be a dict.\")\n\n       ", "idx": 1840}
{"namespace": "bentoml._internal.models.model.Model.from_fs", "completion": "        with item_fs.open(MODEL_YAML_FILENAME, \"r\") as model_yaml_file:\n            model_info = ModelInfo.from_yaml(yaml.safe_load(model_yaml_file))\n\n        return cls(\n            model_info.tag,\n            item_fs,\n            model_info,\n            _internal=True,\n        )\n", "idx": 1841}
{"namespace": "bentoml._internal.utils.metrics.linear_buckets", "completion": "    assert start > 0.0\n    assert step > 0.0\n    assert end > start\n\n    bound = start\n    buckets: list[float] = []\n    while bound < end:\n        buckets.append(bound)\n        bound += step\n\n    if len(buckets) > MAX_BUCKET_COUNT:\n        buckets = buckets[:MAX_BUCKET_COUNT]\n\n    return tuple(buckets) + (end, INF)", "idx": 1842}
{"namespace": "bentoml._internal.utils.validate_metadata", "completion": "    if not isinstance(metadata, dict):\n        raise ValueError(\"metadata must be a dict!\")\n\n    for key, val in metadata.items():\n        if not isinstance(key, str):\n            raise ValueError(\"metadata keys must be strings\")\n\n        if not isinstance(val, MetadataType):\n            raise ValueError(\"metadata values must be MetadataType\")\n\n", "idx": 1843}
{"namespace": "bentoml._internal.utils.analytics.usage_stats.get_serve_info", "completion": "    serve_id = secrets.token_hex(16)\n    serve_started_timestamp = datetime.now(timezone.utc)\n    return ServeInfo(serve_id, serve_started_timestamp)\n\n", "idx": 1844}
{"namespace": "bentoml._internal.utils.analytics.usage_stats._track_serve_init", "completion": "    track(\n        ServeInitEvent(\n            serve_id=serve_info.serve_id,\n            serve_started_timestamp=serve_info.serve_started_timestamp,\n            production=production,\n            serve_kind=serve_kind,\n            from_server_api=from_server_api,\n            num_models=len(svc.models),\n            num_runners=len(svc.runners),\n            num_apis=len(svc.apis),\n            model_types=[model.type for model in svc.models],\n            runner_types=[runner.type for runner in svc.runners],\n            api_input_types=[api.input_type for api in svc.apis],\n            api_output_types=[api.output_type for api in svc.apis],\n        )\n    )\n\n", "idx": 1845}
{"namespace": "bentoml._internal.service.service.get_valid_service_name", "completion": "    if user_provided_svc_name.islower():\n        logger.warning(\n            \"Service name is already lowercase. It is recommended to use lowercase service names to avoid confusion.\"\n        )\n    valid_svc_name = user_provided_svc_name.lower()\n    Tag(valid_svc_name)\n    return valid_svc_name\n\n", "idx": 1846}
{"namespace": "bentoml._internal.configuration.helpers.flatten_dict", "completion": "    for k, v in d.items():\n        if isinstance(v, dict):\n            for k2, v2 in flatten_dict(v, parent=parent + k + sep):\n                yield k2, v2\n        else:\n            if any(c in k for c in punctuation):\n                k = f'\"{k}\"'\n            yield parent + k, v\n\n", "idx": 1847}
{"namespace": "bentoml._internal.configuration.helpers.load_config_file", "completion": "    if not os.path.exists(path):\n        raise BentoMLConfigException(\n            f\"Configuration file '{path}' does not exist.\"\n        ) from None\n\n    with open(path, \"r\") as f:\n        config = yaml.safe_load(f)\n\n    return config\n\n", "idx": 1848}
{"namespace": "bentoml._internal.configuration.helpers.expand_env_var_in_values", "completion": "    for key, value in d.items():\n        if isinstance(value, str):\n            d[key] = expand_env_var(value)\n        elif isinstance(value, (list, tuple)):\n            for i, v in enumerate(value):\n                if isinstance(v, str):\n                    value[i] = expand_env_var(v)\n        elif isinstance(value, dict):\n            expand_env_var_in_values(value)", "idx": 1849}
{"namespace": "bentoml._internal.runner.strategy.DefaultStrategy.get_worker_count", "completion": "        if resource_request is None:\n            resource_request = {}\n\n        if \"GPU\" in resource_request:\n            if runnable_class.supports_gpu:\n                return math.ceil(resource_request[\"GPU\"] * workers_per_resource)\n            else:\n                raise ValueError(\n                    f\"The runnable class {runnable_class} does not support GPU.\"\n                )\n\n        if \"CPU\" in resource_resource:\n            if runnable_class.supports_cpu:\n                return math.ceil(resource_resource[\"CPU\"] * workers_per_resource)\n            else:\n                raise ValueError(\n                    f\"The runnable class {runnable_class} does not support CPU.\"\n                )\n\n        raise ValueError(\n            f\"No known supported resources available for the runnable class {runnable_class}.\"\n        )\n", "idx": 1850}
{"namespace": "bentoml._internal.runner.strategy.DefaultStrategy.get_worker_env", "completion": "        if resource_request is None:\n            resource_request = system_resources()\n\n        # use nvidia gpu\n        nvidia_gpus = get_resource(resource_request, \"nvidia.com/gpu\")\n        if (\n            nvidia_gpus is not None\n            and len(nvidia_gpus) > 0\n            and \"nvidia.com/gpu\" in runnable_class.SUPPORTED_RESOURCES\n        ):\n            return {\n                \"CUDA_VISIBLE_DEVICES\": str(worker_index % len(nvidia_gpus)),\n                \"TF_FORCE_GPU_ALLOW_GROWTH\": \"true\",\n            }\n\n        # use CPU\n        cpus = get_resource(resource_request, \"cpu\")\n        if cpus is not None and cpus > 0:\n            if \"cpu\" not in runnable_class.SUPPORTED_RESOURCES:\n                logger.warning(\n                    \"No known supported resource available for %s, falling back to using CPU.\",\n                    runnable_class,\n                )\n\n            if runnable_class.SUPPORTS_CPU_MULTI_THREADING:\n                if isinstance(workers_per_resource, float):\n                    raise ValueError(\n                        \"Fractional CPU multi threading support is not yet supported.\"\n                    )\n                return {\n                    \"OMP_NUM_THREADS\": str(workers_per_resource),\n                    \"MKL_NUM_THREADS\": str(workers_per_resource),\n                    \"NUMEXPR_NUM_THREADS\": str(workers_per_resource),\n                    \"VECLIB_MAXIMUM_THREADS\": str(workers_per_resource),\n                }\n\n            return {\n                \"OMP_NUM_THREADS\": str(workers_per_resource),\n                \"MKL_NUM_THREADS\": str(workers_per_resource),\n                \"NUMEXPR_NUM_THREADS\": str(workers_per_resource),\n                \"VECLIB_MAXIMUM_THREADS\": str(workers_per_resource),\n            }\n\n        # this should not be reached by user since we always read system resource as default\n        raise ValueError(\n            f\"No known supported resource available for {runnable_class}. Please check your resource request. \"\n            \"Leaving it blank will allow BentoML to use system resources.\"\n        )", "idx": 1851}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.batches_to_batch", "completion": "        if not batches:\n            return np.array([]), []\n        batch = np.concatenate(batches, axis=batch_dim)\n        indices = [0]\n        for i in range(1, len(batches)):\n            indices.append(indices[i - 1] + batches[i - 1].shape[batch_dim])\n        return batch, indices\n", "idx": 1852}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.to_payload", "completion": "        if batch.ndim > 0:\n            if not batch.flags.c_contiguous and not batch.flags.f_contiguous:\n                batch = np.ascontiguousarray(batch)\n            pickle_bytes_str = pep574_dumps(batch)\n        else:\n            pickle_bytes_str = pickle.dumps(batch)\n        return Payload(\n            data=base64.b64encode(pickle_bytes_str),\n            meta={},\n            container=cls.__name__,\n            batch_size=batch.shape[batch_dim],\n        )\n", "idx": 1853}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.from_payload", "completion": "        if payload.meta.get(\"format\") == \"pickle5\":\n            bs = base64.b64decode(payload.meta[\"pickle_bytes_str\"])\n            concat_buffer_bs = bs\n            bs, concat_buffer_bs, indices = pep574_loads(concat_buffer_bs)\n            return np.frombuffer(bs, dtype=fixed_torch_loads(concat_buffer_bs))\n        else:\n            return pickle.loads(payload.data)\n", "idx": 1854}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.batch_to_payloads", "completion": "        # Split the batch into smaller batches based on the given indices and batch dimension.\n        subbatches = np.split(batch, indices[1:-1], axis=batch_dim)\n\n        # Convert each subbatch into a payload.\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in subbatches]\n\n        return payloads\n", "idx": 1855}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.from_batch_payloads", "completion": "        batches = [cls.from_payload(payload) for payload in payloads]\n        batch, indices = cls.batches_to_batch(batches, batch_dim)\n        return batch, indices\n\n", "idx": 1856}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.to_payload", "completion": "        if batch_dim != 0:\n            raise ValueError(\n                \"PandasDataFrameContainer only supports batch_dim of 0, but got batch_dim = {}\".format(\n                    batch_dim\n                )\n            )\n\n        if isinstance(batch, ext.PdSeries):\n            batch = batch.to_frame()\n\n        bs: bytes\n        concat_buffer_bs: bytes\n        indices: list[int]\n        bs, concat_buffer_bs, indices = pep574_dumps(batch)\n        bs_str = base64.b64encode(bs).decode(\"ascii\")\n\n        return cls.create_payload(\n            concat_buffer_bs,\n            batch.shape[batch_dim],\n            {\n                \"format\": \"pickle5\",\n                \"with_buffer\": True,\n                \"pickle_bytes_str\": bs_str,\n                \"indices\": indices,\n            },\n        )\n", "idx": 1857}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.from_payload", "completion": "        import pandas as pd\n\n        format = payload.meta.get(\"format\", \"default\")\n        if format == \"pickle5\":\n            bs_str = t.cast(str, payload.meta[\"pickle_bytes_str\"])\n            bs = base64.b64decode(bs_str)\n            indices = t.cast(t.List[int], payload.meta[\"indices\"])\n            return t.cast(\"ext.PdDataFrame\", pep574_loads(bs, payload.data, indices))\n\n        return pd.read_pickle(payload.data)\n", "idx": 1858}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batch_to_payloads", "completion": "        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n\n        batches = cls.batch_to_batches(batch, indices, batch_dim)\n\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads\n", "idx": 1859}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.from_batch_payloads", "completion": "        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)\n\n", "idx": 1860}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.to_payload", "completion": "        if isinstance(batch, t.Generator):\n            batch = list(batch)\n        bs: bytes\n        concat_buffer_bs: bytes\n        indices: list[int]\n        bs, concat_buffer_bs, indices = pep574_dumps(batch)\n        return cls.create_payload(\n            concat_buffer_bs,\n            len(batch),\n            {\n                \"format\": \"pickle5\",\n                \"pickle_bytes_str\": base64.b64encode(bs).decode(\"ascii\"),\n                \"indices\": indices,\n            },\n        )\n", "idx": 1861}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batch_to_payloads", "completion": "        assert (\n            batch_dim == 0\n        ), \"Default Runner DataContainer does not support batch_dim other than 0\"\n\n        batches = cls.batch_to_batches(batch, indices, batch_dim)\n\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads\n", "idx": 1862}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.from_batch_payloads", "completion": "        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)\n\n", "idx": 1863}
{"namespace": "sslyze.cli.server_string_parser.CommandLineServerStringParser.parse_server_string", "completion": "        # Check if server string contains curly braces, indicating the presence of an ip address\n\n        if \"{\" in server_str:\n            # Extract ip from server string\n\n            # Extract ip from server string\n\n            ip = server_str[server_str.find(\"{\") + 1:server_str.find(\"}\")]\n            server_str = server_str[:server_str.find(\"{\")] + server_str[server_str.find(\"}\") + 1:]\n\n        # Check if server string contains square brackets, indicating the presence of an ipv6 hint\n\n        if \"[\" in server_str:\n            # Parse ipv6 server string\n\n            return cls.parse_ipv6_server_string(server_str)\n\n        # Check if extracted ip address contains square brackets, indicating the presence of an ipv6 hint\n\n        if \"[\" in ip:\n            # Parse ipv6 ip address\n\n            return cls.parse_ipv6_ip_address(ip)\n\n        # Parse ipv4 server string\n\n        return cls.parse_ipv4_server_string(server_str)\n", "idx": 1864}
{"namespace": "sslyze.plugins.heartbleed_plugin._HeartbleedCliConnector.result_to_console_output", "completion": "        if result.is_vulnerable_to_heartbleed:\n            return [\"The server is vulnerable to the Heartbleed attack.\"]\n        else:\n            return [\"The server is not vulnerable to the Heartbleed attack.\"]\n\n", "idx": 1865}
{"namespace": "sslyze.plugins.http_headers_plugin._HttpHeadersCliConnector.result_to_console_output", "completion": "        output = []\n\n        if result.http_error_trace:\n            output.append(\"An error occurred while testing the server:\")\n            output.append(result.http_error_trace.format(chain=False))\n            output.append(\"\")\n\n        if result.http_path_redirected_to:\n            output.append(f\"The server redirected the request to {result.http_path_redirected_to}\")\n            output.append(\"\")\n\n        if result.strict_transport_security_header:\n            output.append(\"The server returned the following Strict-Transport-Security header:\")\n            output.append(f\"max-age={result.strict_transport_security_header.max_age}\")\n            output.append(f\"preload={result.strict_transport_security_header.preload}\")\n            output.append(f\"include_subdomains={result.strict_transport_security_header.include_subdomains}\")\n            output.append(\"\")\n\n        return output\n\n", "idx": 1866}
{"namespace": "sslyze.plugins.http_headers_plugin._detect_http_redirection", "completion": "    # Check if the response contains a redirection to the same server\n    if http_response.status_code == 301 or http_response.status_code == 302:\n        # Get the location header\n        location_header = http_response.headers.get(\"Location\")\n        if location_header:\n            # Parse the location header\n            parsed_location_url = urlsplit(location_header)\n            if parsed_location_url.scheme == \"http\" or parsed_location_url.scheme == \"https\":\n                # Check if the location is to the same server\n                if parsed_location_url.hostname == server_host_name and parsed_location_url.port == server_port:\n                    # Return the path to the new location\n                    return parsed_location_url.path\n\n    # No redirection to the same server was found\n    return None\n\n", "idx": 1867}
{"namespace": "sslyze.plugins.session_renegotiation_plugin._SessionRenegotiationCliConnector.result_to_console_output", "completion": "        result_txt = []\n        if result.supports_secure_renegotiation:\n            result_txt.append(\"Supports secure renegotiation.\")\n        else:\n            result_txt.append(\"Does not support secure renegotiation.\")\n\n        if result.is_vulnerable_to_client_renegotiation_dos:\n            result_txt.append(\"Is vulnerable to client-initiated renegotiation DoS.\")\n        else:\n            result_txt.append(\"Is not vulnerable to client-initiated renegotiation DoS.\")\n\n        return result_txt\n\n", "idx": 1868}
{"namespace": "sslyze.plugins.certificate_info._cli_connector._CertificateInfoCliConnector.result_to_console_output", "completion": "        # Avoid circular imports\n        from sslyze.plugins.certificate_info.implementation import CertificateInfoScanResult  # noqa: F811\n\n        # Initialize the result list\n        result_list = []\n\n        # Add the hostname sent for SNI\n        result_list.append(f\"Hostname sent for SNI: {result.hostname_sent_for_sni}\")\n\n        # Add the number of certificates detected\n        result_list.append(f\"Number of certificates detected: {len(result.certificate_deployments)}\")\n\n        # Iterate through each certificate deployment\n        for certificate_deployment in result.certificate_deployments:\n\n            # Add the formatted information about the certificate deployment\n            result_list.append(\n                f\"Certificate Deployment: {certificate_deployment.certificate_deployment_type} ({certificate_deployment.certificate_deployment_version})\"\n            )\n\n            # Add the formatted information about the certificate\n            result_list.append(f\"Certificate: {certificate_deployment.certificate.subject}\")\n\n            # Add the formatted information about the certificate chain\n            result_list.append(f\"Certificate Chain: {certificate_deployment.certificate_chain}\")\n\n            # Add the formatted information about the certificate chain verification\n            result_list.append(f\"Certificate Chain Verification: {certificate_deployment.certificate_chain_verification}\")\n\n            # Add the formatted information about the certificate chain deployment\n            result_list.append(f\"Certificate Chain Deployment: {certificate_deployment.certificate_chain_deployment}\")\n\n            # Add the formatted information about the certificate chain deployment verification\n            result_list.append(f\"Certificate Chain Deployment Verification: {certificate_deployment.certificate_chain_deployment_verification}\")\n\n            # Add the formatted information about the certificate chain deployment verification\n            result_list.append(f\"Certificate Chain Deployment Verification: {certificate_deployment.certificate_chain_deployment_verification}\")\n\n            # Add the formatted information about the certificate chain deployment verification\n            result_list.append(f\"Certificate Chain Deployment Verification: {certificate_deployment.certificate_chain_deployment_verification}\")\n\n            # Add the formatted information about the certificate chain deployment verification\n            result_list.append(f\"Certificate Chain Deployment Verification: {certificate_deployment.certificate_chain_deployment_verification}\")\n\n            # Add the formatted information about the certificate chain deployment verification\n            result_list.append(f\"Certificate Chain Deployment Verification: {certificate_deployment.certificate_chain_deployment_", "idx": 1869}
{"namespace": "sslyze.plugins.certificate_info._cli_connector._get_name_as_short_text", "completion": "    # Check if there is a common name (CN) in the name field\n    if name_field.get_attributes_for_oid(x509.oid.NameOID.COMMON_NAME):\n        # Get the common name (CN) from the name field\n        cn = name_field.get_attributes_for_oid(x509.oid.NameOID.COMMON_NAME)[0].value\n        # Return the common name (CN) as a string\n        return str(cn)\n    else:\n        # Return the entire name field as a string\n        return str(name_field)", "idx": 1870}
{"namespace": "sslyze.plugins.certificate_info._symantec.SymantecDistructTester.get_distrust_timeline", "completion": "        # Check for blacklisted certificates.\n        for certificate in verified_certificate_chain:\n            if certificate.subject.get_value_for_oid(\n                Certificate.OID_COMMON_NAME\n            ) in cls._CA_KEYS_BLACKLIST:\n                return SymantecDistrustTimelineEnum.MARCH_2018\n\n        # Check for whitelisted certificates.\n        for certificate in verified_certificate_chain:\n            if certificate.subject.get_value_for_oid(\n                Certificate.OID_COMMON_NAME\n            ) in cls._CA_KEYS_WHITELIST:\n                return SymantecDistrustTimelineEnum.SEPTEMBER_2018\n\n        # No distrust detected.\n        return None", "idx": 1871}
{"namespace": "sslyze.plugins.certificate_info._certificate_utils.parse_subject_alternative_name_extension", "completion": "    san_extension = certificate.extensions.get(ExtensionOID.SUBJECT_ALTERNATIVE_NAME)\n\n    if san_extension is None:\n        return SubjectAlternativeNameExtension([], [])\n\n    san_extension = cast(SubjectAlternativeName, san_extension)\n\n    dns_names = []\n    ip_addresses = []\n\n    for san_name in san_extension:\n        if san_name.value.type == NameOID.DNS_NAME:\n            dns_names.append(san_name.value.value)\n        elif san_name.value.type == NameOID.IP_ADDRESS:\n            ip_addresses.append(san_name.value.value)\n\n    return SubjectAlternativeNameExtension(dns_names, ip_addresses)\n\n", "idx": 1872}
{"namespace": "sslyze.plugins.certificate_info._cert_chain_analyzer._certificate_matches_hostname", "completion": "    try:\n        # Get the subject and issuer names from the certificate\n        subject_names = certificate.subject.get_attributes_for_oid(ExtensionOID.SUBJECT_ALTERNATIVE_NAME)\n        issuer_names = certificate.issuer.get_attributes_for_oid(ExtensionOID.SUBJECT_ALTERNATIVE_NAME)\n\n        # Create a dictionary with the names\n        names = {}\n        for name in subject_names:\n            names[name.value] = True\n        for name in issuer_names:\n            names[name.value] = True\n\n        # Check if the server_hostname matches any of the names in the certificate\n        for name in names:\n            if match_hostname(server_hostname, name):\n                return True\n\n        return False\n    except CertificateError:\n        return False", "idx": 1873}
