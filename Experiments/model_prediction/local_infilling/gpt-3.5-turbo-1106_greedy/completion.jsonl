{"namespace": "boto.dynamodb2.table.Table.update_global_secondary_index", "completion": "    gsi_data = []\n\n    for gsi_name, gsi_throughput in global_indexes.items():\n        gsi_data.append({\n            \"Update\": {\n                \"IndexName\": gsi_name,\n                \"ProvisionedThroughput\": {\n                    \"ReadCapacityUnits\": int(gsi_throughput['read']),\n                    \"WriteCapacityUnits\": int(gsi_throughput['write']),\n                },\n            },\n        })\n\n    if gsi_data:\n        self.connection.update_table(\n            self.table_name,\n            global_secondary_index_updates=gsi_data,\n        )\n        return True\n    else:\n        msg = 'No global indexes provided for update'\n        boto.log.error(msg)\n        return False", "idx": 0}
{"namespace": "boto.dynamodb2.table.Table.delete", "completion": "        self.connection.delete_table(self.table_name)\n        return True", "idx": 1}
{"namespace": "boto.dynamodb2.table.Table.get_item", "completion": "        raw_key = self._encode_keys(kwargs)\n        kwargs = {}\n\n        if consistent:\n            kwargs['consistent_read'] = True\n\n        if attributes is not None:\n            kwargs['attributes_to_get'] = attributes\n\n        raw_item = self.connection.get_item(self.table_name, raw_key, **kwargs)\n\n        if 'Item' not in raw_item:\n            raise exceptions.ItemNotFound\n\n        item = Item(self)\n        item.load(raw_item)\n        return item", "idx": 2}
{"namespace": "boto.dynamodb2.table.Table.has_item", "completion": "        try:\n            self.get_item(consistent=kwargs.get('consistent', False), attributes=kwargs.get('attributes', None), **kwargs)\n            return True\n        except exceptions.ItemNotFound:\n            return False", "idx": 3}
{"namespace": "boto.dynamodb2.table.Table._put_item", "completion": "        raw_key = self._encode_keys(item_data)\n        kwargs = {}\n\n        if expects is not None:\n            kwargs['expected'] = expects\n\n        self.connection.put_item(self.table_name, raw_key, item_data, **kwargs)\n        return True", "idx": 4}
{"namespace": "boto.dynamodb2.table.Table.delete_item", "completion": "        raw_key = self._encode_keys(kwargs)\n        kwargs = {}\n\n        if expected is not None:\n            kwargs['expected'] = expected\n\n        if conditional_operator is not None:\n            kwargs['conditional_operator'] = conditional_operator\n\n        try:\n            self.connection.delete_item(\n                self.table_name,\n                raw_key,\n                **kwargs\n            )\n            return True\n        except Exception as e:\n            boto.log.error(\"Delete operation failed: %s\" % e)\n            return False", "idx": 5}
{"namespace": "boto.dynamodb2.table.Table.get_key_fields", "completion": "    if not self.schema:\n        self.describe()\n    key_fields = []\n    for field in self.schema:\n        key_fields.append(field.name)\n    return key_fields", "idx": 6}
{"namespace": "boto.dynamodb2.table.Table._build_filters", "completion": "        filters = {}\n        for key, value in filter_kwargs.items():\n            field, operator = key.rsplit('__', 1)\n            if operator not in using:\n                raise ValueError(f\"Invalid operator: {operator}\")\n            if field not in filters:\n                filters[field] = {}\n            filters[field][using[operator]] = value\n        return filters", "idx": 7}
{"namespace": "boto.dynamodb2.table.Table.batch_get", "completion": "        results = ResultSet(\n            max_page_size=None\n        )\n        kwargs = {\n            'keys': keys,\n            'consistent': consistent,\n            'attributes': attributes\n        }\n        results.to_call(self._batch_get, **kwargs)\n        return results", "idx": 8}
{"namespace": "boto.dynamodb2.table.Table.count", "completion": "        result = self.connection.describe_table(self.table_name)\n        return result['Table']['ItemCount']", "idx": 9}
{"namespace": "boto.dynamodb2.table.BatchTable.put_item", "completion": "    item = Item(self.table, data=data)\n    self._to_put.append(item.prepare_full())\n\n    if self.should_flush():\n        self.flush()", "idx": 10}
{"namespace": "boto.dynamodb2.table.BatchTable.delete_item", "completion": "        self._to_delete.append(kwargs)\n\n        if self.should_flush():\n            self.flush()", "idx": 11}
{"namespace": "boto.dynamodb2.table.BatchTable.flush", "completion": "        batch_data = {}\n        if self._to_put:\n            batch_data[self.table.table_name] = [{'PutRequest': {'Item': item}} for item in self._to_put]\n            self._to_put = []\n\n        if self._to_delete:\n            batch_data[self.table.table_name] = [{'DeleteRequest': {'Key': key}} for key in self._to_delete]\n            self._to_delete = []\n\n        if batch_data:\n            resp = self.table.connection.batch_write_item(batch_data)\n            self.handle_unprocessed(resp)\n\n        return True", "idx": 12}
{"namespace": "boto.dynamodb2.table.BatchTable.resend_unprocessed", "completion": "        while self._unprocessed:\n            batch_data = {\n                self.table.table_name: self._unprocessed[:25]\n            }\n            resp = self.table.connection.batch_write_item(batch_data)\n            self._unprocessed = resp.get('UnprocessedItems', {}).get(self.table.table_name, [])", "idx": 13}
{"namespace": "boto.dynamodb2.fields.BaseSchemaField.definition", "completion": "        return {\n            'AttributeName': self.name,\n            'AttributeType': self.data_type,\n        }", "idx": 14}
{"namespace": "boto.dynamodb2.fields.BaseIndexField.definition", "completion": "        attribute_definition = []\n\n        for part in self.parts:\n            attribute_definition.append(part.definition())\n\n        return attribute_definition", "idx": 15}
{"namespace": "boto.dynamodb2.fields.BaseIndexField.schema", "completion": "        schema_data = super(BaseIndexField, self).schema()\n        schema_data['IndexName'] = self.name\n        schema_data['KeySchema'] = []\n\n        for part in self.parts:\n            schema_data['KeySchema'].append(part.schema())\n\n        schema_data['Projection'] = {\n            'ProjectionType': self.projection_type\n        }\n\n        return schema_data", "idx": 16}
{"namespace": "boto.dynamodb2.fields.GlobalBaseIndexField.schema", "completion": "        # Pick up the base schema structure\n        schema_data = super(GlobalBaseIndexField, self).schema()\n        # Add the provisioned throughput information\n        schema_data['ProvisionedThroughput'] = {\n            'ReadCapacityUnits': self.throughput['read'],\n            'WriteCapacityUnits': self.throughput['write']\n        }\n        return schema_data", "idx": 17}
{"namespace": "boto.dynamodb2.fields.GlobalIncludeIndex.schema", "completion": "        # Pick up the includes.\n        schema_data = super(GlobalIncludeIndex, self).schema()\n\n        # Update with schema data from GlobalBaseIndexField superclass\n        schema_data['ProvisionedThroughput'] = {\n            'ReadCapacityUnits': int(self.throughput['read']),\n            'WriteCapacityUnits': int(self.throughput['write']),\n        }\n        return schema_data", "idx": 18}
{"namespace": "boto.dynamodb2.items.Item.get_keys", "completion": "        keys = {}\n        for key in self.table.key_schema:\n            keys[key] = self._data.get(key, None)\n        return keys", "idx": 19}
{"namespace": "boto.dynamodb2.items.Item.get_raw_keys", "completion": "        raw_keys = {}\n        for key, value in self._data.items():\n            raw_keys[key] = self._dynamizer.encode(value)\n        return raw_keys", "idx": 20}
{"namespace": "boto.dynamodb2.items.Item.build_expects", "completion": "        if fields is None:\n            fields = self.keys()\n\n        expects = {}\n\n        for field in fields:\n            if field in self._data and field in self._orig_data:\n                if self._data[field] != self._orig_data[field]:\n                    expects[field] = self._dynamizer.encode(self._orig_data[field])\n            elif field in self._data and field not in self._orig_data:\n                expects[field] = NEWVALUE\n            elif field not in self._data and field in self._orig_data:\n                expects[field] = None\n\n        return expects", "idx": 21}
{"namespace": "boto.dynamodb2.items.Item.prepare_full", "completion": "        final_data = {}\n        for key, value in self._data.items():\n            final_data[key] = self._dynamizer.encode(value)\n        return final_data", "idx": 22}
{"namespace": "boto.dynamodb2.items.Item.prepare_partial", "completion": "        alterations = self._determine_alterations()\n        final_data = {}\n        fields = set()\n\n        for key, value in alterations['adds'].items():\n            if self._is_storable(value):\n                final_data[key] = self._dynamizer.encode(value)\n                fields.add(key)\n\n        for key, value in alterations['changes'].items():\n            if self._is_storable(value):\n                final_data[key] = self._dynamizer.encode(value)\n                fields.add(key)\n\n        return final_data, fields", "idx": 23}
{"namespace": "boto.dynamodb2.items.Item.partial_save", "completion": "        if not self.needs_save():\n            return False\n\n        final_data, fields = self.prepare_partial()\n        expects = self.build_expects(fields)\n\n        returned = self.table._update_item(final_data, expects=expects)\n        # Mark the object as clean.\n        self.mark_clean()\n        return returned", "idx": 24}
{"namespace": "boto.dynamodb2.items.Item.save", "completion": "        if not overwrite and not self.needs_save():\n            return False\n\n        if overwrite:\n            returned = self.table.put_item(self.prepare_full())\n        else:\n            returned = self.partial_save()\n\n        return returned", "idx": 25}
{"namespace": "boto.dynamodb2.items.Item.delete", "completion": "        key = self.get_keys()\n        return self.table._delete_item(key)", "idx": 26}
{"namespace": "boto.dynamodb2.connect_to_region", "completion": "        from boto.dynamodb2.layer1 import DynamoDBConnection\n    from boto.dynamodb2.layer1 import DynamoDBConnection\n    region = RegionInfo(name=region_name, endpoint='dynamodb.' + region_name + '.amazonaws.com')\n    return DynamoDBConnection(region=region, **kw_params)", "idx": 27}
{"namespace": "litecli.packages.parseutils.extract_tables", "completion": "    parsed = sqlparse.parse(sql)[0]\n    stream = extract_from_part(parsed)\n    return list(extract_table_identifiers(stream))", "idx": 28}
{"namespace": "litecli.packages.parseutils.queries_start_with", "completion": "\n    for query in queries:\n        formatted_sql = sqlparse.format(query.lower(), strip_comments=True)\n        if formatted_sql.split()[0] in prefixes:\n            return True\n    return False", "idx": 29}
{"namespace": "litecli.packages.parseutils.is_destructive", "completion": "\n    destructive_keywords = [\"DROP\", \"DELETE\", \"TRUNCATE\", \"ALTER\"]\n\n    for query in sqlparse.split(queries):\n        if query_starts_with(query, destructive_keywords):\n            return True\n\n    return False", "idx": 30}
{"namespace": "litecli.packages.completion_engine.suggest_type", "completion": "\n    # Parse the text before the cursor to determine the type and scope of the completion\n    parsed = sqlparse.parse(text_before_cursor)\n\n    # Get the last token before the cursor\n    last_token = parsed.token_prev(len(parsed.tokens))\n\n    # Get the identifier if present\n    identifier = None\n    if last_token and isinstance(last_token, Identifier):\n        identifier = last_token\n\n    # Call the suggest_based_on_last_token function to get the suggestions based on the last token\n    suggestions = suggest_based_on_last_token(last_token, text_before_cursor, full_text, identifier)\n\n    return suggestions", "idx": 31}
{"namespace": "datasette.plugins.get_plugins", "completion": "    plugins_info = []\n    for plugin in pm.get_plugins():\n        info = {\n            \"name\": plugin.name,\n            \"static_path\": getattr(plugin, \"static_path\", None),\n            \"templates_path\": getattr(plugin, \"templates_path\", None),\n            \"hooks\": [hook for hook in dir(plugin) if hook.startswith(\"hook_\")],\n            \"version\": getattr(plugin, \"version\", None),\n            \"project_name\": getattr(plugin, \"project_name\", None)\n        }\n        plugins_info.append(info)\n    return plugins_info", "idx": 32}
{"namespace": "datasette.facets.ColumnFacet.suggest", "completion": "        row_count = await self.get_row_count()\n        columns = await self.get_columns(self.sql, self.params)\n        suggested_facets = []\n        for column in columns:\n            facet_sql = f\"\"\"\n                select {column} as value, count(*) as count from (\n                    {self.sql}\n                )\n                where {column} is not null\n                group by {column} order by count desc, value\n            \"\"\"\n            try:\n                facet_rows_results = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_suggest_time_limit_ms\"),\n                )\n                distinct_values = len(facet_rows_results.rows)\n                if 1 < distinct_values <= row_count and distinct_values <= self.get_facet_size():\n                    for row in facet_rows_results.rows:\n                        if row[\"count\"] > 1:\n                            suggested_facets.append(\n                                {\n                                    \"name\": column,\n                                    \"toggle_url\": self.ds.urls.path(\n                                        path_with_added_args(self.request, {\"_facet\": column})\n                                    ),\n                                }\n                            )\n                            break\n            except QueryInterrupted:\n                continue\n        return suggested_facets", "idx": 33}
{"namespace": "datasette.facets.ColumnFacet.facet_results", "completion": "        facet_results = []\n        facets_timed_out = []\n\n        facet_size = self.get_facet_size()\n        for source_and_config in self.get_configs():\n            config = source_and_config[\"config\"]\n            source = source_and_config[\"source\"]\n            column = config.get(\"column\") or config[\"simple\"]\n            suggested_facet_sql = \"\"\"\n                select {column} as value, count(*) as n from (\n                    {sql}\n                ) where value is not null\n                group by value\n                limit {limit}\n            \"\"\".format(\n                column=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            try:\n                distinct_values = await self.ds.execute(\n                    self.database,\n                    suggested_facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_suggest_time_limit_ms\"),\n                )\n                num_distinct_values = len(distinct_values)\n                if (\n                    1 < num_distinct_values < row_count\n                    and num_distinct_values <= facet_size\n                    and any(r[\"n\"] > 1 for r in distinct_values)\n                ):\n                    facet_results.append(\n                        {\n                            \"name\": column,\n                            \"type\": self.type,\n                            \"results\": distinct_values,\n                            \"hideable\": source != \"metadata\",\n                            \"toggle_url\": self.ds.urls.path(\n                                path_with_removed_args(\n                                    self.request, {\"_facet\": column}\n                                )\n                            ),\n                            \"truncated\": len(distinct_values) > facet_size,\n                        }\n                    )\n            except QueryInterrupted:\n                facets_timed_out.append(column)\n\n        return facet_results, facets_timed_out", "idx": 34}
{"namespace": "datasette.facets.ArrayFacet.suggest", "completion": "        columns = await self.get_columns(self.sql, self.params)\n        already_enabled = [c[\"config\"][\"simple\"] for c in self.get_configs()]\n        suggested_facets = []\n        for column in columns:\n            if column in already_enabled:\n                continue\n            suggested_facet_sql = f\"\"\"\n                select {column} from (\n                    {self.sql}\n                ) where {column} is not null limit 100;\n            \"\"\"\n            try:\n                results = await self.ds.execute(\n                    self.database,\n                    suggested_facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_suggest_time_limit_ms\"),\n                    log_sql_errors=False,\n                )\n                values = [r[0] for r in results.rows]\n                if all(isinstance(value, str) for value in values):\n                    suggested_facets.append(\n                        {\n                            \"name\": column,\n                            \"type\": \"array\",\n                            \"toggle_url\": self.ds.absolute_url(\n                                self.request,\n                                self.ds.urls.path(\n                                    path_with_added_args(\n                                        self.request, {\"_facet_array\": column}\n                                    )\n                                ),\n                            ),\n                        }\n                    )\n            except (QueryInterrupted, sqlite3.OperationalError):\n                continue\n        return suggested_facets", "idx": 35}
{"namespace": "datasette.facets.ArrayFacet.facet_results", "completion": "        facet_results = []\n        facets_timed_out = []\n\n        qs_pairs = self.get_querystring_pairs()\n\n        facet_size = self.get_facet_size()\n        for source_and_config in self.get_configs():\n            config = source_and_config[\"config\"]\n            source = source_and_config[\"source\"]\n            column = config.get(\"column\") or config[\"simple\"]\n            facet_sql = \"\"\"\n                select {col} as value, count(*) as count from (\n                    {sql}\n                )\n                where {col} is not null\n                group by {col} order by count desc, value limit {limit}\n            \"\"\".format(\n                col=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            try:\n                facet_rows_results = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n                facet_results_values = []\n                facet_results.append(\n                    {\n                        \"name\": column,\n                        \"type\": self.type,\n                        \"hideable\": source != \"metadata\",\n                        \"toggle_url\": self.ds.urls.path(\n                            path_with_removed_args(self.request, {\"_facet\": column})\n                        ),\n                        \"results\": facet_results_values,\n                        \"truncated\": len(facet_rows_results) > facet_size,\n                    }\n                )\n                facet_rows = facet_rows_results.rows[:facet_size]\n                if self.table:\n                    # Attempt to expand foreign keys into labels\n                    values = [row[\"value\"] for row in facet_rows]\n                    expanded = await self.ds.expand_foreign_keys(\n                        self.database, self.table, column, values\n                    )\n                else:\n                    expanded = {}\n                for row in facet_rows:\n                    column_qs = column\n                    if column.startswith(\"_\"):\n                        column_qs = \"{}__exact\".format(column)\n                    selected = (column_qs, str(row[\"value\"])) in qs_pairs\n                    if selected:\n                        toggle_path = path_with_removed_args(\n                            self.request, {column_qs: str(row[\"value\"])}\n                        )\n                    else:\n                        toggle_path = path_with_added_args(\n                            self.request, {column_qs: row[\"value\"]}\n                        )\n                    facet_results_values.append(\n                        {\n                            \"value\": row[\"value\"],\n                            \"label\": expanded.get((column, row[\"value\"]), row[\"value\"]),\n                            \"count\": row[\"count\"],\n                            \"toggle_url\": self.ds.absolute_url(\n                                self.request, self.ds.urls.path(toggle_path)\n                            ),\n                            \"selected\": selected,\n                        }\n                    )\n            except QueryInterrupted:\n                facets_timed_out.append(column)\n\n        return facet_results, facets_timed_out", "idx": 36}
{"namespace": "datasette.facets.DateFacet.facet_results", "completion": "        facet_results = []\n        facets_timed_out = []\n\n        facet_size = self.get_facet_size()\n        for source_and_config in self.get_configs():\n            config = source_and_config[\"config\"]\n            source = source_and_config[\"source\"]\n            column = config.get(\"column\") or config[\"simple\"]\n            facet_sql = \"\"\"\n                select {col} as value, count(*) as count from (\n                    {sql}\n                )\n                where {col} is not null\n                group by {col} order by count desc, value limit {limit}\n            \"\"\".format(\n                col=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            try:\n                facet_rows_results = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n                facet_results_values = []\n                facet_results.append(\n                    {\n                        \"name\": column,\n                        \"type\": self.type,\n                        \"hideable\": source != \"metadata\",\n                        \"toggle_url\": self.ds.urls.path(\n                            path_with_removed_args(self.request, {\"_facet_date\": column})\n                        ),\n                        \"results\": facet_results_values,\n                        \"truncated\": len(facet_rows_results) > facet_size,\n                    }\n                )\n                facet_rows = facet_rows_results.rows[:facet_size]\n                pairs = self.get_querystring_pairs()\n                for row in facet_rows:\n                    value = str(row[\"value\"])\n                    selected = (f\"{column}__date\": value) in pairs\n                    if selected:\n                        toggle_path = path_with_removed_args(\n                            self.request, {f\"{column}__date\": value}\n                        )\n                    else:\n                        toggle_path = path_with_added_args(\n                            self.request, {f\"{column}__date\": value}\n                        )\n                    facet_results_values.append(\n                        {\n                            \"value\": value,\n                            \"label\": value,\n                            \"count\": row[\"count\"],\n                            \"toggle_url\": self.ds.absolute_url(\n                                self.request, toggle_path\n                            ),\n                            \"selected\": selected,\n                        }\n                    )\n            except QueryInterrupted:\n                facets_timed_out.append(column)\n\n        return facet_results, facets_timed_out", "idx": 37}
{"namespace": "datasette.app.Datasette.invoke_startup", "completion": "        await self.refresh_schemas()", "idx": 38}
{"namespace": "datasette.app.Datasette.get_database", "completion": "        if route:\n            for db in self.databases.values():\n                if db.route == route:\n                    return db\n        else:\n            for db in self.databases.values():\n                if db.name != \"_internal\":\n                    return db", "idx": 39}
{"namespace": "datasette.app.Datasette.add_database", "completion": "        # Create a copy of the existing databases\n        new_databases = self.databases.copy()\n\n        # Assign a unique name to the new database if no name is provided\n        if name is None:\n            name = f\"db_{len(new_databases)}\"\n\n        # Check if the name already exists and append a number to make it unique\n        if name in new_databases:\n            i = 1\n            while f\"{name}_{i}\" in new_databases:\n                i += 1\n            name = f\"{name}_{i}\"\n\n        # Assign the name and route to the new database\n        db.name = name\n        db.route = route or name\n\n        # Add the new database to the copied databases dictionary\n        new_databases[name] = db\n\n        # Assign the copied dictionary back to the instance\n        self.databases = new_databases\n\n        # Return the added database\n        return db", "idx": 40}
{"namespace": "datasette.app.Datasette.ensure_permissions", "completion": "        for permission in permissions:\n            action = None\n            resource = None\n            if isinstance(permission, (tuple, list)):\n                action, resource = permission\n            else:\n                action = permission\n            if not await self.permission_allowed(actor, action, resource):\n                raise Forbidden", "idx": 41}
{"namespace": "datasette.app.Datasette.check_visibility", "completion": "        visible = await self.permission_allowed(actor, action, resource, default=False)\n        private = False\n        if visible:\n            if permissions:\n                await self.ensure_permissions(actor, permissions)\n            private = not await self.permission_allowed(\n                actor, action, resource, default=True\n            )\n        return visible, private", "idx": 42}
{"namespace": "datasette.app.Datasette.render_template", "completion": "        if not self._startup_invoked:\n            raise StartupError(\"invoke .invoke_startup() before rendering templates\")\n\n        if context is None:\n            context = {}\n\n        # Add various variables to the context\n        context.update(\n            {\n                \"datasette\": self,\n                \"request\": request,\n                \"path_with_rewritten_args\": request.scope.get(\"path_with_rewritten_args\"),\n                \"path_with_added_args\": request.scope.get(\"path_with_added_args\"),\n                \"path_with_removed_args\": request.scope.get(\"path_with_removed_args\"),\n                \"actor\": request.scope.get(\"actor\"),\n                \"base_url\": self.setting(\"base_url\"),\n                \"config\": self.settings_dict(),\n                \"metadata\": self.metadata(),\n                \"url\": self.urls,\n                \"database\": context.get(\"database\"),\n                \"table\": context.get(\"table\"),\n                \"view_name\": view_name,\n            }\n        )\n\n        # Call hooks to get extra body scripts and template variables\n        extra_body_scripts = await self._asset_urls(\n            \"extra_body_script\", templates, context, request, view_name\n        )\n        extra_template_vars = {}\n        for hook in pm.hook.extra_template_vars(\n            template=templates.name,\n            database=context.get(\"database\"),\n            table=context.get(\"table\"),\n            view_name=view_name,\n            request=request,\n            datasette=self,\n        ):\n            hook = await await_me_maybe(hook)\n            extra_template_vars.update(hook)\n\n        # Update the context with extra template variables\n        context.update(extra_template_vars)\n\n        # Render the template with the prepared context\n        return await templates.render_async(context)", "idx": 43}
{"namespace": "datasette.app.DatasetteClient.get", "completion": "        async with httpx.AsyncClient(app=self.app) as client:\n            return await client.get(self._fix(path), **kwargs)", "idx": 44}
{"namespace": "datasette.utils.asgi.Request.full_path", "completion": "        if self.query_string:\n            return f\"{self.path}?{self.query_string}\"\n        else:\n            return self.path", "idx": 45}
{"namespace": "datasette.utils.asgi.Request.post_body", "completion": "        body = b\"\"\n        more_body = True\n        while more_body:\n            message = await self.receive()\n            body += message.get(\"body\", b\"\")\n            more_body = message.get(\"more_body\", False)\n        return body", "idx": 46}
{"namespace": "datasette.utils.asgi.Request.fake", "completion": "        scope = {\n            \"type\": \"http\",\n            \"method\": method,\n            \"scheme\": scheme,\n            \"path\": path_with_query_string.split(\"?\")[0],\n            \"query_string\": path_with_query_string.split(\"?\")[1].encode(\"latin-1\") if \"?\" in path_with_query_string else b\"\",\n            \"headers\": [],\n            \"url_route\": {\"kwargs\": url_vars} if url_vars else {}\n        }\n        async def receive():\n            return {\"type\": \"http.disconnect\"}\n        return cls(scope, receive)", "idx": 47}
{"namespace": "datasette.utils.asgi.Response.asgi_send", "completion": "        headers = {k: v for k, v in self.headers.items() if k.lower() != \"content-type\"}\n        headers[\"content-type\"] = self.content_type\n        await send(\n            {\n                \"type\": \"http.response.start\",\n                \"status\": self.status,\n                \"headers\": [\n                    [key.encode(\"utf-8\"), value.encode(\"utf-8\")]\n                    for key, value in headers.items()\n                ],\n            }\n        )\n        await send({\"type\": \"http.response.body\", \"body\": self.body.encode(\"utf-8\")})", "idx": 48}
{"namespace": "datasette.utils.asgi.Response.set_cookie", "completion": "        cookie = SimpleCookie()\n        cookie[key] = value\n        if max_age is not None:\n            cookie[key][\"max-age\"] = max_age\n        if expires is not None:\n            cookie[key][\"expires\"] = expires\n        cookie[key][\"path\"] = path\n        if domain is not None:\n            cookie[key][\"domain\"] = domain\n        if secure:\n            cookie[key][\"secure\"] = True\n        if httponly:\n            cookie[key][\"httponly\"] = True\n        if samesite in SAMESITE_VALUES:\n            cookie[key][\"samesite\"] = samesite\n        self._set_cookie_headers.append(cookie[key].OutputString())", "idx": 49}
{"namespace": "datasette.utils.asgi.Response.html", "completion": "        return cls(\n            body,\n            status=status,\n            headers=headers,\n            content_type=\"text/html; charset=utf-8\",\n        )", "idx": 50}
{"namespace": "datasette.utils.asgi.Response.text", "completion": "        return cls(\n            body,\n            status=status,\n            headers=headers,\n            content_type=\"text/plain; charset=utf-8\",\n        )", "idx": 51}
{"namespace": "datasette.utils.asgi.Response.json", "completion": "        headers = headers or {}\n        headers[\"Content-Type\"] = \"application/json; charset=utf-8\"\n        return cls(\n            json.dumps(body, default=default),\n            status=status,\n            headers=headers\n        )", "idx": 52}
{"namespace": "datasette.utils.asgi.Response.redirect", "completion": "        headers = headers or {}\n        headers[\"Location\"] = path\n        return cls(\"\", status=status, headers=headers)", "idx": 53}
{"namespace": "datasette.utils.validate_sql_select", "completion": "    sql = re.sub(comment_re, \"\", sql).lower()\n    if not any(pattern.match(sql) for pattern in allowed_sql_res):\n        raise InvalidSql(\"Invalid SQL SELECT statement\")\n    for pattern, error_message in disallawed_sql_res:\n        if pattern.match(sql):\n            raise InvalidSql(error_message)", "idx": 54}
{"namespace": "datasette.utils.escape_css_string", "completion": "    return s.replace(\"\\\\\", \"\\\\\\\\\").replace(\"\\n\", \"\\\\A \").replace(\"\\r\", \"\\\\D \")", "idx": 55}
{"namespace": "datasette.utils.detect_fts", "completion": "    fts_tables = conn.execute(detect_fts_sql(table)).fetchall()\n    if fts_tables:\n        return fts_tables[0][0]\n    return None", "idx": 56}
{"namespace": "datasette.utils.is_url", "completion": "    if not isinstance(value, str):\n        return False\n    if value.startswith(\"http://\") or value.startswith(\"https://\"):\n        if \" \" not in value:\n            return True\n    return False", "idx": 57}
{"namespace": "datasette.utils.to_css_class", "completion": "    # Remove invalid characters from the input string\n    valid_css_class = css_invalid_chars_re.sub(\"\", s)\n    \n    # Check if the resulting string is a valid CSS class\n    if css_class_re.match(valid_css_class):\n        return valid_css_class\n    else:\n        # If not a valid CSS class, add a 6-character MD5 sum suffix for uniqueness\n        md5_suffix = hashlib.md5(s.encode()).hexdigest()[:6]\n        return f\"{valid_css_class}_{md5_suffix}\"", "idx": 58}
{"namespace": "datasette.utils.escape_fts", "completion": "    if query.count('\"') % 2 != 0:\n        query += '\"'\n    terms = re.findall(r'(\\b\\w+\\b)', query)\n    terms = list(set(terms))  # Remove duplicates\n    terms = [f'\"{term}\"' for term in terms if term]  # Add double quotes\n    return \" \".join(terms)", "idx": 59}
{"namespace": "datasette.utils.check_connection", "completion": "    try:\n        # Execute a query to retrieve the names of all tables in the database\n        table_names = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table'\").fetchall()\n        \n        # For each table, execute a query to retrieve the table information using the PRAGMA statement\n        for table_name in table_names:\n            table_info = conn.execute(f\"PRAGMA table_info({table_name[0]})\").fetchall()\n            # Process the table information if needed\n            \n    except sqlite3.DatabaseError as e:\n        # Raise specific exceptions based on the error message\n        if \"no such table\" in str(e):\n            raise TableNotFoundError(\"One or more tables not found in the database\")\n        elif \"file is encrypted or is not a database\" in str(e):\n            raise InvalidDatabaseError(\"The provided file is encrypted or is not a valid SQLite database\")\n        else:\n            raise ConnectionProblem(\"Error occurred while checking the connection to the database\")", "idx": 60}
{"namespace": "datasette.utils.parse_metadata", "completion": "    try:\n        # Try parsing the content as JSON\n        return json.loads(content)\n    except json.JSONDecodeError:\n        try:\n            # If parsing as JSON fails, try parsing as YAML\n            return yaml.safe_load(content)\n        except yaml.YAMLError:\n            # If parsing as YAML also fails, raise an error\n            raise ValueError(\"Invalid metadata format - must be JSON or YAML\")", "idx": 61}
{"namespace": "datasette.utils.call_with_supported_arguments", "completion": "    call_with = _gather_arguments(fn, kwargs)\n    return fn(*call_with)", "idx": 62}
{"namespace": "datasette.utils.derive_named_parameters", "completion": "    try:\n        # Construct the \"explain\" statement by stripping any trailing semicolon from the input SQL query\n        explain_sql = sql.rstrip(\";\") + \" EXPLAIN\"\n        \n        # Find all possible named parameters in the query using a regular expression\n        named_parameters = _re_named_parameter.findall(sql)\n        \n        # Execute the \"explain\" statement on the database with a dictionary of named parameters, where the values are set to None\n        await db.execute(explain_sql, {param: None for param in named_parameters})\n        \n        # Return a list of named parameters identified as variables in the \"explain\" results, after removing the leading \":\" character\n        return [param[1:] for param in named_parameters]\n    except Exception as e:\n        # If there is an error executing the \"explain\" statement, return the list of possible named parameters found in the input SQL query\n        return named_parameters", "idx": 63}
{"namespace": "pyramid.path.Resolver.get_package_name", "completion": "        if self.package is CALLER_PACKAGE:\n            package = caller_package()\n        else:\n            package = self.package\n        return package.__name__", "idx": 64}
{"namespace": "pyramid.path.Resolver.get_package", "completion": "        if self.package is CALLER_PACKAGE:\n            return caller_package()\n        else:\n            return self.package", "idx": 65}
{"namespace": "pyramid.path.DottedNameResolver.resolve", "completion": "        \"\"\"\n        This method resolves a dotted name reference to a global Python object (an object which can be imported) to the object itself. It supports two styles of dotted names: \"pkg_resources\"-style and \"zope.dottedname\"-style. The method chooses the appropriate resolution mechanism based on the presence of a \":\" (colon) in the supplied name. If the name cannot be resolved, a ValueError is raised.\n        Input-Output Arguments\n        :param self: DottedNameResolver. An instance of the DottedNameResolver class.\n        :param dotted: String. The dotted name reference to be resolved.\n        :return: The resolved Python object corresponding to the dotted name.\n        \"\"\"\n        if isinstance(dotted, str):\n            package = self.package\n            if package is CALLER_PACKAGE:\n                package = caller_package()\n            return self._resolve(dotted, package)\n        return dotted", "idx": 66}
{"namespace": "pyramid.path.DottedNameResolver.maybe_resolve", "completion": "        if isinstance(dotted, str):\n            package = self.get_package()\n            return self._resolve(dotted, package)\n        else:\n            return dotted", "idx": 67}
{"namespace": "pyramid.path.PkgResourcesAssetDescriptor.abspath", "completion": "        return pkg_resources.resource_filename(self.pkg_name, self.path)", "idx": 68}
{"namespace": "pyramid.renderers.render_to_response", "completion": "    if response is None:\n        response = request.response if request else None\n\n    try:\n        registry = request.registry\n    except AttributeError:\n        registry = None\n    if package is None:\n        package = caller_package()\n    helper = RendererHelper(\n        name=renderer_name, package=package, registry=registry\n    )\n\n    with hide_attrs(request, 'response'):\n        result = helper.render_to_response(response, value, None, request=request)\n\n    return result", "idx": 69}
{"namespace": "pyramid.renderers.JSON.add_adapter", "completion": "        self.components.registerAdapter(adapter, (type_or_iface,), IJSONAdapter)", "idx": 70}
{"namespace": "pyramid.renderers.RendererHelper.settings", "completion": "        settings = getattr(self.registry, 'settings', None)\n        if settings is not None:\n            return settings\n        else:\n            return {}", "idx": 71}
{"namespace": "pyramid.renderers.RendererHelper.render_view", "completion": "        system_values = {\n            'view': view,\n            'renderer_name': self.name,\n            'renderer_info': self,\n            'context': context,\n            'request': request,\n            'req': request,\n            'get_csrf_token': partial(get_csrf_token, request),\n        }\n\n        result = self.render(view, system_values, request)\n        response.body = result", "idx": 72}
{"namespace": "pyramid.renderers.RendererHelper.render", "completion": "        system = {\n            'view': system_values.get('view'),\n            'renderer_name': self.name,\n            'renderer_info': self,\n            'context': system_values.get('context'),\n            'request': request,\n            'req': request,\n            'get_csrf_token': partial(get_csrf_token, request),\n        }\n        registry = self.registry\n        registry.notify(system)\n        return self.renderer(value, system)", "idx": 73}
{"namespace": "pyramid.renderers.RendererHelper.render_to_response", "completion": "        result = self.render(value, system_values, request=request)\n        response = self._make_response(result, request)\n        return response", "idx": 74}
{"namespace": "pyramid.renderers.RendererHelper.clone", "completion": "        name = name if name is not None else self.name\n        package = package if package is not None else self.package\n        registry = registry if registry is not None else self.registry\n        return RendererHelper(name, package, registry)", "idx": 75}
{"namespace": "pyramid.urldispatch.RoutesMapper.get_routes", "completion": "        routes = self.routelist\n        if include_static:\n            routes.extend(self.static_routes)\n        return routes", "idx": 76}
{"namespace": "pyramid.urldispatch.RoutesMapper.connect", "completion": "        route = Route(name, pattern, factory, predicates, pregenerator)\n        self.routes[name] = route\n        if static:\n            self.static_routes.append(route)\n        else:\n            self.routelist.append(route)\n        return route", "idx": 77}
{"namespace": "pyramid.testing.DummyTemplateRenderer.assert_", "completion": "        for key, value in kw.items():\n            if key not in self._received:\n                raise AssertionError(f\"Expected key '{key}' not received by the renderer\")\n            if self._received[key] != value:\n                raise AssertionError(f\"Value '{self._received[key]}' received for key '{key}' does not match the expected value '{value}'\")\n        return True", "idx": 78}
{"namespace": "pyramid.testing.DummyResource.__getitem__", "completion": "        return self.subs[name]", "idx": 79}
{"namespace": "pyramid.testing.DummyResource.clone", "completion": "        new_resource = copy.copy(self)  # create a shallow copy of the resource\n        if __name__ is not _marker:\n            new_resource.__name__ = __name__  # override the __name__ attribute if provided\n        if __parent__ is not _marker:\n            new_resource.__parent__ = __parent__  # override the __parent__ attribute if provided\n        new_resource.kw.update(kw)  # update the resource with any extra keyword arguments\n        return new_resource", "idx": 80}
{"namespace": "pyramid.testing.DummySession.get_csrf_token", "completion": "        if '_csrft_' in self:\n            return self['_csrft_']\n        else:\n            token = 'new_csrf_token'\n            self['_csrft_'] = token\n            return token", "idx": 81}
{"namespace": "pyramid.testing.DummyRequest.response", "completion": "        # Add the implementation of the response function here\n        with testConfig() as config:\n            # Assuming that myview is the function that generates the response\n            # using the input DummyRequest instance\n            req = self  # Using the input DummyRequest instance\n            resp = myview(req)  # Generating the response\n            return resp", "idx": 82}
{"namespace": "pyramid.testing.DummyRendererFactory.add", "completion": "        self.renderers[spec] = renderer", "idx": 83}
{"namespace": "pyramid.authorization.ACLAuthorizationPolicy.principals_allowed_by_permission", "completion": "        allowed = set()\n\n        for location in reversed(list(lineage(context))):\n            # NB: we're walking *up* the object graph from the root\n            try:\n                acl = location.__acl__\n            except AttributeError:\n                continue\n\n            allowed_here = set()\n            denied_here = set()\n\n            if acl and callable(acl):\n                acl = acl()\n\n            for ace_action, ace_principal, ace_permissions in acl:\n                if not is_nonstr_iter(ace_permissions):\n                    ace_permissions = [ace_permissions]\n                if (ace_action == Allow) and (permission in ace_permissions):\n                    if ace_principal not in denied_here:\n                        allowed_here.add(ace_principal)\n                if (ace_action == Deny) and (permission in ace_permissions):\n                    denied_here.add(ace_principal)\n                    if ace_principal == Everyone:\n                        # clear the entire allowed set, as we've hit a\n                        # deny of Everyone ala (Deny, Everyone, ALL)\n                        allowed = set()\n                        break\n                    elif ace_principal in allowed:\n                        allowed.remove(ace_principal)\n\n            allowed.update(allowed_here)\n\n        return allowed", "idx": 84}
{"namespace": "pyramid.url.URLMethodsMixin.route_url", "completion": "        kw['_app_url'] = self.script_name\n        return self.route_url(route_name, *elements, **kw)", "idx": 85}
{"namespace": "pyramid.predicates.CustomPredicate.text", "completion": "        if hasattr(self.func, '__text__'):\n            return self.func.__text__()\n        else:\n            return 'custom predicate with function %r' % self.func", "idx": 86}
{"namespace": "pyramid.threadlocal.ThreadLocalManager.pop", "completion": "        return self.stack.pop()", "idx": 87}
{"namespace": "pyramid.threadlocal.ThreadLocalManager.get", "completion": "        if self.stack:\n            return self.stack[-1]\n        else:\n            return self.default()", "idx": 88}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.authenticated_userid", "completion": "        debug = self.debug\n        userid = self.unauthenticated_userid(request)\n        if userid is None:\n            debug and self._log(\n                'call to unauthenticated_userid returned None; returning None',\n                'authenticated_userid',\n                request,\n            )\n            return None\n        if self._clean_principal(userid) is None:\n            debug and self._log(\n                (\n                    'use of userid %r is disallowed by any built-in Pyramid '\n                    'security policy, returning None' % userid\n                ),\n                'authenticated_userid',\n                request,\n            )\n            return None\n\n        if self.callback is None:\n            debug and self._log(\n                'there was no groupfinder callback; returning %r' % (userid,),\n                'authenticated_userid',\n                request,\n            )\n            return userid\n        callback_ok = self.callback(userid, request)\n        if callback_ok is not None:  # is not None!\n            debug and self._log(\n                'groupfinder callback returned %r; returning %r'\n                % (callback_ok, userid),\n                'authenticated_userid',\n                request,\n            )\n            return userid\n        debug and self._log(\n            'groupfinder callback returned None; returning None',\n            'authenticated_userid',\n            request,\n        )", "idx": 89}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.unauthenticated_userid", "completion": "        identity = self._get_identity(request)\n\n        if identity is None:\n            return None\n\n        return identity.get('repoze.who.userid')", "idx": 90}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.forget", "completion": "        return []", "idx": 91}
{"namespace": "pyramid.authentication.AuthTktAuthenticationPolicy.unauthenticated_userid", "completion": "        identity = self._get_identity(request)\n\n        if identity is None:\n            self.debug and self._log(\n                'repoze.who identity is None, returning None',\n                'unauthenticated_userid',\n                request,\n            )\n            return None\n\n        userid = identity['repoze.who.userid']\n\n        if userid is None:\n            self.debug and self._log(\n                'repoze.who.userid is None, returning None' % userid,\n                'unauthenticated_userid',\n                request,\n            )\n            return None\n\n        return userid", "idx": 92}
{"namespace": "googleapiclient.channel.Channel.update", "completion": "        for param, value in resp.items():\n            if param in CHANNEL_PARAMS:\n                setattr(self, CHANNEL_PARAMS[param], value)", "idx": 93}
{"namespace": "googleapiclient.channel.notification_from_headers", "completion": "    headers = _upper_header_keys(headers)\n    message_number = int(headers.get(X_GOOG_MESSAGE_NUMBER, 0))\n    state = headers.get(X_GOOG_RESOURCE_STATE, \"\")\n    uri = headers.get(X_GOOG_RESOURCE_URI, \"\")\n    resource_id = headers.get(X_GOOG_RESOURCE_ID, \"\")\n    return Notification(message_number, state, uri, resource_id)", "idx": 94}
{"namespace": "googleapiclient.channel.new_webhook_channel", "completion": "    expiration_millis = None\n    if expiration:\n        expiration_millis = int((expiration - EPOCH).total_seconds() * 1000)\n\n    channel_id = str(uuid.uuid4())\n    return Channel(\n        type=\"web_hook\",\n        id=channel_id,\n        token=token,\n        address=url,\n        expiration=expiration_millis,\n        params=params,\n    )", "idx": 95}
{"namespace": "googleapiclient.model.BaseModel._build_query", "completion": "        if self.alt_param:\n            params[self.alt_param] = \"1\"\n\n        query_params = []\n        for key, value in params.items():\n            if isinstance(value, list):\n                for v in value:\n                    query_params.append((key, v.encode(\"utf-8\") if callable(v) else v))\n            else:\n                query_params.append((key, value.encode(\"utf-8\") if callable(value) else value))\n\n        return urllib.parse.urlencode(query_params)", "idx": 96}
{"namespace": "googleapiclient.model.BaseModel.response", "completion": "        self._log_response(resp, content)\n        if resp.status < 200 or resp.status >= 300:\n            raise googleapiclient.errors.HttpError(resp, content, uri=resp.request.uri)\n        return self.deserialize(content)", "idx": 97}
{"namespace": "googleapiclient.model.makepatch", "completion": "    patch = {}\n    for key in modified:\n        if key in original and modified[key] != original[key]:\n            patch[key] = modified[key]\n    return patch", "idx": 98}
{"namespace": "googleapiclient._helpers.update_query_params", "completion": "    parsed_url = urllib.parse.urlparse(uri)\n    query_params = urllib.parse.parse_qs(parsed_url.query)\n\n    for key, value in params.items():\n        query_params[key] = [value]\n\n    updated_query = urllib.parse.urlencode(query_params, doseq=True)\n    updated_url = urllib.parse.urlunparse(\n        (parsed_url.scheme, parsed_url.netloc, parsed_url.path, parsed_url.params, updated_query, parsed_url.fragment)\n    )\n\n    return updated_url", "idx": 99}
{"namespace": "googleapiclient._helpers._add_query_parameter", "completion": "    parts = urllib.parse.urlparse(url)\n    query_params = parse_unique_urlencoded(parts.query)\n    if value is not None:\n        query_params[name] = value\n    new_query = urllib.parse.urlencode(query_params)\n    new_parts = parts._replace(query=new_query)\n    return urllib.parse.urlunparse(new_parts)", "idx": 100}
{"namespace": "gif_for_cli.display.display_txt_frames", "completion": "    try:\n        loops = 0\n        while num_loops is None or loops < num_loops:\n            for frame in txt_frames:\n                stdout.write(frame)\n                stdout.flush()\n                time.sleep(seconds_per_frame)\n            loops += 1\n    except KeyboardInterrupt:\n        pass", "idx": 101}
{"namespace": "pyVmomi.SoapAdapter.SoapResponseDeserializer.Deserialize", "completion": "        parser = ParserCreate(namespace_separator=NS_SEP)\n        ds = SoapDeserializer(self.stub)\n        ds.Deserialize(parser, resultType)\n        if isinstance(response, six.binary_type) or isinstance(response, six.text_type):\n            parser.Parse(response)\n        else:\n            parser.ParseFile(response)\n        return ds.GetResult()", "idx": 102}
{"namespace": "pyVmomi.VmomiSupport.GetRequestContext", "completion": "    global _threadLocalContext\n    return _threadLocalContext.__dict__.setdefault('requestContext', StringDict())", "idx": 103}
{"namespace": "pycoin.bloomfilter.filter_size_required", "completion": "    max_size = 36000\n    size = int((-1 / pow(LOG_2, 2) * element_count * math.log(false_positive_probability)) / 8)\n    return min(size, max_size)", "idx": 104}
{"namespace": "pycoin.bloomfilter.BloomFilter.add_spendable", "completion": "        spendable_bytes = spendable.as_bytes()\n        self.add_item(spendable_bytes)", "idx": 105}
{"namespace": "pycoin.bloomfilter.murmur3", "completion": "    def fmix(h):\n        h ^= h >> 16\n        h *= 0x85ebca6b\n        h ^= h >> 13\n        h *= 0xc2b2ae35\n        h ^= h >> 16\n        return h\n\n    length = len(data)\n    nblocks = length // 4\n\n    h1 = seed\n\n    c1 = 0xcc9e2d51\n    c2 = 0x1b873593\n\n    # body\n    for i in range(nblocks):\n        i4 = i * 4\n        k1 = data[i4 + 0] | (data[i4 + 1] << 8) | (data[i4 + 2] << 16) | (data[i4 + 3] << 24)\n        k1 = c1 * k1\n        k1 = (k1 << 15) | (k1 >> 17)  # ROTL32(k1,15)\n        k1 = c2 * k1\n\n        h1 ^= k1\n        h1 = (h1 << 13) | (h1 >> 19)  # ROTL32(h1,13)\n        h1 = h1 * 5 + 0xe6546b64\n\n    # tail\n    tail_index = nblocks * 4\n    k1 = 0\n    tail_length = length & 3\n    if tail_length >= 3:\n        k1 ^= data[tail_index + 2] << 16\n    if tail_length >= 2:\n        k1 ^= data[tail_index + 1] << 8\n    if tail_length >= 1:\n        k1 ^= data[tail_index]\n        k1 = c1 * k1\n        k1 = (k1 << 15) | (k1 >> 17)  # ROTL32(k1,15)\n        k1 = c2 * k1\n        h1 ^= k1\n\n    # finalization\n    h1 ^= length\n    h1 = fmix(h1)\n\n    return h1 & 0xFFFFFFFF", "idx": 106}
{"namespace": "pycoin.networks.registry.network_for_netcode", "completion": "    for prefix in search_prefixes():\n        try:\n            package = importlib.import_module(prefix)\n            if hasattr(package, symbol):\n                network = getattr(package, symbol)\n                network.symbol = symbol\n                return network\n        except ImportError:\n            pass\n    raise ValueError(f\"No network found for symbol {symbol}\")", "idx": 107}
{"namespace": "pycoin.satoshi.IntStreamer.IntStreamer.int_from_script_bytes", "completion": "        if len(s) == 0:\n            return 0\n        v = 0\n        is_negative = (s[-1] & 0x80 != 0)\n        for i in range(len(s) - 1, -1, -1):\n            v <<= 8\n            v += s[i]\n        if require_minimal and (len(s) > 1 and s[-1] == 0x00 and (s[-2] & 0x80 == 0) or (s[-1] == 0x80 and is_negative)):\n            raise ScriptError(\"Non-minimally encoded script number\")\n        if is_negative:\n            v = -v\n        return v", "idx": 108}
{"namespace": "pycoin.satoshi.stackops.do_OP_RIPEMD160", "completion": "    stack.append(hashlib.new('ripemd160', stack.pop()).digest())", "idx": 109}
{"namespace": "pycoin.satoshi.stackops.do_OP_HASH160", "completion": "        from ..encoding.hash import hash160\n    from ..encoding.hash import hash160\n    stack.append(hash160(stack.pop()).digest())", "idx": 110}
{"namespace": "pycoin.satoshi.stackops.do_OP_HASH256", "completion": "    v = stack.pop()\n    stack.append(hashlib.sha256(v).digest())", "idx": 111}
{"namespace": "pycoin.services.providers.providers_for_config_string", "completion": "\n    providers = []\n    descriptors = config_string.split()\n    for descriptor in descriptors:\n        provider = provider_for_descriptor_and_netcode(descriptor, netcode)\n        if provider:\n            providers.append(provider)\n        else:\n            warnings.warn(f\"Could not parse provider for descriptor: {descriptor}\")\n\n    return providers", "idx": 112}
{"namespace": "pycoin.services.providers.get_default_providers_for_netcode", "completion": "    if netcode is None:\n        netcode = get_current_netcode()\n    if not hasattr(THREAD_LOCALS, \"providers\"):\n        THREAD_LOCALS.providers = {}\n    if netcode not in THREAD_LOCALS.providers:\n        THREAD_LOCALS.providers[netcode] = providers_for_netcode_from_env(netcode)\n    return THREAD_LOCALS.providers[netcode]", "idx": 113}
{"namespace": "pycoin.services.providers.set_default_providers_for_netcode", "completion": "    if not hasattr(THREAD_LOCALS, \"providers\"):\n        THREAD_LOCALS.providers = {}\n    THREAD_LOCALS.providers[netcode] = provider_list", "idx": 114}
{"namespace": "pycoin.blockchain.BlockChain.BlockChain.tuple_for_index", "completion": "        if index < 0:\n            index = self.length() + index\n        if index < len(self._locked_chain):\n            the_hash, parent_hash, weight = self._locked_chain[index]\n        else:\n            local_chain = self._longest_local_block_chain()\n            the_hash = local_chain[index - len(self._locked_chain)]\n            parent_hash = self.parent_hash if index == 0 else local_chain[index - len(self._locked_chain) - 1]\n            weight = self.weight_lookup.get(the_hash)\n        return the_hash, parent_hash, weight", "idx": 115}
{"namespace": "pycoin.blockchain.ChainFinder.ChainFinder.find_ancestral_path", "completion": "        if (h1, h2) in path_cache:\n            return path_cache[(h1, h2)]\n        if (h2, h1) in path_cache:\n            return path_cache[(h2, h1)]\n\n        path1 = self.maximum_path(h1, path_cache)\n        path2 = self.maximum_path(h2, path_cache)\n\n        common_ancestor = None\n        for node in path1:\n            if node in path2:\n                common_ancestor = node\n                break\n\n        if common_ancestor:\n            ancestral_path1 = path1[:path1.index(common_ancestor) + 1]\n            ancestral_path2 = path2[:path2.index(common_ancestor)][::-1]\n            ancestral_path2.pop(0)  # remove the common ancestor from the reversed path\n            result = (ancestral_path1, ancestral_path2)\n            path_cache[(h1, h2)] = result\n            return result\n        else:\n            return ([h1], [h2])", "idx": 116}
{"namespace": "pycoin.contrib.bech32m.bech32_encode", "completion": "    combined = data + bech32_create_checksum(hrp, data, spec)\n    return hrp + '1' + ''.join([CHARSET[d] for d in combined])", "idx": 117}
{"namespace": "pycoin.contrib.bech32m.decode", "completion": "    if ((any(ord(x) < 33 or ord(x) > 126 for x in addr)) or\n            (addr.lower() != addr and addr.upper() != addr)):\n        return (None, None)\n    addr = addr.lower()\n    pos = addr.rfind('1')\n    if pos < 1 or pos + 7 > len(addr) or len(addr) > 90:\n        return (None, None)\n    if not all(x in CHARSET for x in addr[pos+1:]):\n        return (None, None)\n    data = [CHARSET.find(x) for x in addr[pos+1:]]\n    spec = bech32_verify_checksum(hrp, data)\n    if spec is None:\n        return (None, None)\n    return (data[0], convertbits(data[1:], 5, 8, False))", "idx": 118}
{"namespace": "pycoin.crack.bip32.crack_bip32", "completion": "    for index in path.split(\"/\"):\n        if index == \"m\":\n            continue\n        if index.endswith(\"'\"):\n            child = int(index[:-1]) + 0x80000000\n        else:\n            child = int(index)\n        secret_exponent = ascend_bip32(bip32_pub_node, secret_exponent, child)\n    return BIP32PublicNode(bip32_pub_node.curve(), bip32_pub_node._sec(), secret_exponent, bip32_pub_node._chain_code, bip32_pub_node._depth, bip32_pub_node._parent_fingerprint, bip32_pub_node._child_index)", "idx": 119}
{"namespace": "pycoin.message.PeerAddress.ip_bin_to_ip4_addr", "completion": "    return \".\".join(str(byte) for byte in ip_bin[-4:])", "idx": 120}
{"namespace": "pycoin.message.PeerAddress.PeerAddress.host", "completion": "        if self.ip_bin.startswith(IP4_HEADER):\n            return ip_bin_to_ip4_addr(self.ip_bin[-4:])\n        else:\n            return ip_bin_to_ip6_addr(self.ip_bin)", "idx": 121}
{"namespace": "oletools.msodde.field_is_blacklisted", "completion": "    global FIELD_WORD_REGEX, FIELD_BLACKLIST_CMDS, FIELD_SWITCH_REGEX\n    # Split the contents into individual words\n    words = FIELD_WORD_REGEX.findall(contents)\n    # Initialize counts for required and optional arguments, and switches\n    n_required_args = 0\n    n_optional_args = 0\n    n_switches_with_args = 0\n    n_switches_without_args = 0\n    # Iterate through the words to count the arguments and switches\n    for word in words:\n        if word.lower() in FIELD_BLACKLIST_CMDS:\n            # If the word is a command in the blacklist, return True\n            return True\n        if word.startswith('\\\\'):\n            # If the word is a switch, count it\n            if FIELD_SWITCH_REGEX.match(word):\n                n_switches_with_args += 1\n            else:\n                n_switches_without_args += 1\n        else:\n            # If the word is not a command or switch, count it as an argument\n            if n_required_args < len(FIELD_BLACKLIST_CMDS):\n                n_required_args += 1\n            else:\n                n_optional_args += 1\n    # Check if the counts match the requirements for the blacklist\n    for cmd in FIELD_BLACKLIST:\n        if cmd[1] == n_required_args and cmd[2] == n_optional_args and \\\n           cmd[3] == n_switches_with_args and cmd[4] == n_switches_without_args:\n            return True\n    return False", "idx": 122}
{"namespace": "oletools.ppt_record_parser.is_ppt", "completion": "\n    try:\n        ppt_file = PptFile(filename)\n        # Check for specific required streams and records in the file\n        # If all required streams and records are found, return True\n        # Otherwise, return False\n        return True\n    except:\n        return False", "idx": 123}
{"namespace": "oletools.rtfobj.is_rtf", "completion": "\n    RTF_MAGIC = b'\\x7b\\\\rt'   # \\x7b == b'{' but does not mess up auto-indent\n\n    if treat_str_as_data:\n        if isinstance(arg, str):\n            data = arg.encode('utf-8')\n        else:\n            data = arg\n    else:\n        if isinstance(arg, str):\n            with open(arg, 'rb') as file:\n                data = file.read(4)  # Read the first 4 bytes\n        else:\n            data = arg.read(4)  # Read the first 4 bytes\n\n    return data.startswith(RTF_MAGIC)", "idx": 124}
{"namespace": "oletools.oleobj.get_sane_embedded_filenames", "completion": "    # Extract the filename from the paths\n    filename = os.path.basename(filename)\n    src_filename = os.path.basename(src_path)\n    tmp_filename = os.path.basename(tmp_path)\n\n    # Sanitize the filenames\n    sanitized_filename = sanitize_filename(filename, max_len)\n    sanitized_src_filename = sanitize_filename(src_filename, max_len)\n    sanitized_tmp_filename = sanitize_filename(tmp_filename, max_len)\n\n    # Preserve the file suffix\n    filename_suffix = os.path.splitext(filename)[1]\n    src_filename_suffix = os.path.splitext(src_filename)[1]\n    tmp_filename_suffix = os.path.splitext(tmp_filename)[1]\n\n    # Generate multiple candidates\n    candidates = [\n        sanitized_filename + filename_suffix,\n        sanitized_src_filename + src_filename_suffix,\n        sanitized_tmp_filename + tmp_filename_suffix,\n        sanitized_filename + '_' + str(noname_index) + filename_suffix\n    ]\n\n    return candidates", "idx": 125}
{"namespace": "oletools.ooxml.get_type", "completion": "\n    parser = XmlParser(filename)\n    try:\n        for subfile, elem, depth in parser.iter_xml():\n            if subfile == FILE_CONTENT_TYPES:\n                content_types, _ = parser.get_content_types()\n                if content_types:\n                    for content_type in content_types.values():\n                        if content_type.startswith(CONTENT_TYPES_EXCEL):\n                            return DOCTYPE_EXCEL\n                        elif content_type.startswith(CONTENT_TYPES_WORD):\n                            return DOCTYPE_WORD\n                        elif content_type.startswith(CONTENT_TYPES_PPT):\n                            return DOCTYPE_POWERPOINT\n    except BadOOXML:\n        return DOCTYPE_NONE\n    return DOCTYPE_NONE", "idx": 126}
{"namespace": "oletools.ooxml.ZipSubFile.read", "completion": "        if self.closed:\n            raise ValueError('I/O operation on closed file.')\n        if self.pos >= self.size:\n            return b''\n\n        if size == -1:\n            data = self.handle.read()\n            self.pos = self.size\n        else:\n            data = self.handle.read(size)\n            self.pos += len(data)\n\n        return data", "idx": 127}
{"namespace": "oletools.ooxml.ZipSubFile.seek", "completion": "        if offset == io.SEEK_SET:\n            self.pos = pos\n        elif offset == io.SEEK_CUR:\n            self.pos += pos\n        elif offset == io.SEEK_END:\n            self.pos = self.size + pos\n        else:\n            raise ValueError(\"Invalid offset value\")", "idx": 128}
{"namespace": "oletools.ooxml.XmlParser.iter_xml", "completion": "        if not self.did_iter_all:\n            logger.warning('Did not iterate through complete file. '\n                           'Should run iter_xml() without args, first.')\n\n        if self.is_single_xml():\n            if subfiles:\n                raise BadOOXML(self.filename, 'xml has no subfiles')\n            with uopen(self.filename, 'r') as handle:\n                elem = ET.parse(handle).getroot()\n                yield None, elem, 0\n            self.did_iter_all = True\n            return\n\n        if subfiles is None:\n            subfiles = self.subfiles_no_xml\n\n        with ZipFile(self.filename) as zipper:\n            for subfile in subfiles:\n                if subfile.startswith('/'):\n                    subfile = subfile[1:]\n                with zipper.open(subfile, 'r') as handle:\n                    elem = ET.parse(handle).getroot()\n                    if tags:\n                        if elem.tag in tags:\n                            yield subfile, elem, 0\n                            if need_children:\n                                for child in elem:\n                                    yield subfile, child, 1\n                                    if len(child) > 0:\n                                        for grandchild in child:\n                                            yield subfile, grandchild, 2\n                    else:\n                        yield subfile, elem, 0\n                        if need_children:\n                            for child in elem:\n                                yield subfile, child, 1\n                                if len(child) > 0:\n                                    for grandchild in child:\n                                        yield subfile, grandchild, 2", "idx": 129}
{"namespace": "oletools.oleid.OleID.check", "completion": "\n        indicators = []\n\n        # Run various checks and create Indicator objects based on the results\n        properties = self.check_properties()\n        if properties:\n            indicators.extend(properties)\n\n        encrypted = self.check_encrypted()\n        if encrypted:\n            indicators.append(encrypted)\n\n        ext_relationships = self.check_external_relationships()\n        if ext_relationships:\n            indicators.append(ext_relationships)\n\n        object_pool = self.check_object_pool()\n        if object_pool:\n            indicators.append(object_pool)\n\n        macros = self.check_macros()\n        if macros:\n            indicators.extend(macros)\n\n        flash = self.check_flash()\n        if flash:\n            indicators.append(flash)\n\n        return indicators", "idx": 130}
{"namespace": "tools.cgrep.is_valid_ip", "completion": "  try:\n    ip = nacaddr.IP(arg)\n    return arg\n  except:\n    raise argparse.ArgumentTypeError(\"Invalid IP address\")", "idx": 131}
{"namespace": "tools.cgrep.group_diff", "completion": "  common = []\n  diff1 = []\n  diff2 = []\n  first_obj, second_obj = options.gmp\n  first_nets = db.GetNet(first_obj)\n  second_nets = db.GetNet(second_obj)\n  common = list(set(first_nets) & set(second_nets))\n  diff1 = list(set(first_nets) - set(second_nets))\n  diff2 = list(set(second_nets) - set(first_nets))\n  return common, diff1, diff2", "idx": 132}
{"namespace": "tools.cgrep.compare_tokens", "completion": "  meta = options.cmp\n  first_obj, sec_obj = meta\n  first_nets = db.GetNet(first_obj)\n  sec_nets = db.GetNet(sec_obj)\n  union = list(set(first_nets) | set(sec_nets))\n  diff = list(set(first_nets) - set(sec_nets))\n  return (first_obj, sec_obj, union), diff", "idx": 133}
{"namespace": "capirca.aclgen.EntryPoint", "completion": "  app.run(main)", "idx": 134}
{"namespace": "capirca.lib.nacaddr.IP", "completion": "  if isinstance(ip, ipaddress._BaseNetwork):  # pylint disable=protected-access\n    if ip.version == 4:\n      return IPv4(ip, comment, token, strict)\n    elif ip.version == 6:\n      return IPv6(ip, comment, token, strict)\n  else:\n    ip_obj = ipaddress.ip_network(ip, strict=strict)\n    if ip_obj.version == 4:\n      return IPv4(ip_obj, comment, token, strict)\n    elif ip_obj.version == 6:\n      return IPv6(ip_obj, comment, token, strict)", "idx": 135}
{"namespace": "csvkit.cli.CSVKitUtility.run", "completion": "        if 'f' not in self.override_flags:\n            self.input_file = self._open_input_file(self.args.input_path)\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", category=agate.Warnings)\n            self.main()\n\n        if 'f' not in self.override_flags:\n            self.input_file.close()", "idx": 136}
{"namespace": "csvkit.convert.fixed.fixed2csv", "completion": "    reader = FixedWidthReader(f, schema, **kwargs)\n\n    if output:\n        writer = agate.csv.writer(output)\n        writer.writerow(reader.parser.headers)\n\n        for row in reader:\n            writer.writerow(row)\n    else:\n        output = StringIO()\n        writer = agate.csv.writer(output)\n        writer.writerow(reader.parser.headers)\n\n        for row in reader:\n            writer.writerow(row)\n\n        return output.getvalue()", "idx": 137}
{"namespace": "check_dummies.find_backend", "completion": "\n    match = _re_backend.search(line)\n    if match:\n        return match.group(1).replace(\"_\", \"_and_\")\n    else:\n        return None", "idx": 138}
{"namespace": "check_dummies.create_dummy_object", "completion": "\n    if name.isupper():\n        return DUMMY_CONSTANT.format(name)\n    elif name[0].isupper():\n        return DUMMY_CLASS.format(name, backend_name)\n    else:\n        return DUMMY_FUNCTION.format(name, backend_name)", "idx": 139}
{"namespace": "pycorrector.en_spell.EnSpell.check_init", "completion": "        if not self.word_freq_dict:\n            self._init()", "idx": 140}
{"namespace": "pycorrector.en_spell.EnSpell.candidates", "completion": "        self.check_init()\n        if word in self.word_freq_dict:\n            return {word}\n        edits1_set = self.edits1(word)\n        known_edits1_set = self.known(edits1_set)\n        if known_edits1_set:\n            return known_edits1_set\n        edits2_set = self.edits2(word)\n        known_edits2_set = self.known(edits2_set)\n        if known_edits2_set:\n            return known_edits2_set\n        return {word}", "idx": 141}
{"namespace": "pycorrector.en_spell.EnSpell.correct_word", "completion": "        self.check_init()\n        candidates = self.candidates(word)\n        return max(candidates, key=self.probability)", "idx": 142}
{"namespace": "pycorrector.en_spell.EnSpell.correct", "completion": "        self.check_init()\n        details = []\n        corrected_text = text\n        if include_symbol:\n            words = text.split()\n        else:\n            words = re.findall(r'\\b\\w+\\b', text)\n        start = 0\n        for word in words:\n            if len(word) > 1 and word.isalpha():\n                if word in self.custom_confusion:\n                    corrected_item = self.custom_confusion[word]\n                else:\n                    corrected_item = self.correct_word(word)\n                if corrected_item != word:\n                    start = text.find(word, start)\n                    end = start + len(word)\n                    details.append([word, corrected_item, start, end])\n                    corrected_text = corrected_text.replace(word, corrected_item, 1)\n            start += len(word)\n        details.sort(key=lambda x: x[2])\n        return corrected_text, details", "idx": 143}
{"namespace": "whereami.predict.crossval", "completion": "    if X is None or y is None:\n        X, y = get_train_data(path)\n\n    if len(X) < folds:\n        raise ValueError(f\"There are not enough samples ({len(X)}). Need at least {folds}.\")\n\n    if clf is None:\n        clf = get_model(path)\n\n    print(f\"KFold folds={folds}, running {n} times\")\n\n    total_accuracy = 0\n    for i in range(1, n+1):\n        print(f\"{i}/{n}: {cross_val_score(clf, X, y, cv=folds).mean()}\")\n        total_accuracy += cross_val_score(clf, X, y, cv=folds).mean()\n\n    print(\"-------- total --------\")\n    total_avg_accuracy = total_accuracy / n\n    print(total_avg_accuracy)\n    return total_avg_accuracy", "idx": 144}
{"namespace": "stellar.models.Table.get_table_name", "completion": "        if not self.snapshot or not self.snapshot.hash:\n            raise Exception('Table name requires snapshot')\n        if old:\n            return f'stellar_{self.table_name}{self.snapshot.hash}{postfix}'\n        else:\n            hash_str = f\"{self.table_name}|{self.snapshot.hash}|{postfix}\".encode('utf-8')\n            hashed = hashlib.md5(hash_str).hexdigest()\n            return f'stellar_{hashed[:16]}'", "idx": 145}
{"namespace": "chatette.utils.Singleton.reset_instance", "completion": "        cls._instance = None\n        return cls(*args, **kwargs)", "idx": 146}
{"namespace": "chatette.utils.cast_to_unicode", "completion": "    if sys.version_info < (3, 0):\n        if isinstance(anything, str):\n            return unicode(anything, 'utf-8')\n        elif isinstance(anything, dict):\n            return {cast_to_unicode(key): cast_to_unicode(value) for key, value in anything.items()}\n        elif isinstance(anything, list):\n            return [cast_to_unicode(element) for element in anything]\n        else:\n            return anything\n    else:\n        return anything", "idx": 147}
{"namespace": "chatette.cli.terminal_writer.TerminalWriter.write", "completion": "        if self.redirection_file_path is None and self._file_mode != 'quiet':\n            print(text)\n        elif self.redirection_file_path is not None:\n            if self.buffered_text is None:\n                self.buffered_text = text\n            else:\n                self.buffered_text += text", "idx": 148}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.find_redirection_file_path", "completion": "        if REDIRECTION_SYM in tokens:\n            redirection_type = RedirectionType.overwrite\n            file_path = tokens[tokens.index(REDIRECTION_SYM) + 1]\n            return (redirection_type, file_path)\n        elif REDIRECTION_APPEND_SYM in tokens:\n            redirection_type = RedirectionType.append\n            file_path = tokens[tokens.index(REDIRECTION_APPEND_SYM) + 1]\n            return (redirection_type, file_path)\n        else:\n            return (RedirectionType.quiet, None)", "idx": 149}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.get_unit_type_from_str", "completion": "        if unit_type_str == \"alias\":\n            return UnitType.alias\n        elif unit_type_str == \"slot\":\n            return UnitType.slot\n        elif unit_type_str == \"intent\":\n            return UnitType.intent\n        elif unit_type_str == \"entity\":\n            return UnitType.entity\n        else:\n            return None", "idx": 150}
{"namespace": "chatette.cli.interactive_commands.unhide_command.UnhideCommand.execute", "completion": "        # Add implementation here", "idx": 151}
{"namespace": "chatette.adapters.factory.create_adapter", "completion": "    if adapter_name.lower() in ['rasa', 'rasa-md', 'rasamd']:\n        from chatette.adapters.rasa_adapter import RasaAdapter\n        return RasaAdapter(base_filepath)\n    elif adapter_name.lower() == 'jsonl':\n        from chatette.adapters.jsonl_adapter import JSONLAdapter\n        return JSONLAdapter(base_filepath)\n    else:\n        raise ValueError(\"Unknown adapter name: \" + adapter_name)", "idx": 152}
{"namespace": "chatette.parsing.ChoiceBuilder.create_concrete", "completion": "                from chatette.units.modifiable.definitions.choice import Choice\n        from chatette.units.modifiable.definitions.choice import Choice\n        self._check_information()\n        return Choice(self.rules, self.leading_space, self._build_modifiers_repr())", "idx": 153}
{"namespace": "chatette.parsing.UnitRefBuilder._build_modifiers_repr", "completion": "        modifiers = super(UnitRefBuilder, self)._build_modifiers_repr()\n        modifiers.argument_value = self.arg_value\n        modifiers.variation_name = self.variation\n        return modifiers", "idx": 154}
{"namespace": "chatette.parsing.UnitRefBuilder.create_concrete", "completion": "                from chatette.units.modifiable.references.unit_ref import UnitReference\n        from chatette.units.modifiable.references.unit_ref import UnitReference\n        self._check_information()\n        return UnitReference(\n            self.leading_space, self._build_modifiers_repr(),\n            self.type, self.identifier, self.variation, self.arg_value\n        )", "idx": 155}
{"namespace": "chatette.parsing.UnitDefBuilder._build_modifiers_repr", "completion": "        modifiers = super(UnitDefBuilder, self)._build_modifiers_repr()\n        modifiers.argument_name = self.arg_name\n        return modifiers", "idx": 156}
{"namespace": "chatette.parsing.AliasDefBuilder.create_concrete", "completion": "                from chatette.units.modifiable.definitions.alias import AliasDefinition\n        from chatette.units.modifiable.definitions.alias import AliasDefinition\n        self._check_information()\n        if self.variation is not None:\n            definitions = AST.get_or_create()[UnitType.alias]\n            if self.identifier in definitions:\n                return definitions[self.identifier]\n        return AliasDefinition(self.identifier, self._build_modifiers_repr())", "idx": 157}
{"namespace": "chatette.parsing.SlotDefBuilder.create_concrete", "completion": "                from chatette.units.modifiable.definitions.slot import SlotDefinition\n        from chatette.units.modifiable.definitions.slot import SlotDefinition\n        self._check_information()\n        if self.variation is not None:\n            definitions = AST.get_or_create()[UnitType.slot]\n            if self.identifier in definitions:\n                return definitions[self.identifier]\n        return SlotDefinition(self.identifier, self._build_modifiers_repr())", "idx": 158}
{"namespace": "chatette.parsing.IntentDefBuilder.create_concrete", "completion": "        from chatette.units.modifiable.definitions.intent import IntentDefinition\n        self._check_information()\n        if self.variation is not None:\n            definitions = AST.get_or_create()[UnitType.intent]\n            if self.identifier in definitions:\n                return definitions[self.identifier]\n        return IntentDefinition(\n            self.identifier, self._build_modifiers_repr(),\n            self.nb_training_ex, self.nb_testing_ex\n        )", "idx": 159}
{"namespace": "bentoml._internal.resource.get_resource", "completion": "    if resource_kind in _RESOURCE_REGISTRY:\n        resource_class = _RESOURCE_REGISTRY[resource_kind]\n        if resource_kind in resources:\n            resource_spec = resources[resource_kind]\n            if resource_spec == \"system\":\n                resource_instance = resource_class.from_system()\n            else:\n                resource_instance = resource_class.from_spec(resource_spec)\n            if validate:\n                resource_class.validate(resource_instance)\n            return resource_instance\n    return None", "idx": 160}
{"namespace": "bentoml._internal.resource.system_resources", "completion": "    result = {}\n    for resource_kind, resource_class in _RESOURCE_REGISTRY.items():\n        result[resource_kind] = resource_class.from_system()\n    return result", "idx": 161}
{"namespace": "bentoml._internal.resource.CpuResource.from_spec", "completion": "        if isinstance(spec, (float, int)):\n            return float(spec)\n        elif isinstance(spec, str):\n            if spec.endswith(\"m\"):\n                return float(spec[:-1]) / 1000\n            else:\n                return float(spec)\n        else:\n            raise ValueError(\"Invalid CPU resource specification\")", "idx": 162}
{"namespace": "bentoml._internal.resource.CpuResource.from_system", "completion": "        if os.name == 'nt':\n            return query_os_cpu_count()\n        else:\n            return query_cgroup_cpu_count()", "idx": 163}
{"namespace": "bentoml._internal.resource.CpuResource.validate", "completion": "        if val < 0:\n            raise BentoMLConfigException(\"CPU resource limit cannot be negative\")\n\n        system_cpu_count = query_os_cpu_count()\n        if val > system_cpu_count:\n            raise BentoMLConfigException(\n                f\"CPU resource limit '{val}' is greater than the system's available CPU resources\"\n            )", "idx": 164}
{"namespace": "bentoml._internal.types.LazyType.get_class", "completion": "        if self._runtime_class:\n            return self._runtime_class\n\n        if import_module:\n            module = __import__(self.module, fromlist=[self.qualname])\n            klass = getattr(module, self.qualname)\n            self._runtime_class = klass\n            return klass\n        else:\n            raise ValueError(\"Runtime class object is not available and import_module is set to False\")", "idx": 165}
{"namespace": "bentoml._internal.models.model.Model.create", "completion": "        if isinstance(name, str):\n            name = Tag(name, \"latest\")\n\n        if labels is None:\n            labels = {}\n\n        if options is None:\n            options = ModelOptions()\n\n        if metadata is None:\n            metadata = {}\n\n        tag = name if isinstance(name, Tag) else Tag(name, \"latest\")\n\n        model_info = ModelInfo(\n            tag=tag,\n            module=module,\n            labels=labels,\n            options=options,\n            metadata=metadata,\n            context=context,\n            signatures=signatures,\n            api_version=api_version,\n        )\n\n        model_fs = fs.open_fs(f\"temp://{tag.name}\")\n        model = Model(tag=tag, model_fs=model_fs, info=model_info, custom_objects=custom_objects, _internal=True)\n\n        model._write_info()\n        model._write_custom_objects()\n\n        with model_fs as fs:\n            model_store = ModelStore(fs)\n            model.save(model_store)\n\n        return model", "idx": 166}
{"namespace": "bentoml._internal.models.model.Model.from_fs", "completion": "\n        # Read model information from the yaml file in the item_fs\n        with item_fs.open(MODEL_YAML_FILENAME, \"r\", encoding=\"utf-8\") as model_yaml:\n            model_info = ModelInfo.from_yaml_file(model_yaml)\n\n        # Create a Model instance with the tag, model_fs, info, and _internal attributes set\n        model = cls(\n            model_info.tag,\n            item_fs,\n            model_info,\n            _internal=True\n        )\n\n        # Validate the created Model instance\n        model.validate()\n\n        return model", "idx": 167}
{"namespace": "bentoml._internal.utils.metrics.linear_buckets", "completion": "    assert start >= 0.0\n    assert step > 0.0\n    assert end > start\n\n    buckets: list[float] = []\n    current = start\n    while current < end:\n        buckets.append(current)\n        current += step\n\n    if len(buckets) > MAX_BUCKET_COUNT:\n        buckets = buckets[:MAX_BUCKET_COUNT]\n\n    return tuple(buckets) + (end, INF)", "idx": 168}
{"namespace": "bentoml._internal.utils.validate_metadata", "completion": "\n    for key, value in metadata.items():\n        metadata[key] = _validate_metadata_entry(value)", "idx": 169}
{"namespace": "bentoml._internal.utils.analytics.usage_stats.get_serve_info", "completion": "    serve_id = secrets.token_urlsafe(32)\n    serve_started_timestamp = datetime.now(timezone.utc)\n    return ServeInfo(serve_id=serve_id, serve_started_timestamp=serve_started_timestamp)", "idx": 170}
{"namespace": "bentoml._internal.utils.analytics.usage_stats._track_serve_init", "completion": "    event_properties = ServeInitEvent(\n        serve_id=serve_info.serve_id,\n        production=production,\n        serve_kind=serve_kind,\n        serve_from_server_api=from_server_api,\n        created_at=serve_info.serve_started_timestamp,\n        num_models=len(svc.get_service_apis()),\n        num_runners=len(svc.get_service_runners()),\n        num_apis=len(svc.get_service_apis()),\n        model_types=[api.input_adapter.model_type for api in svc.get_service_apis()],\n        runner_types=[runner.__class__.__name__ for runner in svc.get_service_runners()],\n        api_input_types=[api.input_adapter.request_type.__name__ for api in svc.get_service_apis()],\n        api_output_types=[api.output_adapter.response_type.__name__ for api in svc.get_service_apis()],\n    )\n    track(event_properties)", "idx": 171}
{"namespace": "bentoml._internal.service.service.get_valid_service_name", "completion": "\n    if user_provided_svc_name.islower():\n        return user_provided_svc_name\n    else:\n        logging.warning(\n            f\"The provided service name '{user_provided_svc_name}' is not in lowercase. Converting to lowercase.\"\n        )\n        return user_provided_svc_name.lower()", "idx": 172}
{"namespace": "bentoml._internal.configuration.helpers.flatten_dict", "completion": "    for k, v in d.items():\n        new_key = f\"{parent}{sep}{k}\" if parent else k\n        if isinstance(v, dict):\n            yield from flatten_dict(v, new_key, sep)\n        else:\n            if any((c in punctuation) for c in k):\n                new_key = f'\"{new_key}\"'\n            yield new_key, v", "idx": 173}
{"namespace": "bentoml._internal.configuration.helpers.load_config_file", "completion": "    if not os.path.exists(path):\n        raise FileNotFoundError(f\"Configuration file not found at {path}\")\n    with open(path, \"r\") as file:\n        config = yaml.safe_load(file)\n    return config", "idx": 174}
{"namespace": "bentoml._internal.configuration.helpers.expand_env_var_in_values", "completion": "    for key, value in d.items():\n        if isinstance(value, t.MutableMapping):\n            expand_env_var_in_values(value)\n        elif isinstance(value, str):\n            d[key] = expand_env_var(value)\n        elif isinstance(value, t.Sequence):\n            d[key] = [expand_env_var(item) if isinstance(item, str) else item for item in value]", "idx": 175}
{"namespace": "bentoml._internal.runner.strategy.DefaultStrategy.get_worker_count", "completion": "\n        if resource_request is None:\n            resource_request = system_resources()\n\n        nvidia_gpus: list[int] | None = get_resource(resource_request, \"nvidia.com/gpu\")\n        cpus = get_resource(resource_request, \"cpu\")\n\n        if nvidia_gpus is not None and len(nvidia_gpus) > 0 and \"nvidia.com/gpu\" in runnable_class.SUPPORTED_RESOURCES:\n            if isinstance(workers_per_resource, float):\n                assigned_resource_per_worker = round(1 / workers_per_resource)\n                if len(nvidia_gpus) < assigned_resource_per_worker:\n                    raise IndexError(\n                        f\"There aren't enough assigned GPU(s) for given worker id '{worker_index}' [required: {assigned_resource_per_worker}].\"\n                    )\n                return len(nvidia_gpus) // assigned_resource_per_worker\n            else:\n                return len(nvidia_gpus) // workers_per_resource\n\n        elif cpus is not None and cpus > 0 and runnable_class.SUPPORTS_CPU_MULTI_THREADING:\n            return math.ceil(cpus) // workers_per_resource\n\n        else:\n            raise ValueError(\"No known supported resources available for the runnable class.\")", "idx": 176}
{"namespace": "bentoml._internal.runner.strategy.DefaultStrategy.get_worker_env", "completion": "        env = {}\n\n        if resource_request is None:\n            resource_request = system_resources()\n\n        # use nvidia gpu\n        nvidia_gpus = get_resource(resource_request, \"nvidia.com/gpu\")\n        if (\n            nvidia_gpus is not None\n            and len(nvidia_gpus) > 0\n            and \"nvidia.com/gpu\" in runnable_class.SUPPORTED_RESOURCES\n        ):\n            env[\"CUDA_VISIBLE_DEVICES\"] = str(nvidia_gpus[worker_index % len(nvidia_gpus)])\n\n        # use CPU\n        cpus = get_resource(resource_request, \"cpu\")\n        if cpus is not None and cpus > 0:\n            if \"cpu\" not in runnable_class.SUPPORTED_RESOURCES:\n                logger.warning(\n                    \"No known supported resource available for %s, falling back to using CPU.\",\n                    runnable_class,\n                )\n\n            if runnable_class.SUPPORTS_CPU_MULTI_THREADING:\n                for thread_env in THREAD_ENVS:\n                    env[thread_env] = str(cpus)\n\n        return env", "idx": 177}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.batches_to_batch", "completion": "        concatenated_batch = np.concatenate(batches, axis=batch_dim)\n        indices = [0] + list(itertools.accumulate(subbatch.shape[batch_dim] for subbatch in batches))\n        return concatenated_batch, indices", "idx": 178}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.to_payload", "completion": "        if batch.ndim == 0:\n            data = pickle.dumps(batch)\n            return cls.create_payload(data, batch_size=1)\n\n        if not (batch.flags.c_contiguous or batch.flags.f_contiguous):\n            raise ValueError(\"Only C-contiguous or F-contiguous arrays are supported\")\n\n        bs, concat_buffer_bs, indices = pep574_dumps(batch)\n\n        if indices:\n            meta = {\"format\": \"pickle5\", \"with_buffer\": True}\n            data = concat_buffer_bs\n            meta[\"pickle_bytes_str\"] = base64.b64encode(bs).decode(\"ascii\")\n            meta[\"indices\"] = indices\n        else:\n            meta = {\"format\": \"pickle5\", \"with_buffer\": False}\n            data = bs\n\n        return cls.create_payload(data, batch.shape[0], meta)", "idx": 179}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.from_payload", "completion": "        \"\"\"\n        This function creates an NdarrayContainer instance from the given payload. It checks the format of the payload and if it is \"pickle5\", it decodes the pickle bytes and returns the deserialized ndarray. Otherwise, it uses the pickle module to load and return the deserialized ndarray.\n        Input-Output Arguments\n        :param cls: Class. The class itself.\n        :param payload: Payload. The payload containing the data and metadata of the ndarray.\n        :return: ext.NpNDArray. The deserialized ndarray.\n        \"\"\"\n        if payload.meta[\"format\"] == \"pickle5\":\n            bs_str = payload.meta[\"pickle_bytes_str\"]\n            bs = base64.b64decode(bs_str)\n            indices = payload.meta[\"indices\"]\n            return pep574_loads(bs, payload.data, indices)\n        else:\n            return pickle.loads(payload.data)", "idx": 180}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.batch_to_payloads", "completion": "        subbatches = cls.batch_to_batches(batch, indices, batch_dim)\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in subbatches]\n        return payloads", "idx": 181}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.from_batch_payloads", "completion": "        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)", "idx": 182}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.to_payload", "completion": "        if batch_dim != 0:\n            raise ValueError(\"PandasDataFrameContainer only supports batch_dim of 0\")\n\n        if isinstance(batch, ext.PdSeries):\n            batch = batch.to_frame().T\n\n        bs = pickle.dumps(batch)\n        concat_buffer_bs, indices = pep574_dumps(batch)\n        bs_str = base64.b64encode(bs).decode(\"ascii\")\n\n        meta = {\n            \"format\": \"pickle5\",\n            \"with_buffer\": bool(indices),\n            \"pickle_bytes_str\": bs_str,\n            \"indices\": indices,\n        }\n\n        data = concat_buffer_bs if indices else bs\n\n        return cls.create_payload(data, batch.shape[batch_dim], meta)", "idx": 183}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.from_payload", "completion": "        format = payload.meta.get(\"format\", \"default\")\n        if format == \"pickle5\":\n            bs_str = t.cast(str, payload.meta[\"pickle_bytes_str\"])\n            bs = base64.b64decode(bs_str)\n            indices = t.cast(t.List[int], payload.meta[\"indices\"])\n            return t.cast(\"ext.PdDataFrame\", pep574_loads(bs, payload.data, indices))\n\n        return pickle.loads(payload.data)", "idx": 184}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batch_to_payloads", "completion": "        subbatches = cls.batch_to_batches(batch, indices, batch_dim)\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in subbatches]\n        return payloads", "idx": 185}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.from_batch_payloads", "completion": "        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)", "idx": 186}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.to_payload", "completion": "        if isinstance(batch, t.Generator):\n            batch = list(batch)\n\n        serialized_data = pickle.dumps(batch)\n        batch_size = len(batch)\n\n        return cls.create_payload(serialized_data, batch_size, meta={})", "idx": 187}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batch_to_payloads", "completion": "        subbatches = cls.batch_to_batches(batch, indices, batch_dim)\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in subbatches]\n        return payloads", "idx": 188}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.from_batch_payloads", "completion": "\n        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)", "idx": 189}
{"namespace": "sslyze.cli.server_string_parser.CommandLineServerStringParser.parse_server_string", "completion": "        if \"{\" in server_str and \"}\" in server_str:\n            ip_start = server_str.find(\"{\")\n            ip_end = server_str.find(\"}\")\n            ip = server_str[ip_start + 1:ip_end]\n            server_str = server_str[:ip_start] + server_str[ip_end + 1:]\n        else:\n            ip = None\n\n        if \"[\" in server_str and \"]\" in server_str:\n            return cls._parse_ipv6_server_string(server_str)\n\n        if ip and \"[\" in ip and \"]\" in ip:\n            return cls._parse_ipv6_server_string(ip)\n\n        return cls._parse_ipv4_server_string(server_str)", "idx": 190}
{"namespace": "sslyze.plugins.heartbleed_plugin._HeartbleedCliConnector.result_to_console_output", "completion": "        return [\n            f\"Heartbleed vulnerability test result for {result.server_info.hostname}:\",\n            f\"Vulnerable: {result.is_vulnerable_to_heartbleed}\"\n        ]", "idx": 191}
{"namespace": "sslyze.plugins.http_headers_plugin._HttpHeadersCliConnector.result_to_console_output", "completion": "        console_output = [\n            f\"HTTP Request Sent: {result.http_request_sent}\",\n            f\"HTTP Path Redirected To: {result.http_path_redirected_to}\",\n        ]\n\n        if result.http_error_trace:\n            console_output.append(\"HTTP Error Trace:\")\n            console_output.extend(str(result.http_error_trace).splitlines())\n\n        if result.strict_transport_security_header:\n            sts_header = result.strict_transport_security_header\n            console_output.append(\"Strict-Transport-Security Header:\")\n            console_output.append(f\"Max Age: {sts_header.max_age}\")\n            console_output.append(f\"Preload: {sts_header.preload}\")\n            console_output.append(f\"Include Subdomains: {sts_header.include_subdomains}\")\n\n        return console_output", "idx": 192}
{"namespace": "sslyze.plugins.http_headers_plugin._detect_http_redirection", "completion": "    # Check if the response status code is a redirection\n    if 300 <= http_response.status < 400:\n        # Get the location header from the response\n        location_header = http_response.getheader(\"location\")\n        if location_header:\n            # Parse the location URL\n            location_url = urlsplit(location_header)\n            if location_url.hostname == server_host_name and location_url.port == server_port:\n                return location_url.path\n\n    return None", "idx": 193}
{"namespace": "sslyze.plugins.session_renegotiation_plugin._SessionRenegotiationCliConnector.result_to_console_output", "completion": "        result_txt = []\n        result_txt.append(\"Session Renegotiation Scan Result:\")\n        result_txt.append(f\"Supports Secure Renegotiation: {result.supports_secure_renegotiation}\")\n        result_txt.append(f\"Is Vulnerable to Client Renegotiation DoS: {result.is_vulnerable_to_client_renegotiation_dos}\")\n        return result_txt", "idx": 194}
{"namespace": "sslyze.plugins.certificate_info._cli_connector._CertificateInfoCliConnector.result_to_console_output", "completion": "        console_output = []\n\n        # Add hostname and number of certificates detected\n        console_output.append(f\"Hostname: {result.hostname}\")\n        console_output.append(f\"Number of Certificates Detected: {len(result.cert_deployments)}\")\n\n        # Iterate through each certificate deployment and add the formatted information to the result list\n        for index, cert_deployment in enumerate(result.cert_deployments, start=1):\n            console_output.extend(cls._cert_deployment_to_console_output(index, cert_deployment))\n\n        return console_output", "idx": 195}
{"namespace": "sslyze.plugins.certificate_info._cli_connector._get_name_as_short_text", "completion": "    common_name = name_field.get_attributes_for_oid(x509.NameOID.COMMON_NAME)\n    if common_name:\n        return str(common_name[0].value)\n    else:\n        return str(name_field)", "idx": 196}
{"namespace": "sslyze.plugins.certificate_info._symantec.SymantecDistructTester.get_distrust_timeline", "completion": "        for cert in verified_certificate_chain:\n            cert_key = binascii.hexlify(cert.public_key().public_bytes()).decode(\"utf-8\")\n            if cert_key in cls._CA_KEYS_BLACKLIST:\n                return SymantecDistrustTimelineEnum.MARCH_2018\n            elif cert_key in cls._CA_KEYS_WHITELIST:\n                return None\n        return SymantecDistrustTimelineEnum.SEPTEMBER_2018", "idx": 197}
{"namespace": "sslyze.plugins.certificate_info._certificate_utils.parse_subject_alternative_name_extension", "completion": "    try:\n        san_extension = certificate.extensions.get_extension_for_oid(ExtensionOID.SUBJECT_ALTERNATIVE_NAME)\n        san = cast(SubjectAlternativeName, san_extension.value)\n        dns_names = [name.value for name in san.get_values_for_type(DNSName)]\n        ip_addresses = [ip_address.value for ip_address in san.get_values_for_type(IPAddress)]\n        return SubjectAlternativeNameExtension(dns_names=dns_names, ip_addresses=ip_addresses)\n    except (ExtensionNotFound, DuplicateExtension):\n        return SubjectAlternativeNameExtension(dns_names=[], ip_addresses=[])", "idx": 198}
{"namespace": "sslyze.plugins.certificate_info._cert_chain_analyzer._certificate_matches_hostname", "completion": "    try:\n        match_hostname(certificate, server_hostname)\n        return True\n    except CertificateError:\n        return False", "idx": 199}
{"namespace": "pyramid.authentication.SessionAuthenticationHelper.remember", "completion": "        \"\"\"\n        This function takes a request object, a user ID, and any additional keyword arguments. It then stores the user ID in the session.\n        Input-Output Arguments\n        :param self: SessionAuthenticationHelper. An instance of the SessionAuthenticationHelper class.\n        :param request: The request object.\n        :param userid: The user ID to store in the session.\n        :param **kw: Additional keyword arguments.\n        :return: An empty list.\n        \"\"\"\n        request.session[self.userid_key] = userid\n        return []", "idx": 200}
{"namespace": "pyramid.authentication.SessionAuthenticationHelper.forget", "completion": "        if self.userid_key in request.session:\n            del request.session[self.userid_key]\n        return []", "idx": 201}
{"namespace": "pyramid.authentication.BasicAuthAuthenticationPolicy.unauthenticated_userid", "completion": "        credentials = extract_http_basic_credentials(request)\n        if credentials:\n            return credentials.username\n        else:\n            return None", "idx": 202}
{"namespace": "pyramid.request.CallbackMethodsMixin._process_response_callbacks", "completion": "        callbacks = self.response_callbacks\n        while callbacks:\n            callback = callbacks.popleft()\n            callback(self, response)", "idx": 203}
{"namespace": "pyramid.request.CallbackMethodsMixin._process_finished_callbacks", "completion": "        callbacks = self.finished_callbacks\n        while callbacks:\n            callback = callbacks.popleft()\n            callback(self)", "idx": 204}
{"namespace": "pyramid.request.Request.session", "completion": "        session_factory = self.registry.queryUtility(ISessionFactory)\n        if session_factory is None:\n            raise ConfigurationError(\"No session factory registered\")\n        return session_factory(self)", "idx": 205}
{"namespace": "pyramid.request.RequestLocalCache.get_or_create", "completion": "        if creator is None:\n            creator = self._creator\n        value = self.get(request, self.NO_VALUE)\n        if value is self.NO_VALUE:\n            value = creator(request)\n            self.set(request, value)\n        return value", "idx": 206}
{"namespace": "pyramid.request.RequestLocalCache.set", "completion": "        self._store[request] = value\n\n        def remove_request_from_cache(request, cache_ref):\n            cache = cache_ref()\n            if cache is not None:\n                cache.clear(request)\n\n        request.add_finished_callback(\n            functools.partial(remove_request_from_cache, request, weakref.ref(self))\n        )", "idx": 207}
{"namespace": "pyramid.i18n.Localizer.pluralize", "completion": "        if self.pluralizer is None:\n            self.pluralizer = Pluralizer(self.translations)\n        return self.pluralizer(singular, plural, n, domain=domain, mapping=mapping)", "idx": 208}
{"namespace": "pyramid.i18n.Translations.load", "completion": "        if dirname is None:\n            return gettext.NullTranslations()\n\n        translations = Translations()\n        translations._catalog = {}\n\n        for locale in locales:\n            locale_dir = os.path.join(dirname, str(locale))\n            if os.path.isdir(locale_dir):\n                messages_dir = os.path.join(locale_dir, 'LC_MESSAGES')\n                if not os.path.isdir(messages_dir):\n                    continue\n                for mofile in os.listdir(messages_dir):\n                    mopath = os.path.join(messages_dir, mofile)\n                    if mofile.endswith('.mo') and os.path.isfile(mopath):\n                        with open(mopath, 'rb') as mofp:\n                            domain = mofile[:-3]\n                            dtrans = Translations(mofp, domain)\n                            translations.add(dtrans)\n\n        return translations", "idx": 209}
{"namespace": "pyramid.i18n.Translations.add", "completion": "        if merge:\n            return self.merge(translations)\n        else:\n            domain = translations.domain\n            if domain in self._domains:\n                self._domains[domain].merge(translations)\n            else:\n                self._domains[domain] = translations\n            return self", "idx": 210}
{"namespace": "pyramid.i18n.Translations.merge", "completion": "        domain = getattr(translations, 'domain', self.DEFAULT_DOMAIN)\n        if domain == self.DEFAULT_DOMAIN and self.plural is DEFAULT_PLURAL:\n            self.plural = translations.plural\n\n        if domain == self.domain:\n            for file in translations.files:\n                if file not in self.files:\n                    self.files.append(file)\n\n        for key, value in translations._catalog.items():\n            if key not in self._catalog:\n                self._catalog[key] = value\n            else:\n                for k, v in value.items():\n                    if k not in self._catalog[key]:\n                        self._catalog[key][k] = v\n\n        return self", "idx": 211}
{"namespace": "pyramid.i18n.LocalizerRequestMixin.locale_name", "completion": "        return negotiate_locale_name(self)", "idx": 212}
{"namespace": "pyramid.csrf.LegacySessionCSRFStoragePolicy.check_csrf_token", "completion": "        expected_token = self.get_csrf_token(request)\n        return not strings_differ(\n            bytes_(expected_token), bytes_(supplied_token)\n        )", "idx": 213}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.new_csrf_token", "completion": "        \"\"\"\n        This function generates a new CSRF token and stores it in the session. It then returns the generated token.\n        Input-Output Arguments\n        :param self: SessionCSRFStoragePolicy. An instance of the SessionCSRFStoragePolicy class.\n        :param request: The request object.\n        :return: String. The newly generated CSRF token.\n        \"\"\"\n        token = self._token_factory()\n        request.session[self.key] = token\n        return token", "idx": 214}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.get_csrf_token", "completion": "        \"\"\"This function retrieves the currently active CSRF token from the session. If the token is not found in the session, a new one is generated and returned.\"\"\"\n        token = request.session.get(self.key)\n        if token is None:\n            token = self.new_csrf_token(request)\n        return token", "idx": 215}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.check_csrf_token", "completion": "        expected_token = self.get_csrf_token(request)\n        return not strings_differ(\n            bytes_(expected_token), bytes_(supplied_token)\n        )", "idx": 216}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.new_csrf_token", "completion": "        token = self._token_factory()\n        bound_cookies = self.cookie_profile.bind(request)\n        bound_cookies.set_value(token)\n        request.add_response_callback(bound_cookies.add_cookie_to_response)\n        return token", "idx": 217}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.get_csrf_token", "completion": "        token = request.cookies.get(self.cookie_name)\n        if not token:\n            token = self.new_csrf_token(request)\n        return token", "idx": 218}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.check_csrf_token", "completion": "        expected_token = self.get_csrf_token(request)\n        return not strings_differ(\n            bytes_(expected_token), bytes_(supplied_token)\n        )", "idx": 219}
{"namespace": "pyramid.security.PermitsResult.__repr__", "completion": "        return f\"<{self.__class__.__name__} instance at {id(self)} with msg {self.msg}>\"", "idx": 220}
{"namespace": "pyramid.util.InstancePropertyHelper.make_property", "completion": "        if name is None:\n            name = callable.__name__\n\n        if reify:\n            prop = SettableProperty(callable)\n        else:\n            prop = property(callable)\n\n        return name, prop", "idx": 221}
{"namespace": "pyramid.util.InstancePropertyHelper.set_property", "completion": "        name, fn = cls.make_property(callable, name=name, reify=reify)\n        cls.properties[name] = fn\n        cls.apply_properties(target, cls.properties)", "idx": 222}
{"namespace": "pyramid.util.InstancePropertyHelper.add_property", "completion": "        prop = self.make_property(callable, name=name, reify=reify)\n        self.properties[prop[0]] = prop[1]", "idx": 223}
{"namespace": "pyramid.util.InstancePropertyHelper.apply", "completion": "        InstancePropertyHelper.apply_properties(target, self.properties)", "idx": 224}
{"namespace": "pyramid.util.InstancePropertyMixin.set_property", "completion": "        prop = self.make_property(callable, name=name, reify=reify)\n        self.apply_properties(self, [prop])", "idx": 225}
{"namespace": "pyramid.util.TopologicalSorter.remove", "completion": "        if name in self.names:\n            self.names.remove(name)\n            del self.name2val[name]\n            if name in self.name2before:\n                del self.name2before[name]\n            if name in self.name2after:\n                del self.name2after[name]\n            self.order = [(a, b) for a, b in self.order if a != name and b != name]\n            self.req_before.discard(name)\n            self.req_after.discard(name)", "idx": 226}
{"namespace": "pyramid.util.TopologicalSorter.add", "completion": "        if after is not None:\n            if isinstance(after, str):\n                after = [after]\n            for a in after:\n                self.req_before.add(name)\n                self.name2before.setdefault(name, []).append(a)\n                self.order.append((a, name))\n\n        if before is not None:\n            if isinstance(before, str):\n                before = [before]\n            for b in before:\n                self.req_after.add(name)\n                self.name2after.setdefault(name, []).append(b)\n                self.order.append((name, b))\n\n        self.names.append(name)\n        self.name2val[name] = val", "idx": 227}
{"namespace": "pyramid.traversal.find_resource", "completion": "    if isinstance(path, str):\n        # Handle string path\n        if path.startswith('/'):\n            resource = find_root(resource)\n        return traverse(resource, path)\n    elif isinstance(path, tuple):\n        # Handle tuple path\n        if path and path[0] == '':\n            resource = find_root(resource)\n        return traverse(resource, path)\n    else:\n        raise ValueError(\"Path must be a string or a tuple\")", "idx": 228}
{"namespace": "pyramid.static.ManifestCacheBuster.manifest", "completion": "        if self.reload:\n            mtime = self.getmtime(self.manifest_path)\n            if mtime != self._mtime:\n                self._manifest = self.get_manifest()\n                self._mtime = mtime\n        return self._manifest", "idx": 229}
{"namespace": "pyramid.registry.Registry.registerSubscriptionAdapter", "completion": "        result = Components.registerSubscriptionAdapter(self, *arg, **kw)\n        self.has_listeners = True\n        return result", "idx": 230}
{"namespace": "pyramid.registry.Registry.registerHandler", "completion": "        result = Components.registerHandler(self, *arg, **kw)\n        self.has_listeners = True\n        return result", "idx": 231}
{"namespace": "pyramid.registry.Registry.notify", "completion": "        if not self.has_listeners:\n            return\n        for event in events:\n            subscribers = self.adapters.subscribers((event,), None)\n            for subscriber in subscribers:\n                subscriber(event)", "idx": 232}
{"namespace": "pyramid.registry.Introspector.add", "completion": "        category = self._categories.setdefault(intr.category_name, {})\n        intr.order = self._counter\n        self._counter += 1\n        category[intr.discriminator] = intr", "idx": 233}
{"namespace": "pyramid.registry.Introspector.get", "completion": "        category = self._categories.get(category_name)\n        if category is None:\n            return default\n        return category.get(discriminator, default)", "idx": 234}
{"namespace": "pyramid.registry.Introspector.get_category", "completion": "        category = self._categories.get(category_name, default)\n        if category is None:\n            return []\n        \n        introspectable_values = list(category.values())\n        if sort_key is not None:\n            introspectable_values.sort(key=sort_key)\n        else:\n            introspectable_values.sort(key=operator.attrgetter('order'))\n        \n        result = []\n        for introspectable in introspectable_values:\n            related_values = self.related(introspectable)\n            result.append({'introspectable': introspectable, 'related': related_values})\n        \n        return result", "idx": 235}
{"namespace": "pyramid.registry.Introspector.categorized", "completion": "        if sort_key is None:\n            sort_key = operator.attrgetter('order')\n        categories = self.categories()\n        categorized_data = []\n        for category_name in categories:\n            data = self.get_category(category_name, sort_key=sort_key)\n            categorized_data.append((category_name, data))\n        return categorized_data", "idx": 236}
{"namespace": "pyramid.registry.Introspector.remove", "completion": "        intr = self._categories.get(category_name, {}).get(discriminator)\n        if intr:\n            del self._refs[intr]\n            del self._categories[category_name][discriminator]", "idx": 237}
{"namespace": "pyramid.registry.Introspector.relate", "completion": "        introspectables = self._get_intrs_by_pairs(pairs)\n        relatable = ((x, y) for x in introspectables for y in introspectables)\n        for x, y in relatable:\n            L = self._refs.setdefault(x, [])\n            if y not in L:\n                L.append(y)", "idx": 238}
{"namespace": "pyramid.registry.Introspector.related", "completion": "        category_name = intr.category_name\n        discriminator = intr.discriminator\n        category = self._categories.get(category_name)\n        if category:\n            introspector = category.get(discriminator)\n            if introspector:\n                return self._refs.get(introspector, [])\n        raise KeyError((category_name, discriminator))", "idx": 239}
{"namespace": "pyramid.registry.Introspectable.discriminator_hash", "completion": "        \"\"\"\n        Calculate the hash of the discriminator of the Introspectable instance.\n        Input-Output Arguments\n        :param self: Introspectable. An instance of the Introspectable class.\n        :return: int. The hash value of the discriminator in the instance.\n        \"\"\"\n        self._assert_resolved()\n        return hash((self.category_name,) + (self.discriminator,))", "idx": 240}
{"namespace": "pyramid.registry.Introspectable.__repr__", "completion": "        return '<%s category %r, discriminator %r>' % (self.type_name, self.category_name, self.discriminator)", "idx": 241}
{"namespace": "pyramid.scripts.proutes.PRoutesCommand._get_mapper", "completion": "        return registry.queryUtility(IRoutesMapper)", "idx": 242}
{"namespace": "pyramid.scripts.pshell.PShellCommand.make_shell", "completion": "        if self.args.python_shell:\n            shell = self.args.python_shell\n            if shell in self.find_all_shells():\n                return self.find_all_shells()[shell]\n            else:\n                raise ValueError('could not find a shell named \"%s\"' % shell)\n        elif self.preferred_shells:\n            for preferred_shell in self.preferred_shells:\n                if preferred_shell in self.find_all_shells():\n                    return self.find_all_shells()[preferred_shell]\n        for shell in self.find_all_shells():\n            return self.find_all_shells()[shell]\n        return self.default_runner", "idx": 243}
{"namespace": "pyramid.config.assets.PackageOverrides.insert", "completion": "        if path == '' or path.endswith('/'):\n            override = DirectoryOverride(path, source)\n        else:\n            override = FileOverride(path, source)\n        self.overrides.insert(0, override)\n        return override", "idx": 244}
{"namespace": "pyramid.config.assets.PackageOverrides.filtered_sources", "completion": "        for override in self.overrides:\n            result = override(resource_name)\n            if result is not None:\n                yield result", "idx": 245}
{"namespace": "pyramid.config.assets.PackageOverrides.real_loader", "completion": "        if self._real_loader is None:\n            raise NotImplementedError(\"Real loader is not set\")\n        return self._real_loader", "idx": 246}
{"namespace": "pyramid.config.views.MultiView.add", "completion": "        if accept is None:\n            if phash is not None:\n                for i, (o, v, p) in enumerate(self.views):\n                    if p == phash:\n                        self.views[i] = (order, view, phash)\n                        break\n                else:\n                    self.views.append((order, view, phash))\n                    self.views.sort(key=lambda x: x[0])\n            else:\n                self.views.append((order, view, phash))\n                self.views.sort(key=lambda x: x[0])\n        else:\n            if accept_order is not None:\n                if phash is not None:\n                    for i, (o, v, p) in enumerate(self.media_views.get(accept, [])):\n                        if p == phash:\n                            self.media_views[accept][i] = (order, view, phash)\n                            break\n                    else:\n                        self.media_views.setdefault(accept, []).append((order, view, phash))\n                        self.media_views[accept].sort(key=lambda x: x", "idx": 247}
{"namespace": "pyramid.config.views.MultiView.get_views", "completion": "        if request.accept and self.accepts:\n            views = []\n            for offer in request.accept:\n                if offer in self.media_views:\n                    views.extend(self.media_views[offer])\n            views.extend(self.views)\n            return views\n        else:\n            return self.views", "idx": 248}
{"namespace": "pyramid.config.views.MultiView.match", "completion": "        for order, view, phash in self.get_views(request):\n            if hasattr(view, '__predicated__'):\n                if not view.__predicated__(context, request):\n                    continue\n            return view\n        raise PredicateMismatch(self.name)", "idx": 249}
{"namespace": "pyramid.config.views.MultiView.__permitted__", "completion": "        view = self.match(context, request)\n        view = getattr(view, '__permitted__', view)\n        return view(context, request)", "idx": 250}
{"namespace": "pyramid.config.views.MultiView.__call_permissive__", "completion": "        for order, view, phash in self.get_views(request):\n            try:\n                return view(context, request)\n            except PredicateMismatch:\n                continue\n        raise PredicateMismatch(self.name)", "idx": 251}
{"namespace": "pyramid.config.actions.ActionState.processSpec", "completion": "        # Check if the spec is in the list of resolved discriminators\n        if spec in self.resolved_ainfos:\n            return False  # Processing is not needed\n        else:\n            # Mark the spec as processed\n            self.resolved_ainfos[spec] = None\n            return True  # Processing is needed", "idx": 252}
{"namespace": "pyramid.config.actions.ActionState.action", "completion": "        action = expand_action_tuple(\n            discriminator,\n            callable,\n            args,\n            kw,\n            includepath,\n            info,\n            order,\n            introspectables,\n            **extra,\n        )\n        self.actions.append(action)", "idx": 253}
{"namespace": "pyramid.config.actions.ActionInfo.__str__", "completion": "        formatted_source = \"\\n\".join(f\"    {line}\" for line in self.src.split(\"\\n\"))\n        return f\"Line {self.line} of file {self.file}:\\n{formatted_source}\"", "idx": 254}
{"namespace": "pyramid.config.Configurator.__getattr__", "completion": "        # allow directive extension names to work\n        if name in self.registry._directives:\n            c, action_wrap = self.registry._directives[name]\n            if action_wrap:\n                return self._make_action(c)\n            else:\n                return c.__get__(self, self.__class__)\n        raise AttributeError(name)", "idx": 255}
{"namespace": "pyramid.config.Configurator.with_package", "completion": "        new_configurator = self.__class__(\n            registry=self.registry,\n            package=package,\n            root_package=self.root_package,\n            autocommit=self.autocommit,\n            route_prefix=self.route_prefix,\n        )\n        new_configurator.basepath = self.basepath\n        new_configurator.includepath = self.includepath\n        new_configurator.action_state = self.action_state\n        new_configurator.venusian = self.venusian\n        return new_configurator", "idx": 256}
{"namespace": "pyramid.config.Configurator.absolute_asset_spec", "completion": "        return self._make_spec(relative_spec)", "idx": 257}
{"namespace": "pyramid.config.Configurator.begin", "completion": "        if request is _marker:\n            if self.manager.stack:\n                registry, _ = self.manager.stack[-1]\n                self.manager.push({'registry': registry, 'request': _marker})\n            else:\n                self.manager.push({'registry': self.registry, 'request': _marker})\n        else:\n            self.manager.push({'registry': self.registry, 'request': request})", "idx": 258}
{"namespace": "pyramid.config.Configurator.scan", "completion": "        scanner = venusian.Scanner(\n            config=self,\n            categories=categories,\n            onerror=onerror,\n            ignore=ignore,\n            **kw,\n        )\n        scanner.scan(package=package)", "idx": 259}
{"namespace": "pyramid.config.Configurator.make_wsgi_app", "completion": "        self.commit()\n        self.manager.pop()  # pop the manager pushed by begin()\n        app = Router(self.registry)\n        global_registries.add(self.registry)\n        return app", "idx": 260}
{"namespace": "aiohappybase._util.camel_case_to_pep8", "completion": "    return CAPITALS.sub(r'_\\1', name).lstrip('_').lower()", "idx": 261}
{"namespace": "kinto.authorization._relative_object_uri", "completion": "    obj_parts = object_uri.split(\"/\")\n    parent_uri = \"/\".join(obj_parts[:-2])  # Get the parent URI by removing the last two parts\n    parent_resource_name, _ = _resource_endpoint(parent_uri)  # Get the resource name of the parent URI\n\n    if parent_resource_name == resource_name:  # Check if the resource name matches the parent resource name\n        return parent_uri\n    else:\n        raise ValueError(f\"No matching parent resource found for {resource_name} in {object_uri}\")  # Raise an error if no match is found", "idx": 262}
{"namespace": "kinto.core.openapi.OpenAPI.expose_authentication_method", "completion": "        cls.security_definitions[method_name] = definition\n\n        if \"scopes\" in definition:\n            for scope, roles in definition[\"scopes\"].items():\n                cls.security_roles[scope] = roles", "idx": 263}
{"namespace": "kinto.core.openapi.OpenAPI.generate", "completion": "        base_spec = {\n            \"host\": self.request.host,\n            \"schemes\": [self.request.scheme],\n            \"securityDefinitions\": self.security_definitions\n        }\n        return super().generate(swagger=base_spec)", "idx": 264}
{"namespace": "kinto.core.testing.get_user_headers", "completion": "    user_pass = f\"{user}:{password}\"\n    encoded_user_pass = base64.b64encode(user_pass.encode()).decode(\"utf-8\")\n    return {\"Authorization\": f\"Basic {encoded_user_pass}\"}", "idx": 265}
{"namespace": "kinto.core.authorization.RouteFactory.fetch_shared_objects", "completion": "        shared_objects = self._get_accessible_objects(principals, perm, get_bound_permissions)\n        if shared_objects:\n            self.shared_ids = [obj[\"id\"] for obj in shared_objects]\n            return self.shared_ids\n        else:\n            return None", "idx": 266}
{"namespace": "kinto.core.authorization.RouteFactory.get_permission_object_id", "completion": "        if self.on_plural_endpoint and object_id:\n            plural_path = str(utils.current_service(request).plural_path)\n            plural_path = plural_path.format_map(request.matchdict)\n            object_path = str(utils.current_service(request).object_path)\n            object_path = object_path.format_map(request.matchdict)\n            return f\"{plural_path}/{object_id}{object_path}\"\n        return request.path", "idx": 267}
{"namespace": "kinto.core.utils.recursive_update_dict", "completion": "    for key, value in changes.items():\n        if value in ignores:\n            root.pop(key, None)\n        elif isinstance(value, collections_abc.Mapping):\n            root[key] = recursive_update_dict(root.get(key, {}), value, ignores)\n        else:\n            root[key] = value\n    return root", "idx": 268}
{"namespace": "kinto.core.utils.native_value", "completion": "    try:\n        # Try parsing the value as JSON\n        return json.loads(value)\n    except (json.JSONDecodeError, TypeError):\n        # If parsing fails, return the original string value\n        return value", "idx": 269}
{"namespace": "kinto.core.utils.dict_subset", "completion": "    result = {}\n    for key in keys:\n        if \".\" in key:\n            nested_keys = key.split(\".\")\n            value = find_nested_value(d, key)\n            if value is not None:\n                result[key] = value\n        else:\n            if key in d:\n                result[key] = d[key]\n    return result", "idx": 270}
{"namespace": "kinto.core.utils.dict_merge", "completion": "    for k, v in b.items():\n        if isinstance(v, collections_abc.Mapping):\n            a[k] = dict_merge(a.get(k, {}), v)\n        else:\n            a[k] = v\n    return a", "idx": 271}
{"namespace": "kinto.core.utils.find_nested_value", "completion": "    if path in d:\n        return d[path]\n    parts = path.split(\".\")\n    for i in range(len(parts) - 1, 0, -1):\n        root_key = \".\".join(parts[:i])\n        if root_key in d and isinstance(d[root_key], dict):\n            subpath = \".\".join(parts[i:])\n            return find_nested_value(d[root_key], subpath, default)\n    return default", "idx": 272}
{"namespace": "kinto.core.utils.instance_uri_registry", "completion": "    api_prefix = f\"/{registry.route_prefix}\"\n    path = api_prefix + f\"/{resource_name}\"\n    q = registry.queryUtility\n    routes_mapper = q(IRoutesMapper)\n    fakerequest = Request.blank(path=path)\n    info = routes_mapper(fakerequest)\n    matchdict, route = info[\"match\"]\n    if route is None:\n        raise ValueError(\"URI has no route\")\n    return strip_uri_prefix(request.route_path(f\"{resource_name}-object\", **params))", "idx": 273}
{"namespace": "kinto.core.statsd.load_from_config", "completion": "    if statsd_module is None:\n        raise ConfigurationError(\"statsd module is not installed.\")\n\n    settings = config.get_settings()\n    statsd_url = settings.get(\"statsd_url\")\n    if not statsd_url:\n        raise ConfigurationError(\"No statsd_url setting found.\")\n\n    parsed_url = urlparse(statsd_url)\n    host = parsed_url.hostname\n    port = parsed_url.port or 8125\n    prefix = parsed_url.path.lstrip(\"/\")\n    \n    return Client(host, port, prefix)", "idx": 274}
{"namespace": "kinto.core.errors.http_error", "completion": "    response = httpexception\n    response.content_type = \"application/json\"\n    response.json = {\n        \"errno\": errno if errno is not None else ERRORS.UNDEFINED.value,\n        \"code\": code if code is not None else httpexception.code,\n        \"error\": error if error is not None else httpexception.title,\n        \"message\": message,\n        \"info\": info,\n        \"details\": details,\n    }\n    return response", "idx": 275}
{"namespace": "kinto.core.resource.schema.ResourceReponses.get_and_bind", "completion": "        # Clone default schemas.\n        responses = self.default_schemas.copy()\n\n        # Add endpoint-specific schemas.\n        if endpoint_type == \"object\":\n            responses.update(self.default_object_schemas)\n        elif endpoint_type == \"plural\":\n            responses.update(self.default_plural_schemas)\n\n        # Add method-specific schemas.\n        if method == \"GET\":\n            responses.update(self.default_get_schemas)\n            if endpoint_type == \"object\":\n                responses.update(self.object_get_schemas)\n        elif method == \"POST\":\n            responses.update(self.default_post_schemas)\n        elif method == \"PUT\":\n            responses.update(self.default_put_schemas)\n        elif method == \"PATCH\":\n            responses.update(self.default_patch_schemas)\n            if endpoint_type == \"object\":\n                responses.update(self.object_patch_schemas)\n        elif method == \"DELETE\":\n            responses.update(self.default_delete_schemas)\n            if endpoint_type == \"object\":\n                responses.update(self.object_delete_schemas)\n\n        # Bind the responses.\n        bound_responses = {}\n        for status_code, schema in responses.items():\n            bound_responses[status_code] = schema.bind(\n                request=self, response=schema, **kwargs\n            )\n\n        return bound_responses", "idx": 276}
{"namespace": "kinto.core.resource.Resource.timestamp", "completion": "        try:\n            # Try to get the timestamp from the model associated with the resource.\n            return self.model.timestamp()\n        except Exception as e:\n            # If it fails, raise a read-only error exception and save the error information into an HTTP error, raising a JSON-formatted response matching the error HTTP API.\n            error_msg = f\"Failed to retrieve timestamp: {str(e)}\"\n            error_details = {\"location\": \"timestamp\", \"description\": error_msg}\n            raise_invalid(self.request, **error_details)", "idx": 277}
{"namespace": "kinto.core.resource.Resource.plural_post", "completion": "        \"\"\"\n        This function is the \"POST\" endpoint for creating an object in a model. It first checks if the new object id conflicts with an existing one. If it does, the existing object is returned with a status code of 200. If the \"If-Match\" header is provided and the objects have been modified in the meantime, a \"HTTPPreconditionFailed\" exception is raised. If the object id is specified, it is added to the posted body and the existing object is looked up. If the object exists, it is returned with a status code of 200. If the object does not exist, the new object is processed, created, and returned with a status code of 201.\n        Input-Output Arguments\n        :param self: Resource. An instance of the Resource class.\n        :return: The created or existing object with the appropriate status code.\n        \"\"\"\n        self._raise_400_if_invalid_id(self.object_id)\n        try:\n            existing = self._get_object_or_404(self.object_id)\n            return self.postprocess(existing)\n        except HTTPNotFound:\n            post_object = self.request.validated[\"body\"].get(\"data\", {})\n            object_id = post_object.setdefault(self.model.id_field, self.object_id)\n            self._raise_400_if_id_mismatch(object_id, self.object_id)\n            new_object = self.process_object(post_object)\n            obj = self.model.create_object(new_object)\n            timestamp = obj[self.model.modified_field]\n            self._add_timestamp_header(self.request.response, timestamp=timestamp)\n            return self.postprocess(obj)", "idx": 278}
{"namespace": "kinto.core.resource.Resource.get", "completion": "        self._add_timestamp_header(self.request.response)\n        self._add_cache_header(self.request.response)\n        self._raise_304_if_not_modified()\n        obj = self._get_object_or_404(self.object_id)\n        return self.postprocess(obj)", "idx": 279}
{"namespace": "kinto.core.resource.Resource.delete", "completion": "        self._raise_400_if_invalid_id(self.object_id)\n        existing = self._get_object_or_404(self.object_id)\n        self._raise_412_if_modified(existing)\n\n        deleted_object = self.model.delete_object(self.object_id)\n\n        return self.postprocess(deleted_object, action=ACTIONS.DELETE)", "idx": 280}
{"namespace": "kinto.core.permission.memory.Permission.add_principal_to_ace", "completion": "        permission_key = f\"permission:{object_id}:{permission}\"\n        object_permission_principals = self._store.get(permission_key, set())\n        object_permission_principals.add(principal)\n        self._store[permission_key] = object_permission_principals", "idx": 281}
{"namespace": "kinto.core.permission.memory.Permission.get_object_permission_principals", "completion": "        permission_key = f\"permission:{object_id}:{permission}\"\n        return self._store.get(permission_key, set())", "idx": 282}
{"namespace": "kinto.core.storage.generators.Generator.match", "completion": "        if self._regexp is None:\n            self._regexp = re.compile(self.regexp)\n        return bool(self._regexp.match(object_id))", "idx": 283}
{"namespace": "kinto.core.storage.postgresql.migrator.MigratorMixin.create_or_migrate_schema", "completion": "        current_version = self.get_installed_version()\n        if current_version is None:\n            self.create_schema(dry_run)\n        elif current_version == self.schema_version:\n            logger.info(f\"PostgreSQL {self.name} schema is already at version {self.schema_version}.\")\n        else:\n            self.migrate_schema(current_version, dry_run)", "idx": 284}
{"namespace": "kinto.core.views.batch.BatchPayloadSchema.deserialize", "completion": "    if cstruct is colander.null:\n        return colander.null\n\n    # Merge defaults with requests\n    deserialized_data = cstruct.copy()\n    defaults = deserialized_data.pop(\"defaults\", {})\n    for request in deserialized_data.get(\"requests\", []):\n        for key, value in defaults.items():\n            if key not in request:\n                request[key] = value\n\n    return deserialized_data", "idx": 285}
{"namespace": "kinto.plugins.accounts.utils.get_cached_reset_password", "completion": "    hmac_secret = registry.settings[\"userid_hmac_secret\"]\n    cache_key = utils.hmac_digest(hmac_secret, ACCOUNT_RESET_PASSWORD_CACHE_KEY.format(username))\n    cache = registry.cache\n    reset_password = cache.get(cache_key)\n    return reset_password", "idx": 286}
{"namespace": "kinto.plugins.accounts.utils.get_cached_validation_key", "completion": "    hmac_secret = registry.settings[\"userid_hmac_secret\"]\n    cache_key = utils.hmac_digest(hmac_secret, ACCOUNT_VALIDATION_CACHE_KEY.format(username))\n\n    cache = registry.cache\n    cache_result = cache.get(cache_key)\n    return cache_result", "idx": 287}
{"namespace": "kinto.plugins.accounts.views.validation.on_account_activated", "completion": "    settings = event.request.registry.settings\n    account_validation_enabled = settings.get(\"account_validation.enabled\", False)\n\n    if account_validation_enabled:\n        for change in event.impacted_objects:\n            old_account_validated = change.get(\"old\", {}).get(\"validated\", False)\n            new_account_validated = change.get(\"new\", {}).get(\"validated\", False)\n\n            if old_account_validated or not new_account_validated:\n                continue\n\n            # Send confirmation email to the account.\n            user_id = change[\"new\"][\"id\"]\n            user = event.request.registry.storage.get(\n                parent_id=user_id, resource_name=\"account\", object_id=user_id\n            )\n            Emailer(event.request, user).send_account_activation_confirmation()", "idx": 288}
{"namespace": "kinto.plugins.openid.OpenIDConnectPolicy._verify_token", "completion": "        userinfo_endpoint = self.oid_config[\"userinfo_endpoint\"]\n        headers = {\"Authorization\": f\"{self.header_type} {access_token}\"}\n\n        try:\n            response = requests.get(userinfo_endpoint, headers=headers)\n            response.raise_for_status()\n            return response.json()\n        except requests.RequestException as e:\n            # Log the error and return None.\n            print(f\"Error verifying access token: {e}\")\n            return None", "idx": 289}
{"namespace": "kinto.plugins.quotas.scripts.rebuild_quotas", "completion": "    for bucket in storage.list_all(resource_name=\"bucket\"):\n        bucket_id = bucket[\"id\"]\n        bucket_record_count = 0\n        bucket_storage_size = 0\n        bucket_collection_count = 0\n        for collection in storage.list_all(resource_name=\"collection\", parent_id=f\"/buckets/{bucket_id}\"):\n            collection_record_count, collection_storage_size = rebuild_quotas_collection(storage, bucket_id, collection, dry_run)\n            bucket_record_count += collection_record_count\n            bucket_storage_size += collection_storage_size\n            bucket_collection_count += 1\n\n        logger.info(\n            f\"Bucket {bucket_id}. Final size: {bucket_record_count} records, {bucket_storage_size} bytes, {bucket_collection_count} collections.\"\n        )\n        new_quota_info = {\n            \"record_count\": bucket_record_count,\n            \"storage_size\": bucket_storage_size,\n            \"collection_count\": bucket_collection_count,\n        }\n        if not dry_run:\n            storage.update(\n                resource_name=\"quota\",\n                parent_id=f\"/buckets/{bucket_id}\",\n                object_id=COLLECTION_QUOTA_OBJECT_ID,\n                obj=new_quota_info,\n            )", "idx": 290}
{"namespace": "kinto.config.render_template", "completion": "    with codecs.open(template, \"r\", encoding=\"utf-8\") as f:\n        template_content = f.read()\n\n    rendered_content = template_content.format(**kwargs)\n\n    with codecs.open(destination, \"w\", encoding=\"utf-8\") as f:\n        f.write(rendered_content)", "idx": 291}
{"namespace": "trafilatura.sitemaps.SitemapObject.extract_sitemap_langlinks", "completion": "        for match in (m[1] for m in islice(XHTML_REGEX.finditer(self.content), MAX_LINKS)):\n            hreflang_match = HREFLANG_REGEX.search(match)\n            if hreflang_match:\n                hreflang = hreflang_match.group(1)\n                if lang_filter(hreflang, self.target_lang):\n                    self.handle_link(hreflang)\n        LOGGER.debug('%s sitemaps and %s links with hreflang found for %s', len(self.sitemap_urls), len(self.urls), self.sitemap_url)", "idx": 292}
{"namespace": "imapclient.datetime_util.format_criteria_date", "completion": "    return dt.strftime(\"%d-%b-%Y\").encode(\"ascii\")", "idx": 293}
{"namespace": "imapclient.util.assert_imap_protocol", "completion": "    if not condition:\n        raise ProtocolError(message or b\"Server replied with a response that violates the IMAP protocol\")", "idx": 294}
{"namespace": "ehforwarderbot.utils.get_config_path", "completion": "    if not module_id:\n        module_id = coordinator.profile\n    config_path = get_data_path(module_id) / f'config.{ext}'\n    if not config_path.exists():\n        config_path.touch()\n    return config_path", "idx": 295}
{"namespace": "ehforwarderbot.utils.get_custom_modules_path", "completion": "    base_path = get_base_path()\n    custom_modules_path = base_path / 'modules'\n    if not custom_modules_path.exists():\n        custom_modules_path.mkdir(parents=True)\n    return custom_modules_path", "idx": 296}
{"namespace": "ehforwarderbot.chat.Chat.add_member", "completion": "        member = ChatMember(self, name=name, alias=alias, id=id, uid=uid,\n                            vendor_specific=vendor_specific, description=description,\n                            middleware=middleware)\n        self.members.append(member)\n        return member", "idx": 297}
{"namespace": "ehforwarderbot.chat.Chat.add_system_member", "completion": "        name = name or translator.gettext(\"System\")\n        uid = uid or id or SystemChatMember.SYSTEM_ID\n        member = SystemChatMember(self, name=name, alias=alias, id=id, uid=uid,\n                                  vendor_specific=vendor_specific, description=description,\n                                  middleware=middleware)\n        self.members.append(member)\n        return member", "idx": 298}
{"namespace": "ehforwarderbot.chat.PrivateChat.verify", "completion": "        super().verify()\n        assert all(isinstance(member, ChatMember) for member in self.members)", "idx": 299}
{"namespace": "telethon.utils.get_inner_text", "completion": "    result = []\n    for entity in entities:\n        start = entity.offset\n        end = entity.offset + entity.length\n        inner_text = text[start:end]\n        result.append(inner_text)\n    return result", "idx": 300}
{"namespace": "telethon.extensions.html.parse", "completion": "    parser = HTMLToTelegramParser()\n    parser.feed(html)\n    parser.close()\n    return parser.text, parser.entities", "idx": 301}
{"namespace": "telethon.extensions.html.unparse", "completion": "    if not text or not entities:\n        return escape(text)\n\n    result = ''\n    last_offset = 0\n    for entity in entities:\n        result += escape(text[last_offset:entity.offset])\n        formatter = ENTITY_TO_FORMATTER.get(type(entity))\n        if formatter:\n            if callable(formatter):\n                result += formatter(entity, text[entity.offset:entity.offset + entity.length])\n            else:\n                result += formatter[0]\n                result += escape(text[entity.offset:entity.offset + entity.length])\n                result += formatter[1]\n        last_offset = entity.offset + entity.length\n\n    result += escape(text[last_offset:])\n    return result", "idx": 302}
{"namespace": "telethon.crypto.rsa.encrypt", "completion": "    global _server_keys\n    if fingerprint in _server_keys:\n        key, old = _server_keys[fingerprint]\n        if use_old and old:\n            key, old = _server_keys[fingerprint]\n        else:\n            key, old = _server_keys[fingerprint]\n        data_hash = sha1(data).digest()\n        padded_data = b'\\x00' + data_hash + data + os.urandom(255 - len(data_hash) - len(data))\n        return rsa.encrypt(padded_data, key)\n    else:\n        return None", "idx": 303}
{"namespace": "hbmqtt.codecs.encode_string", "completion": "    encoded_string = string.encode('utf-8')\n    return encode_data_with_length(encoded_string)", "idx": 304}
{"namespace": "hbmqtt.plugins.manager.PluginManager.get_plugin", "completion": "        for plugin in self._plugins:\n            if plugin.name == name:\n                return plugin\n        return None", "idx": 305}
{"namespace": "pysimplesoap.simplexml.SimpleXMLElement.add_child", "completion": "        name = self._update_ns(name)  # Change node name to that used by a client\n\n        if ns is True:\n            namespace = self.__ns\n        elif ns is False or self.__ns is None:\n            namespace = None\n        else:\n            namespace = ns\n\n        new_element = self.__document.createElementNS(namespace, name)\n\n        if text is not None:\n            if isinstance(text, xml.dom.minidom.CDATASection):\n                new_element.appendChild(text)\n            else:\n                new_element.appendChild(self.__document.createTextNode(text))\n\n        self._element.appendChild(new_element)\n\n        return SimpleXMLElement(\n            elements=[new_element],\n            document=self.__document,\n            namespace=self.__ns,\n            prefix=self.__prefix,\n            jetty=self.__jetty,\n            namespaces_map=self.__namespaces_map\n        )", "idx": 306}
{"namespace": "pysimplesoap.simplexml.SimpleXMLElement.as_xml", "completion": "        if pretty:\n            xml_str = self._element.toprettyxml()\n        else:\n            xml_str = self._element.toxml()\n\n        if filename:\n            with open(filename, 'w') as file:\n                file.write(xml_str)\n\n        return xml_str", "idx": 307}
{"namespace": "twilio.base.deserialize.iso8601_date", "completion": "    try:\n        return datetime.datetime.strptime(s, ISO8601_DATE_FORMAT).date()\n    except (TypeError, ValueError):\n        return s", "idx": 308}
{"namespace": "twilio.base.deserialize.iso8601_datetime", "completion": "    try:\n        return datetime.datetime.strptime(s, ISO8601_DATETIME_FORMAT).replace(tzinfo=datetime.timezone.utc)\n    except (TypeError, ValueError):\n        return s", "idx": 309}
{"namespace": "twilio.base.serialize.iso8601_date", "completion": "    if d == values.unset:\n        return d\n    elif isinstance(d, datetime.datetime) or isinstance(d, datetime.date):\n        return d.strftime(\"%Y-%m-%d\")\n    elif isinstance(d, str):\n        return d", "idx": 310}
{"namespace": "twilio.base.serialize.iso8601_datetime", "completion": "    if d == values.unset:\n        return d\n    elif isinstance(d, datetime.datetime):\n        return d.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n    elif isinstance(d, datetime.date):\n        return d.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n    elif isinstance(d, str):\n        return d", "idx": 311}
{"namespace": "twilio.base.serialize.prefixed_collapsible_map", "completion": "    if not isinstance(m, dict):\n        return m\n    return {prefix + k: v for k, v in m.items()}", "idx": 312}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.dial", "completion": "        return self.nest(\n            Dial(\n                number=number,\n                action=action,\n                method=method,\n                timeout=timeout,\n                hangup_on_star=hangup_on_star,\n                time_limit=time_limit,\n                caller_id=caller_id,\n                record=record,\n                trim=trim,\n                recording_status_callback=recording_status_callback,\n                recording_status_callback_method=recording_status_callback_method,\n                recording_status_callback_event=recording_status_callback_event,\n                answer_on_bridge=answer_on_bridge,\n                ring_tone=ring_tone,\n                recording_track=recording_track,\n                sequential=sequential,\n                refer_url=refer_url,\n                refer_method=refer_method,\n                **kwargs\n            )\n        )", "idx": 313}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.enqueue", "completion": "        return self.nest(\n            Enqueue(\n                name=name,\n                action=action,\n                max_queue_size=max_queue_size,\n                method=method,\n                wait_url=wait_url,\n                wait_url_method=wait_url_method,\n                workflow_sid=workflow_sid,\n                **kwargs\n            )\n        )", "idx": 314}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.gather", "completion": "        return self.nest(\n            Gather(\n                input=input,\n                action=action,\n                method=method,\n                timeout=timeout,\n                speech_timeout=speech_timeout,\n                max_speech_time=max_speech_time,\n                profanity_filter=profanity_filter,\n                finish_on_key=finish_on_key,\n                num_digits=num_digits,\n                partial_result_callback=partial_result_callback,\n                partial_result_callback_method=partial_result_callback_method,\n                language=language,\n                hints=hints,\n                barge_in=barge_in,\n                debug=debug,\n                action_on_empty_result=action_on_empty_result,\n                speech_model=speech_model,\n                enhanced=enhanced,\n                **kwargs\n            )\n        )", "idx": 315}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.say", "completion": "        return self.nest(\n            Say(message=message, voice=voice, loop=loop, language=language, **kwargs)\n        )", "idx": 316}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.sms", "completion": "        return self.nest(\n            Sms(\n                message=message,\n                to=to,\n                from_=from_,\n                action=action,\n                method=method,\n                status_callback=status_callback,\n                **kwargs\n            )\n        )", "idx": 317}
{"namespace": "twilio.twiml.voice_response.Gather.say", "completion": "        return self.nest(\n            Say(message=message, voice=voice, loop=loop, language=language, **kwargs)\n        )", "idx": 318}
{"namespace": "twilio.twiml.voice_response.Dial.client", "completion": "        return self.nest(\n            Client(\n                identity=identity,\n                url=url,\n                method=method,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                **kwargs\n            )\n        )", "idx": 319}
{"namespace": "twilio.twiml.voice_response.Dial.conference", "completion": "        return self.nest(\n            Conference(\n                name,\n                muted=muted,\n                beep=beep,\n                start_conference_on_enter=start_conference_on_enter,\n                end_conference_on_exit=end_conference_on_exit,\n                wait_url=wait_url,\n                wait_method=wait_method,\n                max_participants=max_participants,\n                record=record,\n                region=region,\n                coach=coach,\n                trim=trim,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                recording_status_callback=recording_status_callback,\n                recording_status_callback_method=recording_status_callback_method,\n                recording_status_callback_event=recording_status_callback_event,\n                event_callback_url=event_callback_url,\n                jitter_buffer_size=jitter_buffer_size,\n                participant_label=participant_label,\n                **kwargs\n            )\n        )", "idx": 320}
{"namespace": "twilio.twiml.voice_response.Dial.queue", "completion": "        return self.nest(\n            Queue(\n                name,\n                url=url,\n                method=method,\n                reservation_sid=reservation_sid,\n                post_work_activity_sid=post_work_activity_sid,\n                **kwargs\n            )\n        )", "idx": 321}
{"namespace": "twilio.twiml.voice_response.Dial.sip", "completion": "        return self.nest(\n            Sip(\n                sip_url,\n                username=username,\n                password=password,\n                url=url,\n                method=method,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                machine_detection=machine_detection,\n                amd_status_callback_method=amd_status_callback_method,\n                amd_status_callback=amd_status_callback,\n                machine_detection_timeout=machine_detection_timeout,\n                machine_detection_speech_threshold=machine_detection_speech_threshold,\n                machine_detection_speech_end_threshold=machine_detection_speech_end_threshold,\n                machine_detection_silence_timeout=machine_detection_silence_timeout,\n                **kwargs\n            )\n        )", "idx": 322}
{"namespace": "twilio.twiml.messaging_response.MessagingResponse.message", "completion": "        message = Message(body, **kwargs)\n        if to:\n            message = message.nest(To(to))\n        if from_:\n            message = message.nest(From(from_))\n        if action:\n            message = message.nest(Action(action, method=method))\n        if status_callback:\n            message = message.nest(StatusCallback(status_callback))\n        return self.nest(message)", "idx": 323}
{"namespace": "twilio.twiml.TwiML.append", "completion": "        if not isinstance(verb, TwiML) and not isinstance(verb, str):\n            raise TwiMLException(\"Only nesting of TwiML and strings are allowed\")\n\n        self.verbs.append(verb)\n        return self", "idx": 324}
{"namespace": "twilio.jwt.Jwt.to_jwt", "completion": "        ttl = ttl or self.ttl\n        payload = self.payload.copy()\n        headers = self.headers.copy()\n\n        payload[\"exp\"] = int(time.time()) + ttl\n\n        return jwt_lib.encode(payload, self.secret_key, algorithm=self.algorithm, headers=headers)", "idx": 325}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_client_outgoing", "completion": "        scope = ScopeURI(\"client\", \"outgoing\", {\"appSid\": application_sid})\n        if kwargs:\n            scope.add_param(\"params\", urlencode(kwargs, doseq=True))\n\n        self.capabilities[\"outgoing\"] = scope", "idx": 326}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_client_incoming", "completion": "        self.client_name = client_name\n        scope = ScopeURI(\"client\", \"incoming\", {\"clientName\": client_name})\n        self.capabilities[\"incoming\"] = scope", "idx": 327}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_event_stream", "completion": "        scope = ScopeURI(\"stream\", \"subscribe\", kwargs)\n        self.capabilities[\"event_stream\"] = scope", "idx": 328}
{"namespace": "twilio.jwt.client.ClientCapabilityToken._generate_payload", "completion": "        payload_values = []\n        if \"outgoing\" in self.capabilities and self.client_name is not None:\n            self.capabilities[\"outgoing\"].add_param(\"clientName\", self.client_name)\n\n        for capability in self.capabilities.values():\n            payload_values.append(capability.to_payload())\n\n        return {\"scope\": \" \".join(payload_values)}", "idx": 329}
{"namespace": "twilio.jwt.client.ScopeURI.to_payload", "completion": "        if self.params:\n            sorted_params = sorted(self.params.items(), key=lambda x: x[0])\n            encoded_params = urlencode(sorted_params, doseq=True)\n            param_string = \"?\" + encoded_params if encoded_params else \"\"\n        else:\n            param_string = \"\"\n\n        return \"scope:{}:{}{}\".format(self.service, self.privilege, param_string)", "idx": 330}
{"namespace": "twilio.jwt.access_token.AccessToken.add_grant", "completion": "        if not isinstance(grant, AccessTokenGrant):\n            raise ValueError(\"Grant must be an instance of AccessTokenGrant.\")\n        self.grants.append(grant)", "idx": 331}
{"namespace": "twilio.jwt.taskrouter.capabilities.WorkerCapabilityToken.allow_update_activities", "completion": "        self._make_policy(self.workspace_url + \"/Activities\", \"POST\", True, {\"ActivitySid\": {\"required\": True}})", "idx": 332}
{"namespace": "zulipterminal.platform_code.successful_GUI_return_code", "completion": "    if PLATFORM == \"WSL\":\n        return 1\n    else:\n        return 0", "idx": 333}
{"namespace": "zulipterminal.platform_code.normalized_file_path", "completion": "    if PLATFORM == \"WSL\":\n        return path.replace(\"/\", \"\\\\\")\n    else:\n        return path", "idx": 334}
{"namespace": "zulipterminal.helper.canonicalize_color", "completion": "    if len(color) == 7:\n        # Convert to lower case and return\n        return color.lower()\n    elif len(color) == 4:\n        # Convert to lower case and return\n        return \"#\" + color[1].lower() + color[2].lower() + color[3].lower()\n    else:\n        # Invalid color format, return as is\n        return color", "idx": 335}
{"namespace": "zulipterminal.helper.get_unused_fence", "completion": "        import re\n    import re\n\n    # Find all continuous back-ticks in the content\n    matches = re.findall(r'`+', content)\n\n    # Calculate the maximum length of the fence\n    max_length = max(len(match) for match in matches) + 1\n\n    # Generate the fence using back-ticks\n    fence = \"`\" * max_length\n\n    return fence", "idx": 336}
{"namespace": "zulipterminal.helper.open_media", "completion": "    try:\n        subprocess.run([tool, media_path], check=True)\n    except subprocess.CalledProcessError as e:\n        controller.report_error(f\"Error opening media: {e}\")", "idx": 337}
{"namespace": "zulipterminal.server_url.encode_stream", "completion": "    encoded_stream_name = urllib.parse.quote(stream_name, safe=b\"\").replace(\".\", \"%2E\").replace(\"%\", \".\")\n    return f\"{stream_id}-{encoded_stream_name}\"", "idx": 338}
{"namespace": "zulipterminal.server_url.near_message_url", "completion": "    if message[\"type\"] == \"stream\":\n        return near_stream_message_url(server_url, message)\n    else:\n        return near_pm_message_url(server_url, message)", "idx": 339}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.update_recipients", "completion": "        # Extract the input text from the ReadlineEdit instance\n        input_text = write_box.edit_text\n\n        # Split the input text by commas to separate multiple recipients\n        recipients = [recipient.strip() for recipient in input_text.split(\",\")]\n\n        # Initialize a list to store the recipient user IDs\n        recipient_user_ids = []\n\n        # Iterate through the recipients to extract user IDs\n        for recipient in recipients:\n            # Extract the email from the recipient text\n            email = re.findall(r'<(.*?)>', recipient)\n            if email:\n                # If email is found, extract the user ID from the email\n                user_id = self.model.user_id_from_email(email[0])\n                if user_id:\n                    # If user ID is found, add it to the recipient_user_ids list\n                    recipient_user_ids.append(user_id)\n\n        # Set the recipient_user_ids in the WriteBox instance\n        self.recipient_user_ids = recipient_user_ids", "idx": 340}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.stream_box_view", "completion": "        self.stream_write_box = urwid.Text(caption)\n        self._setup_common_stream_compose(stream_id, caption, title)\n\n        self.edit_mode_button = EditModeButton(\n            controller=self.model.controller,\n            width=20,\n        )\n        self.header_write_box.widget_list.append(self.edit_mode_button)\n\n        # Use callback to set stream marker - it shouldn't change, so don't need signal\n        self._set_stream_write_box_style(None, caption)", "idx": 341}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.stream_box_edit_view", "completion": "\n        self.set_editor_mode()\n\n        self.compose_box_status = \"open_with_stream\"\n        self.stream_id = stream_id\n        self.recipient_user_ids = self.model.get_other_subscribers_in_stream(\n            stream_id=stream_id\n        )\n\n        self.msg_write_box = ReadlineEdit(\n            multiline=True, max_char=self.model.max_message_length\n        )\n        self.msg_write_box.enable_autocomplete(\n            func=self.generic_autocomplete,\n            key=primary_key_for_command(\"AUTOCOMPLETE\"),\n            key_reverse=primary_key_for_command(\"AUTOCOMPLETE_REVERSE\"),\n        )\n        self.msg_write_box.set_completer_delims(DELIMS_MESSAGE_COMPOSE)\n\n        self.title_write_box = ReadlineEdit(\n            edit_text=title, max_char=self.model.max_topic_length\n        )\n        self.title_write_box.enable_autocomplete(\n            func=self._topic_box_autocomplete,\n            key=primary_key_for_command(\"AUTOCOMPLETE\"),\n            key_reverse=primary_key_for_command(\"AUTOCOMPLETE_REVERSE\"),\n        )\n        self.title_write_box.set_completer_delims(\"\")\n\n        # NOTE: stream marker should be set during initialization\n        self.header_write_box = urwid.Columns(\n            [\n                (\"pack\", urwid.Text((\"default\", \"?\"))", "idx": 342}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._set_stream_write_box_style", "completion": "\n        # Check if the input text is a valid stream name\n        if self.model.is_valid_stream(new_text):\n            stream_id = self.model.stream_id_from_name(new_text)\n            stream_color = self.model.stream_dict[stream_id][\"color\"]\n            bar_color = f\"s{stream_color}\"\n\n            # Set the color and stream marker in the header write box\n            self.header_write_box.widget_list[self.FOCUS_HEADER_PREFIX_STREAM].set_text(\n                (bar_color, new_text)\n            )\n            self.header_write_box.widget_list[self.FOCUS_HEADER_PREFIX_STREAM].set_attr_map(\n                {None: bar_color}\n            )", "idx": 343}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._to_box_autocomplete", "completion": "\n        # Get the list of users from the view\n        users_list = self.view.users\n\n        # Split the text by comma and get the most recent recipient for autocomplete\n        recipients = [recipient.strip() for recipient in text.split(\",\") if recipient.strip()]\n        latest_recipient = recipients[-1] if recipients else \"\"\n\n        # Find the users that match the latest text\n        matching_users = [user for user in users_list if user.startswith(latest_recipient)]\n\n        # Append the autocompleted recipients to the string containing the previous recipients\n        autocompleted_recipients = \", \".join(recipients[:-1] + matching_users)\n\n        # Get the full names of the matching users\n        full_names = [self.view.get_full_name(user) for user in matching_users]\n\n        # Process the typeaheads using the updated recipients, state, and user names\n        typeaheads = [f\"{recipient} <{user}>\" for recipient, user in zip(full_names, matching_users)]\n        return self._process_typeaheads(typeaheads, state, typeaheads)", "idx": 344}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._topic_box_autocomplete", "completion": "        topics_list = self.model.topics_in_stream(self.stream_id)\n        matched_topics = match_topics(topics_list, text)\n\n        return self._process_typeaheads(matched_topics, state, matched_topics)", "idx": 345}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._stream_box_autocomplete", "completion": "        streams_list = self.view.pinned_streams + self.view.unpinned_streams\n        streams = [stream[\"name\"] for stream in streams_list]\n        stream_typeahead = format_string(streams, \"#**{}**\")\n        stream_data = list(zip(stream_typeahead, streams))\n\n        prefix_length = len(\"#**\")\n        matched_data = match_stream(\n            stream_data, text[prefix_length:], self.view.pinned_streams\n        )\n        return matched_data", "idx": 346}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.generic_autocomplete", "completion": "        # Check for specific prefixes in the text\n        if text.startswith(\"@\"):\n            # Call autocomplete_mentions function for user mentions\n            typeahead, suggestions = self.autocomplete_mentions(text, \"@\")\n        elif text.startswith(\"#\"):\n            # Call autocomplete_streams function for stream mentions\n            typeahead, suggestions = self.autocomplete_streams(text, \"#\")\n        elif text.startswith(\":\"):\n            # Call autocomplete_emojis function for emojis\n            typeahead, suggestions = self.autocomplete_emojis(text, \":\")\n        else:\n            # No specific prefix found, return None\n            return None\n\n        # Process the autocomplete suggestions\n        return self._process_typeaheads(typeahead, state, suggestions)", "idx": 347}
{"namespace": "zulipterminal.ui_tools.boxes.PanelSearchBox.reset_search_text", "completion": "        self.set_caption([(\"filter_results\", \" Search Results \"), \" \"])\n        self.set_edit_text(\"\")", "idx": 348}
{"namespace": "zulipterminal.ui_tools.boxes.PanelSearchBox.valid_char", "completion": "        if self.get_edit_text() == \"\":\n            # Check if the character is a valid unicode character and not a control character or space separator\n            return ch.isprintable() and ch != \" \"\n        else:\n            # Use the regular validation method\n            return super().valid_char(ch)", "idx": 349}
{"namespace": "zulipterminal.ui_tools.utils.is_muted", "completion": "    if msg[\"type\"] == \"private\":\n        return False\n    # Check if the message is in a topic narrow\n    if model.narrow:\n        return False\n    # Check if the message's stream or topic is muted in the model\n    if msg[\"type\"] == \"stream\":\n        if msg[\"stream_id\"] in model.muted_streams:\n            return True\n    elif msg[\"type\"] == \"private\":\n        if msg[\"display_recipient\"] in model.muted_topics:\n            return True\n    return False", "idx": 350}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.update_count", "completion": "        self.count = count\n        if text_color is not None:\n            self.original_color = text_color\n        count_text = (None, str(count)) if count > 0 else (None, \"\")\n        self.update_widget(count_text, self.original_color)", "idx": 351}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.update_widget", "completion": "        prefix, count = count_text\n        if prefix:\n            self.button_prefix.set_text(prefix)\n        self._label.set_text(count)\n        if text_color:\n            self._w.set_attr_map({None: text_color})", "idx": 352}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.keypress", "completion": "        if is_command_key(\"ENTER\", key):\n            self.activate(key)\n            return None\n        else:\n            return super().keypress(size, key)", "idx": 353}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._parse_narrow_link", "completion": "        # Supported link formats\n        from zulipterminal.config.regexes import REGEX_INTERNAL_LINK_STREAM\n        from zulipterminal.config.regexes import REGEX_INTERNAL_LINK_STREAM_NEAR\n        from zulipterminal.config.regexes import REGEX_INTERNAL_LINK_STREAM_TOPIC\n        from zulipterminal.config.regexes import REGEX_INTERNAL_LINK_STREAM_TOPIC_NEAR\n\n        parsed_link: ParsedNarrowLink = {}\n\n        # narrow/stream/[{stream_id}-]{stream-name}\n        match = re.match(REGEX_INTERNAL_LINK_STREAM, link)\n        if match:\n            stream_data = match.groupdict()\n            parsed_link[\"narrow\"] = \"stream\"\n            parsed_link[\"stream\"] = cls._decode_stream_data(stream_data[\"stream\"])\n            return parsed_link\n\n        # narrow/stream/[{stream_id}-]{stream-name}/near/{message_id}\n        match = re.match(REGEX_INTERNAL_LINK_STREAM_NEAR, link)\n        if match:\n            stream_data = match.groupdict()\n            parsed_link[\"narrow\"] = \"stream:near\"\n            parsed_link[\"stream\"] = cls._decode_stream_data(stream_data[\"stream\"])\n            parsed_link[\"message_id\"] = cls._decode_message_id(stream_data[\"message_id\"])\n            return parsed_link\n\n        # narrow/stream/[{stream_id}-]{stream-name}/topic/{encoded.20topic.20name}\n        match = re.match(REGEX_INTERNAL_LINK_STREAM_TOPIC, link)\n        if match:\n            stream_data = match.groupdict()\n            parsed_link[\"narrow\"] = \"stream:topic\"\n            parsed_link[\"stream\"] = cls._decode_stream_data(stream_data[\"stream\"])\n            parsed_link[\"topic_name\"] = hash_util_decode(stream_data[\"topic\"])\n            return parsed_link\n\n        # narrow/stream/[{stream_id}-]{stream-name}/topic/{encoded.20topic.20name}/near/{message_id}\n        match = re.match(REGEX_INTERNAL_LINK_STREAM_TOPIC_NEAR, link)\n        if match:\n            stream_data = match.groupdict()\n            parsed_link[\"narrow\"] = \"stream:topic:near\"\n            parsed_link[\"stream\"] = cls._decode_stream_data(stream_data[\"stream\"])\n            parsed_link[\"topic_name\"] = hash_util_decode(stream_data[\"topic\"])\n            parsed_link[\"message_id\"] = cls._decode_message_id(stream_data[\"message_id\"])\n            return parsed_link\n\n        return parsed_link  # Empty dictionary for unsupported links", "idx": 354}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._validate_and_patch_stream_data", "completion": "        if \"stream\" in parsed_link:\n            stream_data = parsed_link[\"stream\"]\n            stream_id = stream_data.get(\"stream_id\")\n            stream_name = stream_data.get(\"stream_name\")\n\n            if stream_id is not None and stream_name is not None:\n                # Check if the stream ID and name are valid and subscribed to by the user\n                if stream_id not in self.model.stream_dict or stream_name not in self.model.stream_dict[stream_id][\"name\"]:\n                    return \"Invalid stream ID or name\"\n\n                # Update the stream name in the parsed link if necessary\n                if stream_name != self.model.stream_dict[stream_id][\"name\"]:\n                    parsed_link[\"stream\"][\"stream_name\"] = self.model.stream_dict[stream_id][\"name\"]\n\n            else:\n                return \"Invalid stream data\"\n\n        return \"\"", "idx": 355}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._validate_narrow_link", "completion": "        # Validate stream data and patch if necessary\n        error = self._validate_and_patch_stream_data(parsed_link)\n        if error:\n            return error\n\n        narrow = parsed_link[\"narrow\"]\n        stream_id = parsed_link[\"stream\"][\"stream_id\"]\n        stream_name = parsed_link[\"stream\"][\"stream_name\"]\n        topic_name = parsed_link.get(\"topic_name\")\n        message_id = parsed_link.get(\"message_id\")\n\n        if narrow not in [\"stream\", \"stream:near\", \"stream:topic\", \"stream:topic:near\"]:\n            return \"Invalid narrow type\"\n\n        if narrow in [\"stream\", \"stream:topic\"] and not stream_name:\n            return \"Stream name is missing\"\n\n        if narrow in [\"stream:near\", \"stream:topic:near\"] and not message_id:\n            return \"Message ID is missing\"\n\n        if narrow in [\"stream:topic\", \"stream:topic:near\"] and not topic_name:\n            return \"Topic name is missing\"\n\n        if narrow in [\"stream\", \"stream:topic\"] and not stream_id:\n            return \"Stream ID is missing\"\n\n        return \"\"", "idx": 356}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._switch_narrow_to", "completion": "        narrow_type = parsed_link.get(\"narrow\")\n        if narrow_type == \"stream\":\n            stream_data = parsed_link.get(\"stream\")\n            if stream_data:\n                stream_name = stream_data.get(\"stream_name\")\n                if stream_name:\n                    self.controller.narrow_to_stream(stream_name=stream_name)\n        elif narrow_type == \"stream:topic\":\n            stream_data = parsed_link.get(\"stream\")\n            topic_name = parsed_link.get(\"topic_name\")\n            if stream_data and topic_name:\n                stream_name = stream_data.get(\"stream_name\")\n                self.controller.narrow_to_topic(stream_name=stream_name, topic_name=topic_name)\n        elif narrow_type == \"stream:near\":\n            stream_data = parsed_link.get(\"stream\")\n            message_id = parsed_link.get(\"message_id\")\n            if stream_data and message_id:\n                stream_name = stream_data.get(\"stream_name\")\n                self.controller.narrow_to_stream(stream_name=stream_name, message_id=message_id)\n        elif narrow_type == \"stream:topic:near\":\n            stream_data = parsed_link.get(\"stream\")\n            topic_name = parsed_link.get(\"topic_name\")\n            message_id = parsed_link.get(\"message_id\")\n            if stream_data and topic_name and message_id:\n                stream_name = stream_data.get(\"stream_name\")\n                self.controller.narrow_to_topic(stream_name=stream_name, topic_name=topic_name, message_id=message_id)", "idx": 357}
{"namespace": "zulipterminal.config.themes.complete_and_incomplete_themes", "completion": "    complete_themes = []\n    incomplete_themes = []\n\n    for theme_name, theme_data in THEMES.items():\n        try:\n            theme_styles = theme_data.STYLES\n            theme_meta = theme_data.META\n            if all(style in theme_styles for style in REQUIRED_STYLES) and all(\n                meta in theme_meta for meta in REQUIRED_META\n            ):\n                complete_themes.append(theme_name)\n            else:\n                incomplete_themes.append(theme_name)\n        except AttributeError:\n            incomplete_themes.append(theme_name)\n\n    complete_themes.sort()\n    incomplete_themes.sort()\n\n    return complete_themes, incomplete_themes", "idx": 358}
{"namespace": "zulipterminal.config.themes.validate_colors", "completion": "    theme_styles = THEMES[theme_name].STYLES\n    for style_name, (fg, bg) in theme_styles.items():\n        fg_code16, _, _, _ = fg.value.split()\n        bg_code16, _, _, _ = bg.value.split()\n        if fg_code16 not in valid_16_color_codes:\n            raise InvalidThemeColorCode(f\"Invalid color code '{fg_code16}' in theme '{theme_name}' for style '{style_name}'\")\n        if bg_code16 not in valid_16_color_codes:\n            raise InvalidThemeColorCode(f\"Invalid color code '{bg_code16}' in theme '{theme_name}' for style '{style_name}'\")", "idx": 359}
{"namespace": "zulipterminal.config.themes.parse_themefile", "completion": "    urwid_theme: ThemeSpec = []\n    for style_name, (foreground, background) in theme_styles.items():\n        if color_depth == 1:\n            foreground = \"standout\"\n            background = \"\"\n        elif color_depth == 16:\n            # Convert foreground and background colors to 16-color codes\n            foreground = convert_to_16_color_code(foreground)\n            background = convert_to_16_color_code(background)\n        elif color_depth == 256:\n            # Convert foreground and background colors to 256-color codes\n            foreground = convert_to_256_color_code(foreground)\n            background = convert_to_256_color_code(background)\n        elif color_depth == 2**24:\n            # Convert foreground and background colors to 2^24-color codes\n            foreground = convert_to_2_24_color_code(foreground)\n            background = convert_to_2_24_color_code(background)\n        else:\n            raise ValueError(\"Invalid color depth specified\")\n\n        urwid_theme.append((style_name, foreground, background))\n\n    return urwid_theme", "idx": 360}
{"namespace": "zulipterminal.config.themes.add_pygments_style", "completion": "    pygments_styles = theme_meta.get('pygments', {}).get('styles')\n    if pygments_styles:\n        for style_name, style_spec in pygments_styles.items():\n            if style_name in STANDARD_TYPES:\n                urwid_theme.append((style_name, style_spec['color'], style_spec['bgcolor']))", "idx": 361}
{"namespace": "zulipterminal.config.keys.is_command_key", "completion": "    try:\n        return key in KEY_BINDINGS[command][\"keys\"]\n    except KeyError:\n        return False", "idx": 362}
{"namespace": "zulipterminal.config.keys.keys_for_command", "completion": "    try:\n        return KEY_BINDINGS[command][\"keys\"]\n    except KeyError:\n        raise InvalidCommand(command)", "idx": 363}
{"namespace": "zulipterminal.config.keys.commands_for_random_tips", "completion": "    return [binding for binding in KEY_BINDINGS.values() if not binding.get(\"excluded_from_random_tips\", False)]", "idx": 364}
{"namespace": "hypertools.datageometry.DataGeometry.transform", "completion": "        if data is None:\n            return self.xform_data\n        else:\n            # perform transformation on new data\n            # ...\n            return transformed_data", "idx": 365}
{"namespace": "hypertools.datageometry.DataGeometry.plot", "completion": "        if data is None:\n            data = self.xform_data\n        else:\n            data = format_data(\n                data,\n                semantic=self.semantic,\n                vectorizer=self.vectorizer,\n                corpus=self.corpus,\n                ppca=True\n            )\n        return hyp.plot(data, **kwargs)", "idx": 366}
{"namespace": "awesome_autodl.autodl_topic2papers", "completion": "        import yaml\n        from awesome_autodl.data_cls import AutoDLpaper\n    from awesome_autodl.data_cls import AutoDLpaper\n    import yaml\n\n    topic2path = autodl_topic2path()\n    topic2papers = OrderedDict()\n\n    for topic, path in topic2path.items():\n        with open(path, \"r\") as file:\n            data = yaml.safe_load(file)\n            papers = [AutoDLpaper(**paper) for paper in data]\n            topic2papers[topic] = papers\n\n    return topic2papers", "idx": 367}
{"namespace": "awesome_autodl.get_bib_abbrv_obj", "completion": "        from awesome_autodl.bib_abbrv import BibAbbreviations\n    from awesome_autodl.bib_abbrv import BibAbbreviations\n    bib_abbrv_file = get_bib_abbrv_file()\n    return BibAbbreviations(bib_abbrv_file)", "idx": 368}
{"namespace": "mssqlcli.localized_strings.translation", "completion": "    global LANGUAGES\n    if languages is None:\n        if LANGUAGES is None:\n            LANGUAGES = [os.environ.get('LANG', 'en_US')]\n        languages = LANGUAGES\n    translation_obj = gettext.translation(domain, localedir, languages=languages, fallback=True)\n    return translation_obj", "idx": 369}
{"namespace": "mssqlcli.mssqlbuffer._is_query_executable", "completion": "    # Remove comments\n    sql = re.sub(r'--.*', '', sql)\n    sql = re.sub(r'/\\*.*\\*/', '', sql, flags=re.DOTALL)\n\n    # Check for open quotes\n    open_quote = False\n    for char in sql:\n        if char in ('\"', \"'\"):\n            open_quote = not open_quote\n\n    # Check if the statement is a complete command\n    if not open_quote and re.search(r'\\bGO\\b', sql, flags=re.IGNORECASE):\n        return True\n    else:\n        return False", "idx": 370}
{"namespace": "mssqlcli.telemetry.conclude", "completion": "    _session.end_time = datetime.now()\n    payload = _session.generate_payload()\n    output_payload_to_file(payload)\n    result = upload_payload(payload, service_endpoint_uri, separate_process)\n    return result", "idx": 371}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.start", "completion": "        self.request_thread = threading.Thread(\n            target=self._listen_for_request, name=self.REQUEST_THREAD_NAME)\n        self.response_thread = threading.Thread(\n            target=self._listen_for_response, name=self.RESPONSE_THREAD_NAME)\n\n        self.request_thread.start()\n        self.response_thread.start()", "idx": 372}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.submit_request", "completion": "        if method is None or params is None:\n            raise ValueError(\"Method and params cannot be None\")\n\n        request = {\n            u'method': method,\n            u'params': params,\n            u'id': request_id\n        }\n        self.request_queue.put(request)", "idx": 373}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.get_response", "completion": "        if request_id in self.response_map:\n            response_queue = self.response_map[request_id]\n            if not response_queue.empty():\n                return response_queue.get()\n\n        if owner_uri in self.response_map:\n            response_queue = self.response_map[owner_uri]\n            if not response_queue.empty():\n                return response_queue.get()\n\n        if not self.exception_queue.empty():\n            return self.exception_queue.get()\n\n        return None", "idx": 374}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.shutdown", "completion": "        self.cancel = True\n        self.request_queue.put(None)\n        self.request_thread.join()\n        self.writer.close()\n        logger.debug('Json Rpc client shutdown.')", "idx": 375}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcWriter.send_request", "completion": "        content = {\n            u'method': method,\n            u'params': params,\n            u'id': request_id\n        }\n        content_str = json.dumps(content, ensure_ascii=False).encode(self.encoding)\n        header = self.HEADER.format(len(content_str))\n        message = header.encode(self.encoding) + content_str\n        try:\n            self.stream.write(message)\n        except ValueError as ex:\n            logger.debug(u'JSON RPC Writer on send_request() encountered exception: %s', ex)\n            raise", "idx": 376}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.read_response", "completion": "\n        content = [None]\n        while True:\n            if self.read_state == ReadState.Header:\n                if not self.try_read_headers():\n                    if not self.read_next_chunk():\n                        raise EOFError(u'End of stream reached, no output.')\n            elif self.read_state == ReadState.Content:\n                if not self.try_read_content(content):\n                    if not self.read_next_chunk():\n                        raise EOFError(u'End of stream reached, no output.')\n                else:\n                    break\n\n        # Trim the buffer after reading the content.\n        self.trim_buffer_and_resize(self.read_offset)\n\n        try:\n            return json.loads(content[0])\n        except ValueError as ex:\n            logger.debug(u'JSON RPC Reader encountered exception while parsing JSON: %s', ex)\n            raise ValueError(u'Failed to parse JSON content.')", "idx": 377}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.read_next_chunk", "completion": "        \"\"\"\n        This function reads a chunk of data from the output stream and stores it in a buffer. It checks if the buffer needs to be resized and resizes it if necessary. It then reads data from the stream into the buffer and updates the buffer offset. If the stream is empty or closed externally, an exception is raised.\n        Input-Output Arguments\n        :param self: JsonRpcReader. An instance of the JsonRpcReader class.\n        :return: bool. True if a chunk was successfully read from the stream, False otherwise.\n        \"\"\"\n        try:\n            # Check if the buffer needs to be resized\n            if self.buffer_end_offset >= len(self.buffer) * self.BUFFER_RESIZE_TRIGGER:\n                new_buffer = bytearray(len(self.buffer) * 2)\n                new_buffer[:self.buffer_end_offset] = self.buffer\n                self.buffer = new_buffer\n\n            # Read data from the stream into the buffer\n            data = self.stream.read(self.DEFAULT_BUFFER_SIZE)\n            if not data:\n                raise ValueError(\"Stream is empty or closed externally\")\n\n            # Update the buffer offset\n            self.buffer[self.buffer_end_offset:self.buffer_end_offset + len(data)] = data\n            self.buffer_end_offset += len(data\n\n            return True\n        except ValueError as ex:\n            logger.debug(u'JSON RPC Reader on read_next_chunk() encountered exception: %s', ex)\n            raise", "idx": 378}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.try_read_headers", "completion": "        # Scan the buffer until the last header containing '\\r\\n\\r\\n'\n        header_end = self.buffer.find(b'\\r\\n\\r\\n', self.read_offset, self.buffer_end_offset)\n        if header_end == -1:\n            # Header not found, need more data\n            return False\n\n        # Split the headers by new line\n        headers = self.buffer[self.read_offset:header_end].decode(self.encoding).split('\\r\\n')\n\n        # Extract the key-value pairs and store them in the headers dictionary\n        for header in headers:\n            key, value = header.split(': ')\n            self.headers[key] = value\n\n        # Check if the 'content-length' header is present\n        if 'Content-Length' in self.headers:\n            self.expected_content_length = int(self.headers['Content-Length'])\n            self.read_offset = header_end + 4  # Move read_offset to the start of the content\n            self.read_state = ReadState.Content\n            return True\n        else:\n            # 'content-length' header not found, need more data\n            return False", "idx": 379}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.close", "completion": "        try:\n            self.stream.close()\n        except AttributeError:\n            pass", "idx": 380}
{"namespace": "mssqlcli.packages.prioritization.PrevalenceCounter.update", "completion": "\n        self.update_keywords(text)\n        self.update_names(text)", "idx": 381}
{"namespace": "mssqlcli.packages.sqlcompletion.suggest_type", "completion": "\n    if text_before_cursor.startswith(\"\\\\i \"):\n        return (Path(),)\n\n    stmt = SqlStatement(full_text, text_before_cursor)\n\n    if stmt.parsed:\n        special_suggestion = suggest_special(text_before_cursor)\n        if special_suggestion:\n            return special_suggestion\n\n        last_token = stmt.parsed.token_prev(len(stmt.parsed.tokens))[1]\n        return suggest_based_on_last_token(last_token, stmt)\n\n    return (Keyword(),)", "idx": 382}
{"namespace": "mssqlcli.packages.parseutils.ctes.extract_ctes", "completion": "    parsed = parse(sql)\n    ctes = []\n    idx, tok = parsed.token_next_by(i=CTE)\n    if tok and tok.value.lower() == 'with':\n        while tok and tok.value.lower() == 'with':\n            idx, tok = parsed.token_next_by(i=CTE, idx=idx + 1)\n            if not tok:\n                break\n            cte = get_cte_from_token(tok, token_start_pos(parsed, idx))\n            if cte:\n                ctes.append(cte)\n    return ctes, str(parsed)", "idx": 383}
{"namespace": "mssqlcli.packages.parseutils.tables.extract_tables", "completion": "    parsed = sqlparse.parse(sql)[0]\n    stream = extract_from_part(parsed)\n    tables = list(extract_table_identifiers(stream))\n    return tuple(tables)", "idx": 384}
{"namespace": "googleapiclient.channel.Channel.body", "completion": "        body_dict = {\n            \"id\": self.id,\n            \"token\": self.token,\n            \"type\": self.type,\n            \"address\": self.address\n        }\n        if self.expiration:\n            body_dict[\"expiration\"] = self.expiration\n        if self.params:\n            body_dict[\"params\"] = self.params\n        if self.resource_id:\n            body_dict[\"resourceId\"] = self.resource_id\n        if self.resource_uri:\n            body_dict[\"resourceUri\"] = self.resource_uri\n\n        return body_dict", "idx": 385}
{"namespace": "sqlitedict.SqliteDict.update", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to update read-only SqliteDict')\n\n        for key, value in items:\n            self[key] = value\n\n        for key, value in kwds.items():\n            self[key] = value\n\n        if self.autocommit:\n            self.commit()", "idx": 386}
{"namespace": "sqlitedict.SqliteDict.clear", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to clear data in read-only SqliteDict')\n\n        CLEAR_TABLE = 'DELETE FROM \"%s\"' % self.tablename\n        self.conn.execute(CLEAR_TABLE)\n        if self.autocommit:\n            self.commit()", "idx": 387}
{"namespace": "sqlitedict.SqliteDict.commit", "completion": "        if blocking:\n            # by default, we await completion of commit() unless\n            # blocking=False.  This ensures any available exceptions for any\n            # previous statement are thrown before returning, and that the\n            # data has actually persisted to disk!\n            self.select_one(_REQUEST_COMMIT)\n        else:\n            # otherwise, we fire and forget as usual.\n            self.execute(_REQUEST_COMMIT)", "idx": 388}
{"namespace": "sqlitedict.SqliteDict.terminate", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to terminate read-only SqliteDict')\n\n        self.close(do_log=False, force=True)  # close the instance\n\n        if self.filename != \":memory:\":\n            try:\n                os.remove(self.filename)  # delete the file from the file system\n            except Exception:\n                pass", "idx": 389}
{"namespace": "boto.utils.retry_url", "completion": "    for i in range(num_retries):\n        try:\n            response = urllib.request.urlopen(url, timeout=timeout)\n            return response.read()\n        except urllib.error.HTTPError as e:\n            if e.code == 404 and not retry_on_404:\n                raise\n        except Exception as e:\n            if i + 1 == num_retries:\n                raise\n            next_sleep = min(random.random() * 2 ** i, boto.config.get('Boto', 'max_retry_delay', 60))\n            time.sleep(next_sleep)", "idx": 390}
{"namespace": "boto.utils.LazyLoadMetadata.values", "completion": "        self._materialize()\n        return super(LazyLoadMetadata, self).values()", "idx": 391}
{"namespace": "boto.utils.get_instance_userdata", "completion": "    try:\n        userdata_url = _build_instance_metadata_url(url, version, 'user-data')\n        userdata = _get_instance_metadata(userdata_url, num_retries=num_retries, timeout=timeout)\n        if sep:\n            userdata = dict(item.split(sep) for item in userdata.split('\\n') if item)\n        return userdata\n    except urllib.error.URLError:\n        boto.log.exception(\"Exception caught when trying to retrieve instance user data\")\n        return None", "idx": 392}
{"namespace": "boto.utils.pythonize_name", "completion": "    if not name:\n        return ''\n    s1 = _first_cap_regex.sub(r'\\1_\\2', name)\n    s2 = _number_cap_regex.sub(r'\\1_\\2', s1)\n    return _end_cap_regex.sub(r'\\1_\\2', s2).lower()", "idx": 393}
{"namespace": "boto.cloudsearchdomain.connect_to_region", "completion": "        from boto.cloudsearchdomain.layer1 import CloudSearchDomainConnection\n    from boto.cloudsearchdomain.layer1 import CloudSearchDomainConnection\n    region = RegionInfo(name=region_name, endpoint='cloudsearch.us-east-1.amazonaws.com')\n    return CloudSearchDomainConnection(region=region, **kw_params)", "idx": 394}
{"namespace": "boto.redshift.connect_to_region", "completion": "        from boto.redshift.layer1 import RedshiftConnection\n    from boto.redshift.layer1 import RedshiftConnection\n    region = RegionInfo(name=region_name, endpoint='redshift.' + region_name + '.amazonaws.com')\n    return RedshiftConnection(region=region, **kw_params)", "idx": 395}
{"namespace": "boto.support.connect_to_region", "completion": "        from boto.support.layer1 import SupportConnection\n    from boto.support.layer1 import SupportConnection\n    return SupportConnection(region=RegionInfo(name=region_name, connection_cls=SupportConnection), **kw_params)", "idx": 396}
{"namespace": "boto.configservice.connect_to_region", "completion": "        from boto.configservice.layer1 import ConfigServiceConnection\n    from boto.configservice.layer1 import ConfigServiceConnection\n    return ConfigServiceConnection(region=region_name, **kw_params)", "idx": 397}
{"namespace": "boto.cloudhsm.connect_to_region", "completion": "        from boto.cloudhsm.layer1 import CloudHSMConnection\n    from boto.cloudhsm.layer1 import CloudHSMConnection\n    return CloudHSMConnection(region=region_name, **kw_params)", "idx": 398}
{"namespace": "boto.logs.connect_to_region", "completion": "        from boto.logs.layer1 import CloudWatchLogsConnection\n    from boto.logs.layer1 import CloudWatchLogsConnection\n    return CloudWatchLogsConnection(region=region_name, **kw_params)", "idx": 399}
{"namespace": "boto.cloudsearch.connect_to_region", "completion": "        from boto.cloudsearch.layer1 import CloudSearchConnection\n    from boto.cloudsearch.layer1 import CloudSearchConnection\n    return CloudSearchConnection(region=region_name, **kw_params)", "idx": 400}
{"namespace": "boto.glacier.job.Job.download_to_fileobj", "completion": "        num_chunks = self._calc_num_chunks(chunk_size)\n        self._download_to_fileob(output_file, num_chunks, chunk_size, verify_hashes, retry_exceptions)", "idx": 401}
{"namespace": "boto.glacier.utils.minimum_part_size", "completion": "    if size_in_bytes <= MAXIMUM_NUMBER_OF_PARTS * default_part_size:\n        return default_part_size\n    else:\n        min_part_size = math.ceil(size_in_bytes / MAXIMUM_NUMBER_OF_PARTS)\n        return min_part_size", "idx": 402}
{"namespace": "boto.glacier.utils.chunk_hashes", "completion": "    if not bytestring:\n        return [hashlib.sha256(b'').digest()]\n    chunks = [bytestring[i:i + chunk_size] for i in range(0, len(bytestring), chunk_size)]\n    return [hashlib.sha256(chunk).digest() for chunk in chunks]", "idx": 403}
{"namespace": "boto.glacier.utils.compute_hashes_from_fileobj", "completion": "\n    linear_hash = hashlib.sha256()\n    tree_hash_chunks = []\n\n    while True:\n        data = fileobj.read(chunk_size)\n        if not data:\n            break\n        linear_hash.update(data)\n        tree_hash_chunks.extend(chunk_hashes(data, chunk_size))\n\n    linear_hash_hex = bytes_to_hex(linear_hash.digest())\n    tree_hash_hex = tree_hash(tree_hash_chunks)\n\n    return linear_hash_hex, bytes_to_hex(tree_hash_hex)", "idx": 404}
{"namespace": "boto.glacier.concurrent.ConcurrentTransferer._calculate_required_part_size", "completion": "        min_part_size = 1 << 20  # 1MB\n        required_parts = int(math.ceil(total_size / float(self._part_size)))\n        final_part_size = max(min_part_size, int(math.ceil(total_size / float(required_parts))))\n        return required_parts, final_part_size", "idx": 405}
{"namespace": "boto.glacier.connect_to_region", "completion": "        from boto.glacier.layer2 import Layer2\n    from boto.glacier.layer2 import Layer2\n    region = RegionInfo(name=region_name, connection_cls=Layer2)\n    return region.connect(**kw_params)", "idx": 406}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.update", "completion": "        updated = self.connection.get_all_network_interfaces([self.id], dry_run=dry_run)\n        if not updated:\n            if validate:\n                raise ValueError('No data returned from EC2 for NetworkInterface %s' % self.id)\n            return\n        updated = updated[0]\n        self._update(updated)\n        return self.status", "idx": 407}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.attach", "completion": "        return self.connection.attach_network_interface(\n            self.id,\n            instance_id,\n            device_index,\n            dry_run=dry_run\n        )", "idx": 408}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.detach", "completion": "        return self.connection.detach_network_interface(\n            self.id,\n            force=force,\n            dry_run=dry_run\n        )", "idx": 409}
{"namespace": "boto.ec2.address.Address.release", "completion": "        if self.allocation_id:\n            return self.connection.release_address(\n                allocation_id=self.allocation_id,\n                dry_run=dry_run\n            )\n        else:\n            return self.connection.release_address(\n                public_ip=self.public_ip,\n                dry_run=dry_run\n            )", "idx": 410}
{"namespace": "boto.ec2.address.Address.associate", "completion": "        if self.allocation_id:\n            return self.connection.associate_address(\n                allocation_id=self.allocation_id,\n                instance_id=instance_id,\n                network_interface_id=network_interface_id,\n                private_ip_address=private_ip_address,\n                allow_reassociation=allow_reassociation,\n                dry_run=dry_run\n            )\n        else:\n            return self.connection.associate_address(\n                public_ip=self.public_ip,\n                instance_id=instance_id,\n                network_interface_id=network_interface_id,\n                private_ip_address=private_ip_address,\n                allow_reassociation=allow_reassociation,\n                dry_run=dry_run\n            )", "idx": 411}
{"namespace": "boto.ec2.address.Address.disassociate", "completion": "        if self.allocation_id:\n            return self.connection.disassociate_address(\n                association_id=self.association_id,\n                allocation_id=self.allocation_id,\n                dry_run=dry_run\n            )\n        else:\n            return self.connection.disassociate_address(\n                public_ip=self.public_ip,\n                dry_run=dry_run\n            )", "idx": 412}
{"namespace": "boto.ec2.ec2object.TaggedEC2Object.add_tags", "completion": "        status = self.connection.create_tags(\n            [self.id],\n            tags,\n            dry_run=dry_run\n        )\n        self.tags.update(tags)", "idx": 413}
{"namespace": "boto.ec2.ec2object.TaggedEC2Object.remove_tags", "completion": "        status = self.connection.delete_tags(\n            [self.id],\n            list(tags.keys()),\n            dry_run=dry_run\n        )\n        for key in tags.keys():\n            if key in self.tags:\n                del self.tags[key]", "idx": 414}
{"namespace": "boto.ec2.connection.EC2Connection.get_all_instance_status", "completion": "        params = {}\n        if instance_ids:\n            self.build_list_params(params, instance_ids, 'InstanceId')\n        if max_results:\n            params['MaxResults'] = max_results\n        if next_token:\n            params['NextToken'] = next_token\n        if filters:\n            self.build_filter_params(params, filters)\n        if dry_run:\n            params['DryRun'] = 'true'\n        if include_all_instances:\n            params['IncludeAllInstances'] = 'true'\n        return self.get_list('DescribeInstanceStatus', params,\n                             [('item', InstanceStatus)], verb='POST')", "idx": 415}
{"namespace": "boto.ec2.volume.Volume.update", "completion": "        updated = self.connection.get_all_volumes([self.id], dry_run=dry_run)\n        if len(updated) > 0:\n            updated = updated[0]\n            self._update(updated)\n            return self.status\n        elif validate:\n            raise ValueError('Volume %s does not exist' % self.id)", "idx": 416}
{"namespace": "boto.ec2.volume.Volume.attach", "completion": "        return self.connection.attach_volume(\n            self.id,\n            instance_id,\n            device,\n            dry_run=dry_run\n        )", "idx": 417}
{"namespace": "boto.ec2.volume.Volume.detach", "completion": "        return self.connection.detach_volume(\n            self.id,\n            instance_id,\n            device,\n            force=force,\n            dry_run=dry_run\n        )", "idx": 418}
{"namespace": "boto.ec2.volume.Volume.create_snapshot", "completion": "        return self.connection.create_snapshot(self.id, description, dry_run=dry_run)", "idx": 419}
{"namespace": "boto.ec2.volume.Volume.attachment_state", "completion": "        if self.attach_data:\n            return self.attach_data.status\n        else:\n            return None", "idx": 420}
{"namespace": "boto.ec2.securitygroup.SecurityGroup.add_rule", "completion": "        new_rule = IPPermissions()\n        new_rule.ip_protocol = ip_protocol\n        new_rule.from_port = from_port\n        new_rule.to_port = to_port\n        new_rule.add_grant(name=src_group_name, owner_id=src_group_owner_id,\n                           cidr_ip=cidr_ip, group_id=src_group_group_id,\n                           dry_run=dry_run)\n        self.rules.append(new_rule)", "idx": 421}
{"namespace": "boto.ec2.connect_to_region", "completion": "    region = get_region(region_name, **kw_params)\n    if region:\n        return EC2Connection(region=region, **kw_params)\n    else:\n        return None", "idx": 422}
{"namespace": "boto.ec2.cloudwatch.connect_to_region", "completion": "    if region_name in RegionData:\n        region = RegionInfo(name=region_name, endpoint=RegionData[region_name])\n        return CloudWatchConnection(region=region, **kw_params)\n    else:\n        return None", "idx": 423}
{"namespace": "boto.ec2.autoscale.connect_to_region", "completion": "    region_info = RegionData.get(region_name)\n    if region_info:\n        return AutoScaleConnection(region=RegionInfo(\n            name=region_name,\n            endpoint=region_info.endpoint,\n            connection_cls=AutoScaleConnection\n        ), **kw_params)\n    else:\n        return None", "idx": 424}
{"namespace": "boto.ec2.elb.connect_to_region", "completion": "    region = RegionInfo(name=region_name, endpoint=RegionData.get(region_name, {}).get('endpoint'))\n    if region.endpoint:\n        return ELBConnection(region=region, **kw_params)\n    else:\n        return None", "idx": 425}
{"namespace": "boto.ec2.elb.ELBConnection.get_all_load_balancers", "completion": "        params = {}\n        if load_balancer_names:\n            self.build_list_params(params, load_balancer_names, 'LoadBalancerNames.member.%d')\n        if marker:\n            params['Marker'] = marker\n        return self.get_list('DescribeLoadBalancers', params, [('member', LoadBalancer)])", "idx": 426}
{"namespace": "boto.ec2.elb.ELBConnection.disable_availability_zones", "completion": "        params = {'LoadBalancerName': load_balancer_name}\n        self.build_list_params(params, zones_to_remove,\n                               'AvailabilityZones.member.%d')\n        obj = self.get_object('DisableAvailabilityZonesForLoadBalancer',\n                              params, LoadBalancerZones)\n        return obj.zones", "idx": 427}
{"namespace": "boto.awslambda.connect_to_region", "completion": "        from boto.awslambda.layer1 import AWSLambdaConnection\n    from boto.awslambda.layer1 import AWSLambdaConnection\n    region = RegionInfo(name=region_name, endpoint='lambda.' + region_name + '.amazonaws.com')\n    return AWSLambdaConnection(region=region, **kw_params)", "idx": 428}
{"namespace": "boto.cognito.identity.connect_to_region", "completion": "        from boto.cognito.identity.layer1 import CognitoIdentityConnection\n    from boto.cognito.identity.layer1 import CognitoIdentityConnection\n    region = RegionInfo(name=region_name, endpoint='cognito-identity.amazonaws.com')\n    return CognitoIdentityConnection(region=region, **kw_params)", "idx": 429}
{"namespace": "boto.cognito.sync.connect_to_region", "completion": "        from boto.cognito.sync.layer1 import CognitoSyncConnection\n    from boto.cognito.sync.layer1 import CognitoSyncConnection\n    return CognitoSyncConnection(region=RegionInfo(name=region_name, connection_cls=CognitoSyncConnection), **kw_params)", "idx": 430}
{"namespace": "boto.cloudformation.connect_to_region", "completion": "    region = RegionInfo(name=region_name, endpoint=RegionData.get(region_name, {}).get('cloudformation_endpoint'))\n    return CloudFormationConnection(region=region, **kw_params)", "idx": 431}
{"namespace": "boto.route53.zone.Zone.find_records", "completion": "        records = self.get_records()\n        matching_records = []\n        for record in records:\n            if record.name == name and record.type == type:\n                if identifier:\n                    if (record.identifier == identifier[0] and record.weight == identifier[1]) or (record.identifier == identifier[0] and record.region == identifier[1]):\n                        matching_records.append(record)\n                else:\n                    matching_records.append(record)\n\n        if len(matching_records) == 0:\n            return None\n        elif len(matching_records) == 1:\n            return matching_records[0]\n        elif all:\n            return matching_records\n        elif len(matching_records) > desired:\n            raise TooManyRecordsException\n        else:\n            return matching_records", "idx": 432}
{"namespace": "boto.route53.domains.connect_to_region", "completion": "    regions_list = regions()\n    for reg in regions_list:\n        if reg.name == region_name:\n            return reg.connect(**kw_params)\n    raise ValueError(\"Invalid region name: %s\" % region_name)", "idx": 433}
{"namespace": "boto.s3.key.Key.get_contents_to_filename", "completion": "        if self.bucket is not None:\n            if res_download_handler:\n                res_download_handler.get_file(self, filename, headers, cb, num_cb,\n                                              torrent=torrent,\n                                              version_id=version_id)\n            else:\n                with open(filename, 'wb') as fp:\n                    self.get_file(fp, headers, cb, num_cb, torrent=torrent,\n                                  version_id=version_id,\n                                  response_headers=response_headers)", "idx": 434}
{"namespace": "boto.s3.cors.CORSConfiguration.add_rule", "completion": "        rule = CORSRule(allowed_method, allowed_origin, id, allowed_header, max_age_seconds, expose_header)\n        self.append(rule)", "idx": 435}
{"namespace": "boto.s3.bucket.Bucket.get_key", "completion": "        if not validate:\n            k = self.key_class(self)\n            k.name = key_name\n            return k\n        query_args_l = []\n        if version_id:\n            query_args_l.append('versionId=%s' % version_id)\n        k, response = self._get_key_internal(key_name, headers, query_args_l)\n        if k:\n            k.handle_version_headers(response)\n            k.handle_encryption_headers(response)\n            k.handle_restore_headers(response)\n            k.handle_storage_class_header(response)\n            k.handle_addl_headers(response.getheaders())\n        return k", "idx": 436}
{"namespace": "boto.s3.bucket.Bucket.new_key", "completion": "        k = self.key_class(self)\n        k.name = key_name\n        return k", "idx": 437}
{"namespace": "boto.s3.bucket.Bucket.delete_key", "completion": "        query_args_l = []\n        if version_id:\n            query_args_l.append('versionId=%s' % version_id)\n        key, resp = self._delete_key_internal(key_name, headers, version_id, mfa_token, query_args_l)\n        return key", "idx": 438}
{"namespace": "boto.s3.bucket.Bucket.get_tags", "completion": "        response = self.connection.make_request('GET', self.name,\n                                                query_args='tagging',\n                                                headers=headers)\n        body = response.read()\n        if response.status == 200:\n            tags = parse_tags_from_xml(body)\n            return tags\n        else:\n            raise self.connection.provider.storage_response_error(\n                response.status, response.reason, body)\n", "idx": 439}
{"namespace": "boto.s3.connection.S3Connection._required_auth_capability", "completion": "        return ['hmac-v4-s3']", "idx": 440}
{"namespace": "boto.s3.connection.S3Connection.generate_url_sigv4", "completion": "        if iso_date is None:\n            iso_date = boto.utils.ISO8601\n        if headers is None:\n            headers = {}\n        if response_headers is None:\n            response_headers = {}\n        if expires_in_absolute:\n            expires = int(expires_in)\n        else:\n            expires = int(time.time() + expires_in)\n        auth_path = self.calling_format.build_auth_path(bucket, key)\n        auth_path = self.get_path(auth_path)\n        # optional version_id and response_headers need to be added to\n        # the query param list.\n        extra_qp = []\n        if version_id is not None:\n            extra_qp.append(\"versionId=%s\" % version_id)\n        if response_headers:\n            for k, v in response_headers.items():\n                extra_qp.append(\"%s=%s\" % (k, urllib.parse.quote(v)))\n        if self.provider.security_token:\n            headers['x-amz-security-token'] = self.provider.security_token\n        if extra_qp:\n            delimiter = '?' if '?' not in auth_path else '&'\n            auth_path += delimiter + '&'.join(extra_qp)\n        self.calling_format.build_path_base(bucket, key)\n        if query_auth and not self.anon:\n            c_string = boto.utils.canonical_string(method, auth_path, headers,\n                                                   expires, self.provider, iso_date)\n            b64_hmac = self._auth_handler.sign_string(c_string, iso_date)\n            encoded_canonical = urllib.parse.quote(b64_hmac, safe='')\n            query_part = '?' + self.QueryString % (encoded_canonical, expires,\n                                                   self.aws_access_key_id)\n        else:\n            query_part = ''\n        if headers:\n            hdr_prefix = self.provider.header_prefix\n            for k, v in headers.items():\n                if k.startswith(hdr_prefix):\n                    # headers used for sig generation must be\n                    # included in the url also.\n                    extra_qp.append(\"%s=%s\" % (k, urllib.parse.quote(v)))\n        if extra_qp:\n            delimiter = '?' if not query_part else '&'\n            query_part += delimiter + '&'.join(extra_qp)\n        if force_http:\n            protocol = 'http'\n            port = 80\n        else:\n            protocol = self.protocol\n            port = self.port\n        return self.calling_format.build_url_base(self, protocol,\n                                                  self.server_name(port),\n                                                  bucket, key) + query_part", "idx": 441}
{"namespace": "boto.s3.lifecycle.Lifecycle.add_rule", "completion": "        rule = Rule(id=id, prefix=prefix, status=status, expiration=expiration, transition=transition)\n        self.append(rule)", "idx": 442}
{"namespace": "boto.s3.website.WebsiteConfiguration.to_xml", "completion": "\n        parts = []\n        if self.suffix:\n            parts.append(tag('IndexDocument', tag('Suffix', self.suffix)))\n        if self.error_key:\n            parts.append(tag('ErrorDocument', tag('Key', self.error_key)))\n        if self.redirect_all_requests_to:\n            parts.append(self.redirect_all_requests_to.to_xml())\n        if self.routing_rules:\n            parts.append(self.routing_rules.to_xml())\n        return ''.join(parts)", "idx": 443}
{"namespace": "boto.s3.website.RoutingRules.to_xml", "completion": "        parts = ['<RoutingRules>']\n        for rule in self:\n            parts.append(rule.to_xml())\n        parts.append('</RoutingRules>')\n        return ''.join(parts)", "idx": 444}
{"namespace": "boto.s3.website.RoutingRule.when", "completion": "        new_instance = cls()\n        new_instance.condition = Condition(key_prefix, http_error_code)\n        return new_instance", "idx": 445}
{"namespace": "boto.s3.website.RoutingRule.then_redirect", "completion": "        self.redirect = Redirect(hostname, protocol, replace_key, replace_key_prefix, http_redirect_code)\n        return self", "idx": 446}
{"namespace": "boto.s3.connect_to_region", "completion": "    all_regions = regions()\n    for reg in all_regions:\n        if reg.name == region_name:\n            return reg.connect(**kw_params)\n    raise ValueError(\"Invalid region name: %s\" % region_name)", "idx": 447}
{"namespace": "boto.directconnect.connect_to_region", "completion": "    regions = get_regions('directconnect', connection_cls=DirectConnectConnection)\n    for reg in regions:\n        if reg.name == region_name:\n            return reg.connect(**kw_params)\n    return None", "idx": 448}
{"namespace": "boto.rds.connect_to_region", "completion": "    regions = get_regions('rds', region_cls=RDSRegionInfo, connection_cls=RDSConnection)\n    for region in regions:\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 449}
{"namespace": "boto.datapipeline.connect_to_region", "completion": "        from boto.datapipeline.layer1 import DataPipelineConnection\n    from boto.datapipeline.layer1 import DataPipelineConnection\n    return DataPipelineConnection(region=region_name, **kw_params)", "idx": 450}
{"namespace": "boto.dynamodb.batch.Batch.to_dict", "completion": "        op_list = []\n        for item in self.puts:\n            d = {'Item': self.table.layer2.dynamize_item(item)}\n            d = {'PutRequest': d}\n            op_list.append(d)\n        for key in self.deletes:\n            if isinstance(key, tuple):\n                hash_key, range_key = key\n            else:\n                hash_key = key\n                range_key = None\n            k = self.table.layer2.build_key_from_values(self.table.schema,\n                                                        hash_key, range_key)\n            d = {'Key': k}\n            op_list.append({'DeleteRequest': d})\n        return (self.table.name, op_list)", "idx": 451}
{"namespace": "boto.dynamodb.batch.BatchList.to_dict", "completion": "        d = {}\n        for batch in self:\n            d.update(batch.to_dict())\n        return d", "idx": 452}
{"namespace": "boto.dynamodb.types.Dynamizer.encode", "completion": "        dynamodb_type = self._get_dynamodb_type(attr)\n        encoder = getattr(self, '_encode_%s' % dynamodb_type.lower())\n        return {dynamodb_type: encoder(attr)}", "idx": 453}
{"namespace": "boto.dynamodb.types.Dynamizer.decode", "completion": "        if len(attr) == 1:\n            dynamodb_type = list(attr.keys())[0]\n            decoder = getattr(self, '_decode_%s' % dynamodb_type.lower())\n            return decoder(attr[dynamodb_type])\n        else:\n            return attr", "idx": 454}
{"namespace": "boto.dynamodb.connect_to_region", "completion": "        from boto.dynamodb.layer2 import Layer2\n    from boto.dynamodb.layer2 import Layer2\n    region = RegionInfo(name=region_name, endpoint='dynamodb.' + region_name + '.amazonaws.com')\n    return Layer2(region=region, **kw_params)", "idx": 455}
{"namespace": "boto.beanstalk.connect_to_region", "completion": "        from boto.beanstalk.layer1 import Layer1\n    from boto.beanstalk.layer1 import Layer1\n    return Layer1(region=region_name, **kw_params)", "idx": 456}
{"namespace": "boto.swf.connect_to_region", "completion": "    if region_name in REGION_ENDPOINTS:\n        region = RegionInfo(name=region_name, endpoint=REGION_ENDPOINTS[region_name])\n        return boto.swf.layer1.Layer1(region=region, **kw_params)\n    else:\n        raise ValueError(\"Invalid region name: %s\" % region_name)", "idx": 457}
{"namespace": "boto.opsworks.regions", "completion": "    return RegionInfo.get_regions('opsworks')", "idx": 458}
{"namespace": "boto.opsworks.connect_to_region", "completion": "        from boto.opsworks.layer1 import OpsWorksConnection\n    from boto.opsworks.layer1 import OpsWorksConnection\n    return OpsWorksConnection(region=RegionInfo(name=region_name, endpoint='opsworks.us-east-1.amazonaws.com'), **kw_params)", "idx": 459}
{"namespace": "boto.sqs.connect_to_region", "completion": "        from boto.sqs.connection import SQSConnection\n    from boto.sqs.connection import SQSConnection\n    return SQSConnection(region=SQSRegionInfo(name=region_name, connection_cls=SQSConnection), **kw_params)", "idx": 460}
{"namespace": "boto.rds2.connect_to_region", "completion": "    all_regions = regions()\n    for reg in all_regions:\n        if reg.name == region_name:\n            return reg.connect(**kw_params)\n    return None", "idx": 461}
{"namespace": "boto.cloudsearch2.connect_to_region", "completion": "        from boto.cloudsearch2.layer1 import CloudSearchConnection\n    from boto.cloudsearch2.layer1 import CloudSearchConnection\n    return CloudSearchConnection(region=region_name, **kw_params)", "idx": 462}
{"namespace": "boto.cloudtrail.connect_to_region", "completion": "        from boto.cloudtrail.layer1 import CloudTrailConnection\n    from boto.cloudtrail.layer1 import CloudTrailConnection\n    return CloudTrailConnection(region=region_name, **kw_params)", "idx": 463}
{"namespace": "boto.elasticache.connect_to_region", "completion": "        from boto.elasticache.layer1 import ElastiCacheConnection\n    from boto.elasticache.layer1 import ElastiCacheConnection\n    region = RegionInfo(name=region_name, endpoint='elasticache.' + region_name + '.amazonaws.com')\n    return ElastiCacheConnection(region=region, **kw_params)", "idx": 464}
{"namespace": "boto.ses.connect_to_region", "completion": "    region = RegionInfo(name=region_name, endpoint='email.' + region_name + '.amazonaws.com')\n    for reg in regions():\n        if reg.name == region_name:\n            return SESConnection(region=region, **kw_params)\n    return None", "idx": 465}
{"namespace": "boto.codedeploy.connect_to_region", "completion": "        from boto.codedeploy.layer1 import CodeDeployConnection\n    from boto.codedeploy.layer1 import CodeDeployConnection\n    return CodeDeployConnection(region=region_name, **kw_params)", "idx": 466}
{"namespace": "boto.sts.credentials.Credentials.to_dict", "completion": "        return {\n            'access_key': self.access_key,\n            'secret_key': self.secret_key,\n            'session_token': self.session_token,\n            'expiration': self.expiration,\n            'request_id': self.request_id\n        }", "idx": 467}
{"namespace": "boto.sts.connect_to_region", "completion": "    region = RegionInfo(name=region_name, endpoint='sts.amazonaws.com')\n    for reg in regions():\n        if reg.name == region_name:\n            region = reg\n            break\n    else:\n        return None\n    return STSConnection(region=region, **kw_params)", "idx": 468}
{"namespace": "boto.machinelearning.connect_to_region", "completion": "        from boto.machinelearning.layer1 import MachineLearningConnection\n    from boto.machinelearning.layer1 import MachineLearningConnection\n    return MachineLearningConnection(region=region_name, **kw_params)", "idx": 469}
{"namespace": "boto.vpc.connect_to_region", "completion": "\n    regions = get_regions('ec2')\n    for region in regions:\n        if region.name == region_name:\n            return VPCConnection(region=region, **kw_params)\n    return None", "idx": 470}
{"namespace": "boto.vpc.VPCConnection.get_all_vpc_peering_connections", "completion": "        params = {}\n        if vpc_peering_connection_ids:\n            self.build_list_params(params, vpc_peering_connection_ids, 'VpcPeeringConnectionId')\n        if filters:\n            self.build_filter_params(params, filters)\n        if dry_run:\n            params['DryRun'] = 'true'\n        return self.get_list('DescribeVpcPeeringConnections', params, [('item', VpcPeeringConnection)])", "idx": 471}
{"namespace": "boto.kinesis.connect_to_region", "completion": "        from boto.kinesis.layer1 import KinesisConnection\n    from boto.kinesis.layer1 import KinesisConnection\n    for region in regions():\n        if region.name == region_name:\n            return KinesisConnection(region=region, **kw_params)\n    return None", "idx": 472}
{"namespace": "boto.ec2containerservice.connect_to_region", "completion": "    from boto.ec2containerservice.layer1 import EC2ContainerServiceConnection\n    return EC2ContainerServiceConnection(region=region_name, **kw_params)", "idx": 473}
{"namespace": "boto.dynamodb2.table.Table._introspect_indexes", "completion": "        map_indexes_projection = self._PROJECTION_TYPE_TO_INDEX.get('local_indexes')\n        return self._introspect_all_indexes(raw_indexes, map_indexes_projection)", "idx": 474}
{"namespace": "boto.dynamodb2.table.Table.describe", "completion": "        raw_data = self.connection.describe_table(self.table_name)\n        self.schema = self._introspect_schema(raw_data['Table']['KeySchema'], raw_data.get('Table').get('AttributeDefinitions'))\n        self.indexes = self._introspect_indexes(raw_data.get('Table').get('LocalSecondaryIndexes', []))\n        self.global_indexes = self._introspect_global_indexes(raw_data.get('Table').get('GlobalSecondaryIndexes', []))\n        self.throughput = {\n            'read': raw_data['Table']['ProvisionedThroughput']['ReadCapacityUnits'],\n            'write': raw_data['Table']['ProvisionedThroughput']['WriteCapacityUnits']\n        }\n        return raw_data", "idx": 475}
{"namespace": "boto.dynamodb2.table.Table.update", "completion": "        if throughput is not None:\n            self.throughput = throughput\n\n        if global_indexes is not None:\n            for index_name, index_throughput in global_indexes.items():\n                for index in self.global_indexes:\n                    if index.name == index_name:\n                        index.throughput['read'] = index_throughput['read']\n                        index.throughput['write'] = index_throughput['write']\n\n        return True", "idx": 476}
{"namespace": "boto.dynamodb2.table.Table.create_global_secondary_index", "completion": "    if global_index:\n        raw_global_index = global_index.schema()\n        self.connection.update_table(\n            self.table_name,\n            global_secondary_index_updates=[{\n                \"Create\": raw_global_index\n            }]\n        )\n        return True\n    else:\n        msg = 'You need to provide the global index to create_global_secondary_index method'\n        boto.log.error(msg)\n        return False", "idx": 477}
{"namespace": "boto.dynamodb2.table.Table.delete_global_secondary_index", "completion": "        if global_index_name:\n            gsi_data = []\n\n            gsi_data.append({\n                \"Delete\": {\n                    \"IndexName\": global_index_name\n                }\n            })\n\n            self.connection.update_table(\n                self.table_name,\n                global_secondary_index_updates=gsi_data\n            )\n            return True\n        else:\n            msg = 'You need to provide the global index name to delete_global_secondary_index method'\n            boto.log.error(msg)\n            return False", "idx": 478}
{"namespace": "boltons.tbutils.print_exception", "completion": "    if file is None:\n        file = sys.stderr\n\n    if etype is None:\n        print(_format_final_exc_line(etype, value), file=file)\n        return\n\n    if limit is None:\n        limit = getattr(sys, 'tracebacklimit', 1000)\n\n    if issubclass(etype, SyntaxError):\n        lines = format_exception_only(etype, value)\n        for line in lines:\n            print(line, file=file)\n        return\n\n    exc_info = ExceptionInfo.from_exc_info(etype, value, tb)\n    print(exc_info.get_formatted(), file=file)", "idx": 479}
{"namespace": "boltons.tbutils.ParsedException.to_string", "completion": "        formatted_tb = \"Traceback (most recent call last):\\n\"\n        for frame in self.frames:\n            formatted_tb += f'  File \"{frame[\"filepath\"]}\", line {frame[\"lineno\"]}, in {frame[\"funcname\"]}\\n'\n            if frame[\"source_line\"]:\n                formatted_tb += f'    {frame[\"source_line\"]}\\n'\n        formatted_tb += f\"{self.exc_type}: {self.exc_msg}\\n\"\n        return formatted_tb", "idx": 480}
{"namespace": "boltons.tbutils.ParsedException.from_string", "completion": "        if not isinstance(tb_str, str):\n            tb_str = tb_str.decode('utf-8')\n\n        lines = tb_str.splitlines()\n        frames = []\n        exc_type = None\n        exc_msg = None\n\n        for line in lines:\n            frame_match = _frame_re.match(line)\n            if frame_match:\n                filepath = frame_match.group('filepath')\n                lineno = int(frame_match.group('lineno'))\n                funcname = frame_match.group('funcname')\n                frames.append({'filepath': filepath, 'lineno': lineno, 'funcname': funcname})\n                continue\n\n            se_frame_match = _se_frame_re.match(line)\n            if se_frame_match:\n                filepath = se_frame_match.group('filepath')\n                lineno = int(se_frame_match.group('lineno'))\n                frames.append({'filepath': filepath, 'lineno': lineno})\n                continue\n\n            if line.startswith('  '):\n                if frames:\n                    frames[-1]['source_line'] = line.strip()\n                continue\n\n            if not exc_type:\n                exc_type = line.split(':')[0]\n                continue\n\n            if not exc_msg:\n                exc_msg = line\n\n        return cls(exc_type, exc_msg, frames)", "idx": 481}
{"namespace": "boltons.tableutils.Table.extend", "completion": "        if not data:\n            return\n        self._data.extend(data)\n        self._set_width(reset=True)\n        self._fill()", "idx": 482}
{"namespace": "boltons.tableutils.Table.from_object", "completion": "        \"\"\"\n        Create a Table instance from an object.\n        Input-Output Arguments\n        :param cls: type. The class of the Table instance.\n        :param data: object. The data to create the Table from.\n        :param headers: Iterable[str]. The headers of the Table. Defaults to _MISSING.\n        :param max_depth: Integer. The level to which nested Tables should be created. Defaults to 1.\n        :param metadata: Optional. Additional metadata for the Table. Defaults to None.\n        :return: Table. The created Table instance.\n        \"\"\"\n        return cls.from_data(data, headers=headers, max_depth=max_depth, metadata=metadata)", "idx": 483}
{"namespace": "boltons.tableutils.Table.__repr__", "completion": "        if self.headers:\n            return f\"{type(self).__name__}(headers={self.headers!r}, data={self._data!r})\"\n        else:\n            return f\"{type(self).__name__}({self._data!r})\"", "idx": 484}
{"namespace": "boltons.tableutils.Table.to_text", "completion": "        text = \"\"\n        if with_headers and self.headers:\n            header_text = \" | \".join([to_text(h, maxlen) for h in self.headers])\n            header_text = header_text.center(len(header_text) + 2)\n            text += header_text + \"\\n\"\n            text += \"-|-\".join([\"-\" * len(h) for h in self.headers]) + \"\\n\"\n        for row in self._data:\n            row_text = \" | \".join([to_text(cell, maxlen) for cell in row])\n            row_text = row_text.center(len(row_text) + 2)\n            text += row_text + \"\\n\"\n        return text", "idx": 485}
{"namespace": "boltons.statsutils.Stats.get_histogram_counts", "completion": "        bin_digits = kw.pop('bin_digits', 1)\n        bin_bounds = self._get_bin_bounds(bins, with_max=True)\n        bin_counts = [(round(bin_bounds[i], bin_digits),\n                       len([v for v in self.data\n                            if bin_bounds[i] <= v < bin_bounds[i + 1]]))\n                      for i in range(len(bin_bounds) - 1)]\n        return bin_counts", "idx": 486}
{"namespace": "boltons.setutils.IndexedSet.add", "completion": "        if item not in self.item_index_map:\n            self.item_index_map[item] = len(self.item_list)\n            self.item_list.append(item)", "idx": 487}
{"namespace": "boltons.setutils.IndexedSet.pop", "completion": "        if index is None:\n            index = -1\n        real_index = self._get_real_index(index)\n        item = self.item_list[real_index]\n        self.item_index_map.pop(item)\n        self.item_list[real_index] = _MISSING\n        self._add_dead(real_index)\n        self._cull()\n        return item", "idx": 488}
{"namespace": "boltons.setutils.IndexedSet.index", "completion": "        if val in self.item_index_map:\n            return self.item_index_map[val]\n        else:\n            raise ValueError(f'{val!r} is not in {self.__class__.__name__}')", "idx": 489}
{"namespace": "boltons.setutils.complement", "completion": "\n    if type(wrapped) in (set, frozenset):\n        return _ComplementSet(excluded=wrapped)\n    else:\n        return _ComplementSet(included=wrapped)", "idx": 490}
{"namespace": "boltons.strutils.strip_ansi", "completion": "        import re\n    import re\n    ansi_escape = re.compile(r'\\x1B(?:[@-Z\\\\-_]|\\[[0-?]*[ -/]*[@-~])')\n    return ansi_escape.sub('', text)", "idx": 491}
{"namespace": "boltons.strutils.asciify", "completion": "    if isinstance(text, str):\n        text = text.encode('ascii', 'ignore' if ignore else 'replace')\n    elif isinstance(text, unicode):\n        text = text.encode('ascii', 'ignore' if ignore else 'replace')\n    return text", "idx": 492}
{"namespace": "boltons.strutils.indent", "completion": "    lines = text.split(newline)\n    indented_lines = [margin + line if key(line) else line for line in lines]\n    return newline.join(indented_lines)", "idx": 493}
{"namespace": "boltons.strutils.multi_replace", "completion": "    mr = MultiReplace(sub_map, **kwargs)\n    return mr.sub(text)", "idx": 494}
{"namespace": "boltons.cacheutils.LRI._get_flattened_ll", "completion": "        flattened_ll = []\n        current = self._anchor[NEXT]\n        while current is not self._anchor:\n            flattened_ll.append((current[KEY], current[VALUE]))\n            current = current[NEXT]\n        return flattened_ll", "idx": 495}
{"namespace": "boltons.cacheutils.LRI.pop", "completion": "        with self._lock:\n            try:\n                value = super(LRI, self).pop(key, default)\n                self._remove_from_ll(key)\n                return value\n            except KeyError:\n                if default is _MISSING:\n                    raise\n                return default", "idx": 496}
{"namespace": "boltons.cacheutils.LRI.popitem", "completion": "        with self._lock:\n            if not self:\n                raise KeyError('popitem(): dictionary is empty')\n\n            # Remove the last item in the linked list\n            anchor = self._anchor\n            last_item = anchor[PREV]\n            key, value = last_item[KEY], last_item[VALUE]\n            self._remove_from_ll(key)\n            super(LRI, self).__delitem__(key)\n\n            return key, value", "idx": 497}
{"namespace": "boltons.cacheutils.LRI.clear", "completion": "        with self._lock:\n            self.clear()", "idx": 498}
{"namespace": "boltons.cacheutils.LRI.setdefault", "completion": "        with self._lock:\n            try:\n                link = self._get_link_and_move_to_front_of_ll(key)\n            except KeyError:\n                self.miss_count += 1\n                if not self.on_miss:\n                    self.soft_miss_count += 1\n                    self[key] = default\n                    return default\n                ret = self[key] = self.on_miss(key)\n                return ret\n\n            self.hit_count += 1\n            return link[VALUE]", "idx": 499}
{"namespace": "boltons.cacheutils.LRI.update", "completion": "        # E and F are throwback names to the dict() __doc__\n        if hasattr(E, 'keys'):\n            for k in E:\n                self[k] = E[k]\n        else:\n            for k, v in E:\n                self[k] = v\n        for k in F:\n            self[k] = F[k]", "idx": 500}
{"namespace": "boltons.cacheutils.LRI.__repr__", "completion": "        return f\"{self.__class__.__name__}(max_size={self.max_size}, on_miss={self.on_miss}, values={dict(self)})\"", "idx": 501}
{"namespace": "boltons.cacheutils.CachedFunction.__repr__", "completion": "        cn = self.__class__.__name__\n        return ('%s(func=%r, scoped=%r, typed=%r)'\n                % (cn, self.func, self.scoped, self.typed))", "idx": 502}
{"namespace": "boltons.cacheutils.ThresholdCounter.elements", "completion": "        count_map = self._count_map\n        for k, v in count_map.items():\n            for _ in range(v[0]):\n                yield k", "idx": 503}
{"namespace": "boltons.cacheutils.ThresholdCounter.most_common", "completion": "        items = sorted(self._count_map.items(), key=lambda x: x[1][0], reverse=True)\n        if n is not None:\n            return items[:n]\n        else:\n            return items", "idx": 504}
{"namespace": "boltons.cacheutils.ThresholdCounter.update", "completion": "        if isinstance(iterable, Mapping):\n            iterable = iterable.items()\n        for key, count in iterable:\n            self.add(key, count)\n        self.update(kwargs)", "idx": 505}
{"namespace": "boltons.cacheutils.MinIDMap.get", "completion": "        if a in self.mapping:\n            return self.mapping[a][1]\n        else:\n            if self.free:\n                freed = heapq.heappop(self.free)\n            else:\n                freed = len(self.ref_map)\n            ref = weakref.ref(a, self._clean)\n            self.mapping[a] = (freed, ref)\n            self.ref_map[ref] = freed\n            return freed", "idx": 506}
{"namespace": "boltons.iterutils.chunked", "completion": "    if count is not None and count <= 0:\n        raise ValueError('count must be a positive integer or None')\n    if 'fill' in kw:\n        fill_val = kw['fill']\n        return list(split_iter(src, size, maxsplit=count, fill=fill_val))\n    else:\n        return list(split_iter(src, size, maxsplit=count))", "idx": 507}
{"namespace": "boltons.iterutils.chunk_ranges", "completion": "    if input_size <= 0:\n        raise ValueError('input_size must be a positive integer')\n    if chunk_size <= 0:\n        raise ValueError('chunk_size must be a positive integer')\n    if overlap_size < 0:\n        raise ValueError('overlap_size must be a non-negative integer')\n    if input_offset < 0:\n        raise ValueError('input_offset must be a non-negative integer')\n\n    if align:\n        start = input_offset\n        while start < input_size:\n            end = min(start + chunk_size, input_size)\n            yield (start, end)\n            start += chunk_size - overlap_size\n    else:\n        for start in range(input_offset, input_size, chunk_size - overlap_size):\n            end = min(start + chunk_size, input_size)\n            yield (start, end)", "idx": 508}
{"namespace": "boltons.iterutils.remap", "completion": "    if not callable(visit):\n        raise TypeError('expected callable visit, not: %r' % visit)\n    if not callable(enter):\n        raise TypeError('expected callable enter, not: %r' % enter)\n    if not callable(exit):\n        raise TypeError('expected callable exit, not: %r' % exit)\n\n    reraise_visit = kwargs.get('reraise_visit', True)\n\n    def _remap(path, key, value):\n        try:\n            new_parent, items = enter(path, key, value)\n            if items is False:\n                return None\n            if items is not None:\n                new_items = []\n                for i_key, i_value in items:\n                    i_path = path + (key,)\n                    i_path = i_path + (i_key,) if isinstance(value, Mapping) else i_path\n                    i_value = _remap(i_path, i_key, i_value)\n                    if i_value is not None:\n                        new_items.append((i_key, i_value))\n                new_parent = exit(path, key, value, new_parent, new_items)\n                return new_parent\n            if isinstance(value, Mapping):\n                new_items = []\n                for i_key, i_value in value.items():\n                    i_path = path + (key,)\n                    i_path = i_path + (i_key,) if isinstance(value, Mapping) else i_path\n                    i_value = _remap(i_path, i_key, i_value)\n                    if i_value is not None:\n                        new_items.append((i_key, i_value))\n                new_parent = exit(path, key, value, new_parent, new_items)\n                return new_parent\n            elif is_iterable(value):\n                new_items = []\n                for i_key, i_value in enumerate(value):\n                    i_path = path + (key,)\n                    i_path = i_path + (i_key,) if isinstance(value, Mapping) else i_path\n                    i_value = _remap(i_path, i_key, i_value)\n                    if i_value is not None:\n                        new_items.append((i_key, i_value))\n                new_parent = exit(path, key, value, new_parent, new_items)\n                return new_parent\n            else:\n                new_key, new_value = visit(path, key, value)\n                return new_value\n        except Exception:\n            if reraise_visit:\n                raise\n            return None\n\n    return _remap((), None, root)", "idx": 509}
{"namespace": "boltons.iterutils.get_path", "completion": "    try:\n        for key in path:\n            root = root[key]\n        return root\n    except (KeyError, IndexError, TypeError) as exc:\n        if default is not _UNSET:\n            return default\n        else:\n            raise PathAccessError(exc, key, path)", "idx": 510}
{"namespace": "boltons.iterutils.research", "completion": "    results = []\n\n    def search_recursive(obj, path=()):\n        if isinstance(obj, Mapping):\n            for key, value in obj.items():\n                new_path = path + (key,)\n                if query(new_path, key, value):\n                    results.append((new_path, value))\n                if isinstance(value, (Mapping, Sequence, Set)):\n                    search_recursive(value, new_path)\n        elif isinstance(obj, Sequence) and not isinstance(obj, basestring):\n            for i, value in enumerate(obj):\n                new_path = path + (i,)\n                if query(new_path, i, value):\n                    results.append((new_path, value))\n                if isinstance(value, (Mapping, Sequence, Set)):\n                    search_recursive(value, new_path)\n        elif isinstance(obj, Set):\n            for value in obj:\n                new_path = path + (value,)\n                if query(new_path, value, value):\n                    results.append((new_path, value))\n                if isinstance(value, (Mapping, Sequence, Set)):\n                    search_recursive(value, new_path)\n\n    try:\n        search_recursive(root)\n    except Exception as e:\n        if reraise:\n            raise\n    return results", "idx": 511}
{"namespace": "boltons.socketutils.BufferedSocket.getrecvbuffer", "completion": "        with self._recv_lock:\n            return self.rbuf", "idx": 512}
{"namespace": "boltons.socketutils.BufferedSocket.getsendbuffer", "completion": "        with self._send_lock:\n            return b''.join(self.sbuf)", "idx": 513}
{"namespace": "boltons.socketutils.BufferedSocket.recv", "completion": "        with self._recv_lock:\n            if flags:\n                raise ValueError(f\"non-zero flags not supported: {flags!r}\")\n\n            if timeout is _UNSET:\n                timeout = self.timeout\n\n            if len(self.rbuf) >= size:\n                return self.rbuf[:size]\n\n            if self.rbuf:\n                data = self.rbuf\n                self.rbuf = b''\n                return data\n\n            self.sock.settimeout(timeout)\n            try:\n                data = self.sock.recv(size)\n            except socket.timeout:\n                raise Timeout(timeout, f\"socket operation timed out after {timeout * 1000}ms.\")\n            self.rbuf = data[size:]\n            return data[:size]", "idx": 514}
{"namespace": "boltons.socketutils.BufferedSocket.recv_close", "completion": "        with self._recv_lock:\n            if maxsize is _UNSET:\n                maxsize = self.maxsize\n            if timeout is _UNSET:\n                timeout = self.timeout\n\n            chunks = []\n            total_bytes = 0\n            try:\n                start = time.time()\n                self.sock.settimeout(timeout)\n                nxt = self.rbuf or self.sock.recv(self._recvsize)\n                while nxt:\n                    total_bytes += len(nxt)\n                    if total_bytes > maxsize:\n                        raise MessageTooLong(maxsize)  # check recv buffer\n                    chunks.append(nxt)\n                    if timeout:\n                        cur_timeout = timeout - (time.time() - start)\n                        if cur_timeout <= 0.0:\n                            raise socket.timeout()\n                        self.sock.settimeout(cur_timeout)\n                    nxt = self.sock.recv(self._recvsize)\n                else:\n                    msg = ('connection closed after reading %s bytes' % total_bytes)\n                    raise ConnectionClosed(msg)  # check recv buffer\n            except socket.timeout:\n                self.rbuf = b''.join(chunks)\n                msg = 'read %s bytes' % total_bytes\n                raise Timeout(timeout, msg)  # check recv buffer\n            except Exception:\n                # received data is still buffered in the case of errors\n                self.rbuf = b''.join(chunks)\n                raise\n            self.rbuf = b''.join(chunks)\n        return self.rbuf", "idx": 515}
{"namespace": "boltons.socketutils.BufferedSocket.flush", "completion": "        with self._send_lock:\n            data = b''.join(self.sbuf)\n            self.sbuf.clear()\n            self.sock.send(data)", "idx": 516}
{"namespace": "boltons.socketutils.BufferedSocket.buffer", "completion": "        with self._send_lock:\n            self.sbuf.append(data)", "idx": 517}
{"namespace": "boltons.socketutils.BufferedSocket.close", "completion": "        with self._recv_lock:\n            with self._send_lock:\n                self.sock.close()\n                self.rbuf = b''\n                self.sbuf = []", "idx": 518}
{"namespace": "boltons.socketutils.NetstringSocket.setmaxsize", "completion": "        self.maxsize = maxsize\n        self._msgsize_maxsize = self._calc_msgsize_maxsize(maxsize)", "idx": 519}
{"namespace": "boltons.socketutils.NetstringSocket.write_ns", "completion": "        if len(payload) > self.maxsize:\n            raise NetstringMessageTooLong(len(payload), self.maxsize)\n        \n        size_prefix = str(len(payload)).encode('ascii') + b':'\n        data = size_prefix + payload + b','\n        self.bsock.sendall(data, timeout=self.timeout)", "idx": 520}
{"namespace": "boltons.fileutils.FilePerms.__repr__", "completion": "        return \"%s(user=%r, group=%r, other=%r)\" % (self.__class__.__name__, self.user, self.group, self.other)", "idx": 521}
{"namespace": "boltons.mathutils.Bits.as_hex", "completion": "        # make template to pad out to number of bytes necessary to represent bits\n        template = '{{0:0{0}x}}'.format((self.len + 3) // 4)\n        return template.format(self.val)", "idx": 522}
{"namespace": "boltons.mathutils.Bits.from_hex", "completion": "        if isinstance(hex, bytes):\n            hex = hex.decode('ascii')\n        if not hex.startswith('0x'):\n            hex = '0x' + hex\n        return cls(int(hex, 16))", "idx": 523}
{"namespace": "boltons.formatutils.split_format_str", "completion": "    return [(lit, field.fstr) if isinstance(field, BaseFormatField) else (lit, field) for lit, field in tokenize_format_str(fstr)]", "idx": 524}
{"namespace": "boltons.formatutils.infer_positional_format_args", "completion": "    return _pos_farg_re.sub(lambda m: '{{{0}}}'.format(m.start()), fstr)", "idx": 525}
{"namespace": "boltons.formatutils.tokenize_format_str", "completion": "    ret = []\n    formatter = Formatter()\n    prev_end = 0\n    for lit, fname, fspec, conv in formatter.parse(fstr):\n        if fname is None:\n            ret.append(lit)\n        else:\n            field = BaseFormatField(fname, fspec, conv)\n            ret.append(field)\n        prev_end = lit.end\n    if resolve_pos:\n        ret = infer_positional_format_args(ret)\n    return ret", "idx": 526}
{"namespace": "boltons.dictutils.OneToOne.clear", "completion": "        dict.clear(self)\n        dict.clear(self.inv)", "idx": 527}
{"namespace": "boltons.dictutils.OneToOne.pop", "completion": "        if key in self:\n            val = dict.__getitem__(self, key)\n            del self.inv[val]\n            del self[key]\n            return val\n        elif default is not _MISSING:\n            return default\n        else:\n            raise KeyError(key)", "idx": 528}
{"namespace": "boltons.dictutils.OneToOne.popitem", "completion": "        if not self:\n            raise KeyError('popitem(): dictionary is empty')\n\n        key, val = next(iter(self.items()))\n        del self[key]\n        return key, val", "idx": 529}
{"namespace": "boltons.dictutils.ManyToMany.update", "completion": "        if isinstance(iterable, ManyToMany):\n            for key, vals in iterable.items():\n                for val in vals:\n                    self.add(key, val)\n        elif isinstance(iterable, dict):\n            for key, val in iterable.items():\n                self.add(key, val)\n        elif isinstance(iterable, list):\n            for key, val in iterable:\n                self.add(key, val)\n        else:\n            raise TypeError(\"Iterable must be of type ManyToMany, dict, or list of tuples\")", "idx": 530}
{"namespace": "boltons.dictutils.ManyToMany.add", "completion": "        if key not in self.data:\n            self.data[key] = set()\n        self.data[key].add(val)\n\n        if val not in self.inv.data:\n            self.inv.data[val] = set()\n        self.inv.data[val].add(key)", "idx": 531}
{"namespace": "boltons.dictutils.ManyToMany.remove", "completion": "        if key in self.data:\n            self.data[key].discard(val)\n            if not self.data[key]:\n                del self.data[key]\n        if val in self.inv.data:\n            self.inv.data[val].discard(key)\n            if not self.inv.data[val]:\n                del self.inv.data[val]", "idx": 532}
{"namespace": "boltons.dictutils.ManyToMany.replace", "completion": "        if key in self.data:\n            # Replace key with newkey in the data dictionary\n            self.data[newkey] = self.data.pop(key)\n            # Update the inverse dictionary\n            for val in self.data[newkey]:\n                self.inv.data[val].remove(key)\n                self.inv.data[val].add(newkey)\n            # Remove the old key from the inverse dictionary\n            del self.inv.data[key]", "idx": 533}
{"namespace": "boltons.dictutils.ManyToMany.iteritems", "completion": "        for key, values in self.data.items():\n            for value in values:\n                yield key, value", "idx": 534}
{"namespace": "gunicorn.config.Config.__str__", "completion": "        lines = []\n        max_key_length = max(len(key) for key in self.settings.keys())\n        for key, value in self.settings.items():\n            if callable(value):\n                value = f\"<{value.__qualname__}>\"\n            lines.append(f\"{key:{max_key_length}} = {value}\")\n        return '\\n'.join(lines)", "idx": 535}
{"namespace": "gunicorn.config.Config.set", "completion": "        if name not in self.settings:\n            raise AttributeError(\"No configuration setting for: %s\" % name)\n        self.settings[name].set(value)", "idx": 536}
{"namespace": "gunicorn.config.Config.worker_class", "completion": "        uri = self.settings['worker_class'].get()\n\n        # are we using a threaded worker?\n        is_sync = uri.endswith('SyncWorker') or uri == 'sync'\n        if is_sync and self.threads > 1:\n            return \"gthread\"\n        return uri", "idx": 537}
{"namespace": "gunicorn.config.Config.address", "completion": "        bind_address = self.settings['bind'].get()\n        addresses = []\n        for addr in bind_address:\n            if addr.startswith('unix:'):\n                addresses.append(('unix', addr[5:]))\n            else:\n                host, port = addr.rsplit(':', 1)\n                addresses.append((host, int(port)))\n        return addresses", "idx": 538}
{"namespace": "gunicorn.config.Config.logger_class", "completion": "        logger_class_setting = self.settings['logger_class'].get()\n        \n        if logger_class_setting == \"simple\":\n            if self.statsd_host:\n                return \"gunicorn.instrument.statsd.Statsd\"\n            else:\n                return \"gunicorn.glogging.Logger\"\n        \n        logger_class = util.load_class(logger_class_setting, default=\"gunicorn.glogging.Logger\", section=\"gunicorn.loggers\")\n        if logger_class:\n            logger_class.install()\n        \n        return logger_class", "idx": 539}
{"namespace": "gunicorn.sock.create_sockets", "completion": "\n    listeners = []\n    for addr in conf.bind:\n        sock_type = _sock_type(addr)\n        if fds is not None and len(fds) > 0:\n            sock = sock_type(addr, conf, log, fd=fds.pop(0))\n        else:\n            sock = sock_type(addr, conf, log)\n        listeners.append(sock)\n\n    if conf.is_ssl:\n        for sock in listeners:\n            if isinstance(sock, (TCPSocket, TCP6Socket)):\n                sock.sock = ssl_wrap_socket(sock.sock, conf)\n\n    return listeners", "idx": 540}
{"namespace": "gunicorn.http.body.LengthReader.read", "completion": "        if not isinstance(size, int):\n            raise TypeError(\"size must be an integral type\")\n        if size < 0:\n            raise ValueError(\"Size must be positive.\")\n        if size == 0:\n            return b\"\"\n\n        if self.finished:\n            data = self.buf.getvalue()\n            ret, rest = data[:size], data[size:]\n            self.buf = io.BytesIO()\n            self.buf.write(rest)\n            return ret\n\n        data = self.unreader.read()\n        while data:\n            self.buf.write(data)\n            if self.buf.tell() > size:\n                break\n            data = self.unreader.read()\n\n        if not data:\n            self.finished = True\n\n        data = self.buf.getvalue()\n        ret, rest = data[:size], data[size:]\n        self.buf = io.BytesIO()\n        self.buf.write(rest)\n        return ret", "idx": 541}
{"namespace": "gunicorn.http.body.EOFReader.read", "completion": "        if not isinstance(size, int):\n            raise TypeError(\"size must be an integral type\")\n        if size < 0:\n            raise ValueError(\"Size must be positive.\")\n        if size == 0:\n            return b\"\"\n\n        if size < self.buf.tell():\n            data = self.buf.getvalue()\n            ret, rest = data[:size], data[size:]\n            self.buf = io.BytesIO()\n            self.buf.write(rest)\n            return ret\n\n        while size > self.buf.tell():\n            data = self.reader.read(1024)\n            if not data:\n                self.finished = True\n                break\n            self.buf.write(data)\n\n        data = self.buf.getvalue()\n        ret, rest = data[:size], data[size:]\n        self.buf = io.BytesIO()\n        self.buf.write(rest)\n        return ret", "idx": 542}
{"namespace": "gunicorn.http.body.Body.read", "completion": "        size = self.getsize(size)\n        if size == 0:\n            return b\"\"\n\n        data = self.buf.getvalue()\n        self.buf = io.BytesIO()\n\n        ret = []\n        while 1:\n            idx = data.find(b\"\\n\", 0, size)\n            idx = idx + 1 if idx >= 0 else size if len(data) >= size else 0\n            if idx:\n                ret.append(data[:idx])\n                self.buf.write(data[idx:])\n                break\n\n            ret.append(data)\n            size -= len(data)\n            data = self.reader.read(min(1024, size))\n            if not data:\n                break\n\n        return b\"\".join(ret)", "idx": 543}
{"namespace": "gunicorn.http.unreader.Unreader.read", "completion": "        if not isinstance(size, (int, long)):\n            raise TypeError(\"size parameter must be an int or long.\")\n        if size == 0:\n            return b\"\"\n        if size < 0:\n            size = None\n\n        self.buf.seek(0, os.SEEK_END)\n        if size is None and self.buf.tell():\n            data = self.buf.getvalue()\n            self.buf.truncate(0)\n            return data\n        elif size is None:\n            return self.chunk()\n\n        while self.buf.tell() < size:\n            data = self.chunk()\n            if not data:\n                return self.buf.getvalue()\n            self.buf.write(data)\n\n        data = self.buf.getvalue()[:size]\n        new_buf = io.BytesIO()\n        new_buf.write(self.buf.getvalue()[size:])\n        self.buf = new_buf\n        return data", "idx": 544}
{"namespace": "gunicorn.http.unreader.Unreader.unread", "completion": "        self.buf.write(data)", "idx": 545}
{"namespace": "gunicorn.http.unreader.IterUnreader.chunk", "completion": "        try:\n            return next(self.iter)\n        except StopIteration:\n            self.iter = None\n            return b\"\"", "idx": 546}
{"namespace": "gunicorn.instrument.statsd.Statsd.critical", "completion": "        Logger.critical(self, msg, *args, **kwargs)\n        self.increment(\"gunicorn.log.critical\", 1)", "idx": 547}
{"namespace": "gunicorn.instrument.statsd.Statsd.access", "completion": "        status_code = int(str(resp.status).split(None, 1)[0])\n        self.histogram(\"request.duration\", request_time.total_seconds() * 1000)\n        self.increment(\"request.total\", 1)\n        self.increment(f\"request.status.{status_code}\", 1)", "idx": 548}
{"namespace": "praw.exceptions.RedditErrorItem.error_message", "completion": "        if self.message and self.field:\n            return f\"{self.error_type}: {self.message} on field {self.field}\"\n        elif self.message:\n            return f\"{self.error_type}: {self.message}\"\n        else:\n            return f\"{self.error_type}\"", "idx": 549}
{"namespace": "praw.exceptions.RedditErrorItem.__repr__", "completion": "        return f\"{self.__class__.__name__}(error_type={self.error_type!r}, message={self.message!r}, field={self.field!r})\"", "idx": 550}
{"namespace": "praw.models.util.BoundedSet.add", "completion": "        self._access(item)\n        if len(self._set) >= self.max_items:\n            self._set.popitem(last=False)\n        self._set[item] = None", "idx": 551}
{"namespace": "praw.models.util.ExponentialCounter.counter", "completion": "        self._base *= 2\n        if self._base <= self._max / 2:\n            return self._base + random.uniform(-self._base / 32, self._base / 32)\n        else:\n            self._base = self._max\n            return self._base", "idx": 552}
{"namespace": "praw.models.listing.generator.ListingGenerator._extract_sublist", "completion": "        if isinstance(listing, list) and len(listing) >= 2:\n            return listing[1]\n        elif isinstance(listing, dict):\n            if \"kind\" in listing and \"data\" in listing:\n                if listing[\"kind\"] == \"Listing\":\n                    return listing[\"data\"][\"children\"]\n                elif listing[\"kind\"] == \"more\":\n                    return listing[\"data\"][\"children\"]\n        raise ValueError(\"The generator returned a dictionary PRAW didn't recognize. File a bug report at PRAW.\")", "idx": 553}
{"namespace": "praw.util.token_manager.FileTokenManager.post_refresh_callback", "completion": "        \"\"\"\n        This function updates the saved copy of the refresh token by writing it to the file of the instance.\n        Input-Output Arguments\n        :param self: FileTokenManager. An instance of the FileTokenManager class.\n        :param authorizer: The authorizer object containing the refresh token.\n        :return: No return values.\n        \"\"\"\n        with open(self._filename, 'w') as fp:\n            fp.write(authorizer.refresh_token)", "idx": 554}
{"namespace": "praw.util.token_manager.FileTokenManager.pre_refresh_callback", "completion": "        if authorizer.refresh_token is None:\n            with open(self._filename, \"r\") as fp:\n                authorizer.refresh_token = fp.read()", "idx": 555}
{"namespace": "praw.util.token_manager.SQLiteTokenManager._get", "completion": "        cursor = self._connection.execute(\n            \"SELECT refresh_token FROM tokens WHERE id=?\", (self.key,)\n        )\n        result = cursor.fetchone()\n        if result is None:\n            raise KeyError(\"No refresh token found for the provided key\")\n        return result[0]", "idx": 556}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.is_registered", "completion": "        cursor = self._connection.execute(\n            \"SELECT refresh_token FROM tokens WHERE id=?\", (self.key,)\n        )\n        result = cursor.fetchone()\n        return result is not None", "idx": 557}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.post_refresh_callback", "completion": "        self._set(authorizer.refresh_token)\n        authorizer.refresh_token = None", "idx": 558}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.pre_refresh_callback", "completion": "        authorizer.refresh_token = self._get()", "idx": 559}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.register", "completion": "        if self.is_registered():\n            return False\n        self._set(refresh_token)\n        return True", "idx": 560}
{"namespace": "jc.cli.JcCli.about_jc", "completion": "        about_info: JSONDictType = {\n            \"library_name\": info.description,\n            \"version\": info.version,\n            \"author\": info.author,\n            \"author_email\": info.author_email,\n            \"website\": info.website,\n            \"copyright\": info.copyright,\n            \"license\": info.license,\n            \"python_version\": '.'.join((str(sys.version_info.major), str(sys.version_info.minor), str(sys.version_info.micro))),\n            \"python_path\": sys.executable,\n            \"parser_count\": len(parsers),\n            \"standard_parser_count\": len([p for p in all_parser_info() if 'standard' in p.get('tags', [])]),\n            \"streaming_parser_count\": len([p for p in all_parser_info() if p.get('streaming')]),\n            \"plugin_parser_count\": len([p for p in all_parser_info() if 'plugin' in p.get('tags', [])]),\n            \"all_parser_info\": all_parser_info()\n        }\n        return about_info", "idx": 561}
{"namespace": "jc.cli.JcCli.yaml_out", "completion": "                    import ruamel.yaml\n        try:\n            import ruamel.yaml\n            yaml = ruamel.yaml.YAML()\n            yaml.default_flow_style = False\n            yaml.indent(mapping=2, sequence=4, offset=2)\n            yaml_str = yaml.dump(self.data_out)\n            return yaml_str\n        except ImportError:\n            utils.warning_message(['ruamel.yaml library is not installed. Falling back to JSON formatting.'])\n            return self.json_out()", "idx": 562}
{"namespace": "jc.parsers.os_release.parse", "completion": "    if not quiet:\n        jc.utils.warning_message('This parser is an alias to the Key/Value parser (`--kv`)')\n\n    if raw:\n        return jc.parsers.kv.parse(data, raw=True)\n\n    raw_output = jc.parsers.kv.parse(data, raw=True)\n    return _process(raw_output)", "idx": 563}
{"namespace": "jc.parsers.xrandr._parse_screen", "completion": "    if not next_lines:\n        return None\n\n    next_line = next_lines.pop()\n    result = re.match(_screen_pattern, next_line)\n    if not result:\n        next_lines.append(next_line)\n        return None\n\n    matches = result.groupdict()\n\n    screen: Screen = {\n        \"screen_number\": int(matches[\"screen_number\"]),\n        \"minimum_width\": int(matches[\"minimum_width\"]),\n        \"minimum_height\": int(matches[\"minimum_height\"]),\n        \"current_width\": int(matches[\"current_width\"]),\n        \"current_height\": int(matches[\"current_height\"]),\n        \"maximum_width\": int(matches[\"maximum_width\"]),\n        \"maximum_height\": int(matches[\"maximum_height\"]),\n        \"devices\": [],\n    }\n\n    while next_lines:\n        next_line = next_lines.pop()\n        next_device: Optional[Device] = _parse_device([next_line])\n        if next_device:\n            screen[\"devices\"].append(next_device)\n        else:\n            next_lines.append(next_line)\n            break\n\n    return screen", "idx": 564}
{"namespace": "jc.parsers.xrandr._parse_model", "completion": "    if not next_lines:\n        return None\n\n    next_line = next_lines.pop()\n    result = re.match(_edid_head_pattern, next_line)\n    if not result:\n        next_lines.append(next_line)\n        return None\n\n    edid_lines = []\n    while next_lines:\n        next_line = next_lines.pop()\n        result = re.match(_edid_line_pattern, next_line)\n        if result:\n            edid_lines.append(result.group(\"edid_line\"))\n        else:\n            next_lines.append(next_line)\n            break\n\n    if not edid_lines:\n        return None\n\n    edid_hex = \"\".join(edid_lines)\n    edid_bytes = bytes.fromhex(edid_hex)\n\n    model: Model = {\n        \"name\": edid_bytes[0x08:0x0a].decode(\"utf-8\"),\n        \"product_id\": edid_bytes[0x0a:0x0e].hex(),\n        \"serial_number\": edid_bytes[0x0e:0x18].decode(\"utf-8\"),\n    }\n\n    return model", "idx": 565}
{"namespace": "jc.parsers.xrandr._parse_mode", "completion": "    result = re.match(_mode_pattern, line)\n    if not result:\n        return None\n\n    raw_matches = result.groupdict()\n\n    mode: Mode = {\n        \"resolution_width\": int(raw_matches[\"resolution_width\"]),\n        \"resolution_height\": int(raw_matches[\"resolution_height\"]),\n        \"is_high_resolution\": raw_matches[\"is_high_resolution\"] == \"i\",\n        \"frequencies\": []\n    }\n\n    frequencies = re.search(_frequencies_pattern, raw_matches[\"rest\"])\n    if frequencies:\n        frequency_values = frequencies.group(\"frequency\").split()\n        frequency_stars = frequencies.group(\"star\").split()\n        frequency_pluses = frequencies.group(\"plus\").split()\n\n        for i, value in enumerate(frequency_values):\n            frequency: Frequency = {\n                \"frequency\": float(value),\n                \"is_current\": frequency_stars[i] == \"*\",\n                \"is_preferred\": frequency_pluses[i] == \"+\"\n            }\n            mode[\"frequencies\"].append(frequency)\n\n    return mode", "idx": 566}
{"namespace": "pythonforandroid.archs.Arch.include_dirs", "completion": "        return [\n            self.ctx.ndk.sysroot_include_dir,\n            join(\n                self.ctx.get_python_install_dir(self.arch),\n                'include/python{}'.format(self.ctx.python_recipe.version[0:3]),\n            )\n        ]", "idx": 567}
{"namespace": "pythonforandroid.archs.Arch.target", "completion": "        target_data = self.command_prefix.split('-')\n        return '{triplet}{ndk_api}'.format(\n            triplet='-'.join([target_data[0], target_data[1]]),\n            ndk_api=self.ctx.ndk_api,\n        )", "idx": 568}
{"namespace": "pythonforandroid.archs.ArchARM.target", "completion": "        return '{triplet}{ndk_api}'.format(\n            triplet=self.command_prefix, ndk_api=self.ctx.ndk_api\n        )", "idx": 569}
{"namespace": "pythonforandroid.recipe.Recipe.get_recipe", "completion": "        if hasattr(cls, '_recipe_cache'):\n            if name in cls._recipe_cache:\n                return cls._recipe_cache[name]\n\n        for recipes_dir in cls.recipe_dirs(ctx):\n            if exists(join(recipes_dir, name)):\n                module = __import__(f'{split(name)[1]}', globals(), locals(), ['*'])\n                for attr in dir(module):\n                    attr_value = getattr(module, attr)\n                    if inspect.isclass(attr_value) and issubclass(attr_value, Recipe) and attr_value is not Recipe:\n                        recipe = attr_value(ctx)\n                        if not hasattr(cls, '_recipe_cache'):\n                            cls._recipe_cache = {}\n                        cls._recipe_cache[name] = recipe\n                        return recipe\n\n        raise ValueError(f'Recipe {name} not found')", "idx": 570}
{"namespace": "pythonforandroid.prerequisites.HomebrewPrerequisite.darwin_helper", "completion": "        info(\n            \"The installer for Homebrew is not supported on macOS. Please follow the instructions at https://brew.sh/ to install Homebrew on macOS.\"\n        )", "idx": 571}
{"namespace": "msticpy.config.query_editor.MetadataEditWidget.save_metadata", "completion": "        \"\"\"\n        This function saves the values from the widgets to the Metadata object. It retrieves the values from various widgets and assigns them to the corresponding attributes of the Metadata object.\n        Input-Output Arguments\n        :param self: MetadataEditWidget. An instance of the MetadataEditWidget class.\n        :param button: The button that triggered the save action. Not used in the function.\n        :return: None.\n        \"\"\"\n        self.metadata.version = self.version_widget.value\n        self.metadata.description = self.description_widget.value\n        self.metadata.data_environments = list(self.data_env_widget.value)\n        self.metadata.data_families = [\n            family.strip() for family in self.data_families_widget.value.split(\",\")\n        ]\n        self.metadata.database = self.database_widget.value\n        self.metadata.cluster = self.cluster_widget.value\n        self.metadata.clusters = [\n            cluster.strip() for cluster in self.clusters_widget.value.split(\"\\n\")\n        ]\n        self.metadata.cluster_groups = [\n            group.strip() for group in self.cluster_groups_widget.value.split(\"\\n\")\n        ]\n        self.metadata.tags = [\n            tag.strip() for tag in self.tags_widget.value.split(\",\")\n        ]\n        self.metadata.data_source = self.data_source_widget.value", "idx": 572}
{"namespace": "msticpy.config.query_editor.QueryEditor._save_file", "completion": "        \"\"\"\n        This function saves the current query collection to a YAML file. It takes the query collection and the current file path as input parameters and saves the collection to the specified file.\n        Input-Output Arguments\n        :param self: QueryEditor. An instance of the QueryEditor class.\n        :param button: The button that triggered the save action. Not used in the function.\n        :return: No return values.\n        \"\"\"\n        if not self.current_file:\n            print(\"No file specified for saving.\")\n            return\n        save_queries_to_yaml(self.query_collection, self.current_file)", "idx": 573}
{"namespace": "msticpy.config.query_editor.QueryEditor._unsaved_changes", "completion": "        return (\n            self.default_param_editor.changed_data\n            or self.metadata_editor.changed_data\n            or self.query_editor.changed_data\n        )", "idx": 574}
{"namespace": "msticpy.config.query_editor.load_queries_from_yaml", "completion": "    yaml_data = Path(yaml_file).read_text(encoding=\"utf-8\")\n    query_dict = yaml.safe_load(yaml_data)\n    query_dict = _rename_data_type(query_dict)\n    query_dict = _remove_none_values(query_dict)\n    query_dict[\"file_name\"] = yaml_file\n    query_collection = QueryCollection(\n        file_name=query_dict[\"file_name\"],\n        metadata=QueryMetadata(**query_dict[\"metadata\"]),\n        defaults=_create_query_defaults(query_dict[\"defaults\"]),\n        sources={\n            name: _create_query(query_data)\n            for name, query_data in query_dict[\"sources\"].items()\n        },\n    )\n    return query_collection", "idx": 575}
{"namespace": "zxcvbn.time_estimates.estimate_attack_times", "completion": "    crack_times_seconds = {\n        'online_throttling_100_per_hour': guesses / 100 / 3600,\n        'online_no_throttling_10_per_second': guesses / 10,\n        'offline_slow_hashing_1e4_per_second': guesses / 1e4,\n        'offline_fast_hashing_1e10_per_second': guesses / 1e10\n    }\n\n    crack_times_display = {\n        'online_throttling_100_per_hour': display_time(crack_times_seconds['online_throttling_100_per_hour']),\n        'online_no_throttling_10_per_second': display_time(crack_times_seconds['online_no_throttling_10_per_second']),\n        'offline_slow_hashing_1e4_per_second': display_time(crack_times_seconds['offline_slow_hashing_1e4_per_second']),\n        'offline_fast_hashing_1e10_per_second': display_time(crack_times_seconds['offline_fast_hashing_1e10_per_second'])\n    }\n\n    score = guesses_to_score(guesses)\n\n    return {\n        'crack_times_seconds': crack_times_seconds,\n        'crack_times_display': crack_times_display,\n        'score': score\n    }", "idx": 576}
{"namespace": "zxcvbn.scoring.estimate_guesses", "completion": "    if 'guesses' in match:\n        return match['guesses']\n\n    min_guesses = len(password)\n\n    if match['pattern'] == 'bruteforce':\n        match['guesses'] = bruteforce_guesses(match)\n    elif match['pattern'] == 'dictionary':\n        match['guesses'] = dictionary_guesses(match)\n    elif match['pattern'] == 'repeat':\n        match['guesses'] = repeat_guesses(match)\n    elif match['pattern'] == 'sequence':\n        match['guesses'] = sequence_guesses(match)\n    elif match['pattern'] == 'regex':\n        match['guesses'] = regex_guesses(match)\n    elif match['pattern'] == 'date':\n        match['guesses'] = date_guesses(match)\n    elif match['pattern'] in ['spatial', 'graph']:\n        match['guesses'] = spatial_guesses(match)\n\n    return match['guesses']", "idx": 577}
{"namespace": "zxcvbn.scoring.dictionary_guesses", "completion": "    match['base_guesses'] = match['rank']\n    match['uppercase_variations'] = uppercase_variations(match)\n    match['l33t_variations'] = l33t_variations(match)\n    match['reversed_variations'] = 1 if match['reversed'] else 2\n\n    return match['base_guesses'] * match['uppercase_variations'] * match['l33t_variations'] * match['reversed_variations']", "idx": 578}
{"namespace": "zxcvbn.scoring.regex_guesses", "completion": "    character_class_bases = {\n        'alpha_lower': 26,\n        'alpha_upper': 26,\n        'alpha': 52,\n        'alphanumeric': 62,\n        'digits': 10,\n        'symbols': 33\n    }\n\n    if match['regex_class'] in character_class_bases:\n        return character_class_bases[match['regex_class']] ** len(match['token'])\n    else:\n        return 1  # Default guess value if the regex class is not recognized", "idx": 579}
{"namespace": "zxcvbn.scoring.date_guesses", "completion": "    year_space = abs(int(match['year']) - REFERENCE_YEAR)\n    year_space = max(year_space, MIN_YEAR_SPACE)\n\n    if match['separator']:\n        return year_space * 12 * 31  # Assuming 12 months and 31 days\n    else:\n        return year_space", "idx": 580}
{"namespace": "zxcvbn.scoring.spatial_guesses", "completion": "    token = match['token']\n    graph_type = match['graph']\n    turns = match['turns']\n    shifted_count = match['shifted_count']\n\n    if graph_type == 'qwerty':\n        starting_positions = KEYBOARD_STARTING_POSITIONS\n        average_degree = KEYBOARD_AVERAGE_DEGREE\n    elif graph_type == 'dvorak':\n        starting_positions = KEYBOARD_STARTING_POSITIONS  # assuming same as qwerty\n        average_degree = KEYBOARD_AVERAGE_DEGREE\n    elif graph_type == 'keypad':\n        starting_positions = KEYPAD_STARTING_POSITIONS\n        average_degree = KEYPAD_AVERAGE_DEGREE\n    else:\n        raise ValueError(\"Invalid graph type\")\n\n    s = len(token)\n    possible_guesses = 0\n\n    for i in range(1, turns + 1):\n        possible_guesses += nCk(s, i) * starting_positions * (average_degree ** i)\n\n    # additional guesses for shifted keys\n    if shifted_count > 0:\n        shifted_guesses = 0\n        for i in range(1, shifted_count + 1):\n            shifted_guesses += nCk(s, i) * starting_positions * (average_degree ** i)\n        possible_guesses += shifted_guesses\n\n    return possible_guesses", "idx": 581}
{"namespace": "zxcvbn.scoring.uppercase_variations", "completion": "    word = match['token']\n    if word.islower() or word.isalpha():\n        return 1\n    elif word[0].isupper() or word[-1].isupper() or word.isupper():\n        return 2\n    else:\n        upper_count = sum(1 for char in word if char.isupper())\n        lower_count = sum(1 for char in word if char.islower())\n        variations = nCk(upper_count + lower_count, upper_count)\n        return variations", "idx": 582}
{"namespace": "zxcvbn.matching.dictionary_match", "completion": "    matches = []\n    for i in range(len(password)):\n        for j in range(i, len(password)):\n            word = password[i:j + 1]\n            if word.lower() in _ranked_dictionaries:\n                rank = _ranked_dictionaries[word.lower()]\n                matches.append({\n                    'pattern': 'dictionary',\n                    'i': i,\n                    'j': j,\n                    'token': password[i:j + 1],\n                    'matched_word': word,\n                    'rank': rank,\n                })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))", "idx": 583}
{"namespace": "zxcvbn.matching.reverse_dictionary_match", "completion": "    matches = []\n    length = len(password)\n    password_lower = password.lower()\n    for dictionary_name, ranked_dict in _ranked_dictionaries.items():\n        reversed_password = password_lower[::-1]\n        for i in range(length):\n            for j in range(i, length):\n                if reversed_password[i:j + 1] in ranked_dict:\n                    word = reversed_password[i:j + 1]\n                    rank = ranked_dict[word]\n                    original_word = word[::-1]\n                    original_i = length - j - 1\n                    original_j = length - i - 1\n                    matches.append({\n                        'pattern': 'reverse_dictionary',\n                        'i': original_i,\n                        'j': original_j,\n                        'token': password[original_i:original_j + 1],\n                        'matched_word': original_word,\n                        'rank': rank,\n                        'dictionary_name': dictionary_name,\n                        'reversed': True,\n                        'l33t': False,\n                    })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))", "idx": 584}
{"namespace": "zxcvbn.matching.l33t_match", "completion": "    matches = []\n    for sub in enumerate_l33t_subs(relevant_l33t_subtable(password, _l33t_table)):\n        subbed_password = translate(password, sub)\n        for match in dictionary_match(subbed_password, _ranked_dictionaries):\n            token = password[match['i']:match['j'] + 1]\n            if token.lower() == match['matched_word']:\n                continue\n            matches.append({\n                'pattern': 'dictionary',\n                'i': match['i'],\n                'j': match['j'],\n                'token': token,\n                'matched_word': match['matched_word'],\n                'rank': match['rank'],\n                'dictionary_name': match['dictionary_name'],\n                'reversed': False,\n                'l33t': True,\n                'sub': sub,\n            })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))", "idx": 585}
{"namespace": "zxcvbn.matching.repeat_match", "completion": "    matches = []\n    greedy_re = re.compile(r'(.+)\\1+')\n    lazy_re = re.compile(r'(.+?)\\1+')\n    for match in greedy_re.finditer(password):\n        base_token = match.group(1)\n        repeat_count = len(match.group(0)) // len(base_token)\n        matches.append({\n            'pattern': 'repeat',\n            'i': match.start(),\n            'j': match.end() - 1,\n            'token': match.group(0),\n            'base_token': base_token,\n            'base_token_guesses': scoring.most_guessable_match_sequence(base_token, _ranked_dictionaries),\n            'base_token_sequence': scoring.omnimatch(base_token, _ranked_dictionaries),\n            'repeat_count': repeat_count\n        })\n    for match in lazy_re.finditer(password):\n        base_token = match.group(1)\n        repeat_count = len(match.group(0)) // len(base_token)\n        matches.append({\n            'pattern': 'repeat',\n            'i': match.start(),\n            'j': match.end() - 1,\n            'token': match.group(0),\n            'base_token': base_token,\n            'base_token_guesses': scoring.most_guessable_match_sequence(base_token, _ranked_dictionaries),\n            'base_token_sequence': scoring.omnimatch(base_token, _ranked_dictionaries),\n            'repeat_count': repeat_count\n        })\n    return sorted(matches, key=lambda x: (x['i'], x['j']))", "idx": 586}
{"namespace": "zxcvbn.matching.spatial_match", "completion": "    matches = []\n    for graph_name, graph in _graphs.items():\n        matches.extend(spatial_match_helper(password, graph, graph_name))\n    return sorted(matches, key=lambda x: (x['i'], x['j']))", "idx": 587}
{"namespace": "zxcvbn.matching.sequence_match", "completion": "\n    matches = []\n    i = 0\n    while i < len(password) - 1:\n        j = i + 1\n        last_delta = ord(password[j]) - ord(password[i])\n        while j < len(password):\n            delta = ord(password[j]) - ord(password[j - 1])\n            if delta == last_delta:\n                j += 1\n            else:\n                if j - i > 2:  # only consider sequences of length 3 or more\n                    token = password[i:j]\n                    sequence_name = \"other\"\n                    sequence_space = \"other\"\n                    if token.isalpha():\n                        sequence_name = \"letters\"\n                        if token.islower():\n                            sequence_space = \"lower\"\n                        elif token.isupper():\n                            sequence_space = \"upper\"\n                    elif token.isdigit():\n                        sequence_name = \"digits\"\n                        sequence_space = \"digits\"\n                    matches.append({\n                        'pattern': 'sequence',\n                        'i': i,\n                        'j': j - 1,\n                        'token': token,\n                        'sequence_name': sequence_name,\n                        'sequence_space': sequence_space,\n                        'ascending': delta > 0\n                    })\n                i = j\n                break\n    return matches", "idx": 588}
{"namespace": "zxcvbn.matching.regex_match", "completion": "    matches = []\n    for regex_name, regex_pattern in _regexen.items():\n        for match in regex_pattern.finditer(password):\n            matches.append({\n                'pattern': 'regex',\n                'token': match.group(),\n                'i': match.start(),\n                'j': match.end() - 1,\n                'regex_name': regex_name,\n                'regex_match_object': match\n            })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))", "idx": 589}
{"namespace": "OpenSSL.rand.add", "completion": "    _lib.RAND_add(buffer, entropy)", "idx": 590}
{"namespace": "asyncssh.kex.register_kex_alg", "completion": "\n    _kex_algs.append(alg)\n\n    if default:\n        _default_kex_algs.append(alg)\n\n    _kex_handlers[alg] = (handler, hash_alg, args)", "idx": 591}
{"namespace": "asyncssh.auth.get_supported_server_auth_methods", "completion": "\n    supported_methods = []\n    for method, handler in _server_auth_handlers.items():\n        if handler.supported(conn):\n            supported_methods.append(method)\n\n    return supported_methods", "idx": 592}
{"namespace": "asyncssh.mac.get_mac", "completion": "    handler, hash_size, args = _mac_handler[mac_alg]\n    return handler(key, hash_size, *args)", "idx": 593}
{"namespace": "asyncssh.auth_keys.SSHAuthorizedKeys.validate", "completion": "        entries = self._ca_entries if ca else self._user_entries\n\n        for entry in entries:\n            if entry.key == key:\n                if entry.match_options(client_host, client_addr, cert_principals):\n                    return entry.options\n\n        return None", "idx": 594}
{"namespace": "asyncssh.saslprep.saslprep", "completion": "\n    return _stringprep(s, True, _map_saslprep, 'NFKC', [stringprep.in_table_c12], True)", "idx": 595}
{"namespace": "asyncssh.asn1.der_decode", "completion": "    value, end = der_decode_partial(data)\n    if end < len(data):\n        raise ASN1DecodeError('Data contains unexpected bytes at end')\n    return value", "idx": 596}
{"namespace": "asyncssh.packet.SSHPacket.check_end", "completion": "        if self._idx != self._len:\n            raise PacketDecodeError('Incomplete packet')", "idx": 597}
{"namespace": "asyncssh.public_key.SSHKey.verify", "completion": "\n        if not self.use_executor:\n            return self._key.verify(data, sig, self.sig_algorithm)\n        else:\n            raise NotImplementedError(\"Verification using executor is not supported\")", "idx": 598}
{"namespace": "asyncssh.public_key.SSHKey.convert_to_public", "completion": "        public_key = SSHKey(self.key_algorithm, self.sig_algorithm, self.sig_algorithms, self.host_key_algorithms, self.key_public_data, self.get_comment_bytes(), use_executor=self.use_executor)\n        return public_key", "idx": 599}
{"namespace": "asyncssh.public_key.SSHKey.generate_x509_user_certificate", "completion": "        return self._generate_x509_certificate(user_key, subject, issuer,\n                                               serial, valid_after,\n                                               valid_before, False, None,\n                                               purposes, (), principals,\n                                               hash_alg, comment)", "idx": 600}
{"namespace": "asyncssh.misc.write_file", "completion": "    with open_file(filename, mode) as f:\n        return f.write(data)", "idx": 601}
{"namespace": "diffprivlib.accountant.BudgetAccountant.__repr__", "completion": "        epsilon_str = f\"epsilon={self.epsilon}\" if self.epsilon != float(\"inf\") else \"\"\n        delta_str = f\"delta={self.delta}\" if self.delta != 1.0 else \"\"\n        slack_str = f\"slack={self.slack}\" if self.slack > 0 else \"\"\n\n        spent_budget_str = \", \".join([f\"({epsilon}, {delta})\" for epsilon, delta in self.spent_budget[:n_budget_max]])\n        if len(self.spent_budget) > n_budget_max:\n            spent_budget_str += \", ...\"\n\n        return f\"BudgetAccountant({', '.join(filter(None, [epsilon_str, delta_str, slack_str, spent_budget_str]))})\"", "idx": 602}
{"namespace": "diffprivlib.accountant.BudgetAccountant.check", "completion": "        remaining_epsilon, remaining_delta = self.remaining()\n        \n        if epsilon > remaining_epsilon or delta > remaining_delta:\n            raise BudgetError(f\"Privacy spend of ({epsilon},{delta}) not permissible; will exceed remaining privacy budget. Use {self.__class__.__name__}.remaining() to check remaining budget.\")\n        \n        return True", "idx": 603}
{"namespace": "diffprivlib.accountant.BudgetAccountant.spend", "completion": "        if not self.check(epsilon, delta):\n            raise BudgetError(f\"Privacy spend of ({epsilon},{delta}) not permissible; will exceed remaining privacy budget.\"\n                              f\" Use {self.__class__.__name__}.{self.remaining.__name__}() to check remaining budget.\")\n\n        self.__spent_budget.append((epsilon, delta))\n        return self", "idx": 604}
{"namespace": "diffprivlib.accountant.BudgetAccountant.load_default", "completion": "        if accountant is None:\n            return BudgetAccountant._default\n        elif isinstance(accountant, BudgetAccountant):\n            return accountant\n        else:\n            raise TypeError(\"The supplied accountant must be an instance of the BudgetAccountant class\")", "idx": 605}
{"namespace": "diffprivlib.accountant.BudgetAccountant.set_default", "completion": "        BudgetAccountant._default = self\n        return self", "idx": 606}
{"namespace": "diffprivlib.accountant.BudgetAccountant.pop_default", "completion": "        default = BudgetAccountant._default\n        BudgetAccountant._default = None\n        return default", "idx": 607}
{"namespace": "diffprivlib.validation.clip_to_bounds", "completion": "    if not isinstance(array, np.ndarray):\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\n    if not isinstance(bounds, tuple):\n        raise TypeError(f\"Bounds must be specified as a tuple of (min, max), got {type(bounds)}.\")\n    if not isinstance(shape, Integral):\n        raise TypeError(f\"shape parameter must be integer-valued, got {type(shape)}.\")\n\n    lower, upper = bounds\n\n    if np.asarray(lower).size == 1 or np.asarray(upper).size == 1:\n        lower = np.ravel(lower).astype(dtype)\n        upper = np.ravel(upper).astype(dtype)\n    else:\n        lower = np.asarray(lower, dtype=dtype)\n        upper = np.asarray(upper, dtype=dtype)\n\n    if lower.shape != upper.shape:\n        raise ValueError(\"lower and upper bounds must be the same shape array\")\n    if lower.ndim > 1:\n        raise ValueError(\"lower and upper bounds must be scalar or a 1-dimensional array\")\n    if lower.size not in (1, shape):\n        raise ValueError(f\"lower and upper bounds must have {shape or 1} element(s), got {lower.size}.\")\n\n    n_bounds = lower.shape[0]\n\n    for i in range(n_bounds):\n        _lower = lower[i]\n        _upper = upper[i]\n\n        if not isinstance(_lower, Real) or not isinstance(_upper, Real):\n            raise TypeError(f\"Each bound must be numeric, got {_lower} ({type(_lower)}) and {_upper} ({type(_upper)}).\")\n\n        if _lower > _upper:\n            raise ValueError(f\"For each bound, lower bound must be smaller than upper bound, got {lower}, {upper})\")\n\n        if _upper - _lower < min_separation:\n            mid = (_upper + _lower) / 2\n            lower[i] = mid - min_separation / 2\n            upper[i] = mid + min_separation / 2\n\n    if shape == 0:\n        return lower.item(), upper.item()\n\n    if n_bounds == 1:\n        lower = np.ones(shape, dtype=dtype) * lower.item()\n        upper = np.ones(shape, dtype=dtype) * upper.item()\n\n    return np.clip(array, lower, upper)", "idx": 608}
{"namespace": "diffprivlib.models.naive_bayes.GaussianNB._update_mean_variance", "completion": "        n_new = X.shape[0]\n\n        if n_noisy is not None:\n            n_new = n_noisy\n\n        if n_past == 0:\n            return np.mean(X, axis=0), np.var(X, axis=0)\n\n        total_mu = (n_past * mu + n_new * np.mean(X, axis=0)) / (n_past + n_new)\n        total_var = (n_past * (var + mu ** 2) + n_new * (np.var(X, axis=0) + np.mean(X, axis=0) ** 2)) / (n_past + n_new) - total_mu ** 2\n\n        return total_mu, total_var", "idx": 609}
{"namespace": "diffprivlib.models.naive_bayes.GaussianNB._noisy_class_counts", "completion": "        unique_y, counts = np.unique(y, return_counts=True)\n        noisy_counts = []\n\n        for count in counts:\n            mech = LaplaceBoundedDomain(epsilon=self.epsilon, delta=0, sensitivity=1, lower=0, upper=count, random_state=random_state)\n            noisy_count = mech.randomise(count)\n            noisy_counts.append(noisy_count)\n\n        return np.array(noisy_counts)", "idx": 610}
{"namespace": "diffprivlib.models.standard_scaler._incremental_mean_and_var", "completion": "    # Initialising new accountant, as budget is tracked in main class. Subject to review in line with GH issue #21\n    if random_state is None:\n        random_state = np.random\n\n    n_samples = X.shape[0]\n    n_features = X.shape[1]\n\n    if last_mean is None:\n        last_mean = np.zeros(n_features)\n\n    if last_variance is None:\n        last_variance = np.zeros(n_features)\n\n    if last_sample_count is None:\n        last_sample_count = np.zeros(n_features)\n\n    for i in range(n_samples):\n        for j in range(n_features):\n            x = X[i, j]\n\n            if np.isnan(x):\n                continue\n\n            last_sample_count[j] += 1\n            delta = x - last_mean[j]\n            last_mean[j] += delta / last_sample_count[j]\n            delta2 = x - last_mean[j]\n            last_variance[j] += delta * delta2\n\n    return last_mean, last_variance, last_sample_count", "idx": 611}
{"namespace": "diffprivlib.models.linear_regression.LinearRegression.fit", "completion": "        X, y, X_offset, y_offset, X_scale = self._preprocess_data(X, y, self.fit_intercept, epsilon=self.epsilon,\n                                                                 bounds_X=self.bounds_X, bounds_y=self.bounds_y, copy=True,\n                                                                 check_input=True, random_state=self.random_state)\n\n        n_samples, n_features = X.shape\n        n_targets = y.shape[1] if y.ndim > 1 else 1\n\n        if sample_weight is not None:\n            warnings.warn(\"Sample weights are ignored in diffprivlib. Present for consistency with the sklearn API.\",\n                          UserWarning)\n\n        if n_targets == 1:\n            y = y.ravel()\n\n        if n_targets == 1:\n            def fun_to_minimize(w):\n                return np.sum((np.dot(X, w) - y) ** 2) + self.alpha * np.dot(w, w)\n        else:\n            def fun_to_minimize(w):\n                return np.sum((np.dot(X, w) - y) ** 2) + self.alpha * np.sum(w ** 2)\n\n        if n_targets == 1:\n            def jac_to_minimize(w):\n                return 2 * np.dot(X.T, np.dot(X, w) - y) + 2 * self.alpha * w\n        else:\n            def jac_to_minimize(w):\n                return 2 * np.dot(X.T, np.dot(X, w) - y) + 2 * self.alpha * w\n\n        if n_targets == 1:\n            def hess_to_minimize(w):\n                return 2 * np.dot(X.T, X) + 2 * self.alpha * np.eye(n_features)\n        else:\n            def hess_to_minimize(w):\n                return 2 * np.dot(X.T, X) + 2 * self.alpha * np.eye(n_features)\n\n        if n_targets == 1:\n            w0 = np.zeros(n_features)\n        else:\n            w0 = np.zeros((n_targets, n_features))\n\n        bounds = [(None, None)] * n_features\n\n        if n_targets == 1:\n            res = minimize(fun_to_minimize, w0, method=\"trust-constr\", jac=jac_to_minimize, hess=hess_to_minimize,\n                           bounds=bounds)\n            self.coef_ = res.x\n        else:\n            self.coef_ = np.zeros((n_targets, n_features))\n            for i in range(n_targets):\n                res = minimize(fun_to_minimize, w0[i], method=\"trust-constr\", jac=jac_to_minimize, hess=hess_to_minimize,\n                               bounds=bounds)\n                self.coef_[i] = res.x\n\n        if self.fit_intercept:\n            self.intercept_ = y_offset - np.dot(X_offset, self.coef_.T)\n        else:\n            self.intercept_ = 0.0\n\n        self.accountant.spend(epsilon, 0)\n\n        return self", "idx": 612}
{"namespace": "diffprivlib.models.k_means.KMeans.fit", "completion": "        if self.bounds is None:\n            warnings.warn(\"Bounds not provided.  Privacy will not be ensured unless bounds are provided at fit time.\",\n                          category=UserWarning)\n\n        X = self._check_fit_data(X)\n\n        random_state = np.random.RandomState(self.random_state)\n\n        n_samples, n_dims = X.shape\n        total_iters = self._calc_iters(n_dims, n_samples)\n\n        self.cluster_centers_ = self._init_centers(n_dims, random_state)\n\n        if self.cluster_centers_ is None:\n            raise ValueError(\"Failed to initialise cluster centers.  This is likely due to the data not being bounded.\")\n\n        for _ in range(total_iters):\n            distances, self.labels_ = self._distances_labels(X, self.cluster_centers_)\n            self.cluster_centers_ = self._update_centers(X, self.cluster_centers_, self.labels_, n_dims, total_iters,\n                                                         random_state)\n\n        self.inertia_ = np.sum(np.min(distances, axis=1))\n        self.n_iter_ = total_iters\n\n        return self", "idx": 613}
{"namespace": "diffprivlib.models.forest._FittingTree.__getstate__", "completion": "        state = {\n            \"max_depth\": self.max_depth,\n            \"node_count\": self.node_count,\n            \"nodes\": self.nodes,\n            \"values\": self.values_\n        }\n        return state", "idx": 614}
{"namespace": "diffprivlib.models.forest._FittingTree.fit", "completion": "        if not self.nodes:\n            self.build()\n\n        leaves = self.apply(X)\n        unique_leaves, counts = np.unique(leaves, return_counts=True)\n\n        self.values_ = np.zeros((self.node_count, len(self.classes)), dtype=int)\n\n        for leaf, count in zip(unique_leaves, counts):\n            if self.nodes[leaf].left_child == self._TREE_LEAF:\n                self.values_[leaf] = np.bincount(y[leaves == leaf], minlength=len(self.classes))\n\n        empty_leaves = np.where(counts == 0)[0]\n\n        for leaf in empty_leaves:\n            self.values_[leaf] = np.ones(len(self.classes))\n\n        return self", "idx": 615}
{"namespace": "diffprivlib.tools.histograms.histogram", "completion": "    warn_unused_args(unused_args)\n\n    random_state = check_random_state(random_state)\n\n    accountant = BudgetAccountant.load_default(accountant)\n    accountant.check(epsilon, 0)\n\n    # Range only required if bin edges not specified\n    if np.array(bins, dtype=object).ndim == 0 or not np.all([np.ndim(_bin) for _bin in bins]):\n        if range is None or (isinstance(range, list) and None in range):\n            warnings.warn(\"Range parameter has not been specified (or has missing elements). Falling back to taking \"\n                          \"range from the data.\\n \"\n                          \"To ensure differential privacy, and no additional privacy leakage, the range must be \"\n                          \"specified for each dimension independently of the data (i.e., using domain knowledge).\",\n                          PrivacyLeakWarning)\n\n    hist, bin_edges = np.histogram(sample, bins=bins, range=range, weights=weights, density=None)\n\n    dp_mech = GeometricTruncated(epsilon=epsilon, sensitivity=1, lower=0, upper=maxsize, random_state=random_state)\n\n    dp_hist = np.zeros_like(hist)\n    iterator = np.nditer(hist, flags=['multi_index'])\n\n    while not iterator.finished:\n        dp_hist[iterator.multi_index] = dp_mech.randomise(int(iterator[0]))\n        iterator.iternext()\n\n    dp_hist = dp_hist.astype(float, casting='safe')\n\n    if density:\n        # calculate the probability density function\n        dims = len(dp_hist.shape)\n        dp_hist_sum = dp_hist.sum()\n        for i in np.arange(dims):\n            shape = np.ones(dims, int)\n            shape[i] = dp_hist.shape[i]\n            # noinspection PyUnresolvedReferences\n            dp_hist = dp_hist / np.diff(bin_edges[i]).reshape(shape)\n\n        if dp_hist_sum > 0:\n            dp_hist /= dp_hist_sum\n\n    accountant.spend(epsilon, 0)\n\n    return dp_hist, bin_edges", "idx": 616}
{"namespace": "diffprivlib.tools.histograms.histogram2d", "completion": "    warn_unused_args(unused_args)\n\n    random_state = check_random_state(random_state)\n\n    accountant = BudgetAccountant.load_default(accountant)\n    accountant.check(epsilon, 0)\n\n    # Range only required if bin edges not specified\n    if np.array(bins, dtype=object).ndim == 0 or not np.all([np.ndim(_bin) for _bin in bins]):\n        if range is None or (isinstance(range, list) and None in range):\n            warnings.warn(\"Range parameter has not been specified (or has missing elements). Falling back to taking \"\n                          \"range from the data.\\n \"\n                          \"To ensure differential privacy, and no additional privacy leakage, the range must be \"\n                          \"specified for each dimension independently of the data (i.e., using domain knowledge).\",\n                          PrivacyLeakWarning)\n\n    hist, xedges, yedges = np.histogram2d(array_x, array_y, bins=bins, range=range, weights=weights, density=None)\n\n    dp_mech = GeometricTruncated(epsilon=epsilon, sensitivity=1, lower=0, upper=maxsize, random_state=random_state)\n\n    dp_hist = np.zeros_like(hist)\n    iterator = np.nditer(hist, flags=['multi_index'])\n\n    while not iterator.finished:\n        dp_hist[iterator.multi_index] = dp_mech.randomise(int(iterator[0]))\n        iterator.iternext()\n\n    dp_hist = dp_hist.astype(float, casting='safe')\n\n    if density:\n        # calculate the probability density function\n        dims = len(dp_hist.shape)\n        dp_hist_sum = dp_hist.sum()\n        for i in np.arange(dims):\n            shape = np.ones(dims, int)\n            shape[i] = dp_hist.shape[i]\n            # noinspection PyUnresolvedReferences\n            dp_hist = dp_hist / np.diff(xedges if i == 0 else yedges).reshape(shape)\n\n        if dp_hist_sum > 0:\n            dp_hist /= dp_hist_sum\n\n    accountant.spend(epsilon, 0)\n\n    return dp_hist, xedges, yedges", "idx": 617}
{"namespace": "diffprivlib.tools.utils.nanmean", "completion": "    warn_unused_args(unused_args)\n\n    return _mean(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                 random_state=random_state, accountant=accountant, nan=True)", "idx": 618}
{"namespace": "diffprivlib.tools.utils.var", "completion": "    warn_unused_args(unused_args)\n\n    return _var(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant)", "idx": 619}
{"namespace": "diffprivlib.tools.utils.nanvar", "completion": "    warn_unused_args(unused_args)\n\n    return _var(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)", "idx": 620}
{"namespace": "diffprivlib.tools.utils.std", "completion": "    warn_unused_args(unused_args)\n\n    return _std(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant)", "idx": 621}
{"namespace": "diffprivlib.tools.utils.nanstd", "completion": "    warn_unused_args(unused_args)\n\n    return _std(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)", "idx": 622}
{"namespace": "diffprivlib.tools.utils.sum", "completion": "    warn_unused_args(unused_args)\n\n    return _sum(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant)", "idx": 623}
{"namespace": "diffprivlib.tools.utils.nansum", "completion": "\n    warn_unused_args(unused_args)\n\n    return _sum(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)", "idx": 624}
{"namespace": "diffprivlib.tools.quantiles.quantile", "completion": "    warn_unused_args(unused_args)\n\n    quant = np.asarray(quant)\n\n    if np.any(quant < 0) or np.any(quant > 1):\n        raise ValueError(\"Quantiles must be between 0 and 1 inclusive\")\n\n    if random_state is not None:\n        if isinstance(random_state, int):\n            np.random.seed(random_state)\n        elif isinstance(random_state, np.random.RandomState):\n            np.random.set_state(random_state.get_state())\n        else:\n            raise ValueError(\"random_state must be an int or a RandomState instance\")\n\n    if bounds is not None:\n        a_min, a_max = bounds\n        array = np.clip(array, a_min, a_max)\n\n    if axis is not None:\n        array = np.moveaxis(array, axis, -1)\n        shape = array.shape[:-1]\n        array = array.reshape(-1, array.shape[-1])\n    else:\n        shape = array.shape\n        array = array.ravel()\n\n    if accountant is not None:\n        accountant.check(epsilon, len(array))\n\n    scale = np.abs(np.max(array) - np.min(array)) / epsilon\n    sensitivity = 2 * scale / len(array)\n\n    mech = Exponential(scale=sensitivity, epsilon=epsilon / 2)\n    quantiles = np.quantile(array, quant)\n\n    if axis is not None:\n        quantiles = quantiles.reshape(shape)\n    else:\n        quantiles = quantiles.item()\n\n    return quantiles", "idx": 625}
{"namespace": "diffprivlib.tools.quantiles.percentile", "completion": "    warn_unused_args(unused_args)\n\n    return quantile(array, percent / 100, epsilon=epsilon, bounds=bounds, axis=axis, keepdims=keepdims, random_state=random_state,\n                    accountant=accountant)", "idx": 626}
{"namespace": "diffprivlib.mechanisms.base.bernoulli_neg_exp", "completion": "    if gamma < 0:\n        raise ValueError(\"Gamma must be non-negative\")\n\n    rng = check_random_state(random_state, True)\n    return int(rng.uniform(0, 1) < 1 - math.exp(-gamma))", "idx": 627}
{"namespace": "discord.utils.snowflake_time", "completion": "    return datetime.datetime.utcfromtimestamp(((id >> 22) + 1420070400000) / 1000)", "idx": 628}
{"namespace": "discord.utils.time_snowflake", "completion": "    timestamp = int(dt.timestamp() * 1000) - DISCORD_EPOCH\n    if high:\n        timestamp = timestamp << 22\n    return timestamp", "idx": 629}
{"namespace": "discord.utils.resolve_invite", "completion": "    if isinstance(invite, Invite):\n        return ResolvedInvite(code=invite.code, event=invite.guild.id)\n    else:\n        code = resolve_template(invite)\n        return ResolvedInvite(code=code, event=None)", "idx": 630}
{"namespace": "discord.utils.resolve_annotation", "completion": "\n    if annotation is None:\n        return type(None)\n\n    if isinstance(annotation, str):\n        annotation = ForwardRef(annotation)\n\n    namespace = localns if localns is not None else globalns\n\n    if cache is None:\n        cache = {}\n\n    return evaluate_annotation(annotation, globalns, namespace, cache)", "idx": 631}
{"namespace": "discord.ext.tasks.loop", "completion": "\n    def decorator(coro: LF) -> Loop[LF]:\n        return Loop(coro, seconds, hours, minutes, time, count, reconnect)\n\n    return decorator", "idx": 632}
{"namespace": "barf.analysis.gadgets.classifier.GadgetClassifier.classify", "completion": "                        import traceback\n\n        classified_gadgets = []\n\n        for gadget_type, classifier in self._classifiers.items():\n            try:\n                classified = self._classify(gadget, classifier, gadget_type, self._emu_iters)\n                classified_gadgets.extend(classified)\n            except Exception as e:\n                print(\"Error during classification: {}\".format(e))\n                import traceback\n                traceback.print_exc()\n\n        # Sort classified gadgets by their string representation.\n        classified_gadgets.sort(key=lambda x: str(x))\n\n        return classified_gadgets", "idx": 633}
{"namespace": "barf.analysis.gadgets.finder.GadgetFinder.find", "completion": "        self._max_bytes = byte_depth\n        self._instrs_depth = instrs_depth\n\n        if self._architecture == ARCH_X86:\n            candidates = self._find_x86_candidates(start_address, end_address)\n        elif self._architecture == ARCH_ARM:\n            candidates = self._find_arm_candidates(start_address, end_address)\n        else:\n            raise Exception(\"Architecture not supported.\")\n\n        candidates.sort(key=lambda x: x.address)\n\n        return candidates", "idx": 634}
{"namespace": "barf.core.reil.parser.ReilParser.parse", "completion": "        parsed_instrs = []\n\n        for instr in instrs:\n            instr_lower = instr.lower()\n\n            if instr_lower in self._cache:\n                parsed_instr = copy.deepcopy(self._cache[instr_lower])\n            else:\n                try:\n                    parsed_instr = instruction.parseString(instr_lower)[0]\n                    self._cache[instr_lower] = copy.deepcopy(parsed_instr)\n                except Exception as e:\n                    logger.error(\"Error parsing instruction: %s\", instr)\n                    continue\n\n            parsed_instrs.append(parsed_instr)\n\n        return parsed_instrs", "idx": 635}
{"namespace": "barf.core.smt.smtfunction.zero_extend", "completion": "    assert type(s) in (Constant, BitVec) and size - s.size >= 0\n\n    if size == s.size:\n        return s\n\n    return BitVec(size, \"(_ zero_extend {})\".format(size - s.size), s)", "idx": 636}
{"namespace": "barf.core.smt.smtfunction.extract", "completion": "    assert type(s) in (Constant, BitVec) and offset >= 0 and size > 0 and offset + size <= s.size\n\n    if offset == 0 and size == s.size:\n        return s\n\n    return BitVec(size, \"(_ extract {} {})\".format(offset + size - 1, offset), s)", "idx": 637}
{"namespace": "barf.core.smt.smtfunction.ite", "completion": "    assert type(cond) == bool\n    assert type(true) in (Constant, BitVec)\n    assert type(false) in (Constant, BitVec)\n\n    return BitVec(size, \"(_ ite {} {} {})\".format(cond, true, false))", "idx": 638}
{"namespace": "barf.core.smt.smtfunction.concat", "completion": "\n    if len(args) == 1:\n        return args[0]\n\n    result = args[0]\n    for i in range(1, len(args)):\n        result = BitVec(result.size + args[i].size, \"concat\", result, args[i])\n\n    return result", "idx": 639}
{"namespace": "barf.core.smt.smtsymbol.BitVecArray.declaration", "completion": "        return \"(declare-fun {} () (Array (_ BitVec {}) (_ BitVec {}))\".format(self.name, self.key_size, self.value_size)", "idx": 640}
{"namespace": "barf.arch.translator.InstructionTranslator.translate", "completion": "\n        try:\n            return self._translate(instruction)\n        except Exception as e:\n            self._log_translation_exception(instruction)\n            raise TranslationError(\"Unknown error\")", "idx": 641}
{"namespace": "barf.arch.emulator.Emulator.load_binary", "completion": "        if binary[:4] == b'\\x7fELF':\n            self._load_binary_elf(binary)\n        elif binary[:2] == b'MZ':\n            self._load_binary_pe(binary)\n        else:\n            raise Exception(\"Unknown file format.\")", "idx": 642}
{"namespace": "barf.arch.arm.parser.ArmParser.parse", "completion": "\n        instr = instr.lower()\n\n        if instr in self._cache:\n            return copy.deepcopy(self._cache[instr])\n\n        try:\n            parsed_instr = instruction.parseString(instr)[0]\n            self._cache[instr] = parsed_instr\n            return copy.deepcopy(parsed_instr)\n        except Exception as e:\n            logger.error(\"Error parsing instruction: %s\", e)\n            return None", "idx": 643}
{"namespace": "barf.arch.x86.parser.X86Parser.parse", "completion": "\n        instr = instr.lower()\n\n        if instr in self._cache:\n            return copy.deepcopy(self._cache[instr])\n\n        try:\n            parsed_instr = instruction.parseString(instr)[0]\n            self._cache[instr] = parsed_instr\n            return copy.deepcopy(parsed_instr)\n        except Exception as e:\n            logger.error(\"Failed to parse instruction: %s\", instr)\n            return None", "idx": 644}
{"namespace": "faker.utils.text.slugify", "completion": "    if allow_unicode:\n        value = unicodedata.normalize(\"NFKC\", value)\n    else:\n        value = unicodedata.normalize(\"NFKD\", value).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n    \n    if allow_dots:\n        re_pattern = _re_pattern_allow_dots\n    else:\n        re_pattern = _re_pattern\n    \n    value = re.sub(re_pattern, \"\", value).strip().lower()\n    return re.sub(_re_spaces, \"-\", value)", "idx": 645}
{"namespace": "faker.utils.checksums.calculate_luhn", "completion": "\n    number = partial_number * 10\n    check_sum = luhn_checksum(number)\n    if check_sum % 10 == 0:\n        return 0\n    else:\n        return 10 - (check_sum % 10)", "idx": 646}
{"namespace": "faker.utils.distribution.choices_distribution_unique", "completion": "    if random is None:\n        random = mod_random\n\n    if p is not None:\n        assert len(a) == len(p)\n\n    if hasattr(random, \"choices\"):\n        if length == 1 and p is None:\n            return [random.choice(a)]\n        else:\n            return random.choices(a, weights=p, k=length)\n    else:\n        choices = []\n\n        if p is None:\n            p = itertools.repeat(1, len(a))  # type: ignore\n\n        cdf = list(cumsum(p))  # type: ignore\n        normal = cdf[-1]\n        cdf2 = [i / normal for i in cdf]\n        indices = list(range(len(a)))\n        for i in range(length):\n            uniform_sample = random_sample(random=random)\n            idx = bisect.bisect_right(cdf2, uniform_sample)\n            item = a[indices.pop(idx)]\n            choices.append(item)\n        return choices", "idx": 647}
{"namespace": "faker.utils.loading.find_available_locales", "completion": "    available_locales = set()\n    for provider in providers:\n        try:\n            provider_module = import_module(provider)\n            if hasattr(provider_module, \"localized\") and provider_module.localized:\n                if hasattr(provider_module, \"languages\"):\n                    available_locales.update(provider_module.languages)\n        except ImportError:\n            pass\n    return sorted(available_locales)", "idx": 648}
{"namespace": "faker.utils.loading.find_available_providers", "completion": "    available_providers = set()\n\n    for module in modules:\n        package_name = module.__package__\n        if package_name:\n            provider_names = [package_name + '.' + name for name in list_module(module) if name != \"__pycache__\"]\n            available_providers.update(provider_names)\n\n    return sorted(available_providers)", "idx": 649}
{"namespace": "faker.providers.credit_card.Provider._generate_number", "completion": "        number = prefix\n        # Generate random digits to fill the remaining length\n        while len(number) < length - 1:\n            number += self.random_digit_not_null()\n        # Calculate the check digit using the Luhn algorithm\n        check_digit = sum(\n            int(digit) if index % 2 == 0 else self.luhn_lookup[digit]\n            for index, digit in enumerate(reversed(number))\n        )\n        check_digit = (10 - (check_digit % 10)) % 10\n        number += str(check_digit)\n        return number", "idx": 650}
{"namespace": "faker.decode.unidecode", "completion": "\n    return ''.join([codes[c] if c in codes else c for c in txt])", "idx": 651}
{"namespace": "dash.fingerprint.build_fingerprint", "completion": "    file_name, file_extension = path.rsplit(\".\", 1)\n    file_path = path.rsplit(\"/\", 1)[0]\n    v_str = version_clean.sub(\"_\", str(version))\n    fingerprint = f\"{file_path}.v{v_str}m{hash_value}.{file_extension}\"\n    return fingerprint", "idx": 652}
{"namespace": "dash.fingerprint.check_fingerprint", "completion": "    match = cache_regex.search(path)\n    if match:\n        fingerprint = match.group(0)\n        original_path = path.replace(fingerprint, \"\")\n        return original_path, True\n    else:\n        return path, False", "idx": 653}
{"namespace": "dash._configs.pages_folder_config", "completion": "    if use_pages:\n        if not pages_folder:\n            pages_folder = \"pages\"\n\n        pages_folder = os.path.join(os.getcwd(), pages_folder)\n\n        if not os.path.exists(pages_folder):\n            raise exceptions.InvalidConfig(\n                f\"The pages folder '{pages_folder}' does not exist.\"\n            )\n\n    return pages_folder", "idx": 654}
{"namespace": "dash._grouping.flatten_grouping", "completion": "    if schema is None:\n        schema = grouping\n\n    if isinstance(schema, (tuple, list)):\n        return [el for group_el in grouping for el in flatten_grouping(group_el)]\n    elif isinstance(schema, dict):\n        return [el for group_el in grouping.values() for el in flatten_grouping(group_el)]\n    else:\n        return [grouping]", "idx": 655}
{"namespace": "dash._grouping.make_grouping_by_index", "completion": "    def create_grouping(schema, flat_values):\n        if isinstance(schema, (tuple, list)):\n            return [create_grouping(s, flat_values) for s in schema]\n        elif isinstance(schema, dict):\n            return {k: create_grouping(v, flat_values) for k, v in schema.items()}\n        else:\n            return flat_values.pop(0)\n\n    return create_grouping(schema, flat_values)", "idx": 656}
{"namespace": "dash._grouping.map_grouping", "completion": "\n    if isinstance(grouping, (tuple, list)):\n        return [map_grouping(fn, el) for el in grouping]\n    elif isinstance(grouping, dict):\n        return {k: map_grouping(fn, v) for k, v in grouping.items()}\n    else:\n        return fn(grouping)", "idx": 657}
{"namespace": "dash._grouping.validate_grouping", "completion": "    if full_schema is None:\n        full_schema = schema\n\n    if isinstance(schema, type):\n        SchemaTypeValidationError.check(grouping, full_schema, path, schema)\n    elif isinstance(schema, int):\n        SchemaLengthValidationError.check(grouping, full_schema, path, schema)\n    elif isinstance(schema, set):\n        SchemaKeysValidationError.check(grouping, full_schema, path, schema)\n    elif isinstance(schema, (tuple, list)):\n        if not isinstance(grouping, (tuple, list)):\n            raise SchemaTypeValidationError(grouping, full_schema, path, (tuple, list))\n        if len(grouping) != len(schema):\n            raise SchemaLengthValidationError(grouping, full_schema, path, len(schema))\n        for i, (group_el, schema_el) in enumerate(zip(grouping, schema)):\n            validate_grouping(group_el, schema_el, full_schema, path + (i,))\n    elif isinstance(schema, dict):\n        if not isinstance(grouping, dict):\n            raise SchemaTypeValidationError(grouping, full_schema, path, dict)\n        if set(grouping.keys()) != set(schema.keys()):\n            raise SchemaKeysValidationError(grouping, full_schema, path, set(schema.keys()))\n        for k, schema_el in schema.items():\n            validate_grouping(grouping[k], schema_el, full_schema, path + (k,))", "idx": 658}
{"namespace": "dash._get_paths.app_get_relative_path", "completion": "    if requests_pathname == \"/\" and not path:\n        return \"/\"\n    elif requests_pathname != \"/\" and not path:\n        return requests_pathname\n    elif not path.startswith(\"/\"):\n        raise exceptions.UnsupportedRelativePath(\n            f\"\"\"\n            Paths that aren't prefixed with requests_pathname_prefix are not supported.\n            You supplied: {path} and requests_pathname_prefix was {requests_pathname}\n            \"\"\"\n        )\n    else:\n        return \"/\".join([requests_pathname.rstrip(\"/\"), path.lstrip(\"/\")])", "idx": 659}
{"namespace": "dash._get_paths.app_strip_relative_path", "completion": "    if requests_pathname == \"/\" and path == \"\":\n        return \"/\"\n    if requests_pathname != \"/\" and path == \"\":\n        return requests_pathname\n    if not path.startswith(requests_pathname):\n        raise exceptions.UnsupportedRelativePath(\n            f\"\"\"\n            The path does not start with the requests_pathname.\n            You supplied: {path}\n            \"\"\"\n        )\n    return path[len(requests_pathname):].lstrip(\"/\")", "idx": 660}
{"namespace": "dash.development._py_components_generation.js_to_py_type", "completion": "    if is_flow_type:\n        type_mapping = map_js_to_py_types_flow_types(type_object)\n    else:\n        type_mapping = map_js_to_py_types_prop_types(type_object, indent_num)\n\n    type_name = type_object.get(\"name\")\n    if type_name in type_mapping:\n        return type_mapping[type_name]()\n    else:\n        return \"\"", "idx": 661}
{"namespace": "dash.development.component_loader.load_components", "completion": "    data = _get_metadata(metadata_path)\n    components = []\n    for component in data:\n        component_class = generate_class(namespace, component)\n        components.append(component_class)\n    return components", "idx": 662}
{"namespace": "dash.development.component_loader.generate_classes", "completion": "    components = load_components(metadata_path, namespace)\n\n    # Create the output directory if it doesn't exist\n    output_dir = os.path.join(os.getcwd(), namespace)\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Generate Python class files based on the converted metadata\n    for component in components:\n        with open(os.path.join(output_dir, f\"{component.name}.py\"), \"w\") as file:\n            file.write(component.to_python())\n\n    # Generate an imports file and add the \"__all__\" value to it\n    with open(os.path.join(output_dir, \"__init__.py\"), \"w\") as init_file:\n        init_file.write(f\"from .{namespace} import *\\n\\n__all__ = [{', '.join([component.name for component in components])}]\")", "idx": 663}
{"namespace": "dash.development.base_component.Component.to_plotly_json", "completion": "        # Add normal properties\n        props = {}\n        for prop_name in self._prop_names:\n            prop_value = getattr(self, prop_name)\n            if prop_value is not None:\n                props[prop_name] = prop_value\n\n        # Add wildcard properties\n        wildcard_props = [k for k in self.__dict__ if k.startswith(\"data-\") or k.startswith(\"aria-\")]\n        for wildcard_prop in wildcard_props:\n            props[wildcard_prop] = getattr(self, wildcard_prop)\n\n        # Add type and namespace\n        props[\"_type\"] = self._type\n        props[\"_namespace\"] = self._namespace\n\n        return props", "idx": 664}
{"namespace": "aioxmpp.entitycaps.caps390.Key.node", "completion": "        return \"{}#{}\".format(self.algo, base64.b64encode(self.digest).decode(\"utf-8\"))", "idx": 665}
{"namespace": "aioxmpp.entitycaps.caps390.Key.path", "completion": "        encoded_digest = base64.b64encode(self.digest).decode(\"ascii\")\n        file_extension = \"xml\"  # Replace with the appropriate file extension\n        return pathlib.Path(f\"{self.algo}.{encoded_digest}.{file_extension}\")", "idx": 666}
{"namespace": "aioxmpp.entitycaps.caps390.Implementation.extract_keys", "completion": "        if presence.xep0390_caps is not None:\n            for algo, digest in presence.xep0390_caps.digests.items():\n                yield Key(algo, digest)", "idx": 667}
{"namespace": "aioxmpp.roster.service.RosterClient.approve", "completion": "        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.SUBSCRIBED,\n                            to=peer_jid)\n        )", "idx": 668}
{"namespace": "aioxmpp.roster.service.RosterClient.subscribe", "completion": "        \"\"\"\n        Request presence subscription with the given `peer_jid`. This is deliberately not a coroutine; we don\u2019t know whether the peer is online (usually) and they may defer the confirmation very long, if they confirm at all. Use :meth:`on_subscribed` to get notified when a peer accepted a subscription request.\n        Input-Output Arguments\n        :param self: RosterClient. An instance of the RosterClient class.\n        :param peer_jid: The peer JID to subscribe to.\n        :return: No return values.\n        \"\"\"\n        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.SUBSCRIBE,\n                            to=peer_jid)\n        )", "idx": 669}
{"namespace": "aioxmpp.roster.service.RosterClient.unsubscribe", "completion": "        \"\"\"\n        Unsubscribe from the presence of the given `peer_jid`.\n        Input-Output Arguments\n        :param self: RosterClient. An instance of the RosterClient class.\n        :param peer_jid: The JID of the peer to unsubscribe from.\n        :return: No return values.\n        \"\"\"\n        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.UNSUBSCRIBE,\n                            to=peer_jid)\n        )", "idx": 670}
{"namespace": "aioxmpp.forms.fields.BoundSingleValueField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass", "idx": 671}
{"namespace": "aioxmpp.forms.fields.BoundMultiValueField.value", "completion": "        try:\n            return self._value\n        except AttributeError:\n            self.value = self.field.default()\n            return self._value", "idx": 672}
{"namespace": "aioxmpp.forms.fields.BoundOptionsField.options", "completion": "        try:\n            del self._options\n        except AttributeError:\n            pass", "idx": 673}
{"namespace": "aioxmpp.forms.fields.BoundSelectField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass", "idx": 674}
{"namespace": "aioxmpp.forms.fields.BoundMultiSelectField.value", "completion": "        try:\n            return self._value\n        except AttributeError:\n            self.value = self.field.default()\n            return self._value", "idx": 675}
{"namespace": "cupy.random._generator.reset_states", "completion": "    global _random_states\n    _random_states = {}", "idx": 676}
{"namespace": "cupy.random._generator._check_and_get_dtype", "completion": "\n    if dtype is None:\n        return numpy.float64\n    elif cupy.isscalar(dtype):\n        dtype = numpy.dtype(dtype)\n    else:\n        dtype = numpy.dtype(dtype)\n    if dtype.kind not in 'fiu':\n        raise TypeError('Invalid data type')\n    return dtype", "idx": 677}
{"namespace": "cupy_builder._command.filter_files_by_extension", "completion": "    filtered_files = [file for file in sources if file.endswith(extension)]\n    other_files = [file for file in sources if not file.endswith(extension)]\n    return filtered_files, other_files", "idx": 678}
{"namespace": "datasets.table._in_memory_arrow_table_from_file", "completion": "    stream = pa.memory_map(filename, 'r')\n    return pa.ipc.open_stream(stream).read_all()", "idx": 679}
{"namespace": "datasets.table._in_memory_arrow_table_from_buffer", "completion": "    reader = pa.ipc.open_stream(pa.BufferReader(buffer))\n    return reader.read_all()", "idx": 680}
{"namespace": "datasets.table._interpolation_search", "completion": "    if not arr:\n        raise IndexError(\"Array is empty\")\n    low, high = 0, len(arr) - 1\n    while low <= high and arr[low] <= x <= arr[high]:\n        mid = low + ((x - arr[low]) * (high - low)) // (arr[high] - arr[low])\n        if arr[mid] < x:\n            low = mid + 1\n        elif arr[mid] > x:\n            high = mid - 1\n        else:\n            return mid\n    raise IndexError(\"Query is outside the array values\")", "idx": 681}
{"namespace": "datasets.data_files._is_inside_unrequested_special_dir", "completion": "    special_dirs = [\"__pycache__\"]  # List of special directories that are ignored by default\n    for special_dir in special_dirs:\n        if f\"/{special_dir}/\" in matched_rel_path:\n            if f\"/{special_dir}/\" in pattern:\n                return False  # Path is explicitly requested inside the special directory\n            else:\n                return True  # Path is inside an unrequested special directory\n    return False  # Path is not inside any unrequested special directory", "idx": 682}
{"namespace": "datasets.data_files._is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir", "completion": "    # Check if the matched path is a hidden file\n    if os.path.basename(matched_rel_path).startswith(\".\"):\n        return True\n    # Check if the matched path is inside a hidden directory\n    parts = PurePath(pattern).parts\n    for i in range(len(parts)):\n        if parts[i].startswith(\".\"):\n            if parts[i] == \"**\":\n                return True  # If the pattern starts with a hidden directory, then the matched path is inside a hidden directory\n            if i < len(PurePath(matched_rel_path).parts) and parts[i] != PurePath(matched_rel_path).parts[i]:\n                return False  # If the pattern doesn't match the path, then the matched path is not inside a hidden directory\n            return True  # If the pattern matches the path, then the matched path is inside a hidden directory\n    return False  # If the pattern doesn't start with a hidden directory, then the matched path is not inside a hidden directory", "idx": 683}
{"namespace": "datasets.iterable_dataset._batch_to_examples", "completion": "    examples = []\n    for i in range(len(batch[next(iter(batch))])):\n        example = {}\n        for key, value in batch.items():\n            example[key] = value[i]\n        examples.append(example)\n    return examples", "idx": 684}
{"namespace": "datasets.iterable_dataset._examples_to_batch", "completion": "    columns = list(OrderedDict.fromkeys([col for example in examples for col in example]))\n    arrays = [[example.get(col, None) for col in columns] for example in examples]\n    return dict(zip(columns, zip(*arrays)))", "idx": 685}
{"namespace": "datasets.iterable_dataset.RandomlyCyclingMultiSourcesExamplesIterable._iter_random_indices", "completion": "        while True:\n            yield from (int(i) for i in rng.integers(0, num_sources, size=random_batch_size))", "idx": 686}
{"namespace": "datasets.iterable_dataset.BufferShuffledExamplesIterable._iter_random_indices", "completion": "        while True:\n            yield from (int(i) for i in rng.integers(0, buffer_size, size=random_batch_size))", "idx": 687}
{"namespace": "datasets.iterable_dataset.IterableDataset.remove_columns", "completion": "        if isinstance(column_names, str):\n            column_names = [column_names]\n\n        # Remove the specified columns from the features\n        features = self._info.features.copy() if self._info.features else None\n        if features:\n            for column_name in column_names:\n                if column_name in features:\n                    del features[column_name]\n\n        # Remove the specified columns from the examples\n        ex_iterable = MappedExamplesIterable(\n            self._ex_iterable,\n            function=lambda example: {k: v for k, v in example.items() if k not in column_names},\n            remove_columns=column_names,\n            features=features,\n        )\n\n        return IterableDataset(\n            ex_iterable=ex_iterable,\n            info=self._info.copy(),\n            split=self._split,\n            formatting=self._formatting,\n            shuffling=copy.deepcopy(self._shuffling),\n            distributed=copy.deepcopy(self._distributed),\n            token_per_repo_id=self._token_per_repo_id,\n        )", "idx": 688}
{"namespace": "datasets.dataset_dict.DatasetDict.with_format", "completion": "        self._check_values_type()\n        return DatasetDict({k: dataset.with_format(type=type, columns=columns, output_all_columns=output_all_columns, **format_kwargs) for k, dataset in self.items()})", "idx": 689}
{"namespace": "datasets.dataset_dict.DatasetDict.with_transform", "completion": "        self._check_values_type()\n        return DatasetDict({k: dataset.with_transform(transform=transform, columns=columns, output_all_columns=output_all_columns) for k, dataset in self.items()})", "idx": 690}
{"namespace": "datasets.dataset_dict.DatasetDict.align_labels_with_mapping", "completion": "        self._check_values_type()\n        return DatasetDict({k: dataset.align_labels_with_mapping(label2id=label2id, label_column=label_column) for k, dataset in self.items()})", "idx": 691}
{"namespace": "datasets.dataset_dict.IterableDatasetDict.map", "completion": "        self._check_values_type()\n        return IterableDatasetDict(\n            {\n                k: dataset.map(\n                    function=function,\n                    with_indices=with_indices,\n                    input_columns=input_columns,\n                    batched=batched,\n                    batch_size=batch_size,\n                    drop_last_batch=drop_last_batch,\n                    remove_columns=remove_columns,\n                    fn_kwargs=fn_kwargs,\n                )\n                for k, dataset in self.items()\n            }\n        )", "idx": 692}
{"namespace": "datasets.dataset_dict.IterableDatasetDict.filter", "completion": "         'text': 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash", "idx": 693}
{"namespace": "datasets.arrow_dataset.Dataset.num_rows", "completion": "        if self._indices is not None:\n            return self._indices.num_rows\n        else:\n            return self._data.num_rows", "idx": 694}
{"namespace": "datasets.filesystems.extract_path_from_uri", "completion": "    if dataset_path.startswith(\"s3://\"):\n        return dataset_path[5:]\n    return dataset_path", "idx": 695}
{"namespace": "datasets.filesystems.is_remote_filesystem", "completion": "    return not fs.protocol.startswith(\"file\")", "idx": 696}
{"namespace": "datasets.utils.file_utils.hash_url_to_filename", "completion": "    url_bytes = url.encode(\"utf-8\")\n    url_hash = sha256(url_bytes)\n    if etag:\n        etag_bytes = etag.encode(\"utf-8\")\n        etag_hash = sha256(etag_bytes)\n        filename = f\"{url_hash.hexdigest()}.{etag_hash.hexdigest()}\"\n    else:\n        filename = url_hash.hexdigest()\n    if url.endswith(\".h5\"):\n        filename += \".h5\"\n    return filename", "idx": 697}
{"namespace": "datasets.utils.hub.hf_hub_url", "completion": "    hub_version = version.parse(hfh.__version__)\n    if hub_version < version.parse(\"0.11.0\"):\n        path = quote(path, safe=\"\")\n    url = f\"https://huggingface.co/{repo_id}/resolve/{revision}/{path}\"\n    return url", "idx": 698}
{"namespace": "datasets.utils.sharding._number_of_shards_in_gen_kwargs", "completion": "    list_lengths = [len(value) for value in gen_kwargs.values() if isinstance(value, list)]\n    if len(set(list_lengths)) > 1:\n        raise ValueError(\"Lists of different sizes make sharding ambiguous\")\n    return list_lengths[0] if list_lengths else 1", "idx": 699}
{"namespace": "datasets.utils.sharding._distribute_shards", "completion": "    shards_per_job = num_shards // max_num_jobs\n    remainder = num_shards % max_num_jobs\n    shard_ranges = []\n    start = 0\n    for i in range(max_num_jobs):\n        if i < remainder:\n            end = start + shards_per_job + 1\n        else:\n            end = start + shards_per_job\n        shard_ranges.append(range(start, end))\n        start = end\n    return shard_ranges", "idx": 700}
{"namespace": "datasets.utils.py_utils.temporary_assignment", "completion": "    original_value = getattr(obj, attr)  # Get the original value of the attribute\n    setattr(obj, attr, value)  # Set the attribute to the new value\n    try:\n        yield\n    finally:\n        setattr(obj, attr, original_value)  # Set the attribute back to the original value", "idx": 701}
{"namespace": "datasets.utils.extract.TarExtractor.extract", "completion": "        os.makedirs(output_path, exist_ok=True)\n        with tarfile.open(input_path, \"r\") as tar_file:\n            tar_file.extractall(output_path)", "idx": 702}
{"namespace": "datasets.utils.extract.Extractor.infer_extractor_format", "completion": "\n        magic_number_max_length = cls._get_magic_number_max_length()\n        magic_number = cls._read_magic_number(path, magic_number_max_length)\n\n        for extractor_format, extractor_class in cls.extractors.items():\n            if extractor_class.is_extractable(path, magic_number=magic_number):\n                return extractor_format\n\n        return \"\"", "idx": 703}
{"namespace": "datasets.utils.py_utils.asdict", "completion": "\n    if is_dataclass(obj):\n        return {f.name: asdict(getattr(obj, f.name)) for f in fields(obj)}\n    elif isinstance(obj, tuple) and hasattr(obj, \"_asdict\"):\n        return {k: asdict(v) for k, v in obj._asdict().items()}\n    elif isinstance(obj, (list, tuple)):\n        return [asdict(v) for v in obj]\n    elif isinstance(obj, dict):\n        return {k: asdict(v) for k, v in obj.items()}\n    else:\n        return obj", "idx": 704}
{"namespace": "datasets.utils.metadata.MetadataConfigs.from_dataset_card_data", "completion": "        metadata_configs = MetadataConfigs()\n        if dataset_card_data and cls.FIELD_NAME in dataset_card_data:\n            for config_metadata in dataset_card_data[cls.FIELD_NAME]:\n                config_name = config_metadata.pop(\"config_name\")\n                metadata_configs[config_name] = config_metadata\n        return metadata_configs", "idx": 705}
{"namespace": "pymorphy2.analyzer.lang_dict_path", "completion": "    paths = _lang_dict_paths()\n    if lang in paths:\n        return paths[lang]\n    else:\n        raise ValueError(f\"Language '{lang}' is not supported. Available languages: {', '.join(paths.keys())}\")", "idx": 706}
{"namespace": "pymorphy2.dawg.assert_can_create", "completion": "    if not EXTENSION_AVAILABLE:\n        raise NotImplementedError(\"Extension is not available, cannot create DAWG.\")", "idx": 707}
{"namespace": "pymorphy2.opencorpora_dict.compile._to_paradigm", "completion": "\n    stem = ''\n    paradigm = []\n\n    for word_form, tag in lexeme:\n        prefix = ''\n        for prefix_candidate in paradigm_prefixes:\n            if word_form.startswith(prefix_candidate):\n                prefix = prefix_candidate\n                break\n\n        if not stem:\n            stem = word_form[len(prefix):]\n\n        suffix = word_form[len(prefix):]\n        paradigm.append((suffix, tag, prefix))\n\n    return stem, tuple(paradigm)", "idx": 708}
{"namespace": "pymorphy2.units.by_analogy.KnownPrefixAnalyzer.tag", "completion": "        result = []\n        for _, unprefixed_word in word_splits(word_lower):\n\n            tags = self.dict_analyzer.tag(unprefixed_word, unprefixed_word, seen_tags)\n            for tag in tags:\n\n                if not tag.is_productive():\n                    continue\n\n                add_tag_if_not_seen(tag, result, seen_tags)\n\n        return result", "idx": 709}
{"namespace": "pymorphy2.units.by_analogy.UnknownPrefixAnalyzer.tag", "completion": "        result = []\n        for prefix, unprefixed_word in word_splits(word_lower):\n            for tag in self.morph.tag(unprefixed_word):\n                if not tag.is_productive():\n                    continue\n                add_tag_if_not_seen(tag, result, seen_tags)\n        return result", "idx": 710}
{"namespace": "benedict.dicts.keylist.keylist_util.get_item", "completion": "    items = get_items(d, keys)\n    if items:\n        return items[-1][2]\n    else:\n        return (None, None, None)", "idx": 711}
{"namespace": "benedict.dicts.keylist.keylist_util.set_item", "completion": "    items = get_items(d, keys)\n    for i in range(len(items) - 1):\n        item, item_key, item_value = items[i]\n        next_item, next_item_key, next_item_value = items[i + 1]\n        if item_key is not None:\n            _set_item_value(item, item_key, item_value)\n        elif type_util.is_dict(item):\n            item[next_item_key] = next_item_value\n        elif type_util.is_list_or_tuple(item):\n            item[next_item_key] = next_item_value\n    last_item, last_item_key, last_item_value = items[-1]\n    if last_item_key is not None:\n        _set_item_value(last_item, last_item_key, value)\n    elif type_util.is_dict(last_item):\n        last_item[last_item_key] = value\n    elif type_util.is_list_or_tuple(last_item):\n        last_item[last_item_key] = value", "idx": 712}
{"namespace": "benedict.dicts.keypath.keypath_util._split_key_indexes", "completion": "    matches = re.findall(KEY_INDEX_RE, key)\n    if matches:\n        return [int(match) if match.isdigit() else match for match in matches]\n    return [key]", "idx": 713}
{"namespace": "feedparser.urls.make_safe_absolute_uri", "completion": "    if not ACCEPTABLE_URI_SCHEMES:\n        return ''\n\n    \"\"\"\n    This function creates a safe absolute URI by joining a base URL and a relative URL. If the base URL is empty, it returns the relative URL. If the relative URL is empty, it outputs the base URL. Finally, if the resulting URI's scheme is not acceptable, it returns an empty string. Otherwise, it returns the resulting URI.\n    Input-Output Arguments\n    :param base: String. The base URL to join with the relative URL.\n    :param rel: String. The relative URL to join with the base URL. Defaults to None.\n    :return: String. The safe absolute URI created by joining the base and relative URLs.\n    \"\"\"\n    if not base:\n        return rel or ''\n    if not rel:\n        return base\n\n    try:\n        uri = urllib.parse.urljoin(base, rel)\n        scheme = urllib.parse.urlparse(uri).scheme\n        if scheme not in ACCEPTABLE_URI_SCHEMES:\n            return ''\n        return uri\n    except ValueError:\n        return ''", "idx": 714}
{"namespace": "feedparser.api._open_resource", "completion": "    if hasattr(url_file_stream_or_string, 'read'):\n        return url_file_stream_or_string.read()\n\n    if isinstance(url_file_stream_or_string, str):\n        if urllib.parse.urlparse(url_file_stream_or_string)[0] in ('http', 'https'):\n            request = urllib.request.Request(url_file_stream_or_string)\n            if etag:\n                request.add_header('If-None-Match', etag)\n            if modified:\n                request.add_header('If-Modified-Since', _parse_date(modified))\n            if agent:\n                request.add_header('User-Agent', agent)\n            if referrer:\n                request.add_header('Referer', referrer)\n            if request_headers:\n                for header, value in request_headers.items():\n                    request.add_header(header, value)\n            if handlers:\n                opener = urllib.request.build_opener(*handlers)\n            else:\n                opener = urllib.request.build_opener()\n            try:\n                response = opener.open(request)\n                result['url'] = response.geturl()\n                result['status'] = response.getcode()\n                result['headers'] = dict(response.info())\n                return response.read()\n            except urllib.error.HTTPError as e:\n                result['status'] = e.code\n                result['headers'] = dict(e.headers)\n                return e.read()\n            except urllib.error.URLError as e:\n                raise\n        else:\n            with open(url_file_stream_or_string, 'rb') as f:\n                return f.read()\n    else:\n        raise TypeError('Expected a file-like object, URL, file path, or string')", "idx": 715}
{"namespace": "feedparser.http._build_urllib2_request", "completion": "    request = urllib.request.Request(url)\n    request.add_header('User-Agent', agent)\n    request.add_header('Accept', accept_header)\n    if etag:\n        request.add_header('If-None-Match', etag)\n    if modified:\n        if isinstance(modified, datetime.datetime):\n            modified = modified.strftime('%a, %d %b %Y %H:%M:%S GMT')\n        request.add_header('If-Modified-Since', modified)\n    if referrer:\n        request.add_header('Referer', referrer)\n    if auth:\n        request.add_header('Authorization', f'Basic {auth}')\n    for key, value in request_headers.items():\n        request.add_header(key, value)\n    return request", "idx": 716}
{"namespace": "pylatex.utils.dumps_list", "completion": "    if mapper is None:\n        mapper = lambda x: x\n\n    if not _is_iterable(l):\n        raise ValueError(\"Input must be an iterable\")\n\n    return NoEscape(token.join(_latex_item_to_string(mapper(item), escape=escape, as_content=as_content) for item in l))", "idx": 717}
{"namespace": "pylatex.utils._latex_item_to_string", "completion": "\n    if isinstance(item, pylatex.base_classes.LatexObject):\n        if as_content:\n            return item.dumps_as_content()\n        else:\n            return item.dumps()\n\n    if escape:\n        return escape_latex(str(item))\n    else:\n        return NoEscape(str(item))", "idx": 718}
{"namespace": "mistune.markdown.Markdown.read", "completion": "        if state is None:\n            state = self.block.state_cls()\n\n        with open(filepath, 'r', encoding=encoding) as file:\n            content = file.read()\n\n        return self.parse(content, state)[0]", "idx": 719}
{"namespace": "mistune.create_markdown", "completion": "    if renderer == 'html':\n        renderer = HTMLRenderer(escape=escape, hard_wrap=hard_wrap)\n    else:\n        raise ValueError(\"Unsupported renderer: {}\".format(renderer))\n\n    if plugins:\n        for plugin in plugins:\n            plugin = import_plugin(plugin)\n            if plugin:\n                renderer = plugin(renderer)\n\n    return Markdown(renderer)", "idx": 720}
{"namespace": "parsel.utils.extract_regex", "completion": "    matches = re.finditer(regex, text)\n    extracted_strings = []\n    for match in matches:\n        if \"extract\" in match.groupdict():\n            extracted_strings.append(match.group(\"extract\"))\n        elif match.groups():\n            extracted_strings.extend(flatten(match.groups()))\n        else:\n            extracted_strings.append(match.group(0))\n    if replace_entities:\n        extracted_strings = [w3lib_replace_entities(s) for s in extracted_strings]\n    return extracted_strings", "idx": 721}
{"namespace": "dominate.dom_tag.dom_tag.render", "completion": "    sb = []\n    self._render(sb, 0, indent, pretty, xhtml)\n    return ''.join(sb)", "idx": 722}
{"namespace": "dominate.util.include", "completion": "  with open(f, 'r') as file:\n    data = file.read()\n  return data", "idx": 723}
{"namespace": "dominate.util.unescape", "completion": "\n  def replace_entity(match):\n    entity = match.group(1)\n    if entity in _unescape:\n      return chr(_unescape[entity])\n    else:\n      return match.group(0)\n\n  return re.sub(r'&(\\w+);', replace_entity, data)", "idx": 724}
{"namespace": "onlinejudge_command.pretty_printers._tokenize_line", "completion": "    tokens = []\n    l = 0\n    while l < len(line):\n        r = l + 1\n        while r < len(line) and (line[l] in ' \\t') == (line[r] in ' \\t'):\n            r += 1\n        if line[l] in ' \\t':\n            typ = _PrettyTokenType.WHITESPACE\n        else:\n            typ = _PrettyTokenType.BODY\n        tokens.append(_PrettyToken(typ, line[l:r]))\n        l = r\n    return tokens", "idx": 725}
{"namespace": "onlinejudge_command.pretty_printers._render_tokens", "completion": "    formatted_tokens = []\n    for token in tokens:\n        if token.type == _PrettyTokenType.BODY:\n            if font_normal:\n                formatted_tokens.append(font_normal(token.value))\n            else:\n                formatted_tokens.append(token.value)\n        elif token.type == _PrettyTokenType.BODY_HIGHLIGHT_LEFT:\n            if font_red:\n                formatted_tokens.append(font_red(token.value))\n            else:\n                formatted_tokens.append(token.value)\n        elif token.type == _PrettyTokenType.BODY_HIGHLIGHT_RIGHT:\n            if font_blue:\n                formatted_tokens.append(font_blue(token.value))\n            else:\n                formatted_tokens.append(token.value)\n        elif token.type == _PrettyTokenType.WHITESPACE:\n            if font_dim:\n                formatted_tokens.append(font_dim(token.value))\n            else:\n                formatted_tokens.append(token.value)\n        elif token.type == _PrettyTokenType.NEWLINE:\n            formatted_tokens.append(token.value)\n        elif token.type == _PrettyTokenType.HINT:\n            if font_dim:\n                formatted_tokens.append(font_dim(token.value))\n            else:\n                formatted_tokens.append(token.value)\n        elif token.type == _PrettyTokenType.LINENO:\n            if font_bold:\n                formatted_tokens.append(font_bold(token.value))\n            else:\n                formatted_tokens.append(token.value)\n        elif token.type == _PrettyTokenType.OTHERS:\n            formatted_tokens.append(token.value)\n\n    return ''.join(formatted_tokens)", "idx": 726}
{"namespace": "onlinejudge_command.pretty_printers._tokenize_file_content_without_snipping", "completion": "    tokens = []\n    try:\n        text = content.decode()\n    except UnicodeDecodeError as e:\n        tokens.append(_PrettyToken(_PrettyTokenType.HINT, str(e)))\n        text = content.decode(errors='replace')\n\n    for line in text.splitlines(keepends=True):\n        tokens += _tokenize_line(line)\n\n    if not tokens:\n        tokens.append(_PrettyToken(_PrettyTokenType.HINT, '(empty)'))\n    if tokens[-1].type == _PrettyTokenType.BODY:\n        tokens.append(_PrettyToken(_PrettyTokenType.HINT, '(no trailing newline)'))\n    if all(token.type == _PrettyTokenType.NEWLINE for token in tokens):\n        tokens.append(_PrettyToken(_PrettyTokenType.HINT, '(only newline)'))\n\n    return tokens", "idx": 727}
{"namespace": "jinja2.environment.Environment.get_template", "completion": "        if isinstance(name, Template):\n            return name\n        if parent is not None:\n            name = self.join_path(name, parent)\n        try:\n            return self._load_template(name, globals)\n        except (TemplateNotFound, UndefinedError):\n            raise TemplateNotFound(name)", "idx": 728}
{"namespace": "jinja2.environment.Environment.from_string", "completion": "        if template_class is None:\n            template_class = self.template_class\n\n        if isinstance(source, nodes.Template):\n            return source\n\n        if isinstance(source, str):\n            source_hint = source\n            source = self._parse(source, None, None)\n        else:\n            source_hint = None\n\n        return template_class.from_code(\n            self,\n            self.compile(source, None, None, True, True),\n            self.make_globals(globals),\n        )", "idx": 729}
{"namespace": "jinja2.environment.Template.render", "completion": "        ctx = self.new_context(dict(*args, **kwargs))\n\n        try:\n            return self.environment.concat(  # type: ignore\n                [n for n in self.root_render_func(ctx)]  # type: ignore\n            )\n        except Exception:\n            return self.environment.handle_exception()", "idx": 730}
{"namespace": "jinja2.utils.generate_lorem_ipsum", "completion": "    words = [\n        \"Lorem\", \"ipsum\", \"dolor\", \"sit\", \"amet\", \"consectetur\",\n        \"adipiscing\", \"elit\", \"sed\", \"do\", \"eiusmod\", \"tempor\",\n        \"incididunt\", \"ut\", \"labore\", \"et\", \"dolore\", \"magna\",\n        \"aliqua\", \"Ut\", \"enim\", \"ad\", \"minim\", \"veniam\", \"quis\",\n        \"nostrud\", \"exercitation\", \"ullamco\", \"laboris\", \"nisi\",\n        \"ut\", \"aliquip\", \"ex\", \"ea\", \"commodo\", \"consequat\", \"Duis\",\n        \"aute\", \"irure\", \"dolor\", \"in\", \"reprehenderit\", \"in\",\n        \"voluptate\", \"velit\", \"esse\", \"cillum\", \"dolore\", \"eu\",\n        \"fugiat\", \"nulla\", \"pariatur\", \"Excepteur\", \"sint\",\n        \"occaecat\", \"cupidatat\", \"non\", \"proident\", \"sunt\", \"in\",\n        \"culpa\", \"qui\", \"officia\", \"deserunt\", \"mollit\", \"anim\", \"id\",\n        \"est\", \"laborum\"\n    ]\n\n    paragraphs = []\n    for _ in range(n):\n        num_words = randrange(min, max)\n        paragraph = [choice(words) for _ in range(num_words)]\n        for i in range(2, len(paragraph), randrange(3, 8)):\n            paragraph[i] += \",\"\n        for i in range(9, len(paragraph), randrange(10, 20)):\n            paragraph[i] += \".\"\n        paragraph[0] = paragraph[0].capitalize()\n        paragraph[-1] += \".\"\n        paragraphs.append(concat(paragraph))\n\n    if html:\n        return \"<p>\" + \"</p>\\n<p>\".join(paragraphs) + \"</p>\"\n    else:\n        return \"\\n\\n\".join(paragraphs)", "idx": 731}
{"namespace": "jinja2.utils.LRUCache.clear", "completion": "        self._mapping.clear()\n        self._queue.clear()", "idx": 732}
{"namespace": "jinja2.utils.LRUCache.items", "completion": "        \"\"\"\n        This function returns a list of items in the LRUCache instance. The order should be reversed from the order in the queue.\n        Input-Output Arguments\n        :param self: LRUCache. An instance of the LRUCache class.\n        :return: Iterable[Tuple]. A list of tuples containing the key-value pairs in the LRUCache instance.\n        \"\"\"\n        for key in reversed(self):\n            yield (key, self._mapping[key])", "idx": 733}
{"namespace": "jinja2.idtracking.symbols_for_node", "completion": "    sym = Symbols(parent=parent_symbols)\n    visitor = RootVisitor(sym)\n    visitor.visit(node)\n    return sym", "idx": 734}
{"namespace": "jinja2.idtracking.Symbols.find_ref", "completion": "        if name in self.refs:\n            return self.refs[name]\n\n        if self.parent is not None:\n            return self.parent.find_ref(name)\n\n        return None", "idx": 735}
{"namespace": "jinja2.idtracking.Symbols.dump_stores", "completion": "        rv = {}\n        node: t.Optional[\"Symbols\"] = self\n\n        while node is not None:\n            for store in node.stores:\n                rv[store] = node.ref(store)\n\n            node = node.parent\n\n        return rv", "idx": 736}
{"namespace": "jinja2.meta.find_undeclared_variables", "completion": "    undeclared_variables: t.Set[str] = set()\n\n    for node in ast.find_all(nodes.Name):\n        if node.ctx == \"load\":\n            undeclared_variables.add(node.name)\n\n    return undeclared_variables", "idx": 737}
{"namespace": "jinja2.loaders.split_template_path", "completion": "    pieces = []\n    for piece in template.split(\"/\"):\n        if (\n            os.path.sep in piece\n            or os.path.altsep and os.path.altsep in piece\n            or piece == os.pardir\n        ):\n            raise TemplateNotFound(template)\n        pieces.append(piece)\n    return pieces", "idx": 738}
{"namespace": "jinja2.bccache.MemcachedBytecodeCache.load_bytecode", "completion": "        key = self.prefix + bucket.key\n        bytecode = self.client.get(key)\n        if bytecode is not None:\n            bucket.bytecode_from_string(bytecode)\n        elif not self.ignore_memcache_errors:\n            raise RuntimeError(\"Failed to load bytecode from memcache\")", "idx": 739}
{"namespace": "jinja2.bccache.MemcachedBytecodeCache.dump_bytecode", "completion": "        key = self.prefix + bucket.key\n        code_string = bucket.bytecode_to_string()\n        try:\n            if self.timeout is not None:\n                self.client.set(key, code_string, timeout=self.timeout)\n            else:\n                self.client.set(key, code_string)\n        except Exception:\n            if not self.ignore_memcache_errors:\n                raise", "idx": 740}
{"namespace": "sumy.utils.get_stop_words", "completion": "    language = normalize_language(language)\n    filename = \"stopwords-%s.txt\" % language\n\n    try:\n        data = pkgutil.get_data(\"sumy\", join(\"data\", filename))\n    except IOError:\n        raise LookupError(\"Stop words are not available for language %s\" % language)\n\n    return parse_stop_words(data)", "idx": 741}
{"namespace": "sumy._compat.to_bytes", "completion": "    if isinstance(object, bytes):\n        return object\n    elif isinstance(object, unicode):\n        return object.encode(\"utf-8\")\n    else:\n        return instance_to_bytes(object)", "idx": 742}
{"namespace": "sumy._compat.to_unicode", "completion": "    if isinstance(object, unicode):\n        return object\n    elif isinstance(object, bytes):\n        return object.decode(\"utf-8\")\n    else:\n        return instance_to_unicode(object)", "idx": 743}
{"namespace": "sumy.summarizers.lsa.LsaSummarizer._create_dictionary", "completion": "        unique_words = set()\n        for sentence in document.sentences:\n            for word in map(self.normalize_word, sentence.words):\n                if word not in self._stop_words:\n                    unique_words.add(word)\n\n        return {word: index for index, word in enumerate(sorted(unique_words))}", "idx": 744}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._get_content_words_in_sentence", "completion": "        words = sentence.words\n        content_words = self._filter_out_stop_words(words)\n        normalized_content_words = self._normalize_words(content_words)\n        stemmed_content_words = self._stem_words(normalized_content_words)\n        return stemmed_content_words", "idx": 745}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._get_all_content_words_in_doc", "completion": "        all_words = self._get_all_words_in_doc(sentences)\n        content_words = self._filter_out_stop_words(all_words)\n        normalized_content_words = self._normalize_words(content_words)\n        return normalized_content_words", "idx": 746}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_tf", "completion": "        all_content_words = self._get_all_content_words_in_doc(sentences)\n        word_freq = self._compute_word_freq(all_content_words)\n        content_words_count = len(all_content_words)\n        \n        for word in word_freq:\n            word_freq[word] /= content_words_count\n        \n        return word_freq", "idx": 747}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_ratings", "completion": "        word_freq = self._compute_tf(sentences)\n        sentences_as_words = [self._get_content_words_in_sentence(s) for s in sentences]\n        ratings = {}\n        for _ in range(len(sentences)):\n            best_sentence_index = self._find_index_of_best_sentence(word_freq, sentences_as_words)\n            best_sentence = sentences[best_sentence_index]\n            ratings[best_sentence] = -(_ + 1)\n            word_freq = self._update_tf(word_freq, sentences_as_words[best_sentence_index])\n            del sentences_as_words[best_sentence_index]\n            del sentences[best_sentence_index]\n        return ratings", "idx": 748}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.cue_method", "completion": "        method = self._build_cue_method_instance()\n        method.bonus_word_value = bonus_word_value\n        method.stigma_word_value = stigma_word_value\n        return method(document, sentences_count)", "idx": 749}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.key_method", "completion": "        summarization_method = self._build_key_method_instance()\n        return summarization_method(document, sentences_count, weight)", "idx": 750}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.title_method", "completion": "        summarization_method = self._build_title_method_instance()\n        return summarization_method(document, sentences_count)", "idx": 751}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.location_method", "completion": "        summarization_method = self._build_location_method_instance()\n        return summarization_method(document, sentences_count, w_h, w_p1, w_p2, w_s1, w_s2)", "idx": 752}
{"namespace": "sumy.summarizers.reduction.ReductionSummarizer.rate_sentences", "completion": "        sentences = document.sentences\n        sentences_count = len(sentences)\n        ratings = defaultdict(float)\n\n        for (i, j) in combinations(range(sentences_count), 2):\n            words1 = self._to_words_set(sentences[i])\n            words2 = self._to_words_set(sentences[j])\n            similarity = self._rate_sentences_edge(words1, words2)\n\n            ratings[i] += similarity\n            ratings[j] += similarity\n\n        return ratings", "idx": 753}
{"namespace": "sumy.summarizers.reduction.ReductionSummarizer._to_words_set", "completion": "        words = sentence.split()\n        words = [self.normalize_word(word) for word in words if word not in self.stop_words]\n        return set(words)", "idx": 754}
{"namespace": "sumy.summarizers.text_rank.TextRankSummarizer._to_words_set", "completion": "        words = sentence.words\n        words = [self.normalize_word(w) for w in words]\n        words = [w for w in words if w not in self._stop_words]\n        return set(words)", "idx": 755}
{"namespace": "sumy.summarizers.kl.KLSummarizer.compute_tf", "completion": "        all_content_words = self._get_all_content_words_in_doc(sentences)\n        word_freq = self._compute_word_freq(all_content_words)\n        total_content_words = len(all_content_words)\n        tf = {word: freq / total_content_words for word, freq in word_freq.items()}\n        return tf", "idx": 756}
{"namespace": "sumy.evaluation.rouge._get_word_ngrams", "completion": "    ngram_set = set()\n    for s in sentences:\n        words = s.words\n        text_length = len(words)\n        max_index_ngram_start = text_length - n\n        for i in range(max_index_ngram_start + 1):\n            ngram_set.add(tuple(words[i:i + n]))\n    return ngram_set", "idx": 757}
{"namespace": "kinto.plugins.accounts.views.on_account_created", "completion": "    request = event.request\n\n    for obj in event.impacted_objects:\n        # Check if account validation is enabled.\n        if obj.context.validation_enabled:\n            # Retrieve account information.\n            user_email = obj.new[obj.model.id_field]\n            activation_key = get_cached_validation_key(user_email, request.registry)\n\n            # If activation key is not found, skip to the next impacted object.\n            if activation_key is None:\n                continue\n\n            # Send activation email to the user.\n            emailer = Emailer(request)\n            emailer.send_activation(user_email, activation_key)", "idx": 758}
{"namespace": "kinto.plugins.accounts.utils.hash_password", "completion": "    salt = bcrypt.gensalt()\n    hashed_password = bcrypt.hashpw(password.encode('utf-8'), salt)\n    return hashed_password.decode('utf-8')", "idx": 759}
{"namespace": "kinto.views.admin.get_parent_uri", "completion": "    parts = object_uri.split('/')\n    if len(parts) < 3:\n        return ''\n    return '/'.join(parts[:-1])", "idx": 760}
{"namespace": "alembic.script.write_hooks.register", "completion": "    def decorator(func: Callable) -> Callable:\n        _registry[name] = func\n        return func\n    return decorator", "idx": 761}
{"namespace": "mongo_connector.namespace_config.match_replace_regex", "completion": "    match = regex.match(src_namespace)\n    if match:\n        return dest_namespace.replace(\"*\", match.group(1))\n    else:\n        return None", "idx": 762}
{"namespace": "mongo_connector.namespace_config.namespace_to_regex", "completion": "    db_name, coll_name = namespace.split(\".\", 1)\n    db_regex = db_name.replace(\"*\", \"(.*)\")\n    coll_regex = coll_name.replace(\"*\", \"(.*)\")\n    full_regex = f\"^{db_regex}\\.{coll_regex}$\"\n    return re.compile(full_regex)", "idx": 763}
{"namespace": "mongo_connector.util.long_to_bson_ts", "completion": "    time = val >> 32\n    inc = val & 0xFFFFFFFF\n    return Timestamp(time, inc)", "idx": 764}
{"namespace": "mongo_connector.doc_managers.formatters.DocumentFlattener.format_document", "completion": "        def _flatten(doc, parent_key='', sep='.'):\n            items = []\n            for k, v in doc.items():\n                new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n                if isinstance(v, dict):\n                    items.extend(_flatten(v, new_key, sep=sep))\n                else:\n                    items.append((new_key, v))\n            return items\n\n        flattened = dict(_flatten(document))\n        return flattened", "idx": 765}
{"namespace": "bplustree.memory.open_file_in_dir", "completion": "    file_fd = io.open(path, 'r+b')\n    dir_fd = None\n    if platform.system() != 'Windows':\n        dir_fd = os.open(os.path.dirname(path), os.O_DIRECTORY)\n    return file_fd, dir_fd", "idx": 766}
{"namespace": "bplustree.memory.FileMemory.read_transaction", "completion": "\n        class ReadTransaction:\n\n            def __enter__(self2):\n                self._lock.reader_lock.acquire()\n\n            def __exit__(self2, exc_type, exc_val, exc_tb):\n                self._lock.reader_lock.release()\n\n        return ReadTransaction()", "idx": 767}
{"namespace": "bplustree.utils.pairwise", "completion": "    it1, it2 = itertools.tee(iterable)\n    next(it2, None)\n    return zip(it1, it2)", "idx": 768}
{"namespace": "bplustree.utils.iter_slice", "completion": "    it = iter(iterable)\n    while True:\n        slice = bytes(itertools.islice(it, n))\n        if not slice:\n            break\n        if len(slice) < n:\n            yield slice, True\n            break\n        else:\n            yield slice, False", "idx": 769}
{"namespace": "bplustree.serializer.StrSerializer.serialize", "completion": "        encoded_str = obj.encode(encoding='utf-8')\n        if len(encoded_str) > key_size:\n            raise ValueError('Serialized string exceeds key size')\n        return encoded_str", "idx": 770}
{"namespace": "psd_tools.utils.pack", "completion": "    fmt = str(\">\" + fmt)\n    return struct.pack(fmt, *args)", "idx": 771}
{"namespace": "psd_tools.utils.unpack", "completion": "    fmt = str(\">\" + fmt)\n    return struct.unpack(fmt, data)", "idx": 772}
{"namespace": "psd_tools.api.numpy_io.get_pattern", "completion": "\n    height, width = pattern.data[2], pattern.data[3]\n    pattern_array = np.zeros((height, width, 1), dtype=np.float32)\n\n    for channel in pattern.data[4:]:\n        channel_data = get_array(channel, 'color')\n        pattern_array = np.concatenate((pattern_array, channel_data), axis=2)\n\n    return pattern_array", "idx": 773}
{"namespace": "sqlite_utils.utils.maximize_csv_field_size_limit", "completion": "    max_size = sys.maxsize\n    while True:\n        try:\n            csv.field_size_limit(max_size)\n            break\n        except OverflowError:\n            max_size = int(max_size / 10)", "idx": 774}
{"namespace": "sqlite_utils.utils.column_affinity", "completion": "\n    if \"INT\" in column_type.upper():\n        return \"INTEGER\"\n    elif \"CHAR\" in column_type.upper() or \"CLOB\" in column_type.upper() or \"TEXT\" in column_type.upper():\n        return \"TEXT\"\n    elif \"BLOB\" in column_type.upper():\n        return \"BLOB\"\n    elif \"REAL\" in column_type.upper() or \"FLOA\" in column_type.upper() or \"DOUB\" in column_type.upper():\n        return \"REAL\"\n    else:\n        return \"NUMERIC\"", "idx": 775}
{"namespace": "sqlite_utils.utils.decode_base64_values", "completion": "    for key, value in doc.items():\n        if isinstance(value, dict) and value.get(\"$base64\"):\n            encoded_value = value.get(\"encoded\")\n            if encoded_value:\n                decoded_value = base64.b64decode(encoded_value).decode(\"utf-8\")\n                doc[key] = decoded_value\n    return doc", "idx": 776}
{"namespace": "sqlite_utils.utils.chunks", "completion": "    it = iter(sequence)\n    chunk = list(itertools.islice(it, size))\n    while chunk:\n        yield chunk\n        chunk = list(itertools.islice(it, size))", "idx": 777}
{"namespace": "sqlite_utils.utils.hash_record", "completion": "    if keys:\n        data = {k: record[k] for k in keys if k in record}\n    else:\n        data = record\n    data_str = json.dumps(data, sort_keys=True)\n    return hashlib.sha1(data_str.encode(\"utf-8\")).hexdigest()", "idx": 778}
{"namespace": "arctic.decorators._get_host", "completion": "    if store:\n        if isinstance(store, (list, tuple)):\n            store = store[0]\n        return {\n            'library_name': store.library_name,\n            'nodes': store._get_nodes(),\n            'host': store._get_host()\n        }\n    else:\n        return {}", "idx": 779}
{"namespace": "arctic.decorators.mongo_retry", "completion": "    @wraps(f)\n    def wrapper(*args, **kwargs):\n        global _in_retry, _retry_count\n        _in_retry = True\n        _retry_count = 0\n        while True:\n            try:\n                return f(*args, **kwargs)\n            except (AutoReconnect, OperationFailure, DuplicateKeyError, ServerSelectionTimeoutError, BulkWriteError) as e:\n                _handle_error(f, e, _retry_count, **kwargs)\n                _retry_count += 1\n            except Exception as e:\n                if 'arctic' in f.__module__:\n                    logger.exception('Unhandled exception in %s' % f.__name__)\n                raise\n            finally:\n                _in_retry = False\n    return wrapper", "idx": 780}
{"namespace": "arctic._util.are_equals", "completion": "    if isinstance(o1, DataFrame) and isinstance(o2, DataFrame):\n        try:\n            assert_frame_equal(o1, o2, **kwargs)\n            return True\n        except:\n            return False\n    else:\n        return o1 == o2", "idx": 781}
{"namespace": "arctic.hooks.register_resolve_mongodb_hook", "completion": "    global _resolve_mongodb_hook\n    _resolve_mongodb_hook = hook", "idx": 782}
{"namespace": "arctic.hooks.register_log_exception_hook", "completion": "    global _log_exception_hook\n    _log_exception_hook = hook", "idx": 783}
{"namespace": "arctic.hooks.register_get_auth_hook", "completion": "    global _get_auth_hook\n    _get_auth_hook = hook", "idx": 784}
{"namespace": "arctic.store._version_store_utils._split_arrs", "completion": "    result = []\n    start = 0\n    for end in slices:\n        result.append(array_2d[start:end])\n        start = end\n    return result", "idx": 785}
{"namespace": "arctic.store._version_store_utils.checksum", "completion": "\n    # Serialize the symbol and doc using pickle\n    serialized_data = pickle.dumps((symbol, doc))\n\n    # Calculate the checksum using SHA1 algorithm\n    checksum_value = hashlib.sha1(serialized_data).digest()\n\n    # Return the checksum as a Binary object\n    return Binary(checksum_value)", "idx": 786}
{"namespace": "arctic.store.versioned_item.VersionedItem.__str__", "completion": "        return f\"VersionedItem(symbol={self.symbol},library={self.library},data={self.data},version={self.version},metadata={self.metadata},host={self.host})\"", "idx": 787}
{"namespace": "arctic.store._ndarray_store.NdarrayStore._dtype", "completion": "        if metadata is None:\n            metadata = {}\n        return np.dtype(string, metadata)", "idx": 788}
{"namespace": "arctic.store._ndarray_store._promote_struct_dtypes", "completion": "    # Check if the fields of dtype1 are a superset of dtype2\n    if set(dtype2.names).issubset(set(dtype1.names)):\n        # Promote the data types of the two structured arrays\n        new_dtype = []\n        for field in dtype1.names:\n            if field in dtype2.names:\n                new_dtype.append((field, np.promote_types(dtype1[field], dtype2[field])))\n            else:\n                new_dtype.append((field, dtype1[field]))\n        return np.dtype(new_dtype)\n    else:\n        raise ValueError(\"Fields of dtype1 are not a superset of dtype2\")", "idx": 789}
{"namespace": "arctic.chunkstore.passthrough_chunker.PassthroughChunker.exclude", "completion": "        return DataFrame() if isinstance(data, DataFrame) else Series()", "idx": 790}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.to_chunks", "completion": "        if isinstance(df, pd.Series):\n            df = df.to_frame()\n\n        if 'date' not in df.index.names and 'date' not in df.columns:\n            raise ValueError(\"No 'date' column found in the dataframe/series\")\n\n        if 'date' in df.index.names:\n            df = df.reset_index()\n\n        date_col = 'date' if 'date' in df.columns else df.index.names[0]\n\n        date_range = pd.date_range(start=df[date_col].min(), end=df[date_col].max(), freq=chunk_size)\n\n        for i in range(len(date_range) - 1):\n            start_date = date_range[i]\n            end_date = date_range[i + 1]\n            chunk = df[(df[date_col] >= start_date) & (df[date_col] < end_date)]\n            yield start_date, end_date, chunk_size, chunk", "idx": 791}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.exclude", "completion": "        if isinstance(range_obj, (pd.DatetimeIndex, tuple)):\n            range_obj = DateRange(range_obj[0], range_obj[-1])\n\n        range_obj = to_pandas_closed_closed(range_obj, add_tz=False)\n        start = range_obj.start\n        end = range_obj.end\n\n        if 'date' in data.index.names:\n            return data[(data.index.get_level_values('date') < start) | (data.index.get_level_values('date') > end)]\n        elif 'date' in data.columns:\n            return data[(data.date < start) | (data.date > end)]\n        else:\n            return data", "idx": 792}
{"namespace": "mopidy.httpclient.format_proxy", "completion": "    scheme = proxy_config.get(\"scheme\")\n    host = proxy_config.get(\"hostname\")\n    port = proxy_config.get(\"port\")\n    username = proxy_config.get(\"username\")\n    password = proxy_config.get(\"password\")\n\n    if not scheme or not host or not port:\n        return None\n\n    if auth and username and password:\n        return f\"{scheme}://{username}:{password}@{host}:{port}\"\n    else:\n        return f\"{scheme}://{host}:{port}\"", "idx": 793}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.filter", "completion": "        if isinstance(range_obj, (pd.DatetimeIndex, tuple)):\n            range_obj = DateRange(range_obj[0], range_obj[-1])\n        if 'date' in data.index.names:\n            return data[(data.index.get_level_values('date') >= range_obj.start) & (data.index.get_level_values('date') <= range_obj.end)]\n        elif 'date' in data.columns:\n            return data[(data.date >= range_obj.start) & (data.date <= range_obj.end)]\n        else:\n            return data", "idx": 794}
{"namespace": "mopidy.config.validators.validate_required", "completion": "    if required and value is None:\n        raise ValueError(\"must be set.\")", "idx": 795}
{"namespace": "mopidy.config.validators.validate_choice", "completion": "    if value not in choices:\n        raise ValueError(f\"must be one of {', '.join(map(repr, choices))}, not {value}.\")", "idx": 796}
{"namespace": "mopidy.config.validators.validate_minimum", "completion": "    if minimum is not None and value < minimum:\n        raise ValueError(f\"{value!r} must be larger than {minimum!r}.\")", "idx": 797}
{"namespace": "mopidy.config.validators.validate_maximum", "completion": "    if maximum is not None and value > maximum:\n        raise ValueError(f\"{value!r} must be smaller than {maximum!r}.\")", "idx": 798}
{"namespace": "mopidy.config.schemas._did_you_mean", "completion": "    if not choices:\n        return None\n\n    distances = [(choice, _levenshtein(name, choice)) for choice in choices]\n    distances.sort(key=lambda x: x[1])\n\n    if distances[0][1] <= 3:\n        return distances[0][0]\n    else:\n        return None", "idx": 799}
{"namespace": "mopidy.config.types.encode", "completion": "    if isinstance(value, str):\n        value = value.replace(\"\\\\\", \"\\\\\\\\\").replace(\"\\n\", \"\\\\n\").replace(\"\\t\", \"\\\\t\")\n        return value.encode(encoding=\"unicode-escape\")\n    elif isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n        value = value.replace(\"\\\\\", \"\\\\n\").replace(\"\\n\", \"\\\\n\").replace(\"\\t\", \"\\\\t\")\n        return value.encode(encoding=\"unicode-escape\")\n    else:\n        return value", "idx": 800}
{"namespace": "mopidy.config.types.decode", "completion": "    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(\n            char, char.encode(encoding=\"unicode-escape\").decode()\n        )\n\n    return value", "idx": 801}
{"namespace": "mopidy.config.types.ConfigValue.serialize", "completion": "        if value is None:\n            return \"\"\n        if isinstance(value, _TransformedValue):\n            value = value.original\n        return encode(value)", "idx": 802}
{"namespace": "mopidy.config.types.Boolean.serialize", "completion": "    if value is True:\n        return \"true\"\n    elif value is False or value is None:\n        return \"false\"\n    else:\n        raise ValueError(f\"{value!r} is not a boolean\")", "idx": 803}
{"namespace": "hypertools.tools.df2mat.df2mat", "completion": "    mat = pd.get_dummies(data).values\n    if return_labels:\n        labels = list(pd.get_dummies(data).columns)\n        return mat, labels\n    else:\n        return mat", "idx": 804}
{"namespace": "hypertools._shared.helpers.center", "completion": "    assert type(x) is list, \"Input data to center must be list\"\n    mean_x = np.mean(x)\n    return [i - mean_x for i in x]", "idx": 805}
{"namespace": "hypertools._shared.helpers.group_by_category", "completion": "    # Flatten the list if it contains any sublists\n    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n\n    # Create a sorted set of unique values and return the index of each value in the sorted set\n    categories = list(sorted(set(vals), key=list(vals).index))\n    return [categories.index(val) for val in vals]", "idx": 806}
{"namespace": "hypertools._shared.helpers.vals2colors", "completion": "    # flatten if list of lists\n    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n    colors = sns.color_palette(cmap, res)\n    norm_vals = np.interp(vals, (np.min(vals), np.max(vals)), (0, res - 1))\n    return [colors[int(val)] for val in norm_vals]", "idx": 807}
{"namespace": "hypertools._shared.helpers.vals2bins", "completion": "    # flatten if list of lists\n    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n\n    # map values to bins based on resolution\n    bins = np.digitize(vals, np.linspace(np.min(vals), np.max(vals)+1, res+1)) - 1\n    return bins", "idx": 808}
{"namespace": "hypertools._shared.helpers.interp_array", "completion": "    x = np.arange(len(arr))\n    x_new = np.linspace(0, len(arr) - 1, interp_val)\n    f = pchip(x, arr)\n    return f(x_new)", "idx": 809}
{"namespace": "hypertools._shared.helpers.parse_args", "completion": "    args_list = []\n    for i, item in enumerate(x):\n        if isinstance(args, (list, tuple)):\n            if len(args) == len(x):\n                args_list.append((item, args[i]))\n            else:\n                print(\"Error: Length of args must be the same as x\")\n                sys.exit(1)\n        else:\n            args_list.append((item, args))\n    return args_list", "idx": 810}
{"namespace": "hypertools._shared.helpers.parse_kwargs", "completion": "    args_list = []\n    for i, item in enumerate(x):\n        tmp = {}\n        for key, value in kwargs.items():\n            if isinstance(value, (tuple, list)):\n                if len(value) == len(x):\n                    tmp[key] = value[i]\n                else:\n                    print('Error: arguments must be a list of the same length as x')\n                    sys.exit(1)\n            else:\n                tmp[key] = value\n        args_list.append(tmp)\n    return args_list", "idx": 811}
{"namespace": "gif_for_cli.utils._get_default_display_mode", "completion": "    term = environ.get('TERM', '')\n    colorterm = environ.get('COLORTERM', '')\n\n    if 'truecolor' in term or 'truecolor' in colorterm:\n        return 'truecolor'\n    elif '256' in term or '256' in colorterm:\n        return '256fgbg'\n    else:\n        return 'nocolor'", "idx": 812}
{"namespace": "gif_for_cli.utils._pool_type", "completion": "    int_val = int(val)\n    if int_val <= 0:\n        raise argparse.ArgumentTypeError(\"Pool size must be greater than 0\")\n    return int_val", "idx": 813}
{"namespace": "gif_for_cli.generate.utils.get_avg_for_em", "completion": "    total_r = 0\n    total_g = 0\n    total_b = 0\n    total_pixels = 0\n\n    for j in range(y, y + cell_height):\n        for i in range(x, x + cell_width):\n            if i < len(px[0]) and j < len(px):\n                r, g, b = px[j][i]\n                total_r += r\n                total_g += g\n                total_b += b\n                total_pixels += 1\n\n    avg_r = total_r / total_pixels\n    avg_g = total_g / total_pixels\n    avg_b = total_b / total_pixels\n\n    return [avg_r, avg_g, avg_b]", "idx": 814}
{"namespace": "gif_for_cli.generate.utils.process_input_source", "completion": "    if input_source.startswith(\"https://tenor.com/view/\"):\n        gif_id = input_source.split(\"/\")[-1]\n        gif_url = f\"https://tenor.com/o{gif_id}\"\n    else:\n        params = {\n            \"q\": input_source,\n            \"key\": api_key,\n            \"limit\": 1\n        }\n        response = requests.get(\"https://api.tenor.com/v1/search\", params=params)\n        try:\n            gif_id = response.json()[\"results\"][0][\"id\"]\n            gif_url = f\"https://tenor.com/o{gif_id}\"\n        except (KeyError, JSONDecodeError, IndexError):\n            gif_url = \"\"\n    return gif_url", "idx": 815}
{"namespace": "hypertools._shared.helpers.reshape_data", "completion": "    if labels is not None:\n        reshaped_data = [np.vstack([x[i] for i in range(len(hue)) if hue[i] == category]) for category in set(hue)]\n        reshaped_labels = [np.array([labels[i] for i in range(len(hue)) if hue[i] == category]) for category in set(hue)]\n        return reshaped_data, reshaped_labels\n    else:\n        reshaped_data = [np.vstack([x[i] for i in range(len(hue)) if hue[i] == category]) for category in set(hue)]\n        return reshaped_data", "idx": 816}
{"namespace": "mingus.extra.lilypond.from_Note", "completion": "\n    if process_octaves:\n        result = note.name\n        if note.modifier:\n            result += note.modifier\n        if standalone:\n            return \"{ %s }\" % result\n        else:\n            return result\n    else:\n        return note.name", "idx": 817}
{"namespace": "mingus.extra.tablature._get_qsize", "completion": "    # Calculate the number of strings in the tuning\n    num_strings = len(tuning.tuning)\n\n    # Calculate the quarter note size based on the number of strings and the width\n    qsize = width / (4 * num_strings)\n\n    return qsize", "idx": 818}
{"namespace": "mingus.core.notes.augment", "completion": "    if note[-1] != \"b\":\n        return note + \"#\"\n    else:\n        return note[:-1]", "idx": 819}
{"namespace": "mingus.core.meter.valid_beat_duration", "completion": "        import math\n    import math\n    return math.log2(duration).is_integer()", "idx": 820}
{"namespace": "mingus.core.notes.diminish", "completion": "    if note[-1] != \"#\":\n        return note + \"b\"\n    else:\n        return note[:-1]", "idx": 821}
{"namespace": "mingus.core.intervals.invert", "completion": "    return interval[::-1]", "idx": 822}
{"namespace": "mingus.core.progressions.parse_string", "completion": "    roman_numeral = \"\"\n    acc = 0\n    suffix = \"\"\n\n    # Parse the input string\n    for char in progression:\n        if char in [\"b\", \"#\"]:\n            if char == \"b\":\n                acc -= 1\n            else:\n                acc += 1\n        elif char.isdigit():\n            suffix += char\n        else:\n            roman_numeral += char\n\n    return (roman_numeral, acc, suffix)", "idx": 823}
{"namespace": "exodus_bundler.bundling.bytes_to_int", "completion": "    return int.from_bytes(bytes, byteorder)", "idx": 824}
{"namespace": "exodus_bundler.templating.render_template", "completion": "    for key, value in context.items():\n        string = string.replace('{{' + key + '}}', str(value))\n    return string", "idx": 825}
{"namespace": "exodus_bundler.input_parsing.strip_pid_prefix", "completion": "    return re.sub(r'^\\[pid \\d+\\] ', '', line)", "idx": 826}
{"namespace": "fs.path.abspath", "completion": "    if not path.startswith(\"/\"):\n        return \"/\" + path\n    return path", "idx": 827}
{"namespace": "fs.path.combine", "completion": "    if not path2:\n        return path1\n    if path1.endswith(\"/\"):\n        return path1 + path2\n    return path1 + \"/\" + path2", "idx": 828}
{"namespace": "fs.path.split", "completion": "    pos = path.rfind(\"/\")\n    if pos == -1:\n        return \"\", path\n    return path[:pos], path[pos + 1:]", "idx": 829}
{"namespace": "fs.path.isparent", "completion": "    return path2.startswith(path1.rstrip(\"/\") + \"/\")", "idx": 830}
{"namespace": "fs.path.forcedir", "completion": "    if not path.endswith(\"/\"):\n        return path + \"/\"\n    return path", "idx": 831}
{"namespace": "fs.wildcard.match_any", "completion": "    if not patterns:\n        return True\n    return any(match(pattern, name) for pattern in patterns)", "idx": 832}
{"namespace": "fs.wildcard.imatch_any", "completion": "    if not patterns:\n        return True\n    return any(imatch(pattern, name) for pattern in patterns)", "idx": 833}
{"namespace": "wal_e.cmd.parse_boolean_envvar", "completion": "\n    if val.lower() in ('false', '0'):\n        return False\n    elif val.lower() in ('true', '1'):\n        return True\n    else:\n        raise ValueError(\"Invalid boolean value for environment variable\")", "idx": 834}
{"namespace": "wal_e.log_help.get_log_destinations", "completion": "    log_destinations = os.getenv('WALE_LOG_DESTINATION', 'stderr,syslog')\n    return log_destinations.split(',')", "idx": 835}
{"namespace": "wal_e.log_help.WalELogger._fmt_structured", "completion": "        if not d:\n            return ''\n\n        formatted_dict = ' '.join([f'{k}={v}' for k, v in sorted(d.items())])\n        return f'time={datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S.%f-00\")} pid={os.getpid()} {formatted_dict}'", "idx": 836}
{"namespace": "wal_e.tar_partition._fsync_files", "completion": "    for filename in filenames:\n        try:\n            with open(filename, 'r') as file:\n                os.fsync(file.fileno())\n        except OSError as e:\n            if e.errno == errno.EACCES:\n                # Permission denied, skip this file\n                continue\n            else:\n                raise\n        # Call fsync on the directory where the file is created\n        directory = os.path.dirname(filename)\n        try:\n            with open(directory, 'r') as dir:\n                os.fsync(dir.fileno())\n        except OSError as e:\n            if e.errno == errno.EACCES:\n                # Permission denied, skip this directory\n                continue\n            else:\n                raise", "idx": 837}
{"namespace": "wal_e.blobstore.file.calling_format.Bucket.list", "completion": "        path = os.path.join(\"/\", prefix.strip(\"/\"))\n        file_keys = []\n        for root, dirs, files in os.walk(path):\n            for f in files:\n                file_path = os.path.join(root, f)\n                file_keys.append(FileKey(bucket=self, name=file_path))\n        return file_keys", "idx": 838}
{"namespace": "pyinfra.operations.util.files.unix_path_join", "completion": "    return '/'.join([part.rstrip('/') for part in path_parts])", "idx": 839}
{"namespace": "pyinfra.operations.server.shell", "completion": "    if isinstance(commands, str):\n        commands = [commands]\n    for command in commands:\n        yield command", "idx": 840}
{"namespace": "pyinfra.api.util.try_int", "completion": "    try:\n        return int(value)\n    except (ValueError, TypeError):\n        return value", "idx": 841}
{"namespace": "mrjob.job.MRJob.mr_job_script", "completion": "        try:\n            # Get the file path of the MRJob class\n            return inspect.getsourcefile(cls)\n        except TypeError:\n            return None", "idx": 842}
{"namespace": "mrjob.compat.map_version", "completion": "    if isinstance(version_map, dict):\n        versions = sorted(version_map.keys(), key=LooseVersion)\n        for v in versions:\n            if LooseVersion(version) >= LooseVersion(v):\n                return version_map[v]\n        return version_map[versions[0]]\n    elif isinstance(version_map, list):\n        version_map.sort(key=lambda x: x[0])\n        for v, value in version_map:\n            if LooseVersion(version) >= v:\n                return value\n        return version_map[0][1]\n    else:\n        raise TypeError(\"version_map must be a map or a list of tuples\")", "idx": 843}
{"namespace": "mrjob.conf.combine_values", "completion": "    for value in reversed(values):\n        if value is not None:\n            return value\n    return None", "idx": 844}
{"namespace": "mrjob.protocol.BytesProtocol.read", "completion": "        \"\"\"\n        This function reads a line and splits it into two parts - key and value - using the tab delimiter. If there is only one part, it uses None as the value.\n        Input-Output Arguments\n        :param self: BytesProtocol. An instance of the BytesProtocol class.\n        :param line: Bytes. The line to be read and processed.\n        :return: Tuple. A tuple containing the key-value pair.\n        \"\"\"\n        key_value = line.split(b'\\t', 1)\n        if len(key_value) == 1:\n            key_value.append(None)\n\n        return tuple(key_value)", "idx": 845}
{"namespace": "mrjob.protocol.TextProtocol.write", "completion": "        \"\"\"\n        Write the key and value to the TextProtocol instance. It encodes the key and value to utf-8 and joins them with a tab character. If either key or value is None, it is not included in the string.\n        Input-Output Arguments\n        :param self: TextProtocol. An instance of the TextProtocol class.\n        :param key: The key to write to the instance.\n        :param value: The value to write to the instance.\n        :return: bytes. The encoded key and value joined by a tab character.\n        \"\"\"\n        if key is not None and value is not None:\n            return (key.encode('utf-8') + b'\\t' + value.encode('utf-8'))\n        elif key is not None:\n            return key.encode('utf-8')\n        elif value is not None:\n            return value.encode('utf-8')\n        else:\n            return b''", "idx": 846}
{"namespace": "mrjob.protocol.TextProtocol.read", "completion": "        try:\n            return tuple(x.decode('utf_8') if x is not None else None for x in line.split(b'\\t'))\n        except UnicodeDecodeError:\n            return tuple(x.decode('latin_1') if x is not None else None for x in line.split(b'\\t'))", "idx": 847}
{"namespace": "mrjob.protocol.TextValueProtocol.read", "completion": "        try:\n            line = line.decode('utf_8')\n        except UnicodeDecodeError:\n            line = line.decode('latin_1')\n\n        return (None, line)", "idx": 848}
{"namespace": "mrjob.util.file_ext", "completion": "    if '.' in filename:\n        return '.' + filename.split('.')[-1]\n    else:\n        return ''", "idx": 849}
{"namespace": "mrjob.util.cmd_line", "completion": "\n    return ' '.join(pipes.quote(arg) for arg in args)", "idx": 850}
{"namespace": "trafilatura.sitemaps.SitemapObject.extract_sitemap_links", "completion": "        for match in (m[1] for m in islice(LINK_REGEX.finditer(self.content), MAX_LINKS)):\n            self.handle_link(match)\n        LOGGER.debug('%s sitemaps and %s links found for %s', len(self.sitemap_urls), len(self.urls), self.sitemap_url)", "idx": 851}
{"namespace": "trafilatura.sitemaps.SitemapObject.process", "completion": "        \"\"\"\n        This function processes a sitemap object by downloading the sitemap and extracting the links it contains. It first checks if the sitemap is plausible. If not, it returns. Then, it tries to extract links from a TXT file. If the content does not match the format of a sitemap, it iterates through the content to find links and handles each link. If the content matches the format of an XML sitemap and a target language is specified, it extracts language links from the sitemap. If there are sitemap URLs or URLs extracted from the sitemap, it returns. Otherwise, it extracts the links from the sitemap.\n        Input-Output Arguments\n        :param self: SitemapObject. An instance of the SitemapObject class.\n        :return: None.\n        \"\"\"\n        if not is_plausible_sitemap(self.sitemap_url, self.content):\n            return\n\n        if self.sitemap_url.endswith('.txt'):\n            self.urls = filter_urls(DETECT_LINKS.findall(self.content), self.base_url)\n            LOGGER.debug('%s links found in sitemap TXT file: %s', len(self.urls), self.sitemap_url)\n            return\n\n        if not SITEMAP_FORMAT.match(self.content):\n            for link in DETECT_LINKS.findall(self.content):\n                self.handle_link(link)\n            return\n\n        if self.target_lang:\n            self.extract_sitemap_langlinks()\n            if self.sitemap_urls or self.urls:\n                return\n\n        self.extract_sitemap_links()", "idx": 852}
{"namespace": "trafilatura.sitemaps.is_plausible_sitemap", "completion": "    if contents is None:\n        return False\n    if SITEMAP_FORMAT.search(contents):\n        return True\n    return False", "idx": 853}
{"namespace": "trafilatura.sitemaps.extract_robots_sitemaps", "completion": "    sitemap_urls = []\n    if robotstxt:\n        lines = robotstxt.split('\\n')\n        for line in lines:\n            line = line.strip()\n            if not line.startswith('#') and line.lower().startswith('sitemap:'):\n                parts = line.split(':', 1)\n                if len(parts) == 2:\n                    sitemap_url = parts[1].strip()\n                    sitemap_url = fix_relative_urls(baseurl, sitemap_url)\n                    sitemap_urls.append(sitemap_url)\n    return sitemap_urls", "idx": 854}
{"namespace": "trafilatura.feeds.handle_link_list", "completion": "    output_links = []\n    for link in linklist:\n        link = fix_relative_urls(baseurl, link)\n        link = clean_url(link)\n        if link is None or link == baseurl or not check_url(link) or not link.startswith('http'):\n            continue\n        if target_lang and not link.endswith(f'/{target_lang}/'):\n            continue\n        if domainname not in link:\n            continue\n        output_links.append(link)\n    return output_links", "idx": 855}
{"namespace": "trafilatura.feeds.find_feed_urls", "completion": "    domainname, baseurl = get_hostinfo(url)\n    htmlstring = load_html(url)\n    if htmlstring is None:\n        LOGGER.debug('Invalid HTML/Feed page: %s', url)\n        return []\n    feed_urls = determine_feed(htmlstring, baseurl, url)\n    if not feed_urls:\n        LOGGER.debug('No feed found on %s', url)\n        return []\n    feed_links = []\n    for feed_url in feed_urls:\n        feed_content = load_html(feed_url)\n        feed_links.extend(extract_links(feed_content, domainname, baseurl, url, target_lang))\n    return sorted(set(feed_links))", "idx": 856}
{"namespace": "trafilatura.hashing.generate_hash_filename", "completion": "    clean_content = re.sub(CLEAN_XML, \"\", content)\n    hash_bytes = generate_bow_hash(clean_content, length=12)\n    encoded_hash = urlsafe_b64encode(hash_bytes).decode()\n    return encoded_hash", "idx": 857}
{"namespace": "trafilatura.cli_utils.download_queue_processing", "completion": "    errors = []\n    while not url_store.is_empty():\n        # load download buffer\n        bufferlist, url_store = load_download_buffer(url_store, config.getfloat('DEFAULT', 'SLEEP_TIME'))\n        # process downloads\n        with ThreadPoolExecutor(max_workers=args.parallel) as executor:\n            futures = [executor.submit(process_result, htmlstring, args, url, counter, config) for url, htmlstring in bufferlist]\n            for future in as_completed(futures):\n                if future.exception() is not None:\n                    errors.append(future.exception())\n                else:\n                    counter = future.result()\n    return errors, counter", "idx": 858}
{"namespace": "trafilatura.utils.decode_response", "completion": "\n    if isinstance(response, HTTPResponse):\n        filecontent = response.data\n    elif isinstance(response, bytes):\n        filecontent = response\n    else:\n        raise TypeError('Invalid input type. Expected urllib3 response object or bytes.')\n\n    # init\n    if isinstance(filecontent, str):\n        return filecontent\n    htmltext = None\n    # GZip and Brotli test\n    filecontent = handle_compressed_file(filecontent)\n    # encoding\n    for guessed_encoding in detect_encoding(filecontent):\n        try:\n            htmltext = filecontent.decode(guessed_encoding)\n        except (LookupError, UnicodeDecodeError):  # VISCII: lookup\n            LOGGER.warning('wrong encoding detected: %s', guessed_encoding)\n            htmltext = None\n        else:\n            break\n    # return original content if nothing else succeeded\n    return htmltext or str(filecontent, encoding='utf-8', errors='replace')", "idx": 859}
{"namespace": "trafilatura.utils.txttocsv", "completion": "\n    # Extract metadata from docmeta object\n    url = docmeta.get('url', '')\n    fingerprint = docmeta.get('fingerprint', '')\n    hostname = docmeta.get('hostname', '')\n    title = docmeta.get('title', '')\n    image = docmeta.get('image', '')\n    date = docmeta.get('date', '')\n    license = docmeta.get('license', '')\n    pagetype = docmeta.get('pagetype', '')\n\n    # Create CSV format\n    csv_data = f\"{url}\\t{fingerprint}\\t{hostname}\\t{title}\\t{image}\\t{date}\\t{license}\\t{pagetype}\\n\"\n    csv_data += f\"Text:\\n{text}\\n\"\n    csv_data += f\"Comments:\\n{comments}\\n\"\n\n    return csv_data", "idx": 860}
{"namespace": "trafilatura.utils.is_similar_domain", "completion": "\n    # Remove file extension from both strings\n    reference = STRIP_EXTENSION.sub('', reference)\n    new_string = STRIP_EXTENSION.sub('', new_string)\n\n    # Calculate similarity ratio using SequenceMatcher\n    similarity_ratio = SequenceMatcher(None, reference, new_string).ratio()\n\n    # Check if the similarity ratio is above the threshold\n    if similarity_ratio >= threshold:\n        return True\n    else:\n        return False", "idx": 861}
{"namespace": "trafilatura.xml.remove_empty_elements", "completion": "    for element in tree.iter():\n        if len(element) == 0 and (element.text is None or element.text.strip() == \"\") and (element.tail is None or element.tail.strip() == \"\"):\n            parent = element.getparent()\n            if parent is not None:\n                parent.remove(element)\n    return tree", "idx": 862}
{"namespace": "trafilatura.xml.strip_double_tags", "completion": "    for tag in NESTING_WHITELIST:\n        elements = tree.findall('.//' + tag)\n        for element in elements:\n            for child in element:\n                if child.tag in NESTING_WHITELIST:\n                    merge_with_parent(child)\n    return tree", "idx": 863}
{"namespace": "trafilatura.xml.check_tei", "completion": "\n    # Handle complex head elements\n    for element in xmldoc.xpath(\".//head\"):\n        new_element = _tei_handle_complex_head(element)\n        element.getparent().replace(element, new_element)\n\n    # Handle unwanted tails on p and ab elements\n    for element in xmldoc.xpath(\".//p | .//ab\"):\n        _handle_unwanted_tails(element)\n\n    # Handle text content of div nodes\n    for element in xmldoc.xpath(\".//div\"):\n        _handle_text_content_of_div_nodes(element)\n\n    # Wrap unwanted siblings of div\n    for element in xmldoc.xpath(\".//div\"):\n        _wrap_unwanted_siblings_of_div(element)\n\n    # Move element one level up\n    for element in xmldoc.xpath(\".//div\"):\n        _move_element_one_level_up(element)\n\n    return xmldoc", "idx": 864}
{"namespace": "trafilatura.xml.validate_tei", "completion": "    global TEI_RELAXNG\n    if TEI_RELAXNG is None:\n        with lzma.open(TEI_SCHEMA, 'rb') as schema_file:\n            TEI_RELAXNG = RelaxNG(load_pickle(schema_file.read()))\n    return TEI_RELAXNG.validate(xmldoc)", "idx": 865}
{"namespace": "trafilatura.xml.merge_with_parent", "completion": "    parent = element.getparent()\n    if parent is not None:\n        if element.text:\n            if parent.text:\n                parent.text += element.text\n            else:\n                parent.text = element.text\n        if element.tail:\n            if parent.tail:\n                parent.tail += element.tail\n            else:\n                parent.tail = element.tail\n        parent.remove(element)", "idx": 866}
{"namespace": "trafilatura.downloads._determine_headers", "completion": "    if headers is None:\n        headers = DEFAULT_HEADERS.copy()\n\n    user_agents, cookie = _parse_config(config)\n\n    if user_agents:\n        headers['User-Agent'] = random.choice(user_agents)\n\n    if cookie:\n        headers['Cookie'] = cookie\n\n    return headers", "idx": 867}
{"namespace": "trafilatura.meta.reset_caches", "completion": "    reset_caches_courlan()\n    reset_caches_htmldate()\n    define_stoplist.clear_cache()\n    gc.collect()", "idx": 868}
{"namespace": "trafilatura.core.handle_table", "completion": "\n    newtable = Element('table')\n    newrow = Element('row')\n    for child in table_elem.iter('*'):\n        if child.tag in {'thead', 'tbody', 'tfoot'}:\n            continue\n        if child.tag == 'tr':\n            if len(newrow) > 0:\n                newtable.append(newrow)\n                newrow = Element('row')\n        elif child.tag in {'td', 'th'}:\n            cell_element = define_cell_type(child)\n            for subchild in child.iter('*'):\n                if subchild.tag in potential_tags:\n                    processed_child = process_node(subchild, options)\n                    if processed_child is not None:\n                        cell_element.append(processed_child)\n            newrow.append(cell_element)\n    if len(newrow) > 0:\n        newtable.append(newrow)\n    if len(newtable) > 0:\n        return newtable\n    else:\n        return None", "idx": 869}
{"namespace": "trafilatura.filters.language_filter", "completion": "    if target_language:\n        detected_language = language_classifier(temp_text, temp_comments)\n        if detected_language and detected_language != target_language:\n            LOGGER.warning('Detected language (%s) does not match target language (%s)', detected_language, target_language)\n            return True, docmeta\n    return False, docmeta", "idx": 870}
{"namespace": "trafilatura.filters.textfilter", "completion": "\n    # Add your filtering conditions here\n    # For example:\n    # if some_condition:\n    #     return True\n    # else:\n    #     return False\n    # You can add multiple conditions and return True if any condition is met\n    # If no conditions are met, return False", "idx": 871}
{"namespace": "trafilatura.metadata.extract_meta_json", "completion": "\n    for elem in tree.xpath('.//script[@type=\"application/ld+json\" or @type=\"application/settings+json\"]'):\n        if elem.text:\n            try:\n                json_data = json.loads(elem.text)\n                if isinstance(json_data, dict):\n                    for key, value in json_data.items():\n                        if key in metadata.__slots__:\n                            setattr(metadata, key, value)\n            except json.JSONDecodeError:\n                # Extract metadata from the JSON text\n                json_text = elem.text\n                # Extract metadata from the JSON text (parse as needed)\n    return metadata", "idx": 872}
{"namespace": "trafilatura.external.try_justext", "completion": "    if JT_STOPLIST is None:\n        jt_stoplist_init()\n    stoplist = JT_STOPLIST\n    if target_language is not None:\n        stoplist = get_stoplist(target_language) or stoplist\n    try:\n        return custom_justext(tree, stoplist)\n    except Exception as err:\n        LOGGER.warning('justext failed: %s', err)\n        return None", "idx": 873}
{"namespace": "mongoengine.base.datastructures.BaseDict.get", "completion": "        try:\n            return self[key]\n        except KeyError:\n            return default", "idx": 874}
{"namespace": "sqlite_utils.utils.suggest_column_types", "completion": "    all_column_types = {}\n    for record in records:\n        for key, value in record.items():\n            if key not in all_column_types:\n                all_column_types[key] = {type(value)}\n            else:\n                all_column_types[key].add(type(value))\n    return types_for_column_types(all_column_types)", "idx": 875}
{"namespace": "sqlite_utils.plugins.get_plugins", "completion": "    plugins = []\n    for plugin_name in pm.list_name_plugin():\n        plugin = {\"name\": plugin_name}\n        hooks = pm.hookimpls.get(plugin_name, [])\n        plugin[\"hooks\"] = [hook.name for hook in hooks]\n        dist = pm.get_plugin_dist(plugin_name)\n        if dist:\n            plugin[\"version\"] = dist.version\n            plugin[\"project_name\"] = dist.project_name\n        plugins.append(plugin)\n    return plugins", "idx": 876}
{"namespace": "alembic.config.Config.print_stdout", "completion": "        if not self.messaging_opts.get(\"quiet\", False):\n            if arg:\n                text = text.format(*arg)\n            self.stdout.write(text + \"\\n\")", "idx": 877}
{"namespace": "alembic.config.Config.set_section_option", "completion": "        if not self.file_config.has_section(section):\n            self.file_config.add_section(section)\n        self.file_config.set(section, name, value)", "idx": 878}
{"namespace": "alembic.command.merge", "completion": "\n    script_directory = ScriptDirectory.from_config(config)\n\n    def do_merge(rev, context):\n        return script_directory._merge_revisions(\n            revisions, message=message, branch_label=branch_label, rev_id=rev_id\n        )\n\n    with EnvironmentContext(config, script_directory, fn=do_merge):\n        script_directory.run_env()", "idx": 879}
{"namespace": "alembic.command.upgrade", "completion": "\n    script = ScriptDirectory.from_config(config)\n    starting_rev = None\n    if \":\" in revision:\n        if not sql:\n            raise util.CommandError(\"Range revision not allowed\")\n        starting_rev, revision = revision.split(\":\", 2)\n    elif sql:\n        raise util.CommandError(\n            \"upgrade with --sql requires <fromrev>:<torev>\"\n        )\n\n    def upgrade(rev, context):\n        return script._upgrade_revs(revision, rev)\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=upgrade,\n        as_sql=sql,\n        starting_rev=starting_rev,\n        destination_rev=revision,\n        tag=tag,\n    ):\n        script.run_env()", "idx": 880}
{"namespace": "alembic.command.downgrade", "completion": "\n    script = ScriptDirectory.from_config(config)\n\n    starting_rev = None\n    if \":\" in revision:\n        if not sql:\n            raise util.CommandError(\"Range revision not allowed\")\n        starting_rev, revision = revision.split(\":\", 2)\n\n    def downgrade(rev, context):\n        return script._downgrade_revs(revision, rev)\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=downgrade,\n        as_sql=sql,\n        starting_rev=starting_rev,\n        destination_rev=revision,\n        tag=tag,\n    ):\n        script.run_env()", "idx": 881}
{"namespace": "alembic.command.history", "completion": "    script = ScriptDirectory.from_config(config)\n    revs = script.get_revisions(rev_range)\n    for sc in revs:\n        config.print_stdout(\n            sc.cmd_format(verbose, include_branches=True, tree_indicators=False)\n        )\n    if indicate_current:\n        current(config, verbose)", "idx": 882}
{"namespace": "alembic.command.stamp", "completion": "    script = ScriptDirectory.from_config(config)\n\n    def do_stamp(rev, context):\n        context.stamp(revision, purge=purge)\n        return []\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=do_stamp,\n        as_sql=sql,\n        tag=tag,\n    ):\n        script.run_env()", "idx": 883}
{"namespace": "alembic.command.ensure_version", "completion": "\n    script = ScriptDirectory.from_config(config)\n\n    def do_ensure_version(rev, context):\n        return script._ensure_version_table(rev)\n\n    with EnvironmentContext(config, script, fn=do_ensure_version, as_sql=sql):\n        script.run_env()", "idx": 884}
{"namespace": "alembic.autogenerate.compare._compare_server_default", "completion": "    conn_default = _render_server_default_for_compare(\n        conn_col.server_default, autogen_context\n    )\n    metadata_default = _render_server_default_for_compare(\n        metadata_col.server_default, autogen_context\n    )\n\n    if conn_default != metadata_default:\n        alter_column_op.modify_server_default = metadata_default\n        log.info(\n            \"Detected server default change from %r to %r on '%s.%s'\",\n            conn_default,\n            metadata_default,\n            tname,\n            cname,\n        )\n\n    return None", "idx": 885}
{"namespace": "alembic.autogenerate.render._render_server_default", "completion": "    rendered = _user_defined_render(\"server_default\", default, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    if default is None:\n        return None\n\n    if isinstance(default, Computed):\n        return _render_computed(default, autogen_context)\n\n    if isinstance(default, Identity):\n        return _render_identity(default, autogen_context)\n\n    if isinstance(default, sql.ClauseElement):\n        return _render_potential_expr(default, autogen_context)\n\n    if isinstance(default, ColumnElement):\n        return _render_potential_expr(default, autogen_context)\n\n    if isinstance(default, str):\n        if repr_:\n            return default.strip(\"'\")\n        else:\n            return default\n\n    return None", "idx": 886}
{"namespace": "alembic.autogenerate.render._render_constraint", "completion": "    renderer = _constraint_renderers.dispatch(constraint)\n    if renderer:\n        return renderer(constraint, autogen_context, namespace_metadata)\n    else:\n        return f\"Unknown Python object: {type(constraint).__name__}\"", "idx": 887}
{"namespace": "alembic.autogenerate.render._render_unique_constraint", "completion": "    rendered = _user_defined_render(\"unique_constraint\", constraint, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    opts = []\n    if constraint.name:\n        opts.append(\n            (\"name\", repr(_render_gen_name(autogen_context, constraint.name)))\n        )\n\n    return (\n        \"%(prefix)sUniqueConstraint([%(cols)s], %(args)s)\"\n        % {\n            \"prefix\": _sqlalchemy_autogenerate_prefix(autogen_context),\n            \"cols\": \", \".join(\n                \"%r\" % _ident(col.name) for col in constraint.columns\n            ),\n            \"args\": \", \".join(\n                [\"%s=%s\" % (kwname, val) for kwname, val in opts]\n            ),\n        }\n    )", "idx": 888}
{"namespace": "alembic.autogenerate.render._render_check_constraint", "completion": "    rendered = _user_defined_render(\"check_constraint\", constraint, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    if constraint.parent is not None and constraint.parent.table is not None:\n        if constraint in constraint.parent.table.constraints:\n            return None\n\n    opts = []\n    if constraint.name:\n        opts.append(\n            (\"name\", repr(_render_gen_name(autogen_context, constraint.name)))\n        )\n\n    return (\n        \"%(prefix)sCheckConstraint(%(sql)s, %(args)s)\"\n        % {\n            \"prefix\": _sqlalchemy_autogenerate_prefix(autogen_context),\n            \"sql\": _render_potential_expr(constraint.sqltext, autogen_context),\n            \"args\": \", \".join(\n                [\"%s=%s\" % (kwname, val) for kwname, val in opts]\n            ),\n        }\n    )", "idx": 889}
{"namespace": "alembic.autogenerate.api.compare_metadata", "completion": "\n    with context.begin_transaction():\n        insp = inspect(context.connection)\n        diff = compare._compare_schemas(\n            insp, metadata, context.opts.get(\"comparison_opts\", {})\n        )\n        return diff", "idx": 890}
{"namespace": "alembic.autogenerate.api.AutogenContext._within_batch", "completion": "\n        self._has_batch = True\n        yield\n        self._has_batch = False", "idx": 891}
{"namespace": "alembic.util.sqla_compat._connectable_has_table", "completion": "\n    if sqla_14:\n        with connectable.connect() as conn:\n            return conn.dialect.has_table(conn, tablename, schema=schemaname)\n    else:\n        insp = inspect(connectable)\n        return insp.has_table(tablename, schema=schemaname)", "idx": 892}
{"namespace": "alembic.util.sqla_compat._get_constraint_final_name", "completion": "    if constraint.name:\n        if sqla_14:\n            assert dialect is not None\n            return dialect.identifier_preparer.format_constraint(\n                constraint, _alembic_quote=False\n            )\n        else:\n            return constraint.name\n    else:\n        return None", "idx": 893}
{"namespace": "alembic.testing.env.env_file_fixture", "completion": "    dir_ = os.path.join(_get_staging_directory(), \"scripts\")\n    path = os.path.join(dir_, \"env.py\")\n    with open(path, \"w\") as f:\n        f.write(txt)", "idx": 894}
{"namespace": "alembic.testing.env._no_sql_testing_config", "completion": "    dir_ = os.path.join(_get_staging_directory(), \"scripts\")\n    sqlalchemy_future = \"future\" in config.db.__class__.__module__\n\n    return _write_config_file(\n        \"\"\"\n[alembic]\nscript_location = %s\nsqlalchemy.url = %s\nsqlalchemy.future = %s\nsourceless = false\n%s\n\n[loggers]\nkeys = root\n\n[handlers]\nkeys = console\n\n[logger_root]\nlevel = WARN\nhandlers = console\nqualname =\n\n[handler_console]\nclass = StreamHandler\nargs = (sys.stderr,)\nlevel = NOTSET\nformatter = generic\n\n[formatters]\nkeys = generic\n\n[formatter_generic]\nformat = %%(levelname)-5.5s [%%(name)s] %%(message)s\ndatefmt = %%H:%%M:%%S\n        \"\"\"\n        % (\n            dir_,\n            f\"{dialect}:///{dir_}/foo.db\",\n            \"true\" if sqlalchemy_future else \"false\",\n            directives,\n        )\n    )", "idx": 895}
{"namespace": "alembic.testing.env._write_config_file", "completion": "\n    cfg = _testing_config()\n    with open(os.path.join(_get_staging_directory(), \"alembic.ini\"), \"w\") as file:\n        file.write(text)\n    return cfg", "idx": 896}
{"namespace": "alembic.testing.env.three_rev_fixture", "completion": "\n    a = util.rev_id()\n    b = util.rev_id()\n    c = util.rev_id()\n\n    script = ScriptDirectory.from_config(cfg)\n    script.generate_revision(a, \"revision a\", head=None, refresh=True)\n    write_script(\n        script,\n        a,\n        \"\"\"\\\n\"Rev A\"\nrevision = '%s'\ndown_revision = None\n\nfrom alembic import op\n\n\ndef upgrade():\n    op.execute(\"CREATE STEP 1\")\n\n\ndef downgrade():\n    op.execute(\"DROP STEP 1\")\n\n\"\"\"\n        % a,\n    )\n\n    script.generate_revision(b, \"revision b from a\", head=a, refresh=True)\n    write_script(\n        script,\n        b,\n        \"\"\"\\\n\"Rev B\"\nrevision = '%s'\ndown_revision = '%s'\n\nfrom alembic import op\n\n\ndef upgrade():\n    op.execute(\"CREATE STEP 2\")\n\n\ndef downgrade():\n    op.execute(\"DROP STEP 2\")\n\n\"\"\"\n        % (b, a),\n    )\n\n    script.generate_revision(c, \"revision c from b\", head=b, refresh=True)\n    write_script(\n        script,\n        c,\n        \"\"\"\\\n\"Rev C\"\nrevision = '%s'\ndown_revision = '%s'\n\nfrom alembic import op\n\n\ndef upgrade():\n    op.execute(\"CREATE STEP 3\")\n\n\ndef downgrade():\n    op.execute(\"DROP STEP 3\")\n\n\"\"\"\n        % (c, b),\n    )\n\n    return a, b, c", "idx": 897}
{"namespace": "alembic.testing.env.multi_heads_fixture", "completion": "\n    d = util.rev_id()\n    e = util.rev_id()\n    f = util.rev_id()\n\n    script = ScriptDirectory.from_config(cfg)\n    script.generate_revision(d, \"revision d\", refresh=True, head=c)\n    write_script(\n        script,\n        d,\n        \"\"\"\\\n\"Rev D\"\nrevision = '%s'\ndown_revision = '%s'\n\nfrom alembic import op\n\n\ndef upgrade():\n    op.execute(\"CREATE STEP 4\")\n\n\ndef downgrade():\n    op.execute(\"DROP STEP 4\")\n\n\"\"\"\n        % (d, c),\n    )\n\n    script.generate_revision(e, \"revision e\", refresh=True, head=d)\n    write_script(\n        script,\n        e,\n        \"\"\"\\\n\"Rev E\"\nrevision = '%s'\ndown_revision = '%s'\n\nfrom alembic import op\n\n\ndef upgrade():\n    op.execute(\"CREATE STEP 5\")\n\n\ndef downgrade():\n    op.execute(\"DROP STEP 5\")\n\n\"\"\"\n        % (e, d),\n    )\n\n    script.generate_revision(f, \"revision f\", refresh=True, head=e)\n    write_script(\n        script,\n        f,\n        \"\"\"\\\n\"Rev F\"\nrevision = '%s'\ndown_revision = '%s'\n\nfrom alembic import op\n\n\ndef upgrade():\n    op.execute(\"CREATE STEP 6\")\n\n\ndef downgrade():\n    op.execute(\"DROP STEP 6\")\n\n\"\"\"\n        % (f, e),\n    )\n\n    return d, e, f", "idx": 898}
{"namespace": "alembic.testing.fixtures.capture_db", "completion": "    buf = io.StringIO()\n    eng = testing.create_engine(dialect, options={\"buffer\": buf})\n    return eng, buf", "idx": 899}
{"namespace": "alembic.testing.fixtures.capture_engine_context_buffer", "completion": "    if kw.pop(\"bytes_io\", False):\n        buf = io.BytesIO()\n    else:\n        buf = io.StringIO()\n\n    kw.update({\"dialect_name\": \"sqlite\", \"output_buffer\": buf})\n    conf = EnvironmentContext.configure\n\n    def configure(*arg, **opt):\n        opt.update(**kw)\n        return conf(*arg, **opt)\n\n    with mock.patch.object(EnvironmentContext, \"configure\", configure):\n        yield buf", "idx": 900}
{"namespace": "alembic.operations.schemaobj.SchemaObjects.unique_constraint", "completion": "        t = sa_schema.Table(\n            source,\n            self.metadata(),\n            *[\n                sa_schema.Column(n, NULLTYPE)\n                for n in util.unique_list(local_cols)\n            ],\n            schema=schema,\n        )\n        uc = sa_schema.UniqueConstraint(*[t.c[n] for n in local_cols], name=name, **kw)\n        t.append_constraint(uc)\n        return uc", "idx": 901}
{"namespace": "alembic.operations.schemaobj.SchemaObjects.index", "completion": "        t = sa_schema.Table(\n            tablename,\n            self.metadata(),\n            *[sa_schema.Column(n, NULLTYPE) for n in columns],\n            schema=schema,\n        )\n        kw[\"name\"] = name\n        idx = sa_schema.Index(*[t.c[n] for n in columns], **kw)\n        t.append_constraint(idx)\n        return idx", "idx": 902}
{"namespace": "alembic.operations.ops.DropConstraintOp.from_constraint", "completion": "        if isinstance(constraint, ForeignKeyConstraint):\n            return DropConstraintOp(\n                constraint_name=constraint.name,\n                table_name=constraint.parent.name,\n                type_=\"foreignkey\",\n                schema=constraint.parent.schema,\n            )\n        elif isinstance(constraint, PrimaryKeyConstraint):\n            return DropConstraintOp(\n                constraint_name=constraint.name,\n                table_name=constraint.table.name,\n                type_=\"primary\",\n                schema=constraint.table.schema,\n            )\n        elif isinstance(constraint, UniqueConstraint):\n            return DropConstraintOp(\n                constraint_name=constraint.name,\n                table_name=constraint.table.name,\n                type_=\"unique\",\n                schema=constraint.table.schema,\n            )\n        elif isinstance(constraint, CheckConstraint):\n            return DropConstraintOp(\n                constraint_name=constraint.name,\n                table_name=constraint.table.name,\n                type_=\"check\",\n                schema=constraint.table.schema,\n            )\n        else:\n            raise ValueError(\"Unsupported constraint type: {}\".format(type(constraint)))", "idx": 903}
{"namespace": "alembic.operations.ops.DropConstraintOp.to_constraint", "completion": "        if self._reverse is not None:\n            return self._reverse.to_constraint()\n        else:\n            raise ValueError(\"Reverse operation is not present\")", "idx": 904}
{"namespace": "alembic.operations.ops.CreatePrimaryKeyOp.to_constraint", "completion": "\n        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.primary_key_constraint(\n            self.constraint_name,\n            self.table_name,\n            self.columns,\n            schema=self.schema,\n            **self.kw,\n        )", "idx": 905}
{"namespace": "alembic.operations.ops.CreateIndexOp.from_index", "completion": "        constraint_name = index.name\n        table_name = index.table.name\n        columns = [c.name for c in index.columns]\n        schema = index.table.schema\n        unique = index.unique\n\n        op = cls(\n            constraint_name,\n            table_name,\n            columns,\n            schema=schema,\n            unique=unique\n        )\n        return op", "idx": 906}
{"namespace": "alembic.operations.ops.DropIndexOp.from_index", "completion": "        return cls(\n            index.table.name,\n            index.name,\n            schema=index.table.schema,\n            _reverse=CreateIndexOp.from_index(index),\n        )", "idx": 907}
{"namespace": "alembic.operations.ops.DropIndexOp.to_index", "completion": "        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.index(\n            self.index_name,\n            self.table_name,\n            *self.columns,\n            schema=self.schema,\n            **self.kw\n        )", "idx": 908}
{"namespace": "alembic.operations.ops.CreateTableOp.from_table", "completion": "        return cls(\n            table.name,\n            table.columns,\n            schema=table.schema,\n            _namespace_metadata=_namespace_metadata,\n            _constraints_included=True,\n            comment=table.comment,\n            info=table.info,\n            prefixes=table._prefixes if table._prefixes else [],\n        )", "idx": 909}
{"namespace": "alembic.operations.ops.DropTableOp.from_table", "completion": "        if _namespace_metadata is None:\n            _namespace_metadata = table.metadata\n\n        return cls(\n            table.name,\n            schema=table.schema,\n            table_kw={\"comment\": table.comment, \"info\": dict(table.info)},\n            _reverse=CreateTableOp.from_table(table, _namespace_metadata),\n        )", "idx": 910}
{"namespace": "alembic.operations.ops.DropTableOp.to_table", "completion": "        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.table(\n            self.table_name,\n            schema=self.schema,\n            comment=self.comment,\n            info=self.info.copy() if self.info else {},\n            prefixes=list(self.prefixes) if self.prefixes else [],\n            **self.table_kw,\n        )", "idx": 911}
{"namespace": "alembic.operations.ops.AlterColumnOp.to_diff_tuple", "completion": "        return (\n            \"alter_column\",\n            self.table_name,\n            self.column_name,\n            self.existing_type,\n            self.existing_nullable,\n            self.existing_server_default,\n            self.existing_comment,\n        )", "idx": 912}
{"namespace": "alembic.operations.ops.AddColumnOp.reverse", "completion": "        if self._reverse is None:\n            raise ValueError(\n                \"operation is not reversible; \"\n                \"original column is not present\"\n            )\n\n        return DropColumnOp.from_column_and_tablename(\n            self.schema, self.table_name, self._reverse.column\n        )", "idx": 913}
{"namespace": "alembic.operations.ops.DropColumnOp.reverse", "completion": "        if self._reverse is None:\n            raise ValueError(\"Reverse operation is not available.\")\n        return self._reverse", "idx": 914}
{"namespace": "alembic.operations.ops.DropColumnOp.from_column_and_tablename", "completion": "        return cls(tname, col.name, schema=schema)", "idx": 915}
{"namespace": "alembic.operations.ops.DropColumnOp.to_column", "completion": "\n        if self._reverse:\n            return self._reverse.column\n        else:\n            if migration_context:\n                schema_obj = schemaobj.SchemaObjects(migration_context)\n                return schema_obj.column(self.column_name, NULLTYPE)\n            else:\n                raise ValueError(\"MigrationContext is required to create a column without a reverse option.\")", "idx": 916}
{"namespace": "alembic.script.revision.RevisionMap.heads", "completion": "        self._revision_map\n        return self.heads", "idx": 917}
{"namespace": "alembic.script.revision.RevisionMap.add_revision", "completion": "        if _replace:\n            self._revision_map[revision.revision] = revision\n        else:\n            if revision.revision in self._revision_map:\n                raise RevisionError(f\"Revision {revision.revision} already exists\")\n            self._revision_map[revision.revision] = revision", "idx": 918}
{"namespace": "alembic.script.revision.RevisionMap.get_revisions", "completion": "        if isinstance(id_, str):\n            return (self._revision_map.get(id_),)\n        elif isinstance(id_, (list, tuple)):\n            return tuple(self._revision_map.get(i) for i in id_)\n        else:\n            return ()", "idx": 919}
{"namespace": "alembic.script.revision.RevisionMap.get_revision", "completion": "        resolved_id, branch_label = self._resolve_revision_number(id_)\n        revisions = self.get_revisions(resolved_id)\n        if len(revisions) > 1:\n            raise MultipleHeads(revisions, id_)\n        elif revisions:\n            return revisions[0]\n        else:\n            return None", "idx": 920}
{"namespace": "alembic.script.revision.RevisionMap.filter_for_lineage", "completion": "        if check_against is None:\n            return tuple(targets)\n        else:\n            result = []\n            for target in targets:\n                if self._shares_lineage(\n                    target, check_against, include_dependencies\n                ):\n                    result.append(target)\n            return tuple(result)", "idx": 921}
{"namespace": "alembic.script.revision.RevisionMap.iterate_revisions", "completion": "        if select_for_downgrade:\n            revisions, heads = self._collect_downgrade_revisions(\n                upper, lower, inclusive, implicit_base, assert_relative_length\n            )\n        else:\n            revisions, heads = self._collect_upgrade_revisions(\n                upper, lower, inclusive, implicit_base, assert_relative_length\n            )\n\n        if not revisions:\n            return iter(())\n\n        if select_for_downgrade:\n            return iter(\n                self._topological_sort(revisions, heads)\n            )\n        else:\n            return iter(\n                self._topological_sort(revisions, heads)\n            )", "idx": 922}
{"namespace": "alembic.script.revision.RevisionMap._topological_sort", "completion": "        # Create a dictionary to store the number of incoming edges for each revision\n        in_degree: Dict[str, int] = {}\n\n        # Create a dictionary to store the list of revisions that each revision is dependent on\n        dependencies: Dict[str, List[str]] = {}\n\n        # Initialize the in_degree and dependencies dictionaries\n        for rev in revisions:\n            rev_id = rev.revision\n            in_degree[rev_id] = 0\n            dependencies[rev_id] = []\n\n        # Populate the in_degree and dependencies dictionaries based on the dependencies of each revision\n        for rev in revisions:\n            rev_id = rev.revision\n            for dep in rev._all_down_revisions:\n                dependencies[dep].append(rev_id)\n                in_degree[rev_id] += 1\n\n        # Perform topological sort using Kahn's algorithm\n        queue: Deque[str] = deque()\n        for rev_id, degree in in_degree.items():\n            if degree == 0:\n                queue.append(rev_id)\n\n        sorted_revisions: List[str] = []\n        while queue:\n            current_rev = queue.popleft()\n            sorted_revisions.append(current_rev)\n            for dep in dependencies[current_rev]:\n                in_degree[dep] -= 1\n                if in_degree[dep] == 0:\n                    queue.append(dep)\n\n        # Check for cycles\n        if len(sorted_revisions) != len(revisions):\n            raise CycleDetected(\"Cycle detected in revisions\")\n\n        return sorted_revisions", "idx": 923}
{"namespace": "alembic.script.revision.Revision._all_down_revisions", "completion": "        return util.dedupe_tuple(\n            util.to_tuple(self.down_revision, default=())\n            + self._resolved_dependencies\n        )", "idx": 924}
{"namespace": "alembic.script.revision.Revision._normalized_down_revisions", "completion": "\n        return self._versioned_down_revisions", "idx": 925}
{"namespace": "alembic.script.write_hooks._invoke", "completion": "    formatter = _registry.get(name)\n    if formatter is None:\n        raise util.CommandError(f\"No formatter with name '{name}' registered\")\n    formatter(revision, **options)", "idx": 926}
{"namespace": "bplustree.memory.FileMemory.get_node", "completion": "        if page in self._cache:\n            return self._cache[page]\n        else:\n            data = self.get_page(page)\n            node = Node.load(data)\n            self._cache[page] = node\n            return node", "idx": 927}
{"namespace": "bplustree.memory.FileMemory.next_available_page", "completion": "        self._lock.writer_lock.acquire()\n        try:\n            self.last_page += 1\n            return self.last_page\n        finally:\n            self._lock.writer_lock.release()", "idx": 928}
{"namespace": "bplustree.memory.FileMemory.get_metadata", "completion": "        data = read_from_file(self._fd, 0, self._tree_conf.page_size)\n        root_node_page = int.from_bytes(data[:PAGE_REFERENCE_BYTES], ENDIAN)\n        page_size = int.from_bytes(data[PAGE_REFERENCE_BYTES:PAGE_REFERENCE_BYTES + OTHERS_BYTES], ENDIAN)\n        order = int.from_bytes(data[PAGE_REFERENCE_BYTES + OTHERS_BYTES:PAGE_REFERENCE_BYTES + 2 * OTHERS_BYTES], ENDIAN)\n        key_size = int.from_bytes(data[PAGE_REFERENCE_BYTES + 2 * OTHERS_BYTES:PAGE_REFERENCE_BYTES + 3 * OTHERS_BYTES], ENDIAN)\n        value_size = int.from_bytes(data[PAGE_REFERENCE_BYTES + 3 * OTHERS_BYTES:PAGE_REFERENCE_BYTES + 4 * OTHERS_BYTES], ENDIAN)\n        tree_conf = TreeConf(page_size, order, key_size, value_size)\n        return root_node_page, tree_conf", "idx": 929}
{"namespace": "bplustree.memory.FileMemory.set_metadata", "completion": "        metadata = (\n            root_node_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN) +\n            tree_conf.page_size.to_bytes(OTHERS_BYTES, ENDIAN) +\n            tree_conf.order.to_bytes(OTHERS_BYTES, ENDIAN) +\n            tree_conf.key_size.to_bytes(OTHERS_BYTES, ENDIAN) +\n            tree_conf.value_size.to_bytes(OTHERS_BYTES, ENDIAN)\n        )\n        self._write_page_in_tree(0, metadata)", "idx": 930}
{"namespace": "bplustree.memory.WAL.checkpoint", "completion": "        if self._not_committed_pages:\n            logger.warning('WAL has uncommitted data, discarding it')\n\n        fsync_file_and_dir(self._fd.fileno(), self._dir_fd)\n\n        for page, page_data in self._committed_pages.items():\n            yield page, read_from_file(self._fd, page_data, page_data + self._page_size)\n\n        self._fd.close()\n        os.remove(self.filename)\n\n        if self._dir_fd is not None:\n            fsync_file_and_dir(self._dir_fd, None)", "idx": 931}
{"namespace": "bplustree.memory.WAL.commit", "completion": "        if self._not_committed_pages:\n            self._add_frame(FrameType.COMMIT)", "idx": 932}
{"namespace": "bplustree.memory.WAL.rollback", "completion": "        if self._not_committed_pages:\n            self._add_frame(FrameType.ROLLBACK)", "idx": 933}
{"namespace": "bplustree.entry.Record.dump", "completion": "        key_as_bytes = self._tree_conf.serializer.serialize(\n            self.key, self._tree_conf.key_size\n        )\n        used_key_length = len(key_as_bytes)\n\n        if self.overflow_page:\n            value_length = 0\n            value_as_bytes = b''\n        else:\n            value_as_bytes = self.value\n            value_length = len(value_as_bytes)\n\n        data = (\n            used_key_length.to_bytes(USED_KEY_LENGTH_BYTES, ENDIAN) +\n            key_as_bytes +\n            value_length.to_bytes(USED_VALUE_LENGTH_BYTES, ENDIAN) +\n            value_as_bytes +\n            self.overflow_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN)\n        )\n        return data", "idx": 934}
{"namespace": "bplustree.entry.Reference.__repr__", "completion": "        return '<Reference: key={} before={} after={}>'.format(self.key, self.before, self.after)", "idx": 935}
{"namespace": "bplustree.node.Node.dump", "completion": "        data = bytearray()\n        for entry in self.entries:\n            data.extend(entry.dump())\n\n        used_page_length = NODE_TYPE_BYTES + USED_PAGE_LENGTH_BYTES + len(data) + PAGE_REFERENCE_BYTES\n        data = (self._node_type_int.to_bytes(NODE_TYPE_BYTES, ENDIAN) +\n                used_page_length.to_bytes(USED_PAGE_LENGTH_BYTES, ENDIAN) +\n                (self.next_page or 0).to_bytes(PAGE_REFERENCE_BYTES, ENDIAN) +\n                data)\n\n        if len(data) < self._tree_conf.page_size:\n            data += b'\\x00' * (self._tree_conf.page_size - len(data))\n\n        return data", "idx": 936}
{"namespace": "bplustree.node.Node._find_entry_index", "completion": "        entry = self._entry_class(self._tree_conf, key=key)\n        return bisect.bisect_left(self.entries, entry)", "idx": 937}
{"namespace": "bplustree.node.Node.from_page_data", "completion": "        node_type = int.from_bytes(data[:NODE_TYPE_BYTES], ENDIAN)\n        if node_type == 1:\n            return LonelyRootNode(tree_conf, data, page)\n        elif node_type == 2:\n            return RootNode(tree_conf, data, page)\n        elif node_type == 3:\n            return InternalNode(tree_conf, data, page)\n        elif node_type == 4:\n            return LeafNode(tree_conf, data, page)\n        else:\n            raise ValueError('Unknown node type: {}'.format(node_type))", "idx": 938}
{"namespace": "bplustree.tree.BPlusTree._root_node", "completion": "        return self._mem.get_node(self._root_node_page)", "idx": 939}
{"namespace": "bplustree.tree.BPlusTree._left_record_node", "completion": "        node = self._root_node\n        while not isinstance(node, (LonelyRootNode, LeafNode)):\n            node = self._mem.get_node(node.entries[0].before)\n        return node", "idx": 940}
{"namespace": "mopidy.ext.Extension.get_cache_dir", "completion": "        \"\"\"Get or create cache directory for the extension.\n\n        :param config: the Mopidy config object\n        :return: pathlib.Path\n        \"\"\"\n        if cls.ext_name is None:\n            raise AssertionError\n        cache_dir_path = (\n            path.expand_path(config[\"core\"][\"cache_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(cache_dir_path)\n        return cache_dir_path", "idx": 941}
{"namespace": "mopidy.ext.Extension.get_config_dir", "completion": "        if cls.ext_name is None:\n            raise AssertionError\n        config_dir_path = (\n            path.expand_path(config[\"core\"][\"config_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(config_dir_path)\n        return config_dir_path", "idx": 942}
{"namespace": "mopidy.ext.Extension.get_data_dir", "completion": "        if cls.ext_name is None:\n            raise AssertionError\n        data_dir_path = (\n            path.expand_path(config[\"core\"][\"data_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(data_dir_path)\n        return data_dir_path", "idx": 943}
{"namespace": "fs.time.epoch_to_datetime", "completion": "    if t is None:\n        return None\n    else:\n        return datetime.utcfromtimestamp(t)", "idx": 944}
{"namespace": "fs.path.normpath", "completion": "    path = path.replace(\"//\", \"/\")  # Remove duplicated separators\n    while _requires_normalization(path):\n        path = path.replace(\"/./\", \"/\")  # Remove current directory references\n        path = path.replace(\"/../\", \"/\")  # Remove back-references\n        path = path.replace(\"/..\", \"\")  # Remove back-references at the beginning of the path\n    if path == \"\":\n        return \"/\"  # Return root path if the normalized path is empty\n    return path", "idx": 945}
{"namespace": "fs.path.iteratepath", "completion": "    return [component for component in normpath(path).split(\"/\") if component]", "idx": 946}
{"namespace": "fs.path.recursepath", "completion": "    components = iteratepath(path)\n    if reverse:\n        components.reverse()\n    paths = [\"/\" + \"/\".join(components[:i+1]) for i in range(len(components))]\n    return paths", "idx": 947}
{"namespace": "fs.path.join", "completion": "    return \"/\".join([normpath(p).strip(\"/\") for p in paths])", "idx": 948}
{"namespace": "fs.path.parts", "completion": "    path = normpath(path)\n    if not path:\n        return []\n    return path.split(\"/\")", "idx": 949}
{"namespace": "fs.path.splitext", "completion": "    pos = path.rfind(\".\")\n    if pos == -1:\n        return path, \"\"\n    else:\n        return path[:pos], path[pos:]", "idx": 950}
{"namespace": "fs.path.isbase", "completion": "    return normpath(path2).startswith(normpath(path1))", "idx": 951}
{"namespace": "fs.path.frombase", "completion": "    if not isparent(path1, path2):\n        raise ValueError(f\"{path1} is not a parent directory of {path2}\")\n    return normpath(path2[len(path1):])", "idx": 952}
{"namespace": "fs.path.relativefrom", "completion": "    base_parts = iteratepath(normpath(base))\n    path_parts = iteratepath(normpath(path))\n\n    i = 0\n    for i, (base_part, path_part) in enumerate(zip(base_parts, path_parts)):\n        if base_part != path_part:\n            break\n    else:\n        i += 1\n\n    rel_parts = [\"..\"] * (len(base_parts) - i) + path_parts[i:]\n    if not rel_parts:\n        return \".\"\n    return \"/\".join(rel_parts)", "idx": 953}
{"namespace": "fs.path.iswildcard", "completion": "    return any(char in _WILD_CHARS for char in path)", "idx": 954}
{"namespace": "fs.wildcard.match", "completion": "    try:\n        re_pat = _PATTERN_CACHE[(pattern, True)]\n    except KeyError:\n        res = \"(?ms)\" + _translate(pattern, case_sensitive=True) + r\"\\Z\"\n        _PATTERN_CACHE[(pattern, True)] = re_pat = re.compile(res)\n    return re_pat.match(name) is not None", "idx": 955}
{"namespace": "fs.wildcard.imatch", "completion": "    try:\n        re_pat = _PATTERN_CACHE[(pattern, False)]\n    except KeyError:\n        res = \"(?i)(?ms)\" + _translate(pattern, False) + r\"\\Z\"\n        _PATTERN_CACHE[(pattern, False)] = re_pat = re.compile(res)\n    return re_pat.match(name) is not None", "idx": 956}
{"namespace": "fs.wildcard.get_matcher", "completion": "    if case_sensitive:\n        return partial(match_any, patterns)\n    else:\n        return partial(imatch_any, patterns)", "idx": 957}
{"namespace": "fs._url_tools.url_quote", "completion": "\n    if _WINDOWS_PLATFORM and _has_drive_letter(path_snippet):\n        drive, tail = os.path.splitdrive(path_snippet)\n        return drive + six.moves.urllib.parse.quote(tail)\n    else:\n        return six.moves.urllib.parse.pathname2url(path_snippet)", "idx": 958}
{"namespace": "fs._ftp_parse.parse", "completion": "\n    parsed_info = []\n    for line in lines:\n        if line.strip():  # Check if the line is not blank\n            parsed_line = parse_line(line)\n            if parsed_line:\n                parsed_info.append(parsed_line)\n    return parsed_info", "idx": 959}
{"namespace": "fs._ftp_parse._parse_time", "completion": "    for fmt in formats:\n        try:\n            dt = datetime.strptime(t, fmt)\n            return (dt - EPOCH_DT).total_seconds()\n        except ValueError:\n            pass\n    return None", "idx": 960}
{"namespace": "fs.permissions.Permissions.parse", "completion": "        \"\"\"\n        This function parses permissions in Linux notation and returns an instance of the Permissions class with the parsed permissions.\n        Input-Output Arguments\n        :param cls: Class. The class object of the Permissions class.\n        :param ls: Text. The string containing the permissions in Linux notation.\n        :return: Permissions. An instance of the Permissions class with the parsed permissions.\n        \"\"\"\n        user = ls[0:3]\n        group = ls[3:6]\n        other = ls[6:9]\n        return cls(user=user, group=group, other=other)", "idx": 961}
{"namespace": "fs.permissions.Permissions.create", "completion": "        if init is None:\n            return Permissions()\n        elif isinstance(init, int):\n            return Permissions(mode=init)\n        elif isinstance(init, Iterable):\n            return Permissions(names=init)\n        else:\n            raise ValueError(\"Invalid initial value for creating Permissions object\")", "idx": 962}
{"namespace": "fs.info.Info.suffix", "completion": "        name = self.get(\"basic\", \"name\")\n        if name.startswith(\".\") and name.count(\".\") == 1:\n            return \"\"\n        return name[name.rfind(\".\"):]", "idx": 963}
{"namespace": "fs.info.Info.suffixes", "completion": "        name = self.get(\"basic\", \"name\")\n        if name.startswith(\".\") and name.count(\".\") == 1:\n            return []\n        else:\n            return name.split(\".\")[1:]", "idx": 964}
{"namespace": "fs.info.Info.stem", "completion": "        name = self.get(\"basic\", \"name\")\n        return name.split(\".\")[0]", "idx": 965}
{"namespace": "fs.info.Info.type", "completion": "        \"\"\"\n        This function returns the type of the resource stored in the Info instance. It requires the \"details\" namespace to be present in the Info instance. If the \"details\" namespace is not found, it raises a MissingInfoNamespace exception.\n        Input-Output Arguments\n        :param self: Info. An instance of the Info class.\n        :return: ResourceType. The type of the resource stored in the Info instance.\n        \"\"\"\n        self._require_namespace(\"details\")\n        return cast(ResourceType, self.get(\"details\", \"type\"))", "idx": 966}
{"namespace": "fs.info.Info.created", "completion": "        \"\"\"This function returns the creation time of a resource. It checks if the \"details\" namespace is present in the Info instance and raises an exception if it is not. It then retrieves the creation time from the \"details\" namespace and returns it.\n        Input-Output Arguments\n        :param self: Info. An instance of the Info class.\n        :return: Optional[datetime]. The creation time of the resource, or None if it is not available.\n        \"\"\"\n        self._require_namespace(\"details\")\n        _time = self._make_datetime(self.get(\"details\", \"created\"))\n        return _time", "idx": 967}
{"namespace": "pyinfra.connectors.mech.MechInventoryConnector.make_names_data", "completion": "        mech_config = get_mech_config(limit)\n        names_data = []\n\n        for line in mech_config:\n            if line.startswith(\"Host \"):\n                host_name = line.split()[1]\n                host_data = {\n                    \"ssh_hostname\": \"\",\n                    \"ssh_port\": \"\",\n                    \"ssh_user\": \"\",\n                    \"ssh_key\": \"\",\n                }\n\n                for config_key, data_key in (\n                    (\"HostName\", \"ssh_hostname\"),\n                    (\"Port\", \"ssh_port\"),\n                    (\"User\", \"ssh_user\"),\n                    (\"IdentityFile\", \"ssh_key\"),\n                ):\n                    if config_key in line:\n                        host_data[data_key] = line.split()[line.split().index(config_key) + 1]\n\n                names_data.append({host_name: host_data})\n\n        return names_data", "idx": 968}
{"namespace": "pyinfra.connectors.ansible.AnsibleInventoryConnector.make_names_data", "completion": "        if not inventory_filename:\n            raise InventoryError(\"No Ansible inventory filename provided!\")\n\n        if not path.isfile(inventory_filename):\n            raise InventoryError(f\"Could not find Ansible inventory file: {inventory_filename}\")\n\n        return parse_inventory(inventory_filename)", "idx": 969}
{"namespace": "pyinfra.operations.files.rsync", "completion": "\n    yield RsyncCommand(src, dest, flags=flags)", "idx": 970}
{"namespace": "pyinfra.operations.files.get", "completion": "    # Add deploy directory?\n    if add_deploy_dir and state.cwd:\n        src = os.path.join(state.cwd, src)\n\n    # Ensure the destination directory exists\n    if create_local_dir:\n        os.makedirs(os.path.dirname(dest), exist_ok=True)\n\n    # Do we download the file? Force by default\n    download = force\n\n    # Doesn't exist, lets download it\n    if not os.path.exists(dest):\n        download = True\n\n    # Destination file exists & force: check if the local file matches the remote file\n    elif force:\n        remote_file_sha1 = host.get_fact(Sha1File, path=src)\n        local_file_sha1 = get_file_sha1(dest)\n\n        if remote_file_sha1 != local_file_sha1:\n            download = True\n\n    # If we download, perform the download\n    if download:\n        with open(dest, 'wb') as f:\n            with host.get_file(src) as remote_file:\n                f.write(remote_file.read())", "idx": 971}
{"namespace": "pyinfra.operations.files.put", "completion": "\n    if not hasattr(src, \"read\") and state.cwd and add_deploy_dir:\n        src = os.path.join(state.cwd, src)\n\n    # Ensure the source file exists\n    if not assume_exists and not os.path.exists(src):\n        raise IOError(\"No such file: {0}\".format(src))\n\n    # Ensure the destination directory exists\n    if create_remote_dir:\n        yield from directory(\n            path=os.path.dirname(dest),\n            user=user,\n            group=group,\n            mode=dir_mode or get_path_permissions_mode(src),\n        )\n\n    # If force is True, always upload the file\n    if force:\n        yield FileUploadCommand(src, dest)\n\n    # If force is False, check if the remote copy matches\n    else:\n        remote_file = host.get_fact(File, path=dest)\n\n        # No remote file, so assume it doesn't match\n        if not remote_file:\n            yield FileUploadCommand(src, dest)\n\n        # Remote file exists - check if it matches the local file\n        else:\n            local_sum = get_file_sha1(src)\n            remote_sum = host.get_fact(Sha1File, path=dest)\n\n            # Check if the local and remote file hashes match\n            if local_sum != remote_sum:\n                yield FileUploadCommand(src, dest)", "idx": 972}
{"namespace": "pyinfra.operations.files.file", "completion": "\n    path = _validate_path(path)\n\n    info = host.get_fact(File, path=path)\n\n    if info is False:  # not a file\n        yield from _raise_or_remove_invalid_path(\n            \"file\",\n            path,\n            force,\n            force_backup,\n            force_backup_dir,\n        )\n        info = None\n\n    if not present:\n        if info:\n            yield StringCommand(\"rm\", \"-f\", QuoteString(path))\n        else:\n            host.noop(\"file {0} does not exist\")\n        return\n\n    if info is None:  # create\n        if create_remote_dir:\n            yield from _create_remote_dir(state, host, path, user, group)\n\n        if touch:\n            yield StringCommand(\"touch\", QuoteString(path))\n        else:\n            yield StringCommand(\"touch\", QuoteString(path))\n\n        if user or group:\n            yield file_utils.chown(path, user, group)\n\n        if mode:\n            yield file_utils.chmod(path, mode)\n\n    else:  # update\n        changed = False\n\n        if mode and info[\"mode\"] != mode:\n            yield file_utils.chmod(path, mode)\n            changed = True\n\n        if (user and info[\"user\"] != user) or (group and info[\"group\"] != group):\n            yield file_utils.chown(path, user, group)\n            changed = True\n\n        if not changed:\n            host.noop(\"file {0} already exists\".format(path))", "idx": 973}
{"namespace": "pyinfra.operations.python.call", "completion": "\n    yield FunctionCommand(function, args, kwargs)", "idx": 974}
{"namespace": "pyinfra.api.operation.add_op", "completion": "\n    for host in state.inventory:\n        op_func = operation(op_func)\n        op_func(state, host, *args, **kwargs)", "idx": 975}
{"namespace": "pyinfra.api.facts.get_facts", "completion": "\n    facts = {}\n\n    # Iterate over the active hosts in the state's inventory\n    for host in state.inventory:\n        # Spawn a greenlet for each host to retrieve the facts\n        greenlet = gevent.spawn(get_fact, state, host, *args, **kwargs)\n        facts[host] = greenlet.get()\n\n    return facts", "idx": 976}
{"namespace": "pyinfra.api.operations.run_ops", "completion": "\n    if serial:\n        _run_serial_ops(state)\n    elif no_wait:\n        _run_no_wait_ops(state)\n    else:\n        _run_ops(state)", "idx": 977}
{"namespace": "pyinfra.api.connect.connect_all", "completion": "\n    jobs = [gevent.spawn(host.connect) for host in state.inventory]\n\n    gevent.joinall(jobs)\n\n    for job, host in zip(jobs, state.inventory):\n        if job.successful():\n            state.activated_hosts.append(host)\n        else:\n            print(f\"Failed to connect to {host.name}\")", "idx": 978}
{"namespace": "pyinfra.api.arguments.pop_global_arguments", "completion": "    if keys_to_check is None:\n        keys_to_check = all_argument_meta.keys()\n\n    popped_arguments = {}\n    found_keys = []\n\n    for key in keys_to_check:\n        if key in kwargs:\n            popped_arguments[key] = kwargs.pop(key)\n            found_keys.append(key)\n\n    return popped_arguments, found_keys", "idx": 979}
{"namespace": "pyinfra_cli.commands.get_func_and_args", "completion": "    operation_function = try_import_module_attribute(commands[0], prefix=\"pyinfra.operations\")\n    operation_args = commands[1:]\n    return operation_function, operation_args", "idx": 980}
{"namespace": "viztracer.tracer._VizTracer.start", "completion": "        self.enable = True\n        self.parsed = False\n        if self.log_print:\n            self.overload_print()\n        if self.include_files and self.exclude_files:\n            raise ValueError(\"Both include_files and exclude_files cannot be specified at the same time\")\n        self.config()\n        self._tracer.start()", "idx": 981}
{"namespace": "viztracer.tracer._VizTracer.stop", "completion": "        if self.enable:\n            self.enable = False\n            if self.log_print:\n                self.restore_print()\n            self._tracer.stop()", "idx": 982}
{"namespace": "viztracer.report_builder.ReportBuilder.save", "completion": "        if isinstance(output_file, str):\n            file_extension = os.path.splitext(output_file)[1]\n            if file_extension == \".html\":\n                with open(output_file, \"w\", encoding=\"utf-8\") as f:\n                    self.generate_report(f, \"html\", file_info)\n                    self.final_messages.append((\"view_command\", {\"output_file\": os.path.abspath(output_file)}))\n            elif file_extension == \".json\":\n                with open(output_file, \"w\", encoding=\"utf-8\") as f:\n                    self.generate_report(f, \"json\", file_info)\n                    self.final_messages.append((\"view_command\", {\"output_file\": os.path.abspath(output_file)}))\n            elif file_extension == \".gz\":\n                with gzip.open(output_file, \"wt\", encoding=\"utf-8\") as f:\n                    self.generate_report(f, \"json\", file_info)\n                    self.final_messages.append((\"view_command\", {\"output_file\": os.path.abspath(output_file)}))\n            else:\n                raise ValueError(\"Unsupported file format\")\n        else:\n            self.generate_report(output_file, \"json\", file_info)\n            self.final_messages.append((\"view_command\", {\"output_file\": os.path.abspath(output_file.name)})", "idx": 983}
{"namespace": "viztracer.code_monkey.AstTransformer.get_assign_targets_with_attr", "completion": "        if isinstance(node, ast.Attribute):\n            return [node]\n        elif isinstance(node, (ast.Name, ast.Subscript, ast.Starred)):\n            return []\n        elif isinstance(node, (ast.Tuple, ast.List)):\n            return reduce(lambda a, b: a + b, [self.get_assign_targets_with_attr(elt) for elt in node.elts])\n        color_print(\"WARNING\", \"Unexpected node type {} for ast.Assign. \\\n            Please report to the author github.com/gaogaotiantian/viztracer\".format(type(node)))\n        return []", "idx": 984}
{"namespace": "viztracer.code_monkey.SourceProcessor.process", "completion": "        if isinstance(source, bytes):\n            source = source.decode(\"utf-8\")\n        if not isinstance(source, str):\n            return source\n\n        new_lines = []\n        for line in source.split(\"\\n\"):\n            matched = False\n            for pattern, transform_func in self.re_patterns:\n                match = pattern.match(line)\n                if match:\n                    new_line = transform_func(self, match)\n                    new_lines.append(new_line)\n                    matched = True\n                    break\n            if not matched:\n                new_lines.append(line)\n\n        return \"\\n\".join(new_lines)", "idx": 985}
{"namespace": "wal_e.log_help.WalELogger.fmt_logline", "completion": "        log_items = [\n            f'MSG: {msg}',\n            f'DETAIL: {detail}' if detail else '',\n            f'HINT: {hint}' if hint else '',\n            f'STRUCTURED: {structured}' if structured else ''\n        ]\n        return '\\n'.join(log_items)", "idx": 986}
{"namespace": "wal_e.blobstore.file.calling_format.Bucket.delete_keys", "completion": "        for key in keys:\n            file_path = os.path.join(\"/\", key.strip(\"/\"))\n            if os.path.exists(file_path):\n                os.remove(file_path)\n                dir_path = os.path.dirname(file_path)\n                remove_empty_dirs(dir_path)", "idx": 987}
{"namespace": "wal_e.worker.upload_pool.TarUploadPool.put", "completion": "        if self.closed:\n            raise ValueError('put() on closed TarUploadPool')\n\n        if self.member_burden + len(tpart) > self.max_members:\n            raise ValueError('too much work outstanding')\n\n        if self.concurrency_burden >= self.max_concurrency:\n            raise ValueError('too much work outstanding')\n\n        self._start(tpart)", "idx": 988}
{"namespace": "wal_e.worker.pg.wal_transfer.WalSegment.from_ready_archive_status", "completion": "        archive_status_dir = os.path.join(xlog_dir, 'archive_status')\n        for file in os.listdir(archive_status_dir):\n            if file.endswith('.ready'):\n                seg_path = os.path.join(archive_status_dir, file[:-6])\n                yield WalSegment(seg_path)", "idx": 989}
{"namespace": "wal_e.worker.pg.wal_transfer.WalTransferGroup.join", "completion": "        for g in list(self.greenlets):\n            g.join(timeout=30)\n            if g.ready():\n                if g.exception:\n                    raise g.exception\n                self.greenlets.remove(g)\n        self.closed = True", "idx": 990}
{"namespace": "wal_e.worker.pg.wal_transfer.WalTransferGroup.start", "completion": "        g = gevent.spawn(self.transferer, segment)\n        self.greenlets.add(g)\n        g.link(self._complete_execution)", "idx": 991}
{"namespace": "mrjob.py2.to_unicode", "completion": "    if isinstance(s, bytes):\n        try:\n            return s.decode('utf-8')\n        except UnicodeDecodeError:\n            return s.decode('latin-1')\n    else:\n        return s", "idx": 992}
{"namespace": "mrjob.job.MRJob.steps", "completion": "        steps_dict = {}\n\n        if hasattr(self, 'mapper'):\n            steps_dict['mapper'] = self.mapper\n        if hasattr(self, 'reducer'):\n            steps_dict['reducer'] = self.reducer\n        if hasattr(self, 'combiner'):\n            steps_dict['combiner'] = self.combiner\n        if hasattr(self, 'mapper_init'):\n            steps_dict['mapper_init'] = self.mapper_init\n        if hasattr(self, 'mapper_final'):\n            steps_dict['mapper_final'] = self.mapper_final\n        if hasattr(self, 'reducer_init'):\n            steps_dict['reducer_init'] = self.reducer_init\n        if hasattr(self, 'reducer_final'):\n            steps_dict['reducer_final'] = self.reducer_final\n        if hasattr(self, 'combiner_init'):\n            steps_dict['combiner_init'] = self.combiner_init\n        if hasattr(self, 'combiner_final'):\n            steps_dict['combiner_final'] = self.combiner_final\n        if hasattr(self, 'mapper_cmd'):\n            steps_dict['mapper_cmd'] = self.mapper_cmd\n        if hasattr(self, 'mapper_pre_filter'):\n            steps_dict['mapper_pre_filter'] = self.mapper_pre_filter\n        if hasattr(self, 'mapper_raw'):\n            steps_dict['mapper_raw'] = self.mapper_raw\n        if hasattr(self, 'reducer_cmd'):\n            steps_dict['reducer_cmd'] = self.reducer_cmd\n        if hasattr(self, 'reducer_pre_filter'):\n            steps_dict['reducer_pre_filter'] = self.reducer_pre_filter\n        if hasattr(self, 'combiner_cmd'):\n            steps_dict['combiner_cmd'] = self.combiner_cmd\n        if hasattr(self, 'combiner_pre_filter'):\n            steps_dict['combiner_pre_filter'] = self.combiner_pre_filter\n        if hasattr(self, 'spark'):\n            steps_dict['spark'] = self.spark\n\n        steps = []\n\n        for step_type, method in steps_dict.items():\n            if step_type == 'spark':\n                steps.append(SparkStep(mapper=self.spark, input_path=self.options.args[0], output_path=self.options.args[1], step_num=0))\n            else:\n                kwargs = {}\n                if method == self.mapper_cmd or method == self.reducer_cmd or method == self.combiner_cmd:\n                    kwargs['type'] = 'command'\n                    kwargs['command'] = method()\n                else:\n                    kwargs['type'] = 'script'\n                    kwargs['mapper'] = method\n                steps.append(MRStep(step_type, **kwargs))\n\n        return steps", "idx": 993}
{"namespace": "mrjob.job.MRJob.increment_counter", "completion": "        group = group.replace(',', ';')\n        counter = counter.replace(',', ';')\n\n        line = 'reporter:counter:%s,%s,%d\\n' % (group, counter, amount)\n\n        if not isinstance(line, bytes):\n            line = line.encode('utf_8')\n\n        self.stderr.write(line)\n        self.stderr.flush()", "idx": 994}
{"namespace": "mrjob.job.MRJob.set_status", "completion": "        line = 'reporter:status:%s\\n' % msg\n        if not isinstance(line, bytes):\n            line = line.encode('utf_8')\n        self.stderr.write(line)\n        self.stderr.flush()", "idx": 995}
{"namespace": "mrjob.job.MRJob.run_job", "completion": "        self.set_up_logging(quiet=self.options.quiet, verbose=self.options.verbose)\n\n        try:\n            with self.make_runner() as runner:\n                runner.run()\n                output_needs_to_be_concatenated = self._should_cat_output()\n                if output_needs_to_be_concatenated:\n                    for line in runner.cat_output():\n                        self.stdout.write(line)\n        except Exception as e:\n            log.exception(e)\n            sys.exit(1)", "idx": 996}
{"namespace": "mrjob.job.MRJob.set_up_logging", "completion": "        # set up logging for the mrjob logger\n        mrjob_log = logging.getLogger('mrjob')\n        mrjob_log.setLevel(logging.DEBUG if verbose else logging.INFO)\n        mrjob_log.addHandler(logging.StreamHandler(stream or sys.stderr))\n\n        # set up logging for the __main__ logger\n        main_log = logging.getLogger('__main__')\n        main_log.setLevel(logging.DEBUG if verbose else logging.INFO)\n        main_log.addHandler(logging.StreamHandler(stream or sys.stderr))\n\n        # disable logging if quiet is set\n        if quiet:\n            mrjob_log.disabled = True\n            main_log.disabled = True", "idx": 997}
{"namespace": "mrjob.job.MRJob.execute", "completion": "\n        if self.options.run_mapper:\n            self.run_mapper()\n        elif self.options.run_combiner:\n            self.run_combiner()\n        elif self.options.run_reducer:\n            self.run_reducer()\n        elif self.options.run_spark:\n            self.run_spark()\n        else:\n            self.run_job()", "idx": 998}
{"namespace": "mrjob.job.MRJob._runner_kwargs", "completion": "    non_option_kwargs = self._non_option_kwargs()\n\n    # keyword arguments from switches\n    switch_kwargs = self._kwargs_from_switches([\n        'archive', 'bootstrap_mrjob', 'bootstrap_python', 'cleanup',\n        'cleanup_on_failure', 'cmdenv', 'file', 'hadoop_extra_args',\n        'hadoop_input_format', 'hadoop_output_format', 'hadoop_streaming_jar',\n        'hadoop_streaming_java', 'hadoop_streaming_mapper',\n        'hadoop_streaming_output_protocol', 'hadoop_streaming_reducer',\n        'hadoop_streaming_submit_cmd', 'hadoop_streaming_jobconf',\n        'hadoop_streaming_partitioner', 'hadoop_streaming_files',\n        'hadoop_streaming_libjars', 'hadoop_streaming_archives',\n        'hadoop_streaming_cache_files', 'hadoop_streaming_cache_archives',\n        'hadoop_streaming_file', 'hadoop_streaming_mapper_cmd',\n        'hadoop_streaming_reducer_cmd', 'hadoop_streaming_combiner_cmd',\n        'hadoop_streaming_input_format', 'hadoop_streaming_output_format',\n        'hadoop_streaming_map_debug_script', 'hadoop_streaming_reduce_debug_script',\n        'hadoop_streaming_map_debug_cmd', 'hadoop_streaming_reduce_debug_cmd',\n        'hadoop_streaming_map_debug', 'hadoop_streaming_reduce_debug',\n        'hadoop_streaming_map_debug_args', 'hadoop_streaming_reduce_debug_args',\n        'hadoop_streaming_map_debug_input', 'hadoop_streaming_reduce_debug_input',\n        'hadoop_streaming_map_debug_output', 'hadoop_streaming_reduce_debug_output',\n        'hadoop_streaming_map_debug_input_args', 'hadoop_streaming_reduce_debug_input_args',\n        'hadoop_streaming_map_debug_output_args', 'hadoop_streaming_reduce_debug_output_args',\n        'hadoop_streaming_map_debug_input_file', 'hadoop_streaming_reduce_debug_input_file',\n        'hadoop_streaming_map_debug_output_file', 'hadoop_streaming_reduce_debug_output_file',\n        'hadoop_streaming_map_debug_input_dir', 'hadoop_streaming_reduce_debug_input_dir',\n        'hadoop_streaming_map_debug_output_dir', 'hadoop_streaming_reduce_debug_output_dir',\n        'hadoop_streaming_map_debug_input_protocol', 'hadoop_streaming_reduce_debug_input_protocol',\n        'hadoop_streaming_map_debug_output_protocol', 'hadoop_streaming_reduce_debug_output_protocol',\n        'hadoop_streaming_map_debug_input_file_args', 'hadoop_streaming_reduce_debug_input_file_args',\n        'hadoop_streaming_map_debug_output_file_args', 'hadoop_streaming_reduce_debug_output_file_args',\n        'hadoop_streaming_map_debug_input_dir_args', 'hadoop_streaming_reduce_debug_input_dir_args',\n        'hadoop_streaming_map_debug_output_dir_args', 'hadoop_streaming_reduce_debug_output_dir_args',\n        'hadoop_streaming_map_debug_input_protocol_args', 'hadoop_streaming_reduce_debug_input_protocol_args',\n        'hadoop_streaming_map_debug_output_protocol_args', 'hadoop_streaming_reduce_debug_output_protocol_args',\n        'hadoop_streaming_map_debug_input_file_protocol', 'hadoop_streaming_reduce_debug_input_file_protocol',\n        'hadoop_streaming_map_debug_output_file_protocol', 'hadoop_streaming_reduce_debug_output_file_protocol',\n        'hadoop_streaming_map_debug_input_dir_protocol', 'hadoop_streaming_reduce_debug_input_dir_protocol',\n        'hadoop_streaming_map_debug_output_dir_protocol', 'hadoop_streaming_reduce_debug_output_dir_protocol',\n        'hadoop_streaming_map_debug_input_file_protocol_args', 'hadoop_streaming_reduce_debug_input_file_protocol_args',\n        'hadoop_streaming_map_debug_output_file_protocol_args', 'hadoop_streaming_reduce_debug_output_file_protocol_args',\n        'hadoop_streaming_map_debug_input_dir_protocol_args', 'hadoop_streaming_reduce_debug_input_dir_protocol_args',\n        'hadoop_streaming_map_debug_output_dir_protocol_args', 'hadoop_streaming_reduce_debug_output_dir_protocol_args',\n        'hadoop_streaming_map_debug_input_file_protocol_args', 'hadoop_streaming_reduce_debug_input_file_protocol_args',\n        'hadoop_streaming_map_debug_output_file_protocol_args', 'hadoop_streaming_reduce_debug_output_file_protocol_args',\n        'hadoop_streaming_map_debug_input_dir_protocol_args', 'hadoop_streaming_reduce_debug_input_dir_protocol_args',\n        'hadoop_streaming_map_debug_output_dir_protocol_args', 'hadoop_streaming_reduce_debug_output_dir_protocol_args',\n        'hadoop_streaming_map_debug_input_file_protocol_args', 'hadoop_streaming_reduce_debug_input_file_protocol_args',\n        'hadoop_streaming_map_debug_output_file_protocol_args', 'hadoop_streaming_reduce_debug_output_file_protocol_args',\n        'hadoop_streaming_map_debug_input_dir_protocol_args', 'hadoop_streaming_reduce_debug_input_dir_protocol_args',\n        'hadoop_streaming_map_debug_output_dir_protocol_args', 'hadoop_streaming_reduce_debug_output_dir_protocol_args',\n        'hadoop_streaming_map_debug_input_file_protocol_args', 'hadoop_streaming_reduce_debug_input_file_protocol_args',\n        'hadoop_streaming_map_debug_output_file_protocol_args', 'hadoop_streaming_reduce_debug_output_file_protocol_args',\n        'hadoop_streaming_map_debug_input_dir_protocol_args', 'hadoop_streaming_reduce_debug_input_dir_protocol_args',\n        'hadoop_streaming_map_debug_output_dir_protocol_args', 'hadoop_streaming_reduce_debug_output_dir_protocol_args',\n        'hadoop_streaming_map_debug_input_file_protocol_args', 'hadoop_streaming_reduce_debug_input_file_protocol_args',\n        'hadoop_streaming_map_debug_output_file_protocol_args', 'hadoop_streaming_reduce_debug_output_file_protocol_args',\n        'hadoop_streaming_map_debug_input_dir_protocol_args', 'hadoop_streaming_reduce_debug_input_dir_protocol_args',\n        'hadoop_streaming_map_debug_output_dir_protocol_args', 'hadoop_streaming_reduce_debug_output_dir_protocol_args',\n        'hadoop_streaming_map_debug_input_file_protocol_args', 'hadoop_streaming_reduce_debug_input_file_protocol_args',\n        'hadoop_streaming_map_debug_output_file_protocol_args', 'hadoop_streaming_reduce_debug_output_file_protocol_args',\n        'hadoop_streaming_map_debug_input_dir_protocol_args', 'hadoop_streaming_reduce_debug_input_dir_protocol_args',\n        'hadoop_streaming_map_debug_output_dir_protocol_args', 'hadoop_streaming_reduce_debug_output_dir_protocol_args',\n        'hadoop_streaming_map_debug_input_file_protocol_args', 'hadoop_streaming_reduce_debug_input_file_protocol_args',\n        'hadoop_streaming_map_debug_output_file_protocol_args', 'hadoop_streaming_reduce_debug_output_file_protocol_args',\n        'hadoop_streaming_map_debug_input_dir_protocol_args', 'hadoop_streaming_reduce_debug_input_dir_protocol_args',\n        'hadoop_streaming_map_debug_output_dir_protocol_args', 'hadoop_streaming_reduce_debug_output_dir_protocol_args',\n        'hadoop_streaming_map_debug_input_file_protocol_args', 'hadoop_streaming_reduce_debug_input_file_protocol_args',\n        'hadoop_streaming_map_debug_output_file_protocol_args', 'hadoop_streaming_reduce_debug_output_file_protocol_args',\n        'hadoop_streaming_map_debug_input_dir_protocol_args', 'hadoop_streaming_reduce_debug_input_dir_protocol_args',\n        'hadoop_streaming_map_debug_output_dir_protocol_args', 'hadoop_streaming_reduce_debug_output_dir_protocol_args',\n        'hadoop_streaming_map_debug_input_file_protocol_args', 'hadoop_streaming_reduce_debug_input_file_protocol_args',\n        'hadoop_streaming_map_debug_output_file_protocol_args', 'hadoop_streaming_reduce_debug_output_file_protocol_args',\n        'hadoop_streaming_map_debug_input_dir_protocol_args', 'hadoop_streaming_reduce_debug_input_dir_protocol_args',\n        'hadoop_streaming_map_debug_output_dir_protocol_args', 'hadoop_streaming_reduce_debug_output_dir_protocol_args',\n        'hadoop_streaming_map_debug_input_file_protocol_args', 'hadoop_streaming_reduce_debug_input_file_protocol_args',\n        'hadoop_streaming_map_debug_output_file_protocol_args', 'hadoop_streaming_reduce_debug_output_file_protocol_args',\n        'hadoop_streaming_map_debug_input_dir_protocol_args', 'hadoop_streaming_reduce_debug_input_dir_protocol_args',\n        'hadoop_streaming_map_debug_output_dir_protocol_args', 'hadoop_streaming_reduce_debug_output_dir_protocol_args',\n        'hadoop_streaming_map_debug_input_file_protocol_args', 'hadoop_streaming_reduce_debug_input_file_protocol_args',\n        'hadoop_streaming_map_debug_output_file_protocol_args', 'hadoop_streaming_reduce_debug_output_file_protocol_args',\n        'hadoop_streaming_map_debug_input_dir_protocol_args', 'hadoop_streaming_reduce_debug_input_dir_protocol_args',\n        'hadoop_streaming_map_debug_output_dir_protocol_args', 'hadoop_streaming_reduce_debug_output_dir_protocol_args',\n        'hadoop_streaming_map_debug_input_file_protocol_args', 'hadoop_streaming_reduce_debug_input_file_protocol_args',\n        'hadoop_streaming_map_debug_output_file_protocol_args', 'hadoop_streaming_reduce_debug_output_file_protocol_args',\n        'hadoop_streaming_map_debug_input_dir_protocol_args', 'hadoop_streaming_reduce_debug_input_dir_protocol_args',\n        'hadoop_streaming_map_debug_output_dir_protocol_args', 'hadoop_streaming_reduce_debug_output_dir_protocol_args',\n        'hadoop_streaming_map_debug_input_file_protocol_args', 'hadoop_streaming_reduce_debug_input_file_protocol_args',\n        'hadoop_streaming_map_debug_output_file_protocol_args', 'hadoop_streaming_reduce_debug_output_file_protocol_args',\n        'hadoop_streaming_map_debug_input_dir_protocol_args', 'hadoop_streaming_reduce_debug_input_dir_protocol_args',\n        'hadoop_streaming_map_debug_output_dir_protocol_args', 'hadoop_streaming_reduce_debug_output_dir_protocol_args',\n        'hadoop_streaming_map_debug_input_file_protocol_args', 'hadoop_streaming_reduce_debug_input_file_protocol_args',\n        'hadoop_streaming_map_debug_output_file_protocol_args', 'hadoop_streaming_reduce_debug_output_file_protocol_args',\n        'hadoop_streaming_map_debug_input_dir_protocol_args', 'hadoop_streaming_reduce_debug_input_dir_protocol_args',\n        'hadoop_streaming_map_debug_output_dir_protocol_args', 'hadoop_streaming_reduce_debug_output_dir_protocol_args',\n        'hadoop_streaming_map_debug_input_file_protocol_args', 'hadoop_streaming_reduce_debug_input_file_protocol_args',\n        'hadoop_streaming_map_debug_output_file_protocol_args', 'hadoop_streaming_reduce_debug_output_file_protocol_args',\n        'hadoop_streaming_map_debug_input_dir_protocol_args', 'hadoop_streaming_reduce_debug_input_dir_protocol_args',\n        'hadoop_streaming_map_debug_output_dir_protocol_args', 'hadoop_streaming_reduce_debug_output_dir_protocol_args',\n        'hadoop_streaming_map_debug_input_file_protocol_args', 'hadoop_streaming_reduce_debug_input_file_protocol_args',\n        'hadoop_streaming_map_debug_output_file_protocol_args', 'hadoop_streaming_reduce_debug_output_file_protocol_args',\n        'hadoop_streaming_map_debug_input_dir_protocol_args', 'hadoop_streaming_reduce_debug_input_dir_protocol_args',\n        'hadoop_streaming_map_debug_output_dir_protocol_args', 'hadoop_streaming_reduce_debug_output_dir_protocol_args',\n        'hadoop_streaming_map_debug_input_file_protocol_args', 'hadoop_streaming_reduce_debug_input_file_protocol_args',\n        'hadoop_streaming_map_debug_output_file_protocol_args', 'hadoop_streaming_reduce_debug_output_file_protocol_args',\n        'hadoop_streaming_map_debug_input_dir_protocol_args', 'hadoop_streaming_reduce_debug_input_dir_protocol_args',\n        'hadoop_streaming_map_debug_output_dir_protocol_args', 'hadoop_streaming_reduce_debug_output_dir_protocol_args',\n        'hadoop_streaming_map_debug_input_file_protocol_args', 'hadoop_streaming_reduce_debug_input_file_protocol_args',\n        'hadoop_streaming_map_debug_output_file_protocol_args', 'hadoop_streaming_reduce_debug_output_file_protocol_args',\n        'hadoop_streaming_map_debug_input_dir_protocol_args', 'hadoop_streaming_reduce_debug_input_dir_protocol_args',\n        'hadoop_streaming_map_debug_output_dir_protocol_args', 'hadoop_streaming_reduce_debug_output_dir_protocol_args',\n        'hadoop_streaming_map_debug_input_file_protocol_args', 'hadoop_stream", "idx": 999}
{"namespace": "mrjob.job.MRJob.run_mapper", "completion": "\n        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'mapper')\n\n        for k, v in self.map_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)", "idx": 1000}
{"namespace": "mrjob.job.MRJob.run_combiner", "completion": "        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'combiner')\n\n        for k, v in self.combine_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)", "idx": 1001}
{"namespace": "mrjob.job.MRJob.add_passthru_arg", "completion": "        pass_opt = self.arg_parser.add_argument(*args, **kwargs)\n        self._passthru_arg_dests.add(pass_opt.dest)", "idx": 1002}
{"namespace": "mrjob.job.MRJob.is_task", "completion": "        return hasattr(self, 'mapper') or hasattr(self, 'combiner') or hasattr(self, 'reducer') or hasattr(self, 'spark')", "idx": 1003}
{"namespace": "mrjob.job.MRJob.parse_output", "completion": "\n        output_protocol = self.output_protocol()\n\n        for chunk in chunks:\n            for line in to_lines(chunk):\n                key, value = output_protocol.read(line.rstrip(b'\\r\\n'))\n                yield key, value", "idx": 1004}
{"namespace": "mrjob.job.MRJob.sandbox", "completion": "        if stdin:\n            self._stdin = stdin\n        else:\n            self._stdin = BytesIO()\n\n        if stdout:\n            self._stdout = stdout\n        else:\n            self._stdout = BytesIO()\n\n        if stderr:\n            self._stderr = stderr\n        else:\n            self._stderr = BytesIO()\n\n        return self", "idx": 1005}
{"namespace": "mrjob.hadoop.fully_qualify_hdfs_path", "completion": "    if path.startswith('hdfs://'):\n        return path\n    elif path.startswith('/'):\n        return 'hdfs://' + path\n    else:\n        username = getpass.getuser()\n        return 'hdfs:///user/{}/{}'.format(username, path)", "idx": 1006}
{"namespace": "mrjob.hadoop.HadoopJobRunner.fs", "completion": "        if not hasattr(self, '_fs'):\n            self._fs = CompositeFilesystem()\n            self._fs.add_fs('file', LocalFilesystem())\n            self._fs.add_fs('hdfs', HadoopFilesystem())\n        return self._fs", "idx": 1007}
{"namespace": "mrjob.hadoop.HadoopJobRunner._find_hadoop_streaming_jar", "completion": "        for dir in self._hadoop_streaming_jar_dirs():\n            log.info('Looking for Hadoop streaming jar in %s...' % dir)\n            for file in self.fs.ls(dir):\n                if _HADOOP_STREAMING_JAR_RE.match(file):\n                    return file\n        return None", "idx": 1008}
{"namespace": "mrjob.hadoop.HadoopJobRunner._find_binaries_and_jars", "completion": "\n        # Load the Hadoop binary\n        self.get_hadoop_bin()\n\n        # Check if there are Hadoop streaming steps or Spark steps in the job\n        for step_num, step in enumerate(self._get_steps()):\n            step_type = step['type']\n            if step_type in ('streaming', 'spark', 'spark_jar', 'spark_script'):\n                # Load the Hadoop streaming jar\n                if step_type == 'streaming':\n                    self.get_hadoop_streaming_jar()\n                # Load the Spark submit binary\n                elif _is_spark_step_type(step_type):\n                    self._spark_master()", "idx": 1009}
{"namespace": "mrjob.hadoop.HadoopJobRunner._args_for_streaming_step", "completion": "        step = self._get_step(step_num)\n\n        args = []\n\n        hadoop_bin = self.get_hadoop_bin()\n        if not hadoop_bin:\n            raise Exception('no Hadoop binary')\n\n        args.extend(hadoop_bin)\n\n        streaming_jar = self.get_hadoop_streaming_jar()\n        if not streaming_jar:\n            raise Exception('no Hadoop streaming jar')\n\n        args.extend(['jar', streaming_jar])\n\n        if step.get('mapper'):\n            args.extend(['-mapper', step['mapper']])\n\n        if step.get('combiner'):\n            args.extend(['-combiner', step['combiner']])\n\n        if step.get('reducer'):\n            args.extend(['-reducer', step['reducer']])\n\n        if step.get('input'):\n            args.extend(['-input', step['input']])\n\n        if step.get('output'):\n            args.extend(['-output', step['output']])\n\n        if step.get('cmdenv'):\n            for key, value in step['cmdenv'].items():\n                args.extend(['-cmdenv', '%s=%s' % (key, value)])\n\n        if step.get('file'):\n            for file in step['file']:\n                args.extend(['-file', file])\n\n        if step.get('cache_file'):\n            for file in step['cache_file']:\n                args.extend(['-cacheFile', file])\n\n        if step.get('cache_archive'):\n            for archive in step['cache_archive']:\n                args.extend(['-cacheArchive', archive])\n\n        if step.get('jobconf'):\n            for key, value in step['jobconf'].items():\n                args.extend(['-D', '%s=%s' % (key, value)])\n\n        if step.get('inputformat'):\n            args.extend(['-inputformat', step['inputformat']])\n\n        if step.get('outputformat'):\n            args.extend(['-outputformat', step['outputformat']])\n\n        if step.get('partitioner'):\n            args.extend(['-partitioner', step['partitioner']])\n\n        if step.get('cmdenv'):\n            for key, value in step['cmdenv'].items():\n                args.extend(['-cmdenv', '%s=%s' % (key, value)])\n\n        if step.get('jobconf'):\n            for key, value in step['jobconf'].items():\n                args.extend(['-D', '%s=%s' % (key, value)])\n\n        return args", "idx": 1010}
{"namespace": "mrjob.hadoop.HadoopJobRunner._stream_history_log_dirs", "completion": "        if not self._read_logs():\n            return\n\n        for log_dir in unique(self._hadoop_log_dirs(output_dir=output_dir)):\n            if _logs_exist(self.fs, log_dir):\n                log.info('Looking for history log in %s...' % log_dir)\n                yield [log_dir]", "idx": 1011}
{"namespace": "mrjob.hadoop.HadoopJobRunner._stream_task_log_dirs", "completion": "        if not self._read_logs():\n            return\n\n        for log_dir in unique(self._hadoop_log_dirs(output_dir=output_dir)):\n            if application_id:\n                userlogs_dir = posixpath.join(log_dir, 'userlogs', application_id)\n            else:\n                userlogs_dir = posixpath.join(log_dir, 'userlogs')\n\n            log.info('Looking for task logs in %s...' % userlogs_dir)\n            yield [userlogs_dir]", "idx": 1012}
{"namespace": "mrjob.setup.UploadDirManager.add", "completion": "        if is_uri(path):\n            return path\n\n        if path not in self._path_to_name:\n            name = name_uniquely(path, names_taken=self._names_taken, unhide=True)\n            self._path_to_name[path] = name\n            self._names_taken.add(name)\n\n        return posixpath.join(self.prefix, self._path_to_name[path])", "idx": 1013}
{"namespace": "mrjob.setup.UploadDirManager.uri", "completion": "        if is_uri(path):\n            return path\n        elif path in self._path_to_name:\n            name = self._path_to_name[path]\n            return posixpath.join(self.prefix, name)\n        else:\n            raise ValueError('%r is not a URI or a known local file' % path)", "idx": 1014}
{"namespace": "mrjob.setup.UploadDirManager.path_to_uri", "completion": "        uri_mapping = {}\n        for path in self._path_to_name:\n            uri_mapping[path] = self.uri(path)\n        return uri_mapping", "idx": 1015}
{"namespace": "mrjob.setup.WorkingDirManager.name_to_path", "completion": "        paths_map = {}\n\n        for name, (path_type, path) in self._name_to_typed_path.items():\n            if type is None or path_type == type:\n                paths_map[name] = path\n\n        for (path_type, path), name in self._typed_path_to_auto_name.items():\n            if type is None or path_type == type:\n                paths_map[name] = path\n\n        return paths_map", "idx": 1016}
{"namespace": "mrjob.setup.WorkingDirManager.paths", "completion": "        self._check_type(type)\n\n        paths_set = set()\n        for path_type, path in self._typed_path_to_auto_name:\n            if type is None or path_type == type:\n                paths_set.add(path)\n\n        return paths_set", "idx": 1017}
{"namespace": "mrjob.compat.jobconf_from_env", "completion": "    if variable in os.environ:\n        return os.environ[variable]\n\n    # try alternatives (arbitrary order)\n    for alternative in _JOBCONF_MAP.get(variable, {}).values():\n        if alternative in os.environ:\n            return os.environ[alternative]\n\n    return default", "idx": 1018}
{"namespace": "mrjob.compat.jobconf_from_dict", "completion": "    if name in jobconf:\n        return jobconf[name]\n    else:\n        # try alternatives (arbitrary order)\n        for var in _JOBCONF_MAP.get(name, {}).values():\n            if var in jobconf:\n                return jobconf[var]\n        return default", "idx": 1019}
{"namespace": "mrjob.compat.translate_jobconf", "completion": "    return map_version(version, _JOBCONF_MAP.get(variable, {}))", "idx": 1020}
{"namespace": "mrjob.compat.translate_jobconf_for_all_versions", "completion": "    variants = _JOBCONF_MAP.get(variable, {}).values()\n    return sorted(variants)", "idx": 1021}
{"namespace": "mrjob.compat.translate_jobconf_dict", "completion": "\n    translated_jobconf = {}\n\n    if hadoop_version is not None:\n        translated_names = []\n        for key, value in jobconf.items():\n            translated_key = translate_jobconf(key, hadoop_version)\n            if translated_key != key:\n                translated_names.append(f\"{key}:{translated_key}\")\n            translated_jobconf[translated_key] = value\n\n        if translated_names:\n            translated_names = '\\n'.join(sorted(translated_names))\n            warning_msg = f\"Detected hadoop configuration property names that do not match version {hadoop_version}:\\nThe have been translated to the following names:\\n{translated_names}\"\n            print(warning_msg)\n\n    translated_jobconf.update(jobconf)\n    return translated_jobconf", "idx": 1022}
{"namespace": "mrjob.compat.uses_yarn", "completion": "    return version_gte(version, '2.0')", "idx": 1023}
{"namespace": "mrjob.local.LocalMRJobRunner._spark_master", "completion": "        num_executors = self._opts['num_executors'] or 2\n        cores_per_executor = self._opts['cores_per_executor'] or 1\n        executor_memory = self._opts['executor_memory'] or _DEFAULT_EXECUTOR_MEMORY\n\n        # Calculate executor memory in MB (rounded up)\n        executor_memory_mb = int(math.ceil(_to_num_bytes(executor_memory) / (1024 * 1024)))\n\n        return 'local-cluster[%d,%d,%d]' % (num_executors, cores_per_executor, executor_memory_mb)", "idx": 1024}
{"namespace": "mrjob.runner.MRJobRunner._bootstrap_mrjob", "completion": "        return self._opts.get('bootstrap_mrjob', True)", "idx": 1025}
{"namespace": "mrjob.conf._fix_clear_tags", "completion": "    if isinstance(x, list):\n        return [_fix_clear_tags(item) for item in x]\n    elif isinstance(x, dict):\n        result = {}\n        for key, value in x.items():\n            if isinstance(value, ClearedValue):\n                result[key] = value\n            elif isinstance(value, dict):\n                result[key] = _fix_clear_tags(value)\n            else:\n                result[key] = value\n        return result\n    elif isinstance(x, ClearedValue):\n        return x.value\n    else:\n        return x", "idx": 1026}
{"namespace": "mrjob.conf.load_opts_from_mrjob_conf", "completion": "    if already_loaded is None:\n        already_loaded = []\n\n    conf_path = _expanded_mrjob_conf_path(conf_path)\n    return _load_opts_from_mrjob_conf(runner_alias, conf_path, already_loaded)", "idx": 1027}
{"namespace": "mrjob.conf.load_opts_from_mrjob_confs", "completion": "    if conf_paths is None:\n        conf_paths = [find_mrjob_conf()]\n\n    if not conf_paths:\n        log.warning('No config specified for %s runner' % runner_alias)\n\n    return sum((_load_opts_from_mrjob_conf(runner_alias, conf_path) for conf_path in conf_paths), [])", "idx": 1028}
{"namespace": "mrjob.conf.dump_mrjob_conf", "completion": "    if yaml:\n        _dump_yaml_with_clear_tags(conf, f, default_flow_style=False)\n    else:\n        json.dump(conf, f, indent=2)", "idx": 1029}
{"namespace": "mrjob.conf.combine_lists", "completion": "    result = []\n    for seq in seqs:\n        if seq is not None:\n            if isinstance(seq, (str, bytes)) or not hasattr(seq, '__iter__'):\n                result.append(seq)\n            else:\n                result.extend([item for item in seq if item is not None])\n    return result", "idx": 1030}
{"namespace": "mrjob.conf.combine_cmds", "completion": "    last_cmd = None\n    for cmd in cmds:\n        if cmd is not None:\n            if isinstance(cmd, list):\n                last_cmd = cmd\n            elif isinstance(cmd, str):\n                last_cmd = shlex_split(cmd)\n    return last_cmd", "idx": 1031}
{"namespace": "mrjob.conf.combine_dicts", "completion": "    result = {}\n    for d in dicts:\n        if d:\n            for key, value in d.items():\n                if isinstance(value, ClearedValue) and value.value is None:\n                    result.pop(key, None)\n                else:\n                    result[key] = value\n    return result", "idx": 1032}
{"namespace": "mrjob.conf.combine_jobconfs", "completion": "    combiners = {\n        'args': combine_lists,\n        'bootstrap': combine_path_lists,\n        'bootstrap_files': combine_path_lists,\n        'bootstrap_cmds': combine_cmds,\n        'env': combine_envs,\n        'local_env': combine_local_envs,\n        'python_archives': combine_path_lists,\n        'python_bin': combine_paths,\n        'setup_cmds': combine_cmds,\n        'steps': combine_dicts,\n    }\n\n    return combine_opts(combiners, *jobconfs)", "idx": 1033}
{"namespace": "mrjob.conf.combine_path_lists", "completion": "    combined_paths = []\n    for path_seq in path_seqs:\n        if isinstance(path_seq, str):\n            combined_paths.append(expand_path(path_seq))\n        elif isinstance(path_seq, list):\n            for path in path_seq:\n                combined_paths.append(expand_path(path))\n    return combined_paths", "idx": 1034}
{"namespace": "mrjob.conf.combine_opts", "completion": "    combined_opts = {}\n\n    for opts in opts_list:\n        for key, value in opts.items():\n            if not isinstance(value, ClearedValue):\n                combiner = combiners.get(key, combine_values)\n                combined_opts[key] = combiner(combined_opts.get(key), value)\n\n    return combined_opts", "idx": 1035}
{"namespace": "mrjob.bin.MRJobBinRunner._task_python_bin", "completion": "        return self._opts['task_python_bin'] or self._default_python_bin()", "idx": 1036}
{"namespace": "django.utils.ipv6.is_valid_ipv6_address", "completion": "    try:\n        ipaddress.IPv6Address(ip_str)\n        return True\n    except ValueError:\n        return False", "idx": 1037}
{"namespace": "django.utils.http.urlsafe_base64_decode", "completion": "    # Add back any trailing equal signs that might have been stripped\n    padding = len(s) % 4\n    if padding:\n        s += b\"=\" * (4 - padding)\n    return base64.urlsafe_b64decode(s)", "idx": 1038}
{"namespace": "django.utils.http.parse_etags", "completion": "    if etag_str.strip() == \"*\":\n        return [\"*\"]\n    else:\n        return [quote_etag(etag) for etag in etag_str.split(\",\")]", "idx": 1039}
{"namespace": "django.utils.http.is_same_domain", "completion": "    if host == pattern:\n        return True\n    pattern_parts = pattern.split('.')\n    host_parts = host.split('.')\n    if len(pattern_parts) != len(host_parts):\n        return False\n    for pattern_part, host_part in zip(pattern_parts, host_parts):\n        if pattern_part != '*' and pattern_part != host_part:\n            return False\n    return True", "idx": 1040}
{"namespace": "django.utils.http.content_disposition_header", "completion": "    if not filename:\n        return 'inline' if not as_attachment else 'attachment'\n    try:\n        filename.encode('ascii')\n        ascii_filename = filename\n    except UnicodeEncodeError:\n        ascii_filename = filename.encode('utf-8')\n        quoted_filename = quote(ascii_filename)\n        return 'attachment; filename=\"{}\"; filename*=UTF-8\\'\\'{}'.format(quoted_filename, quoted_filename.decode('ascii'))\n    if ' ' in ascii_filename:\n        return 'attachment; filename=\"{}\"'.format(ascii_filename)\n    return 'attachment; filename={}'.format(ascii_filename)", "idx": 1041}
{"namespace": "pysnooper.utils.truncate", "completion": "    if len(string) <= max_length:\n        return string\n    else:\n        return f\"{string[:max_length//2]}...{string[-max_length//2:]}\"", "idx": 1042}
{"namespace": "pysnooper.variables.needs_parentheses", "completion": "    try:\n        compile(source, '<variable>', 'eval')\n        return False\n    except SyntaxError:\n        return True", "idx": 1043}
{"namespace": "django.test.utils.extend_sys_path", "completion": "    class ExtendSysPathContextManager:\n        def __enter__(self):\n            self.original_sys_path = sys.path.copy()\n            sys.path.extend(paths)\n\n        def __exit__(self, exc_type, exc_value, traceback):\n            sys.path = self.original_sys_path\n\n    return ExtendSysPathContextManager()", "idx": 1044}
{"namespace": "albumentations.augmentations.functional.normalize_cv2", "completion": "    mean = np.array(mean, dtype=np.float32)\n    mean *= 255.0\n\n    denominator = np.array(denominator, dtype=np.float32)\n    denominator *= 255.0\n\n    img = img.astype(np.float32)\n    img -= mean\n    img *= denominator\n    return img", "idx": 1045}
{"namespace": "albumentations.augmentations.functional.normalize_numpy", "completion": "    img = np.ascontiguousarray(img.astype(\"float32\"))\n    img -= mean.astype(np.float64)\n    img *= denominator.astype(np.float64)\n    return img", "idx": 1046}
{"namespace": "albumentations.augmentations.functional.gamma_transform", "completion": "    if img.dtype == np.uint8:\n        lut = np.array([((i / 255.0) ** (1.0 / gamma)) * 255 for i in range(256)], dtype=np.uint8)\n        return cv2.LUT(img, lut)\n    else:\n        return np.power(img, 1.0 / gamma)", "idx": 1047}
{"namespace": "albumentations.augmentations.functional.swap_tiles_on_image", "completion": "    output_image = np.array(image, copy=True)\n    for tile in tiles:\n        current_x, current_y, old_x, old_y, height, width = tile\n        output_image[current_y:current_y + height, current_x:current_x + width], output_image[old_y:old_y + height, old_x:old_x + width] = output_image[old_y:old_y + height, old_x:old_x + width], output_image[current_y:current_y + height, current_x:current_x + width].copy()\n    return output_image", "idx": 1048}
{"namespace": "albumentations.augmentations.geometric.functional.keypoint_rotate", "completion": "    x, y, a, scale = keypoint[:4]\n\n    # Calculate the new position of the keypoint after rotation\n    x_new = x * np.cos(angle) - y * np.sin(angle)\n    y_new = x * np.sin(angle) + y * np.cos(angle)\n\n    # Update the angle\n    a += angle\n\n    return x_new, y_new, a, scale", "idx": 1049}
{"namespace": "albumentations.augmentations.geometric.functional.keypoint_shift_scale_rotate", "completion": "\n    x, y, a, s = keypoint[:4]\n\n    center = (cols - 1) * 0.5, (rows - 1) * 0.5\n    matrix = cv2.getRotationMatrix2D(center, angle, scale)\n    x, y = cv2.transform(np.array([[[x, y]]]), matrix).squeeze()\n    a += math.radians(angle)\n    s *= scale\n\n    return x, y, a, s", "idx": 1050}
{"namespace": "albumentations.core.keypoints_utils.angle_to_2pi_range", "completion": "    while angle < 0:\n        angle += 2 * math.pi\n    while angle >= 2 * math.pi:\n        angle -= 2 * math.pi\n    return angle", "idx": 1051}
{"namespace": "albumentations.augmentations.geometric.functional.rot90", "completion": "\n    return np.rot90(img, k=factor)", "idx": 1052}
{"namespace": "albumentations.core.keypoints_utils.convert_keypoints_to_albumentations", "completion": "    return [\n        convert_keypoint_to_albumentations(kp, source_format, rows, cols, check_validity, angle_in_degrees)\n        for kp in keypoints\n    ]", "idx": 1053}
{"namespace": "albumentations.core.keypoints_utils.convert_keypoints_from_albumentations", "completion": "    return [\n        convert_keypoint_from_albumentations(kp, target_format, rows, cols, check_validity, angle_in_degrees)\n        for kp in keypoints\n    ]", "idx": 1054}
{"namespace": "albumentations.core.transforms_interface.to_tuple", "completion": "    if isinstance(param, (int, float)):\n        if low is None:\n            low = -param\n        return (low - bias, param + bias)\n    elif isinstance(param, (tuple, list)):\n        return tuple(p + bias for p in param)\n    else:\n        raise ValueError(\"Input argument must be a scalar, tuple, or list of 2+ elements.\")", "idx": 1055}
{"namespace": "albumentations.core.composition.ReplayCompose.replay", "completion": "    replayed_augmentations = saved_augmentations\n    for key, value in replayed_augmentations.items():\n        if isinstance(value, dict) and \"replay\" in value:\n            replayed_augmentations[key] = _restore_for_replay(value[\"replay\"], kwargs)\n    return replayed_augmentations", "idx": 1056}
{"namespace": "albumentations.core.serialization.shorten_class_name", "completion": "    if class_fullname.startswith(\"albumentations.\"):\n        return class_fullname[len(\"albumentations.\"):]\n    return class_fullname", "idx": 1057}
{"namespace": "wandb.util.to_forward_slash_path", "completion": "    if platform.system() == \"Windows\":\n        return path.replace(\"\\\\\", \"/\")\n    else:\n        return path", "idx": 1058}
{"namespace": "wandb.util.make_artifact_name_safe", "completion": "    cleaned_name = re.sub(r'[^a-zA-Z0-9\\-\\._]', '_', name)\n    \n    # If the length of the cleaned name is greater than 128, truncate the name with dots in the middle\n    if len(cleaned_name) > 128:\n        prefix_length = 64\n        suffix_length = 64\n        truncated_name = f\"{cleaned_name[:prefix_length]}...{cleaned_name[-suffix_length:]}\"\n        return truncated_name\n    else:\n        return cleaned_name", "idx": 1059}
{"namespace": "wandb.sdk.wandb_settings._redact_dict", "completion": "    redacted_dict = d.copy()\n    for key in unsafe_keys:\n        if key in redacted_dict:\n            redacted_dict[key] = redact_str\n    return redacted_dict", "idx": 1060}
{"namespace": "wandb.sdk.launch.builder.build.get_current_python_version", "completion": "    return sys.version.split(\" \")[0], sys.version_info[0]", "idx": 1061}
{"namespace": "wandb.sdk.artifacts.storage_policy.StoragePolicy.lookup_by_name", "completion": "        for subclass in cls.__subclasses__():\n            if subclass.name() == name:\n                return subclass\n        raise NotImplementedError(f\"No storage policy found with name {name}\")", "idx": 1062}
{"namespace": "wandb.sdk.lib.runid.generate_id", "completion": "    characters = string.ascii_lowercase + string.digits\n    return ''.join(secrets.choice(characters) for _ in range(length))", "idx": 1063}
{"namespace": "wandb.sdk.internal.file_stream.CRDedupeFilePolicy.get_consecutive_offsets", "completion": "        intervals = []\n        start = None\n        end = None\n        for offset in sorted(console.keys()):\n            if start is None:\n                start = offset\n                end = offset\n            elif offset == end + 1:\n                end = offset\n            else:\n                intervals.append([start, end])\n                start = offset\n                end = offset\n        if start is not None:\n            intervals.append([start, end])\n        return intervals", "idx": 1064}
{"namespace": "wandb.sdk.internal.system.assets.ipu.IPUStats.sample", "completion": "        device_data = self._gc_ipu_info.getDevices()  # type: ignore\n        for device in device_data:\n            device_id = device.get(\"id\")\n            if device_id not in self._devices_called or any(\n                key in device for key in self.variable_metric_keys\n            ):\n                filtered_metrics = {\n                    key: value\n                    for key, value in device.items()\n                    if key.endswith(self.variable_metric_keys)\n                }\n                self.samples.append(filtered_metrics)\n                self._devices_called.add(device_id)", "idx": 1065}
{"namespace": "csvkit.cleanup.join_rows", "completion": "    return [joiner.join(row) for row in zip(*rows)]", "idx": 1066}
{"namespace": "csvkit.convert.guess_format", "completion": "    file_extension = filename.split('.')[-1]\n    if file_extension in ['csv', 'dbf', 'fixed', 'xls', 'xlsx']:\n        return file_extension\n    elif file_extension == 'js':\n        return 'json'\n    else:\n        return None", "idx": 1067}
{"namespace": "folium.utilities.normalize", "completion": "    # Remove non-functional spaces and newlines\n    return re.sub(r'[\\t\\n\\r\\f\\v]', '', rendered)", "idx": 1068}
{"namespace": "tpot.gp_deap.initialize_stats_dict", "completion": "\n    individual.statistics = {\n        'generation': 'INVALID',\n        'mutation_count': 0,\n        'crossover_count': 0,\n        'predecessor': None\n    }", "idx": 1069}
{"namespace": "bentoml_cli.env_manager.remove_env_arg", "completion": "    env_arg_pattern = re.compile(r'--env(=|\\s+)(\\w+)')\n    updated_cmd_args = []\n    for arg in cmd_args:\n        if not env_arg_pattern.match(arg):\n            updated_cmd_args.append(arg)\n    return updated_cmd_args", "idx": 1070}
{"namespace": "bentoml._internal.utils.uri.path_to_uri", "completion": "    path = os.path.abspath(path)\n    if os.name == \"nt\":\n        return \"file:///\" + quote(path.replace(os.sep, \"/\"))\n    else:\n        return \"file://\" + quote(path.replace(os.sep, \"/\"))", "idx": 1071}
{"namespace": "bentoml._internal.utils.uri.uri_to_path", "completion": "    parsed_uri = urlparse(uri)\n    if parsed_uri.scheme == 'file':\n        path = url2pathname(parsed_uri.path)\n        return unquote(path)\n    else:\n        raise ValueError(\"Unsupported URI scheme\")", "idx": 1072}
{"namespace": "bentoml._internal.utils.validate_labels", "completion": "    if not isinstance(labels, dict):\n        raise ValueError(\"labels must be a dictionary\")\n    for key, value in labels.items():\n        if not isinstance(key, str):\n            raise ValueError(\"Label keys must be strings\")\n        if not isinstance(value, str):\n            raise ValueError(\"Label values must be strings\")", "idx": 1073}
{"namespace": "bentoml._internal.configuration.helpers.is_valid_ip_address", "completion": "    try:\n        ipaddress.ip_address(addr)\n        return True\n    except ValueError:\n        return False", "idx": 1074}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batches_to_batch", "completion": "        # Concatenate the batches along the specified batch dimension\n        concatenated_df = pd.concat(batches, axis=batch_dim)\n\n        # Calculate the indices of the subbatches\n        indices = list(itertools.accumulate(df.shape[0] for df in batches))\n        indices = [0] + indices\n\n        return concatenated_df, indices", "idx": 1075}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batch_to_batches", "completion": "        return [batch.iloc[indices[i]:indices[i+1]] for i in range(len(indices)-1)]", "idx": 1076}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batches_to_batch", "completion": "        assert batch_dim == 0, \"Default Runner DataContainer does not support batch_dim other than 0\"\n        batch = list(itertools.chain.from_iterable(batches))\n        indices = list(itertools.accumulate(len(subbatch) for subbatch in batches))\n        indices = [0] + indices\n        return batch, indices", "idx": 1077}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batch_to_batches", "completion": "        subbatches = []\n        start = 0\n        for end in indices[1:]:\n            subbatch = batch[start:end]\n            subbatches.append(subbatch)\n            start = end\n        return subbatches", "idx": 1078}
{"namespace": "jwt.utils.force_bytes", "completion": "    if isinstance(value, str):\n        return value.encode('utf-8')\n    elif isinstance(value, bytes):\n        return value\n    else:\n        raise TypeError(\"Unsupported type, value must be bytes or string\")", "idx": 1079}
{"namespace": "pytube.cli.display_progress_bar", "completion": "    bar_length = 100\n    percent = float(bytes_received) / filesize\n    arrow = ch * int(round(percent * bar_length * scale))\n    spaces = ' ' * (int(bar_length * scale) - len(arrow))\n    sys.stdout.write(f\"\\rDownloading: [{arrow + spaces}] {int(percent * 100)}%\")\n    sys.stdout.flush()", "idx": 1080}
{"namespace": "pytube.cli._download", "completion": "    target = target or os.getcwd()\n    filename = filename or safe_filename(stream.title)\n\n    print(f\"Downloading {filename}...\")\n\n    stream.download(output_path=target, filename=filename)\n\n    print(f\"{filename} has been downloaded to {target}\")", "idx": 1081}
{"namespace": "pytube.cli.display_streams", "completion": "    streams = youtube.streams\n    for stream in streams:\n        print(stream)", "idx": 1082}
{"namespace": "pytube.cli._unique_name", "completion": "    filename = f\"{base}.{subtype}\"\n    if media_type == \"audio\":\n        filename = f\"{base}_audio.{subtype}\"\n    elif media_type == \"video\":\n        filename = f\"{base}_video.{subtype}\"\n\n    file_path = os.path.join(target, filename)\n    if not os.path.exists(file_path):\n        return filename\n\n    index = 1\n    while True:\n        new_filename = f\"{base}_{index}.{subtype}\"\n        if media_type == \"audio\":\n            new_filename = f\"{base}_audio_{index}.{subtype}\"\n        elif media_type == \"video\":\n            new_filename = f\"{base}_video_{index}.{subtype}\"\n\n        new_file_path = os.path.join(target, new_filename)\n        if not os.path.exists(new_file_path):\n            return new_filename\n        index += 1", "idx": 1083}
{"namespace": "pytube.cli._print_available_captions", "completion": "    print(\"Available caption codes:\")\n    for caption in captions:\n        print(caption.code)", "idx": 1084}
{"namespace": "pytube.cipher.throttling_reverse", "completion": "    arr.reverse()", "idx": 1085}
{"namespace": "pytube.helpers.setup_logger", "completion": "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n    stream_handler = logging.StreamHandler()\n    stream_handler.setFormatter(formatter)\n    logger.addHandler(stream_handler)\n    \n    if log_filename:\n        file_handler = logging.FileHandler(log_filename)\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n    \n    logger.setLevel(level)", "idx": 1086}
{"namespace": "pytube.helpers.deprecated", "completion": "    def decorator(func: Callable) -> Callable:\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            warnings.simplefilter('always', DeprecationWarning)  # turn off filter\n            warnings.warn(f\"Call to deprecated function {func.__name__}. Reason: {reason}\",\n                          category=DeprecationWarning,\n                          stacklevel=2)\n            warnings.simplefilter('default', DeprecationWarning)  # reset filter\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator", "idx": 1087}
{"namespace": "pytube.helpers.uniqueify", "completion": "    seen = set()\n    seen_add = seen.add\n    return [x for x in duped_list if not (x in seen or seen_add(x))]", "idx": 1088}
{"namespace": "pytube.helpers.target_directory", "completion": "    if output_path is None:\n        return os.getcwd()\n    elif os.path.isabs(output_path):\n        if not os.path.exists(output_path):\n            os.makedirs(output_path)\n        return output_path\n    else:\n        abs_path = os.path.abspath(output_path)\n        if not os.path.exists(abs_path):\n            os.makedirs(abs_path)\n        return abs_path", "idx": 1089}
{"namespace": "pytube.extract.is_private", "completion": "    private_strings = [\n        'This video is private.'\n    ]\n    for string in private_strings:\n        if string in watch_html:\n            return True\n    return False", "idx": 1090}
{"namespace": "pymc.math.cartesian", "completion": "    return np.array(np.meshgrid(*arrays)).T.reshape(-1, len(arrays))", "idx": 1091}
{"namespace": "pymc.math.log1mexp", "completion": "    x = pytensor.tensor.as_tensor_variable(x)\n    if not negative_input:\n        warnings.warn(\n            \"pymc.math.log1mexp will expect a negative input in a future \"\n            \"version of PyMC.\\n To suppress this warning set `negative_input=True`\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        x = -x\n\n    out = pt.switch(x < -0.6931471805599453, pt.log1p(-pt.exp(x)), pt.log(-pt.expm1(x)))\n    return out", "idx": 1092}
{"namespace": "pymc.math.log1mexp_numpy", "completion": "    if not negative_input:\n        warnings.warn(\n            \"pymc.math.log1mexp will expect a negative input in a future \"\n            \"version of PyMC.\\n To suppress this warning set `negative_input=True`\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        x = -x\n\n    return np.log1p(-np.exp(x))", "idx": 1093}
{"namespace": "pymc.util.drop_warning_stat", "completion": "\n    for group in idata.sample_stats.groups():\n        if \"warning\" in idata.sample_stats[group].data_vars:\n            idata.sample_stats[group] = idata.sample_stats[group].drop_vars(\"warning\")\n\n    return idata", "idx": 1094}
{"namespace": "pymc.pytensorf.walk_model", "completion": "\n    seen = set()\n    stack = list(graphs)\n\n    while stack:\n        var = stack.pop()\n        if var in seen:\n            continue\n        seen.add(var)\n        yield var\n        if stop_at_vars and var in stop_at_vars:\n            continue\n        stack.extend(expand_fn(var))", "idx": 1095}
{"namespace": "pymc.testing.select_by_precision", "completion": "    if pytensor.config.floatX == \"float64\":\n        return float64\n    elif pytensor.config.floatX == \"float32\":\n        return float32\n    else:\n        raise ValueError(\"Unsupported floatX mode\")", "idx": 1096}
{"namespace": "pymc.gp.cov.handle_args", "completion": "    @wraps(func)\n    def wrapper(X, args=None):\n        if args is None:\n            return func(X)\n        else:\n            return func(X, *args)\n    return wrapper", "idx": 1097}
{"namespace": "pymc.gp.util.kmeans_inducing_points", "completion": "\n    # Use K-means algorithm to initialize the locations of the inducing points\n    centroids, _ = kmeans(X, n_inducing, **kmeans_kwargs)\n    return centroids", "idx": 1098}
{"namespace": "pymc.pytensorf.floatX", "completion": "    floatX = pytensor.config.floatX\n    try:\n        return X.astype(floatX)\n    except AttributeError:\n        # Scalar passed\n        return np.asarray(X, dtype=floatX)", "idx": 1099}
{"namespace": "pymc.distributions.multivariate.posdef", "completion": "    try:\n        np.linalg.cholesky(AA)\n        return True\n    except np.linalg.LinAlgError:\n        return False", "idx": 1100}
{"namespace": "pymc.distributions.dist_math.multigammaln", "completion": "    return p * (p - 1) * np.log(np.pi) / 4 + np.sum(gammaln(a - 0.5 * np.arange(p)[:, None]), axis=0)", "idx": 1101}
{"namespace": "pymc.distributions.dist_math.incomplete_beta", "completion": "    return pt.betainc(a, b, value)", "idx": 1102}
{"namespace": "pymc.sampling.forward.observed_dependent_deterministics", "completion": "    observed_rvs = model.observed_RVs\n    basic_rvs = model.basic_RVs\n    deterministics = model.deterministics\n\n    observed_deps = set()\n    for obs in observed_rvs:\n        observed_deps.update(ancestors(obs, deterministics))\n\n    return [d for d in deterministics if any(dep in observed_deps for dep in ancestors(d, basic_rvs))]", "idx": 1103}
{"namespace": "pymc.smc.kernels.systematic_resampling", "completion": "    N = len(weights)\n    u = (np.arange(N) + rng.random()) / N\n    new_indices = np.zeros(N, dtype=int)\n    cum_weights = np.cumsum(weights)\n    i, j = 0, 0\n    while i < N:\n        if u[i] < cum_weights[j]:\n            new_indices[i] = j\n            i += 1\n        else:\n            j += 1\n    return new_indices", "idx": 1104}
{"namespace": "pymc.backends.base._squeeze_cat", "completion": "\n    if combine:\n        if squeeze:\n            return [np.squeeze(np.concatenate(results))]\n        else:\n            return [np.concatenate(results)]\n    else:\n        if squeeze:\n            return [np.squeeze(result) for result in results]\n        else:\n            return results", "idx": 1105}
{"namespace": "pymc.logprob.transforms.SimplexTransform.forward", "completion": "        value = pt.as_tensor(value)\n        value_sum = pt.sum(value, -1, keepdims=True)\n        value_sum_expanded = pt.concatenate([value, value_sum], -1)\n        return pt.log(value_sum_expanded)", "idx": 1106}
{"namespace": "pymc.logprob.transforms.SimplexTransform.backward", "completion": "        value = pt.as_tensor(value)\n        log_value = pt.log(value)\n        N = value.shape[-1].astype(value.dtype)\n        shift = pt.sum(log_value, -1, keepdims=True) / N\n        return log_value[..., :-1] - shift", "idx": 1107}
{"namespace": "pymc.logprob.utils.walk_model", "completion": "    for graph in graphs:\n        if not isinstance(graph, TensorVariable):\n            raise TypeError(\"Input graphs must be instances of TensorVariable\")\n\n        if stop_at_vars is None:\n            stop_at_vars = set()\n\n        seen = set()\n        stack = [graph]\n\n        while stack:\n            var = stack.pop()\n            if var not in seen:\n                seen.add(var)\n                yield var\n                if var.owner and (walk_past_rvs or not isinstance(var.owner.op, MeasurableVariable)):\n                    stack.extend(expand_fn(var))\n                if var in stop_at_vars:\n                    continue\n                if var.owner and isinstance(var.owner.op, HasInnerGraph):\n                    stack.extend(var.owner.op.inner_outputs)", "idx": 1108}
{"namespace": "sacred.metrics_logger.linearize_metrics", "completion": "    linearized = {}\n    for metric in logged_metrics:\n        if metric.name not in linearized:\n            linearized[metric.name] = {\"steps\": [], \"values\": [], \"timestamps\": []}\n        linearized[metric.name][\"steps\"].append(metric.step)\n        linearized[metric.name][\"values\"].append(metric.value)\n        linearized[metric.name][\"timestamps\"].append(metric.timestamp)\n    return linearized", "idx": 1109}
{"namespace": "sacred.utils.set_by_dotted_path", "completion": "    split_path = path.split(\".\")\n    current_option = d\n    for p in split_path[:-1]:\n        if p not in current_option:\n            current_option[p] = {}\n        current_option = current_option[p]\n    current_option[split_path[-1]] = value", "idx": 1110}
{"namespace": "sacred.utils.get_by_dotted_path", "completion": "    try:\n        for key in path.split('.'):\n            d = d[key]\n        return d\n    except (KeyError, TypeError):\n        return default", "idx": 1111}
{"namespace": "pymc.logprob.scan.construct_scan", "completion": "\n    # Construct the scan operation\n    scan_op = Scan(scan_args.inner_inputs, scan_args.inner_outputs, scan_args.info, mode=kwargs.get('mode'))\n\n    # Create a node based on the input and output variables\n    scan_node = scan_op.make_node(*scan_args.outer_inputs)\n\n    # Return the node outputs and updates\n    return scan_node.outputs, scan_node.updates", "idx": 1112}
{"namespace": "sacred.utils.is_prefix", "completion": "    return path.startswith(pre_path)", "idx": 1113}
{"namespace": "sacred.utils.get_inheritors", "completion": "    inheritors = set()\n    for subclass in cls.__subclasses__():\n        inheritors.add(subclass)\n        inheritors |= get_inheritors(subclass)\n    return inheritors", "idx": 1114}
{"namespace": "sacred.utils.convert_camel_case_to_snake_case", "completion": "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()", "idx": 1115}
{"namespace": "sacred.utils.module_exists", "completion": "    return pkgutil.find_loader(modname) is not None", "idx": 1116}
{"namespace": "sacred.utils.apply_backspaces_and_linefeeds", "completion": "    result = []\n    for char in text:\n        if char == '\\b':\n            if result:\n                result.pop()\n        elif char == '\\n':\n            result.append(char)\n        else:\n            result.append(char)\n    return ''.join(result)", "idx": 1117}
{"namespace": "sacred.commands.help_for_command", "completion": "    return pydoc.getdoc(command).replace('\\b', '')", "idx": 1118}
{"namespace": "sacred.optional.optional_import", "completion": "    for package_name in package_names:\n        try:\n            imported_package = importlib.import_module(package_name)\n            return True, imported_package\n        except ImportError:\n            pass\n    return False, None", "idx": 1119}
{"namespace": "sacred.dependencies.get_py_file_if_possible", "completion": "    if pyc_name.endswith((\".py\", \".so\", \".pyd\", \".ipynb\")):\n        return pyc_name\n    py_name = pyc_name[:-1] if pyc_name.endswith(\".pyc\") else pyc_name + \"c\"\n    if os.path.exists(py_name):\n        return py_name\n    return pyc_name", "idx": 1120}
{"namespace": "sacred.config.custom_containers.DogmaticDict.update", "completion": "        if iterable is not None:\n            if hasattr(iterable, \"keys\"):\n                for key in iterable:\n                    self[key] = iterable[key]\n            else:\n                for item in iterable:\n                    key, value = item\n                    self[key] = value\n        for key, value in kwargs.items():\n            self[key] = value", "idx": 1121}
{"namespace": "sacred.config.config_scope.is_empty_or_comment", "completion": "    return line.strip() == \"\" or line.strip().startswith(\"#\")", "idx": 1122}
{"namespace": "boltons.funcutils.copy_function", "completion": "        import copy\n        import types\n    import types\n    import copy\n\n    if not isinstance(orig, types.FunctionType):\n        raise TypeError(\"The 'orig' parameter must be a function.\")\n\n    copied_function = types.FunctionType(orig.__code__, orig.__globals__, orig.__name__, orig.__defaults__,\n                                         orig.__closure__)\n\n    if copy_dict:\n        for key, value in orig.__dict__.items():\n            setattr(copied_function, key, copy.copy(value))\n\n    return copied_function", "idx": 1123}
{"namespace": "sacred.config.config_scope.dedent_line", "completion": "    if line.startswith(indent):\n        return line[len(indent):]\n    else:\n        return line", "idx": 1124}
{"namespace": "boltons.funcutils.format_invocation", "completion": "    if kwargs is None:\n        kwargs = {}\n    args_str = ', '.join(map(repr, args))\n    kwargs_str = ', '.join('%s=%r' % (k, v) for k, v in kwargs.items())\n    if args_str and kwargs_str:\n        kwargs_str = ', ' + kwargs_str\n    return '%s(%s%s)' % (name, args_str, kwargs_str)", "idx": 1125}
{"namespace": "boltons.listutils.SplayList.shift", "completion": "        item = self.pop(item_index)\n        self.insert(dest_index, item)", "idx": 1126}
{"namespace": "boltons.strutils.gzip_bytes", "completion": "    out = BytesIO()\n    with GzipFile(fileobj=out, mode='wb', compresslevel=level) as f:\n        f.write(bytestring)\n    return out.getvalue()", "idx": 1127}
{"namespace": "boltons.strutils.is_uuid", "completion": "    if version not in [1, 2, 3, 4, 5]:\n        raise ValueError(\"Invalid UUID version specified\")\n\n    if isinstance(obj, uuid.UUID):\n        if version == 0:\n            return True\n        return obj.version == version\n    elif isinstance(obj, str):\n        try:\n            uuid_obj = uuid.UUID(obj)\n            if version == 0:\n                return True\n            return uuid_obj.version == version\n        except ValueError:\n            return False\n    else:\n        return False", "idx": 1128}
{"namespace": "boltons.strutils.parse_int_list", "completion": "    int_list = []\n    for part in range_string.split(delim):\n        if range_delim in part:\n            start, end = map(int, part.split(range_delim))\n            int_list.extend(range(start, end + 1))\n        else:\n            int_list.append(int(part))\n    return sorted(int_list)", "idx": 1129}
{"namespace": "dash.development.base_component.Component._traverse", "completion": "        children = getattr(self, \"children\", None)\n\n        if isinstance(children, Component):\n            yield children\n            yield from children._traverse()\n        elif isinstance(children, (tuple, MutableSequence)):\n            for item in children:\n                if isinstance(item, Component):\n                    yield item\n                    yield from item._traverse()", "idx": 1130}
{"namespace": "dash.development._r_components_generation.make_namespace_exports", "completion": "    export_string = \"\"\n    for component in components:\n        if prefix:\n            export_string += f\"export({prefix}, {component})\\n\"\n        export_string += f\"export({component})\\n\"\n    return export_string", "idx": 1131}
{"namespace": "dash.development._collect_nodes.collect_nodes", "completion": "    if nodes is None:\n        nodes = []\n\n    for key, value in metadata.items():\n        if is_node(value):\n            nodes.append(base + key)\n        elif is_shape(value):\n            nodes = collect_nodes(value, base + key + \".\", nodes)\n        elif value.get(\"name\") == \"arrayOf\":\n            nodes = collect_array(value, base + key + \"[]\", nodes)\n        elif value.get(\"name\") == \"union\":\n            nodes = collect_union(value.get(\"value\"), base + key + \".\", nodes)\n        elif value.get(\"name\") == \"objectOf\":\n            nodes = collect_object(value, base + key + \"{}\", nodes)\n\n    return nodes", "idx": 1132}
{"namespace": "peewee.Index.where", "completion": "        # Combine the expressions using AND and add them to the where clause.\n        self._where = reduce(operator.and_, expressions)", "idx": 1133}
{"namespace": "playhouse.dataset.DataSet.tables", "completion": "        tables = self._database.get_tables()\n        if self._include_views:\n            tables.extend(self.views)\n        return tables", "idx": 1134}
{"namespace": "playhouse.dataset.DataSet.update_cache", "completion": "        if table is not None:\n            tables_to_update = [table] + self.get_table_dependencies(table)\n        else:\n            tables_to_update = self.tables\n\n        for table_name in tables_to_update:\n            if table_name in self._models:\n                del self._models[table_name]\n\n        self._models.update(\n            self._introspector.generate_models(\n                tables=tables_to_update,\n                skip_invalid=True,\n                literal_column_names=True,\n                include_views=self._include_views,\n                **kwargs)\n        )", "idx": 1135}
{"namespace": "playhouse.dataset.DataSet.freeze", "completion": "        self._check_arguments(filename, file_obj, format, self._export_formats)\n        if filename:\n            file_obj = open_file(filename, 'w', encoding)\n\n        exporter = self._export_formats[format](query)\n        exporter.export(file_obj, **kwargs)\n\n        if filename:\n            file_obj.close()", "idx": 1136}
{"namespace": "playhouse.db_url.parse", "completion": "    parsed = urlparse(url)\n    return parseresult_to_dict(parsed, unquote_password)", "idx": 1137}
{"namespace": "playhouse.db_url.connect", "completion": "    parsed = parse(url, unquote_password)\n    parsed.update(connect_params)\n    db_class = schemes[parsed['scheme']]\n    return db_class(**parsed)", "idx": 1138}
{"namespace": "playhouse.sqlite_changelog.ChangeLog.install", "completion": "        if create_table:\n            self.model.create_table()\n\n        if drop:\n            for action in self._actions:\n                self.db.execute_sql(self.drop_trigger_sql(model, action))\n\n        if insert:\n            self.db.execute_sql(self.trigger_sql(model, 'INSERT', skip_fields))\n\n        if update:\n            self.db.execute_sql(self.trigger_sql(model, 'UPDATE', skip_fields))\n\n        if delete:\n            self.db.execute_sql(self.trigger_sql(model, 'DELETE', skip_fields))", "idx": 1139}
{"namespace": "playhouse.kv.KeyValue.pop", "completion": "        with self._database.atomic():\n            try:\n                value = self[key]\n                del self[key]\n                return value\n            except KeyError:\n                if default is not Sentinel:\n                    return default\n                raise", "idx": 1140}
{"namespace": "playhouse.signals.Signal.connect", "completion": "        if name is None:\n            name = receiver.__name__\n\n        if (name, sender) in self._receivers:\n            raise ValueError('Receiver named %s for sender=%s already exists.' % (name, sender))\n\n        self._receivers.add((name, sender))\n        self._receiver_list.append((name, receiver, sender))", "idx": 1141}
{"namespace": "playhouse.signals.Signal.disconnect", "completion": "        if receiver:\n            name = name or receiver.__name__\n            key = (name, sender)\n            if key in self._receivers:\n                self._receivers.remove(key)\n                self._receiver_list = [(n, r, s) for n, r, s in self._receiver_list if (n, s) != key]", "idx": 1142}
{"namespace": "backtrader.trade.Trade.update", "completion": "        oldsize = self.size\n        oldprice = self.price\n\n        self.commission += commission\n\n        self.size += size\n        self.price = price\n\n        if size:\n            self.isopen = True\n\n        self.barlen += 1\n\n        if not self.size:\n            self.isclosed = True\n\n        if self.size * oldsize < 0:\n            self.barclose = self.data.barnum\n            self.dtclose = self.data.datetime(0)\n\n        if abs(self.size) >= abs(oldsize):\n            self.baropen = self.data.barnum\n            self.dtopen = self.data.datetime(0)\n\n        if self.historyon:\n            self.history.append(\n                TradeHistory(\n                    status=self.status,\n                    dt=self.data.datetime(0),\n                    barlen=self.barlen,\n                    size=self.size,\n                    price=self.price,\n                    value=self.value,\n                    pnl=self.pnl,\n                    pnlcomm=self.pnlcomm,\n                    tz=self.data._tz\n                )\n            )", "idx": 1143}
{"namespace": "ydata_profiling.profile_report.ProfileReport.typeset", "completion": "        return self._typeset", "idx": 1144}
{"namespace": "ydata_profiling.report.presentation.flavours.html.frequency_table.HTMLFrequencyTable.render", "completion": "        if isinstance(self.content, list):\n            rows = \"\"\n            for row in self.content:\n                rows += \"<tr>\"\n                for cell in row:\n                    rows += f\"<td>{cell}</td>\"\n                rows += \"</tr>\"\n            html = f\"\"\"\n            <table>\n                {rows}\n            </table>\n            \"\"\"\n        else:\n            html = f\"\"\"\n            <table>\n                <tr>\n                    <td>{self.content}</td>\n                </tr>\n            </table>\n            \"\"\"\n        return html", "idx": 1145}
{"namespace": "ydata_profiling.report.presentation.flavours.html.image.HTMLImage.render", "completion": "        template = self.template_env.get_template(\"diagram.html\")\n        return template.render(image=self.content)", "idx": 1146}
{"namespace": "ydata_profiling.model.summary_algorithms.histogram_compute", "completion": "    bins = config.histogram.bins\n    if bins == \"auto\":\n        bins = \"auto\" if n_unique > config.histogram.max_unique else min(n_unique, config.histogram.max_bins)\n    histogram, bin_edges = np.histogram(finite_values, bins=bins, weights=weights)\n    return {\n        f\"{name}_bin_edges\": bin_edges.tolist(),\n        f\"{name}_values\": histogram.tolist(),\n        f\"{name}_chi_square\": chi_square(histogram=histogram),\n    }", "idx": 1147}
{"namespace": "ydata_profiling.model.summarizer.BaseSummarizer.summarize", "completion": "\n        summarizer = self.get_summarizer(dtype)\n        return summarizer.summarize(config, series)", "idx": 1148}
{"namespace": "ydata_profiling.model.pandas.discretize_pandas.Discretizer.discretize_dataframe", "completion": "        discretized_df = dataframe.copy()\n        numerical_columns = self._get_numerical_columns(discretized_df)\n\n        for column in numerical_columns:\n            discretized_column = self._discretize_column(discretized_df[column])\n            discretized_df[column] = discretized_column\n\n        if self.reset_index:\n            discretized_df.reset_index(drop=True, inplace=True)\n\n        return discretized_df", "idx": 1149}
{"namespace": "ydata_profiling.model.pandas.correlations_pandas.pandas_cramers_compute", "completion": "\n    df_cols_dict = {i: list(df.columns).index(i) for i in df.columns}\n\n    selcols = {\n        key\n        for key, value in summary.items()\n        if value[\"type\"] != \"Unsupported\"\n        and 1 < value[\"n_distinct\"] <= config.categorical_maximum_correlation_distinct\n    }\n    selected_cols = sorted(selcols, key=lambda i: df_cols_dict[i])\n\n    if len(selected_cols) <= 1:\n        return None\n\n    correlation_matrix = pd.DataFrame(\n        np.ones((len(selected_cols), len(selected_cols))),\n        index=selected_cols,\n        columns=selected_cols,\n    )\n    for col_1_name, col_2_name in itertools.combinations(selected_cols, 2):\n        score = _pairwise_cramers(df[col_1_name], df[col_2_name])\n        (\n            correlation_matrix.loc[col_1_name, col_2_name],\n            correlation_matrix.loc[col_2_name, col_1_name],\n        ) = (score, score)\n\n    return correlation_matrix", "idx": 1150}
{"namespace": "ydata_profiling.model.pandas.correlations_pandas.pandas_auto_compute", "completion": "\n    threshold = config.categorical_maximum_correlation_distinct\n\n    numerical_cols = {\n        key\n        for key, value in summary.items()\n        if value[\"type\"] == \"Numeric\" and value[\"n_distinct\"] > 1\n    }\n\n    categorical_cols = {\n        key\n        for key, value in summary.items()\n        if value[\"type\"] in {\"Categorical\", \"Boolean\"} and 1 < value[\"n_distinct\"] <= threshold\n    }\n\n    if len(numerical_cols) <= 1 and len(categorical_cols) <= 1:\n        return None\n\n    discretized_df = df.copy()\n    for col in numerical_cols:\n        discretized_df[col] = pd.qcut(df[col], q=10, duplicates=\"drop\")\n\n    correlation_matrix = pd.DataFrame(index=numerical_cols | categorical_cols, columns=numerical_cols | categorical_cols)\n\n    for col1, col2 in itertools.product(numerical_cols | categorical_cols, repeat=2):\n        if col1 in numerical_cols and col2 in numerical_cols:\n            correlation_matrix.loc[col1, col2] = _pairwise_spearman(discretized_df[col1], discretized_df[col2])\n        elif col1 in categorical_cols and col2 in categorical_cols:\n            confusion_matrix = pd.crosstab(discretized_df[col1], discretized_df[col2])\n            correlation_matrix.loc[col1, col2] = _cramers_corrected_stat(confusion_matrix, correction=True)\n        else:\n            correlation_matrix.loc[col1, col2] = np.nan\n\n    return correlation_matrix", "idx": 1151}
{"namespace": "ydata_profiling.controller.console.main", "completion": "\n    # Parse command line arguments\n    parsed_args = parse_args(args)\n\n    # Generate profiling report\n    profile = ProfileReport(\n        Path(parsed_args.input_file),\n        title=parsed_args.title,\n        minimal=parsed_args.minimal,\n        explorative=parsed_args.explorative,\n        infer_dtypes=parsed_args.infer_dtypes,\n        config_file=parsed_args.config_file,\n    )\n\n    # Save report to output file\n    if parsed_args.output_file:\n        profile.to_file(parsed_args.output_file)\n    else:\n        profile.to_file(Path(parsed_args.input_file).with_suffix(\".html\"))\n\n    # Open report if not in silent mode\n    if not parsed_args.silent:\n        profile.to_notebook_iframe()", "idx": 1152}
{"namespace": "ydata_profiling.utils.cache.cache_file", "completion": "    data_path = get_data_path()\n    data_path.mkdir(exist_ok=True)\n\n    file_path = data_path / file_name\n\n    if not file_path.exists():\n        response = request.urlopen(url)\n        file_path.write_bytes(response.read())\n\n    return file_path", "idx": 1153}
{"namespace": "ydata_profiling.utils.dataframe.expand_mixed", "completion": "    if types is None:\n        types = [list, dict, tuple]\n\n    for column in df.columns:\n        if df[column].apply(lambda x: any(isinstance(x, t) for t in types)).any():\n            expanded = df[column].apply(lambda x: pd.Series(x) if isinstance(x, (list, dict, tuple)) else x)\n            expanded.columns = [f\"{column}_{c}\" for c in expanded.columns]\n            df = pd.concat([df, expanded], axis=1)\n            df = df.drop(columns=[column])\n\n    return df", "idx": 1154}
{"namespace": "pysnooper.utils.ensure_tuple", "completion": "    if isinstance(x, str):\n        return (x,)\n    try:\n        return tuple(x)\n    except TypeError:\n        return (x,)", "idx": 1155}
{"namespace": "rq.serializers.resolve_serializer", "completion": "    if serializer is None:\n        return DefaultSerializer\n    elif isinstance(serializer, str):\n        # Load the serializer from the provided string path\n        # Assuming the serializer is a class with 'dumps' and 'loads' methods\n        module_name, class_name = serializer.rsplit('.', 1)\n        module = __import__(module_name, fromlist=[class_name])\n        serializer_class = getattr(module, class_name)\n        if hasattr(serializer_class, 'dumps') and hasattr(serializer_class, 'loads'):\n            return serializer_class\n        else:\n            raise NotImplementedError(\"The provided serializer does not implement 'dumps' and 'loads' methods\")\n    elif issubclass(serializer, DefaultSerializer):\n        return serializer\n    else:\n        raise ValueError(\"Invalid serializer type\")", "idx": 1156}
{"namespace": "lux.vis.Vis.Vis.get_attr_by_channel", "completion": "        return list(filter(lambda x: x.channel == channel, self._inferred_intent))", "idx": 1157}
{"namespace": "lux.action.default.register_default_actions", "completion": "    import action_module_1\n    import action_module_2\n    import action_module_3\n\n    display_condition_1 = action_module_1.get_display_condition()\n    display_condition_2 = action_module_2.get_display_condition()\n    display_condition_3 = action_module_3.get_display_condition()\n\n    global_actions = {\n        \"action_1\": display_condition_1,\n        \"action_2\": display_condition_2,\n        \"action_3\": display_condition_3\n    }\n\n    for action, display_condition in global_actions.items():\n        register_action(action, display_condition)", "idx": 1158}
{"namespace": "folium.utilities.get_bounds", "completion": "\n    if lonlat:\n        locations = _locations_mirror(locations)\n\n    lat_min = lon_min = float(\"inf\")\n    lat_max = lon_max = float(\"-inf\")\n\n    for location in locations:\n        lat, lon = location\n        lat_min = none_min(lat_min, lat)\n        lat_max = none_max(lat_max, lat)\n        lon_min = none_min(lon_min, lon)\n        lon_max = none_max(lon_max, lon)\n\n    return [[lat_min, lon_min], [lat_max, lon_max]]", "idx": 1159}
{"namespace": "folium.features.VegaLite.vegalite_major_version", "completion": "        schema = self.data[\"$schema\"]\n        version = int(schema.split(\"/\")[5][1])\n        return version", "idx": 1160}
{"namespace": "music_dl.utils.colorize", "completion": "    if platform.system() == \"Windows\" or color not in colors:\n        return string\n    return f\"{colors[color]}{string}\\033[0m\"", "idx": 1161}
{"namespace": "music_dl.source.MusicSource.search", "completion": "        ret_songs_list = []\n        ret_errors = []\n        threads = []\n        for source in sources_list:\n            t = threading.Thread(target=self.search_thread, args=(source, keyword, ret_songs_list, ret_errors))\n            threads.append(t)\n            t.start()\n\n        for t in threads:\n            t.join()\n\n        # Sort and remove duplicates from the search results\n        ret_songs_list = sorted(ret_songs_list, key=lambda x: (x[\"title\"], x[\"singer\"], x[\"size\"]))\n        ret_songs_list = [dict(t) for t in {tuple(d.items()) for d in ret_songs_list}]\n\n        return ret_songs_list", "idx": 1162}
{"namespace": "jwt.utils.base64url_decode", "completion": "    if isinstance(input, str):\n        input = input.encode(\"utf-8\")\n\n    # Pad the input with \"=\" characters if necessary\n    input += b\"=\" * (4 - len(input) % 4) % 4\n\n    return base64.urlsafe_b64decode(input)", "idx": 1163}
{"namespace": "jwt.utils.to_base64url_uint", "completion": "    if val < 0:\n        raise ValueError(\"Input value must be a positive integer\")\n    data = val.to_bytes((val.bit_length() + 7) // 8, byteorder=\"big\")\n    if not data:\n        data = b\"\\x00\"\n    return base64url_encode(data)", "idx": 1164}
{"namespace": "jwt.algorithms.HMACAlgorithm.prepare_key", "completion": "        if isinstance(key, str):\n            key = key.encode(\"utf-8\")\n        if key.startswith(b\"-----BEGIN \"):\n            raise InvalidKeyError(\"The specified key is an asymmetric key or x509 certificate and should not be used as an HMAC secret.\")\n        return key", "idx": 1165}
{"namespace": "jwt.algorithms.HMACAlgorithm.to_jwk", "completion": "        if isinstance(key_obj, str):\n            key_bytes = key_obj.encode(\"utf-8\")\n        elif isinstance(key_obj, bytes):\n            key_bytes = key_obj\n        else:\n            raise TypeError(\"Key object must be a string or bytes\")\n\n        # Convert the key to base64url encoding\n        key_base64url = base64url_encode(key_bytes).decode()\n\n        # Create the JWK dictionary\n        jwk_dict: JWKDict = {\n            \"kty\": \"oct\",\n            \"k\": key_base64url\n        }\n\n        # Return the JWK as a dictionary or JSON string based on the value of as_dict\n        if as_dict:\n            return jwk_dict\n        else:\n            return json.dumps(jwk_dict)", "idx": 1166}
{"namespace": "jwt.algorithms.HMACAlgorithm.from_jwk", "completion": "        if isinstance(jwk, str):\n            jwk_dict = json.loads(jwk)\n        elif isinstance(jwk, dict):\n            jwk_dict = jwk\n        else:\n            raise ValueError(\"Invalid JWK format\")\n\n        if jwk_dict.get(\"kty\") != \"oct\":\n            raise ValueError(\"Invalid key type\")\n\n        return base64url_decode(jwk_dict[\"k\"])", "idx": 1167}
{"namespace": "sacred.arg_parser._convert_value", "completion": "    try:\n        return ast.literal_eval(value)\n    except (ValueError, SyntaxError):\n        return value", "idx": 1168}
{"namespace": "sacred.utils.recursive_update", "completion": "    for key, value in u.items():\n        if isinstance(value, collections.Mapping):\n            d[key] = recursive_update(d.get(key, {}), value)\n        else:\n            d[key] = value\n    return d", "idx": 1169}
{"namespace": "sacred.utils.iterate_flattened_separately", "completion": "    if manually_sorted_keys is None:\n        manually_sorted_keys = []\n\n    manually_sorted_keys_set = set(manually_sorted_keys)\n    remaining_keys = set(dictionary.keys()) - manually_sorted_keys_set\n\n    for key in manually_sorted_keys:\n        value = dictionary[key]\n        if isinstance(value, dict) and value:\n            for k, v in iterate_flattened_separately(value):\n                yield join_paths(key, k), v\n        else:\n            yield key, value\n\n    non_dict_keys = sorted(\n        (k for k in remaining_keys if not isinstance(dictionary[k], dict))\n    )\n    for key in non_dict_keys:\n        yield key, dictionary[key]\n\n    remaining_keys -= set(non_dict_keys)\n    remaining_keys = sorted(remaining_keys)\n\n    for key in remaining_keys:\n        value = dictionary[key]\n        if isinstance(value, dict) and value:\n            yield key, PATHCHANGE\n            for k, v in iterate_flattened_separately(value):\n                yield join_paths(key, k), v\n        else:\n            yield key, value", "idx": 1170}
{"namespace": "sacred.utils.iterate_flattened", "completion": "    for key, value in iterate_flattened_separately(d):\n        yield key, value", "idx": 1171}
{"namespace": "sacred.utils.iter_prefixes", "completion": "    parts = path.split(\".\")\n    for i in range(1, len(parts) + 1):\n        yield \".\".join(parts[:i])", "idx": 1172}
{"namespace": "sacred.utils.rel_path", "completion": "    if path.startswith(base):\n        return path[len(base):].lstrip(\"/\")\n    else:\n        assert False, f\"{base} not a prefix of {path}\"", "idx": 1173}
{"namespace": "sacred.utils.convert_to_nested_dict", "completion": "    nested_dict = {}\n    for key, value in dotted_dict.items():\n        set_by_dotted_path(nested_dict, key, value)\n    return nested_dict", "idx": 1174}
{"namespace": "sacred.utils.format_sacred_error", "completion": "    formatted_error = [short_usage]\n    if e.print_traceback:\n        formatted_error.append(format_filtered_stacktrace(e.filter_traceback))\n    else:\n        formatted_error.append(f\"{type(e).__name__}: {str(e)}\")\n    return \"\\n\".join(formatted_error)", "idx": 1175}
{"namespace": "sacred.utils.get_package_version", "completion": "    try:\n        package = importlib.import_module(name)\n        version_string = getattr(package, \"__version__\", None)\n        if version_string:\n            return version.parse(version_string)\n        else:\n            return None\n    except (ModuleNotFoundError, AttributeError):\n        return None", "idx": 1176}
{"namespace": "sacred.experiment.Experiment.main", "completion": "        \"\"\"\n        This function is a decorator that is used to define the main function of an experiment. The main function is the default command that is executed when no command is specified or when calling the run() method. It captures the decorated function and sets it as the default command for the experiment.\n        Input-Output Arguments\n        :param self: Experiment. An instance of the Experiment class.\n        :param function: The function to be decorated and set as the main function.\n        :return: The captured function that is set as the default command.\n        \"\"\"\n        captured = self.main(function)\n        if function.__module__ == \"__main__\":\n            # Ensure that automain is not used in interactive mode.\n            import inspect\n\n            main_filename = inspect.getfile(function)\n            if main_filename == \"<stdin>\" or (\n                main_filename.startswith(\"<ipython-input-\")\n                and main_filename.endswith(\">\")\n            ):\n                raise RuntimeError(\n                    \"Cannot use @ex.automain decorator in \"\n                    \"interactive mode. Use @ex.main instead.\"\n                )\n\n            self.run_commandline()\n        return captured", "idx": 1177}
{"namespace": "sacred.experiment.Experiment.run", "completion": "        run = self._create_run(command_name, config_updates, named_configs, info, meta_info, options)\n        try:\n            run()\n        except Exception as e:\n            if run.pdb:\n                import traceback\n                import pdb\n\n                traceback.print_exception(*sys.exc_info())\n                pdb.post_mortem()\n            else:\n                raise\n        return run", "idx": 1178}
{"namespace": "sacred.host_info.host_info_getter", "completion": "    if name is None:\n        name = func.__name__\n    host_info_gatherers[name] = HostInfoGetter(func, name)\n    return func", "idx": 1179}
{"namespace": "sacred.ingredient.Ingredient.command", "completion": "        if function in self.commands:\n            return function\n        self.commands[function.__name__] = function\n        return function", "idx": 1180}
{"namespace": "sacred.ingredient.Ingredient.config", "completion": "        config_scope = ConfigScope(function)\n        self.configurations.append(config_scope)\n        return config_scope", "idx": 1181}
{"namespace": "sacred.ingredient.Ingredient.named_config", "completion": "        \"\"\"\n        This function is a decorator that turns a function into a named configuration. It creates a ConfigScope instance based on the input function and adds it to the named configurations of the Ingredient instance.\n        Input-Output Arguments\n        :param self: Ingredient. An instance of the Ingredient class.\n        :param func: Function. The function to be turned into a named configuration.\n        :return: ConfigScope. The created ConfigScope object.\n        \"\"\"\n        config_scope = ConfigScope(func)\n        self.named_configs[func.__name__] = config_scope\n        return config_scope", "idx": 1182}
{"namespace": "sacred.ingredient.Ingredient.gather_commands", "completion": "        for ingredient, _ in self.traverse_ingredients():\n            for cmd_name, cmd in ingredient.commands.items():\n                full_cmd_name = join_paths(ingredient.path, cmd_name)\n                yield full_cmd_name, cmd", "idx": 1183}
{"namespace": "sacred.ingredient.Ingredient.gather_named_configs", "completion": "        for ingredient, _ in self.traverse_ingredients():\n            for name, config in ingredient.named_configs.items():\n                full_name = join_paths(ingredient.path, name)\n                yield full_name, config", "idx": 1184}
{"namespace": "sacred.dependencies.Source.create", "completion": "        if not os.path.isfile(filename):\n            raise ValueError(f\"Invalid filename or file not found: {filename}\")\n\n        filename = get_py_file_if_possible(filename)\n        digest = get_digest(filename)\n        repo, commit, is_dirty = get_commit_if_possible(filename, save_git_info)\n\n        return Source(filename, digest, repo, commit, is_dirty)", "idx": 1185}
{"namespace": "sacred.dependencies.Source.to_json", "completion": "        if base_dir:\n            rel_path = os.path.relpath(self.filename, base_dir)\n            return rel_path, self.digest\n        else:\n            return self.filename, self.digest", "idx": 1186}
{"namespace": "sacred.dependencies.PackageDependency.create", "completion": "        try:\n            if mod.__name__ in cls.modname_to_dist:\n                dist = cls.modname_to_dist[mod.__name__]\n            else:\n                dist = pkg_resources.working_set.find(pkg_resources.Requirement.parse(mod.__name__))\n                cls.modname_to_dist[mod.__name__] = dist\n            return cls(dist.project_name, dist.version)\n        except Exception as e:\n            raise ValueError(f\"Failed to create PackageDependency for module {mod.__name__}\") from e", "idx": 1187}
{"namespace": "sacred.dependencies.is_local_source", "completion": "    return os.path.abspath(filename).startswith(os.path.abspath(experiment_path))", "idx": 1188}
{"namespace": "sacred.dependencies.gather_sources_and_dependencies", "completion": "    experiment_path, main = get_main_file(globs, save_git_info)\n    source_discovery_strategy = SETTINGS.SOURCE_DISCOVERY_STRATEGY\n    dependency_discovery_strategy = SETTINGS.DEPENDENCY_DISCOVERY_STRATEGY\n\n    sources = source_discovery_strategies[source_discovery_strategy](\n        globs, base_dir or experiment_path, save_git_info\n    )\n    dependencies = dependency_discovery_strategies[dependency_discovery_strategy](\n        globs, base_dir or experiment_path\n    )\n\n    if \"numpy\" in sys.modules:\n        dependencies.add(PackageDependency(\"numpy\", sys.modules[\"numpy\"].__version__))\n\n    return main, sources, dependencies", "idx": 1189}
{"namespace": "sacred.observers.file_storage.FileStorageObserver.resource_event", "completion": "        store_path = self.find_or_save(filename, self.resource_dir)\n        self.run_entry[\"resources\"].append(store_path)\n        self.save_json(self.run_entry, \"run.json\")", "idx": 1190}
{"namespace": "sacred.config.signature.Signature.get_free_parameters", "completion": "        if bound:\n            expected_args = self.arguments[1:]\n        else:\n            expected_args = self.arguments\n\n        free_params = []\n        for arg in expected_args:\n            if arg not in kwargs:\n                free_params.append(arg)\n\n        return free_params", "idx": 1191}
{"namespace": "sacred.config.signature.Signature.construct_arguments", "completion": "        self._assert_no_unexpected_args(self._get_expected_args(bound), args)\n        self._assert_no_unexpected_kwargs(self._get_expected_args(bound), kwargs)\n        self._assert_no_duplicate_args(self._get_expected_args(bound), args, kwargs)\n        args, kwargs = self._fill_in_options(args, kwargs, options, bound)\n        self._assert_no_missing_args(args, kwargs, bound)\n        return args, kwargs", "idx": 1192}
{"namespace": "sacred.config.config_files.load_config_file", "completion": "    handler = get_handler(filename)\n    with open(filename, \"r\" + handler.mode) as f:\n        return handler.load(f)", "idx": 1193}
{"namespace": "sacred.config.custom_containers.DogmaticDict.get", "completion": "        if k in self:\n            return self[k]\n        elif self.fallback and k in self.fallback:\n            return self.fallback[k]\n        else:\n            return d", "idx": 1194}
{"namespace": "sacred.config.custom_containers.DogmaticDict.revelation", "completion": "        missing_keys = set(self.fixed.keys()) - set(self.keys())\n        for key in missing_keys:\n            self[key] = self.fixed[key]\n            if isinstance(self.fixed[key], DogmaticDict):\n                missing_subkeys = {f\"{key}.{subkey}\" for subkey in self.fixed[key].revelation()}\n                missing_keys |= missing_subkeys\n        return missing_keys", "idx": 1195}
{"namespace": "sacred.config.custom_containers.make_read_only", "completion": "    if isinstance(o, dict):\n        return ReadOnlyDict({k: make_read_only(v) for k, v in o.items()})\n    elif isinstance(o, list):\n        return ReadOnlyList([make_read_only(v) for v in o])\n    elif isinstance(o, tuple):\n        return tuple(make_read_only(v) for v in o)\n    else:\n        return o", "idx": 1196}
{"namespace": "sacred.config.config_scope.dedent_function_body", "completion": "    lines = body.split(\"\\n\")\n    non_empty_lines = [line for line in lines if not is_empty_or_comment(line)]\n    indent = re.match(r\"^\\s*\", non_empty_lines[0]).group(0)\n    dedented_lines = [dedent_line(line, indent) for line in lines]\n    return \"\".join(dedented_lines)", "idx": 1197}
{"namespace": "boltons.funcutils.FunctionBuilder.get_sig_str", "completion": "            if with_annotations:\n                return inspect_formatargspec(self.args, self.varargs,\n                                             self.varkw, self.defaults,\n                                             self.kwonlyargs, self.kwonlydefaults,\n                                             self.annotations)\n            else:\n                return inspect_formatargspec(self.args, self.varargs,\n                                             self.varkw, self.defaults,\n                                             self.kwonlyargs, self.kwonlydefaults)", "idx": 1198}
{"namespace": "boltons.funcutils.FunctionBuilder.get_invocation_str", "completion": "            args = ', '.join(self.args)\n            varargs = '*' + self.varargs if self.varargs else ''\n            kwonlyargs = ', '.join(self.kwonlyargs)\n            varkw = '**' + self.varkw if self.varkw else ''\n            return f\"{args}, {varargs}, {kwonlyargs}, {varkw}\"", "idx": 1199}
{"namespace": "boltons.funcutils.FunctionBuilder.from_func", "completion": "        args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, annotations = cls._argspec_to_dict(func).values()\n        if isinstance(func, functools.partial):\n            build_from = func.func\n        else:\n            build_from = func\n\n        fb = cls(name=func.__name__,\n                 doc=func.__doc__,\n                 module=func.__module__,\n                 body='pass',\n                 args=args,\n                 varargs=varargs,\n                 varkw=varkw,\n                 defaults=defaults,\n                 kwonlyargs=kwonlyargs,\n                 kwonlydefaults=kwonlydefaults,\n                 annotations=annotations)\n\n        return fb", "idx": 1200}
{"namespace": "boltons.funcutils.FunctionBuilder.get_defaults_dict", "completion": "        defaults_dict = {}\n        if self.defaults:\n            for i, arg in enumerate(self.args[-len(self.defaults):]):\n                defaults_dict[arg] = self.defaults[i]\n        if self.kwonlydefaults:\n            defaults_dict.update(self.kwonlydefaults)\n        return defaults_dict", "idx": 1201}
{"namespace": "boltons.funcutils.FunctionBuilder.get_arg_names", "completion": "        if only_required:\n            return tuple(arg for arg in self.args if arg not in self.defaults)\n        else:\n            return tuple(self.args + self.kwonlyargs)", "idx": 1202}
{"namespace": "boltons.ioutils.SpooledIOBase.writelines", "completion": "        self._checkClosed()\n        for line in lines:\n            self.write(line)", "idx": 1203}
{"namespace": "boltons.ioutils.SpooledBytesIO.write", "completion": "        self._checkClosed()\n        if not isinstance(s, binary_type):\n            raise TypeError(f\"{binary_type.__name__} expected, got {type(s).__name__}\")\n        current_pos = self.tell()\n        if self.buffer.tell() + len(s) >= self._max_size:\n            self.rollover()\n        self.buffer.write(s)\n        self._tell = current_pos + len(s)", "idx": 1204}
{"namespace": "boltons.ioutils.SpooledBytesIO.seek", "completion": "        self._checkClosed()\n        return self.buffer.seek(pos, mode)", "idx": 1205}
{"namespace": "boltons.ioutils.SpooledBytesIO.len", "completion": "        pos = self.buffer.tell()\n        if self._rolled:\n            self.buffer.seek(0, os.SEEK_END)\n            length = self.buffer.tell()\n            self.buffer.seek(pos)\n        else:\n            length = pos\n        return length", "idx": 1206}
{"namespace": "boltons.ioutils.SpooledStringIO.read", "completion": "        self._checkClosed()\n        ret = self.buffer.read(n).decode('utf-8')\n        self._tell = self.tell() + len(ret)\n        return ret", "idx": 1207}
{"namespace": "boltons.ioutils.SpooledStringIO.write", "completion": "        self._checkClosed()\n        if not isinstance(s, text_type):\n            raise TypeError(\"{} expected, got {}\".format(\n                text_type.__name__,\n                type(s).__name__\n            ))\n\n        s_bytes = s.encode('utf-8')\n        if self.tell() + len(s_bytes) >= self._max_size:\n            self.rollover()\n        self.buffer.write(s_bytes)\n        self._tell += len(s)", "idx": 1208}
{"namespace": "boltons.ioutils.SpooledStringIO.seek", "completion": "        self._checkClosed()\n        if mode not in (0, 1, 2):\n            raise ValueError(f'Invalid whence ({mode}, should be 0, 1, or 2)')\n        if mode == 0:\n            self._tell = pos\n        elif mode == 1:\n            self._tell += pos\n        elif mode == 2:\n            self._tell = len(self.getvalue()) + pos\n        return self._tell", "idx": 1209}
{"namespace": "boltons.ioutils.SpooledStringIO.tell", "completion": "        \"\"\"\n        Return the codepoint position in the SpooledStringIO instance.\n\n        Input-Output Arguments\n        :param self: SpooledStringIO, an instance of SpooledStringIO class.\n        :return: Int, the codepoint position.\n\n        \"\"\"\n        return self._tell", "idx": 1210}
{"namespace": "boltons.ioutils.SpooledStringIO.len", "completion": "        pos = self.tell()\n        self.seek(0)\n        codepoint_count = 0\n        while True:\n            chunk = self.read(READ_CHUNK_SIZE)\n            if not chunk:\n                break\n            codepoint_count += len(chunk.decode('utf-8'))\n        self.seek(pos)\n        return codepoint_count", "idx": 1211}
{"namespace": "boltons.ioutils.MultiFileReader.read", "completion": "        if self._joiner == b'':\n            data = b''\n            for f in self._fileobjs:\n                data += f.read(amt)\n        else:\n            data = u''\n            for f in self._fileobjs:\n                data += f.read(amt).decode('utf-8')\n        return data", "idx": 1212}
{"namespace": "boltons.ioutils.MultiFileReader.seek", "completion": "        if whence != os.SEEK_SET:\n            raise NotImplementedError(\"MultiFileReader.seek() only supports os.SEEK_SET\")\n        if offset != 0:\n            raise NotImplementedError(\"MultiFileReader only supports seeking to start at this time\")\n        for fileobj in self._fileobjs:\n            fileobj.seek(0)", "idx": 1213}
{"namespace": "boltons.listutils.BarrelList.insert", "completion": "        list_idx, rel_idx = self._translate_index(index)\n        if list_idx is None:\n            raise IndexError()\n        self.lists[list_idx].insert(rel_idx, item)\n        self._balance_list(list_idx)", "idx": 1214}
{"namespace": "boltons.listutils.BarrelList.pop", "completion": "        if not self.lists:\n            return\n        if not a:\n            return self.lists[-1].pop()\n        index = a[0]\n        if index < 0:\n            index += len(self)\n        list_idx, rel_idx = self._translate_index(index)\n        if list_idx is None:\n            return\n        return self.lists[list_idx].pop(rel_idx)", "idx": 1215}
{"namespace": "boltons.listutils.BarrelList.sort", "completion": "        # poor pythonist's mergesort, it's faster than sorted(self)\n        # when the lists' average length is greater than 512.\n        lists = self.lists\n        for i in range(len(lists)):\n            lists[i].sort()\n        merged = list(heapq.merge(*lists))\n        self.lists = [merged]\n        self._balance_list(0)", "idx": 1216}
{"namespace": "boltons.urlutils.URL.path", "completion": "        self.path_parts = tuple([unquote(p) if '%' in p else p for p in path_text.split('/')])", "idx": 1217}
{"namespace": "boltons.urlutils.URL.navigate", "completion": "        if isinstance(dest, str):\n            dest = URL(dest)\n        new_url = URL.from_parts(\n            scheme=dest.scheme or self.scheme,\n            host=dest.host or self.host,\n            path_parts=dest.path_parts,\n            query_params=dest.query_params,\n            fragment=dest.fragment,\n            port=dest.port or self.port,\n            username=dest.username,\n            password=dest.password\n        )\n        new_url.normalize()\n        return new_url", "idx": 1218}
{"namespace": "boltons.urlutils.URL.to_text", "completion": "        ret = self.scheme + ':'\n        if self.uses_netloc:\n            ret += '//'\n            if self.username:\n                ret += quote_userinfo_part(self.username, full_quote=full_quote)\n                if self.password:\n                    ret += ':' + quote_userinfo_part(self.password, full_quote=full_quote)\n                ret += '@'\n            ret += self.get_authority(full_quote=full_quote)\n        ret += quote_path_part(self.path, full_quote=full_quote)\n        if self._query:\n            ret += '?' + self._query\n        if self.fragment:\n            ret += '#' + quote_fragment_part(self.fragment, full_quote=full_quote)\n        return ret", "idx": 1219}
{"namespace": "boltons.urlutils.QueryParamDict.to_text", "completion": "        query_string = ''\n        for key, value in self.items(multi=True):\n            if isinstance(value, list):\n                for v in value:\n                    if full_quote:\n                        query_string += quote(key) + '=' + quote(v) + '&'\n                    else:\n                        query_string += key + '=' + v + '&'\n            else:\n                if full_quote:\n                    query_string += quote(key) + '=' + quote(value) + '&'\n                else:\n                    query_string += key + '=' + value + '&'\n        return query_string.rstrip('&')", "idx": 1220}
{"namespace": "boltons.tbutils.TracebackInfo.from_traceback", "completion": "        if tb is None:\n            exc_type, exc_value, tb = sys.exc_info()\n            if tb is None:\n                raise ValueError('no tb set and no exception being handled')\n        return cls.from_frame(frame=tb.tb_frame, limit=limit)", "idx": 1221}
{"namespace": "boltons.tbutils.ExceptionInfo.get_formatted", "completion": "        return '%s%s' % (self.tb_info.get_formatted(), self.get_formatted_exception_only())", "idx": 1222}
{"namespace": "boltons.cacheutils.ThresholdCounter.get", "completion": "        try:\n            return self._count_map[key][0]\n        except KeyError:\n            return default", "idx": 1223}
{"namespace": "boltons.iterutils.backoff_iter", "completion": "    if count == 'repeat':\n        count = None\n    if count is not None:\n        count = int(count)\n        if count <= 0:\n            raise ValueError('expected a positive integer count')\n    if start <= 0 or stop <= 0:\n        raise ValueError('expected positive start and stop values')\n    if factor <= 1:\n        raise ValueError('expected a factor greater than 1')\n\n    cur = start\n    i = 0\n    while True:\n        if count is not None and i >= count:\n            break\n        if cur > stop:\n            break\n        yield cur\n        i += 1\n        cur *= factor\n        if jitter:\n            cur += cur * random.uniform(-1, 1) * jitter\n    return", "idx": 1224}
{"namespace": "boltons.cacheutils.cached", "completion": "\n    def cached_decorator(func):\n        return CachedFunction(func, cache, scoped=scoped, typed=typed, key=key)\n\n    return cached_decorator", "idx": 1225}
{"namespace": "boltons.timeutils.total_seconds", "completion": "    return td.days * 24 * 3600 + td.seconds + td.microseconds / 1e6", "idx": 1226}
{"namespace": "boltons.gcutils.get_all", "completion": "    if include_subtypes:\n        return [obj for obj in gc.get_objects() if isinstance(obj, type_obj)]\n    else:\n        return [obj for obj in gc.get_objects() if type(obj) is type_obj]", "idx": 1227}
{"namespace": "boltons.timeutils.daterange", "completion": "    current = start\n    if step > 0:\n        while stop is None or (current < stop) or (inclusive and current == stop):\n            yield current\n            current += timedelta(days=step)\n    elif step < 0:\n        while stop is None or (current > stop) or (inclusive and current == stop):\n            yield current\n            current += timedelta(days=step)\n    else:\n        raise ValueError(\"Step cannot be zero\")", "idx": 1228}
{"namespace": "boltons.mathutils.clamp", "completion": "    return max(lower, min(x, upper))", "idx": 1229}
{"namespace": "boltons.mathutils.ceil", "completion": "    if options is None:\n        return _ceil(x)\n    options = sorted(options)\n\n    i = bisect.bisect_left(options, x)\n    if i == len(options):\n        raise ValueError(\"no ceiling options greater than or equal to: %r\" % x)\n    return options[i]", "idx": 1230}
{"namespace": "boltons.formatutils.get_format_args", "completion": "    positional_args = []\n    named_args = []\n    \n    for token in tokenize_format_str(fstr):\n        if isinstance(token, BaseFormatField):\n            if token.is_positional:\n                positional_args.append((token.base_name, token.type_func))\n            else:\n                named_args.append((token.base_name, token.type_func))\n    \n    return positional_args, named_args", "idx": 1231}
{"namespace": "boltons.mathutils.floor", "completion": "    if options is None:\n        return _floor(x)\n    options = sorted(options)\n    i = bisect.bisect_right(options, x)\n    if i == 0:\n        raise ValueError(\"no floor options less than or equal to: %r\" % x)\n    return options[i - 1]", "idx": 1232}
{"namespace": "boltons.dictutils.OneToOne.setdefault", "completion": "        if key not in self:\n            self[key] = default\n        return self[key]", "idx": 1233}
{"namespace": "boltons.dictutils.OneToOne.update", "completion": "        if isinstance(dict_or_iterable, dict):\n            for key, value in dict_or_iterable.items():\n                self[key] = value\n        else:\n            for key, value in dict_or_iterable:\n                self[key] = value\n        for key, value in kw.items():\n            self[key] = value", "idx": 1234}
{"namespace": "boltons.dictutils.ManyToMany.get", "completion": "        return frozenset(self.data.get(key, default))", "idx": 1235}
{"namespace": "boltons.dictutils.FrozenDict.updated", "completion": "        new_fd = FrozenDict(self)\n        new_fd.update(*a, **kw)\n        return new_fd", "idx": 1236}
{"namespace": "boltons.dictutils.subdict", "completion": "    if keep is None:\n        keep = d.keys()\n    if drop is None:\n        drop = []\n    return {k: v for k, v in d.items() if k in keep and k not in drop}", "idx": 1237}
{"namespace": "boltons.dictutils.FrozenDict.__repr__", "completion": "        \"\"\"\n        This function returns a string representation of the FrozenDict instance. The format is f'{class_name}({dict_repr})'\n        Input-Output Arguments\n        :param self: FrozenDict. An instance of the FrozenDict class.\n        :return: String. The string representation of the FrozenDict instance.\n        \"\"\"\n        cn = self.__class__.__name__\n        dict_repr = dict.__repr__(self)\n        return f\"{cn}({dict_repr})\"", "idx": 1238}
{"namespace": "gunicorn.config.validate_callable", "completion": "    def validator(val):\n        if not val:\n            return None\n        if isinstance(val, str):\n            module_name, obj_name = val.rsplit('.', 1)\n            module = __import__(module_name, fromlist=[obj_name])\n            val = getattr(module, obj_name)\n        if not callable(val):\n            raise TypeError(\"Value must be a callable object\")\n        if arity != -1 and util.get_arity(val) != arity:\n            raise TypeError(f\"Value must have an arity of {arity}\")\n        return val\n    return validator", "idx": 1239}
{"namespace": "gunicorn.config.get_default_config_file", "completion": "    default_config_file = os.path.join(os.getcwd(), 'gunicorn.conf.py')\n    if os.path.exists(default_config_file):\n        return default_config_file\n    else:\n        return None", "idx": 1240}
{"namespace": "gunicorn.util.is_ipv6", "completion": "    try:\n        socket.inet_pton(socket.AF_INET6, addr)\n        return True\n    except socket.error:\n        return False", "idx": 1241}
{"namespace": "gunicorn.systemd.listen_fds", "completion": "    listen_pid = os.environ.get('LISTEN_PID')\n    if listen_pid != str(os.getpid()):\n        return 0\n    listen_fds = int(os.environ.get('LISTEN_FDS', 0))\n    if unset_environment:\n        os.environ.pop('LISTEN_PID', None)\n        os.environ.pop('LISTEN_FDS', None)\n    return listen_fds", "idx": 1242}
{"namespace": "gunicorn.util.http_date", "completion": "    if timestamp is None:\n        timestamp = time.time()\n    return email.utils.formatdate(timestamp, usegmt=True)", "idx": 1243}
{"namespace": "gunicorn.util.parse_address", "completion": "    if netloc.startswith('[') and ']' in netloc:\n        # IPv6 address\n        host, _, port = netloc[1:].partition(']')\n        if port.startswith(':'):\n            port = port[1:]\n        else:\n            port = default_port\n        return host, port\n    elif netloc.startswith('fd://'):\n        # File descriptor\n        return netloc, None\n    elif netloc.startswith('unix:'):\n        # Unix socket\n        return netloc[6:], None\n    else:\n        # TCP address\n        host, _, port = netloc.partition(':')\n        return host, port if port else default_port", "idx": 1244}
{"namespace": "gunicorn.util.to_bytestring", "completion": "    if isinstance(value, bytes):\n        return value\n    if isinstance(value, str):\n        return value.encode(encoding)\n    raise TypeError(\"Expected bytes or string, but got %r\" % type(value))", "idx": 1245}
{"namespace": "gunicorn.util.warn", "completion": "    sys.stderr.write(\"WARNING: %s\\n\" % msg)", "idx": 1246}
{"namespace": "gunicorn.util.split_request_uri", "completion": "    if uri.startswith(\"//\"):\n        uri = \".\" + uri\n    return urllib.parse.urlparse(uri)", "idx": 1247}
{"namespace": "praw.models.listing.listing.ModNoteListing.after", "completion": "        if not self.has_next_page:\n            return None\n        else:\n            return self.end_cursor", "idx": 1248}
{"namespace": "praw.models.util.permissions_string", "completion": "    if permissions is None:\n        return \"+all\"\n    else:\n        return \",\".join(permissions)", "idx": 1249}
{"namespace": "jc.cli.JcCli.json_out", "completion": "        if self.pretty:\n            return json.dumps(self.data_out, indent=4, separators=(',', ': '), ensure_ascii=self.ascii_only)\n        else:\n            return json.dumps(self.data_out, separators=(',', ': '), ensure_ascii=self.ascii_only)", "idx": 1250}
{"namespace": "pythonforandroid.pythonpackage.transform_dep_for_pip", "completion": "\n    if \"@\" in dependency and \"://\" in dependency:\n        return dependency  # Already in URL format\n    else:\n        return dependency  # Original dependency", "idx": 1251}
{"namespace": "pythonforandroid.graph.fix_deplist", "completion": "    modified_deps = []\n    for dep in deps:\n        if isinstance(dep, str):\n            modified_deps.append((dep.lower(),))\n        else:\n            modified_deps.append(tuple(d.lower() for d in dep))\n    return modified_deps", "idx": 1252}
{"namespace": "pythonforandroid.util.walk_valid_filens", "completion": "    for root, dirs, files in walk(base_dir):\n        for dir_name in invalid_dir_names:\n            if dir_name in dirs:\n                dirs.remove(dir_name)\n        for file_pattern in invalid_file_patterns:\n            files = [f for f in files if not fnmatch(join(root, f), file_pattern)]\n        for file in files:\n            yield join(root, file)", "idx": 1253}
{"namespace": "pythonforandroid.bootstrap._cmp_bootstraps_by_priority", "completion": "    priority_order = {name: index for index, name in enumerate(default_recipe_priorities)}\n    priority_a = priority_order.get(a.name, len(default_recipe_priorities))\n    priority_b = priority_order.get(b.name, len(default_recipe_priorities))\n    return priority_a - priority_b", "idx": 1254}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.all_bootstraps", "completion": "        bootstraps_dir = join(cls.ctx.root_dir, 'bootstraps')\n        available_bootstraps = set()\n        for entry in listdir(bootstraps_dir):\n            if isdir(join(bootstraps_dir, entry)):\n                available_bootstraps.add(entry)\n        return available_bootstraps", "idx": 1255}
{"namespace": "mmcv.image.colorspace._convert_input_type_range", "completion": "    if img.dtype == np.uint8:\n        img = img.astype(np.float32) / 255.0\n    elif img.dtype == np.float32:\n        img = np.clip(img, 0, 1)\n    else:\n        raise TypeError('The input image should be np.uint8 or np.float32, '\n                        f'but got {img.dtype}')\n    return img", "idx": 1256}
{"namespace": "mackup.utils.error", "completion": "    print(message)\n    sys.exit(1)", "idx": 1257}
{"namespace": "mmcv.image.colorspace._convert_output_type_range", "completion": "    if dst_type == np.float32:\n        img = img / 255.0\n    elif dst_type == np.uint8:\n        img = (img * 255).clip(0, 255).astype(np.uint8)\n    else:\n        raise TypeError('The dst_type should be np.float32 or np.uint8, '\n                        f'but got {dst_type}')\n    return img", "idx": 1258}
{"namespace": "mackup.utils.is_process_running", "completion": "    try:\n        subprocess.check_output([\"pgrep\", process_name])\n        return True\n    except subprocess.CalledProcessError:\n        return False", "idx": 1259}
{"namespace": "stellar.operations._get_pid_column", "completion": "    server_version = raw_conn.execute('SELECT version()').scalar()\n    version_number = re.search(r'\\d+\\.\\d+', server_version).group()\n    if raw_conn.engine.dialect.name == 'postgresql':\n        if float(version_number) >= 9.2:\n            return 'pid'\n        else:\n            return 'procpid'\n    else:\n        raise NotSupportedDatabase()", "idx": 1260}
{"namespace": "imapclient.imap_utf7.encode", "completion": "    if not isinstance(s, str):\n        return s\n\n    res = []\n    buffer = []\n    for c in s:\n        if 0x20 <= ord(c) <= 0x7e:\n            if buffer:\n                res.extend([AMPERSAND_ORD] + base64_utf7_encode(buffer) + [DASH_ORD])\n                buffer = []\n            res.append(ord(c))\n        elif c == \"&\":\n            if buffer:\n                res.extend([AMPERSAND_ORD] + base64_utf7_encode(buffer) + [DASH_ORD])\n                buffer = []\n            res.extend([AMPERSAND_ORD, ord(\"-\")])\n        else:\n            buffer.append(c)\n\n    if buffer:\n        res.extend([AMPERSAND_ORD] + base64_utf7_encode(buffer) + [DASH_ORD])\n\n    return bytes(res)", "idx": 1261}
{"namespace": "imapclient.version._imapclient_version_string", "completion": "    major, minor, micro, releaselevel = vinfo\n    if releaselevel != \"final\":\n        version = f\"{major}.{minor}.{micro}-{releaselevel}\"\n    else:\n        version = f\"{major}.{minor}.{micro}\"\n    return version", "idx": 1262}
{"namespace": "telethon.helpers.generate_key_data_from_nonce", "completion": "    server_nonce_bytes = server_nonce.to_bytes(16, byteorder='little')\n    new_nonce_bytes = new_nonce.to_bytes(32, byteorder='little')\n\n    hash1 = sha1(new_nonce_bytes + server_nonce_bytes).digest()\n    hash2 = sha1(server_nonce_bytes + new_nonce_bytes).digest()\n    hash3 = sha1(new_nonce_bytes + new_nonce_bytes).digest()\n\n    key = hash1 + hash2[:12]\n    iv = hash2[12:] + hash3 + new_nonce_bytes[:4]\n\n    return key, iv", "idx": 1263}
{"namespace": "hbmqtt.codecs.bytes_to_int", "completion": "    return int.from_bytes(data, byteorder='big')", "idx": 1264}
{"namespace": "zulipterminal.helper.display_error_if_present", "completion": "    if \"result\" in response and response[\"result\"] == \"error\" and hasattr(controller, \"view\"):\n        error_message = response.get(\"msg\", \"An error occurred\")\n        controller.report_error(error_message)", "idx": 1265}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._decode_message_id", "completion": "        try:\n            return int(message_id)\n        except ValueError:\n            return None", "idx": 1266}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton.handle_narrow_link", "completion": "        parsed_link = self._parse_narrow_link(self.link)\n        validation_error = self._validate_narrow_link(parsed_link)\n\n        if validation_error:\n            self.controller.update_screen(\n                \"Error: \" + validation_error,\n                is_reply_to_stream=False,\n                is_edit_mode=False,\n            )\n        else:\n            self._switch_narrow_to(parsed_link)", "idx": 1267}
{"namespace": "zulipterminal.config.color.color_properties", "completion": "    new_colors = {}\n    for color in colors:\n        color_name = color.name\n        color_value = color.value\n        for p in prop:\n            color_name = f\"{p}_{color_name}\"\n        new_colors[color_name] = color_value\n    return Enum(\"NewColor\", new_colors)", "idx": 1268}
{"namespace": "twilio.base.deserialize.decimal", "completion": "    if d:\n        try:\n            return Decimal(d, context=BasicContext)\n        except (TypeError, ValueError):\n            return d\n    else:\n        return d", "idx": 1269}
{"namespace": "twilio.base.deserialize.integer", "completion": "    try:\n        return int(i)\n    except ValueError:\n        return i", "idx": 1270}
{"namespace": "twilio.base.serialize.object", "completion": "    if isinstance(obj, (dict, list, tuple)):\n        return json.dumps(obj)\n    else:\n        return obj", "idx": 1271}
{"namespace": "twilio.base.serialize.map", "completion": "    return [serialize_func(item) for item in lst]", "idx": 1272}
{"namespace": "twilio.base.obsolete.deprecated_method", "completion": "\n    def deprecated_method_wrapper(func):\n        @functools.wraps(func)\n        def new_func(*args, **kwargs):\n            warnings.simplefilter('always', DeprecationWarning)  # turn off filter\n            warnings.warn(\"Call to deprecated method {}.\".format(func.__name__),\n                          category=DeprecationWarning,\n                          stacklevel=2)\n            warnings.simplefilter('default', DeprecationWarning)  # reset filter\n            return func(*args, **kwargs)\n\n        return new_func\n\n    return deprecated_method_wrapper", "idx": 1273}
{"namespace": "chatette.utils.sample_indulgent", "completion": "    if nb_items >= len(array):\n        return array.copy()\n    return sample(array, nb_items)", "idx": 1274}
{"namespace": "chatette.utils.rchop", "completion": "    if string.endswith(ending):\n        return string[:-len(ending)]\n    return string", "idx": 1275}
{"namespace": "chatette.utils.str_to_bool", "completion": "    if text.lower() == 'true':\n        return True\n    elif text.lower() == 'false':\n        return False\n    else:\n        raise ValueError(\"Input text must be 'True' or 'False'\")", "idx": 1276}
{"namespace": "chatette.utils.min_if_exist", "completion": "    if n1 is not None and n2 is not None:\n        return min(n1, n2)\n    elif n1 is not None:\n        return n1\n    elif n2 is not None:\n        return n2\n    else:\n        return None", "idx": 1277}
{"namespace": "chatette.utils.append_to_list_in_dict", "completion": "    if key not in dict_of_lists:\n        dict_of_lists[key] = [value]\n    else:\n        dict_of_lists[key].append(value)", "idx": 1278}
{"namespace": "chatette.utils.extend_list_in_dict", "completion": "    if key not in dict_of_lists:\n        dict_of_lists[key] = values\n    else:\n        dict_of_lists[key].extend(values)", "idx": 1279}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy._is_end_regex", "completion": "        return re.match(r'\\/(g?i?|i?g?)$', word) is not None", "idx": 1280}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.execute", "completion": "        raise NotImplementedError()", "idx": 1281}
{"namespace": "aioxmpp.network.group_and_order_srv_records", "completion": "    all_records.sort(key=lambda x: (x[0], x[1]))  # Sort by priority and weight\n\n    if rng is None:\n        rng = random\n\n    groups = itertools.groupby(all_records, key=lambda x: x[0])  # Group by priority\n\n    for _, group in groups:\n        records = list(group)\n        records.sort(key=lambda x: x[1])  # Sort by weight\n        for record in records:\n            yield record[2]  # Yield (hostname, port) tuple", "idx": 1282}
{"namespace": "aioxmpp.nonza.StreamFeatures.get_feature", "completion": "    try:\n        return self.features[feature_cls.TAG][0]\n    except (IndexError, KeyError):\n        return default", "idx": 1283}
{"namespace": "aioxmpp.connector.XMPPOverTLSConnector._context_factory_factory", "completion": "\n            def context_factory():\n                ssl_context = metadata.ssl_context_factory()\n                verifier.setup_context(ssl_context, None)\n                if hasattr(ssl_context, \"set_alpn_protos\"):\n                    ssl_context.set_alpn_protos([b'xmpp-client'])\n                return ssl_context\n\n            return context_factory", "idx": 1284}
{"namespace": "aioxmpp.xmltestutils.element_path", "completion": "    path = [el.tag]\n    parent = el.getparent()\n    while parent is not None and parent != upto:\n        siblings = parent.findall(el.tag)\n        if len(siblings) > 1:\n            path.append(f\"[{siblings.index(el) + 1}]\")\n        else:\n            path.append(\"[1]\")\n        el = parent\n        parent = el.getparent()\n    path.reverse()\n    return \"/\".join(path)", "idx": 1285}
{"namespace": "aioxmpp.structs.JID.fromstr", "completion": "\n        parts = s.split(\"@\", 1)\n        if len(parts) == 1:\n            domain, resource = parts[0], None\n        else:\n            domain, resource = parts[1].split(\"/\", 1)\n            localpart = parts[0]\n\n        return cls(localpart, domain, resource, strict=strict)", "idx": 1286}
{"namespace": "aioxmpp.security_layer.extract_python_dict_from_x509", "completion": "\n    subject = x509.get_subject()\n    subject_alt_name = x509.get_extension_count()\n\n    result = {\n        \"subject\": dict(subject.get_components()),\n        \"subjectAltName\": subject_alt_name\n    }\n\n    return result", "idx": 1287}
{"namespace": "aioxmpp.security_layer.extract_blob", "completion": "    return x509.to_cryptography().public_bytes(serialization.Encoding.DER)", "idx": 1288}
{"namespace": "aioxmpp.security_layer.blob_to_pyasn1", "completion": "    pyasn1_struct, _ = pyasn1.codec.der.decoder.decode(blob)\n    return pyasn1_struct", "idx": 1289}
{"namespace": "aioxmpp.security_layer.extract_pk_blob_from_pyasn1", "completion": "    # Add your implementation here\n    public_key_info = pyasn1_struct['tbsCertificate']['subjectPublicKeyInfo']\n    public_key_blob = pyasn1.codec.der.encoder.encode(public_key_info)\n    return public_key_blob", "idx": 1290}
{"namespace": "aioxmpp.callbacks.AdHocSignal.ASYNC_WITH_LOOP", "completion": "        loop = asyncio.get_event_loop() if loop is None else loop\n\n        def async_with_loop_wrapper(f):\n            if not asyncio.iscoroutinefunction(f):\n                raise TypeError(\"must be coroutine, got {!r}\".format(f))\n\n            async def wrapper(*args, **kwargs):\n                return await f(*args, **kwargs)\n\n            return wrapper\n\n        return async_with_loop_wrapper", "idx": 1291}
{"namespace": "aioxmpp.callbacks.AdHocSignal.SPAWN_WITH_LOOP", "completion": "        if loop is None:\n            loop = asyncio.get_event_loop()\n\n        def create_wrapper(f):\n            if not asyncio.iscoroutinefunction(f):\n                raise TypeError(\"must be a coroutine function, got {!r}\".format(f))\n\n            async def spawn_coroutine(*args, **kwargs):\n                task = asyncio.ensure_future(f(*args, **kwargs), loop=loop)\n                task.add_done_callback(lambda fut: log_spawned(cls.logger, fut))\n                return task\n\n            return spawn_coroutine\n\n        return create_wrapper", "idx": 1292}
{"namespace": "aioxmpp.callbacks.first_signal", "completion": "    async def awaitable():\n        fut = asyncio.Future()\n\n        def callback(data):\n            if isinstance(data, Exception):\n                fut.set_exception(data)\n            else:\n                fut.set_result(data)\n\n        for signal in signals:\n            signal.connect(callback)\n\n        result = await fut\n        return result\n\n    return awaitable()", "idx": 1293}
{"namespace": "aioxmpp.tasks.TaskPool.spawn", "completion": "        for group in __groups:\n            limit = self._group_limits.get(group, self.default_limit)\n            if limit is not None and self.get_task_count(group) >= limit:\n                raise RuntimeError(f\"Limit for group {group} exhausted\")\n        total_limit = self._group_limits.get((), self.default_limit)\n        if total_limit is not None and self.get_task_count(()) >= total_limit:\n            raise RuntimeError(\"Total limit exhausted\")\n        task = asyncio.ensure_future(__coro_fun(*args, **kwargs))\n        self._group_tasks.setdefault((), []).append(task)\n        for group in __groups:\n            self._group_tasks.setdefault(group, []).append(task)\n        return task", "idx": 1294}
{"namespace": "aioxmpp.protocol.send_and_wait_for", "completion": "\n    response_received = asyncio.Future()\n\n    def on_response_received(response):\n        if cb:\n            cb(response)\n        response_received.set_result(response)\n\n    xmlstream.stanza_parser.add_class(wait_for, on_response_received)\n\n    try:\n        await xmlstream.send_xso(send)\n        if timeout is not None:\n            response = await asyncio.wait_for(response_received, timeout=timeout)\n        else:\n            response = await response_received\n    except asyncio.TimeoutError:\n        raise TimeoutError\n    finally:\n        xmlstream.stanza_parser.remove_class(wait_for, on_response_received)\n\n    return response", "idx": 1295}
{"namespace": "aioxmpp.testutils.run_coroutine_with_peer", "completion": "\n    if not loop:\n        loop = asyncio.get_event_loop()\n\n    local_future = asyncio.ensure_future(coroutine, loop=loop)\n    peer_future = asyncio.ensure_future(peer_coroutine, loop=loop)\n\n    try:\n        local_result, _ = loop.run_until_complete(\n            asyncio.wait(\n                [local_future, peer_future],\n                timeout=timeout\n            )\n        )\n    except asyncio.TimeoutError:\n        raise TimeoutError(\"Timeout reached while waiting for coroutines to complete\")\n\n    return local_future.result()", "idx": 1296}
{"namespace": "aioxmpp.testutils.make_listener", "completion": "    listener = unittest.mock.Mock()\n    signals = [attr for attr in dir(instance) if isinstance(getattr(instance, attr), callbacks.Signal)]\n    for signal in signals:\n        setattr(listener, signal, callbacks.Signal())\n    return listener", "idx": 1297}
{"namespace": "aioxmpp.vcard.service.VCardService.set_vcard", "completion": "        iq = aioxmpp.IQ(\n            type_=aioxmpp.IQType.SET,\n            to=jid,\n            payload=vcard,\n        )\n\n        await self.client.send(iq)", "idx": 1298}
{"namespace": "aioxmpp.rsm.xso.ResultSetMetadata.limit", "completion": "        result = type(self)()\n        result.max_ = max_\n        return result", "idx": 1299}
{"namespace": "aioxmpp.muc.service.Room.features", "completion": "        return self._service.features", "idx": 1300}
{"namespace": "aioxmpp.xso.query.EvaluationContext.eval_bool", "completion": "        result = self.eval(expr)\n        return bool(result)", "idx": 1301}
{"namespace": "aioxmpp.xso.query._BoolOpMixin.eval", "completion": "        vs1 = self.operand1.eval_leaf(ec)\n        vs2 = self.operand2.eval_leaf(ec)\n\n        for v1 in vs1:\n            for v2 in vs2:\n                if self.operator(v1, v2):\n                    return True\n        return False", "idx": 1302}
{"namespace": "aioxmpp.xso.model.drop_handler", "completion": "    depth = 1\n    while True:\n        ev_type, *ev_args = yield\n        if ev_type == \"start\":\n            depth += 1\n        elif ev_type == \"end\":\n            depth -= 1\n            if depth == 0:\n                break", "idx": 1303}
{"namespace": "aioxmpp.xso.model.guard", "completion": "    depth = 0\n    try:\n        while True:\n            ev_type, *ev_args = yield\n            if ev_type == \"start\":\n                depth += 1\n            elif ev_type == \"end\":\n                depth -= 1\n            try:\n                value = dest.send((ev_type, *ev_args))\n            except StopIteration as e:\n                return e.value\n            except Exception as e:\n                dest.throw(e)\n    except GeneratorExit:\n        dest.close()\n        raise", "idx": 1304}
{"namespace": "aioxmpp.xso.model.capture_events", "completion": "    try:\n        while True:\n            value = yield\n            dest.append(value)\n            receiver.send(value)\n    except GeneratorExit:\n        dest.clear()\n    except Exception:\n        dest.clear()\n        raise", "idx": 1305}
{"namespace": "aioxmpp.xso.model.events_to_sax", "completion": "    for event in events:\n        if event[0] == \"start\":\n            dest.startElementNS(event[1], event[2], event[3])\n        elif event[0] == \"text\":\n            dest.characters(event[1])\n        elif event[0] == \"end\":\n            dest.endElementNS(event[1], event[2])", "idx": 1306}
{"namespace": "aioxmpp.adhoc.service.AdHocClient.get_command_info", "completion": "        disco = self.dependencies[aioxmpp.disco.DiscoClient]\n        response = await disco.query_info(\n            peer_jid,\n            node=command_name,\n        )\n        return response", "idx": 1307}
{"namespace": "aioxmpp.entitycaps.caps115.build_identities_string", "completion": "    identities = list(\n        escape(identity.category).encode(\"utf-8\") +\n        b\"<\" +\n        escape(identity.type_).encode(\"utf-8\") +\n        b\"<\" +\n        (escape(identity.lang).encode(\"utf-8\") if identity.lang else b\"\")\n        for identity in identities\n    )\n\n    if len(set(identities)) != len(identities):\n        raise ValueError(\"duplicate identity\")\n\n    identities.sort()\n    identities.append(b\"\")\n    return b\"<\".join(identities)", "idx": 1308}
{"namespace": "aioxmpp.entitycaps.caps115.build_features_string", "completion": "    features = [\n        escape(feature).encode(\"utf-8\")\n        for feature in features\n    ]\n\n    if len(set(features)) != len(features):\n        raise ValueError(\"duplicate feature\")\n\n    features.sort()\n    features.append(b\"\")\n    return b\"<\".join(features)", "idx": 1309}
{"namespace": "aioxmpp.entitycaps.caps115.build_forms_string", "completion": "    forms = [\n        b\"/\".join([\n            escape(form.type_).encode(\"utf-8\"),\n            escape(form.var).encode(\"utf-8\"),\n            escape(form.label or \"\").encode(\"utf-8\"),\n            escape(form.desc or \"\").encode(\"utf-8\"),\n        ])\n        for form in forms\n    ]\n\n    if len(set(forms)) != len(forms):\n        raise ValueError(\"duplicate form\")\n\n    forms.sort()\n    forms.append(b\"\")\n    return b\"<\".join(forms)", "idx": 1310}
{"namespace": "aioxmpp.entitycaps.caps115.Key.path", "completion": "        quoted_node = urllib.parse.quote(self.node, safe=\"\")\n        return pathlib.Path(\"hashes\") / self.algo / quoted_node", "idx": 1311}
{"namespace": "aioxmpp.entitycaps.caps390._process_features", "completion": "    parts = [\n        feature.encode(\"utf-8\") + b\"\\x1f\"\n        for feature in features\n    ]\n    parts.sort()\n    return b\"\".join(parts) + b\"\\x1e\"", "idx": 1312}
{"namespace": "aioxmpp.entitycaps.caps390._process_identities", "completion": "    parts = [\n        _process_identity(identity)\n        for identity in identities\n    ]\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1c\"", "idx": 1313}
{"namespace": "aioxmpp.entitycaps.caps390._process_extensions", "completion": "    parts = [\n        _process_form(ext)\n        for ext in exts\n    ]\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1c\"", "idx": 1314}
{"namespace": "aioxmpp.entitycaps.caps390._calculate_hash", "completion": "    if algo == \"sha-1\":\n        return aioxmpp.hashes.hash_sha1(hash_input)\n    elif algo == \"sha-256\":\n        return aioxmpp.hashes.hash_sha256(hash_input)\n    elif algo == \"sha-512\":\n        return aioxmpp.hashes.hash_sha512(hash_input)\n    else:\n        raise ValueError(\"Unsupported algorithm: {}\".format(algo))", "idx": 1315}
{"namespace": "sumy.evaluation.rouge._len_lcs", "completion": "    n, m = _get_index_of_lcs(x, y)\n    table = dict()\n    for i in range(n + 1):\n        for j in range(m + 1):\n            if i == 0 or j == 0:\n                table[i, j] = 0\n            elif x[i - 1] == y[j - 1]:\n                table[i, j] = table[i - 1, j - 1] + 1\n            else:\n                table[i, j] = max(table[i - 1, j], table[i, j - 1])\n    return table[n, m]", "idx": 1316}
{"namespace": "sumy.evaluation.rouge._recon_lcs", "completion": "    table = _lcs(x, y)\n    i, j = len(x), len(y)\n    lcs = []\n    while i > 0 and j > 0:\n        if x[i - 1] == y[j - 1]:\n            lcs.insert(0, x[i - 1])\n            i -= 1\n            j -= 1\n        elif table[i - 1, j] > table[i, j - 1]:\n            i -= 1\n        else:\n            j -= 1\n    return lcs", "idx": 1317}
{"namespace": "sumy.evaluation.rouge._union_lcs", "completion": "    reference_words = _split_into_words([reference_sentence])\n    evaluated_words = _split_into_words(evaluated_sentences)\n    lcs = _len_lcs(evaluated_words, reference_words)\n    return lcs / len(reference_words)", "idx": 1318}
{"namespace": "sumy.parsers.html.HtmlParser.from_file", "completion": "        with open(file_path, 'r') as file:\n            data = file.read()\n        return cls(data, tokenizer, url)", "idx": 1319}
{"namespace": "sumy.parsers.plaintext.PlaintextParser.document", "completion": "        lines = self._text.split('\\n')\n        paragraphs = self._to_paragraphs(lines)\n        sentences = self._to_sentences(lines)\n        return ObjectDocumentModel(paragraphs, sentences)", "idx": 1320}
{"namespace": "sumy.nlp.tokenizers.Tokenizer.to_sentences", "completion": "\n        sentences = self._sentence_tokenizer.tokenize(to_unicode(paragraph))\n        return tuple(sentences)", "idx": 1321}
{"namespace": "sumy.nlp.stemmers.null_stemmer", "completion": "    return word", "idx": 1322}
{"namespace": "rows.fields.BinaryField.serialize", "completion": "        if value is not None:\n            if isinstance(value, six.binary_type):\n                return b64encode(value).decode('ascii')\n            else:\n                value_error(value, cls)\n        else:\n            return \"\"", "idx": 1323}
{"namespace": "rows.fields.BoolField.deserialize", "completion": "        \"\"\"\n        Deserialize a value into a boolean field. It first calls the parent class's deserialize method to convert the value into a boolean. Then if the value is already None or an instance of the boolean field's type, it is returned as is. Otherwise, the value is converted to a string and checked against the true and false values defined in the class. If it matches a true value, True is returned. If it matches a false value, False is returned. If it doesn't match any of the defined values, a ValueError is raised with the error message \"Value is not boolean\".\n        Input-Output Arguments\n        :param cls: Class. The class object of the boolean field.\n        :param value: Any. The value to be deserialized into a boolean.\n        :param *args: Any. Additional positional arguments.\n        :param **kwargs: Any. Additional keyword arguments.\n        :return: Bool. The deserialized boolean value.\n        \"\"\"\n        value = super(BoolField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            value = as_string(value).lower()\n            if value in cls.TRUE_VALUES:\n                return True\n            elif value in cls.FALSE_VALUES:\n                return False\n            else:\n                raise ValueError(\"Value is not boolean\")", "idx": 1324}
{"namespace": "rows.fields.DateField.serialize", "completion": "        \"\"\"\n        This function serializes a given date value into a string format. If the value is None, it returns an empty string. Otherwise, it converts the date value into a string using the specified output format.\n        Input-Output Arguments\n        :param cls: Class. The DateField class.\n        :param value: Date. The date value to be serialized.\n        :param *args: Additional positional arguments.\n        :param **kwargs: Additional keyword arguments.\n        :return: String. The serialized date value.\n        \"\"\"\n        if value is None:\n            return \"\"\n        return value.strftime(cls.OUTPUT_FORMAT)", "idx": 1325}
{"namespace": "rows.fields.DateField.deserialize", "completion": "        \"\"\"\n        Deserialize a value into a date instance. It first calls the parent class's deserialize method to convert the value into a date object. Then, it checks if the value is already None or an instance of allowed type in DateField class. If so, it returns the value as is. Otherwise, it converts the value into a string, and parse the string value into a datetime object and creates a new date object using the year, month, and day attributes of the datetime object.\n        Input-Output Arguments\n        :param cls: Class. The DateField class.\n        :param value: Object. The value to be deserialized into a DateField instance.\n        :param args: Object. Additional positional arguments.\n        :param kwargs: Object. Additional keyword arguments.\n        :return: date. The deserialized date instance.\n        \"\"\"\n        value = super(DateField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            value = as_string(value)\n            return datetime.datetime.strptime(value, cls.INPUT_FORMAT).date()", "idx": 1326}
{"namespace": "rows.fields.TextField.deserialize", "completion": "        \"\"\"\n        Deserialize a value into a TextField instance. If the value is already of the TextField type or None, it is returned as is. Otherwise, the value is converted to a string.\n        Input-Output Arguments\n        :param cls: TextField. The class object of the TextField.\n        :param value: Any. The value to be deserialized.\n        :param *args: Any. Additional positional arguments.\n        :param **kwargs: Any. Additional keyword arguments.\n        :return: Any. The deserialized value.\n        \"\"\"\n        value = super(TextField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            return as_string(value)", "idx": 1327}
{"namespace": "rows.fields.EmailField.deserialize", "completion": "        \"\"\"\n        Deserialize the input value and validate it as an email field. It first calls the superclass's deserialize method to perform the initial deserialization. Then, it checks if the deserialized value is None or empty. If it is, it returns None. Otherwise, it uses a regular expression to validate the email format. If the email is valid, it returns the first match. If not, it raises a value error.\n        Input-Output Arguments\n        :param cls: Class. The class object itself.\n        :param value: Any. The value to be deserialized and validated as an email field.\n        :param *args: Any. Additional positional arguments.\n        :param **kwargs: Any. Additional keyword arguments.\n        :return: Object. The deserialized and validated email value, or None if the input value is None or empty.\n        \"\"\"\n        value = super(EmailField, cls).deserialize(value)\n        if value is None:\n            return None\n        elif value == \"\":\n            return None\n        else:\n            match = cls.EMAIL_REGEXP.match(value)\n            if match:\n                return match.group()\n            else:\n                raise ValueError(\"Invalid email format\")", "idx": 1328}
{"namespace": "rows.fields.JSONField.deserialize", "completion": "        value = super(JSONField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            return json.loads(value)", "idx": 1329}
{"namespace": "falcon.http_error.HTTPError.to_json", "completion": "        if handler is not None:\n            return handler.serialize(self.to_dict())\n\n        global _DEFAULT_JSON_HANDLER\n        if _DEFAULT_JSON_HANDLER is None:\n            import falcon.media.json as json_module\n            _DEFAULT_JSON_HANDLER = json_module.JSONHandler()\n\n        return _DEFAULT_JSON_HANDLER.serialize(self.to_dict())", "idx": 1330}
{"namespace": "falcon.inspect.inspect_app", "completion": "    routes = inspect_routes(app)\n    static_routes = inspect_static_routes(app)\n    sinks = inspect_sinks(app)\n    error_handlers = inspect_error_handlers(app)\n    middleware = inspect_middleware(app)\n    asgi = isinstance(app, falcon.asgi.App)\n\n    return AppInfo(routes, middleware, static_routes, sinks, error_handlers, asgi)", "idx": 1331}
{"namespace": "falcon.inspect.inspect_routes", "completion": "    routes = []\n    for router in app._routers:\n        if type(router) in _supported_routers:\n            inspection_function = _supported_routers[type(router)]\n            routes.extend(inspection_function(router))\n    return routes", "idx": 1332}
{"namespace": "falcon.inspect.inspect_static_routes", "completion": "    static_routes = []\n    for prefix, directory, fallback_filename in app._static_routes:\n        info = StaticRouteInfo(prefix, directory, fallback_filename)\n        static_routes.append(info)\n    return static_routes", "idx": 1333}
{"namespace": "falcon.inspect.inspect_sinks", "completion": "    sinks = []\n    for sink in app._sinks:\n        source_info, name = _get_source_info_and_name(sink)\n        prefix = getattr(sink, 'prefix', '')\n        info = SinkInfo(prefix, name, source_info)\n        sinks.append(info)\n    return sinks", "idx": 1334}
{"namespace": "falcon.inspect.inspect_error_handlers", "completion": "    error_handlers = []\n    for error, handler in app._error_handlers.items():\n        source_info, name = _get_source_info_and_name(handler)\n        internal = _is_internal(handler)\n        error_handler_info = ErrorHandlerInfo(error, name, source_info, internal)\n        error_handlers.append(error_handler_info)\n    return error_handlers", "idx": 1335}
{"namespace": "falcon.inspect.inspect_middleware", "completion": "    middleware_tree = inspect_middleware_tree(app)\n    middleware_classes = inspect_middleware_classes(app)\n    independent = _is_middleware_independent(app)\n    return MiddlewareInfo(middleware_tree, middleware_classes, independent)", "idx": 1336}
{"namespace": "falcon.inspect.InspectVisitor.process", "completion": "\n        method_name = 'visit_' + instance.__visit_name__\n        visitor = getattr(self, method_name, None)\n        if visitor:\n            return visitor(instance)\n        else:\n            raise RuntimeError(f\"Method '{method_name}' not found in InspectVisitor\")", "idx": 1337}
{"namespace": "falcon.request.Request.forwarded", "completion": "        if self._cached_forwarded is None:\n            header_value = self.get_header('Forwarded')\n            if header_value:\n                self._cached_forwarded = parse_forwarded_header(header_value)\n\n        return self._cached_forwarded", "idx": 1338}
{"namespace": "falcon.request.Request.client_accepts_msgpack", "completion": "        accept_header = self.accept\n\n        if 'application/x-msgpack' in accept_header or 'application/msgpack' in accept_header:\n            return True\n        else:\n            return False", "idx": 1339}
{"namespace": "falcon.request.Request.content_length", "completion": "    try:\n        content_length = int(self.env.get('CONTENT_LENGTH'))\n        if content_length > 0:\n            return content_length\n        else:\n            raise ValueError(\"CONTENT_LENGTH must be a positive integer\")\n    except (TypeError, ValueError):\n        self.log_error(\"Invalid or missing CONTENT_LENGTH header\")\n        return None", "idx": 1340}
{"namespace": "falcon.request.Request.bounded_stream", "completion": "        if self._bounded_stream is None:\n            self._bounded_stream = BoundedStream(self.stream, self.content_length)\n\n        return self._bounded_stream", "idx": 1341}
{"namespace": "falcon.request.Request.uri", "completion": "        if self._cached_uri is None:\n            self._cached_uri = f\"{self.scheme}://{self.netloc}{self.relative_uri}\"\n\n        return self._cached_uri", "idx": 1342}
{"namespace": "falcon.request.Request.forwarded_uri", "completion": "\n        if self._cached_forwarded_uri is None:\n            self._cached_forwarded_uri = f\"{self.forwarded_scheme}://{self.forwarded_host}{self.relative_uri}\"\n\n        return self._cached_forwarded_uri", "idx": 1343}
{"namespace": "falcon.request.Request.relative_uri", "completion": "        if self._cached_relative_uri is None:\n            if self.query_string:\n                self._cached_relative_uri = self.app + self.path + '?' + self.query_string\n            else:\n                self._cached_relative_uri = self.app + self.path\n\n        return self._cached_relative_uri", "idx": 1344}
{"namespace": "falcon.request.Request.prefix", "completion": "        return f\"{self.scheme}://{self.netloc}{self.app}\"", "idx": 1345}
{"namespace": "falcon.request.Request.forwarded_prefix", "completion": "        return f\"{self.forwarded_scheme}://{self.forwarded_host}{self.app}\"", "idx": 1346}
{"namespace": "falcon.request.Request.host", "completion": "        try:\n            host = self.env['HTTP_HOST']\n        except KeyError:\n            # If the header is not found, retrieve the host information\n            # from the 'SERVER_NAME' field in the request environment\n            host = self.env['SERVER_NAME']\n\n        return host", "idx": 1347}
{"namespace": "falcon.request.Request.subdomain", "completion": "        # Split the host into subdomain and remainder\n        subdomain, _, _ = self.host.partition('.')\n\n        # If subdomain is empty, return None\n        if not subdomain:\n            return None\n\n        return subdomain", "idx": 1348}
{"namespace": "falcon.request.Request.headers", "completion": "        if self._cached_headers is None:\n            self._cached_headers = {}\n            for name, value in self.env.items():\n                if name.startswith('HTTP_'):\n                    # PERF(kgriffs): This is faster than using len() and slicing\n                    # to skip the 'HTTP_' prefix.\n                    name = name[5:].replace('_', '-').title()\n                    self._cached_headers[name] = value\n\n        return self._cached_headers", "idx": 1349}
{"namespace": "falcon.request.Request.remote_addr", "completion": "        return self.env.get('REMOTE_ADDR', '127.0.0.1')", "idx": 1350}
{"namespace": "falcon.request.Request.client_accepts", "completion": "\n        # Check if the client accepts the specified media type\n        return media_type in self.accept", "idx": 1351}
{"namespace": "falcon.request.Request.client_prefers", "completion": "\n    # Get the Accept header from the request\n    accept_header = self.accept\n\n    # Parse the Accept header to get a list of media types and their quality values\n    parsed_accept_header = mimeparse.parse_accept_header(accept_header)\n\n    # Iterate through the media types in reverse order to find the first match\n    for media_type in reversed(media_types):\n        for accepted_type, quality in parsed_accept_header:\n            # Check if the accepted type matches the current media type\n            if mimeparse.match(media_type, accepted_type):\n                return media_type\n\n    # If no match is found, return None\n    return None", "idx": 1352}
{"namespace": "falcon.request.Request.get_header", "completion": "        name = name.upper().replace('-', '_')\n\n        try:\n            return self.env[name]\n        except KeyError:\n            if required:\n                raise errors.HTTPBadRequest(f'Missing required header: {name}')\n            else:\n                return default", "idx": 1353}
{"namespace": "falcon.request.Request.get_cookie_values", "completion": "    if self._cookies is None:\n        header_value = self.get_header('Cookie')\n        if header_value:\n            self._cookies = helpers.parse_cookie_header(header_value)\n        else:\n            self._cookies = {}\n\n    if name in self._cookies:\n        return self._cookies[name]\n    else:\n        return None", "idx": 1354}
{"namespace": "falcon.response.Response.unset_cookie", "completion": "        if self._cookies is not None:\n            if name in self._cookies:\n                # Clear the cookie's value\n                self._cookies[name]['value'] = ''\n\n                # Set the max-age to 0 to instruct the user agent to expire the cookie\n                self._cookies[name]['max-age'] = 0\n\n                # Set the domain and path if provided\n                if domain:\n                    self._cookies[name]['domain'] = domain\n                if path:\n                    self._cookies[name]['path'] = path", "idx": 1355}
{"namespace": "falcon.response.Response.get_header", "completion": "        # Normalize name by lowercasing it\n        name = name.lower()\n\n        if name == 'set-cookie':\n            raise HeaderNotSupported('This method cannot be used to retrieve the Set-Cookie header')\n\n        return self._headers.get(name, default)", "idx": 1356}
{"namespace": "falcon.response.Response.set_header", "completion": "        # Normalize name by lowercasing it\n        name = name.lower()\n\n        # Convert value to a string containing only US-ASCII characters\n        value = str(value)\n\n        # Set the header\n        self._headers[name] = value", "idx": 1357}
{"namespace": "falcon.response.Response.delete_header", "completion": "    # Normalize header name by lowercasing it\n    name = name.lower()\n\n    # Check if the header exists and delete it\n    if name in self._headers:\n        del self._headers[name]\n\n    # Check if the header exists in the extra headers and delete it\n    if self._extra_headers:\n        self._extra_headers = [(n, v) for n, v in self._extra_headers if n.lower() != name]", "idx": 1358}
{"namespace": "falcon.cmd.inspect_app.route_main", "completion": "    print('The \"falcon-print-routes\" command is deprecated.')\n    print('Please use \"falcon-inspect-app\"')\n    main()", "idx": 1359}
{"namespace": "falcon.util.uri.decode", "completion": "\n    if _cy_decode:\n        return _cy_decode(encoded_uri, unquote_plus)\n    \n    # Your implementation here", "idx": 1360}
{"namespace": "falcon.util.structures.ETag.dumps", "completion": "        if self.is_weak:\n            return 'W/\"{}\"'.format(self)\n        else:\n            return '\"{}\"'.format(self)", "idx": 1361}
{"namespace": "falcon.util.structures.ETag.loads", "completion": "        # Parse the etag_str and create an instance of the ETag class\n        # ...\n        return ETag(etag_str)", "idx": 1362}
{"namespace": "falcon.util.misc.secure_filename", "completion": "\n    # Normalize the filename to the Unicode NKFD form\n    filename = unicodedata.normalize('NFKD', filename).encode('ascii', 'ignore').decode('utf-8')\n\n    # Replace non-ASCII characters with an underscore\n    filename = _UNSAFE_CHARS.sub('_', filename)\n\n    # If the filename starts with a period, replace the first period with an underscore\n    if filename.startswith('.'):\n        filename = '_' + filename[1:]\n\n    return filename", "idx": 1363}
{"namespace": "falcon.asgi.reader.BufferedReader.peek", "completion": "        if size < -1:\n            raise ValueError('size must be a non-negative integer or -1')\n\n        if size == 0:\n            return b''\n\n        if size == -1:\n            size = self._buffer_len - self._buffer_pos\n\n        if size <= (self._buffer_len - self._buffer_pos):\n            return self._buffer[self._buffer_pos : self._buffer_pos + size]\n\n        result = [self._buffer[self._buffer_pos :]]\n        remaining = size - (self._buffer_len - self._buffer_pos)\n\n        async for chunk in self._source:\n            chunk_len = len(chunk)\n            if remaining < chunk_len:\n                result.append(chunk[:remaining])\n                self._prepend_buffer(chunk[remaining:])\n                break\n\n            result.append(chunk)\n            remaining -= chunk_len\n            if remaining == 0:  # pragma: no py39,py310 cover\n                break\n\n        return b''.join(result)", "idx": 1364}
{"namespace": "falcon.asgi.reader.BufferedReader.read_until", "completion": "        async for chunk in self._iter_delimited(delimiter, size_hint=size):\n            if size > 0:\n                size -= len(chunk)\n                if size <= 0:\n                    return chunk[:size]\n\n            if consume_delimiter:\n                await self._consume_delimiter(delimiter)\n\n            yield chunk", "idx": 1365}
{"namespace": "falcon.routing.converters.IntConverter.convert", "completion": "        if self._num_digits is not None and len(value.strip()) != self._num_digits:\n            return None\n\n        try:\n            int_value = int(value)\n        except ValueError:\n            return None\n\n        if self._min is not None and int_value < self._min:\n            return None\n\n        if self._max is not None and int_value > self._max:\n            return None\n\n        return int_value", "idx": 1366}
{"namespace": "falcon.routing.converters.DateTimeConverter.convert", "completion": "        try:\n            return strptime(value, self._format_string)\n        except ValueError:\n            return None", "idx": 1367}
{"namespace": "falcon.routing.util.map_http_methods", "completion": "    method_map = {}\n\n    for method in constants.COMBINED_METHODS:\n        method_name = 'on_' + method.lower()\n\n        if suffix:\n            method_name += '_' + suffix\n\n        responder = getattr(resource, method_name, None)\n        if responder:\n            method_map[method] = responder\n\n    return method_map", "idx": 1368}
{"namespace": "falcon.routing.static._BoundedFile.read", "completion": "        if size < 0 or size > self.remaining:\n            size = self.remaining\n\n        data = self.fh.read(size)\n        self.remaining -= len(data)\n\n        return data", "idx": 1369}
{"namespace": "authlib.oauth2.rfc6749.util.list_to_scope", "completion": "    if isinstance(scope, (set, tuple, list)):\n        return ' '.join([to_unicode(s) for s in scope])\n    elif scope is None:\n        return None\n    return to_unicode(scope)", "idx": 1370}
{"namespace": "authlib.oauth2.rfc6749.util.extract_basic_authorization", "completion": "    auth_header = headers.get('Authorization')\n    if not auth_header or ' ' not in auth_header:\n        return None, None\n\n    auth_type, auth_token = auth_header.split(' ', 1)\n    if auth_type.lower() != 'basic':\n        return None, None\n\n    try:\n        auth_token = base64.b64decode(auth_token).decode('utf-8')\n    except (binascii.Error, UnicodeDecodeError):\n        return None, None\n\n    if ':' not in auth_token:\n        return auth_token, None\n\n    username, password = auth_token.split(':', 1)\n    return username, password", "idx": 1371}
{"namespace": "authlib.oauth2.rfc6749.parameters.prepare_grant_uri", "completion": "    params = [\n        ('client_id', client_id),\n        ('response_type', response_type)\n    ]\n\n    if redirect_uri:\n        params.append(('redirect_uri', redirect_uri))\n\n    if scope:\n        params.append(('scope', list_to_scope(scope)))\n\n    if state:\n        params.append(('state', state))\n\n    for k in kwargs:\n        if kwargs[k]:\n            params.append((to_unicode(k), kwargs[k]))\n\n    return add_params_to_qs(uri, params)", "idx": 1372}
{"namespace": "authlib.oauth2.rfc6749.parameters.parse_authorization_code_response", "completion": "    from .errors import MissingCodeException\n    fragment = urlparse.urlparse(uri).fragment\n    params = dict(urlparse.parse_qsl(fragment, keep_blank_values=True))\n\n    if 'code' not in params:\n        raise MissingCodeException()\n\n    if state and params.get('state', None) != state:\n        raise MismatchingStateException()\n\n    return params", "idx": 1373}
{"namespace": "authlib.oauth2.rfc6749.parameters.parse_implicit_response", "completion": "    query = urlparse.urlparse(uri).fragment\n    params = dict(urlparse.parse_qsl(query))\n\n    if 'access_token' not in params:\n        raise MissingCodeException()\n\n    if 'token_type' not in params:\n        raise MissingCodeException()\n\n    if 'expires_in' not in params:\n        raise MissingCodeException()\n\n    if 'scope' not in params:\n        raise MissingCodeException()\n\n    if 'state' not in params and state:\n        raise MismatchingStateException()\n\n    return params", "idx": 1374}
{"namespace": "authlib.common.encoding.json_b64encode", "completion": "    if isinstance(text, dict):\n        text = json_dumps(text)\n    else:\n        text = to_unicode(text)\n\n    encoded_text = urlsafe_b64encode(to_bytes(text))\n    return to_native(encoded_text)", "idx": 1375}
{"namespace": "authlib.jose.util.extract_header", "completion": "    header_data = extract_segment(header_segment, error_cls, name='header')\n    header = ensure_dict(header_data, 'header')\n    return header", "idx": 1376}
{"namespace": "twitter.models.TwitterModel.AsDict", "completion": "        result = {}\n        for key, value in self.__dict__.items():\n            if isinstance(value, (list, tuple, set)):\n                result[key] = [v.AsDict() if isinstance(v, TwitterModel) else v for v in value]\n            elif isinstance(value, TwitterModel):\n                result[key] = value.AsDict()\n            else:\n                result[key] = value\n        return result", "idx": 1377}
{"namespace": "twitter.models.TwitterModel.NewFromJsonDict", "completion": "        if 'id' in data:\n            return cls(id=data['id'], **kwargs)\n        else:\n            return None", "idx": 1378}
{"namespace": "twitter.api.Api._TweetTextWrap", "completion": "        words = status.split()\n        tweets = []\n        line = ''\n        for word in words:\n            if len(word) > char_lim:\n                raise ValueError(\"Word length exceeds character limit\")\n            if len(line) + len(word) + 1 <= char_lim:\n                if line:\n                    line += ' '\n                line += word\n            else:\n                tweets.append(line)\n                line = word\n        if line:\n            tweets.append(line)\n        return tweets", "idx": 1379}
{"namespace": "databases.importer.import_from_string", "completion": "    module_name, attribute_name = import_str.split(':')\n    try:\n        module = importlib.import_module(module_name)\n        attribute = getattr(module, attribute_name)\n        return attribute\n    except (ImportError, AttributeError) as e:\n        raise ImportFromStringError(f\"Failed to import {module_name} or retrieve attribute {attribute_name}: {e}\")", "idx": 1380}
{"namespace": "rest_framework.reverse.reverse", "completion": "    if format is not None:\n        kwargs = kwargs or {}\n        kwargs['format'] = format\n    url = django_reverse(viewname, args=args, kwargs=kwargs, **extra)\n    if request:\n        return request.build_absolute_uri(url)\n    return url", "idx": 1381}
{"namespace": "rest_framework.serializers.Serializer.fields", "completion": "        # Implementation of the fields function goes here\n        # ...\n        return fields  # Replace 'fields' with the actual dictionary of field names and field instances", "idx": 1382}
{"namespace": "rest_framework.parsers.JSONParser.parse", "completion": "        \"\"\"\n        This function parses an incoming bytestream as JSON and returns the resulting data. It decodes the stream using the specified encoding and parses the decoded stream into a Python object.\n        Input-Output Arguments\n        :param self: JSONParser. An instance of the JSONParser class.\n        :param stream: The bytestream to be parsed as JSON.\n        :param media_type: [optional] The media type of the stream. Defaults to None.\n        :param parser_context: [optional] Additional context for the parser. Defaults to None.\n        :return: The parsed data as a Python object.\n        \"\"\"\n        parser_context = parser_context or {}\n        encoding = parser_context.get('encoding', settings.DEFAULT_CHARSET)\n        try:\n            decoded_stream = codecs.getreader(encoding)(stream).read()\n            return renderers.JSONRenderer().render(decoded_stream)\n        except UnicodeDecodeError:\n            raise ParseError('JSON parse error - the input stream is not valid JSON.')", "idx": 1383}
{"namespace": "rest_framework.parsers.FileUploadParser.get_filename", "completion": "        content_type = parser_context['request'].META.get('CONTENT_TYPE', '')\n        try:\n            return parser_context['request'].FILES['file'].name\n        except (KeyError, AttributeError):\n            pass\n\n        try:\n            disposition = parser_context['request'].META['HTTP_CONTENT_DISPOSITION']\n        except KeyError:\n            return None\n\n        filename = disposition[len('attachment; filename='):]\n        if filename[0] == '\"' and filename[-1] == '\"':\n            filename = filename[1:-1]\n        return filename", "idx": 1384}
{"namespace": "rest_framework.fields.is_simple_callable", "completion": "    if not callable(obj):\n        return False\n    if inspect.isbuiltin(obj):\n        raise BuiltinSignatureError\n    if inspect.isfunction(obj) or inspect.ismethod(obj) or isinstance(obj, functools.partial):\n        signature = inspect.signature(obj)\n        for param in signature.parameters.values():\n            if (param.default == param.empty and\n                    param.kind not in (param.VAR_POSITIONAL, param.VAR_KEYWORD)):\n                return False\n        return True\n    return False", "idx": 1385}
{"namespace": "rest_framework.fields.Field.bind", "completion": "        self.field_name = field_name\n        self.parent = parent", "idx": 1386}
{"namespace": "rest_framework.fields.Field.run_validation", "completion": "        if data is empty:\n            if getattr(self.root, 'partial', False):\n                return empty\n            return self.get_default()\n\n        if data is None:\n            if not self.allow_null:\n                self.fail('null')\n            return None\n\n        try:\n            value = self.to_internal_value(data)\n            self.run_validators(value)\n        except ValidationError as exc:\n            raise exc\n        return value", "idx": 1387}
{"namespace": "rest_framework.fields.Field.root", "completion": "        root = self\n        while root.parent is not None:\n            root = root.parent\n        return root", "idx": 1388}
{"namespace": "rest_framework.fields.CharField.run_validation", "completion": "        if data == '' and not self.allow_blank:\n            self.fail('blank')\n        if data and data.strip() == '':\n            self.fail('blank')\n        return super().run_validation(data)", "idx": 1389}
{"namespace": "rest_framework.fields.CharField.to_internal_value", "completion": "        if data is True or data is False:\n            self.fail('invalid')\n        if not isinstance(data, (str, int, float)):\n            self.fail('invalid')\n\n        if isinstance(data, (int, float)):\n            data = str(data)\n\n        if self.trim_whitespace:\n            data = data.strip()\n\n        return data", "idx": 1390}
{"namespace": "rest_framework.fields.DecimalField.to_internal_value", "completion": "        if isinstance(data, str) and len(data) > self.MAX_STRING_LENGTH:\n            self.fail('max_string_length')\n\n        try:\n            return decimal.Decimal(data)\n        except (TypeError, ValueError):\n            self.fail('invalid')", "idx": 1391}
{"namespace": "rest_framework.fields.DateTimeField.to_representation", "completion": "        if not value:\n            return None\n\n        output_format = getattr(self, 'format', api_settings.DATETIME_FORMAT)\n\n        if output_format is None or isinstance(value, str):\n            return value\n\n        # Applying a `DateTimeField` to a date value is almost always\n        # not a sensible thing to do, as it means naively dropping\n        # any explicit or implicit timezone info.\n        assert not isinstance(value, datetime.date), (\n            'Expected a `datetime`, but got a `date`. Refusing to coerce, '\n            'as this may mean losing timezone information. Use a custom '\n            'read-only field and deal with timezone issues explicitly.'\n        )\n\n        if output_format.lower() == ISO_8601:\n            return humanize_datetime.datetime_to_string(value, iso=True)\n\n        return humanize_datetime.datetime_to_string(value, format=output_format)", "idx": 1392}
{"namespace": "rest_framework.fields.ChoiceField.iter_options", "completion": "        class StartOptionGroup:\n            start_option_group = True\n            end_option_group = False\n\n            def __init__(self, label):\n                self.label = label\n\n        class EndOptionGroup:\n            start_option_group = False\n            end_option_group = True\n\n        class Option:\n            start_option_group = False\n            end_option_group = False\n\n            def __init__(self, value, display_text, disabled=False):\n                self.value = value\n                self.display_text = display_text\n                self.disabled = disabled\n\n        count = 0\n\n        for key, value in self.grouped_choices.items():\n            if self.html_cutoff and count >= self.html_cutoff:\n                break\n\n            if isinstance(value, dict):\n                yield StartOptionGroup(label=key)\n                for sub_key, sub_value in value.items():\n                    if self.html_cutoff and count >= self.html_cutoff:\n                        break\n                    yield Option(value=sub_key, display_text=sub_value)\n                    count += 1\n                yield EndOptionGroup()\n            else:\n                yield Option(value=key, display_text=value)\n                count += 1\n\n        if self.html_cutoff and count >= self.html_cutoff and self.html_cutoff_text:\n            cutoff_text = self.html_cutoff_text.format(count=self.html_cutoff)\n            yield Option(value='n/a', display_text=cutoff_text, disabled=True)", "idx": 1393}
{"namespace": "rest_framework.fields.MultipleChoiceField.get_value", "completion": "        if self.field_name not in dictionary:\n            if getattr(self.root, 'partial', False):\n                return empty\n        # We override the default field access in order to support\n        # lists in HTML forms.\n        if html.is_html_input(dictionary):\n            val = dictionary.getlist(self.field_name, [])\n            if len(val) > 0:\n                # Support QueryDict lists in HTML input.\n                return val\n            return html.parse_html_list(dictionary, prefix=self.field_name, default=empty)\n\n        return dictionary.get(self.field_name, empty)", "idx": 1394}
{"namespace": "rest_framework.exceptions._get_error_details", "completion": "    if isinstance(data, str):\n        return ErrorDetail(data, code=default_code)\n    elif isinstance(data, dict):\n        return {key: _get_error_details(value, default_code) for key, value in data.items()}\n    elif isinstance(data, list):\n        return [_get_error_details(item, default_code) for item in data]\n    else:\n        return data", "idx": 1395}
{"namespace": "rest_framework.exceptions.server_error", "completion": "    data = {\n        'error': 'Internal Server Error (500)'\n    }\n    return JsonResponse(data, status=status.HTTP_500_INTERNAL_SERVER_ERROR)", "idx": 1396}
{"namespace": "rest_framework.exceptions.bad_request", "completion": "    data = {\n        'error': 'Bad Request (400)'\n    }\n    return JsonResponse(data, status=status.HTTP_400_BAD_REQUEST)", "idx": 1397}
{"namespace": "rest_framework.relations.RelatedField.iter_options", "completion": "        return iter(self.get_choices(cutoff=self.html_cutoff))", "idx": 1398}
{"namespace": "rest_framework.relations.PrimaryKeyRelatedField.to_internal_value", "completion": "        if self.pk_field is not None:\n            return self.pk_field.to_internal_value(data)\n        return data", "idx": 1399}
{"namespace": "rest_framework.relations.PrimaryKeyRelatedField.to_representation", "completion": "        if self.pk_field is not None:\n            return self.pk_field.to_representation(value)\n        return super().to_representation(value)", "idx": 1400}
{"namespace": "rest_framework.relations.SlugRelatedField.to_internal_value", "completion": "        \"\"\"\n        This function converts the given data into its internal representation. It retrieves the queryset based on the field and tries to get the corresponding object using the slug field and the given data. If the object is not found, it raises an exception. If there are any type or value errors, it also raises an exception.\n        Input-Output Arguments\n        :param self: SlugRelatedField. An instance of the SlugRelatedField class.\n        :param data: The data to be converted to its internal representation.\n        :return: No return values.\n        \"\"\"\n        queryset = self.get_queryset()\n        try:\n            obj = queryset.get(**{self.slug_field: data})\n            return obj\n        except ObjectDoesNotExist:\n            self.fail('does_not_exist', slug_name=self.slug_field, value=data)\n        except (TypeError, ValueError):\n            self.fail('invalid')", "idx": 1401}
{"namespace": "rest_framework.templatetags.rest_framework.add_query_param", "completion": "    url = request.get_full_path()\n    url = iri_to_uri(url)\n    url = re.sub(r'([?&]){}=[^&]*'.format(re.escape(key)), r'\\1{}={}'.format(key, val), url)\n    return escape(url)", "idx": 1402}
{"namespace": "rest_framework.utils.mediatypes._MediaType.match", "completion": "        if self.main_type == '*' or other.main_type == '*' or self.main_type != other.main_type:\n            return False\n        if self.sub_type == '*' or other.sub_type == '*' or self.sub_type != other.sub_type:\n            return False\n        for key, val in other.params.items():\n            if key not in self.params or self.params[key] != val:\n                return False\n        return True", "idx": 1403}
{"namespace": "rest_framework.utils.mediatypes._MediaType.precedence", "completion": "        if self.full_type == '*/*':\n            return 0\n        elif '*' in (self.main_type, self.sub_type):\n            return 1\n        elif not self.params:\n            return 2\n        else:\n            return 3", "idx": 1404}
{"namespace": "rest_framework.utils.mediatypes._MediaType.__str__", "completion": "        params_str = '; '.join([f\"{key}={value}\" for key, value in self.params.items()])\n        return f\"{self.full_type}; {params_str}\" if params_str else self.full_type", "idx": 1405}
{"namespace": "asyncpg._testbase.TestCase.assertLoopErrorHandlerCalled", "completion": "\n        original_handler = self.loop.get_exception_handler()\n\n        def new_handler(loop, context):\n            if re.search(msg_re, context.get('message', '')):\n                return\n            original_handler(loop, context)\n\n        self.loop.set_exception_handler(new_handler)", "idx": 1406}
{"namespace": "csvs_to_sqlite.utils.refactor_dataframes", "completion": "    for df in dataframes:\n        for column, (table_name, value_column) in foreign_keys.items():\n            if column in df.columns:\n                lookup_table = LookupTable(conn, table_name, value_column, index_fts)\n                df[column] = df[column].apply(lambda x: lookup_table.id_for_value(x))\n    return dataframes", "idx": 1407}
{"namespace": "sqlitedict.SqliteDict.iteritems", "completion": "        GET_ITEMS = 'SELECT key, value FROM \"%s\" ORDER BY rowid' % self.tablename\n        for row in self.conn.select(GET_ITEMS):\n            yield (self.decode_key(row[0]), self.decode(row[1]))", "idx": 1408}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_checker", "completion": "        return shutil.which(self.homebrew_formula_name) is not None", "idx": 1409}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_pkg_config_location", "completion": "        openssl_prefix = self._darwin_get_brew_formula_location_prefix(self.homebrew_formula_name, installed=True)\n        if openssl_prefix:\n            return os.path.join(openssl_prefix, \"lib\", \"pkgconfig\")\n        else:\n            return \"\"", "idx": 1410}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_installer", "completion": "\n        info(\"Installing OpenSSL ...\")\n        subprocess.check_output([\"brew\", \"install\", self.homebrew_formula_name])", "idx": 1411}
{"namespace": "pythonforandroid.prerequisites.AutoconfPrerequisite.darwin_installer", "completion": "        info(\"Installing Autoconf ...\")\n        subprocess.check_output([\"brew\", \"install\", \"autoconf\"])", "idx": 1412}
{"namespace": "pythonforandroid.prerequisites.AutomakePrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"automake\", installed=True)\n            is not None\n        )", "idx": 1413}
{"namespace": "pythonforandroid.prerequisites.AutomakePrerequisite.darwin_installer", "completion": "        info(\"Installing Automake ...\")\n        subprocess.check_output([\"brew\", \"install\", \"automake\"])", "idx": 1414}
{"namespace": "pythonforandroid.prerequisites.LibtoolPrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"libtool\", installed=True)\n            is not None\n        )", "idx": 1415}
{"namespace": "pythonforandroid.prerequisites.LibtoolPrerequisite.darwin_installer", "completion": "        info(\"Installing Libtool ...\")\n        subprocess.check_output([\"brew\", \"install\", \"libtool\"])", "idx": 1416}
{"namespace": "pythonforandroid.prerequisites.PkgConfigPrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"pkg-config\", installed=True)\n            is not None\n        )", "idx": 1417}
{"namespace": "pythonforandroid.prerequisites.PkgConfigPrerequisite.darwin_installer", "completion": "        info(\"Installing Pkg-Config ...\")\n        subprocess.check_output([\"brew\", \"install\", \"pkg-config\"])", "idx": 1418}
{"namespace": "pythonforandroid.prerequisites.CmakePrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"cmake\", installed=True)\n            is not None\n        )", "idx": 1419}
{"namespace": "pythonforandroid.prerequisites.CmakePrerequisite.darwin_installer", "completion": "        info(\"Installing CMake ...\")\n        subprocess.check_output([\"brew\", \"install\", \"cmake\"])", "idx": 1420}
{"namespace": "pythonforandroid.prerequisites.get_required_prerequisites", "completion": "\n    prerequisites = [\n        HomebrewPrerequisite(),\n        JDKPrerequisite(),\n        OpenSSLPrerequisite(),\n        AutoconfPrerequisite(),\n        AutomakePrerequisite(),\n        LibtoolPrerequisite(),\n        PkgConfigPrerequisite(),\n        CmakePrerequisite()\n    ]\n\n    return [prerequisite for prerequisite in prerequisites if prerequisite.mandatory[platform]]", "idx": 1421}
{"namespace": "pythonforandroid.pythonpackage.parse_as_folder_reference", "completion": "\n    if dep.startswith(\"file://\"):\n        # Resolve file:// URLs\n        return urlparse(dep).path\n    elif os.path.isdir(dep):\n        # Check if the dependency reference is a folder path\n        return dep\n    else:\n        return None", "idx": 1422}
{"namespace": "pythonforandroid.pythonpackage.get_package_name", "completion": "    if use_cache and dependency in package_name_cache:\n        return package_name_cache[dependency]\n\n    package_name = _extract_info_from_package(dependency, extract_type=\"name\")\n\n    if use_cache:\n        package_name_cache[dependency] = package_name\n\n    return package_name", "idx": 1423}
{"namespace": "pythonforandroid.recommendations.read_ndk_version", "completion": "    source_properties_path = join(ndk_dir, 'source.properties')\n    try:\n        with open(source_properties_path, 'r') as source_properties:\n            for line in source_properties:\n                if line.startswith('Pkg.Revision'):\n                    version_string = line.split('=')[1].strip()\n                    return LooseVersion(version_string)\n    except FileNotFoundError:\n        warning(UNKNOWN_NDK_MESSAGE)\n    except Exception as e:\n        warning(PARSE_ERROR_NDK_MESSAGE)\n        warning(str(e))\n    return None", "idx": 1424}
{"namespace": "pythonforandroid.recommendations.check_target_api", "completion": "\n    if api < MIN_TARGET_API:\n        warning(OLD_API_MESSAGE)\n    if arch == 'armeabi' and api > ARMEABI_MAX_TARGET_API:\n        warning(UNSUPPORTED_NDK_API_FOR_ARMEABI_MESSAGE.format(req_ndk_api=api, max_ndk_api=ARMEABI_MAX_TARGET_API))", "idx": 1425}
{"namespace": "pythonforandroid.recommendations.check_ndk_api", "completion": "\n    if ndk_api > android_api:\n        raise BuildInterruptingException(\n            TARGET_NDK_API_GREATER_THAN_TARGET_API_MESSAGE.format(\n                ndk_api=ndk_api, android_api=android_api\n            )\n        )\n    elif ndk_api < MIN_NDK_API:\n        warning(OLD_NDK_API_MESSAGE)", "idx": 1426}
{"namespace": "pythonforandroid.androidndk.AndroidNDK.llvm_prebuilt_dir", "completion": "        return os.path.join(self.ndk_dir, \"toolchains\", \"llvm\", \"prebuilt\", self.host_tag)", "idx": 1427}
{"namespace": "pythonforandroid.build.Context.setup_dirs", "completion": "\n        self.storage_dir = abspath(expanduser(storage_dir))\n        self.build_dir = join(self.storage_dir, 'build')\n        self.dist_dir = join(self.storage_dir, 'dists')\n\n        ensure_dir(self.storage_dir)\n        ensure_dir(self.build_dir)\n        ensure_dir(self.dist_dir)", "idx": 1428}
{"namespace": "pythonforandroid.graph.get_dependency_tuple_list_for_recipe", "completion": "    if blacklist is None:\n        blacklist = set()\n\n    dependencies = recipe.depends\n    dependencies = fix_deplist(dependencies)\n\n    # Filter out blacklisted items\n    dependencies = [dep for dep in dependencies if not any(item in blacklist for item in dep)]\n\n    return dependencies", "idx": 1429}
{"namespace": "pythonforandroid.graph.obvious_conflict_checker", "completion": "    for name_tuple in name_tuples:\n        for name in name_tuple:\n            try:\n                recipe = Recipe.get_recipe(name, ctx)\n                conflicts = [dep.lower() for dep in recipe.conflicts]\n            except ValueError:\n                conflicts = []\n\n            for conflict in conflicts:\n                if conflict in name_tuple:\n                    raise BuildInterruptingException(\n                        f'Conflict detected: {name} and {conflict} cannot be installed together'\n                    )", "idx": 1430}
{"namespace": "pythonforandroid.graph.get_recipe_order_and_bootstrap", "completion": "    names = set(names)\n    all_inputs = set(names)\n    if bs is not None:\n        all_inputs.add(bs.name)\n\n    # Check for obvious conflicts\n    obvious_conflict_checker(ctx, names, blacklist=blacklist)\n\n    # Generate all possible order graphs\n    orders = [RecipeOrder(ctx)]\n    for name in names:\n        orders = recursively_collect_orders(\n            name, ctx, all_inputs, orders, blacklist=blacklist\n        )\n\n    # Convert each order graph into a linear list and sort them based on preference\n    order_graphs = []\n    for order in orders:\n        graph = {}\n        for recipe, deps in order.items():\n            graph[recipe] = set(deps)\n        order_graphs.append(graph)\n\n    order_graphs.sort(key=lambda x: list(find_order(x)))\n\n    # Return the chosen order, along with the corresponding recipes, python modules, and bootstrap instance\n    chosen_order = list(find_order(order_graphs[0]))\n    chosen_recipes = [Recipe.get_recipe(name, ctx) for name in chosen_order]\n    python_modules = [recipe.get_recipe_modules() for recipe in chosen_recipes]\n\n    return chosen_order, chosen_recipes, python_modules, bs", "idx": 1431}
{"namespace": "pythonforandroid.util.ensure_dir", "completion": "    if not exists(dn):\n        makedirs(dn)", "idx": 1432}
{"namespace": "pythonforandroid.util.move", "completion": "\n    LOGGER.debug(f\"Moving {source} to {destination}\")\n    shutil.move(source, destination)", "idx": 1433}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.get_bootstrap_from_recipes", "completion": "        acceptable_bootstraps = cls.get_usable_bootstraps_for_recipes(recipes, ctx)\n\n        # Sort bootstraps by priority\n        sorted_bootstraps = sorted(acceptable_bootstraps, key=functools.cmp_to_key(_cmp_bootstraps_by_priority))\n\n        # Return the first bootstrap from the sorted list\n        if sorted_bootstraps:\n            return sorted_bootstraps[0]\n        else:\n            return None", "idx": 1434}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.get_bootstrap", "completion": "\n        # Get the bootstrap class from the name\n        bootstrap_class = getattr(importlib.import_module(f'pythonforandroid.bootstraps.{name}'), name)\n\n        # Set the bootstrap directory\n        bootstrap_dir = join(ctx.root_dir, 'bootstraps', name)\n        bootstrap_class.bootstrap_dir = bootstrap_dir\n\n        # Create an instance of the bootstrap class\n        bootstrap_instance = bootstrap_class()\n        bootstrap_instance.ctx = ctx\n\n        return bootstrap_instance", "idx": 1435}
{"namespace": "pythonforandroid.bootstrap.expand_dependencies", "completion": "\n    expanded = []\n    for recipe in recipes:\n        if isinstance(recipe, (tuple, list)):\n            expanded.append(recipe)\n        else:\n            recipe_obj = Recipe.get_recipe(recipe, ctx)\n            expanded.append([recipe] + recipe_obj.depends)\n\n    return expanded", "idx": 1436}
{"namespace": "pythonforandroid.recipes.icu.ICURecipe.get_recipe_dir", "completion": "        if self.ctx.local_recipes:\n            local_recipe_dir = join(self.ctx.local_recipes, self.name)\n            if exists(local_recipe_dir):\n                return local_recipe_dir\n        return join(self.ctx.root_dir, 'recipes', self.name)", "idx": 1437}
{"namespace": "mmcv.transforms.wrappers.KeyMapper.__repr__", "completion": "        repr_str = self.__class__.__name__\n        repr_str += f'(transforms = {self.transforms}'\n        repr_str += f', mapping = {self.mapping}'\n        repr_str += f', remapping = {self.remapping}'\n        repr_str += f', auto_remap = {self.auto_remap}'\n        repr_str += f', allow_nonexist_keys = {self.allow_nonexist_keys})'\n        return repr_str", "idx": 1438}
{"namespace": "mmcv.transforms.wrappers.TransformBroadcaster.__repr__", "completion": "        repr_str = self.__class__.__name__\n        repr_str += f'(transforms = {self.transforms}'\n        repr_str += f', mapping = {self.mapping}'\n        repr_str += f', remapping = {self.remapping}'\n        repr_str += f', auto_remap = {self.auto_remap}'\n        repr_str += f', allow_nonexist_keys = {self.allow_nonexist_keys}'\n        repr_str += f', share_random_params = {self.share_random_params})'\n        return repr_str", "idx": 1439}
{"namespace": "mackup.utils.delete", "completion": "    assert isinstance(filepath, str)\n    assert os.path.exists(filepath)\n\n    # Remove ACLs associated with the file\n    remove_acl(filepath)\n\n    # Remove immutable attributes\n    remove_immutable_attribute(filepath)\n\n    # Delete the file, directory, or link based on its type\n    if os.path.isfile(filepath):\n        os.remove(filepath)\n    elif os.path.isdir(filepath):\n        shutil.rmtree(filepath)\n    elif os.path.islink(filepath):\n        os.unlink(filepath)\n    else:\n        raise ValueError(\"Unsupported file type: {}\".format(filepath))", "idx": 1440}
{"namespace": "mackup.utils.copy", "completion": "    assert isinstance(src, str)\n    assert os.path.exists(src)\n    assert isinstance(dst, str)\n\n    # Create the path to the destination if it does not exist\n    abs_path = os.path.dirname(os.path.abspath(dst))\n    if not os.path.isdir(abs_path):\n        os.makedirs(abs_path)\n\n    if os.path.isfile(src):\n        shutil.copy2(src, dst)\n    elif os.path.isdir(src):\n        shutil.copytree(src, dst)\n    else:\n        raise ValueError(\"Unsupported file type: {}\".format(src))\n\n    # Set appropriate file permissions for the copied file or folder\n    chmod(dst)", "idx": 1441}
{"namespace": "mackup.utils.get_dropbox_folder_location", "completion": "    dropbox_db_path = \".dropbox/host.db\"\n    dropbox_home = None\n\n    db_path = os.path.join(os.environ[\"HOME\"], dropbox_db_path)\n    if os.path.isfile(db_path):\n        with open(db_path, \"rb\") as f:\n            data = f.read()\n            data = data[4:]\n            data = data.decode(\"utf-8\")\n            dropbox_home = data\n\n    if not dropbox_home:\n        error(\n            constants.ERROR_UNABLE_TO_FIND_STORAGE.format(provider=\"Dropbox install\")\n        )\n\n    return dropbox_home", "idx": 1442}
{"namespace": "mackup.utils.get_copy_folder_location", "completion": "    copy_db_path = os.path.join(os.environ[\"HOME\"], \".c2s\", \"c2s.db\")\n    try:\n        con = sqlite3.connect(copy_db_path)\n        if con:\n            cur = con.cursor()\n            query = \"SELECT value FROM config WHERE option = 'csmRootPath';\"\n            cur.execute(query)\n            data = cur.fetchone()\n            copy_home = str(data[0])\n            con.close()\n    except IOError:\n        error(constants.ERROR_UNABLE_TO_FIND_STORAGE.format(provider=\"Copy install\"))\n\n    return copy_home", "idx": 1443}
{"namespace": "mackup.utils.can_file_be_synced_on_current_platform", "completion": "    if platform.system() == constants.PLATFORM_DARWIN:\n        # Check if the file is in the iCloud Drive folder\n        icloud_folder = get_icloud_folder_location()\n        if path.startswith(icloud_folder):\n            return True\n    elif platform.system() == constants.PLATFORM_LINUX:\n        # Check if the file is in the Google Drive folder\n        google_drive_folder = get_google_drive_folder_location()\n        if path.startswith(google_drive_folder):\n            return True\n        # Check if the file is in the Copy folder\n        copy_folder = get_copy_folder_location()\n        if path.startswith(copy_folder):\n            return True\n    elif platform.system() == constants.PLATFORM_WINDOWS:\n        # Add Windows specific checks here\n        pass\n    return False", "idx": 1444}
{"namespace": "hl7.client.MLLPClient.send_message", "completion": "\n        if isinstance(message, hl7.Message):\n            message = message.to_er7()\n\n        if isinstance(message, str):\n            message = message.encode(self.encoding)\n\n        wrapped_message = SB + message + EB + CR\n\n        # upload the wrapped message\n        self.socket.send(wrapped_message)\n\n        # wait for the ACK/NACK\n        return self.socket.recv(RECV_BUFFER)", "idx": 1445}
{"namespace": "hl7.client.MLLPClient.send", "completion": "        self.socket.sendall(data)\n        return self.socket.recv(RECV_BUFFER)", "idx": 1446}
{"namespace": "hl7.datatypes._UTCOffset.tzname", "completion": "        if self.minutes < 0:\n            sign = \"-\"\n        else:\n            sign = \"+\"\n        hours = abs(self.minutes) // 60\n        minutes = abs(self.minutes) % 60\n        return f\"{sign}{hours:02d}{minutes:02d}\"", "idx": 1447}
{"namespace": "hl7.datatypes.parse_datetime", "completion": "    if not value:\n        return None\n\n    match = DTM_TZ_RE.match(value)\n    if match:\n        groups = match.groups()\n        year = int(groups[0][:4])\n        month = int(groups[0][4:6]) if len(groups[0]) >= 6 else 1\n        day = int(groups[0][6:8]) if len(groups[0]) >= 8 else 1\n        hour = int(groups[0][8:10]) if len(groups[0]) >= 10 else 0\n        minute = int(groups[0][10:12]) if len(groups[0]) >= 12 else 0\n        second = int(groups[0][12:14]) if len(groups[0]) >= 14 else 0\n        microsecond = int(groups[0][14:]) * 1000 if len(groups[0]) >= 17 else 0\n\n        if groups[1]:\n            tz_offset = int(groups[1]) * 60 + int(groups[2])\n            tz = _UTCOffset(-tz_offset) if value[-1] == \"-\" else _UTCOffset(tz_offset)\n        else:\n            tz = None\n\n        return datetime.datetime(year, month, day, hour, minute, second, microsecond, tz)\n    else:\n        raise ValueError(\"Invalid DTM format\")", "idx": 1448}
{"namespace": "hl7.parser._ParsePlan.container", "completion": "        return self.containers[0](sequence=data, esc=self.esc, separators=self.separators, factory=self.factory)", "idx": 1449}
{"namespace": "hl7.parser._ParsePlan.next", "completion": "        \"\"\"\n        This function generates the next level of the plan by creating a copy of the current plan with the level of the container and the separator starting at the next index.\n        Input-Output Arguments\n        :param self: _ParsePlan. An instance of the _ParsePlan class.\n        :return: _ParsePlan. An instance of the _ParsePlan class representing the next level of the plan.\n        \"\"\"\n        return _ParsePlan(self.separators[self.separators.find(self.separator) + 1], self.separators, self.containers[1:], self.esc, self.factory)", "idx": 1450}
{"namespace": "hl7.version.get_version", "completion": "    if len(VERSION) < 4 or VERSION[3] == \"final\":\n        return \".\".join(str(x) for x in VERSION[:3])\n    elif VERSION[3] == \"dev\":\n        return \".\".join(str(x) for x in VERSION[:3]) + \".dev\"\n    else:\n        return \".\".join(str(x) for x in VERSION[:3]) + VERSION[3]", "idx": 1451}
{"namespace": "twtxt.config.Config.from_file", "completion": "        if os.path.exists(file):\n            cfg = configparser.ConfigParser()\n            cfg.read(file)\n            conf = cls(file, cfg)\n            if conf.check_config_sanity():\n                return conf\n            else:\n                return None\n        else:\n            return None", "idx": 1452}
{"namespace": "twtxt.config.Config.discover", "completion": "        config_file = os.path.join(cls.config_dir, cls.config_name)\n        return cls.from_file(config_file)", "idx": 1453}
{"namespace": "twtxt.config.Config.create_config", "completion": "        config = configparser.ConfigParser()\n        config.add_section(\"twtxt\")\n        config.set(\"twtxt\", \"nick\", nick)\n        config.set(\"twtxt\", \"twtfile\", twtfile)\n        config.set(\"twtxt\", \"twturl\", twturl)\n        config.set(\"twtxt\", \"disclose_identity\", str(disclose_identity))\n        config.set(\"twtxt\", \"add_news\", str(add_news))\n\n        with open(cfgfile, \"w\") as configfile:\n            config.write(configfile)\n\n        return cls.from_file(cfgfile)", "idx": 1454}
{"namespace": "twtxt.config.Config.following", "completion": "        following_list = []\n        if self.cfg.has_section(\"following\"):\n            for nick, url in self.cfg.items(\"following\"):\n                following_list.append(Source(nick, url))\n        else:\n            logger.debug(\"No 'following' section found in the config.\")\n        return following_list", "idx": 1455}
{"namespace": "twtxt.config.Config.options", "completion": "        options_dict = {}\n        if self.cfg.has_section(\"twtxt\"):\n            for option in self.cfg.options(\"twtxt\"):\n                options_dict[option] = self.cfg.get(\"twtxt\", option)\n        return options_dict", "idx": 1456}
{"namespace": "twtxt.models.Tweet.relative_datetime", "completion": "        now = datetime.now(timezone.utc)\n        delta = now - self.created_at\n\n        if delta.days > 7:\n            return self.absolute_datetime\n        else:\n            return humanize.naturaltime(self.created_at)", "idx": 1457}
{"namespace": "twtxt.mentions.format_mentions", "completion": "    mention_re = re.compile(r'@<(?:(?P<name>\\S+?)\\s+)?(?P<url>\\S+?://.*?)>')\n\n    def handle_mention(match):\n        name = match.group('name')\n        url = match.group('url')\n        return format_callback(name, url)\n\n    return mention_re.sub(handle_mention, text)", "idx": 1458}
{"namespace": "twtxt.parser.parse_tweets", "completion": "    parsed_tweets = []\n    for raw_tweet in raw_tweets:\n        try:\n            parsed_tweet = parse_tweet(raw_tweet, source, now)\n            parsed_tweets.append(parsed_tweet)\n        except ValueError as e:\n            logger.warning(f\"Skipping tweet '{raw_tweet}': {e}\")\n    return parsed_tweets", "idx": 1459}
{"namespace": "wikipediaapi.Wikipedia.page", "completion": "        return WikipediaPage(\n            wiki=self,\n            title=title,\n            ns=ns,\n            language=self.language,\n        )", "idx": 1460}
{"namespace": "wikipediaapi.Wikipedia.article", "completion": "        if unquote:\n            title = parse.unquote(title)\n\n        return WikipediaPage(self, title=title, ns=ns, language=self.language)", "idx": 1461}
{"namespace": "wikipediaapi.WikipediaPageSection.__repr__", "completion": "        subsections = \", \".join(repr(sub) for sub in self.sections)\n        return f\"WikipediaPageSection(title={self.title}, level={self.level}, text={self.text}, num_subsections={len(self.sections)}, subsections=[{subsections}])\"", "idx": 1462}
{"namespace": "wikipediaapi.WikipediaPage.sections", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        return self._section", "idx": 1463}
{"namespace": "wikipediaapi.WikipediaPage.section_by_title", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        sections = self._section_mapping.get(title)\n        if sections is None:\n            return None\n        return sections[-1]", "idx": 1464}
{"namespace": "wikipediaapi.WikipediaPage.sections_by_title", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        sections = self._section_mapping.get(title, [])\n        return sections", "idx": 1465}
{"namespace": "wikipediaapi.WikipediaPage.text", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        text = self._summary\n        for section in self.sections:\n            text += section.full_text()\n        return text.strip()", "idx": 1466}
{"namespace": "wikipediaapi.WikipediaPage.langlinks", "completion": "        if not self._called[\"langlinks\"]:\n            self._fetch(\"langlinks\")\n        return self._langlinks", "idx": 1467}
{"namespace": "wikipediaapi.WikipediaPage.links", "completion": "        if not self._called[\"links\"]:\n            self._fetch(\"links\")\n        return self._links", "idx": 1468}
{"namespace": "wikipediaapi.WikipediaPage.backlinks", "completion": "        if not self._called[\"backlinks\"]:\n            self._fetch(\"backlinks\")\n        return self._backlinks", "idx": 1469}
{"namespace": "wikipediaapi.WikipediaPage.categorymembers", "completion": "        params = {\n            \"action\": \"query\",\n            \"list\": \"categorymembers\",\n            \"cmtitle\": self.title,\n            \"cmlimit\": 500,\n        }\n\n        used_params = kwargs\n        used_params.update(params)\n\n        raw = self.wiki._query(self, used_params)\n\n        self.wiki._common_attributes(raw[\"query\"], self)\n        v = raw[\"query\"]\n        while \"continue\" in raw:\n            params[\"cmcontinue\"] = raw[\"continue\"][\"cmcontinue\"]\n            raw = self.wiki._query(self, params)\n            v[\"categorymembers\"] += raw[\"query\"][\"categorymembers\"]\n\n        return self.wiki._build_categorymembers(v, self)", "idx": 1470}
{"namespace": "wikipediaapi.WikipediaPage._fetch", "completion": "        getattr(self.wiki, call)(self)\n        self._called[call] = True\n        return self", "idx": 1471}
{"namespace": "wikipediaapi.WikipediaPage.__repr__", "completion": "        if any(self._called.values()):\n            return f\"{self.title} (id: {self.pageid}, ns: {self.namespace})\"\n        else:\n            return f\"{self.title} (id: ??, ns: {self.namespace})\"", "idx": 1472}
{"namespace": "imapclient.imapclient.IMAPClient.starttls", "completion": "        if not self.has_capability(\"STARTTLS\"):\n            raise exceptions.CapabilityError(\"Server does not support STARTTLS capability\")\n\n        if ssl_context is None:\n            ssl_context = ssl.create_default_context()\n\n        # Check if the SSL context has a hostname and if not, set it to the host\n        if not ssl_context.check_hostname:\n            ssl_context.check_hostname = True\n            ssl_context.verify_mode = ssl.CERT_REQUIRED\n\n        # Send the STARTTLS command\n        typ, data = self._imap._simple_command(\"STARTTLS\")\n        self._check_resp(\"OK\", \"starttls\", typ, data)\n\n        # Upgrade the connection to SSL\n        self._imap.sock = ssl_context.wrap_socket(self._imap.sock, server_hostname=self.host)\n        self._imap.file = self._imap.sock.makefile(\"rb\")\n\n        return data[0]", "idx": 1473}
{"namespace": "imapclient.imapclient.IMAPClient.shutdown", "completion": "        self._imap.shutdown()\n        logger.info(\"Connection to the IMAP server has been closed.\")", "idx": 1474}
{"namespace": "imapclient.imapclient.IMAPClient.enable", "completion": "        capabilities = list(capabilities)\n        response = self._command_and_check(\"enable\", *capabilities)\n        return response", "idx": 1475}
{"namespace": "imapclient.imapclient.IMAPClient._proc_folder_list", "completion": "        folder_data = [f for f in folder_data if f]\n        if not folder_data:\n            return []\n\n        # Parse the response and extract the flags, delimiter, and name of each folder\n        folders = []\n        for flags, delimiter, name in folder_data:\n            flags = flags.decode(\"utf-8\")\n            delimiter = delimiter.decode(\"utf-8\")\n            name = name.decode(\"utf-8\")\n\n            # If the folder name is an integer, convert it back to a string\n            if name.isdigit():\n                name = str(name)\n\n            # If folder encoding is enabled, decode the folder name using UTF-7 encoding\n            if self.folder_encode:\n                name = decode_utf7(name)\n\n            folders.append((flags, delimiter, name))\n\n        return folders", "idx": 1476}
{"namespace": "imapclient.imapclient.IMAPClient.select_folder", "completion": "        if readonly:\n            return self._command_and_check(\"select\", self._normalise_folder(folder), unpack=True)\n        else:\n            return self._command_and_check(\"select\", self._normalise_folder(folder), unpack=True)", "idx": 1477}
{"namespace": "imapclient.imapclient.IMAPClient.unselect_folder", "completion": "        return self._command_and_check(\"unselect\", unpack=True)", "idx": 1478}
{"namespace": "imapclient.imapclient.IMAPClient.noop", "completion": "        tag = self._imap._command(\"NOOP\")\n        return self._consume_until_tagged_response(tag, \"NOOP\")", "idx": 1479}
{"namespace": "imapclient.imapclient.IMAPClient.idle", "completion": "        self._idle_tag = self._imap._new_tag()\n        self._imap._simple_command(\"IDLE\", uid=False, response_name=\"IDLE\")", "idx": 1480}
{"namespace": "imapclient.imapclient.IMAPClient.idle_check", "completion": "        if not hasattr(self._imap, \"idle\"):\n            raise exceptions.IllegalStateError(\"Server is not in IDLE mode\")\n\n        if timeout is not None:\n            poller = self._poll_socket(self.socket(), timeout)\n        else:\n            poller", "idx": 1481}
{"namespace": "imapclient.imapclient.IMAPClient.idle_done", "completion": "        tag = self._idle_tag\n        self._idle_tag = None\n        return self._consume_until_tagged_response(tag, \"IDLE\")", "idx": 1482}
{"namespace": "imapclient.imapclient.IMAPClient.folder_status", "completion": "        if what is None:\n            what = ['MESSAGES', 'RECENT', 'UIDNEXT', 'UIDVALIDITY', 'UNSEEN']\n        \n        status = self._command_and_check(\"status\", self._normalise_folder(folder), seq_to_parenstr(what))\n        return dict(zip(what, status))", "idx": 1483}
{"namespace": "imapclient.imapclient.IMAPClient.sort", "completion": "        args = [seq_to_parenstr_upper(sort_criteria), to_bytes(criteria, charset)]\n        data = self._raw_command_untagged(b\"SORT\", args, unpack=True)\n        return parse_message_list(data)", "idx": 1484}
{"namespace": "imapclient.imapclient.IMAPClient.thread", "completion": "        args = [to_bytes(algorithm), to_bytes(criteria, charset)]\n        data = self._raw_command_untagged(b\"THREAD\", args, response_name=\"THREAD\", unpack=True)\n        return parse_thread_response(data)", "idx": 1485}
{"namespace": "imapclient.imapclient.IMAPClient.get_flags", "completion": "        response = self.fetch(messages, [b'FLAGS'])\n        return {msg: flags[b'FLAGS'] for msg, flags in response.items()}", "idx": 1486}
{"namespace": "imapclient.imapclient.IMAPClient.get_gmail_labels", "completion": "        response = self.fetch(messages, [\"X-GM-LABELS\"])\n        label_sets = {}\n        for msg_id, data in response.items():\n            labels = data.get(b\"X-GM-LABELS\", [])\n            label_sets[msg_id] = utf7_decode_sequence(labels)\n        return label_sets", "idx": 1487}
{"namespace": "imapclient.imapclient.IMAPClient.append", "completion": "        if msg_time is not None:\n            msg_time = msg_time.strftime(\"%d-%b-%Y %H:%M:%S %z\")\n            return self._raw_command_untagged(\n                b\"APPEND\",\n                [self._normalise_folder(folder), to_bytes(seq_to_parenstr(flags)), msg_time, _literal(to_bytes(msg))],\n                response_name=\"APPEND\",\n                unpack=True,\n            )\n        else:\n            return self._raw_command_untagged(\n                b\"APPEND\",\n                [self._normalise_folder(folder), to_bytes(seq_to_parenstr(flags)), _literal(to_bytes(msg))],\n                response_name=\"APPEND\",\n                unpack=True,\n            )", "idx": 1488}
{"namespace": "imapclient.imapclient.IMAPClient.multiappend", "completion": "        if not msgs:\n            return\n\n        args = [to_bytes(folder)]\n        for msg in msgs:\n            if isinstance(msg, str):\n                args.append(to_bytes(msg))\n            elif isinstance(msg, dict):\n                msg_args = []\n                if \"flags\" in msg:\n                    msg_args.append(seq_to_parenstr(msg[\"flags\"]))\n                if \"date\" in msg:\n                    msg_args.append(to_bytes('\"' + datetime_to_INTERNALDATE(msg[\"date\"]) + '\"'))\n                msg_args.append(to_bytes(msg[\"msg\"]))\n                args.append(seq_to_parenstr(msg_args))\n            else:\n                raise ValueError(\"msgs must be an iterable containing strings or dictionaries\")\n\n        return self._raw_command_untagged(b\"MULTIAPPEND\", args, unpack=True)", "idx": 1489}
{"namespace": "imapclient.imapclient.IMAPClient.expunge", "completion": "        This function is used to exp", "idx": 1490}
{"namespace": "imapclient.imapclient.IMAPClient.getacl", "completion": "        return self._command_and_check(\"getacl\", self._normalise_folder(folder), unpack=True)", "idx": 1491}
{"namespace": "imapclient.imapclient.IMAPClient.setacl", "completion": "        return self._command_and_check(\n            \"setacl\",\n            self._normalise_folder(folder),\n            to_bytes(who),\n            to_bytes(what),\n            unpack=True,\n        )", "idx": 1492}
{"namespace": "imapclient.imapclient.IMAPClient.get_quota_root", "completion": "        response = self._command_and_check(\"getquotaroot\", _quote(mailbox))\n        parsed_response = parse_response(response)\n        mailbox = parsed_response[0]\n        quota_roots = parsed_response[1:]\n        return MailboxQuotaRoots(mailbox, quota_roots), [self._get_quota(quota_root) for quota_root in quota_roots]", "idx": 1493}
{"namespace": "imapclient.imapclient.IMAPClient.set_quota", "completion": "        args = []\n        for quota in quotas:\n            args.append(quota.quota_root)\n            args.append(quota.resource)\n            args.append(quota.limit)\n        self._command_and_check(\"setquota\", *args)", "idx": 1494}
{"namespace": "imapclient.imapclient.IMAPClient._consume_until_tagged_response", "completion": "        untagged_responses = []\n        while True:\n            typ, data = self._imap._get_response()\n            if typ == tag:\n                return data, untagged_responses\n            elif typ == \"OK\" or typ == \"NO\":\n                raise exceptions.IMAPClientError(\n                    f\"Unexpected response while waiting for {command} response: {typ} {data}\"\n                )\n            else:\n                untagged_responses.append((typ, data))", "idx": 1495}
{"namespace": "imapclient.imapclient._normalise_search_criteria", "completion": "    if not criteria:\n        raise ValueError(\"No search criteria specified\")\n\n    if isinstance(criteria, (str, bytes, int)):\n        criteria = [criteria]\n    elif isinstance(criteria, (list, tuple)):\n        criteria = list(criteria)\n    elif isinstance(criteria, datetime):\n        criteria = [criteria]\n    else:\n        raise ValueError(\"Invalid search criteria type\")\n\n    return [to_bytes(c, charset) for c in criteria]", "idx": 1496}
{"namespace": "imapclient.response_lexer.TokenSource.current_literal", "completion": "        if self.lex.current_source is not None:\n            return self.lex.current_source.literal\n        else:\n            return None", "idx": 1497}
{"namespace": "imapclient.imap_utf7.decode", "completion": "\n    if not isinstance(s, (bytes, str)):\n        return s\n\n    if isinstance(s, bytes):\n        s = s.decode('utf-7')\n\n    res = []\n    i = 0\n    while i < len(s):\n        if s[i] == '&':\n            j = i\n            while j < len(s) and s[j] != '-':\n                j += 1\n            res.append(base64_utf7_decode(s[i+1:j].encode('utf-7')))\n            i = j + 1\n        else:\n            res.append(s[i])\n            i += 1\n\n    return ''.join(res)", "idx": 1498}
{"namespace": "imapclient.fixed_offset.FixedOffset.for_system", "completion": "        current_time = datetime.datetime.now()\n        if time.localtime().tm_isdst == 1 and time.daylight == 1:\n            offset = time.altzone / 60\n        else:\n            offset = time.timezone / 60\n        return cls(offset)", "idx": 1499}
{"namespace": "imapclient.datetime_util.parse_to_datetime", "completion": "    dt = datetime(*parsedate_tz(_munge(timestamp))[:6])\n    if normalise:\n        return datetime_to_native(dt)\n    return dt", "idx": 1500}
{"namespace": "imapclient.datetime_util.datetime_to_INTERNALDATE", "completion": "    if dt.tzinfo is None:\n        dt = dt.astimezone(FixedOffset.for_system())\n    return dt.strftime(\"%d-%b-%Y %H:%M:%S %z\")", "idx": 1501}
{"namespace": "mrjob.bin.MRJobBinRunner.get_spark_submit_bin", "completion": "    if not self._spark_submit_bin:\n        self._spark_submit_bin = self._find_spark_submit_bin()\n    return self._spark_submit_bin", "idx": 1502}
{"namespace": "mrjob.step.StepFailedException.__str__", "completion": "        if self.reason:\n            step_desc = self.step_desc or 'Step %d of %d' % (self.step_num + 1, self.num_steps)\n            return '%s failed: %s' % (step_desc, self.reason)\n        else:\n            step_desc = self.step_desc or 'Step %d' % (self.step_num + 1)\n            if self.last_step_num is not None:\n                return 'Steps %d-%d failed' % (self.step_num + 1, self.last_step_num + 1)\n            else:\n                return '%s failed' % step_desc", "idx": 1503}
{"namespace": "mrjob.step.StepFailedException.__repr__", "completion": "        fields = ', '.join('%s=%r' % (field, getattr(self, field)) for field in self._FIELDS)\n        return '%s(%s)' % (self.__class__.__name__, fields)", "idx": 1504}
{"namespace": "mrjob.step.MRStep.description", "completion": "        desc = {'type': 'streaming'}\n\n        if step_num == 0 or self.has_explicit_mapper:\n            desc['mapper'] = self.render_mapper()\n\n        if self.has_explicit_combiner:\n            desc['combiner'] = self.render_combiner()\n\n        if self.has_explicit_reducer:\n            desc['reducer'] = self.render_reducer()\n\n        if self._steps['mapper_raw']:\n            desc['input_manifest'] = True\n\n        if 'jobconf' in self._steps:\n            desc['jobconf'] = self._steps['jobconf']\n\n        return desc", "idx": 1505}
{"namespace": "mrjob.step._Step.description", "completion": "        step_dict = {'type': self._STEP_TYPE}\n        for attr in self._STEP_ATTRS:\n            if attr not in self._HIDDEN_ATTRS and hasattr(self, attr):\n                step_dict[attr] = getattr(self, attr)\n        return step_dict", "idx": 1506}
{"namespace": "mrjob.protocol._KeyCachingProtocol.read", "completion": "        \"\"\"\n        Decodes a line of raw input into a tuple of key and value.\n        Splits the input line at the first occurrence of the tab character. Then it updates the last key encoded by loading the key we obtained. It also decodes the value and returns a tuple of the last key decoded and the decoded value.\n\n        Input-Output Arguments\n        :param line: String. A line of raw input to the job, without trailing newline.\n        :return: tuple. A tuple of ``(key, value)``.\n\n        \"\"\"\n        key, value = line.split(b'\\t', 1)\n        self._last_key_decoded = self._loads(key)\n        return self._last_key_decoded, self._loads(value)", "idx": 1507}
{"namespace": "mrjob.util.safeeval", "completion": "\n    safe_globals = {\n        'True': True,\n        'False': False,\n        'None': None,\n        'set': set,\n        'range': range,\n        'xrange': range  # for Python 2.x compatibility\n    }\n\n    try:\n        return eval(expr, safe_globals, locals)\n    except NameError as e:\n        if 'open' in str(e):\n            raise NameError(\"name 'open' is not defined\")\n        else:\n            raise", "idx": 1508}
{"namespace": "mrjob.util.to_lines", "completion": "    if hasattr(chunks, 'readline'):\n        return chunks\n\n    # rest of the function remains the same\n    # ...", "idx": 1509}
{"namespace": "mrjob.parse.is_s3_uri", "completion": "    try:\n        parse_s3_uri(uri)\n        return True\n    except ValueError:\n        return False", "idx": 1510}
{"namespace": "mrjob.parse.parse_s3_uri", "completion": "    if not is_s3_uri(uri):\n        raise ValueError('Not an S3 URI: %r' % uri)\n\n    parsed = urlparse(uri)\n\n    if not parsed.netloc or not parsed.path:\n        raise ValueError('Not an S3 URI: %r' % uri)\n\n    bucket_name = parsed.netloc\n    key = parsed.path.lstrip('/')\n\n    return bucket_name, key", "idx": 1511}
{"namespace": "mrjob.parse.to_uri", "completion": "    if is_uri(path_or_uri):\n        return path_or_uri\n    else:\n        return 'file://' + pathname2url(abspath(path_or_uri))", "idx": 1512}
{"namespace": "mrjob.parse.parse_mr_job_stderr", "completion": "    if isinstance(stderr, bytes):\n        stderr = BytesIO(stderr)\n    elif isinstance(stderr, list):\n        stderr = BytesIO(b''.join(stderr))\n\n    if counters is None:\n        counters = {}\n\n    statuses = []\n    other = []\n\n    for line in stderr:\n        line = line.strip()\n        counter_match = _COUNTER_RE.match(line)\n        status_match = _STATUS_RE.match(line)\n\n        if counter_match:\n            group, counter, amount = counter_match.groups()\n            group = group.decode('utf_8')\n            counter = counter.decode('utf_8')\n            amount = int(amount)\n            if group not in counters:\n                counters[group] = {}\n            counters[group][counter] = counters[group].get(counter, 0) + amount\n        elif status_match:\n            statuses.append(status_match.group(1).decode('utf_8'))\n        else:\n            other.append(line.decode('utf_8'))\n\n    return {'counters': counters, 'statuses': statuses, 'other': other}", "idx": 1513}
{"namespace": "mrjob.parse._parse_progress_from_job_tracker", "completion": "    running_jobs_start = html_bytes.find(b'Running Jobs')\n    if running_jobs_start == -1:\n        return (None, None)\n\n    running_jobs_end = html_bytes.find(b'Jobs', running_jobs_start)\n    if running_jobs_end == -1:\n        return (None, None)\n\n    running_jobs_content = html_bytes[running_jobs_start:running_jobs_end]\n\n    map_percent_match = _JOB_TRACKER_HTML_RE.search(running_jobs_content)\n    reduce_percent_match = _JOB_TRACKER_HTML_RE.search(running_jobs_content, map_percent_match.end() if map_percent_match else 0)\n\n    map_percent = float(map_percent_match.group(1)) if map_percent_match else None\n    reduce_percent = float(reduce_percent_match.group(1)) if reduce_percent_match else None\n\n    return (map_percent, reduce_percent)", "idx": 1514}
{"namespace": "mrjob.parse._parse_progress_from_resource_manager", "completion": "    match = _RESOURCE_MANAGER_JS_RE.search(html_bytes)\n    if match:\n        return float(match.group('percent'))\n    else:\n        return None", "idx": 1515}
{"namespace": "mrjob.logs.task._match_task_log_path", "completion": "    if application_id:\n        match = _YARN_TASK_LOG_PATH_RE.match(path)\n        if match:\n            if match.group('application_id') != application_id:\n                return None\n            return {\n                'application_id': match.group('application_id'),\n                'container_id': match.group('container_id'),\n                'log_type': match.group('log_type')\n            }\n    elif job_id:\n        match = _PRE_YARN_TASK_LOG_PATH_RE.match(path)\n        if match:\n            if match.group('timestamp') != job_id:\n                return None\n            return {\n                'attempt_id': match.group('attempt_id'),\n                'log_type': match.group('log_type')\n            }\n    return None", "idx": 1516}
{"namespace": "mrjob.logs.task._parse_task_syslog", "completion": "    records = parse._parse_log4j_records(lines)\n    return _parse_task_syslog_records(records)", "idx": 1517}
{"namespace": "mrjob.logs.ids._sort_for_spark", "completion": "    return sorted(ds, key=_step_sort_key, reverse=False)", "idx": 1518}
{"namespace": "mrjob.logs.spark._parse_spark_log", "completion": "\n    result = {}\n\n    for line in lines:\n        if record_callback:\n            record_callback(line)\n\n    return result", "idx": 1519}
{"namespace": "mrjob.logs.mixin.LogInterpretationMixin._pick_error", "completion": "        log.info('Scanning logs for probable cause of failure...')\n        # Add your code here to pick the probable cause of failure\n        # based on the log interpretation and step type", "idx": 1520}
{"namespace": "mrjob.logs.history._match_history_log_path", "completion": "    match = _HISTORY_LOG_PATH_RE.match(path)\n    if match:\n        if job_id and job_id != match.group('job_id'):\n            return None\n        return {'job_id': match.group('job_id'), 'yarn': '.jhist' in match.group('suffix')}\n    else:\n        return None", "idx": 1521}
{"namespace": "mrjob.logs.history._parse_pre_yarn_history_log", "completion": "    result = {}\n    task_to_counters = {}\n\n    for record in _parse_pre_yarn_history_records(lines):\n        record_type = record['type']\n        fields = record['fields']\n\n        if record_type == 'Task' and 'COUNTERS' in fields and 'TASKID' in fields:\n            task_id = fields['TASKID']\n            counters_str = fields['COUNTERS']\n            task_to_counters[task_id] = _parse_pre_yarn_counters(counters_str)\n\n        elif record_type == 'FAILED' and fields.get('ERROR'):\n            error = {\n                'hadoop_error': {\n                    'message': fields['ERROR'],\n                    'start_line': record['start_line'],\n                    'num_lines': record['num_lines']\n                }\n            }\n            if 'TASK_ATTEMPT_ID' in fields:\n                error['attempt_id'] = fields['TASK_ATTEMPT_ID']\n            result.setdefault('errors', []).append(error)\n\n    if not result.get('counters') and task_to_counters:\n        result['counters'] = _sum_counters(*task_to_counters.values())\n\n    return result", "idx": 1522}
{"namespace": "mrjob.logs.history._parse_pre_yarn_history_records", "completion": "    start_line = 0\n    current_record = {'fields': {}, 'num_lines': 0, 'start_line': 0, 'type': None}\n\n    for line_num, line in enumerate(lines):\n        if line.strip() == '.':\n            current_record['num_lines'] = line_num - current_record['start_line'] + 1\n            yield current_record\n            current_record = {'fields': {}, 'num_lines': 0, 'start_line': line_num + 1, 'type': None}\n            continue\n\n        if current_record['type'] is None:\n            record_match = _PRE_YARN_HISTORY_RECORD.match(line)\n            if record_match:\n                current_record['type'] = record_match.group('type')\n                line = line[record_match.end('type'):]\n\n        key_pairs = current_record.get('key_pairs', '')\n        key_pair_match = _PRE_YARN_HISTORY_KEY_PAIR.search(line)\n        if key_pair_match:\n            key = key_pair_match.group('key')\n            value = _pre_yarn_history_unescape(key_pair_match.group('escaped_value'))\n            current_record['fields'][key] = value\n\n    current_record['num_lines'] = len(lines) - current_record['start_line']\n    yield current_record", "idx": 1523}
{"namespace": "mrjob.logs.step._parse_step_syslog", "completion": "    return _parse_step_syslog_from_log4j_records(\n        _parse_hadoop_log4j_records(lines))", "idx": 1524}
{"namespace": "mrjob.logs.errors._merge_and_sort_errors", "completion": "    merged_errors = {}\n    for error in errors:\n        container_id = None\n        if 'container_id' in error:\n            container_id = error['container_id']\n        elif 'attempt_id' in error and attempt_to_container_id:\n            container_id = attempt_to_container_id.get(error['attempt_id'])\n\n        if container_id in merged_errors:\n            merged_errors[container_id].append(error)\n        else:\n            merged_errors[container_id] = [error]\n\n    sorted_errors = []\n    for key in sorted(merged_errors, key=_sort_key, reverse=True):\n        sorted_errors.extend(merged_errors[key])\n\n    return sorted_errors", "idx": 1525}
{"namespace": "mrjob.fs.ssh.SSHFilesystem.ls", "completion": "\n        m = _SSH_URI_RE.match(path_glob)\n        addr = m.group('hostname')\n        fs_path = m.group('filesystem_path')\n\n        stdout, _ = self._ssh_run(addr, ['find', fs_path, '-type', 'f'])\n\n        for line in stdout.splitlines():\n            yield line.decode('utf-8')", "idx": 1526}
{"namespace": "mrjob.fs.ssh.SSHFilesystem._cat_file", "completion": "        p = self._ssh_launch(path, ['cat', path])\n        return p.stdout", "idx": 1527}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.get_hadoop_bin", "completion": "        if self._hadoop_bin is None:\n            self._hadoop_bin = self._find_hadoop_bin()\n        return self._hadoop_bin", "idx": 1528}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.du", "completion": "\n        try:\n            output = self.invoke_hadoop(['fs', '-du', '-s', path_glob], ok_returncodes=[0, 1, 255], return_stdout=True)\n            lines = output.split(b'\\n')\n            if len(lines) > 0:\n                size_str = lines[0].split(b'\\t')[0]\n                return int(size_str)\n            else:\n                return 0\n        except CalledProcessError:\n            raise IOError(\"Unexpected output from Hadoop fs -du: {output!r}\")", "idx": 1529}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.mkdir", "completion": "        version = self.get_hadoop_version()\n        if uses_yarn(version):\n            args = ['fs', '-mkdir', '-p', path]\n        else:\n            args = ['fs', '-mkdir', path]\n\n        try:\n            self.invoke_hadoop(\n                args,\n                ok_stderr=[_HADOOP_FILE_EXISTS_RE])\n        except CalledProcessError:\n            raise IOError(\"Could not mkdir %s\" % path)", "idx": 1530}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.exists", "completion": "        version = self.get_hadoop_version()\n\n        if path_glob.endswith('/'):\n            # check if there are any files starting with the path\n            for _ in self.ls(path_glob):\n                return True\n            return False\n        else:\n            # invoke Hadoop 'fs -ls' command to check if the path exists\n            if uses_yarn(version):\n                args = ['fs', '-ls', '-R', path_glob]\n            else:\n                args = ['fs', '-lsr', path_glob]\n\n            try:\n                self.invoke_hadoop(args, ok_returncodes=[0, 255], ok_stderr=[_HADOOP_LS_NO_SUCH_FILE])\n                return True\n            except CalledProcessError:\n                return False", "idx": 1531}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.rm", "completion": "        if not is_uri(path_glob):\n            super(HadoopFilesystem, self).rm(path_glob)\n            return\n\n        version = self.get_hadoop_version()\n\n        # use -r on Hadoop 2 (see #991, #845)\n        if uses_yarn(version):\n            args = ['fs', '-rm', '-r', path_glob]\n        else:\n            args = ['fs', '-rmr', path_glob]\n\n        try:\n            self.invoke_hadoop(args, ok_stderr=[_HADOOP_RM_NO_SUCH_FILE])\n        except CalledProcessError:\n            raise IOError(\"Could not remove %s\" % path_glob)", "idx": 1532}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.touchz", "completion": "        try:\n            self.invoke_hadoop(['fs', '-touchz', path])\n        except CalledProcessError:\n            raise IOError(\"Could not touchz %s\" % path)", "idx": 1533}
{"namespace": "mrjob.fs.local.LocalFilesystem.du", "completion": "        path_glob = _from_file_uri(path_glob)\n        total_size = 0\n\n        for path in glob.glob(path_glob):\n            if os.path.isfile(path):\n                total_size += os.path.getsize(path)\n\n        return total_size", "idx": 1534}
{"namespace": "mrjob.fs.local.LocalFilesystem.ls", "completion": "        path_glob = _from_file_uri(path_glob)\n        if os.path.isdir(path_glob):\n            for root, dirs, files in os.walk(path_glob):\n                for file in files:\n                    yield 'file://' + os.path.join(root, file)\n                for dir in dirs:\n                    yield 'file://' + os.path.join(root, dir)\n        else:\n            yield 'file://' + path_glob", "idx": 1535}
{"namespace": "mrjob.fs.local.LocalFilesystem._cat_file", "completion": "        path = _from_file_uri(path)\n        with open(path, 'rb') as f:\n            while True:\n                chunk = f.read(8192)  # read in 8KB chunks\n                if not chunk:\n                    break\n                yield chunk", "idx": 1536}
{"namespace": "mrjob.fs.local.LocalFilesystem.exists", "completion": "        path_glob = _from_file_uri(path_glob)\n        return any(os.path.exists(path) for path in glob.glob(path_glob))", "idx": 1537}
{"namespace": "mrjob.fs.local.LocalFilesystem.mkdir", "completion": "        path = _from_file_uri(path)\n        if not os.path.exists(path):\n            os.makedirs(path)", "idx": 1538}
{"namespace": "mrjob.fs.local.LocalFilesystem.put", "completion": "        src = _from_file_uri(src)\n        shutil.copy(src, path)", "idx": 1539}
{"namespace": "mrjob.fs.local.LocalFilesystem.rm", "completion": "        path_glob = _from_file_uri(path_glob)\n        for path in glob.glob(path_glob):\n            if os.path.isdir(path):\n                shutil.rmtree(path)\n            else:\n                os.remove(path)", "idx": 1540}
{"namespace": "mrjob.fs.local.LocalFilesystem.touchz", "completion": "        path = _from_file_uri(path)\n        if os.path.exists(path) and os.path.getsize(path) > 0:\n            raise OSError('File already exists and is not empty: %s' % path)\n        with open(path, 'w'):\n            pass", "idx": 1541}
{"namespace": "mrjob.fs.local.LocalFilesystem.md5sum", "completion": "        path = _from_file_uri(path)\n        with open(path, 'rb') as f:\n            return self._md5sum_file(f)", "idx": 1542}
{"namespace": "mrjob.fs.composite.CompositeFilesystem.add_fs", "completion": "        setattr(self, name, fs)\n        self._fs_names.append(name)\n        if disable_if:\n            self._disable_if[name] = disable_if", "idx": 1543}
{"namespace": "mrjob.fs.base.Filesystem.cat", "completion": "        raise NotImplementedError", "idx": 1544}
{"namespace": "mrjob.fs.base.Filesystem.join", "completion": "        if '://' in path:\n            scheme, netloc, base_path, _, _ = urlparse(path)\n            joined_path = posixpath.join(base_path, *paths)\n            return urlunparse((scheme, netloc, joined_path, '', '', ''))\n        else:\n            return posixpath.join(path, *paths)", "idx": 1545}
{"namespace": "mrjob.examples.mr_text_classifier.parse_doc_filename", "completion": "    filename = posixpath.basename(input_uri)\n    id_and_cats = filename.split('.')[0]\n    unique_id, cat_ids = id_and_cats.split('-')[0], id_and_cats.split('-')[1:]\n    cats = {}\n    for cat_id in cat_ids:\n        if cat_id.startswith('not_'):\n            cats[cat_id[4:]] = False\n        else:\n            cats[cat_id] = True\n    return {'id': unique_id, 'cats': cats}", "idx": 1546}
{"namespace": "prometheus_client.mmap_dict.MmapedDict.read_value", "completion": "        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        value, timestamp = _unpack_two_doubles(self._m, pos)\n        return value", "idx": 1547}
{"namespace": "prometheus_client.mmap_dict.MmapedDict.write_value", "completion": "        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        _pack_two_doubles(self._m, pos, value, timestamp)", "idx": 1548}
{"namespace": "prometheus_client.multiprocess.MultiProcessCollector.merge", "completion": "        metrics = MultiProcessCollector._read_metrics(files)\n        return MultiProcessCollector._accumulate_metrics(metrics, accumulate)", "idx": 1549}
{"namespace": "prometheus_client.multiprocess.MultiProcessCollector.collect", "completion": "        files = glob.glob(os.path.join(self._path, '*.db'))\n        return self.merge(files, accumulate=True)", "idx": 1550}
{"namespace": "prometheus_client.exposition.choose_encoder", "completion": "\n    if accept_header and \"application/openmetrics-text\" in accept_header:\n        return generate_latest, \"application/openmetrics-text; version=1.0.0; charset=utf-8\"\n    else:\n        return generate_latest, CONTENT_TYPE_LATEST", "idx": 1551}
{"namespace": "flower.command.apply_options", "completion": "\n    parse_command_line([prog_name] + argv)\n    if options.conf:\n        try:\n            parse_config_file(options.conf, final=False)\n        except IOError as e:\n            if str(e) != f\"[Errno 2] No such file or directory: '{options.conf}'\":\n                raise", "idx": 1552}
{"namespace": "trackerjacker.ieee_mac_vendor_db.MacVendorDB.lookup", "completion": "        mac = mac.upper().replace(':', '')\n        prefix = mac[:6]\n        return self.db.get(prefix, '')", "idx": 1553}
{"namespace": "trailscraper.iam.Statement.merge", "completion": "\n        if self.Effect != other.Effect:\n            raise ValueError(f\"Trying to combine two statements with differing effects: {self.Effect} {other.Effect}\")\n\n        merged_actions = sorted(self.Action + other.Action, key=lambda a: a.json_repr())\n        merged_resources = sorted(self.Resource + other.Resource)\n\n        return Statement(Action=merged_actions, Effect=self.Effect, Resource=merged_resources)", "idx": 1554}
{"namespace": "trailscraper.iam.parse_policy_document", "completion": "\n    if isinstance(stream, six.string_types):\n        json_data = json.loads(stream)\n    else:\n        json_data = json.load(stream)\n\n    statements = _parse_statements(json_data['Statement'])\n    version = json_data.get('Version', \"2012-10-17\")\n\n    return PolicyDocument(Statement=statements, Version=version)", "idx": 1555}
{"namespace": "trailscraper.iam.known_iam_actions", "completion": "    all_actions = all_known_iam_permissions()\n    actions_by_prefix = pipe(\n        all_actions,\n        mapz(lambda action: _parse_action(action)),\n        groupbyz(lambda action: action.prefix)\n    )\n    return [action.json_repr() for action in actions_by_prefix.get(prefix, [])]", "idx": 1556}
{"namespace": "trailscraper.boto_service_definitions.service_definition_file", "completion": "    service_definition_files = boto_service_definition_files()\n    filtered_files = [file for file in service_definition_files if fnmatch.fnmatch(file, '**/' + servicename + '/*/service-*.json')]\n    sorted_files = sorted(filtered_files)\n    return sorted_files[-1]", "idx": 1557}
{"namespace": "trailscraper.boto_service_definitions.operation_definition", "completion": "\n    service_definition_path = service_definition_file(servicename)\n    with open(service_definition_path, 'r') as file:\n        service_definition = json.load(file)\n        return service_definition['operations'][operationname]", "idx": 1558}
{"namespace": "trailscraper.cloudtrail.Record.to_statement", "completion": "\n        if self.event_source == \"sts.amazonaws.com\" and self.event_name == \"GetCallerIdentity\":\n            return None\n        elif self.event_source == \"apigateway.amazonaws.com\":\n            return self._to_api_gateway_statement()\n        else:\n            # Handle other event sources and event names here\n            return None", "idx": 1559}
{"namespace": "trailscraper.cloudtrail.filter_records", "completion": "    return list(filterz(_by_timeframe(from_date, to_date), filterz(_by_role_arns(arns_to_filter_for), records)))", "idx": 1560}
{"namespace": "trailscraper.record_sources.local_directory_record_source.LocalDirectoryRecordSource.load_from_dir", "completion": "        records_within_date_range = []\n        for log_file in self._valid_log_files():\n            if log_file.contains_events_within_date_range(from_date, to_date):\n                records_within_date_range.extend(log_file.records())\n\n        return records_within_date_range", "idx": 1561}
{"namespace": "pyt.__main__.discover_files", "completion": "    included_files = []\n    for target in targets:\n        if os.path.isfile(target):\n            included_files.append(target)\n        elif os.path.isdir(target):\n            for root, dirs, files in os.walk(target):\n                for file in files:\n                    if file.endswith(\".py\") and file not in excluded_files:\n                        included_files.append(os.path.join(root, file))\n                        log.debug('Discovered file: %s', os.path.join(root, file))\n    return included_files", "idx": 1562}
{"namespace": "pyt.core.project_handler.get_directory_modules", "completion": "    global _local_modules\n    if _local_modules and directory == os.path.dirname(_local_modules[0][1]):\n        return _local_modules\n    if not os.path.isdir(directory):\n        directory = os.path.dirname(directory)\n    modules = get_modules(directory)\n    _local_modules = modules\n    return modules", "idx": 1563}
{"namespace": "pyt.vulnerabilities.vulnerabilities.find_triggers", "completion": "    trigger_nodes = []\n    for node in nodes:\n        if node.line_number not in nosec_lines:\n            for trigger in trigger_words:\n                if trigger.trigger_word in node.label:\n                    trigger_nodes.append(TriggerNode(trigger, node))\n    return trigger_nodes", "idx": 1564}
{"namespace": "pyt.vulnerabilities.vulnerabilities.label_contains", "completion": "    for trigger in triggers:\n        if trigger in node.label:\n            yield TriggerNode(trigger, node)", "idx": 1565}
{"namespace": "pyt.vulnerabilities.vulnerabilities.build_sanitiser_node_dict", "completion": "    sanitiser_node_dict = {}\n    for sink in sinks_in_file:\n        for sanitiser in sink.sanitisers:\n            sanitiser_nodes = find_sanitiser_nodes(sanitiser, sinks_in_file)\n            sanitiser_node_dict[sanitiser] = sanitiser_nodes\n    return sanitiser_node_dict", "idx": 1566}
{"namespace": "pyt.vulnerabilities.trigger_definitions_parser.parse", "completion": "\n    with open(trigger_word_file, 'r') as file:\n        data = json.load(file)\n\n    sources = [Source.from_json(key, value) for key, value in data['sources'].items()]\n    sinks = [Sink.from_json(key, value) for key, value in data['sinks'].items()]\n\n    return Definitions(sources, sinks)", "idx": 1567}
{"namespace": "principalmapper.querying.local_policy_simulation._statement_matches_resource", "completion": "    if 'Resource' in statement:\n        for item in _listify_string(statement['Resource']):\n            if _matches_after_expansion(resource, item, condition_keys):\n                return True\n    elif 'NotResource' in statement:\n        for item in _listify_string(statement['NotResource']):\n            if _matches_after_expansion(resource, item, condition_keys):\n                return False\n        return True\n    else:\n        return True", "idx": 1568}
{"namespace": "principalmapper.querying.local_policy_simulation._matches_after_expansion", "completion": "\n    if condition_keys:\n        for key, value in condition_keys.items():\n            string_to_check_against = string_to_check_against.replace('${' + key + '}', value)\n\n    pattern = _compose_pattern(string_to_check)\n    return bool(pattern.match(string_to_check_against))", "idx": 1569}
{"namespace": "passpie.database.PasspieStorage.delete", "completion": "        for cred in credentials:\n            credpath = self.make_credpath(cred[\"name\"], cred[\"login\"])\n            if os.path.exists(credpath):\n                os.remove(credpath)\n                dirname = os.path.dirname(credpath)\n                if not os.listdir(dirname):\n                    os.rmdir(dirname)", "idx": 1570}
{"namespace": "passpie.database.PasspieStorage.read", "completion": "        data = {}\n        for root, dirs, files in os.walk(self.path):\n            for file in files:\n                if file.endswith(self.extension):\n                    with open(os.path.join(root, file), 'r') as f:\n                        content = yaml.safe_load(f)\n                        data[len(data)] = content\n        return data", "idx": 1571}
{"namespace": "threatingestor.state.State.save_state", "completion": "        logger.debug(f\"Saving state for '{name}'\")\n        try:\n            self.cursor.execute('INSERT OR REPLACE INTO states (name, state) VALUES (?, ?)', (name, state))\n            self.conn.commit()\n        except sqlite3.Error:\n            raise threatingestor.exceptions.IngestorError(\"Failed to save state in the database\")", "idx": 1572}
{"namespace": "threatingestor.state.State.get_state", "completion": "        try:\n            self.cursor.execute('SELECT state FROM states WHERE name = ?', (name,))\n            result = self.cursor.fetchone()\n            if result:\n                return result[0]\n            else:\n                return None\n        except sqlite3.Error:\n            raise threatingestor.exceptions.IngestorError(\"Error retrieving state from database\")", "idx": 1573}
{"namespace": "threatingestor.Ingestor.run", "completion": "        if self.config.run_forever():\n            self.run_forever()\n        else:\n            self.run_once()", "idx": 1574}
{"namespace": "msticpy.analysis.anomalous_sequence.model.Model.compute_scores", "completion": "        self.compute_likelihoods_of_sessions(use_start_end_tokens)\n        self.compute_geomean_lik_of_sessions()\n        self.compute_rarest_windows(2, use_start_end_tokens, False)\n        self.compute_rarest_windows(3, use_start_end_tokens, False)", "idx": 1575}
{"namespace": "msticpy.analysis.anomalous_sequence.model.Model.compute_rarest_windows", "completion": "        if self.session_likelihoods is None:\n            self.compute_likelihoods_of_sessions(use_start_end_tokens=use_start_end_tokens)\n\n        result_windows = {}\n        result_likelihoods = {}\n\n        for idx, session in enumerate(self.sessions):\n            if use_start_end_tokens:\n                session = [self.start_token] + session + [self.end_token]\n            rarest_window, rarest_likelihood = self._compute_rarest_window(\n                session=session, window_len=window_len, use_geo_mean=use_geo_mean\n            )\n            result_windows[idx] = rarest_window\n            result_likelihoods[idx] = rarest_likelihood\n\n        if use_geo_mean:\n            self.rarest_windows_geo[window_len] = result_windows\n            self.rarest_window_likelihoods_geo[window_len] = result_likelihoods\n        else:\n            self.rarest_windows[window_len] = result_windows\n            self.rarest_window_likelihoods[window_len] = result_likelihoods", "idx": 1576}
{"namespace": "msticpy.analysis.anomalous_sequence.anomalous.score_sessions", "completion": "\n    # Your code here\n    # Model the sessions and compute likelihood metrics\n    # Append two additional columns to the input DataFrame\n    # Return the modified DataFrame\n    pass", "idx": 1577}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.laplace_smooth_counts", "completion": "    # Apply Laplace smoothing to the counts of commands and parameters\n    # Add 1 to each count to shift some probability mass from very probable commands/parameters to unseen and unlikely commands/parameters\n    # Handle unseen commands, sequences of commands, and parameters using the `unk_token`\n    seq1_counts_sm = defaultdict(lambda: 1)\n    seq2_counts_sm = defaultdict(lambda: defaultdict(lambda: 1))\n    param_counts_sm = defaultdict(lambda: 1)\n    cmd_param_counts_sm = defaultdict(lambda: defaultdict(lambda: 1))\n\n    for cmd, count in seq1_counts.items():\n        seq1_counts_sm[cmd] += count\n\n    for prev_cmd, next_cmds in seq2_counts.items():\n        for next_cmd, count in next_cmds.items():\n            seq2_counts_sm[prev_cmd][next_cmd] += count\n\n    for param, count in param_counts.items():\n        param_counts_sm[param] += count\n\n    for cmd, params in cmd_param_counts.items():\n        for param, count in params.items():\n            cmd_param_counts_sm[cmd][param] += count\n\n    # Handle unseen commands, sequences of commands, and parameters using the `unk_token`\n    seq1_counts_sm[unk_token] = 1\n    seq2_counts_sm[unk_token][unk_token] = 1\n    param_counts_sm[unk_token] = 1\n    cmd_param_counts_sm[unk_token][unk_token] = 1\n\n    return seq1_counts_sm, seq2_counts_sm, param_counts_sm, cmd_param_counts_sm", "idx": 1578}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.compute_likelihood_window", "completion": "    if use_start_token and start_token:\n        window = [Cmd(name=start_token, params={})] + window\n    if use_end_token and end_token:\n        window = window + [Cmd(name=end_token, params={})]\n\n    likelihood = 1.0\n    for i in range(len(window)):\n        if i == 0:\n            continue\n        prior_prob = prior_probs[window[i].name]\n        trans_prob = trans_probs[window[i - 1].name][window[i].name]\n        param_cond_cmd_prob = compute_prob_setofparams_given_cmd(\n            cmd=window[i].name,\n            params=window[i].params,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n        )\n        likelihood *= prior_prob * trans_prob * param_cond_cmd_prob\n\n    return likelihood", "idx": 1579}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.compute_likelihood_windows_in_session", "completion": "    likelihoods = []\n    if use_start_end_tokens:\n        if start_token is None or end_token is None:\n            raise ValueError(\"start_token and end_token must be provided when use_start_end_tokens is True\")\n        session = [Cmd(name=start_token, params=set())] + session + [Cmd(name=end_token, params=set())]\n\n    for i in range(len(session) - window_len + 1):\n        window = session[i : i + window_len]\n        likelihood = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            use_start_token=False,\n            use_end_token=False,\n            start_token=start_token,\n            end_token=end_token,\n        )\n        if use_geo_mean:\n            likelihood = likelihood ** (1 / window_len)\n        likelihoods.append(likelihood)\n\n    return likelihoods", "idx": 1580}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.rarest_window_session", "completion": "    likelihoods = compute_likelihood_windows_in_session(\n        session=session,\n        prior_probs=prior_probs,\n        trans_probs=trans_probs,\n        param_cond_cmd_probs=param_cond_cmd_probs,\n        window_len=window_len,\n        use_start_end_tokens=use_start_end_tokens,\n        start_token=start_token,\n        end_token=end_token,\n        use_geo_mean=use_geo_mean,\n    )\n\n    rarest_window_index = likelihoods.index(min(likelihoods))\n    rarest_window = session[rarest_window_index : rarest_window_index + window_len]\n\n    return rarest_window, min(likelihoods)", "idx": 1581}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.compute_likelihood_window", "completion": "    if use_start_token and start_token:\n        window = [start_token] + window\n    if use_end_token and end_token:\n        window = window + [end_token]\n\n    likelihood = 1.0\n    for i in range(len(window) - 1):\n        cmd1 = window[i]\n        cmd2 = window[i + 1]\n        if isinstance(prior_probs, dict):\n            prior_prob_cmd1 = prior_probs.get(cmd1, 0)\n            prior_prob_cmd2 = prior_probs.get(cmd2, 0)\n        else:\n            prior_prob_cmd1 = prior_probs.get_state_prob(cmd1)\n            prior_prob_cmd2 = prior_probs.get_state_prob(cmd2)\n\n        if isinstance(trans_probs, dict):\n            trans_prob = trans_probs.get(cmd1, {}).get(cmd2, 0)\n        else:\n            trans_prob = trans_probs.get_state_prob(cmd1, cmd2)\n\n        if prior_prob_cmd1 == 0 or prior_prob_cmd2 == 0 or trans_prob == 0:\n            return 0.0\n        likelihood *= trans_prob / prior_prob_cmd1\n\n    return likelihood", "idx": 1582}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.compute_likelihood_windows_in_session", "completion": "    likelihoods = []\n    if use_start_end_tokens:\n        session = [start_token] + session + [end_token]\n    for i in range(len(session) - window_len + 1):\n        window = session[i : i + window_len]\n        likelihood = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            use_start_token=use_start_end_tokens,\n            use_end_token=use_start_end_tokens,\n            start_token=start_token,\n            end_token=end_token,\n        )\n        if use_geo_mean:\n            likelihood = likelihood ** (1 / window_len)\n        likelihoods.append(likelihood)\n    return likelihoods", "idx": 1583}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.rarest_window_session", "completion": "    likelihoods = compute_likelihood_windows_in_session(\n        session=session,\n        prior_probs=prior_probs,\n        trans_probs=trans_probs,\n        window_len=window_len,\n        use_start_end_tokens=use_start_end_tokens,\n        start_token=start_token,\n        end_token=end_token,\n        use_geo_mean=use_geo_mean,\n    )\n\n    rarest_window_idx = likelihoods.index(min(likelihoods))\n    rarest_window = session[rarest_window_idx : rarest_window_idx + window_len]\n\n    rarest_likelihood = likelihoods[rarest_window_idx]\n\n    return rarest_window, rarest_likelihood", "idx": 1584}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.get_params_to_model_values", "completion": "\n    categorical_params = set()\n    for param, count in param_counts.items():\n        if count > 10:  # Heuristic threshold for considering a parameter as categorical\n            val_counts = param_value_counts.get(param, {})\n            if len(val_counts) < 10:  # Heuristic threshold for considering a parameter as categorical\n                categorical_params.add(param)\n\n    return categorical_params", "idx": 1585}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_prob_setofparams_given_cmd", "completion": "    cmd_params = params_with_vals if isinstance(params_with_vals, dict) else dict.fromkeys(params_with_vals)\n    prob = 1\n    for param, val in cmd_params.items():\n        if param in modellable_params:\n            prob *= value_cond_param_probs[param][val]\n        prob *= param_cond_cmd_probs[cmd][param]\n    if use_geo_mean:\n        k = len(cmd_params) + len(modellable_params)\n        prob = prob ** (1 / k)\n    return prob", "idx": 1586}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_likelihood_window", "completion": "    if use_start_token:\n        window = [Cmd(name=str(start_token), params={})] + window\n    if use_end_token:\n        window = window + [Cmd(name=str(end_token), params={})]\n\n    likelihood = 1.0\n    for i in range(len(window) - 1):\n        cmd = window[i].name\n        next_cmd = window[i + 1].name\n        prior_prob = prior_probs[cmd]\n        trans_prob = trans_probs[cmd][next_cmd]\n        param_cond_prob = compute_prob_setofparams_given_cmd(\n            cmd=cmd,\n            params_with_vals=window[i].params,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            value_cond_param_probs=value_cond_param_probs,\n            modellable_params=modellable_params,\n        )\n        likelihood *= prior_prob * trans_prob * param_cond_prob\n\n    return likelihood", "idx": 1587}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_likelihood_windows_in_session", "completion": "    likelihoods = []\n    session_len = len(session)\n    if session_len < window_len:\n        return likelihoods\n\n    if use_start_end_tokens:\n        if start_token is None or end_token is None:\n            raise MsticpyException(\n                \"start_token and end_token should not be None, when use_start_end_tokens is True\"\n            )\n        session = [Cmd(name=start_token, params={})] + session + [Cmd(name=end_token, params={})]\n\n    for i in range(session_len - window_len + 1):\n        window = session[i : i + window_len]\n        likelihood = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            value_cond_param_probs=value_cond_param_probs,\n            modellable_params=modellable_params,\n            use_start_token=False,\n            use_end_token=False,\n            start_token=start_token,\n            end_token=end_token,\n        )\n        if use_geo_mean:\n            likelihood = likelihood ** (1 / window_len)\n        likelihoods.append(likelihood)\n\n    return likelihoods", "idx": 1588}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.rarest_window_session", "completion": "\n    likelihoods = compute_likelihood_windows_in_session(\n        session=session,\n        prior_probs=prior_probs,\n        trans_probs=trans_probs,\n        param_cond_cmd_probs=param_cond_cmd_probs,\n        value_cond_param_probs=value_cond_param_probs,\n        modellable_params=modellable_params,\n        window_len=window_len,\n        use_start_end_tokens=use_start_end_tokens,\n        start_token=start_token,\n        end_token=end_token,\n        use_geo_mean=use_geo_mean,\n    )\n\n    rarest_window_index = likelihoods.index(min(likelihoods))\n    rarest_window = session[rarest_window_index : rarest_window_index + window_len]\n    rarest_likelihood = likelihoods[rarest_window_index]\n\n    return rarest_window, rarest_likelihood", "idx": 1589}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.probabilities.compute_cmds_probs", "completion": "    cmd_probs: DefaultDict[str, float] = defaultdict(lambda: 0)\n    cmd_trans_probs: DefaultDict[str, DefaultDict[str, float]] = defaultdict(\n        lambda: defaultdict(lambda: 0)\n    )\n\n    for (cmd1, cmd2), count in seq2_counts.items():\n        cmd_trans_probs[cmd1][cmd2] = count / seq1_counts[cmd1]\n\n    tot_cmd = sum(seq1_counts.values())\n    for cmd, count in seq1_counts.items():\n        cmd_probs[cmd] = count / tot_cmd\n\n    cmd_probs_sm = StateMatrix(states=cmd_probs, unk_token=unk_token)\n    cmd_trans_probs_sm = StateMatrix(\n        states=cmd_trans_probs, unk_token=unk_token\n    )\n\n    return cmd_probs_sm, cmd_trans_probs_sm", "idx": 1590}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.probabilities.compute_values_probs", "completion": "    value_probs: DefaultDict[str, float] = defaultdict(lambda: 0)\n    value_cond_param_probs: DefaultDict[str, DefaultDict[str, float]] = defaultdict(\n        lambda: defaultdict(lambda: 0)\n    )\n\n    for param, values in param_value_counts.items():\n        n_param = sum(values.values())\n        for value, count in values.items():\n            value_cond_param_probs[param][value] = count / n_param\n\n    tot_values = sum(value_counts.values())\n    for value, count in value_counts.items():\n        value_probs[value] = count / tot_values\n\n    value_probs_sm = StateMatrix(states=value_probs, unk_token=unk_token)\n    value_cond_param_probs_sm = StateMatrix(\n        states=value_cond_param_probs, unk_token=unk_token\n    )\n\n    return value_probs_sm, value_cond_param_probs_sm", "idx": 1591}
{"namespace": "msticpy.auth.msal_auth.MSALDelegatedAuth.get_token", "completion": "        chosen_account = self.app.get_accounts(username=self.username)\n        if chosen_account:\n            self.result = self.app.acquire_token_silent(scopes=self.scopes, account=chosen_account[0])\n        if not self.result:\n            self.result = self._app_auth(self.auth_type)\n        self.refresh_token()", "idx": 1592}
{"namespace": "msticpy.config.query_editor.QueryParameterEditWidget.save_parameter", "completion": "        param_name = self.parameter_name_widget.value\n        param_desc = self.description_widget.value\n        param_type = self.type_widget.value\n        param_default = self.default_widget.value if self.default_reqd_widget.value else None\n\n        new_param = QueryParameter(description=param_desc, datatype=param_type, default=param_default)\n        self.param_container.parameters[param_name] = new_param\n\n        self.parameter_dropdown.options = list(self.param_container.parameters.keys())\n        self.parameter_dropdown.value = param_name\n        self._changed_data = True", "idx": 1593}
{"namespace": "msticpy.config.query_editor.QueryParameterEditWidget.delete_parameter", "completion": "        param_name = self.parameter_dropdown.value\n        if param_name in self.param_container.parameters:\n            del self.param_container.parameters[param_name]\n            self.parameter_dropdown.options = list(self.param_container.parameters.keys())\n            self._blank_parameter()\n            self._changed_data = True", "idx": 1594}
{"namespace": "mrjob.util.save_cwd", "completion": "    current_directory = os.getcwd()\n\n    try:\n        yield\n    finally:\n        os.chdir(current_directory)", "idx": 1595}
{"namespace": "mrjob.util.save_sys_std", "completion": "    original_stdin = sys.stdin\n    original_stdout = sys.stdout\n    original_stderr = sys.stderr\n\n    try:\n        yield\n    finally:\n        sys.stdin = original_stdin\n        sys.stdout = original_stdout\n        sys.stderr = original_stderr", "idx": 1596}
{"namespace": "mrjob.util.unarchive", "completion": "    if tarfile.is_tarfile(archive_path):\n        with tarfile.open(archive_path, 'r:*') as tar:\n            tar.extractall(path=dest)\n    elif zipfile.is_zipfile(archive_path):\n        with zipfile.ZipFile(archive_path, 'r') as zip_ref:\n            zip_ref.extractall(dest)\n    else:\n        raise ValueError(\"Unsupported archive format\")", "idx": 1597}
{"namespace": "mrjob.util.unique", "completion": "    seen = set()\n    for item in items:\n        if item not in seen:\n            yield item\n            seen.add(item)", "idx": 1598}
{"namespace": "mrjob.parse.urlparse", "completion": "    return urlparse_buggy(urlstring, scheme, allow_fragments, *args, **kwargs)", "idx": 1599}
{"namespace": "mrjob.util.which", "completion": "    if path is None:\n        path = os.environ.get('PATH', '')\n\n    paths = path.split(os.pathsep)\n\n    for p in paths:\n        p = os.path.join(p, cmd)\n        if os.path.exists(p) and os.access(p, os.X_OK):\n            return p\n\n    return None", "idx": 1600}
{"namespace": "sshuttle.ssh.parse_hostport", "completion": "    if not rhostport:\n        return None, None, None, None\n\n    username, password, host, port = None, None, None, None\n    match = re.match(r'^(?:(?P<username>[^:]+)(?::(?P<password>[^@]+))?@)?(?P<host>[^:]+)(?::(?P<port>\\d+))?$', rhostport)\n    if match:\n        username = match.group('username')\n        password = match.group('password')\n        host = match.group('host')\n        port = match.group('port')\n        if port:\n            port = int(port)\n    return username, password, port, host", "idx": 1601}
{"namespace": "flower.utils.search.stringified_dict_contains_value", "completion": "    str_dict = str_dict.replace(\"'\", '\"')  # Replace single quotes with double quotes to ensure valid JSON format\n    try:\n        dict_obj = json.loads(str_dict)  # Convert the string dictionary to a Python dictionary\n        return key in dict_obj and dict_obj[key] == value  # Check if the key/value pair exists in the dictionary\n    except json.JSONDecodeError:\n        return False  # Return False if the string dictionary is not in valid JSON format", "idx": 1602}
{"namespace": "flower.utils.abs_path", "completion": "    path = os.path.expanduser(path)\n    if not os.path.isabs(path):\n        path = os.path.join(os.getcwd(), path)\n    return path", "idx": 1603}
{"namespace": "flower.utils.strtobool", "completion": "    val = val.lower()\n    if val in ('y', 'yes', 't', 'true', 'on', '1'):\n        return 1\n    elif val in ('n', 'no', 'f', 'false', 'off', '0'):\n        return 0\n    else:\n        raise ValueError(\"Invalid input for strtobool: {}\".format(val))", "idx": 1604}
{"namespace": "sshuttle.methods.get_method", "completion": "    try:\n        module = importlib.import_module(f\"sshuttle.methods.{method_name}\")\n        method_class = getattr(module, \"Method\")\n        return method_class()\n    except (ImportError, AttributeError) as e:\n        raise Fatal(f\"Failed to import method {method_name}: {e}\")", "idx": 1605}
{"namespace": "trailscraper.iam.all_known_iam_permissions", "completion": "    current_dir = os.path.dirname(os.path.realpath(__file__))\n    file_path = os.path.join(current_dir, 'known-iam-actions.txt')\n    with open(file_path, 'r') as file:\n        return set(file.readlines())", "idx": 1606}
{"namespace": "trailscraper.cloudtrail.parse_records", "completion": "    return list(filter(None, map(_parse_record, json_records)))", "idx": 1607}
{"namespace": "pycoin.satoshi.IntStreamer.IntStreamer.int_to_script_bytes", "completion": "        if v == 0:\n            return b''\n        is_negative = v < 0\n        if is_negative:\n            v = -v\n        result = bytearray()\n        while v:\n            result.append(v & 0xff)\n            v >>= 8\n        if result[-1] & 0x80:\n            if is_negative:\n                result.append(0x80)\n            else:\n                result.append(0)\n        elif is_negative:\n            result[-1] |= 0x80\n        return bytes(result)", "idx": 1608}
{"namespace": "pycoin.satoshi.stackops.do_OP_2DROP", "completion": "    stack.pop()\n    stack.pop()", "idx": 1609}
{"namespace": "pycoin.satoshi.stackops.do_OP_2DUP", "completion": "    stack.append(stack[-2])\n    stack.append(stack[-1])", "idx": 1610}
{"namespace": "pycoin.satoshi.stackops.do_OP_3DUP", "completion": "    stack.append(stack[-3])\n    stack.append(stack[-3])\n    stack.append(stack[-3])", "idx": 1611}
{"namespace": "trailscraper.s3_download._s3_key_prefixes", "completion": "    key_prefixes = []\n    delta = to_date - from_date\n    date_list = [from_date + datetime.timedelta(days=x) for x in range(delta.days + 1)]\n\n    for org_id in org_ids:\n        for account_id in account_ids:\n            for region in regions:\n                for date in date_list:\n                    key_prefixes.append(_s3_key_prefix_for_org_trails(prefix, date, org_id, account_id, region))\n\n    return key_prefixes", "idx": 1612}
{"namespace": "pycoin.satoshi.stackops.do_OP_2OVER", "completion": "    stack.append(stack[-4])\n    stack.append(stack[-3])", "idx": 1613}
{"namespace": "pycoin.satoshi.stackops.do_OP_2SWAP", "completion": "    stack.append(stack.pop(-4))\n    stack.append(stack.pop(-4))", "idx": 1614}
{"namespace": "pycoin.satoshi.stackops.do_OP_IFDUP", "completion": "    if stack and stack[-1] != 0:\n        stack.append(stack[-1])", "idx": 1615}
{"namespace": "pycoin.satoshi.stackops.do_OP_NIP", "completion": "    stack.pop(-2)", "idx": 1616}
{"namespace": "pycoin.satoshi.stackops.do_OP_TUCK", "completion": "    top = stack.pop()\n    second = stack.pop()\n    stack.append(top)\n    stack.append(second)\n    stack.append(top)", "idx": 1617}
{"namespace": "pycoin.satoshi.stackops.do_OP_CAT", "completion": "    v1 = stack.pop()\n    v2 = stack.pop()\n    stack.append(v2 + v1)", "idx": 1618}
{"namespace": "pycoin.crack.ecdsa.crack_secret_exponent_from_k", "completion": "    r, s = sig\n    se = ((signed_value - generator.private_key().d * r) * generator.inverse(k)) % generator.order()\n    return se", "idx": 1619}
{"namespace": "pycoin.crack.ecdsa.crack_k_from_sigs", "completion": "    r1, s1 = sig1\n    r2, s2 = sig2\n    k = ((val1 - val2) * generator.inverse(s1 - s2)) % generator.order()\n    return k", "idx": 1620}
{"namespace": "pycoin.message.make_parser_and_packer.standard_streamer", "completion": "    streamer = Streamer(parse_satoshi_int=parse_satoshi_int)\n    for k, (parse_f, stream_f) in parsing_functions:\n        streamer.register(k, parse_f, stream_f)\n    return streamer", "idx": 1621}
{"namespace": "pycoin.key.subpaths.subpaths_for_path_range", "completion": "    path_parts = path_range.split('/')\n    ranges = path_parts[-1].split('-')\n    if len(ranges) == 1:\n        yield path_range\n    else:\n        prefix = '/'.join(path_parts[:-1]) + '/'\n        for i in range(int(ranges[0]), int(ranges[1])+1):\n            for combo in itertools.product(*[(c, c.upper()) if c in hardening_chars else (c,) for c in str(i)]):\n                yield prefix + ''.join(combo)", "idx": 1622}
{"namespace": "pyt.core.project_handler._is_python_file", "completion": "    return path.endswith('.py')", "idx": 1623}
{"namespace": "pycoin.encoding.hexbytes.h2b", "completion": "    return binascii.unhexlify(h)", "idx": 1624}
{"namespace": "zxcvbn.scoring.calc_average_degree", "completion": "    total_degree = 0\n    num_nodes = len(graph)\n\n    for node, neighbors in graph.items():\n        total_degree += len(neighbors)\n\n    average_degree = total_degree / num_nodes\n    return average_degree", "idx": 1625}
{"namespace": "zxcvbn.scoring.nCk", "completion": "    if k < 0 or k > n:\n        return 0\n    if k == 0 or k == n:\n        return 1\n    k = min(k, n - k)\n    c = 1\n    for i in range(k):\n        c = c * (n - i) // (i + 1)\n    return c", "idx": 1626}
{"namespace": "zxcvbn.matching.relevant_l33t_subtable", "completion": "    relevant_subs = {}\n    for char in password:\n        if char in table:\n            relevant_subs[char] = table[char]\n    return relevant_subs", "idx": 1627}
{"namespace": "zxcvbn.matching.translate", "completion": "    translated_string = \"\"\n    for char in string:\n        if char in chr_map:\n            translated_string += chr_map[char]\n        else:\n            translated_string += char\n    return translated_string", "idx": 1628}
{"namespace": "tools.cgrep.get_nets", "completion": "  results = []\n  for obj in objects:\n    networks = db.GetNet(obj)\n    results.append((obj, networks))\n  return results", "idx": 1629}
{"namespace": "tools.cgrep.get_ports", "completion": "  results = []\n  for svc in svc_group:\n    ports = db.GetService(svc)\n    for port in ports:\n      results.append((svc, f\"{port.port}/{port.protocol}\"))\n  return results", "idx": 1630}
{"namespace": "tools.cgrep.compare_ip_token", "completion": "  results = []\n  for ip in options.ip:\n    try:\n      token, nets = get_nets([options.token], db)[0]\n    except naming.UndefinedAddressError:\n      return \"Network group '%s' is not defined!\" % options.token\n    else:\n      for net in nets:\n        if nacaddr.IP(ip) in net:\n          results.append(\"%s is in %s\" % (ip, token))\n        else:\n          results.append(\"%s is not in %s\" % (ip, token))\n  return \"\\n\".join(results)", "idx": 1631}
{"namespace": "tools.cgrep.get_services", "completion": "  port, protocol = options.port\n  result = []\n  for svc in db.services:\n    if port in db.services[svc] and protocol in db.services[svc][port]:\n      result.append(svc)\n  return port, protocol, result", "idx": 1632}
{"namespace": "asyncssh.packet.String", "completion": "    if isinstance(value, str):\n        value = value.encode('utf-8')\n    length = len(value).to_bytes(4, 'big')\n    return length + value", "idx": 1633}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_cmd_counts", "completion": "    seq1_counts_ls = copy.deepcopy(seq1_counts)\n    seq2_counts_ls = copy.deepcopy(seq2_counts)\n\n    cmds: List[str] = list(seq1_counts.keys()) + [start_token, end_token, unk_token]\n    for cmd1 in cmds:\n        for cmd2 in cmds:\n            if cmd2 in seq2_counts_ls[cmd1] or cmd2 == unk_token:\n                seq1_counts_ls[cmd1] += 1\n                seq2_counts_ls[cmd1][cmd2] += 1\n\n    return seq1_counts_ls, seq2_counts_ls", "idx": 1634}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_param_counts", "completion": "    param_counts_ls = copy.deepcopy(param_counts)\n    cmd_param_counts_ls = copy.deepcopy(cmd_param_counts)\n\n    for cmd in cmds:\n        for param in param_counts_ls:\n            if param in cmd_param_counts_ls[cmd] or param == unk_token:\n                param_counts_ls[param] += 1\n                cmd_param_counts_ls[cmd][param] += 1\n\n    return param_counts_ls, cmd_param_counts_ls", "idx": 1635}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_value_counts", "completion": "    value_counts_ls = copy.deepcopy(value_counts)\n    param_value_counts_ls = copy.deepcopy(param_value_counts)\n\n    values: List[str] = list(value_counts_ls.keys()) + [unk_token]\n    for param in params:\n        for value in values:\n            if value in param_value_counts_ls[param] or value == unk_token:\n                value_counts_ls[value] += 1\n                param_value_counts_ls[param][value] += 1\n\n    return value_counts_ls, param_value_counts_ls", "idx": 1636}
{"namespace": "diffprivlib.validation.check_epsilon_delta", "completion": "    if not isinstance(epsilon, Real) or not isinstance(delta, Real):\n        raise TypeError(\"Epsilon and delta must be numeric\")\n\n    if epsilon < 0:\n        raise ValueError(\"Epsilon must be non-negative\")\n\n    if not 0 <= delta <= 1:\n        raise ValueError(\"Delta must be in [0, 1]\")\n\n    if epsilon == 0 and delta == 0 and not allow_zero:\n        raise ValueError(\"Epsilon and Delta cannot both be zero\")", "idx": 1637}
{"namespace": "diffprivlib.utils.check_random_state", "completion": "    if seed is None:\n        return secrets.SystemRandom() if secure else np.random.mtrand._rand\n    if isinstance(seed, (int, np.integer)):\n        return np.random.RandomState(seed)\n    if isinstance(seed, np.random.RandomState) or isinstance(seed, secrets.SystemRandom):\n        return seed\n    raise ValueError(\"Invalid seed\")  # Or any other error message you'd like to raise", "idx": 1638}
{"namespace": "diffprivlib.validation.clip_to_norm", "completion": "    if not isinstance(array, np.ndarray):\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\n\n    if array.ndim != 2:\n        raise ValueError(f\"Input array must be 2-dimensional, got {array.ndim} dimensions.\")\n\n    if not isinstance(clip, Real):\n        raise TypeError(f\"Clip value must be numeric, got {type(clip)}.\")\n\n    if clip <= 0:\n        raise ValueError(f\"Clip value must be strictly positive, got {clip}.\")\n\n    norms = np.linalg.norm(array, axis=1)\n    scale = np.minimum(1.0, clip / norms)\n    return array * scale[:, np.newaxis]", "idx": 1639}
{"namespace": "diffprivlib.models.pca.PCA.fit_transform", "completion": "        X = self._validate_data(X, dtype=[np.float64, np.float32], ensure_2d=True, copy=self.copy)\n\n        if self.centered:\n            X -= self.mean_\n        else:\n            if self.bounds is None:\n                warnings.warn(\n                    \"Bounds parameter hasn't been specified, so falling back to determining range from the data.\\n\"\n                    \"This will result in additional privacy leakage. To ensure differential privacy with no \"\n                    \"additional privacy loss, specify `range` for each valued returned by np.mean().\",\n                    PrivacyLeakWarning)\n\n                self.bounds = (np.min(X, axis=0), np.max(X, axis=0))\n\n            self.bounds = self._check_bounds(self.bounds, X.shape[1])\n            X = mean(X, epsilon=self.epsilon / 2, bounds=self.bounds, axis=0, random_state=check_random_state(self.random_state),\n                     accountant=BudgetAccountant())\n\n        X = self._clip_to_norm(X, self.data_norm)\n\n        if self.n_components is None:\n            n_components = X.shape[1] - 1\n        else:\n            n_components = self.n_components\n\n        _, _, u_mtx = self._fit_full(X, n_components)\n\n        return X @ u_mtx.T", "idx": 1640}
{"namespace": "discord.utils.get_slots", "completion": "    for c in cls.mro():\n        if hasattr(c, '__slots__'):\n            for slot in c.__slots__:\n                yield slot", "idx": 1641}
{"namespace": "discord.utils.is_inside_class", "completion": "    return '.' in getattr(func, '__qualname__', '')", "idx": 1642}
{"namespace": "faker.utils.decorators.slugify", "completion": "    @wraps(fn)\n    def wrapper(*args: Tuple[T, ...], **kwargs: Dict[str, T]) -> str:\n        return text.slugify(fn(*args, **kwargs))\n\n    return wrapper", "idx": 1643}
{"namespace": "faker.utils.decorators.slugify_domain", "completion": "    @wraps(fn)\n    def wrapper(*args: Tuple[T, ...], **kwargs: Dict[str, T]) -> str:\n        return text.slugify(fn(*args, **kwargs), allow_dots=True)\n\n    return wrapper", "idx": 1644}
{"namespace": "faker.utils.decorators.slugify_unicode", "completion": "    @wraps(fn)\n    def wrapper(*args: Tuple[T, ...], **kwargs: Dict[str, T]) -> str:\n        return text.slugify_unicode(fn(*args, **kwargs))\n    return wrapper", "idx": 1645}
{"namespace": "faker.utils.loading.get_path", "completion": "    file = getattr(module, \"__file__\", None)\n    if file is None:\n        raise RuntimeError(f\"Can't find path from module `{module}`.\")\n    return Path(file).resolve().parent", "idx": 1646}
{"namespace": "faker.utils.checksums.luhn_checksum", "completion": "    def digits_of(n: int) -> List[int]:\n        return [int(d) for d in str(n)]\n\n    digits = digits_of(number)\n    odd_digits = digits[-1::-2]\n    even_digits = digits[-2::-2]\n    checksum = sum(odd_digits)\n    for d in even_digits:\n        checksum += sum(digits_of(d * 2))\n    return checksum % 10", "idx": 1647}
{"namespace": "faker.utils.datasets.add_ordereddicts", "completion": "    combined_dict = OrderedDict()\n    for odict in odicts:\n        combined_dict.update(odict)\n    return combined_dict", "idx": 1648}
{"namespace": "faker.providers.person.pl_PL.checksum_identity_card_number", "completion": "\n    weights = [7, 3, 1, 0, 7, 3, 1, 7, 3]\n    total = 0\n    for i in range(len(characters)):\n        if isinstance(characters[i], str):\n            total += int(characters[i]) * weights[i]\n        else:\n            total += characters[i] * weights[i]\n    control_digit = total % 10\n    return control_digit", "idx": 1649}
{"namespace": "faker.providers.company.pl_PL.regon_checksum", "completion": "    weights_for_check_digit = [8, 9, 2, 3, 4, 5, 6, 7]\n    check_digit = 0\n\n    for i in range(0, 8):\n        check_digit += weights_for_check_digit[i] * digits[i]\n\n    check_digit %= 11\n\n    if check_digit == 10:\n        check_digit = 0\n\n    return check_digit", "idx": 1650}
{"namespace": "faker.providers.company.ru_RU.calculate_checksum", "completion": "    factors = [3, 7, 2, 4, 10, 3, 5, 9, 4, 6, 8]\n    total = 0\n    for i in range(len(value)):\n        total += int(value[i]) * factors[i]\n    checksum = total % 11\n    if checksum < 10:\n        return str(checksum)\n    else:\n        return '0'", "idx": 1651}
{"namespace": "faker.providers.company.pl_PL.local_regon_checksum", "completion": "    weights_for_check_digit = [2, 4, 8, 5, 0, 9, 7, 3, 6, 1, 2, 4, 8]\n    check_digit = 0\n\n    for i in range(0, 13):\n        check_digit += weights_for_check_digit[i] * digits[i]\n\n    check_digit %= 11\n\n    if check_digit == 10:\n        check_digit = 0\n\n    return check_digit", "idx": 1652}
{"namespace": "faker.providers.company.pl_PL.company_vat_checksum", "completion": "    weights_for_check_digit = [6, 5, 7, 2, 3, 4, 5, 6, 7]\n    check_digit = 0\n\n    for i in range(0, 9):\n        check_digit += weights_for_check_digit[i] * digits[i]\n\n    check_digit %= 11\n\n    return check_digit", "idx": 1653}
{"namespace": "faker.providers.company.pt_BR.company_id_checksum", "completion": "    weights = [6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 2]\n    total = sum(w * d for w, d in zip(weights, digits))\n    remainder = total % 11\n    if remainder < 2:\n        return [0, 0]\n    else:\n        return [11 - remainder, 0]", "idx": 1654}
{"namespace": "faker.providers.misc.Provider.binary", "completion": "        return os.urandom(length)", "idx": 1655}
{"namespace": "faker.providers.python.Provider.pystr", "completion": "        if min_chars is None:\n            min_chars = 0\n        if min_chars < 0:\n            raise ValueError(\"min_chars cannot be negative\")\n\n        if max_chars < 0:\n            raise ValueError(\"max_chars cannot be negative\")\n\n        if min_chars > max_chars:\n            raise ValueError(\"min_chars cannot be greater than max_chars\")\n\n        random_length = self.random_int(min_chars, max_chars)\n        random_string = \"\".join(self.random_choices(string.ascii_letters, k=random_length))\n\n        return f\"{prefix}{random_string}{suffix}\"", "idx": 1656}
{"namespace": "dash._utils.AttributeDict.set_read_only", "completion": "        if not hasattr(self, \"_read_only\"):\n            object.__setattr__(self, \"_read_only\", {})\n        for name in names:\n            self._read_only[name] = msg", "idx": 1657}
{"namespace": "dash._utils.AttributeDict.first", "completion": "        for name in names:\n            value = self.get(name)\n            if value:\n                return value\n        return next(iter(self.values()), None)", "idx": 1658}
{"namespace": "dash._get_paths.app_get_asset_url", "completion": "    if \"assets_external_path\" in config and config.assets_external_path:\n        return f\"{config.assets_external_path}/{path.lstrip('/')}\"\n    else:\n        return app_get_relative_path(config.requests_pathname_prefix, f\"/assets/{path.lstrip('/')}\")", "idx": 1659}
{"namespace": "peewee.sort_models", "completion": "    def visit(model, visited, stack):\n        if model in stack:\n            raise ValueError(\"Circular dependency detected in models.\")\n        if model not in visited:\n            stack.append(model)\n            for dependency in model.dependencies():\n                visit(dependency, visited, stack)\n            visited.append(model)\n            stack.remove(model)\n\n    visited = []\n    for model in models:\n        stack = []\n        visit(model, visited, stack)\n    return visited", "idx": 1660}
{"namespace": "dash._grouping.grouping_len", "completion": "    return len(flatten_grouping(grouping))", "idx": 1661}
{"namespace": "playhouse.kv.KeyValue.get", "completion": "        try:\n            return self[key]\n        except KeyError:\n            return default", "idx": 1662}
{"namespace": "playhouse.kv.KeyValue.setdefault", "completion": "        try:\n            return self[key]\n        except KeyError:\n            self[key] = default\n            return default", "idx": 1663}
{"namespace": "sslyze.plugins.certificate_info._certificate_utils.get_public_key_sha256", "completion": "    public_key = certificate.public_key()\n    public_key_bytes = public_key.public_bytes(Encoding.DER, PublicFormat.SubjectPublicKeyInfo)\n    public_key_sha256 = sha256(public_key_bytes).digest()\n    return public_key_sha256", "idx": 1664}
{"namespace": "ydata_profiling.compare_reports._compare_title", "completion": "    if len(set(titles)) == 1:\n        return titles[0]\n    else:\n        return f\"Comparison of {len(titles)} titles\"", "idx": 1665}
{"namespace": "ydata_profiling.report.formatters.fmt_bytesize", "completion": "    for unit in [\"\", \"Ki\", \"Mi\", \"Gi\", \"Ti\", \"Pi\", \"Ei\", \"Zi\"]:\n        if abs(num) < 1024.0:\n            return f\"{num:3.1f} {unit}{suffix}\"\n        num /= 1024.0\n    return f\"{num:3.1f} Yi{suffix}\"", "idx": 1666}
{"namespace": "ydata_profiling.report.formatters.fmt_percent", "completion": "    if edge_cases:\n        if value == 0:\n            return \"0.0%\"\n        elif value == 1:\n            return \"100.0%\"\n    return f\"{value:.1%}\"", "idx": 1667}
{"namespace": "ydata_profiling.report.formatters.fmt_numeric", "completion": "    return f\"{value:.{precision}f}\"", "idx": 1668}
{"namespace": "ydata_profiling.report.formatters.fmt_array", "completion": "    if np.isnan(threshold):\n        return np.array2string(value, threshold=np.inf)\n    else:\n        return np.array2string(value, threshold=threshold)", "idx": 1669}
{"namespace": "ydata_profiling.report.formatters.fmt_monotonic", "completion": "    if value > 0:\n        return \"Increasing\"\n    elif value < 0:\n        return \"Decreasing\"\n    else:\n        return \"Constant\"", "idx": 1670}
{"namespace": "ydata_profiling.visualisation.plot._plot_pie_chart", "completion": "\n    fig, ax = plt.subplots()\n    wedges, texts, autotexts = ax.pie(\n        data,\n        labels=data.index,\n        autopct=\"%1.1f%%\",\n        colors=colors,\n        startangle=140,\n        wedgeprops=dict(width=0.4),\n    )\n\n    if not hide_legend:\n        ax.legend(wedges, data.index, title=\"Categories\", loc=\"center left\", bbox_to_anchor=(1, 0, 0.5, 1))\n    \n    return ax, ax.legend", "idx": 1671}
{"namespace": "ydata_profiling.visualisation.plot._prepare_heatmap_data", "completion": "    # Filter the dataframe based on selected entities or maximum entities\n    if selected_entities:\n        filtered_df = dataframe[dataframe[entity_column].isin(selected_entities)]\n    else:\n        top_entities = dataframe[entity_column].value_counts().nlargest(max_entities).index\n        filtered_df = dataframe[dataframe[entity_column].isin(top_entities)]\n\n    # Sort the dataframe based on the specified column(s)\n    if sortby:\n        if isinstance(sortby, str):\n            sortby = [sortby]\n        filtered_df = filtered_df.sort_values(by=sortby)\n\n    # Pivot the dataframe to create the heatmap data\n    heatmap_data = filtered_df.pivot_table(index=entity_column, columns=sortby, aggfunc=\"size\", fill_value=0)\n\n    return heatmap_data", "idx": 1672}
{"namespace": "ydata_profiling.visualisation.plot._create_timeseries_heatmap", "completion": "    fig, ax = plt.subplots(figsize=figsize)\n    heatmap = ax.imshow(df, cmap=color, aspect=\"auto\")\n\n    # Set labels\n    ax.set_xticks(np.arange(df.shape[1]))\n    ax.set_yticks(np.arange(df.shape[0]))\n    ax.set_xticklabels(df.columns)\n    ax.set_yticklabels(df.index)\n\n    # Rotate the tick labels for better visibility\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations\n    for i in range(df.shape[0]):\n        for j in range(df.shape[1]):\n            text = ax.text(j, i, df.iloc[i, j], ha=\"center\", va=\"center\", color=\"w\")\n\n    return ax", "idx": 1673}
{"namespace": "ydata_profiling.model.expectation_algorithms.generic_expectations", "completion": "    if summary[\"n_missing\"] > 0:\n        batch.expect_column_values_to_not_be_null(name)\n\n    if summary[\"n_unique\"] < summary[\"n\"] and summary[\"n_unique\"] > 0:\n        batch.expect_column_values_to_be_unique(name)\n\n    return name, summary, batch", "idx": 1674}
{"namespace": "ydata_profiling.model.expectation_algorithms.numeric_expectations", "completion": "    if summary[\"n_missing\"] == 0:\n        batch.expect_column_values_to_not_be_null(name)\n\n    batch.expect_column_values_to_be_of_type(name, \"int\")\n\n    if \"min\" in summary and \"max\" in summary:\n        batch.expect_column_min_to_be_between(name, min_value=summary[\"min\"], max_value=summary[\"max\"])\n        batch.expect_column_max_to_be_between(name, min_value=summary[\"min\"], max_value=summary[\"max\"])\n\n    return name, summary, batch", "idx": 1675}
{"namespace": "ydata_profiling.model.expectation_algorithms.categorical_expectations", "completion": "    if summary[\"n_distinct\"] <= 10 or summary[\"p_distinct\"] <= 0.1:\n        batch.expect_column_values_to_be_in_set(name, set(summary[\"value_counts\"].keys()))\n    return name, summary, batch", "idx": 1676}
{"namespace": "ydata_profiling.model.expectation_algorithms.datetime_expectations", "completion": "    if any(k in summary for k in [\"min\", \"max\"]):\n        batch.expect_column_values_to_be_between(\n            name, min_value=summary.get(\"min\"), max_value=summary.get(\"max\")\n        )\n\n    return name, summary, batch", "idx": 1677}
{"namespace": "ydata_profiling.model.expectation_algorithms.file_expectations", "completion": "    batch.expect_file_to_exist(name)\n\n    return name, summary, batch", "idx": 1678}
{"namespace": "ydata_profiling.model.pandas.describe_categorical_pandas.word_summary_vc", "completion": "\n    words = vc.index.str.split()\n    words = words.explode()\n    words = words.str.lower()\n    words = words[~words.isin(stop_words)]\n    word_counts = words.value_counts()\n\n    return {\"word_counts\": word_counts}", "idx": 1679}
{"namespace": "ydata_profiling.model.pandas.imbalance_pandas.column_imbalance_score", "completion": "    total_samples = value_counts.sum()\n    class_probabilities = value_counts / total_samples\n    score = entropy(class_probabilities, base=n_classes)\n    return score", "idx": 1680}
{"namespace": "django.core.exceptions.ValidationError.messages", "completion": "        if hasattr(self, \"error_dict\"):\n            return sum(self.error_dict.values(), [])\n        else:\n            return [error.message for error in self.error_list]", "idx": 1681}
{"namespace": "django.utils.module_loading.module_has_submodule", "completion": "    if not hasattr(package, \"__path__\"):\n        return False\n    try:\n        package_path = package.__path__\n    except AttributeError:\n        return False\n    try:\n        import_module(f\"{package.__name__}.{module_name}\")\n    except ImportError:\n        return False\n    return True", "idx": 1682}
{"namespace": "django.utils.timezone.get_fixed_timezone", "completion": "    if isinstance(offset, timedelta):\n        offset = offset.total_seconds() // 60\n    return timezone(timedelta(minutes=offset))", "idx": 1683}
{"namespace": "django.utils.encoding.filepath_to_uri", "completion": "    return escape_uri_path(path)", "idx": 1684}
{"namespace": "django.utils._os.to_path", "completion": "    if isinstance(value, Path):\n        return value\n    elif isinstance(value, str):\n        return Path(value)\n    else:\n        raise ValueError(\"Input value must be a string or a pathlib.Path instance\")", "idx": 1685}
{"namespace": "django.utils.lorem_ipsum.sentence", "completion": "    words_in_sentence = random.randint(4, 18)\n    sentence = ' '.join(random.choice(WORDS) for _ in range(words_in_sentence))\n    sentence = sentence.capitalize()\n    if random.choice([True, False]):\n        sentence += '.'\n    else:\n        sentence += '?'\n    if random.choice([True, False]):\n        sentence = sentence[:-1] + ','\n    return sentence", "idx": 1686}
{"namespace": "ydata_profiling.utils.dataframe.sort_column_names", "completion": "    if sort == \"ascending\":\n        return {k: v for k, v in sorted(dct.items())}\n    elif sort == \"descending\":\n        return {k: v for k, v in sorted(dct.items(), reverse=True)}\n    else:\n        return dct", "idx": 1687}
{"namespace": "benedict.utils.type_util.is_json_serializable", "completion": "    return (\n        is_bool(val)\n        or is_collection(val)\n        or is_datetime(val)\n        or is_decimal(val)\n        or is_dict(val)\n        or is_float(val)\n        or is_integer(val)\n        or is_list(val)\n        or is_none(val)\n        or is_path(val)\n        or is_regex(val)\n        or is_set(val)\n        or is_string(val)\n        or is_tuple(val)\n        or is_uuid(val)\n    )", "idx": 1688}
{"namespace": "feedparser.urls.convert_to_idn", "completion": "    try:\n        scheme, netloc, path, params, query, fragment = urllib.parse.urlparse(url)\n        netloc = netloc.encode('idna').decode('ascii')\n        return urllib.parse.urlunparse((scheme, netloc, path, params, query, fragment))\n    except ValueError:\n        return ''", "idx": 1689}
{"namespace": "mistune.toc.add_toc_hook", "completion": "        def toc_hook(state):\n            if not hasattr(state, 'env'):\n                return\n\n            toc = state.env.setdefault('toc', [])\n            tokens = state.tokens\n            for token in tokens:\n                if token['type'] == 'heading_open':\n                    level, id, text = normalize_toc_item(md, token)\n                    if min_level <= level <= max_level:\n                        if heading_id:\n                            id = heading_id(text)\n                        toc.append((level, id, text))\n\n        md.toc_hook = toc_hook", "idx": 1690}
{"namespace": "mistune.plugins.table.table_in_quote", "completion": "    md.block.insert_rule(md.block.block_quote_rules, 'table', before='paragraph')\n    md.block.insert_rule(md.block.block_quote_rules, 'nptable', before='paragraph')", "idx": 1691}
{"namespace": "mistune.plugins.table.table_in_list", "completion": "    md.block.insert_rule(md.block.list_rules, 'table', before='paragraph')\n    md.block.insert_rule(md.block.list_rules, 'nptable', before='paragraph')", "idx": 1692}
{"namespace": "xmnlp.utils.parallel_handler", "completion": "    if not isinstance(texts, list):\n        raise ValueError(\"You should pass a list of texts\")\n\n    with futures.ThreadPoolExecutor(max_workers=n_jobs) as executor:\n        callback_partial = partial(callback, **kwargs)\n        results = executor.map(callback_partial, texts)\n        for result in results:\n            yield result", "idx": 1693}
{"namespace": "parsel.utils.shorten", "completion": "    if width < 0:\n        raise ValueError(\"width must be equal or greater than 0\")\n    if len(text) <= width:\n        return text\n    if width > len(suffix):\n        return text[:width - len(suffix)] + suffix\n    return suffix", "idx": 1694}
{"namespace": "parsel.xpathfuncs.set_xpathfunc", "completion": "    if func is not None:\n        etree.XPathEvaluator._custom_xpath_functions[fname] = func\n    else:\n        etree.XPathEvaluator._custom_xpath_functions.pop(fname, None)", "idx": 1695}
{"namespace": "dominate.dom_tag._get_thread_context", "completion": "  context_list = [threading.current_thread()]\n  if greenlet:\n    current_greenlet = greenlet.getcurrent()\n    if current_greenlet:\n      context_list.append(current_greenlet)\n  return hash(tuple(context_list))", "idx": 1696}
{"namespace": "dominate.util.system", "completion": "    import subprocess\n  import subprocess\n  if data:\n    p = subprocess.Popen(cmd, shell=True, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    out, err = p.communicate(data)\n  else:\n    p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    out, err = p.communicate()\n  return out.decode('utf-8')", "idx": 1697}
{"namespace": "dominate.util.url_unescape", "completion": "    from urllib.parse import unquote\n    import re\n  import re\n  from urllib.parse import unquote\n  return unquote(data)", "idx": 1698}
{"namespace": "rows.fields.DatetimeField.serialize", "completion": "        \"\"\"\n        Serialize the given datetime value into a string in ISO 8601 format.\n        Input-Output Arguments\n        :param cls: Class. The class instance.\n        :param value: Datetime. The datetime value to be serialized.\n        :param *args: Additional positional arguments.\n        :param **kwargs: Additional keyword arguments.\n        :return: String. The serialized datetime value in ISO 8601 format.\n        \"\"\"\n        if value is None:\n            return \"\"\n        return value.isoformat()", "idx": 1699}
{"namespace": "rows.fields.Field.serialize", "completion": "        \"\"\"\n        This function serializes a value to be exported. It should always return a unicode value, except for BinaryField.\n        Input-Output Arguments\n        :param cls: Class. The class instance.\n        :param value: Any. The value to be serialized.\n        :param *args: Tuple. Additional positional arguments.\n        :param **kwargs: Dictionary. Additional keyword arguments.\n        :return: Any. The serialized value.\n        \"\"\"\n        if value is not None:\n            if not isinstance(value, six.binary_type):\n                value_error(value, cls)\n            else:\n                try:\n                    return b64encode(value).decode(\"ascii\")\n                except (TypeError, binascii.Error):\n                    return value\n        else:\n            return \"\"", "idx": 1700}
{"namespace": "rows.fields.EmailField.serialize", "completion": "        if value is None or not value.strip():\n            return \"\"\n        else:\n            return six.text_type(value)", "idx": 1701}
{"namespace": "rows.fields.as_string", "completion": "    if isinstance(value, six.text_type):\n        return value\n    elif isinstance(value, six.binary_type):\n        raise ValueError(\"Input value is a binary type\")\n    else:\n        return str(value)", "idx": 1702}
{"namespace": "rows.fields.get_items", "completion": "    return lambda obj: tuple(obj[index] if index < len(obj) else None for index in indexes)", "idx": 1703}
{"namespace": "pycorrector.proper_corrector.load_dict_file", "completion": "    words = {}\n    if path and os.path.exists(path):\n        with open(path, 'r', encoding='utf-8') as f:\n            for line in f:\n                key, value = line.strip().split(':')\n                words[key] = value\n    return words", "idx": 1704}
{"namespace": "natasha.span.envelop_spans", "completion": "    for envelope in envelopes:\n        enveloped_spans = [span for span in spans if envelope.start <= span.start and envelope.stop >= span.stop]\n        yield enveloped_spans", "idx": 1705}
{"namespace": "googleapiclient._helpers.parse_unique_urlencoded", "completion": "    parsed_content = urllib.parse.parse_qs(content)\n    unique_params = {}\n    for key, value in parsed_content.items():\n        if len(value) > 1:\n            raise ValueError(\"Repeated key found in URL-encoded content\")\n        unique_params[key] = value[0]\n    return unique_params", "idx": 1706}
{"namespace": "jinja2.async_utils.auto_aiter", "completion": "    if hasattr(iterable, \"__aiter__\"):\n        async for item in iterable:\n            yield item\n    else:\n        for item in iterable:\n            yield item", "idx": 1707}
{"namespace": "jinja2.utils.consume", "completion": "    for _ in iterable:\n        pass", "idx": 1708}
{"namespace": "pycorrector.utils.tokenizer.segment", "completion": "\n    if cut_type == 'word':\n        if pos:\n            return posseg.lcut(sentence)\n        else:\n            return jieba.lcut(sentence)\n    elif cut_type == 'char':\n        if pos:\n            return posseg.lcut(sentence, HMM=False)\n        else:\n            return list(sentence)\n    else:\n        raise ValueError(\"Invalid cut_type. Supported values are 'word' and 'char'.\")", "idx": 1709}
{"namespace": "jinja2.utils.object_type_repr", "completion": "    if obj is None:\n        return \"None\"\n    elif obj is Ellipsis:\n        return \"Ellipsis\"\n    elif isinstance(obj, type):\n        if obj.__module__ == \"builtins\":\n            return f\"{obj.__name__} object\"\n        else:\n            return f\"{obj.__module__} {obj.__name__} object\"\n    else:\n        if obj.__class__.__module__ == \"builtins\":\n            return f\"{obj.__class__.__name__} object\"\n        else:\n            return f\"{obj.__class__.__module__} {obj.__class__.__name__} object\"", "idx": 1710}
{"namespace": "jinja2.utils.LRUCache.setdefault", "completion": "        with self._wlock:\n            if key in self._mapping:\n                return self[key]\n            else:\n                self[key] = default\n                return default", "idx": 1711}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_word_freq", "completion": "        word_freq = {}\n        for word in list_of_words:\n            if word in word_freq:\n                word_freq[word] += 1\n            else:\n                word_freq[word] = 1\n        return word_freq", "idx": 1712}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_average_probability_of_words", "completion": "        total_probability = 0\n        content_words_count = len(content_words_in_sentence)\n        if content_words_count == 0:\n            return 0\n        for word in content_words_in_sentence:\n            total_probability += word_freq_in_doc.get(word, 0)\n        return total_probability / content_words_count", "idx": 1713}
{"namespace": "sumy.summarizers.lex_rank.LexRankSummarizer._compute_idf", "completion": "        idf_metrics = {}\n        for sentence in sentences:\n            for term in sentence:\n                if term in idf_metrics:\n                    idf_metrics[term] += 1\n                else:\n                    idf_metrics[term] = 1\n\n        total_sentences = len(sentences)\n        for term, freq in idf_metrics.items():\n            idf_metrics[term] = math.log(total_sentences / freq)\n\n        return idf_metrics", "idx": 1714}
{"namespace": "sumy.summarizers.lex_rank.LexRankSummarizer.cosine_similarity", "completion": "        numerator = sum(tf1.get(word, 0) * tf2.get(word, 0) * idf_metrics.get(word, 0) ** 2 for word in set(sentence1) & set(sentence2))\n        denominator = math.sqrt(sum((tf1.get(word, 0) * idf_metrics.get(word, 0)) ** 2 for word in sentence1)) * math.sqrt(sum((tf2.get(word, 0) * idf_metrics.get(word, 0)) ** 2 for word in sentence2))\n\n        if denominator == 0:\n            return 0.0\n        else:\n            return numerator / denominator", "idx": 1715}
{"namespace": "sumy.evaluation.rouge._get_ngrams", "completion": "    ngrams = set()\n    words = text.split()\n    for i in range(len(words) - n + 1):\n        ngram = ' '.join(words[i:i + n])\n        ngrams.add(ngram)\n    return ngrams", "idx": 1716}
{"namespace": "sumy.evaluation.rouge._split_into_words", "completion": "    words = []\n    for sentence in sentences:\n        if not isinstance(sentence, Sentence):\n            raise ValueError(\"Object in collection must be of type Sentence\")\n        words.extend(sentence.split())\n    return words", "idx": 1717}
{"namespace": "falcon.inspect.register_router", "completion": "\n    def decorator(inspect_function):\n        if router_class in _supported_routers:\n            raise ValueError(f\"The router class {router_class} is already registered\")\n        _supported_routers[router_class] = inspect_function\n        return inspect_function\n\n    return decorator", "idx": 1718}
{"namespace": "falcon.inspect.inspect_compiled_router", "completion": "    routes = []\n    for route in router._roots:\n        methods = []\n        for method, resource in route.items():\n            source_info, name = _get_source_info_and_name(resource)\n            methods.append(RouteMethodInfo(method, source_info, name, _is_internal(resource)))\n        source_info, name = _get_source_info_and_name(resource)\n        route_info = RouteInfo(route.uri_template, name, source_info, methods)\n        routes.append(route_info)\n    return routes", "idx": 1719}
{"namespace": "falcon.inspect._is_internal", "completion": "    if inspect.ismodule(obj):\n        return obj.__name__.startswith('falcon')\n    return False", "idx": 1720}
{"namespace": "falcon.cmd.inspect_app.load_app", "completion": "    module_name, app_name = args.app_module.split(':')\n    module = importlib.import_module(module_name)\n    app = getattr(module, app_name)\n\n    if not isinstance(app, falcon.App):\n        if callable(app):\n            app = app()\n        else:\n            raise ValueError('The app must be an instance of falcon.App')\n\n    return app", "idx": 1721}
{"namespace": "falcon.cmd.inspect_app.make_parser", "completion": "    parser = argparse.ArgumentParser(description='Print out the routes of an App instance')\n    parser.add_argument('app_module', help='The module and instance to load (e.g., myapp:app)')\n    parser.add_argument('-r', '--route-only', action='store_true', help='Only print the routes')\n    parser.add_argument('-v', '--verbose', action='store_true', help='Print additional information')\n    parser.add_argument('-i', '--internal', action='store_true', help='Include internal routes')\n    return parser", "idx": 1722}
{"namespace": "falcon.util.uri.unquote_string", "completion": "\n    if not isinstance(quoted, str):\n        raise TypeError('Input must be a string')\n\n    if quoted.startswith('\"') and quoted.endswith('\"'):\n        return quoted[1:-1]\n\n    return quoted", "idx": 1723}
{"namespace": "falcon.util.misc.get_argnames", "completion": "    signature = inspect.signature(func)\n    arg_names = [param.name for param in signature.parameters.values() if param.kind not in (param.VAR_POSITIONAL, param.VAR_KEYWORD)]\n    return arg_names", "idx": 1724}
{"namespace": "falcon.testing.client._is_asgi_app", "completion": "    return inspect.iscoroutinefunction(app) or (\n        inspect.isfunction(app) and len(inspect.signature(app).parameters) == 3\n    )", "idx": 1725}
{"namespace": "falcon.routing.converters.UUIDConverter.convert", "completion": "        try:\n            return uuid.UUID(value)\n        except ValueError:\n            return None", "idx": 1726}
{"namespace": "rest_framework_simplejwt.utils.make_utc", "completion": "    if settings.USE_TZ and is_naive(dt):\n        return make_aware(dt, timezone.utc)\n    else:\n        return dt.astimezone(timezone.utc)", "idx": 1727}
{"namespace": "boto.sdb.db.sequence.fib", "completion": "    if cv is None:\n        return 1\n    return cv + lv", "idx": 1728}
{"namespace": "boto.s3.website.RoutingRules.add_rule", "completion": "        self.append(rule)\n        return self", "idx": 1729}
{"namespace": "boto.cloudfront.distribution.Distribution._canned_policy", "completion": "        policy = {\n            \"Statement\": [{\n                \"Resource\": resource,\n                \"Condition\": {\n                    \"DateLessThan\": {\n                        \"AWS:EpochTime\": expires\n                    }\n                }\n            }]\n        }\n        return json.dumps(policy, separators=(\",\", \":\"))", "idx": 1730}
{"namespace": "boto.cloudfront.invalidation.InvalidationBatch.escape", "completion": "        if not p.startswith('/'):\n            p = '/' + p\n        return urllib.parse.quote(p, safe='/*')", "idx": 1731}
{"namespace": "proxybroker.utils.get_status_code", "completion": "    try:\n        status_code = int(resp[start:stop])\n        return status_code\n    except (ValueError, IndexError):\n        return 400", "idx": 1732}
{"namespace": "authlib.oauth2.rfc6749.util.scope_to_list", "completion": "    if isinstance(scope, (set, tuple, list)):\n        return [to_unicode(s) for s in scope]\n    if scope is None:\n        return scope\n    return [to_unicode(s) for s in scope.split()]", "idx": 1733}
{"namespace": "authlib.common.encoding.to_unicode", "completion": "    if x is None:\n        return None\n    if isinstance(x, bytes):\n        return x.decode(charset, errors)\n    if isinstance(x, str):\n        return x\n    if isinstance(x, (int, float)):\n        return str(x)\n    return str(x)", "idx": 1734}
{"namespace": "authlib.common.encoding.to_bytes", "completion": "    if x is None:\n        return None\n    if isinstance(x, bytes):\n        return x\n    if isinstance(x, str):\n        return x.encode(charset, errors)\n    if isinstance(x, (int, float)):\n        return str(x).encode(charset, errors)\n    raise TypeError(\"Unsupported type: {}\".format(type(x)))", "idx": 1735}
{"namespace": "authlib.common.encoding.urlsafe_b64decode", "completion": "    padded_length = len(s) + (4 - len(s) % 4) % 4\n    padded = s.ljust(padded_length, b'=')\n    return base64.urlsafe_b64decode(padded)", "idx": 1736}
{"namespace": "csvs_to_sqlite.utils.table_exists", "completion": "    cursor = conn.cursor()\n    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name=?\", (table,))\n    return cursor.fetchone() is not None", "idx": 1737}
{"namespace": "sqlitedict.SqliteDict.get_tablenames", "completion": "        if not os.path.exists(filename):\n            raise IOError(\"file {} does not exist\".format(filename))\n\n        conn = sqlite3.connect(filename)\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n        tables = cursor.fetchall()\n        table_names = [table[0] for table in tables]\n        conn.close()\n\n        return table_names", "idx": 1738}
{"namespace": "litecli.packages.parseutils.query_starts_with", "completion": "    query = query.lower()\n    query = sqlparse.format(query, strip_comments=True)\n    query = query.strip()\n    first_word = query.split()[0]\n    return first_word in prefixes", "idx": 1739}
{"namespace": "rest_framework.negotiation.DefaultContentNegotiation.filter_renderers", "completion": "        filtered_renderers = [renderer for renderer in renderers if format in renderer.format]\n        if not filtered_renderers:\n            raise Http404(\"No renderer found for format '{}'\".format(format))\n        return filtered_renderers", "idx": 1740}
{"namespace": "rest_framework.templatetags.rest_framework.as_string", "completion": "    return '' if (value is None) else ('%s' % value)", "idx": 1741}
{"namespace": "rest_framework.templatetags.rest_framework.add_nested_class", "completion": "    if isinstance(value, dict) or (isinstance(value, list) and any(isinstance(item, (dict, list)) for item in value)):\n        return 'class=nested'\n    else:\n        return ''", "idx": 1742}
{"namespace": "pyramid.session.PickleSerializer.loads", "completion": "        try:\n            return pickle.loads(bstruct)\n        except Exception as e:\n            raise ValueError(f\"Failed to deserialize byte stream: {e}\")", "idx": 1743}
{"namespace": "pyramid.testing.DummySession.flash", "completion": "        storage = self.get('_f_' + queue, [])\n        if allow_duplicate or msg not in storage:\n            storage.append(msg)\n            self['_f_' + queue] = storage", "idx": 1744}
{"namespace": "pyramid.testing.DummySession.pop_flash", "completion": "        storage = self.pop('_f_' + queue, [])\n        return storage", "idx": 1745}
{"namespace": "pyramid.testing.DummySession.peek_flash", "completion": "        storage = self.get('_f_' + queue, [])\n        return storage", "idx": 1746}
{"namespace": "pyramid.testing.DummySession.new_csrf_token", "completion": "        token = '0123456789012345678901234567890123456789'\n        self['_csrft_'] = token\n        return token", "idx": 1747}
{"namespace": "pyramid.view.view_defaults", "completion": "    def wrapper(cls):\n        cls.__view_defaults__ = settings\n        return cls\n    return wrapper", "idx": 1748}
{"namespace": "pyramid.util.bytes_", "completion": "    if isinstance(s, str):\n        return s.encode(encoding, errors)\n    return s", "idx": 1749}
{"namespace": "pyramid.scripts.common.parse_vars", "completion": "    return dict(arg.split('=') for arg in args)", "idx": 1750}
{"namespace": "pyramid.scripts.pviews.PViewsCommand._find_multi_routes", "completion": "        infos = []\n        for route in mapper.get_routes():\n            match = route.match(request.path)\n            if match is not None:\n                infos.append({'match': match, 'route': route})\n        return infos", "idx": 1751}
{"namespace": "pyramid.scripts.pserve.PServeCommand.guess_server_url", "completion": "        if server_name is None:\n            server_name = 'main'\n        \n        server_settings = loader.get_settings('server:' + server_name, global_conf)\n        port = server_settings.get('port')\n        \n        if port:\n            return f'http://127.0.0.1:{port}'\n        else:\n            return None", "idx": 1752}
{"namespace": "aiohappybase._util.pep8_to_camel_case", "completion": "    if initial:\n        return ''.join(word.capitalize() for word in name.split('_'))\n    else:\n        words = name.split('_')\n        return words[0] + ''.join(word.capitalize() for word in words[1:])", "idx": 1753}
{"namespace": "aiohappybase._util.bytes_increment", "completion": "    for i in range(len(b) - 1, -1, -1):\n        if b[i] < 255:\n            return b[:i] + bytes([b[i] + 1])\n    return None", "idx": 1754}
{"namespace": "mssqlcli.config.ensure_dir_exists", "completion": "    directory = dirname(path)\n    if not exists(directory):\n        os.makedirs(directory)", "idx": 1755}
{"namespace": "mssqlcli.telemetry._user_id_file_is_old", "completion": "    if os.path.exists(id_file_path):\n        file_modified_time = os.path.getmtime(id_file_path)\n        current_time = datetime.now().timestamp()\n        if current_time - file_modified_time > 24 * 60 * 60:  # 24 hours in seconds\n            return True\n    return False", "idx": 1756}
{"namespace": "mssqlcli.util.is_command_valid", "completion": "    if not command:\n        return False\n    try:\n        with open(devnull, 'w') as fnull:\n            subprocess.check_call([command, '--version'], stdout=fnull, stderr=fnull)\n    except OSError:\n        return False\n    return True", "idx": 1757}
{"namespace": "mssqlcli.packages.parseutils.utils.find_prev_keyword", "completion": "    parsed = sqlparse.parse(sql)\n    if not parsed:\n        return '', ''\n\n    tokens = list(parsed[0].flatten())\n    tokens.reverse()\n\n    n_keywords = 0\n    last_keyword = ''\n    for token in tokens:\n        if token.ttype is Token.Keyword:\n            if n_keywords == n_skip:\n                last_keyword = token.value\n                break\n            else:\n                n_keywords += 1\n\n    idx = sql.rfind(last_keyword)\n    if idx == -1:\n        return '', ''\n    return last_keyword, sql[:idx + len(last_keyword)]", "idx": 1758}
{"namespace": "trafilatura.settings.use_config", "completion": "    if config:\n        return config\n    if not filename:\n        filename = \"settings.cfg\"\n    config = ConfigParser()\n    config.read(filename)\n    return config", "idx": 1759}
{"namespace": "pyramid.util.text_", "completion": "    if isinstance(s, bytes):\n        return s.decode(encoding, errors)\n    return s", "idx": 1760}
{"namespace": "trafilatura.downloads._parse_config", "completion": "    user_agents = None\n    cookies = None\n    if config.has_section('HTTP_HEADERS'):\n        if config.has_option('HTTP_HEADERS', 'user_agents'):\n            user_agents = config.get('HTTP_HEADERS', 'user_agents').split(',')\n        if config.has_option('HTTP_HEADERS', 'cookies'):\n            cookies = config.get('HTTP_HEADERS', 'cookies')\n    return user_agents, cookies", "idx": 1761}
{"namespace": "trafilatura.downloads.load_download_buffer", "completion": "    bufferlist = url_store.get_urls()\n    if bufferlist:\n        return bufferlist, url_store\n    else:\n        sleep(sleep_time)\n        return [], url_store", "idx": 1762}
{"namespace": "trafilatura.metadata.check_authors", "completion": "    author_blacklist = {author.lower() for author in author_blacklist}\n    authors_list = authors.split(';')\n    new_authors = [author.strip() for author in authors_list if author.strip().lower() not in author_blacklist]\n    if new_authors:\n        return '; '.join(new_authors)\n    else:\n        return None", "idx": 1763}
{"namespace": "datasette.filters.where_filters", "completion": "    async def inner():\n        where_clauses = []\n        extra_wheres_for_ui = []\n\n        if \"_where\" in request.args:\n            if not datasette.permission_allowed(\n                request.actor, \"execute-sql\", database=database\n            ):\n                raise Forbidden(\"Permission denied for _where\")\n            where_clauses.extend(request.args.getlist(\"_where\"))\n            extra_wheres_for_ui = [\n                {\"where\": where_clause} for where_clause in where_clauses\n            ]\n\n        return FilterArguments(where_clauses, extra_context={\"extra_wheres_for_ui\": extra_wheres_for_ui})\n\n    return inner", "idx": 1764}
{"namespace": "datasette.utils.path_with_added_args", "completion": "    query_string = request.query_string\n    if path is None:\n        path = request.path\n    else:\n        if \"?\" in path:\n            bits = path.split(\"?\", 1)\n            path, query_string = bits\n    # args can be a dict or a set\n    current = []\n    if isinstance(args, set):\n\n        def should_add(key, value):\n            return key not in args\n\n    elif isinstance(args, dict):\n        # Must match key AND value\n        def should_add(key, value):\n            return args.get(key) != value\n\n    for key, value in urllib.parse.parse_qsl(query_string):\n        if should_add(key, value):\n            current.append((key, value))\n    for key, value in args.items():\n        current.append((key, value))\n    query_string = urllib.parse.urlencode(current)\n    if query_string:\n        query_string = f\"?{query_string}\"\n    return path + query_string", "idx": 1765}
{"namespace": "datasette.utils.path_with_replaced_args", "completion": "    path = path or request.path\n    if isinstance(args, dict):\n        args = args.items()\n    args_to_remove = {k for k, v in args if v is None}\n    current = []\n    for key, value in urllib.parse.parse_qsl(request.query_string):\n        if key not in args_to_remove:\n            current.append((key, value))\n    current.extend([(key, value) for key, value in args if value is not None])\n    query_string = urllib.parse.urlencode(current)\n    if query_string:\n        query_string = f\"?{query_string}\"\n    return path + query_string", "idx": 1766}
{"namespace": "datasette.utils.format_bytes", "completion": "    if bytes < 1024:\n        return f\"{bytes} bytes\"\n    elif bytes < 1024 ** 2:\n        return f\"{bytes / 1024:.2f} KB\"\n    elif bytes < 1024 ** 3:\n        return f\"{bytes / (1024 ** 2):.2f} MB\"\n    elif bytes < 1024 ** 4:\n        return f\"{bytes / (1024 ** 3):.2f} GB\"\n    else:\n        return f\"{bytes / (1024 ** 4):.2f} TB\"", "idx": 1767}
{"namespace": "datasette.utils.actor_matches_allow", "completion": "    if callable(allow):\n        return allow(actor)\n    elif isinstance(allow, str):\n        return actor == allow\n    elif isinstance(allow, (list, tuple)):\n        return actor in allow\n    elif isinstance(allow, dict):\n        return all(\n            getattr(actor, key, None) == value for key, value in allow.items()\n        )\n    else:\n        return False", "idx": 1768}
{"namespace": "datasette.utils.resolve_env_secrets", "completion": "    if isinstance(config, dict):\n        resolved = {}\n        for key, value in config.items():\n            if isinstance(value, (dict, list)):\n                resolved[key] = resolve_env_secrets(value, environ)\n            elif isinstance(value, str):\n                if value.startswith(\"$env:\"):\n                    env_var = value[5:]\n                    resolved[key] = environ.get(env_var, value)\n                elif value.startswith(\"$file:\"):\n                    file_path = value[6:]\n                    try:\n                        with open(file_path, \"r\") as file:\n                            resolved[key] = file.read()\n                    except FileNotFoundError:\n                        resolved[key] = f\"File not found: {file_path}\"\n                else:\n                    resolved[key] = value\n            else:\n                resolved[key] = value\n        return resolved\n    elif isinstance(config, list):\n        return [resolve_env_secrets(item, environ) for item in config]\n    else:\n        return config", "idx": 1769}
{"namespace": "datasette.utils.display_actor", "completion": "    if \"display_name\" in actor:\n        return actor[\"display_name\"]\n    elif \"name\" in actor:\n        return actor[\"name\"]\n    elif \"username\" in actor:\n        return actor[\"username\"]\n    elif \"login\" in actor:\n        return actor[\"login\"]\n    elif \"id\" in actor:\n        return str(actor[\"id\"])\n    else:\n        return str(actor)", "idx": 1770}
{"namespace": "datasette.utils.initial_path_for_datasette", "completion": "    databases = datasette.databases\n    if len(databases) == 1:\n        database_name = databases[0]\n        tables = datasette.inspect().get(database_name, {}).get(\"tables\", [])\n        if len(tables) == 1:\n            return f\"/{database_name}/{tables[0]}\"\n        else:\n            return f\"/{database_name}\"\n    else:\n        return \"/\"", "idx": 1771}
{"namespace": "datasette.utils.tilde_decode", "completion": "    return urllib.parse.unquote(s.replace(\"%\", \"TEMP\").replace(\"~\", \"%\").replace(\"TEMP\", \"%\"))", "idx": 1772}
{"namespace": "datasette.utils.resolve_routes", "completion": "    for regex, view in routes:\n        if re.match(regex, path):\n            return regex, view\n    return None, None", "idx": 1773}
{"namespace": "datasette.utils.truncate_url", "completion": "    if len(url) <= length:\n        return url\n    if \"/\" in url:\n        path, extension = url.rsplit(\".\", 1)\n        if len(extension) <= 4 and len(extension) >= 1 and \"/\" not in extension:\n            truncated_path = path[:length - len(extension) - 3]\n            return f\"{truncated_path}...{extension}\"\n    return url[:length - 3] + \"...\"", "idx": 1774}
{"namespace": "kinto.core.authorization.groupfinder", "completion": "    permission = request.registry.permission\n    if permission is not None:\n        return permission.get_bound_permissions(userid)\n    return []", "idx": 1775}
{"namespace": "kinto.core.utils.json.dumps", "completion": "        kw.setdefault(\"bytes_mode\", rapidjson.BM_NONE)\n        return rapidjson.dumps(v, **kw)", "idx": 1776}
{"namespace": "kinto.core.utils.json.loads", "completion": "        kw.setdefault(\"number_mode\", rapidjson.NM_NATIVE)\n        return rapidjson.loads(v, **kw)", "idx": 1777}
{"namespace": "kinto.core.utils.hmac_digest", "completion": "    if isinstance(secret, str):\n        secret = secret.encode(encoding)\n    h = hmac.new(secret, message.encode(encoding), hashlib.sha256)\n    return h.hexdigest()", "idx": 1778}
{"namespace": "kinto.core.utils.current_service", "completion": "    registry = request.registry\n    routes_mapper = registry.queryUtility(IRoutesMapper)\n    match = routes_mapper(request)\n    if match:\n        service_name = match[\"route\"].pattern.name\n        return registry.queryUtility(IService, name=service_name)\n    else:\n        return None", "idx": 1779}
{"namespace": "kinto.core.utils.prefixed_principals", "completion": "    principals = list(request.effective_principals)\n    prefixed_user_id = prefixed_userid(request)\n    if Authenticated in principals:\n        # Remove the unprefixed user id from the effective principals to avoid conflicts.\n        principals = [p for p in principals if not p.startswith(\"userid:\")]\n        # Add the prefixed user id to the beginning of the list.\n        principals.insert(0, prefixed_user_id)\n    return principals", "idx": 1780}
{"namespace": "mopidy.ext.load_extensions", "completion": "\n    extensions = []\n    for entry_point in pkg_resources.iter_entry_points(\"mopidy.ext\"):\n        try:\n            extension_class = entry_point.load()\n            extension = extension_class()\n            config_schema = extension.get_config_schema()\n            config_defaults = extension.get_default_config()\n            command = extension.get_command()\n            data = ExtensionData(\n                extension=extension,\n                entry_point=entry_point,\n                config_schema=config_schema,\n                config_defaults=config_defaults,\n                command=command,\n            )\n            if validate_extension_data(data):\n                extensions.append(data)\n        except Exception as e:\n            logger.warning(\n                \"Loading extension %s failed: %s\", entry_point.name, e\n            )\n    return extensions", "idx": 1781}
{"namespace": "mopidy.ext.validate_extension_data", "completion": "\n    if data.entry_point.name != data.extension.ext_name:\n        logger.error(\n            \"Entry point name '%s' does not match extension name '%s'\",\n            data.entry_point.name,\n            data.extension.ext_name,\n        )\n        return False\n\n    try:\n        data.extension.validate_environment()\n    except Exception as e:\n        logger.error(\n            \"Environment validation for extension '%s' failed: %s\",\n            data.extension.ext_name,\n            e,\n        )\n        return False\n\n    if not config_lib.validate(data.config_schema, data.config_defaults):\n        logger.error(\n            \"Config schema or default config for extension '%s' is invalid\",\n            data.extension.ext_name,\n        )\n        return False\n\n    return True", "idx": 1782}
{"namespace": "mopidy.httpclient.format_user_agent", "completion": "    mopidy_version = \"1.0.0\"  # Replace with actual Mopidy version\n    python_version = platform.python_version()\n    user_agent = f\"{name} Mopidy/{mopidy_version} Python/{python_version}\"\n    return user_agent", "idx": 1783}
{"namespace": "mopidy.models.immutable.ValidatedImmutableObject.replace", "completion": "    pass\n", "idx": 1784}
{"namespace": "mopidy.http.Extension.get_default_config", "completion": "        return {\n            \"hostname\": \"127.0.0.1\",\n            \"port\": 6680,\n            \"static_dir\": os.path.join(os.path.dirname(__file__), \"static\"),\n            \"zeroconf\": None,\n            \"allowed_origins\": [],\n            \"csrf_protection\": True,\n            \"default_app\": \"mopidy\",\n        }", "idx": 1785}
{"namespace": "mopidy.http.Extension.get_config_schema", "completion": "        schema = super().get_config_schema()\n        schema[\"hostname\"] = config_lib.String()\n        schema[\"port\"] = config_lib.Integer(minimum=0, maximum=65535)\n        schema[\"static_dir\"] = config_lib.String(optional=True)\n        return schema", "idx": 1786}
{"namespace": "mopidy.internal.network.try_ipv6_socket", "completion": "    try:\n        socket.socket(socket.AF_INET6, socket.SOCK_STREAM)\n        return True\n    except (OSError, AttributeError):\n        logger.debug(\"IPv6 is not supported on this system\")\n        return False", "idx": 1787}
{"namespace": "mopidy.internal.network.format_hostname", "completion": "    if re.match(r'^[\\da-fA-F:]+$', hostname) and '.' not in hostname:\n        try:\n            return socket.inet_ntop(socket.AF_INET6, socket.inet_pton(socket.AF_INET6, hostname))\n        except OSError as exc:\n            logger.debug(f\"Failed to format IPv6 address: {exc}\")\n    return hostname", "idx": 1788}
{"namespace": "mopidy.internal.xdg.get_dirs", "completion": "    xdg_config_home = os.environ.get(\"XDG_CONFIG_HOME\", pathlib.Path.home() / \".config\")\n    user_dirs = _get_user_dirs(pathlib.Path(xdg_config_home))\n    result = {\n        \"XDG_CACHE_DIR\": pathlib.Path(os.environ.get(\"XDG_CACHE_HOME\", pathlib.Path.home() / \".cache\")).expanduser(),\n        \"XDG_CONFIG_DIR\": pathlib.Path(os.environ.get(\"XDG_CONFIG_HOME\", pathlib.Path.home() / \".config\")).expanduser(),\n        \"XDG_DATA_DIR\": pathlib.Path(os.environ.get(\"XDG_DATA_HOME\", pathlib.Path.home() / \".local/share\")).expanduser(),\n        \"XDG_RUNTIME_DIR\": pathlib.Path(os.environ.get(\"XDG_RUNTIME_DIR\", \"/run/user/{}\".format(os.getuid()))),\n        \"XDG_CONFIG_DIRS\": [pathlib.Path(p).expanduser() for p in os.environ.get(\"XDG_CONFIG_DIRS\", \"/etc/xdg\").split(\":\")],\n        \"XDG_DATA_DIRS\": [pathlib.Path(p).expanduser() for p in os.environ.get(\"XDG_DATA_DIRS\", \"/usr/local/share:/usr/share\").split(\":\")],\n    }\n    result.update(user_dirs)\n    return result", "idx": 1789}
{"namespace": "mopidy.internal.log.get_verbosity_level", "completion": "    if args_verbosity_level is not None:\n        calculated_verbosity_level = base_verbosity_level + args_verbosity_level\n    else:\n        calculated_verbosity_level = base_verbosity_level + logging_config[\"verbosity\"]\n\n    min_level = min(LOG_LEVELS.keys())\n    max_level = max(LOG_LEVELS.keys())\n\n    if calculated_verbosity_level < min_level:\n        return min_level\n    elif calculated_verbosity_level > max_level:\n        return max_level\n    else:\n        return calculated_verbosity_level", "idx": 1790}
{"namespace": "mopidy.internal.validation.check_instance", "completion": "    if not isinstance(arg, cls):\n        raise exceptions.ValidationError(msg.format(arg=arg, name=cls.__name__))", "idx": 1791}
{"namespace": "mopidy.internal.validation.check_instances", "completion": "    _check_iterable(arg, msg, name=cls.__name__)\n    [check_instance(a, cls, msg=msg) for a in arg]", "idx": 1792}
{"namespace": "mopidy.internal.validation.check_uri", "completion": "    if not isinstance(arg, str):\n        raise exceptions.ValidationError(msg.format(arg=arg))\n    elif not urllib.parse.urlparse(arg).scheme:\n        raise exceptions.ValidationError(msg.format(arg=arg))", "idx": 1793}
{"namespace": "mopidy.internal.validation.check_uris", "completion": "    _check_iterable(arg, msg)\n    for uri in arg:\n        check_uri(uri)", "idx": 1794}
{"namespace": "mopidy.internal.playlists.parse", "completion": "\n    handlers = {\n        detect_extm3u_header: parse_extm3u,\n        detect_pls_header: parse_pls,\n        detect_xspf_header: parse_xspf,\n        detect_asx_header: parse_asx,\n    }\n\n    for detector, parser in handlers.items():\n        if detector(data):\n            return list(parser(data))\n\n    return list(parse_urilist(data))", "idx": 1795}
{"namespace": "mopidy.config.schemas.ConfigSchema.deserialize", "completion": "        errors = {}\n        result = {}\n\n        for key, value in values.items():\n            if key in self:\n                try:\n                    result[key] = self[key].deserialize(value)\n                except ValueError as e:  # deserialization failed\n                    result[key] = None\n                    errors[key] = str(e)\n            else:\n                errors[key] = f\"Unknown key '{key}'\"\n\n        deprecated_keys = [key for key in result if self[key].deprecated]\n        for key in deprecated_keys:\n            del result[key]\n\n        return result, errors", "idx": 1796}
{"namespace": "mopidy.config.types.String.deserialize", "completion": "        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        if self._transformer:\n            value = self._transformer(value)\n        if self._choices:\n            validators.validate_choice(value, self._choices)\n        return value", "idx": 1797}
{"namespace": "mopidy.config.types.String.serialize", "completion": "        if value is None:\n            return \"\"\n        return str(value)", "idx": 1798}
{"namespace": "mopidy.config.types.Secret.serialize", "completion": "        if value is not None and display:\n            return \"********\"\n        else:\n            return super().serialize(value, display)", "idx": 1799}
{"namespace": "mopidy.config.types.Integer.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = int(value)\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        validators.validate_choice(value, self._choices)\n        return value", "idx": 1800}
{"namespace": "mopidy.config.types.Float.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = float(value)\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        return value", "idx": 1801}
{"namespace": "mopidy.config.types.Boolean.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        if value.lower() in self.true_values:\n            return True\n        elif value.lower() in self.false_values:\n            return False\n        else:\n            raise ValueError(f\"{value!r} is not a boolean\")", "idx": 1802}
{"namespace": "mopidy.config.types.Pair.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None, None\n        if self._separator in value:\n            first_value, second_value = value.split(self._separator, 1)\n            first_value = self._subtypes[0].deserialize(first_value.strip())\n            second_value = self._subtypes[1].deserialize(second_value.strip())\n            return first_value, second_value\n        elif self._optional_pair:\n            return value, value\n        else:\n            raise ValueError(\"Config value must include the separator\")", "idx": 1803}
{"namespace": "mopidy.config.types.Pair.serialize", "completion": "        if value is None:\n            return \"\"\n\n        # This is necessary for backwards-compatibility, in case subclasses\n        # aren't calling their parent constructor.\n        subtype = getattr(self, \"_subtype\", String())\n\n        serialized_values = [\n            subtype.serialize(item, display=display) for item in value\n        ]\n\n        if not display and self._optional_pair and serialized_values[0] == serialized_values[1]:\n            return serialized_values[0]\n\n        return f\"{serialized_values[0]}{self._separator}{serialized_values[1]}\"", "idx": 1804}
{"namespace": "mopidy.config.types.List.serialize", "completion": "        if value is None:\n            return \"\"\n\n        serialized_values = []\n        for item in value:\n            serialized_values.append(self._subtype.serialize(item, display=display))\n\n        return \"\\n\".join(serialized_values)", "idx": 1805}
{"namespace": "mopidy.config.types.LogColor.deserialize", "completion": "        value = decode(value)\n        if value.lower() in log.COLORS:\n            return encode(value.lower())\n        return value", "idx": 1806}
{"namespace": "mopidy.config.types.LogColor.serialize", "completion": "        lookup = {v: k for k, v in log.COLORS.items()}\n        if value in lookup:\n            return encode(lookup[value])\n        return \"\"", "idx": 1807}
{"namespace": "mopidy.config.types.LogLevel.deserialize", "completion": "        value = decode(value)\n        validators.validate_choice(value.lower(), self.levels)\n        return self.levels[value.lower()]", "idx": 1808}
{"namespace": "mopidy.config.types.LogLevel.serialize", "completion": "        if value in self.levels.values():\n            return next((k for k, v in self.levels.items() if v == value), \"\")\n        return \"\"", "idx": 1809}
{"namespace": "mopidy.config.types.Hostname.deserialize", "completion": "        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        try:\n            socket.inet_aton(value)\n            return value\n        except socket.error:\n            try:\n                socket.getaddrinfo(value, None)\n                return value\n            except socket.gaierror:\n                raise ValueError(f\"Invalid hostname or IP address: {value}\")", "idx": 1810}
{"namespace": "mopidy.config.load", "completion": "        from mopidy.internal import path\n    from mopidy.internal import path\n    parser = configparser.RawConfigParser(inline_comment_prefixes=(\";\",))\n\n    # Load default configuration files\n    logger.info(\"Loading config from builtin defaults\")\n    for default in ext_defaults:\n        if isinstance(default, bytes):\n            default = default.decode()\n        parser.read_string(default)\n\n    # Load config from a series of config files\n    for f in files:\n        f = path.expand_path(f)\n        if f.is_dir():\n            for g in f.iterdir():\n                if g.is_file() and g.suffix == \".conf\":\n                    _load_file(parser, g.resolve())\n        else:\n            _load_file(parser, f.resolve())\n\n    raw_config = {}\n    for section in parser.sections():\n        raw_config[section] = dict(parser.items(section))\n\n    logger.info(\"Loading config from command line options\")\n    for section, key, value in overrides:\n        raw_config.setdefault(section, {})[key] = value\n\n    return raw_config", "idx": 1811}
{"namespace": "mopidy.config.format_initial", "completion": "    ext_schemas = []\n    ext_defaults = []\n    for extension_data in extensions_data:\n        ext_schemas.append(extension_data.schema)\n        ext_defaults.append(extension_data.default_config)\n\n    raw_config = _load([], [], ext_defaults, [])\n    config, _ = _validate(raw_config, ext_schemas)\n\n    comments = {}\n    for extension_data in extensions_data:\n        comments[extension_data.schema.name] = _INITIAL_HELP.format(\n            versions=extension_data.versions\n        )\n\n    return _format(config, ext_schemas, comments, True, False)", "idx": 1812}
{"namespace": "mopidy.config._load", "completion": "    parser = configparser.RawConfigParser(inline_comment_prefixes=(\";\", \"#\"))\n    raw_config = {}\n\n    for default in defaults:\n        parser.read_string(_preprocess(default))\n\n    for file in files:\n        file_path = pathlib.Path(file)\n        if file_path.is_dir():\n            for conf_file in file_path.glob(\"*.conf\"):\n                _load_file(parser, conf_file)\n        else:\n            _load_file(parser, file_path)\n\n    for section, key, value in overrides:\n        if section not in raw_config:\n            raw_config[section] = {}\n        raw_config[section][key] = value\n\n    for section in parser.sections():\n        raw_config[section] = dict(parser.items(section))\n\n    return raw_config", "idx": 1813}
{"namespace": "mopidy.config._validate", "completion": "    validated_config = {}\n    errors = {}\n\n    for schema in schemas:\n        section_name = schema.name\n        if section_name in raw_config:\n            try:\n                deserialized_values = schema.deserialize(raw_config[section_name])\n                validated_config[section_name] = deserialized_values\n            except Exception as e:\n                errors[section_name] = str(e)\n        else:\n            logger.warning(f\"Section '{section_name}' not found in raw configuration\")\n\n    return validated_config, errors", "idx": 1814}
{"namespace": "mingus.extra.tunings.get_tunings", "completion": "    matching_tunings = []\n    for key, value in _known.items():\n        if instrument is not None and key.lower().startswith(instrument.lower()):\n            for desc, tuning in value[1].items():\n                if (nr_of_strings is None or tuning.count_strings() == nr_of_strings) and (\n                    nr_of_courses is None or tuning.count_courses() == nr_of_courses\n                ):\n                    matching_tunings.append(tuning)\n        elif instrument is None:\n            for desc, tuning in value[1].items():\n                if (nr_of_strings is None or tuning.count_strings() == nr_of_strings) and (\n                    nr_of_courses is None or tuning.count_courses() == nr_of_courses\n                ):\n                    matching_tunings.append(tuning)\n    return matching_tunings", "idx": 1815}
{"namespace": "mingus.containers.instrument.Instrument.note_in_range", "completion": "        if isinstance(note, six.string_types):\n            note = Note(note)\n        if not hasattr(note, \"name\"):\n            raise UnexpectedObjectError(\n                \"Unexpected object '%s'. \"\n                \"Expecting a mingus.containers.Note object\" % note\n            )\n        return self.range[0] <= note <= self.range[1]", "idx": 1816}
{"namespace": "mingus.containers.instrument.Guitar.can_play_notes", "completion": "        if len(notes) > 6:\n            return False\n        else:\n            return super().can_play_notes(notes)", "idx": 1817}
{"namespace": "mingus.containers.bar.Bar.get_range", "completion": "        note_names = self.get_note_names()\n        if note_names:\n            note_numbers = [keys.note_to_int(note) for note in note_names]\n            return max(note_numbers), min(note_numbers)\n        else:\n            return 0, 0", "idx": 1818}
{"namespace": "mingus.containers.bar.Bar.transpose", "completion": "        for x in self.bar:\n            x[2].transpose(interval, up)", "idx": 1819}
{"namespace": "mingus.containers.bar.Bar.determine_chords", "completion": "        from mingus.core import chords\n        res = []\n        for x in self.bar:\n            res.append(\n                [\n                    x[0],\n                    chords.determine(\n                        x[2].get_note_names(), shorthand\n                    ),\n                ]\n            )\n        return res", "idx": 1820}
{"namespace": "mingus.containers.note.Note.transpose", "completion": "        if up:\n            semitones = notes.interval_to_semitones(interval)\n        else:\n            semitones = -notes.interval_to_semitones(interval)\n\n        new_note_int = int(self) + semitones\n        self.from_int(new_note_int)", "idx": 1821}
{"namespace": "mingus.containers.note.Note.from_int", "completion": "        octave = integer // 12 - 1\n        note_index = integer % 12\n        note_name = notes.int_to_note(note_index)\n        self.name = note_name\n        self.octave = octave\n        return self", "idx": 1822}
{"namespace": "mingus.containers.note.Note.to_hertz", "completion": "        value = (self.octave + 6) * 12 + notes.note_to_int(self.name)\n        hertz = (2 ** ((value - 9) / 12)) * standard_pitch / 1024\n        return hertz", "idx": 1823}
{"namespace": "mingus.containers.note.Note.from_hertz", "completion": "        # Calculate the difference in pitch from the standard pitch of A-4\n        diff = 12 * log(hertz / standard_pitch, 2)\n\n        # Calculate the note and octave based on the difference\n        note_int = 57 + round(diff)\n        octave = note_int // 12\n        note_name = notes.int_to_note(note_int % 12)\n\n        # Set the note and octave\n        self.set_note(note_name, octave)\n        return self", "idx": 1824}
{"namespace": "mingus.containers.note.Note.to_shorthand", "completion": "        octave_mapping = {0: \",\", 1: \"\", 2: \"'\", 3: \"''\", 4: \"'''\", 5: \"''''\", 6: \"'''''\", 7: \"''''''\", 8: \"'''''''\"}\n        shorthand = self.name\n        shorthand += octave_mapping[self.octave]\n        return shorthand", "idx": 1825}
{"namespace": "mingus.containers.note_container.NoteContainer.from_chord_shorthand", "completion": "        \"\"\"\n        This function clears the NoteContainer and adds the notes corresponding to the shorthand notation.\n\n        Input-Output Arguments\n        :param self: NoteContainer. An instance of the NoteContainer class.\n        :param shorthand: str. The shorthand notation representing the chords.\n        :return: NoteContainer. The updated NoteContainer instance.\n\n        \"\"\"\n        self.empty()\n        chord_notes = chords.from_shorthand(shorthand)\n        self.add_notes(chord_notes)\n        return self", "idx": 1826}
{"namespace": "mingus.containers.note_container.NoteContainer.from_interval_shorthand", "completion": "        \"\"\"\n        This function empties the NoteContainer instance and adds a note to it based on the given startnote and shorthand. It first empties the NoteContainer instance and converts startnote to a Note object if its type is a string. The shorthand is used to determine the interval to transpose the startnote by. The resulting notes are then added to the NoteContainer instance.\n        Input-Output Arguments\n        :param self: NoteContainer. An instance of the NoteContainer class.\n        :param startnote: String or Note. The starting note for the interval transposition. If it is a string, it will be converted to a Note object.\n        :param shorthand: String. The shorthand representation of the interval to transpose the startnote by. See core.intervals for the recognized format.\n        :param up: Bool. Whether to transpose the interval up or down. Defaults to True.\n        :return: NoteContainer. The modified NoteContainer instance.\n        \"\"\"\n        self.empty()\n        if isinstance(startnote, str):\n            startnote = Note(startnote)\n        interval = intervals.from_shorthand(shorthand)\n        if up:\n            transposed_note = startnote + interval\n        else:\n            transposed_note = startnote - interval\n        self.add_note(transposed_note)\n        return self", "idx": 1827}
{"namespace": "mingus.containers.note_container.NoteContainer.from_progression_shorthand", "completion": "        \"\"\"\n        This function clears the NoteContainer and adds notes to it based on the given progression shorthand.\n\n        Input-Output Arguments\n        :param self: NoteContainer. An instance of the NoteContainer class.\n        :param shorthand: str. The progression shorthand describing the notes to be added.\n        :param key: str. The key to be used for the progression shorthand. It defaults to \"C\" if not specified.\n        :return: NoteContainer. The modified instance of the NoteContainer.\n\n        \"\"\"\n        self.empty()\n        self.add_notes(chords.from_shorthand(shorthand, key))\n        return self", "idx": 1828}
{"namespace": "mingus.containers.note_container.NoteContainer.transpose", "completion": "        for note in self.notes:\n            note.transpose(interval, up)\n        return self", "idx": 1829}
{"namespace": "mingus.containers.note_container.NoteContainer.get_note_names", "completion": "        unique_note_names = list(set([note.name for note in self.notes]))\n        return unique_note_names", "idx": 1830}
{"namespace": "mingus.core.notes.int_to_note", "completion": "    if note_int < 0 or note_int > 11:\n        raise RangeError(\"Note integer must be in the range 0-11\")\n\n    note_name = fifths[note_int % 7]\n    if accidentals == \"#\":\n        note_name += \"#\" * (note_int // 7)\n    else:\n        note_name += \"b\" * (note_int // 7)\n\n    return note_name", "idx": 1831}
{"namespace": "mingus.core.notes.is_valid_note", "completion": "    if len(note) == 1 and note[0] in _note_dict:\n        return True\n    elif len(note) > 1 and note[0] in _note_dict and (note[1] == \"#\" or note[1] == \"b\"):\n        return True\n    else:\n        return False", "idx": 1832}
{"namespace": "mingus.core.notes.reduce_accidentals", "completion": "    return remove_redundant_accidentals(note)", "idx": 1833}
{"namespace": "mingus.core.notes.remove_redundant_accidentals", "completion": "    return reduce_accidentals(note)", "idx": 1834}
{"namespace": "mingus.core.intervals.minor_second", "completion": "    sec = second(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sec, 1)", "idx": 1835}
{"namespace": "mingus.core.intervals.major_second", "completion": "    sec = second(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sec, 2)", "idx": 1836}
{"namespace": "mingus.core.intervals.minor_third", "completion": "    trd = third(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, trd, 3)", "idx": 1837}
{"namespace": "mingus.core.intervals.minor_fourth", "completion": "    frt = fourth(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, frt, 4)", "idx": 1838}
{"namespace": "mingus.core.intervals.minor_seventh", "completion": "    sth = seventh(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sth, 10)", "idx": 1839}
{"namespace": "mingus.core.intervals.major_seventh", "completion": "    sth = seventh(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sth, 11)", "idx": 1840}
{"namespace": "mingus.core.intervals.measure", "completion": "    notes_order = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n    index1 = notes_order.index(note1)\n    index2 = notes_order.index(note2)\n    return (index2 - index1) % 12", "idx": 1841}
{"namespace": "mingus.core.intervals.determine", "completion": "\n    if shorthand:\n        return determine_shorthand(note1, note2)\n    else:\n        return determine_full(note1, note2)", "idx": 1842}
{"namespace": "mingus.core.intervals.from_shorthand", "completion": "    if up:\n        return get_interval(note, int(interval), \"C\")\n    else:\n        return get_interval(note, -int(interval), \"C\")", "idx": 1843}
{"namespace": "mingus.core.intervals.is_consonant", "completion": "    dhalf = measure(note1, note2)\n    return dhalf in [0, 3, 4, 7, 8, 9] or (include_fourths and dhalf == 5)", "idx": 1844}
{"namespace": "mingus.core.intervals.is_perfect_consonant", "completion": "    if measure(note1, note2) in [0, 5, 7, 12]:\n        return True\n    elif include_fourths and measure(note1, note2) == 5:\n        return True\n    else:\n        return False", "idx": 1845}
{"namespace": "mingus.core.keys.get_key", "completion": "    if accidentals < 0:\n        return major_keys[abs(accidentals) - 1], minor_keys[abs(accidentals) - 1]\n    elif accidentals > 0:\n        return major_keys[accidentals - 1], minor_keys[accidentals - 1]\n    else:\n        return \"C\", \"a\"", "idx": 1846}
{"namespace": "mingus.core.keys.get_key_signature", "completion": "    for couple in keys:\n        if key in couple:\n            return keys.index(couple) - 7\n    return 0", "idx": 1847}
{"namespace": "mingus.core.keys.get_key_signature_accidentals", "completion": "    accidentals = get_key_signature(key)\n    if accidentals < 0:\n        return [\"b\"] * abs(accidentals)\n    elif accidentals > 0:\n        return [\"#\"] * accidentals\n    else:\n        return []", "idx": 1848}
{"namespace": "mingus.core.keys.get_notes", "completion": "    notes = base_scale\n    key_index = notes.index(key.upper())\n    return notes[key_index:] + notes[:key_index]", "idx": 1849}
{"namespace": "mingus.core.keys.relative_major", "completion": "    for couple in keys:\n        if key == couple[1]:\n            return couple[0]\n    raise NoteFormatError(\"'%s' is not a minor key\" % key)", "idx": 1850}
{"namespace": "mingus.core.chords.augmented_triad", "completion": "    return [note, intervals.major_third(note), notes.augment(intervals.perfect_fifth(note))]", "idx": 1851}
{"namespace": "mingus.core.chords.determine", "completion": "\n    if len(chord) == 3:\n        return determine_triad(chord, shorthand, no_inversions, None)\n    elif len(chord) == 4:\n        return determine_seventh(chord, shorthand, no_inversions, no_polychords)\n    elif len(chord) == 5:\n        return determine_extended_chord5(chord, shorthand, no_inversions, no_polychords)\n    elif len(chord) == 6:\n        return determine_extended_chord6(chord, shorthand, no_inversions, no_polychords)\n    elif len(chord) == 7:\n        return determine_extended_chord7(chord, shorthand, no_inversions, no_polychords)\n    else:\n        return \"Chord length not supported\"", "idx": 1852}
{"namespace": "mingus.core.value.septuplet", "completion": "    if in_fourths:\n        return tuplet(value, 7, 4)\n    else:\n        return tuplet(value, 7, 8)", "idx": 1853}
{"namespace": "mingus.core.value.determine", "completion": "\n    if value in base_values:\n        return value, 0, 1\n    elif value in base_triplets:\n        return value * 2, 0, 3\n    elif value in base_quintuplets:\n        return value * 4, 0, 5\n    elif value in base_septuplets:\n        return value * 4, 0, 7\n    else:\n        dots = 0\n        while value % 2 == 0:\n            value /= 2\n            dots += 1\n        return value, dots, 1", "idx": 1854}
{"namespace": "mingus.core.progressions.substitute_major_for_minor", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Major to minor substitution\n    if (\n        suff == \"M\" or suff == \"M7\"\n        or suff == \"\"\n        and roman in [\"II\", \"III\", \"VI\"]\n        or ignore_suffix\n    ):\n        n = skip(roman, 5)\n        a = interval_diff(roman, n, 9) + acc\n        if suff == \"M\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"m\")))\n        elif suff == \"M7\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"m7\")))\n        elif suff == \"\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"\")))\n    return res", "idx": 1855}
{"namespace": "mingus.core.progressions.substitute_diminished_for_diminished", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Diminished progressions\n    if (\n        suff == \"dim7\"\n        or suff == \"dim\"\n        or suff == \"\"\n        and roman in [\"VII\"]\n        or ignore_suffix\n    ):\n        if suff == \"\":\n            suff = \"dim\"\n\n        # Add diminished chord\n        last = roman\n        for x in range(3):\n            next = skip(last, 2)\n            acc += interval_diff(last, next, 3)\n            res.append(tuple_to_string((next, acc, suff)))\n            last = next\n    return res", "idx": 1856}
{"namespace": "mingus.core.progressions.substitute_diminished_for_dominant", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Diminished to dominant substitution\n    if (\n        suff == \"dim7\"\n        or suff == \"dim\"\n        or suff == \"\"\n        and roman in [\"VII\"]\n        or ignore_suffix\n    ):\n        if suff == \"\":\n            suff = \"dim\"\n\n        # Add the corresponding dominant seventh\n        res.append(tuple_to_string((skip(roman, 5), acc, \"dom7\")))\n\n        n = skip(roman, 1)\n        res.append(tuple_to_string((n, acc + interval_diff(roman, n, 1), \"dom7\")))\n\n        # Add diminished chord\n        last = roman\n        for x in range(4):\n            next = skip(last, 2)\n            acc += interval_diff(last, next, 3)\n            res.append(tuple_to_string((next, acc, suff)))\n            last = next\n    return res", "idx": 1857}
{"namespace": "mingus.core.progressions.substitute", "completion": "    if depth < 0:\n        return []\n\n    # Check if the given index is within the range of the progression\n    if substitute_index < 0 or substitute_index >= len(progression):\n        return []\n\n    # Get the chord at the specified index\n    chord_to_substitute = progression[substitute_index]\n\n    # Check if the chord has a suffix\n    has_suffix = False\n    if chord_to_substitute[-1].isdigit():\n        has_suffix = True\n\n    # Apply harmonic substitutions based on the suffix of the chord\n    if has_suffix:\n        if chord_to_substitute.endswith(\"m\"):\n            return substitute_minor_for_major(progression, substitute_index)\n        elif chord_to_substitute.endswith(\"M\") or chord_to_substitute.endswith(\"M7\"):\n            return substitute_major_for_minor(progression, substitute_index)\n        elif chord_to_substitute.endswith(\"dim\"):\n            return substitute_diminished_for_diminished(progression, substitute_index)\n        elif chord_to_substitute.endswith(\"dim7\"):\n            return substitute_diminished_for_dominant(progression, substitute_index)\n        else:\n            return []\n    else:\n        return substitute_harmonic(progression, substitute_index)", "idx": 1858}
{"namespace": "mingus.core.progressions.skip", "completion": "    index = numerals.index(roman_numeral)\n    new_index = (index - skip_count) % 7\n    return numerals[new_index]", "idx": 1859}
{"namespace": "exodus_bundler.cli.configure_logging", "completion": "    log_level = logging.ERROR if quiet else logging.INFO if verbose else logging.WARNING\n    logging.basicConfig(level=log_level)\n\n    # Add stderr handler for warning and error messages.\n    stderr_handler = logging.StreamHandler(sys.stderr)\n    stderr_handler.setLevel(logging.WARNING)\n    formatter = logging.Formatter('%(levelname)s: %(message)s')\n    stderr_handler.setFormatter(formatter)\n    logging.getLogger().addHandler(stderr_handler)\n\n    # Add stdout handler for debug and info messages if suppress_stdout is False.\n    if not suppress_stdout:\n        stdout_handler = logging.StreamHandler(sys.stdout)\n        stdout_handler.setLevel(logging.DEBUG)\n        formatter = logging.Formatter('%(levelname)s: %(message)s')\n        stdout_handler.setFormatter(formatter)\n        logging.getLogger().addHandler(stdout_handler)", "idx": 1860}
{"namespace": "exodus_bundler.bundling.create_unpackaged_bundle", "completion": "\n    # Create a temporary directory for the unpackaged bundle\n    root_directory = tempfile.mkdtemp()\n\n    try:\n        # Add the main executable files and their dependencies to the bundle\n        for executable in executables:\n            file = File(executable, chroot=chroot)\n            self.files.add(file)\n            if detect:\n                dependencies = file.elf.find_direct_dependencies()\n                for dependency in dependencies:\n                    self.files.add(dependency)\n\n        # Add additional files to the bundle\n        for additional_file in add:\n            file = File(additional_file, chroot=chroot)\n            self.files.add(file)\n\n        # Create shell launchers for the executables if required\n        if shell_launchers:\n            for executable in executables:\n                file = File(executable, chroot=chroot)\n                file.create_launcher(self.working_directory, self.bundle_root, 'linker', 'symlink', shell_launcher=True)\n\n        return root_directory\n    except Exception as e:\n        print(f\"Error creating unpackaged bundle: {e}\")\n        return None", "idx": 1861}
{"namespace": "exodus_bundler.bundling.detect_elf_binary", "completion": "    with open(filename, 'rb') as f:\n        # Read the first four bytes\n        first_four_bytes = f.read(4)\n        # Compare with the ELF header signature\n        if first_four_bytes == b'\\x7fELF':\n            return True\n        else:\n            return False", "idx": 1862}
{"namespace": "exodus_bundler.bundling.resolve_binary", "completion": "    if os.path.exists(binary):\n        return os.path.abspath(binary)\n    else:\n        for path in os.environ[\"PATH\"].split(os.pathsep):\n            full_path = os.path.join(path, binary)\n            if os.path.exists(full_path):\n                return os.path.abspath(full_path)\n    raise MissingFileError('The \"%s\" binary could not be found in $PATH.' % binary)", "idx": 1863}
{"namespace": "exodus_bundler.bundling.resolve_file_path", "completion": "    if search_environment_path:\n        # Check if the file exists in the PATH environment variable\n        for directory in os.getenv('PATH', '/bin/:/usr/bin/').split(os.pathsep):\n            absolute_path = os.path.normpath(os.path.abspath(os.path.join(directory, path)))\n            if os.path.exists(absolute_path) and not os.path.isdir(absolute_path):\n                return absolute_path\n        raise MissingFileError('The \"%s\" binary could not be found in $PATH.' % path)\n    else:\n        # Check if the file exists and is not a directory\n        absolute_path = os.path.normpath(os.path.abspath(path))\n        if os.path.exists(absolute_path) and not os.path.isdir(absolute_path):\n            return absolute_path\n        else:\n            raise MissingFileError('The file \"%s\" was not found.' % path)", "idx": 1864}
{"namespace": "exodus_bundler.bundling.run_ldd", "completion": "    if not detect_elf_binary(binary):\n        raise InvalidElfBinaryError('The provided file is not a valid ELF binary.')\n\n    process = Popen([ldd, binary], stdout=PIPE, stderr=PIPE)\n    stdout, stderr = process.communicate()\n    combined_output = stdout.decode('utf-8').split('\\n') + stderr.decode('utf-8').split('\\n')\n    return combined_output", "idx": 1865}
{"namespace": "exodus_bundler.bundling.Elf.dependencies", "completion": "        direct_dependencies = self.find_direct_dependencies()\n        all_dependencies = set(direct_dependencies)\n        new_dependencies = set(direct_dependencies)\n\n        while new_dependencies:\n            current_dependencies = set(new_dependencies)\n            new_dependencies = set()\n            for dependency in current_dependencies:\n                dependencies = dependency.find_direct_dependencies()\n                new_dependencies.update(dependencies)\n            all_dependencies.update(new_dependencies)\n\n        return all_dependencies", "idx": 1866}
{"namespace": "exodus_bundler.bundling.File.hash", "completion": "        \"\"\"\n        This function computes a hash based on the content of the file. It opens the file in binary mode, reads its content, and computes the SHA256 hash value.\n        Input-Output Arguments\n        :param self: File. An instance of the File class.\n        :return: str. The computed hash value as a hexadecimal string.\n        \"\"\"\n        with open(self.path, 'rb') as f:\n            content = f.read()\n            hash_value = hashlib.sha256(content).hexdigest()\n        return hash_value", "idx": 1867}
{"namespace": "exodus_bundler.bundling.Bundle.add_file", "completion": "        # Check if the path is a directory\n        if os.path.isdir(path):\n            # Add the directory recursively\n            for root, dirs, files in os.walk(path):\n                for file in files:\n                    file_path = os.path.join(root, file)\n                    self.add_file(file_path)\n            return None\n        else:\n            # Create a new File instance and add it to the bundle\n            file = self.file_factory(path, entry_point, chroot=self.chroot)\n            self.files.add(file)\n            return file", "idx": 1868}
{"namespace": "exodus_bundler.bundling.Bundle.bundle_root", "completion": "        bundles_folder = os.path.join(self.working_directory, 'bundles')\n        bundle_hash = self.hash\n        root_directory = os.path.join(bundles_folder, bundle_hash)\n        return os.path.normpath(os.path.abspath(root_directory))", "idx": 1869}
{"namespace": "exodus_bundler.bundling.Bundle.hash", "completion": "        file_hashes = [file.hash for file in self.files]\n        file_hashes.sort()\n        combined_string = ''.join(file_hashes).encode('utf-8')\n        return hashlib.sha256(combined_string).hexdigest()", "idx": 1870}
{"namespace": "exodus_bundler.launchers.construct_bash_launcher", "completion": "    linker_dirname, linker_basename = os.path.split(linker)\n    full_linker = '1' if full_linker else '0'\n    code = render_template_file('bash_launcher_template.sh', linker_basename=linker_basename,\n                                linker_dirname=linker_dirname, library_path=library_path,\n                                executable=executable, full_linker=full_linker)\n    return code", "idx": 1871}
{"namespace": "exodus_bundler.input_parsing.extract_open_path", "completion": "    line = strip_pid_prefix(line)\n    prefixes = ['open(\"', 'openat(']\n    for prefix in prefixes:\n        if line.startswith(prefix):\n            parts = line[len(prefix):].split('\", ')\n            if len(parts) > 1:\n                return parts[0]\n    return None", "idx": 1872}
{"namespace": "exodus_bundler.input_parsing.extract_paths", "completion": "    paths = []\n    if isinstance(content, str):\n        lines = content.split('\\n')\n    else:\n        lines = content\n\n    for line in lines:\n        path = extract_exec_path(line) or extract_open_path(line) or extract_stat_path(line)\n        if path and (not existing_only or (os.path.exists(path) and not os.path.isdir(path))):\n            paths.append(path)\n\n    return paths", "idx": 1873}
