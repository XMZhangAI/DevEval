{"namespace": "benedict.utils.type_util.is_json_serializable", "completion": "    if is_bool(val) or is_integer(val) or is_float(val) or is_string(val) or is_none(\n        val\n    ):\n        return True\n    elif is_list(val):\n        return all(is_json_serializable(item) for item in val)\n    elif is_dict(val):\n        return all(is_json_serializable(item) for item in val.values())\n    elif is_set(val):\n        return all(is_json_serializable(item) for item in val)\n    elif is_tuple(val):\n        return all(is_json_serializable(item) for item in val)\n    elif is_decimal(val):\n        return True\n    elif is_datetime(val):\n        return True\n    elif is_path(val):\n        return True\n    elif is_regex(val):\n        return True\n    elif is_uuid(val):\n        return True\n    else:\n        return False", "idx": 0}
{"namespace": "feedparser.urls.convert_to_idn", "completion": "    if not url:\n        return url\n    try:\n        scheme, netloc, path, params, query, fragment = urllib.parse.urlparse(url)\n    except ValueError:\n        return url\n    if scheme not in ACCEPTABLE_URI_SCHEMES:\n        return url\n    if not netloc:\n        return url\n    try:\n        netloc = netloc.encode('idna')\n    except UnicodeError:\n        return url\n    return urllib.parse.urlunparse((scheme, netloc.decode('utf-8'), path, params, query, fragment))", "idx": 1}
{"namespace": "mistune.toc.add_toc_hook", "completion": "    if heading_id is None:\n        heading_id = default_heading_id\n\n    def toc_hook(md, tokens, idx):\n        if idx < len(tokens) - 1:\n            token = tokens[idx]\n            if token['type'] == 'heading_open':\n                level = token['attrs']['level']\n                if level >= min_level and level <= max_level:\n                    text = tokens[idx + 1]['text']\n                    tokens[idx]['attrs']['id'] = heading_id(text)\n                    md.env['toc'].append(normalize_toc_item(md, token))\n        return idx\n\n    md.preprocessors.register(toc_hook, 'toc', 175)\n\n", "idx": 2}
{"namespace": "mistune.plugins.table.table_in_quote", "completion": "    md.block.insert_rule(md.block.quote_rules, 'table', before='paragraph')\n    md.block.insert_rule(md.block.quote_rules, 'nptable', before='paragraph')", "idx": 3}
{"namespace": "mistune.plugins.table.table_in_list", "completion": "    md.block.insert_rule(md.block.list_rules, 'table', before='paragraph')\n    md.block.insert_rule(md.block.list_rules, 'nptable', before='paragraph')", "idx": 4}
{"namespace": "xmnlp.utils.parallel_handler", "completion": "    if not isinstance(texts, list):\n        raise ValueError(\"You should pass a list of texts\")\n    with futures.ThreadPoolExecutor(max_workers=n_jobs) as executor:\n        for result in executor.map(partial(callback, **kwargs), texts):\n            yield result", "idx": 5}
{"namespace": "parsel.utils.shorten", "completion": "    if width < 0:\n        raise ValueError(\"width must be equal or greater than 0\")\n    if width == 0:\n        return suffix\n    if len(text) <= width:\n        return text\n    return text[: width - len(suffix)] + suffix", "idx": 6}
{"namespace": "parsel.xpathfuncs.set_xpathfunc", "completion": "    if func is None:\n        etree.XPath.unregister_function(fname)\n    else:\n        etree.XPath.register_function(fname, func)", "idx": 7}
{"namespace": "dominate.dom_tag._get_thread_context", "completion": "  if greenlet:\n    return hash((threading.current_thread(), greenlet.getcurrent()))\n  return hash(threading.current_thread())", "idx": 8}
{"namespace": "dominate.util.system", "completion": "  import subprocess\n  if data is None:\n    data = b''\n  p = subprocess.Popen(cmd, shell=True, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n  stdout, stderr = p.communicate(data)\n  return stdout.decode('utf-8')", "idx": 9}
{"namespace": "dominate.util.url_unescape", "completion": "  return data.replace('%2F', '/').replace('%3A', ':').replace('%40', '@').replace('%26', '&').replace('%3D', '=').replace('%2B', '+').replace('%24', '$').replace('%2C', ',').replace('%3B', ';').replace('%3F', '?').replace('%21', '!')\\\n    .replace('%25', '%').replace('%23', '#').replace('%27', '\\'').replace('%22', '\"').replace('%7E', '~').replace('%20', ' ').replace('%22', '\"').replace('%27', '\\'').replace('%3C', '<').replace('%3E', '>').replace('%7C', '|').replace('%5C', '\\\\').replace('%5E', '^').replace('%7E', '~').replace('%5B', '[').replace('%5D', ']').replace('%7B', '{').replace('%7D', '}').replace('%09', '\\t').replace('%0A', '\\n').replace('%0D', '\\r')", "idx": 10}
{"namespace": "rows.fields.DatetimeField.serialize", "completion": "        if value is None:\n            return \"\"\n\n        return value.strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    ", "idx": 11}
{"namespace": "rows.fields.Field.serialize", "completion": "        if value is None:\n            return \"\"\n        elif isinstance(value, cls.TYPE):\n            return value\n        else:\n            return as_string(value)", "idx": 12}
{"namespace": "rows.fields.EmailField.serialize", "completion": "        if value is None:\n            return \"\"\n\n        return as_string(value)", "idx": 13}
{"namespace": "rows.fields.as_string", "completion": "    if isinstance(value, six.text_type):\n        return value\n    elif isinstance(value, six.binary_type):\n        return value.decode(\"utf-8\")\n    else:\n        return six.text_type(value)", "idx": 14}
{"namespace": "rows.fields.get_items", "completion": "    def getter(obj):\n        return tuple(obj[index] if index < len(obj) else None for index in indexes)\n\n    return getter", "idx": 15}
{"namespace": "pycorrector.proper_corrector.load_dict_file", "completion": "    words = {}\n    if path and os.path.exists(path):\n        with open(path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line.startswith('#'):\n                    continue\n                if line:\n                    k, v = line.split('\\t')\n                    words[k] = v\n    return words", "idx": 16}
{"namespace": "natasha.span.envelop_spans", "completion": "    for envelope in envelopes:\n        yield from envelop_spans_by_envelope(spans, envelope)\n\n", "idx": 17}
{"namespace": "googleapiclient._helpers.parse_unique_urlencoded", "completion": "    params = urllib.parse.parse_qs(content)\n    for key in params:\n        if len(params[key]) > 1:\n            raise ValueError(\"Repeated key: \" + key)\n    return params", "idx": 18}
{"namespace": "jinja2.async_utils.auto_aiter", "completion": "    if hasattr(iterable, \"__aiter__\"):\n        async for item in iterable:\n            yield item\n    else:\n        for item in iterable:\n            yield item", "idx": 19}
{"namespace": "jinja2.utils.consume", "completion": "    for _ in iterable:\n        pass", "idx": 20}
{"namespace": "pycorrector.utils.tokenizer.segment", "completion": "    if cut_type == 'word':\n        if pos:\n            word_pos_seq = posseg.cut(sentence)\n            word_pos_seq = [(w, p) for w, p in word_pos_seq]\n            return word_pos_seq\n        else:\n            return jieba.lcut(sentence)\n    elif cut_type == 'char':\n        return list(sentence)\n    else:\n        raise Exception('Unknown cut_type: ' + cut_type)", "idx": 21}
{"namespace": "jinja2.utils.object_type_repr", "completion": "    if obj is None:\n        return \"None\"\n    if obj is Ellipsis:\n        return \"Ellipsis\"\n    if obj is True:\n        return \"True\"\n    if obj is False:\n        return \"False\"\n    if obj is not None and not isinstance(obj, str):\n        return type(obj).__name__\n    return obj", "idx": 22}
{"namespace": "jinja2.utils.LRUCache.setdefault", "completion": "        try:\n            return self[key]\n        except KeyError:\n            self[key] = default\n            return default", "idx": 23}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_word_freq", "completion": "        word_freq = {}\n        for w in list_of_words:\n            if w in word_freq:\n                word_freq[w] += 1\n            else:\n                word_freq[w] = 1\n        return word_freq\n", "idx": 24}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_average_probability_of_words", "completion": "        if len(content_words_in_sentence) == 0:\n            return 0\n        else:\n            return sum(word_freq_in_doc[w] for w in content_words_in_sentence) / len(content_words_in_sentence)\n\n    ", "idx": 25}
{"namespace": "sumy.summarizers.lex_rank.LexRankSummarizer._compute_idf", "completion": "        idf_metrics = {}\n        sentences_count = len(sentences)\n\n        for sentence in sentences:\n            for term in sentence:\n                if term not in idf_metrics:\n                    n_j = sum(1 for s in sentences if term in s)\n                    idf_metrics[term] = math.log(sentences_count / (1 + n_j))\n\n        return idf_metrics\n\n    ", "idx": 26}
{"namespace": "sumy.summarizers.lex_rank.LexRankSummarizer.cosine_similarity", "completion": "        sentence1 = set(sentence1)\n        sentence2 = set(sentence2)\n        common_terms = sentence1.intersection(sentence2)\n\n        if not sentence1 or not sentence2:\n            return 0.0\n\n        numerator = 0.0\n        for term in common_terms:\n            numerator += tf1[term] * tf2[term] * idf_metrics[term]\n\n        denominator1 = sum(tf1[t] * idf_metrics[t] for t in sentence1)\n        denominator2 = sum(tf2[t] * idf_metrics[t] for t in sentence2)\n\n        return numerator / (denominator1 * denominator2)\n\n    ", "idx": 27}
{"namespace": "sumy.evaluation.rouge._get_ngrams", "completion": "    ngram_set = set()\n    text_length = len(text)\n    max_index_ngram_start = text_length - n\n    for i in range(max_index_ngram_start + 1):\n        ngram_set.add(tuple(text[i:i + n]))\n    return ngram_set", "idx": 28}
{"namespace": "sumy.evaluation.rouge._split_into_words", "completion": "    if not all(isinstance(abjad_obj, Sentence) for abjad_obj in sentences):\n        raise ValueError(\"Object in collection must be of type Sentence.\")\n    words = []\n    for sentence in sentences:\n        words.extend(sentence.words)\n    return words", "idx": 29}
{"namespace": "falcon.inspect.register_router", "completion": "    if router_class in _supported_routers:\n        raise ValueError(\n            'Router class {} is already registered'.format(router_class)\n        )\n\n    def decorator(func):\n        _supported_routers[router_class] = func\n        return func\n\n    return decorator", "idx": 30}
{"namespace": "falcon.inspect.inspect_compiled_router", "completion": "    routes = []\n    for route in router._routes:\n        for method in route.methods:\n            source_info, name = _get_source_info_and_name(method)\n            info = RouteMethodInfo(\n                route.uri_template.pattern, source_info, name, _is_internal(method)\n            )\n            routes.append(info)\n    return routes", "idx": 31}
{"namespace": "falcon.inspect._is_internal", "completion": "    module = inspect.getmodule(obj)\n    if module is None:\n        return False\n    return module.__name__.startswith('falcon')", "idx": 32}
{"namespace": "falcon.cmd.inspect_app.load_app", "completion": "    module_name, app_name = args.app_module.split(':')\n    module = importlib.import_module(module_name)\n    app = getattr(module, app_name, None)\n    if not isinstance(app, falcon.App):\n        app = app()\n    if not isinstance(app, falcon.App):\n        parser.error('The app is not a falcon.App instance')\n    return app", "idx": 33}
{"namespace": "falcon.cmd.inspect_app.make_parser", "completion": "    parser = argparse.ArgumentParser(\n        description='Prints out the routes of an App instance'\n    )\n    parser.add_argument(\n        '-r',\n        '--route-only',\n        action='store_true',\n        help='Only print out the routes, not the app',\n    )\n    parser.add_argument(\n        '-v',\n        '--verbose',\n        action='store_true',\n        help='Print out the full route information',\n    )\n    parser.add_argument(\n        '-i',\n        '--internal',\n        action='store_true',\n        help='Print out the internal routes',\n    )\n    parser.add_argument(\n        'app_module',\n        help='The module containing the app instance to inspect. '\n        'The module must be importable and the instance must be '\n        'a falcon.App instance or a callable that returns one. '\n        'The instance can be specified as a string in the form '\n        'module:instance.',\n    )\n    return parser", "idx": 34}
{"namespace": "falcon.util.uri.unquote_string", "completion": "    if not isinstance(quoted, str):\n        raise TypeError('unquote_string() argument must be str, not %s' % type(quoted).__name__)\n\n    if quoted[0] != '\"' or quoted[-1] != '\"':\n        raise ValueError('unquote_string() argument must be a quoted string')\n\n    unquoted = quoted[1:-1]\n\n    # TODO(vytas): This is a very naive implementation. It should be replaced\n    # with a more efficient one.\n    unquoted = unquoted.replace('\\\\\"', '\"')\n    unquoted = unquoted.replace('\\\\\\\\', '\\\\')\n\n    return unquoted", "idx": 35}
{"namespace": "falcon.util.misc.get_argnames", "completion": "    # NOTE(kgriffs): This is a bit of a hack, but it's the best we can do\n    #   without using the inspect module.\n    argspec = inspect.getargspec(func)\n    if argspec.args:\n        return argspec.args\n\n    if argspec.varargs:\n        return argspec.args + [argspec.varargs]\n\n    if argspec.varkw:\n        return argspec.args + [argspec.varkw]\n\n    return []", "idx": 36}
{"namespace": "falcon.testing.client._is_asgi_app", "completion": "    return inspect.signature(app).parameters.__len__() == 3", "idx": 37}
{"namespace": "falcon.routing.converters.UUIDConverter.convert", "completion": "        try:\n            return uuid.UUID(value)\n        except ValueError:\n            return None", "idx": 38}
{"namespace": "rest_framework_simplejwt.utils.make_utc", "completion": "    if settings.USE_TZ and is_naive(dt):\n        return make_aware(dt, timezone.utc)\n    return dt\n\n", "idx": 39}
{"namespace": "boto.sdb.db.sequence.fib", "completion": "    if cv is None:\n        return 1\n    if lv is None:\n        return cv + 1\n    return cv + lv\n\n", "idx": 40}
{"namespace": "boto.s3.website.RoutingRules.add_rule", "completion": "        self.append(rule)\n        return self", "idx": 41}
{"namespace": "boto.cloudfront.distribution.Distribution._canned_policy", "completion": "        policy = '{\"Statement\":[{\"Resource\":\"%(resource)s\",\"Condition\":{\"DateLessThan\":{\"AWS:EpochTime\":%(expires)s}}}]}' % locals()\n        return policy", "idx": 42}
{"namespace": "boto.cloudfront.invalidation.InvalidationBatch.escape", "completion": "        if p[0] != '/':\n            p = '/' + p\n        return urllib.quote(p, safe='/')", "idx": 43}
{"namespace": "proxybroker.utils.get_status_code", "completion": "    try:\n        return int(resp[start:stop])\n    except (ValueError, IndexError):\n        return 400", "idx": 44}
{"namespace": "authlib.oauth2.rfc6749.util.scope_to_list", "completion": "    if isinstance(scope, (set, tuple, list)):\n        return [to_unicode(s) for s in scope]\n    if scope is None:\n        return scope\n    return [to_unicode(s) for s in scope.split()]", "idx": 45}
{"namespace": "authlib.common.encoding.to_unicode", "completion": "    if x is None:\n        return None\n    if isinstance(x, str):\n        return x\n    if isinstance(x, bytes):\n        return x.decode(charset, errors)\n    return str(x)", "idx": 46}
{"namespace": "authlib.common.encoding.to_bytes", "completion": "    if x is None:\n        return None\n    if isinstance(x, bytes):\n        return x\n    if isinstance(x, str):\n        return x.encode(charset, errors)\n    if isinstance(x, int):\n        return str(x).encode(charset, errors)\n    if isinstance(x, float):\n        return str(x).encode(charset, errors)\n    return str(x).encode(charset, errors)", "idx": 47}
{"namespace": "authlib.common.encoding.urlsafe_b64decode", "completion": "    s = to_bytes(s, charset='ascii')\n    s += b'=' * (4 - len(s) % 4)\n    return base64.urlsafe_b64decode(s)", "idx": 48}
{"namespace": "csvs_to_sqlite.utils.table_exists", "completion": "    cursor = conn.execute(\n        \"SELECT name FROM sqlite_master WHERE type='table' AND name=?\", [table]\n    )\n    return cursor.fetchone() is not None", "idx": 49}
{"namespace": "sqlitedict.SqliteDict.get_tablenames", "completion": "        conn = sqlite3.connect(filename)\n        cursor = conn.cursor()\n        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n        return [row[0] for row in cursor.fetchall()]\n", "idx": 50}
{"namespace": "litecli.packages.parseutils.query_starts_with", "completion": "    query = query.lower()\n    query = sqlparse.format(query, strip_comments=True)\n    if not query.strip():\n        return False\n    return last_word(query) in prefixes", "idx": 51}
{"namespace": "rest_framework.negotiation.DefaultContentNegotiation.filter_renderers", "completion": "        filtered_renderers = [renderer for renderer in renderers if renderer.format == format]\n        if not filtered_renderers:\n            raise Http404()\n        return filtered_renderers", "idx": 52}
{"namespace": "rest_framework.templatetags.rest_framework.as_string", "completion": "    if value is None:\n        return ''\n    return '%s' % value", "idx": 53}
{"namespace": "rest_framework.templatetags.rest_framework.add_nested_class", "completion": "    if isinstance(value, dict) or (isinstance(value, list) and isinstance(value[0], dict)):\n        return 'class=nested'\n    return ''", "idx": 54}
{"namespace": "pyramid.session.PickleSerializer.loads", "completion": "        return pickle.loads(bstruct)", "idx": 55}
{"namespace": "pyramid.testing.DummySession.flash", "completion": "        if not allow_duplicate:\n            if msg in self.peek_flash(queue):\n                return\n        storage = self.get('_f_' + queue, [])\n        storage.append(msg)\n        self['_f_' + queue] = storage\n", "idx": 56}
{"namespace": "pyramid.testing.DummySession.pop_flash", "completion": "        storage = self.get('_f_' + queue, [])\n        self['_f_' + queue] = []\n        return storage", "idx": 57}
{"namespace": "pyramid.testing.DummySession.peek_flash", "completion": "        storage = self.get('_f_' + queue, [])\n        return storage", "idx": 58}
{"namespace": "pyramid.testing.DummySession.new_csrf_token", "completion": "        import random\n\n        token = ''.join(\n            random.choice('0123456789') for i in range(32)\n        )  # 32-character random string\n        self['_csrft_'] = token\n        return token", "idx": 59}
{"namespace": "pyramid.view.view_defaults", "completion": "    def decorator(cls):\n        cls.__view_defaults__ = settings\n        return cls\n\n    return decorator\n\n", "idx": 60}
{"namespace": "pyramid.util.bytes_", "completion": "    if isinstance(s, str):\n        s = s.encode(encoding, errors)\n    return s\n\n", "idx": 61}
{"namespace": "pyramid.scripts.common.parse_vars", "completion": "    return dict(arg.split('=', 1) for arg in args)", "idx": 62}
{"namespace": "pyramid.scripts.pviews.PViewsCommand._find_multi_routes", "completion": "        infos = []\n        for route in mapper.routes:\n            match = route.match(request.path)\n            if match is not None:\n                infos.append({'match': match, 'route': route})\n        return infos", "idx": 63}
{"namespace": "pyramid.scripts.pserve.PServeCommand.guess_server_url", "completion": "        if server_name is None:\n            server_name = 'main'\n        settings = loader.get_settings('server:' + server_name, global_conf)\n        port = settings.get('port')\n        if port:\n            return 'http://127.0.0.1:%s' % port", "idx": 64}
{"namespace": "aiohappybase._util.pep8_to_camel_case", "completion": "    if initial:\n        return name.title().replace('_', '')\n    return name[0].lower() + pep8_to_camel_case(name)[1:]", "idx": 65}
{"namespace": "aiohappybase._util.bytes_increment", "completion": "    if b == b'\\xff' * len(b):\n        return None\n    return b[:-1] + bytes([b[-1] + 1])", "idx": 66}
{"namespace": "mssqlcli.config.ensure_dir_exists", "completion": "    if not exists(dirname(path)):\n        os.makedirs(dirname(path))", "idx": 67}
{"namespace": "mssqlcli.telemetry._user_id_file_is_old", "completion": "    if os.path.exists(id_file_path):\n        file_modified_time = datetime.fromtimestamp(os.path.getmtime(id_file_path))\n        return datetime.now() - file_modified_time > timedelta(hours=24)\n    return False", "idx": 68}
{"namespace": "mssqlcli.util.is_command_valid", "completion": "    try:\n        subprocess.call(encode(command), stdout=open(devnull, 'wb'), stderr=open(devnull, 'wb'))\n    except OSError:\n        return False\n    return True", "idx": 69}
{"namespace": "mssqlcli.packages.parseutils.utils.find_prev_keyword", "completion": "    # parsed can contain one or more semi-colon separated commands\n    parsed = sqlparse.parse(sql)\n    if len(parsed) == 1:\n        # If there is only one command, then we can just use the last_word function\n        return last_word(sql), sql\n\n    # If there are multiple commands, then we need to find the last keyword in the last command\n    last_command = parsed[-1]\n    last_keyword = last_word(last_command.value, include='most_punctuations')\n    if last_keyword:\n        # If the last keyword is not empty, then we can just return the last keyword and the text of the query with everything after the last keyword stripped\n        return last_keyword, sql[:-len(last_keyword)]\n\n    # If the last keyword is empty, then we need to find the last keyword in the second to last command\n    second_last_command = parsed[-2]\n    second_last_keyword = last_word(second_last_command.value, include='most_punctuations')\n    if second_last_keyword:\n        # If the second to last keyword is not empty, then we can just return the second to last keyword and the text of the query with everything after the second to last keyword stripped\n        return second_last_keyword, sql[:-len(second_last_keyword)]\n\n    # If the second to last keyword is empty, then we need to find the last keyword in the third to last command\n    third_last_command = parsed[-3]\n    third_last_keyword = last_word(third_last_command.value, include='most_punctuations')\n    if third_last_keyword:\n        # If the third to last keyword is not empty, then we can just return the third to last keyword and the text of the query with everything after the third to last keyword stripped\n        return third_last_keyword, sql[:-len(third_last_keyword)]\n\n    # If the third to last keyword is empty, then we need to find the last keyword in the fourth to last command\n    fourth_last_command = parsed[-4]\n    fourth_last_keyword =", "idx": 70}
{"namespace": "trafilatura.settings.use_config", "completion": "    if config is not None:\n        return config\n\n    if filename is None:\n        filename = Path(__file__).parent / 'settings.cfg'\n\n    config = ConfigParser()\n    config.read(filename)\n    return config", "idx": 71}
{"namespace": "pyramid.util.text_", "completion": "    if isinstance(s, bytes):\n        return s.decode(encoding, errors)\n    return s", "idx": 72}
{"namespace": "trafilatura.downloads._parse_config", "completion": "    myagents = None\n    mycookie = None\n    if config.has_section('USER_AGENT'):\n        myagents = config.get('USER_AGENT', 'user_agents').split('\\n')\n    if config.has_section('COOKIE'):\n        mycookie = config.get('COOKIE', 'cookie')\n    return myagents, mycookie", "idx": 73}
{"namespace": "trafilatura.downloads.load_download_buffer", "completion": "    # https://stackoverflow.com/questions/42409000/python-threading-how-to-wait-for-all-threads-to-finish-before-continuing\n    # https://stackoverflow.com/questions/12435211/threading-timer-repeat-function-every-n-seconds\n    # https://stackoverflow.com/questions/12435211/threading-timer-repeat-function-every-n-seconds\n    # https://stackoverflow.com/questions/12435211/threading-timer-repeat-function-every-n-seconds\n    # https://stackoverflow.com/questions/12435211/threading-timer-repeat-function-every-n-seconds\n    # https://stackoverflow.com/questions/12435211/threading-timer-repeat-function-every-n-seconds\n    # https://stackoverflow.com/questions/12435211/threading-timer-repeat-function-every-n-seconds\n    # https://stackoverflow.com/questions/12435211/threading-timer-repeat-function-every-n-seconds\n    # https://stackoverflow.com/questions/12435211/threading-timer-repeat-function-every-n-seconds\n    # https://stackoverflow.com/questions/12435211/threading-timer-repeat-function-every-n-seconds\n    # https://stackoverflow.com/questions/12435211/threading-timer-repeat-function-every-n-seconds\n    # https://stackoverflow.com/questions/12435211/threading-timer-repeat-function-every-n-seconds\n    # https://stackoverflow.com/questions/12435211/threading-timer-repeat-function-every-n-seconds\n    # https://stackoverflow.com/questions/12435211/threading-timer-repeat-function-every-", "idx": 74}
{"namespace": "trafilatura.metadata.check_authors", "completion": "    new_authors = []\n    for author in authors.split(';'):\n        if author.lower() not in author_blacklist:\n            new_authors.append(author)\n    if new_authors:\n        return '; '.join(new_authors)\n    return None", "idx": 75}
{"namespace": "datasette.filters.where_filters", "completion": "    async def inner():\n        where_clauses = []\n        extra_wheres_for_ui = []\n        if \"_where\" in request.args:\n            if not datasette.permission_allowed(\n                request, \"execute-sql\", database=database\n            ):\n                raise PermissionDenied(\"You don't have permission to execute SQL\")\n            where_clauses.append(request.args[\"_where\"])\n            extra_wheres_for_ui.append(request.args[\"_where\"])\n        return FilterArguments(where_clauses, extra_wheres_for_ui=extra_wheres_for_ui)\n\n    return inner\n\n", "idx": 76}
{"namespace": "datasette.utils.path_with_added_args", "completion": "    if path is None:\n        path = request.path\n    if isinstance(args, dict):\n        args = args.items()\n    query_string = urllib.parse.urlencode(args)\n    if query_string:\n        query_string = f\"?{query_string}\"\n    return path + query_string", "idx": 77}
{"namespace": "datasette.utils.path_with_replaced_args", "completion": "    path = path or request.path\n    if isinstance(args, dict):\n        args = args.items()\n    current = []\n    for key, value in urllib.parse.parse_qsl(request.query_string):\n        if (key, value) in args:\n            current.append((key, args[(key, value)]))\n        else:\n            current.append((key, value))\n    query_string = urllib.parse.urlencode(current)\n    if query_string:\n        query_string = f\"?{query_string}\"\n    return path + query_string", "idx": 78}
{"namespace": "datasette.utils.format_bytes", "completion": "    if bytes == 0:\n        return \"0B\"\n    size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n    i = int(math.floor(math.log(bytes, 1024)))\n    p = math.pow(1024, i)\n    s = round(bytes / p, 2)\n    return \"%s %s\" % (s, size_name[i])", "idx": 79}
{"namespace": "datasette.utils.actor_matches_allow", "completion": "    if allow is None:\n        return True\n    if isinstance(allow, str):\n        return actor == allow\n    if isinstance(allow, list):\n        return actor in allow\n    if isinstance(allow, dict):\n        if \"$or\" in allow:\n            return any(actor_matches_allow(actor, a) for a in allow[\"$or\"])\n        if \"$and\" in allow:\n            return all(actor_matches_allow(actor, a) for a in allow[\"$and\"])\n        if \"$not\" in allow:\n            return not actor_matches_allow(actor, allow[\"$not\"])\n        if \"$eq\" in allow:\n            return actor == allow[\"$eq\"]\n        if \"$ne\" in allow:\n            return actor != allow[\"$ne\"]\n        if \"$in\" in allow:\n            return actor in allow[\"$in\"]\n        if \"$nin\" in allow:\n            return actor not in allow[\"$nin\"]\n        if \"$regex\" in allow:\n            return re.search(allow[\"$regex\"], actor)\n        if \"$not\" in allow:\n            return not actor_matches_allow(actor, allow[\"$not\"])\n        if \"$not\" in allow:\n            return not actor_matches_allow(actor, allow[\"$not\"])\n        if \"$not\" in allow:\n            return not actor_matches_allow(actor, allow[\"$not\"])\n        if \"$not\" in allow:\n            return not actor_matches_allow(actor, allow[\"$not\"])\n        if \"$not\" in allow:\n            return not actor_matches_allow(actor, allow[\"$not\"])\n        if \"$not\" in allow:\n            return not actor_matches_allow(actor, allow[\"$not\"])\n        if \"$not\" in allow:\n            return not actor_matches_allow(actor, allow[\"$not\"])\n        if \"$not\" in allow:\n            return not actor_matches_allow(actor, allow[\"$not\"])\n        if \"$not\" in", "idx": 80}
{"namespace": "datasette.utils.resolve_env_secrets", "completion": "    if isinstance(config, dict):\n        return {\n            key: resolve_env_secrets(value, environ)\n            for key, value in config.items()\n        }\n    elif isinstance(config, list):\n        return [resolve_env_secrets(item, environ) for item in config]\n    elif isinstance(config, str):\n        if config.startswith(\"$env:\"):\n            return environ.get(config[5:])\n        elif config.startswith(\"$file:\"):\n            with open(config[6:]) as fp:\n                return fp.read()\n        else:\n            return config\n    else:\n        return config", "idx": 81}
{"namespace": "datasette.utils.display_actor", "completion": "    if actor.get(\"display_name\"):\n        return actor[\"display_name\"]\n    elif actor.get(\"name\"):\n        return actor[\"name\"]\n    elif actor.get(\"username\"):\n        return actor[\"username\"]\n    elif actor.get(\"login\"):\n        return actor[\"login\"]\n    elif actor.get(\"id\"):\n        return actor[\"id\"]\n    else:\n        return str(actor)", "idx": 82}
{"namespace": "datasette.utils.initial_path_for_datasette", "completion": "    databases = datasette.databases\n    if len(databases) == 1:\n        database = list(databases.keys())[0]\n        tables = list(databases[database].tables.keys())\n        if len(tables) == 1:\n            return f\"/{database}/{tables[0]}\"\n        else:\n            return f\"/{database}\"\n    else:\n        return \"/\"", "idx": 83}
{"namespace": "datasette.utils.tilde_decode", "completion": "    s = s.replace(\"%\", \"~p\")\n    s = urllib.parse.unquote(s)\n    s = s.replace(\"~p\", \"%\")\n    return s", "idx": 84}
{"namespace": "datasette.utils.resolve_routes", "completion": "    for regex, view in routes:\n        match = re.match(regex, path)\n        if match:\n            return match, view", "idx": 85}
{"namespace": "datasette.utils.truncate_url", "completion": "    if len(url) <= length:\n        return url\n\n    if url.endswith(\".html\"):\n        return url[:length - 4] + \"...\" + url[-4:]\n\n    if url.endswith(\".htm\"):\n        return url[:length - 3] + \"...\" + url[-3:]\n\n    if url.endswith(\".xml\"):\n        return url[:length - 3] + \"...\" + url[-3:]\n\n    if url.endswith(\".pdf\"):\n        return url[:length - 3] + \"...\" + url[-3:]\n\n    if url.endswith(\".jpg\"):\n        return url[:length - 4] + \"...\" + url[-4:]\n\n    if url.endswith(\".jpeg\"):\n        return url[:length - 5] + \"...\" + url[-5:]\n\n    if url.endswith(\".png\"):\n        return url[:length - 4] + \"...\" + url[-4:]\n\n    if url.endswith(\".gif\"):\n        return url[:length - 4] + \"...\" + url[-4:]\n\n    if url.endswith(\".svg\"):\n        return url[:length - 4] + \"...\" + url[-4:]\n\n    if url.endswith(\".ico\"):\n        return url[:length - 4] + \"...\" + url[-4:]\n\n    if url.endswith(\".css\"):\n        return url[:length - 4] + \"...\" + url[-4:]\n\n    if url.endswith(\".js\"):\n        return url[:length - 3] + \"...\" + url[-3:]\n\n    if url.endswith(\".txt\"):\n        return url[:length - 4] + \"...\" + url[-4:]\n\n    if url.endswith(\".csv\"):\n        return url[:length - 4] + \"...\" + url[-4:]\n\n    if url.endswith(\".xls\"):\n        return url[:length - 4] + \"...\" + url[-4:]\n\n    if url.endswith(\".xlsx\"):\n       ", "idx": 86}
{"namespace": "kinto.core.authorization.groupfinder", "completion": "    if not request.registry.settings.get(\"permission_backend\"):\n        return []\n\n    principals = request.registry.permission.get_principals(userid)\n    logger.info(\"Principals fetched from permission backend: %r\", principals)\n    return principals", "idx": 87}
{"namespace": "kinto.core.utils.json.dumps", "completion": "        kw.setdefault(\"bytes_mode\", rapidjson.BM_NONE)\n        return rapidjson.dumps(v, **kw)\n", "idx": 88}
{"namespace": "kinto.core.utils.json.loads", "completion": "        kw.setdefault(\"number_mode\", rapidjson.NM_NATIVE)\n        return rapidjson.loads(v, **kw)", "idx": 89}
{"namespace": "kinto.core.utils.hmac_digest", "completion": "    if isinstance(secret, str):\n        secret = secret.encode(encoding)\n    return hmac.new(secret, message.encode(encoding), hashlib.sha256).hexdigest()", "idx": 90}
{"namespace": "kinto.core.utils.current_service", "completion": "    service = request.current_service\n    if service:\n        return service\n\n    # No existing service is concerned, and Cornice is not implied.\n    # Import service here because kinto.core import utils\n    from kinto.core import Service\n\n    # We need to get the routes mapper from the registry.\n    routes_mapper = request.registry.queryUtility(IRoutesMapper)\n    if routes_mapper is None:\n        return None\n\n    # We need to build a fake request to get the service.\n    fakerequest = Request.blank(path=request.path)\n    fakerequest.registry = request.registry\n    apply_request_extensions(fakerequest)\n\n    # We need to get the service from the fake request.\n    info = routes_mapper(fakerequest)\n    service = info[\"service\"]\n\n    # We need to get the resource from the service.\n    resource = service.resource\n\n    # We need to get the resource name from the resource.\n    resource_name = resource.get_name(resource)\n\n    # We need to get the resource name from the resource.\n    resource_name = resource.get_name(resource)\n\n    # We need to get the resource name from the resource.\n    resource_name = resource.get_name(resource)\n\n    # We need to get the resource name from the resource.\n    resource_name = resource.get_name(resource)\n\n    # We need to get the resource name from the resource.\n    resource_name = resource.get_name(resource)\n\n    # We need to get the resource name from the resource.\n    resource_name = resource.get_name(resource)\n\n    # We need to get the resource name from the resource.\n    resource_name = resource.get_name(resource)\n\n    # We need to get the resource name from the resource.\n    resource_name = resource.get_name(resource)\n\n    # We need to get the resource name from the resource.\n    resource_name = resource.get_name(resource)\n\n    # We need to get the resource name from the resource.\n    resource_", "idx": 91}
{"namespace": "kinto.core.utils.prefixed_principals", "completion": "    principals = request.effective_principals\n    if \"Authenticated\" not in principals:\n        return principals\n\n    # Remove the unprefixed user id from the effective principals to avoid\n    # conflicts.\n    principals = [p for p in principals if p != request.selected_userid]\n\n    # Add the prefixed user id to the beginning of the list.\n    principals.insert(0, prefixed_userid(request))\n\n    return principals", "idx": 92}
{"namespace": "kinto.plugins.accounts.views.on_account_created", "completion": "    request = event.request\n    settings = request.registry.settings\n\n    if settings.get(\"account_validation\", False):\n        for obj in event.impacted_objects:\n            account = obj[\"new\"]\n            activation_key = get_cached_validation_key(account[\"id\"], request.registry)\n            if activation_key is None:\n                continue\n            emailer = Emailer(request)\n            emailer.send_activation(account, activation_key)", "idx": 93}
{"namespace": "kinto.plugins.accounts.utils.hash_password", "completion": "    return bcrypt.hashpw(password.encode(\"utf-8\"), bcrypt.gensalt()).decode(\"utf-8\")\n\n", "idx": 94}
{"namespace": "kinto.views.admin.get_parent_uri", "completion": "    if not object_uri.startswith(\"http://\"):\n        return \"\"\n\n    path = object_uri.split(\"/\")\n    if len(path) < 3:\n        return \"\"\n\n    return \"/\".join(path[:-1])", "idx": 95}
{"namespace": "alembic.script.write_hooks.register", "completion": "    def decorator(func: Callable) -> Callable:\n        _registry[name] = func\n        return func\n\n    return decorator", "idx": 96}
{"namespace": "mongo_connector.namespace_config.match_replace_regex", "completion": "    match = regex.match(src_namespace)\n    if not match:\n        return None\n    return dest_namespace.replace(\"*\", match.group(1))", "idx": 97}
{"namespace": "mongo_connector.namespace_config.namespace_to_regex", "completion": "    database_name, collection_name = namespace.split(\".\", 1)\n    database_regex = re.compile(r\"^%s$\" % database_name.replace(\"*\", \".*\"))\n    collection_regex = re.compile(r\"^%s$\" % collection_name.replace(\"*\", \".*\"))\n    return re.compile(r\"^%s\\.%s$\" % (database_regex.pattern, collection_regex.pattern))", "idx": 98}
{"namespace": "mongo_connector.util.long_to_bson_ts", "completion": "    return Timestamp(val >> 32, val & 0xFFFFFFFF)", "idx": 99}
{"namespace": "mongo_connector.doc_managers.formatters.DocumentFlattener.format_document", "completion": "        def _kernel(doc, prefix=\"\"):\n            for key in doc:\n                value = doc[key]\n                if isinstance(value, dict):\n                    for inner_k, inner_v in _kernel(value, prefix + key + \".\"):\n                        yield inner_k, inner_v\n                elif isinstance(value, list):\n                    for li, lv in enumerate(value):\n                        for inner_k, inner_v in _kernel({\"%s%s\" % (prefix, key): lv}, prefix + key + \".\"):\n                            yield inner_k, inner_v\n                else:\n                    yield prefix + key, value\n\n        return dict(_kernel(document))", "idx": 100}
{"namespace": "bplustree.memory.open_file_in_dir", "completion": "    if platform.system() == 'Windows':\n        dir_fd = None\n    else:\n        dir_fd = os.open(os.path.dirname(path), os.O_RDONLY)\n    fd = open(path, 'rb+')\n    return fd, dir_fd", "idx": 101}
{"namespace": "bplustree.memory.FileMemory.read_transaction", "completion": "        class ReadTransaction:\n\n            def __enter__(self2):\n                self._lock.reader_lock.acquire()\n\n            def __exit__(self2, exc_type, exc_val, exc_tb):\n                self._lock.reader_lock.release()\n\n        return ReadTransaction()\n\n    ", "idx": 102}
{"namespace": "bplustree.utils.pairwise", "completion": "    a, b = itertools.tee(iterable)\n    next(b, None)\n    return zip(a, b)", "idx": 103}
{"namespace": "bplustree.utils.iter_slice", "completion": "    for i in range(0, len(iterable), n):\n        yield iterable[i:i + n], i + n == len(iterable)", "idx": 104}
{"namespace": "bplustree.serializer.StrSerializer.serialize", "completion": "        data = obj.encode(encoding='utf-8')\n        if len(data) > key_size:\n            raise ValueError('String is too long')\n        return data", "idx": 105}
{"namespace": "psd_tools.utils.pack", "completion": "    return struct.pack(str(\">\" + fmt), *args)", "idx": 106}
{"namespace": "psd_tools.utils.unpack", "completion": "    fmt = str(\">\" + fmt)\n    return struct.unpack(fmt, data)", "idx": 107}
{"namespace": "psd_tools.api.numpy_io.get_pattern", "completion": "    height = pattern.data[3]\n    width = pattern.data[4]\n    pattern_array = np.zeros((height, width, 4), dtype=np.float32)\n    for channel in pattern.channels:\n        if channel.id == 0:\n            pattern_array[:, :, 0] = _parse_array(channel.data, channel.depth)\n        elif channel.id == 1:\n            pattern_array[:, :, 1] = _parse_array(channel.data, channel.depth)\n        elif channel.id == 2:\n            pattern_array[:, :, 2] = _parse_array(channel.data, channel.depth)\n        elif channel.id == 3:\n            pattern_array[:, :, 3] = _parse_array(channel.data, channel.depth)\n    return pattern_array", "idx": 108}
{"namespace": "sqlite_utils.utils.maximize_csv_field_size_limit", "completion": "    csv.field_size_limit(sys.maxsize)\n    while True:\n        try:\n            csv.field_size_limit(csv.field_size_limit() * 2)\n        except OverflowError:\n            break", "idx": 109}
{"namespace": "sqlite_utils.utils.column_affinity", "completion": "    if column_type == \"INT\":\n        return \"INT\"\n    elif column_type == \"CHAR\":\n        return \"CHAR\"\n    elif column_type == \"CLOB\":\n        return \"CLOB\"\n    elif column_type == \"TEXT\":\n        return \"TEXT\"\n    elif column_type == \"BLOB\":\n        return \"BLOB\"\n    elif column_type == \"REAL\":\n        return \"REAL\"\n    elif column_type == \"FLOA\":\n        return \"FLOA\"\n    elif column_type == \"DOUB\":\n        return \"DOUB\"\n    else:\n        return \"TEXT\"", "idx": 110}
{"namespace": "sqlite_utils.utils.decode_base64_values", "completion": "    if isinstance(doc, dict):\n        for key, value in doc.items():\n            if isinstance(value, dict) and \"$base64\" in value:\n                doc[key] = base64.b64decode(value[\"encoded\"])\n            elif isinstance(value, list):\n                for i, item in enumerate(value):\n                    if isinstance(item, dict) and \"$base64\" in item:\n                        value[i] = base64.b64decode(item[\"encoded\"])\n    elif isinstance(doc, list):\n        for item in doc:\n            decode_base64_values(item)\n    return doc", "idx": 111}
{"namespace": "sqlite_utils.utils.chunks", "completion": "    for i in range(0, len(sequence), size):\n        yield sequence[i : i + size]", "idx": 112}
{"namespace": "sqlite_utils.utils.hash_record", "completion": "    if keys is None:\n        keys = record.keys()\n    return hashlib.sha1(\n        json.dumps(\n            {key: record[key] for key in keys},\n            sort_keys=True,\n        ).encode(\"utf-8\")\n    ).hexdigest()", "idx": 113}
{"namespace": "arctic.decorators._get_host", "completion": "    if not store:\n        return {}\n    if isinstance(store, (list, tuple)):\n        store = store[0]\n    return {\n        'library': store.library,\n        'nodes': store.nodes,\n        'host': store.host\n    }", "idx": 114}
{"namespace": "arctic.decorators.mongo_retry", "completion": "    @wraps(f)\n    def wrapper(*args, **kwargs):\n        global _in_retry, _retry_count\n        _retry_count = 0\n        _in_retry = True\n        while True:\n            try:\n                return f(*args, **kwargs)\n            except (AutoReconnect, OperationFailure, DuplicateKeyError, ServerSelectionTimeoutError,\n                    BulkWriteError) as e:\n                _handle_error(f, e, _retry_count, **kwargs)\n                _retry_count += 1\n            except Exception as e:\n                _in_retry = False\n                raise\n\n    return wrapper", "idx": 115}
{"namespace": "arctic._util.are_equals", "completion": "    if isinstance(o1, DataFrame) and isinstance(o2, DataFrame):\n        try:\n            assert_frame_equal(o1, o2, **kwargs)\n            return True\n        except AssertionError:\n            return False\n    else:\n        return o1 == o2", "idx": 116}
{"namespace": "arctic.hooks.register_resolve_mongodb_hook", "completion": "    global _resolve_mongodb_hook\n    _resolve_mongodb_hook = hook", "idx": 117}
{"namespace": "arctic.hooks.register_log_exception_hook", "completion": "    global _log_exception_hook\n    _log_exception_hook = hook", "idx": 118}
{"namespace": "arctic.hooks.register_get_auth_hook", "completion": "    global _get_auth_hook\n    _get_auth_hook = hook\n\n", "idx": 119}
{"namespace": "arctic.store._version_store_utils._split_arrs", "completion": "    if not slices:\n        return array_2d\n\n    slices = np.array(slices)\n    slices = np.concatenate(([0], slices, [array_2d.shape[0]]))\n    return np.split(array_2d, slices[1:-1] - slices[:-1])\n\n", "idx": 120}
{"namespace": "arctic.store._version_store_utils.checksum", "completion": "    pickle_compat_load(doc)\n    return Binary(hashlib.sha1(symbol.encode('utf-8') + pickle.dumps(doc)).digest())\n\n", "idx": 121}
{"namespace": "arctic.store.versioned_item.VersionedItem.__str__", "completion": "        return f\"VersionedItem(symbol={self.symbol},library={self.library},data={self.data},version={self.version},metadata={self.metadata},host={self.host})\"", "idx": 122}
{"namespace": "arctic.store._ndarray_store.NdarrayStore._dtype", "completion": "        if metadata is None:\n            metadata = {}\n        return np.dtype(string, metadata=metadata)", "idx": 123}
{"namespace": "arctic.store._ndarray_store._promote_struct_dtypes", "completion": "    if dtype1.fields is None or dtype2.fields is None:\n        raise ValueError(\"type changes to or from structured array not supported\")\n\n    if dtype1.names is None:\n        return np.promote_types(dtype1, dtype2)\n\n    if dtype1.names != dtype2.names:\n        raise ValueError(\"type changes to or from structured array not supported\")\n\n    dtype1_fields = dict(dtype1.fields)\n    dtype2_fields = dict(dtype2.fields)\n\n    for field in dtype1_fields:\n        if field not in dtype2_fields:\n            raise ValueError(\"type changes to or from structured array not supported\")\n\n        dtype1_fields[field] = _promote_struct_dtypes(dtype1_fields[field], dtype2_fields[field])\n\n    return np.dtype(dtype1_fields)", "idx": 124}
{"namespace": "arctic.chunkstore.passthrough_chunker.PassthroughChunker.exclude", "completion": "        return data", "idx": 125}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.to_chunks", "completion": "        if isinstance(df.index, pd.DatetimeIndex):\n            df = df.reset_index()\n\n        if 'date' in df.columns:\n            df = df.set_index('date')\n\n        if 'date' in df.index.names:\n            df = df.reset_index()\n\n        if 'date' in df.columns:\n            df = df.set_index('date')\n\n        if 'date' in df.index.names:\n            df = df.reset_index()\n\n        if 'date' in df.columns:\n            df = df.set_index('date')\n\n        if 'date' in df.index.names:\n            df = df.reset_index()\n\n        if 'date' in df.columns:\n            df = df.set_index('date')\n\n        if 'date' in df.index.names:\n            df = df.reset_index()\n\n        if 'date' in df.columns:\n            df = df.set_index('date')\n\n        if 'date' in df.index.names:\n            df = df.reset_index()\n\n        if 'date' in df.columns:\n            df = df.set_index('date')\n\n        if 'date' in df.index.names:\n            df = df.reset_index()\n\n        if 'date' in df.columns:\n            df = df.set_index('date')\n\n        if 'date' in df.index.names:\n            df = df.reset_index()\n\n        if 'date' in df.columns:\n            df = df.set_index('date')\n\n        if 'date' in df.index.names:\n            df = df.reset_index()\n\n        if 'date' in df.columns:\n            df = df.set_index('date')\n\n        if 'date' in df.index.names:\n            df = df.reset_index()\n\n        if 'date' in df.columns:\n            df = df.set_index('date')\n\n        if 'date' in df.index.names:\n            df", "idx": 126}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.exclude", "completion": "        if isinstance(range_obj, (pd.DatetimeIndex, tuple)):\n            range_obj = DateRange(range_obj[0], range_obj[-1])\n\n        range_obj = to_pandas_closed_closed(range_obj, add_tz=False)\n        start = range_obj.start\n        end = range_obj.end\n\n        if 'date' in data.index.names:\n            return data[~((data.index.get_level_values('date') >= start) & (data.index.get_level_values('date') <= end))]\n        elif 'date' in data.columns:\n            if start and end:\n                return data[(data.date < start) | (data.date > end)]\n            elif start:\n                return data[(data.date < start)]\n            elif end:\n                return data[(data.date > end)]\n            else:\n                return data\n        else:\n            return data", "idx": 127}
{"namespace": "mopidy.httpclient.format_proxy", "completion": "    if not proxy_config:\n        return None\n\n    scheme = proxy_config.get(\"scheme\", \"http\")\n    host = proxy_config.get(\"hostname\")\n    port = proxy_config.get(\"port\")\n    username = proxy_config.get(\"username\")\n    password = proxy_config.get(\"password\")\n\n    if not host:\n        return None\n\n    if not port:\n        return f\"{scheme}://{host}\"\n\n    if not auth:\n        return f\"{scheme}://{host}:{port}\"\n\n    if not username:\n        return f\"{scheme}://{host}:{port}\"\n\n    if not password:\n        return f\"{scheme}://{username}@{host}:{port}\"\n\n    return f\"{scheme}://{username}:{password}@{host}:{port}\"", "idx": 128}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.filter", "completion": "        if isinstance(range_obj, (pd.DatetimeIndex, tuple)):\n            range_obj = DateRange(range_obj[0], range_obj[-1])\n        if 'date' in data.index.names:\n            return data[(data.index.get_level_values('date') >= range_obj.start) & (data.index.get_level_values('date') <= range_obj.end)]\n        elif 'date' in data.columns:\n            return data[(data.date >= range_obj.start) & (data.date <= range_obj.end)]\n        else:\n            return data", "idx": 129}
{"namespace": "mopidy.config.validators.validate_required", "completion": "    if required and not value:\n        raise ValueError(\"must be set.\")", "idx": 130}
{"namespace": "mopidy.config.validators.validate_choice", "completion": "    if value not in choices:\n        raise ValueError(f\"must be one of {choices}, not {value}.\")", "idx": 131}
{"namespace": "mopidy.config.validators.validate_minimum", "completion": "    if minimum is not None and value < minimum:\n        raise ValueError(f\"{value} must be larger than {minimum}.\")", "idx": 132}
{"namespace": "mopidy.config.validators.validate_maximum", "completion": "    if maximum is not None and value > maximum:\n        raise ValueError(f\"{value} must be smaller than {maximum}.\")", "idx": 133}
{"namespace": "mopidy.config.schemas._did_you_mean", "completion": "    if not choices:\n        return None\n    distances = [(name, _levenshtein(name, choice)) for choice in choices]\n    distances.sort(key=lambda x: x[1])\n    if distances[0][1] <= 3:\n        return distances[0][0]\n    return None", "idx": 134}
{"namespace": "mopidy.config.types.encode", "completion": "    value = str(value)\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(char, \"\\\\\" + char)\n    return value.encode(encoding=\"unicode-escape\")\n\n", "idx": 135}
{"namespace": "mopidy.config.types.decode", "completion": "    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\", \"\\n\", \"\\t\"):\n        value = value.replace(\n            char, char.encode(encoding=\"unicode-escape\").decode()\n        )\n\n    return value", "idx": 136}
{"namespace": "mopidy.config.types.ConfigValue.serialize", "completion": "        if value is None:\n            return \"\"\n        if isinstance(value, _TransformedValue):\n            value = value.original\n        return encode(value)\n\n", "idx": 137}
{"namespace": "mopidy.config.types.Boolean.serialize", "completion": "class Enum(ConfigValue):\n    \"\"\"Enum value.\n\n    Expects one of the given choices, with any casing.\n    \"\"\"\n\n    def __init__(self, choices, optional=False):\n        self._required = not optional\n        self._choices = choices\n\n    def deserialize(self, value):\n        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        validators.validate_choice(value, self._choices)\n        return value\n\n\nclass Url(ConfigValue):\n    \"\"\"URL value.\n\n    Expects a valid URL.\n    \"\"\"\n\n    def __init__(self, optional=False):\n        self._required = not optional\n\n    def deserialize(self, value):\n        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        validators.validate_url(value)\n        return value\n\n\nclass TimeDelta(ConfigValue):\n    \"\"\"Time delta value.\n\n    Expects a time delta in the format ``[<days>] <hours>:<minutes>:<seconds>``.\n    \"\"\"\n\n    def __init__(self, optional=False):\n        self._required = not optional\n\n    def deserialize(self, value):\n        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        validators.validate_timedelta(value)\n        return value\n\n\nclass Time(ConfigValue):\n    \"\"\"Time value.\n\n    Expects a time in the format ``<hours>:<minutes>:<seconds>``.\n    \"\"\"\n\n    def __init__(self, optional=False):\n        self._required = not optional\n\n    def deserialize(self, value):\n        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        valid", "idx": 138}
{"namespace": "hypertools.tools.df2mat.df2mat", "completion": "    ", "idx": 139}
{"namespace": "hypertools._shared.helpers.center", "completion": "    assert type(x) is list, \"Input data to center must be list\"\n    return [i - np.mean(x) for i in x]", "idx": 140}
{"namespace": "hypertools._shared.helpers.group_by_category", "completion": "    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n    return [list(vals).index(i) for i in vals]", "idx": 141}
{"namespace": "hypertools._shared.helpers.vals2colors", "completion": "    # flatten if list of lists\n    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n    palette = sns.color_palette(cmap, res)\n    return [palette[int(np.round(i))] for i in np.linspace(0, res-1, len(vals))]", "idx": 142}
{"namespace": "hypertools._shared.helpers.vals2bins", "completion": "    # flatten if list of lists\n    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n\n    # get bins\n    bins = np.linspace(np.min(vals), np.max(vals)+1, res+1)\n    return [np.digitize(val, bins) - 1 for val in vals]", "idx": 143}
{"namespace": "hypertools._shared.helpers.interp_array", "completion": "    x = np.linspace(0,1,interp_val)\n    y = np.linspace(0,1,interp_val)\n    xx,yy = np.meshgrid(x,y)\n    xx = xx.flatten()\n    yy = yy.flatten()\n    arr_flat = arr.flatten()\n    interp = pchip(xx,arr_flat)\n    return interp(yy).reshape(interp_val,interp_val)", "idx": 144}
{"namespace": "hypertools._shared.helpers.parse_args", "completion": "    if len(args) == 1:\n        if isinstance(args[0], (tuple, list)):\n            args = args[0]\n        else:\n            args = [args]\n    if len(args) != len(x):\n        print('Error: Number of arguments must be equal to number of elements in x')\n        sys.exit()\n    return [tuple(i) for i in list(itertools.product(x, *args))]", "idx": 145}
{"namespace": "hypertools._shared.helpers.parse_kwargs", "completion": "    kwargs_list = []\n    for i, item in enumerate(x):\n        tmp = {}\n        for key, value in kwargs.items():\n            if isinstance(value, (tuple, list)):\n                if len(value) == len(x):\n                    tmp[key] = value[i]\n                else:\n                    print('Error: arguments must be a list of the same length as x')\n                    sys.exit(1)\n            else:\n                tmp[key] = value\n        kwargs_list.append(tmp)\n    return kwargs_list", "idx": 146}
{"namespace": "gif_for_cli.utils._get_default_display_mode", "completion": "    term = environ.get('TERM', '')\n    colorterm = environ.get('COLORTERM', '')\n    if 'truecolor' in term or 'truecolor' in colorterm:\n        return 'truecolor'\n    if '256' in term or '256' in colorterm:\n        return '256'\n    return 'nocolor'", "idx": 147}
{"namespace": "gif_for_cli.utils._pool_type", "completion": "    val = int(val)\n    if val <= 0:\n        raise argparse.ArgumentTypeError('Pool size must be greater than 0.')\n    return val", "idx": 148}
{"namespace": "gif_for_cli.generate.utils.get_avg_for_em", "completion": "    r_sum = 0\n    g_sum = 0\n    b_sum = 0\n    for i in range(cell_height):\n        for j in range(cell_width):\n            r_sum += px[x + j, y + i][0]\n            g_sum += px[x + j, y + i][1]\n            b_sum += px[x + j, y + i][2]\n    return [r_sum / (cell_height * cell_width),\n            g_sum / (cell_height * cell_width),\n            b_sum / (cell_height * cell_width)]", "idx": 149}
{"namespace": "gif_for_cli.generate.utils.process_input_source", "completion": "    if input_source.startswith('https://tenor.com/view/'):\n        gif_id = input_source.split('/')[-1]\n        gif_url = f'https://tenor.com/gifapi/gifs?ids={gif_id}'\n    else:\n        gif_url = f'https://tenor.googleapis.com/v2/search?q={input_source}&key={api_key}&limit=1'\n\n    return gif_url\n\n", "idx": 150}
{"namespace": "hypertools._shared.helpers.reshape_data", "completion": "    # Check if the input data is a list of arrays\n    if not isinstance(x, list):\n        x = [x]\n\n    # Check if the hue is a list of arrays\n    if not isinstance(hue, list):\n        hue = [hue]\n\n    # Check if the labels are a list of arrays\n    if labels is not None:\n        if not isinstance(labels, list):\n            labels = [labels]\n\n    # Check if the input data and hue have the same length\n    if len(x) != len(hue):\n        raise ValueError(\"The input data and hue must have the same length.\")\n\n    # Check if the input data and labels have the same length\n    if labels is not None:\n        if len(x) != len(labels):\n            raise ValueError(\"The input data and labels must have the same length.\")\n\n    # Check if the input data and hue have the same number of dimensions\n    if not all(len(item.shape) == len(x[0].shape) for item in x):\n        raise ValueError(\"The input data and hue must have the same number of dimensions.\")\n\n    # Check if the input data and labels have the same number of dimensions\n    if labels is not None:\n        if not all(len(item.shape) == len(labels[0].shape) for item in labels):\n            raise ValueError(\"The input data and labels must have the same number of dimensions.\")\n\n    # Check if the input data and hue have the same number of dimensions\n    if not all(item.shape[0] == x[0].shape[0] for item in x):\n        raise ValueError(\"The input data and hue must have the same number of rows.\")\n\n    # Check if the input data and labels have the same number of dimensions\n    if labels is not None:\n        if not all(item.shape[0] == labels[0].shape[0] for item in labels):\n            raise ValueError(\"The input data and labels must have the same number of rows.\")\n\n    # Check if the input data and hue have the same number of dimensions\n    if not all(item.", "idx": 151}
{"namespace": "mingus.extra.lilypond.from_Note", "completion": "    # Throw exception\n    if not isinstance(note, Note):\n        return False\n\n    # Process the note\n    if note.octave == -1:\n        result = \",\"\n    elif note.octave == 1:\n        result = \"'\"\n    elif note.octave == 2:\n        result = \",\"\n    elif note.octave == 3:\n        result = \"'\"\n    elif note.octave == 4:\n        result = \",\"\n    elif note.octave == 5:\n        result = \"'\"\n    elif note.octave == 6:\n        result = \",\"\n    elif note.octave == 7:\n        result = \"'\"\n    elif note.octave == 8:\n        result = \",\"\n    elif note.octave == 9:\n        result = \"'\"\n    else:\n        result = str(note.octave)\n\n    # Process the accidentals\n    if note.accidental == \"double_sharp\":\n        result = \"x\" + result\n    elif note.accidental == \"sharp\":\n        result = \"is\" + result\n    elif note.accidental == \"flat\":\n        result = \"es\" + result\n    elif note.accidental == \"double_flat\":\n        result = \"ef\" + result\n\n    # Process the note\n    if note.note == \"C\":\n        result = \"c\" + result\n    elif note.note == \"D\":\n        result = \"d\" + result\n    elif note.note == \"E\":\n        result = \"e\" + result\n    elif note.note == \"F\":\n        result = \"f\" + result\n    elif note.note == \"G\":\n        result = \"g\" + result\n    elif note.note == \"A\":\n        result = \"a\" + result\n    elif note.note == \"B\":\n        result = \"b\" + result\n\n    # Process the octaves\n    if process_octaves:\n        if note.octave == -1:\n            result = \",\"\n        elif note.octave == 1:\n            result = \"'\"\n        elif note.octave == 2:\n            result = \"", "idx": 152}
{"namespace": "mingus.extra.tablature._get_qsize", "completion": "    # Size of a quarter note\n    qsize = width / 3\n    if width <= 60:\n        qsize = width\n    elif 60 < width <= 120:\n        qsize = width / 2\n    return qsize", "idx": 153}
{"namespace": "mingus.core.notes.augment", "completion": "    if note[-1] != \"b\":\n        return note + \"#\"\n    else:\n        return note[:-1]", "idx": 154}
{"namespace": "mingus.core.meter.valid_beat_duration", "completion": "    return duration > 0 and duration & (duration - 1) == 0", "idx": 155}
{"namespace": "mingus.core.notes.diminish", "completion": "    if note[-1] != \"#\":\n        return note + \"b\"\n    else:\n        return note[:-1]", "idx": 156}
{"namespace": "mingus.core.intervals.invert", "completion": "    return interval[::-1]", "idx": 157}
{"namespace": "mingus.core.progressions.parse_string", "completion": "    roman = \"\"\n    acc = 0\n    suff = \"\"\n    for x in progression:\n        if x == \"b\":\n            acc -= 1\n        elif x == \"#\":\n            acc += 1\n        elif x == \"7\":\n            suff = \"7\"\n        else:\n            roman += x\n    return (roman, acc, suff)", "idx": 158}
{"namespace": "exodus_bundler.bundling.bytes_to_int", "completion": "    return int.from_bytes(bytes, byteorder=byteorder)", "idx": 159}
{"namespace": "exodus_bundler.templating.render_template", "completion": "    for key, value in context.items():\n        string = string.replace('{{' + key + '}}', str(value))\n    return string", "idx": 160}
{"namespace": "exodus_bundler.input_parsing.strip_pid_prefix", "completion": "    \"\"\"\n    This function removes the `[pid XXX] ` prefix from the input line if it is present.\n    Input-Output Arguments\n    :param line: String. The input line that may contain the `[pid XXX] ` prefix.\n    :return: String. The line with the `[pid XXX] ` prefix removed if present. If not present, the original line is returned.\n    \"\"\"\n\n    \"\"\"\n    This function removes the `[pid XXX] ` prefix from the input line if it is present.\n    Input-Output Arguments\n    :param line: String. The input line that may contain the `[pid XXX] ` prefix.\n    :return: String. The line with the `[pid XXX] ` prefix removed if present. If not present, the original line is returned.\n    \"\"\"\n\n    \"\"\"\n    This function removes the `[pid XXX] ` prefix from the input line if it is present.\n    Input-Output Arguments\n    :param line: String. The input line that may contain the `[pid XXX] ` prefix.\n    :return: String. The line with the `[pid XXX] ` prefix removed if present. If not present, the original line is returned.\n    \"\"\"\n\n    \"\"\"\n    This function removes the `[pid XXX] ` prefix from the input line if it is present.\n    Input-Output Arguments\n    :param line: String. The input line that may contain the `[pid XXX] ` prefix.\n    :return: String. The line with the `[pid XXX] ` prefix removed if present. If not present, the original line is returned.\n    \"\"\"\n\n    \"\"\"\n    This function removes the `[pid XXX] ` prefix from the input line if it is present.\n    Input-Output Arguments\n    :param line: String. The input line that may contain the `[pid XXX] ` prefix.\n    :return: String. The line with the `[pid XXX] ` prefix removed if present. If not present, the original line is returned.\n    \"\"\"\n\n    \"\"\"\n    This function removes the `[pid XXX] ` prefix from the input line if it is present.\n    Input-Output Arguments\n    :param line: String. The input line that", "idx": 161}
{"namespace": "fs.path.abspath", "completion": "    if path.startswith(\"/\"):\n        return path\n    return \"/\" + path", "idx": 162}
{"namespace": "fs.path.combine", "completion": "    if path1.endswith(\"/\"):\n        return path1 + path2\n    return path1 + \"/\" + path2", "idx": 163}
{"namespace": "fs.path.split", "completion": "    if path.endswith(\"/\"):\n        path = path[:-1]\n    if path.endswith(\".\"):\n        path = path[:-1]\n    if path.endswith(\"/\"):\n        path = path[:-1]\n    if path.endswith(\".\"):\n        path = path[:-1]\n    if path.endswith(\"/\"):\n        path = path[:-1]\n    if path.endswith(\".\"):\n        path = path[:-1]\n    if path.endswith(\"/\"):\n        path = path[:-1]\n    if path.endswith(\".\"):\n        path = path[:-1]\n    if path.endswith(\"/\"):\n        path = path[:-1]\n    if path.endswith(\".\"):\n        path = path[:-1]\n    if path.endswith(\"/\"):\n        path = path[:-1]\n    if path.endswith(\".\"):\n        path = path[:-1]\n    if path.endswith(\"/\"):\n        path = path[:-1]\n    if path.endswith(\".\"):\n        path = path[:-1]\n    if path.endswith(\"/\"):\n        path = path[:-1]\n    if path.endswith(\".\"):\n        path = path[:-1]\n    if path.endswith(\"/\"):\n        path = path[:-1]\n    if path.endswith(\".\"):\n        path = path[:-1]\n    if path.endswith(\"/\"):\n        path = path[:-1]\n    if path.endswith(\".\"):\n        path = path[:-1]\n    if path.endswith(\"/\"):\n        path = path[:-1]\n    if path.endswith(\".\"):\n        path = path[:-1]\n    if path.endswith(\"/\"):\n        path = path[:-1]\n    if path.endswith(\".\"):\n        path = path[:-1]\n    if path.endswith(\"/\"):\n        path = path[:-1]", "idx": 164}
{"namespace": "fs.path.isparent", "completion": "    if path1 == path2:\n        return True\n    if path1 == \"/\":\n        return False\n    if path1.endswith(\"/\"):\n        path1 = path1[:-1]\n    if path2.endswith(\"/\"):\n        path2 = path2[:-1]\n    return path2.startswith(path1 + \"/\")", "idx": 165}
{"namespace": "fs.path.forcedir", "completion": "    if path.endswith(\"/\"):\n        return path\n    return path + \"/\"", "idx": 166}
{"namespace": "fs.wildcard.match_any", "completion": "    if not patterns:\n        return True\n    return any(match(pattern, name) for pattern in patterns)", "idx": 167}
{"namespace": "fs.wildcard.imatch_any", "completion": "    if not patterns:\n        return True\n    return any(imatch(pattern, name) for pattern in patterns)", "idx": 168}
{"namespace": "wal_e.cmd.parse_boolean_envvar", "completion": "    if val.lower() in ['false', '0']:\n        return False\n    elif val.lower() in ['true', '1']:\n        return True\n    else:\n        raise UserException(\n            msg='Invalid value for boolean environment variable',\n            detail='The value of the environment variable is {0}.'.format(val),\n            hint='The value must be one of \"false\", \"0\", \"true\", \"1\".')", "idx": 169}
{"namespace": "wal_e.log_help.get_log_destinations", "completion": "    log_destinations = os.getenv('WALE_LOG_DESTINATION', 'stderr,syslog')\n    return log_destinations.split(',')", "idx": 170}
{"namespace": "wal_e.log_help.WalELogger._fmt_structured", "completion": "        # Sort the dictionary lexically, except for the time and pid.\n        # The time and pid are always the first two elements.\n        sorted_keys = sorted(d.keys())\n        sorted_keys.remove('time')\n        sorted_keys.remove('pid')\n        sorted_keys = ['time', 'pid'] + sorted_keys\n\n        # Format the dictionary into a string.\n        s = ''\n        for k in sorted_keys:\n            s += ' ' + k + '=' + str(d[k])\n\n        # Format the time into a string.\n        d['time'] = datetime.datetime.utcnow().isoformat() + '-00'\n\n        return s", "idx": 171}
{"namespace": "wal_e.tar_partition._fsync_files", "completion": "    for filename in filenames:\n        with open(filename, 'rb') as f:\n            f.flush()\n            os.fsync(f.fileno())\n\n    # fsync the directory where the file is created\n    dirname = os.path.dirname(filenames[0])\n    os.fsync(os.open(dirname, os.O_RDONLY))", "idx": 172}
{"namespace": "wal_e.blobstore.file.calling_format.Bucket.list", "completion": "        path = os.path.join(\"/\", prefix.strip(\"/\"))\n        file_paths = [os.path.join(path, f) for f in os.listdir(path)]\n        return [FileKey(bucket=self, name=f) for f in file_paths]", "idx": 173}
{"namespace": "pyinfra.operations.util.files.unix_path_join", "completion": "    return \"/\".join(path_parts)", "idx": 174}
{"namespace": "pyinfra.operations.server.shell", "completion": "    if isinstance(commands, str):\n        commands = [commands]\n\n    for command in commands:\n        yield command\n\n", "idx": 175}
{"namespace": "pyinfra.api.util.try_int", "completion": "def try_int(value):\n    try:\n        return int(value)\n    except (ValueError, TypeError):\n        return value\n\n\ndef try_bool(value):\n    if isinstance(value, bool):\n        return value\n\n    if isinstance(value, str):\n        value = value.lower()\n\n    if value in (\"true\", \"yes\", \"on\", \"1\"):\n        return True\n\n    if value in (\"false\", \"no\", \"off\", \"0\"):\n        return False\n\n    raise ValueError(\"Cannot convert {0} to a boolean\".format(value))\n\n\ndef try_float(value):\n    try:\n        return float(value)\n    except (ValueError, TypeError):\n        return value\n\n\ndef try_list(value):\n    if isinstance(value, list):\n        return value\n\n    if isinstance(value, str):\n        return [e.strip() for e in value.split(\",\")]\n\n    return [value]\n\n\ndef try_dict(value):\n    if isinstance(value, dict):\n        return value\n\n    if isinstance(value, str):\n        return dict(e.split(\"=\") for e in value.split(\",\"))\n\n    return {value: True}\n\n\ndef try_tuple(value):\n    if isinstance(value, tuple):\n        return value\n\n    if isinstance(value, str):\n        return tuple(e.strip() for e in value.split(\",\"))\n\n    return (value,)\n\n\ndef try_string(value):\n    if isinstance(value, str):\n        return value\n\n    return str(value)\n\n\ndef try_string_list(value):\n    if isinstance(value, str):\n        return [e.strip() for e in value.split(\",\")]\n\n    return [str(e) for e in value]\n\n\ndef try_string_tuple(value):\n    if isinstance(value, str):\n        return tuple(e.strip() for e in value.split(\",\"))\n\n    return tuple(str(e) for", "idx": 176}
{"namespace": "mrjob.job.MRJob.mr_job_script", "completion": "        return inspect.getfile(cls)\n", "idx": 177}
{"namespace": "mrjob.compat.map_version", "completion": "    if version_map is None:\n        return None\n\n    if isinstance(version_map, list):\n        for version_tuple in version_map:\n            if version_tuple[0] <= LooseVersion(version):\n                return version_tuple[1]\n    else:\n        for version_str, value in sorted(version_map.items()):\n            if LooseVersion(version_str) <= LooseVersion(version):\n                return value\n\n    return None", "idx": 178}
{"namespace": "mrjob.conf.combine_values", "completion": "    for value in reversed(values):\n        if value is not None:\n            return value\n    return None", "idx": 179}
{"namespace": "mrjob.protocol.BytesProtocol.read", "completion": "        key_value = line.split(b'\\t', 1)\n        if len(key_value) == 1:\n            key_value.append(None)\n\n        return tuple(key_value)", "idx": 180}
{"namespace": "mrjob.protocol.TextProtocol.write", "completion": "        if key is not None:\n            key = key.encode('utf_8')\n        if value is not None:\n            value = value.encode('utf_8')\n        return b'\\t'.join(x for x in (key, value) if x is not None)", "idx": 181}
{"namespace": "mrjob.protocol.TextProtocol.read", "completion": "        key_value = line.split(b'\\t', 1)\n        if len(key_value) == 1:\n            key_value.append(None)\n\n        try:\n            return tuple(x.decode('utf_8') for x in key_value)\n        except UnicodeDecodeError:\n            return tuple(x.decode('latin_1') for x in key_value)", "idx": 182}
{"namespace": "mrjob.protocol.TextValueProtocol.read", "completion": "        try:\n            return (None, line.decode('utf_8'))\n        except UnicodeDecodeError:\n            return (None, line.decode('latin_1'))", "idx": 183}
{"namespace": "mrjob.util.file_ext", "completion": "    filename = filename.lstrip('.')\n    index = filename.find('.')\n    if index == -1:\n        return ''\n    else:\n        return filename[index:]\n\n", "idx": 184}
{"namespace": "mrjob.util.cmd_line", "completion": "    return ' '.join(pipes.quote(str(arg)) for arg in args)\n\n", "idx": 185}
{"namespace": "mrjob.util.save_cwd", "completion": "    original_cwd = os.getcwd()\n\n    try:\n        yield\n\n    finally:\n        os.chdir(original_cwd)\n\n", "idx": 186}
{"namespace": "mrjob.util.save_sys_std", "completion": "    original_stdin = sys.stdin\n    original_stdout = sys.stdout\n    original_stderr = sys.stderr\n\n    try:\n        yield\n\n    finally:\n        sys.stdin = original_stdin\n        sys.stdout = original_stdout\n        sys.stderr = original_stderr", "idx": 187}
{"namespace": "mrjob.util.unarchive", "completion": "    if is_zipfile(archive_path):\n        with ZipFile(archive_path, 'r') as zip_file:\n            zip_file.extractall(dest)\n    else:\n        with tarfile.open(archive_path, 'r:*') as tar_file:\n            tar_file.extractall(dest)", "idx": 188}
{"namespace": "mrjob.util.unique", "completion": "    seen = set()\n    for item in items:\n        if item not in seen:\n            seen.add(item)\n            yield item", "idx": 189}
{"namespace": "mrjob.parse.urlparse", "completion": "    if allow_fragments:\n        return urlparse_buggy(urlstring, scheme, *args, **kwargs)\n    else:\n        # split the fragment off\n        components = urlparse_buggy(urlstring, scheme, *args, **kwargs)\n        if components.fragment:\n            urlstring = urlstring.replace('#' + components.fragment, '')\n            components = urlparse_buggy(urlstring, scheme, *args, **kwargs)\n            components = ParseResult(\n                scheme=components.scheme,\n                netloc=components.netloc,\n                path=components.path,\n                params=components.params,\n                query=components.query,\n                fragment=components.fragment,\n            )\n        return components", "idx": 190}
{"namespace": "mrjob.util.which", "completion": "    if path is None:\n        path = os.environ['PATH']\n\n    if os.path.isabs(cmd):\n        if os.path.exists(cmd):\n            return cmd\n        else:\n            return None\n\n    if os.path.dirname(cmd):\n        return None\n\n    for dir in path.split(os.pathsep):\n        dir = dir.strip('\"')\n        exe_file = os.path.join(dir, cmd)\n        if os.path.isfile(exe_file) and os.access(exe_file, os.X_OK):\n            return exe_file\n\n    return None\n\n", "idx": 191}
{"namespace": "sshuttle.ssh.parse_hostport", "completion": "    if not rhostport:\n        return None, None, None, None\n\n    m = re.match(r'^(?:(?P<username>[^@]+)@)?(?P<host>[^:]+)(?::(?P<port>\\d+))?$', rhostport)\n    if not m:\n        raise Fatal(\"Invalid host:port: %s\" % rhostport)\n\n    username = m.group('username')\n    host = m.group('host')\n    port = m.group('port')\n    if port:\n        port = int(port)\n\n    if username:\n        m = re.match(r'^(?P<username>[^:]+)(?::(?P<password>.+))?$', username)\n        username = m.group('username')\n        password = m.group('password')\n    else:\n        password = None\n\n    return username, password, port, host", "idx": 192}
{"namespace": "flower.utils.search.stringified_dict_contains_value", "completion": "    if not str_dict:\n        return False\n    if not key:\n        return False\n    if not value:\n        return True\n    if not str_dict.startswith('{'):\n        return False\n    if not str_dict.endswith('}'):\n        return False\n    str_dict = str_dict[1:-1]\n    if not str_dict:\n        return False\n    for kv in str_dict.split(','):\n        k, v = kv.split(':')\n        if k.strip() == key.strip() and v.strip() == value.strip():\n            return True\n    return False", "idx": 193}
{"namespace": "flower.utils.abs_path", "completion": "    path = os.path.expanduser(path)\n    if not os.path.isabs(path):\n        path = os.path.join(os.getcwd(), path)\n    return path", "idx": 194}
{"namespace": "flower.utils.strtobool", "completion": "    val = val.lower()\n    if val in ('y', 'yes', 't', 'true', 'on', '1'):\n        return 1\n    if val in ('n', 'no', 'f', 'false', 'off', '0'):\n        return 0\n    raise ValueError(\"invalid truth value %r\" % (val,))", "idx": 195}
{"namespace": "sshuttle.methods.get_method", "completion": "    try:\n        module = importlib.import_module(\"sshuttle.methods.%s\" % method_name)\n    except ImportError:\n        raise Fatal(\"Method '%s' not found.\" % method_name)\n\n    try:\n        method = module.Method()\n    except AttributeError:\n        raise Fatal(\"Method '%s' does not have a Method class.\" % method_name)\n\n    if not isinstance(method, BaseMethod):\n        raise Fatal(\"Method '%s' does not inherit from BaseMethod.\" %\n                    method_name)\n\n    return method", "idx": 196}
{"namespace": "trailscraper.iam.all_known_iam_permissions", "completion": "    script_dir = os.path.dirname(os.path.realpath(__file__))\n    with open(os.path.join(script_dir, 'known-iam-actions.txt'), 'r') as f:\n        return set(f.read().splitlines())", "idx": 197}
{"namespace": "trailscraper.cloudtrail.parse_records", "completion": "    return list(filter(None, map(_parse_record, json_records)))", "idx": 198}
{"namespace": "pycoin.satoshi.IntStreamer.IntStreamer.int_to_script_bytes", "completion": "        if v == 0:\n            return b''\n        if v < 0:\n            v = -v\n            is_negative = True\n        else:\n            is_negative = False\n        s = bytearray()\n        while v > 0:\n            s.append(v & 0xff)\n            v >>= 8\n        if is_negative:\n            s[0] |= 0x80\n        s.reverse()\n        return bytes(s)", "idx": 199}
{"namespace": "pycoin.satoshi.stackops.do_OP_2DROP", "completion": "    stack.pop()\n    stack.pop()\n\n", "idx": 200}
{"namespace": "pycoin.satoshi.stackops.do_OP_2DUP", "completion": "    stack.append(stack[-2])\n    stack.append(stack[-2])\n\n", "idx": 201}
{"namespace": "pycoin.satoshi.stackops.do_OP_3DUP", "completion": "    stack.append(stack[-3])\n    stack.append(stack[-3])\n    stack.append(stack[-3])\n\n", "idx": 202}
{"namespace": "trailscraper.s3_download._s3_key_prefixes", "completion": "    delta = to_date - from_date\n    dates = [from_date + datetime.timedelta(days=i) for i in range(delta.days + 1)]\n    prefixes = []\n    for org_id in org_ids:\n        for account_id in account_ids:\n            for region in regions:\n                for date in dates:\n                    prefixes.append(_s3_key_prefix_for_org_trails(prefix, date, org_id, account_id, region))\n    return prefixes", "idx": 203}
{"namespace": "pycoin.satoshi.stackops.do_OP_2OVER", "completion": "    stack.append(stack[-4])\n    stack.append(stack[-4])\n\n", "idx": 204}
{"namespace": "pycoin.satoshi.stackops.do_OP_2SWAP", "completion": "    stack.append(stack.pop(-3))\n    stack.append(stack.pop(-3))\n\n", "idx": 205}
{"namespace": "pycoin.satoshi.stackops.do_OP_IFDUP", "completion": "    if stack[-1]:\n        stack.append(stack[-1])\n\n", "idx": 206}
{"namespace": "pycoin.satoshi.stackops.do_OP_NIP", "completion": "    stack.append(stack.pop(-2))\n\n", "idx": 207}
{"namespace": "pycoin.satoshi.stackops.do_OP_TUCK", "completion": "    v1 = stack.pop()\n    v2 = stack.pop()\n    stack.append(v1)\n    stack.append(v2)\n    stack.append(v1)\n\n", "idx": 208}
{"namespace": "pycoin.satoshi.stackops.do_OP_CAT", "completion": "    stack.append(stack.pop() + stack.pop())\n\n", "idx": 209}
{"namespace": "pycoin.crack.ecdsa.crack_secret_exponent_from_k", "completion": "    # s = v / k + (se * r) / k\n    # so\n    # k * s = v + (se * r)\n    # so\n    # k = (v + (se * r)) / s\n\n    r, s = sig\n    k_inv = generator.inverse(k)\n    return (signed_value * k_inv + r * s * k_inv) % generator.order()", "idx": 210}
{"namespace": "pycoin.crack.ecdsa.crack_k_from_sigs", "completion": "    r1, s1 = sig1\n    r2, s2 = sig2\n    k = (val1 - val2) * generator.inverse(r1 - r2) % generator.order()\n    return k", "idx": 211}
{"namespace": "pycoin.message.make_parser_and_packer.standard_streamer", "completion": "    streamer = Streamer(parse_satoshi_int)\n    for name, (parse_func, stream_func) in parsing_functions:\n        streamer.register_parsing_function(name, parse_func)\n        streamer.register_streaming_function(name, stream_func)\n    return streamer", "idx": 212}
{"namespace": "pycoin.key.subpaths.subpaths_for_path_range", "completion": "    path_range = path_range.strip()\n    if not path_range:\n        return\n\n    path_range_parts = path_range.split('/')\n    if len(path_range_parts) != 3:\n        return\n\n    path_range_parts[0] = path_range_parts[0].split('-')\n    path_range_parts[1] = path_range_parts[1].split('-')\n    path_range_parts[2] = path_range_parts[2].split('-')\n\n    for path_range_part in path_range_parts:\n        if len(path_range_part) != 2:\n            return\n\n    for path_range_part in path_range_parts:\n        if not path_range_part[0].isdigit() or not path_range_part[1].isdigit():\n            return\n\n    for path_range_part in path_range_parts:\n        if int(path_range_part[0]) > int(path_range_part[1]):\n            return\n\n    for path_range_part in path_range_parts:\n        if int(path_range_part[0]) < 0 or int(path_range_part[1]) < 0:\n            return\n\n    for path_range_part in path_range_parts:\n        if int(path_range_part[0]) > 99 or int(path_range_part[1]) > 99:\n            return\n\n    for path_range_part in path_range_parts:\n        if int(path_range_part[0]) < 10:\n            path_range_part[0] = '0' + path_range_part[0]\n        if int(path_range_part[1]) < 10:\n            path_range_part[1] = '0' + path_range_part[1]\n\n    for path_range_part in path_range_parts:\n        if int(path_range_part[0]) > 99:\n            path_range_part[0] = path_range_part[0", "idx": 213}
{"namespace": "pyt.core.project_handler._is_python_file", "completion": "    return path.endswith('.py')", "idx": 214}
{"namespace": "pycoin.encoding.hexbytes.h2b", "completion": "    return binascii.unhexlify(h)", "idx": 215}
{"namespace": "zxcvbn.scoring.calc_average_degree", "completion": "    total_degree = 0\n    for node, neighbors in graph.items():\n        total_degree += len(neighbors)\n    return total_degree / len(graph)\n\n", "idx": 216}
{"namespace": "zxcvbn.scoring.nCk", "completion": "    return factorial(n) / (factorial(k) * factorial(n - k))", "idx": 217}
{"namespace": "zxcvbn.matching.relevant_l33t_subtable", "completion": "    chars = list(password)\n    subtable = {}\n    for char in chars:\n        if char in table:\n            subtable[char] = table[char]\n    return subtable", "idx": 218}
{"namespace": "zxcvbn.matching.translate", "completion": "    for key, value in chr_map.items():\n        string = string.replace(key, value)\n    return string.split()", "idx": 219}
{"namespace": "tools.cgrep.get_nets", "completion": "  results = []\n  for obj in objects:\n    results.append((obj, db.GetNet(obj)))\n  return results", "idx": 220}
{"namespace": "tools.cgrep.get_ports", "completion": "  results = []\n  for svc in svc_group:\n    svc_obj = db.GetSvc(svc)\n    for port in svc_obj:\n      results.append((svc, str(port)))\n  return results", "idx": 221}
{"namespace": "tools.cgrep.compare_ip_token", "completion": "  results = []\n  for ip in options.ip:\n    ip = nacaddr.IP(ip)\n    for token in options.token:\n      token = db.GetNet(token)\n      if ip in token:\n        results.append(str(ip))\n  return results", "idx": 222}
{"namespace": "tools.cgrep.get_services", "completion": "  port = options.port[0]\n  protocol = options.port[1]\n  results = []\n  for svc in db.GetServiceList():\n    if db.GetService(svc).port == port and db.GetService(svc).protocol == protocol:\n      results.append(svc)\n  return port, protocol, results", "idx": 223}
{"namespace": "asyncssh.packet.String", "completion": "    if isinstance(value, str):\n        value = value.encode('utf-8')\n\n    return UInt32(len(value)) + value", "idx": 224}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_cmd_counts", "completion": "    seq1_counts_ls = copy.deepcopy(seq1_counts)\n    seq2_counts_ls = copy.deepcopy(seq2_counts)\n\n    cmds: List[str] = list(seq1_counts.keys()) + [start_token, end_token, unk_token]\n    for cmd in cmds:\n        seq1_counts_ls[cmd] += 1\n\n    for cmd1 in cmds:\n        for cmd2 in cmds:\n            if cmd2 in seq2_counts_ls[cmd1]:\n                seq2_counts_ls[cmd1][cmd2] += 1\n\n    return seq1_counts_ls, seq2_counts_ls", "idx": 225}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_param_counts", "completion": "    param_counts_ls = copy.deepcopy(param_counts)\n    cmd_param_counts_ls = copy.deepcopy(cmd_param_counts)\n\n    params: List[str] = list(param_counts_ls.keys()) + [unk_token]\n    for cmd in cmds:\n        for param in params:\n            if param in cmd_param_counts_ls[cmd] or param == unk_token:\n                param_counts_ls[param] += 1\n                cmd_param_counts_ls[cmd][param] += 1\n\n    return param_counts_ls, cmd_param_counts_ls", "idx": 226}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_value_counts", "completion": "    value_counts_ls = copy.deepcopy(value_counts)\n    param_value_counts_ls = copy.deepcopy(param_value_counts)\n\n    values: List[str] = list(value_counts.keys()) + [unk_token]\n    for param in params:\n        for value in values:\n            if value in param_value_counts_ls[param] or value == unk_token:\n                value_counts_ls[value] += 1\n                param_value_counts_ls[param][value] += 1\n\n    return value_counts_ls, param_value_counts_ls", "idx": 227}
{"namespace": "diffprivlib.validation.check_epsilon_delta", "completion": "    if not isinstance(epsilon, Real):\n        raise TypeError(f\"Epsilon must be numeric, got {type(epsilon)}.\")\n    if not isinstance(delta, Real):\n        raise TypeError(f\"Delta must be numeric, got {type(delta)}.\")\n\n    if epsilon < 0:\n        raise ValueError(f\"Epsilon must be non-negative, got {epsilon}.\")\n    if not 0 <= delta <= 1:\n        raise ValueError(f\"Delta must be on the unit interval, [0, 1], got {delta}.\")\n\n    if epsilon == 0 and delta == 0 and not allow_zero:\n        raise ValueError(\"Epsilon and Delta cannot both be zero.\")\n\n", "idx": 228}
{"namespace": "diffprivlib.utils.check_random_state", "completion": "    if seed is None:\n        if secure:\n            return secrets.SystemRandom()\n        else:\n            return np.random.RandomState()\n    elif isinstance(seed, int):\n        return np.random.RandomState(seed)\n    elif isinstance(seed, np.random.RandomState):\n        return seed\n    elif isinstance(seed, secrets.SystemRandom):\n        return seed\n    else:\n        raise ValueError(\"seed must be None, an int, or a RandomState instance\")", "idx": 229}
{"namespace": "diffprivlib.validation.clip_to_norm", "completion": "    if not isinstance(array, np.ndarray):\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\n\n    if array.ndim != 2:\n        raise ValueError(f\"Input array must be 2-dimensional, got {array.ndim} dimensions.\")\n\n    if not isinstance(clip, Real):\n        raise TypeError(f\"Clip value must be numeric, got {type(clip)}.\")\n\n    if clip <= 0:\n        raise ValueError(f\"Clip value must be strictly positive, got {clip}.\")\n\n    norms = np.linalg.norm(array, axis=1)\n    clipped_array = array.copy()\n\n    for i in range(array.shape[0]):\n        if norms[i] > clip:\n            clipped_array[i] = array[i] * clip / norms[i]\n\n    return clipped_array", "idx": 230}
{"namespace": "diffprivlib.models.pca.PCA.fit_transform", "completion": "        self.fit(X, y)\n        return self.transform(X)\n", "idx": 231}
{"namespace": "discord.utils.get_slots", "completion": "    for cls in cls.mro():\n        for slot in cls.__slots__:\n            yield slot", "idx": 232}
{"namespace": "discord.utils.is_inside_class", "completion": "    if not hasattr(func, '__qualname__'):\n        return False\n\n    qualname = func.__qualname__\n    if '.' not in qualname:\n        return False\n\n    # If the function is defined inside a class, the qualname has a dotted path\n    # denoting which class it belongs to. So, e.g. for A.foo the qualname\n    # would be A.foo while a global foo() would just be foo.\n    #\n    # Unfortunately, for nested functions this breaks. So inside an outer\n    # function named outer, those two would end up having a qualname with\n    # outer.<locals>.A.foo and outer.<locals>.foo\n    #\n    # So we check for the presence of <locals> in the qualname to determine\n    # if the function is defined inside a class.\n    return '<locals>' in qualname", "idx": 233}
{"namespace": "faker.utils.decorators.slugify", "completion": "    def wrapper(*args: Tuple[T, ...], **kwargs: Dict[str, T]) -> str:\n        return text.slugify(fn(*args, **kwargs))\n\n    return wrapper", "idx": 234}
{"namespace": "faker.utils.decorators.slugify_domain", "completion": "    def wrapper(*args: Tuple[T, ...], **kwargs: Dict[str, T]) -> str:\n        return text.slugify(fn(*args, **kwargs), allow_dots=True)\n\n    return wrapper", "idx": 235}
{"namespace": "faker.utils.decorators.slugify_unicode", "completion": "    @wraps(fn)\n    def wrapper(*args: Tuple[T, ...], **kwargs: Dict[str, T]) -> str:\n        return text.slugify_unicode(fn(*args, **kwargs))\n\n    return wrapper", "idx": 236}
{"namespace": "faker.utils.loading.get_path", "completion": "    if getattr(sys, \"_MEIPASS\", False):\n        # PyInstaller\n        if getattr(sys, \"_MEIPYCACHE\", False):\n            # PyInstaller with --onefile\n            return sys._MEIPASS\n        else:\n            # PyInstaller with --onedir\n            return sys._MEIPYCACHE\n    else:\n        # Normal Python\n        if module.__file__ is None:\n            raise RuntimeError(f\"Can't find path from module `{module}.\")\n        return str(Path(module.__file__).parent)", "idx": 237}
{"namespace": "faker.utils.checksums.luhn_checksum", "completion": "    digits = [int(d) for d in str(number)]\n    odd_digits = digits[-1::-2]\n    even_digits = digits[-2::-2]\n    checksum = 0\n    checksum += sum(odd_digits)\n    for d in even_digits:\n        checksum += sum(divmod(d * 2, 10))\n    return checksum % 10", "idx": 238}
{"namespace": "faker.utils.datasets.add_ordereddicts", "completion": "    return OrderedDict(chain.from_iterable(odict.items() for odict in odicts))", "idx": 239}
{"namespace": "faker.providers.person.pl_PL.checksum_identity_card_number", "completion": "    weights = [7, 3, 1, 0, 7, 3, 1, 7, 3]\n    characters = [int(char) for char in characters]\n    check_digit = sum(weight * char for weight, char in zip(weights, characters)) % 10\n    return check_digit", "idx": 240}
{"namespace": "faker.providers.company.pl_PL.regon_checksum", "completion": "    weights_for_check_digit = [8, 9, 2, 3, 4, 5, 6, 7]\n    check_digit = 0\n\n    for i in range(0, 8):\n        check_digit += weights_for_check_digit[i] * digits[i]\n\n    check_digit %= 11\n\n    if check_digit == 10:\n        check_digit = 0\n\n    return check_digit", "idx": 241}
{"namespace": "faker.providers.company.ru_RU.calculate_checksum", "completion": "    factors = [3, 7, 2, 4, 10, 3, 5, 9, 4, 6, 8]\n    checksum = 0\n    for i in range(len(value)):\n        checksum += int(value[i]) * factors[i]\n    return str(checksum % 11 % 10)", "idx": 242}
{"namespace": "faker.providers.company.pl_PL.local_regon_checksum", "completion": "    weights_for_check_digit = [2, 4, 8, 5, 0, 9, 7, 3, 6, 1, 2, 4, 8]\n    check_digit = 0\n\n    for i in range(0, 13):\n        check_digit += weights_for_check_digit[i] * digits[i]\n\n    check_digit %= 11\n\n    if check_digit == 10:\n        check_digit = 0\n\n    return check_digit", "idx": 243}
{"namespace": "faker.providers.company.pl_PL.company_vat_checksum", "completion": "    weights_for_check_digit = [6, 5, 7, 2, 3, 4, 5, 6, 7]\n    check_digit = 0\n\n    for i in range(0, 9):\n        check_digit += weights_for_check_digit[i] * digits[i]\n\n    check_digit %= 11\n\n    if check_digit == 10:\n        check_digit = 0\n\n    return check_digit", "idx": 244}
{"namespace": "faker.providers.company.pt_BR.company_id_checksum", "completion": "    weights = [6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 2]\n    checksum = sum(d * w for d, w in zip(digits, weights)) % 11\n    if checksum < 2:\n        return [checksum]\n    return [checksum, 11 - checksum]", "idx": 245}
{"namespace": "faker.providers.misc.Provider.binary", "completion": "        return os.urandom(length)", "idx": 246}
{"namespace": "faker.providers.python.Provider.pystr", "completion": "        if min_chars is None:\n            min_chars = self.random_int(0, max_chars)\n        return prefix + self.generator.lexify(self.generator.random_string(min_chars, max_chars)) + suffix\n", "idx": 247}
{"namespace": "dash._utils.AttributeDict.set_read_only", "completion": "        if not isinstance(names, list):\n            names = [names]\n        if not isinstance(msg, list):\n            msg = [msg]\n        if len(names) != len(msg):\n            raise ValueError(\"Number of names and messages must be equal\")\n        if not self.__dict__.get(\"_read_only\"):\n            self.__dict__[\"_read_only\"] = {}\n        for name, message in zip(names, msg):\n            self._read_only[name] = message", "idx": 248}
{"namespace": "dash._utils.AttributeDict.first", "completion": "        if not names:\n            return self.get(next(iter(self)))\n        for name in names:\n            if name in self:\n                return self[name]\n", "idx": 249}
{"namespace": "dash._get_paths.app_get_asset_url", "completion": "    if config.assets_external_path:\n        return config.assets_external_path + path\n    if config.requests_pathname_prefix:\n        return config.requests_pathname_prefix + path\n    raise exceptions.AssetNotFound(\n        f\"\"\"\n        Asset not found: {path}\n        You must either set the assets_external_path or requests_pathname_prefix\n        configuration options.\n        \"\"\"\n    )", "idx": 250}
{"namespace": "peewee.sort_models", "completion": "    # Create a graph of the models and their dependencies.\n    graph = {}\n    for model in models:\n        graph[model] = model._meta.model_graph(refs=True, backrefs=False)\n\n    # Sort the models using a depth-first search.\n    sorted_models = []\n    seen = set()\n    stack = [(model, None) for model in models]\n    while stack:\n        model, parent = stack.pop()\n        if model in seen:\n            continue\n        seen.add(model)\n        sorted_models.append(model)\n        for _, rel_model, _ in graph[model]:\n            if rel_model is not parent:\n                stack.append((rel_model, model))\n\n    # Reverse the sorted list to get the correct order.\n    sorted_models.reverse()\n    return sorted_models", "idx": 251}
{"namespace": "dash._grouping.grouping_len", "completion": "    if isinstance(grouping, (tuple, list)):\n        return sum(grouping_len(g) for g in grouping)\n\n    if isinstance(grouping, dict):\n        return sum(grouping_len(g) for g in grouping.values())\n\n    return 1", "idx": 252}
{"namespace": "playhouse.kv.KeyValue.get", "completion": "        try:\n            return self[key]\n        except KeyError:\n            return default\n\n    ", "idx": 253}
{"namespace": "playhouse.kv.KeyValue.setdefault", "completion": "        if key not in self:\n            self[key] = default\n        return self[key]", "idx": 254}
{"namespace": "sslyze.plugins.certificate_info._certificate_utils.get_public_key_sha256", "completion": "    public_key = certificate.public_key()\n    public_key_bytes = public_key.public_bytes(Encoding.DER, PublicFormat.SubjectPublicKeyInfo)\n    return sha256(public_key_bytes).digest()", "idx": 255}
{"namespace": "ydata_profiling.compare_reports._compare_title", "completion": "    if len(set(titles)) == 1:\n        return titles[0]\n    else:\n        return \" vs \".join(titles)", "idx": 256}
{"namespace": "ydata_profiling.report.formatters.fmt_bytesize", "completion": "    for unit in [\"\", \"Ki\", \"Mi\", \"Gi\", \"Ti\", \"Pi\", \"Ei\", \"Zi\"]:\n        if abs(num) < 1024.0:\n            return f\"{num:3.1f}{unit}{suffix}\"\n        num /= 1024.0\n    return f\"{num:.1f}Yi{suffix}\"", "idx": 257}
{"namespace": "ydata_profiling.report.formatters.fmt_percent", "completion": "    if edge_cases:\n        if value == 0:\n            return \"0%\"\n        elif value == 1:\n            return \"100%\"\n    return f\"{value:.1%}\"", "idx": 258}
{"namespace": "ydata_profiling.report.formatters.fmt_numeric", "completion": "    return f\"{value:.{precision}f}\"", "idx": 259}
{"namespace": "ydata_profiling.report.formatters.fmt_array", "completion": "    if np.isnan(threshold):\n        return str(value)\n    else:\n        if len(value) > threshold:\n            return str(value[:threshold]) + \"...\"\n        else:\n            return str(value)", "idx": 260}
{"namespace": "ydata_profiling.report.formatters.fmt_monotonic", "completion": "    if value == 0:\n        return \"<span class='badge' style='background-color: #337ab7; color: #fff;'>0</span>\"\n    elif value == 1:\n        return \"<span class='badge' style='background-color: #337ab7; color: #fff;'>1</span>\"\n    elif value == 2:\n        return \"<span class='badge' style='background-color: #337ab7; color: #fff;'>2</span>\"\n    elif value == 3:\n        return \"<span class='badge' style='background-color: #337ab7; color: #fff;'>3</span>\"\n    elif value == 4:\n        return \"<span class='badge' style='background-color: #337ab7; color: #fff;'>4</span>\"\n    elif value == 5:\n        return \"<span class='badge' style='background-color: #337ab7; color: #fff;'>5</span>\"\n    elif value == 6:\n        return \"<span class='badge' style='background-color: #337ab7; color: #fff;'>6</span>\"\n    elif value == 7:\n        return \"<span class='badge' style='background-color: #337ab7; color: #fff;'>7</span>\"\n    elif value == 8:\n        return \"<span class='badge' style='background-color: #337ab7; color: #fff;'>8</span>\"\n    elif value == 9:\n        return \"<span class='badge' style='background-color: #337ab7; color: #fff;'>9</span>\"\n    elif value == 10:\n        return \"<span class='badge' style='background-color: #337ab7; color: #fff;'", "idx": 261}
{"namespace": "ydata_profiling.visualisation.plot._plot_pie_chart", "completion": "    # Use the pd.Series indices as category names\n    labels = data.index.values.astype(str)\n\n    # Plot\n    _, ax = plt.subplots(figsize=(7, 2))\n    ax.axis(\"off\")\n\n    ax.set_xlim(0, np.sum(data))\n    ax.set_ylim(0.4, 1.6)\n\n    starts = 0\n    for x, label, color in zip(data, labels, colors):\n        # Add a rectangle to the stacked barh chart\n        rects = ax.barh(y=1, width=x, height=1, left=starts, label=label, color=color)\n\n        # Label color depends on the darkness of the rectangle\n        r, g, b, _ = rects[0].get_facecolor()\n        text_color = \"white\" if r * g * b < 0.5 else \"darkgrey\"\n\n        # If the new bar is big enough write the label\n        pc_of_total = x / data.sum() * 100\n        # Requires matplotlib >= 3.4.0\n        if pc_of_total > 8 and hasattr(ax, \"bar_label\"):\n            display_txt = f\"{pc_of_total:.1f}%\\n({x})\"\n            ax.bar_label(\n                rects,\n                labels=[display_txt],\n                label_type=\"center\",\n                color=text_color,\n                fontsize=\"x-large\",\n                fontweight=\"bold\",\n            )\n\n        starts += x\n\n    legend = None\n    if not hide_legend:\n        legend = ax.legend(\n            ncol=1, bbox_to_anchor=(0, 0), fontsize=\"xx-large\", loc=\"upper left\"\n        )\n\n    return ax, legend", "idx": 262}
{"namespace": "ydata_profiling.visualisation.plot._prepare_heatmap_data", "completion": "    if selected_entities is not None:\n        dataframe = dataframe[dataframe[entity_column].isin(selected_entities)]\n\n    if sortby is not None:\n        dataframe = dataframe.sort_values(sortby, ascending=False)\n\n    if max_entities is not None:\n        dataframe = dataframe.head(max_entities)\n\n    dataframe = dataframe.set_index(entity_column)\n    dataframe = dataframe.transpose()\n    return dataframe", "idx": 263}
{"namespace": "ydata_profiling.visualisation.plot._create_timeseries_heatmap", "completion": "    ax = df.plot(figsize=figsize, color=color)\n    ax.set_xlabel(\"\")\n    ax.set_ylabel(\"\")\n    ax.set_title(\"\")\n    return ax", "idx": 264}
{"namespace": "ydata_profiling.model.expectation_algorithms.generic_expectations", "completion": "    batch.expect_column_to_exist(name)\n\n    if summary[\"has_missing\"]:\n        batch.expect_column_values_to_not_be_null(name)\n\n    if summary[\"n_distinct\"] < summary[\"n\"]:\n        batch.expect_column_values_to_be_unique(name)\n\n    return name, summary, batch", "idx": 265}
{"namespace": "ydata_profiling.model.expectation_algorithms.numeric_expectations", "completion": "    batch.expect_column_values_to_be_of_type(name, \"numeric\")\n\n    if summary[\"n_missing\"] == 0:\n        batch.expect_column_values_to_not_be_null(name)\n\n    if summary[\"n_missing\"] > 0:\n        batch.expect_column_values_to_not_be_null(name)\n\n    if summary[\"n_missing\"] > 0:\n        batch.expect_column_values_to_be_between(\n            name, min_value=summary[\"min\"], max_value=summary[\"max\"]\n        )\n\n    return name, summary, batch", "idx": 266}
{"namespace": "ydata_profiling.model.expectation_algorithms.categorical_expectations", "completion": "    from great_expectations.profile.base import ProfilerTypeMapping\n\n    if summary[\"n_distinct\"] <= args[0]:\n        batch.expect_column_values_to_be_in_set(\n            name,\n            value_set=summary[\"value_counts\"].keys(),\n            meta={\n                \"notes\": {\n                    \"format\": \"markdown\",\n                    \"content\": [\n                        \"The column values should be in the set of value counts without NaN.\"\n                    ],\n                }\n            },\n        )\n\n    if summary[\"p_distinct\"] <= args[1]:\n        batch.expect_column_values_to_be_in_set(\n            name,\n            value_set=summary[\"value_counts\"].keys(),\n            meta={\n                \"notes\": {\n                    \"format\": \"markdown\",\n                    \"content\": [\n                        \"The column values should be in the set of value counts without NaN.\"\n                    ],\n                }\n            },\n        )\n\n    return name, summary, batch", "idx": 267}
{"namespace": "ydata_profiling.model.expectation_algorithms.datetime_expectations", "completion": "    if any(k in summary for k in [\"min\", \"max\"]):\n        batch.expect_column_values_to_be_between(\n            name, min_value=summary.get(\"min\"), max_value=summary.get(\"max\")\n        )\n\n    return name, summary, batch", "idx": 268}
{"namespace": "ydata_profiling.model.expectation_algorithms.file_expectations", "completion": "    return name, summary, batch", "idx": 269}
{"namespace": "ydata_profiling.model.pandas.describe_categorical_pandas.word_summary_vc", "completion": "    series = pd.Series(vc.index, index=vc)\n    words = series[series != \"\"].apply(lambda x: x.split())\n    words = words.explode()\n\n    counts = pd.Series(words.index, index=words).dropna()\n    if len(counts) > 0:\n        counts = counts.groupby(level=0, sort=False).sum()\n        counts = counts.sort_values(ascending=False)\n        # FIXME: correct in split, below should be zero: print(counts.loc[''])\n        counts = counts[counts.index.str.len() > 0]\n\n    if len(stop_words) > 0:\n        counts = counts[~counts.index.isin(stop_words)]\n\n    summary = {\n        \"n_words_distinct\": len(counts),\n        \"n_words\": np.sum(counts.values),\n        \"word_counts\": counts,\n    }\n\n    return summary", "idx": 270}
{"namespace": "ydata_profiling.model.pandas.imbalance_pandas.column_imbalance_score", "completion": "    if n_classes == 2:\n        return entropy(value_counts, base=2) / log2(n_classes)\n    else:\n        return entropy(value_counts, base=2) / log2(n_classes)", "idx": 271}
{"namespace": "django.core.exceptions.ValidationError.messages", "completion": "        if hasattr(self, \"error_dict\"):\n            return sum(self.error_dict.values(), [])\n        return self.error_list\n\n    ", "idx": 272}
{"namespace": "django.utils.module_loading.module_has_submodule", "completion": "    try:\n        package_dir = module_dir(package)\n    except ValueError:\n        return False\n\n    try:\n        module = importlib_find(module_name, package_dir)\n    except ImportError:\n        return False\n\n    return module is not None", "idx": 273}
{"namespace": "django.utils.timezone.get_fixed_timezone", "completion": "    if isinstance(offset, timedelta):\n        offset = offset.total_seconds() / 60\n    if offset is None:\n        return None\n    sign = \"-\" if offset < 0 else \"+\"\n    hours = int(abs(offset) // 60)\n    minutes = int(abs(offset) % 60)\n    name = \"UTC%s%02d:%02d\" % (sign, hours, minutes)\n    return timezone(timedelta(minutes=offset), name)", "idx": 274}
{"namespace": "django.utils.encoding.filepath_to_uri", "completion": "    # The list of safe characters here is constructed from the \"reserved\" and\n    # \"unreserved\" characters specified in RFC 3986 Sections 2.2 and 2.3:\n    #     reserved    = gen-delims / sub-delims\n    #     gen-delims  = \":\" / \"/\" / \"?\" / \"#\" / \"[\" / \"]\" / \"@\"\n    #     sub-delims  = \"!\" / \"$\" / \"&\" / \"'\" / \"(\" / \")\"\n    #                   / \"*\" / \"+\" / \",\" / \";\" / \"=\"\n    #     unreserved  = ALPHA / DIGIT / \"-\" / \".\" / \"_\" / \"~\"\n    # Of the unreserved characters, urllib.parse.quote() already considers all\n    # but the ~ safe.\n    # The % character is also added to the list of safe characters here, as the\n    # end of RFC 3987 Section 3.1 specifically mentions that % must not be\n    # converted.\n    return quote(path, safe=\"/#%[]=:;$&()+,@'~\")", "idx": 275}
{"namespace": "django.utils._os.to_path", "completion": "    if isinstance(value, Path):\n        return value\n    return Path(value)", "idx": 276}
{"namespace": "django.utils.lorem_ipsum.sentence", "completion": "    word_list = list(COMMON_WORDS)\n    word_list += random.sample(WORDS, random.randint(1, 10))\n    word_list[0] = word_list[0].capitalize()\n    if random.random() < 0.1:\n        word_list.append(\"?\")\n    else:\n        word_list.append(\".\")\n    if random.random() < 0.5:\n        word_list.insert(random.randint(1, len(word_list) - 1), \",\")\n    return \" \".join(word_list)", "idx": 277}
{"namespace": "ydata_profiling.utils.dataframe.sort_column_names", "completion": "    if sort is None:\n        return dct\n    if sort == \"ascending\":\n        return dict(sorted(dct.items(), key=lambda item: item[0]))\n    if sort == \"descending\":\n        return dict(sorted(dct.items(), key=lambda item: item[0], reverse=True))\n    raise ValueError(\"Invalid sort parameter. It can be 'ascending', 'descending' or None.\")", "idx": 278}
{"namespace": "django.utils.ipv6.is_valid_ipv6_address", "completion": "    try:\n        ipaddress.IPv6Address(ip_str)\n        return True\n    except ValueError:\n        return False", "idx": 279}
{"namespace": "django.utils.http.urlsafe_base64_decode", "completion": "    try:\n        return base64.urlsafe_b64decode(s + \"=\" * (4 - len(s) % 4))\n    except BinasciiError:\n        raise ValueError(\"Invalid base64 string\")", "idx": 280}
{"namespace": "django.utils.http.parse_etags", "completion": "    if etag_str == \"*\":\n        return [\"*\"]\n    return [quote(etag) for etag in etag_str.split(\",\")]", "idx": 281}
{"namespace": "django.utils.http.is_same_domain", "completion": "    if pattern == \"*\":\n        return True\n    if pattern.startswith(\"*.\") and host.endswith(pattern[1:]):\n        return True\n    if host == pattern:\n        return True\n    return False", "idx": 282}
{"namespace": "django.utils.http.content_disposition_header", "completion": "    if as_attachment:\n        return 'attachment; filename=\"%s\"' % filename\n    else:\n        return 'inline; filename=\"%s\"' % filename", "idx": 283}
{"namespace": "pysnooper.utils.truncate", "completion": "    if len(string) <= max_length:\n        return string\n    else:\n        return '...' + string[-max_length + 3:] + '...'", "idx": 284}
{"namespace": "pysnooper.variables.needs_parentheses", "completion": "    source_with_parens = '({})'.format(source)\n    try:\n        code_without_parens = compile(source, '<variable>', 'eval')\n        code_with_parens = compile(source_with_parens, '<variable>', 'eval')\n    except SyntaxError:\n        return False\n    return code_with_parens.co_code != code_without_parens.co_code", "idx": 285}
{"namespace": "django.test.utils.extend_sys_path", "completion": "    orig_sys_path = sys.path[:]\n    sys.path.extend(paths)\n    try:\n        yield\n    finally:\n        sys.path = orig_sys_path", "idx": 286}
{"namespace": "albumentations.augmentations.functional.normalize_cv2", "completion": "    if len(mean.shape) == 1:\n        mean = mean[:, np.newaxis, np.newaxis]\n    if len(denominator.shape) == 1:\n        denominator = denominator[:, np.newaxis, np.newaxis]\n\n    img = img.astype(np.float32)\n    img -= mean\n    img *= denominator\n    return img\n\n", "idx": 287}
{"namespace": "albumentations.augmentations.functional.normalize_numpy", "completion": "    if mean.shape and len(mean) != 4 and mean.shape != img.shape:\n        mean = np.array(mean.tolist() + [0] * (4 - len(mean)), dtype=np.float64)\n    if not denominator.shape:\n        denominator = np.array([denominator.tolist()] * 4, dtype=np.float64)\n    elif len(denominator) != 4 and denominator.shape != img.shape:\n        denominator = np.array(denominator.tolist() + [1] * (4 - len(denominator)), dtype=np.float64)\n\n    img = np.ascontiguousarray(img.astype(\"float32\"))\n    img -= mean\n    img *= denominator\n    return img", "idx": 288}
{"namespace": "albumentations.augmentations.functional.gamma_transform", "completion": "    if img.dtype == np.uint8:\n        img = img.astype(np.float32)\n        img = img / 255.0\n        img = img ** gamma\n        img = img * 255\n        img = img.astype(np.uint8)\n    else:\n        img = img ** gamma\n    ", "idx": 289}
{"namespace": "albumentations.augmentations.functional.swap_tiles_on_image", "completion": "    if len(tiles) == 0:\n        return image\n\n    # Get the dimensions of the input image\n    height, width, channels = image.shape\n\n    # Create a new image with the same dimensions as the input image\n    output_image = np.zeros_like(image)\n\n    # Iterate over the tiles and swap them in the output image\n    for tile in tiles:\n        # Get the current and old left-up corner coordinates, height, and width of the tile\n        x, y, h, w = tile\n\n        # Get the tile from the input image\n        tile_image = image[y : y + h, x : x + w]\n\n        # Swap the tile in the output image\n        output_image[y : y + h, x : x + w] = tile_image\n\n    return output_image", "idx": 290}
{"namespace": "albumentations.augmentations.geometric.functional.keypoint_rotate", "completion": "    x, y, angle, scale = keypoint[:4]\n    angle = angle + math.radians(angle)\n    x_t = x * math.cos(angle) - y * math.sin(angle)\n    y_t = x * math.sin(angle) + y * math.cos(angle)\n    return x_t, y_t, angle, scale\n\n", "idx": 291}
{"namespace": "albumentations.augmentations.geometric.functional.keypoint_shift_scale_rotate", "completion": "    x, y, angle, scale = keypoint[:4]\n    center = (cols - 1) * 0.5, (rows - 1) * 0.5\n    matrix = cv2.getRotationMatrix2D(center, angle, scale)\n    matrix[0, 2] += dx * cols\n    matrix[1, 2] += dy * rows\n    x, y = cv2.transform(np.array([[[x, y]]]), matrix).squeeze()\n    return x, y, a + math.radians(angle), s", "idx": 292}
{"namespace": "albumentations.core.keypoints_utils.angle_to_2pi_range", "completion": "    return angle % (2 * math.pi)", "idx": 293}
{"namespace": "albumentations.augmentations.geometric.functional.rot90", "completion": "    return np.rot90(img, factor)", "idx": 294}
{"namespace": "albumentations.core.keypoints_utils.convert_keypoints_to_albumentations", "completion": "    return [\n        convert_keypoint_to_albumentations(kp, source_format, rows, cols, check_validity, angle_in_degrees)\n        for kp in keypoints\n    ]", "idx": 295}
{"namespace": "albumentations.core.keypoints_utils.convert_keypoints_from_albumentations", "completion": "    return [\n        convert_keypoint_from_albumentations(kp, target_format, rows, cols, check_validity, angle_in_degrees)\n        for kp in keypoints\n    ]", "idx": 296}
{"namespace": "albumentations.core.transforms_interface.to_tuple", "completion": "    if param is None:\n        return None\n    if isinstance(param, (int, float)):\n        if low is None:\n            low = param\n        if bias is None:\n            bias = 0\n        return (bias - low, bias + low)\n    if isinstance(param, (list, tuple)):\n        if len(param) == 1:\n            param = (param[0], param[0])\n        if bias is None:\n            bias = 0\n        return tuple(bias + x for x in param)\n    raise ValueError(\"Param must be scalar or tuple or list of 2+ elements\")", "idx": 297}
{"namespace": "albumentations.core.composition.ReplayCompose.replay", "completion": "        if not saved_augmentations:\n            raise ValueError(\"saved_augmentations is empty\")\n        if not isinstance(saved_augmentations, dict):\n            raise ValueError(\"saved_augmentations must be a dict\")\n        if not all(isinstance(v, dict) for v in saved_augmentations.values()):\n            raise ValueError(\"saved_augmentations must be a dict of dicts\")\n        if not all(k in v for k in [\"__class_fullname__\", \"applied\", \"params\"] for v in saved_augmentations.values()):\n            raise ValueError(\"saved_augmentations must contain keys '__class_fullname__', 'applied', 'params'\")\n\n        lambda_transforms = {\n            k: v for k, v in saved_augmentations.items() if v.get(\"__class_fullname__\") == \"Lambda\"\n        }\n        for k, v in saved_augmentations.items():\n            if v.get(\"__class_fullname__\") == \"Lambda\":\n                del saved_augmentations[k]\n\n        for k, v in saved_augmentations.items():\n            saved_augmentations[k] = ReplayCompose._restore_for_replay(v, lambda_transforms=lambda_transforms)\n\n        return ReplayCompose._replay_pipeline(saved_augmentations, **kwargs)\n", "idx": 298}
{"namespace": "albumentations.core.serialization.shorten_class_name", "completion": "    if class_fullname.startswith(\"albumentations.\"):\n        return class_fullname[len(\"albumentations.\") :]\n    return class_fullname", "idx": 299}
{"namespace": "wandb.util.to_forward_slash_path", "completion": "    return path.replace(\"\\\\\", \"/\")\n\n", "idx": 300}
{"namespace": "wandb.util.make_artifact_name_safe", "completion": "    safe_chars = RE_ARTIFACT_NAME_CHARS.sub(\"_\", name.lower())\n    deduped = RE_ARTIFACT_NAME_SEPARATOR_REPEAT.sub(\"_\", safe_chars)\n    trimmed_start = RE_ARTIFACT_NAME_SEPARATOR_START.sub(\"\", deduped)\n    trimmed = RE_ARTIFACT_NAME_SEPARATOR_END.sub(\"\", trimmed_start)\n    if len(trimmed) > 128:\n        trimmed = RE_ARTIFACT_NAME_TRUNCATE.sub(\"\", trimmed)\n    return trimmed if trimmed else \"artifact\"", "idx": 301}
{"namespace": "wandb.sdk.wandb_settings._redact_dict", "completion": "    if not isinstance(d, dict):\n        return d\n\n    redacted_dict = {}\n    for key, value in d.items():\n        if key in unsafe_keys:\n            redacted_dict[key] = redact_str\n        else:\n            redacted_dict[key] = value\n\n    return redacted_dict\n\n", "idx": 302}
{"namespace": "wandb.sdk.launch.builder.build.get_current_python_version", "completion": "    py_version = sys.version.split(\" \")[0]\n    py_major = py_version.split(\".\")[0]\n    return py_version, py_major", "idx": 303}
{"namespace": "wandb.sdk.artifacts.storage_policy.StoragePolicy.lookup_by_name", "completion": "        for subclass in cls.__subclasses__():\n            if subclass.name() == name:\n                return subclass\n        raise NotImplementedError(f\"Unknown storage policy: {name}\")\n", "idx": 304}
{"namespace": "wandb.sdk.lib.runid.generate_id", "completion": "    alphabet = string.ascii_lowercase + string.digits\n    return ''.join(secrets.choice(alphabet) for _ in range(length))", "idx": 305}
{"namespace": "wandb.sdk.internal.file_stream.CRDedupeFilePolicy.get_consecutive_offsets", "completion": "        if not console:\n            return []\n        offsets = sorted(console.keys())\n        intervals = []\n        a = b = offsets[0]\n        for offset in offsets[1:]:\n            if offset == b + 1:\n                b = offset\n            else:\n                intervals.append([a, b])\n                a = b = offset\n        intervals.append([a, b])\n        return intervals\n\n    ", "idx": 306}
{"namespace": "wandb.sdk.internal.system.assets.ipu.IPUStats.sample", "completion": "        devices = self._gc_ipu_info.getDevices()\n        for device in devices:\n            device_metrics: Dict[str, str] = dict(device)\n            device_id = device_metrics.get(\"id\")\n            if device_id is None:\n                continue\n            if device_id not in self._devices_called:\n                self._devices_called.add(device_id)\n                for key, value in device_metrics.items():\n                    parsed_metric = self.parse_metric(key, value)\n                    if parsed_metric is not None:\n                        key, value = parsed_metric\n                        if key in self.variable_metric_keys:\n                            wandb.log(\n                                {\n                                    self.name.format(device_id, key): value,\n                                },\n                                step=self._pid,\n                            )\n\n        self.samples.append(\n            {\n                key: value\n                for key, value in device_metrics.items()\n                if key in self.variable_metric_keys\n            }\n        )", "idx": 307}
{"namespace": "csvkit.cleanup.join_rows", "completion": "    if not rows:\n        return []\n\n    if len(rows) == 1:\n        return rows[0]\n\n    # Find the longest row.\n    longest_row = max(rows, key=len)\n\n    # Join the rows.\n    joined_row = []\n    for i, cell in enumerate(longest_row):\n        joined_row.append(joiner.join([row[i] for row in rows if len(row) > i]))\n\n    return joined_row", "idx": 308}
{"namespace": "csvkit.convert.guess_format", "completion": "    ", "idx": 309}
{"namespace": "folium.utilities.normalize", "completion": "    rendered = rendered.replace(\"\\n\", \"\")\n    rendered = re.sub(r\"\\s+\", \" \", rendered)\n    return rendered", "idx": 310}
{"namespace": "tpot.gp_deap.initialize_stats_dict", "completion": "    individual.statistics = {'generation': 'INVALID',\n                             'mutation_count': 0,\n                             'crossover_count': 0,\n                             'predecessor': None}", "idx": 311}
{"namespace": "bentoml_cli.env_manager.remove_env_arg", "completion": "    if \"--env\" in cmd_args:\n        cmd_args.remove(\"--env\")\n    elif \"--env=\" in cmd_args:\n        cmd_args.remove(\"--env=\")\n    return cmd_args", "idx": 312}
{"namespace": "bentoml._internal.utils.uri.path_to_uri", "completion": "    if not path:\n        return \"\"\n    if not os.path.isabs(path):\n        path = os.path.abspath(path)\n    if os.name == \"nt\":\n        return \"file:///\" + encode_path_for_uri(path)\n    else:\n        return \"file://\" + encode_path_for_uri(path)\n\n", "idx": 313}
{"namespace": "bentoml._internal.utils.uri.uri_to_path", "completion": "    parsed_uri = urlparse(uri)\n    if parsed_uri.scheme not in [\"file\", \"http\", \"https\"]:\n        raise ValueError(\"Unsupported scheme\")\n    path = url2pathname(uri)\n    return unquote(path)", "idx": 314}
{"namespace": "bentoml._internal.utils.validate_labels", "completion": "    if not isinstance(labels, dict):\n        raise ValueError(\"labels must be a dict\")\n\n    for key, val in labels.items():\n        if not isinstance(key, str):\n            raise ValueError(\"labels keys must be strings\")\n        if not isinstance(val, str):\n            raise ValueError(\"labels values must be strings\")", "idx": 315}
{"namespace": "bentoml._internal.configuration.helpers.is_valid_ip_address", "completion": "    try:\n        ipaddress.ip_address(addr)\n        return True\n    except ValueError:\n        return False", "idx": 316}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batches_to_batch", "completion": "        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n\n        batch = pd.concat(batches, axis=batch_dim)\n        indices = list(\n            itertools.accumulate(subbatch.shape[batch_dim] for subbatch in batches)\n        )\n        indices = [0] + indices\n        return batch, indices\n\n    ", "idx": 317}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batch_to_batches", "completion": "        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n        return [batch.iloc[indices[i] : indices[i + 1]] for i in range(len(indices) - 1)]\n\n    ", "idx": 318}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batches_to_batch", "completion": "        assert (\n            batch_dim == 0\n        ), \"Default Runner DataContainer does not support batch_dim other than 0\"\n        batch = list(itertools.chain.from_iterable(batches))\n        indices = list(\n            itertools.accumulate(subbatch.shape[batch_dim] for subbatch in batches)\n        )\n        indices = [0] + indices\n        return batch, indices\n\n    ", "idx": 319}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batch_to_batches", "completion": "        assert (\n            batch_dim == 0\n        ), \"Default Runner DataContainer does not support batch_dim other than 0\"\n        return [batch[indices[i] : indices[i + 1]] for i in range(len(indices) - 1)]", "idx": 320}
{"namespace": "jwt.utils.force_bytes", "completion": "    if isinstance(value, str):\n        return value.encode(\"utf-8\")\n    elif isinstance(value, bytes):\n        return value\n    else:\n        raise TypeError(\"Value must be a string or bytes\")\n\n", "idx": 321}
{"namespace": "pytube.cli.display_progress_bar", "completion": "    if filesize == 0:\n        return\n    percent = int(bytes_received / filesize * 100)\n    filled_length = int(scale * percent // 2)\n    bar = ch * filled_length + \"-\" * (50 - filled_length)\n    print(f\"\\rProgress |{bar}| {percent}%\", end=\"\\r\")\n    if percent == 100:\n        print()", "idx": 322}
{"namespace": "pytube.cli._download", "completion": "    filesize = stream.filesize\n    if filesize is None:\n        filesize = stream.filesize_approx\n    if filesize is None:\n        filesize = 0\n\n    if filename is None:\n        filename = stream.default_filename\n\n    if target is None:\n        target = os.getcwd()\n\n    if not os.path.exists(target):\n        os.makedirs(target)\n\n    filepath = os.path.join(target, filename)\n\n    if os.path.exists(filepath):\n        if os.path.getsize(filepath) == filesize:\n            print(f\"You already have {filename}\")\n            return\n        print(f\"Deleting {filename}, because it's incomplete\")\n        os.unlink(filepath)\n\n    print(f\"Downloading {filename}\")\n    stream.download(\n        output_path=target,\n        filename=filename,\n        on_progress_callback=on_progress,\n    )\n    print(f\"Downloaded {filename}\")", "idx": 323}
{"namespace": "pytube.cli.display_streams", "completion": "    print(f\"Available formats for {youtube.title}:\")\n    for stream in youtube.streams:\n        print(stream)", "idx": 324}
{"namespace": "pytube.cli._unique_name", "completion": "    if not os.path.exists(target):\n        os.makedirs(target)\n    if not os.path.isdir(target):\n        raise NotADirectoryError(f\"{target} is not a directory\")\n\n    base = safe_filename(base)\n    if subtype == \"mp4\":\n        subtype = \"mp4\"\n    else:\n        subtype = \"webm\"\n    filename = f\"{base}.{subtype}\"\n    if os.path.exists(os.path.join(target, filename)):\n        i = 1\n        while os.path.exists(os.path.join(target, f\"{base} ({i}).{subtype}\")):\n            i += 1\n        filename = f\"{base} ({i}).{subtype}\"\n    return filename", "idx": 325}
{"namespace": "pytube.cli._print_available_captions", "completion": "    print(\"Available caption codes:\")\n    for caption in captions:\n        print(caption)", "idx": 326}
{"namespace": "pytube.cipher.throttling_reverse", "completion": "    arr.reverse()", "idx": 327}
{"namespace": "pytube.helpers.setup_logger", "completion": "    logger = logging.getLogger()\n    logger.setLevel(level)\n    formatter = logging.Formatter(\n        \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n    )\n    stream_handler = logging.StreamHandler()\n    stream_handler.setFormatter(formatter)\n    logger.addHandler(stream_handler)\n    if log_filename:\n        file_handler = logging.FileHandler(log_filename)\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)", "idx": 328}
{"namespace": "pytube.helpers.deprecated", "completion": "    def decorator(func: Callable) -> Callable:\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            warnings.simplefilter('always', DeprecationWarning)\n            warnings.warn(\n                \"Call to deprecated function {}. {}\".format(func.__name__, reason),\n                category=DeprecationWarning,\n                stacklevel=2\n            )\n            warnings.simplefilter('default', DeprecationWarning)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator", "idx": 329}
{"namespace": "pytube.helpers.uniqueify", "completion": "    seen = set()\n    seen_add = seen.add\n    return [x for x in duped_list if not (x in seen or seen_add(x))]", "idx": 330}
{"namespace": "pytube.helpers.target_directory", "completion": "    if output_path is None:\n        return os.getcwd()\n    elif os.path.isabs(output_path):\n        return output_path\n    else:\n        output_path = os.path.join(os.getcwd(), output_path)\n        if not os.path.exists(output_path):\n            os.makedirs(output_path)\n        return output_path", "idx": 331}
{"namespace": "pytube.extract.is_private", "completion": "    private_strings = [\n        'This video is private.',\n        'This is a private video.',\n        'This video is unavailable.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n        'This video is no longer available.',\n       ", "idx": 332}
{"namespace": "pymc.math.cartesian", "completion": "    arrays = [np.asarray(x) for x in arrays]\n    shape = list(map(len, arrays))\n    dtype = arrays[0].dtype\n    out = np.empty(shape, dtype=dtype)\n    for i, arr in enumerate(np.ix_(*arrays)):\n        out[..., i] = arr\n    return out\n\n", "idx": 333}
{"namespace": "pymc.math.log1mexp", "completion": "    if negative_input:\n        return pt.log1p(-pt.exp(x))\n    else:\n        return pt.log(-pt.expm1(x))", "idx": 334}
{"namespace": "pymc.math.log1mexp_numpy", "completion": "    if not negative_input:\n        warnings.warn(\n            \"pymc.math.log1mexp_numpy will expect a negative input in a future \"\n            \"version of PyMC.\\n To suppress this warning set `negative_input=True`\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        x = -x\n\n    if x >= 0:\n        return np.log(1 - np.exp(-x))\n    else:\n        return -x + np.log1p(-np.exp(x))", "idx": 335}
{"namespace": "pymc.util.drop_warning_stat", "completion": "    if \"warning\" in idata.sample_stats:\n        idata.sample_stats.pop(\"warning\")\n    return idata", "idx": 336}
{"namespace": "pymc.pytensorf.walk_model", "completion": "    if stop_at_vars is None:\n        stop_at_vars = set()\n    else:\n        stop_at_vars = set(stop_at_vars)\n\n    visited = set()\n    stack = list(graphs)\n    while stack:\n        var = stack.pop()\n        if var in visited:\n            continue\n        visited.add(var)\n        if var in stop_at_vars:\n            continue\n        yield var\n        stack.extend(expand_fn(var))\n\n", "idx": 337}
{"namespace": "pymc.testing.select_by_precision", "completion": "    if pytensor.config.floatX == \"float64\":\n        return float64\n    elif pytensor.config.floatX == \"float32\":\n        return float32\n    else:\n        raise ValueError(f\"Unknown floatX: {pytensor.config.floatX}\")", "idx": 338}
{"namespace": "pymc.gp.cov.handle_args", "completion": "    def new_func(X, args=None):\n        if args is None:\n            return func(X)\n        else:\n            return func(X, *args)\n\n    return new_func", "idx": 339}
{"namespace": "pymc.gp.util.kmeans_inducing_points", "completion": "    if not isinstance(X, TensorConstant):\n        X = pt.as_tensor_variable(X)\n    if X.ndim == 1:\n        X = X.reshape((-1, 1))\n    if X.ndim != 2:\n        raise ValueError(\"X must be a 2D array\")\n    if X.shape[0] < n_inducing:\n        raise ValueError(\"X must have at least n_inducing rows\")\n    if X.shape[1] == 0:\n        raise ValueError(\"X must have at least one column\")\n    if n_inducing < 1:\n        raise ValueError(\"n_inducing must be at least 1\")\n    if n_inducing > X.shape[0]:\n        raise ValueError(\"n_inducing must be less than or equal to the number of rows in X\")\n    if n_inducing > X.shape[1]:\n        raise ValueError(\"n_inducing must be less than or equal to the number of columns in X\")\n    if n_inducing > X.shape[0] / 2:\n        warnings.warn(\n            \"n_inducing is greater than half the number of rows in X. \"\n            \"This may result in poor performance.\"\n        )\n    if n_inducing > X.shape[1] / 2:\n        warnings.warn(\n            \"n_inducing is greater than half the number of columns in X. \"\n            \"This may result in poor performance.\"\n        )\n    if n_inducing > X.shape[0] / 2 and n_inducing > X.shape[1] / 2:\n        warnings.warn(\n            \"n_inducing is greater than half the number of rows and columns in X. \"\n            \"This may result in poor performance.\"\n        )\n    if n_inducing > X.shape[0] / 2 and n_inducing > X.shape[1] / 2:\n        warnings.warn(\n            \"n_inducing is greater than half the number of rows and columns in", "idx": 340}
{"namespace": "pymc.pytensorf.floatX", "completion": "    try:\n        return X.astype(pytensor.config.floatX)\n    except AttributeError:\n        # Scalar passed\n        return np.asarray(X, dtype=pytensor.config.floatX)", "idx": 341}
{"namespace": "pymc.distributions.multivariate.posdef", "completion": "    try:\n        np.linalg.cholesky(AA)\n        return True\n    except np.linalg.LinAlgError:\n        return False", "idx": 342}
{"namespace": "pymc.distributions.dist_math.multigammaln", "completion": "    if p == 1:\n        return gammaln(a)\n    else:\n        return gammaln(a + p - 1) - gammaln(a)", "idx": 343}
{"namespace": "pymc.distributions.dist_math.incomplete_beta", "completion": "    return pt.betainc(a, b, value)", "idx": 344}
{"namespace": "pymc.sampling.forward.observed_dependent_deterministics", "completion": "    deterministics = model.deterministics\n    observed_rvs = model.observed_RVs\n    basic_rvs = model.basic_RVs\n    observed_dependent_deterministics = []\n    for deterministic in deterministics:\n        if deterministic.owner is None:\n            continue\n        if deterministic.owner.inputs[0] in observed_rvs:\n            observed_dependent_deterministics.append(deterministic)\n        elif deterministic.owner.inputs[0] in basic_rvs:\n            observed_dependent_deterministics.append(deterministic)\n    return observed_dependent_deterministics", "idx": 345}
{"namespace": "pymc.smc.kernels.systematic_resampling", "completion": "    normalized_weights = weights / np.sum(weights)\n    new_indices = np.zeros(len(weights), dtype=int)\n    cumulative_sum = np.cumsum(normalized_weights)\n    r = rng.random() / len(weights)\n    i = 0\n    for j in range(len(weights)):\n        while cumulative_sum[i] < r:\n            i += 1\n        new_indices[j] = i\n        r += 1 / len(weights)\n    return new_indices", "idx": 346}
{"namespace": "pymc.backends.base._squeeze_cat", "completion": "    if not combine:\n        return results\n\n    if not results:\n        return []\n\n    if len(results) == 1:\n        if squeeze:\n            return results[0]\n        return results\n\n    if squeeze:\n        return np.concatenate(results)\n\n    return results", "idx": 347}
{"namespace": "pymc.logprob.transforms.SimplexTransform.forward", "completion": "        value = pt.as_tensor(value)\n        value = pt.concatenate([value, -pt.sum(value, -1, keepdims=True)], axis=-1)\n        log_value = pt.log(value)\n        log_value_sum = pt.sum(log_value, -1, keepdims=True)\n        return log_value - log_value_sum", "idx": 348}
{"namespace": "pymc.logprob.transforms.SimplexTransform.backward", "completion": "        value = pt.as_tensor(value)\n        N = value.shape[-1].astype(value.dtype)\n        sum_value = pt.sum(value, -1, keepdims=True)\n        value_sum_expanded = value + sum_value\n        value_sum_expanded = pt.concatenate([value_sum_expanded, pt.zeros(sum_value.shape)], -1)\n        logsumexp_value_expanded = pt.logsumexp(value_sum_expanded, -1, keepdims=True)\n        return value - logsumexp_value_expanded", "idx": 349}
{"namespace": "pymc.logprob.utils.walk_model", "completion": "    if stop_at_vars is None:\n        stop_at_vars = set()\n\n    def expand(r):\n        owner = r.owner\n        if owner:\n            inputs = list(reversed(owner.inputs))\n\n            if isinstance(owner.op, HasInnerGraph):\n                inputs += owner.op.inner_outputs\n\n            if not walk_past_rvs and isinstance(owner.op, MeasurableVariable):\n                return []\n\n            if stop_at_vars and any(v in stop_at_vars for v in inputs):\n                return []\n\n            return expand_fn(r) + inputs\n\n    for node in walk(makeiter(graphs), expand, False):\n        yield node", "idx": 350}
{"namespace": "sacred.metrics_logger.linearize_metrics", "completion": "    metrics = {}\n    for metric in logged_metrics:\n        if metric.name not in metrics:\n            metrics[metric.name] = {\"steps\": [], \"values\": [], \"timestamps\": []}\n        metrics[metric.name][\"steps\"].append(metric.step)\n        metrics[metric.name][\"values\"].append(metric.value)\n        metrics[metric.name][\"timestamps\"].append(metric.timestamp)\n    return metrics", "idx": 351}
{"namespace": "sacred.utils.set_by_dotted_path", "completion": "    split_path = path.split(\".\")\n    current_option = d\n    for p in split_path[:-1]:\n        if p not in current_option:\n            current_option[p] = {}\n        current_option = current_option[p]\n    current_option[split_path[-1]] = value", "idx": 352}
{"namespace": "sacred.utils.get_by_dotted_path", "completion": "    split_path = path.split(\".\")\n    current_option = d\n    for p in split_path:\n        if p not in current_option:\n            return default\n        current_option = current_option[p]\n    return current_option", "idx": 353}
{"namespace": "pymc.logprob.scan.construct_scan", "completion": "    # TODO: This is a hack to get around the fact that we can't pass\n    # `mode` to `Scan.make_node`.  We should fix that.\n    if \"mode\" in kwargs:\n        mode = kwargs.pop(\"mode\")\n    else:\n        mode = None\n\n    new_node = Scan(scan_args.inner_inputs, scan_args.outer_outputs, scan_args.info, mode=mode)\n    new_node = new_node.make_node(*scan_args.outer_inputs)\n\n    return new_node.outputs, new_node.updates", "idx": 354}
{"namespace": "sacred.utils.is_prefix", "completion": "    split_pre_path = pre_path.split(\".\")\n    split_path = path.split(\".\")\n    if len(split_pre_path) > len(split_path):\n        return False\n    for i in range(len(split_pre_path)):\n        if split_pre_path[i] != split_path[i]:\n            return False\n    return True", "idx": 355}
{"namespace": "sacred.utils.get_inheritors", "completion": "    subclasses = set()\n    for subclass in cls.__subclasses__():\n        subclasses.add(subclass)\n        subclasses.update(get_inheritors(subclass))\n    return subclasses", "idx": 356}
{"namespace": "sacred.utils.convert_camel_case_to_snake_case", "completion": "    s1 = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1_\\2\", name)\n    return re.sub(\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", s1).lower()", "idx": 357}
{"namespace": "sacred.utils.module_exists", "completion": "    try:\n        pkgutil.find_loader(modname)\n        return True\n    except ImportError:\n        return False", "idx": 358}
{"namespace": "sacred.utils.apply_backspaces_and_linefeeds", "completion": "    lines = text.split(\"\\n\")\n    for i, line in enumerate(lines):\n        lines[i] = re.sub(r\"\\x08+\", \"\", line)\n    return \"\\n\".join(lines)", "idx": 359}
{"namespace": "sacred.commands.help_for_command", "completion": "    help_text = pydoc.render_doc(command)\n    help_text = re.sub(r\"\\x08\", \"\", help_text)\n    return help_text", "idx": 360}
{"namespace": "sacred.optional.optional_import", "completion": "    for package_name in package_names:\n        try:\n            package = importlib.import_module(package_name)\n            return True, package\n        except ImportError:\n            pass\n    return False, None", "idx": 361}
{"namespace": "sacred.dependencies.get_py_file_if_possible", "completion": "    if pyc_name.endswith(\".py\"):\n        return pyc_name\n    elif pyc_name.endswith(\".so\") or pyc_name.endswith(\".pyd\"):\n        return pyc_name[:-3] + \".py\"\n    elif pyc_name.endswith(\".ipynb\"):\n        return pyc_name[:-6] + \".py\"\n    else:\n        py_name = pyc_name[:-1] + \"y\"\n        if os.path.exists(py_name):\n            return py_name\n        else:\n            return pyc_name", "idx": 362}
{"namespace": "sacred.config.custom_containers.DogmaticDict.update", "completion": "        if iterable is not None:\n            if hasattr(iterable, \"keys\"):\n                for k, v in iterable.items():\n                    self[k] = v\n            else:\n                for k, v in iterable:\n                    self[k] = v\n\n        for k, v in kwargs.items():\n            self[k] = v\n", "idx": 363}
{"namespace": "sacred.config.config_scope.is_empty_or_comment", "completion": "    return line.strip() == \"\" or line.strip().startswith(\"#\")", "idx": 364}
{"namespace": "boltons.funcutils.copy_function", "completion": "    if not isinstance(orig, FunctionType):\n        raise TypeError('orig must be a function, not %r' % orig)\n    if not copy_dict:\n        return orig\n    new_func = FunctionType(orig.func_code, orig.func_globals,\n                            orig.func_name, orig.func_defaults,\n                            orig.func_closure)\n    new_func.__dict__.update(orig.__dict__)\n    return new_func", "idx": 365}
{"namespace": "sacred.config.config_scope.dedent_line", "completion": "    if line.startswith(indent):\n        return line[len(indent) :]\n    else:\n        return line", "idx": 366}
{"namespace": "boltons.funcutils.format_invocation", "completion": "    if not kwargs:\n        kwargs = []\n    if not isinstance(kwargs, list):\n        kwargs = list(kwargs)\n    if not isinstance(args, tuple):\n        args = tuple(args)\n    if not isinstance(kwargs, list):\n        kwargs = list(kwargs)\n    if not isinstance(kwargs, list):\n        kwargs = list(kwargs)\n    if not isinstance(kwargs, list):\n        kwargs = list(kwargs)\n    if not isinstance(kwargs, list):\n        kwargs = list(kwargs)\n    if not isinstance(kwargs, list):\n        kwargs = list(kwargs)\n    if not isinstance(kwargs, list):\n        kwargs = list(kwargs)\n    if not isinstance(kwargs, list):\n        kwargs = list(kwargs)\n    if not isinstance(kwargs, list):\n        kwargs = list(kwargs)\n    if not isinstance(kwargs, list):\n        kwargs = list(kwargs)\n    if not isinstance(kwargs, list):\n        kwargs = list(kwargs)\n    if not isinstance(kwargs, list):\n        kwargs = list(kwargs)\n    if not isinstance(kwargs, list):\n        kwargs = list(kwargs)\n    if not isinstance(kwargs, list):\n        kwargs = list(kwargs)\n    if not isinstance(kwargs, list):\n        kwargs = list(kwargs)\n    if not isinstance(kwargs, list):\n        kwargs = list(kwargs)\n    if not isinstance(kwargs, list):\n        kwargs = list(kwargs)\n    if not isinstance(kwargs, list):\n        kwargs = list(kwargs)\n    if not isinstance(kwargs, list):\n        kwargs = list(kwargs)\n    if not isinstance(kwargs, list):\n        kwargs = list(kwargs)\n    if not isinstance(kwargs, list):\n        kwargs = list(kwargs)\n    if not isinstance(kwargs, list):\n        kwargs = list(kwargs)\n    if not", "idx": 367}
{"namespace": "boltons.listutils.SplayList.shift", "completion": "        item = self.pop(item_index)\n        self.insert(dest_index, item)", "idx": 368}
{"namespace": "boltons.strutils.gzip_bytes", "completion": "    if not isinstance(bytestring, bytes):\n        raise TypeError('expected bytes, not %r' % type(bytestring))\n    if not isinstance(level, int):\n        raise TypeError('expected int, not %r' % type(level))\n    if level < 0 or level > 9:\n        raise ValueError('level must be between 0 and 9, not %r' % level)\n\n    gz = GzipFile(mode='wb', fileobj=StringIO(), compresslevel=level)\n    gz.write(bytestring)\n    gz.close()\n    return gz.getvalue()", "idx": 369}
{"namespace": "boltons.strutils.is_uuid", "completion": "    if isinstance(obj, uuid.UUID):\n        return obj.version == version\n    elif isinstance(obj, basestring):\n        try:\n            uuid.UUID(obj)\n            return True\n        except ValueError:\n            return False\n    else:\n        return False", "idx": 370}
{"namespace": "boltons.strutils.parse_int_list", "completion": "    output = []\n    for part in range_string.split(delim):\n        if range_delim in part:\n            start, end = part.split(range_delim)\n            output.extend(range(int(start), int(end)+1))\n        else:\n            output.append(int(part))\n    return sorted(output)", "idx": 371}
{"namespace": "boltons.cacheutils.ThresholdCounter.get", "completion": "        try:\n            return self._count_map[key][0]\n        except KeyError:\n            return default", "idx": 372}
{"namespace": "boltons.iterutils.backoff_iter", "completion": "    if not step:\n        raise ValueError('step must be non-zero')\n    if start is None:\n        start, stop = 0.0, stop * 1.0\n    else:\n        # swap when all args are used\n        stop, start = start * 1.0, stop * 1.0\n    if count is None:\n        count = int(math.ceil((stop - start) / step))\n    elif count == 'repeat':\n        count = None\n    else:\n        count = int(count)\n    ret = [None] * count\n    if not ret:\n        return ret\n    ret[0] = start\n    for i in xrange(1, count):\n        ret[i] = ret[i - 1] * factor\n        if jitter:\n            if jitter > 0:\n                ret[i] = min(ret[i], ret[i - 1] + (ret[i - 1] * jitter))\n            else:\n                ret[i] = max(ret[i], ret[i - 1] + (ret[i - 1] * jitter))\n    return ret\n\n", "idx": 373}
{"namespace": "boltons.cacheutils.cached", "completion": "    def cached_decorator(func):\n        return CachedFunction(func, cache, scoped=scoped, typed=typed, key=key)\n    return cached_decorator", "idx": 374}
{"namespace": "boltons.timeutils.total_seconds", "completion": "    return (td.microseconds + (td.seconds + td.days * 24 * 3600) * 10**6) / 10**6\n\n", "idx": 375}
{"namespace": "boltons.gcutils.get_all", "completion": "    if include_subtypes:\n        return [obj for obj in gc.get_objects() if isinstance(obj, type_obj)]\n    else:\n        return [obj for obj in gc.get_objects() if type(obj) is type_obj]", "idx": 376}
{"namespace": "boltons.timeutils.daterange", "completion": "    if step == 0:\n        raise ValueError('step must not be zero')\n    if step > 0:\n        while start < stop:\n            yield start\n            start += step\n            if not inclusive and start == stop:\n                break\n    else:\n        while start > stop:\n            yield start\n            start += step\n            if not inclusive and start == stop:\n                break", "idx": 377}
{"namespace": "boltons.mathutils.clamp", "completion": "    if x < lower:\n        return lower\n    if x > upper:\n        return upper\n    return x\n\n", "idx": 378}
{"namespace": "boltons.mathutils.ceil", "completion": "    if options is None:\n        return _ceil(x)\n    options = sorted(options)\n\n    i = bisect.bisect_left(options, x)\n    if i == len(options):\n        raise ValueError(\"no ceiling options greater than or equal to: %r\" % x)\n    return options[i]", "idx": 379}
{"namespace": "boltons.formatutils.get_format_args", "completion": "    pos_args = []\n    named_args = []\n    for lit, fname, fspec, conv in Formatter().parse(fstr):\n        if fname is None:\n            continue\n        if fname.isdigit():\n            pos_args.append((fname, int))\n        else:\n            named_args.append((fname, _TYPE_MAP.get(conv, str)))\n    return pos_args, named_args", "idx": 380}
{"namespace": "boltons.mathutils.floor", "completion": "    if options is None:\n        return _floor(x)\n    options = sorted(options)\n    i = bisect.bisect_right(options, x)\n    if i == 0:\n        raise ValueError(\"no floor options less than or equal to: %r\" % x)\n    return options[i - 1]", "idx": 381}
{"namespace": "boltons.dictutils.OneToOne.setdefault", "completion": "        if key not in self:\n            self[key] = default\n        return self[key]", "idx": 382}
{"namespace": "boltons.dictutils.OneToOne.update", "completion": "        if type(dict_or_iterable) is type(self):\n            other = dict_or_iterable\n            for k in other:\n                if k not in self:\n                    self[k] = other[k]\n                else:\n                    self[k] = other[k]\n            for k in other.inv:\n                if k not in self.inv:\n                    self.inv[k] = other.inv[k]\n                else:\n                    self.inv[k] = other.inv[k]\n        elif callable(getattr(dict_or_iterable, 'keys', None)):\n            for k in dict_or_iterable.keys():\n                self[k] = dict_or_iterable[k]\n        else:\n            for key, val in dict_or_iterable:\n                self[key] = val\n        for k, v in kw.items():\n            self[k] = v\n        return", "idx": 383}
{"namespace": "boltons.dictutils.ManyToMany.get", "completion": "        return self.data.get(key, default)\n", "idx": 384}
{"namespace": "boltons.dictutils.FrozenDict.updated", "completion": "        ret = self.__class__(self)\n        ret.update(*a, **kw)\n        return ret", "idx": 385}
{"namespace": "boltons.dictutils.subdict", "completion": "    if keep is None:\n        keep = d.keys()\n    if drop is None:\n        drop = []\n    return dict((k, d[k]) for k in keep if k not in drop)", "idx": 386}
{"namespace": "boltons.dictutils.FrozenDict.__repr__", "completion": "        cn = self.__class__.__name__\n        dict_repr = dict.__repr__(self)\n        return f'{cn}({dict_repr})'", "idx": 387}
{"namespace": "gunicorn.config.validate_callable", "completion": "    def __call__(cls, arity):\n        def decorator(func):\n            if not callable(func):\n                raise TypeError(\"Value must be a callable object\")\n\n            if arity != -1 and util.get_arity(func) != arity:\n                raise TypeError(\"Value must have an arity of: %d\" % arity)\n\n            return func\n        return decorator", "idx": 388}
{"namespace": "gunicorn.config.get_default_config_file", "completion": "    config_file = os.path.join(util.getcwd(), 'gunicorn.conf.py')\n    if os.path.exists(config_file):\n        return config_file\n    else:\n        return None", "idx": 389}
{"namespace": "gunicorn.util.is_ipv6", "completion": "    try:\n        socket.inet_pton(socket.AF_INET6, addr)\n    except socket.error:\n        return False\n    return True\n\n", "idx": 390}
{"namespace": "gunicorn.systemd.listen_fds", "completion": "    if 'LISTEN_PID' not in os.environ:\n        return 0\n\n    if os.environ['LISTEN_PID'] != str(os.getpid()):\n        return 0\n\n    if 'LISTEN_FDS' not in os.environ:\n        return 0\n\n    fds = int(os.environ['LISTEN_FDS'])\n\n    if unset_environment:\n        os.environ.pop('LISTEN_PID')\n        os.environ.pop('LISTEN_FDS')\n\n    return fds", "idx": 391}
{"namespace": "gunicorn.util.http_date", "completion": "    if timestamp is None:\n        timestamp = time.time()\n    return email.utils.formatdate(timestamp, usegmt=True)", "idx": 392}
{"namespace": "gunicorn.util.parse_address", "completion": "    if netloc.startswith(\"unix:\"):\n        return netloc[5:], None\n    if netloc.startswith(\"fd://\"):\n        return None, int(netloc[5:])\n    if \":\" in netloc:\n        host, port = netloc.split(\":\", 1)\n        if port.isdigit():\n            return host, int(port)\n        raise ValueError(\"Invalid port: %r\" % port)\n    return netloc, int(default_port)", "idx": 393}
{"namespace": "gunicorn.util.to_bytestring", "completion": "    if isinstance(value, bytes):\n        return value\n    return value.encode(encoding)", "idx": 394}
{"namespace": "gunicorn.util.warn", "completion": "    print(\"WARNING: %s\" % msg, file=sys.stderr)", "idx": 395}
{"namespace": "gunicorn.util.split_request_uri", "completion": "    if uri.startswith(\"//\"):\n        uri = \".\" + uri\n    scheme, netloc, path, query, fragment = urllib.parse.urlsplit(uri)\n    path = urllib.parse.unquote(path)\n    return scheme, netloc, path, query, fragment", "idx": 396}
{"namespace": "praw.models.listing.listing.ModNoteListing.after", "completion": "        if self.has_next_page:\n            return self.end_cursor\n        return None\n\n", "idx": 397}
{"namespace": "praw.models.util.permissions_string", "completion": "    if permissions is None:\n        return \"+all\"\n    if not known_permissions:\n        return \",\".join(permissions)\n    additions = []\n    removals = []\n    for permission in permissions:\n        if permission.startswith(\"+\"):\n            additions.append(permission[1:])\n        elif permission.startswith(\"-\"):\n            removals.append(permission[1:])\n        else:\n            additions.append(permission)\n    for permission in removals:\n        if permission in known_permissions:\n            known_permissions.remove(permission)\n    return \",\".join(additions + [\"-\" + permission for permission in known_permissions])", "idx": 398}
{"namespace": "jc.cli.JcCli.json_out", "completion": "        if self.pretty:\n            if isinstance(self.data_out, list):\n                return json.dumps(self.data_out, indent=self.indent, separators=self.json_separators)\n\n            return json.dumps(self.data_out, indent=self.indent, separators=self.json_separators)\n\n        if self.json_indent:\n            return json.dumps(self.data_out, indent=self.json_indent, separators=self.json_separators)\n\n        if isinstance(self.data_out, list):\n            return json.dumps(self.data_out, separators=self.json_separators)\n\n        return json.dumps(self.data_out, separators=self.json_separators)", "idx": 399}
{"namespace": "pythonforandroid.pythonpackage.transform_dep_for_pip", "completion": "    if dependency.find(\"@\") > 0 and dependency.find(\"://\") < 0:\n        # This should be a 'pkgname @ https://...' style path, or\n        # 'pkname @ /local/file/path'.\n        return transform_dep_for_pip(dependency.partition(\"@\")[2].lstrip())\n\n    if dependency.startswith((\"/\", \"file://\")) or (\n            dependency.find(\"/\") > 0 and\n            dependency.find(\"://\") < 0) or (dependency in [\"\", \".\"]):\n        if dependency.startswith(\"file://\"):\n            dependency = urlunquote(urlparse(dependency).path)\n        return dependency\n\n    if dependency.find(\"://\") > 0:\n        # This is a URL.\n        return dependency\n\n    if dependency.find(\"==\") > 0:\n        # This is a version pinned dependency.\n        return dependency\n\n    if dependency.find(\";\") > 0:\n        # This is a conditional dependency.\n        return dependency\n\n    return dependency + \"==0.0\"", "idx": 400}
{"namespace": "pythonforandroid.graph.fix_deplist", "completion": "    for i, dep in enumerate(deps):\n        if isinstance(dep, str):\n            deps[i] = (dep,)\n        else:\n            deps[i] = tuple(dep)\n    return deps", "idx": 401}
{"namespace": "pythonforandroid.util.walk_valid_filens", "completion": "    for dirpath, dirnames, filenames in walk(base_dir):\n        for invalid_dir_name in invalid_dir_names:\n            if invalid_dir_name in dirnames:\n                dirnames.remove(invalid_dir_name)\n        for filename in filenames:\n            full_path = join(dirpath, filename)\n            if not any(fnmatch(full_path, pattern) for pattern in\n                       invalid_file_patterns):\n                yield full_path", "idx": 402}
{"namespace": "pythonforandroid.bootstrap._cmp_bootstraps_by_priority", "completion": "    if a.priority == b.priority:\n        return a.name < b.name\n    return a.priority < b.priority", "idx": 403}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.all_bootstraps", "completion": "        bootstrap_dir = join(cls.ctx.root_dir, 'bootstraps')\n        if not isdir(bootstrap_dir):\n            raise BuildInterruptingException(\n                'Could not find bootstraps directory at {}'.format(bootstrap_dir))\n        bootstraps = set()\n        for filename in listdir(bootstrap_dir):\n            if filename.startswith('__'):\n                continue\n            if not isdir(join(bootstrap_dir, filename)):\n                continue\n            bootstraps.add(filename)\n        return bootstraps\n\n    ", "idx": 404}
{"namespace": "mmcv.image.colorspace._convert_input_type_range", "completion": "    if img.dtype == np.float32:\n        return img\n    img = img.astype(np.float32) / 255.\n    return img", "idx": 405}
{"namespace": "mackup.utils.error", "completion": "    sys.stderr.write(message + \"\\n\")\n    sys.exit(1)", "idx": 406}
{"namespace": "mmcv.image.colorspace._convert_output_type_range", "completion": "    if dst_type not in (np.uint8, np.float32):\n        raise TypeError('The dst_type should be np.float32 or np.uint8, '\n                        f'but got {dst_type}')\n    if dst_type == np.uint8:\n        img = img.round()\n        img8 = img.astype(np.uint8)\n        return img8\n    else:\n        return img", "idx": 407}
{"namespace": "mackup.utils.is_process_running", "completion": "    # Check if the process is running\n    process_list = subprocess.Popen(\n        [\"pgrep\", \"-f\", process_name], stdout=subprocess.PIPE, stderr=subprocess.PIPE\n    )\n    output, error = process_list.communicate()\n    if output:\n        return True\n    else:\n        return False", "idx": 408}
{"namespace": "stellar.operations._get_pid_column", "completion": "    server_version = raw_conn.engine.dialect.server_version_info\n    if server_version[0] == 9 and server_version[1] == 2:\n        return 'procpid'\n    elif server_version[0] == 9 and server_version[1] == 3:\n        return 'pid'\n    elif server_version[0] == 9 and server_version[1] == 4:\n        return 'pid'\n    elif server_version[0] == 9 and server_version[1] == 5:\n        return 'pid'\n    elif server_version[0] == 9 and server_version[1] == 6:\n        return 'pid'\n    elif server_version[0] == 10 and server_version[1] == 0:\n        return 'pid'\n    elif server_version[0] == 10 and server_version[1] == 1:\n        return 'pid'\n    elif server_version[0] == 10 and server_version[1] == 2:\n        return 'pid'\n    elif server_version[0] == 10 and server_version[1] == 3:\n        return 'pid'\n    elif server_version[0] == 10 and server_version[1] == 4:\n        return 'pid'\n    elif server_version[0] == 10 and server_version[1] == 5:\n        return 'pid'\n    elif server_version[0] == 10 and server_version[1] == 6:\n        return 'pid'\n    elif server_version[0] == 10 and server_version[1] == 7:\n        return 'pid'\n    elif server_version[0] == 10 and server_version[1] == 8:\n        return 'pid'\n    elif server_version[0] == 10 and server_version[1] == 9:\n        return 'pid'\n    elif server_version[0] == 11 and server_version[1] == 0:\n        return 'pid'\n    elif server_version[0] == 11 and server_version[1] == 1:\n        return 'pid'\n    elif server_version", "idx": 409}
{"namespace": "imapclient.imap_utf7.encode", "completion": "    if not isinstance(s, str):\n        return s\n\n    res = []\n    # Store base64 substring that will be encoded once stepping on end shift character\n    b64_buffer = []\n    for c in s:\n        # Shift character without anything in buffer -> starts storing base64 substring\n        if c == \"&\" and not b64_buffer:\n            b64_buffer.append(c)\n        # End shift char. -> append the encoded buffer to the result and reset it\n        elif c == \"-\" and b64_buffer:\n            res.append(base64_utf7_encode(b64_buffer))\n            b64_buffer = []\n        # Still buffering between the shift character and the shift back to ASCII\n        elif b64_buffer:\n            b64_buffer.append(c)\n        # No buffer initialized yet, should be an ASCII printable char\n        else:\n            res.append(c)\n\n    # Encode the remaining buffer if any\n    if b64_buffer:\n        res.append(base64_utf7_encode(b64_buffer))\n\n    return \"\".join(res).encode(\"utf-8\")", "idx": 410}
{"namespace": "imapclient.version._imapclient_version_string", "completion": "    major, minor, micro, releaselevel = vinfo\n\n    if releaselevel == \"final\":\n        return f\"{major}.{minor}.{micro}\"\n    else:\n        return f\"{major}.{minor}.{micro}-{releaselevel}\"", "idx": 411}
{"namespace": "telethon.helpers.generate_key_data_from_nonce", "completion": "    server_nonce_bytes = server_nonce.to_bytes(8, byteorder='little')\n    new_nonce_bytes = new_nonce.to_bytes(8, byteorder='little')\n\n    hash1 = sha1(server_nonce_bytes).digest()\n    hash2 = sha1(new_nonce_bytes).digest()\n    hash3 = sha1(hash2[:12] + hash1).digest()\n\n    key = hash1 + hash2[:12]\n    iv = hash2[12:] + hash3[:4] + new_nonce_bytes[:4]\n\n    return key, iv", "idx": 412}
{"namespace": "hbmqtt.codecs.bytes_to_int", "completion": "    return int.from_bytes(data, byteorder='big')", "idx": 413}
{"namespace": "zulipterminal.helper.display_error_if_present", "completion": "    if \"error\" in response:\n        if hasattr(controller, \"view\"):\n            controller.report_error(response[\"error\"])\n\n", "idx": 414}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._decode_message_id", "completion": "        from zulipterminal.config.regexes import REGEX_MESSAGE_ID\n        if re.match(REGEX_MESSAGE_ID, message_id):\n            return int(message_id)\n        else:\n            return None", "idx": 415}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton.handle_narrow_link", "completion": "        parsed_link = self._parse_narrow_link(self.link)\n        error = self._validate_narrow_link(parsed_link)\n        if error:\n            self.view.footer.update_footer(error)\n        else:\n            self._switch_narrow_to(parsed_link)", "idx": 416}
{"namespace": "zulipterminal.config.color.color_properties", "completion": "    new_colors = colors\n    for p in prop:\n        for color in new_colors:\n            setattr(color, p, f\"{color.value} {p}\")\n    return new_colors", "idx": 417}
{"namespace": "twilio.base.deserialize.decimal", "completion": "    if d is None:\n        return d\n    try:\n        return Decimal(d, BasicContext)\n    except (TypeError, ValueError):\n        return d", "idx": 418}
{"namespace": "twilio.base.deserialize.integer", "completion": "    try:\n        return int(i)\n    except ValueError:\n        return i", "idx": 419}
{"namespace": "twilio.base.serialize.object", "completion": "    if isinstance(obj, dict):\n        return json.dumps(obj)\n    return obj", "idx": 420}
{"namespace": "twilio.base.serialize.map", "completion": "    return [serialize_func(x) for x in lst]", "idx": 421}
{"namespace": "twilio.base.obsolete.deprecated_method", "completion": "    if new_func is None:\n        return functools.partial(deprecated_method, new_func=new_func)\n\n    @functools.wraps(new_func)\n    def deprecated_method_wrapper(*args, **kwargs):\n        warnings.warn(\n            \"{} has been deprecated and will be removed in a future version of the library. \"\n            \"Please refer to current documentation for guidance.\".format(new_func.__name__),\n            category=DeprecationWarning,\n            stacklevel=2,\n        )\n        return new_func(*args, **kwargs)\n\n    return deprecated_method_wrapper", "idx": 422}
{"namespace": "chatette.utils.sample_indulgent", "completion": "    if nb_items > len(array):\n        return deepcopy(array)\n    return sample(array, nb_items)", "idx": 423}
{"namespace": "chatette.utils.rchop", "completion": "    if string.endswith(ending):\n        return string[:-len(ending)]\n    return string", "idx": 424}
{"namespace": "chatette.utils.str_to_bool", "completion": "    if text.lower() == \"true\":\n        return True\n    if text.lower() == \"false\":\n        return False\n    raise ValueError(\"The string '{}' is neither 'True' nor 'False'.\".format(text))", "idx": 425}
{"namespace": "chatette.utils.min_if_exist", "completion": "    if n1 is None and n2 is None:\n        return None\n    if n1 is None:\n        return n2\n    if n2 is None:\n        return n1\n    return min(n1, n2)", "idx": 426}
{"namespace": "chatette.utils.append_to_list_in_dict", "completion": "    if key not in dict_of_lists:\n        dict_of_lists[key] = [value]\n    else:\n        dict_of_lists[key].append(value)", "idx": 427}
{"namespace": "chatette.utils.extend_list_in_dict", "completion": "    if key not in dict_of_lists:\n        dict_of_lists[key] = values\n    else:\n        dict_of_lists[key].extend(values)", "idx": 428}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy._is_end_regex", "completion": "        if word.endswith(REGEX_SYM):\n            if len(word) < 2:\n                return True\n            if word[-2] == REGEX_SYM:\n                return True\n        return False", "idx": 429}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.execute", "completion": "        raise NotImplementedError()", "idx": 430}
{"namespace": "aioxmpp.network.group_and_order_srv_records", "completion": "    if rng is None:\n        rng = random.Random()\n\n    def key(item):\n        return item[0]\n\n    def cmp(a, b):\n        if a[0] < b[0]:\n            return -1\n        elif a[0] > b[0]:\n            return 1\n        else:\n            return 0\n\n    def cmp_weight(a, b):\n        if a[1] < b[1]:\n            return -1\n        elif a[1] > b[1]:\n            return 1\n        else:\n            return 0\n\n    def cmp_host(a, b):\n        if a[2][0] < b[2][0]:\n            return -1\n        elif a[2][0] > b[2][0]:\n            return 1\n        else:\n            return 0\n\n    def cmp_port(a, b):\n        if a[2][1] < b[2][1]:\n            return -1\n        elif a[2][1] > b[2][1]:\n            return 1\n        else:\n            return 0\n\n    def cmp_host_port(a, b):\n        if a[2] < b[2]:\n            return -1\n        elif a[2] > b[2]:\n            return 1\n        else:\n            return 0\n\n    def cmp_host_port_weight(a, b):\n        if a[1] < b[1]:\n            return -1\n        elif a[1] > b[1]:\n            return 1\n        else:\n            return 0\n\n    def cmp_host_port_weight_rng(a, b):\n        if a[1] < b[1]:\n            return -1\n        elif a[1] > b[1]:\n            return 1\n        else:\n            return rng.randint(0, 1)\n\n    def cmp_host_port_weight_rng_host(a, b):\n        if a[2] < b[2]:\n            return -1", "idx": 431}
{"namespace": "aioxmpp.nonza.StreamFeatures.get_feature", "completion": "        try:\n            return self.features[feature_cls.TAG][0]\n        except IndexError:\n            return default", "idx": 432}
{"namespace": "aioxmpp.connector.XMPPOverTLSConnector._context_factory_factory", "completion": "        def context_factory():\n            ssl_context = metadata.ssl_context_factory()\n            verifier.setup_context(ssl_context, None)\n            if hasattr(ssl_context, \"set_alpn_protocols\"):\n                ssl_context.set_alpn_protocols([\"xmpp-client\"])\n            else:\n                logger.warning(\n                    \"The ssl_context does not have the set_alpn_protos method, \"\n                    \"so the ALPN protocol will not be set.\")\n            return ssl_context\n\n        return context_factory", "idx": 433}
{"namespace": "aioxmpp.xmltestutils.element_path", "completion": "    path = []\n    while el is not upto:\n        if el.getparent() is None:\n            raise ValueError(\"Element is not in the tree\")\n        path.append(el.tag)\n        el = el.getparent()\n    path.reverse()\n    return \"/\".join(path)", "idx": 434}
{"namespace": "aioxmpp.structs.JID.fromstr", "completion": "        if not s:\n            raise ValueError(\"empty JID\")\n\n        if s[0] == \"@\":\n            raise ValueError(\"JID must not start with @\")\n\n        if s[-1] == \"@\":\n            raise ValueError(\"JID must not end with @\")\n\n        if s[0] == \"/\":\n            raise ValueError(\"JID must not start with /\")\n\n        if s[-1] == \"/\":\n            raise ValueError(\"JID must not end with /\")\n\n        if \"//\" in s:\n            raise ValueError(\"JID must not contain //\")\n\n        if \"@/\" in s:\n            raise ValueError(\"JID must not contain @/\")\n\n        if \"/@\" in s:\n            raise ValueError(\"JID must not contain /@\")\n\n        if \"//\" in s:\n            raise ValueError(\"JID must not contain //\")\n\n        if \"@/\" in s:\n            raise ValueError(\"JID must not contain @/\")\n\n        if \"/@\" in s:\n            raise ValueError(\"JID must not contain /@\")\n\n        if \"//\" in s:\n            raise ValueError(\"JID must not contain //\")\n\n        if \"@/\" in s:\n            raise ValueError(\"JID must not contain @/\")\n\n        if \"/@\" in s:\n            raise ValueError(\"JID must not contain /@\")\n\n        if \"//\" in s:\n            raise ValueError(\"JID must not contain //\")\n\n        if \"@/\" in s:\n            raise ValueError(\"JID must not contain @/\")\n\n        if \"/@\" in s:\n            raise ValueError(\"JID must not contain /@\")\n\n        if \"//\" in s:\n            raise ValueError(\"JID must not contain //\")\n\n        if \"@/\" in s:\n            raise ValueError(\"JID must not contain @/\")\n\n        if \"/@\" in s:\n            raise ValueError(\"JID must not contain /@\")\n\n        if \"//\" in s:\n            raise ValueError(\"JID must not contain //\")\n\n        if", "idx": 435}
{"namespace": "aioxmpp.security_layer.extract_python_dict_from_x509", "completion": "    subject = x509.get_subject()\n    subjectAltName = x509.get_extension('subjectAltName')\n    subjectAltName = subjectAltName.get_data()\n    subjectAltName = subjectAltName.get_values()\n    subjectAltName = subjectAltName[0]\n    subjectAltName = subjectAltName.split(',')\n    subjectAltName = [x.strip() for x in subjectAltName]\n    subjectAltName = [x.split(':')[1] for x in subjectAltName]\n    subjectAltName = [x.strip() for x in subjectAltName]\n    subjectAltName = [x.strip('\"') for x in subjectAltName]\n    subjectAltName = [x.strip(\"'\") for x in subjectAltName]\n    subjectAltName = [x.strip() for x in subjectAltName]\n    subjectAltName = [x.strip(',') for x in subjectAltName]\n    subjectAltName = [x.strip(' ') for x in subjectAltName]\n    subjectAltName = [x.strip('\\n') for x in subjectAltName]\n    subjectAltName = [x.strip('\\r') for x in subjectAltName]\n    subjectAltName = [x.strip('\\t') for x in subjectAltName]\n    subjectAltName = [x.strip('\\v') for x in subjectAltName]\n    subjectAltName = [x.strip('\\f') for x in subjectAltName]\n    subjectAltName = [x.strip('\\b') for x in subjectAltName]\n    subjectAltName = [x.strip('\\a') for x in subjectAltName]\n    subjectAltName = [x.strip('\\0') for x in subjectAltName]\n    subjectAltName = [x.strip('\\x0b') for x in subjectAltName]\n    subjectAltName = [x.strip('\\x0c') for x in subjectAltName]\n    subjectAltName = [x.strip('\\x0d') for x in subjectAltName]\n    subjectAltName = [x.strip('\\x0e') for x in", "idx": 436}
{"namespace": "aioxmpp.security_layer.extract_blob", "completion": "    return x509.to_cryptography().public_bytes(\n        encoding=serialization.Encoding.DER\n    )\n\n", "idx": 437}
{"namespace": "aioxmpp.security_layer.blob_to_pyasn1", "completion": "    return pyasn1.codec.der.decoder.decode(blob, asn1Spec=pyasn1_modules.rfc2459.Certificate())[0]", "idx": 438}
{"namespace": "aioxmpp.security_layer.extract_pk_blob_from_pyasn1", "completion": "    return extract_blob(pyasn1_struct.getComponentByPosition(0))", "idx": 439}
{"namespace": "aioxmpp.callbacks.AdHocSignal.ASYNC_WITH_LOOP", "completion": "        loop = asyncio.get_event_loop() if loop is None else loop\n\n        def async_wrapper(f, args, kwargs):\n            if kwargs:\n                functools.partial(f, *args, **kwargs)\n            loop.call_soon(cls._async_wrapper, f, loop, args, kwargs)\n            return True\n\n        return async_wrapper", "idx": 440}
{"namespace": "aioxmpp.callbacks.AdHocSignal.SPAWN_WITH_LOOP", "completion": "        if loop is None:\n            loop = asyncio.get_event_loop()\n\n        def create_wrapper(f):\n            if not asyncio.iscoroutinefunction(f):\n                raise TypeError(\"must be a coroutine function, got {}\"\n                                .format(f))\n            return functools.partial(cls._spawn_wrapper,\n                                     f,\n                                     loop)\n\n        return create_wrapper\n", "idx": 441}
{"namespace": "aioxmpp.callbacks.first_signal", "completion": "    fut = asyncio.Future()\n    for signal in signals:\n        signal.connect(fut.set_result)\n    return fut", "idx": 442}
{"namespace": "aioxmpp.tasks.TaskPool.spawn", "completion": "", "idx": 443}
{"namespace": "aioxmpp.protocol.send_and_wait_for", "completion": "    xmlstream.send_xso(send)\n    if timeout is not None:\n        try:\n            result = await asyncio.wait_for(wait_for, timeout=timeout)\n        except asyncio.TimeoutError:\n            raise TimeoutError from None\n    else:\n        result = await wait_for\n    if cb is not None:\n        cb(result)\n    return result", "idx": 444}
{"namespace": "aioxmpp.testutils.run_coroutine_with_peer", "completion": "    if not loop:\n        loop = asyncio.get_event_loop()\n\n    fut = asyncio.Future()\n    fut.set_result(None)\n    fut = asyncio.wait_for(fut, timeout=timeout)\n\n    fut2 = asyncio.Future()\n    fut2.set_result(None)\n    fut2 = asyncio.wait_for(fut2, timeout=timeout)\n\n    fut3 = asyncio.gather(\n        coroutine,\n        peer_coroutine,\n        fut,\n        fut2,\n        loop=loop,\n        return_exceptions=True)\n    try:\n        loop.run_until_complete(fut3)\n    except asyncio.TimeoutError:\n        raise AssertionError(\"timeout\")\n    except Exception as exc:\n        raise AssertionError(\"unexpected exception: \"+repr(exc))\n\n    if fut3.exception():\n        raise fut3.exception()\n\n    if fut.exception():\n        raise fut.exception()\n\n    if fut2.exception():\n        raise fut2.exception()\n\n    return fut3.result()", "idx": 445}
{"namespace": "aioxmpp.testutils.make_listener", "completion": "    listener = unittest.mock.Mock()\n    for name, signal in instance.__dict__.items():\n        if isinstance(signal, callbacks.Signal):\n            listener.add_child(name, signal)\n    return listener\n\n", "idx": 446}
{"namespace": "aioxmpp.vcard.service.VCardService.set_vcard", "completion": "        iq = aioxmpp.IQ(\n            type_=aioxmpp.IQType.SET,\n            to=jid,\n            payload=vcard,\n        )\n\n        try:\n            return await self.client.send(iq)\n        except aioxmpp.XMPPCancelError as e:\n            if e.condition in (\n                    aioxmpp.ErrorCondition.FEATURE_NOT_IMPLEMENTED,\n                    aioxmpp.ErrorCondition.ITEM_NOT_FOUND):\n                return vcard_xso.VCard()\n            else:\n                raise", "idx": 447}
{"namespace": "aioxmpp.rsm.xso.ResultSetMetadata.limit", "completion": "        result = copy.copy(self)\n        result.max_ = max_\n        return result\n", "idx": 448}
{"namespace": "aioxmpp.muc.service.Room.features", "completion": "        return self._service.features\n", "idx": 449}
{"namespace": "aioxmpp.xso.query.EvaluationContext.eval_bool", "completion": "    def eval_bool(self, expr):\n        \"\"\"\n        Evaluate the expression `expr` and return the truthness of its result. A result of an expression is said to be true if it contains at least one value. It has the same semantics as :func:`bool` on sequences.\n        Input-Output Arguments\n        :param self: EvaluationContext. An instance of the EvaluationContext class.\n        :param expr: The expression to be evaluated.\n        :return: Boolean. The truthness of the evaluated expression.\n        \"\"\"\n        return bool(self.eval(expr))", "idx": 450}
{"namespace": "aioxmpp.xso.query._BoolOpMixin.eval", "completion": "        return self.eval_leaf(ec)", "idx": 451}
{"namespace": "aioxmpp.xso.model.drop_handler", "completion": "    depth = 1\n    while depth > 0:\n        ev_type, *_ = yield\n        if ev_type == \"start\":\n            depth += 1\n        elif ev_type == \"end\":\n            depth -= 1", "idx": 452}
{"namespace": "aioxmpp.xso.model.guard", "completion": "    depth = 1\n    while depth:\n        ev = yield\n        if ev[0] == \"start\":\n            depth += 1\n        elif ev[0] == \"end\":\n            depth -= 1\n\n    try:\n        return dest.send(ev_args)\n    except StopIteration as err:\n        return err.value\n    except Exception:\n        if error_handler is not None:\n            if error_handler(None, ev_args, sys.exc_info()):\n                return\n        raise", "idx": 453}
{"namespace": "aioxmpp.xso.model.capture_events", "completion": "    try:\n        while True:\n            ev = yield\n            dest.append(ev)\n            receiver.send(ev)\n    finally:\n        dest.clear()", "idx": 454}
{"namespace": "aioxmpp.xso.model.events_to_sax", "completion": "    for ev_type, *ev_args in events:\n        if ev_type == \"start\":\n            dest.startElementNS(ev_args[0], None, ev_args[2])\n        elif ev_type == \"end\":\n            dest.endElementNS(ev_args[0], None)\n        elif ev_type == \"text\":\n            dest.characters(ev_args[0])\n\n", "idx": 455}
{"namespace": "aioxmpp.adhoc.service.AdHocClient.get_command_info", "completion": "        disco = self.dependencies[aioxmpp.disco.DiscoClient]\n        response = await disco.query_info(\n            peer_jid,\n            node=command_name,\n        )\n        return response", "idx": 456}
{"namespace": "aioxmpp.entitycaps.caps115.build_identities_string", "completion": "    identities = list(\n        escape(\n            \"{}/{}/{}\".format(\n                identity.category,\n                identity.type_,\n                identity.name,\n            )\n        ).encode(\"utf-8\")\n        for identity in identities\n    )\n\n    if len(set(identities)) != len(identities):\n        raise ValueError(\"duplicate identity\")\n\n    identities.sort()\n    identities.append(b\"\")\n    return b\"<\".join(identities)", "idx": 457}
{"namespace": "aioxmpp.entitycaps.caps115.build_features_string", "completion": "    features = [\n        escape(feature).encode(\"utf-8\")\n        for feature in features\n    ]\n\n    if len(set(features)) != len(features):\n        raise ValueError(\"duplicate feature\")\n\n    features.sort()\n    features.append(b\"\")\n    return b\"<\".join(features)", "idx": 458}
{"namespace": "aioxmpp.entitycaps.caps115.build_forms_string", "completion": "    forms = [\n        b\"/\".join([\n            escape(form.type_).encode(\"utf-8\"),\n            escape(form.name or \"\").encode(\"utf-8\"),\n            escape(form.label or \"\").encode(\"utf-8\"),\n        ])\n        for form in forms\n    ]\n\n    if len(set(forms)) != len(forms):\n        raise ValueError(\"duplicate form\")\n\n    forms.sort()\n    forms.append(b\"\")\n    return b\"<\".join(forms)", "idx": 459}
{"namespace": "aioxmpp.entitycaps.caps115.Key.path", "completion": "        return pathlib.Path(\n            \"hashes\",\n            urllib.parse.quote(self.node, safe=\"\"),\n            \"{}.{}\".format(self.algo, self.ver),\n        )\n\n    ", "idx": 460}
{"namespace": "aioxmpp.entitycaps.caps390._process_features", "completion": "    return b\"\".join(\n        feature.encode(\"utf-8\") + b\"\\x1f\"\n        for feature in features\n    ) + b\"\\x1e\"", "idx": 461}
{"namespace": "aioxmpp.entitycaps.caps390._process_identities", "completion": "    parts = [\n        _process_identity(identity)\n        for identity in identities\n    ]\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1d\"", "idx": 462}
{"namespace": "aioxmpp.entitycaps.caps390._process_extensions", "completion": "    parts = [\n        _process_form(ext)\n        for ext in exts\n    ]\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1c\"", "idx": 463}
{"namespace": "aioxmpp.entitycaps.caps390._calculate_hash", "completion": "    if algo == \"sha-1\":\n        return aioxmpp.hashes.sha1(hash_input)\n    elif algo == \"sha-224\":\n        return aioxmpp.hashes.sha224(hash_input)\n    elif algo == \"sha-256\":\n        return aioxmpp.hashes.sha256(hash_input)\n    elif algo == \"sha-384\":\n        return aioxmpp.hashes.sha384(hash_input)\n    elif algo == \"sha-512\":\n        return aioxmpp.hashes.sha512(hash_input)\n    else:\n        raise ValueError(\"Unsupported hash algorithm: {}\".format(algo))", "idx": 464}
{"namespace": "aioxmpp.entitycaps.caps390.Key.node", "completion": "        return \"sha-{}\".format(self.algo)\n\n    ", "idx": 465}
{"namespace": "aioxmpp.entitycaps.caps390.Key.path", "completion": "        return pathlib.PurePosixPath(\n            \"caps\",\n            base64.b64encode(self.digest).decode(\"ascii\"),\n            \"{}.{}\".format(self.algo, \"bin\")\n        )\n", "idx": 466}
{"namespace": "aioxmpp.entitycaps.caps390.Implementation.extract_keys", "completion": "        if presence.xep0390_caps is None:\n            return\n        for algo, digest in presence.xep0390_caps.digests.items():\n            yield Key(algo, digest)\n\n    ", "idx": 467}
{"namespace": "aioxmpp.roster.service.RosterClient.approve", "completion": "        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.SUBSCRIBED,\n                            to=peer_jid)\n        )", "idx": 468}
{"namespace": "aioxmpp.roster.service.RosterClient.subscribe", "completion": "        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.SUBSCRIBE,\n                            to=peer_jid)\n        )", "idx": 469}
{"namespace": "aioxmpp.roster.service.RosterClient.unsubscribe", "completion": "        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.UNSUBSCRIBE,\n                            to=peer_jid)\n        )", "idx": 470}
{"namespace": "aioxmpp.forms.fields.BoundSingleValueField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass\n\n    ", "idx": 471}
{"namespace": "aioxmpp.forms.fields.BoundMultiValueField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass\n\n    ", "idx": 472}
{"namespace": "aioxmpp.forms.fields.BoundOptionsField.options", "completion": "        try:\n            del self._options\n        except AttributeError:\n            pass\n", "idx": 473}
{"namespace": "aioxmpp.forms.fields.BoundSelectField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass\n\n    ", "idx": 474}
{"namespace": "aioxmpp.forms.fields.BoundMultiSelectField.value", "completion": "        try:\n            return self._value\n        except AttributeError:\n            self._value = self.field.default()\n            return self._value\n", "idx": 475}
{"namespace": "cupy.random._generator.reset_states", "completion": "    _random_states.clear()", "idx": 476}
{"namespace": "cupy.random._generator._check_and_get_dtype", "completion": "    if dtype is None:\n        dtype = float\n    dtype = numpy.dtype(dtype)\n    if dtype.char not in 'fd':\n        raise TypeError('dtype must be float or double')\n    return dtype", "idx": 477}
{"namespace": "cupy_builder._command.filter_files_by_extension", "completion": "    sources_ext = []\n    sources_others = []\n    for src in sources:\n        if src.endswith(extension):\n            sources_ext.append(src)\n        else:\n            sources_others.append(src)\n    return sources_ext, sources_others", "idx": 478}
{"namespace": "datasets.table._in_memory_arrow_table_from_file", "completion": "    table = pa.Table.from_pandas(pd.read_csv(filename))\n    return table", "idx": 479}
{"namespace": "datasets.table._in_memory_arrow_table_from_buffer", "completion": "    buffer_reader = pa.BufferReader(buffer)\n    opened_stream = pa.ipc.open_stream(buffer_reader)\n    pa_table = opened_stream.read_all()\n    return pa_table\n\n", "idx": 480}
{"namespace": "datasets.table._interpolation_search", "completion": "    if len(arr) == 0:\n        raise IndexError(\"Array is empty\")\n    if x < arr[0] or x > arr[-1]:\n        raise IndexError(\"Query is outside the array values\")\n    i, j = 0, len(arr) - 1\n    while i < j:\n        m = (i + j) // 2\n        if arr[m] <= x < arr[m + 1]:\n            return m\n        elif x < arr[m]:\n            j = m - 1\n        else:\n        i = m + 1\n    return i\n\n", "idx": 481}
{"namespace": "datasets.data_files._is_inside_unrequested_special_dir", "completion": "    # We just need to check if every hidden part from the path is present explicly in the pattern.\n    # Since we assume that the path matches the pattern, it's equivalent to counting that both\n    # the path and the pattern have the same number of hidden parts.\n    hidden_directories_in_path = [\n        part for part in PurePath(matched_rel_path).parts if part.startswith(\".\") and not set(part) == {\".\"}\n    ]\n    hidden_directories_in_pattern = [\n        part for part in PurePath(pattern).parts if part.startswith(\".\") and not set(part) == {\".\"}\n    ]\n    return len(hidden_directories_in_path) != len(hidden_directories_in_pattern)", "idx": 482}
{"namespace": "datasets.data_files._is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir", "completion": "    # We just need to check if every special directories from the path is present explicly in the pattern.\n    # Since we assume that the path matches the pattern, it's equivalent to counting that both\n    # the parent path and the parent pattern have the same number of special directories.\n    data_dirs_to_ignore_in_path = [part for part in PurePath(matched_rel_path).parent.parts if part.startswith(\".\")]\n    data_dirs_to_ignore_in_pattern = [part for part in PurePath(pattern).parent.parts if part.startswith(\".\")]\n    return len(data_dirs_to_ignore_in_path) != len(data_dirs_to_ignore_in_pattern)", "idx": 483}
{"namespace": "datasets.iterable_dataset._batch_to_examples", "completion": "    return [dict(zip(batch, t)) for t in zip(*batch.values())]\n\n", "idx": 484}
{"namespace": "datasets.iterable_dataset._examples_to_batch", "completion": "    columns = {}\n    for example in examples:\n        for column_name, value in example.items():\n            if column_name not in columns:\n                columns[column_name] = []\n            columns[column_name].append(value)\n    return columns\n\n", "idx": 485}
{"namespace": "datasets.iterable_dataset.RandomlyCyclingMultiSourcesExamplesIterable._iter_random_indices", "completion": "        while True:\n            yield from rng.choice(num_sources, size=random_batch_size, replace=True, p=p)\n\n    ", "idx": 486}
{"namespace": "datasets.iterable_dataset.BufferShuffledExamplesIterable._iter_random_indices", "completion": "        while True:\n            yield from (int(i) for i in rng.integers(0, buffer_size, size=random_batch_size))\n\n    ", "idx": 487}
{"namespace": "datasets.iterable_dataset.IterableDataset.remove_columns", "completion": "        if isinstance(column_names, str):\n            column_names = [column_names]\n\n        if self._info:\n            info = copy.deepcopy(self._info)\n            if self._info.features is not None:\n                for column_name in column_names:\n                    if column_name not in self._info.features:\n                        raise ValueError(\n                            f\"Column name {column_name} not in the \"\n                            \"dataset. Columns in the dataset: \"\n                            f\"{list(self._info.features.keys())}.\"\n                        )\n                info.features = Features({c: info.features[c] for c in self._info.features if c not in column_names})\n                # check that it's still valid, especially with regard to task templates\n                try:\n                    info.copy()\n                except ValueError:\n                    info.task_templates = None\n\n        ex_iterable = RemoveColumnsIterable(self._ex_iterable, column_names)\n        return IterableDataset(\n            ex_iterable=ex_iterable,\n            info=info,\n            split=self._split,\n            formatting=self._formatting,\n            shuffling=self._shuffling,\n            distributed=self._distributed,\n            token_per_repo_id=self._token_per_repo_id,\n        )", "idx": 488}
{"namespace": "datasets.dataset_dict.DatasetDict.with_format", "completion": "        dataset = copy.deepcopy(self)\n        dataset.set_format(type=type, columns=columns, output_all_columns=output_all_columns, **format_kwargs)\n        return dataset", "idx": 489}
{"namespace": "datasets.dataset_dict.DatasetDict.with_transform", "completion": "        dataset = copy.deepcopy(self)\n        dataset.set_transform(transform=transform, columns=columns, output_all_columns=output_all_columns)\n        return dataset", "idx": 490}
{"namespace": "datasets.dataset_dict.DatasetDict.align_labels_with_mapping", "completion": "        self._check_values_type()\n        return DatasetDict({k: dataset.align_labels_with_mapping(label2id=label2id, label_column=label_column) for k, dataset in self.items()})\n", "idx": 491}
{"namespace": "datasets.dataset_dict.IterableDatasetDict.map", "completion": "        return IterableDatasetDict(\n            {\n                k: dataset.map(\n                    function=function,\n                    with_indices=with_indices,\n                    input_columns=input_columns,\n                    batched=batched,\n                    batch_size=batch_size,\n                    drop_last_batch=drop_last_batch,\n                    remove_columns=remove_columns,\n                    fn_kwargs=fn_kwargs,\n                )\n                for k, dataset in self.items()\n            }\n        )", "idx": 492}
{"namespace": "datasets.dataset_dict.IterableDatasetDict.filter", "completion": "        return IterableDatasetDict(\n            {\n                k: dataset.filter(\n                    function=function,\n                    with_indices=with_indices,\n                    input_columns=input_columns,\n                    batched=batched,\n                    batch_size=batch_size,\n                    fn_kwargs=fn_kwargs,\n                )\n                for k, dataset in self.items()\n            }\n        )", "idx": 493}
{"namespace": "datasets.arrow_dataset.Dataset.num_rows", "completion": "        if self._indices is not None:\n            return self._indices.num_rows\n        return self._data.num_rows\n\n    ", "idx": 494}
{"namespace": "datasets.filesystems.extract_path_from_uri", "completion": "    if dataset_path.startswith(\"s3://\"):\n        dataset_path = dataset_path[5:]\n    return dataset_path", "idx": 495}
{"namespace": "datasets.filesystems.is_remote_filesystem", "completion": "    return fs.protocol.startswith(\"s3\") or fs.protocol.startswith(\"gs\")", "idx": 496}
{"namespace": "datasets.utils.file_utils.hash_url_to_filename", "completion": "    if etag is not None:\n        url += \".\" + hash_text(etag)\n    if url.endswith(\".h5\"):\n        url += \".h5\"\n    return hash_text(url)\n\n", "idx": 497}
{"namespace": "datasets.utils.hub.hf_hub_url", "completion": "    ", "idx": 498}
{"namespace": "datasets.utils.sharding._number_of_shards_in_gen_kwargs", "completion": "    list_sizes = {len(value) for value in gen_kwargs.values() if isinstance(value, list)}\n    if len(list_sizes) > 1:\n        raise ValueError(\n            f\"The gen_kwargs must contain lists of the same size. \"\n            f\"Found lists of sizes {list_sizes} in the gen_kwargs.\"\n        )\n    return list_sizes.pop() if list_sizes else 1", "idx": 499}
{"namespace": "datasets.utils.sharding._distribute_shards", "completion": "    if num_shards <= max_num_jobs:\n        return [range(num_shards)]\n    else:\n        shard_indices_per_group = []\n        num_shards_per_group = num_shards // max_num_jobs\n        for group_idx in range(max_num_jobs):\n            start_idx = group_idx * num_shards_per_group\n            end_idx = (group_idx + 1) * num_shards_per_group\n            if group_idx == max_num_jobs - 1:\n                end_idx = num_shards\n            shard_indices_per_group.append(range(start_idx, end_idx))\n        return shard_indices_per_group", "idx": 500}
{"namespace": "datasets.utils.py_utils.temporary_assignment", "completion": "    original_value = getattr(obj, attr)\n    setattr(obj, attr, value)\n    try:\n        yield\n    finally:\n        setattr(obj, attr, original_value)\n\n", "idx": 501}
{"namespace": "datasets.utils.extract.TarExtractor.extract", "completion": "        os.makedirs(output_path, exist_ok=True)\n        with tarfile.open(input_path, \"r\") as tar_file:\n            tar_file.extractall(output_path, members=cls.safemembers(tar_file, output_path))\n            tar_file.close()", "idx": 502}
{"namespace": "datasets.utils.extract.Extractor.infer_extractor_format", "completion": "        magic_number_length = cls._get_magic_number_max_length()\n        magic_number = cls._read_magic_number(path, magic_number_length=magic_number_length)\n        for extractor_format, extractor in cls.extractors.items():\n            if extractor.is_extractable(path, magic_number=magic_number):\n                return extractor_format\n        return \"\"\n\n    ", "idx": 503}
{"namespace": "datasets.utils.py_utils.asdict", "completion": "    if is_dataclass(obj):\n        return {field.name: asdict(getattr(obj, field.name)) for field in fields(obj)}\n    if isinstance(obj, (list, tuple)):\n        return [asdict(item) for item in obj]\n    if isinstance(obj, dict):\n        return {key: asdict(value) for key, value in obj.items()}\n    return obj\n\n", "idx": 504}
{"namespace": "datasets.utils.metadata.MetadataConfigs.from_dataset_card_data", "completion": "        if cls.FIELD_NAME in dataset_card_data:\n            metadata_configs = dataset_card_data[cls.FIELD_NAME]\n            if not isinstance(metadata_configs, list):\n                raise ValueError(\n                    f\"Expected {cls.FIELD_NAME} to be a list of dicts, but got {metadata_configs}\"\n                )\n            for metadata_config in metadata_configs:\n                if not isinstance(metadata_config, dict):\n                    raise ValueError(\n                        f\"Expected {cls.FIELD_NAME} to be a list of dicts, but got {metadata_config}\"\n                    )\n                for key, value in metadata_config.items():\n                    if key == \"config_name\":\n                        raise ValueError(\n                            f\"Expected {cls.FIELD_NAME} to not contain 'config_name' key, but got {metadata_config}\"\n                        )\n                    if key == \"data_files\":\n                        cls._raise_if_data_files_field_not_valid(metadata_config)\n            return cls({metadata_config.pop(\"config_name\"): metadata_config for metadata_config in metadata_configs})\n        else:\n            return cls()\n\n    ", "idx": 505}
{"namespace": "pymorphy2.analyzer.lang_dict_path", "completion": "    paths = _lang_dict_paths()\n    if lang in paths:\n        return paths[lang]\n    raise ValueError(\"Unknown language: %r\" % lang)", "idx": 506}
{"namespace": "pymorphy2.dawg.assert_can_create", "completion": "    if not EXTENSION_AVAILABLE:\n        raise NotImplementedError(\"DAWG extension is not available\")", "idx": 507}
{"namespace": "pymorphy2.opencorpora_dict.compile._to_paradigm", "completion": "    # extract prefixes\n    prefixes = set()\n    for form, tag in lexeme:\n        for prefix in form.split('-'):\n            if prefix in paradigm_prefixes:\n                prefixes.add(prefix)\n\n    if not prefixes:\n        # if no prefixes found, set stem to empty string\n        stem = ''\n        prefixes = [''] * len(lexeme)\n    else:\n        # if prefixes found, set stem to the first prefix\n        stem = prefixes.pop()\n\n    # extract suffixes\n    suffixes = []\n    tags = []\n    for form, tag in lexeme:\n        # remove prefix from form\n        form = form.replace(stem, '', 1)\n        # remove suffix from form\n        form = form.replace(stem, '', 1)\n        # add suffix and tag to lists\n        suffixes.append(form)\n        tags.append(tag)\n\n    # create paradigm\n    paradigm = zip(suffixes, tags, prefixes)\n\n    return stem, paradigm", "idx": 508}
{"namespace": "pymorphy2.units.by_analogy.KnownPrefixAnalyzer.tag", "completion": "        result = []\n        for prefix, unprefixed_word in self.possible_splits(word_lower):\n\n            method = (self, prefix)\n\n            tags = self.morph.tag(unprefixed_word)\n            for tag in tags:\n\n                if not tag.is_productive():\n                    continue\n\n                add_tag_if_not_seen(tag, result, seen_tags)\n\n        return result", "idx": 509}
{"namespace": "pymorphy2.units.by_analogy.UnknownPrefixAnalyzer.tag", "completion": "        result = []\n        for prefix, unprefixed_word in word_splits(word_lower):\n            for tag in self.morph.tag(unprefixed_word):\n                if not tag.is_productive():\n                    continue\n                add_tag_if_not_seen(tag, result, seen_tags)\n        return result", "idx": 510}
{"namespace": "benedict.dicts.keylist.keylist_util.get_item", "completion": "    item = d\n    for key in keys:\n        try:\n            item_key, item_value = _get_item_key_and_value(item, key)\n            item = item_value\n        except (IndexError, KeyError):\n            return None, None, None\n    return item, item_key, item_value", "idx": 511}
{"namespace": "benedict.dicts.keylist.keylist_util.set_item", "completion": "    \"\"\"\n    This function sets a value in a nested dictionary based on a list of keys. It iterates through the keys and checks if each key exists in the dictionary. If a key does not exist, it creates a new dictionary and assigns it as the value for that key. Finally, it sets the desired value in the last nested dictionary.\n    Input-Output Arguments\n    :param d: Dictionary. The dictionary in which to set the value.\n    :param keys: List of keys. The list of keys representing the nested structure in the dictionary.\n    :param value: Any. The value to be set in the nested dictionary.\n    :return: None.\n    \"\"\"\n\n    \"\"\"\n    This function sets a value in a nested dictionary based on a list of keys. It iterates through the keys and checks if each key exists in the dictionary. If a key does not exist, it creates a new dictionary and assigns it as the value for that key. Finally, it sets the desired value in the last nested dictionary.\n    Input-Output Arguments\n    :param d: Dictionary. The dictionary in which to set the value.\n    :param keys: List of keys. The list of keys representing the nested structure in the dictionary.\n    :param value: Any. The value to be set in the nested dictionary.\n    :return: None.\n    \"\"\"\n\n    \"\"\"\n    This function sets a value in a nested dictionary based on a list of keys. It iterates through the keys and checks if each key exists in the dictionary. If a key does not exist, it creates a new dictionary and assigns it as the value for that key. Finally, it sets the desired value in the last nested dictionary.\n    Input-Output Arguments\n    :param d: Dictionary. The dictionary in which to set the value.\n    :param keys: List of keys. The list of keys representing the nested structure in the dictionary.\n    :param value: Any. The value to be set in the nested dictionary.\n    :return: None.\n    \"\"\"\n\n    \"\"\"\n    This function sets a value in a nested dictionary based on a list of keys. It iterates through the keys and checks if each key exists in the dictionary. If a key does not exist, it creates a new dictionary and assigns it as the", "idx": 512}
{"namespace": "benedict.dicts.keypath.keypath_util._split_key_indexes", "completion": "    if not key:\n        return []\n\n    if key.endswith(\"]\") and key.count(\"[\") == key.count(\"]\"):\n        indexes = re.findall(KEY_INDEX_RE, key)\n        if indexes:\n            return [key.split(\"[\")[0]] + [int(index) for index in indexes]\n    return [key]", "idx": 513}
{"namespace": "feedparser.urls.make_safe_absolute_uri", "completion": "    if not rel:\n        return base\n    if not base:\n        return rel\n    if not rel.startswith('/'):\n        rel = '/' + rel\n    uri = _urljoin(base, rel)\n    if not uri:\n        return ''\n    scheme = urllib.parse.urlparse(uri).scheme\n    if scheme not in ACCEPTABLE_URI_SCHEMES:\n        return ''\n    return uri", "idx": 514}
{"namespace": "feedparser.api._open_resource", "completion": "    if isinstance(url_file_stream_or_string, str):\n        if url_file_stream_or_string.startswith('http://') or url_file_stream_or_string.startswith('https://'):\n            # URL\n            if handlers is None:\n                import urllib.request\n                handlers = []\n            opener = urllib.request.build_opener(*handlers)\n            if agent:\n                opener.addheaders = [('User-Agent', agent)]\n            if request_headers:\n                opener.addheaders = request_headers\n            if referrer:\n                opener.addheaders = [('Referer', referrer)]\n            if etag:\n                opener.addheaders = [('If-None-Match', etag)]\n            if modified:\n                if isinstance(modified, tuple):\n                    modified = _parse_date(modified)\n                opener.addheaders = [('If-Modified-Since', modified)]\n            try:\n                response = opener.open(url_file_stream_or_string)\n            except urllib.error.HTTPError as e:\n                if e.code == 304:\n                    result['status'] = 304\n                    result['reason'] = e.reason\n                    result['headers'] = e.headers\n                    return None\n                else:\n                    raise\n            except urllib.error.URLError as e:\n                raise\n            result['status'] = response.status\n            result['reason'] = response.reason\n            result['headers'] = response.headers\n            result['url'] = response.url\n            result['encoding'] = response.headers.get_content_charset()\n            return response.read()\n        else:\n            # File\n            try:\n                with open(url_file_stream_or_string, 'rb') as f:\n                    result['encoding'] = f.encoding\n                    return f.read()\n            except IOError as e:\n                raise\n    elif isinstance(url_file_stream_or_string, io.IOBase):\n        # Stream\n        result['encoding']", "idx": 515}
{"namespace": "feedparser.http._build_urllib2_request", "completion": "    # Create a request object with the given URL and headers\n    request = urllib.request.Request(url)\n\n    # Add the user agent header to the request\n    request.add_header('User-Agent', agent)\n\n    # Add the accept header to the request\n    if accept_header:\n        request.add_header('Accept', accept_header)\n\n    # Add the etag header to the request\n    if etag:\n        request.add_header('If-None-Match', etag)\n\n    # Add the modified header to the request\n    if modified:\n        if isinstance(modified, datetime.datetime):\n            modified = modified.strftime('%a, %d %b %Y %H:%M:%S GMT')\n        request.add_header('If-Modified-Since', modified)\n\n    # Add the referrer header to the request\n    if referrer:\n        request.add_header('Referer', referrer)\n\n    # Add the authorization header to the request\n    if auth:\n        request.add_header('Authorization', auth)\n\n    # Add any additional headers to the request\n    for key, value in request_headers.items():\n        request.add_header(key, value)\n\n    return request", "idx": 516}
{"namespace": "pylatex.utils.dumps_list", "completion": "    if mapper is None:\n        mapper = []\n\n    if not _is_iterable(mapper):\n        mapper = [mapper]\n\n    if not _is_iterable(l):\n        l = [l]\n\n    if not isinstance(mapper, list):\n        mapper = [mapper]\n\n    for m in mapper:\n        l = [m(item) for item in l]\n\n    return NoEscape(token.join([_latex_item_to_string(item, escape=escape, as_content=as_content) for item in l]))", "idx": 517}
{"namespace": "pylatex.utils._latex_item_to_string", "completion": "    if isinstance(item, pylatex.base_classes.LatexObject):\n        if as_content:\n            return item.dumps_as_content()\n        return item.dumps()\n\n    if escape:\n        return escape_latex(str(item))\n\n    return str(item)", "idx": 518}
{"namespace": "mistune.markdown.Markdown.read", "completion": "        if state is None:\n            state = self.block.state_cls()\n        with open(filepath, 'rb') as f:\n            content = f.read()\n        content = content.decode(encoding)\n        return self.parse(content, state)", "idx": 519}
{"namespace": "mistune.create_markdown", "completion": "    if renderer == 'html':\n        renderer = HTMLRenderer(escape=escape, hard_wrap=hard_wrap)\n    elif renderer == 'ast':\n        renderer = None\n    else:\n        raise ValueError('Invalid renderer: %r' % renderer)\n\n    if plugins is None:\n        plugins = []\n    elif isinstance(plugins, str):\n        plugins = [plugins]\n\n    plugins = [import_plugin(plugin) for plugin in plugins]\n\n    return Markdown(renderer=renderer, plugins=plugins)", "idx": 520}
{"namespace": "parsel.utils.extract_regex", "completion": "    if isinstance(regex, str):\n        regex = re.compile(regex)\n    if replace_entities:\n        text = w3lib_replace_entities(text)\n    match = regex.search(text)\n    if match is None:\n        return []\n    if \"extract\" in match.groupdict():\n        return [match.group(\"extract\")]\n    return cast(List[str], flatten(match.groups()))", "idx": 521}
{"namespace": "dominate.dom_tag.dom_tag.render", "completion": "    sb = util.StringBuilder()\n    self._render(sb, 0, indent, pretty, xhtml)\n    return sb.to_string()", "idx": 522}
{"namespace": "dominate.util.include", "completion": "  with open(f, 'r') as f:\n    return f.read()\n\n", "idx": 523}
{"namespace": "dominate.util.unescape", "completion": "  # Replace HTML entities with their corresponding characters.\n  data = re.sub('&(%s);' % '|'.join(_unescape.keys()),\n    lambda m: unichr(_unescape[m.group(1)]), data)\n  return data", "idx": 524}
{"namespace": "onlinejudge_command.pretty_printers._tokenize_line", "completion": "    tokens = []\n    l = 0\n    while l < len(line):\n        r = l + 1\n        while r < len(line) and (line[l] in ' \\t') == (line[r] in ' \\t'):\n            r += 1\n        if line[l] in ' \\t':\n            typ = _PrettyTokenType.WHITESPACE\n        else:\n            typ = _PrettyTokenType.BODY\n        tokens.append(_PrettyToken(typ, line[l:r]))\n        l = r\n    return tokens", "idx": 525}
{"namespace": "onlinejudge_command.pretty_printers._render_tokens", "completion": "    result = ''\n    for token in tokens:\n        if token.type == _PrettyTokenType.BODY:\n            result += token.value\n        elif token.type == _PrettyTokenType.BODY_HIGHLIGHT_LEFT:\n            if font_red:\n                result += font_red(token.value)\n            else:\n                result += colorama.Fore.RED + token.value + colorama.Style.RESET_ALL\n        elif token.type == _PrettyTokenType.BODY_HIGHLIGHT_RIGHT:\n            if font_blue:\n                result += font_blue(token.value)\n            else:\n                result += colorama.Fore.BLUE + token.value + colorama.Style.RESET_ALL\n        elif token.type == _PrettyTokenType.WHITESPACE:\n            if font_dim:\n                result += font_dim(token.value)\n            else:\n                result += colorama.Style.DIM + token.value + colorama.Style.RESET_ALL\n        elif token.type == _PrettyTokenType.NEWLINE:\n            if font_normal:\n                result += font_normal(token.value)\n            else:\n                result += colorama.Style.RESET_ALL + token.value\n        elif token.type == _PrettyTokenType.HINT:\n            if font_bold:\n                result += font_bold(token.value)\n            else:\n                result += colorama.Style.BRIGHT + token.value + colorama.Style.RESET_ALL\n        elif token.type == _PrettyTokenType.LINENO:\n            if font_dim:\n                result += font_dim(token.value)\n            else:\n                result += colorama.Style.DIM + token.value + colorama.Style.RESET_ALL\n        elif token.type == _PrettyTokenType.OTHERS:\n            result += token.value\n        else:\n            assert False\n    return result", "idx": 526}
{"namespace": "onlinejudge_command.pretty_printers._tokenize_file_content_without_snipping", "completion": "    tokens = []\n    try:\n        text = content.decode()\n    except UnicodeDecodeError as e:\n        tokens.append(_PrettyToken(_PrettyTokenType.HINT, str(e)))\n        text = content.decode(errors='replace')\n    for line in text.splitlines(keepends=True):\n        tokens += _tokenize_line(line)\n    tokens = _warn_if_empty(tokens)\n    return tokens", "idx": 527}
{"namespace": "jinja2.environment.Environment.get_template", "completion": "        if isinstance(name, Template):\n            return name\n        if parent is not None:\n            name = self.join_path(name, parent)\n        try:\n            return self._load_template(name, globals)\n        except (TemplateNotFound, UndefinedError):\n            raise\n", "idx": 528}
{"namespace": "jinja2.environment.Environment.from_string", "completion": "        if isinstance(source, str):\n            source = self.parse(source)\n        if not isinstance(source, nodes.Template):\n            raise TypeError(\"source must be a string or a Template node\")\n        if globals is None:\n            globals = {}\n        if template_class is None:\n            template_class = self.template_class\n        return template_class(\n            source,\n            self,\n            globals=self.make_globals(globals),\n            name=None,\n            filename=None,\n        )", "idx": 529}
{"namespace": "jinja2.environment.Template.render", "completion": "        if self.environment.is_async:\n            import asyncio\n\n            return asyncio.run(self.render_async(*args, **kwargs))\n\n        ctx = self.new_context(dict(*args, **kwargs))\n\n        try:\n            return self.environment.concat(  # type: ignore\n                [n for n in self.root_render_func(ctx)]  # type: ignore\n            )\n        except Exception:\n            return self.environment.handle_exception()", "idx": 530}
{"namespace": "jinja2.utils.generate_lorem_ipsum", "completion": "    import random\n\n    words = [\n        \"lorem\",\n        \"ipsum\",\n        \"dolor\",\n        \"sit\",\n        \"amet\",\n        \"consectetur\",\n        \"adipiscing\",\n        \"elit\",\n        \"sed\",\n        \"do\",\n        \"eiusmod\",\n        \"tempor\",\n        \"incididunt\",\n        \"ut\",\n        \"labore\",\n        \"et\",\n        \"dolore\",\n        \"magna\",\n        \"aliqua\",\n        \"ut\",\n        \"enim\",\n        \"ad\",\n        \"minim\",\n        \"veniam\",\n        \"quis\",\n        \"nostrud\",\n        \"exercitation\",\n        \"ullamco\",\n        \"laboris\",\n        \"nisi\",\n        \"ut\",\n        \"aliquip\",\n        \"ex\",\n        \"ea\",\n        \"commodo\",\n        \"consequat\",\n        \"duis\",\n        \"aute\",\n        \"irure\",\n        \"dolor\",\n        \"in\",\n        \"reprehenderit\",\n        \"in\",\n        \"voluptate\",\n        \"velit\",\n        \"esse\",\n        \"cillum\",\n        \"dolore\",\n        \"eu\",\n        \"fugiat\",\n        \"nulla\",\n        \"pariatur\",\n        \"excepteur\",\n        \"sint\",\n        \"occaecat\",\n        \"cupidatat\",\n        \"non\",\n        \"proident\",\n        \"sunt\",\n        \"in\",\n        \"culpa\",\n        \"qui\",\n        \"officia\",\n        \"deserunt\",\n        \"mollit\",\n        \"anim\",\n        \"id\",\n        \"est\",\n        \"laborum\",\n    ]\n\n    paragraphs = []\n    for i in range(n):\n        num_words = random.randint(min, max)\n        sentence = []\n        for j in range(num_words):\n            word = random.choice(words)\n           ", "idx": 531}
{"namespace": "jinja2.utils.LRUCache.clear", "completion": "        with self._wlock:\n            self._mapping.clear()\n            self._queue.clear()\n\n    ", "idx": 532}
{"namespace": "jinja2.utils.LRUCache.items", "completion": "        return self._mapping.items()\n\n    ", "idx": 533}
{"namespace": "jinja2.idtracking.symbols_for_node", "completion": "    sym = Symbols(parent=parent_symbols)\n    visitor = FrameSymbolVisitor(sym)\n    visitor.visit(node)\n    return sym", "idx": 534}
{"namespace": "jinja2.idtracking.Symbols.find_ref", "completion": "        if name in self.refs:\n            return self.refs[name]\n\n        if self.parent is not None:\n            return self.parent.find_ref(name)\n\n        return None", "idx": 535}
{"namespace": "jinja2.idtracking.Symbols.dump_stores", "completion": "        rv = {}\n        node: t.Optional[\"Symbols\"] = self\n\n        while node is not None:\n            for target, (instr, _) in self.loads.items():\n                if instr == VAR_LOAD_RESOLVE:\n                    rv[target] = instr\n\n            node = node.parent\n\n        return rv", "idx": 536}
{"namespace": "jinja2.meta.find_undeclared_variables", "completion": "    generator = TrackingCodeGenerator(ast.environment)\n    generator.visit(ast)\n    return generator.undeclared_identifiers", "idx": 537}
{"namespace": "jinja2.loaders.split_template_path", "completion": "    if template.startswith(\"./\"):\n        template = template[2:]\n    if template.startswith(\"../\"):\n        raise TemplateNotFound(template)\n    if \"..\" in template:\n        raise TemplateNotFound(template)\n    return template.split(\"/\")", "idx": 538}
{"namespace": "jinja2.bccache.MemcachedBytecodeCache.load_bytecode", "completion": "        key = self.prefix + bucket.key\n\n        try:\n            value = self.client.get(key)\n        except Exception:\n            if not self.ignore_memcache_errors:\n                raise\n            return\n\n        if value is not None:\n            bucket.bytecode_from_string(value)", "idx": 539}
{"namespace": "jinja2.bccache.MemcachedBytecodeCache.dump_bytecode", "completion": "        try:\n            self.client.set(\n                self.prefix + bucket.key,\n                bucket.bytecode_to_string(),\n                self.timeout,\n            )\n        except Exception:\n            if not self.ignore_memcache_errors:\n                raise", "idx": 540}
{"namespace": "sumy.utils.get_stop_words", "completion": "    language = normalize_language(language)\n    filename = expand_resource_path(\"stopwords/%s.txt\" % language)\n\n    if not exists(filename):\n        raise LookupError(\"Stop words are not available for language '%s'.\" % language)\n\n    return read_stop_words(filename)", "idx": 541}
{"namespace": "sumy._compat.to_bytes", "completion": "    if isinstance(object, bytes):\n        return object\n    elif isinstance(object, unicode):\n        return object.encode(\"utf-8\")\n    else:\n        # try encode instance to bytes\n        return instance_to_bytes(object)", "idx": 542}
{"namespace": "sumy._compat.to_unicode", "completion": "    if isinstance(object, unicode):\n        return object\n    elif isinstance(object, bytes):\n        return object.decode(\"utf-8\")\n    else:\n        # try encode instance to unicode\n        return instance_to_unicode(object)", "idx": 543}
{"namespace": "sumy.summarizers.lsa.LsaSummarizer._create_dictionary", "completion": "        dictionary = {}\n        for word, row in zip(document.words, range(len(document.words))):\n            word = self.normalize_word(word)\n            if word not in self.stop_words:\n                dictionary[word] = row\n\n        return dictionary", "idx": 544}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._get_content_words_in_sentence", "completion": "        words = sentence.words\n        normalized_words = self._normalize_words(words)\n        filtered_words = self._filter_out_stop_words(normalized_words)\n        stemmed_words = self._stem_words(filtered_words)\n        return stemmed_words", "idx": 545}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._get_all_content_words_in_doc", "completion": "        all_words = self._get_all_words_in_doc(sentences)\n        normalized_words = self._normalize_words(all_words)\n        normalized_content_words = self._filter_out_stop_words(normalized_words)\n        return normalized_content_words", "idx": 546}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_tf", "completion": "        content_words = self._get_all_content_words_in_doc(sentences)\n        word_freq = self._compute_word_freq(content_words)\n        total_content_words_count = len(content_words)\n        for w in word_freq:\n            word_freq[w] /= total_content_words_count\n        return word_freq", "idx": 547}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_ratings", "completion": "        sentences_as_words = [self._get_content_words_in_sentence(s) for s in sentences]\n        word_freq = self._compute_tf(sentences)\n        ratings = {}\n        for i in range(len(sentences)):\n            best_sentence_index = self._find_index_of_best_sentence(word_freq, sentences_as_words)\n            ratings[sentences[best_sentence_index]] = i * -1\n            word_freq = self._update_tf(word_freq, sentences_as_words[best_sentence_index])\n            del sentences[best_sentence_index]\n            del sentences_as_words[best_sentence_index]\n        return ratings", "idx": 548}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.cue_method", "completion": "        summarization_method = self._build_cue_method_instance()\n        return summarization_method(document, sentences_count, bonus_word_value, stigma_word_value)\n\n    ", "idx": 549}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.key_method", "completion": "        summarization_method = self._build_key_method_instance()\n        return summarization_method(document, sentences_count, weight)", "idx": 550}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.title_method", "completion": "", "idx": 551}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.location_method", "completion": "        summarization_method = self._build_location_method_instance()\n        return summarization_method(document, sentences_count, w_h, w_p1, w_p2, w_s1, w_s2)", "idx": 552}
{"namespace": "sumy.summarizers.reduction.ReductionSummarizer.rate_sentences", "completion": "        ratings = defaultdict(float)\n        for s1, s2 in combinations(document.sentences, 2):\n            words1 = self._to_words_set(s1)\n            words2 = self._to_words_set(s2)\n            if len(words1) == 0 or len(words2) == 0:\n                continue\n\n            rating = self._rate_sentences_edge(words1, words2)\n            ratings[s1] += rating\n            ratings[s2] += rating\n\n        return ratings", "idx": 553}
{"namespace": "sumy.summarizers.reduction.ReductionSummarizer._to_words_set", "completion": "        words = self.normalize_words(sentence.words)\n        return [w for w in words if w not in self.stop_words]", "idx": 554}
{"namespace": "sumy.summarizers.text_rank.TextRankSummarizer._to_words_set", "completion": "        return set(self.normalize_word(w) for w in sentence.words if w not in self.stop_words)", "idx": 555}
{"namespace": "sumy.summarizers.kl.KLSummarizer.compute_tf", "completion": "        all_content_words = self._get_all_content_words_in_doc(sentences)\n        word_freq = self._compute_word_freq(all_content_words)\n\n        # normalize the term frequency\n        for w in word_freq:\n            word_freq[w] /= float(len(all_content_words))\n\n        return word_freq\n\n    ", "idx": 556}
{"namespace": "sumy.evaluation.rouge._get_word_ngrams", "completion": "    assert (n > 0)\n    assert (len(sentences) > 0)\n    assert (all(isinstance(s, Sentence) for s in sentences))\n\n    word_ngrams = set()\n    for s in sentences:\n        new_word_ngrams = _get_ngrams(n, s.words)\n        word_ngrams = word_ngrams.union(new_word_ngrams)\n    return word_ngrams", "idx": 557}
{"namespace": "sumy.evaluation.rouge._len_lcs", "completion": "    table = _lcs(x, y)\n    n, m = _get_index_of_lcs(x, y)\n    return table[n, m]", "idx": 558}
{"namespace": "sumy.evaluation.rouge._recon_lcs", "completion": "    i, j = _get_index_of_lcs(x, y)\n    table = _lcs(x, y)\n    recon = []\n    while i * j != 0:\n        if table[i, j] == table[i - 1, j]:\n            i -= 1\n        elif table[i, j] == table[i, j - 1]:\n            j -= 1\n        else:\n            recon.append(x[i - 1])\n            i -= 1\n            j -= 1\n    return list(reversed(recon))", "idx": 559}
{"namespace": "sumy.evaluation.rouge._union_lcs", "completion": "    evaluated_words = _split_into_words(evaluated_sentences)\n    reference_words = _split_into_words([reference_sentence])\n    evaluated_lcs = _recon_lcs(evaluated_words, reference_words)\n    reference_lcs = _recon_lcs(reference_words, evaluated_words)\n    union_lcs = _recon_lcs(evaluated_lcs, reference_lcs)\n    return len(union_lcs) / len(reference_words)", "idx": 560}
{"namespace": "sumy.parsers.html.HtmlParser.from_file", "completion": "        with open(file_path, \"rb\") as f:\n            data = f.read()\n        return cls(data, tokenizer, url)\n\n    ", "idx": 561}
{"namespace": "sumy.parsers.plaintext.PlaintextParser.document", "completion": "        lines = self._to_lines(self._text)\n        paragraphs = self._to_paragraphs(lines)\n        return ObjectDocumentModel(paragraphs)\n", "idx": 562}
{"namespace": "sumy.nlp.tokenizers.Tokenizer.to_sentences", "completion": "        self._sentence_tokenizer.abbrev_types.update(self.LANGUAGE_EXTRA_ABREVS.get(self._language, []))\n        return tuple(self._sentence_tokenizer.tokenize(to_unicode(paragraph)))", "idx": 563}
{"namespace": "sumy.nlp.stemmers.null_stemmer", "completion": "    return str(object).lower()", "idx": 564}
{"namespace": "rows.fields.BinaryField.serialize", "completion": "        if value is not None:\n            if isinstance(value, six.binary_type):\n                return b64encode(value).decode(\"ascii\")\n            elif isinstance(value, six.text_type):\n                try:\n                    return b64encode(value.encode(\"ascii\")).decode(\"ascii\")\n                except UnicodeEncodeError:\n                    raise ValueError(\"Can't encode base64\")\n            else:\n                value_error(value, cls)\n        else:\n            return \"\"", "idx": 565}
{"namespace": "rows.fields.BoolField.deserialize", "completion": "        value = super(BoolField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value).lower()\n        if value in cls.TRUE_VALUES:\n            return True\n        elif value in cls.FALSE_VALUES:\n            return False\n        else:\n            raise ValueError(\"Value is not boolean\")", "idx": 566}
{"namespace": "rows.fields.DateField.serialize", "completion": "        if value is None:\n            return \"\"\n\n        return value.strftime(cls.OUTPUT_FORMAT)", "idx": 567}
{"namespace": "rows.fields.DateField.deserialize", "completion": "        value = super(DateField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n        try:\n            return datetime.datetime.strptime(value, cls.INPUT_FORMAT).date()\n        except ValueError:\n            value_error(value, cls)", "idx": 568}
{"namespace": "rows.fields.TextField.deserialize", "completion": "        value = super(TextField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            return six.text_type(value)", "idx": 569}
{"namespace": "rows.fields.EmailField.deserialize", "completion": "        value = super(EmailField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n        if cls.EMAIL_REGEXP.match(value):\n            return value\n        else:\n            raise ValueError(\"Value is not an e-mail\")", "idx": 570}
{"namespace": "rows.fields.JSONField.deserialize", "completion": "        value = super(JSONField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        try:\n            return json.loads(value)\n        except ValueError:\n            value_error(value, cls)\n\n", "idx": 571}
{"namespace": "falcon.http_error.HTTPError.to_json", "completion": "        if handler is None:\n            handler = _DEFAULT_JSON_HANDLER\n\n        return handler.dumps(self.to_dict())", "idx": 572}
{"namespace": "falcon.inspect.inspect_app", "completion": "    routes = inspect_routes(app)\n    middleware = inspect_middleware(app)\n    static_routes = inspect_static_routes(app)\n    sinks = inspect_sinks(app)\n    error_handlers = inspect_error_handlers(app)\n    asgi = isinstance(app, App)\n    return AppInfo(routes, middleware, static_routes, sinks, error_handlers, asgi)", "idx": 573}
{"namespace": "falcon.inspect.inspect_routes", "completion": "    router = app._router\n    if router is None:\n        return []\n    if router.__class__ in _supported_routers:\n        return _supported_routers[router.__class__](router)\n    raise ValueError(\n        'No inspection function registered for router {}'.format(router.__class__)\n    )", "idx": 574}
{"namespace": "falcon.inspect.inspect_static_routes", "completion": "    static_routes = []\n    for prefix, directory, fallback_filename in app._static_routes:\n        source_info, class_name = _get_source_info_and_name(directory)\n        info = StaticRouteInfo(prefix.pattern, class_name, fallback_filename)\n        static_routes.append(info)\n    return static_routes", "idx": 575}
{"namespace": "falcon.inspect.inspect_sinks", "completion": "    sinks = []\n    for prefix, sink in app._sinks.items():\n        source_info, name = _get_source_info_and_name(sink)\n        info = SinkInfo(prefix, name, source_info)\n        sinks.append(info)\n    return sinks", "idx": 576}
{"namespace": "falcon.inspect.inspect_error_handlers", "completion": "    error_handlers = []\n    for error, handler, _ in app._error_handlers:\n        source_info, name = _get_source_info_and_name(handler)\n        internal = _is_internal(handler)\n        info = ErrorHandlerInfo(error, name, source_info, internal)\n        error_handlers.append(info)\n    return error_handlers", "idx": 577}
{"namespace": "falcon.inspect.inspect_middleware", "completion": "    middleware_tree = MiddlewareTreeInfo(\n        request=[], resource=[], response=[]\n    )\n    middleware_classes = []\n    independent = False\n\n    for m in app._middleware:\n        if isinstance(m, type):\n            methods = []\n            for method in m.__dict__:\n                if method.startswith('on_'):\n                    source_info = _get_source_info(m.__dict__[method])\n                    method_info = MiddlewareMethodInfo(\n                        method, source_info\n                    )\n                    methods.append(method_info)\n            source_info, class_name = _get_source_info_and_name(m)\n            middleware_class = MiddlewareClassInfo(\n                class_name, source_info, methods\n            )\n            middleware_classes.append(middleware_class)\n        else:\n            if m.process_request:\n                middleware_tree.request.append(\n                    MiddlewareTreeItemInfo(\n                        'process_request', m.__class__.__name__\n                    )\n                )\n            if m.process_resource:\n                middleware_tree.resource.append(\n                    MiddlewareTreeItemInfo(\n                        'process_resource', m.__class__.__name__\n                    )\n                )\n            if m.process_response:\n                middleware_tree.response.append(\n                    MiddlewareTreeItemInfo(\n                        'process_response', m.__class__.__name__\n                    )\n                )\n            if not (\n                m.process_request\n                or m.process_resource\n                or m.process_response\n            ):\n                independent = True\n\n    middleware_info = MiddlewareInfo(\n        middleware_tree, middleware_classes, independent\n    )\n    return middleware_info", "idx": 578}
{"namespace": "falcon.inspect.InspectVisitor.process", "completion": "        visit_name = instance.__visit_name__\n        visit_method = 'visit_{}'.format(visit_name)\n        try:\n            visit_method = getattr(self, visit_method)\n        except AttributeError:\n            raise RuntimeError(\n                'No visit method found for {}'.format(visit_name)\n            ) from None\n        return visit_method(instance)", "idx": 579}
{"namespace": "falcon.request.Request.forwarded", "completion": "        if self._cached_forwarded is None:\n            header_value = self.env.get('HTTP_FORWARDED')\n            if header_value:\n                self._cached_forwarded = helpers._parse_forwarded(header_value)\n\n        return self._cached_forwarded\n", "idx": 580}
{"namespace": "falcon.request.Request.client_accepts_msgpack", "completion": "        return self.client_accepts('application/x-msgpack')\n", "idx": 581}
{"namespace": "falcon.request.Request.content_length", "completion": "        try:\n            value = self.env['CONTENT_LENGTH']\n        except KeyError:\n            return None\n\n        try:\n            value = int(value)\n        except ValueError:\n            msg = 'The value must be a positive integer.'\n            raise errors.HTTPInvalidHeader(msg, 'Content-Length')\n\n        if value < 0:\n            msg = 'The value must be a positive integer.'\n            raise errors.HTTPInvalidHeader(msg, 'Content-Length')\n\n        return value\n", "idx": 582}
{"namespace": "falcon.request.Request.bounded_stream", "completion": "        if self._bounded_stream is None:\n            self._bounded_stream = helpers.BoundedStream(\n                self.stream, self.content_length\n            )\n\n        return self._bounded_stream\n", "idx": 583}
{"namespace": "falcon.request.Request.uri", "completion": "        if self._cached_uri is None:\n            self._cached_uri = self.scheme + '://' + self.netloc + self.relative_uri\n\n        return self._cached_uri\n", "idx": 584}
{"namespace": "falcon.request.Request.forwarded_uri", "completion": "        if self._cached_forwarded_uri is None:\n            if self.forwarded:\n                # Use first hop, fall back on self\n                scheme = self.forwarded[0].scheme or self.scheme\n                host = self.forwarded[0].host or self.forwarded_host\n                self._cached_forwarded_uri = scheme + '://' + host + self.relative_uri\n            else:\n                self._cached_forwarded_uri = self.uri\n\n        return self._cached_forwarded_uri\n", "idx": 585}
{"namespace": "falcon.request.Request.relative_uri", "completion": "        if self._cached_relative_uri is None:\n            # PERF: For small numbers of items, '+' is faster\n            # than ''.join(...). Concatenation is also generally\n            # faster than formatting.\n            value = self.app + self.path\n\n            if self.query_string:\n                value += '?' + self.query_string\n\n            self._cached_relative_uri = value\n\n        return self._cached_relative_uri\n", "idx": 586}
{"namespace": "falcon.request.Request.prefix", "completion": "        if self._cached_prefix is None:\n            self._cached_prefix = self.scheme + '://' + self.netloc + self.app\n\n        return self._cached_prefix\n", "idx": 587}
{"namespace": "falcon.request.Request.forwarded_prefix", "completion": "        if self._cached_forwarded_prefix is None:\n            self._cached_forwarded_prefix = self.forwarded_scheme + '://' + self.forwarded_host + self.app\n\n        return self._cached_forwarded_prefix\n", "idx": 588}
{"namespace": "falcon.request.Request.host", "completion": "        try:\n            host_header = self.env['HTTP_HOST']\n        except KeyError:\n            host_header = self.env['SERVER_NAME']\n\n            port = self.env['SERVER_PORT']\n            if self.scheme == 'https':\n                if port != '443':\n                    host_header += ':' + port\n            else:\n                if port != '80':\n                    host_header += ':' + port\n\n        return host_header\n", "idx": 589}
{"namespace": "falcon.request.Request.subdomain", "completion": "        # PERF(kgriffs): try..except is faster than get() assuming that\n        # we normally expect the key to exist. Even though PEP-3333\n        # allows WSGI servers to omit the key when the value is an\n        # empty string, uwsgi, gunicorn, waitress, and wsgiref all\n        # include it even in that case.\n        try:\n            host_header = self.env['HTTP_HOST']\n        except KeyError:\n            return None\n\n        # PERF(kgriffs): .partition is slightly faster than .split\n        host, sep, remainder = host_header.partition(':')\n\n        if sep:\n            # NOTE(kgriffs): The port is optional, so we need to\n            # check for the presence of a separator.\n            return host\n        else:\n            return None\n", "idx": 590}
{"namespace": "falcon.request.Request.headers", "completion": "        if self._cached_headers is None:\n            self._cached_headers = {}\n            for key, value in self.env.items():\n                if key.startswith('HTTP_'):\n                    name = key[5:].replace('_', '-')\n                    self._cached_headers[name] = value\n\n        return self._cached_headers.copy()\n", "idx": 591}
{"namespace": "falcon.request.Request.remote_addr", "completion": "        try:\n            return self.env['REMOTE_ADDR']\n        except KeyError:\n            return '127.0.0.1'\n", "idx": 592}
{"namespace": "falcon.request.Request.client_accepts", "completion": "        try:\n            # NOTE(kgriffs): best_match will return '' if no match is found\n            return mimeparse.best_match([media_type], self.accept) == media_type\n        except ValueError:\n            # Value for the accept header was not formatted correctly\n            return False\n", "idx": 593}
{"namespace": "falcon.request.Request.client_prefers", "completion": "        accept = self.accept\n\n        # PERF(kgriffs): Usually the following will be true, so\n        # try it first.\n        if (accept == media_types) or (accept == '*/*'):\n            return media_types[0]\n\n        # Fall back to full-blown parsing\n        try:\n            return mimeparse.best_match(media_types, accept)\n        except ValueError:\n            return None\n", "idx": 594}
{"namespace": "falcon.request.Request.get_header", "completion": "        # NOTE(kgriffs): According to PEP-3333, we should first\n        # try to use the header name as-is, then fall back to\n        # uppercasing it and replacing hyphens with underscores.\n        #\n        # PERF(kgriffs): try..except is faster than get() when we\n        # expect the key to be present most of the time.\n        try:\n            value = self.env[name]\n        except KeyError:\n            name = name.upper().replace('-', '_')\n            try:\n                value = self.env[name]\n            except KeyError:\n                if required:\n                    raise errors.HTTPBadRequest(\n                        'The header is missing: %s' % name,\n                        href='https://tools.ietf.org/html/rfc7230#section-3.2.2',\n                        href_text='RFC 7230, Section 3.2.2',\n                    )\n                else:\n                    return default\n\n        return value\n", "idx": 595}
{"namespace": "falcon.request.Request.get_cookie_values", "completion": "        if self._cookies is None:\n            header_value = self.get_header('Cookie')\n            if header_value:\n                self._cookies = helpers.parse_cookie_header(header_value)\n            else:\n                self._cookies = {}\n\n        if name in self._cookies:\n            return self._cookies[name]\n        else:\n            return None", "idx": 596}
{"namespace": "falcon.response.Response.unset_cookie", "completion": "        # NOTE(kgriffs): We don't need to do anything special here.\n        #   The SimpleCookie object will handle the unsetting of\n        #   the cookie for us.\n        if self._cookies is None:\n            self._cookies = http_cookies.SimpleCookie()\n\n        if name in self._cookies:\n            self._cookies[name]['expires'] = 'Thu, 01 Jan 1970 00:00:00 GMT'\n            self._cookies[name]['max-age'] = 0\n\n            if domain:\n                self._cookies[name]['domain'] = domain\n\n            if path:\n                self._cookies[name]['path'] = path\n\n        # PERF(kgriffs): Morsel.__setitem__() will lowercase this anyway,\n        #   so we can just pass this in and when __setitem__() calls\n        #   lower() it will be very slightly faster.\n        if same_site:\n            same_site = same_site.lower()\n\n            if same_site not in _RESERVED_SAMESITE_VALUES:\n                raise ValueError(\n                    \"same_site must be set to either 'lax', 'strict', or 'none'\"\n                )\n\n            self._cookies[name]['samesite'] = same_site.capitalize()", "idx": 597}
{"namespace": "falcon.response.Response.get_header", "completion": "        # NOTE(kgriffs): normalize name by lowercasing it\n        name = name.lower()\n\n        if name == 'set-cookie':\n            raise HeaderNotSupported('This method cannot be used to get cookies')\n\n        if name in self._headers:\n            return self._headers[name]\n        else:\n            return default", "idx": 598}
{"namespace": "falcon.response.Response.set_header", "completion": "        # PERF(kgriffs): Heuristic to detect possiblity of an extension\n        # relation type, in which case it will be a URL that may contain\n        # reserved characters. Otherwise, don't waste time running the\n        # string through uri.encode\n        #\n        # Example values for rel:\n        #\n        #     \"next\"\n        #     \"http://example.com/ext-type\"\n        #     \"https://example.com/ext-type\"\n        #     \"alternate http://example.com/ext-type\"\n        #     \"http://example.com/ext-type alternate\"\n        #\n        if '//' in name:\n            name = '\"' + ' '.join([uri_encode(n) for n in name.split()]) + '\"'\n\n        # PERF(kgriffs): uwsgi fails with a TypeError if any header\n        # is not a str, so do the conversion here. It's actually\n        # faster to not do an isinstance check. str() will encode\n        # to US-ASCII.\n        value = str(value)\n\n        # NOTE(kgriffs): normalize name by lowercasing it\n        name = name.lower()\n\n        if name == 'set-cookie':\n            if not self._extra_headers:\n                self._extra_headers = [(name, value)]\n            else:\n                self._extra_headers.append((name, value))\n        else:\n            self._headers[name] = value", "idx": 599}
{"namespace": "falcon.response.Response.delete_header", "completion": "        # NOTE(kgriffs): normalize name by lowercasing it\n        name = name.lower()\n\n        if name == 'set-cookie':\n            raise HeaderNotSupported('This method cannot be used to delete cookies')\n\n        if name in self._headers:\n            del self._headers[name]", "idx": 600}
{"namespace": "falcon.cmd.inspect_app.route_main", "completion": "    print(\n        'The \"falcon-print-routes\" command is deprecated. '\n        'Please use \"falcon-inspect-app\"'\n    )\n    main()", "idx": 601}
{"namespace": "falcon.util.uri.decode", "completion": "    # PERF(kgriffs): This was found to be faster than using a regex, for\n    # both short and long query strings. Tested on CPython 3.4.\n    tokens = encoded_uri.split('%')\n\n    if not unquote_plus:\n        # PERF(kgriffs): This was found to be faster than using a regex, for\n        # both short and long query strings. Tested on CPython 3.4.\n        for token in tokens[1:]:\n            if token[0] == '2' and token[1] == 'B':\n                # PERF(kgriffs): This was found to be faster than using a regex, for\n                # both short and long query strings. Tested on CPython 3.4.\n                if token[2] == '+':\n                    tokens[tokens.index(token)] = ' '\n\n    return _join_tokens(tokens)", "idx": 602}
{"namespace": "falcon.util.structures.ETag.dumps", "completion": "        if self.is_weak:\n            return 'W/' + str(self)\n        else:\n            return str(self)", "idx": 603}
{"namespace": "falcon.util.structures.ETag.loads", "completion": "        # TODO(kgriffs): If we ever diverge from what is upstream in Requests,\n        # then we will need write tests and remove the \"no cover\" pragma.\n        #\n        # Copied from the Requests library by Kenneth Reitz et al.\n        #\n        # Copyright 2013 Kenneth Reitz\n        #\n        #    Licensed under the Apache License, Version 2.0 (the \"License\");\n        #    you may not use this file except in compliance with the License.\n        #    You may obtain a copy of the License at\n        #\n        #        http://www.apache.org/licenses/LICENSE-2.0\n        #\n        #    Unless required by applicable law or agreed to in writing, software\n        #    distributed under the License is distributed on an \"AS IS\" BASIS,\n        #    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n        #    See the License for the specific language governing permissions and\n        #    limitations under the License.\n\n        # TODO(kgriffs): If we ever diverge from what is upstream in Requests,\n        # then we will need write tests and remove the \"no cover\" pragma.\n        #\n        # Copied from the Requests library by Kenneth Reitz et al.\n        #\n        # Copyright 2013 Kenneth Reitz\n        #\n        #    Licensed under the Apache License, Version 2.0 (the \"License\");\n        #    you may not use this file except in compliance with the License.\n        #    You may obtain a copy of the License at\n        #\n        #        http://www.apache.org/licenses/LICENSE-2.0\n        #\n        #    Unless required by applicable law or agreed to in writing, software\n        #    distributed under the License is distributed on an \"AS IS\" BASIS,\n        #    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n        #    See the License for the specific language governing permissions and\n        #    limitations under the License.\n\n        # TODO(kgriffs): If we ever diver", "idx": 604}
{"namespace": "falcon.util.misc.secure_filename", "completion": "    # Normalize the filename to the Unicode NKFD form.\n    filename = unicodedata.normalize('NFKD', filename)\n\n    # Replace any non-ASCII characters with an underscore.\n    filename = _UNSAFE_CHARS.sub('_', filename)\n\n    # If the filename starts with a period, replace the first period with an underscore.\n    if filename.startswith('.'):\n        filename = '_' + filename[1:]\n\n    return filename", "idx": 605}
{"namespace": "falcon.asgi.reader.BufferedReader.peek", "completion": "        if size == -1 or size is None:\n            return self._buffer[self._buffer_pos :]\n\n        if size <= 0:\n            return b''\n\n        if size <= self._max_join_size:\n            if self._buffer_len > self._buffer_pos:\n                if size <= self._buffer_len - self._buffer_pos:\n                    return self._buffer[self._buffer_pos : self._buffer_pos + size]\n\n                result = self._buffer[self._buffer_pos:]\n                self._buffer_pos = self._buffer_len\n                return result\n\n            # PERF(vytas): Do not wrap unless needed.\n            async for chunk in self._source:\n                chunk_len = len(chunk)\n                if size <= chunk_len:\n                    self._prepend_buffer(chunk[chunk_len - size :])\n                    return chunk[:size]\n\n                result = chunk\n                size -= chunk_len\n                if size == 0:  # pragma: no py39,py310 cover\n                    return result\n\n            return b''\n\n        # NOTE(vytas): size > self._max_join_size\n        if self._buffer_len > self._buffer_pos:\n            result = self._buffer[self._buffer_pos :]\n            self._buffer_pos = self._buffer_len\n            return result\n\n        # PERF(vytas): Do not wrap unless needed.\n        result = io.BytesIO()\n        async for chunk in self._source:\n            chunk_len = len(chunk)\n            if size <= chunk_len:\n                self._prepend_buffer(chunk[chunk_len - size :])\n                result.write(chunk[:size])\n                return result.getvalue()\n\n            result.write(chunk)\n            size -= chunk_len\n            if size == 0:  # pragma: no py39,py310 cover\n                return result.getvalue()\n\n        return b''", "idx": 606}
{"namespace": "falcon.asgi.reader.BufferedReader.read_until", "completion": "        return await self._read_from(\n            self._iter_delimited(delimiter, size_hint=size or 0), size\n        )\n\n    ", "idx": 607}
{"namespace": "falcon.routing.converters.IntConverter.convert", "completion": "        if self._num_digits is not None and len(value) != self._num_digits:\n            return None\n\n        if not value.isdigit():\n            return None\n\n        try:\n            value = int(value)\n        except ValueError:\n            return None\n\n        if self._min is not None and value < self._min:\n            return None\n\n        if self._max is not None and value > self._max:\n            return None\n\n        return value", "idx": 608}
{"namespace": "falcon.routing.converters.DateTimeConverter.convert", "completion": "        try:\n            return strptime(value, self._format_string)\n        except ValueError:\n            return None", "idx": 609}
{"namespace": "falcon.routing.util.map_http_methods", "completion": "    method_map = {}\n\n    for method in constants.COMBINED_METHODS:\n        if suffix is not None:\n            method_name = 'on_' + method + '_' + suffix\n        else:\n            method_name = 'on_' + method\n\n        if hasattr(resource, method_name):\n            method_map[method] = getattr(resource, method_name)\n\n    return method_map", "idx": 610}
{"namespace": "falcon.routing.static._BoundedFile.read", "completion": "        if size < 0:\n            size = self.remaining\n        else:\n            size = min(size, self.remaining)\n        self.remaining -= size\n        return self.fh.read(size)\n", "idx": 611}
{"namespace": "authlib.oauth2.rfc6749.util.list_to_scope", "completion": "    if isinstance(scope, (tuple, list, set)):\n        return ' '.join([to_unicode(s) for s in scope])\n    elif scope is None:\n        return None\n    return to_unicode(scope)", "idx": 612}
{"namespace": "authlib.oauth2.rfc6749.util.extract_basic_authorization", "completion": "    auth_header = headers.get(\"Authorization\")\n    if not auth_header:\n        return None, None\n\n    auth_header = auth_header.strip()\n    if not auth_header.startswith(\"Basic \"):\n        return None, None\n\n    auth_token = auth_header[6:]\n    if \":\" not in auth_token:\n        return auth_token, None\n\n    try:\n        auth_token = base64.b64decode(auth_token)\n    except binascii.Error:\n        return None, None\n\n    username, password = auth_token.decode(\"utf-8\").split(\":\", 1)\n    return username, password", "idx": 613}
{"namespace": "authlib.oauth2.rfc6749.parameters.prepare_grant_uri", "completion": "    params = [('client_id', client_id), ('response_type', response_type)]\n\n    if redirect_uri:\n        params.append(('redirect_uri', redirect_uri))\n\n    if scope:\n        params.append(('scope', list_to_scope(scope)))\n\n    if state:\n        params.append(('state', state))\n\n    for k in kwargs:\n        if kwargs[k]:\n            params.append((to_unicode(k), kwargs[k]))\n\n    return add_params_to_qs(uri, params)", "idx": 614}
{"namespace": "authlib.oauth2.rfc6749.parameters.parse_authorization_code_response", "completion": "    from .errors import MissingCodeException\n    from .errors import MismatchingStateException\n    from .errors import MissingTokenException\n    from .errors import MissingTokenTypeException\n    from .errors import MissingExpiresInException\n    from .errors import MissingScopeException\n\n    params = dict(urlparse.parse_qsl(uri, keep_blank_values=True))\n\n    if 'code' not in params:\n        raise MissingCodeException()\n\n    if state and params.get('state', None) != state:\n        raise MismatchingStateException()\n\n    if 'access_token' not in params:\n        raise MissingTokenException()\n\n    if 'token_type' not in params:\n        raise MissingTokenTypeException()\n\n    if 'expires_in' not in params:\n        raise MissingExpiresInException()\n\n    if 'scope' not in params:\n        raise MissingScopeException()\n\n    return params", "idx": 615}
{"namespace": "authlib.oauth2.rfc6749.parameters.parse_implicit_response", "completion": "    query = urlparse.urlparse(uri).fragment\n    params = dict(urlparse.parse_qsl(query))\n\n    if 'access_token' not in params:\n        raise MissingCodeException()\n\n    params_state = params.get('state')\n    if state and params_state != state:\n        raise MismatchingStateException()\n\n    return params", "idx": 616}
{"namespace": "authlib.common.encoding.json_b64encode", "completion": "    if isinstance(text, dict):\n        text = json_dumps(text)\n\n    return urlsafe_b64encode(to_bytes(text, charset='utf-8'))", "idx": 617}
{"namespace": "authlib.jose.util.extract_header", "completion": "    try:\n        header_segment = extract_segment(header_segment, error_cls, 'header')\n    except DecodeError:\n        raise error_cls('Invalid header')\n\n    header = ensure_dict(header_segment, 'header')\n    return header", "idx": 618}
{"namespace": "twitter.models.TwitterModel.AsDict", "completion": "        result = {}\n        for attr, value in self.__dict__.items():\n            if attr.startswith('_'):\n                continue\n            if isinstance(value, list):\n                result[attr] = [v.AsDict() if hasattr(v, 'AsDict') else v for v in value]\n            elif isinstance(value, tuple):\n                result[attr] = [v.AsDict() if hasattr(v, 'AsDict') else v for v in value]\n            elif isinstance(value, set):\n                result[attr] = [v.AsDict() if hasattr(v, 'AsDict') else v for v in value]\n            elif hasattr(value, 'AsDict'):\n                result[attr] = value.AsDict()\n            else:\n                result[attr] = value\n        return result\n", "idx": 619}
{"namespace": "twitter.models.TwitterModel.NewFromJsonDict", "completion": "        return cls(**kwargs)\n\n", "idx": 620}
{"namespace": "twitter.api.Api._TweetTextWrap", "completion": "        if char_lim < 280:\n            raise TwitterError({'message': \"Character limit must be at least 280\"})\n\n        words = status.split()\n        line = ''\n        tweets = []\n\n        for word in words:\n            if len(line) + len(word) + 1 > char_lim:\n                tweets.append(line)\n                line = ''\n            if line:\n                line += ' '\n            line += word\n\n        if line:\n            tweets.append(line)\n\n        return tweets", "idx": 621}
{"namespace": "databases.importer.import_from_string", "completion": "    module_str, _, attrs_str = import_str.partition(\":\")\n    try:\n        module = importlib.import_module(module_str)\n        return getattr(module, attrs_str)\n    except (ImportError, AttributeError) as e:\n        raise ImportFromStringError(\n            f\"Error importing {import_str}. {e.__class__.__name__}: {e}\"\n        ) from e", "idx": 622}
{"namespace": "rest_framework.reverse.reverse", "completion": "    if request is not None:\n        url = _reverse(viewname, args=args, kwargs=kwargs, request=request, format=format, **extra)\n        return preserve_builtin_query_params(url, request)\n    else:\n        try:\n            return _reverse(viewname, args=args, kwargs=kwargs, format=format, **extra)\n        except NoReverseMatch:\n            return None", "idx": 623}
{"namespace": "rest_framework.serializers.Serializer.fields", "completion": "        if not hasattr(self, '_fields'):\n            self._fields = self.get_fields()\n        return self._fields\n", "idx": 624}
{"namespace": "rest_framework.parsers.JSONParser.parse", "completion": "        parser_context = parser_context or {}\n        encoding = parser_context.get('encoding', settings.DEFAULT_CHARSET)\n        try:\n            data = stream.read().decode(encoding)\n            return self.renderer_class().parse(data, media_type, parser_context)\n        except UnicodeDecodeError as exc:\n            msg = \"JSON parse error - %s: %s\" % (exc.obj, exc.reason)\n            raise ParseError(msg)", "idx": 625}
{"namespace": "rest_framework.parsers.FileUploadParser.get_filename", "completion": "        parser_context = parser_context or {}\n        request = parser_context['request']\n        encoding = parser_context.get('encoding', settings.DEFAULT_CHARSET)\n        meta = request.META\n        upload_handlers = request.upload_handlers\n        filename = self.get_filename(stream, media_type, parser_context)\n\n        if not filename:\n            raise ParseError(self.errors['no_filename'])\n\n        # Note that this code is extracted from Django's handling of\n        # file uploads in MultiPartParser.\n        content_type = meta.get('HTTP_CONTENT_TYPE',\n                                meta.get('CONTENT_TYPE', ''))\n        try:\n            content_length = int(meta.get('HTTP_CONTENT_LENGTH',\n                                          meta.get('CONTENT_LENGTH', 0)))\n        except (ValueError, TypeError):\n            content_length = None\n\n        # See if the handler will want to take care of the parsing.\n        for handler in upload_handlers:\n            result = handler.handle_raw_input(stream,\n                                              meta,\n                                              content_length,\n                                              None,\n                                              encoding)\n            if result is not None:\n                return DataAndFiles({}, {'file': result[1]})\n\n        # This is the standard case.\n        possible_sizes = [x.chunk_size for x in upload_handlers if x.chunk_size]\n        chunk_size = min([2 ** 31 - 4] + possible_sizes)\n        chunks = ChunkIter(stream, chunk_size)\n        counters = [0] * len(upload_handlers)\n\n        for index, handler in enumerate(upload_handlers):\n            try:\n                handler.new_file(None, filename, content_type,\n                                 content_length, encoding)\n            except StopFutureHandlers:\n                upload_handlers = upload_handlers[:index + 1]\n                break\n\n        for chunk in chunks:\n            for index, handler in enumerate(upload_handlers):\n                chunk_length = len", "idx": 626}
{"namespace": "rest_framework.fields.is_simple_callable", "completion": "    return callable(obj) and not inspect.isbuiltin(obj) and not isinstance(obj, functools.partial)\n\n", "idx": 627}
{"namespace": "rest_framework.fields.Field.bind", "completion": "        self.field_name = field_name\n        self.parent = parent\n", "idx": 628}
{"namespace": "rest_framework.fields.Field.run_validation", "completion": "        if data is empty:\n            return data\n        value = self.to_internal_value(data)\n        self.run_validators(value)\n        return value", "idx": 629}
{"namespace": "rest_framework.fields.Field.root", "completion": "        parent = self\n        while hasattr(parent, 'parent'):\n            parent = parent.parent\n        return parent\n", "idx": 630}
{"namespace": "rest_framework.fields.CharField.run_validation", "completion": "        if data == '' and self.allow_blank:\n            return ''\n        return super().run_validation(data)", "idx": 631}
{"namespace": "rest_framework.fields.CharField.to_internal_value", "completion": "        if isinstance(data, bool):\n            return str(data)\n        elif isinstance(data, (int, float)):\n            return str(data)\n        elif isinstance(data, str):\n            return data\n        else:\n            self.fail('invalid', value=data)", "idx": 632}
{"namespace": "rest_framework.fields.DecimalField.to_internal_value", "completion": "        if isinstance(data, str) and len(data) > self.MAX_STRING_LENGTH:\n            self.fail('max_string_length')\n\n        try:\n            return decimal.Decimal(data)\n        except (TypeError, ValueError):\n            self.fail('invalid')\n\n        value = self.validate_precision(value)\n\n        return value", "idx": 633}
{"namespace": "rest_framework.fields.DateTimeField.to_representation", "completion": "        if value is None:\n            return None\n\n        output_format = getattr(self, 'format', api_settings.DATETIME_FORMAT)\n\n        if output_format is None or isinstance(value, str):\n            return value\n\n        if output_format.lower() == ISO_8601:\n            return value.isoformat()\n\n        return value.strftime(output_format)", "idx": 634}
{"namespace": "rest_framework.fields.ChoiceField.iter_options", "completion": "        for group_name, group_choices in self.grouped_choices.items():\n            yield (group_name, [])\n            for key, value in group_choices:\n                yield (key, value)\n", "idx": 635}
{"namespace": "rest_framework.fields.MultipleChoiceField.get_value", "completion": "        if self.field_name not in dictionary:\n            if getattr(self.root, 'partial', False):\n                return empty\n        # We override the default field access in order to support\n        # lists in HTML forms.\n        if html.is_html_input(dictionary):\n            val = dictionary.getlist(self.field_name, [])\n            if len(val) > 0:\n                # Support QueryDict lists in HTML input.\n                return val\n            return html.parse_html_list(dictionary, prefix=self.field_name, default=empty)\n\n        return dictionary.get(self.field_name, empty)", "idx": 636}
{"namespace": "rest_framework.exceptions._get_error_details", "completion": "    if isinstance(data, list):\n        return [_get_error_details(item, default_code) for item in data]\n    elif isinstance(data, dict):\n        return {key: _get_error_details(value, default_code) for key, value in data.items()}\n    elif isinstance(data, ErrorDetail):\n        return data\n    elif isinstance(data, Exception):\n        return ErrorDetail(str(data), code=getattr(data, 'code', default_code))\n    return ErrorDetail(force_str(data), code=default_code)", "idx": 637}
{"namespace": "rest_framework.exceptions.server_error", "completion": "    data = {\n        'error': 'Internal Server Error (500)'\n    }\n    return JsonResponse(data, status=status.HTTP_500_INTERNAL_SERVER_ERROR)", "idx": 638}
{"namespace": "rest_framework.exceptions.bad_request", "completion": "    data = {\n        'error': 'Bad Request (400)'\n    }\n    return JsonResponse(data, status=status.HTTP_400_BAD_REQUEST)\n\n", "idx": 639}
{"namespace": "rest_framework.relations.RelatedField.iter_options", "completion": "        return iter_options(\n            self.get_choices(cutoff=self.html_cutoff),\n            cutoff=self.html_cutoff,\n            cutoff_text=self.html_cutoff_text\n        )", "idx": 640}
{"namespace": "rest_framework.relations.PrimaryKeyRelatedField.to_internal_value", "completion": "        if data == '':\n            data = None\n\n        if self.pk_field is not None:\n            data = self.pk_field.to_internal_value(data)\n\n        try:\n            return self.get_queryset().get(pk=data)\n        except ObjectDoesNotExist:\n            self.fail('does_not_exist', pk_value=data)\n        except (TypeError, ValueError):\n            self.fail('incorrect_type', data_type=type(data).__name__)", "idx": 641}
{"namespace": "rest_framework.relations.PrimaryKeyRelatedField.to_representation", "completion": "        if self.pk_field is not None:\n            return self.pk_field.to_representation(value)\n        return value.pk\n\n", "idx": 642}
{"namespace": "rest_framework.relations.SlugRelatedField.to_internal_value", "completion": "        queryset = self.get_queryset()\n        try:\n            return queryset.get(**{self.slug_field: data})\n        except ObjectDoesNotExist:\n            self.fail('does_not_exist', slug_name=self.slug_field, value=data)\n        except (TypeError, ValueError):\n            self.fail('invalid')", "idx": 643}
{"namespace": "rest_framework.templatetags.rest_framework.add_query_param", "completion": "    uri = iri_to_uri(request.get_full_path())\n    if '?' in uri:\n        uri = uri.split('?')[0]\n        uri += '&'\n    else:\n        uri += '?'\n    uri += '%s=%s' % (key, val)\n    return escape(uri)", "idx": 644}
{"namespace": "rest_framework.utils.mediatypes._MediaType.match", "completion": "        if self.main_type != other.main_type and other.main_type != '*':\n            return False\n        if self.sub_type != other.sub_type and other.sub_type != '*':\n            return False\n        for key, val in self.params.items():\n            if key not in other.params:\n                return False\n            if other.params[key] != val:\n                return False\n        return True", "idx": 645}
{"namespace": "rest_framework.utils.mediatypes._MediaType.precedence", "completion": "        if self.main_type == '*':\n            if self.sub_type == '*':\n                if self.params:\n                    return 0\n                else:\n                    return 3\n            else:\n                if self.params:\n                    return 1\n                else:\n                    return 2\n        else:\n            if self.sub_type == '*':\n                if self.params:\n                    return 1\n                else:\n                    return 2\n            else:\n                if self.params:\n                    return 2\n                else:\n                    return 3\n\n    ", "idx": 646}
{"namespace": "rest_framework.utils.mediatypes._MediaType.__str__", "completion": "        ret = self.main_type + '/' + self.sub_type\n        if self.params:\n            ret += '; ' + '; '.join(\n                [key + ('=' + val if val else '') for key, val in self.params.items()]\n            )\n        return ret", "idx": 647}
{"namespace": "asyncpg._testbase.TestCase.assertLoopErrorHandlerCalled", "completion": "        def handler(loop, context):\n            exc = context['exception']\n            if re.search(msg_re, str(exc)):\n                return\n            raise AssertionError(\n                'expected exception matching \"{}\" not found'.format(msg_re))\n\n        old_handler = self.loop.get_exception_handler()\n        self.loop.set_exception_handler(handler)\n        try:\n            yield\n        finally:\n            self.loop.set_exception_handler(old_handler)\n\n    ", "idx": 648}
{"namespace": "csvs_to_sqlite.utils.refactor_dataframes", "completion": "    lookup_tables = {}\n    for column, (table, value_column) in foreign_keys.items():\n        lookup_tables[column] = LookupTable(\n            conn, table, value_column, index_fts=index_fts\n        )\n\n    for df in dataframes:\n        for column, (table, value_column) in foreign_keys.items():\n            if column in df.columns:\n                df[column] = df[column].apply(lookup_tables[column].id_for_value)", "idx": 649}
{"namespace": "sqlitedict.SqliteDict.iteritems", "completion": "        GET_ITEMS = 'SELECT key, value FROM \"%s\" ORDER BY rowid' % self.tablename\n        for key, value in self.conn.select(GET_ITEMS):\n            yield self.decode_key(key), self.decode(value)", "idx": 650}
{"namespace": "sqlitedict.SqliteDict.update", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to update read-only SqliteDict')\n\n        if isinstance(items, dict):\n            items = items.items()\n\n        for key, value in items:\n            self[key] = value\n\n        for key, value in kwds.items():\n            self[key] = value\n\n        if self.autocommit:\n            self.commit()", "idx": 651}
{"namespace": "sqlitedict.SqliteDict.clear", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to clear read-only SqliteDict')\n\n        CLEAR_TABLE = 'DELETE FROM \"%s\"' % self.tablename\n        self.conn.execute(CLEAR_TABLE)\n        if self.autocommit:\n            self.commit()", "idx": 652}
{"namespace": "sqlitedict.SqliteDict.commit", "completion": "", "idx": 653}
{"namespace": "sqlitedict.SqliteDict.terminate", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to delete read-only SqliteDict')\n\n        self.close()\n        os.remove(self.filename)", "idx": 654}
{"namespace": "boto.utils.retry_url", "completion": "    if not timeout:\n        timeout = boto.config.get('Boto', 'timeout', 60)\n    if not num_retries:\n        num_retries = boto.config.get('Boto', 'num_retries', 5)\n    if not retry_on_404:\n        retry_on_404 = boto.config.get('Boto', 'retry_on_404', True)\n\n    for i in range(0, num_retries):\n        try:\n            s = urllib.request.urlopen(url, timeout=timeout)\n            return s.read()\n        except urllib.error.HTTPError as e:\n            if e.code == 404 and not retry_on_404:\n                return None\n            boto.log.debug(\"Caught exception reading metadata for the '%s' try\" % (i + 1))\n            boto.log.debug(\"encountered '%s' exception: %s\" % (e.__class__.__name__, e))\n            if i + 1 != num_retries:\n                next_sleep = min(random.random() * 2 ** i, boto.config.get('Boto', 'max_retry_delay', 60))\n                time.sleep(next_sleep)\n            else:\n                boto.log.error('Unable to read metadata, giving up')\n                raise e", "idx": 655}
{"namespace": "boto.utils.LazyLoadMetadata.values", "completion": "        self._materialize()\n        return super(LazyLoadMetadata, self).values()\n", "idx": 656}
{"namespace": "boto.utils.get_instance_userdata", "completion": "    try:\n        userdata_url = _build_instance_metadata_url(url, version, 'user-data')\n        userdata = retry_url(userdata_url, num_retries=num_retries, timeout=timeout)\n        if sep:\n            userdata = userdata.split(sep)\n        return userdata\n    except urllib.error.URLError:\n        boto.log.exception(\"Exception caught when trying to retrieve \"\n                           \"instance user data\")\n        return None", "idx": 657}
{"namespace": "boto.utils.pythonize_name", "completion": "    s1 = _first_cap_regex.sub(r'\\1_\\2', name)\n    return _number_cap_regex.sub(r'\\1_\\2', s1).lower()", "idx": 658}
{"namespace": "boto.cloudsearchdomain.connect_to_region", "completion": "    from boto.cloudsearchdomain.layer1 import CloudSearchDomainConnection\n    return CloudSearchDomainConnection(region=region_name, **kw_params)", "idx": 659}
{"namespace": "boto.redshift.connect_to_region", "completion": "    from boto.redshift.layer1 import RedshiftConnection\n    return RedshiftConnection(region=region_name, **kw_params)", "idx": 660}
{"namespace": "boto.support.connect_to_region", "completion": "    from boto.support.layer1 import SupportConnection\n    return SupportConnection(region=region_name, **kw_params)", "idx": 661}
{"namespace": "boto.configservice.connect_to_region", "completion": "    from boto.configservice.layer1 import ConfigServiceConnection\n    return ConfigServiceConnection(region=region_name, **kw_params)", "idx": 662}
{"namespace": "boto.cloudhsm.connect_to_region", "completion": "    from boto.cloudhsm.layer1 import CloudHSMConnection\n    return CloudHSMConnection(region=region_name, **kw_params)", "idx": 663}
{"namespace": "boto.logs.connect_to_region", "completion": "    from boto.logs.layer1 import CloudWatchLogsConnection\n    return CloudWatchLogsConnection(region_name, **kw_params)", "idx": 664}
{"namespace": "boto.cloudsearch.connect_to_region", "completion": "    from boto.cloudsearch.layer1 import Layer1\n    return Layer1().connect_to_region(region_name, **kw_params)", "idx": 665}
{"namespace": "boto.glacier.job.Job.download_to_fileobj", "completion": "        num_chunks = self._calc_num_chunks(chunk_size)\n        self._download_to_fileob(output_file, num_chunks, chunk_size,\n                                 verify_hashes, retry_exceptions)", "idx": 666}
{"namespace": "boto.glacier.utils.minimum_part_size", "completion": "    if size_in_bytes <= default_part_size:\n        return default_part_size\n    if size_in_bytes > MAXIMUM_NUMBER_OF_PARTS * default_part_size:\n        raise ValueError(\"File size exceeds maximum allowed archive size of %d bytes\" % (MAXIMUM_NUMBER_OF_PARTS * default_part_size))\n    return int(math.ceil(size_in_bytes / float(default_part_size))) * default_part_size", "idx": 667}
{"namespace": "boto.glacier.utils.chunk_hashes", "completion": "    chunks = []\n    while bytestring:\n        chunk, bytestring = bytestring[:chunk_size], bytestring[chunk_size:]\n        chunks.append(hashlib.sha256(chunk).digest())\n    return chunks", "idx": 668}
{"namespace": "boto.glacier.utils.compute_hashes_from_fileobj", "completion": "    linear_hash = hashlib.sha256()\n    tree_hash_list = []\n    while True:\n        data = fileobj.read(chunk_size)\n        if not data:\n            break\n        linear_hash.update(data)\n        tree_hash_list.append(hashlib.sha256(data).digest())\n    tree_hash = bytes_to_hex(tree_hash(tree_hash_list))\n    return linear_hash.hexdigest(), tree_hash", "idx": 669}
{"namespace": "boto.glacier.concurrent.ConcurrentTransferer._calculate_required_part_size", "completion": "        # The minimum part size is 1MB\n        min_part_size = 1024 * 1024\n        # The maximum part size is 4GB\n        max_part_size = 4 * 1024 * 1024 * 1024\n        # The part size must be a power of two between 1MB and 4GB\n        if part_size < min_part_size:\n            part_size = min_part_size\n        elif part_size > max_part_size:\n            part_size = max_part_size\n        else:\n            # The part size must be a power of two\n            part_size = 1024 * 1024\n            while part_size < total_size:\n                part_size *= 2\n        total_parts = int(math.ceil(total_size / float(part_size)))\n        return total_parts, part_size", "idx": 670}
{"namespace": "boto.glacier.connect_to_region", "completion": "    from boto.glacier.layer2 import Layer2\n    return Layer2(region_name, **kw_params)", "idx": 671}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.update", "completion": "        if self.id is None:\n            raise ValueError('NetworkInterface.update requires an ENI ID')\n\n        if self.connection is None:\n            raise ValueError('NetworkInterface.update requires a connection')\n\n        response = self.connection.get_all_network_interfaces(\n            network_interface_ids=[self.id],\n            dry_run=dry_run\n        )\n\n        if len(response.network_interfaces) == 0:\n            if validate:\n                raise ValueError('NetworkInterface.update: No data returned for ENI %s' % self.id)\n            else:\n                return\n\n        updated = response.network_interfaces[0]\n        self._update(updated)\n\n        return self.status\n", "idx": 672}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.attach", "completion": "        return self.connection.attach_network_interface(\n            self.id,\n            instance_id,\n            device_index,\n            dry_run=dry_run\n        )", "idx": 673}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.detach", "completion": "        return self.connection.detach_network_interface(\n            self.id,\n            force=force,\n            dry_run=dry_run\n        )", "idx": 674}
{"namespace": "boto.ec2.address.Address.release", "completion": "        if self.allocation_id:\n            return self.connection.release_address(\n                allocation_id=self.allocation_id,\n                dry_run=dry_run\n            )\n        else:\n            return self.connection.release_address(\n                public_ip=self.public_ip,\n                dry_run=dry_run\n            )", "idx": 675}
{"namespace": "boto.ec2.address.Address.associate", "completion": "        if self.allocation_id:\n            return self.connection.associate_address(\n                allocation_id=self.allocation_id,\n                instance_id=instance_id,\n                network_interface_id=network_interface_id,\n                private_ip_address=private_ip_address,\n                allow_reassociation=allow_reassociation,\n                dry_run=dry_run\n            )\n        else:\n            return self.connection.associate_address(\n                public_ip=self.public_ip,\n                instance_id=instance_id,\n                network_interface_id=network_interface_id,\n                private_ip_address=private_ip_address,\n                allow_reassociation=allow_reassociation,\n                dry_run=dry_run\n            )", "idx": 676}
{"namespace": "boto.ec2.address.Address.disassociate", "completion": "        if self.allocation_id:\n            return self.connection.disassociate_address(\n                allocation_id=self.allocation_id,\n                dry_run=dry_run\n            )\n        else:\n            return self.connection.disassociate_address(\n                public_ip=self.public_ip,\n                dry_run=dry_run\n            )", "idx": 677}
{"namespace": "boto.ec2.ec2object.TaggedEC2Object.add_tags", "completion": "        status = self.connection.create_tags(\n            [self.id],\n            tags,\n            dry_run=dry_run\n        )\n        for key, value in tags.items():\n            self.tags[key] = value", "idx": 678}
{"namespace": "boto.ec2.ec2object.TaggedEC2Object.remove_tags", "completion": "        status = self.connection.delete_tags(\n            [self.id],\n            tags,\n            dry_run=dry_run\n        )\n        if self.tags is None:\n            self.tags = TagSet()\n        self.tags.remove(tags)", "idx": 679}
{"namespace": "boto.ec2.connection.EC2Connection.get_all_instance_status", "completion": "        params = {}\n        if instance_ids:\n            self.build_list_params(params, instance_ids, 'InstanceId')\n        if max_results:\n            params['MaxResults'] = max_results\n        if next_token:\n            params['NextToken'] = next_token\n        if filters:\n            self.build_filter_params(params, filters)\n        if include_all_instances:\n            params['IncludeAllInstances'] = 'true'\n        if dry_run:\n            params['DryRun'] = 'true'\n        return self.get_list('DescribeInstanceStatus', params,\n                             [('item', InstanceStatus)], verb='POST')\n", "idx": 680}
{"namespace": "boto.ec2.volume.Volume.update", "completion": "        if self.id is None:\n            raise ValueError('Cannot update a volume that has no id')\n        if validate:\n            if self.connection.get_all_volumes([self.id]) == []:\n                raise ValueError('Volume %s does not exist' % self.id)\n        return self.connection.update_volume(self.id, dry_run=dry_run)", "idx": 681}
{"namespace": "boto.ec2.volume.Volume.attach", "completion": "        return self.connection.attach_volume(\n            self.id,\n            instance_id,\n            device,\n            dry_run=dry_run\n        )", "idx": 682}
{"namespace": "boto.ec2.volume.Volume.detach", "completion": "        return self.connection.detach_volume(self.id, force, dry_run)", "idx": 683}
{"namespace": "boto.ec2.volume.Volume.create_snapshot", "completion": "        return self.connection.create_snapshot(self.id, description, dry_run)", "idx": 684}
{"namespace": "boto.ec2.volume.Volume.attachment_state", "completion": "        if self.attach_data:\n            return self.attach_data.status\n        else:\n            return None", "idx": 685}
{"namespace": "boto.ec2.securitygroup.SecurityGroup.add_rule", "completion": "        if not self.rules:\n            self.rules.append(IPPermissions(self))\n        rule = self.rules[-1]\n        rule.ip_protocol = ip_protocol\n        rule.from_port = from_port\n        rule.to_port = to_port\n        rule.add_grant(src_group_name, src_group_owner_id, cidr_ip,\n                       src_group_group_id, dry_run=dry_run)", "idx": 686}
{"namespace": "boto.ec2.connect_to_region", "completion": "    region = get_region(region_name, **kw_params)\n    if region is None:\n        return None\n    return region.connect(**kw_params)", "idx": 687}
{"namespace": "boto.ec2.cloudwatch.connect_to_region", "completion": "    region = RegionInfo(None, region_name, RegionData[region_name])\n    return CloudWatchConnection(region=region, **kw_params)", "idx": 688}
{"namespace": "boto.ec2.autoscale.connect_to_region", "completion": "    region = RegionData.get(region_name)\n    if region is None:\n        raise NoSuchRegionError(region_name)\n    return region.connect(**kw_params)", "idx": 689}
{"namespace": "boto.ec2.elb.connect_to_region", "completion": "    region = RegionInfo(None, region_name, RegionData[region_name])\n    return ELBConnection(region=region, **kw_params)", "idx": 690}
{"namespace": "boto.ec2.elb.ELBConnection.get_all_load_balancers", "completion": "        params = {}\n        if load_balancer_names:\n            self.build_list_params(params, load_balancer_names,\n                                   'LoadBalancerNames.member.%d')\n        if marker:\n            params['Marker'] = marker\n        return self.get_list('DescribeLoadBalancers', params,\n                             [('member', LoadBalancer)])\n", "idx": 691}
{"namespace": "boto.ec2.elb.ELBConnection.disable_availability_zones", "completion": "        params = {'LoadBalancerName': load_balancer_name}\n        self.build_list_params(params, zones_to_remove,\n                               'AvailabilityZones.member.%d')\n        obj = self.get_object('DisableAvailabilityZonesForLoadBalancer',\n                              params, LoadBalancerZones)\n        return obj.zones", "idx": 692}
{"namespace": "boto.awslambda.connect_to_region", "completion": "    from boto.awslambda.layer1 import AWSLambdaConnection\n    return AWSLambdaConnection(region=region_name, **kw_params)", "idx": 693}
{"namespace": "boto.cognito.identity.connect_to_region", "completion": "    from boto.cognito.identity.layer1 import CognitoIdentityConnection\n    return CognitoIdentityConnection().connect(region_name, **kw_params)", "idx": 694}
{"namespace": "boto.cognito.sync.connect_to_region", "completion": "    from boto.cognito.sync.layer1 import CognitoSyncConnection\n    return CognitoSyncConnection(region_name, **kw_params)", "idx": 695}
{"namespace": "boto.cloudformation.connect_to_region", "completion": "    region = RegionInfo(**RegionData[region_name])\n    return region.connect(**kw_params)", "idx": 696}
{"namespace": "boto.route53.zone.Zone.find_records", "completion": "        name = self.route53connection._make_qualified(name)\n        if identifier is not None:\n            try:\n                int(identifier[1])\n                weight = identifier[1]\n                identifier = identifier[0]\n            except:\n                region = identifier[1]\n                identifier = identifier[0]\n        return self.route53connection.get_all_rrsets(self.id, name, type,\n                                                     identifier=identifier,\n                                                     weight=weight,\n                                                     region=region,\n                                                     maxitems=desired,\n                                                     all=all)", "idx": 697}
{"namespace": "boto.route53.domains.connect_to_region", "completion": "    from boto.route53.domains.layer1 import Route53DomainsConnection\n    return Route53DomainsConnection(region=region_name, **kw_params)", "idx": 698}
{"namespace": "boto.s3.key.Key.get_contents_to_filename", "completion": "        with open(filename, 'wb') as fp:\n            self.get_contents_to_file(fp, headers, cb, num_cb,\n                                      torrent=torrent,\n                                      version_id=version_id,\n                                      res_download_handler=res_download_handler,\n                                      response_headers=response_headers)", "idx": 699}
{"namespace": "boto.s3.cors.CORSConfiguration.add_rule", "completion": "        rule = CORSRule(allowed_method, allowed_origin, id, allowed_header, max_age_seconds, expose_header)\n        self.append(rule)", "idx": 700}
{"namespace": "boto.s3.bucket.Bucket.get_key", "completion": "        return self._get_key_internal(key_name, headers,\n                                      ['versionId=%s' % version_id,\n                                       'response-content-type=%s' % response_headers.get('Content-Type', ''),\n                                       'response-content-language=%s' % response_headers.get('Content-Language', ''),\n                                       'response-expires=%s' % response_headers.get('Expires', ''),\n                                       'response-cache-control=%s' % response_headers.get('Cache-Control', ''),\n                                       'response-content-disposition=%s' % response_headers.get('Content-Disposition', ''),\n                                       'response-content-encoding=%s' % response_headers.get('Content-Encoding', '')])[0]\n", "idx": 701}
{"namespace": "boto.s3.bucket.Bucket.new_key", "completion": "        if key_name is None:\n            key_name = ''\n        return self.key_class(self, key_name)\n", "idx": 702}
{"namespace": "boto.s3.bucket.Bucket.delete_key", "completion": "        if not key_name:\n            raise ValueError('Empty key names are not allowed')\n        return self._delete_key_internal(key_name, headers, version_id,\n                                         mfa_token)", "idx": 703}
{"namespace": "boto.s3.bucket.Bucket.get_tags", "completion": "        response = self.connection.make_request('GET', self.name,\n                                                query_args='tagging',\n                                                headers=headers)\n        body = response.read()\n        if response.status == 200:\n            tags = Tags()\n            h = handler.XmlHandler(tags, self)\n            if not isinstance(body, bytes):\n                body = body.encode('utf-8')\n            xml.sax.parseString(body, h)\n            return tags\n        else:\n            raise self.connection.provider.storage_response_error(\n                response.status, response.reason, body)", "idx": 704}
{"namespace": "boto.s3.connection.S3Connection._required_auth_capability", "completion": "        if self.anon:\n            return ['anonymous']\n        else:\n            return ['hmac-v2']\n\n", "idx": 705}
{"namespace": "boto.s3.connection.S3Connection.generate_url_sigv4", "completion": "        if not iso_date:\n            iso_date = boto.utils.iso8601_date()\n        if not headers:\n            headers = {}\n        if response_headers:\n            for k, v in response_headers.items():\n                headers[k] = v\n        if version_id:\n            headers['x-amz-version-id'] = version_id\n        if self.provider.security_token:\n            headers['x-amz-security-token'] = self.provider.security_token\n        if force_http:\n            protocol = 'http'\n            port = 80\n        else:\n            protocol = self.protocol\n            port = self.port\n        url = self.calling_format.build_url_base(self, protocol,\n                                                 self.server_name(port),\n                                                 bucket, key)\n        if query_auth:\n            c_string = boto.utils.canonical_string(method, url, headers,\n                                                   expires_in, self.provider)\n            b64_hmac = self._auth_handler.sign_string(c_string)\n            encoded_canonical = urllib.parse.quote(b64_hmac, safe='')\n            url += '?' + self.QueryString % (encoded_canonical, expires_in,\n                                             self.aws_access_key_id)\n        return url", "idx": 706}
{"namespace": "boto.s3.lifecycle.Lifecycle.add_rule", "completion": "        rule = Rule(id, prefix, status, expiration, transition)\n        self.append(rule)", "idx": 707}
{"namespace": "boto.s3.website.WebsiteConfiguration.to_xml", "completion": "        parts = []\n        if self.suffix:\n            parts.append(tag('IndexDocument', self.suffix))\n        if self.error_key:\n            parts.append(tag('ErrorDocument', self.error_key))\n        if self.redirect_all_requests_to:\n            parts.append(self.redirect_all_requests_to.to_xml())\n        if self.routing_rules:\n            parts.append(self.routing_rules.to_xml())\n        return ''.join(parts)", "idx": 708}
{"namespace": "boto.s3.website.RoutingRules.to_xml", "completion": "        parts = []\n        for rule in self:\n            parts.append(rule.to_xml())\n        return tag('RoutingRules', '\\n'.join(parts))", "idx": 709}
{"namespace": "boto.s3.website.RoutingRule.when", "completion": "        condition = Condition(key_prefix=key_prefix,\n                              http_error_code=http_error_code)\n        return cls(condition=condition)", "idx": 710}
{"namespace": "boto.s3.website.RoutingRule.then_redirect", "completion": "        self.redirect = Redirect(hostname=hostname, protocol=protocol, replace_key=replace_key, replace_key_prefix=replace_key_prefix, http_redirect_code=http_redirect_code)\n        return self", "idx": 711}
{"namespace": "boto.s3.connect_to_region", "completion": "    if 'host' in kw_params:\n        region = S3RegionInfo(name=region_name, connection_cls=S3Connection, **kw_params)\n        return region.connect(**kw_params)\n    else:\n        return S3Connection(region_name, **kw_params)", "idx": 712}
{"namespace": "boto.directconnect.connect_to_region", "completion": "    from boto.directconnect.layer1 import DirectConnectConnection\n    return DirectConnectConnection(region=region_name, **kw_params)", "idx": 713}
{"namespace": "boto.rds.connect_to_region", "completion": "    region = RDSRegionInfo(None, region_name, 'rds.amazonaws.com')\n    return region.connect(**kw_params)", "idx": 714}
{"namespace": "boto.datapipeline.connect_to_region", "completion": "    from boto.datapipeline.layer1 import DataPipelineConnection\n    return DataPipelineConnection(region_name, **kw_params)", "idx": 715}
{"namespace": "boto.dynamodb.batch.Batch.to_dict", "completion": "        d = {'Keys': []}\n        for key in self.keys:\n            if isinstance(key, tuple):\n                hash_key, range_key = key\n            else:\n                hash_key = key\n                range_key = None\n            k = self.table.layer2.build_key_from_values(self.table.schema,\n                                                        hash_key, range_key)\n            d['Keys'].append(k)\n        if self.attributes_to_get:\n            d['AttributesToGet'] = self.attributes_to_get\n        if self.consistent_read:\n            d['ConsistentRead'] = self.consistent_read\n        return d", "idx": 716}
{"namespace": "boto.dynamodb.batch.BatchList.to_dict", "completion": "        d = {}\n        for batch in self:\n            d[batch.table.name] = batch.to_dict()\n        return d", "idx": 717}
{"namespace": "boto.dynamodb.types.Dynamizer.encode", "completion": "        dynamodb_type = self._get_dynamodb_type(attr)\n        try:\n            encoder = getattr(self, '_encode_%s' % dynamodb_type.lower())\n        except AttributeError:\n            return attr\n        return {dynamodb_type: encoder(attr)}", "idx": 718}
{"namespace": "boto.dynamodb.types.Dynamizer.decode", "completion": "        if len(attr) > 1:\n            return attr\n        if 'S' in attr:\n            return self._decode_s(attr['S'])\n        if 'N' in attr:\n            return self._decode_n(attr['N'])\n        if 'SS' in attr:\n            return self._decode_ss(attr['SS'])\n        if 'NS' in attr:\n            return self._decode_ns(attr['NS'])\n        if 'B' in attr:\n            return self._decode_b(attr['B'])\n        if 'BS' in attr:\n            return self._decode_bs(attr['BS'])\n        if 'NULL' in attr:\n            return self._decode_null(attr['NULL'])\n        if 'BOOL' in attr:\n            return self._decode_bool(attr['BOOL'])\n        if 'M' in attr:\n            return self._decode_m(attr['M'])\n        if 'L' in attr:\n            return self._decode_l(attr['L'])", "idx": 719}
{"namespace": "boto.dynamodb.connect_to_region", "completion": "    from boto.dynamodb.layer2 import Layer2\n    return Layer2(region_name, **kw_params)", "idx": 720}
{"namespace": "boto.beanstalk.connect_to_region", "completion": "    from boto.beanstalk.layer1 import Layer1\n    return Layer1(region_name, **kw_params)", "idx": 721}
{"namespace": "boto.swf.connect_to_region", "completion": "    region = RegionInfo(None, region_name, 'swf', REGION_ENDPOINTS[region_name])\n    return region.connect(**kw_params)", "idx": 722}
{"namespace": "boto.opsworks.regions", "completion": "    from boto.opsworks.layer1 import OpsWorksConnection\n    return OpsWorksConnection().get_all_regions()", "idx": 723}
{"namespace": "boto.opsworks.connect_to_region", "completion": "    region = RegionInfo(name=region_name, endpoint='opsworks.%s.amazonaws.com' % (region_name))\n    return OpsWorksConnection(region=region, **kw_params)", "idx": 724}
{"namespace": "boto.sqs.connect_to_region", "completion": "    from boto.sqs.connection import SQSConnection\n    return SQSConnection(region=region_name, **kw_params)", "idx": 725}
{"namespace": "boto.rds2.connect_to_region", "completion": "    from boto.rds2.layer1 import RDSConnection\n    return get_region(region_name, 'rds', connection_cls=RDSConnection, **kw_params)", "idx": 726}
{"namespace": "boto.cloudsearch2.connect_to_region", "completion": "    from boto.cloudsearch2.layer1 import CloudSearchConnection\n    return CloudSearchConnection(region=region_name, **kw_params)", "idx": 727}
{"namespace": "boto.cloudtrail.connect_to_region", "completion": "    from boto.cloudtrail.layer1 import CloudTrailConnection\n    return CloudTrailConnection(region=region_name, **kw_params)", "idx": 728}
{"namespace": "boto.elasticache.connect_to_region", "completion": "    from boto.elasticache.layer1 import ElastiCacheConnection\n    return ElastiCacheConnection(region=region_name, **kw_params)", "idx": 729}
{"namespace": "boto.ses.connect_to_region", "completion": "    region = RegionInfo.get_region(region_name)\n    if region:\n        return region.connect(**kw_params)\n    else:\n        return None", "idx": 730}
{"namespace": "boto.codedeploy.connect_to_region", "completion": "    from boto.codedeploy.layer1 import CodeDeployConnection\n    return CodeDeployConnection(region=region_name, **kw_params)", "idx": 731}
{"namespace": "boto.sts.credentials.Credentials.to_dict", "completion": "        return {\n            'access_key': self.access_key,\n            'secret_key': self.secret_key,\n            'session_token': self.session_token,\n            'expiration': self.expiration,\n            'request_id': self.request_id\n        }", "idx": 732}
{"namespace": "boto.sts.connect_to_region", "completion": "    region = RegionInfo.get(region_name, 'sts', connection_cls=STSConnection)\n    if region:\n        return region.connect(**kw_params)\n    else:\n        return None", "idx": 733}
{"namespace": "boto.machinelearning.connect_to_region", "completion": "    from boto.machinelearning.layer1 import MachineLearningConnection\n    return MachineLearningConnection(region=region_name, **kw_params)", "idx": 734}
{"namespace": "boto.vpc.connect_to_region", "completion": "    region = RegionInfo(name=region_name, connection_cls=VPCConnection)\n    return region.connect(**kw_params)", "idx": 735}
{"namespace": "boto.vpc.VPCConnection.get_all_vpc_peering_connections", "completion": "        params = {}\n        if vpc_peering_connection_ids:\n            self.build_list_params(params, vpc_peering_connection_ids, \n                                   'VpcPeeringConnectionId')\n        if filters:\n            self.build_filter_params(params, filters)\n        if dry_run:\n            params['DryRun'] = 'true'\n        return self.get_list('DescribeVpcPeeringConnections', params, \n                             [('item', VpcPeeringConnection)])", "idx": 736}
{"namespace": "boto.kinesis.connect_to_region", "completion": "    from boto.kinesis.layer1 import KinesisConnection\n    return KinesisConnection(region=region_name, **kw_params)", "idx": 737}
{"namespace": "boto.ec2containerservice.connect_to_region", "completion": "    from boto.ec2containerservice.layer1 import EC2ContainerServiceConnection\n    return EC2ContainerServiceConnection(region_name, **kw_params)", "idx": 738}
{"namespace": "boto.dynamodb2.table.Table._introspect_indexes", "completion": "        indexes = []\n\n        for field in raw_indexes:\n            index_klass = self._PROJECTION_TYPE_TO_INDEX.get('local_indexes').get('ALL')\n            kwargs = {\n                'parts': []\n            }\n\n            if field['Projection']['ProjectionType'] == 'ALL':\n                index_klass = self._PROJECTION_TYPE_TO_INDEX.get('local_indexes').get('ALL')\n            elif field['Projection']['ProjectionType'] == 'KEYS_ONLY':\n                index_klass = self._PROJECTION_TYPE_TO_INDEX.get('local_indexes').get('KEYS_ONLY')\n            elif field['Projection']['ProjectionType'] == 'INCLUDE':\n                index_klass = self._PROJECTION_TYPE_TO_INDEX.get('local_indexes').get('INCLUDE')\n                kwargs['includes'] = field['Projection']['NonKeyAttributes']\n            else:\n                raise exceptions.UnknownIndexFieldError(\n                    \"%s was seen, but is unknown. Please report this at \"\n                    \"https://github.com/boto/boto/issues.\" % \\\n                    field['Projection']['ProjectionType']\n                )\n\n            name = field['IndexName']\n            kwargs['parts'] = self._introspect_schema(field['KeySchema'], None)\n            indexes.append(index_klass(name, **kwargs))\n\n        return indexes\n", "idx": 739}
{"namespace": "boto.dynamodb2.table.Table.describe", "completion": "        raw_table = self.connection.describe_table(self.table_name)\n        self.schema = self._introspect_schema(raw_table['Table']['KeySchema'])\n        self.indexes = self._introspect_indexes(raw_table['Table'].get('LocalSecondaryIndexes', []))\n        self.global_indexes = self._introspect_global_indexes(raw_table['Table'].get('GlobalSecondaryIndexes', []))\n        self.throughput = {\n            'read': raw_table['Table']['ProvisionedThroughput']['ReadCapacityUnits'],\n            'write': raw_table['Table']['ProvisionedThroughput']['WriteCapacityUnits'],\n        }\n        return raw_table\n", "idx": 740}
{"namespace": "boto.dynamodb2.table.Table.update", "completion": "        if throughput:\n            self.connection.update_table(\n                self.table_name,\n                provisioned_throughput={\n                    'ReadCapacityUnits': int(throughput['read']),\n                    'WriteCapacityUnits': int(throughput['write']),\n                }\n            )\n\n        if global_indexes:\n            gsi_data = []\n\n            for gsi_name, gsi_throughput in global_indexes.items():\n                gsi_data.append({\n                    \"Update\": {\n                        \"IndexName\": gsi_name,\n                        \"ProvisionedThroughput\": {\n                            \"ReadCapacityUnits\": int(gsi_throughput['read']),\n                            \"WriteCapacityUnits\": int(gsi_throughput['write']),\n                        },\n                    },\n                })\n\n            self.connection.update_table(\n                self.table_name,\n                global_secondary_index_updates=gsi_data,\n            )\n\n        return True\n", "idx": 741}
{"namespace": "boto.dynamodb2.table.Table.create_global_secondary_index", "completion": "        if global_index:\n            raw_indexes = []\n            raw_indexes.append(global_index.schema())\n            kwargs = {\n                'global_secondary_indexes': raw_indexes,\n            }\n            self.connection.update_table(\n                self.table_name,\n                **kwargs\n            )\n            self.describe()\n            return True\n        else:\n            msg = 'You need to provide the global index to ' \\\n                  'create_global_secondary_index method'\n            boto.log.error(msg)\n\n            return False", "idx": 742}
{"namespace": "boto.dynamodb2.table.Table.delete_global_secondary_index", "completion": "        # TODO: This function is not implemented in boto.dynamodb2.\n        #       It is not used anywhere in the code.\n        #       It should be removed from the code.\n        #       It should be removed from the documentation.\n        #       It should be removed from the tests.\n        #       It should be removed from the documentation.\n        #       It should be removed from the documentation.\n        #       It should be removed from the documentation.\n        #       It should be removed from the documentation.\n        #       It should be removed from the documentation.\n        #       It should be removed from the documentation.\n        #       It should be removed from the documentation.\n        #       It should be removed from the documentation.\n        #       It should be removed from the documentation.\n        #       It should be removed from the documentation.\n        #       It should be removed from the documentation.\n        #       It should be removed from the documentation.\n        #       It should be removed from the documentation.\n        #       It should be removed from the documentation.\n        #       It should be removed from the documentation.\n        #       It should be removed from the documentation.\n        #       It should be removed from the documentation.\n        #       It should be removed from the documentation.\n        #       It should be removed from the documentation.\n        #       It should be removed from the documentation.\n        #       It should be removed from the documentation.\n        #       It should be removed from the documentation.\n        #       It should be removed from the documentation.\n        #       It should be removed from the documentation.\n        #       It should be removed from the documentation.\n        #       It should be removed from the documentation.\n        #       It should be removed from the documentation.\n        #       It should be removed from the documentation.\n        #       It should be removed from the documentation.\n        #       It should be removed from the documentation.\n        #       It should be removed from the documentation.\n        #       It should be removed from the documentation.\n        #       It should be removed from the documentation.\n        #       It should be removed from the documentation.\n        #       It should be removed from the documentation.\n        #       It should be removed from the documentation.", "idx": 743}
{"namespace": "boto.dynamodb2.table.Table.update_global_secondary_index", "completion": "        if global_indexes:\n            gsi_data = []\n\n            for gsi_name, gsi_throughput in global_indexes.items():\n                gsi_data.append({\n                    \"Update\": {\n                        \"IndexName\": gsi_name,\n                        \"ProvisionedThroughput\": {\n                            \"ReadCapacityUnits\": int(gsi_throughput['read']),\n                            \"WriteCapacityUnits\": int(gsi_throughput['write']),\n                        },\n                    },\n                })\n\n            self.connection.update_table(\n                self.table_name,\n                global_secondary_index_updates=gsi_data,\n            )\n\n            return True\n        else:\n            msg = 'You need to provide the global_indexes to ' \\\n                  'update_global_secondary_index method'\n            boto.log.error(msg)\n\n            return False", "idx": 744}
{"namespace": "boto.dynamodb2.table.Table.delete", "completion": "        self.connection.delete_table(self.table_name)\n        return True", "idx": 745}
{"namespace": "boto.dynamodb2.table.Table.get_item", "completion": "        if not self.schema:\n            self.describe()\n        raw_key = self._encode_keys(kwargs)\n        raw_item = self.connection.get_item(self.table_name, raw_key,\n                                            consistent=consistent,\n                                            attributes_to_get=attributes)\n        if not raw_item.get('Item', None):\n            raise exceptions.ItemNotFound(\n                \"Item not found in table %s\" % self.table_name\n            )\n        item = Item(self)\n        item.load(raw_item)\n        return item", "idx": 746}
{"namespace": "boto.dynamodb2.table.Table.has_item", "completion": "        raw_key = self._encode_keys(kwargs)\n        item_data = self.connection.get_item(\n            self.table_name,\n            raw_key,\n            consistent_read=consistent\n        )\n        if 'Item' not in item_data:\n            return False\n        return True", "idx": 747}
{"namespace": "boto.dynamodb2.table.Table._put_item", "completion": "        raw_key = self._encode_keys(key)\n        kwargs = {}\n\n        if expects is not None:\n            kwargs['expected'] = expects\n\n        self.connection.put_item(self.table_name, raw_key, item_data, **kwargs)\n        return True", "idx": 748}
{"namespace": "boto.dynamodb2.table.Table.delete_item", "completion": "        raw_key = self._encode_keys(kwargs)\n        kwargs = {}\n\n        if expected is not None:\n            kwargs['expected'] = expected\n\n        if conditional_operator is not None:\n            kwargs['conditional_operator'] = conditional_operator\n\n        self.connection.delete_item(self.table_name, raw_key, **kwargs)\n        return True", "idx": 749}
{"namespace": "boto.dynamodb2.table.Table.get_key_fields", "completion": "        if not self.schema:\n            self.describe()\n        return [field.name for field in self.schema]", "idx": 750}
{"namespace": "boto.dynamodb2.table.Table._build_filters", "completion": "        filters = {}\n        for key, value in filter_kwargs.items():\n            if '__' not in key:\n                raise exceptions.QueryError(\n                    \"You must specify a filter operation for %s.\" % key\n                )\n\n            fieldname, operation = key.split('__', 1)\n\n            if operation not in using:\n                raise exceptions.QueryError(\n                    \"You must specify a valid filter operation for %s.\" % key\n                )\n\n            filters[fieldname] = {\n                'AttributeValueList': [self._dynamizer.encode(value)],\n                'ComparisonOperator': using[operation],\n            }\n\n        return filters", "idx": 751}
{"namespace": "boto.dynamodb2.table.Table.batch_get", "completion": "        results = BatchGetResultSet(\n            table=self,\n            keys=keys,\n            consistent=consistent,\n            attributes=attributes,\n        )\n        return results", "idx": 752}
{"namespace": "boto.dynamodb2.table.Table.count", "completion": "        # TODO: This is a hack. We should use the count API.\n        # http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_DescribeTable.html\n        # http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_DescribeTable.html\n        # http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_DescribeTable.html\n        # http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_DescribeTable.html\n        # http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_DescribeTable.html\n        # http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_DescribeTable.html\n        # http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_DescribeTable.html\n        # http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_DescribeTable.html\n        # http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_DescribeTable.html\n        # http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_DescribeTable.html\n        # http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_DescribeTable.html\n        # http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_DescribeTable.html\n        # http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_DescribeTable.html\n        # http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_DescribeTable.html\n        # http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_DescribeTable.html\n        # http://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_DescribeTable.html\n        #", "idx": 753}
{"namespace": "boto.dynamodb2.table.BatchTable.put_item", "completion": "        self._to_put.append(data)\n\n        if self.should_flush():\n            self.flush()", "idx": 754}
{"namespace": "boto.dynamodb2.table.BatchTable.delete_item", "completion": "        self._to_delete.append(kwargs)\n\n        if self.should_flush():\n            self.flush()", "idx": 755}
{"namespace": "boto.dynamodb2.table.BatchTable.flush", "completion": "        # Do 25 at a time.\n        to_put = self._to_put[:25]\n        # Remove them from the list.\n        self._to_put = self._to_put[25:]\n        to_delete = self._to_delete[:25]\n        # Remove them from the list.\n        self._to_delete = self._to_delete[25:]\n        batch_data = {\n            self.table.table_name: {\n                'WriteRequests': [],\n            },\n        }\n\n        for data in to_put:\n            batch_data[self.table.table_name]['WriteRequests'].append({\n                'PutRequest': {\n                    'Item': data,\n                },\n            })\n\n        for data in to_delete:\n            batch_data[self.table.table_name]['WriteRequests'].append({\n                'DeleteRequest': {\n                    'Key': data,\n                },\n            })\n\n        resp = self.table.connection.batch_write_item(batch_data)\n        self.handle_unprocessed(resp)\n        return True", "idx": 756}
{"namespace": "boto.dynamodb2.table.BatchTable.resend_unprocessed", "completion": "        if len(self._unprocessed) > 0:\n            batch_data = {\n                self.table.table_name: [\n                    # We'll insert data here shortly.\n                ],\n            }\n\n            for unprocessed in self._unprocessed:\n                if unprocessed.get('PutRequest'):\n                    item = Item(self.table, data=unprocessed['PutRequest']['Item'])\n                    batch_data[self.table.table_name].append({\n                        'PutRequest': {\n                            'Item': item.prepare_full(),\n                        }\n                    })\n                elif unprocessed.get('DeleteRequest'):\n                    batch_data[self.table.table_name].append({\n                        'DeleteRequest': {\n                            'Key': unprocessed['DeleteRequest']['Key'],\n                        }\n                    })\n\n            resp = self.table.connection.batch_write_item(batch_data)\n            self.handle_unprocessed(resp)\n\n            self._unprocessed = []\n            return True\n", "idx": 757}
{"namespace": "boto.dynamodb2.fields.BaseSchemaField.definition", "completion": "        return {\n            'AttributeName': self.name,\n            'AttributeType': self.data_type,\n        }", "idx": 758}
{"namespace": "boto.dynamodb2.fields.BaseIndexField.definition", "completion": "        definition = []\n        for part in self.parts:\n            definition.append(part.definition())\n        return definition", "idx": 759}
{"namespace": "boto.dynamodb2.fields.BaseIndexField.schema", "completion": "        schema_data = {\n            'IndexName': self.name,\n            'KeySchema': self.definition(),\n            'Projection': {\n                'ProjectionType': self.projection_type,\n            },\n        }\n        return schema_data", "idx": 760}
{"namespace": "boto.dynamodb2.fields.GlobalBaseIndexField.schema", "completion": "        schema_data = super(GlobalBaseIndexField, self).schema()\n        schema_data['ProvisionedThroughput'] = self.throughput\n        return schema_data", "idx": 761}
{"namespace": "boto.dynamodb2.fields.GlobalIncludeIndex.schema", "completion": "        schema_data = super(GlobalIncludeIndex, self).schema()\n        schema_data['Projection']['NonKeyAttributes'] = self.includes_fields\n        return schema_data", "idx": 762}
{"namespace": "boto.dynamodb2.items.Item.get_keys", "completion": "        key_data = {}\n\n        for key, value in self._data.items():\n            if key in self.table.key_fields:\n                key_data[key] = value\n\n        return key_data", "idx": 763}
{"namespace": "boto.dynamodb2.items.Item.get_raw_keys", "completion": "        key_fields = self.table.get_key_fields()\n        key_data = {}\n\n        for key in key_fields:\n            key_data[key] = self._dynamizer.encode(self[key])\n\n        return key_data\n", "idx": 764}
{"namespace": "boto.dynamodb2.items.Item.build_expects", "completion": "        if fields is None:\n            fields = self.table.get_fields()\n\n        expects = {}\n\n        for field in fields:\n            if field not in self._data:\n                continue\n\n            if field not in self._orig_data:\n                # New field.\n                expects[field] = {\n                    'Action': 'PUT',\n                    'Value': self._dynamizer.encode(self._data[field])\n                }\n            elif self._data[field] != self._orig_data[field]:\n                # Modified field.\n                expects[field] = {\n                    'Action': 'PUT',\n                    'Value': self._dynamizer.encode(self._data[field])\n                }\n            elif self._data[field] == self._orig_data[field]:\n                # Unchanged field.\n                expects[field] = {\n                    'Action': 'NOT_NULL',\n                }\n\n        return expects", "idx": 765}
{"namespace": "boto.dynamodb2.items.Item.prepare_full", "completion": "        final_data = {}\n\n        for key, value in self._data.items():\n            if self._is_storable(value):\n                final_data[key] = self._dynamizer.encode(value)\n\n        return final_data", "idx": 766}
{"namespace": "boto.dynamodb2.items.Item.prepare_partial", "completion": "        # This doesn't save on its own. Rather, we prepare the datastructure\n        # and hand-off to the table to handle creation/update.\n        final_data = {}\n        fields = set()\n\n        for key, value in self._data.items():\n            if not self._is_storable(value):\n                continue\n\n            if key in self._orig_data and self._orig_data[key] == value:\n                continue\n\n            final_data[key] = self._dynamizer.encode(value)\n            fields.add(key)\n\n        return final_data, fields", "idx": 767}
{"namespace": "boto.dynamodb2.items.Item.partial_save", "completion": "        if not self.needs_save():\n            return False\n\n        final_data, fields = self.prepare_partial()\n        key_data = self.get_keys()\n        expects = self.build_expects(fields)\n\n        returned = self.table._update_item(key_data, final_data, expects=expects)\n        # Mark the object as clean.\n        self.mark_clean()\n        return returned", "idx": 768}
{"namespace": "boto.dynamodb2.items.Item.save", "completion": "        key = self.get_keys()\n        final_data = self.prepare_full()\n        if not final_data:\n            return False\n        if not overwrite:\n            if self.needs_save():\n                return False\n        self.table._put_item(final_data)\n        self.mark_clean()\n        return True", "idx": 769}
{"namespace": "boto.dynamodb2.items.Item.delete", "completion": "        key = self.get_keys()\n        self.table._delete_item(key)\n        return True", "idx": 770}
{"namespace": "boto.dynamodb2.connect_to_region", "completion": "    from boto.dynamodb2.layer1 import DynamoDBConnection\n    return DynamoDBConnection(region=RegionInfo(name=region_name), **kw_params)", "idx": 771}
{"namespace": "litecli.packages.parseutils.extract_tables", "completion": "    parsed = sqlparse.parse(sql)[0]\n    from_seen = False\n    for item in parsed.tokens:\n        if from_seen:\n            if is_subselect(item):\n                for identifier in extract_table_identifiers(\n                    extract_from_part(item)\n                ):\n                    yield identifier\n            elif item.ttype is Keyword and item.value.upper() in (\n                \"SELECT\",\n                \"INSERT\",\n                \"UPDATE\",\n                \"CREATE\",\n                \"DELETE\",\n            ):\n                return\n            elif item.ttype is Punctuation:\n                return\n            else:\n                return\n        elif item.ttype is Keyword and item.value.upper() in (\n            \"FROM\",\n            \"INTO\",\n            \"UPDATE\",\n            \"TABLE\",\n            \"JOIN\",\n        ):\n            from_seen = True\n\n", "idx": 772}
{"namespace": "litecli.packages.parseutils.queries_start_with", "completion": "    for query in queries:\n        if query_starts_with(query, prefixes):\n            return True\n    return False", "idx": 773}
{"namespace": "litecli.packages.parseutils.is_destructive", "completion": "    destructive_keywords = [\n        \"DROP\",\n        \"TRUNCATE\",\n        \"DELETE\",\n        \"RENAME\",\n        \"ALTER\",\n        \"REPLACE\",\n        \"UPDATE\",\n        \"INSERT\",\n        \"CREATE\",\n        \"DROP\",\n        \"TRUNCATE\",\n        \"DELETE\",\n        \"RENAME\",\n        \"ALTER\",\n        \"REPLACE\",\n        \"UPDATE\",\n        \"INSERT\",\n        \"CREATE\",\n        \"DROP\",\n        \"TRUNCATE\",\n        \"DELETE\",\n        \"RENAME\",\n        \"ALTER\",\n        \"REPLACE\",\n        \"UPDATE\",\n        \"INSERT\",\n        \"CREATE\",\n        \"DROP\",\n        \"TRUNCATE\",\n        \"DELETE\",\n        \"RENAME\",\n        \"ALTER\",\n        \"REPLACE\",\n        \"UPDATE\",\n        \"INSERT\",\n        \"CREATE\",\n        \"DROP\",\n        \"TRUNCATE\",\n        \"DELETE\",\n        \"RENAME\",\n        \"ALTER\",\n        \"REPLACE\",\n        \"UPDATE\",\n        \"INSERT\",\n        \"CREATE\",\n        \"DROP\",\n        \"TRUNCATE\",\n        \"DELETE\",\n        \"RENAME\",\n        \"ALTER\",\n        \"REPLACE\",\n        \"UPDATE\",\n        \"INSERT\",\n        \"CREATE\",\n        \"DROP\",\n        \"TRUNCATE\",\n        \"DELETE\",\n        \"RENAME\",\n        \"ALTER\",\n        \"REPLACE\",\n        \"UPDATE\",\n        \"INSERT\",\n        \"CREATE\",\n        \"DROP\",\n        \"TRUNCATE\",\n        \"DELETE\",\n        \"RENAME\",\n        \"ALTER\",\n        \"REPLACE\",\n        \"UPDATE\",\n        \"INSERT\",\n        \"CREATE\",\n        \"DROP\",\n        \"TRUNCATE\",\n        \"DELETE\",\n        \"RENAME\",\n        \"ALTER\",\n        \"REPLACE\",\n        \"UPDATE\",\n        \"INSERT\",\n        \"CREATE\",\n        \"DROP\",\n        \"TRUNCATE\",", "idx": 774}
{"namespace": "litecli.packages.completion_engine.suggest_type", "completion": "    if not text_before_cursor:\n        return [{\"type\": \"keyword\"}, {\"type\": \"special\"}]\n\n    # Parse the text before the cursor\n    p = sqlparse.parse(text_before_cursor)[0]\n\n    # Get the last token\n    token = p.token_last()\n\n    # Get the identifier that the token belongs to\n    identifier = token.get_identifiers()[0] if token else None\n\n    # Get the type of the last token\n    return suggest_based_on_last_token(token, text_before_cursor, full_text, identifier)\n\n", "idx": 775}
{"namespace": "datasette.plugins.get_plugins", "completion": "    plugins = []\n    for ep in pkg_resources.iter_entry_points(\"datasette.plugins\"):\n        mod = ep.load()\n        plugin = {\n            \"name\": ep.name,\n            \"static_path\": getattr(mod, \"static_path\", None),\n            \"templates_path\": getattr(mod, \"templates_path\", None),\n            \"hooks\": getattr(mod, \"hooks\", None),\n        }\n        if hasattr(mod, \"__version__\"):\n            plugin[\"version\"] = mod.__version__\n        if hasattr(mod, \"__project_name__\"):\n            plugin[\"project_name\"] = mod.__project_name__\n        plugins.append(plugin)\n    return plugins", "idx": 776}
{"namespace": "datasette.facets.ColumnFacet.suggest", "completion": "        columns = await self.get_columns(self.sql, self.params)\n        suggested_facets = []\n        already_enabled = [c[\"config\"][\"simple\"] for c in self.get_configs()]\n        for column in columns:\n            if column in already_enabled:\n                continue\n            # Is this column a string?\n            suggested_facet_sql = \"\"\"\n                select distinct typeof({column})\n                from ({sql})\n                where {column} is not null and {column} != ''\n            \"\"\".format(\n                column=escape_sqlite(column), sql=self.sql\n            )\n            try:\n                results = await self.ds.execute(\n                    self.database,\n                    suggested_facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_suggest_time_limit_ms\"),\n                    log_sql_errors=False,\n                )\n                types = tuple(r[0] for r in results.rows)\n                if types in ((\"text\",), (\"text\", None)):\n                    suggested_facets.append(\n                        {\n                            \"name\": column,\n                            \"type\": \"column\",\n                            \"toggle_url\": self.ds.absolute_url(\n                                self.request,\n                                self.ds.urls.path(\n                                    path_with_added_args(\n                                        self.request, {\"_facet\": column}\n                                    )\n                                ),\n                            ),\n                        }\n                    )\n            except (QueryInterrupted, sqlite3.OperationalError):\n                continue\n        return suggested_facets", "idx": 777}
{"namespace": "datasette.facets.ColumnFacet.facet_results", "completion": "        facet_results = []\n        facets_timed_out = []\n        args = dict(self.get_querystring_pairs())\n        facet_size = self.get_facet_size()\n        for source_and_config in self.get_configs():\n            config = source_and_config[\"config\"]\n            source = source_and_config[\"source\"]\n            column = config.get(\"column\") or config[\"simple\"]\n            facet_sql = \"\"\"\n                select {col} as value, count(*) as count from (\n                    {sql}\n                )\n                where {col} is not null\n                group by {col} order by count desc, value limit {limit}\n            \"\"\".format(\n                col=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            try:\n                facet_rows_results = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n                facet_results_values = []\n                facet_results.append(\n                    {\n                        \"name\": column,\n                        \"type\": self.type,\n                        \"results\": facet_results_values,\n                        \"hideable\": source != \"metadata\",\n                        \"toggle_url\": self.ds.urls.path(\n                            path_with_removed_args(\n                                self.request, {\"_facet\": column}\n                            )\n                        ),\n                        \"truncated\": len(facet_rows_results) > facet_size,\n                    }\n                )\n                facet_rows = facet_rows_results.rows[:facet_size]\n                for row in facet_rows:\n                    selected = str(args.get(column)) == str(row[\"value\"])\n                    if selected:\n                        toggle_path = path_with_removed_args(\n                            self.request, {column: str(row[\"value\"])}\n                        )\n                    else", "idx": 778}
{"namespace": "datasette.facets.ArrayFacet.suggest", "completion": "        row_count = await self.get_row_count()\n        columns = await self.get_columns(self.sql, self.params)\n        facet_size = self.get_facet_size()\n        suggested_facets = []\n        already_enabled = [c[\"config\"][\"simple\"] for c in self.get_configs()]\n        for column in columns:\n            if column in already_enabled:\n                continue\n            suggested_facet_sql = \"\"\"\n                select {column} as value, count(*) as n from (\n                    {sql}\n                ) where value is not null\n                group by value\n                limit {limit}\n            \"\"\".format(\n                column=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            distinct_values = None\n            try:\n                distinct_values = await self.ds.execute(\n                    self.database,\n                    suggested_facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_suggest_time_limit_ms\"),\n                )\n                num_distinct_values = len(distinct_values)\n                if (\n                    1 < num_distinct_values < row_count\n                    and num_distinct_values <= facet_size\n                    # And at least one has n > 1\n                    and any(r[\"n\"] > 1 for r in distinct_values)\n                ):\n                    suggested_facets.append(\n                        {\n                            \"name\": column,\n                            \"toggle_url\": self.ds.absolute_url(\n                                self.request,\n                                self.ds.urls.path(\n                                    path_with_added_args(\n                                        self.request, {\"_facet_array\": column}\n                                    )\n                                ),\n                            ),\n                        }\n                    )\n            except QueryInterrupted:\n                continue\n        return suggested_facets", "idx": 779}
{"namespace": "datasette.facets.ArrayFacet.facet_results", "completion": "        facet_results = []\n        facets_timed_out = []\n        args = dict(self.get_querystring_pairs())\n        facet_size = self.get_facet_size()\n        for source_and_config in self.get_configs():\n            config = source_and_config[\"config\"]\n            source = source_and_config[\"source\"]\n            column = config.get(\"column\") or config[\"simple\"]\n            facet_sql = \"\"\"\n                select value, count(*) as count from (\n                    select json_extract(json_array_elements({column}), '$') as value\n                    from ({sql})\n                    where {column} is not null and {column} != ''\n                )\n                group by value order by count desc, value limit {limit}\n            \"\"\".format(\n                column=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            try:\n                facet_rows_results = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n                facet_results_values = []\n                facet_results.append(\n                    {\n                        \"name\": column,\n                        \"type\": self.type,\n                        \"results\": facet_results_values,\n                        \"hideable\": source != \"metadata\",\n                        \"toggle_url\": path_with_removed_args(\n                            self.request, {\"_facet_array\": column}\n                        ),\n                        \"truncated\": len(facet_rows_results) > facet_size,\n                    }\n                )\n                facet_rows = facet_rows_results.rows[:facet_size]\n                for row in facet_rows:\n                    selected = str(args.get(f\"{column}__array\")) == str(row[\"value\"])\n                    if selected:\n                        toggle_path = path_with_removed_args(\n                            self.", "idx": 780}
{"namespace": "datasette.facets.DateFacet.facet_results", "completion": "        facet_results = []\n        facets_timed_out = []\n\n        facet_size = self.get_facet_size()\n        for source_and_config in self.get_configs():\n            config = source_and_config[\"config\"]\n            source = source_and_config[\"source\"]\n            column = config.get(\"column\") or config[\"simple\"]\n            facet_sql = \"\"\"\n                select\n                    date({column}) as value,\n                    count(*) as count\n                from ({sql})\n                where {column} glob \"????-??-*\"\n                group by value\n                order by count desc, value limit {limit}\n            \"\"\".format(\n                column=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            try:\n                facet_rows_results = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n                facet_results_values = []\n                facet_results.append(\n                    {\n                        \"name\": column,\n                        \"type\": self.type,\n                        \"results\": facet_results_values,\n                        \"hideable\": source != \"metadata\",\n                        \"toggle_url\": self.ds.urls.path(\n                            path_with_removed_args(\n                                self.request, {\"_facet_date\": column}\n                            )\n                        ),\n                        \"truncated\": len(facet_rows_results) > facet_size,\n                    }\n                )\n                facet_rows = facet_rows_results.rows[:facet_size]\n                pairs = self.get_querystring_pairs()\n                for row in facet_rows:\n                    value = str(row[\"value\"])\n                    selected = (f\"{column}__date\", value) in pairs\n                    if selected:\n                        toggle_path = path_with_removed_args(\n                            self.request, {", "idx": 781}
{"namespace": "datasette.app.Datasette.invoke_startup", "completion": "        if self._startup_invoked:\n            return\n        self._startup_invoked = True\n        # Resolve secrets\n        self._secret = resolve_env_secrets(self._secret, os.environ)\n        # Resolve routes\n        self.routes = resolve_routes(self._routes())\n        # Resolve plugins\n        for plugin in get_plugins():\n            plugin[\"plugin\"].setup(self)\n        # Resolve settings\n        for key, value in self._settings.items():\n            if key in OBSOLETE_SETTINGS:\n                raise StartupError(OBSOLETE_SETTINGS[key])\n            self._settings[key] = resolve_env_secrets(value, os.environ)\n        # Resolve metadata\n        for db in self.databases.values():\n            db.metadata = self.metadata(database=db.name)\n            db.update_with_inherited_metadata(db.metadata)\n        # Resolve permissions\n        for hook in pm.hook.register_permissions(datasette=self):\n            hook(self)\n        # Resolve hooks\n        for hook in pm.hook.register_output_renderer(datasette=self):\n            hook(self)\n        # Resolve hooks\n        for hook in pm.hook.register_routes(datasette=self):\n            hook(self)\n        # Resolve hooks\n        for hook in pm.hook.register_custom_units(datasette=self):\n            hook(self)\n        # Resolve hooks\n        for hook in pm.hook.register_custom_css(datasette=self):\n            hook(self)\n        # Resolve hooks\n        for hook in pm.hook.register_custom_js(datasette=self):\n            hook(self)\n        # Resolve hooks\n        for hook in pm.hook.register_extra_css_urls(datasette=self):\n            hook(self)\n        # Resolve hooks\n        for hook in pm.hook.register_extra_", "idx": 782}
{"namespace": "datasette.app.Datasette.get_database", "completion": "        if route is not None:\n            for db in self.databases.values():\n                if db.route == route:\n                    return db\n        if name is not None:\n            return self.databases.get(name)\n        for db in self.databases.values():\n            if not db.is_memory:\n                return db\n", "idx": 783}
{"namespace": "datasette.app.Datasette.add_database", "completion": "        if name is None:\n            name = secrets.token_hex(4)\n        if route is None:\n            route = name\n        if name in self.databases:\n            # Append a number to the name to make it unique\n            i = 1\n            while True:\n                new_name = \"{}-{}\".format(name, i)\n                if new_name not in self.databases:\n                    name = new_name\n                    break\n                i += 1\n        if name in self.databases:\n            raise ValueError(\"Database name {} already exists\".format(name))\n        if route in self.databases:\n            raise ValueError(\"Route {} already exists\".format(route))\n        self.databases[name] = db\n        db.name = name\n        db.route = route\n        return db\n", "idx": 784}
{"namespace": "datasette.app.Datasette.ensure_permissions", "completion": "        for action, resource in permissions:\n            if not await self.permission_allowed(actor, action, resource):\n                raise Forbidden(\n                    \"You do not have permission to {} {}\".format(\n                        action, resource or \"this resource\"\n                    )\n                )\n\n    ", "idx": 785}
{"namespace": "datasette.app.Datasette.check_visibility", "completion": "        if actor is None:\n            return False, False\n        if permissions is None:\n            permissions = [(action, resource)]\n        for permission in permissions:\n            if isinstance(permission, str):\n                action = permission\n                resource = None\n            elif isinstance(permission, (tuple, list)) and len(permission) == 2:\n                action, resource = permission\n            else:\n                assert (\n                    False\n                ), \"permission should be string or tuple of two items: {}\".format(\n                    repr(permission)\n                )\n            ok = await self.permission_allowed(\n                actor,\n                action,\n                resource=resource,\n                default=None,\n            )\n            if ok is not None:\n                if ok:\n                    return True, False\n                else:\n                    return False, True\n        return False, False\n", "idx": 786}
{"namespace": "datasette.app.Datasette.render_template", "completion": "        if not self._startup_invoked:\n            raise StartupError(\"Datasette.render_template() called before startup\")\n        if isinstance(templates, str):\n            templates = [templates]\n        if isinstance(templates, Template):\n            templates = [templates]\n        if not isinstance(templates, list):\n            raise ValueError(\"templates must be a string or list of strings\")\n        if context is None:\n            context = {}\n        if request is not None:\n            context.update(request.url_vars)\n        if view_name is not None:\n            context[\"view_name\"] = view_name\n        # Add some variables to the context\n        context.update(\n            {\n                \"datasette\": self,\n                \"request\": request,\n                \"settings\": self.settings_dict(),\n                \"urls\": self.urls,\n                \"base_url\": self.setting(\"base_url\"),\n                \"app_css_hash\": self.app_css_hash(),\n                \"csrf_token\": request.csrf_token,\n                \"csrf_token_input\": Markup(\n                    '<input type=\"hidden\" name=\"_csrf_token\" value=\"{}\">'.format(\n                        request.csrf_token\n                    )\n                ),\n                \"csrf_token_meta\": Markup(\n                    '<meta name=\"_csrf_token\" content=\"{}\">'.format(request.csrf_token)\n                ),\n                \"csrf_token_header\": Markup(\n                    '<meta name=\"_csrf_token\" content=\"{}\">'.format(request.csrf_token)\n                ),\n                \"csrf_token_cookie\": Markup(\n                    '<meta name=\"_csrf_token\" content=\"{}\">'.format(request.csrf_token)\n                ),\n                \"csrf_token_javascript\": Markup(\n                    '<script>window._csrf_token = \"{}\";</script>'.format(\n                        request.csrf_token\n                    )\n                ),\n                \"csrf", "idx": 787}
{"namespace": "datasette.app.DatasetteClient.get", "completion": "        async with httpx.AsyncClient(app=self.app) as client:\n            return await client.get(self._fix(path), **kwargs)\n", "idx": 788}
{"namespace": "datasette.utils.asgi.Request.full_path", "completion": "        if self.scope.get(\"raw_path\") is not None:\n            return self.scope[\"raw_path\"].decode(\"latin-1\")\n        else:\n            path = self.scope[\"path\"]\n            if isinstance(path, str):\n                return path\n            else:\n                return path.decode(\"utf-8\")\n", "idx": 789}
{"namespace": "datasette.utils.asgi.Request.post_body", "completion": "        body = b\"\"\n        more_body = True\n        while more_body:\n            message = await self.receive()\n            body += message.get(\"body\", b\"\")\n            more_body = message.get(\"more_body\", False)\n        return body", "idx": 790}
{"namespace": "datasette.utils.asgi.Request.fake", "completion": "        if url_vars is None:\n            url_vars = {}\n        if \"?\" in path_with_query_string:\n            path, query_string = path_with_query_string.split(\"?\")\n        else:\n            path = path_with_query_string\n            query_string = \"\"\n        return cls(\n            {\n                \"method\": method,\n                \"path\": path,\n                \"query_string\": query_string.encode(\"latin-1\"),\n                \"headers\": [\n                    (b\"host\", b\"localhost\"),\n                    (b\"content-type\", b\"application/x-www-form-urlencoded\"),\n                ],\n                \"url_route\": {\"kwargs\": url_vars},\n                \"scheme\": scheme,\n            },\n            lambda: None,\n        )\n\n", "idx": 791}
{"namespace": "datasette.utils.asgi.Response.asgi_send", "completion": "        headers = {k: v for k, v in self.headers.items() if k.lower() != \"content-type\"}\n        headers[\"content-type\"] = self.content_type\n        await send(\n            {\n                \"type\": \"http.response.start\",\n                \"status\": self.status,\n                \"headers\": [\n                    [key.encode(\"utf-8\"), value.encode(\"utf-8\")]\n                    for key, value in headers.items()\n                ],\n            }\n        )\n        if self._set_cookie_headers:\n            await send(\n                {\n                    \"type\": \"http.response.start\",\n                    \"status\": self.status,\n                    \"headers\": [\n                        [b\"set-cookie\", value.encode(\"utf-8\")]\n                        for value in self._set_cookie_headers\n                    ],\n                }\n            )\n        if self.body:\n            await send({\"type\": \"http.response.body\", \"body\": self.body.encode(\"utf-8\")})\n\n    ", "idx": 792}
{"namespace": "datasette.utils.asgi.Response.set_cookie", "completion": "        if samesite not in SAMESITE_VALUES:\n            raise ValueError(\n                \"samesite should be one of {}\".format(\", \".join(SAMESITE_VALUES))\n            )\n        cookie = Morsel()\n        cookie[\"path\"] = path\n        if domain:\n            cookie[\"domain\"] = domain\n        if secure:\n            cookie[\"secure\"] = \"\"\n        if httponly:\n            cookie[\"httponly\"] = \"\"\n        if samesite != \"none\":\n            cookie[\"samesite\"] = samesite\n        if max_age:\n            cookie[\"max-age\"] = max_age\n        if expires:\n            cookie[\"expires\"] = expires\n        cookie[key] = value\n        self._set_cookie_headers.append(cookie.OutputString())\n\n", "idx": 793}
{"namespace": "datasette.utils.asgi.Response.html", "completion": "        return cls(\n            str(body),\n            status=status,\n            headers=headers,\n            content_type=\"text/html; charset=utf-8\",\n        )", "idx": 794}
{"namespace": "datasette.utils.asgi.Response.text", "completion": "        return cls(\n            body,\n            status=status,\n            headers=headers,\n            content_type=\"text/plain; charset=utf-8\",\n        )\n\n    ", "idx": 795}
{"namespace": "datasette.utils.asgi.Response.json", "completion": "        return cls(\n            json.dumps(body, default=default),\n            status=status,\n            headers=headers,\n            content_type=\"application/json; charset=utf-8\",\n        )\n", "idx": 796}
{"namespace": "datasette.utils.asgi.Response.redirect", "completion": "        headers = headers or {}\n        headers[\"Location\"] = path\n        return cls(\n            \"\",\n            status=status,\n            headers=headers,\n            content_type=\"text/html; charset=utf-8\",\n        )\n\n", "idx": 797}
{"namespace": "datasette.utils.validate_sql_select", "completion": "    sql = sql.strip()\n    if not sql.lower().startswith(\"select\"):\n        raise InvalidSql(\"SQL statement must start with SELECT\")\n    sql = re.sub(comment_re, \"\", sql, flags=re.IGNORECASE)\n    sql = sql.lower()\n    for pattern in allowed_sql_res:\n        if pattern.match(sql):\n            return\n    for pattern, error in disallawed_sql_res:\n        if pattern.match(sql):\n            raise InvalidSql(error)\n    raise InvalidSql(\"SQL statement did not match any allowed patterns\")\n\n", "idx": 798}
{"namespace": "datasette.utils.escape_css_string", "completion": "    return _css_re.sub(lambda m: \"\\\\\" + m.group(0), s)", "idx": 799}
{"namespace": "datasette.utils.detect_fts", "completion": "    #", "idx": 800}
{"namespace": "datasette.utils.is_url", "completion": "    if not isinstance(value, str):\n        return False\n    if not value.startswith((\"http://\", \"https://\")):\n        return False\n    if whitespace_re.search(value):\n        return False\n    return True\n\n", "idx": 801}
{"namespace": "datasette.utils.to_css_class", "completion": "    if not isinstance(s, str):\n        raise TypeError(\"to_css_class() expects a string\")\n    if css_class_re.match(s):\n        return s\n    s = css_invalid_chars_re.sub(\"\", s)\n    if not s:\n        raise ValueError(\"to_css_class() expects a non-empty string\")\n    return f\"{s}-{hashlib.md5(s.encode('utf8')).hexdigest()[:6]}\"", "idx": 802}
{"namespace": "datasette.utils.escape_fts", "completion": "    if not query:\n        return query\n    if query.startswith('\"'):\n        query = query[1:]\n    if query.endswith('\"'):\n        query = query[:-1]\n    terms = _escape_fts_re.split(query)\n    terms = [term for term in terms if term]\n    if len(terms) == 1:\n        return f'\"{terms[0]}\"'\n    return '\"' + '\"' + '\"'.join(terms) + '\"'", "idx": 803}
{"namespace": "datasette.utils.check_connection", "completion": "    try:\n        conn.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n    except sqlite3.OperationalError as e:\n        if \"no such table\" in str(e):\n            raise ConnectionProblem(\"No such table\")\n        elif \"unable to open database file\" in str(e):\n            raise ConnectionProblem(\"Unable to open database file\")\n        elif \"unable to open database file\" in str(e):\n            raise ConnectionProblem(\"Unable to open database file\")\n        elif \"unable to open database file\" in str(e):\n            raise ConnectionProblem(\"Unable to open database file\")\n        elif \"unable to open database file\" in str(e):\n            raise ConnectionProblem(\"Unable to open database file\")\n        elif \"unable to open database file\" in str(e):\n            raise ConnectionProblem(\"Unable to open database file\")\n        elif \"unable to open database file\" in str(e):\n            raise ConnectionProblem(\"Unable to open database file\")\n        elif \"unable to open database file\" in str(e):\n            raise ConnectionProblem(\"Unable to open database file\")\n        elif \"unable to open database file\" in str(e):\n            raise ConnectionProblem(\"Unable to open database file\")\n        elif \"unable to open database file\" in str(e):\n            raise ConnectionProblem(\"Unable to open database file\")\n        elif \"unable to open database file\" in str(e):\n            raise ConnectionProblem(\"Unable to open database file\")\n        elif \"unable to open database file\" in str(e):\n            raise ConnectionProblem(\"Unable to open database file\")\n        elif \"unable to open database file\" in str(e):\n            raise ConnectionProblem(\"Unable to open database file\")\n        elif \"unable to open database file\" in str(e):\n            raise ConnectionProblem(\"Unable to open database file\")\n        elif \"unable to open database file\" in str(e):\n            raise ConnectionProblem(\"Unable to open database file\")\n        elif \"unable to open database file\" in str(e):\n            raise", "idx": 804}
{"namespace": "datasette.utils.parse_metadata", "completion": "    if isinstance(content, dict):\n        return content\n    try:\n        return json.loads(content)\n    except json.JSONDecodeError:\n        try:\n            return yaml.safe_load(content)\n        except yaml.YAMLError:\n            raise BadMetadataError(\"Could not parse metadata as JSON or YAML\")", "idx": 805}
{"namespace": "datasette.utils.call_with_supported_arguments", "completion": "    call_with = _gather_arguments(fn, kwargs)\n    return fn(*call_with)", "idx": 806}
{"namespace": "datasette.utils.derive_named_parameters", "completion": "    sql = sql.strip().rstrip(\";\")\n    possible_named_parameters = _re_named_parameter.findall(sql)\n    if not possible_named_parameters:\n        return possible_named_parameters\n    explain_sql = f\"explain {sql}\"\n    try:\n        explain_results = await db.execute(explain_sql, dict.fromkeys(possible_named_parameters, None))\n    except Exception as e:\n        return possible_named_parameters\n    variables = [row[\"variable\"] for row in explain_results if row[\"variable\"]]\n    return [variable[1:] for variable in variables]", "idx": 807}
{"namespace": "pyramid.path.Resolver.get_package_name", "completion": "        if self.package is CALLER_PACKAGE:\n            package = caller_package()\n        else:\n            package = self.package\n        return package.__name__", "idx": 808}
{"namespace": "pyramid.path.Resolver.get_package", "completion": "        if self.package is CALLER_PACKAGE:\n            package = caller_package()\n        else:\n            package = self.package\n        return package", "idx": 809}
{"namespace": "pyramid.path.DottedNameResolver.resolve", "completion": "", "idx": 810}
{"namespace": "pyramid.path.DottedNameResolver.maybe_resolve", "completion": "        if not isinstance(dotted, str):\n            return dotted\n        package = self.get_package()\n        return self._resolve(dotted, package)\n\n    ", "idx": 811}
{"namespace": "pyramid.path.PkgResourcesAssetDescriptor.abspath", "completion": "        return self.pkg_resources.resource_filename(self.pkg_name, self.path)", "idx": 812}
{"namespace": "pyramid.renderers.render_to_response", "completion": "    try:\n        registry = request.registry\n    except AttributeError:\n        registry = None\n    if package is None:\n        package = caller_package()\n    helper = RendererHelper(\n        name=renderer_name, package=package, registry=registry\n    )\n\n    with hide_attrs(request, 'response'):\n        result = helper.render(value, None, request=request)\n\n    if response is None:\n        response = request.response\n    response.body = result\n    return response", "idx": 813}
{"namespace": "pyramid.renderers.JSON.add_adapter", "completion": "        self.components.registerAdapter(adapter, type_or_iface, IJSONAdapter)", "idx": 814}
{"namespace": "pyramid.renderers.RendererHelper.settings", "completion": "        settings = self.registry.settings\n        if settings is None:\n            settings = {}\n        return settings\n\n    ", "idx": 815}
{"namespace": "pyramid.renderers.RendererHelper.render_view", "completion": "        from pyramid.events import BeforeRender\n        renderer = self.renderer\n        if system_values is None:\n            system_values = {\n                'view': view,\n                'renderer_name': self.name,  # b/c\n                'renderer_info': self,\n                'context': context,\n                'request': request,\n                'req': request,\n                'get_csrf_token': partial(get_csrf_token, request),\n            }\n\n        system_values = BeforeRender(system_values, value)\n\n        registry = self.registry\n        registry.notify(system_values)\n        result = renderer(value, system_values)\n        return self._make_response(result, request)", "idx": 816}
{"namespace": "pyramid.renderers.RendererHelper.render", "completion": "        if request is not None:\n            system_values['request'] = request\n        if system_values is None:\n            system_values = {}\n        system_values.update(self.registry.get_system_values())\n        self.registry.notify('renderer_system_values', system_values)\n        return self.renderer(value, system_values)", "idx": 817}
{"namespace": "pyramid.renderers.RendererHelper.render_to_response", "completion": "        from pyramid.events import BeforeRender\n\n        renderer = self.renderer\n        if system_values is None:\n            system_values = {\n                'view': None,\n                'renderer_name': self.name,  # b/c\n                'renderer_info': self,\n                'context': getattr(request, 'context', None),\n                'request': request,\n                'req': request,\n                'get_csrf_token': partial(get_csrf_token, request),\n            }\n\n        system_values = BeforeRender(system_values, value)\n\n        registry = self.registry\n        registry.notify(system_values)\n        result = renderer(value, system_values)\n        response = self._make_response(result, request)\n        return response\n\n    ", "idx": 818}
{"namespace": "pyramid.renderers.RendererHelper.clone", "completion": "        if name is None:\n            name = self.name\n        if package is None:\n            package = self.package\n        if registry is None:\n            registry = self.registry\n        return RendererHelper(name=name, package=package, registry=registry)", "idx": 819}
{"namespace": "pyramid.urldispatch.RoutesMapper.get_routes", "completion": "        if include_static:\n            return self.routelist + self.static_routes\n        else:\n            return self.routelist\n\n    ", "idx": 820}
{"namespace": "pyramid.urldispatch.RoutesMapper.connect", "completion": "        route = Route(name, pattern, factory, predicates, pregenerator)\n        self.routes[name] = route\n        if static is True:\n            self.static_routes.append(route)\n        else:\n            self.routelist.append(route)\n        return route\n", "idx": 821}
{"namespace": "pyramid.testing.DummyTemplateRenderer.assert_", "completion": "        for k, v in kw.items():\n            if k not in self._received:\n                raise AssertionError(\n                    'Expected key %r not received' % k, self._received\n                )\n            if self._received[k] != v:\n                raise AssertionError(\n                    'Expected value %r for key %r, got %r'\n                    % (v, k, self._received[k]),\n                    self._received,\n                )\n", "idx": 822}
{"namespace": "pyramid.testing.DummyResource.__getitem__", "completion": "        return self.subs[name]\n\n    ", "idx": 823}
{"namespace": "pyramid.testing.DummyResource.clone", "completion": "        clone = copy.copy(self)\n        if __name__ is not _marker:\n            clone.__name__ = __name__\n        if __parent__ is not _marker:\n            clone.__parent__ = __parent__\n        clone.__dict__.update(**kw)\n        return clone", "idx": 824}
{"namespace": "pyramid.testing.DummySession.get_csrf_token", "completion": "", "idx": 825}
{"namespace": "pyramid.testing.DummyRequest.response", "completion": "        if self.response_factory is None:\n            raise ValueError(\n                'No response factory has been set on the request'\n            )\n        return self.response_factory(self)\n", "idx": 826}
{"namespace": "pyramid.testing.DummyRendererFactory.add", "completion": "        self.renderers[spec] = renderer", "idx": 827}
{"namespace": "pyramid.authorization.ACLAuthorizationPolicy.principals_allowed_by_permission", "completion": "        return self.helper.principals_allowed_by_permission(context, permission)", "idx": 828}
{"namespace": "pyramid.url.URLMethodsMixin.route_url", "completion": "        try:\n            reg = self.registry\n        except AttributeError:\n            reg = get_current_registry()  # b/c\n\n        route = reg.queryUtility(IRouteURL, name=route_name)\n        if route is None:\n            raise KeyError('No route named %s' % route_name)\n\n        app_url, qs, anchor = parse_url_overrides(self, kw)\n\n        if '_query' in kw:\n            query = kw.pop('_query')\n            if query:\n                if isinstance(query, str):\n                    qs = '?' + url_quote(query, QUERY_SAFE)\n                else:\n                    qs = '?' + urlencode(query, doseq=True)\n\n        if '_anchor' in kw:\n            anchor = kw.pop('_anchor')\n            if anchor:\n                anchor = '#' + url_quote(anchor, ANCHOR_SAFE)\n\n        return route.generate(self, *elements, **kw) + qs + anchor\n\n    ", "idx": 829}
{"namespace": "pyramid.predicates.CustomPredicate.text", "completion": "        if hasattr(self.func, '__text__'):\n            return self.func.__text__()\n        return 'custom:%s' % self.func.__doc__", "idx": 830}
{"namespace": "pyramid.threadlocal.ThreadLocalManager.pop", "completion": "        return self.stack.pop()\n\n    ", "idx": 831}
{"namespace": "pyramid.threadlocal.ThreadLocalManager.get", "completion": "        if self.stack:\n            return self.stack[-1]\n        else:\n            return self.default()\n\n", "idx": 832}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.authenticated_userid", "completion": "        userid = self.unauthenticated_userid(request)\n        if userid is None:\n            self.debug and self._log(\n                'unauthenticated_userid returned %r; returning None'\n                % (userid,),\n                'authenticated_userid',\n                request,\n            )\n            return None\n\n        if self._clean_principal(userid) is None:\n            self.debug and self._log(\n                (\n                    'unauthenticated_userid returned disallowed %r; returning '\n                    '%r as if it was None' % (userid, None)\n                ),\n                'authenticated_userid',\n                request,\n            )\n            return None\n\n        if self.callback is None:\n            self.debug and self._log(\n                'groupfinder callback is None, so groups is None',\n                'authenticated_userid',\n                request,\n            )\n            groups = None\n        else:\n            groups = self.callback(userid, request)\n            self.debug and self._log(\n                'groupfinder callback returned %r as groups' % (groups,),\n                'authenticated_userid',\n                request,\n            )\n\n        if groups is None:  # is None\n            self.debug and self._log(\n                'returning %r' % (userid,), 'authenticated_userid', request\n            )\n            return userid\n\n        if groups is not None:  # is not None\n            self.debug and self._log(\n                'returning %r' % (userid,), 'authenticated_userid', request\n            )\n            return userid\n\n    ", "idx": 833}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.unauthenticated_userid", "completion": "        identity = self._get_identity(request)\n        if identity is None:\n            self.debug and self._log(\n                'repoze.who identity is None, returning None',\n                'unauthenticated_userid',\n                request,\n            )\n            return None\n        userid = identity['repoze.who.userid']\n        if userid is None:\n            self.debug and self._log(\n                'repoze.who.userid is None, returning None' % userid,\n                'unauthenticated_userid',\n                request,\n            )\n            return None\n        self.debug and self._log(\n            'repoze.who.userid is %r, returning %r' % (userid, userid),\n            'unauthenticated_userid',\n            request,\n        )\n        return userid", "idx": 834}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.forget", "completion": "        identifier = self._get_identifier(request)\n        if identifier is None:\n            return []\n        environ = request.environ\n        return identifier.forget(environ)\n\n", "idx": 835}
{"namespace": "pyramid.authentication.AuthTktAuthenticationPolicy.unauthenticated_userid", "completion": "        return self.cookie.authenticated_userid(request)\n", "idx": 836}
{"namespace": "pyramid.authentication.SessionAuthenticationHelper.remember", "completion": "        request.session[self.userid_key] = userid\n        return []", "idx": 837}
{"namespace": "pyramid.authentication.SessionAuthenticationHelper.forget", "completion": "        if self.userid_key in request.session:\n            del request.session[self.userid_key]\n        return []", "idx": 838}
{"namespace": "pyramid.authentication.BasicAuthAuthenticationPolicy.unauthenticated_userid", "completion": "        credentials = extract_http_basic_credentials(request)\n        if credentials:\n            username, password = credentials\n            return username\n\n    ", "idx": 839}
{"namespace": "pyramid.request.CallbackMethodsMixin._process_response_callbacks", "completion": "        callbacks = self.response_callbacks\n        while callbacks:\n            callback = callbacks.popleft()\n            callback(self, response)", "idx": 840}
{"namespace": "pyramid.request.CallbackMethodsMixin._process_finished_callbacks", "completion": "        callbacks = self.finished_callbacks\n        while callbacks:\n            callback = callbacks.popleft()\n            callback(self)", "idx": 841}
{"namespace": "pyramid.request.Request.session", "completion": "        if not hasattr(self, '_session'):\n            self._session = self.registry.getUtility(ISessionFactory)()\n        return self._session\n", "idx": 842}
{"namespace": "pyramid.request.RequestLocalCache.get_or_create", "completion": "        if creator is None:\n            creator = self._creator\n        value = self.get(request, self.NO_VALUE)\n        if value is self.NO_VALUE:\n            value = creator(request)\n            self.set(request, value)\n        return value", "idx": 843}
{"namespace": "pyramid.request.RequestLocalCache.set", "completion": "        if request not in self._store:\n            self._store[request] = value\n            request.add_finished_callback(self._remove_request)\n", "idx": 844}
{"namespace": "pyramid.i18n.Localizer.pluralize", "completion": "        if self.pluralizer is None:\n            self.pluralizer = Pluralizer(self.translations)\n        return self.pluralizer(\n            singular, plural, n, domain=domain, mapping=mapping\n        )", "idx": 845}
{"namespace": "pyramid.i18n.Translations.load", "completion": "        if dirname is None:\n            dirname = os.path.join(os.path.dirname(__file__), 'locale')\n        if locales is None:\n            locales = [locale.getlocale()[0]]\n        if not isinstance(locales, list):\n            locales = [locales]\n\n        translations = cls()\n        for locale in locales:\n            if isinstance(locale, str):\n                locale = Locale.parse(locale)\n            locale_dir = os.path.join(dirname, locale.language)\n            if not os.path.isdir(locale_dir):\n                continue\n            locale_dir = os.path.join(locale_dir, 'LC_MESSAGES')\n            if not os.path.isdir(locale_dir):\n                continue\n            for filename in os.listdir(locale_dir):\n                if filename.endswith('.mo'):\n                    with open(os.path.join(locale_dir, filename), 'rb') as fp:\n                        catalog = cls(fp, domain)\n                        translations.add(catalog)\n\n        if translations._catalog:\n            return translations\n        else:\n            return gettext.NullTranslations()\n\n    ", "idx": 846}
{"namespace": "pyramid.i18n.Translations.add", "completion": "        if translations.domain != self.domain:\n            if merge:\n                self.merge(translations)\n            else:\n                self._domains[translations.domain] = translations\n        else:\n            self.merge(translations)\n        return self", "idx": 847}
{"namespace": "pyramid.i18n.Translations.merge", "completion": "        for msgid in translations._catalog:\n            if msgid in self._catalog:\n                self._catalog[msgid].update(translations._catalog[msgid])\n            else:\n                self._catalog[msgid] = translations._catalog[msgid]\n        self.files.extend(translations.files)\n        return self", "idx": 848}
{"namespace": "pyramid.i18n.LocalizerRequestMixin.locale_name", "completion": "        return negotiate_locale_name(self)\n\n", "idx": 849}
{"namespace": "pyramid.csrf.LegacySessionCSRFStoragePolicy.check_csrf_token", "completion": "        expected_token = self.get_csrf_token(request)\n        return not strings_differ(\n            bytes_(expected_token), bytes_(supplied_token)\n        )", "idx": 850}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.new_csrf_token", "completion": "        token = self._token_factory()\n        request.session[self.key] = token\n        return token", "idx": 851}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.get_csrf_token", "completion": "        token = request.session.get(self.key)\n        if token is None:\n            token = self.new_csrf_token(request)\n        return token", "idx": 852}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.check_csrf_token", "completion": "        expected_token = self.get_csrf_token(request)\n        return not strings_differ(\n            bytes_(expected_token), bytes_(supplied_token)\n        )", "idx": 853}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.new_csrf_token", "completion": "        token = self._token_factory()\n        bound_cookies = self.cookie_profile.bind(request)\n        bound_cookies.set_value(token)\n        request.add_response_callback(\n            lambda response: bound_cookies.save(response)\n        )\n        return token", "idx": 854}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.get_csrf_token", "completion": "        token = request.cookies.get(self.cookie_name, None)\n        if not token:\n            token = self.new_csrf_token(request)\n        return token", "idx": 855}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.check_csrf_token", "completion": "        expected_token = self.get_csrf_token(request)\n        return not strings_differ(\n            bytes_(expected_token), bytes_(supplied_token)\n        )", "idx": 856}
{"namespace": "pyramid.security.PermitsResult.__repr__", "completion": "        return '<{class_name} instance at {id} with msg {msg}>'.format(\n            class_name=self.__class__.__name__, id=hex(id(self)), msg=self.msg\n        )", "idx": 857}
{"namespace": "pyramid.util.InstancePropertyHelper.make_property", "completion": "        if name is None:\n            name = callable.__name__\n        if reify:\n            callable = reify(callable)\n        return name, property(callable)\n", "idx": 858}
{"namespace": "pyramid.util.InstancePropertyHelper.set_property", "completion": "        name, fn = cls.make_property(callable, name=name, reify=reify)\n        cls.apply_properties(target, {name: fn})\n", "idx": 859}
{"namespace": "pyramid.util.InstancePropertyHelper.add_property", "completion": "        prop = self.make_property(callable, name=name, reify=reify)\n        self.properties[prop[0]] = prop[1]", "idx": 860}
{"namespace": "pyramid.util.InstancePropertyHelper.apply", "completion": "        self.apply_properties(target, self.properties)", "idx": 861}
{"namespace": "pyramid.util.InstancePropertyMixin.set_property", "completion": "        InstancePropertyMixin.set_property(self, callable, name=name, reify=reify)\n", "idx": 862}
{"namespace": "pyramid.util.TopologicalSorter.remove", "completion": "        if name in self.names:\n            self.names.remove(name)\n            self.req_before.discard(name)\n            self.req_after.discard(name)\n            del self.name2val[name]\n            del self.name2before[name]\n            del self.name2after[name]\n            self.order = [\n                (a, b)\n                for a, b in self.order\n                if a != name and b != name\n            ]", "idx": 863}
{"namespace": "pyramid.util.TopologicalSorter.add", "completion": "        if name in self.names:\n            raise ValueError('Duplicate name: %s' % name)\n        self.names.append(name)\n        self.name2val[name] = val\n        if after is None:\n            after = self.default_after\n        if before is None:\n            before = self.default_before\n        if after is not None:\n            if isinstance(after, str):\n                after = [after]\n            for a in after:\n                self.req_after.add(a)\n                self.order.append((a, name))\n                self.name2after.setdefault(a, []).append(name)\n        if before is not None:\n            if isinstance(before, str):\n                before = [before]\n            for b in before:\n                self.req_before.add(b)\n                self.order.append((name, b))\n                self.name2before.setdefault(b, []).append(name)", "idx": 864}
{"namespace": "pyramid.traversal.find_resource", "completion": "    if isinstance(path, str):\n        path = traversal_path_info(path)\n    if not is_nonstr_iter(path):\n        raise TypeError(\n            \"find_resource() argument must be a string or a tuple, not %r\"\n            % (path,)\n        )\n    for segment in path:\n        try:\n            resource = resource[segment]\n        except KeyError:\n            raise\n        except TypeError:\n            raise KeyError(segment)\n    return resource", "idx": 865}
{"namespace": "pyramid.static.ManifestCacheBuster.manifest", "completion": "        if self.reload:\n            if self.exists(self.manifest_path):\n                mtime = self.getmtime(self.manifest_path)\n                if mtime != self._mtime:\n                    self._mtime = mtime\n                    self._manifest = self.get_manifest()\n        return self._manifest\n\n    ", "idx": 866}
{"namespace": "pyramid.registry.Registry.registerSubscriptionAdapter", "completion": "        result = Components.registerSubscriptionAdapter(self, *arg, **kw)\n        self.has_listeners = True\n        return result\n", "idx": 867}
{"namespace": "pyramid.registry.Registry.registerHandler", "completion": "        result = Components.registerHandler(self, *arg, **kw)\n        self.has_listeners = True\n        return result\n", "idx": 868}
{"namespace": "pyramid.registry.Registry.notify", "completion": "        if not self.has_listeners:\n            return\n\n        for event in events:\n            self.notifyListeners(event)\n", "idx": 869}
{"namespace": "pyramid.registry.Introspector.add", "completion": "        if not isinstance(intr, IIntrospectable):\n            raise TypeError(\n                'introspectable must be an instance of IIntrospectable'\n            )\n        category = self._categories.setdefault(intr.category_name, {})\n        if intr.discriminator in category:\n            raise KeyError(\n                'discriminator %r already exists in category %r'\n                % (intr.discriminator, intr.category_name)\n            )\n        category[intr.discriminator] = intr\n        category[intr.discriminator_hash] = intr\n        self._counter += 1\n        intr.order = self._counter", "idx": 870}
{"namespace": "pyramid.registry.Introspector.get", "completion": "        category = self._categories.get(category_name)\n        if category is None:\n            return default\n        return category.get(discriminator, default)", "idx": 871}
{"namespace": "pyramid.registry.Introspector.get_category", "completion": "        category = self._categories.get(category_name, default)\n        if category is default:\n            return default\n        if sort_key is None:\n            sort_key = operator.attrgetter('order')\n        L = []\n        for discriminator, intr in sorted(category.items(), key=sort_key):\n            L.append(\n                {\n                    'introspectable': intr,\n                    'related': self.related(intr),\n                }\n            )\n        return L", "idx": 872}
{"namespace": "pyramid.registry.Introspector.categorized", "completion": "        if sort_key is None:\n            sort_key = operator.attrgetter('order')\n        categories = self._categories.items()\n        categories = sorted(categories, key=lambda x: sort_key(x[1]))\n        return [\n            (category_name, sorted(introspectables, key=sort_key))\n            for category_name, introspectables in categories\n        ]", "idx": 873}
{"namespace": "pyramid.registry.Introspector.remove", "completion": "        category = self._categories.get(category_name)\n        if category is None:\n            raise KeyError(category_name)\n        intr = category.get(discriminator)\n        if intr is None:\n            raise KeyError((category_name, discriminator))\n        del category[discriminator]\n        del category[intr.discriminator_hash]\n        del self._refs[intr]", "idx": 874}
{"namespace": "pyramid.registry.Introspector.relate", "completion": "        introspectables = self._get_intrs_by_pairs(pairs)\n        relatable = ((x, y) for x in introspectables for y in introspectables)\n        for x, y in relatable:\n            L = self._refs.setdefault(x, [])\n            if y not in L:\n                L.append(y)", "idx": 875}
{"namespace": "pyramid.registry.Introspector.related", "completion": "        category_name, discriminator = intr.category_name, intr.discriminator\n        category = self._categories.get(category_name)\n        if category is None:\n            raise KeyError((category_name, discriminator))\n        return self._refs.get(intr, [])", "idx": 876}
{"namespace": "pyramid.registry.Introspectable.discriminator_hash", "completion": "        self._assert_resolved()\n        return hash(self.discriminator)\n", "idx": 877}
{"namespace": "pyramid.registry.Introspectable.__repr__", "completion": "        return '<%s category %r, discriminator %r>' % (\n            self.type_name,\n            self.category_name,\n            self.discriminator,\n        )", "idx": 878}
{"namespace": "pyramid.scripts.proutes.PRoutesCommand._get_mapper", "completion": "        mapper = registry.queryUtility(IRouteRequest)\n        return mapper", "idx": 879}
{"namespace": "pyramid.scripts.pshell.PShellCommand.make_shell", "completion": "        shells = self.find_all_shells()\n        if self.args.python_shell:\n            if self.args.python_shell in shells:\n                return shells[self.args.python_shell]\n            else:\n                raise ValueError(\n                    'could not find a shell named \"%s\"' % self.args.python_shell\n                )\n        else:\n            if self.preferred_shells:\n                for shell in self.preferred_shells:\n                    if shell in shells:\n                        return shells[shell]\n            else:\n                if shells:\n                    return list(shells.values())[0]\n        return self.default_runner", "idx": 880}
{"namespace": "pyramid.config.assets.PackageOverrides.insert", "completion": "        if path.endswith('/'):\n            self.overrides.insert(0, DirectoryOverride(path, source))\n        else:\n            self.overrides.insert(0, FileOverride(path, source))\n\n    ", "idx": 881}
{"namespace": "pyramid.config.assets.PackageOverrides.filtered_sources", "completion": "        for override in self.overrides:\n            yield from override(resource_name)\n\n    ", "idx": 882}
{"namespace": "pyramid.config.assets.PackageOverrides.real_loader", "completion": "        if self._real_loader is None:\n            raise NotImplementedError(\n                'This PackageOverrides instance does not have a real loader'\n            )\n        return self._real_loader\n", "idx": 883}
{"namespace": "pyramid.config.views.MultiView.add", "completion": "        if phash is None:\n            phash = DEFAULT_PHASH\n        if accept is None:\n            accept = None\n        if accept_order is None:\n            accept_order = self.accepts\n        if accept is not None:\n            accept = normalize_accept_offer(accept)\n        if accept in self.media_views:\n            self.media_views[accept].append((order, view, phash))\n        else:\n            self.media_views[accept] = [(order, view, phash)]\n        self.views.append((order, view, phash))\n        if accept not in self.accepts:\n            self.accepts.append(accept)\n            self.accepts.sort(key=accept_order.index)\n", "idx": 884}
{"namespace": "pyramid.config.views.MultiView.get_views", "completion": "        if request.accept:\n            views = []\n            for accept in request.accept:\n                if accept in self.accepts:\n                    for order, view, phash in self.media_views[accept]:\n                        views.append((order, view, phash))\n            views.sort(key=operator.itemgetter(0))\n            return views\n        return self.views\n", "idx": 885}
{"namespace": "pyramid.config.views.MultiView.match", "completion": "        for order, view, phash in self.get_views(request):\n            try:\n                if view.__predicated__(context, request):\n                    return view\n            except AttributeError:\n                pass\n        raise PredicateMismatch(self.name)\n", "idx": 886}
{"namespace": "pyramid.config.views.MultiView.__permitted__", "completion": "        if hasattr(view, '__permission__'):\n            permission = view.__permission__\n        else:\n            permission = self.permission\n        if permission is None:\n            return True\n        if permission == NO_PERMISSION_REQUIRED:\n            return True\n        if not hasattr(request, 'effective_principals'):\n            return False\n        return request.has_permission(permission, context)\n", "idx": 887}
{"namespace": "pyramid.config.views.MultiView.__call_permissive__", "completion": "        view = self.match(context, request)\n        if hasattr(view, '__call_permissive__'):\n            return view.__call_permissive__(context, request)\n        return view(context, request)\n\n", "idx": 888}
{"namespace": "pyramid.config.actions.ActionState.processSpec", "completion": "        if spec in self.actions:\n            return False\n        self.actions.append(spec)\n        return True", "idx": 889}
{"namespace": "pyramid.config.actions.ActionState.action", "completion": "        if kw is None:\n            kw = {}\n\n        action = extra\n        action.update(\n            dict(\n                discriminator=discriminator,\n                callable=callable,\n                args=args,\n                kw=kw,\n                order=order,\n                includepath=includepath,\n                info=info,\n                introspectables=introspectables,\n            )\n        )\n        self.actions.append(action)", "idx": 890}
{"namespace": "pyramid.config.actions.ActionInfo.__str__", "completion": "        if self.file is None:\n            return self.src\n        return 'Line %s of file %s:\\n%s' % (\n            self.line,\n            self.file,\n            '\\n'.join('    ' + line for line in self.src.splitlines()),\n        )", "idx": 891}
{"namespace": "pyramid.config.Configurator.__getattr__", "completion": "        if name in self.registry._directives:\n            c, action_wrap = self.registry._directives[name]\n            if action_wrap:\n                c = self.action(c)\n            return c\n        raise AttributeError(name)", "idx": 892}
{"namespace": "pyramid.config.Configurator.with_package", "completion": "        if not isinstance(package, str):\n            package = package.__name__\n        return self.__class__(\n            registry=self.registry,\n            package=package,\n            settings=self.registry.settings,\n            root_factory=self.registry.root_factory,\n            security_policy=self.registry.security_policy,\n            authentication_policy=self.registry.authentication_policy,\n            authorization_policy=self.registry.authorization_policy,\n            renderers=self.registry.renderers,\n            debug_logger=self.registry.debug_logger,\n            locale_negotiator=self.registry.locale_negotiator,\n            request_factory=self.registry.request_factory,\n            response_factory=self.registry.response_factory,\n            default_permission=self.registry.default_permission,\n            session_factory=self.registry.session_factory,\n            default_view_mapper=self.registry.default_view_mapper,\n            autocommit=self.autocommit,\n            exceptionresponse_view=self.registry.exceptionresponse_view,\n            route_prefix=self.route_prefix,\n            introspection=self.introspection,\n            root_package=self.root_package,\n        )", "idx": 893}
{"namespace": "pyramid.config.Configurator.absolute_asset_spec", "completion": "        if not isinstance(relative_spec, str):\n            return relative_spec\n        if relative_spec.startswith(':'):\n            return relative_spec\n        if relative_spec.startswith('/'):\n            return ':'.join((self.package_name, relative_spec))\n        return ':'.join((self.package_name, relative_spec))", "idx": 894}
{"namespace": "pyramid.config.Configurator.begin", "completion": "        if request is _marker:\n            request = self.manager.get()\n        self.manager.push(self.registry, request)", "idx": 895}
{"namespace": "pyramid.config.Configurator.scan", "completion": "        if package is None:\n            package = caller_package()\n        scanner = venusian.Scanner(\n            categories=categories, onerror=onerror, ignore=ignore, **kw\n        )\n        scanner.scan(package)", "idx": 896}
{"namespace": "pyramid.config.Configurator.make_wsgi_app", "completion": "        self.commit()\n        self.registry.notify(ApplicationCreated(self.registry))\n        global_registries.add(self.registry)\n        return self.registry.get_wsgi_app()", "idx": 897}
{"namespace": "aiohappybase._util.camel_case_to_pep8", "completion": "    return CAPITALS.sub(r'_\\1', name).lower()", "idx": 898}
{"namespace": "kinto.authorization._relative_object_uri", "completion": "    parts = object_uri.split(\"/\")\n    for i in range(len(parts) - 1, 1, -2):\n        parent_uri = \"/\".join(parts[:i])\n        parent_resource_name, _ = _resource_endpoint(parent_uri)\n        if parent_resource_name == resource_name:\n            return parent_uri\n    raise ValueError(f\"{resource_name} not found in {object_uri}\")", "idx": 899}
{"namespace": "kinto.core.openapi.OpenAPI.expose_authentication_method", "completion": "        cls.security_definitions[method_name] = definition\n        for scope in definition[\"scopes\"]:\n            cls.security_roles[scope] = cls.security_roles.get(scope, set())\n            cls.security_roles[scope].add(method_name)\n", "idx": 900}
{"namespace": "kinto.core.openapi.OpenAPI.generate", "completion": "        swagger = {\n            \"host\": self.settings[\"http_host\"],\n            \"schemes\": [self.settings[\"http_scheme\"]],\n            \"securityDefinitions\": self.security_definitions,\n        }\n\n        return super().generate(swagger=swagger)\n", "idx": 901}
{"namespace": "kinto.core.testing.get_user_headers", "completion": "    auth = f\"{user}:{password}\".encode(\"utf-8\")\n    auth = base64.b64encode(auth).decode(\"utf-8\")\n    return {\"Authorization\": f\"Basic {auth}\"}", "idx": 902}
{"namespace": "kinto.core.authorization.RouteFactory.fetch_shared_objects", "completion": "        if self.shared_ids is not None:\n            return self.shared_ids\n\n        if self.on_plural_endpoint:\n            object_id_match = self._object_id_match\n        else:\n            object_id_match = self.permission_object_id\n\n        if object_id_match is None:\n            return None\n\n        bound_perms = get_bound_permissions(object_id_match, perm)\n        self.shared_ids = self._get_accessible_objects(principals, bound_perms)\n        return self.shared_ids", "idx": 903}
{"namespace": "kinto.core.authorization.RouteFactory.get_permission_object_id", "completion": "        # If the request is on a plural endpoint and object_id is provided,\n        # find the object URI by inspecting the \"plural\" service and its sibling \"object\" service.\n        if self.on_plural_endpoint and object_id is not None:\n            object_uri = f\"{self.resource_name}/{object_id}\"\n        else:\n            # Otherwise, use the object URI from the request path.\n            object_uri = request.path\n\n        # If the object URI is a wildcard, replace it with the object_id.\n        if object_uri == \"*\":\n            object_uri = f\"{self.resource_name}/{object_id}\"\n\n        return object_uri", "idx": 904}
{"namespace": "kinto.core.utils.recursive_update_dict", "completion": "    for key, value in changes.items():\n        if key in ignores:\n            if key in root:\n                del root[key]\n        elif isinstance(value, dict):\n            if key not in root:\n                root[key] = {}\n            recursive_update_dict(root[key], value, ignores)\n        else:\n            root[key] = value", "idx": 905}
{"namespace": "kinto.core.utils.native_value", "completion": "    if isinstance(value, str):\n        try:\n            return json.loads(value)\n        except ValueError:\n            pass\n    return value", "idx": 906}
{"namespace": "kinto.core.utils.dict_subset", "completion": "    if not isinstance(d, dict):\n        raise TypeError(\"d must be a dict\")\n    if not isinstance(keys, list):\n        raise TypeError(\"keys must be a list\")\n    if not keys:\n        return {}\n    if len(keys) == 1:\n        return {keys[0]: d.get(keys[0])}\n    result = {}\n    for key in keys:\n        if \".\" in key:\n            key_parts = key.split(\".\")\n            value = d\n            for part in key_parts:\n                value = value.get(part)\n                if value is None:\n                    break\n            if value is not None:\n                result[key] = value\n        else:\n            result[key] = d.get(key)\n    return result", "idx": 907}
{"namespace": "kinto.core.utils.dict_merge", "completion": "    for key, value in b.items():\n        if isinstance(value, dict):\n            if key not in a:\n                a[key] = {}\n            dict_merge(a[key], value)\n        else:\n            a[key] = value\n    return a", "idx": 908}
{"namespace": "kinto.core.utils.find_nested_value", "completion": "    if path in d:\n        return d[path]\n    parts = path.split(\".\")\n    keys = [d]\n    for part in parts:\n        keys = [k.get(part) for k in keys if isinstance(k, dict)]\n    if not keys:\n        return default\n    return find_nested_value(keys[0], \".\".join(parts[len(keys[0]) :]), default)", "idx": 909}
{"namespace": "kinto.core.utils.instance_uri_registry", "completion": "    api_prefix = f\"/{registry.route_prefix}\"\n    path = api_prefix + f\"/{resource_name}-object\"\n\n    q = registry.queryUtility\n    routes_mapper = q(IRoutesMapper)\n\n    fakerequest = Request.blank(path=path)\n    fakerequest.registry = registry\n    info = routes_mapper(fakerequest)\n    matchdict, route = info[\"match\"], info[\"route\"]\n    if route is None:\n        raise ValueError(\"URI has no route\")\n\n    return route.url(**params)", "idx": 910}
{"namespace": "kinto.core.statsd.load_from_config", "completion": "    if statsd_module is None:\n        raise ConfigurationError(\"statsd module is not installed\")\n\n    statsd_url = config.get(\"statsd_url\")\n    if statsd_url is None:\n        raise ConfigurationError(\"statsd_url is not set\")\n\n    parsed_url = urlparse(statsd_url)\n    hostname = parsed_url.hostname\n    port = parsed_url.port\n    prefix = parsed_url.path.strip(\"/\")\n\n    return Client(hostname, port, prefix)", "idx": 911}
{"namespace": "kinto.core.errors.http_error", "completion": "    if errno is None:\n        errno = ERRORS.UNDEFINED.value\n    if code is None:\n        code = httpexception.code\n    if error is None:\n        error = httpexception.title\n    if message is None:\n        message = httpexception.detail\n    if info is None:\n        info = httpexception.explanation\n\n    response = httpexception(\n        json.dumps(\n            {\n                \"code\": code,\n                \"errno\": errno,\n                \"error\": error,\n                \"message\": message,\n                \"info\": info,\n                \"details\": details,\n            }\n        )\n    )\n    response.content_type = \"application/json\"\n    return response", "idx": 912}
{"namespace": "kinto.core.resource.schema.ResourceReponses.get_and_bind", "completion": "        # Get the default schemas for the endpoint type and method\n        default_schemas = self.get_default_schemas(endpoint_type, method)\n\n        # Get the endpoint-specific schemas for the endpoint type and method\n        endpoint_schemas = self.get_endpoint_schemas(endpoint_type, method)\n\n        # Get the method-specific schemas for the endpoint type and method\n        method_schemas = self.get_method_schemas(endpoint_type, method)\n\n        # Merge the default schemas with the endpoint-specific schemas\n        schemas = {**default_schemas, **endpoint_schemas}\n\n        # Merge the schemas with the method-specific schemas\n        schemas = {**schemas, **method_schemas}\n\n        # Clone and bind the schemas\n        cloned_schemas = {}\n        for status_code, schema in schemas.items():\n            cloned_schemas[status_code] = schema.bind(**kwargs)\n\n        return cloned_schemas", "idx": 913}
{"namespace": "kinto.core.resource.Resource.timestamp", "completion": "        try:\n            return self.model.timestamp()\n        except HTTPServiceUnavailable as e:\n            raise http_error(e, errno=ERRORS.STORAGE_ERROR)\n\n    ", "idx": 914}
{"namespace": "kinto.core.resource.Resource.plural_post", "completion": "        self._add_timestamp_header(self.request.response)\n        self._add_cache_header(self.request.response)\n        self._raise_304_if_not_modified()\n        # Plural endpoints are considered resources that always exist\n        self._raise_412_if_modified(obj={})\n\n        # If `data` is not provided, use existing object (or empty if creation)\n        post_object = self.request.validated[\"body\"].get(\"data\", {})\n\n        object_id = post_object.setdefault(self.model.id_field, self.object_id)\n        self._raise_400_if_id_mismatch(object_id, self.object_id)\n\n        new_object = self.process_object(post_object)\n\n        try:\n            existing = self.model.get_object(object_id)\n        except storage_exceptions.ObjectNotFoundError:\n            existing = None\n\n        if existing:\n            self._raise_412_if_modified(existing)\n            obj = self.model.update_object(new_object)\n            self.request.response.status_code = 200\n        else:\n            obj = self.model.create_object(new_object)\n            self.request.response.status_code = 201\n\n        timestamp = obj[self.model.modified_field]\n        self._add_timestamp_header(self.request.response, timestamp=timestamp)\n\n        action = existing and ACTIONS.UPDATE or ACTIONS.CREATE\n        return self.postprocess(obj, action=action, old=existing)", "idx": 915}
{"namespace": "kinto.core.resource.Resource.get", "completion": "        self._raise_400_if_invalid_id(self.object_id)\n        obj = self._get_object_or_404(self.object_id)\n        self._raise_304_if_not_modified(obj)\n        self._raise_412_if_modified(obj)\n\n        self._add_timestamp_header(self.request.response)\n        self._add_cache_header(self.request.response)\n\n        partial_fields = self._extract_partial_fields()\n        if partial_fields:\n            obj = dict_subset(obj, partial_fields)\n\n        return self.postprocess(obj)", "idx": 916}
{"namespace": "kinto.core.resource.Resource.delete", "completion": "        self._raise_400_if_invalid_id(self.object_id)\n        existing = self._get_object_or_404(self.object_id)\n        self._raise_412_if_modified(existing)\n        deleted = self.model.delete_object(self.object_id)\n        timestamp = deleted[self.model.modified_field]\n        self._add_timestamp_header(self.request.response, timestamp=timestamp)\n        action = ACTIONS.DELETE\n        return self.postprocess(deleted, action=action, old=existing)", "idx": 917}
{"namespace": "kinto.core.permission.memory.Permission.add_principal_to_ace", "completion": "        permission_key = f\"permission:{object_id}:{permission}\"\n        object_permission_principals = self._store.get(permission_key, set())\n        object_permission_principals.add(principal)\n        self._store[permission_key] = object_permission_principals\n\n    ", "idx": 918}
{"namespace": "kinto.core.permission.memory.Permission.get_object_permission_principals", "completion": "        permission_key = f\"permission:{object_id}:{permission}\"\n        return self._store.get(permission_key, set())\n\n    ", "idx": 919}
{"namespace": "kinto.core.storage.generators.Generator.match", "completion": "        if self._regexp is None:\n            self._regexp = re.compile(self.regexp)\n        return self._regexp.match(object_id)", "idx": 920}
{"namespace": "kinto.core.storage.postgresql.migrator.MigratorMixin.create_or_migrate_schema", "completion": "        if not self.schema_version:\n            raise AssertionError(f\"PostgreSQL {self.name} schema: No schema version specified.\")\n\n        if not self.schema_file:\n            raise AssertionError(f\"PostgreSQL {self.name} schema: No schema file specified.\")\n\n        if not self.migrations_directory:\n            raise AssertionError(f\"PostgreSQL {self.name} schema: No migrations directory specified.\")\n\n        if not os.path.isdir(self.migrations_directory):\n            raise AssertionError(\n                f\"PostgreSQL {self.name} schema: Migrations directory {self.migrations_directory} does not exist.\"\n            )\n\n        if not os.path.isfile(self.schema_file):\n            raise AssertionError(\n                f\"PostgreSQL {self.name} schema: Schema file {self.schema_file} does not exist.\"\n            )\n\n        start_version = self.get_installed_version()\n        if start_version is None:\n            logger.info(f\"PostgreSQL {self.name} schema: No schema found. Creating new schema.\")\n            self.create_schema(dry_run)\n        else:\n            logger.info(\n                f\"PostgreSQL {self.name} schema: Schema found at version {start_version}. Migrating schema.\"\n            )\n            self.migrate_schema(start_version, dry_run)\n\n    ", "idx": 921}
{"namespace": "kinto.core.views.batch.BatchPayloadSchema.deserialize", "completion": "        if cstruct is colander.null:\n            return cstruct\n\n        defaults = self.get(\"defaults\").deserialize(cstruct.get(\"defaults\"))\n        requests = self.get(\"requests\").deserialize(cstruct.get(\"requests\"))\n\n        for request in requests:\n            for key, value in defaults.items():\n                if key not in request:\n                    request[key] = value\n\n        return {\"requests\": requests}", "idx": 922}
{"namespace": "kinto.plugins.accounts.utils.get_cached_reset_password", "completion": "    hmac_secret = registry.settings[\"userid_hmac_secret\"]\n    cache_key = utils.hmac_digest(hmac_secret, ACCOUNT_RESET_PASSWORD_CACHE_KEY.format(username))\n    cache = registry.cache\n    reset_password = cache.get(cache_key)\n    return reset_password", "idx": 923}
{"namespace": "kinto.plugins.accounts.utils.get_cached_validation_key", "completion": "    hmac_secret = registry.settings[\"userid_hmac_secret\"]\n    cache_key = utils.hmac_digest(hmac_secret, ACCOUNT_VALIDATION_CACHE_KEY.format(username))\n    cache = registry.cache\n    cached_validation_key = cache.get(cache_key)\n    return cached_validation_key", "idx": 924}
{"namespace": "kinto.plugins.accounts.views.validation.on_account_activated", "completion": "    settings = event.request.registry.settings\n    if not settings.get(\"account_validation.enabled\", False):\n        return\n\n    for impacted_object in event.impacted_objects:\n        if impacted_object.action == ACTIONS.UPDATE:\n            old_account = impacted_object.old\n            new_account = impacted_object.new\n            if old_account.get(\"validated\", False) or not new_account.get(\"validated\", False):\n                continue\n            Emailer(event.request, new_account).send_confirmation_email()", "idx": 925}
{"namespace": "kinto.plugins.openid.OpenIDConnectPolicy._verify_token", "completion": "        userinfo_endpoint = self.oid_config[\"userinfo_endpoint\"]\n        headers = {\"Authorization\": f\"Bearer {access_token}\"}\n        response = requests.get(userinfo_endpoint, headers=headers)\n        if response.status_code != 200:\n            self.logger.debug(\n                \"Error while fetching userinfo from %s: %s\", userinfo_endpoint, response.text\n            )\n            return None\n        return response.json()", "idx": 926}
{"namespace": "kinto.plugins.quotas.scripts.rebuild_quotas", "completion": "    for bucket in storage.get_all(resource_name=\"bucket\"):\n        bucket_id = bucket[\"id\"]\n        bucket_record_count = 0\n        bucket_storage_size = 0\n        bucket_collection_count = 0\n        for collection in storage.get_all(\n            resource_name=\"collection\", parent_id=f\"/buckets/{bucket_id}\"\n        ):\n            collection_record_count, collection_storage_size = rebuild_quotas_collection(\n                storage, bucket_id, collection, dry_run\n            )\n            bucket_record_count += collection_record_count\n            bucket_storage_size += collection_storage_size\n            bucket_collection_count += 1\n\n        logger.info(\n            f\"Bucket {bucket_id}. Final size: {bucket_record_count} records, {bucket_storage_size} bytes, {bucket_collection_count} collections.\"\n        )\n        new_quota_info = {\n            \"record_count\": bucket_record_count,\n            \"storage_size\": bucket_storage_size,\n            \"collection_count\": bucket_collection_count,\n        }\n        if not dry_run:\n            storage.update(\n                resource_name=\"quota\",\n                parent_id=f\"/buckets/{bucket_id}\",\n                object_id=COLLECTION_QUOTA_OBJECT_ID,\n                obj=new_quota_info,\n            )", "idx": 927}
{"namespace": "kinto.config.render_template", "completion": "    with codecs.open(template, encoding=\"utf-8\") as f:\n        template_str = f.read()\n\n    with codecs.open(destination, \"w\", encoding=\"utf-8\") as f:\n        f.write(template_str.format(**kwargs))", "idx": 928}
{"namespace": "trafilatura.sitemaps.SitemapObject.extract_sitemap_langlinks", "completion": "        # extract\n        for match in (m[1] for m in islice(HREFLANG_REGEX.finditer(self.content), MAX_LINKS)):\n            # process middle part of the match tuple\n            if match.startswith(self.target_lang):\n                self.handle_link(match)\n        LOGGER.debug('%s sitemaps and %s links with hreflang found for %s', len(self.sitemap_urls), len(self.urls), self.sitemap_url)", "idx": 929}
{"namespace": "trafilatura.sitemaps.SitemapObject.extract_sitemap_links", "completion": "        # compile regex here for modularity and efficiency\n        sitemap_regex = re.compile(r'<loc>(?:<\\!\\[CDATA\\[)?(http.+?)(?:\\]\\]>)?</loc>', re.DOTALL)\n        # extract\n        for match in (m[1] for m in islice(sitemap_regex.finditer(self.content), MAX_LINKS)):\n            self.handle_link(match)\n        LOGGER.debug('%s sitemaps and %s links found for %s', len(self.sitemap_urls), len(self.urls), self.sitemap_url)", "idx": 930}
{"namespace": "trafilatura.sitemaps.SitemapObject.process", "completion": "        if not is_plausible_sitemap(self.sitemap_url, self.content):\n            return\n\n        if self.target_lang:\n            self.extract_sitemap_langlinks()\n            return\n\n        # try to extract links from a TXT file\n        if self.content.startswith('<?xml'):\n            self.extract_sitemap_links()\n            return\n\n        # iterate through content to find links\n        for link in (m[0] for m in islice(DETECT_LINKS.finditer(self.content), MAX_LINKS)):\n            self.handle_link(link)", "idx": 931}
{"namespace": "trafilatura.sitemaps.is_plausible_sitemap", "completion": "    # Check if the URL ends with .xml or .gz\n    if url.endswith(('.xml', '.gz')):\n        return True\n\n    # Check if the contents starts with <?xml\n    if contents is not None and contents.startswith('<?xml'):\n        return True\n\n    # If the URL doesn't end with .xml or .gz, and the contents doesn't start with <?xml,\n    # then the sitemap is not plausible\n    return False", "idx": 932}
{"namespace": "trafilatura.sitemaps.extract_robots_sitemaps", "completion": "    # ", "idx": 933}
{"namespace": "trafilatura.feeds.handle_link_list", "completion": "    output_links = []\n    for link in linklist:\n        # skip if it's a comment feed\n        if BLACKLIST.search(link):\n            continue\n        # skip if it's a relative URL\n        if not check_url(link):\n            link = fix_relative_urls(baseurl, link)\n        # skip if it's not a valid URL\n        if not validate_url(link)[0]:\n            continue\n        # skip if it's not a web page\n        if not check_url(link, 'webpage'):\n            continue\n        # skip if it's not the same domain\n        if domainname not in link:\n            continue\n        # skip if it's not in the target language\n        if target_lang is not None and not check_url(link, target_lang):\n            continue\n        # skip if it's a blacklisted domain\n        if check_url(link, 'blacklist'):\n            continue\n        # skip if it's a blacklisted country\n        if check_url(link, 'country'):\n            continue\n        # skip if it's a blacklisted category\n        if check_url(link, 'category'):\n            continue\n        # skip if it's a blacklisted keyword\n        if check_url(link, 'keyword'):\n            continue\n        # skip if it's a blacklisted keyword\n        if check_url(link, 'keyword'):\n            continue\n        # skip if it's a blacklisted keyword\n        if check_url(link, 'keyword'):\n            continue\n        # skip if it's a blacklisted keyword\n        if check_url(link, 'keyword'):\n            continue\n        # skip if it's a blacklisted keyword\n        if check_url(link, 'keyword'):\n            continue\n        # skip if it's a blacklisted keyword\n        if check_url(link, 'keyword'):\n            continue\n        # skip if it's a blacklisted keyword\n        if check_url(link, 'keyword'):\n            continue\n        # skip if it's a blacklisted keyword\n        if check_url(link, 'keyword'", "idx": 934}
{"namespace": "trafilatura.feeds.find_feed_urls", "completion": "    # get domain name and base URL\n    domainname, baseurl = get_hostinfo(url)\n    # fetch the webpage content\n    htmlstring = fetch_url(url)\n    # check if it's a feed\n    if htmlstring is None:\n        LOGGER.debug('Invalid feed page: %s', url)\n        return []\n    # check if it's a feed\n    if htmlstring.startswith('<?xml'):\n        # extract feed links\n        feed_links = extract_links(htmlstring, domainname, baseurl, url, target_lang)\n        # return feed links\n        return feed_links\n    # determine feed\n    feed_urls = determine_feed(htmlstring, baseurl, url)\n    # return feed URLs\n    return feed_urls", "idx": 935}
{"namespace": "trafilatura.hashing.generate_hash_filename", "completion": "    content = CLEAN_XML.sub(\"\", content)\n    return urlsafe_b64encode(generate_bow_hash(content)).decode()", "idx": 936}
{"namespace": "trafilatura.cli_utils.download_queue_processing", "completion": "    # load download buffer\n    bufferlist, url_store = load_download_buffer(url_store, args.sleep_time)\n    # start several threads\n    with ThreadPoolExecutor(max_workers=args.parallel) as executor:\n        futures = (executor.submit(process_result, htmlstring, args, url, counter, config) for url, htmlstring in bufferlist)\n        # process results from the parallel threads and add them\n        # to the compressed URL dictionary for further processing\n        for future in as_completed(futures):\n            counter = future.result()\n    # return errors and updated counter\n    return url_store.dump_errors(), counter", "idx": 937}
{"namespace": "trafilatura.utils.decode_response", "completion": "    # init\n    htmltext = None\n    # GZip and Brotli test\n    if isinstance(response, HTTPResponse):\n        response = handle_compressed_file(response.data)\n    # encoding\n    for guessed_encoding in detect_encoding(response):\n        try:\n            htmltext = response.decode(guessed_encoding)\n        except (LookupError, UnicodeDecodeError): # VISCII: lookup\n            LOGGER.warning('wrong encoding detected: %s', guessed_encoding)\n            htmltext = None\n        else:\n            break\n    # return original content if nothing else succeeded\n    return htmltext or str(response, encoding='utf-8', errors='replace')", "idx": 938}
{"namespace": "trafilatura.utils.txttocsv", "completion": "    # init\n    csv_string = ''\n    # check input\n    if not isinstance(text, str) or not isinstance(comments, str) or not isinstance(docmeta, dict):\n        return csv_string\n    # init\n    csv_string = ''\n    # check input\n    if not isinstance(text, str) or not isinstance(comments, str) or not isinstance(docmeta, dict):\n        return csv_string\n    # init\n    csv_string = ''\n    # check input\n    if not isinstance(text, str) or not isinstance(comments, str) or not isinstance(docmeta, dict):\n        return csv_string\n    # init\n    csv_string = ''\n    # check input\n    if not isinstance(text, str) or not isinstance(comments, str) or not isinstance(docmeta, dict):\n        return csv_string\n    # init\n    csv_string = ''\n    # check input\n    if not isinstance(text, str) or not isinstance(comments, str) or not isinstance(docmeta, dict):\n        return csv_string\n    # init\n    csv_string = ''\n    # check input\n    if not isinstance(text, str) or not isinstance(comments, str) or not isinstance(docmeta, dict):\n        return csv_string\n    # init\n    csv_string = ''\n    # check input\n    if not isinstance(text, str) or not isinstance(comments, str) or not isinstance(docmeta, dict):\n        return csv_string\n    # init\n    csv_string = ''\n    # check input\n    if not isinstance(text, str) or not isinstance(comments, str) or not isinstance(docmeta, dict):\n        return csv_string\n    # init\n    csv_string = ''\n    # check input\n    if not isinstance(text, str) or not isinstance(comments, str) or not isinstance(docmeta, dict):\n        return csv_string\n    # init\n    csv_string = ''\n   ", "idx": 939}
{"namespace": "trafilatura.utils.is_similar_domain", "completion": "    # remove file extension\n    reference = STRIP_EXTENSION.sub('', reference)\n    new_string = STRIP_EXTENSION.sub('', new_string)\n    # calculate similarity ratio\n    similarity = SequenceMatcher(None, reference, new_string).ratio()\n    # return result\n    return similarity >= threshold", "idx": 940}
{"namespace": "trafilatura.xml.remove_empty_elements", "completion": "    for elem in reversed(tree.iter()):\n        if elem.getchildren() or elem.text or elem.tail:\n            continue\n        parent = elem.getparent()\n        if parent is not None:\n            parent.remove(elem)\n    return tree", "idx": 941}
{"namespace": "trafilatura.xml.strip_double_tags", "completion": "    for element in reversed(tree.iter()):\n        if element.tag in NESTING_WHITELIST:\n            for child in element.getchildren():\n                if child.tag in NESTING_WHITELIST:\n                    element.insert(element.index(child), child)\n                    element.remove(child)\n    return tree", "idx": 942}
{"namespace": "trafilatura.xml.check_tei", "completion": "    # remove empty elements\n    remove_empty_elements(xmldoc)\n    # remove double tags\n    strip_double_tags(xmldoc)\n    # remove unwanted tags\n    remove_unwanted_tags(xmldoc)\n    # remove unwanted attributes\n    remove_unwanted_attributes(xmldoc)\n    # remove unwanted text\n    remove_unwanted_text(xmldoc)\n    # remove unwanted tails\n    remove_unwanted_tails(xmldoc)\n    # remove unwanted elements\n    remove_unwanted_elements(xmldoc)\n    # remove unwanted attributes\n    remove_unwanted_attributes(xmldoc)\n    # remove unwanted text\n    remove_unwanted_text(xmldoc)\n    # remove unwanted tails\n    remove_unwanted_tails(xmldoc)\n    # remove unwanted elements\n    remove_unwanted_elements(xmldoc)\n    # remove unwanted attributes\n    remove_unwanted_attributes(xmldoc)\n    # remove unwanted text\n    remove_unwanted_text(xmldoc)\n    # remove unwanted tails\n    remove_unwanted_tails(xmldoc)\n    # remove unwanted elements\n    remove_unwanted_elements(xmldoc)\n    # remove unwanted attributes\n    remove_unwanted_attributes(xmldoc)\n    # remove unwanted text\n    remove_unwanted_text(xmldoc)\n    # remove unwanted tails\n    remove_unwanted_tails(xmldoc)\n    # remove unwanted elements\n    remove_unwanted_elements(xmldoc)\n    # remove unwanted attributes\n    remove_unwanted_attributes(xmldoc)\n    # remove unwanted text\n    remove_unwanted_text(xmldoc)\n    # remove unwanted tails\n    remove_unwanted_tails(xmldoc)\n    # remove unwanted elements\n    remove_un", "idx": 943}
{"namespace": "trafilatura.xml.validate_tei", "completion": "    global TEI_RELAXNG\n    if TEI_RELAXNG is None:\n        with lzma.open(TEI_SCHEMA, 'rb') as schemafile:\n            TEI_RELAXNG = RelaxNG(load_pickle(schemafile))\n    return TEI_RELAXNG.validate(xmldoc)", "idx": 944}
{"namespace": "trafilatura.xml.merge_with_parent", "completion": "    # get parent\n    parent = element.getparent()\n    # get text\n    text = replace_element_text(element, include_formatting)\n    # add text to parent\n    if parent.text is None:\n        parent.text = text\n    else:\n        parent.text += text\n    # remove element\n    parent.remove(element)", "idx": 945}
{"namespace": "trafilatura.downloads._determine_headers", "completion": "    # read config\n    myagents, mycookie = _parse_config(config)\n    # set headers\n    if headers is None:\n        headers = DEFAULT_HEADERS\n    if myagents is not None:\n        headers['User-Agent'] = random.choice(myagents)\n    if mycookie is not None:\n        headers['Cookie'] = mycookie\n    return headers", "idx": 946}
{"namespace": "trafilatura.meta.reset_caches", "completion": "    reset_caches_courlan()\n    reset_caches_htmldate()\n    define_stoplist()\n    gc.collect()", "idx": 947}
{"namespace": "trafilatura.core.handle_table", "completion": "    newtable = Element('table')\n    newrow = []\n    for child in table_elem.iter('*'):\n        if child.tag == 'tr':\n            if newrow:\n                newtable.append(newrow)\n                newrow = []\n        elif child.tag in ('td', 'th'):\n            newcell = define_cell_type(child)\n            if child.text:\n                newcell.text = child.text\n            if child.tail:\n                newcell.tail = child.tail\n            newrow.append(newcell)\n        elif child.tag in ('thead', 'tbody', 'tfoot'):\n            for subchild in child.iter('*'):\n                if subchild.tag in ('tr', 'td', 'th'):\n                    newcell = define_cell_type(subchild)\n                    if subchild.text:\n                        newcell.text = subchild.text\n                    if subchild.tail:\n                        newcell.tail = subchild.tail\n                    newrow.append(newcell)\n        elif child.tag == 'caption':\n            newcaption = Element('caption')\n            if child.text:\n                newcaption.text = child.text\n            if child.tail:\n                newcaption.tail = child.tail\n            newtable.append(newcaption)\n        elif child.tag == 'colgroup':\n            newcolgroup = Element('colgroup')\n            for subchild in child.iter('*'):\n                if subchild.tag == 'col':\n                    newcol = Element('col')\n                    for attr in subchild.attrib:\n                        newcol.set(attr, subchild.get(attr))\n                    newcolgroup.append(newcol)\n            newtable.append(newcolgroup)\n        elif child.tag == 'col':\n            newcol = Element('col')\n            for attr in child.attrib:\n                newcol.set(attr, child.get(attr))\n            newtable.append(newcol)\n        elif child.tag == 'table':\n            # nested table\n            newsubtable = handle_table(child, potential_tags, options)\n            if", "idx": 948}
{"namespace": "trafilatura.filters.language_filter", "completion": "    if target_language is not None:\n        detected_language = language_classifier(temp_text, temp_comments)\n        if detected_language != target_language:\n            LOGGER.warning('Language %s detected, expected %s', detected_language, target_language)\n            docmeta['language'] = detected_language\n            return True, docmeta\n    return False, docmeta", "idx": 949}
{"namespace": "trafilatura.filters.textfilter", "completion": "    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo: check if this is still needed\n    # todo:", "idx": 950}
{"namespace": "trafilatura.metadata.extract_meta_json", "completion": "    for elem in tree.iterfind('.//script[@type=\"application/ld+json\" or @type=\"application/settings+json\"]'):\n        if elem.text is not None:\n            try:\n                json_data = json.loads(elem.text)\n            except json.JSONDecodeError as err:\n                LOGGER.warning('error in JSON metadata extraction: %s', err)\n                json_data = json.loads(JSON_MINIFY.sub('', elem.text))\n            # extract metadata\n            if '@type' in json_data:\n                if json_data['@type'] == 'Article':\n                    if 'author' in json_data:\n                        metadata.author = normalize_authors(metadata.author, json_data['author'])\n                    if 'headline' in json_data:\n                        metadata.title = json_data['headline']\n                    if 'datePublished' in json_data:\n                        metadata.date = json_data['datePublished']\n                    if 'description' in json_data:\n                        metadata.description = json_data['description']\n                    if 'publisher' in json_data:\n                        metadata.sitename = json_data['publisher']\n                    if 'image' in json_data:\n                        metadata.image = json_data['image']\n                elif json_data['@type'] == 'NewsArticle':\n                    if 'author' in json_data:\n                        metadata.author = normalize_authors(metadata.author, json_data['author'])\n                    if 'headline' in json_data:\n                        metadata.title = json_data['headline']\n                    if 'datePublished' in json_data:\n                        metadata.date = json_data['datePublished']\n                    if 'description' in json_data:\n                        metadata.description = json_data['description']\n                    if 'publisher' in json_data:\n                        metadata.sitename = json_data['publisher']\n                    if 'image' in json_data:\n                        metadata.image = json_data['image']\n                elif json_data['@type'] == 'BlogPosting':\n                    if 'author'", "idx": 951}
{"namespace": "trafilatura.external.try_justext", "completion": "    # init\n    body = Element('body')\n    # determine language\n    if target_language is None:\n        target_language = 'en'\n    # extract paragraphs\n    paragraphs = custom_justext(tree, JT_STOPLIST)\n    # populate body\n    for paragraph in paragraphs:\n        if paragraph.is_boilerplate is False:\n            body.append(paragraph.element)\n    # return\n    return body", "idx": 952}
{"namespace": "mongoengine.base.datastructures.BaseDict.get", "completion": "        try:\n            return super().get(key, default)\n        except KeyError:\n            return default\n", "idx": 953}
{"namespace": "sqlite_utils.utils.suggest_column_types", "completion": "    all_column_types = {}\n    for record in records:\n        for key, value in record.items():\n            if key not in all_column_types:\n                all_column_types[key] = set()\n            all_column_types[key].add(type(value))\n    return types_for_column_types(all_column_types)", "idx": 954}
{"namespace": "sqlite_utils.plugins.get_plugins", "completion": "    plugins = pm.get_plugins()\n    plugin_list = []\n    for plugin in plugins:\n        plugin_dict = {\"name\": plugin.name, \"hooks\": plugin.hook.list_hooks()}\n        if plugin.dist_name:\n            plugin_dict[\"version\"] = plugin.dist.version\n            plugin_dict[\"project_name\"] = plugin.dist.project_name\n        plugin_list.append(plugin_dict)\n    return plugin_list", "idx": 955}
{"namespace": "alembic.config.Config.print_stdout", "completion": "        if not self.messaging_opts[\"quiet\"]:\n            print(text % arg, file=self.stdout)\n", "idx": 956}
{"namespace": "alembic.config.Config.set_section_option", "completion": "        if not self.file_config.has_section(section):\n            self.file_config.add_section(section)\n        self.file_config.set(section, name, value)\n\n    ", "idx": 957}
{"namespace": "alembic.command.merge", "completion": "    script_directory = ScriptDirectory.from_config(config)\n\n    command_args = dict(\n        message=message,\n        autogenerate=False,\n        sql=False,\n        head=\"head\",\n        splice=False,\n        branch_label=branch_label,\n        version_path=None,\n        rev_id=rev_id,\n        depends_on=None,\n    )\n    revision_context = autogen.RevisionContext(\n        config,\n        script_directory,\n        command_args,\n    )\n\n    def retrieve_migrations(rev, context):\n        revision_context.run_no_autogenerate(rev, context)\n        return []\n\n    with EnvironmentContext(\n        config,\n        script_directory,\n        fn=retrieve_migrations,\n        as_sql=False,\n        template_args=revision_context.template_args,\n        revision_context=revision_context,\n    ):\n        script_directory.run_env()\n\n    # the revision_context now has MigrationScript structure(s) present.\n\n    migration_script = revision_context.generated_revisions[-1]\n    return migration_script", "idx": 958}
{"namespace": "alembic.command.upgrade", "completion": "    script = ScriptDirectory.from_config(config)\n\n    def upgrade(rev, context):\n        return script._upgrade_revs(revision, rev)\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=upgrade,\n        as_sql=sql,\n        destination_rev=revision,\n        tag=tag,\n    ):\n        script.run_env()", "idx": 959}
{"namespace": "alembic.command.downgrade", "completion": "    script = ScriptDirectory.from_config(config)\n\n    starting_rev = None\n    if \":\" in revision:\n        if not sql:\n            raise util.CommandError(\"Range revision not allowed\")\n        starting_rev, revision = revision.split(\":\", 2)\n\n    def downgrade(rev, context):\n        return script._downgrade_revs(revision, rev)\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=downgrade,\n        as_sql=sql,\n        starting_rev=starting_rev,\n        destination_rev=revision,\n        tag=tag,\n    ):\n        script.run_env()", "idx": 960}
{"namespace": "alembic.command.history", "completion": "    script = ScriptDirectory.from_config(config)\n\n    def show_history(rev, context):\n        for sc in script.get_revisions(rev):\n            config.print_stdout(\n                sc.cmd_format(\n                    verbose,\n                    include_branches=True,\n                    include_doc=verbose,\n                    indicate_current=indicate_current,\n                )\n            )\n        return []\n\n    with EnvironmentContext(\n        config, script, fn=show_history, dont_mutate=True\n    ):\n        script.run_env()", "idx": 961}
{"namespace": "alembic.command.stamp", "completion": "    script = ScriptDirectory.from_config(config)\n\n    def stamp_revision(rev, context):\n        if purge:\n            context.connection.execute(\n                script.get_current_head_revision_table().delete()\n            )\n        script.stamp(revision, purge=purge)\n        return []\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=stamp_revision,\n        as_sql=sql,\n        tag=tag,\n    ):\n        script.run_env()", "idx": 962}
{"namespace": "alembic.command.ensure_version", "completion": "    script = ScriptDirectory.from_config(config)\n\n    def ensure_version(rev, context):\n        return script._ensure_version(rev, context)\n\n    with EnvironmentContext(config, script, fn=ensure_version, as_sql=sql):\n        script.run_env()", "idx": 963}
{"namespace": "alembic.autogenerate.compare._compare_server_default", "completion": "    metadata_default = metadata_col.server_default\n    conn_default = conn_col.server_default\n\n    if metadata_default is None and conn_default is None:\n        return None\n\n    if metadata_default is None:\n        if conn_default is not None:\n            alter_column_op.modify_server_default = None\n            log.info(\n                \"Detected removed server default on '%s.%s'\", tname, cname\n            )\n        return None\n\n    if conn_default is None:\n        alter_column_op.modify_server_default = metadata_default\n        log.info(\n            \"Detected added server default on '%s.%s'\", tname, cname\n        )\n        return None\n\n    if isinstance(metadata_default, sa_schema.Computed):\n        _compare_computed_default(\n            autogen_context,\n            alter_column_op,\n            schema,\n            tname,\n            cname,\n            conn_col,\n            metadata_col,\n        )\n        return None\n\n    if isinstance(conn_default, sa_schema.Identity):\n        diff, is_alter = _compare_identity_default(\n            autogen_context,\n            alter_column_op,\n            schema,\n            tname,\n            cname,\n            conn_col,\n            metadata_col,\n        )\n        if diff:\n            alter_column_op.modify_server_default = metadata_default\n            log.info(\n                \"Detected added server default on '%s.%s'\", tname, cname\n            )\n        return is_alter\n\n    if isinstance(metadata_default, sa_schema.DefaultClause):\n        if isinstance(metadata_default.arg, str):\n            metadata_default = metadata_default.arg\n        else:\n            metadata_default = str(\n                metadata_default.arg.compile(\n                    dialect=autogen_context.dialect,\n                    compile_kwargs={\"literal_binds\": True},\n                )\n            )\n    if isinstance(conn_default, sa", "idx": 964}
{"namespace": "alembic.autogenerate.render._render_server_default", "completion": "    rendered = _user_defined_render(\"server_default\", default, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    if isinstance(default, FetchedValue):\n        return \"%(prefix)sFetchedValue()\" % {\n            \"prefix\": _sqlalchemy_autogenerate_prefix(autogen_context)\n        }\n    elif isinstance(default, str):\n        if repr_:\n            return repr(default)\n        else:\n            return default\n    elif isinstance(default, TextClause):\n        return _render_potential_expr(default, autogen_context)\n    elif isinstance(default, ColumnElement):\n        return _render_potential_expr(default, autogen_context)\n    elif isinstance(default, Computed):\n        return _render_computed(default, autogen_context)\n    elif isinstance(default, Identity):\n        return _render_identity(default, autogen_context)\n    elif isinstance(default, DefaultClause):\n        if isinstance(default.arg, str):\n            return _render_potential_expr(default.arg, autogen_context)\n        else:\n            return _render_potential_expr(default.arg, autogen_context)\n    else:\n        return None", "idx": 965}
{"namespace": "alembic.autogenerate.render._render_constraint", "completion": "    renderer = _constraint_renderers.dispatch(constraint)\n    return renderer(constraint, autogen_context, namespace_metadata)", "idx": 966}
{"namespace": "alembic.autogenerate.render._render_unique_constraint", "completion": "    rendered = _user_defined_render(\"unique\", constraint, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    return _uq_constraint(constraint, autogen_context, False)", "idx": 967}
{"namespace": "alembic.autogenerate.render._render_check_constraint", "completion": "    rendered = _user_defined_render(\"check\", constraint, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    if constraint.parent is not None:\n        return None\n\n    opts = []\n    if constraint.name:\n        opts.append(\n            (\"name\", repr(_render_gen_name(autogen_context, constraint.name)))\n        )\n\n    return \"%(prefix)sCheckConstraint(%(sqltext)s, %(args)s)\" % {\n        \"prefix\": _sqlalchemy_autogenerate_prefix(autogen_context),\n        \"sqltext\": _render_potential_expr(\n            constraint.sqltext, autogen_context, wrap_in_text=False\n        ),\n        \"args\": \", \".join(\n            [\"%s=%s\" % (kwname, val) for kwname, val in opts]\n        ),\n    }", "idx": 968}
{"namespace": "alembic.autogenerate.api.compare_metadata", "completion": "    autogen_context = AutogenContext(context)\n\n    upgrade_ops = ops.UpgradeOps([])\n    compare._produce_net_changes(autogen_context, upgrade_ops)\n\n    downgrade_ops = upgrade_ops.reverse()\n\n    migration_script = ops.MigrationScript(\n        rev_id=None,\n        upgrade_ops=upgrade_ops,\n        downgrade_ops=downgrade_ops,\n    )\n\n    return migration_script\n\n", "idx": 969}
{"namespace": "alembic.autogenerate.api.AutogenContext._within_batch", "completion": "        if self._has_batch:\n            yield\n        else:\n            self._has_batch = True\n            yield\n            self._has_batch = False", "idx": 970}
{"namespace": "alembic.util.sqla_compat._connectable_has_table", "completion": "    if sqla_14:\n        return inspect(connectable).has_table(tablename, schemaname)\n    else:\n        return inspect(connectable).has_table(tablename, schemaname)\n\n", "idx": 971}
{"namespace": "alembic.util.sqla_compat._get_constraint_final_name", "completion": "    if sqla_14:\n        if constraint.name is None:\n            return None\n        assert dialect is not None\n        return dialect.identifier_preparer.format_constraint(\n            constraint, _alembic_quote=False\n        )\n    else:\n        if constraint.name is None:\n            return None\n        return constraint.name", "idx": 972}
{"namespace": "alembic.testing.env.env_file_fixture", "completion": "    dir_ = os.path.join(_get_staging_directory(), \"scripts\")\n    path = os.path.join(dir_, \"env.py\")\n    with open(path, \"w\") as f:\n        f.write(txt)", "idx": 973}
{"namespace": "alembic.testing.env._no_sql_testing_config", "completion": "    dir_ = os.path.join(_get_staging_directory(), \"scripts\")\n    url = \"sqlite:///%s/foo.db\" % dir_\n\n    return _write_config_file(\n        \"\"\"", "idx": 974}
{"namespace": "alembic.testing.env._write_config_file", "completion": "    cfg = _testing_config()\n    with open(cfg.config_file_name, \"w\") as f:\n        f.write(text)\n    return cfg", "idx": 975}
{"namespace": "alembic.testing.env.three_rev_fixture", "completion": "    script = ScriptDirectory.from_config(cfg)\n    a = util.rev_id()\n    b = util.rev_id()\n    c = util.rev_id()\n\n    script.generate_revision(a, \"revision a\", head=None, refresh=True)\n    write_script(\n        script,\n        a,\n        \"\"\"\\", "idx": 976}
{"namespace": "alembic.testing.env.multi_heads_fixture", "completion": "    script = ScriptDirectory.from_config(cfg)\n    d = util.rev_id()\n    e = util.rev_id()\n    f = util.rev_id()\n\n    script.generate_revision(d, \"revision d\", refresh=True, head=a)\n    write_script(\n        script,\n        d,\n        \"\"\"\\", "idx": 977}
{"namespace": "alembic.testing.fixtures.capture_db", "completion": "    dialect = dialect\n    buf = io.StringIO()\n    eng = mock.create_engine(dialect, echo=True, execution_options={\"stream_results\": False})\n    eng.echo = True\n    eng.execution_options = {\"stream_results\": False}\n    eng.connect().connection.connection.set_trace_callback(buf.write)\n    return eng, buf", "idx": 978}
{"namespace": "alembic.testing.fixtures.capture_engine_context_buffer", "completion": "    dialect = kw.pop(\"dialect\", \"sqlite\")\n    dialect = dialect.lower()\n    if dialect not in _engs:\n        _engs[dialect] = capture_db(dialect)\n    engine, buf = _engs[dialect]\n    with engine.connect() as conn:\n        yield buf\n\n", "idx": 979}
{"namespace": "alembic.operations.schemaobj.SchemaObjects.unique_constraint", "completion": "        t = sa_schema.Table(\n            source,\n            self.metadata(),\n            sa_schema.Column(\"x\", Integer),\n            schema=schema,\n        )\n        u = sa_schema.UniqueConstraint(*local_cols, name=name, **kw)\n        t.append_constraint(u)\n        return u", "idx": 980}
{"namespace": "alembic.operations.schemaobj.SchemaObjects.index", "completion": "        m = self.metadata()\n        if tablename is not None:\n            t = sa_schema.Table(\n                tablename,\n                m,\n                *[sa_schema.Column(n, NULLTYPE) for n in columns],\n                schema=schema,\n            )\n            i = sa_schema.Index(name, *[t.c[n] for n in columns], **kw)\n            t.append_constraint(i)\n        else:\n            i = sa_schema.Index(name, *columns, **kw)\n        return i", "idx": 981}
{"namespace": "alembic.operations.ops.DropConstraintOp.from_constraint", "completion": "        constraint_table = sqla_compat._table_for_constraint(constraint)\n        constraint_name = sqla_compat.constraint_name_or_none(constraint.name)\n        if isinstance(constraint, PrimaryKeyConstraint):\n            return cls(\n                constraint_name,\n                constraint_table.name,\n                constraint.columns.keys(),\n                schema=constraint_table.schema,\n                _reverse=CreatePrimaryKeyOp.from_constraint(constraint),\n            )\n        elif isinstance(constraint, UniqueConstraint):\n            return cls(\n                constraint_name,\n                constraint_table.name,\n                [c.name for c in constraint.columns],\n                schema=constraint_table.schema,\n                _reverse=CreateUniqueConstraintOp.from_constraint(constraint),\n            )\n        elif isinstance(constraint, ForeignKeyConstraint):\n            return cls(\n                constraint_name,\n                constraint_table.name,\n                constraint.columns.keys(),\n                schema=constraint_table.schema,\n                _reverse=CreateForeignKeyOp.from_constraint(constraint),\n            )\n        elif isinstance(constraint, CheckConstraint):\n            return cls(\n                constraint_name,\n                constraint_table.name,\n                cast(\"ColumnElement[Any]\", constraint.sqltext),\n                schema=constraint_table.schema,\n                _reverse=CreateCheckConstraintOp.from_constraint(constraint),\n            )\n        else:\n            raise ValueError(\n                \"constraint type %s not supported\" % constraint.__visit_name__\n            )\n", "idx": 982}
{"namespace": "alembic.operations.ops.DropConstraintOp.to_constraint", "completion": "        if self._reverse:\n            return self._reverse.to_constraint()\n        else:\n            raise ValueError(\"DropConstraintOp has no reverse operation\")\n", "idx": 983}
{"namespace": "alembic.operations.ops.CreatePrimaryKeyOp.to_constraint", "completion": "        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.primary_key_constraint(\n            self.constraint_name,\n            self.table_name,\n            self.columns,\n            schema=self.schema,\n            **self.kw,\n        )\n", "idx": 984}
{"namespace": "alembic.operations.ops.CreateIndexOp.from_index", "completion": "        assert index.table is not None\n        return cls(\n            index.name,  # type: ignore[arg-type]\n            index.table.name,\n            index.columns,\n            schema=index.table.schema,\n            unique=index.unique,\n            _reverse=DropIndexOp.from_index(index),\n            **index.kwargs,\n        )\n", "idx": 985}
{"namespace": "alembic.operations.ops.DropIndexOp.from_index", "completion": "        assert index.table is not None\n        return cls(\n            index.name,  # type: ignore[arg-type]\n            index.table.name,\n            schema=index.table.schema,\n            _reverse=CreateIndexOp.from_index(index),\n            **index.kwargs,\n        )\n", "idx": 986}
{"namespace": "alembic.operations.ops.DropIndexOp.to_index", "completion": "        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        idx = schema_obj.index(\n            self.index_name,\n            self.table_name,\n            sqla_compat._get_index_expressions(self._reverse),\n            schema=self.schema,\n            unique=self._reverse.unique,\n            **self._reverse.kw,\n        )\n        return idx", "idx": 987}
{"namespace": "alembic.operations.ops.CreateTableOp.from_table", "completion": "        cols_and_constraints = []\n        for col in table.columns:\n            cols_and_constraints.append(col)\n        for constraint in table.constraints:\n            cols_and_constraints.append(constraint)\n        for index in table.indexes:\n            cols_and_constraints.append(index)\n\n        return cls(\n            table.name,\n            cols_and_constraints,\n            schema=table.schema,\n            _namespace_metadata=_namespace_metadata,\n            _constraints_included=True,\n            comment=table.comment,\n            info=dict(table.info),\n            prefixes=list(table._prefixes),\n            **table.kwargs,\n        )\n", "idx": 988}
{"namespace": "alembic.operations.ops.DropTableOp.from_table", "completion": "        if _namespace_metadata is None:\n            _namespace_metadata = table.metadata\n\n        return cls(\n            table.name,\n            _namespace_metadata=_namespace_metadata,\n            _reverse=CreateTableOp.from_table(table, _namespace_metadata=None),\n            schema=table.schema,\n            table_kw=dict(table.kwargs),\n        )\n", "idx": 989}
{"namespace": "alembic.operations.ops.DropTableOp.to_table", "completion": "        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.table(\n            self.table_name,\n            schema=self.schema,\n            comment=self.comment,\n            info=self.info,\n            prefixes=list(self.prefixes) if self.prefixes else [],\n            **self.table_kw,\n        )", "idx": 990}
{"namespace": "alembic.operations.ops.AlterColumnOp.to_diff_tuple", "completion": "        # TODO: make this a little simpler\n        all_keys = {\n            m.group(1)\n            for m in [re.match(r\"^(?:existing_|modify_)(.+)$\", k) for k in kw]\n            if m\n        }\n\n        for k in all_keys:\n            if \"modify_%s\" % k in kw:\n                swap = kw[\"existing_%s\" % k]\n                kw[\"existing_%s\" % k] = kw[\"modify_%s\" % k]\n                kw[\"modify_%s\" % k] = swap\n\n        return (\n            \"alter_column\",\n            table_name,\n            column_name,\n            schema,\n            existing_type,\n            existing_server_default,\n            existing_nullable,\n            existing_comment,\n            modify_nullable,\n            modify_comment,\n            modify_server_default,\n            modify_name,\n            modify_type,\n            kw,\n        )\n", "idx": 991}
{"namespace": "alembic.operations.ops.AddColumnOp.reverse", "completion": "        return DropColumnOp.from_column_and_tablename(\n            self.schema, self.table_name, self.column\n        )", "idx": 992}
{"namespace": "alembic.operations.ops.DropColumnOp.reverse", "completion": "        if self._reverse is None:\n            raise ValueError(\n                \"The reverse operation is not available for this operation.\"\n            )\n        return self._reverse", "idx": 993}
{"namespace": "alembic.operations.ops.DropColumnOp.from_column_and_tablename", "completion": "        return cls(tname, col.name, schema=schema, _reverse=AddColumnOp(tname, col))\n\n    ", "idx": 994}
{"namespace": "alembic.operations.ops.DropColumnOp.to_column", "completion": "        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        if self._reverse:\n            return self._reverse.to_column()\n        else:\n            return schema_obj.column(\n                self.column_name,\n                nullable=True,\n                schema=self.schema,\n                **self.kw,\n            )", "idx": 995}
{"namespace": "alembic.script.revision.RevisionMap.heads", "completion": "        self._revision_map\n        return self.heads\n", "idx": 996}
{"namespace": "alembic.script.revision.RevisionMap.add_revision", "completion": "        if revision.revision in self._revision_map:\n            if _replace:\n                self._revision_map[revision.revision] = revision\n            else:\n                raise RevisionError(\n                    \"Revision %s is present more than once\"\n                    % revision.revision\n                )\n        else:\n            self._revision_map[revision.revision] = revision\n\n        if revision.branch_labels:\n            self._map_branch_labels(\n                [revision], cast(_RevisionMapType, self._revision_map)\n            )\n\n        if revision.dependencies:\n            self._add_depends_on([revision], cast(_RevisionMapType, self._revision_map))\n\n        if revision.is_base:\n            self.bases += (revision,)\n        if revision._is_real_base:\n            self._real_bases += (revision,)\n\n        if revision.nextrev:\n            self.heads -= set(revision.nextrev)\n        if revision._is_real_head:\n            self._real_heads -= set(revision.nextrev)\n\n        self._normalize_depends_on(\n            [revision], cast(_RevisionMapType, self._revision_map)\n        )\n\n        if revision.is_head:\n            self.heads += (revision,)\n        if revision._is_real_head:\n            self._real_heads += (revision,)\n\n        if revision.is_branch_point:\n            self.bases -= set(revision.nextrev)\n        if revision._is_real_branch_point:\n            self._real_bases -= set(revision.nextrev)\n\n        if revision.is_merge_point:\n            self.heads -= set(revision._versioned_down_revisions)\n        if revision._is_real_head:\n            self._real_heads -= set(revision._versioned_down_revisions)\n\n        if revision.branch_labels:\n            self._add_branches", "idx": 997}
{"namespace": "alembic.script.revision.RevisionMap.get_revisions", "completion": "        if id_ is None:\n            return ()\n        elif isinstance(id_, str):\n            return (self.get_revision(id_),)\n        elif isinstance(id_, tuple):\n            return tuple(self.get_revision(i) for i in id_)\n        else:\n            raise RevisionError(\n                \"revision identifier %r is not a string; ensure database \"\n                \"driver settings are correct\" % (id_,)\n            )", "idx": 998}
{"namespace": "alembic.script.revision.RevisionMap.get_revision", "completion": "        if isinstance(id_, str):\n            return self._revision_for_ident(id_)\n        elif isinstance(id_, tuple):\n            return tuple(self._revision_for_ident(id_) for id_ in id_)\n        else:\n            return None\n\n    ", "idx": 999}
{"namespace": "alembic.script.revision.RevisionMap.filter_for_lineage", "completion": "        if check_against is None:\n            return tuple(targets)\n        else:\n            return tuple(\n                target\n                for target in targets\n                if self._shares_lineage(\n                    target,\n                    check_against,\n                    include_dependencies=include_dependencies,\n                )\n            )", "idx": 1000}
{"namespace": "alembic.script.revision.RevisionMap.iterate_revisions", "completion": "        if isinstance(upper, str):\n            upper = self.get_revisions(upper)\n        if isinstance(lower, str):\n            lower = self.get_revisions(lower)\n\n        if not isinstance(upper, tuple):\n            raise RevisionError(\n                \"upper must be a string or tuple of strings, not %r\" % upper\n            )\n        if not isinstance(lower, tuple):\n            raise RevisionError(\n                \"lower must be a string or tuple of strings, not %r\" % lower\n            )\n\n        if not upper:\n            raise RevisionError(\"upper must be non-empty\")\n        if not lower:\n            raise RevisionError(\"lower must be non-empty\")\n\n        if assert_relative_length:\n            if len(upper) != len(lower):\n                raise RevisionError(\n                    \"upper and lower must have the same length\"\n                )\n\n        if select_for_downgrade:\n            downgrade_revisions, heads = self._collect_downgrade_revisions(\n                upper,\n                lower,\n                inclusive,\n                implicit_base,\n                assert_relative_length,\n            )\n        else:\n            needs, heads = self._collect_upgrade_revisions(\n                upper,\n                lower,\n                inclusive,\n                implicit_base,\n                assert_relative_length,\n            )\n\n        if select_for_downgrade:\n            needs = downgrade_revisions\n\n        if not needs:\n            return\n\n        if not heads:\n            heads = self.get_revisions(upper)\n\n        sorted_revisions = self._topological_sort(needs, heads)\n\n        if assert_relative_length:\n            if len(sorted_revisions) != len(upper):\n                raise RevisionError(\n                    \"upper and sorted_revisions must have the same length\"\n                )\n\n        for rev_id in sorted_revisions:\n            yield self._revision_map[rev_id]\n", "idx": 1001}
{"namespace": "alembic.script.revision.RevisionMap._topological_sort", "completion": "        # This function performs a topological sort on a collection of Revision objects based on their dependencies. It returns a list of revision ids in the sorted order.\n        # Input-Output Arguments\n        # :param self: RevisionMap. An instance of the RevisionMap class.\n        # :param revisions: Collection of Revision. A collection of Revision objects to be sorted.\n        # :param heads: Any. The heads of the revisions to be sorted.\n        # :return: List of str. The revision ids in the topological sorted order.\n        # \"\"\"\n        # This function performs a topological sort on a collection of Revision objects based on their dependencies. It returns a list of revision ids in the sorted order.\n        # Input-Output Arguments\n        # :param self: RevisionMap. An instance of the RevisionMap class.\n        # :param revisions: Collection of Revision. A collection of Revision objects to be sorted.\n        # :param heads: Any. The heads of the revisions to be sorted.\n        # :return: List of str. The revision ids in the topological sorted order.\n        # \"\"\"\n        # This function performs a topological sort on a collection of Revision objects based on their dependencies. It returns a list of revision ids in the sorted order.\n        # Input-Output Arguments\n        # :param self: RevisionMap. An instance of the RevisionMap class.\n        # :param revisions: Collection of Revision. A collection of Revision objects to be sorted.\n        # :param heads: Any. The heads of the revisions to be sorted.\n        # :return: List of str. The revision ids in the topological sorted order.\n        # \"\"\"\n        # This function performs a topological sort on a collection of Revision objects based on their dependencies. It returns a list of revision ids in the sorted order.\n        # Input-Output Arguments\n        # :param self: RevisionMap. An instance of the RevisionMap class.\n        # :param revisions: Collection of Revision. A collection of Revision objects to be sorted.\n        # :param heads: Any. The heads of the revisions to be sorted.\n        # :return: List of str. The revision", "idx": 1002}
{"namespace": "alembic.script.revision.Revision._all_down_revisions", "completion": "        return util.dedupe_tuple(\n            util.to_tuple(self.down_revision, default=())\n            + self._resolved_dependencies\n        )\n\n    ", "idx": 1003}
{"namespace": "alembic.script.revision.Revision._normalized_down_revisions", "completion": "        return util.to_tuple(self.down_revision, default=()) + tuple(\n            self._normalized_resolved_dependencies\n        )\n", "idx": 1004}
{"namespace": "alembic.script.write_hooks._invoke", "completion": "    try:\n        formatter = _registry[name]\n    except KeyError as ke:\n        raise util.CommandError(\n            f\"No formatter with name '{name}' registered\"\n        ) from ke\n    else:\n        return formatter(revision, options)", "idx": 1005}
{"namespace": "bplustree.memory.FileMemory.get_node", "completion": "        with self._lock.reader_lock:\n            node = self._cache.get(page)\n            if node is None:\n                data = self._read_page(page)\n                node = Node.load(data, self._tree_conf)\n                self._cache[page] = node\n            return node\n\n    ", "idx": 1006}
{"namespace": "bplustree.memory.FileMemory.next_available_page", "completion": "        self.last_page += 1\n        return self.last_page\n\n    ", "idx": 1007}
{"namespace": "bplustree.memory.FileMemory.get_metadata", "completion": "        data = self._read_page(0)\n        root_node_page = int.from_bytes(data[0:PAGE_REFERENCE_BYTES], ENDIAN)\n        page_size = int.from_bytes(data[PAGE_REFERENCE_BYTES:\n                                        PAGE_REFERENCE_BYTES+OTHERS_BYTES],\n                                   ENDIAN)\n        order = int.from_bytes(data[PAGE_REFERENCE_BYTES+OTHERS_BYTES:\n                                    PAGE_REFERENCE_BYTES+2*OTHERS_BYTES],\n                               ENDIAN)\n        key_size = int.from_bytes(data[PAGE_REFERENCE_BYTES+2*OTHERS_BYTES:\n                                       PAGE_REFERENCE_BYTES+3*OTHERS_BYTES],\n                                  ENDIAN)\n        value_size = int.from_bytes(data[PAGE_REFERENCE_BYTES+3*OTHERS_BYTES:\n                                         PAGE_REFERENCE_BYTES+4*OTHERS_BYTES],\n                                    ENDIAN)\n        tree_conf = TreeConf(page_size, order, key_size, value_size)\n        return root_node_page, tree_conf", "idx": 1008}
{"namespace": "bplustree.memory.FileMemory.set_metadata", "completion": "        data = bytearray(\n            root_node_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN) +\n            tree_conf.page_size.to_bytes(OTHERS_BYTES, ENDIAN) +\n            tree_conf.order.to_bytes(OTHERS_BYTES, ENDIAN) +\n            tree_conf.key_size.to_bytes(OTHERS_BYTES, ENDIAN) +\n            tree_conf.value_size.to_bytes(OTHERS_BYTES, ENDIAN)\n        )\n        self._write_page_in_tree(0, data, fsync=True)\n        self._tree_conf = tree_conf", "idx": 1009}
{"namespace": "bplustree.memory.WAL.checkpoint", "completion": "        if self._not_committed_pages:\n            logger.warning('WAL has uncommitted data, discarding it')\n            self._not_committed_pages = dict()\n\n        fsync_file_and_dir(self._fd.fileno(), self._dir_fd)\n\n        for page, page_start in self._committed_pages.items():\n            data = read_from_file(self._fd, page_start,\n                                  page_start + self._page_size)\n            yield page, data\n\n        self._fd.close()\n        os.remove(self.filename)\n        if self._dir_fd is not None:\n            os.fsync(self._dir_fd)", "idx": 1010}
{"namespace": "bplustree.memory.WAL.commit", "completion": "        if self._not_committed_pages:\n            self._add_frame(FrameType.COMMIT)", "idx": 1011}
{"namespace": "bplustree.memory.WAL.rollback", "completion": "        if self._not_committed_pages:\n            self._add_frame(FrameType.ROLLBACK)", "idx": 1012}
{"namespace": "bplustree.entry.Record.dump", "completion": "        key_as_bytes = self._tree_conf.serializer.serialize(\n            self.key, self._tree_conf.key_size\n        )\n        used_key_length = len(key_as_bytes)\n\n        if self.overflow_page:\n            value_as_bytes = None\n            used_value_length = 0\n        else:\n            value_as_bytes = self.value\n            used_value_length = len(value_as_bytes)\n\n        data = (\n            used_key_length.to_bytes(USED_KEY_LENGTH_BYTES, ENDIAN) +\n            key_as_bytes +\n            bytes(self._tree_conf.key_size - used_key_length) +\n            used_value_length.to_bytes(USED_VALUE_LENGTH_BYTES, ENDIAN) +\n            value_as_bytes +\n            bytes(self._tree_conf.value_size - used_value_length) +\n            self.overflow_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN)\n        )\n        return data", "idx": 1013}
{"namespace": "bplustree.entry.Reference.__repr__", "completion": "        return '<Reference: key={} before={} after={}>'.format(\n            self.key, self.before, self.after\n        )", "idx": 1014}
{"namespace": "bplustree.node.Node.dump", "completion": "        data = bytearray()\n        for entry in self.entries:\n            data.extend(entry.dump())\n        used_page_length = len(data)\n        node_type_byte = self._node_type_int.to_bytes(NODE_TYPE_BYTES, ENDIAN)\n        data[0:NODE_TYPE_BYTES] = node_type_byte\n        used_page_length_byte = used_page_length.to_bytes(\n            USED_PAGE_LENGTH_BYTES, ENDIAN\n        )\n        data[NODE_TYPE_BYTES:NODE_TYPE_BYTES+USED_PAGE_LENGTH_BYTES] = used_page_length_byte\n        next_page_byte = self.next_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN)\n        data[NODE_TYPE_BYTES+USED_PAGE_LENGTH_BYTES:\n             NODE_TYPE_BYTES+USED_PAGE_LENGTH_BYTES+PAGE_REFERENCE_BYTES] = next_page_byte\n        padding_length = self._tree_conf.page_size - len(data)\n        data.extend(b'\\x00' * padding_length)\n        return data", "idx": 1015}
{"namespace": "bplustree.node.Node._find_entry_index", "completion": "        entry = self._entry_class(self._tree_conf, key=key)\n        index = bisect.bisect_left(self.entries, entry)\n        return index", "idx": 1016}
{"namespace": "bplustree.node.Node.from_page_data", "completion": "        node_type = int.from_bytes(data[:NODE_TYPE_BYTES], ENDIAN)\n        if node_type == 1:\n            return LonelyRootNode(tree_conf, data, page)\n        elif node_type == 2:\n            return RootNode(tree_conf, data, page)\n        elif node_type == 3:\n            return InternalNode(tree_conf, data, page)\n        elif node_type == 4:\n            return LeafNode(tree_conf, data, page)\n        else:\n            raise ValueError('Unknown node type {}'.format(node_type))", "idx": 1017}
{"namespace": "bplustree.tree.BPlusTree._root_node", "completion": "        return self._mem.get_node(self._root_node_page)\n\n    ", "idx": 1018}
{"namespace": "bplustree.tree.BPlusTree._left_record_node", "completion": "        node = self._root_node\n        while True:\n            if isinstance(node, LonelyRootNode):\n                return node\n            elif isinstance(node, LeafNode):\n                return node\n            else:\n                node = self._mem.get_node(node.smallest_entry.after)\n\n    ", "idx": 1019}
{"namespace": "mopidy.ext.Extension.get_cache_dir", "completion": "        if cls.ext_name is None:\n            raise AssertionError\n        cache_dir_path = (\n            path.expand_path(config[\"core\"][\"cache_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(cache_dir_path)\n        return cache_dir_path", "idx": 1020}
{"namespace": "mopidy.ext.Extension.get_config_dir", "completion": "        if cls.ext_name is None:\n            raise AssertionError\n        config_dir_path = (\n            path.expand_path(config[\"core\"][\"config_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(config_dir_path)\n        return config_dir_path", "idx": 1021}
{"namespace": "mopidy.ext.Extension.get_data_dir", "completion": "        if cls.ext_name is None:\n            raise AssertionError\n        data_dir_path = (\n            path.expand_path(config[\"core\"][\"data_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(data_dir_path)\n        return data_dir_path\n", "idx": 1022}
{"namespace": "mopidy.ext.load_extensions", "completion": "    extensions = []\n    for entry_point in pkg_resources.iter_entry_points(\"mopidy.ext\"):\n        try:\n            extension = entry_point.load()()\n        except Exception:\n            logger.exception(\n                \"Failed to load extension %s\", entry_point.name\n            )\n            continue\n\n        if not isinstance(extension, Extension):\n            logger.warning(\n                \"Extension %s is not a valid extension class\", entry_point.name\n            )\n            continue\n\n        config_schema = extension.get_config_schema()\n        config_defaults = extension.get_default_config()\n        command = extension.get_command()\n\n        data = ExtensionData(\n            extension=extension,\n            entry_point=entry_point,\n            config_schema=config_schema,\n            config_defaults=config_defaults,\n            command=command,\n        )\n\n        if validate_extension_data(data):\n            extensions.append(data)\n\n    return extensions", "idx": 1023}
{"namespace": "mopidy.ext.validate_extension_data", "completion": "    # Check if the entry point name matches the extension name\n    if data.extension.ext_name != data.entry_point.name:\n        logger.error(\n            \"Entry point name %s does not match extension name %s\",\n            data.entry_point.name,\n            data.extension.ext_name,\n        )\n        return False\n\n    # Check if the required dependencies are installed\n    try:\n        data.extension.validate_environment()\n    except Exception as e:\n        logger.error(\n            \"Extension %s failed to validate environment: %s\",\n            data.extension.ext_name,\n            e,\n        )\n        return False\n\n    # Check if the extension has a valid config schema and default config\n    if not data.config_schema or not data.config_defaults:\n        logger.error(\n            \"Extension %s does not have a valid config schema or default config\",\n            data.extension.ext_name,\n        )\n        return False\n\n    return True", "idx": 1024}
{"namespace": "mopidy.httpclient.format_user_agent", "completion": "    if name:\n        return f\"{name} ({platform.python_implementation()} {platform.python_version()}) Mopidy/{mopidy_version}\"\n    else:\n        return f\"{platform.python_implementation()} {platform.python_version()} Mopidy/{mopidy_version}\"", "idx": 1025}
{"namespace": "mopidy.models.immutable.ValidatedImmutableObject.replace", "completion": "        other = copy.copy(self)\n        for key, value in kwargs.items():\n            if not self._is_valid_field(key):\n                raise TypeError(\n                    f\"replace() got an unexpected keyword argument {key}\"\n                )\n            other._set_field(key, value)\n        return other\n", "idx": 1026}
{"namespace": "mopidy.http.Extension.get_default_config", "completion": "        conf_file = os.path.join(os.path.dirname(__file__), \"ext.conf\")\n        return config_lib.read(conf_file)", "idx": 1027}
{"namespace": "mopidy.http.Extension.get_config_schema", "completion": "        schema = super().get_config_schema()\n        schema[\"hostname\"] = config_lib.Hostname()\n        schema[\"port\"] = config_lib.Port()\n        schema[\"static_dir\"] = config_lib.Path()\n        schema[\"debug\"] = config_lib.Boolean()\n        schema[\"set_mime_type\"] = config_lib.Boolean()\n        schema[\"set_mime_type_from_extension\"] = config_lib.Boolean()\n        schema[\"set_mime_type_from_content\"] = config_lib.Boolean()\n        schema[\"set_mime_type_from_path\"] = config_lib.Boolean()\n        schema[\"set_mime_type_from_extension_map\"] = config_lib.List()\n        schema[\"set_mime_type_from_content_map\"] = config_lib.List()\n        schema[\"set_mime_type_from_path_map\"] = config_lib.List()\n        schema[\"set_mime_type_from_extension_map_file\"] = config_lib.Path()\n        schema[\"set_mime_type_from_content_map_file\"] = config_lib.Path()\n        schema[\"set_mime_type_from_path_map_file\"] = config_lib.Path()\n        schema[\"set_mime_type_from_extension_map_file_encoding\"] = config_lib.String()\n        schema[\"set_mime_type_from_content_map_file_encoding\"] = config_lib.String()\n        schema[\"set_mime_type_from_path_map_file_encoding\"] = config_lib.String()\n        schema[\"set_mime_type_from_extension_map_file_comment_char\"] = config_lib.String()\n        schema[\"set_mime_type_from_content_map_file_comment_char\"] = config_lib.String()\n        schema[\"set_mime_type_from_path_map_file_comment_char\"] = config_lib.String()\n        schema[\"set_mime_type_from_extension_map_file_delimiter\"] =", "idx": 1028}
{"namespace": "mopidy.internal.network.try_ipv6_socket", "completion": "    try:\n        socket.socket(socket.AF_INET6, socket.SOCK_STREAM)\n        return True\n    except socket.error:\n        logger.debug(\"IPv6 not supported\")\n        return False", "idx": 1029}
{"namespace": "mopidy.internal.network.format_hostname", "completion": "    if re.match(r\"^\\[[0-9a-f:]+\\]$\", hostname):\n        hostname = re.sub(r\"^\\[|\\]$\", \"\", hostname)\n        hostname = f\"::ffff:{hostname}\"\n    ", "idx": 1030}
{"namespace": "mopidy.internal.xdg.get_dirs", "completion": "    dirs = {}\n\n    for name in [\n        \"XDG_CACHE_HOME\",\n        \"XDG_CONFIG_HOME\",\n        \"XDG_DATA_HOME\",\n        \"XDG_RUNTIME_DIR\",\n        \"XDG_STATE_HOME\",\n    ]:\n        dirs[name] = pathlib.Path(os.environ.get(name, \"~/.cache\")).expanduser()\n\n    dirs[\"XDG_DATA_DIRS\"] = [\n        pathlib.Path(p).expanduser()\n        for p in os.environ.get(\"XDG_DATA_DIRS\", \"/usr/local/share:/usr/share\").split(\":\")\n    ]\n\n    dirs.update(_get_user_dirs(dirs[\"XDG_CONFIG_HOME\"]))\n\n    return dirs", "idx": 1031}
{"namespace": "mopidy.internal.log.get_verbosity_level", "completion": "    verbosity_level = base_verbosity_level + args_verbosity_level\n    if verbosity_level < min(LOG_LEVELS):\n        verbosity_level = min(LOG_LEVELS)\n    elif verbosity_level > max(LOG_LEVELS):\n        verbosity_level = max(LOG_LEVELS)\n    return verbosity_level", "idx": 1032}
{"namespace": "mopidy.internal.validation.check_instance", "completion": "def check_instances(arg, cls, msg=\"Expected a list of {name}, not {arg}\"):\n    _check_iterable(arg, msg, name=cls.__name__)\n    if not all(isinstance(instance, cls) for instance in arg):\n        raise exceptions.ValidationError(msg.format(arg=arg, name=cls.__name__))\n\n\ndef check_integer(arg, min=None, max=None):\n    if not isinstance(arg, int):\n        raise exceptions.ValidationError(f\"Expected an integer, not {arg}\")\n    elif min is not None and arg < min:\n        raise exceptions.ValidationError(\n            f\"Expected number larger or equal to {min}, not {arg}\"\n        )\n    elif max is not None and arg > max:\n        raise exceptions.ValidationError(\n            f\"Expected number smaller or equal to {max}, not {arg}\"\n        )\n\n\ndef check_query(arg, fields=None, list_values=True):\n    if fields is None:\n        fields = SEARCH_FIELDS\n    # TODO: normalize name  -> track_name\n    # TODO: normalize value -> [value]\n    # TODO: normalize blank -> [] or just remove field?\n    # TODO: remove list_values?\n\n    if not isinstance(arg, Mapping):\n        raise exceptions.ValidationError(\n            f\"Expected a query dictionary, not {arg}\"\n        )\n\n    for key, value in arg.items():\n        check_choice(\n            key,\n            fields,\n            msg=\"Expected query field to be one of \" \"{choices}, not {arg}\",\n        )\n        if list_values:\n            msg = 'Expected \"{key}\" to be list of strings, not {arg}'\n            _check_iterable(value, msg, key=key)\n            [_check_query_value(key, v, msg) for v in value]\n        else:\n            _check_query_value(\n                key, value, 'Expected \"{key}\" to be a string, not {arg}'\n            )\n\n\ndef", "idx": 1033}
{"namespace": "mopidy.internal.validation.check_instances", "completion": "def check_list(arg, msg=\"Expected a list, not {arg}\"):\n    if not isinstance(arg, list):\n        raise exceptions.ValidationError(msg.format(arg=arg))\n\n\ndef check_dict(arg, msg=\"Expected a dict, not {arg}\"):\n    if not isinstance(arg, dict):\n        raise exceptions.ValidationError(msg.format(arg=arg))\n\n\ndef check_string(arg, msg=\"Expected a string, not {arg}\"):\n    if not isinstance(arg, str):\n        raise exceptions.ValidationError(msg.format(arg=arg))\n\n\ndef check_strings(arg, msg=\"Expected a list of strings, not {arg}\"):\n    _check_iterable(arg, msg)\n    [check_string(a, msg) for a in arg]\n\n\ndef check_track_fields(arg, msg=\"Expected a dict of track fields, not {arg}\"):\n    if not isinstance(arg, dict):\n        raise exceptions.ValidationError(msg.format(arg=arg))\n    for key, value in arg.items():\n        check_choice(\n            key,\n            TRACK_FIELDS_WITH_TYPES,\n            msg=\"Expected track field to be one of \"\n            \"{choices}, not {arg}\",\n        )\n        if key == \"uri\":\n            check_uri(value, msg=\"Expected a valid URI, not {arg}\")\n        else:\n            check_instance(\n                value,\n                TRACK_FIELDS_WITH_TYPES[key],\n                msg=\"Expected a {name} instance, not {arg}\",\n            )\n\n\ndef check_track_fields_list(arg, msg=\"Expected a list of track fields, not {arg}\"):\n    _check_iterable(arg, msg)\n    [check_track_fields(a, msg) for a in arg]\n\n\ndef check_playlist_fields(arg, msg=\"Expected a dict of playlist fields, not {arg}\"):\n    if not isinstance(arg, dict):\n        raise exceptions", "idx": 1034}
{"namespace": "mopidy.internal.validation.check_uri", "completion": "    if not isinstance(arg, str):\n        raise exceptions.ValidationError(msg.format(arg=arg))\n    try:\n        urllib.parse.urlparse(arg)\n    except ValueError:\n        raise exceptions.ValidationError(msg.format(arg=arg))", "idx": 1035}
{"namespace": "mopidy.internal.validation.check_uris", "completion": "    _check_iterable(arg, msg)\n    [check_uri(uri) for uri in arg]", "idx": 1036}
{"namespace": "mopidy.internal.playlists.parse", "completion": "    for handler in HANDLERS:\n        if handler[\"detector\"](data):\n            return handler[\"parser\"](data)\n\n    return list(parse_urilist(data))\n\n", "idx": 1037}
{"namespace": "mopidy.config.schemas.ConfigSchema.deserialize", "completion": "        errors = {}\n        result = {}\n\n        for key, value in values.items():\n            if key not in self:\n                errors[key] = \"Unknown key\"\n                continue\n\n            try:\n                result[key], error = self[key].deserialize(value)\n            except ValueError as e:  # deserialization failed\n                result[key] = None\n                error = str(e)\n\n            if error:\n                errors[key] = error\n\n        # check for deprecated keys\n        for key in self.keys():\n            if key not in result:\n                result[key] = None\n\n        return result, errors\n", "idx": 1038}
{"namespace": "mopidy.config.types.String.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        if self._transformer:\n            value = self._transformer(value)\n        validators.validate_choice(value, self._choices)\n        return _TransformedValue(value, value)", "idx": 1039}
{"namespace": "mopidy.config.types.String.serialize", "completion": "        if value is None:\n            return \"\"\n        if isinstance(value, _TransformedValue):\n            value = value.original\n        return encode(value)", "idx": 1040}
{"namespace": "mopidy.config.types.Secret.serialize", "completion": "        if value is None:\n            return \"\"\n        if isinstance(value, _TransformedValue):\n            value = value.original\n        if display:\n            return \"********\"\n        else:\n            return super().serialize(value, display)", "idx": 1041}
{"namespace": "mopidy.config.types.Integer.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = int(value)\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        validators.validate_choice(value, self._choices)\n        return value", "idx": 1042}
{"namespace": "mopidy.config.types.Float.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = float(value)\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        return value", "idx": 1043}
{"namespace": "mopidy.config.types.Boolean.deserialize", "completion": "        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = value.lower()\n        if value in self.true_values:\n            return True\n        elif value in self.false_values:\n            return False\n        else:\n            raise ValueError(\n                f\"{value} is not a valid boolean value, \"\n                f\"must be one of {self.true_values} or {self.false_values}\"\n            )", "idx": 1044}
{"namespace": "mopidy.config.types.Pair.deserialize", "completion": "        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        if self._separator in value:\n            first_value, second_value = value.split(self._separator, 1)\n        else:\n            if self._optional_pair:\n                first_value = second_value = value\n            else:\n                raise ValueError(\n                    f\"config value must include separator {self._separator}\"\n                )\n\n        first_value = self._subtypes[0].deserialize(first_value)\n        second_value = self._subtypes[1].deserialize(second_value)\n\n        return first_value, second_value", "idx": 1045}
{"namespace": "mopidy.config.types.Pair.serialize", "completion": "        if value is None:\n            return \"\"\n\n        serialized_values = []\n        for i, subtype in enumerate(self._subtypes):\n            serialized_value = subtype.serialize(value[i], display=display)\n            if serialized_value:\n                serialized_values.append(serialized_value)\n\n        if len(serialized_values) == 2:\n            return f\"{serialized_values[0]}{self._separator}{serialized_values[1]}\"\n        elif len(serialized_values) == 1:\n            return serialized_values[0]\n        else:\n            raise ValueError(\n                f\"Invalid pair value: {value}. Expected a pair of values, got {len(serialized_values)} values.\"\n            )", "idx": 1046}
{"namespace": "mopidy.config.types.List.serialize", "completion": "        serialized_values = []\n        for item in value:\n            serialized_value = self._subtype.serialize(item, display=display)\n            serialized_values.append(serialized_value)\n\n        return \"\\n\".join(serialized_values)\n\n", "idx": 1047}
{"namespace": "mopidy.config.types.LogColor.deserialize", "completion": "        value = decode(value).strip()\n        validators.validate_choice(value.lower(), log.COLORS)\n        return value.lower()", "idx": 1048}
{"namespace": "mopidy.config.types.LogColor.serialize", "completion": "", "idx": 1049}
{"namespace": "mopidy.config.types.LogLevel.deserialize", "completion": "        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = value.lower()\n        validators.validate_choice(value, self.levels)\n        return self.levels[value]", "idx": 1050}
{"namespace": "mopidy.config.types.LogLevel.serialize", "completion": "        if value in self.levels.values():\n            for key, val in self.levels.items():\n                if val == value:\n                    return encode(key)\n        return \"\"\n\n", "idx": 1051}
{"namespace": "mopidy.config.types.Hostname.deserialize", "completion": "        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        # This is necessary for backwards-compatibility, in case subclasses\n        # aren't calling their parent constructor.\n        transformer = getattr(self, \"_transformer\", None)\n        if transformer:\n            transformed_value = transformer(value)\n            value = _TransformedValue(value, transformed_value)\n\n        if value.startswith(\"unix:\"):\n            return str(value)\n\n        try:\n            socket.gethostbyname(value)\n        except socket.gaierror:\n            raise ValueError(f\"invalid hostname: {value}\")\n\n        return value\n", "idx": 1052}
{"namespace": "mopidy.config.load", "completion": "    config_dir = pathlib.Path(__file__).parent\n    defaults = [read(config_dir / \"default.conf\")]\n    defaults.extend(read(d) for d in ext_defaults)\n    raw_config = _load(files, defaults, overrides)\n\n    schemas = _schemas[:]\n    schemas.extend(ConfigSchema(name, schema) for name, schema in ext_schemas)\n\n    config, errors = _validate(raw_config, schemas)\n    if errors:\n        logger.warning(\n            \"Configuration errors:\\n\"\n            + \"\\n\".join(\n                f\"{section}: {', '.join(errors[section])}\"\n                for section in sorted(errors)\n            )\n        )\n\n    return Proxy(config)", "idx": 1053}
{"namespace": "mopidy.config.format_initial", "completion": "    from mopidy.config import keyring\n    config_dir = pathlib.Path(__file__).parent\n    defaults = [read(config_dir / \"default.conf\")]\n    raw_config = _load([], defaults, keyring.fetch())\n\n    schemas = _schemas[:]\n    schemas.extend(extensions_data)\n    config, errors = _validate(raw_config, schemas)\n\n    comments = {}\n    for schema in schemas:\n        comments[schema.name] = schema.comments\n\n    output = _format(config, comments, schemas, False, True)\n    output = _preprocess(output)\n    output = _postprocess(output)\n\n    return output", "idx": 1054}
{"namespace": "mopidy.config._load", "completion": "    parser = configparser.RawConfigParser()\n    parser.read_dict({\"__COMMENTS__\": {}})\n    parser.comment_prefixes = [\n        \"__HASH\",\n        \"__SEMICOLON\",\n        \"__INLINE\",\n        \"__BLANK\",\n        \"__SECTION\",\n    ]\n\n    for default in defaults:\n        parser.read_string(_preprocess(default))\n\n    for file_path in files:\n        if file_path.is_dir():\n            for file_path in file_path.iterdir():\n                if file_path.suffix == \".conf\":\n                    _load_file(parser, file_path)\n        else:\n            _load_file(parser, file_path)\n\n    raw_config = {}\n    for section in parser.sections():\n        raw_config[section] = dict(parser.items(section))\n\n    for section, key, value in overrides:\n        raw_config.setdefault(section, {})[key] = value\n\n    return raw_config", "idx": 1055}
{"namespace": "mopidy.config._validate", "completion": "    config = {}\n    errors = {}\n    for schema in schemas:\n        section = raw_config.get(schema.name)\n        if section is None:\n            continue\n        try:\n            config[schema.name] = schema.deserialize(section)\n        except ValueError as e:\n            errors[schema.name] = e\n            logger.warning(\n                f\"Config section [{schema.name}] has errors; \"\n                f\"{e.args[0]} ({e.args[1]})\"\n            )\n    return config, errors", "idx": 1056}
{"namespace": "mingus.extra.tunings.get_tunings", "completion": "    searchi = str.upper(instrument)\n    searchd = str.upper(description)\n    keys = list(_known.keys())\n    result = []\n    for x in keys:\n        if (\n            searchi not in keys\n            and x.find(searchi) == 0\n            or searchi in keys\n            and x == searchi\n        ):\n            for (desc, tun) in six.iteritems(_known[x][1]):\n                if desc.find(searchd) == 0:\n                    if nr_of_strings is None and nr_of_courses is None:\n                        result.append(tun)\n                    elif nr_of_strings is not None and nr_of_courses is None:\n                        if tun.count_strings() == nr_of_strings:\n                            result.append(tun)\n                    elif nr_of_strings is None and nr_of_courses is not None:\n                        if tun.count_courses() == nr_of_courses:\n                            result.append(tun)\n                    else:\n                        if (\n                            tun.count_courses() == nr_of_courses\n                            and tun.count_strings() == nr_of_strings\n                        ):\n                            result.append(tun)\n    return result", "idx": 1057}
{"namespace": "mingus.containers.instrument.Instrument.note_in_range", "completion": "        if isinstance(note, six.string_types):\n            note = Note(note)\n        if not hasattr(note, \"name\"):\n            raise UnexpectedObjectError(\n                \"Unexpected object '%s'. \"\n                \"Expecting a mingus.containers.Note object\" % note\n            )\n        if note < self.range[0] or note > self.range[1]:\n            return False\n        return True", "idx": 1058}
{"namespace": "mingus.containers.instrument.Guitar.can_play_notes", "completion": "        if len(notes) > 6:\n            return False\n        return Instrument.can_play_notes(self, notes)", "idx": 1059}
{"namespace": "mingus.containers.bar.Bar.get_range", "completion": "        highest = lowest = self.bar[0][2].get_note_names()[0]\n        for cont in self.bar:\n            for note in cont[2].get_note_names():\n                if note > highest:\n                    highest = note\n                if note < lowest:\n                    lowest = note\n        return (highest, lowest)\n\n    ", "idx": 1060}
{"namespace": "mingus.containers.bar.Bar.transpose", "completion": "        if up:\n            for cont in self.bar:\n                cont[2].transpose(interval)\n        else:\n            for cont in self.bar:\n                cont[2].transpose(-interval)\n", "idx": 1061}
{"namespace": "mingus.containers.bar.Bar.determine_chords", "completion": "        from mingus.core import chords\n\n        res = []\n        for x in self.bar:\n            res.append(\n                [\n                    x[0],\n                    chords.determine(\n                        x[2].get_note_names(), self.key.key, shorthand\n                    ),\n                ]\n            )\n        return res", "idx": 1062}
{"namespace": "mingus.containers.note.Note.transpose", "completion": "        if up:\n            self.name = notes.transpose(self.name, interval)\n        else:\n            self.name = notes.transpose(self.name, interval, up=False)\n", "idx": 1063}
{"namespace": "mingus.containers.note.Note.from_int", "completion": "        if integer < 0:\n            raise ValueError(\"Note value must be positive\")\n        self.name = notes.int_to_note(integer % 12)\n        self.octave = int(integer / 12) - 6\n        return self", "idx": 1064}
{"namespace": "mingus.containers.note.Note.to_hertz", "completion": "        return standard_pitch * 2 ** ((int(self) - 9) / 12.0)", "idx": 1065}
{"namespace": "mingus.containers.note.Note.from_hertz", "completion": "        # int(Note(\"A\")) == 57\n        diff = int(round(12 * log(hertz / standard_pitch, 2)))\n        self.name = notes.int_to_note(diff % 12)\n        self.octave = diff // 12\n        return self", "idx": 1066}
{"namespace": "mingus.containers.note.Note.to_shorthand", "completion": "        if self.name == \"C\":\n            if self.octave == 0:\n                return \"C,\"\n            elif self.octave == 1:\n                return \"C\"\n            elif self.octave == 2:\n                return \"C'\"\n            elif self.octave == 3:\n                return \"C''\"\n            elif self.octave == 4:\n                return \"C'''\"\n            elif self.octave == 5:\n                return \"C''''\"\n            elif self.octave == 6:\n                return \"C'''''\"\n            elif self.octave == 7:\n                return \"C''''''\"\n            elif self.octave == 8:\n                return \"C'''''''\"\n            elif self.octave == 9:\n                return \"C''''''''\"\n            elif self.octave == 10:\n                return \"C'''''''''\"\n            elif self.octave == 11:\n                return \"C''''''''''\"\n            elif self.octave == 12:\n                return \"C'''''''''''\"\n            elif self.octave == 13:\n                return \"C''''''''''''\"\n            elif self.octave == 14:\n                return \"C'''''''''''''\"\n            elif self.octave == 15:\n                return \"C''''''''''''''\"\n            elif self.octave == 16:\n                return \"C'''''''''''''''\"\n            elif self.octave == 17:\n                return \"C''''''''''''''''\"\n            elif self.octave == 18:\n                return \"C'''''''''''''''''\"\n            elif self.octave == 19:\n                return \"C''''''''''''''''''\"\n            elif self.octave == 20:\n                return \"C'''''''''''''''''''\"\n            elif self.octave == 21:\n                return \"C''''''''''''''''''''\"\n            elif self.octave == 22:\n                return \"C''''''''''''''''''''", "idx": 1067}
{"namespace": "mingus.containers.note_container.NoteContainer.from_chord_shorthand", "completion": "        self.empty()\n        self.add_notes(chords.from_shorthand(shorthand))\n        return self\n\n    ", "idx": 1068}
{"namespace": "mingus.containers.note_container.NoteContainer.from_interval_shorthand", "completion": "        self.empty()\n        if isinstance(startnote, six.string_types):\n            startnote = Note(startnote)\n        self.add_note(startnote)\n        self.add_notes(intervals.to_notes(shorthand, startnote, up))\n        return self\n", "idx": 1069}
{"namespace": "mingus.containers.note_container.NoteContainer.from_progression_shorthand", "completion": "        self.empty()\n        self.add_notes(chords.from_progression_shorthand(shorthand, key))\n        return self\n", "idx": 1070}
{"namespace": "mingus.containers.note_container.NoteContainer.transpose", "completion": "        if up:\n            for n in self.notes:\n                n.transpose(interval)\n        else:\n            for n in self.notes:\n                n.transpose(-interval)\n        return self", "idx": 1071}
{"namespace": "mingus.containers.note_container.NoteContainer.get_note_names", "completion": "        return [x.name for x in self.notes]\n", "idx": 1072}
{"namespace": "mingus.core.notes.int_to_note", "completion": "    if note_int < 0 or note_int > 11:\n        raise RangeError(\"Note integer must be in the range 0-11\")\n    note_list = [\"C\", \"C#\", \"D\", \"Eb\", \"E\", \"F\", \"F#\", \"G\", \"Ab\", \"A\", \"Bb\", \"B\"]\n    return note_list[note_int]\n\n", "idx": 1073}
{"namespace": "mingus.core.notes.is_valid_note", "completion": "    if note[0] not in _note_dict:\n        return False\n    if len(note) == 1:\n        return True\n    for token in note[1:]:\n        if token not in [\"b\", \"#\"]:\n            return False\n    return True", "idx": 1074}
{"namespace": "mingus.core.notes.reduce_accidentals", "completion": "    val = 0\n    for token in note[1:]:\n        if token == \"b\":\n            val -= 1\n        elif token == \"#\":\n            val += 1\n    result = note[0]\n    while val > 0:\n        result = augment(result)\n        val -= 1\n    while val < 0:\n        result = diminish(result)\n        val += 1\n    return result", "idx": 1075}
{"namespace": "mingus.core.notes.remove_redundant_accidentals", "completion": "    val = note_to_int(note[0])\n    for token in note[1:]:\n        if token == \"b\":\n            val -= 1\n        elif token == \"#\":\n            val += 1\n        else:\n            raise NoteFormatError(\"Unknown note format '%s'\" % note)\n    if val >= note_to_int(note[0]):\n        return int_to_note(val % 12)\n    else:\n        return int_to_note(val % 12, \"b\")", "idx": 1076}
{"namespace": "mingus.core.intervals.minor_second", "completion": "    sec = second(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sec, 1)", "idx": 1077}
{"namespace": "mingus.core.intervals.major_second", "completion": "    sec = second(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sec, 2)", "idx": 1078}
{"namespace": "mingus.core.intervals.minor_third", "completion": "    trd = third(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, trd, 3)", "idx": 1079}
{"namespace": "mingus.core.intervals.minor_fourth", "completion": "    frt = fourth(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, frt, 1)", "idx": 1080}
{"namespace": "mingus.core.intervals.minor_seventh", "completion": "    sth = seventh(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sth, 10)", "idx": 1081}
{"namespace": "mingus.core.intervals.major_seventh", "completion": "    # Calculate the seventh interval\n    sec = seventh(note[0], \"C\")\n    # Adjust the note by augmenting or diminishing it until the interval is equal to 11\n    return augment_or_diminish_until_the_interval_is_right(note, sec, 11)", "idx": 1082}
{"namespace": "mingus.core.intervals.measure", "completion": "    # Convert notes to integers\n    n1 = notes.note_to_int(note1)\n    n2 = notes.note_to_int(note2)\n\n    # Calculate the interval\n    interval = n2 - n1\n\n    # If the interval is negative, add 12 to it\n    if interval < 0:\n        interval += 12\n\n    # Return the interval\n    return interval", "idx": 1083}
{"namespace": "mingus.core.intervals.determine", "completion": "    # warning should be a valid note.\n    if not notes.is_valid_note(note1) or not notes.is_valid_note(note2):\n        return False\n\n    # warning should be a valid interval.\n    if not is_valid_interval(note1, note2):\n        return False\n\n    # warning should be a valid interval.\n    if not is_valid_interval(note2, note1):\n        return False\n\n    # warning should be a valid interval.\n    if not is_valid_interval(note1, note1):\n        return False\n\n    # warning should be a valid interval.\n    if not is_valid_interval(note2, note2):\n        return False\n\n    # warning should be a valid interval.\n    if not is_valid_interval(note1, note2):\n        return False\n\n    # warning should be a valid interval.\n    if not is_valid_interval(note2, note1):\n        return False\n\n    # warning should be a valid interval.\n    if not is_valid_interval(note1, note1):\n        return False\n\n    # warning should be a valid interval.\n    if not is_valid_interval(note2, note2):\n        return False\n\n    # warning should be a valid interval.\n    if not is_valid_interval(note1, note2):\n        return False\n\n    # warning should be a valid interval.\n    if not is_valid_interval(note2, note1):\n        return False\n\n    # warning should be a valid interval.\n    if not is_valid_interval(note1, note1):\n        return False\n\n    # warning should be a valid interval.\n    if not is_valid_interval(note2, note2):\n        return False\n\n    # warning should be a valid interval.\n    if not is_valid_interval(note1, note2):\n        return False\n\n    # warning should be a valid interval.\n    if not is_valid_interval(note2, note1):\n        return False\n\n    # warning should be a valid interval.\n    if not is_valid_interval(note1,", "idx": 1084}
{"namespace": "mingus.core.intervals.from_shorthand", "completion": "    if not isinstance(note, str) or not isinstance(interval, str):\n        return False\n\n    if not notes.is_valid_note(note):\n        return False\n\n    if not interval.isdigit():\n        return False\n\n    if int(interval) < 1 or int(interval) > 7:\n        return False\n\n    if interval.endswith(\"#\") or interval.endswith(\"b\"):\n        if interval.endswith(\"#\"):\n            interval = interval[:-1]\n            sharp = True\n        else:\n            interval = interval[:-1]\n            sharp = False\n    else:\n        sharp = False\n\n    if interval.endswith(\"b\"):\n        interval = interval[:-1]\n        flat = True\n    else:\n        flat = False\n\n    if interval.endswith(\"b\"):\n        interval = interval[:-1]\n        flat = True\n    else:\n        flat = False\n\n    if interval.endswith(\"b\"):\n        interval = interval[:-1]\n        flat = True\n    else:\n        flat = False\n\n    if interval.endswith(\"b\"):\n        interval = interval[:-1]\n        flat = True\n    else:\n        flat = False\n\n    if interval.endswith(\"b\"):\n        interval = interval[:-1]\n        flat = True\n    else:\n        flat = False\n\n    if interval.endswith(\"b\"):\n        interval = interval[:-1]\n        flat = True\n    else:\n        flat = False\n\n    if interval.endswith(\"b\"):\n        interval = interval[:-1]\n        flat = True\n    else:\n        flat = False\n\n    if interval.endswith(\"b\"):\n        interval = interval[:-1]\n        flat = True\n    else:\n        flat = False\n\n    if interval.endswith(\"b\"):\n        interval = interval[:-1]\n        flat = True\n    else:\n        flat = False\n\n    if interval.endswith(\"b\"):\n        interval = interval[:-1]", "idx": 1085}
{"namespace": "mingus.core.intervals.is_consonant", "completion": "    dhalf = measure(note1, note2)\n    return dhalf in [0, 2, 4, 5, 7, 9, 11] or include_fourths and dhalf == 3", "idx": 1086}
{"namespace": "mingus.core.intervals.is_perfect_consonant", "completion": "    if note1 == note2:\n        return True\n    if note1[0] == note2[0]:\n        return False\n    if measure(note1, note2) in [0, 7]:\n        return True\n    if include_fourths and measure(note1, note2) in [4, 5]:\n        return True\n    return False", "idx": 1087}
{"namespace": "mingus.core.keys.get_key", "completion": "    if accidentals not in range(-7, 8):\n        raise NoteFormatError(\"accidentals must be between -7 and 7\")\n\n    if accidentals < 0:\n        return (major_keys[7 + accidentals], minor_keys[7 + accidentals])\n    elif accidentals > 0:\n        return (major_keys[accidentals], minor_keys[accidentals])\n    else:\n        return (major_keys[0], minor_keys[0])", "idx": 1088}
{"namespace": "mingus.core.keys.get_key_signature", "completion": "    if key in _key_cache:\n        return _key_cache[key]\n    if not is_valid_key(key):\n        raise NoteFormatError(\"unrecognized format for key '%s'\" % key)\n    result = 0\n\n    if key.upper() in major_keys:\n        result = major_keys.index(key.upper()) - 7\n    elif key.upper() in minor_keys:\n        result = minor_keys.index(key.upper()) - 7\n\n    # Save result to cache\n    _key_cache[key] = result\n    return result", "idx": 1089}
{"namespace": "mingus.core.keys.get_key_signature_accidentals", "completion": "    if not is_valid_key(key):\n        raise NoteFormatError(\"unrecognized format for key '%s'\" % key)\n\n    accidentals = get_key_signature(key)\n    if accidentals == 0:\n        return []\n    elif accidentals < 0:\n        return [base_scale[i] for i in range(abs(accidentals))]\n    else:\n        return [base_scale[i] + \"#\" for i in range(accidentals)]", "idx": 1090}
{"namespace": "mingus.core.keys.get_notes", "completion": "    if not is_valid_key(key):\n        raise NoteFormatError(\"unrecognized format for key '%s'\" % key)\n\n    if key in _key_cache:\n        return _key_cache[key]\n\n    if key == \"C\":\n        res = base_scale\n    else:\n        res = base_scale[:]\n        res.append(res[0])\n        res.append(res[1])\n        res.append(res[2])\n        res.append(res[3])\n        res.append(res[4])\n        res.append(res[5])\n        res.append(res[6])\n        res.append(res[0])\n        res.append(res[1])\n        res.append(res[2])\n        res.append(res[3])\n        res.append(res[4])\n        res.append(res[5])\n        res.append(res[6])\n        res.append(res[0])\n        res.append(res[1])\n        res.append(res[2])\n        res.append(res[3])\n        res.append(res[4])\n        res.append(res[5])\n        res.append(res[6])\n        res.append(res[0])\n        res.append(res[1])\n        res.append(res[2])\n        res.append(res[3])\n        res.append(res[4])\n        res.append(res[5])\n        res.append(res[6])\n        res.append(res[0])\n        res.append(res[1])\n        res.append(res[2])\n        res.append(res[3])\n        res.append(res[4])\n        res.append(res[5])\n        res.append(res[6])\n        res.append(res[0])\n        res.append(res[1])\n        res.append(res[2])\n        res.append(res[3])\n        res.append(res[4])\n        res.append(res[5])\n        res.append(res[6", "idx": 1091}
{"namespace": "mingus.core.keys.relative_major", "completion": "    for couple in keys:\n        if key == couple[1]:\n            return couple[0]\n    raise NoteFormatError(\"'%s' is not a minor key\" % key)", "idx": 1092}
{"namespace": "mingus.core.chords.augmented_triad", "completion": "    return [note, intervals.major_third(note), intervals.augment(intervals.perfect_fifth(note))]", "idx": 1093}
{"namespace": "mingus.core.chords.determine", "completion": "    # Determine the number of notes in the chord\n    chord_length = len(chord)\n\n    # Determine the chord type based on the number of notes in the chord\n    if chord_length == 3:\n        chord_type = determine_triad(chord, shorthand, no_inversions)\n    elif chord_length == 4:\n        chord_type = determine_seventh(chord, shorthand, no_inversions, no_polychords)\n    elif chord_length == 5:\n        chord_type = determine_extended_chord5(\n            chord, shorthand, no_inversions, no_polychords\n        )\n    elif chord_length == 6:\n        chord_type = determine_extended_chord6(\n            chord, shorthand, no_inversions, no_polychords\n        )\n    elif chord_length == 7:\n        chord_type = determine_extended_chord7(\n            chord, shorthand, no_inversions, no_polychords\n        )\n    else:\n        # warning raise exception: chord is not a chord\n        return False\n\n    # Return the chord type\n    return chord_type", "idx": 1094}
{"namespace": "mingus.core.value.septuplet", "completion": "    if in_fourths:\n        return tuplet(value, 7, 4)\n    else:\n        return tuplet(value, 7, 8)", "idx": 1095}
{"namespace": "mingus.core.value.determine", "completion": "    if value in base_values:\n        return (value, 0, 1)\n    elif value in base_triplets:\n        return (value, 0, 3)\n    elif value in base_quintuplets:\n        return (value, 0, 5)\n    elif value in base_septuplets:\n        return (value, 0, 7)\n    elif value in base_values + base_triplets + base_quintuplets + base_septuplets:\n        return (value, 1, 1)\n    elif value in base_values + base_triplets + base_quintuplets + base_septuplets:\n        return (value, 2, 1)\n    elif value in base_values + base_triplets + base_quintuplets + base_septuplets:\n        return (value, 3, 1)\n    elif value in base_values + base_triplets + base_quintuplets + base_septuplets:\n        return (value, 4, 1)\n    else:\n        return (value, 0, 1)", "idx": 1096}
{"namespace": "mingus.core.progressions.substitute_major_for_minor", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Major to minor substitution\n    if (\n        suff == \"M\"\n        or suff == \"M7\"\n        or suff == \"\"\n        and roman in [\"I\", \"IV\", \"V\"]\n        or ignore_suffix\n    ):\n        n = skip(roman, 5)\n        a = interval_diff(roman, n, 9) + acc\n        if suff == \"M\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"m\")))\n        elif suff == \"M7\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"m7\")))\n        elif suff == \"\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"\")))\n    return res", "idx": 1097}
{"namespace": "mingus.core.progressions.substitute_diminished_for_diminished", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Diminished progressions\n    if (\n        suff == \"dim7\"\n        or suff == \"dim\"\n        or suff == \"\"\n        and roman in [\"VII\"]\n        or ignore_suffix\n    ):\n        if suff == \"\":\n            suff = \"dim\"\n\n        # Add diminished chord\n        last = roman\n        for x in range(4):\n            next = skip(last, 2)\n            a = interval_diff(last, next, 3) + acc\n            res.append(tuple_to_string((next, a, suff)))\n            last = next\n    return res", "idx": 1098}
{"namespace": "mingus.core.progressions.substitute_diminished_for_dominant", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Diminished to dominant substitution\n    if (\n        suff == \"dim7\"\n        or suff == \"dim\"\n        or suff == \"\"\n        and roman in [\"VII\"]\n        or ignore_suffix\n    ):\n        # Add the corresponding dominant seventh\n        res.append(tuple_to_string((skip(roman, 5), acc, \"dom7\")))\n\n        n = skip(roman, 1)\n        res.append(tuple_to_string((n, acc + interval_diff(roman, n, 1), \"dom7\")))\n\n        # Add diminished chord\n        last = roman\n        for x in range(4):\n            next = skip(last, 2)\n            acc += interval_diff(last, next, 3)\n            res.append(tuple_to_string((next, acc, suff)))\n            last = next\n    return res", "idx": 1099}
{"namespace": "mingus.core.progressions.substitute", "completion": "    res = []\n    (roman, acc, suff) = parse_string(progression[substitute_index])\n\n    # Harmonic substitutions\n    if roman == \"I\":\n        res += substitute_harmonic(progression, substitute_index, ignore_suffix)\n    elif roman == \"IV\":\n        res += substitute_harmonic(progression, substitute_index, ignore_suffix)\n    elif roman == \"V\":\n        res += substitute_harmonic(progression, substitute_index, ignore_suffix)\n\n    # Minor to major substitution\n    if (\n        suff == \"m\"\n        or suff == \"m7\"\n        or suff == \"\"\n        and roman in [\"II\", \"III\", \"VI\"]\n        or ignore_suffix\n    ):\n        res += substitute_minor_for_major(progression, substitute_index, ignore_suffix)\n\n    # Major to minor substitution\n    if (\n        suff == \"M\"\n        or suff == \"M7\"\n        or suff == \"\"\n        and roman in [\"I\", \"IV\", \"V\"]\n        or ignore_suffix\n    ):\n        res += substitute_major_for_minor(progression, substitute_index, ignore_suffix)\n\n    # Diminished progressions\n    if (\n        suff == \"dim7\"\n        or suff == \"dim\"\n        or suff == \"\"\n        and roman in [\"VII\"]\n        or ignore_suffix\n    ):\n        res += substitute_diminished_for_diminished(\n            progression, substitute_index, ignore_suffix\n        )\n        res += substitute_diminished_for_dominant(\n            progression, substitute_index, ignore_suffix\n        )\n\n    # Recursive substitutions\n    if depth > 0:\n        for i in range(len(progression)):\n            if i != substitute_index:\n                res += substitute(progression, i, depth - 1)\n\n    return res", "idx": 1100}
{"namespace": "mingus.core.progressions.skip", "completion": "    \"\"\"\n    This function takes a roman numeral and an optional skip count as input and returns the roman numeral that is a certain number of places behind in the sequence. \n    Add the skip count to the index of the given Roman numeral and divide the result by 7 to handle wraparound. The roman numeral at the calculated index is then returned as the output.\n\n    Input-Output Arguments\n    :param roman_numeral: String. The roman numeral that needs to be skipped.\n    :param skip_count: Int. The number of places to skip. It is optional and defaults to 1.\n    :return: String. The roman numeral that is a certain number of places behind in the sequence.\n\n    \"\"\"\n    \"\"\"\n    This function takes a roman numeral and an optional skip count as input and returns the roman numeral that is a certain number of places behind in the sequence. \n    Add the skip count to the index of the given Roman numeral and divide the result by 7 to handle wraparound. The roman numeral at the calculated index is then returned as the output.\n\n    Input-Output Arguments\n    :param roman_numeral: String. The roman numeral that needs to be skipped.\n    :param skip_count: Int. The number of places to skip. It is optional and defaults to 1.\n    :return: String. The roman numeral that is a certain number of places behind in the sequence.\n\n    \"\"\"\n    \"\"\"\n    This function takes a roman numeral and an optional skip count as input and returns the roman numeral that is a certain number of places behind in the sequence. \n    Add the skip count to the index of the given Roman numeral and divide the result by 7 to handle wraparound. The roman numeral at the calculated index is then returned as the output.\n\n    Input-Output Arguments\n    :param roman_numeral: String. The roman numeral that needs to be skipped.\n    :param skip_count: Int. The number of places to skip. It is optional and defaults to 1.\n    :return: String. The roman numeral that is a certain number of places behind in the sequence.\n\n    \"\"\"\n    \"\"\"\n    This function takes a roman num", "idx": 1101}
{"namespace": "exodus_bundler.cli.configure_logging", "completion": "    if quiet:\n        level = logging.ERROR\n    elif verbose:\n        level = logging.INFO\n    else:\n        level = logging.WARNING\n\n    # Set the level.\n    logging.basicConfig(level=level)\n\n    # Add a stderr handler to log warning and error messages.\n    stderr_handler = logging.StreamHandler()\n    stderr_handler.setLevel(logging.WARNING)\n    stderr_handler.setFormatter(logging.Formatter('%(levelname)s: %(message)s'))\n    logging.getLogger().addHandler(stderr_handler)\n\n    # Add an optional stdout handler to log debug and info messages.\n    if not suppress_stdout:\n        stdout_handler = logging.StreamHandler(sys.stdout)\n        stdout_handler.setLevel(logging.DEBUG)\n        stdout_handler.setFormatter(logging.Formatter('%(levelname)s: %(message)s'))\n        logging.getLogger().addHandler(stdout_handler)", "idx": 1102}
{"namespace": "exodus_bundler.bundling.create_unpackaged_bundle", "completion": "    # Create a temporary directory to store the unpackaged bundle.\n    root_directory = tempfile.mkdtemp(prefix='exodus-bundle-')\n\n    # Create a `Bundle` instance to manage the files.\n    bundle = Bundle(working_directory=root_directory, chroot=chroot)\n\n    # Add the executables to the bundle.\n    for executable in executables:\n        bundle.add_file(executable, entry_point=rename.get(executable, True))\n\n    # Add any additional files to the bundle.\n    for file in add:\n        bundle.add_file(file)\n\n    # Add any files that should not be symlinked to the bundle.\n    for file in no_symlink:\n        bundle.add_file(file, entry_point=False)\n\n    # Create the bundle.\n    bundle.create_bundle(shell_launchers=shell_launchers)\n\n    # Detect dependencies for the executables.\n    if detect:\n        for executable in executables:\n            bundle.detect_dependencies(executable)\n\n    return root_directory", "idx": 1103}
{"namespace": "exodus_bundler.bundling.detect_elf_binary", "completion": "    with open(filename, 'rb') as f:\n        first_four_bytes = f.read(4)\n        return first_four_bytes == b'\\x7fELF'", "idx": 1104}
{"namespace": "exodus_bundler.bundling.resolve_binary", "completion": "    if os.path.exists(binary):\n        return os.path.abspath(binary)\n    for directory in os.environ['PATH'].split(':'):\n        path = os.path.join(directory, binary)\n        if os.path.exists(path):\n            return os.path.abspath(path)\n    raise MissingFileError('The \"%s\" binary could not be found in $PATH.' % binary)", "idx": 1105}
{"namespace": "exodus_bundler.bundling.resolve_file_path", "completion": "    if os.path.isabs(path):\n        return os.path.normpath(os.path.abspath(path))\n    if search_environment_path:\n        return resolve_binary(path)\n    return os.path.normpath(os.path.abspath(path))", "idx": 1106}
{"namespace": "exodus_bundler.bundling.run_ldd", "completion": "    process = Popen([ldd, binary], stdout=PIPE, stderr=PIPE)\n    stdout, stderr = process.communicate()\n    combined_output = stdout.decode('utf-8').split('\\n') + stderr.decode('utf-8').split('\\n')\n    return combined_output", "idx": 1107}
{"namespace": "exodus_bundler.bundling.Elf.dependencies", "completion": "        # Initialize the set of dependencies.\n        dependencies = self.direct_dependencies\n\n        # Iteratively find the dependencies of the dependencies.\n        while True:\n            new_dependencies = set()\n            for dependency in dependencies:\n                new_dependencies |= dependency.find_direct_dependencies()\n            if not new_dependencies:\n                break\n            dependencies |= new_dependencies\n\n        return dependencies\n\n    ", "idx": 1108}
{"namespace": "exodus_bundler.bundling.File.hash", "completion": "        with open(self.path, 'rb') as f:\n            content = f.read()\n        return hashlib.sha256(content).hexdigest()\n", "idx": 1109}
{"namespace": "exodus_bundler.bundling.Bundle.add_file", "completion": "        # Resolve the path to the file.\n        path = resolve_file_path(path, search_environment_path=(entry_point is not None))\n\n        # If this is a directory, we'll add it recursively.\n        if os.path.isdir(path):\n            for root, directories, filenames in os.walk(path):\n                for filename in filenames:\n                    self.add_file(os.path.join(root, filename))\n            return\n\n        # Create a new `File` instance.\n        file = self.file_factory(path, entry_point=entry_point)\n\n        # If this is an ELF binary, we'll add all of its dependencies.\n        if file.elf:\n            for dependency in file.elf.dependencies:\n                self.add_file(dependency.path)\n\n        # Add the file to the bundle.\n        self.files.add(file)\n\n        # If this is a linker, we'll add it to the list of observed linkers.\n        if file.elf and file.elf.linker_file:\n            self.linker_files.add(file.elf.linker_file)\n\n        return file", "idx": 1110}
{"namespace": "exodus_bundler.bundling.Bundle.bundle_root", "completion": "        return os.path.join(self.working_directory, 'bundles', self.hash)\n\n    ", "idx": 1111}
{"namespace": "exodus_bundler.bundling.Bundle.hash", "completion": "        hashes = [file.hash for file in self.files]\n        hashes.sort()\n        combined_hashes = ''.join(hashes)\n        combined_hashes = combined_hashes.encode('utf-8')\n        return hashlib.sha256(combined_hashes).hexdigest()", "idx": 1112}
{"namespace": "exodus_bundler.launchers.construct_bash_launcher", "completion": "    linker_dirname, linker_basename = os.path.split(linker)\n    full_linker = '1' if full_linker else '0'\n    return render_template_file('launcher.sh', linker_basename=linker_basename,\n                                linker_dirname=linker_dirname, library_path=library_path,\n                                executable=executable, full_linker=full_linker)", "idx": 1113}
{"namespace": "exodus_bundler.input_parsing.extract_open_path", "completion": "    line = strip_pid_prefix(line)\n    for method in ['open', 'openat']:\n        prefix = method + '(\"'\n        if line.startswith(prefix):\n            line = line[len(prefix):]\n            parts = line.split('\", ')\n            if len(parts) > 1:\n                return parts[0]\n    return None", "idx": 1114}
{"namespace": "exodus_bundler.input_parsing.extract_paths", "completion": "    paths = set()\n    for line in content.splitlines():\n        line = line.strip()\n        if not line:\n            continue\n        if existing_only:\n            if os.path.isdir(line):\n                continue\n            if not os.path.exists(line):\n                continue\n        paths.add(line)\n    return list(paths)", "idx": 1115}
{"namespace": "fs.time.epoch_to_datetime", "completion": "    if t is None:\n        return None\n    return datetime.utcfromtimestamp(t).replace(tzinfo=timezone.utc)", "idx": 1116}
{"namespace": "fs.path.normpath", "completion": "    if not path:\n        return path\n\n    if _requires_normalization(path):\n        raise ValueError(\"Illegal back reference in path: {}\".format(path))\n\n    path = path.replace(\"//\", \"/\")\n    path = path.replace(\"/./\", \"/\")\n    path = path.replace(\"/../\", \"/\")\n\n    if path.endswith(\"/\"):\n        path = path[:-1]\n\n    return path", "idx": 1117}
{"namespace": "fs.path.iteratepath", "completion": "    if path in \"/\":\n        return [\"/\"]\n    return path.split(\"/\")", "idx": 1118}
{"namespace": "fs.path.recursepath", "completion": "    if reverse:\n        path = path[::-1]\n    path = normpath(path)\n    if not path:\n        return []\n    return path.split(\"/\")", "idx": 1119}
{"namespace": "fs.path.join", "completion": "    return \"/\".join(path.strip(\"/\") for path in paths)", "idx": 1120}
{"namespace": "fs.path.parts", "completion": "    path = normpath(path)\n    if path in \"/\":\n        return [\"/\"]\n    return path.split(\"/\")", "idx": 1121}
{"namespace": "fs.path.splitext", "completion": "    # TODO: This is a bit of a hack, but it's the best we can do for now.\n    #       We should probably use a regex here.\n    #       See https://github.com/PyFilesystem/pyfilesystem2/issues/100\n    if \".\" not in path:\n        return (path, \"\")\n    return path.rsplit(\".\", 1)", "idx": 1122}
{"namespace": "fs.path.isbase", "completion": "    path1 = abspath(path1)\n    path2 = abspath(path2)\n    return path1 == path2 or path1 in path2", "idx": 1123}
{"namespace": "fs.path.frombase", "completion": "    if not isparent(path1, path2):\n        raise ValueError(\"{} is not a parent directory of {}\".format(path1, path2))\n    return path2[len(path1) + 1 :]", "idx": 1124}
{"namespace": "fs.path.relativefrom", "completion": "    if not isparent(base, path):\n        raise ValueError(\"base must be a prefix of path\")\n    return path[len(base) :]", "idx": 1125}
{"namespace": "fs.path.iswildcard", "completion": "    return any(char in path for char in _WILD_CHARS)", "idx": 1126}
{"namespace": "fs.wildcard.match", "completion": "    try:\n        re_pat = _PATTERN_CACHE[(pattern, True)]\n    except KeyError:\n        res = \"(?ms)\" + _translate(pattern, case_sensitive=True) + r\"\\Z\"\n        _PATTERN_CACHE[(pattern, True)] = re_pat = re.compile(res)\n    return re_pat.match(name) is not None", "idx": 1127}
{"namespace": "fs.wildcard.imatch", "completion": "    try:\n        re_pat = _PATTERN_CACHE[(pattern, False)]\n    except KeyError:\n        res = \"(?ms)\" + _translate(pattern, False) + r\"\\Z\"\n        _PATTERN_CACHE[(pattern, False)] = re_pat = re.compile(res)\n    return re_pat.match(name) is not None", "idx": 1128}
{"namespace": "fs.wildcard.get_matcher", "completion": "    if not case_sensitive:\n        matcher = partial(imatch_any, patterns)\n    else:\n        matcher = partial(match_any, patterns)\n    return matcher", "idx": 1129}
{"namespace": "fs._url_tools.url_quote", "completion": "    if _has_drive_letter(path_snippet):\n        drive_letter, path_snippet = path_snippet.split(\":\", 1)\n        return \"{}:{}\".format(drive_letter, pathname2url(path_snippet))\n    else:\n        return pathname2url(path_snippet)", "idx": 1130}
{"namespace": "fs._ftp_parse.parse", "completion": "    parsed_lines = []\n    for line in lines:\n        line = line.strip()\n        if not line:\n            continue\n        parsed_line = parse_line(line)\n        if parsed_line is not None:\n            parsed_lines.append(parsed_line)\n    return parsed_lines", "idx": 1131}
{"namespace": "fs._ftp_parse._parse_time", "completion": "    for fmt in formats:\n        try:\n            return time.mktime(datetime.strptime(t, fmt).timetuple())\n        except ValueError:\n            pass\n    return None", "idx": 1132}
{"namespace": "fs.permissions.Permissions.parse", "completion": "        if len(ls) != 10:\n            raise ValueError(\"Invalid permissions string\")\n        perms = cls()\n        for i, c in enumerate(ls):\n            if c == \"-\":\n                continue\n            if c == \"s\":\n                perms.add(\"setuid\")\n            elif c == \"S\":\n                perms.add(\"setguid\")\n            elif c == \"t\":\n                perms.add(\"sticky\")\n            elif c == \"T\":\n                perms.add(\"sticky\")\n                perms.add(\"setguid\")\n            else:\n                perms.add(cls._LINUX_PERMS_NAMES[i])\n        return perms\n\n    ", "idx": 1133}
{"namespace": "fs.permissions.Permissions.create", "completion": "        if init is None:\n            return cls()\n        elif isinstance(init, int):\n            return cls(mode=init)\n        elif isinstance(init, Iterable):\n            return cls(names=init)\n        else:\n            raise TypeError(\"Invalid initial value\")", "idx": 1134}
{"namespace": "fs.info.Info.suffix", "completion": "        name = self.get(\"basic\", \"name\")\n        if name.startswith(\".\") and name.count(\".\") == 1:\n            return \"\"\n        return \".\" + name.split(\".\")[-1]\n\n    ", "idx": 1135}
{"namespace": "fs.info.Info.suffixes", "completion": "        name = self.get(\"basic\", \"name\")\n        if name.startswith(\".\") and name.count(\".\") == 1:\n            return []\n        return name.split(\".\")\n\n    ", "idx": 1136}
{"namespace": "fs.info.Info.stem", "completion": "        name = self.get(\"basic\", \"name\")\n        if name.startswith(\".\") and name.count(\".\") == 1:\n            return name[1:]\n        basename, dot, ext = name.rpartition(\".\")\n        return basename\n\n    ", "idx": 1137}
{"namespace": "fs.info.Info.type", "completion": "        self._require_namespace(\"details\")\n        return ResourceType(self.get(\"details\", \"type\"))\n\n    ", "idx": 1138}
{"namespace": "fs.info.Info.created", "completion": "        self._require_namespace(\"details\")\n        _time = self._make_datetime(self.get(\"details\", \"created\"))\n        return _time\n\n    ", "idx": 1139}
{"namespace": "pyinfra.connectors.mech.MechInventoryConnector.make_names_data", "completion": "        mech_config = get_mech_config(limit)\n\n        # Parse the config\n        config = {}\n        for line in mech_config:\n            if line.startswith(\"Host \"):\n                config = {}\n                config[\"Host\"] = line.split()[1]\n            else:\n                key, value = line.split(\" \", 1)\n                config[key] = value\n\n        return [_make_name_data(config)]", "idx": 1140}
{"namespace": "pyinfra.connectors.ansible.AnsibleInventoryConnector.make_names_data", "completion": "        if inventory_filename is None:\n            raise InventoryError(\"No Ansible inventory filename provided\")\n\n        if not path.exists(inventory_filename):\n            raise InventoryError(\n                \"Could not find Ansible inventory file: {0}\".format(inventory_filename),\n            )\n\n        return parse_inventory(inventory_filename)", "idx": 1141}
{"namespace": "pyinfra.operations.files.rsync", "completion": "    show_rsync_warning()\n\n    yield RsyncCommand(src, dest, flags)", "idx": 1142}
{"namespace": "pyinfra.operations.files.get", "completion": "    # Add deploy directory?\n    if add_deploy_dir and state.cwd:\n        src = os.path.join(state.cwd, src)\n\n    # Ensure the source file exists\n    if not os.path.isfile(src):\n        raise IOError(\"No such file: {0}\".format(src))\n\n    # Ensure the destination directory - if the destination is a link, ensure\n    # the link target is a directory.\n    dest_to_ensure = dest\n    dest_link_info = host.get_fact(Link, path=dest)\n    if dest_link_info:\n        dest_to_ensure = dest_link_info[\"link_target\"]\n\n    if create_local_dir:\n        dest_dir = os.path.dirname(dest)\n        if dest_dir and not os.path.isdir(dest_dir):\n            os.makedirs(dest_dir)\n\n    # Ensure the destination directory\n    yield from directory(\n        path=dest_to_ensure,\n        _no_check_owner_mode=True,  # don't check existing user/mode\n        _no_fail_on_link=True,  # don't fail if the path is a link\n    )\n\n    # Download the file\n    yield FileDownloadCommand(src, dest)", "idx": 1143}
{"namespace": "pyinfra.operations.files.put", "completion": "    if add_deploy_dir and state.cwd:\n        dest = os.path.join(state.cwd, dest)\n\n    # Ensure the destination directory - if the destination is a link, ensure\n    # the link target is a directory.\n    dest_to_ensure = dest\n    dest_link_info = host.get_fact(Link, path=dest)\n    if dest_link_info:\n        dest_to_ensure = dest_link_info[\"link_target\"]\n\n    yield from directory(\n        path=dest_to_ensure,\n        user=user,\n        group=group,\n        _no_check_owner_mode=True,  # don't check existing user/mode\n        _no_fail_on_link=True,  # don't fail if the path is a link\n    )\n\n    # Ensure the local file exists\n    if not os.path.exists(src):\n        raise OperationError(\"No such file: {0}\".format(src))\n\n    # Ensure the local file is a file\n    if not os.path.isfile(src):\n        raise OperationError(\"{0} is not a file\".format(src))\n\n    # Ensure the local file is not a link\n    if os.path.islink(src):\n        raise OperationError(\"{0} is a link\".format(src))\n\n    # Ensure the local file is not a directory\n    if os.path.isdir(src):\n        raise OperationError(\"{0} is a directory\".format(src))\n\n    # Ensure the local file is not a broken link\n    if not os.path.exists(src):\n        raise OperationError(\"{0} is a broken link\".format(src))\n\n    # Ensure the local file is not a broken link\n    if not os.path.isfile(src):\n        raise OperationError(\"{0} is not a file\".format(src))\n\n    # Ensure the local file is not a broken link\n    if not os.path.exists(src):\n        raise OperationError(\"{0} is a broken link\".format(src))\n\n   ", "idx": 1144}
{"namespace": "pyinfra.operations.files.file", "completion": "    path = _validate_path(path)\n\n    mode = ensure_mode_int(mode)\n    info = host.get_fact(File, path=path)\n\n    if info is False:  # not a file\n        yield from _raise_or_remove_invalid_path(\n            \"file\",\n            path,\n            force,\n            force_backup,\n            force_backup_dir,\n        )\n        info = None\n\n    if not present:\n        if info:\n            yield StringCommand(\"rm\", \"-f\", QuoteString(path))\n        else:\n            host.noop(\"file {0} does not exist\")\n        return\n\n    if info is None:  # create\n        if create_remote_dir:\n            yield from _create_remote_dir(state, host, path, user, group)\n\n        if touch:\n            yield StringCommand(\"touch\", QuoteString(path))\n        else:\n            yield StringCommand(\"touch\", QuoteString(path))\n\n        if user or group:\n            yield file_utils.chown(path, user, group)\n\n        if mode:\n            yield file_utils.chmod(path, mode)\n\n    else:  # update\n        changed = False\n\n        if mode and (not info or info[\"mode\"] != mode):\n            yield file_utils.chmod(path, mode)\n            changed = True\n\n        if (user and info[\"user\"] != user) or (group and info[\"group\"] != group):\n            yield file_utils.chown(path, user, group)\n            changed = True\n\n        if not changed:\n            host.noop(\"file {0} already exists\".format(path))", "idx": 1145}
{"namespace": "pyinfra.operations.python.call", "completion": "    kwargs.pop(\"state\", None)\n    kwargs.pop(\"host\", None)\n    yield FunctionCommand(function, args, kwargs)", "idx": 1146}
{"namespace": "pyinfra.api.operation.add_op", "completion": "    # Get the meta kwargs (globals that apply to all hosts)\n    global_arguments, global_argument_keys = pop_global_arguments(kwargs)\n\n    # If this op is being called inside another, just return here\n    # (any unwanted/op-related kwargs removed above).\n    if host.in_op and not host.in_callback_op:\n        if global_argument_keys:\n            _error_msg = \"Nested operation called with global arguments: {0} ({1})\".format(\n                global_argument_keys,\n                get_call_location(),\n            )\n            raise PyinfraError(_error_msg)\n        return\n\n    names, add_args = _generate_operation_name(op_func, host, kwargs, global_arguments)\n    op_order, op_hash = _solve_operation_consistency(names, state, host)\n\n    # Ensure shared (between servers) operation meta, mutates state\n    op_meta = _ensure_shared_op_meta(state, op_hash, op_order, global_arguments, names)\n\n    # Attach normal args, if we're auto-naming this operation\n    if add_args:\n        op_meta = _attach_args(op_meta, args, kwargs)\n\n    # Check if we're actually running the operation on this host\n    # Run once and we've already added meta for this op? Stop here.\n    if op_meta.global_arguments[\"_run_once\"]:\n        has_run = False\n        for ops in state.ops.values():\n            if op_hash in ops:\n                has_run = True\n                break\n\n        if has_run:\n            return OperationMeta(op_hash)\n\n    # \"Run\" operation - here we make a generator that will yield out actual commands to execute\n    # and, if we're diff-ing, we then iterate the generator now to determine if any changes\n    # *would* be made based on the *current* remote state.\n\n    def command_generator() -> Iterator[PyinfraCommand", "idx": 1147}
{"namespace": "pyinfra.api.facts.get_facts", "completion": "    facts = {}\n    greenlets = []\n\n    for host in state.get_active_hosts():\n        greenlets.append(\n            gevent.spawn(\n                get_fact,\n                state,\n                host,\n                *args,\n                **kwargs,\n            ),\n        )\n\n    gevent.joinall(greenlets)\n\n    for greenlet in greenlets:\n        host, data = greenlet.value\n        facts[host] = data\n\n    return facts\n\n", "idx": 1148}
{"namespace": "pyinfra.api.operations.run_ops", "completion": "    if serial:\n        _run_serial_ops(state)\n    elif no_wait:\n        _run_no_wait_ops(state)\n    else:\n        _run_single_op(state, state.get_op_order()[0])", "idx": 1149}
{"namespace": "pyinfra.api.connect.connect_all", "completion": "    # Connect to all hosts in parallel\n    gevent.joinall([gevent.spawn(host.connect) for host in state.hosts])\n\n    # Activate hosts that are connected\n    state.activate_hosts(state.connected_hosts)\n\n    # Update the state inventory\n    state.update_inventory()", "idx": 1150}
{"namespace": "pyinfra.api.arguments.pop_global_arguments", "completion": "    # Get the keys to check\n    if keys_to_check is None:\n        keys_to_check = list(all_argument_meta.keys())\n\n    # Get the arguments from the state and host\n    if state is not None and host is not None:\n        state_args = state.get_host_data(host)\n        host_args = host.data\n    else:\n        state_args = {}\n        host_args = {}\n\n    # Get the arguments from the config\n    config_args = state.config.get_all_data()\n\n    # Get the arguments from the kwargs\n    kwargs_args = {}\n    for key in keys_to_check:\n        if key in kwargs:\n            kwargs_args[key] = kwargs.pop(key)\n\n    # Merge the arguments\n    args = {**kwargs_args, **state_args, **host_args, **config_args}\n\n    # Handle the arguments\n    for key, meta in all_argument_meta.items():\n        if key in args:\n            if meta.handler is not default_sentinel:\n                args[key] = meta.handler(state.config, args[key])\n            else:\n                args[key] = meta.default(state.config)\n\n    # Return the arguments\n    return args, list(args.keys())", "idx": 1151}
{"namespace": "pyinfra_cli.commands.get_func_and_args", "completion": "    operation = commands[0]\n    operation_func = try_import_module_attribute(operation, prefix=\"pyinfra.operations\")\n\n    args = commands[1:]\n\n    if operation_func.requires_facts:\n        args = get_facts_and_args(args)\n\n    return operation_func, args\n\n", "idx": 1152}
{"namespace": "viztracer.tracer._VizTracer.start", "completion": "        if self.enable:\n            return\n        self.enable = True\n        if self.log_print:\n            self.overload_print()\n        self._tracer.enable()\n\n    ", "idx": 1153}
{"namespace": "viztracer.tracer._VizTracer.stop", "completion": "        self.enable = False\n        if self.log_print:\n            self.restore_print()\n        self._tracer.stop()\n\n    ", "idx": 1154}
{"namespace": "viztracer.report_builder.ReportBuilder.save", "completion": "        if isinstance(output_file, str):\n            if output_file.endswith(\".html\"):\n                with open(output_file, \"w\", encoding=\"utf-8\") as f:\n                    self.generate_report(f, \"html\", file_info=file_info)\n            elif output_file.endswith(\".json\"):\n                with open(output_file, \"w\", encoding=\"utf-8\") as f:\n                    self.generate_report(f, \"json\", file_info=file_info)\n            elif output_file.endswith(\".gz\"):\n                with gzip.open(output_file, \"wb\") as f:\n                    self.generate_report(f, \"json\", file_info=file_info)\n            else:\n                raise ValueError(\"Invalid output file format\")\n            self.final_messages.append((\"view_command\", {\"output_file\": output_file}))\n        elif isinstance(output_file, TextIO):\n            self.generate_report(output_file, \"json\", file_info=file_info)\n        else:\n            raise TypeError(\"Invalid output file type\")", "idx": 1155}
{"namespace": "viztracer.code_monkey.AstTransformer.get_assign_targets_with_attr", "completion": "        ret: List[ast.Attribute] = []\n        if isinstance(node, ast.Name):\n            return ret\n        elif isinstance(node, (ast.Attribute, ast.Subscript, ast.Starred)):\n            return self.get_assign_targets_with_attr(node.value)\n        elif isinstance(node, ast.Tuple) or isinstance(node, ast.List):\n            return reduce(lambda a, b: a + b, [self.get_assign_targets_with_attr(elt) for elt in node.elts])\n        elif isinstance(node, ast.Slice):\n            return self.get_assign_targets_with_attr(node.lower) + self.get_assign_targets_with_attr(node.upper)\n        elif isinstance(node, ast.ExtSlice):\n            return reduce(lambda a, b: a + b, [self.get_assign_targets_with_attr(dim) for dim in node.dims])\n        elif isinstance(node, ast.Index):\n            return self.get_assign_targets_with_attr(node.value)\n        color_print(\"WARNING\", \"Unexpected node type {} for ast.Assign. \\\n            Please report to the author github.com/gaogaotiantian/viztracer\".format(type(node)))\n        return ret", "idx": 1156}
{"namespace": "viztracer.code_monkey.SourceProcessor.process", "completion": "        if isinstance(source, bytes):\n            source = source.decode(\"utf-8\")\n        if not isinstance(source, str):\n            return source\n        lines = source.splitlines()\n        new_lines = []\n        for line in lines:\n            for pattern, transform in self.re_patterns:\n                re_match = pattern.match(line)\n                if re_match:\n                    new_lines.append(transform(re_match))\n                    break\n            else:\n                new_lines.append(line)\n        return \"\\n\".join(new_lines)", "idx": 1157}
{"namespace": "wal_e.log_help.WalELogger.fmt_logline", "completion": "        if detail:\n            msg = \"MSG: {0} DETAIL: {1}\".format(msg, detail)\n        if hint:\n            msg = \"{0} HINT: {1}\".format(msg, hint)\n        if structured:\n            msg = \"{0} STRUCTURED: {1}\".format(msg, self._fmt_structured(structured))\n        return '\\n'.join([msg])", "idx": 1158}
{"namespace": "wal_e.blobstore.file.calling_format.Bucket.delete_keys", "completion": "        for key in keys:\n            os.remove(key.path)\n        remove_empty_dirs(common_dir_path(keys))", "idx": 1159}
{"namespace": "wal_e.worker.upload_pool.TarUploadPool.put", "completion": "        if self.closed:\n            raise tar_partition.TarPartitionError(\"Pool closed\")\n\n        if self.member_burden >= self.max_members:\n            raise tar_partition.TarPartitionError(\"Too many members\")\n\n        if self.concurrency_burden >= self.max_concurrency:\n            self._wait()\n\n        self._start(tpart)", "idx": 1160}
{"namespace": "wal_e.worker.pg.wal_transfer.WalSegment.from_ready_archive_status", "completion": "        # Iterate through the files in the archive_status directory\n        for filename in os.listdir(path.join(xlog_dir, 'archive_status')):\n            # Filter out non-segment files\n            if not re.match(storage.SEGMENT_REGEXP, filename):\n                continue\n\n            # Create a WalSegment instance for each segment file\n            yield WalSegment(path.join(xlog_dir, 'archive_status', filename))\n\n", "idx": 1161}
{"namespace": "wal_e.worker.pg.wal_transfer.WalTransferGroup.join", "completion": "        # Wait for the transfer to exit.\n        self.wait_change.get()\n\n        # Close the input WalTransferGroup instance.\n        self.closed = True\n\n        # Wait a while for all running greenlets to exit.\n        gevent.sleep(30)\n\n        # Attempt to force all running greenlets to exit.\n        gevent.killall(self.greenlets)", "idx": 1162}
{"namespace": "wal_e.worker.pg.wal_transfer.WalTransferGroup.start", "completion": "        # Start a transfer of a segment.\n        #\n        # This is a blocking call, so it's safe to call from the\n        # main thread.\n        #\n        # This function creates a gevent.Greenlet instance to execute the transferer function with the given segment as an argument. It then adds the gevent.Greenlet instance to the set of greenlets and starts the execution.\n        # Input-Output Arguments\n        # :param self: WalTransferGroup. An instance of the WalTransferGroup class.\n        # :param segment: The wal segment to transfer.\n        # :return: No return values.\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #\n        #", "idx": 1163}
{"namespace": "mrjob.py2.to_unicode", "completion": "    if isinstance(s, bytes):\n        return s.decode('utf_8')\n    else:\n        return s", "idx": 1164}
{"namespace": "mrjob.job.MRJob.steps", "completion": "        steps = []\n        step_kwargs = {}\n        for method in dir(self):\n            if method.startswith('_'):\n                continue\n            if method == 'steps':\n                continue\n            if method == 'mapper':\n                step_kwargs['mapper'] = self._get_mapper_step(method)\n            elif method == 'reducer':\n                step_kwargs['reducer'] = self._get_reducer_step(method)\n            elif method == 'combiner':\n                step_kwargs['combiner'] = self._get_combiner_step(method)\n            elif method == 'spark':\n                steps.append(SparkStep(spark=self._get_spark_step(method)))\n            else:\n                continue\n        steps.append(MRStep(**step_kwargs))\n        return steps\n", "idx": 1165}
{"namespace": "mrjob.job.MRJob.increment_counter", "completion": "        line = 'reporter:counter:{group},{counter},{amount}\\n'.format(group=group.replace(',', ';'), counter=counter.replace(',', ';'), amount=amount)\n        if not isinstance(line, bytes):\n            line = line.encode('utf_8')\n\n        self.stderr.write(line)\n        self.stderr.flush()", "idx": 1166}
{"namespace": "mrjob.job.MRJob.set_status", "completion": "        # don't allow people to pass in floats\n        from mrjob.py2 import integer_types\n        if not isinstance(msg, integer_types):\n            raise TypeError('msg must be an integer, not %r' % (msg,))\n\n        # cast non-strings to strings (if people pass in exceptions, etc)\n        if not isinstance(msg, string_types):\n            msg = str(msg)\n\n        line = 'reporter:status:%s\\n' % msg\n        if not isinstance(line, bytes):\n            line = line.encode('utf_8')\n\n        self.stderr.write(line)\n        self.stderr.flush()\n", "idx": 1167}
{"namespace": "mrjob.job.MRJob.run_job", "completion": "        # set up logging\n        self.set_up_logging(quiet=self.options.quiet,\n                            verbose=self.options.verbose,\n                            stream=self.stdout)\n\n        # set up runner\n        runner = self.make_runner()\n\n        # run the job\n        runner.run()\n\n        # if we're not running in a Spark executor, concatenate the output\n        if not self.options.run_spark and self._should_cat_output():\n            self.cat_output(runner.cat_output())\n\n        # if we're running in a Spark executor, we don't need to do anything\n        # else\n", "idx": 1168}
{"namespace": "mrjob.job.MRJob.set_up_logging", "completion": "        if quiet:\n            log.setLevel(logging.ERROR)\n        elif verbose:\n            log.setLevel(logging.DEBUG)\n        else:\n            log.setLevel(logging.INFO)\n\n        if stream is None:\n            stream = sys.stderr\n\n        # set up the main logger\n        log.addHandler(logging.StreamHandler(stream))\n\n        # set up the __main__ logger\n        main_log = logging.getLogger('__main__')\n        main_log.setLevel(logging.DEBUG)\n        main_log.addHandler(logging.StreamHandler(stream))\n\n        # set up the mrjob logger\n        mrjob_log = logging.getLogger('mrjob')\n        mrjob_log.setLevel(logging.DEBUG)\n        mrjob_log.addHandler(logging.StreamHandler(stream))\n\n        # set up the mrjob.utils logger\n        utils_log = logging.getLogger('mrjob.utils')\n        utils_log.setLevel(logging.DEBUG)\n        utils_log.addHandler(logging.StreamHandler(stream))\n\n        # set up the mrjob.protocol logger\n        protocol_log = logging.getLogger('mrjob.protocol')\n        protocol_log.setLevel(logging.DEBUG)\n        protocol_log.addHandler(logging.StreamHandler(stream))\n\n        # set up the mrjob.step logger\n        step_log = logging.getLogger('mrjob.step')\n        step_log.setLevel(logging.DEBUG)\n        step_log.addHandler(logging.StreamHandler(stream))\n\n        # set up the mrjob.runner logger\n        runner_log = logging.getLogger('mrjob.runner')\n        runner_log.setLevel(logging.DEBUG)\n        runner_log.addHandler(logging.StreamHandler(stream))\n\n        # set up the mrjob.sandbox logger\n        sandbox_log = logging.getLogger('mrjob.sandbox')\n        sandbox_log.setLevel(logging.DEBUG)\n        sandbox_log.addHandler(logging.StreamHandler(", "idx": 1169}
{"namespace": "mrjob.job.MRJob.execute", "completion": "        # load options from the command line\n        self.execute()\n", "idx": 1170}
{"namespace": "mrjob.job.MRJob._runner_kwargs", "completion": "        kwargs = self._non_option_kwargs()\n        kwargs.update(self._job_kwargs())\n        kwargs.update(self._kwargs_from_switches(\n            ('hadoop_input_format', 'hadoop_output_format', 'jobconf',\n             'libjars', 'output_dir', 'partitioner', 'sort_values',\n             'upload_archives', 'upload_dirs', 'upload_files')))\n        kwargs.update(self._steps_desc())\n        return kwargs", "idx": 1171}
{"namespace": "mrjob.job.MRJob.run_mapper", "completion": "        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'mapper')\n\n        for k, v in self.map_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)", "idx": 1172}
{"namespace": "mrjob.job.MRJob.run_combiner", "completion": "        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'combiner')\n\n        for k, v in self.combine_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)", "idx": 1173}
{"namespace": "mrjob.job.MRJob.add_passthru_arg", "completion": "        pass_opt = self.arg_parser.add_argument(*args, **kwargs)\n\n        self._passthru_arg_dests.add(pass_opt.dest)", "idx": 1174}
{"namespace": "mrjob.job.MRJob.is_task", "completion": "        return self.options.task", "idx": 1175}
{"namespace": "mrjob.job.MRJob.parse_output", "completion": "        # TODO: this is a bit of a hack, but it's the best way to get\n        #       around the fact that we can't pass a generator to\n        #       self.output_protocol.read()\n        #       (see https://github.com/Yelp/mrjob/issues/1690)\n        #       We should probably fix that.\n        #       (see https://github.com/Yelp/mrjob/issues/1690)\n        #       (see https://github.com/Yelp/mrjob/issues/1690)\n        #       (see https://github.com/Yelp/mrjob/issues/1690)\n        #       (see https://github.com/Yelp/mrjob/issues/1690)\n        #       (see https://github.com/Yelp/mrjob/issues/1690)\n        #       (see https://github.com/Yelp/mrjob/issues/1690)\n        #       (see https://github.com/Yelp/mrjob/issues/1690)\n        #       (see https://github.com/Yelp/mrjob/issues/1690)\n        #       (see https://github.com/Yelp/mrjob/issues/1690)\n        #       (see https://github.com/Yelp/mrjob/issues/1690)\n        #       (see https://github.com/Yelp/mrjob/issues/1690)\n        #       (see https://github.com/Yelp/mrjob/issues/1690)\n        #       (see https://github.com/Yelp/mrjob/issues/1690)\n        #       (see https://github.com/Yelp/mrjob/issues/1690)\n        #       (see https://github.com/Yelp/mrjob/issues/1690)\n        #       (see https://github.com/Yelp/mrjob/issues/169", "idx": 1176}
{"namespace": "mrjob.job.MRJob.sandbox", "completion": "        if stdin is not None:\n            self.stdin = stdin\n        if stdout is not None:\n            self.stdout = stdout\n        if stderr is not None:\n            self.stderr = stderr\n        return self", "idx": 1177}
{"namespace": "mrjob.hadoop.fully_qualify_hdfs_path", "completion": "    if path.startswith('hdfs://'):\n        return path\n    elif path.startswith('/'):\n        return 'hdfs://' + path\n    else:\n        return 'hdfs:///user/%s/%s' % (getpass.getuser(), path)", "idx": 1178}
{"namespace": "mrjob.hadoop.HadoopJobRunner.fs", "completion": "        if not hasattr(self, '_fs'):\n            self._fs = CompositeFilesystem()\n            self._fs.add_filesystem(self.fs.hadoop)\n            self._fs.add_filesystem(self.fs.local)\n        return self._fs\n", "idx": 1179}
{"namespace": "mrjob.hadoop.HadoopJobRunner._find_hadoop_streaming_jar", "completion": "        for dir in self._hadoop_streaming_jar_dirs():\n            log.info('Looking for Hadoop streaming jar in %s...' % dir)\n            for file in os.listdir(dir):\n                if _HADOOP_STREAMING_JAR_RE.match(file):\n                    return os.path.join(dir, file)\n        return None\n", "idx": 1180}
{"namespace": "mrjob.hadoop.HadoopJobRunner._find_binaries_and_jars", "completion": "        # Find the Hadoop binary\n        hadoop_bin = self.get_hadoop_bin()\n        if not hadoop_bin:\n            raise Exception('no Hadoop binary')\n\n        # Check if there are any Hadoop streaming steps\n        if self._has_hadoop_streaming_steps():\n            # Find the Hadoop streaming jar\n            hadoop_streaming_jar = self.get_hadoop_streaming_jar()\n            if not hadoop_streaming_jar:\n                raise Exception('no Hadoop streaming jar')\n\n        # Check if there are any Spark steps\n        if self._has_spark_steps():\n            # Find the Spark submit binary\n            spark_submit_bin = self.get_spark_submit_bin()\n            if not spark_submit_bin:\n                raise Exception('no Spark submit binary')\n", "idx": 1181}
{"namespace": "mrjob.hadoop.HadoopJobRunner._args_for_streaming_step", "completion": "        if not self.get_hadoop_streaming_jar():\n            raise ValueError('no Hadoop streaming jar')\n\n        step = self._get_step(step_num)\n\n        args = []\n\n        args.extend(self.get_hadoop_bin())\n\n        args.extend(['jar', self.get_hadoop_streaming_jar()])\n\n        args.extend(self._interpolate_streaming_step_args(step['args'],\n                                                          step_num))\n\n        return args", "idx": 1182}
{"namespace": "mrjob.hadoop.HadoopJobRunner._stream_history_log_dirs", "completion": "        if not self._read_logs():\n            return\n\n        for log_dir in unique(self._hadoop_log_dirs(output_dir=output_dir)):\n            path = self.fs.join(log_dir, 'history')\n            if _logs_exist(self.fs, path):\n                log.info('Looking for history log in %s...' % path)\n                yield [path]", "idx": 1183}
{"namespace": "mrjob.hadoop.HadoopJobRunner._stream_task_log_dirs", "completion": "        if not self._read_logs():\n            return\n\n        for log_dir in unique(self._hadoop_log_dirs(output_dir=output_dir)):\n            if _logs_exist(self.fs, log_dir):\n                if application_id:\n                    log_dir = posixpath.join(log_dir, 'userlogs', application_id)\n                else:\n                    log_dir = posixpath.join(log_dir, 'userlogs')\n\n                log.info('Looking for task logs in %s...' % log_dir)\n                yield [log_dir]", "idx": 1184}
{"namespace": "mrjob.setup.UploadDirManager.add", "completion": "        if is_uri(path):\n            return path\n\n        if path in self._path_to_name:\n            return posixpath.join(self.prefix, self._path_to_name[path])\n\n        filename = os.path.basename(path)\n        name = name_uniquely(path, self._names_taken)\n        self._path_to_name[path] = name\n        self._names_taken.add(name)\n\n        return posixpath.join(self.prefix, name)", "idx": 1185}
{"namespace": "mrjob.setup.UploadDirManager.uri", "completion": "        if path not in self._path_to_name:\n            raise ValueError('%r is not a URI or a known local file' % path)\n\n        return posixpath.join(self.prefix, self._path_to_name[path])", "idx": 1186}
{"namespace": "mrjob.setup.UploadDirManager.path_to_uri", "completion": "        return self._path_to_name", "idx": 1187}
{"namespace": "mrjob.setup.WorkingDirManager.name_to_path", "completion": "        if type not in self._SUPPORTED_TYPES:\n            raise ValueError('bad path type %r, must be one of: %s' % (\n                type, ', '.join(sorted(self._SUPPORTED_TYPES))))\n\n        name_to_path = {}\n\n        for name, (path_type, path) in self._name_to_typed_path.items():\n            if type is None or path_type == type:\n                name_to_path[name] = path\n\n        return name_to_path", "idx": 1188}
{"namespace": "mrjob.setup.WorkingDirManager.paths", "completion": "        if type is not None:\n            self._check_type(type)\n\n        paths = set()\n        for path_type, path in self._typed_path_to_auto_name.items():\n            if type is None or path_type == type:\n                paths.add(path)\n\n        for path in self._name_to_typed_path.values():\n            if type is None or path[0] == type:\n                paths.add(path)\n\n        return paths\n\n    ", "idx": 1189}
{"namespace": "mrjob.compat.jobconf_from_env", "completion": "    if variable in os.environ:\n        return os.environ[variable]\n\n    for alternative in _JOBCONF_MAP.get(variable, {}).values():\n        if alternative in os.environ:\n            return os.environ[alternative]\n\n    return default\n\n", "idx": 1190}
{"namespace": "mrjob.compat.jobconf_from_dict", "completion": "    # try variable verbatim first\n    if name in jobconf:\n        return jobconf[name]\n\n    # try alternatives (arbitrary order)\n    for var in _JOBCONF_MAP.get(name, {}).values():\n        if var in jobconf:\n            return jobconf[var]\n\n    return default", "idx": 1191}
{"namespace": "mrjob.compat.translate_jobconf", "completion": "    if variable in _JOBCONF_MAP:\n        return map_version(version, _JOBCONF_MAP[variable])\n    else:\n        return variable", "idx": 1192}
{"namespace": "mrjob.compat.translate_jobconf_for_all_versions", "completion": "    if variable in _JOBCONF_MAP:\n        return sorted(_JOBCONF_MAP[variable].keys())\n    else:\n        return [variable]", "idx": 1193}
{"namespace": "mrjob.compat.translate_jobconf_dict", "completion": "    if hadoop_version is None:\n        hadoop_version = '2.0'\n\n    if not isinstance(jobconf, dict):\n        raise TypeError('%r is not a dictionary' % jobconf)\n\n    if not isinstance(hadoop_version, string_types):\n        raise TypeError('%r is not a string' % hadoop_version)\n\n    translated_jobconf = {}\n    untranslated_jobconf = []\n\n    for key, value in jobconf.items():\n        translated_key = translate_jobconf(key, hadoop_version)\n        if translated_key != key:\n            untranslated_jobconf.append(key)\n        translated_jobconf[translated_key] = value\n\n    if untranslated_jobconf:\n        log.warning('Detected hadoop configuration property names that do not match version %s:\\nThe have been translated to the following names:\\n%s',\n                    hadoop_version, '\\n'.join(sorted(translate_jobconf_for_all_versions(key) for key in untranslated_jobconf)))\n\n    return translated_jobconf", "idx": 1194}
{"namespace": "mrjob.compat.uses_yarn", "completion": "    if not isinstance(version, string_types):\n        raise TypeError('%r is not a string' % version)\n\n    return LooseVersion(version) >= LooseVersion('2.0.0')", "idx": 1195}
{"namespace": "mrjob.local.LocalMRJobRunner._spark_master", "completion": "        # TODO: this is a hack to get the number of cores on the machine\n        # TODO: this should be a jobconf\n        num_cores = self._opts['num_cores']\n        num_executors = int(math.ceil(num_cores / 2.0))\n        cores_per_executor = int(math.ceil(num_cores / num_executors))\n        executor_memory = _to_num_bytes(\n            self._opts.get('executor_memory', _DEFAULT_EXECUTOR_MEMORY))\n        executor_memory = int(math.ceil(executor_memory / 1024.0 / 1024.0))\n        return 'local-cluster[{},{},{}]'.format(\n            num_executors, cores_per_executor, executor_memory)", "idx": 1196}
{"namespace": "mrjob.runner.MRJobRunner._bootstrap_mrjob", "completion": "        if self._opts.get('bootstrap_mrjob'):\n            return True\n        else:\n            return False\n", "idx": 1197}
{"namespace": "mrjob.conf._fix_clear_tags", "completion": "    if isinstance(x, list):\n        return [_fix_clear_tags(item) for item in x]\n    elif isinstance(x, dict):\n        return {key: _fix_clear_tags(value) for key, value in x.items()}\n    elif isinstance(x, ClearedValue):\n        return x.value\n    else:\n        return x", "idx": 1198}
{"namespace": "mrjob.conf.load_opts_from_mrjob_conf", "completion": "    if already_loaded is None:\n        already_loaded = []\n\n    conf_path = _expanded_mrjob_conf_path(conf_path)\n    return _load_opts_from_mrjob_conf(runner_alias, conf_path, already_loaded)", "idx": 1199}
{"namespace": "mrjob.conf.load_opts_from_mrjob_confs", "completion": "    if conf_paths is None:\n        conf_paths = [find_mrjob_conf()]\n\n    confs = []\n    for conf_path in conf_paths:\n        confs.extend(load_opts_from_mrjob_conf(runner_alias, conf_path))\n\n    if not confs:\n        log.warning('No config specified for %s runner' % runner_alias)\n\n    return confs", "idx": 1200}
{"namespace": "mrjob.conf.dump_mrjob_conf", "completion": "    if yaml:\n        _dump_yaml_with_clear_tags(conf, f)\n    else:\n        json.dump(conf, f, indent=2, sort_keys=True)", "idx": 1201}
{"namespace": "mrjob.conf.combine_lists", "completion": "    result = []\n    for seq in seqs:\n        if seq is not None:\n            if isinstance(seq, string_types):\n                result.append(seq)\n            else:\n                result.extend(seq)\n    return result", "idx": 1202}
{"namespace": "mrjob.conf.combine_cmds", "completion": "    for cmd in reversed(cmds):\n        if cmd is not None:\n            if isinstance(cmd, string_types):\n                return shlex_split(cmd)\n            else:\n                return cmd\n    else:\n        return None", "idx": 1203}
{"namespace": "mrjob.conf.combine_dicts", "completion": "    result = {}\n    for d in dicts:\n        if d:\n            for k, v in d.items():\n                if isinstance(v, ClearedValue) and v.value is None:\n                    result.pop(k, None)\n                else:\n                    result[k] = _strip_clear_tag(v)\n    return result", "idx": 1204}
{"namespace": "mrjob.conf.combine_jobconfs", "completion": "    result = {}\n    for jobconf in jobconfs:\n        if jobconf:\n            for k, v in jobconf.items():\n                if v is not None:\n                    result[k] = _to_java_str(v)\n    return result", "idx": 1205}
{"namespace": "mrjob.conf.combine_path_lists", "completion": "    result = []\n\n    for path_seq in path_seqs:\n        if path_seq is None:\n            continue\n\n        if isinstance(path_seq, string_types):\n            path_seq = [path_seq]\n\n        for path in path_seq:\n            path = expand_path(path)\n            if os.path.isdir(path):\n                result.extend(glob.glob(path))\n            else:\n                result.append(path)\n\n    return result", "idx": 1206}
{"namespace": "mrjob.conf.combine_opts", "completion": "    # collect all keys from the dictionaries that are not wrapped in ClearedValue\n    keys = set()\n    for opts in opts_list:\n        for key in opts.keys():\n            if not isinstance(opts[key], ClearedValue):\n                keys.add(key)\n\n    # iterate through each key and use the sub-combiner specified in the combiners map for that key, or defaults to a function\n    combined_opts = {}\n    for key in keys:\n        sub_combiner = combiners.get(key, combine_values)\n        combined_opts[key] = sub_combiner(*[opts.get(key) for opts in opts_list])\n\n    return combined_opts", "idx": 1207}
{"namespace": "mrjob.bin.MRJobBinRunner._task_python_bin", "completion": "        return self._opts.get('task_python_bin') or self._python_bin()", "idx": 1208}
{"namespace": "mrjob.bin.MRJobBinRunner.get_spark_submit_bin", "completion": "        if not self._spark_submit_bin:\n            self._spark_submit_bin = self._find_spark_submit_bin()\n        return self._spark_submit_bin", "idx": 1209}
{"namespace": "mrjob.step.StepFailedException.__str__", "completion": "        if self.reason:\n            return '%s failed: %s' % (self.step_description(), self.reason)\n        else:\n            return '%s failed' % self.step_description()\n\n    ", "idx": 1210}
{"namespace": "mrjob.step.StepFailedException.__repr__", "completion": "        return '%s(%s)' % (\n            self.__class__.__name__,\n            ', '.join('%s=%r' % (k, getattr(self, k)) for k in self._FIELDS))", "idx": 1211}
{"namespace": "mrjob.step.MRStep.description", "completion": "        desc = {'type': 'streaming'}\n        if (step_num == 0 or self.has_explicit_mapper or\n                self.has_explicit_combiner):\n            desc['mapper'] = self.render_mapper()\n        if self.has_explicit_combiner:\n            desc['combiner'] = self.render_combiner()\n        if self.has_explicit_reducer:\n            desc['reducer'] = self.render_reducer()\n        if self._steps['mapper_raw']:\n            desc['input_manifest'] = True\n        if 'jobconf' in self._steps:\n            desc['jobconf'] = self._steps['jobconf']\n        return desc", "idx": 1212}
{"namespace": "mrjob.step._Step.description", "completion": "        desc = {'type': self._STEP_TYPE}\n\n        for k in self._STEP_ATTRS:\n            if k not in self._HIDDEN_ATTRS:\n                desc[k] = getattr(self, k)\n\n        return desc", "idx": 1213}
{"namespace": "mrjob.protocol._KeyCachingProtocol.read", "completion": "        key_value = line.split(b'\\t', 1)\n        if len(key_value) == 1:\n            key_value.append(None)\n\n        self._last_key_encoded = key_value[0]\n        self._last_key_decoded = self._loads(self._last_key_encoded)\n\n        return (self._last_key_decoded, self._loads(key_value[1]))", "idx": 1214}
{"namespace": "mrjob.util.safeeval", "completion": "    # create a dictionary of safe global variables\n    safe_globals = {\n        '__builtins__': __builtins__,\n        'True': True,\n        'False': False,\n        'None': None,\n        'set': set,\n        'range': range,\n        'xrange': xrange,\n    }\n\n    # open is not safe, so raise a NameError\n    def open(name, mode='r'):\n        raise NameError(\"name 'open' is not defined\")\n\n    safe_globals['open'] = open\n\n    # if globals is not None, add it to the safe_globals dictionary\n    if globals is not None:\n        safe_globals.update(globals)\n\n    # evaluate the expression\n    return eval(expr, safe_globals, locals)", "idx": 1215}
{"namespace": "mrjob.util.to_lines", "completion": "    return _to_lines(chunks)", "idx": 1216}
{"namespace": "mrjob.parse.is_s3_uri", "completion": "    try:\n        parse_s3_uri(uri)\n        return True\n    except ValueError:\n        return False", "idx": 1217}
{"namespace": "mrjob.parse.parse_s3_uri", "completion": "    if not is_s3_uri(uri):\n        raise ValueError('%s is not a valid S3 URI' % uri)\n\n    parsed_uri = urlparse(uri)\n    bucket = parsed_uri.netloc\n    key = parsed_uri.path.lstrip('/')\n\n    return bucket, key", "idx": 1218}
{"namespace": "mrjob.parse.to_uri", "completion": "    if is_uri(path_or_uri):\n        return path_or_uri\n    else:\n        return 'file:///' + pathname2url(abspath(path_or_uri))\n\n", "idx": 1219}
{"namespace": "mrjob.parse.parse_mr_job_stderr", "completion": "    if isinstance(stderr, (list, tuple)):\n        stderr = b'\\n'.join(stderr)\n    elif isinstance(stderr, BytesIO):\n        stderr = stderr.getvalue()\n\n    if counters is None:\n        counters = {}\n\n    statuses = []\n    other = []\n\n    for line in stderr.splitlines():\n        m = _COUNTER_RE.match(line)\n        if m:\n            group, name, count = m.groups()\n            if group not in counters:\n                counters[group] = {}\n            counters[group][name] = int(count)\n        else:\n            m = _STATUS_RE.match(line)\n            if m:\n                statuses.append(m.group(1))\n            else:\n                other.append(line)\n\n    return {\n        'counters': counters,\n        'statuses': statuses,\n        'other': other,\n    }", "idx": 1220}
{"namespace": "mrjob.parse._parse_progress_from_job_tracker", "completion": "    # this is for Hadoop 1.x\n    html_str = html_bytes.decode('utf-8')\n    html_str = html_str[html_str.find('Running Jobs'):html_str.find('Jobs')]\n    map_percent = reduce_percent = None\n    for line in html_str.splitlines():\n        m = _JOB_TRACKER_HTML_RE.match(line)\n        if m:\n            if 'map' in line:\n                map_percent = float(m.group(1))\n            elif 'reduce' in line:\n                reduce_percent = float(m.group(1))\n    return map_percent, reduce_percent", "idx": 1221}
{"namespace": "mrjob.parse._parse_progress_from_resource_manager", "completion": "    # snip out the Running Jobs section (ignore the header)\n    start = html_bytes.rfind(b'Running Jobs')\n    if start == -1:\n        return None\n    end = html_bytes.find(b'Jobs', start + len(b'Running Jobs'))\n    if end == -1:\n        end = None\n\n    html_bytes = html_bytes[start:end]\n\n    # search it for percents\n    matches = _RESOURCE_MANAGER_JS_RE.findall(html_bytes)\n    if len(matches) >= 1:\n        return float(matches[0])\n    else:\n        return None", "idx": 1222}
{"namespace": "mrjob.logs.task._match_task_log_path", "completion": "    if _PRE_YARN_TASK_LOG_PATH_RE.match(path):\n        match = _PRE_YARN_TASK_LOG_PATH_RE.match(path)\n        if job_id and match.group('job_id') != job_id:\n            return None\n        return dict(\n            attempt_id=match.group('attempt_id'),\n            log_type=match.group('log_type'),\n            path=path,\n        )\n    elif _YARN_TASK_LOG_PATH_RE.match(path):\n        match = _YARN_TASK_LOG_PATH_RE.match(path)\n        if application_id and match.group('application_id') != application_id:\n            return None\n        return dict(\n            application_id=match.group('application_id'),\n            container_id=match.group('container_id'),\n            log_type=match.group('log_type'),\n            path=path,\n        )\n    else:\n        return None", "idx": 1223}
{"namespace": "mrjob.logs.task._parse_task_syslog", "completion": "    records = parse._parse_log4j_records(lines)\n    return _parse_task_syslog_records(records)", "idx": 1224}
{"namespace": "mrjob.logs.ids._sort_for_spark", "completion": "    # sort by timestamp\n    ds = sorted(ds, key=_step_sort_key, reverse=True)\n\n    # sort by step number\n    ds = sorted(ds, key=lambda d: d['step_num'], reverse=True)\n\n    # sort by task type\n    ds = sorted(ds, key=lambda d: d['task_type'], reverse=True)\n\n    # sort by task number\n    ds = sorted(ds, key=lambda d: d['task_num'], reverse=True)\n\n    # sort by attempt number\n    ds = sorted(ds, key=lambda d: d['attempt_num'], reverse=True)\n\n    # sort by container number\n    ds = sorted(ds, key=lambda d: d['container_num'], reverse=True)\n\n    return ds", "idx": 1225}
{"namespace": "mrjob.logs.spark._parse_spark_log", "completion": "    records = _parse_hadoop_log4j_records(lines)\n    return _parse_spark_log_from_log4j_records(records)", "idx": 1226}
{"namespace": "mrjob.logs.mixin.LogInterpretationMixin._pick_error", "completion": "        if self._step_type_uses_spark(step_type):\n            return\n\n        if not self._read_logs():\n            return\n\n        log.info('Scanning logs for probable cause of failure...')\n\n        self._interpret_step_logs(log_interpretation, step_type)\n        self._interpret_history_log(log_interpretation)\n        self._interpret_task_logs(log_interpretation, step_type)", "idx": 1227}
{"namespace": "mrjob.logs.history._match_history_log_path", "completion": "    match = _HISTORY_LOG_PATH_RE.match(path)\n    if not match:\n        return None\n\n    if job_id and match.group('job_id') != job_id:\n        return None\n\n    return dict(\n        job_id=match.group('job_id'),\n        yarn=match.group('suffix') == 'jhist',\n    )", "idx": 1228}
{"namespace": "mrjob.logs.history._parse_pre_yarn_history_log", "completion": "    result = {}\n    task_to_counters = {}\n\n    for record in _parse_pre_yarn_history_records(lines):\n        if record['type'] == 'Task':\n            if 'COUNTERS' in record['fields'] and 'TASKID' in record['fields']:\n                task_to_counters[record['fields']['TASKID']] = _parse_pre_yarn_counters(record['fields']['COUNTERS'])\n\n        elif record['type'] == 'FAILED':\n            if 'ERROR' in record['fields'] and record['fields']['ERROR']:\n                error = dict(\n                    hadoop_error=dict(\n                        message=record['fields']['ERROR'],\n                        start_line=record['start_line'],\n                        num_lines=record['num_lines']))\n\n                if 'TASKID' in record['fields']:\n                    error['task_id'] = record['fields']['TASKID']\n\n                if 'ATTEMPTID' in record['fields']:\n                    error['attempt_id'] = record['fields']['ATTEMPTID']\n\n                result.setdefault('errors', [])\n                result['errors'].append(error)\n\n    if 'counters' not in result and task_to_counters:\n        result['counters'] = _sum_counters(*task_to_counters.values())\n\n    return result", "idx": 1229}
{"namespace": "mrjob.logs.history._parse_pre_yarn_history_records", "completion": "    record = None\n    start_line = None\n    num_lines = 0\n\n    for line_num, line in enumerate(lines):\n        if record is None:\n            record = _PRE_YARN_HISTORY_RECORD.match(line)\n            if record is None:\n                continue\n\n            start_line = line_num\n            num_lines = 1\n\n            fields = {}\n            for key, value in _PRE_YARN_HISTORY_KEY_PAIR.findall(\n                    record.group('key_pairs')):\n                fields[key] = _pre_yarn_history_unescape(value)\n\n            yield dict(fields=fields, num_lines=num_lines,\n                       start_line=start_line, type=record.group('type'))\n\n        else:\n            if line.endswith(' .'):\n                record = None\n                num_lines += 1\n            else:\n                num_lines += 1\n\n    if record is not None:\n        raise ValueError('unexpected end of file')", "idx": 1230}
{"namespace": "mrjob.logs.step._parse_step_syslog", "completion": "    result = {}\n    for line in lines:\n        if line.startswith('Counters:'):\n            result['counters'] = _parse_indented_counters(line.splitlines())\n        elif line.startswith('Output directory:'):\n            result['output_dir'] = line.split(':')[1].strip()\n        elif line.startswith('Submitted application'):\n            result['application_id'] = line.split()[2]\n        elif line.startswith('Running job:'):\n            result['job_id'] = line.split()[2]\n        elif line.startswith('map'):\n            result['progress'] = dict(\n                map=int(line.split()[1].rstrip('%')),\n                reduce=int(line.split()[3].rstrip('%')),\n                message=line,\n            )\n        elif line.startswith('Not a valid JAR:'):\n            result.setdefault('errors', [])\n            result['errors'].append(dict(\n                hadoop_error=dict(\n                    message=line,\n                    num_lines=1,\n                    start_line=0,\n                ),\n            ))\n        elif line.startswith('Task Id'):\n            result.setdefault('errors', [])\n            result['errors'].append(dict(\n                attempt_id=line.split()[2],\n                hadoop_error=dict(\n                    message='\\n'.join(lines[1:]),\n                    num_lines=len(lines),\n                    start_line=0,\n                )\n            ))\n            break\n\n    _add_implied_job_id(result)\n    for error in result.get('errors') or ():\n        _add_implied_task_id(error)\n\n    return result", "idx": 1231}
{"namespace": "mrjob.logs.errors._merge_and_sort_errors", "completion": "    error_dict = {}\n    for error in errors:\n        if error.get('container_id'):\n            if error['container_id'] not in error_dict:\n                error_dict[error['container_id']] = error\n            else:\n                error_dict[error['container_id']] = _merge_errors(error_dict[error['container_id']], error)\n        else:\n            error_dict[error['time']] = error\n\n    if attempt_to_container_id:\n        for attempt_id, container_id in attempt_to_container_id.items():\n            if container_id in error_dict:\n                error_dict[attempt_id] = error_dict[container_id]\n\n    def custom_key(error):\n        if 'task_error' in error:\n            return 1\n        elif 'hadoop_error' in error:\n            return 2\n        else:\n            return 3\n\n    return sorted(error_dict.values(), key=custom_key)", "idx": 1232}
{"namespace": "mrjob.fs.ssh.SSHFilesystem.ls", "completion": "        m = _SSH_URI_RE.match(path_glob)\n        addr = m.group('hostname')\n        fs_path = m.group('filesystem_path')\n\n        p = self._ssh_launch(addr, ['find', fs_path, '-type', 'f'])\n\n        for line in p.stdout:\n            yield line.strip()\n\n        self._ssh_finish_run(p)\n", "idx": 1233}
{"namespace": "mrjob.fs.ssh.SSHFilesystem._cat_file", "completion": "        m = _SSH_URI_RE.match(path)\n        addr = m.group('hostname')\n        path_to_cat = m.group('filesystem_path')\n\n        p = self._ssh_launch(addr, ['cat', path_to_cat])\n\n        for line in p.stdout:\n            yield to_unicode(line)\n\n        self._ssh_finish_run(p)\n", "idx": 1234}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.get_hadoop_bin", "completion": "        if self._hadoop_bin is None:\n            self._hadoop_bin = self._find_hadoop_bin()\n\n        return self._hadoop_bin", "idx": 1235}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.du", "completion": "        try:\n            stdout = self.invoke_hadoop(['fs', '-du', path_glob], return_stdout=True)\n        except CalledProcessError:\n            raise IOError(\"Could not du %s\" % path_glob)\n\n        # The output of the command is in the format:\n        # <size in bytes> <path>\n        # We need to sum up the size of all the files in the directory.\n        total_size = 0\n        for line in BytesIO(stdout):\n            line = line.rstrip(b'\\r\\n')\n            fields = line.split(b' ')\n            if len(fields) == 2:\n                total_size += int(fields[0])\n\n        return total_size", "idx": 1236}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.mkdir", "completion": "        # use -p option on Hadoop 2\n        version = self.get_hadoop_version()\n        if uses_yarn(version):\n            args = ['fs', '-mkdir', '-p', path]\n        else:\n            args = ['fs', '-mkdir', path]\n\n        try:\n            self.invoke_hadoop(args, ok_stderr=[_HADOOP_FILE_EXISTS_RE])\n        except CalledProcessError:\n            raise IOError(\"Could not mkdir %s\" % path)", "idx": 1237}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.exists", "completion": "        try:\n            self.invoke_hadoop(['fs', '-ls', path_glob],\n                                ok_returncodes=[0, 1, 255],\n                                ok_stderr=[_HADOOP_LS_NO_SUCH_FILE])\n            return True\n        except CalledProcessError:\n            return False", "idx": 1238}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.rm", "completion": "        try:\n            self.invoke_hadoop(['fs', '-rm', '-r', '-f', path_glob])\n        except CalledProcessError:\n            raise IOError(\"Could not rm %s\" % path_glob)", "idx": 1239}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.touchz", "completion": "        self.invoke_hadoop(['fs', '-touchz', path])", "idx": 1240}
{"namespace": "mrjob.fs.local.LocalFilesystem.du", "completion": "        path_glob = _from_file_uri(path_glob)\n        total_size = 0\n        for path in glob.glob(path_glob):\n            if os.path.isdir(path):\n                for dirname, _, filenames in os.walk(path, followlinks=True):\n                    for filename in filenames:\n                        total_size += os.path.getsize(os.path.join(dirname, filename))\n            else:\n                total_size += os.path.getsize(path)\n        return total_size\n\n    ", "idx": 1241}
{"namespace": "mrjob.fs.local.LocalFilesystem.ls", "completion": "        path_glob = _from_file_uri(path_glob)\n        if os.path.isdir(path_glob):\n            for root, dirs, files in os.walk(path_glob):\n                for f in files:\n                    yield os.path.join(root, f)\n        else:\n            yield path_glob\n", "idx": 1242}
{"namespace": "mrjob.fs.local.LocalFilesystem._cat_file", "completion": "        path = _from_file_uri(path)\n        with open(path, 'rb') as f:\n            while True:\n                data = f.read(1024)\n                if not data:\n                    break\n                yield data\n", "idx": 1243}
{"namespace": "mrjob.fs.local.LocalFilesystem.exists", "completion": "        path_glob = _from_file_uri(path_glob)\n        return len(glob.glob(path_glob)) > 0\n\n    ", "idx": 1244}
{"namespace": "mrjob.fs.local.LocalFilesystem.mkdir", "completion": "        path = _from_file_uri(path)\n        if not os.path.exists(path):\n            os.makedirs(path)\n", "idx": 1245}
{"namespace": "mrjob.fs.local.LocalFilesystem.put", "completion": "        src = _from_file_uri(src)\n        path = _from_file_uri(path)\n        if os.path.isdir(path):\n            path = os.path.join(path, os.path.basename(src))\n\n        log.debug('Copying %s to %s' % (src, path))\n        shutil.copyfile(src, path)", "idx": 1246}
{"namespace": "mrjob.fs.local.LocalFilesystem.rm", "completion": "        path_glob = _from_file_uri(path_glob)\n        for path in glob.glob(path_glob):\n            if os.path.isdir(path):\n                shutil.rmtree(path)\n            else:\n                os.remove(path)", "idx": 1247}
{"namespace": "mrjob.fs.local.LocalFilesystem.touchz", "completion": "        path = _from_file_uri(path)\n        if os.path.exists(path):\n            if os.path.getsize(path) > 0:\n                raise OSError(\"File already exists and is not empty\")\n        else:\n            open(path, 'w').close()", "idx": 1248}
{"namespace": "mrjob.fs.local.LocalFilesystem.md5sum", "completion": "        path = _from_file_uri(path)\n        with open(path, 'rb') as f:\n            return self._md5sum_file(f)", "idx": 1249}
{"namespace": "mrjob.fs.composite.CompositeFilesystem.add_fs", "completion": "        setattr(self, name, fs)\n        self._fs_names.append(name)\n        if disable_if:\n            self._disable_if[name] = disable_if\n\n    ", "idx": 1250}
{"namespace": "mrjob.fs.base.Filesystem.cat", "completion": "        raise NotImplementedError", "idx": 1251}
{"namespace": "mrjob.fs.base.Filesystem.join", "completion": "        if path.startswith('s3://'):\n            path = path.split('s3://')[1]\n            path = path.split('/')\n            path = '/'.join(path)\n            path = 's3://' + path\n        elif path.startswith('gs://'):\n            path = path.split('gs://')[1]\n            path = path.split('/')\n            path = '/'.join(path)\n            path = 'gs://' + path\n        elif path.startswith('hdfs://'):\n            path = path.split('hdfs://')[1]\n            path = path.split('/')\n            path = '/'.join(path)\n            path = 'hdfs://' + path\n        elif path.startswith('ssh://'):\n            path = path.split('ssh://')[1]\n            path = path.split('/')\n            path = '/'.join(path)\n            path = 'ssh://' + path\n        elif path.startswith('file://'):\n            path = path.split('file://')[1]\n            path = path.split('/')\n            path = '/'.join(path)\n            path = 'file://' + path\n        else:\n            path = path.split('/')\n            path = '/'.join(path)\n\n        if len(paths) > 0:\n            for p in paths:\n                if p.startswith('s3://'):\n                    p = p.split('s3://')[1]\n                    p = p.split('/')\n                    p = '/'.join(p)\n                    p = 's3://' + p\n                elif p.startswith('gs://'):\n                    p = p.split('gs://')[1]\n                    p = p.split('/')\n                    p = '/'.join(p)\n                    p = 'gs://' + p\n                elif p.startswith('hdfs://'):\n                    p = p.split('hdfs://')[1]\n                    p = p.split('/')\n                    p = '/'.join(p)\n                    p = '", "idx": 1252}
{"namespace": "mrjob.examples.mr_text_classifier.parse_doc_filename", "completion": "    # Extract the unique ID from the filename\n    unique_id = posixpath.basename(input_uri).split('.')[0].split('-')[0]\n\n    # Extract the categories from the filename\n    categories = posixpath.basename(input_uri).split('.')[0].split('-')[1:]\n    cats = {}\n    for category in categories:\n        if category.startswith('not_'):\n            cats[category[4:]] = False\n        else:\n            cats[category] = True\n\n    # Return a dictionary containing the unique ID and categories\n    return dict(id=unique_id, cats=cats)", "idx": 1253}
{"namespace": "prometheus_client.mmap_dict.MmapedDict.read_value", "completion": "        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        return _unpack_two_doubles(self._m, pos)[0]", "idx": 1254}
{"namespace": "prometheus_client.mmap_dict.MmapedDict.write_value", "completion": "        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        _pack_two_doubles(self._m, pos, value, timestamp)", "idx": 1255}
{"namespace": "prometheus_client.multiprocess.MultiProcessCollector.merge", "completion": "        metrics = MultiProcessCollector._read_metrics(files)\n        return MultiProcessCollector._accumulate_metrics(metrics, accumulate)", "idx": 1256}
{"namespace": "prometheus_client.multiprocess.MultiProcessCollector.collect", "completion": "        files = glob.glob(os.path.join(self._path, '*.db'))\n        return MultiProcessCollector.merge(files)", "idx": 1257}
{"namespace": "prometheus_client.exposition.choose_encoder", "completion": "    if accept_header and 'application/openmetrics-text' in accept_header:\n        return generate_latest, CONTENT_TYPE_LATEST\n    return generate_latest, CONTENT_TYPE_LATEST", "idx": 1258}
{"namespace": "flower.command.apply_options", "completion": "    parse_command_line(argv)\n    if options.conf:\n        parse_config_file(options.conf)\n        parse_command_line(argv)\n    if options.conf == prog_name:\n        raise IOError(\"Can't use the default configuration file as a custom configuration file\")", "idx": 1259}
{"namespace": "trackerjacker.ieee_mac_vendor_db.MacVendorDB.lookup", "completion": "        mac = mac.upper().replace(':', '')\n        for prefix in self.db:\n            if mac.startswith(prefix):\n                return self.db[prefix]\n        return ''", "idx": 1260}
{"namespace": "trailscraper.iam.Statement.merge", "completion": "        if self.Effect != other.Effect:\n            raise ValueError(f\"Trying to combine two statements with differing effects: {self.Effect} {other.Effect}\")\n\n        merged_actions = sorted(set(self.Action + other.Action))\n        merged_resources = sorted(set(self.Resource + other.Resource))\n\n        return Statement(merged_actions, self.Effect, merged_resources)", "idx": 1261}
{"namespace": "trailscraper.iam.parse_policy_document", "completion": "    if isinstance(stream, six.string_types):\n        json_data = json.loads(stream)\n    else:\n        json_data = json.load(stream)\n\n    return PolicyDocument(\n        Statement=_parse_statements(json_data['Statement']),\n        Version=json_data['Version']\n    )", "idx": 1262}
{"namespace": "trailscraper.iam.known_iam_actions", "completion": "    actions = all_known_iam_permissions()\n    actions_by_prefix = groupbyz(lambda action: action.split(\":\")[0], actions)\n    return actions_by_prefix[prefix]", "idx": 1263}
{"namespace": "trailscraper.boto_service_definitions.service_definition_file", "completion": "    files = boto_service_definition_files()\n    filtered_files = [file for file in files if fnmatch.fnmatch(file, \"**/\" + servicename + \"/*/service-*.json\")]\n    filtered_files.sort()\n    return filtered_files[-1]", "idx": 1264}
{"namespace": "trailscraper.boto_service_definitions.operation_definition", "completion": "    service_definition_file_path = service_definition_file(servicename)\n    with open(service_definition_file_path, 'r') as f:\n        service_definition = json.load(f)\n\n    operation_definition = service_definition['operations'][operationname]\n\n    return operation_definition", "idx": 1265}
{"namespace": "trailscraper.cloudtrail.Record.to_statement", "completion": "        if self.event_source == \"sts.amazonaws.com\" and self.event_name == \"GetCallerIdentity\":\n            return None\n\n        if self.event_source == \"apigateway.amazonaws.com\":\n            return self._to_api_gateway_statement()\n\n        iam_prefix = self._source_to_iam_prefix()\n        iam_action = self._event_name_to_iam_action()\n\n        return Statement(\n            Effect=\"Allow\",\n            Action=[Action(iam_prefix, iam_action)],\n            Resource=self.resource_arns\n        )", "idx": 1266}
{"namespace": "trailscraper.cloudtrail.filter_records", "completion": "    return filterz(_by_timeframe(from_date, to_date),\n                   filterz(_by_role_arns(arns_to_filter_for),\n                           records))", "idx": 1267}
{"namespace": "trailscraper.record_sources.local_directory_record_source.LocalDirectoryRecordSource.load_from_dir", "completion": "        return pipe(self._valid_log_files(),\n                    filterz(lambda log_file: log_file.has_events_in_range(from_date, to_date)),\n                    mapcatz(LogFile.records))", "idx": 1268}
{"namespace": "pyt.__main__.discover_files", "completion": "    included_files = list()\n    for target in targets:\n        if os.path.isdir(target):\n            for root, _, files in os.walk(target):\n                for file in files:\n                    if file.endswith('.py'):\n                        included_files.append(os.path.join(root, file))\n                        log.debug(\"Discovered file: %s\", os.path.join(root, file))\n        elif os.path.isfile(target):\n            included_files.append(target)\n            log.debug(\"Discovered file: %s\", target)\n\n    if excluded_files:\n        excluded_files = excluded_files.split(',')\n        included_files = [\n            file for file in included_files\n            if os.path.basename(file) not in excluded_files\n        ]\n\n    return included_files", "idx": 1269}
{"namespace": "pyt.core.project_handler.get_directory_modules", "completion": "    if _local_modules and _local_modules[0][1].split(os.sep)[-2] == directory.split(os.sep)[-1]:\n        return _local_modules\n\n    if not os.path.isdir(directory):\n        directory = os.path.dirname(directory)\n\n    for filename in os.listdir(directory):\n        if _is_python_file(filename):\n            module_name = os.path.splitext(filename)[0]\n            module_path = os.path.join(directory, filename)\n            _local_modules.append((module_name, module_path))\n\n    return _local_modules", "idx": 1270}
{"namespace": "pyt.vulnerabilities.vulnerabilities.find_triggers", "completion": "    trigger_nodes = list()\n    for node in nodes:\n        if node.line_number not in nosec_lines:\n            trigger_nodes.extend(label_contains(node, trigger_words))\n    return trigger_nodes", "idx": 1271}
{"namespace": "pyt.vulnerabilities.vulnerabilities.label_contains", "completion": "    for trigger in triggers:\n        if trigger.trigger_word in node.label:\n            yield TriggerNode(trigger, node)", "idx": 1272}
{"namespace": "pyt.vulnerabilities.vulnerabilities.build_sanitiser_node_dict", "completion": "    sanitisers = set()\n    for sink in sinks_in_file:\n        sanitisers.add(sink.trigger.sanitiser)\n\n    sanitiser_node_dict = dict()\n    for sanitiser in sanitisers:\n        sanitiser_node_dict[sanitiser] = list(find_sanitiser_nodes(sanitiser, sinks_in_file))\n\n    return sanitiser_node_dict", "idx": 1273}
{"namespace": "pyt.vulnerabilities.trigger_definitions_parser.parse", "completion": "    with open(trigger_word_file, 'r') as f:\n        data = json.load(f)\n\n    sources = [Source(trigger_word=trigger_word) for trigger_word in data['sources']]\n    sinks = [Sink.from_json(key, value) for key, value in data['sinks'].items()]\n\n    return Definitions(sources=sources, sinks=sinks)", "idx": 1274}
{"namespace": "principalmapper.querying.local_policy_simulation._statement_matches_resource", "completion": "    if 'Resource' in statement:\n        for item in _listify_string(statement['Resource']):\n            if _matches_after_expansion(resource, item, condition_keys):\n                return True\n        return False\n    elif 'NotResource' in statement:\n        result = True\n        for item in _listify_string(statement['NotResource']):\n            if _matches_after_expansion(resource, item, condition_keys):\n                result = False\n                break\n        return result\n    else:\n        return True", "idx": 1275}
{"namespace": "principalmapper.querying.local_policy_simulation._matches_after_expansion", "completion": "    if condition_keys is not None:\n        for key, value in condition_keys.items():\n            string_to_check = string_to_check.replace(\"${\" + key + \"}\", value)\n            string_to_check_against = string_to_check_against.replace(\"${\" + key + \"}\", value)\n\n    if \"*\" in string_to_check_against or \"?\" in string_to_check_against:\n        pattern = _compose_pattern(string_to_check_against)\n        return pattern.match(string_to_check) is not None\n    else:\n        return string_to_check == string_to_check_against", "idx": 1276}
{"namespace": "passpie.database.PasspieStorage.delete", "completion": "        for cred in credentials:\n            credpath = self.make_credpath(cred[\"name\"], cred[\"login\"])\n            if os.path.exists(credpath):\n                os.remove(credpath)\n                dirname = os.path.dirname(credpath)\n                if not os.listdir(dirname):\n                    shutil.rmtree(dirname)", "idx": 1277}
{"namespace": "passpie.database.PasspieStorage.read", "completion": "        credentials = []\n        for dirpath, dirnames, filenames in os.walk(self.path):\n            for filename in filenames:\n                if filename.endswith(self.extension):\n                    credpath = os.path.join(dirpath, filename)\n                    with open(credpath, \"r\") as f:\n                        credentials.append(yaml.safe_load(f))\n        return {\"_default\": {i: c for i, c in enumerate(credentials)}}", "idx": 1278}
{"namespace": "threatingestor.state.State.save_state", "completion": "        logger.debug(f\"Saving state for '{name}'\")\n        self.cursor.execute('INSERT OR REPLACE INTO states (name, state) VALUES (?, ?)', (name, state))\n        self.conn.commit()", "idx": 1279}
{"namespace": "threatingestor.state.State.get_state", "completion": "        self.cursor.execute('SELECT state FROM states WHERE name=?', (name,))\n        result = self.cursor.fetchone()\n        if result:\n            return result[0]\n        else:\n            return None", "idx": 1280}
{"namespace": "threatingestor.Ingestor.run", "completion": "        if self.config.daemon():\n            logger.info(\"Running as a daemon\")\n            self.run_forever()\n        else:\n            logger.info(\"Running once\")\n            self.run_once()", "idx": 1281}
{"namespace": "msticpy.analysis.anomalous_sequence.model.Model.compute_scores", "completion": "        self.compute_likelihoods_of_sessions(use_start_end_tokens)\n        self.compute_geomean_lik_of_sessions()\n        self.compute_rarest_windows(window_len=2, use_start_end_tokens=use_start_end_tokens)\n        self.compute_rarest_windows(window_len=3, use_start_end_tokens=use_start_end_tokens)\n        self.compute_rarest_windows(window_len=2, use_start_end_tokens=use_start_end_tokens, use_geo_mean=True)\n        self.compute_rarest_windows(window_len=3, use_start_end_tokens=use_start_end_tokens, use_geo_mean=True)\n        self.compute_setof_params_cond_cmd(use_geo_mean=False)\n        self.compute_setof_params_cond_cmd(use_geo_mean=True)", "idx": 1282}
{"namespace": "msticpy.analysis.anomalous_sequence.model.Model.compute_rarest_windows", "completion": "        if self.session_likelihoods is None:\n            self.compute_likelihoods_of_sessions(use_start_end_tokens)\n        if use_geo_mean:\n            if self.session_geomean_likelihoods is None:\n                self.compute_geomean_lik_of_sessions()\n            session_likelihoods = self.session_geomean_likelihoods\n        else:\n            session_likelihoods = self.session_likelihoods\n        result = []\n        for idx, session in enumerate(self.sessions):\n            if len(session) < window_len:\n                result.append([])\n                continue\n            if use_start_end_tokens:\n                window_likelihoods = [\n                    session_likelihoods[idx][i : i + window_len]\n                    for i in range(len(session) - window_len + 1)\n                ]\n            else:\n                window_likelihoods = [\n                    session_likelihoods[idx][i : i + window_len - 1]\n                    for i in range(len(session) - window_len + 2)\n                ]\n            window_likelihoods = [\n                np.prod(likelihoods) for likelihoods in window_likelihoods\n            ]\n            window_likelihoods = np.array(window_likelihoods)\n            rarest_window_idx = np.argmin(window_likelihoods)\n            rarest_window = session[rarest_window_idx : rarest_window_idx + window_len]\n            result.append(rarest_window)\n        self.rare_windows[window_len] = result\n        self.rare_window_likelihoods[window_len] = window_likelihoods", "idx": 1283}
{"namespace": "msticpy.analysis.anomalous_sequence.anomalous.score_sessions", "completion": "    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable=too-many-branches\n    # pylint: disable=too-many-nested-blocks\n    # pylint: disable=too-many-arguments\n    # pylint: disable=too-many-locals\n    # pylint: disable=too-many-statements\n    # pylint: disable", "idx": 1284}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.laplace_smooth_counts", "completion": "    seq1_counts_sm = StateMatrix()\n    seq2_counts_sm = StateMatrix()\n    param_counts_sm = StateMatrix()\n    cmd_param_counts_sm = StateMatrix()\n\n    for cmd, count in seq1_counts.items():\n        seq1_counts_sm[cmd] = count + 1\n\n    for cmd1, cmd2_counts in seq2_counts.items():\n        for cmd2, count in cmd2_counts.items():\n            seq2_counts_sm[cmd1][cmd2] = count + 1\n\n    for param, count in param_counts.items():\n        param_counts_sm[param] = count + 1\n\n    for cmd, param_counts in cmd_param_counts.items():\n        for param, count in param_counts.items():\n            cmd_param_counts_sm[cmd][param] = count + 1\n\n    for cmd in seq1_counts_sm.keys():\n        if cmd not in seq2_counts_sm.keys():\n            seq2_counts_sm[cmd] = defaultdict(lambda: 0)\n\n    for cmd1 in seq2_counts_sm.keys():\n        for cmd2 in seq2_counts_sm[cmd1].keys():\n            if cmd2 not in seq2_counts_sm.keys():\n                seq2_counts_sm[cmd2] = defaultdict(lambda: 0)\n\n    for param in param_counts_sm.keys():\n        if param not in cmd_param_counts_sm.keys():\n            cmd_param_counts_sm[unk_token] = defaultdict(lambda: 0)\n\n    for cmd in cmd_param_counts_sm.keys():\n        for param in cmd_param_counts_sm[cmd].keys():\n            if param not in param_counts_sm.keys():\n                param_counts_sm[param] = 0\n\n    return seq1_counts_sm, seq2", "idx": 1285}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.compute_likelihood_window", "completion": "    if use_start_token:\n        if start_token is None:\n            raise MsticpyException(\n                \"start_token should not be set to None when use_start_token is set to True\"\n            )\n        window = [Cmd(name=str(start_token), params={})] + window\n    if use_end_token:\n        if end_token is None:\n            raise MsticpyException(\n                \"end_token should not be set to None when use_end_token is set to True\"\n            )\n        window += [Cmd(name=str(end_token), params={})]\n\n    if len(window) == 0:\n        return 1.0\n\n    if len(window) == 1:\n        return prior_probs[window[0].name]\n\n    if len(window) == 2:\n        return prior_probs[window[0].name] * trans_probs[window[0].name][\n            window[1].name\n        ] * compute_prob_setofparams_given_cmd(\n            cmd=window[1].name,\n            params=window[1].params,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n        )\n\n    if len(window) > 2:\n        likelihood = prior_probs[window[0].name]\n        for i in range(1, len(window)):\n            if i == 1:\n                likelihood *= trans_probs[window[i - 1].name][window[i].name]\n            likelihood *= compute_prob_setofparams_given_cmd(\n                cmd=window[i].name,\n                params=window[i].params,\n                param_cond_cmd_probs=param_cond_cmd_probs,\n            )\n        return likelihood", "idx": 1286}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.compute_likelihood_windows_in_session", "completion": "    if use_end_token:\n        if end_token is None:\n            raise MsticpyException(\n                \"end_token should not be None, when use_end_token is True\"\n            )\n\n    if use_start_token:\n        if start_token is None:\n            raise MsticpyException(\n                \"start_token should not be None, when use_start_token is True\"\n            )\n\n    s_len = len(session)\n    if s_len == 0:\n        return []\n    if s_len < window_len:\n        return []\n\n    likelihoods: List[float] = []\n    for i in range(s_len - window_len + 1):\n        window = session[i : i + window_len]  # noqa E203\n        likelihood = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            use_start_token=use_start_end_tokens,\n            use_end_token=use_start_end_tokens,\n            start_token=start_token,\n            end_token=end_token,\n        )\n        if use_geo_mean:\n            likelihood = likelihood ** (1 / window_len)\n        likelihoods.append(likelihood)\n\n    return likelihoods", "idx": 1287}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.rarest_window_session", "completion": "    # pylint: disable=too-many-locals, too-many-arguments, too-many-branches\n    def rarest_window_session(\n        session: List[Cmd],\n        prior_probs: StateMatrix,\n        trans_probs: StateMatrix,\n        param_cond_cmd_probs: StateMatrix,\n        window_len: int,\n        use_start_end_tokens: bool,\n        start_token: str,\n        end_token: str,\n        use_geo_mean=False,\n    ) -> Tuple[List[Cmd], float]:\n\n        \"\"\"\n        This function finds and computes the likelihood of the rarest window of a given length in a session. It calculates the likelihoods of all sliding windows in the session and returns the rarest window and its likelihood.\n        Input-Output Arguments\n        :param session: List[Cmd]. A list of Cmd objects representing a session.\n        :param prior_probs: Union[StateMatrix, dict]. Computed probabilities of individual commands.\n        :param trans_probs: Union[StateMatrix, dict]. Computed probabilities of sequences of commands (length 2).\n        :param param_cond_cmd_probs: Union[StateMatrix, dict]. Computed probabilities of the params conditional on the command.\n        :param window_len: int. The length of the sliding window for likelihood calculations.\n        :param use_start_end_tokens: bool. If True, the `start_token` and `end_token` will be added to the session before calculations.\n        :param start_token: str. A dummy command to signify the start of the session.\n        :param end_token: str. A dummy command to signify the end of the session.\n        :param use_geo_mean: bool. If True, each likelihood of the sliding windows will be raised to the power of (1/`window_len`).\n        :return: Tuple[List[Cmd], float]. The rarest window part of the session and the likelihood of the rarest window.\n        \"\"\"\n\n        # pylint: disable=too-many-locals, too-many-arguments", "idx": 1288}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.compute_likelihood_window", "completion": "    if use_start_token:\n        window = [start_token] + window\n    if use_end_token:\n        window += [end_token]\n\n    if isinstance(prior_probs, StateMatrix):\n        prior_probs = prior_probs.states\n    if isinstance(trans_probs, StateMatrix):\n        trans_probs = trans_probs.states\n\n    log_likelihood = 0\n    for i in range(len(window) - 1):\n        cmd1 = window[i]\n        cmd2 = window[i + 1]\n        log_likelihood += np.log(trans_probs[cmd1][cmd2])\n\n    log_likelihood += np.log(prior_probs[window[-1]])\n\n    return np.exp(log_likelihood)", "idx": 1289}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.compute_likelihood_windows_in_session", "completion": "    if use_start_end_tokens:\n        if start_token is None:\n            raise MsticpyException(\n                \"start_token should not be None, when use_start_end_tokens is True\"\n            )\n        if end_token is None:\n            raise MsticpyException(\n                \"end_token should not be None, when use_start_end_tokens is True\"\n            )\n\n    if window_len < 1:\n        raise MsticpyException(\"window_len should be >= 1\")\n\n    if window_len > len(session):\n        raise MsticpyException(\"window_len should be <= len(session)\")\n\n    if window_len == 1:\n        return [prior_probs[cmd] for cmd in session]\n\n    if use_start_end_tokens:\n        session = [start_token] + session + [end_token]\n\n    w_len = len(session)\n    likelihoods = []\n    for i in range(w_len - window_len + 1):\n        window = session[i : i + window_len]  # noqa: E203\n        likelihood = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            use_start_token=False,\n            use_end_token=False,\n        )\n        if use_geo_mean:\n            likelihood **= 1 / window_len\n        likelihoods.append(likelihood)\n\n    return likelihoods", "idx": 1290}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.rarest_window_session", "completion": "    # pylint: disable=too-many-locals, too-many-branches\n    ", "idx": 1291}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.get_params_to_model_values", "completion": "    params_to_model_values = set()\n    for param, values in param_value_counts.items():\n        if param_counts[param] > 100:\n            if len(values) > 10:\n                params_to_model_values.add(param)\n    return params_to_model_values", "idx": 1292}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_prob_setofparams_given_cmd", "completion": "    if isinstance(params_with_vals, set):\n        params_with_vals = dict.fromkeys(params_with_vals)\n    if isinstance(param_cond_cmd_probs, StateMatrix):\n        param_cond_cmd_probs = param_cond_cmd_probs.states\n    if isinstance(value_cond_param_probs, StateMatrix):\n        value_cond_param_probs = value_cond_param_probs.states\n\n    prob = 1\n    for param, val in params_with_vals.items():\n        if param in modellable_params:\n            prob *= value_cond_param_probs[param][val]\n        prob *= param_cond_cmd_probs[cmd][param]\n\n    if use_geo_mean:\n        k = len(params_with_vals) + len(modellable_params)\n        prob = prob ** (1 / k)\n\n    return prob", "idx": 1293}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_likelihood_window", "completion": "    if use_start_token and start_token is None:\n        raise MsticpyException(\"start_token should not be set to None when use_start_token is set to True\")\n    if use_end_token and end_token is None:\n        raise MsticpyException(\"end_token should not be set to None when use_end_token is set to True\")\n\n    sess = window.copy()\n    if use_start_token:\n        sess = [Cmd(name=str(start_token), params={})] + sess\n    if use_end_token:\n        sess += [Cmd(name=str(end_token), params={})]\n\n    lik = 1\n    for i in range(len(sess) - 1):\n        cmd1 = sess[i].name\n        cmd2 = sess[i + 1].name\n        pars = sess[i].params\n        if isinstance(pars, set):\n            pars = dict.fromkeys(pars)\n        lik *= trans_probs[cmd1][cmd2]\n        lik *= compute_prob_setofparams_given_cmd(\n            cmd=cmd1,\n            params_with_vals=pars,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            value_cond_param_probs=value_cond_param_probs,\n            modellable_params=modellable_params,\n        )\n    lik *= prior_probs[sess[-1].name]\n\n    return lik", "idx": 1294}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_likelihood_windows_in_session", "completion": "    if use_start_end_tokens:\n        if start_token is None:\n            raise MsticpyException(\n                \"start_token should not be None, when use_start_end_tokens is True\"\n            )\n        if end_token is None:\n            raise MsticpyException(\n                \"end_token should not be None, when use_start_end_tokens is True\"\n            )\n\n    if window_len <= 0:\n        raise MsticpyException(\"window_len should be greater than 0\")\n\n    if window_len == 1:\n        return [\n            compute_likelihood_window(\n                window=[cmd],\n                prior_probs=prior_probs,\n                trans_probs=trans_probs,\n                param_cond_cmd_probs=param_cond_cmd_probs,\n                value_cond_param_probs=value_cond_param_probs,\n                modellable_params=modellable_params,\n                use_start_token=False,\n                use_end_token=False,\n            )\n            for cmd in session\n        ]\n\n    if use_start_end_tokens:\n        session = [start_token] + session + [end_token]\n\n    w_len = len(session)\n    if w_len < window_len:\n        return []\n\n    likelihoods = []\n    for i in range(w_len - window_len + 1):\n        window = session[i : i + window_len]  # noqa E203\n        likelihood = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            value_cond_param_probs=value_cond_param_probs,\n            modellable_params=modellable_params,\n            use_start_token=False,\n            use_end_token=False,\n        )\n        if use_geo_mean:\n            likelihood =", "idx": 1295}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.rarest_window_session", "completion": "    # Compute the likelihoods of all sliding windows in the session\n    likelihoods = compute_likelihood_windows_in_session(\n        session=session,\n        prior_probs=prior_probs,\n        trans_probs=trans_probs,\n        param_cond_cmd_probs=param_cond_cmd_probs,\n        value_cond_param_probs=value_cond_param_probs,\n        modellable_params=modellable_params,\n        window_len=window_len,\n        use_start_end_tokens=use_start_end_tokens,\n        start_token=start_token,\n        end_token=end_token,\n        use_geo_mean=use_geo_mean,\n    )\n\n    # Find the rarest window\n    rarest_window = session[np.argmin(likelihoods) : np.argmin(likelihoods) + window_len]  # noqa E203\n    rarest_window_likelihood = likelihoods[np.argmin(likelihoods)]\n\n    return rarest_window, rarest_window_likelihood", "idx": 1296}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.probabilities.compute_cmds_probs", "completion": "    seq1_probs: DefaultDict[str, float] = defaultdict(lambda: 0)\n    seq2_probs: DefaultDict[str, DefaultDict[str, float]] = defaultdict(\n        lambda: defaultdict(lambda: 0)\n    )\n\n    for cmd, count in seq1_counts.items():\n        seq1_probs[cmd] = count / sum(seq1_counts.values())\n\n    for cmd1, cmd2_counts in seq2_counts.items():\n        n_cmd1 = seq1_counts[cmd1]\n        for cmd2, count in cmd2_counts.items():\n            seq2_probs[cmd1][cmd2] = count / n_cmd1\n\n    seq1_probs_sm = StateMatrix(states=seq1_probs, unk_token=unk_token)\n    seq2_probs_sm = StateMatrix(states=seq2_probs, unk_token=unk_token)\n\n    return seq1_probs_sm, seq2_probs_sm", "idx": 1297}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.probabilities.compute_values_probs", "completion": "    value_probs: DefaultDict[str, float] = defaultdict(lambda: 0)\n    value_cond_param_probs: DefaultDict[str, DefaultDict[str, float]] = defaultdict(\n        lambda: defaultdict(lambda: 0)\n    )\n\n    for param, values in param_value_counts.items():\n        for value, count in values.items():\n            value_cond_param_probs[param][value] = count / sum(\n                param_value_counts[param].values()\n            )\n\n    for value, count in value_counts.items():\n        value_probs[value] = count / sum(value_counts.values())\n\n    value_probs_sm = StateMatrix(states=value_probs, unk_token=unk_token)\n    value_cond_param_probs_sm = StateMatrix(\n        states=value_cond_param_probs, unk_token=unk_token\n    )\n\n    return value_probs_sm, value_cond_param_probs_sm", "idx": 1298}
{"namespace": "msticpy.auth.msal_auth.MSALDelegatedAuth.get_token", "completion": "        chosen_account = self.app.get_accounts(username=self.username)\n        if chosen_account:\n            self.result = self.app.acquire_token_silent(\n                scopes=self.scopes, account=chosen_account[0]\n            )\n        if not self.result:\n            self.result = self._app_auth(self.auth_type)\n        if \"error\" in self.result and self.result[\"error\"] == \"consent_required\":\n            self.result = self._app_auth(\"interactive\")\n        if \"error\" in self.result:\n            raise ValueError(\n                f\"Fail to get token. Err: {json.dumps(self.result, indent=4)}\"\n            )", "idx": 1299}
{"namespace": "msticpy.config.query_editor.QueryParameterEditWidget.save_parameter", "completion": "        del button\n        parameter_name = self.parameter_name_widget.value\n        parameter_description = self.description_widget.value\n        parameter_datatype = self.type_widget.value\n        parameter_default = (\n            self.default_widget.value if self.default_reqd_widget.value else None\n        )\n        parameter = QueryParameter(\n            description=parameter_description,\n            datatype=parameter_datatype,\n            default=parameter_default,\n        )\n        self.param_container.parameters[parameter_name] = parameter\n        self.parameter_dropdown.options = list(self.param_container.parameters.keys())\n        self.parameter_dropdown.value = parameter_name\n        self._changed_data = True", "idx": 1300}
{"namespace": "msticpy.config.query_editor.QueryParameterEditWidget.delete_parameter", "completion": "        del button\n        del self.param_container.parameters[self.parameter_name_widget.value]\n        self.parameter_dropdown.options = list(self.param_container.parameters.keys())\n        self.parameter_dropdown.value = \"\"\n        self._blank_parameter()\n        self._changed_data = True", "idx": 1301}
{"namespace": "msticpy.config.query_editor.MetadataEditWidget.save_metadata", "completion": "        del button\n        self.metadata.version = self.version_widget.value\n        self.metadata.description = self.description_widget.value\n        self.metadata.data_environments = self.data_env_widget.value\n        self.metadata.data_families = self.data_families_widget.value.split(\", \")\n        self.metadata.database = self.database_widget.value\n        self.metadata.cluster = self.cluster_widget.value\n        self.metadata.clusters = self.clusters_widget.value.split(\"\\n\")\n        self.metadata.cluster_groups = self.cluster_groups_widget.value.split(\"\\n\")\n        self.metadata.tags = self.tags_widget.value.split(\", \")\n        self.metadata.data_source = self.data_source_widget.value\n        self._changed_data = True", "idx": 1302}
{"namespace": "msticpy.config.query_editor.QueryEditor._save_file", "completion": "        del button\n        if not self.query_collection.file_name:\n            self.query_collection.file_name = self.current_file\n        save_queries_to_yaml(self.query_collection, self.current_file)\n        self._reset_change_state()", "idx": 1303}
{"namespace": "msticpy.config.query_editor.QueryEditor._unsaved_changes", "completion": "        return (\n            self.default_param_editor.changed_data\n            or self.metadata_editor.changed_data\n            or self.query_editor.changed_data\n        )", "idx": 1304}
{"namespace": "msticpy.config.query_editor.load_queries_from_yaml", "completion": "    yaml_data = Path(yaml_file).read_text(encoding=\"utf-8\")\n    query_dict = yaml.safe_load(yaml_data)\n    query_dict[\"file_name\"] = yaml_file\n    query_dict[\"metadata\"] = QueryMetadata(**query_dict[\"metadata\"])\n    query_dict[\"defaults\"] = _create_query_defaults(query_dict[\"defaults\"])\n    query_dict[\"sources\"] = {\n        name: _create_query(query) for name, query in query_dict[\"sources\"].items()\n    }\n    return QueryCollection(**query_dict)", "idx": 1305}
{"namespace": "zxcvbn.time_estimates.estimate_attack_times", "completion": "    seconds = guesses / 1e10\n    display_time_ = display_time(seconds)\n    score = guesses_to_score(guesses)\n    return {\n        'guesses': guesses,\n        'guesses_log10': math.log10(guesses),\n        'seconds': seconds,\n        'display_time': display_time_,\n        'score': score,\n    }", "idx": 1306}
{"namespace": "zxcvbn.scoring.estimate_guesses", "completion": "    if match.get('guesses', False):\n        return match['guesses']\n\n    if match['pattern'] == 'bruteforce':\n        match['guesses'] = bruteforce_guesses(match)\n    elif match['pattern'] == 'dictionary':\n        match['guesses'] = dictionary_guesses(match)\n    elif match['pattern'] == 'repeat':\n        match['guesses'] = repeat_guesses(match)\n    elif match['pattern'] == 'sequence':\n        match['guesses'] = sequence_guesses(match)\n    elif match['pattern'] == 'regex':\n        match['guesses'] = regex_guesses(match)\n    elif match['pattern'] == 'date':\n        match['guesses'] = date_guesses(match)\n    elif match['pattern'] == 'spatial':\n        match['guesses'] = spatial_guesses(match)\n\n    return match['guesses']", "idx": 1307}
{"namespace": "zxcvbn.scoring.dictionary_guesses", "completion": "    base_guesses = match['rank']\n    uppercase_variations = uppercase_variations(match)\n    l33t_variations = l33t_variations(match)\n    reversed_variations = 1\n    if match['reversed']:\n        reversed_variations = 2\n\n    return base_guesses * uppercase_variations * l33t_variations * reversed_variations", "idx": 1308}
{"namespace": "zxcvbn.scoring.regex_guesses", "completion": "    character_class_bases = {\n        'digit': 10,\n        'alpha': 26,\n        'alphanum': 36,\n        'space': 32,\n        'graph': 95,\n        'print': 95,\n        'punct': 33,\n        'lower': 26,\n        'upper': 26,\n        'xdigit': 16,\n        'word': 65,\n        'ascii': 128,\n        'unicode': 1112064,\n    }\n\n    if match['regex_name'] == 'digit':\n        return character_class_bases['digit'] ** len(match['token'])\n    elif match['regex_name'] == 'alpha':\n        return character_class_bases['alpha'] ** len(match['token'])\n    elif match['regex_name'] == 'alphanum':\n        return character_class_bases['alphanum'] ** len(match['token'])\n    elif match['regex_name'] == 'space':\n        return character_class_bases['space'] ** len(match['token'])\n    elif match['regex_name'] == 'graph':\n        return character_class_bases['graph'] ** len(match['token'])\n    elif match['regex_name'] == 'print':\n        return character_class_bases['print'] ** len(match['token'])\n    elif match['regex_name'] == 'punct':\n        return character_class_bases['punct'] ** len(match['token'])\n    elif match['regex_name'] == 'lower':\n        return character_class_bases['lower'] ** len(match['token'])\n    elif match['regex_name'] == 'upper':\n        return character_class_bases['upper'] ** len(match['token'])\n    elif match['regex_name'] == 'xdigit':\n        return character_class_bases['xdigit'] ** len(match['token'])\n    elif match['regex_name'] == 'word':\n        return character_class_bases", "idx": 1309}
{"namespace": "zxcvbn.scoring.date_guesses", "completion": "    year = int(match['year'])\n    separator = match['separator']\n    guesses = 0\n    if year > 2017:\n        guesses += (year - 2017) * 12\n    elif year < 2017:\n        guesses += (2017 - year) * 12\n    if separator:\n        guesses *= 4\n    else:\n        guesses *= 12\n    return guesses", "idx": 1310}
{"namespace": "zxcvbn.scoring.spatial_guesses", "completion": "    graph = ADJACENCY_GRAPHS[match['graph']]\n    token_length = len(match['token'])\n    turns = match['turns']\n    shifted_keys = match['shifted_count']\n\n    # Calculate the average degree of the keyboard or keypad\n    if match['graph'] == 'qwerty':\n        average_degree = KEYBOARD_AVERAGE_DEGREE\n        starting_positions = KEYBOARD_STARTING_POSITIONS\n    else:\n        average_degree = KEYPAD_AVERAGE_DEGREE\n        starting_positions = KEYPAD_STARTING_POSITIONS\n\n    # Calculate the number of possible guesses\n    guesses = starting_positions * average_degree ** token_length\n\n    # Consider the number of turns\n    if turns > 0:\n        guesses *= turns + 1\n\n    # Consider the number of shifted keys\n    if shifted_keys > 0:\n        guesses *= 2 ** shifted_keys\n\n    return guesses", "idx": 1311}
{"namespace": "zxcvbn.scoring.uppercase_variations", "completion": "    if ALL_LOWER.match(match['token']):\n        return 1\n    if ALL_UPPER.match(match['token']):\n        return 1\n    if START_UPPER.match(match['token']):\n        return 2\n    if END_UPPER.match(match['token']):\n        return 2\n\n    upper = 0\n    lower = 0\n    for chr in match['token']:\n        if chr.isupper():\n            upper += 1\n        else:\n            lower += 1\n\n    if upper == 0 or lower == 0:\n        return 1\n\n    return upper + lower", "idx": 1312}
{"namespace": "zxcvbn.matching.dictionary_match", "completion": "    matches = []\n    for name, ranked_dictionary in _ranked_dictionaries.items():\n        for word, rank in ranked_dictionary.items():\n            i = password.find(word)\n            while i != -1:\n                matches.append({\n                    'pattern': 'dictionary',\n                    'i': i,\n                    'j': i + len(word) - 1,\n                    'token': word,\n                    'matched_word': word,\n                    'rank': rank,\n                    'dictionary_name': name,\n                })\n                i = password.find(word, i + 1)\n\n    return matches", "idx": 1313}
{"namespace": "zxcvbn.matching.reverse_dictionary_match", "completion": "    matches = []\n    length = len(password)\n    password_lower = password.lower()\n    for dictionary_name, ranked_dict in _ranked_dictionaries.items():\n        for i in range(length):\n            for j in range(i, length):\n                if password_lower[i:j + 1] in ranked_dict:\n                    word = password_lower[i:j + 1]\n                    rank = ranked_dict[word]\n                    matches.append({\n                        'pattern': 'dictionary',\n                        'i': i,\n                        'j': j,\n                        'token': password[i:j + 1],\n                        'matched_word': word,\n                        'rank': rank,\n                        'dictionary_name': dictionary_name,\n                        'reversed': True,\n                        'l33t': False,\n                    })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))", "idx": 1314}
{"namespace": "zxcvbn.matching.l33t_match", "completion": "    matches = []\n    password_lower = password.lower()\n    for dictionary_name, ranked_dict in _ranked_dictionaries.items():\n        for i in range(len(password)):\n            for j in range(i, len(password)):\n                subtable = relevant_l33t_subtable(password_lower[i:j + 1], _l33t_table)\n                if not len(subtable):\n                    continue\n                for sub_dict in enumerate_l33t_subs(subtable):\n                    subbed = translate(password_lower[i:j + 1], sub_dict)\n                    if subbed in ranked_dict:\n                        word = subbed\n                        rank = ranked_dict[word]\n                        matches.append({\n                            'pattern': 'dictionary',\n                            'i': i,\n                            'j': j,\n                            'token': translate(password[i:j + 1], sub_dict),\n                            'matched_word': word,\n                            'rank': rank,\n                            'dictionary_name': dictionary_name,\n                            'reversed': False,\n                            'l33t': True,\n                            'sub': sub_dict,\n                        })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))", "idx": 1315}
{"namespace": "zxcvbn.matching.repeat_match", "completion": "    matches = []\n    greedy_matches = re.finditer(r'(\\w+)\\1+', password)\n    lazy_matches = re.finditer(r'(\\w+?)\\1+', password)\n    for match in greedy_matches:\n        base_token = match.group(1)\n        base_token_length = len(base_token)\n        greedy_match_length = len(match.group(0))\n        lazy_match_length = greedy_match_length\n        if greedy_match_length > lazy_match_length:\n            # greedy match is always longer than lazy match\n            continue\n        repeat_count = greedy_match_length // base_token_length\n        if repeat_count < 2:\n            # repeat count must be at least 2\n            continue\n        # greedy match is always longer than lazy match\n        # greedy match is always a multiple of base token length\n        # lazy match is always a multiple of base token length\n        # lazy match is always at least as long as greedy match\n        # lazy match is always at most as long as greedy match\n        # lazy match is always a subset of greedy match\n        # lazy match is always a subset of password\n        # lazy match is always a subset of greedy match\n        # lazy match is always a subset of password\n        # lazy match is always a subset of greedy match\n        # lazy match is always a subset of password\n        # lazy match is always a subset of greedy match\n        # lazy match is always a subset of password\n        # lazy match is always a subset of greedy match\n        # lazy match is always a subset of password\n        # lazy match is always a subset of greedy match\n        # lazy match is always a subset of password\n        # lazy match is always a subset of greedy match\n        # lazy match is always a subset of password\n        # lazy match is always a subset of greedy match\n        # lazy match is always a subset of password\n        # lazy match is always a subset of greedy match\n        # lazy match is always a subset of password\n        # lazy match is always a subset of greedy match\n        # lazy match is always a subset", "idx": 1316}
{"namespace": "zxcvbn.matching.spatial_match", "completion": "    matches = []\n    for graph_name, graph in _graphs.items():\n        matches.extend(spatial_match_helper(password, graph, graph_name))\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))", "idx": 1317}
{"namespace": "zxcvbn.matching.sequence_match", "completion": "    matches = []\n    for dictionary_name, ranked_dict in _ranked_dictionaries.items():\n        for i in range(len(password)):\n            for j in range(i + 1, len(password) + 1):\n                token = password[i:j]\n                if len(token) < 3:\n                    continue\n                if token in ranked_dict:\n                    continue\n                # if the token is not in the dictionary, check for sequences\n                # of lowercase letters, uppercase letters, digits, and other\n                # unicode characters.\n                #\n                # for example, consider the input 'abcdb975zy'\n                #\n                # password: a   b   c   d   b    9   7   5   z   y\n                # index:    0   1   2   3   4    5   6   7   8   9\n                # delta:      1   1   1  -2  -41  -2  -2  69   1\n                #\n                # expected result:\n                # [(i, j, delta), ...] = [(0, 3, 1), (5, 7, -2), (8, 9, 1)]\n                #\n                # note: this is not true date parsing in that \"feb 31st\" is\n                # allowed, this doesn't check for leap years, etc.\n                #\n                # recipe:\n                # 1. find all sequences of lowercase letters, uppercase letters,\n                #    digits, and other unicode characters.\n                # 2. for each sequence, find the difference between the first\n                #    and last character's unicode codepoint.\n                # 3. if the difference is less than MAX_DELTA, add the sequence\n                #    to the list of matches.\n                #\n                # note: this is not true date parsing in that \"feb 31st\" is\n                # allowed, this doesn't check for leap years, etc.\n                #\n                # recipe:\n                # 1. find all sequences of lowercase letters, uppercase letters,\n                #    digits,", "idx": 1318}
{"namespace": "zxcvbn.matching.regex_match", "completion": "    matches = []\n    for regex_name, regex in _regexen.items():\n        for match in regex.finditer(password):\n            matches.append({\n                'pattern': 'regex',\n                'i': match.start(),\n                'j': match.end() - 1,\n                'token': match.group(),\n                'regex_name': regex_name,\n                'regex_match': match,\n            })\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))", "idx": 1319}
{"namespace": "OpenSSL.rand.add", "completion": "    _lib.RAND_add(buffer, entropy)", "idx": 1320}
{"namespace": "asyncssh.kex.register_kex_alg", "completion": "    _kex_algs.append(alg)\n\n    if default:\n        _default_kex_algs.append(alg)\n\n    _kex_handlers[alg] = (handler, hash_alg, args)", "idx": 1321}
{"namespace": "asyncssh.auth.get_supported_server_auth_methods", "completion": "    return [method for method in _auth_methods\n            if _server_auth_handlers[method].supported(conn)]", "idx": 1322}
{"namespace": "asyncssh.mac.get_mac", "completion": "    handler, hash_size, args = _mac_handler[mac_alg]\n    return handler(key, hash_size, *args)", "idx": 1323}
{"namespace": "asyncssh.auth_keys.SSHAuthorizedKeys.validate", "completion": "        if ca:\n            for entry in self._ca_entries:\n                if entry.key and entry.key == key:\n                    if entry.match_options(client_host, client_addr,\n                                           cert_principals):\n                        return entry.options\n        else:\n            for entry in self._user_entries:\n                if entry.key and entry.key == key:\n                    if entry.match_options(client_host, client_addr,\n                                           cert_principals):\n                        return entry.options\n\n        return None", "idx": 1324}
{"namespace": "asyncssh.saslprep.saslprep", "completion": "    return _stringprep(s, True, _map_saslprep, 'NFKC', (stringprep.in_table_c2,), True)", "idx": 1325}
{"namespace": "asyncssh.asn1.der_decode", "completion": "    value, end = der_decode_partial(data)\n    if end < len(data):\n        raise ASN1DecodeError('Data contains unexpected bytes at end')\n    return value", "idx": 1326}
{"namespace": "asyncssh.packet.SSHPacket.check_end", "completion": "        if self._idx != self._len:\n            raise PacketDecodeError('Incomplete packet')\n\n    ", "idx": 1327}
{"namespace": "asyncssh.public_key.SSHKey.verify", "completion": "        packet = SSHPacket(sig)\n        sig_algorithm = packet.get_string()\n\n        if sig_algorithm not in self.all_sig_algorithms:\n            raise ValueError('Unrecognized signature algorithm')\n\n        if sig_algorithm.startswith(b'x509v3-'):\n            sig_algorithm = sig_algorithm[7:]\n\n        if sig_algorithm not in self.sig_algorithms:\n            raise ValueError('Unrecognized signature algorithm')\n\n        return self.verify_ssh(data, sig_algorithm, packet)\n", "idx": 1328}
{"namespace": "asyncssh.public_key.SSHKey.convert_to_public", "completion": "        if self.is_private:\n            if self.algorithm == b'ssh-rsa':\n                return SSHKey.make_public(self.key_params)\n            elif self.algorithm == b'ssh-dss':\n                return SSHKey.make_public(self.key_params)\n            elif self.algorithm == b'ssh-ed25519':\n                return SSHKey.make_public(self.key_params)\n            elif self.algorithm == b'ssh-ecdsa':\n                return SSHKey.make_public(self.key_params)\n            else:\n                raise KeyExportError('Unsupported key type')\n        else:\n            return self\n", "idx": 1329}
{"namespace": "asyncssh.public_key.SSHKey.generate_x509_user_certificate", "completion": "        return self._generate_x509_certificate(user_key, subject, issuer,\n                                               serial, valid_after,\n                                               valid_before, False, None,\n                                               purposes, hash_alg, principals,\n                                               hash_alg, comment)\n", "idx": 1330}
{"namespace": "asyncssh.misc.write_file", "completion": "    with open_file(filename, mode) as f:\n        return f.write(data)", "idx": 1331}
{"namespace": "diffprivlib.accountant.BudgetAccountant.__repr__", "completion": "        if self.epsilon == float(\"inf\") and self.delta == 1:\n            return f\"{self.__class__.__name__}()\"\n\n        attributes = []\n        if self.epsilon != float(\"inf\"):\n            attributes.append(f\"epsilon={self.epsilon}\")\n        if self.delta != 1:\n            attributes.append(f\"delta={self.delta}\")\n        if self.slack > 0:\n            attributes.append(f\"slack={self.slack}\")\n\n        if len(self.spent_budget) > n_budget_max:\n            attributes.append(f\"spent_budget={self.spent_budget[:n_budget_max]}...\")\n        else:\n            attributes.append(f\"spent_budget={self.spent_budget}\")\n\n        return f\"{self.__class__.__name__}({', '.join(attributes)})\"\n\n    ", "idx": 1332}
{"namespace": "diffprivlib.accountant.BudgetAccountant.check", "completion": "        if epsilon < 0 or delta < 0:\n            raise ValueError(f\"Epsilon and delta must be non-negative, got ({epsilon}, {delta})\")\n\n        if epsilon == 0 and delta == 0:\n            return\n\n        if epsilon < self.__min_epsilon:\n            raise ValueError(f\"Epsilon must be at least {self.__min_epsilon}, got {epsilon}\")\n\n        if epsilon > self.epsilon:\n            raise BudgetError(f\"Privacy spend of ({epsilon},{delta}) not permissible; will exceed remaining privacy \"\n                              f\"budget. Use {self.__class__.__name__}.remaining() to check remaining budget.\")\n\n        if delta > self.delta:\n            raise BudgetError(f\"Privacy spend of ({epsilon},{delta}) not permissible; will exceed remaining privacy \"\n                              f\"budget. Use {self.__class__.__name__}.remaining() to check remaining budget.\")\n\n        if epsilon == 0:\n            return\n\n        if delta == 0:\n            return\n\n        if epsilon == 0 and delta == 0:\n            return\n\n        if epsilon == 0:\n            return\n\n        if delta == 0:\n            return\n\n        if epsilon == 0 and delta == 0:\n            return\n\n        if epsilon == 0:\n            return\n\n        if delta == 0:\n            return\n\n        if epsilon == 0 and delta == 0:\n            return\n\n        if epsilon == 0:\n            return\n\n        if delta == 0:\n            return\n\n        if epsilon == 0 and delta == 0:\n            return\n\n        if epsilon == 0:\n            return\n\n        if delta == 0:\n            return\n\n        if epsilon == 0 and delta == 0:\n            return\n\n        if epsilon == 0:\n            return\n\n        if delta == 0:\n            return\n\n        if epsilon == 0 and delta == 0:\n            return\n\n        if epsilon == 0:\n            return\n\n        if delta == 0:\n            return\n\n        if epsilon == 0 and delta == 0:\n            return\n\n        if", "idx": 1333}
{"namespace": "diffprivlib.accountant.BudgetAccountant.spend", "completion": "        if not self.check(epsilon, delta):\n            raise BudgetError(f\"Privacy spend of ({epsilon},{delta}) not permissible; will exceed remaining privacy \"\n                              f\"budget. Use {self.__class__.__name__}.{self.remaining.__name__}() to check remaining \"\n                              f\"budget.\")\n\n        self.__spent_budget.append((epsilon, delta))\n        return self", "idx": 1334}
{"namespace": "diffprivlib.accountant.BudgetAccountant.load_default", "completion": "        if accountant is None:\n            if BudgetAccountant._default is None:\n                accountant = BudgetAccountant()\n            else:\n                accountant = BudgetAccountant._default\n        elif not isinstance(accountant, BudgetAccountant):\n            raise TypeError(\"accountant must be an instance of BudgetAccountant\")\n        return accountant\n\n    ", "idx": 1335}
{"namespace": "diffprivlib.accountant.BudgetAccountant.set_default", "completion": "        BudgetAccountant._default = self\n        return self", "idx": 1336}
{"namespace": "diffprivlib.accountant.BudgetAccountant.pop_default", "completion": "        default = BudgetAccountant._default\n        BudgetAccountant._default = None\n        return default", "idx": 1337}
{"namespace": "diffprivlib.validation.clip_to_bounds", "completion": "    if not isinstance(array, np.ndarray):\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\n    if array.ndim != 2:\n        raise ValueError(f\"input array must be 2-dimensional, got {array.ndim} dimensions.\")\n    if not isinstance(bounds, tuple):\n        raise TypeError(f\"Bounds must be specified as a tuple of (min, max), got {type(bounds)}.\")\n    if not isinstance(shape, Integral):\n        raise TypeError(f\"shape parameter must be integer-valued, got {type(shape)}.\")\n\n    lower, upper = bounds\n\n    if np.asarray(lower).size == 1 or np.asarray(upper).size == 1:\n        lower = np.ravel(lower).astype(dtype)\n        upper = np.ravel(upper).astype(dtype)\n    else:\n        lower = np.asarray(lower, dtype=dtype)\n        upper = np.asarray(upper, dtype=dtype)\n\n    if lower.shape != upper.shape:\n        raise ValueError(\"lower and upper bounds must be the same shape array\")\n    if lower.ndim > 1:\n        raise ValueError(\"lower and upper bounds must be scalar or a 1-dimensional array\")\n    if lower.size not in (1, shape):\n        raise ValueError(f\"lower and upper bounds must have {shape or 1} element(s), got {lower.size}.\")\n\n    n_bounds = lower.shape[0]\n\n    for i in range(n_bounds):\n        _lower = lower[i]\n        _upper = upper[i]\n\n        if not isinstance(_lower, Real) or not isinstance(_upper, Real):\n            raise TypeError(f\"Each bound must be numeric, got {_lower} ({type(_lower)}) and {_upper} ({type(_upper)}).\")\n\n        if _lower > _upper:\n            raise ValueError(f\"For each bound, lower bound must be smaller than upper bound, got {", "idx": 1338}
{"namespace": "diffprivlib.models.naive_bayes.GaussianNB._update_mean_variance", "completion": "        if n_noisy is None:\n            n_noisy = X.shape[0]\n\n        # Calculate the new mean\n        total_mu = (mu * n_past + X.sum(axis=0)) / (n_past + n_noisy)\n\n        # Calculate the new variance\n        total_var = (var * n_past + np.sum((X - mu) ** 2, axis=0)) / (n_past + n_noisy)\n\n        return total_mu, total_var\n\n    ", "idx": 1339}
{"namespace": "diffprivlib.models.naive_bayes.GaussianNB._noisy_class_counts", "completion": "        unique_y = np.unique(y)\n        n_classes = len(unique_y)\n        class_counts = np.zeros(n_classes)\n\n        for i, y_i in enumerate(unique_y):\n            class_counts[i] = np.sum(y == y_i)\n\n        # Add noise to the class counts to satisfy differential privacy\n        epsilon_per_class = self.epsilon / n_classes\n        noisy_class_counts = np.zeros(n_classes)\n\n        for i, count in enumerate(class_counts):\n            noisy_class_counts[i] = GeometricTruncated(epsilon=epsilon_per_class, delta=0, sensitivity=1,\n                                                       lower=0, upper=count, random_state=random_state).randomise(count)\n\n        return noisy_class_counts", "idx": 1340}
{"namespace": "diffprivlib.models.standard_scaler._incremental_mean_and_var", "completion": "    random_state = check_random_state(random_state)\n\n    # Calculate the mean and variance of the new data\n    new_mean = np.mean(X, axis=0)\n    new_variance = nanvar(X, axis=0)\n    new_sample_count = X.shape[0]\n\n    # Update the mean and variance using the previous mean and variance\n    if last_sample_count > 0:\n        last_mean = last_mean * last_sample_count\n        last_variance = last_variance * last_sample_count\n        new_mean = new_mean * new_sample_count\n        new_variance = new_variance * new_sample_count\n\n        last_sample_count += new_sample_count\n        new_mean += last_mean\n        new_variance += last_variance\n\n        new_mean /= last_sample_count\n        new_variance /= last_sample_count\n        new_variance -= new_mean ** 2\n\n    # Add noise to the mean and variance\n    new_mean = random_state.laplace(new_mean, 2 / epsilon)\n    new_variance = random_state.laplace(new_variance, 2 / epsilon)\n\n    # Clip the variance to be non-negative\n    new_variance = np.maximum(new_variance, 0)\n\n    # Clip the mean to be within the bounds\n    new_mean = np.clip(new_mean, bounds[0], bounds[1])\n\n    return new_mean, new_variance, last_sample_count", "idx": 1341}
{"namespace": "diffprivlib.models.linear_regression.LinearRegression.fit", "completion": "        X, y, X_offset, y_offset, X_scale = self._preprocess_data(X, y, self.fit_intercept, self.epsilon,\n                                                                  self.bounds_X, self.bounds_y, self.copy_X,\n                                                                  check_input=True, random_state=self.random_state)\n\n        if self.fit_intercept:\n            X = np.hstack((X, np.ones((X.shape[0], 1), dtype=X.dtype)))\n\n        self.regression_obj, self.noisy_coefs = _construct_regression_obj(X, y, self.bounds_X, self.bounds_y,\n                                                                          self.epsilon, self.alpha,\n                                                                          self.random_state)\n\n        self.coef_ = np.zeros(X.shape[1], dtype=X.dtype)\n        self.intercept_ = np.zeros(y.shape[1] if y.ndim > 1 else 1, dtype=X.dtype)\n\n        for i in range(y.shape[1] if y.ndim > 1 else 1):\n            self.coef_, self.intercept_[i] = minimize(self.regression_obj[i], self.coef_, method='L-BFGS-B',\n                                                      jac=True, options={'disp': False})\n\n        self.coef_ = self.coef_ / X_scale\n        self.intercept_ = self.intercept_ - np.dot(X_offset, self.coef_) + y_offset\n\n        self.accountant.add_sensitivity(self.epsilon, self.bounds_X, self.bounds_y)\n\n        return self\n", "idx": 1342}
{"namespace": "diffprivlib.models.k_means.KMeans.fit", "completion": "        if self.bounds is None:\n            warnings.warn(\"Bounds not provided.  Results may be inaccurate.\", PrivacyLeakWarning)\n\n        if self.epsilon <= 0:\n            raise ValueError(\"Epsilon must be positive.\")\n\n        if self.n_clusters <= 0:\n            raise ValueError(\"Number of clusters must be positive.\")\n\n        if self.n_clusters > X.shape[0]:\n            raise ValueError(\"Number of clusters must be less than or equal to the number of samples.\")\n\n        if self.random_state is None:\n            random_state = np.random.RandomState()\n        elif isinstance(self.random_state, int):\n            random_state = np.random.RandomState(self.random_state)\n        else:\n            random_state = self.random_state\n\n        dims = X.shape[1]\n        total_iters = self._calc_iters(dims, X.shape[0])\n\n        centers = self._init_centers(dims, random_state)\n\n        if centers is None:\n            raise ValueError(\"Could not find suitable cluster centers.\")\n\n        for i in range(total_iters):\n            distances, labels = self._distances_labels(X, centers)\n            centers = self._update_centers(X, centers, labels, dims, total_iters, random_state)\n\n        self.cluster_centers_ = centers\n        self.labels_ = labels\n        self.inertia_ = distances[np.arange(X.shape[0]), labels].sum()\n        self.n_iter_ = total_iters\n\n        return self", "idx": 1343}
{"namespace": "diffprivlib.models.forest._FittingTree.__getstate__", "completion": "        return {\n            \"max_depth\": self.max_depth,\n            \"node_count\": self.node_count,\n            \"nodes\": self.nodes,\n            \"values\": self.values_\n        }\n", "idx": 1344}
{"namespace": "diffprivlib.models.forest._FittingTree.fit", "completion": "        if not self.nodes:\n            raise ValueError(\"Tree has not been built. Call .build() first.\")\n\n        leaves = self.apply(X)\n        unique_leaves = np.unique(leaves)\n        self.values_ = np.zeros((len(unique_leaves), self.n_classes_), dtype=DTYPE)\n\n        for leaf, y_val in zip(leaves, y):\n            self.values_[leaf] += y_val\n\n        for leaf in unique_leaves:\n            if leaf == self._TREE_LEAF:\n                continue\n\n            self.values_[leaf] /= np.sum(self.values_[leaf])\n            self.values_[leaf] = PermuteAndFlip(self.classes, self.epsilon, self.random_state).randomise(\n                self.values_[leaf])\n\n        return self", "idx": 1345}
{"namespace": "diffprivlib.tools.histograms.histogram", "completion": "    warn_unused_args(unused_args)\n\n    random_state = check_random_state(random_state)\n\n    accountant = BudgetAccountant.load_default(accountant)\n    accountant.check(epsilon, 0)\n\n    hist, bin_edges = np.histogram(sample, bins=bins, range=range, weights=weights, density=density)\n\n    dp_mech = GeometricTruncated(epsilon=epsilon, sensitivity=1, lower=0, upper=maxsize, random_state=random_state)\n\n    dp_hist = np.zeros_like(hist)\n    iterator = np.nditer(hist, flags=['multi_index'])\n\n    while not iterator.finished:\n        dp_hist[iterator.multi_index] = dp_mech.randomise(int(iterator[0]))\n        iterator.iternext()\n\n    dp_hist = dp_hist.astype(float, casting='safe')\n\n    accountant.spend(epsilon, 0)\n\n    return dp_hist, bin_edges", "idx": 1346}
{"namespace": "diffprivlib.tools.histograms.histogram2d", "completion": "    warn_unused_args(unused_args)\n\n    random_state = check_random_state(random_state)\n\n    accountant = BudgetAccountant.load_default(accountant)\n    accountant.check(epsilon, 0)\n\n    # Range only required if bin edges not specified\n    if np.array(bins, dtype=object).ndim == 0 or not np.all([np.ndim(_bin) for _bin in bins]):\n        if range is None or (isinstance(range, list) and None in range):\n            warnings.warn(\"Range parameter has not been specified (or has missing elements). Falling back to taking \"\n                          \"range from the data.\\n \"\n                          \"To ensure differential privacy, and no additional privacy leakage, the range must be \"\n                          \"specified for each dimension independently of the data (i.e., using domain knowledge).\",\n                          PrivacyLeakWarning)\n\n    hist, xedges, yedges = np.histogram2d(array_x, array_y, bins=bins, range=range, weights=weights, density=None)\n\n    dp_mech = GeometricTruncated(epsilon=epsilon, sensitivity=1, lower=0, upper=maxsize, random_state=random_state)\n\n    dp_hist = np.zeros_like(hist)\n    iterator = np.nditer(hist, flags=['multi_index'])\n\n    while not iterator.finished:\n        dp_hist[iterator.multi_index] = dp_mech.randomise(int(iterator[0]))\n        iterator.iternext()\n\n    dp_hist = dp_hist.astype(float, casting='safe')\n\n    if density:\n        # calculate the probability density function\n        dims = len(dp_hist.shape)\n        dp_hist_sum = dp_hist.sum()\n        for i in np.arange(dims):\n            shape = np.ones(dims, int)\n            shape[i] = dp_hist.shape[i]\n            # noinspection PyUn", "idx": 1347}
{"namespace": "diffprivlib.tools.utils.nanmean", "completion": "    warn_unused_args(unused_args)\n\n    return _mean(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                 random_state=random_state, accountant=accountant, nan=True)", "idx": 1348}
{"namespace": "diffprivlib.tools.utils.var", "completion": "    warn_unused_args(unused_args)\n\n    return _var(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=False)", "idx": 1349}
{"namespace": "diffprivlib.tools.utils.nanvar", "completion": "    warn_unused_args(unused_args)\n\n    return _var(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)", "idx": 1350}
{"namespace": "diffprivlib.tools.utils.std", "completion": "    warn_unused_args(unused_args)\n\n    return _std(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=False)", "idx": 1351}
{"namespace": "diffprivlib.tools.utils.nanstd", "completion": "    warn_unused_args(unused_args)\n\n    return _std(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)", "idx": 1352}
{"namespace": "diffprivlib.tools.utils.sum", "completion": "    warn_unused_args(unused_args)\n\n    return _sum(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=False)", "idx": 1353}
{"namespace": "diffprivlib.tools.utils.nansum", "completion": "    warn_unused_args(unused_args)\n\n    return _sum(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)", "idx": 1354}
{"namespace": "diffprivlib.tools.quantiles.quantile", "completion": "    warn_unused_args(unused_args)\n\n    if not isinstance(quant, np.ndarray):\n        quant = np.asarray(quant)\n\n    if np.any(quant < 0) or np.any(quant > 1):\n        raise ValueError(\"Quantiles must be between 0 and 1 inclusive\")\n\n    if bounds is None:\n        bounds = (np.min(array), np.max(array))\n\n    if bounds[0] > bounds[1]:\n        raise ValueError(\"Bounds must be (min, max) with min <= max\")\n\n    if not isinstance(epsilon, (int, float)) or epsilon < 0:\n        raise ValueError(\"Epsilon must be a non-negative number\")\n\n    if not isinstance(keepdims, bool):\n        raise ValueError(\"Keepdims must be a boolean\")\n\n    if axis is not None and not isinstance(axis, int):\n        raise ValueError(\"Axis must be an integer\")\n\n    if accountant is not None and not isinstance(accountant, BudgetAccountant):\n        raise ValueError(\"Accountant must be a BudgetAccountant\")\n\n    if accountant is not None:\n        accountant.account_for_random_sampling(epsilon)\n\n    if random_state is not None:\n        if not isinstance(random_state, np.random.RandomState):\n            raise ValueError(\"Random state must be a RandomState\")\n\n    if axis is None:\n        array = array.ravel()\n        axis = 0\n\n    if not isinstance(axis, int):\n        raise ValueError(\"Axis must be an integer\")\n\n    if axis < 0:\n        axis = array.ndim + axis\n\n    if axis >= array.ndim:\n        raise ValueError(\"Axis must be less than the number of dimensions of the array\")\n\n    if not isinstance(keepdims, bool):\n        raise ValueError(\"Keepdims must be a boolean\")\n\n    if not isinstance(quant, np.ndarray):\n        quant = np.asarray(quant)\n\n    if quant.ndim > 1:\n        raise Value", "idx": 1355}
{"namespace": "diffprivlib.tools.quantiles.percentile", "completion": "    from diffprivlib.utils import check_random_state\n    from diffprivlib.utils import PrivacyLeakWarning\n    from diffprivlib.accountant import BudgetAccountant\n    from diffprivlib.validation import clip_to_bounds\n    from diffprivlib.tools.utils import _wrap_axis\n    from diffprivlib.validation import check_bounds\n    warn_unused_args(unused_args)\n\n    random_state = check_random_state(random_state)\n\n    if bounds is None:\n        warnings.warn(\"Bounds have not been specified and will be calculated on the data provided. This will \"\n                      \"result in additional privacy leakage. To ensure differential privacy and no additional \"\n                      \"privacy leakage, specify bounds for each dimension.\", PrivacyLeakWarning)\n        bounds = (np.min(array), np.max(array))\n\n    percent = np.ravel(percent)\n\n    if np.any(percent < 0) or np.any(percent > 100):\n        raise ValueError(\"Percentiles must be in the range [0, 100].\")\n\n    if len(percent) > 1:\n        return np.array([percentile(array, p_i, epsilon=epsilon / len(percent), bounds=bounds, axis=axis, keepdims=keepdims,\n                                    accountant=accountant, random_state=random_state) for p_i in percent])\n\n    # Dealing with a single percent from now on\n    percent = percent.item()\n\n    if axis is not None or keepdims:\n        return _wrap_axis(percentile, array, percent=percent, epsilon=epsilon, bounds=bounds, axis=axis, keepdims=keepdims,\n                          random_state=random_state, accountant=accountant)\n\n    # Dealing with a scalar output from now on\n    bounds = check_bounds(bounds, shape=0, min_separation=1e-5)\n\n    accountant = BudgetAccountant.load_default(accountant)\n    accountant.check(epsilon, 0)\n\n    # Let's ra", "idx": 1356}
{"namespace": "diffprivlib.mechanisms.base.bernoulli_neg_exp", "completion": "    if not isinstance(gamma, Real):\n        raise TypeError(\"Gamma must be numeric\")\n\n    if gamma < 0:\n        raise ValueError(\"Gamma must be non-negative\")\n\n    rng = check_random_state(random_state, True)\n\n    if gamma == 0:\n        return rng.randint(0, 1)\n\n    return rng.binomial(1, 1 - np.exp(-gamma))", "idx": 1357}
{"namespace": "discord.utils.snowflake_time", "completion": "    return datetime.datetime.fromtimestamp(((id >> 22) + 1420070400000) / 1000, datetime.timezone.utc)", "idx": 1358}
{"namespace": "discord.utils.time_snowflake", "completion": "    if dt.tzinfo is None:\n        dt = dt.astimezone()\n    timestamp = int((dt - datetime.datetime(1970, 1, 1, tzinfo=datetime.timezone.utc)).total_seconds() * 1000)\n    if high:\n        timestamp = timestamp >> 22\n    return timestamp", "idx": 1359}
{"namespace": "discord.utils.resolve_invite", "completion": "    from .invite import Invite  # circular import\n\n    if isinstance(invite, Invite):\n        return ResolvedInvite(invite.code, invite.event_id)\n    elif isinstance(invite, str):\n        rx = r'(?:https?\\:\\/\\/)?(?:ptb\\.|canary\\.)?discord(?:\\.gg|(?:app)?\\.com\\/invite)\\/([a-zA-Z0-9\\-]+)'\n        m = re.match(rx, invite)\n        if m:\n            return ResolvedInvite(m.group(1), None)\n        return ResolvedInvite(invite, None)\n    raise TypeError('invite must be a string or an Invite object')", "idx": 1360}
{"namespace": "discord.utils.resolve_annotation", "completion": "    if annotation is None:\n        return type(None)\n\n    if isinstance(annotation, str):\n        return ForwardRef(annotation)\n\n    if localns is None:\n        namespace = globalns\n    else:\n        namespace = localns\n\n    if cache is None:\n        cache = {}\n\n    return evaluate_annotation(annotation, globalns, namespace, cache)", "idx": 1361}
{"namespace": "discord.ext.tasks.loop", "completion": "    def decorator(coro: LF) -> Loop[LF]:\n        return Loop(\n            coro,\n            seconds=seconds,\n            minutes=minutes,\n            hours=hours,\n            time=time,\n            count=count,\n            reconnect=reconnect,\n        )\n\n    return decorator", "idx": 1362}
{"namespace": "barf.analysis.gadgets.classifier.GadgetClassifier.classify", "completion": "        # Classify gadgets.\n        classified = []\n\n        for gadget_type, classifier in self._classifiers.items():\n            classified += self._classify(gadget, classifier, gadget_type, self._emu_iters)\n\n        # Sort classified gadgets.\n        classified = sorted(classified, key=lambda x: str(x))\n\n        return classified", "idx": 1363}
{"namespace": "barf.analysis.gadgets.finder.GadgetFinder.find", "completion": "        self._max_bytes = byte_depth\n        self._instrs_depth = instrs_depth\n\n        if self._architecture == ARCH_X86:\n            candidates = self._find_x86_candidates(start_address, end_address)\n        elif self._architecture == ARCH_ARM:\n            candidates = self._find_arm_candidates(start_address, end_address)\n        else:\n            raise NotImplementedError\n\n        return sorted(candidates, key=lambda x: x.address)", "idx": 1364}
{"namespace": "barf.core.reil.parser.ReilParser.parse", "completion": "        parsed_instrs = []\n\n        for instr in instrs:\n            instr = instr.lower()\n\n            if instr in self._cache:\n                parsed_instrs.append(copy.deepcopy(self._cache[instr]))\n\n            else:\n                try:\n                    parsed_instr = instruction.parseString(instr)\n\n                    self._cache[instr] = parsed_instr\n\n                    parsed_instrs.append(copy.deepcopy(parsed_instr))\n\n                except Exception as e:\n                    logger.error(\"Error parsing instruction: %s\", e)\n\n        return parsed_instrs", "idx": 1365}
{"namespace": "barf.core.smt.smtfunction.zero_extend", "completion": "    assert type(s) in (Constant, BitVec) and size - s.size >= 0\n\n    if size == s.size:\n        return s\n\n    return BitVec(size, \"(_ zero_extend {})\".format(size - s.size), s)", "idx": 1366}
{"namespace": "barf.core.smt.smtfunction.extract", "completion": "    assert type(s) in (Constant, BitVec) and offset >= 0 and size >= 0 and offset + size <= s.size\n\n    if offset == 0 and size == s.size:\n        return s\n\n    return BitVec(size, \"(_ extract {} {})\".format(offset, size), s)", "idx": 1367}
{"namespace": "barf.core.smt.smtfunction.ite", "completion": "    assert type(cond) == Bool\n    assert type(true) == BitVec and type(false) == BitVec\n    assert true.size == size and false.size == size\n\n    return BitVec(size, \"ite\", cond, true, false)", "idx": 1368}
{"namespace": "barf.core.smt.smtfunction.concat", "completion": "    if len(args) == 1:\n        return args[0]\n\n    return BitVec(size, \"concat\", *args)", "idx": 1369}
{"namespace": "barf.core.smt.smtsymbol.BitVecArray.declaration", "completion": "        return \"(declare-fun {:s} () (Array (_ BitVec {:d}) (_ BitVec {:d})))\".format(self.name, self.key_size,\n                                                                                      self.value_size)\n\n    ", "idx": 1370}
{"namespace": "barf.arch.translator.InstructionTranslator.translate", "completion": "        try:\n            return self._translate(instruction)\n        except Exception as e:\n            self._log_translation_exception(instruction)\n            raise TranslationError(\"Unknown error\")", "idx": 1371}
{"namespace": "barf.arch.emulator.Emulator.load_binary", "completion": "        f = open(binary, 'rb')\n        file_signature = f.read(2)\n        f.close()\n\n        if file_signature == b'\\x7fELF':\n            self._load_binary_elf(binary)\n        elif file_signature == b'MZ':\n            self._load_binary_pe(binary)\n        else:\n            raise Exception(\"Unknown file format.\")\n\n    ", "idx": 1372}
{"namespace": "barf.arch.arm.parser.ArmParser.parse", "completion": "        instr = instr.lower()\n\n        if instr in self._cache:\n            return copy.deepcopy(self._cache[instr])\n\n        try:\n            parsed_instr = instruction.parseString(instr, parseAll=True)[0]\n            self._cache[instr] = parsed_instr\n            self._check_instruction(parsed_instr)\n            return copy.deepcopy(parsed_instr)\n        except Exception as e:\n            logger.error(\"Error parsing instruction: %s\" % e)\n            return None", "idx": 1373}
{"namespace": "barf.arch.x86.parser.X86Parser.parse", "completion": "        instr = instr.lower()\n\n        if instr in self._cache:\n            return copy.deepcopy(self._cache[instr])\n\n        try:\n            instr = instruction.parseString(instr, parseAll=True)[0]\n        except Exception as e:\n            logger.error(\"Error parsing instruction: %s\" % e)\n            return None\n\n        self._cache[instr] = instr\n\n        return copy.deepcopy(instr)", "idx": 1374}
{"namespace": "faker.utils.text.slugify", "completion": "    if allow_dots:\n        value = _re_pattern_allow_dots.sub(\"\", value)\n    else:\n        value = _re_pattern.sub(\"\", value)\n    if allow_unicode:\n        value = unicodedata.normalize(\"NFKC\", value)\n    else:\n        value = unicodedata.normalize(\"NFKD\", value).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n    value = _re_spaces.sub(\"-\", value).strip(\"-_\")\n    return value", "idx": 1375}
{"namespace": "faker.utils.checksums.calculate_luhn", "completion": "    check_digit = luhn_checksum(partial_number * 10)\n    if check_digit == 0:\n        return check_digit\n    else:\n        return 10 - check_digit", "idx": 1376}
{"namespace": "faker.utils.distribution.choices_distribution_unique", "completion": "    if random is None:\n        random = mod_random\n\n    if p is not None:\n        assert len(a) == len(p)\n\n    if hasattr(random, \"choices\"):\n        if length == 1 and p is None:\n            return [random.choice(a)]\n        else:\n            return random.choices(a, weights=p, k=length)\n    else:\n        choices = []\n\n        if p is None:\n            p = itertools.repeat(1, len(a))  # type: ignore\n\n        cdf = list(cumsum(p))  # type: ignore\n        normal = cdf[-1]\n        cdf2 = [i / normal for i in cdf]\n        for i in range(length):\n            uniform_sample = random_sample(random=random)\n            idx = bisect.bisect_right(cdf2, uniform_sample)\n            item = a[idx]\n            choices.append(item)\n        return choices", "idx": 1377}
{"namespace": "faker.utils.loading.find_available_locales", "completion": "    available_locales = set()\n    for provider in providers:\n        provider_mod = import_module(provider)\n        if hasattr(provider_mod, \"is_localized\"):\n            if provider_mod.is_localized():\n                available_locales.update(provider_mod.get_languages())\n    return sorted(available_locales)", "idx": 1378}
{"namespace": "faker.utils.loading.find_available_providers", "completion": "    available_providers = set()\n\n    for module in modules:\n        if getattr(module, \"__package__\", False):\n            providers = [f\"{module.__package__}.{name}\" for name in list_module(module)]\n            available_providers.update(providers)\n    return sorted(available_providers)", "idx": 1379}
{"namespace": "faker.providers.credit_card.Provider._generate_number", "completion": "        number = prefix\n        for _ in range(length - len(prefix)):\n            number += self.random_element([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"])\n\n        check_digit = self._calculate_check_digit(number)\n        return number + str(check_digit)\n", "idx": 1380}
{"namespace": "faker.decode.unidecode", "completion": "    ", "idx": 1381}
{"namespace": "dash.fingerprint.build_fingerprint", "completion": "    # Extract the filename and extension from the path\n    filename, extension = path.split(\".\")\n\n    # Construct the file path without the filename\n    file_path = \"/\".join(path.split(\"/\")[:-1])\n\n    # Replace the version with underscores\n    v_str = version_clean.sub(\"_\", str(version))\n\n    # Construct the fingerprint\n    fingerprint = f\"{file_path}.v{v_str}m{hash_value}.{extension}\"\n\n    return fingerprint", "idx": 1382}
{"namespace": "dash.fingerprint.check_fingerprint", "completion": "    if cache_regex.match(path):\n        return path.split(\".\")[0], True\n    else:\n        return path, False", "idx": 1383}
{"namespace": "dash._configs.pages_folder_config", "completion": "    if use_pages:\n        if pages_folder is None:\n            raise exceptions.InvalidConfig(\n                f\"{name} `use_pages` is True but `pages_folder` is not set.\"\n            )\n        if not os.path.exists(pages_folder):\n            raise exceptions.InvalidConfig(\n                f\"{name} `use_pages` is True but `pages_folder` does not exist.\"\n            )\n        return pages_folder\n    else:\n        return None", "idx": 1384}
{"namespace": "dash._grouping.flatten_grouping", "completion": "    if isinstance(grouping, (tuple, list)):\n        return [flatten_grouping(g, schema=schema) for g in grouping]\n\n    if isinstance(grouping, dict):\n        return [\n            flatten_grouping(g, schema=schema)\n            for g in grouping.values()\n            if g is not None\n        ]\n\n    if schema is not None:\n        return [grouping]\n\n    return [grouping]", "idx": 1385}
{"namespace": "dash._grouping.make_grouping_by_index", "completion": "    if isinstance(schema, (tuple, list)):\n        grouping = []\n        for i, s in enumerate(schema):\n            grouping.append(make_grouping_by_index(s, flat_values[i]))\n        return grouping\n\n    if isinstance(schema, dict):\n        grouping = {}\n        for k, s in schema.items():\n            grouping[k] = make_grouping_by_index(s, flat_values[k])\n        return grouping\n\n    return flat_values", "idx": 1386}
{"namespace": "dash._grouping.map_grouping", "completion": "    if isinstance(grouping, (tuple, list)):\n        return [map_grouping(fn, el) for el in grouping]\n\n    if isinstance(grouping, dict):\n        return {k: map_grouping(fn, v) for k, v in grouping.items()}\n\n    return fn(grouping)", "idx": 1387}
{"namespace": "dash._grouping.validate_grouping", "completion": "    if full_schema is None:\n        full_schema = schema\n\n    if isinstance(schema, (tuple, list)):\n        SchemaTypeValidationError.check(grouping, full_schema, path, list)\n        SchemaLengthValidationError.check(grouping, full_schema, path, len(schema))\n\n        for i, (group_el, schema_el) in enumerate(zip(grouping, schema)):\n            validate_grouping(group_el, schema_el, full_schema, path + (i,))\n\n    elif isinstance(schema, dict):\n        SchemaTypeValidationError.check(grouping, full_schema, path, dict)\n        SchemaKeysValidationError.check(grouping, full_schema, path, schema.keys())\n\n        for k, v in schema.items():\n            validate_grouping(grouping[k], v, full_schema, path + (k,))\n\n    else:\n        SchemaTypeValidationError.check(grouping, full_schema, path, type(schema))", "idx": 1388}
{"namespace": "dash._get_paths.app_get_relative_path", "completion": "    if requests_pathname == \"/\" and path == \"\":\n        return \"/\"\n    if requests_pathname != \"/\" and path == \"\":\n        return requests_pathname\n    if not path.startswith(\"/\"):\n        raise exceptions.UnsupportedRelativePath(\n            f\"\"\"\n            Paths that aren't prefixed with / are not supported.\n            You supplied: {path}\n            \"\"\"\n        )\n    return \"/\".join([requests_pathname.rstrip(\"/\"), path.lstrip(\"/\")])", "idx": 1389}
{"namespace": "dash._get_paths.app_strip_relative_path", "completion": "    if requests_pathname != \"/\" and path.startswith(requests_pathname):\n        path = path.replace(requests_pathname, \"\")\n    if requests_pathname.endswith(\"/\") and path.startswith(\"/\"):\n        path = path.replace(\"/\", \"\", 1)\n    return path", "idx": 1390}
{"namespace": "dash.development._py_components_generation.js_to_py_type", "completion": "    if is_flow_type:\n        return map_js_to_py_types_flow_types(type_object)(indent_num)\n    return map_js_to_py_types_prop_types(type_object, indent_num)()", "idx": 1391}
{"namespace": "dash.development.component_loader.load_components", "completion": "    from ._py_components_generation import generate_imports\n    from ._py_components_generation import generate_class_file\n    from ._py_components_generation import generate_classes_files\n    data = _get_metadata(metadata_path)\n    imports_path = os.path.join(namespace, \"_imports_.py\")\n\n    # Make sure the file doesn't exist, as we use append write\n    if os.path.exists(imports_path):\n        os.remove(imports_path)\n\n    components = generate_classes_files(namespace, data, generate_class_file)\n\n    # Add the __all__ value so we can import * from _imports_\n    generate_imports(namespace, components)\n\n    return components", "idx": 1392}
{"namespace": "dash.development.component_loader.generate_classes", "completion": "    # Start processing\n    data = _get_metadata(metadata_path)\n\n    # Iterate over each property name (which is a path to the component)\n    for componentPath in data:\n        componentData = data[componentPath]\n\n        # Extract component name from path\n        # e.g. src/components/MyControl.react.js\n        # TODO Make more robust - some folks will write .jsx and others\n        # will be on windows. Unfortunately react-docgen doesn't include\n        # the name of the component atm.\n        name = componentPath.split(\"/\").pop().split(\".\")[0]\n        component = generate_class(\n            name, componentData[\"props\"], componentData[\"description\"], namespace, None\n        )\n\n        # Write the class to a file\n        component.write_to_file(namespace)\n\n    # Generate the imports file\n    generate_imports(namespace)\n\n    # Add the \"__all__\" value to the imports file\n    add_all_to_imports(namespace)", "idx": 1393}
{"namespace": "dash.development.base_component.Component.to_plotly_json", "completion": "        # pylint: disable=no-member\n        json_obj = {\n            \"props\": {\n                k: v\n                for k, v in self.__dict__.items()\n                if k not in self._base_nodes\n                and not k.startswith(\"_\")\n                and not k.startswith(\"aria-\")\n                and not k.startswith(\"data-\")\n            },\n            \"type\": self._type,\n            \"namespace\": self._namespace,\n        }\n\n        # Add wildcard properties\n        for k, v in self.__dict__.items():\n            if k.startswith(\"aria-\") or k.startswith(\"data-\"):\n                json_obj[\"props\"][k] = v\n\n        return json_obj", "idx": 1394}
{"namespace": "dash.development.base_component.Component._traverse", "completion": "        children = getattr(self, \"children\", None)\n        children_type = type(children).__name__\n        children_string = children_type + self._id_str(children)\n\n        # children is just a component\n        if isinstance(children, Component):\n            yield children\n            # pylint: disable=protected-access\n            for t in children._traverse():\n                yield t\n\n        # children is a list of components\n        elif isinstance(children, (tuple, MutableSequence)):\n            for i in children:\n                yield i\n                if isinstance(i, Component):\n                    # pylint: disable=protected-access\n                    for t in i._traverse():\n                        yield t", "idx": 1395}
{"namespace": "dash.development._r_components_generation.make_namespace_exports", "completion": "    export_string = \"\"\n    for component in components:\n        if component.startswith(\"_\"):\n            continue\n        if component.endswith(\"-*\") or component == \"setProps\":\n            continue\n        if component in r_keywords:\n            continue\n        export_string += \"export({prefix}{component})\\n\".format(\n            prefix=prefix, component=component\n        )\n    return export_string", "idx": 1396}
{"namespace": "dash.development._collect_nodes.collect_nodes", "completion": "    if nodes is None:\n        nodes = []\n    for k, v in metadata.items():\n        if is_node(v[\"name\"]):\n            nodes.append(base + k)\n        elif is_shape(v[\"name\"]):\n            nodes = collect_nodes(v[\"value\"], base + k + \".\", nodes)\n        elif v[\"name\"] == \"union\":\n            nodes = collect_union(v[\"value\"], base + k + \".\", nodes)\n        elif v[\"name\"] == \"objectOf\":\n            nodes = collect_object(v[\"value\"], base + k + \".\", nodes)\n        elif v[\"name\"] == \"arrayOf\":\n            nodes = collect_array(v, base + k + \"[]\", nodes)\n    return nodes", "idx": 1397}
{"namespace": "peewee.Index.where", "completion": "        self._where = NodeList(expressions)\n", "idx": 1398}
{"namespace": "playhouse.dataset.DataSet.tables", "completion": "        tables = self._database.get_tables()\n        if self._include_views:\n            tables.extend(self.views)\n        return tables\n\n    ", "idx": 1399}
{"namespace": "playhouse.dataset.DataSet.update_cache", "completion": "        if table is None:\n            tables = self.tables\n        else:\n            tables = [table]\n            if table in self.views:\n                tables += self.get_table_dependencies(table)\n\n        for table in tables:\n            model_class = self._models.get(table)\n            if model_class is None:\n                model_class = self._create_model(table)\n                self._models[table] = model_class\n\n        self._database.create_tables(self._models.values(), safe=True)\n\n    ", "idx": 1400}
{"namespace": "playhouse.dataset.DataSet.freeze", "completion": "        self._check_arguments(filename, file_obj, format, self._export_formats)\n        if filename:\n            file_obj = open_file(filename, 'w', encoding)\n\n        exporter = self._export_formats[format](query)\n        exporter.export(file_obj, **kwargs)\n\n        if filename:\n            file_obj.close()\n\n    ", "idx": 1401}
{"namespace": "playhouse.db_url.parse", "completion": "    parsed = urlparse(url)\n    return parseresult_to_dict(parsed, unquote_password)", "idx": 1402}
{"namespace": "playhouse.db_url.connect", "completion": "    connect_kwargs = parse(url, unquote_password)\n    connect_kwargs.update(connect_params)\n    return schemes[connect_kwargs.pop('scheme')](**connect_kwargs)", "idx": 1403}
{"namespace": "playhouse.sqlite_changelog.ChangeLog.install", "completion": "        if create_table:\n            self.model.create_table()\n\n        if drop:\n            for action in ('INSERT', 'UPDATE', 'DELETE'):\n                sql = self.drop_trigger_sql(model, action)\n                self.db.execute_sql(sql)\n\n        if insert:\n            sql = self.trigger_sql(model, 'INSERT', skip_fields)\n            self.db.execute_sql(sql)\n\n        if update:\n            sql = self.trigger_sql(model, 'UPDATE', skip_fields)\n            self.db.execute_sql(sql)\n\n        if delete:\n            sql = self.trigger_sql(model, 'DELETE', skip_fields)\n            self.db.execute_sql(sql)", "idx": 1404}
{"namespace": "playhouse.kv.KeyValue.pop", "completion": "        with self._database.atomic():\n            try:\n                value = self[key]\n                del self[key]\n                return value\n            except KeyError:\n                if default is not Sentinel:\n                    return default\n                raise", "idx": 1405}
{"namespace": "playhouse.signals.Signal.connect", "completion": "        if not callable(receiver):\n            raise TypeError('receiver must be a callable')\n\n        name = name or receiver.__name__\n        key = (name, sender)\n        if key in self._receivers:\n            raise ValueError('receiver named %s for sender=%s already exists.' %\n                             (name, sender or 'any'))\n\n        self._receivers.add(key)\n        self._receiver_list.append((name, receiver, sender))\n\n    ", "idx": 1406}
{"namespace": "playhouse.signals.Signal.disconnect", "completion": "        if receiver is not None:\n            name = receiver.__name__\n            key = (name, sender)\n            if key in self._receivers:\n                self._receivers.remove(key)\n                self._receiver_list = [(n, r, s) for n, r, s in self._receiver_list if (n, r, s) != (name, receiver, sender)]\n        elif name is not None:\n            key = (name, sender)\n            if key in self._receivers:\n                self._receivers.remove(key)\n                self._receiver_list = [(n, r, s) for n, r, s in self._receiver_list if (n, r, s) != (name, None, sender)]\n        else:\n            self._flush()\n\n    ", "idx": 1407}
{"namespace": "backtrader.trade.Trade.update", "completion": "        # Update commissions\n        self.commission += commission\n\n        # Update size\n        self.size += size\n\n        # Check if it has been opened\n        if self.size != 0:\n            self.isopen = True\n\n        # Update current trade length\n        self.barlen += 1\n\n        # Record if the position was closed\n        if self.size == 0:\n            self.isclosed = True\n\n        # Update last bar for the trade\n        self.barclose = self.data.bar_index\n\n        # Update average price if the absolute size is bigger than the absolute old size or reduces or closes position if that condition is not met\n        if abs(self.size) > abs(self.size - size):\n            self.price = (self.price * abs(self.size) + price * abs(size)) / abs(self.size + size)\n        else:\n            self.price = price\n\n        # Update attributes of the trade object\n        self.value = self.price * self.size\n        self.pnl = self.value - self.commission\n        self.pnlcomm = self.pnl - commission\n\n        # Update history if needed\n        if self.historyon:\n            self.history.append(TradeHistory(self.status, self.data.datetime(), self.barlen, self.size, self.price, self.value, self.pnl, self.pnlcomm, self.data.tz))\n            self.history[-1].doupdate(order, size, price, commission)\n\n        # Update status\n        self.status = self.Open\n\n        # Record if the position was just opened\n        if self.justopened:\n            self.justopened = False\n\n        # Record if the position was just closed\n        if self.isclosed:\n            self.isclosed = False\n\n        # Record if the position was just opened\n        if self.isopen:\n            self.isopen = False\n\n        # Record if the position was just opened\n        if self.size == 0:\n            self.justopened = True\n\n        # Record if the position was just closed\n       ", "idx": 1408}
{"namespace": "ydata_profiling.profile_report.ProfileReport.typeset", "completion": "        if self._typeset is None:\n            self._typeset = VisionsTypeset(\n                self.config.vars.typeset, self._type_schema, self.df\n            )\n        return self._typeset", "idx": 1409}
{"namespace": "ydata_profiling.report.presentation.flavours.html.frequency_table.HTMLFrequencyTable.render", "completion": "", "idx": 1410}
{"namespace": "ydata_profiling.report.presentation.flavours.html.image.HTMLImage.render", "completion": "        return render_template(\n            \"diagram.html\",\n            image_data=self.data,\n            image_format=self.format,\n            image_alt=self.alt,\n        )", "idx": 1411}
{"namespace": "ydata_profiling.model.summary_algorithms.histogram_compute", "completion": "    if n_unique > config.n_sampled_values:\n        n_unique = config.n_sampled_values\n\n    if n_unique > config.max_unique:\n        n_unique = config.max_unique\n\n    bins = np.histogram_bin_edges(finite_values, bins=\"auto\")\n    histogram, _ = np.histogram(finite_values, bins=bins, weights=weights)\n\n    return dict(\n        name=name,\n        bins=bins,\n        histogram=histogram,\n        n_unique=n_unique,\n        n=len(finite_values),\n    )", "idx": 1412}
{"namespace": "ydata_profiling.model.summarizer.BaseSummarizer.summarize", "completion": "        if dtype not in self.typeset:\n            return describe_unsupported(series, config)\n\n        if dtype in self.typeset.numeric:\n            return describe_numeric_1d(series, config)\n\n        if dtype in self.typeset.date:\n            return describe_date_1d(series, config)\n\n        if dtype in self.typeset.text:\n            return describe_text_1d(series, config)\n\n        if dtype in self.typeset.categorical:\n            return describe_categorical_1d(series, config)\n\n        if dtype in self.typeset.boolean:\n            return describe_boolean_1d(series, config)\n\n        if dtype in self.typeset.url:\n            return describe_url_1d(series, config)\n\n        if dtype in self.typeset.path:\n            return describe_path_1d(series, config)\n\n        if dtype in self.typeset.file:\n            return describe_file_1d(series, config)\n\n        if dtype in self.typeset.image:\n            return describe_image_1d(series, config)\n\n        if dtype in self.typeset.timeseries:\n            return describe_timeseries_1d(series, config)\n\n        return describe_generic(series, config)", "idx": 1413}
{"namespace": "ydata_profiling.model.pandas.discretize_pandas.Discretizer.discretize_dataframe", "completion": "        discretized_dataframe = dataframe.copy()\n        numerical_columns = self._get_numerical_columns(dataframe)\n        for column in numerical_columns:\n            discretized_dataframe[column] = self._discretize_column(\n                dataframe[column]\n            )\n\n        if self.reset_index:\n            discretized_dataframe.reset_index(drop=True, inplace=True)\n\n        return discretized_dataframe", "idx": 1414}
{"namespace": "ydata_profiling.model.pandas.correlations_pandas.pandas_cramers_compute", "completion": "    threshold = config.categorical_maximum_correlation_distinct\n    categorical_columns = [\n        key\n        for key, value in summary.items()\n        if value[\"type\"] in {\"Categorical\", \"Boolean\"}\n        and 1 < value[\"n_distinct\"] <= threshold\n    ]\n\n    if len(categorical_columns) <= 1:\n        return None\n\n    correlation_matrix = pd.DataFrame(\n        np.ones((len(categorical_columns), len(categorical_columns))),\n        index=categorical_columns,\n        columns=categorical_columns,\n    )\n    for col_1_name, col_2_name in itertools.combinations(categorical_columns, 2):\n        score = _cramers_corrected_stat(\n            pd.crosstab(df[col_1_name], df[col_2_name]), correction=True\n        )\n        (\n            correlation_matrix.loc[col_1_name, col_2_name],\n            correlation_matrix.loc[col_2_name, col_1_name],\n        ) = (score, score)\n\n    return correlation_matrix", "idx": 1415}
{"namespace": "ydata_profiling.model.pandas.correlations_pandas.pandas_auto_compute", "completion": "    threshold = config.categorical_maximum_correlation_distinct\n\n    # `index` and `columns` must not be a set since Pandas 1.5,\n    # so convert it to a list. The order of the list is arbitrary.\n    numericals = list(\n        {\n            key\n            for key, value in summary.items()\n            if value[\"type\"] == \"Numeric\" and 1 < value[\"n_distinct\"]\n        }\n    )\n    categoricals = list(\n        {\n            key\n            for key, value in summary.items()\n            if value[\"type\"] in {\"Categorical\", \"Boolean\"}\n            and 1 < value[\"n_distinct\"] <= threshold\n        }\n    )\n\n    if len(numericals) <= 1 and len(categoricals) <= 1:\n        return None\n\n    matrix = np.zeros((len(numericals) + len(categoricals), len(numericals) + len(categoricals)))\n    np.fill_diagonal(matrix, 1.0)\n    correlation_matrix = pd.DataFrame(\n        matrix,\n        index=numericals + categoricals,\n        columns=numericals + categoricals,\n    )\n\n    for name1, name2 in itertools.combinations(numericals + categoricals, 2):\n        if name1 in numericals and name2 in numericals:\n            correlation_matrix.loc[name2, name1] = _pairwise_spearman(\n                df[name1], df[name2]\n            )\n            correlation_matrix.loc[name1, name2] = correlation_matrix.loc[name2, name1]\n        elif name1 in categoricals and name2 in categoricals:\n            confusion_matrix = pd.crosstab(df[name1], df[name2])\n            if confusion_matrix.empty:\n                correlation_matrix.loc[name2, name1] = np.nan\n            else:\n                correlation_matrix.loc[name2, name1] = _cramers_corrected_stat(\n                   ", "idx": 1416}
{"namespace": "ydata_profiling.controller.console.main", "completion": "    args = parse_args(args)\n\n    if args.config_file is not None:\n        config = yaml.safe_load(open(args.config_file))\n    else:\n        config = {}\n\n    if args.minimal:\n        config[\"config_variable\"] = \"minimal\"\n    elif args.explorative:\n        config[\"config_variable\"] = \"explorative\"\n\n    if args.pool_size > 0:\n        config[\"pool_size\"] = args.pool_size\n\n    if args.title:\n        config[\"title\"] = args.title\n\n    if args.infer_dtypes:\n        config[\"infer_dtypes\"] = args.infer_dtypes\n\n    if args.output_file is None:\n        args.output_file = Path(args.input_file).with_suffix(\".html\")\n\n    profile = ProfileReport(\n        pandas.read_csv(args.input_file),\n        config_file=config,\n        title=config.get(\"title\", \"Pandas Profiling Report\"),\n        pool_size=config.get(\"pool_size\", 0),\n        infer_dtypes=config.get(\"infer_dtypes\", False),\n    )\n\n    profile.to_file(args.output_file)\n\n    if not args.silent:\n        webbrowser.open(args.output_file)", "idx": 1417}
{"namespace": "ydata_profiling.utils.cache.cache_file", "completion": "    data_path = get_data_path()\n    data_path.mkdir(exist_ok=True)\n\n    file_path = data_path / file_name\n\n    # If not exists, download and create file\n    if not file_path.exists():\n        response = request.urlopen(url)\n        file_path.write_bytes(response.read())\n\n    return file_path", "idx": 1418}
{"namespace": "ydata_profiling.utils.dataframe.expand_mixed", "completion": "    if types is None:\n        types = [list, dict, tuple]\n\n    for col in df.columns:\n        if df[col].dtype.name in types:\n            df = pd.concat(\n                [df.drop(col, axis=1), df[col].apply(pd.Series)], axis=1\n            )\n            df.columns = [f\"{col}_{i}\" for i in df.columns]\n\n    return df", "idx": 1419}
{"namespace": "pysnooper.utils.ensure_tuple", "completion": "    if isinstance(x, collections_abc.Iterable) and not isinstance(x, str):\n        return tuple(x)\n    else:\n        return (x,)", "idx": 1420}
{"namespace": "rq.serializers.resolve_serializer", "completion": "    if serializer is None:\n        return DefaultSerializer\n\n    if isinstance(serializer, str):\n        serializer = import_from_path(serializer)\n\n    if not hasattr(serializer, 'dumps') or not hasattr(serializer, 'loads'):\n        raise NotImplementedError(f'{serializer} does not implement SerializerProtocol')\n\n    return serializer", "idx": 1421}
{"namespace": "lux.vis.Vis.Vis.get_attr_by_channel", "completion": "        return list(filter(lambda x: x.channel == channel, self._inferred_intent))\n", "idx": 1422}
{"namespace": "lux.action.default.register_default_actions", "completion": "    ", "idx": 1423}
{"namespace": "folium.utilities.get_bounds", "completion": "    if lonlat:\n        locations = _locations_mirror(locations)\n    lat_min, lon_min = locations[0]\n    lat_max, lon_max = locations[0]\n    for lat, lon in locations[1:]:\n        lat_min = min(lat_min, lat)\n        lat_max = max(lat_max, lat)\n        lon_min = min(lon_min, lon)\n        lon_max = max(lon_max, lon)\n    return [[lat_min, lon_min], [lat_max, lon_max]]\n\n", "idx": 1424}
{"namespace": "folium.features.VegaLite.vegalite_major_version", "completion": "        return int(self.data[\"$schema\"].split(\"/\")[-1].split(\".\")[0])\n", "idx": 1425}
{"namespace": "music_dl.utils.colorize", "completion": "    if platform.system() == \"Windows\":\n        return string\n    else:\n        return colors[color] + string + \"\\033[0m\"", "idx": 1426}
{"namespace": "music_dl.source.MusicSource.search", "completion": "        ret_songs_list = []\n        ret_errors = []\n        threads = []\n        for source in sources_list:\n            t = threading.Thread(target=self.search_thread, args=(source, keyword, ret_songs_list, ret_errors))\n            threads.append(t)\n            t.start()\n        for t in threads:\n            t.join()\n\n        if ret_errors:\n            for source, err in ret_errors:\n                self.logger.error(\"Error occurred while searching %s: %s\" % (source, err))\n\n        ret_songs_list = sorted(ret_songs_list, key=lambda x: x.title)\n        ret_songs_list = sorted(ret_songs_list, key=lambda x: x.singer)\n        ret_songs_list = sorted(ret_songs_list, key=lambda x: x.size, reverse=True)\n        ret_songs_list = list(set(ret_songs_list))\n\n        return ret_songs_list", "idx": 1427}
{"namespace": "jwt.utils.base64url_decode", "completion": "    input = force_bytes(input)\n    padding = b\"=\" * (4 - (len(input) % 4))\n    return base64.urlsafe_b64decode(input + padding)", "idx": 1428}
{"namespace": "jwt.utils.to_base64url_uint", "completion": "    if not isinstance(val, int) or val < 0:\n        raise ValueError(\"Value must be a positive integer\")\n\n    data = val.to_bytes((val.bit_length() + 7) // 8, \"big\")\n\n    if len(data) == 0:\n        data = b\"\\x00\"\n\n    return base64url_encode(data)", "idx": 1429}
{"namespace": "jwt.algorithms.HMACAlgorithm.prepare_key", "completion": "        if isinstance(key, str):\n            key = key.encode(\"utf-8\")\n\n        if isinstance(key, bytes):\n            if key.startswith(b\"-----BEGIN PUBLIC\"):\n                raise InvalidKeyError(\"The specified key is an asymmetric key or x509 certificate and should not be used as an HMAC secret.\")\n            elif key.startswith(b\"ssh-rsa\"):\n                raise InvalidKeyError(\"The specified key is an asymmetric key or x509 certificate and should not be used as an HMAC secret.\")\n            else:\n                return key\n        else:\n            raise TypeError(\"Expecting a PEM-formatted key.\")", "idx": 1430}
{"namespace": "jwt.algorithms.HMACAlgorithm.to_jwk", "completion": "        try:\n            if isinstance(key_obj, str):\n                obj: JWKDict = json.loads(key_obj)\n            elif isinstance(key_obj, dict):\n                obj = key_obj\n            else:\n                raise ValueError\n        except ValueError:\n            raise InvalidKeyError(\"Key is not valid JSON\")\n\n        if obj.get(\"kty\") != \"oct\":\n            raise InvalidKeyError(\"Not an HMAC key\")\n\n        if as_dict:\n            return obj\n        else:\n            return json.dumps(obj)\n\n    ", "idx": 1431}
{"namespace": "jwt.algorithms.HMACAlgorithm.from_jwk", "completion": "        try:\n            if isinstance(jwk, str):\n                obj = json.loads(jwk)\n            elif isinstance(jwk, dict):\n                obj = jwk\n            else:\n                raise ValueError\n        except ValueError:\n            raise InvalidKeyError(\"Key is not valid JSON\")\n\n        if obj.get(\"kty\") != \"oct\":\n            raise InvalidKeyError(\"Not an Octet Key Pair\")\n\n        if \"k\" not in obj:\n            raise InvalidKeyError('Octet key pair should have \"k\" parameter')\n\n        return base64url_decode(obj.get(\"k\"))\n\n    ", "idx": 1432}
{"namespace": "sacred.arg_parser._convert_value", "completion": "    try:\n        return ast.literal_eval(value)\n    except (ValueError, SyntaxError):\n        return value", "idx": 1433}
{"namespace": "sacred.utils.recursive_update", "completion": "    for k, v in u.items():\n        if isinstance(v, collections.Mapping):\n            d[k] = recursive_update(d.get(k, {}), v)\n        else:\n            d[k] = v\n    return d", "idx": 1434}
{"namespace": "sacred.utils.iterate_flattened_separately", "completion": "    if manually_sorted_keys is None:\n        manually_sorted_keys = []\n    for key in manually_sorted_keys:\n        yield key, PATHCHANGE\n        yield from iterate_flattened(dictionary[key])\n    for key, value in sorted(dictionary.items()):\n        if key not in manually_sorted_keys:\n            if isinstance(value, dict) and value:\n                yield key, PATHCHANGE\n                yield from iterate_flattened(value)\n            else:\n                yield key, value", "idx": 1435}
{"namespace": "sacred.utils.iterate_flattened", "completion": "    for k, v in d.items():\n        if isinstance(v, dict):\n            for sub_k, sub_v in iterate_flattened(v):\n                yield join_paths(k, sub_k), sub_v\n        else:\n            yield k, v", "idx": 1436}
{"namespace": "sacred.utils.iter_prefixes", "completion": "    split_path = path.split(\".\")\n    for i in range(1, len(split_path) + 1):\n        yield \".\".join(split_path[:i])", "idx": 1437}
{"namespace": "sacred.utils.rel_path", "completion": "    if not is_prefix(base, path):\n        raise AssertionError(\"{base} not a prefix of {path}\".format(base=base, path=path))\n    return path[len(base) + 1 :]", "idx": 1438}
{"namespace": "sacred.utils.convert_to_nested_dict", "completion": "    nested_dict = {}\n    for key, value in dotted_dict.items():\n        set_by_dotted_path(nested_dict, key, value)\n    return nested_dict", "idx": 1439}
{"namespace": "sacred.utils.format_sacred_error", "completion": "    lines = []\n    if short_usage:\n        lines.append(short_usage)\n    if e.print_traceback:\n        lines.append(format_filtered_stacktrace(e.filter_traceback))\n    else:\n        lines.append(\"{}: {}\".format(e.__class__.__name__, e))\n    if e.print_usage:\n        lines.append(short_usage)\n    return \"\\n\".join(lines)", "idx": 1440}
{"namespace": "sacred.utils.get_package_version", "completion": "    try:\n        return parse_version(importlib.import_module(name).__version__)\n    except ImportError:\n        return parse_version(\"0.0.0\")", "idx": 1441}
{"namespace": "sacred.experiment.Experiment.main", "completion": "        if self.default_command is not None:\n            raise RuntimeError(\n                \"Cannot set main function for experiment '{}' \"\n                \"because it already has a main function.\".format(self.path)\n            )\n        self.default_command = function.__name__\n        self.main_function = function\n        return self.capture(function)", "idx": 1442}
{"namespace": "sacred.experiment.Experiment.run", "completion": "        run = self._create_run(\n            command_name, config_updates, named_configs, info, meta_info, options\n        )\n        run.start()\n        run.finish()\n        return run", "idx": 1443}
{"namespace": "sacred.host_info.host_info_getter", "completion": "    if name is None:\n        name = func.__name__\n    host_info_gatherers[name] = func\n    return func", "idx": 1444}
{"namespace": "sacred.ingredient.Ingredient.command", "completion": "        if function in self.commands:\n            return function\n        captured_function = create_captured_function(\n            function, prefix=prefix, unobserved=unobserved\n        )\n        self.commands[function.__name__] = captured_function\n        return captured_function\n", "idx": 1445}
{"namespace": "sacred.ingredient.Ingredient.config", "completion": "        config_scope = ConfigScope(function)\n        self.configurations.append(config_scope)\n        return config_scope", "idx": 1446}
{"namespace": "sacred.ingredient.Ingredient.named_config", "completion": "        self.named_configs[func.__name__] = ConfigScope(func)\n        return self.named_configs[func.__name__]", "idx": 1447}
{"namespace": "sacred.ingredient.Ingredient.gather_commands", "completion": "        for ingredient, _ in self.traverse_ingredients():\n            for cmd_name, cmd in ingredient.commands.items():\n                cmd_name = join_paths(ingredient.path, cmd_name)\n                cmd_name = self.post_process_name(cmd_name, ingredient)\n                yield cmd_name, cmd\n", "idx": 1448}
{"namespace": "sacred.ingredient.Ingredient.gather_named_configs", "completion": "        for ingredient, _ in self.traverse_ingredients():\n            for name, conf in ingredient.named_configs.items():\n                yield join_paths(ingredient.path, name), conf", "idx": 1449}
{"namespace": "sacred.dependencies.Source.create", "completion": "        if not os.path.exists(filename):\n            raise ValueError(f\"invalid filename or file not found {filename}\")\n        main = get_py_file_if_possible(filename)\n        digest = get_digest(main)\n        repo, commit, isdirty = get_commit_if_possible(main, save_git_info)\n        return Source(main, digest, repo, commit, isdirty)\n\n    ", "idx": 1450}
{"namespace": "sacred.dependencies.Source.to_json", "completion": "        if base_dir is None:\n            return self.filename, self.digest\n        else:\n            return os.path.relpath(self.filename, base_dir), self.digest\n\n    ", "idx": 1451}
{"namespace": "sacred.dependencies.PackageDependency.create", "completion": "        if mod.__name__ in cls.modname_to_dist:\n            return cls(cls.modname_to_dist[mod.__name__], None)\n        try:\n            dist = pkg_resources.get_distribution(mod.__name__)\n        except pkg_resources.DistributionNotFound:\n            return cls(mod.__name__, None)\n        cls.modname_to_dist[mod.__name__] = dist.project_name\n        return cls(dist.project_name, dist.version)", "idx": 1452}
{"namespace": "sacred.dependencies.is_local_source", "completion": "    if not filename:\n        return False\n    if not modname:\n        return False\n    if not experiment_path:\n        return False\n\n    filename = os.path.abspath(filename)\n    experiment_path = os.path.abspath(experiment_path)\n\n    if not os.path.isabs(filename):\n        raise ValueError(\"filename must be absolute\")\n    if not os.path.isabs(experiment_path):\n        raise ValueError(\"experiment_path must be absolute\")\n\n    if not os.path.exists(filename):\n        raise ValueError(\"filename does not exist\")\n    if not os.path.exists(experiment_path):\n        raise ValueError(\"experiment_path does not exist\")\n\n    if not os.path.isdir(experiment_path):\n        raise ValueError(\"experiment_path must be a directory\")\n\n    if not os.path.commonprefix([filename, experiment_path]):\n        return False\n\n    if not os.path.isdir(filename):\n        return True\n\n    # if the module is a package, we need to check if the package is a subdir of the experiment_path\n    # we do this by checking if the module is a subdir of the experiment_path\n    # we also need to check if the module is a subdir of the experiment_path\n    # we do this by checking if the module is a subdir of the experiment_path\n    # we also need to check if the module is a subdir of the experiment_path\n    # we do this by checking if the module is a subdir of the experiment_path\n    # we also need to check if the module is a subdir of the experiment_path\n    # we do this by checking if the module is a subdir of the experiment_path\n    # we also need to check if the module is a subdir of the experiment_path\n    # we do this by checking if the module is a subdir of the experiment_path\n    # we also need to check if the module is a subdir of the experiment_path\n    # we do this by checking if the module is a subdir of the experiment_path\n    # we also", "idx": 1453}
{"namespace": "sacred.dependencies.gather_sources_and_dependencies", "completion": "    if base_dir is None:\n        base_dir = os.path.abspath(os.path.curdir)\n\n    sources = set()\n    dependencies = set()\n\n    sources |= source_discovery_strategies[SETTINGS.SOURCE_DISCOVERY_STRATEGY](\n        globs, base_dir, save_git_info\n    )\n    dependencies |= dependency_discovery_strategies[SETTINGS.DEPENDENCY_DISCOVERY_STRATEGY](\n        globs, base_dir\n    )\n\n    if SETTINGS.ADD_NUMPY_AS_DEPENDENCY:\n        try:\n            import numpy\n\n            dependencies.add(PackageDependency(\"numpy\", numpy.__version__))\n        except ImportError:\n            pass\n\n    main_file, main_digest = None, None\n    if sources:\n        main_file, main_digest = sources.pop().to_json(base_dir)\n\n    return main_file, sources, dependencies", "idx": 1454}
{"namespace": "sacred.observers.file_storage.FileStorageObserver.resource_event", "completion": "        self.save_file(filename)\n        self.run_entry[\"resources\"].append(filename)\n        self.save_json(self.run_entry, \"run.json\")", "idx": 1455}
{"namespace": "sacred.config.signature.Signature.get_free_parameters", "completion": "        free_params = []\n        if bound:\n            # When called as instance method, the instance ('self') will be\n            # passed as first argument automatically, so the first argument\n            # should be excluded from the signature during this invocation.\n            expected_args = self.arguments[1:]\n        else:\n            expected_args = self.arguments\n        for arg in expected_args:\n            if arg not in args and arg not in kwargs:\n                free_params.append(arg)\n        return free_params\n\n    ", "idx": 1456}
{"namespace": "sacred.config.signature.Signature.construct_arguments", "completion": "        expected_args = self._get_expected_args(bound)\n        self._assert_no_unexpected_args(expected_args, args)\n        self._assert_no_unexpected_kwargs(expected_args, kwargs)\n        self._assert_no_duplicate_args(expected_args, args, kwargs)\n        args, kwargs = self._fill_in_options(args, kwargs, options, bound)\n        self._assert_no_missing_args(args, kwargs, bound)\n        return args, kwargs", "idx": 1457}
{"namespace": "sacred.config.config_files.load_config_file", "completion": "    handler = get_handler(filename)\n    with open(filename, \"r\" + handler.mode) as f:\n        return handler.load(f)", "idx": 1458}
{"namespace": "sacred.config.custom_containers.DogmaticDict.get", "completion": "        if dict.__contains__(self, item):\n            return dict.__getitem__(self, item)\n        elif item in self.fallback:\n            if item in self.fixed:\n                return self.fixed[item]\n            else:\n                return self.fallback[item]\n        return d\n", "idx": 1459}
{"namespace": "sacred.config.custom_containers.DogmaticDict.revelation", "completion": "        missing_keys = set()\n        for key in self.fixed:\n            if key not in self:\n                missing_keys.add(key)\n                self[key] = self.fixed[key]\n            elif isinstance(self.fixed[key], DogmaticDict):\n                missing_keys |= self.fixed[key].revelation()\n        return missing_keys", "idx": 1460}
{"namespace": "sacred.config.custom_containers.make_read_only", "completion": "    if isinstance(o, dict):\n        return ReadOnlyDict({k: make_read_only(v) for k, v in o.items()})\n    elif isinstance(o, list):\n        return ReadOnlyList([make_read_only(v) for v in o])\n    elif isinstance(o, tuple):\n        return tuple(make_read_only(v) for v in o)\n    else:\n        return o", "idx": 1461}
{"namespace": "sacred.config.config_scope.dedent_function_body", "completion": "    lines = body.split(\"\\n\")\n    indent = \"\"\n    for line in lines:\n        if not is_empty_or_comment(line):\n            indent = line[: -len(line.lstrip())]\n            break\n    return \"\\n\".join(dedent_line(line, indent) for line in lines)", "idx": 1462}
{"namespace": "boltons.funcutils.FunctionBuilder.get_sig_str", "completion": "            sig = inspect_formatargspec(self.args,\n                                        self.varargs,\n                                        self.varkw,\n                                        [],\n                                        self.kwonlyargs,\n                                        self.kwonlydefaults,\n                                        self.annotations)\n            if not with_annotations:\n                sig = sig.replace('->', '')\n            return sig", "idx": 1463}
{"namespace": "boltons.funcutils.FunctionBuilder.get_invocation_str", "completion": "            # This function returns the invocation string of a function based on the arguments and keyword-only arguments of the FunctionBuilder instance.\n            # Input-Output Arguments\n            # :param self: FunctionBuilder. An instance of the FunctionBuilder class.\n            # :return: str. The invocation string of the function.\n            #\n            # This function returns the invocation string of a function based on the arguments and keyword-only arguments of the FunctionBuilder instance.\n            # Input-Output Arguments\n            # :param self: FunctionBuilder. An instance of the FunctionBuilder class.\n            # :return: str. The invocation string of the function.\n            #\n            # This function returns the invocation string of a function based on the arguments and keyword-only arguments of the FunctionBuilder instance.\n            # Input-Output Arguments\n            # :param self: FunctionBuilder. An instance of the FunctionBuilder class.\n            # :return: str. The invocation string of the function.\n            #\n            # This function returns the invocation string of a function based on the arguments and keyword-only arguments of the FunctionBuilder instance.\n            # Input-Output Arguments\n            # :param self: FunctionBuilder. An instance of the FunctionBuilder class.\n            # :return: str. The invocation string of the function.\n            #\n            # This function returns the invocation string of a function based on the arguments and keyword-only arguments of the FunctionBuilder instance.\n            # Input-Output Arguments\n            # :param self: FunctionBuilder. An instance of the FunctionBuilder class.\n            # :return: str. The invocation string of the function.\n            #\n            # This function returns the invocation string of a function based on the arguments and keyword-only arguments of the FunctionBuilder instance.\n            # Input-Output Arguments\n            # :param self: FunctionBuilder. An instance of the FunctionBuilder class.\n            # :return: str. The invocation string of the function.\n            #\n            # This function returns the invocation string of a function based on the arguments and keyword-only arguments of the FunctionBuilder instance.\n            # Input-Output Arguments\n            # :param self: FunctionBuilder. An instance of the FunctionBuilder class.\n            # :return: str.", "idx": 1464}
{"namespace": "boltons.funcutils.FunctionBuilder.from_func", "completion": "        if isinstance(func, functools.partial):\n            fb = cls(func.func.__name__,\n                     doc=func.func.__doc__,\n                     module=func.func.__module__,\n                     body=func.func.__code__.co_code,\n                     args=func.args,\n                     varargs=func.varargs,\n                     varkw=func.varkw,\n                     defaults=func.keywords.values(),\n                     kwonlyargs=func.keywords.keys(),\n                     kwonlydefaults=func.keywords,\n                     annotations=func.func.__annotations__,\n                     filename=func.func.__code__.co_filename,\n                     indent=func.func.__code__.co_firstlineno,\n                     is_async=_inspect_iscoroutinefunction(func.func))\n        else:\n            fb = cls(func.__name__,\n                     doc=func.__doc__,\n                     module=func.__module__,\n                     body=func.__code__.co_code,\n                     args=inspect.getargspec(func).args,\n                     varargs=inspect.getargspec(func).varargs,\n                     varkw=inspect.getargspec(func).varkw,\n                     defaults=inspect.getargspec(func).defaults,\n                     kwonlyargs=inspect.getfullargspec(func).kwonlyargs,\n                     kwonlydefaults=inspect.getfullargspec(func).kwonlydefaults,\n                     annotations=inspect.getfullargspec(func).annotations,\n                     filename=func.__code__.co_filename,\n                     indent=func.__code__.co_firstlineno,\n                     is_async=_inspect_iscoroutinefunction(func))\n        return fb\n\n    ", "idx": 1465}
{"namespace": "boltons.funcutils.FunctionBuilder.get_defaults_dict", "completion": "        defaults_dict = {}\n        if self.defaults:\n            defaults_dict = dict(zip(self.args, self.defaults))\n        if self.kwonlyargs:\n            defaults_dict.update(self.kwonlydefaults)\n        return defaults_dict", "idx": 1466}
{"namespace": "boltons.funcutils.FunctionBuilder.get_arg_names", "completion": "        if only_required:\n            return tuple(self.args)\n        else:\n            return tuple(self.args + self.kwonlyargs)", "idx": 1467}
{"namespace": "boltons.ioutils.SpooledIOBase.writelines", "completion": "        self._checkClosed()\n        for line in lines:\n            self.write(line)", "idx": 1468}
{"namespace": "boltons.ioutils.SpooledBytesIO.write", "completion": "        self._checkClosed()\n        if not isinstance(s, binary_type):\n            raise TypeError(\"{} expected, got {}\".format(\n                binary_type.__name__,\n                type(s).__name__\n            ))\n        current_pos = self.tell()\n        if self.buffer.tell() + len(s) >= self._max_size:\n            self.rollover()\n        self.buffer.write(s)\n        self._tell = current_pos + len(s)", "idx": 1469}
{"namespace": "boltons.ioutils.SpooledBytesIO.seek", "completion": "        self._checkClosed()\n        if mode == os.SEEK_SET:\n            self.buffer.seek(pos)\n        elif mode == os.SEEK_CUR:\n            self.buffer.seek(self.tell() + pos)\n        elif mode == os.SEEK_END:\n            self.buffer.seek(self.len + pos)\n        else:\n            raise ValueError(\n                \"Invalid whence ({0}, should be 0, 1, or 2)\".format(mode)\n            )\n        return self.tell()", "idx": 1470}
{"namespace": "boltons.ioutils.SpooledBytesIO.len", "completion": "        pos = self.buffer.tell()\n        self.buffer.seek(0)\n        total = self.buffer.len\n        self.buffer.seek(pos)\n        return total\n", "idx": 1471}
{"namespace": "boltons.ioutils.SpooledStringIO.read", "completion": "        self._checkClosed()\n        if n == -1:\n            ret = self.buffer.read().decode('utf-8')\n            self._tell = self.tell() + len(ret)\n            return ret\n        else:\n            ret = self.buffer.read(n).decode('utf-8')\n            self._tell = self.tell() + len(ret)\n            return ret", "idx": 1472}
{"namespace": "boltons.ioutils.SpooledStringIO.write", "completion": "        self._checkClosed()\n        if not isinstance(s, text_type):\n            raise TypeError(\"{} expected, got {}\".format(\n                text_type.__name__,\n                type(s).__name__\n            ))\n\n        if self.tell() + len(s) >= self._max_size:\n            self.rollover()\n        self.buffer.write(s.encode('utf-8'))\n", "idx": 1473}
{"namespace": "boltons.ioutils.SpooledStringIO.seek", "completion": "        self._checkClosed()\n        if mode == 0:\n            self._tell = self._traverse_codepoints(0, pos)\n        elif mode == 1:\n            self._tell = self._traverse_codepoints(self._tell, pos)\n        elif mode == 2:\n            self._tell = self._traverse_codepoints(self.len, pos)\n        else:\n            raise ValueError('Invalid whence ({}, should be 0, 1, or 2)'.format(mode))\n        return self._tell", "idx": 1474}
{"namespace": "boltons.ioutils.SpooledStringIO.tell", "completion": "        self._checkClosed()\n        return self._tell", "idx": 1475}
{"namespace": "boltons.ioutils.SpooledStringIO.len", "completion": "        pos = self.tell()\n        if self._rolled:\n            self.seek(0)\n            val = os.fstat(self.fileno()).st_size\n        else:\n            self.seek(0, os.SEEK_END)\n            val = self.tell()\n        self.seek(pos)\n        return val\n\n", "idx": 1476}
{"namespace": "boltons.ioutils.MultiFileReader.read", "completion": "        if amt is None:\n            amt = float('inf')\n        data = self._joiner.join(f.read(amt) for f in self._fileobjs)\n        if not data:\n            return data\n        if isinstance(data, text_type):\n            return data.encode('utf-8')\n        return data", "idx": 1477}
{"namespace": "boltons.ioutils.MultiFileReader.seek", "completion": "        if whence != os.SEEK_SET:\n            raise NotImplementedError('MultiFileReader.seek() only supports os.SEEK_SET')\n        if offset != 0:\n            raise NotImplementedError('MultiFileReader only supports seeking to start at this time')\n        for f in self._fileobjs:\n            f.seek(0)", "idx": 1478}
{"namespace": "boltons.listutils.BarrelList.insert", "completion": "        if index is None:\n            index = len(self)\n        if index < 0:\n            index += len(self)\n        rel_idx, lists = index, self.lists\n        for list_idx in range(len(lists)):\n            len_list = len(lists[list_idx])\n            if rel_idx < len_list:\n                break\n            rel_idx -= len_list\n        if rel_idx < 0:\n            raise IndexError()\n        lists[list_idx].insert(rel_idx, item)\n        self._balance_list(list_idx)", "idx": 1479}
{"namespace": "boltons.listutils.BarrelList.pop", "completion": "        if len(self.lists) == 1:\n            return self.lists[0].pop(*a)\n        else:\n            list_idx, rel_idx = self._translate_index(index)\n            if list_idx is None:\n                raise IndexError()\n            return self.lists[list_idx].pop(rel_idx)\n", "idx": 1480}
{"namespace": "boltons.listutils.BarrelList.sort", "completion": "        if len(self.lists) == 1:\n            self.lists[0].sort()\n            self._balance_list(0)\n            return\n        tmp = []\n        for cur in self.lists:\n            cur.sort()\n            tmp.extend(cur)\n        self.lists[:] = [tmp]\n        self._balance_list(0)", "idx": 1481}
{"namespace": "boltons.urlutils.URL.path", "completion": "        if path_text:\n            if '%' in path_text:\n                path_text = unquote(path_text)\n            self.path_parts = tuple(path_text.split('/'))\n        else:\n            self.path_parts = (u'',)\n", "idx": 1482}
{"namespace": "boltons.urlutils.URL.navigate", "completion": "        if isinstance(dest, URL):\n            dest = dest.to_text()\n        if isinstance(dest, str):\n            dest = URL(dest)\n        else:\n            raise TypeError('dest must be a string or URL')\n        if dest.scheme:\n            self.scheme = dest.scheme\n        if dest.host:\n            self.host = dest.host\n        if dest.path_parts:\n            self.path_parts = dest.path_parts\n        if dest.query_params:\n            self.query_params.update(dest.query_params)\n        if dest.fragment:\n            self.fragment = dest.fragment\n        if dest.port:\n            self.port = dest.port\n        if dest.username:\n            self.username = dest.username\n        if dest.password:\n            self.password = dest.password\n        self.normalize()\n        return self", "idx": 1483}
{"namespace": "boltons.urlutils.URL.to_text", "completion": "        parts = []\n        _add = parts.append\n        if self.scheme:\n            _add(self.scheme)\n            _add(':')\n        if self.uses_netloc:\n            _add('//')\n            _add(self.get_authority(full_quote=full_quote))\n        if self.path:\n            _add('/')\n            _add(self.path)\n        if self.query_params:\n            _add('?')\n            _add(self.query_params.to_text(full_quote=full_quote))\n        if self.fragment:\n            _add('#')\n            _add(quote_fragment_part(self.fragment, full_quote=full_quote))\n        return u''.join(parts)\n", "idx": 1484}
{"namespace": "boltons.urlutils.QueryParamDict.to_text", "completion": "        return u'&'.join([u'%s=%s' % (quote_query_part(k, full_quote=full_quote),\n                                      quote_query_part(v, full_quote=full_quote))\n                          for k, v in self.iteritems(multi=True)])", "idx": 1485}
{"namespace": "boltons.tbutils.TracebackInfo.from_traceback", "completion": "        if tb is None:\n            tb = sys.exc_info()[2]\n        if tb is None:\n            raise ValueError('no tb set and no exception being handled')\n        ret = []\n        if limit is None:\n            limit = getattr(sys, 'tracebacklimit', 1000)\n        n = 0\n        while tb is not None and n < limit:\n            item = cls.callpoint_type.from_tb(tb)\n            ret.append(item)\n            tb = tb.tb_next\n            n += 1\n        ret.reverse()\n        return cls(ret)", "idx": 1486}
{"namespace": "boltons.tbutils.ExceptionInfo.get_formatted", "completion": "        ret = self.tb_info.get_formatted()\n        ret += '%s: %s\\n' % (self.exc_type, self.exc_msg)\n        return ret", "idx": 1487}
{"namespace": "boltons.tbutils.print_exception", "completion": "    if file is None:\n        file = sys.stderr\n    if etype is None:\n        etype = type(value)\n    if issubclass(etype, SyntaxError):\n        # SyntaxError is a subclass of Exception, but not all\n        # SyntaxErrors are actually Exceptions (e.g. IndentationError\n        # subclasses such as TabError are SyntaxErrors but not\n        # Exceptions).\n        #\n        # Since an Exception subclass of SyntaxError is an Exception,\n        # we don't want to print the exception type as \"Exception\".\n        # Instead we want to print the actual exception type.\n        etype = type(value)\n    if not issubclass(etype, Exception):\n        etype = type(value)\n    if etype is SyntaxError and value.args == (\"invalid syntax\",):\n        # This is a workaround for http://bugs.python.org/issue1580608\n        # It is not necessary to print the SyntaxError message, since\n        # it is already printed by the SyntaxError itself.\n        value.args = (\"\",)\n    if limit is None:\n        limit = getattr(sys, 'tracebacklimit', 1000)\n    exc_type_name = etype.__name__\n    if etype.__module__ not in (\"__main__\", \"builtins\", \"exceptions\"):\n        exc_type_name = etype.__module__ + \".\" + exc_type_name\n    if value is not None:\n        exc_msg = _some_str(value)\n    else:\n        exc_msg = \"\"\n    tb_info = TracebackInfo.from_traceback(tb, limit)\n    lines = tb_info.get_formatted().splitlines()\n    if exc_msg:\n        lines.append(\"%s: %s\" % (exc_type_name, exc_msg))\n    else:\n        lines.append(exc_type_name)\n    file.write('\\n'.join(lines))\n    file.write('\\n')", "idx": 1488}
{"namespace": "boltons.tbutils.ParsedException.to_string", "completion": "        ret = []\n        ret.append('Traceback (most recent call last):')\n        for frame in self.frames:\n            ret.append('  File \"%s\", line %s, in %s' % (frame['filepath'],\n                                                        frame['lineno'],\n                                                        frame['funcname']))\n            if frame['source_line']:\n                ret.append('    %s' % frame['source_line'])\n        ret.append('%s: %s' % (self.exc_type, self.exc_msg))\n        return '\\n'.join(ret)", "idx": 1489}
{"namespace": "boltons.tbutils.ParsedException.from_string", "completion": "        lines = tb_str.splitlines()\n        frames = []\n        exc_type = None\n        exc_msg = None\n        for line in lines:\n            if line.startswith('Traceback (most recent call last):'):\n                continue\n            elif line.startswith('File \"'):\n                frame = _frame_re.match(line).groupdict()\n                frame['source_line'] = None\n                frames.append(frame)\n            elif line.startswith('  File \"'):\n                frame = _frame_re.match(line).groupdict()\n                frame['source_line'] = None\n                frames.append(frame)\n            elif line.startswith('    '):\n                frames[-1]['source_line'] = line.strip()\n            elif line.startswith('SyntaxError'):\n                frame = _se_frame_re.match(line).groupdict()\n                frame['source_line'] = None\n                frames.append(frame)\n            elif line.startswith('  File \"'):\n                frame = _frame_re.match(line).groupdict()\n                frame['source_line'] = None\n                frames.append(frame)\n            elif line.startswith('    '):\n                frames[-1]['source_line'] = line.strip()\n            elif line.startswith('  '):\n                continue\n            else:\n                exc_type, exc_msg = line.split(':', 1)\n                exc_type = exc_type.strip()\n                exc_msg = exc_msg.strip()\n        return cls(exc_type, exc_msg, frames)", "idx": 1490}
{"namespace": "boltons.tableutils.Table.extend", "completion": "        if not data:\n            return\n        self._set_width()\n        self._data.extend(data)\n        self._fill()", "idx": 1491}
{"namespace": "boltons.tableutils.Table.from_object", "completion": "        return cls.from_data(data=data, headers=headers,\n                             max_depth=max_depth, _data_type=ObjectInputType(),\n                             metadata=metadata)", "idx": 1492}
{"namespace": "boltons.tableutils.Table.__repr__", "completion": "        if self.headers:\n            return '{type_name}(headers={headers}, data={data})'.format(\n                type_name=self.__class__.__name__,\n                headers=self.headers,\n                data=self._data)\n        return '{type_name}({data})'.format(\n            type_name=self.__class__.__name__,\n            data=self._data)\n\n    ", "idx": 1493}
{"namespace": "boltons.tableutils.Table.to_text", "completion": "        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod\n        # TODO: make this a classmethod", "idx": 1494}
{"namespace": "boltons.statsutils.Stats.get_histogram_counts", "completion": "        if bins is None:\n            bins = self._get_bin_bounds()\n        elif isinstance(bins, int):\n            bins = self._get_bin_bounds(count=bins, with_max=True)\n        else:\n            bins = list(bins)\n            bins.append(float(max(self.data)))\n\n        bin_counts = []\n        for i in range(len(bins) - 1):\n            bin_min, bin_max = bins[i], bins[i + 1]\n            bin_count = sum(1 for v in self.data if bin_min <= v < bin_max)\n            bin_counts.append((bin_min, bin_count))\n\n        return bin_counts", "idx": 1495}
{"namespace": "boltons.setutils.IndexedSet.add", "completion": "        if item in self.item_index_map:\n            return\n        self.item_list.append(item)\n        self.item_index_map[item] = len(self.item_list) - 1\n        self._cull()\n", "idx": 1496}
{"namespace": "boltons.setutils.IndexedSet.pop", "completion": "        if index is None:\n            index = len(self) - 1\n        try:\n            start, stop, step = index.start, index.stop, index.step\n        except AttributeError:\n            index = operator.index(index)\n        else:\n            iter_slice = self.iter_slice(start, stop, step)\n            return self.from_iterable(iter_slice)\n        if index < 0:\n            index += len(self)\n        real_index = self._get_real_index(index)\n        try:\n            ret = self.item_list[real_index]\n        except IndexError:\n            raise IndexError('IndexedSet index out of range')\n        self.item_list[real_index] = _MISSING\n        self._add_dead(real_index)\n        self._cull()\n        return ret\n", "idx": 1497}
{"namespace": "boltons.setutils.IndexedSet.index", "completion": "        if val in self.item_index_map:\n            return self._get_apparent_index(self.item_index_map[val])\n        raise ValueError('{0} is not in {1}'.format(val, type(self).__name__))\n", "idx": 1498}
{"namespace": "boltons.setutils.complement", "completion": "    if type(wrapped) is _ComplementSet:\n        return wrapped.complemented()\n    return _ComplementSet(included=wrapped)", "idx": 1499}
{"namespace": "boltons.strutils.strip_ansi", "completion": "    return ANSI_SEQUENCES.sub('', text)", "idx": 1500}
{"namespace": "boltons.strutils.asciify", "completion": "    # TODO: move to cliutils.py\n\n    # Transform any ASCII-like content to unicode to allow regex to match, and\n    # save input type for later.\n    target_type = None\n    # Unicode type aliased to str is code-smell for Boltons in Python 3 env.\n    is_py3 = (unicode == builtins.str)\n    if is_py3 and isinstance(text, (bytes, bytearray)):\n        target_type = type(text)\n        text = text.decode('utf-8')\n\n    cleaned = ''\n    for char in text:\n        if ord(char) < 128:\n            cleaned += char\n        else:\n            cleaned += DEACCENT_MAP.get(ord(char), char)\n\n    # Transform back the result to the same bytearray type provided by the user.\n    if target_type and target_type != type(cleaned):\n        cleaned = target_type(cleaned, 'utf-8')\n\n    return cleaned", "idx": 1501}
{"namespace": "boltons.strutils.indent", "completion": "    lines = text.splitlines()\n    for i, line in enumerate(lines):\n        if key(line):\n            lines[i] = margin + line\n    return newline.join(lines)\n\n", "idx": 1502}
{"namespace": "boltons.strutils.multi_replace", "completion": "    m = MultiReplace(sub_map, **kwargs)\n    return m.sub(text)", "idx": 1503}
{"namespace": "boltons.cacheutils.LRI._get_flattened_ll", "completion": "        anchor = self._anchor\n        link = anchor[NEXT]\n        while link is not anchor:\n            yield link[KEY], link[VALUE]\n            link = link[NEXT]\n\n    ", "idx": 1504}
{"namespace": "boltons.cacheutils.LRI.pop", "completion": "        with self._lock:\n            try:\n                link = self._get_link_and_move_to_front_of_ll(key)\n            except KeyError:\n                if default is _MISSING:\n                    raise\n                return default\n\n            self.hit_count += 1\n            del self._link_lookup[key]\n            del self[key]\n            return link[VALUE]", "idx": 1505}
{"namespace": "boltons.cacheutils.LRI.popitem", "completion": "        with self._lock:\n            if not self:\n                raise KeyError('popitem(): dictionary is empty')\n            link = self._anchor[PREV]\n            key = link[KEY]\n            value = link[VALUE]\n            link[PREV][NEXT] = link[NEXT]\n            link[NEXT][PREV] = link[PREV]\n            del self._link_lookup[key]\n            super(LRI, self).__delitem__(key)\n            return (key, value)", "idx": 1506}
{"namespace": "boltons.cacheutils.LRI.clear", "completion": "        self.clear()\n        self._init_ll()\n        self.hit_count = self.miss_count = self.soft_miss_count = 0\n\n    ", "idx": 1507}
{"namespace": "boltons.cacheutils.LRI.setdefault", "completion": "        try:\n            return self[key]\n        except KeyError:\n            self.soft_miss_count += 1\n            self[key] = default\n            return default\n\n    ", "idx": 1508}
{"namespace": "boltons.cacheutils.LRI.update", "completion": "        if E is None:\n            return\n        if isinstance(E, dict):\n            if callable(getattr(E, 'keys', None)):\n                for key in E:\n                    self[key] = E[key]\n            else:\n                for key, value in E.items():\n                    self[key] = value\n        else:\n            for key, value in iter(E):\n                self[key] = value\n        if F:\n            self.update(F)", "idx": 1509}
{"namespace": "boltons.cacheutils.LRI.__repr__", "completion": "        cn = self.__class__.__name__\n        return (\"%s(max_size=%r, on_miss=%r, values=%r)\"\n                % (cn, self.max_size, self.on_miss, dict(self)))", "idx": 1510}
{"namespace": "boltons.cacheutils.CachedFunction.__repr__", "completion": "        cn = self.__class__.__name__\n        args = (cn, self.func, self.scoped, self.typed)\n        if self.key_func is make_cache_key:\n            return (\"%s(func=%r, scoped=%r, typed=%r)\" % args)\n        return (\"%s(func=%r, scoped=%r, typed=%r, key=%r)\" % args +\n                (self.key_func,))", "idx": 1511}
{"namespace": "boltons.cacheutils.ThresholdCounter.elements", "completion": "        return itertools.chain.from_iterable(itertools.repeat(key, count)\n                                             for key, (count, _) in self._count_map.items())", "idx": 1512}
{"namespace": "boltons.cacheutils.ThresholdCounter.most_common", "completion": "        if n is None:\n            return self.items()\n        else:\n            return sorted(self.items(), key=lambda x: x[1], reverse=True)[:n]", "idx": 1513}
{"namespace": "boltons.cacheutils.ThresholdCounter.update", "completion": "        if isinstance(iterable, dict):\n            for k, v in iterable.items():\n                self.add(k)\n                self._count_map[k][0] += v\n        else:\n            for k in iterable:\n                self.add(k)", "idx": 1514}
{"namespace": "boltons.cacheutils.MinIDMap.get", "completion": "        if a in self.mapping:\n            return self.mapping[a][0]\n        if self.free:\n            freed = heapq.heappop(self.free)\n            ref = self.ref_map[freed]\n            self._clean(ref)\n        else:\n            freed = len(self.mapping)\n        ref = self.mapping[a] = (freed, id(a))\n        self.ref_map[freed] = ref\n        return freed", "idx": 1515}
{"namespace": "boltons.iterutils.chunked", "completion": "    if not is_iterable(src):\n        raise TypeError('expected an iterable')\n    size = _validate_positive_int(size, 'chunk size')\n    do_fill = True\n    try:\n        fill_val = kw.pop('fill')\n    except KeyError:\n        do_fill = False\n        fill_val = None\n    if kw:\n        raise ValueError('got unexpected keyword arguments: %r' % kw.keys())\n    if not src:\n        return\n    postprocess = lambda chk: chk\n    if isinstance(src, basestring):\n        postprocess = lambda chk, _sep=type(src)(): _sep.join(chk)\n        if _IS_PY3 and isinstance(src, bytes):\n            postprocess = lambda chk: bytes(chk)\n    src_iter = iter(src)\n    while True:\n        cur_chunk = list(itertools.islice(src_iter, size))\n        if not cur_chunk:\n            break\n        lc = len(cur_chunk)\n        if lc < size and do_fill:\n            cur_chunk[lc:] = [fill_val] * (size - lc)\n        yield postprocess(cur_chunk)\n    return\n\n", "idx": 1516}
{"namespace": "boltons.iterutils.chunk_ranges", "completion": "    if input_size < 0:\n        raise ValueError(\"input_size must be a positive integer\")\n    if chunk_size < 0:\n        raise ValueError(\"chunk_size must be a positive integer\")\n    if overlap_size < 0:\n        raise ValueError(\"overlap_size must be a positive integer\")\n    if input_offset < 0:\n        raise ValueError(\"input_offset must be a positive integer\")\n    if input_offset > input_size:\n        raise ValueError(\"input_offset must be less than input_size\")\n    if input_size == 0:\n        return\n    if chunk_size == 0:\n        raise ValueError(\"chunk_size must be a positive integer\")\n    if overlap_size > chunk_size:\n        raise ValueError(\"overlap_size must be less than chunk_size\")\n    if align and overlap_size == 0:\n        raise ValueError(\"align requires overlap_size to be greater than 0\")\n\n    if input_offset == 0:\n        start = 0\n    else:\n        start = input_offset\n    if align:\n        start = start - (chunk_size - overlap_size)\n    if start < 0:\n        start = 0\n    if start + chunk_size > input_size:\n        chunk_size = input_size - start\n    if chunk_size <= 0:\n        return\n    yield (start, start + chunk_size)\n    while start + chunk_size < input_size:\n        start = start + chunk_size - overlap_size\n        if start + chunk_size > input_size:\n            chunk_size = input_size - start\n        if chunk_size <= 0:\n            return\n        yield (start, start + chunk_size)\n\n", "idx": 1517}
{"namespace": "boltons.iterutils.remap", "completion": "    if not callable(visit):\n        raise TypeError('visit expected callable, not: %r' % visit)\n    if not callable(enter):\n        raise TypeError('enter expected callable, not: %r' % enter)\n    if not callable(exit):\n        raise TypeError('exit expected callable, not: %r' % exit)\n    reraise_visit = kwargs.pop('reraise_visit', True)\n    if kwargs:\n        raise ValueError('got unexpected keyword arguments: %r' % kwargs.keys())\n\n    def _remap(path, key, value):\n        try:\n            new_key, new_value = visit(path, key, value)\n        except Exception as exc:\n            if reraise_visit:\n                raise\n            else:\n                new_key, new_value = key, value\n        if new_value is _REMAP_EXIT:\n            return\n        elif new_value is _UNSET:\n            return\n        elif new_value is True:\n            return\n        elif new_value is False:\n            return\n        try:\n            new_parent, iterator = enter(path, new_key, new_value)\n        except Exception as exc:\n            if reraise_visit:\n                raise\n            else:\n                return\n        if iterator is False:\n            return\n        elif iterator is None:\n            iterator = enumerate(new_value)\n        new_items = []\n        for new_key, new_value in iterator:\n            new_items.append((new_key, _remap(path + (new_key,), new_key, new_value)))\n        new_parent = exit(path, key, value, new_parent, new_items)\n        return new_parent\n\n    return _remap((), None, root)", "idx": 1518}
{"namespace": "boltons.iterutils.get_path", "completion": "    if not is_iterable(path):\n        raise TypeError('expected iterable path, not: %r' % path)\n    if default is not _UNSET and not is_scalar(default):\n        raise TypeError('expected scalar default, not: %r' % default)\n    if not is_iterable(root):\n        raise TypeError('expected iterable root, not: %r' % root)\n\n    try:\n        for seg in path:\n            root = root[seg]\n    except KeyError as e:\n        raise PathAccessError(e, seg, path)\n    except IndexError as e:\n        raise PathAccessError(e, seg, path)\n    except TypeError as e:\n        raise PathAccessError(e, seg, path)\n    else:\n        return root\n\n", "idx": 1519}
{"namespace": "boltons.iterutils.research", "completion": "    if not callable(query):\n        raise TypeError('expected callable for query, not: %r' % query)\n    if not is_iterable(root):\n        raise TypeError('expected iterable for root, not: %r' % root)\n    if reraise:\n        def _reraise(exc):\n            raise exc\n    else:\n        def _reraise(exc):\n            return exc\n    results = []\n    for path, value in remap(root, visit=lambda p, k, v: (p, v)):\n        try:\n            if query(path, k, v):\n                results.append((path, value))\n        except Exception as exc:\n            _reraise(exc)\n    return results", "idx": 1520}
{"namespace": "boltons.socketutils.BufferedSocket.getrecvbuffer", "completion": "        with self._recv_lock:\n            return self.rbuf", "idx": 1521}
{"namespace": "boltons.socketutils.BufferedSocket.getsendbuffer", "completion": "        with self._send_lock:\n            return self.sbuf[:]", "idx": 1522}
{"namespace": "boltons.socketutils.BufferedSocket.recv", "completion": "        with self._recv_lock:\n            if timeout is _UNSET:\n                timeout = self.timeout\n            if flags:\n                raise ValueError(\"non-zero flags not supported\")\n            if size > self.maxsize:\n                raise MessageTooLong(size, self.maxsize)\n            if size > len(self.rbuf):\n                self.sock.settimeout(timeout)\n                start = time.time()\n                try:\n                    nxt = self.sock.recv(self._recvsize)\n                except socket.timeout:\n                    raise Timeout(timeout, 'read %s bytes' % size)\n                if not nxt:\n                    raise ConnectionClosed('connection closed after reading %s'\n                                           ' bytes' % size)\n                self.rbuf += nxt\n            data, self.rbuf = self.rbuf[:size], self.rbuf[size:]\n        return data", "idx": 1523}
{"namespace": "boltons.socketutils.BufferedSocket.recv_close", "completion": "        with self._recv_lock:\n            if timeout is _UNSET:\n                timeout = self.timeout\n            if maxsize is _UNSET:\n                maxsize = self.maxsize\n            if maxsize is None:\n                maxsize = _RECV_LARGE_MAXSIZE\n            if timeout is _UNSET:\n                timeout = self.timeout\n            sock = self.sock\n            recvd = bytearray(self.rbuf)\n            start = time.time()\n            if not timeout:  # covers None (no timeout) and 0 (nonblocking)\n                sock.settimeout(timeout)\n            try:\n                while 1:\n                    if len(recvd) > maxsize:\n                        raise MessageTooLong(maxsize)  # see rbuf\n                    if timeout:\n                        cur_timeout = timeout - (time.time() - start)\n                        if cur_timeout <= 0.0:\n                            raise socket.timeout()\n                        sock.settimeout(cur_timeout)\n                    nxt = sock.recv(self._recvsize)\n                    if not nxt:\n                        break\n                    recvd.extend(nxt)\n            except socket.timeout:\n                self.rbuf = bytes(recvd)\n                msg = 'read %s bytes without closing connection' % len(recvd)\n                raise Timeout(timeout, msg)  # check the recv buffer\n            except Exception:\n                self.rbuf = bytes(recvd)\n                raise\n            val, self.rbuf = bytes(recvd), b''\n        return val", "idx": 1524}
{"namespace": "boltons.socketutils.BufferedSocket.flush", "completion": "        with self._send_lock:\n            if self.sbuf:\n                self.sock.send(b'')\n                self.sbuf[:] = []\n        return", "idx": 1525}
{"namespace": "boltons.socketutils.BufferedSocket.buffer", "completion": "        with self._send_lock:\n            self.sbuf.append(data)\n        return", "idx": 1526}
{"namespace": "boltons.socketutils.BufferedSocket.close", "completion": "        with self._recv_lock:\n            with self._send_lock:\n                self.sock.close()\n        self.rbuf = b''\n        self.sbuf = []\n        return", "idx": 1527}
{"namespace": "boltons.socketutils.NetstringSocket.setmaxsize", "completion": "        self.maxsize = maxsize\n        self._msgsize_maxsize = self._calc_msgsize_maxsize(maxsize)", "idx": 1528}
{"namespace": "boltons.socketutils.NetstringSocket.write_ns", "completion": "        size = len(payload)\n        if size > self.maxsize:\n            raise NetstringMessageTooLong(size, self.maxsize)\n        size_prefix = str(size)\n        if len(size_prefix) > self._msgsize_maxsize:\n            raise NetstringMessageTooLong(size, self.maxsize)\n        size_prefix = size_prefix.encode('ascii')\n        self.bsock.send(size_prefix)\n        self.bsock.send(b':')\n        self.bsock.send(payload)\n        self.bsock.send(b',')", "idx": 1529}
{"namespace": "boltons.fileutils.FilePerms.__repr__", "completion": "        return '%s(user=%r, group=%r, other=%r)' % (\n            self.__class__.__name__, self.user, self.group, self.other)\n\n", "idx": 1530}
{"namespace": "boltons.mathutils.Bits.as_hex", "completion": "        return '{{0:0{0}x}}'.format(self.len // 4).format(self.val)", "idx": 1531}
{"namespace": "boltons.mathutils.Bits.from_hex", "completion": "        if type(hex) is bytes:\n            hex = hex.decode('ascii')\n        if not hex.startswith('0x'):\n            hex = '0x' + hex\n        return cls(hex)", "idx": 1532}
{"namespace": "boltons.formatutils.split_format_str", "completion": "    # TODO: memoize\n    ret, max_anon = [], 0\n    # look for {: or {. or {[ or {}\n    start, end, prev_end = 0, 0, 0\n    for match in _pos_farg_re.finditer(fstr):\n        start, end, group = match.start(), match.end(), match.group()\n        if prev_end < start:\n            ret.append(fstr[prev_end:start])\n        prev_end = end\n        if group == '{{' or group == '}}':\n            ret.append(group)\n            continue\n        ret.append(construct_format_field_str(max_anon, group[1:], None))\n        max_anon += 1\n    ret.append(fstr[prev_end:])\n    return ret", "idx": 1533}
{"namespace": "boltons.formatutils.infer_positional_format_args", "completion": "    # TODO: memoize\n    formatter = Formatter()\n    fstr_list = []\n    for lit, fname, fspec, conv in formatter.parse(fstr):\n        if fname is not None:\n            fstr_list.append(lit)\n            fstr_list.append('{' + str(len(fstr_list)) + '}')\n        else:\n            fstr_list.append(lit)\n    return ''.join(fstr_list)", "idx": 1534}
{"namespace": "boltons.formatutils.tokenize_format_str", "completion": "    if resolve_pos:\n        fstr = infer_positional_format_args(fstr)\n\n    ret = []\n    for lit, fname, fspec, conv in Formatter().parse(fstr):\n        if fname is None:\n            ret.append(lit)\n            continue\n        ret.append(BaseFormatField(fname, fspec, conv))\n    return ret", "idx": 1535}
{"namespace": "boltons.dictutils.OneToOne.clear", "completion": "        dict.clear(self)\n        dict.clear(self.inv)\n\n    ", "idx": 1536}
{"namespace": "boltons.dictutils.OneToOne.pop", "completion": "        val = dict.pop(self, key, default)\n        if val is not _MISSING:\n            dict.__delitem__(self.inv, val)\n        return val", "idx": 1537}
{"namespace": "boltons.dictutils.OneToOne.popitem", "completion": "        key, val = dict.popitem(self)\n        del self.inv[val]\n        return key, val", "idx": 1538}
{"namespace": "boltons.dictutils.ManyToMany.update", "completion": "        if type(iterable) is ManyToMany:\n            for key, vals in iterable.data.items():\n                self.data[key] = vals\n            for key, vals in iterable.inv.data.items():\n                self.inv.data[key] = vals\n        elif type(iterable) is dict:\n            for key, vals in iterable.items():\n                self.data[key] = vals\n            for key, vals in iterable.items():\n                self.inv.data[key] = vals\n        elif type(iterable) is list:\n            for key, val in iterable:\n                self.data[key] = val\n            for key, val in iterable:\n                self.inv.data[key] = val\n        else:\n            raise TypeError('unsupported iterable type: %s' % type(iterable))\n        return", "idx": 1539}
{"namespace": "boltons.dictutils.ManyToMany.add", "completion": "        self.data.setdefault(key, set()).add(val)\n        self.inv.data.setdefault(val, set()).add(key)", "idx": 1540}
{"namespace": "boltons.dictutils.ManyToMany.remove", "completion": "        if key not in self.data:\n            return\n        self.data[key].remove(val)\n        if not self.data[key]:\n            del self.data[key]\n        if val not in self.inv.data:\n            return\n        self.inv.data[val].remove(key)\n        if not self.inv.data[val]:\n            del self.inv.data[val]", "idx": 1541}
{"namespace": "boltons.dictutils.ManyToMany.replace", "completion": "        if key not in self.data:\n            raise KeyError(\"Key not found in ManyToMany instance\")\n        if newkey in self.data:\n            raise KeyError(\"New key already exists in ManyToMany instance\")\n        self.data[newkey] = self.data.pop(key)\n        for val in self.data[newkey]:\n            self.inv.data[val].remove(key)\n            self.inv.data[val].add(newkey)\n        return", "idx": 1542}
{"namespace": "boltons.dictutils.ManyToMany.iteritems", "completion": "        for key in self.data:\n            yield key, self[key]\n", "idx": 1543}
{"namespace": "gunicorn.config.Config.__str__", "completion": "        lines = []\n        keys = sorted(self.settings, key=self.settings.__getitem__)\n        for k in keys:\n            setting = self.settings[k]\n            if callable(setting.value):\n                lines.append(\"%s = <%s()>\" % (k, setting.value.__qualname__))\n            else:\n                lines.append(\"%s = %s\" % (k, setting.value))\n        return \"\\n\".join(lines)\n\n    ", "idx": 1544}
{"namespace": "gunicorn.config.Config.set", "completion": "        self.settings[name].set(value)\n", "idx": 1545}
{"namespace": "gunicorn.config.Config.worker_class", "completion": "        worker_class = self.worker_class_str\n        if worker_class == \"sync\":\n            return util.load_class(\"gunicorn.workers.sync.SyncWorker\")\n        elif worker_class == \"eventlet\":\n            return util.load_class(\"gunicorn.workers.ggevent.EventletWorker\")\n        elif worker_class == \"gevent\":\n            return util.load_class(\"gunicorn.workers.ggevent.GeventWorker\")\n        elif worker_class == \"tornado\":\n            return util.load_class(\"gunicorn.workers.ggevent.TornadoWorker\")\n        elif worker_class == \"gthread\":\n            return util.load_class(\"gunicorn.workers.gthread.GthreadWorker\")\n        else:\n            return util.load_class(worker_class)\n", "idx": 1546}
{"namespace": "gunicorn.config.Config.address", "completion": "        addresses = []\n        for address in self.bind:\n            if address.startswith('unix:'):\n                address = 'unix://' + address[5:]\n            addresses.extend(util.parse_address(address))\n        return addresses\n", "idx": 1547}
{"namespace": "gunicorn.config.Config.logger_class", "completion": "        logger_class = self.settings['logger_class'].get()\n        if logger_class == \"simple\":\n            logger_class = \"gunicorn.glogging.Logger\"\n\n        if logger_class == \"gunicorn.glogging.Logger\":\n            if self.settings['statsd_host'].get() is not None:\n                logger_class = \"gunicorn.instrument.statsd.Statsd\"\n\n        logger_class = util.load_class(logger_class)\n        if hasattr(logger_class, \"setup\"):\n            logger_class.setup()\n        return logger_class\n", "idx": 1548}
{"namespace": "gunicorn.sock.create_sockets", "completion": "    sockets = []\n    if fds is None:\n        fds = []\n\n    for addr in conf.bind:\n        sock_type = _sock_type(addr)\n        if sock_type is UnixSocket:\n            sock = sock_type(addr, conf, log)\n        else:\n            sock = sock_type(addr, conf, log)\n        sockets.append(sock)\n\n    for fd in fds:\n        sock_type = _sock_type(conf.bind[0])\n        sock = sock_type(conf.bind[0], conf, log, fd=fd)\n        sockets.append(sock)\n\n    if conf.is_ssl:\n        for sock in sockets:\n            if not isinstance(sock, UnixSocket):\n                sock.sock = ssl_wrap_socket(sock.sock, conf)\n\n    return sockets", "idx": 1549}
{"namespace": "gunicorn.http.body.LengthReader.read", "completion": "        if not isinstance(size, int):\n            raise TypeError(\"size must be an integral type\")\n        if size < 0:\n            raise ValueError(\"Size must be positive.\")\n        if size == 0:\n            return b\"\"\n\n        buf = io.BytesIO()\n        while size > len(buf.getvalue()):\n            data = self.unreader.read()\n            if not data:\n                break\n            buf.write(data)\n\n        data = buf.getvalue()\n        ret, rest = data[:size], data[size:]\n        self.unreader.unread(rest)\n        self.length -= size\n        return ret", "idx": 1550}
{"namespace": "gunicorn.http.body.EOFReader.read", "completion": "        if not isinstance(size, int):\n            raise TypeError(\"size must be an integral type\")\n        if size < 0:\n            raise ValueError(\"Size must be positive.\")\n        if size == 0:\n            return b\"\"\n\n        if self.finished:\n            data = self.buf.getvalue()\n            ret, rest = data[:size], data[size:]\n            self.buf = io.BytesIO()\n            self.buf.write(rest)\n            return ret\n\n        while size > self.buf.tell():\n            data = self.unreader.read()\n            if not data:\n                self.finished = True\n                break\n            self.buf.write(data)\n\n        data = self.buf.getvalue()\n        ret, rest = data[:size], data[size:]\n        self.buf = io.BytesIO()\n        self.buf.write(rest)\n        return ret", "idx": 1551}
{"namespace": "gunicorn.http.body.Body.read", "completion": "        size = self.getsize(size)\n        if size == 0:\n            return b\"\"\n\n        data = self.buf.getvalue()\n        self.buf = io.BytesIO()\n\n        if size < len(data):\n            ret, rest = data[:size], data[size:]\n            self.buf.write(rest)\n            return ret\n\n        ret = []\n        while 1:\n            data = self.reader.read(min(1024, size - len(ret)))\n            if not data:\n                break\n            ret.append(data)\n            if len(ret) >= size:\n                break\n\n        ret = b\"\".join(ret)\n        if len(ret) < size:\n            self.buf.write(ret)\n            return ret\n\n        ret, rest = ret[:size], ret[size:]\n        self.buf.write(rest)\n        return ret", "idx": 1552}
{"namespace": "gunicorn.http.unreader.Unreader.read", "completion": "        if not isinstance(size, (int, long)):\n            raise TypeError(\"size parameter must be an int or long\")\n        if size == 0:\n            return b\"\"\n        if size < 0:\n            size = None\n        self.buf.seek(0, os.SEEK_END)\n        if size is None:\n            if self.buf.tell():\n                data = self.buf.getvalue()\n                self.buf.seek(0)\n                self.buf.truncate()\n                return data\n            return self.chunk()\n        while self.buf.tell() < size:\n            data = self.chunk()\n            if not data:\n                data = self.buf.getvalue()\n                self.buf.seek(0)\n                self.buf.truncate()\n                return data\n            self.buf.write(data)\n        data = self.buf.getvalue()\n        self.buf.seek(size)\n        self.buf.truncate()\n        return data", "idx": 1553}
{"namespace": "gunicorn.http.unreader.Unreader.unread", "completion": "        self.buf.seek(0, os.SEEK_END)\n        self.buf.write(data)\n        self.buf.seek(0)", "idx": 1554}
{"namespace": "gunicorn.http.unreader.IterUnreader.chunk", "completion": "        try:\n            return next(self.iter)\n        except StopIteration:\n            self.iter = None\n            return b\"\"", "idx": 1555}
{"namespace": "gunicorn.instrument.statsd.Statsd.critical", "completion": "        Logger.critical(self, msg, *args, **kwargs)\n        self.increment(\"gunicorn.log.critical\", 1)", "idx": 1556}
{"namespace": "gunicorn.instrument.statsd.Statsd.access", "completion": "        # Calculate the duration in milliseconds\n        duration = int(request_time.total_seconds() * 1000)\n\n        # Log the duration as a histogram\n        self.histogram(\"request.duration\", duration)\n\n        # Increment the count of total requests\n        self.increment(\"request.count\", 1)\n\n        # Increment the count of requests with different status codes\n        status_code = resp.status_code\n        if isinstance(status_code, str):\n            status_code = int(status_code.split(None, 1)[0])\n        self.increment(\"request.status_code.{0}xx\".format(status_code // 100), 1)", "idx": 1557}
{"namespace": "praw.exceptions.RedditErrorItem.error_message", "completion": "        if self.field:\n            return f\"{self.error_type}: {self.message} on field {self.field}\"\n        return f\"{self.error_type}: {self.message}\"\n", "idx": 1558}
{"namespace": "praw.exceptions.RedditErrorItem.__repr__", "completion": "        return f\"{self.__class__.__name__}(error_type={self.error_type}, message={self.message}, field={self.field})\"", "idx": 1559}
{"namespace": "praw.models.util.BoundedSet.add", "completion": "        self._set[item] = None\n        if len(self._set) > self.max_items:\n            self._set.popitem(last=False)", "idx": 1560}
{"namespace": "praw.models.util.ExponentialCounter.counter", "completion": "        jitter = random.uniform(-0.0625 * self._base, 0.0625 * self._base)\n        value = self._base + jitter\n        if value > self._max / 2:\n            self._base = self._max\n        else:\n            self._base *= 2\n        return value", "idx": 1561}
{"namespace": "praw.models.listing.generator.ListingGenerator._extract_sublist", "completion": "        if isinstance(listing, list):\n            return listing[1]\n        elif isinstance(listing, dict):\n            if listing[\"kind\"] == \"Listing\":\n                return listing[\"data\"][\"children\"]\n            elif listing[\"kind\"] == \"t5_flairlist\":\n                return listing[\"data\"][\"flairs\"]\n            elif listing[\"kind\"] == \"t5_modlist\":\n                return listing[\"data\"][\"mods\"]\n            elif listing[\"kind\"] == \"t5_moderatorlist\":\n                return listing[\"data\"][\"moderators\"]\n            elif listing[\"kind\"] == \"t5_moderatorlist_v2\":\n                return listing[\"data\"][\"moderators\"]\n            elif listing[\"kind\"] == \"t5_moderatorlist_v3\":\n                return listing[\"data\"][\"moderators\"]\n            elif listing[\"kind\"] == \"t5_moderatorlist_v4\":\n                return listing[\"data\"][\"moderators\"]\n            elif listing[\"kind\"] == \"t5_moderatorlist_v5\":\n                return listing[\"data\"][\"moderators\"]\n            elif listing[\"kind\"] == \"t5_moderatorlist_v6\":\n                return listing[\"data\"][\"moderators\"]\n            elif listing[\"kind\"] == \"t5_moderatorlist_v7\":\n                return listing[\"data\"][\"moderators\"]\n            elif listing[\"kind\"] == \"t5_moderatorlist_v8\":\n                return listing[\"data\"][\"moderators\"]\n            elif listing[\"kind\"] == \"t5_moderatorlist_v9\":\n                return listing[\"data\"][\"moderators\"]\n            elif listing[\"kind\"] == \"t5_moderatorlist_v10\":\n                return listing[\"data\"][\"moderators\"]\n            elif listing[\"kind\"] == \"t5_moderatorlist_v11\":\n                return listing[\"data\"][\"moderators\"]\n            elif listing[\"kind\"] == \"t5_moderatorlist_v12\":\n                return listing[\"data\"][\"moderators\"]\n            elif listing[\"kind\"] == \"t5_", "idx": 1562}
{"namespace": "praw.util.token_manager.FileTokenManager.post_refresh_callback", "completion": "        with open(self._filename, \"w\") as fp:\n            fp.write(authorizer.refresh_token)", "idx": 1563}
{"namespace": "praw.util.token_manager.FileTokenManager.pre_refresh_callback", "completion": "        with open(self._filename, \"r\") as fp:\n            authorizer.refresh_token = fp.read()", "idx": 1564}
{"namespace": "praw.util.token_manager.SQLiteTokenManager._get", "completion": "        cursor = self._connection.execute(\n            \"SELECT refresh_token FROM tokens WHERE id=?\", (self.key,)\n        )\n        result = cursor.fetchone()\n        if result is None:\n            raise KeyError(f\"No refresh token found for key: {self.key}\")\n        return result[0]", "idx": 1565}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.is_registered", "completion": "        cursor = self._connection.execute(\n            \"SELECT COUNT(*) FROM tokens WHERE id=?\", (self.key,)\n        )\n        result = cursor.fetchone()\n        return result[0] > 0", "idx": 1566}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.post_refresh_callback", "completion": "        self._set(authorizer.refresh_token)\n        authorizer.refresh_token = None\n\n    ", "idx": 1567}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.pre_refresh_callback", "completion": "        if authorizer.refresh_token is None:\n            authorizer.refresh_token = self._get()", "idx": 1568}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.register", "completion": "        if self.is_registered():\n            return False\n        self._set(refresh_token)\n        return True", "idx": 1569}
{"namespace": "jc.cli.JcCli.about_jc", "completion": "        from . import __version__\n        from . import __path__\n        from . import __file__\n        from . import __doc__\n        from . import __author__\n        from . import __author_email__\n        from . import __website__\n        from . import __copyright__\n        from . import __license__\n        from . import __python_version__\n        from . import __python_path__\n        from . import __parser_count__\n        from . import __standard_parser_count__\n        from . import __streaming_parser_count__\n        from . import __plugin_parser_count__\n        from . import __all_parser_info__\n\n        about_jc: JSONDictType = {\n            'jc': {\n                'name': 'jc',\n                'version': __version__,\n                'description': __doc__,\n                'author': __author__,\n                'author_email': __author_email__,\n                'website': __website__,\n                'copyright': __copyright__,\n                'license': __license__,\n                'python_version': __python_version__,\n                'python_path': __python_path__,\n                'parser_count': __parser_count__,\n                'standard_parser_count': __standard_parser_count__,\n                'streaming_parser_count': __streaming_parser_count__,\n                'plugin_parser_count': __plugin_parser_count__,\n                'all_parser_info': __all_parser_info__\n            }\n        }\n\n        return about_jc", "idx": 1570}
{"namespace": "jc.cli.JcCli.yaml_out", "completion": "        try:\n            import ruamel.yaml\n            from ruamel.yaml.compat import StringIO\n            from ruamel.yaml.representer import RoundTripRepresenter\n            from ruamel.yaml.scalarstring import LiteralScalarString\n\n            class MyRepresenter(RoundTripRepresenter):\n                def represent_scalar(self, tag, value, style=None):\n                    if style is None:\n                        style = self.default_style\n                    if style == '|':\n                        return super().represent_scalar(tag, value, style)\n                    if value.startswith('\\n') and value.endswith('\\n'):\n                        return super().represent_scalar(tag, value, style)\n                    if value.startswith('\\n') or value.endswith('\\n'):\n                        return super().represent_scalar(tag, value, '|')\n                    if '\\n' in value:\n                        return super().represent_scalar(tag, value, '|')\n                    if value.startswith('\"') and value.endswith('\"'):\n                        return super().represent_scalar(tag, value, style)\n                    if value.startswith(\"'\") and value.endswith(\"'\"):\n                        return super().represent_scalar(tag, value, style)\n                    if value.startswith('\"') or value.endswith('\"'):\n                        return super().represent_scalar(tag, value, '|')\n                    if value.startswith(\"'\") or value.endswith(\"'\"):\n                        return super().represent_scalar(tag, value, '|')\n                    if value.startswith('{') and value.endswith('}'):\n                        return super().represent_scalar(tag, value, style)\n                    if value.startswith('[') and value.endswith(']'):\n                        return super().represent_scalar(tag, value, style)\n                    if value.startswith('{') or value.endswith('}'):\n                        return super().represent_scalar(tag, value, '|')\n                    if", "idx": 1571}
{"namespace": "jc.parsers.os_release.parse", "completion": "    jc.utils.compatibility(__name__, info.compatible)\n    jc.utils.input_type_check(data)\n\n    raw_output: JSONDictType = {}\n\n    if jc.utils.has_data(data):\n        raw_output = jc.parsers.kv.parse(data, raw=raw, quiet=quiet)\n\n    return raw_output if raw else _process(raw_output)", "idx": 1572}
{"namespace": "jc.parsers.xrandr._parse_screen", "completion": "    result = re.match(_screen_pattern, next_lines.pop())\n    if not result:\n        return None\n\n    matches = result.groupdict()\n    screen: Screen = {\n        \"devices\": [],\n    }\n    for k, v in matches.items():\n        try:\n            screen[k] = int(v)\n        except ValueError:\n            screen[k] = v\n\n    while next_lines:\n        next_line = next_lines.pop()\n        next_device: Optional[Device] = _parse_device(next_lines, quiet)\n        if next_device:\n            screen[\"devices\"].append(next_device)\n        else:\n            if re.match(_screen_pattern, next_line):\n                next_lines.append(next_line)\n                break\n    return screen", "idx": 1573}
{"namespace": "jc.parsers.xrandr._parse_model", "completion": "    if not next_lines:\n        return None\n\n    next_line = next_lines.pop()\n    result = re.match(_edid_head_pattern, next_line)\n    if not result:\n        next_lines.append(next_line)\n        return None\n\n    model: Model = {}\n    while next_lines:\n        next_line = next_lines.pop()\n        result = re.match(_edid_line_pattern, next_line)\n        if not result:\n            next_lines.append(next_line)\n            break\n\n        hex_value = result.groupdict()[\"edid_line\"]\n        bytes_value = bytes.fromhex(hex_value)\n        model[\"name\"] = bytes_value[10:20].decode(\"utf-8\").strip()\n        model[\"product_id\"] = bytes_value[20:22].decode(\"utf-8\").strip()\n        model[\"serial_number\"] = bytes_value[22:30].decode(\"utf-8\").strip()\n\n    return model", "idx": 1574}
{"namespace": "jc.parsers.xrandr._parse_mode", "completion": "    result = re.match(_mode_pattern, line)\n    if not result:\n        return None\n\n    matches = result.groupdict()\n    mode: Mode = {\n        \"resolution_width\": int(matches[\"resolution_width\"]),\n        \"resolution_height\": int(matches[\"resolution_height\"]),\n        \"is_high_resolution\": matches[\"is_high_resolution\"] is not None,\n        \"frequencies\": [],\n    }\n\n    frequencies = re.findall(_frequencies_pattern, matches[\"rest\"])\n    for frequency in frequencies:\n        frequency_matches = re.match(_frequencies_pattern, frequency)\n        if not frequency_matches:\n            continue\n        frequency_matches = frequency_matches.groupdict()\n        mode[\"frequencies\"].append(\n            {\n                \"frequency\": float(frequency_matches[\"frequency\"]),\n                \"is_current\": frequency_matches[\"plus\"] == \"+\",\n                \"is_preferred\": frequency_matches[\"star\"] == \"*\",\n            }\n        )\n\n    return mode", "idx": 1575}
{"namespace": "pythonforandroid.archs.Arch.include_dirs", "completion": "        return [\n            join(self.ctx.ndk.sysroot_include_dir, self.command_prefix),\n            join(self.ctx.ndk.sysroot_include_dir, 'c++/{}'.format(self.arch)),\n            join(self.ctx.ndk.sysroot_include_dir, 'c++/{}'.format(self.arch)),\n            join(self.ctx.ndk.sysroot_include_dir, 'c++/{}'.format(self.arch)),\n            join(self.ctx.ndk.sysroot_include_dir, 'c++/{}'.format(self.arch)),\n            join(self.ctx.ndk.sysroot_include_dir, 'c++/{}'.format(self.arch)),\n            join(self.ctx.ndk.sysroot_include_dir, 'c++/{}'.format(self.arch)),\n            join(self.ctx.ndk.sysroot_include_dir, 'c++/{}'.format(self.arch)),\n            join(self.ctx.ndk.sysroot_include_dir, 'c++/{}'.format(self.arch)),\n            join(self.ctx.ndk.sysroot_include_dir, 'c++/{}'.format(self.arch)),\n            join(self.ctx.ndk.sysroot_include_dir, 'c++/{}'.format(self.arch)),\n            join(self.ctx.ndk.sysroot_include_dir, 'c++/{}'.format(self.arch)),\n            join(self.ctx.ndk.sysroot_include_dir, 'c++/{}'.format(self.arch)),\n            join(self.ctx.ndk.sysroot_include_dir, 'c++/{}'.format(self.arch)),\n            join(self.ctx.ndk.sysroot_include_dir, 'c++/{}'.format(self.arch)),\n            join(self.ctx.ndk.sysroot_include_dir, 'c++/{}'.format(self.arch)),\n            join(self.ctx.ndk.sysroot_include_dir, 'c++/", "idx": 1576}
{"namespace": "pythonforandroid.archs.Arch.target", "completion": "        return '{command_prefix}{ndk_api}'.format(\n            command_prefix=self.command_prefix,\n            ndk_api=self.ctx.ndk_api,\n        )\n\n    ", "idx": 1577}
{"namespace": "pythonforandroid.archs.ArchARM.target", "completion": "        return '{triplet}{ndk_api}'.format(\n            triplet=self.command_prefix, ndk_api=self.ctx.ndk_api\n        )", "idx": 1578}
{"namespace": "pythonforandroid.recipe.Recipe.get_recipe", "completion": "        if name in cls._recipes:\n            return cls._recipes[name]\n\n        recipe_dirs = cls.recipe_dirs(ctx)\n        for recipes_dir in recipe_dirs:\n            if recipes_dir and exists(recipes_dir):\n                fn = join(recipes_dir, name)\n                if isdir(fn):\n                    modname = 'pythonforandroid.recipes.{}'.format(name)\n                    try:\n                        __import__(modname)\n                    except ImportError:\n                        pass\n                    mod = sys.modules[modname]\n                    recipe = mod.Recipe(ctx)\n                    cls._recipes[name] = recipe\n                    return recipe\n\n        raise BuildInterruptingException(\n            'Could not find recipe {}'.format(name))\n", "idx": 1579}
{"namespace": "pythonforandroid.prerequisites.HomebrewPrerequisite.darwin_helper", "completion": "        info(\n            \"python-for-android requires Homebrew to be installed on macOS,\"\n            \"but seems like you don't have it installed.\"\n        )\n        info(\n            \"If you think that Homebrew is already installed, please verify that \"\n            \"you have it installed and that `brew --prefix` shows the correct path.\"\n        )\n        info(\n            \"If you have multiple Homebrew installations, please make sure that you have \"\n            \"`HOMEBREW_PREFIX` environment variable set to the correct Homebrew installation.\"\n        )\n", "idx": 1580}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_checker", "completion": "        if self._darwin_get_brew_formula_location_prefix(self.homebrew_formula_name):\n            info(\"OpenSSL is installed\")\n            return True\n        else:\n            info(\"OpenSSL is not installed\")\n            return False\n\n    ", "idx": 1581}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_pkg_config_location", "completion": "        prefix = self._darwin_get_brew_formula_location_prefix(\n            self.homebrew_formula_name, installed=True\n        )\n        return os.path.join(prefix, \"lib\", \"pkgconfig\")", "idx": 1582}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_installer", "completion": "        info(\"Installing OpenSSL ...\")\n        subprocess.check_output([\"brew\", \"install\", self.homebrew_formula_name])\n\n", "idx": 1583}
{"namespace": "pythonforandroid.prerequisites.AutoconfPrerequisite.darwin_installer", "completion": "        info(\"Installing Autoconf ...\")\n        subprocess.check_output([\"brew\", \"install\", \"autoconf\"])", "idx": 1584}
{"namespace": "pythonforandroid.prerequisites.AutomakePrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"automake\", installed=True)\n            is not None\n        )", "idx": 1585}
{"namespace": "pythonforandroid.prerequisites.AutomakePrerequisite.darwin_installer", "completion": "        info(\"Installing Automake ...\")\n        subprocess.check_output([\"brew\", \"install\", \"automake\"])", "idx": 1586}
{"namespace": "pythonforandroid.prerequisites.LibtoolPrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"libtool\", installed=True)\n            is not None\n        )", "idx": 1587}
{"namespace": "pythonforandroid.prerequisites.LibtoolPrerequisite.darwin_installer", "completion": "        info(\"Installing Libtool ...\")\n        subprocess.check_output([\"brew\", \"install\", \"libtool\"])", "idx": 1588}
{"namespace": "pythonforandroid.prerequisites.PkgConfigPrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"pkg-config\", installed=True)\n            is not None\n        )", "idx": 1589}
{"namespace": "pythonforandroid.prerequisites.PkgConfigPrerequisite.darwin_installer", "completion": "        info(\"Installing Pkg-Config ...\")\n        subprocess.check_output([\"brew\", \"install\", \"pkg-config\"])", "idx": 1590}
{"namespace": "pythonforandroid.prerequisites.CmakePrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"cmake\", installed=True)\n            is not None\n        )", "idx": 1591}
{"namespace": "pythonforandroid.prerequisites.CmakePrerequisite.darwin_installer", "completion": "        info(\"Installing Cmake ...\")\n        subprocess.check_output([\"brew\", \"install\", \"cmake\"])", "idx": 1592}
{"namespace": "pythonforandroid.prerequisites.get_required_prerequisites", "completion": "    prerequisites = [\n        HomebrewPrerequisite,\n        JDKPrerequisite,\n        OpenSSLPrerequisite,\n        AutoconfPrerequisite,\n        AutomakePrerequisite,\n        LibtoolPrerequisite,\n        PkgConfigPrerequisite,\n        CmakePrerequisite,\n    ]\n\n    if platform == \"linux\":\n        return [prerequisite() for prerequisite in prerequisites if prerequisite.mandatory[platform]]\n    elif platform == \"darwin\":\n        return [prerequisite() for prerequisite in prerequisites if prerequisite.mandatory[platform]]\n    else:\n        raise Exception(\"Unsupported platform\")", "idx": 1593}
{"namespace": "pythonforandroid.pythonpackage.parse_as_folder_reference", "completion": "    if dep.startswith(\"file://\"):\n        return urlunquote(urlparse(dep).path)\n    else:\n        return None", "idx": 1594}
{"namespace": "pythonforandroid.pythonpackage.get_package_name", "completion": "    if dependency in package_name_cache:\n        if time.time() - package_name_cache[dependency][1] < 60 * 60 * 24:\n            if use_cache:\n                return package_name_cache[dependency][0]\n    package_name = _extract_info_from_package(dependency, extract_type=\"name\")\n    package_name_cache[dependency] = (package_name, time.time())\n    return package_name", "idx": 1595}
{"namespace": "pythonforandroid.recommendations.read_ndk_version", "completion": "    try:\n        with open(join(ndk_dir, 'source.properties')) as f:\n            for line in f:\n                if line.startswith('Pkg.Revision'):\n                    return LooseVersion(line.split('=')[1])\n    except IOError:\n        warning(UNKNOWN_NDK_MESSAGE)\n    except IndexError:\n        warning(PARSE_ERROR_NDK_MESSAGE)", "idx": 1596}
{"namespace": "pythonforandroid.recommendations.check_target_api", "completion": "    if api < MIN_TARGET_API:\n        warning(OLD_API_MESSAGE)\n\n    if arch == 'armeabi' and api > ARMEABI_MAX_TARGET_API:\n        raise BuildInterruptingException(\n            UNSUPPORTED_NDK_API_FOR_ARMEABI_MESSAGE.format(\n                req_ndk_api=api, max_ndk_api=ARMEABI_MAX_TARGET_API\n            ),\n            instructions=(\n                'The NDK API is a minimum supported API number and must be lower '\n                'than the target Android API'\n            ),\n        )", "idx": 1597}
{"namespace": "pythonforandroid.recommendations.check_ndk_api", "completion": "    if ndk_api < MIN_NDK_API:\n        warning(OLD_NDK_API_MESSAGE)\n    elif ndk_api > android_api:\n        raise BuildInterruptingException(\n            TARGET_NDK_API_GREATER_THAN_TARGET_API_MESSAGE.format(\n                ndk_api=ndk_api, android_api=android_api\n            ),\n            instructions='You probably want to build with --ndk-api={} instead'.format(\n                android_api\n            ),\n        )", "idx": 1598}
{"namespace": "pythonforandroid.androidndk.AndroidNDK.llvm_prebuilt_dir", "completion": "        return os.path.join(self.ndk_dir, \"toolchains\", \"llvm\", \"prebuilt\", self.host_tag)\n\n    ", "idx": 1599}
{"namespace": "pythonforandroid.build.Context.setup_dirs", "completion": "        self.storage_dir = storage_dir\n        self.build_dir = join(self.storage_dir, 'build')\n        self.dist_dir = join(self.storage_dir, 'dist')\n\n        self.ensure_dirs()\n", "idx": 1600}
{"namespace": "pythonforandroid.graph.get_dependency_tuple_list_for_recipe", "completion": "    if blacklist is None:\n        blacklist = set()\n    dependencies = []\n    for dep in recipe.depends:\n        if isinstance(dep, (list, tuple)):\n            dependencies.append(tuple([d.lower() for d in dep]))\n        else:\n            dependencies.append((dep.lower(),))\n    dependencies = [dep for dep in dependencies if dep[0] not in blacklist]\n    return dependencies", "idx": 1601}
{"namespace": "pythonforandroid.graph.obvious_conflict_checker", "completion": "    if blacklist is None:\n        blacklist = set()\n    blacklist = {bitem.lower() for bitem in blacklist}\n\n    # Get all recipes and their dependencies\n    recipes = Recipe.get_all_recipes(ctx)\n    all_dependencies = {}\n    for recipe in recipes:\n        all_dependencies[recipe.name] = get_dependency_tuple_list_for_recipe(\n            recipe, blacklist=blacklist\n        )\n\n    # Add all recipes to the list of dependencies\n    for recipe in recipes:\n        all_dependencies[recipe.name] = [\n            tuple(set(deptuple) - blacklist)\n            for deptuple in all_dependencies[recipe.name]\n            if tuple(set(deptuple) - blacklist)\n        ]\n\n    # Check for conflicts\n    for name_tuple in name_tuples:\n        for name in name_tuple:\n            if name not in all_dependencies:\n                raise ValueError(\n                    'Recipe {} not found in the list of recipes.'.format(name)\n                )\n\n    # Check for conflicts\n    for name_tuple in name_tuples:\n        for name in name_tuple:\n            conflicts = []\n            for dependency_set in product(*all_dependencies[name]):\n                for dependency in dependency_set:\n                    if dependency in name_tuple:\n                        conflicts.append(dependency)\n            if conflicts:\n                raise ValueError(\n                    'Conflict found between {} and {}.'.format(\n                        name, ', '.join(conflicts)\n                    )\n                )", "idx": 1602}
{"namespace": "pythonforandroid.graph.get_recipe_order_and_bootstrap", "completion": "    # Get set of recipe/dependency names, clean up and add bootstrap deps:\n    names = set(names)\n    if bs is not None:\n        names.add(bs.name)\n        names.add(bs.python_module)\n    if blacklist is None:\n        blacklist = set()\n    assert type(blacklist) is set\n\n    # Check for obvious conflicts:\n    obvious_conflict_checker(ctx, names, blacklist=blacklist)\n\n    # Get all possible orders:\n    orders = recursively_collect_orders(\n        bs.name, ctx, names, blacklist=blacklist\n    )\n\n    # Convert each order graph into a linear list and sort them based on preference:\n    order_lists = [\n        list(find_order(order))\n        for order in orders\n    ]\n    order_lists.sort(key=lambda order: order.index(bs.name))\n\n    # Return the chosen order, along with the corresponding recipes, python modules, and bootstrap instance:\n    return order_lists[0], [\n        Recipe.get_recipe(name, ctx)\n        for name in order_lists[0]\n    ], [\n        Recipe.get_recipe(name, ctx).python_module\n        for name in order_lists[0]\n    ], bs", "idx": 1603}
{"namespace": "pythonforandroid.util.ensure_dir", "completion": "    if not exists(dn):\n        LOGGER.debug(\"Creating directory {}\".format(dn))\n        makedirs(dn)", "idx": 1604}
{"namespace": "pythonforandroid.util.move", "completion": "    LOGGER.debug(\"Move {} to {}\".format(source, destination))\n    shutil.move(source, destination)", "idx": 1605}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.get_bootstrap_from_recipes", "completion": "        bootstraps = cls.get_usable_bootstraps_for_recipes(recipes, ctx)\n        if not bootstraps:\n            raise BuildInterruptingException(\n                \"No bootstrap found that satisfies the given recipes: {}\".format(\n                    recipes))\n        bootstraps = sorted(bootstraps, key=functools.cmp_to_key(_cmp_bootstraps_by_priority))\n        return bootstraps[0]", "idx": 1606}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.get_bootstrap", "completion": "        # Get the bootstrap class:\n        modname = 'pythonforandroid.bootstraps.' + name\n        mod = importlib.import_module(modname)\n        cls = getattr(mod, name.capitalize())\n\n        # Set the bootstrap directory:\n        cls.bootstrap_dir = join(dirname(__file__), 'bootstraps', name)\n\n        # Return the bootstrap instance:\n        return cls(ctx)\n", "idx": 1607}
{"namespace": "pythonforandroid.bootstrap.expand_dependencies", "completion": "    # Get all available recipes:\n    all_recipes = Recipe.all_recipes(ctx)\n\n    # Get all available alternatives:\n    all_alternatives = Recipe.all_alternatives(ctx)\n\n    # Get all available dependencies:\n    all_dependencies = Recipe.all_dependencies(ctx)\n\n    # Get all available conflicts:\n    all_conflicts = Recipe.all_conflicts(ctx)\n\n    # Get all available dependencies for the recipes:\n    all_recipe_dependencies = Recipe.all_recipe_dependencies(ctx)\n\n    # Get all available alternatives for the recipes:\n    all_recipe_alternatives = Recipe.all_recipe_alternatives(ctx)\n\n    # Get all available conflicts for the recipes:\n    all_recipe_conflicts = Recipe.all_recipe_conflicts(ctx)\n\n    # Get all available dependencies for the alternatives:\n    all_alternative_dependencies = Recipe.all_alternative_dependencies(ctx)\n\n    # Get all available conflicts for the alternatives:\n    all_alternative_conflicts = Recipe.all_alternative_conflicts(ctx)\n\n    # Get all available dependencies for the conflicts:\n    all_conflict_dependencies = Recipe.all_conflict_dependencies(ctx)\n\n    # Get all available alternatives for the conflicts:\n    all_conflict_alternatives = Recipe.all_conflict_alternatives(ctx)\n\n    # Get all available conflicts for the conflicts:\n    all_conflict_conflicts = Recipe.all_conflict_conflicts(ctx)\n\n    # Get all available dependencies for the recipes:\n    all_recipe_dependencies = Recipe.all_recipe_dependencies(ctx)\n\n    # Get all available alternatives for the recipes:\n    all_recipe_alternatives = Recipe.all_recipe_alternatives(ctx)\n\n    # Get all available conflicts for the recipes:\n    all_recipe_conflicts = Recipe.all_recipe_conflicts(ctx)", "idx": 1608}
{"namespace": "pythonforandroid.recipes.icu.ICURecipe.get_recipe_dir", "completion": "        local_recipes_dir = self.get_recipe_dir_in_local_dir()\n        if local_recipes_dir is not None:\n            return local_recipes_dir\n        return join(self.get_recipe_dir_in_root_dir(), self.name)\n", "idx": 1609}
{"namespace": "mmcv.transforms.wrappers.KeyMapper.__repr__", "completion": "        repr_str = self.__class__.__name__\n        repr_str += f'(transforms = {self.transforms}'\n        repr_str += f', mapping = {self.mapping}'\n        repr_str += f', remapping = {self.remapping}'\n        repr_str += f', auto_remap = {self.auto_remap}'\n        repr_str += f', allow_nonexist_keys = {self.allow_nonexist_keys})'\n        return repr_str", "idx": 1610}
{"namespace": "mmcv.transforms.wrappers.TransformBroadcaster.__repr__", "completion": "        repr_str = self.__class__.__name__\n        repr_str += f'(transforms = {self.transforms}'\n        repr_str += f', mapping = {self.mapping}'\n        repr_str += f', remapping = {self.remapping}'\n        repr_str += f', auto_remap = {self.auto_remap}'\n        repr_str += f', allow_nonexist_keys = {self.allow_nonexist_keys}'\n        repr_str += f', share_random_params = {self.share_random_params})'\n        return repr_str", "idx": 1611}
{"namespace": "mackup.utils.delete", "completion": "    if os.path.isfile(filepath):\n        remove_acl(filepath)\n        remove_immutable_attribute(filepath)\n        os.remove(filepath)\n    elif os.path.isdir(filepath):\n        remove_acl(filepath)\n        remove_immutable_attribute(filepath)\n        shutil.rmtree(filepath)\n    elif os.path.islink(filepath):\n        os.unlink(filepath)\n    else:\n        raise ValueError(\"Unsupported file: {}\".format(filepath))", "idx": 1612}
{"namespace": "mackup.utils.copy", "completion": "    if not os.path.exists(src):\n        raise ValueError(\"Source path does not exist: {}\".format(src))\n    if not os.path.exists(dst):\n        os.makedirs(dst)\n    if os.path.isfile(src):\n        shutil.copy(src, dst)\n    elif os.path.isdir(src):\n        shutil.copytree(src, dst)\n    else:\n        raise ValueError(\"Unsupported file type: {}\".format(src))\n    chmod(dst)", "idx": 1613}
{"namespace": "mackup.utils.get_dropbox_folder_location", "completion": "    dropbox_home = None\n\n    dropbox_host_db = os.path.join(os.environ[\"HOME\"], \".dropbox/host.db\")\n    if os.path.isfile(dropbox_host_db):\n        con = sqlite3.connect(dropbox_host_db)\n        if con:\n            cur = con.cursor()\n            query = \"SELECT value \" \"FROM config \" \"WHERE key = 'home';\"\n            cur.execute(query)\n            data = cur.fetchone()\n            dropbox_home = str(data[0])\n            con.close()\n\n    if not dropbox_home:\n        error(constants.ERROR_UNABLE_TO_FIND_STORAGE.format(provider=\"Dropbox install\"))\n\n    return dropbox_home", "idx": 1614}
{"namespace": "mackup.utils.get_copy_folder_location", "completion": "    copy_settings_path = \"~/Library/Application Support/Copy/Settings.db\"\n    copy_settings_db = os.path.expanduser(copy_settings_path)\n    if not os.path.isfile(copy_settings_db):\n        error(constants.ERROR_UNABLE_TO_FIND_STORAGE.format(provider=\"Copy\"))\n\n    con = sqlite3.connect(copy_settings_db)\n    if con:\n        cur = con.cursor()\n        query = \"SELECT value FROM settings WHERE option = 'csmRootPath';\"\n        cur.execute(query)\n        data = cur.fetchone()\n        copy_home = str(data[0])\n        con.close()\n\n    return copy_home", "idx": 1615}
{"namespace": "mackup.utils.can_file_be_synced_on_current_platform", "completion": "    # ", "idx": 1616}
{"namespace": "hl7.client.MLLPClient.send_message", "completion": "        if isinstance(message, hl7.Message):\n            message = message.to_string()\n        elif isinstance(message, bytes):\n            pass\n        elif isinstance(message, str):\n            message = message.encode(self.encoding)\n        else:\n            raise MLLPException(\"unsupported message type: %s\" % type(message))\n\n        # wrap the message in the MLLP container\n        data = SB + message + EB + CR\n\n        # upload the data\n        self.socket.send(data)\n        # wait for the ACK/NACK\n        return self.socket.recv(RECV_BUFFER)", "idx": 1617}
{"namespace": "hl7.client.MLLPClient.send", "completion": "        self.socket.sendall(data)\n        return self.socket.recv(RECV_BUFFER)", "idx": 1618}
{"namespace": "hl7.datatypes._UTCOffset.tzname", "completion": "        if self.minutes == 0:\n            return \"Z\"\n        else:\n            return \"{0}{1:04d}\".format(\n                \"+\" if self.minutes > 0 else \"-\", abs(self.minutes)\n            )", "idx": 1619}
{"namespace": "hl7.datatypes.parse_datetime", "completion": "    if not value:\n        return None\n\n    match = DTM_TZ_RE.match(value)\n    if not match:\n        raise ValueError(\"Invalid DTM format\")\n\n    year, month, day, hour, minute, second, microsecond = [\n        int(x) if x else 0 for x in match.groups()\n    ]\n\n    if year < 100:\n        year += 2000\n\n    if month is None:\n        month = 1\n\n    if day is None:\n        day = 1\n\n    if hour is None:\n        hour = 0\n\n    if minute is None:\n        minute = 0\n\n    if second is None:\n        second = 0\n\n    if microsecond is None:\n        microsecond = 0\n\n    if match.group(2):\n        tz = _UTCOffset(int(match.group(2)) * 60 + int(match.group(3)))\n    else:\n        tz = None\n\n    return datetime.datetime(\n        year, month, day, hour, minute, second, microsecond, tzinfo=tz\n    )", "idx": 1620}
{"namespace": "hl7.parser._ParsePlan.container", "completion": "        # Return the instance of the current message part according\n        # to the plan\n        return self.containers[0](\n            sequence=data, esc=self.esc, separators=self.separators, factory=self.factory\n        )", "idx": 1621}
{"namespace": "hl7.parser._ParsePlan.next", "completion": "        return _ParsePlan(\n            self.separators[self.separators.find(self.separator) + 1],\n            self.separators,\n            self.containers[1:],\n            self.esc,\n            self.factory,\n        )", "idx": 1622}
{"namespace": "hl7.version.get_version", "completion": "    if len(VERSION) < 4:\n        return \".\".join(map(str, VERSION))\n    elif VERSION[3] == \"final\":\n        return \".\".join(map(str, VERSION[:3]))\n    else:\n        return \".\".join(map(str, VERSION[:3])) + \".\" + VERSION[3]", "idx": 1623}
{"namespace": "twtxt.config.Config.from_file", "completion": "        if not os.path.exists(file):\n            return None\n\n        cfg = configparser.ConfigParser()\n        cfg.read(file)\n\n        return cls(file, cfg)\n\n    ", "idx": 1624}
{"namespace": "twtxt.config.Config.discover", "completion": "        config_file = os.path.join(cls.config_dir, cls.config_name)\n        return cls.from_file(config_file)\n\n    ", "idx": 1625}
{"namespace": "twtxt.config.Config.create_config", "completion": "        config = configparser.ConfigParser()\n        config.add_section(\"twtxt\")\n        config.set(\"twtxt\", \"nick\", nick)\n        config.set(\"twtxt\", \"twtfile\", twtfile)\n        config.set(\"twtxt\", \"twturl\", twturl)\n        config.set(\"twtxt\", \"disclose_identity\", str(disclose_identity))\n        config.set(\"twtxt\", \"add_news\", str(add_news))\n        config.add_section(\"following\")\n        config.write(open(cfgfile, \"w\"))\n        return cls(cfgfile, config)\n\n    ", "idx": 1626}
{"namespace": "twtxt.config.Config.following", "completion": "        following = []\n        if self.cfg.has_section(\"following\"):\n            for nick, url in self.cfg.items(\"following\"):\n                following.append(Source(nick, url))\n        else:\n            logger.debug(\"No following section found in config.\")\n        return following\n", "idx": 1627}
{"namespace": "twtxt.config.Config.options", "completion": "        options = {}\n        try:\n            for option in self.cfg.options(\"twtxt\"):\n                options[option] = self.cfg.get(\"twtxt\", option)\n        except configparser.NoSectionError as e:\n            logger.debug(e)\n\n        return options\n\n    ", "idx": 1628}
{"namespace": "twtxt.models.Tweet.relative_datetime", "completion": "        delta = humanize.naturaltime(datetime.now(timezone.utc) - self.created_at)\n        return delta\n\n    ", "idx": 1629}
{"namespace": "twtxt.mentions.format_mentions", "completion": "    def handle_mention(match):\n        return format_callback(match.group(1), match.group(2))\n\n    return mention_re.sub(handle_mention, text)", "idx": 1630}
{"namespace": "twtxt.parser.parse_tweets", "completion": "    from twtxt.models import Tweet\n    if now is None:\n        now = datetime.now(timezone.utc)\n\n    tweets = []\n    for raw_tweet in raw_tweets:\n        try:\n            tweets.append(parse_tweet(raw_tweet, source, now))\n        except ValueError as e:\n            logger.warning(e)\n\n    return tweets", "idx": 1631}
{"namespace": "wikipediaapi.Wikipedia.page", "completion": "        if unquote:\n            title = parse.unquote(title)\n\n        return WikipediaPage(\n            wiki=self,\n            title=title,\n            ns=namespace2int(ns),\n            language=self.language,\n        )", "idx": 1632}
{"namespace": "wikipediaapi.Wikipedia.article", "completion": "        return self.page(title, ns, unquote)", "idx": 1633}
{"namespace": "wikipediaapi.WikipediaPageSection.__repr__", "completion": "        res = f\"{self.title} (level: {self.level})\"\n        if len(self.sections) > 0:\n            res += f\" ({len(self.sections)} subsections)\"\n        return res", "idx": 1634}
{"namespace": "wikipediaapi.WikipediaPage.sections", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        return self._section\n\n    ", "idx": 1635}
{"namespace": "wikipediaapi.WikipediaPage.section_by_title", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        sections = self._section_mapping.get(title)\n        if sections is None:\n            return None\n        return sections[-1]", "idx": 1636}
{"namespace": "wikipediaapi.WikipediaPage.sections_by_title", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        sections = self._section_mapping.get(title)\n        if sections:\n            return sections\n        return []", "idx": 1637}
{"namespace": "wikipediaapi.WikipediaPage.text", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        text = self.summary\n        for section in self.sections:\n            text += section.full_text()\n        return text.strip()\n\n    ", "idx": 1638}
{"namespace": "wikipediaapi.WikipediaPage.langlinks", "completion": "        if not self._called[\"langlinks\"]:\n            self._fetch(\"langlinks\")\n        return self._langlinks", "idx": 1639}
{"namespace": "wikipediaapi.WikipediaPage.links", "completion": "        if not self._called[\"links\"]:\n            self._fetch(\"links\")\n        return self._links\n\n    ", "idx": 1640}
{"namespace": "wikipediaapi.WikipediaPage.backlinks", "completion": "        if not self._called[\"backlinks\"]:\n            self._fetch(\"backlinks\")\n        return self._backlinks\n\n    ", "idx": 1641}
{"namespace": "wikipediaapi.WikipediaPage.categorymembers", "completion": "        if not self._called[\"categorymembers\"]:\n            self._fetch(\"categorymembers\")\n        return self._categorymembers", "idx": 1642}
{"namespace": "wikipediaapi.WikipediaPage._fetch", "completion": "        if call == \"extracts\":\n            self.wiki.extracts(self)\n        elif call == \"info\":\n            self.wiki.info(self)\n        elif call == \"langlinks\":\n            self.wiki.langlinks(self)\n        elif call == \"links\":\n            self.wiki.links(self)\n        elif call == \"backlinks\":\n            self.wiki.backlinks(self)\n        elif call == \"categories\":\n            self.wiki.categories(self)\n        elif call == \"categorymembers\":\n            self.wiki.categorymembers(self)\n        else:\n            raise NotImplementedError(\"Unknown call type\")\n\n        self._called[call] = True\n        return self", "idx": 1643}
{"namespace": "wikipediaapi.WikipediaPage.__repr__", "completion": "        if self._called[\"info\"]:\n            return \"{title} (id: {pageid}, ns: {ns})\".format(**self._attributes)\n        return \"{title} (id: ??, ns: {ns})\".format(**self._attributes)", "idx": 1644}
{"namespace": "imapclient.imapclient.IMAPClient.starttls", "completion": "        if self._starttls_done:\n            raise exceptions.IllegalStateError(\"STARTTLS already done\")\n\n        if ssl_context is None:\n            ssl_context = ssl_lib.create_default_context()\n\n        if self.ssl:\n            raise exceptions.IllegalStateError(\"STARTTLS already done\")\n\n        self._imap.starttls(ssl_context)\n        self._starttls_done = True\n\n        # Check the hostname in the server's certificate against the hostname used for connecting.\n        # If the SSL connection cannot be established or the server does not support STARTTLS, appropriate exceptions are raised.\n        self._imap.sock.do_handshake_on_connect = True\n        self._imap.sock.set_tlsext_host_name(self.host)\n        self._imap.sock.do_handshake_on_connect = False\n\n        # Check the hostname in the server's certificate against the hostname used for connecting.\n        # If the SSL connection cannot be established or the server does not support STARTTLS, appropriate exceptions are raised.\n        self._imap.sock.do_handshake_on_connect = True\n        self._imap.sock.set_tlsext_host_name(self.host)\n        self._imap.sock.do_handshake_on_connect = False\n\n        # Check the hostname in the server's certificate against the hostname used for connecting.\n        # If the SSL connection cannot be established or the server does not support STARTTLS, appropriate exceptions are raised.\n        self._imap.sock.do_handshake_on_connect = True\n        self._imap.sock.set_tlsext_host_name(self.host)\n        self._imap.sock.do_handshake_on_connect = False\n\n        # Check the hostname in the server's certificate against the hostname used for connecting.\n        # If the SSL connection cannot be established or the server does not support STARTTLS, appropriate exceptions are raised.\n        self._imap.", "idx": 1645}
{"namespace": "imapclient.imapclient.IMAPClient.shutdown", "completion": "        self._imap.shutdown()\n        logger.debug(\"Connection closed\")\n", "idx": 1646}
{"namespace": "imapclient.imapclient.IMAPClient.enable", "completion": "        if not capabilities:\n            raise ValueError(\"At least one capability must be specified\")\n        typ, data = self._imap._simple_command(\"ENABLE\", *capabilities)\n        self._checkok(\"enable\", typ, data)\n        return data[0]", "idx": 1647}
{"namespace": "imapclient.imapclient.IMAPClient._proc_folder_list", "completion": "        # Filter out empty strings and None's.\n        # This also deals with the special case of - no 'untagged'\n        # responses (ie, no folders). This comes back as [None].\n        folders = [\n            folder\n            for folder in parse_response(folder_data)\n            if folder and len(folder) > 0\n        ]\n\n        # If folder encoding is enabled, decode the folder name using UTF-7 encoding.\n        if self.folder_encode:\n            folders = [\n                (flags, delimiter, decode_utf7(name))\n                for flags, delimiter, name in folders\n            ]\n\n        # If folder names are integers, convert them back to strings.\n        folders = [\n            (flags, delimiter, str(name))\n            for flags, delimiter, name in folders\n            if not isinstance(name, int)\n        ]\n\n        return folders\n", "idx": 1648}
{"namespace": "imapclient.imapclient.IMAPClient.select_folder", "completion": "        if self.use_uid:\n            args = [b\"UID\"]\n        else:\n            args = []\n        args.extend([b\"SELECT\", self._normalise_folder(folder)])\n        if readonly:\n            args.append(b\"READ-ONLY\")\n        typ, data = self._imap._simple_command(*args)\n        self._checkok(\"select\", typ, data)\n        return self._process_select_response(self._imap.untagged_responses)\n", "idx": 1649}
{"namespace": "imapclient.imapclient.IMAPClient.unselect_folder", "completion": "        self._command_and_check(\"UNSELECT\")\n        return self._process_select_response(self._imap.untagged_responses)\n", "idx": 1650}
{"namespace": "imapclient.imapclient.IMAPClient.noop", "completion": "        return self._command_and_check(\"NOOP\", unpack=True)\n", "idx": 1651}
{"namespace": "imapclient.imapclient.IMAPClient.idle", "completion": "        if self._idle_tag is not None:\n            raise exceptions.IllegalStateError(\"IDLE already in progress\")\n\n        self._idle_tag = self._imap._command(\"IDLE\")\n        return self._consume_until_tagged_response(self._idle_tag, \"IDLE\")\n", "idx": 1652}
{"namespace": "imapclient.imapclient.IMAPClient.idle_check", "completion": "        if not self._idle_tag:\n            raise exceptions.IllegalStateError(\"idle_check() called before idle()\")\n\n        # Poll the socket for data\n        if self._poll_socket(self._imap.sock, timeout):\n            # If data is available, read it\n            data = self._imap.sock.recv(1024)\n            if not data:\n                raise exceptions.IMAPClientError(\"Connection closed while IDLEing\")\n\n            # Parse the data\n            self._imap._parse_data(data)\n\n            # Check for IDLE responses\n            return self._imap.untagged_responses.pop(self._idle_tag, [])\n\n        # If no data is available, return an empty list\n        return []", "idx": 1653}
{"namespace": "imapclient.imapclient.IMAPClient.idle_done", "completion": "        if self._idle_tag is None:\n            raise exceptions.IllegalStateError(\"idle() must be called first\")\n        tag = self._idle_tag\n        self._idle_tag = None\n        self._imap._command(\"DONE\")\n        return self._consume_until_tagged_response(tag, \"DONE\")\n", "idx": 1654}
{"namespace": "imapclient.imapclient.IMAPClient.folder_status", "completion": "        if what is None:\n            what = ['MESSAGES', 'RECENT', 'UIDNEXT', 'UIDVALIDITY', 'UNSEEN']\n        else:\n            what = [to_bytes(item) for item in what]\n        return self._command_and_check(\"STATUS\", self._normalise_folder(folder), seq_to_parenstr_upper(what), unpack=True)", "idx": 1655}
{"namespace": "imapclient.imapclient.IMAPClient.sort", "completion": "        if not isinstance(sort_criteria, list):\n            sort_criteria = [sort_criteria]\n        sort_criteria = [to_bytes(sort_criteria)]\n        if not isinstance(criteria, list):\n            criteria = [criteria]\n        criteria = [to_bytes(criteria)]\n        args = [to_bytes(charset)] + sort_criteria + criteria\n        data = self._raw_command_untagged(b\"SORT\", args)\n        return parse_message_list(data)\n", "idx": 1656}
{"namespace": "imapclient.imapclient.IMAPClient.thread", "completion": "        if not isinstance(algorithm, str):\n            raise TypeError(\"algorithm must be a string\")\n        if not isinstance(criteria, str):\n            raise TypeError(\"criteria must be a string\")\n        if not isinstance(charset, str):\n            raise TypeError(\"charset must be a string\")\n\n        args = [\n            to_bytes(algorithm),\n            to_bytes(charset),\n        ]\n        args.extend(_normalise_search_criteria(criteria, charset))\n        ids = self._raw_command_untagged(b\"THREAD\", args, unpack=True)\n        return [tuple(map(int, thread.split())) for thread in ids.split()]", "idx": 1657}
{"namespace": "imapclient.imapclient.IMAPClient.get_flags", "completion": "        if not messages:\n            return {}\n\n        data = self._command_and_check(\n            \"FETCH\", join_message_ids(messages), \"FLAGS\", uid=True\n        )\n        return self._filter_fetch_dict(data, b\"FLAGS\")", "idx": 1658}
{"namespace": "imapclient.imapclient.IMAPClient.get_gmail_labels", "completion": "        if not messages:\n            return {}\n\n        args = [\n            \"FETCH\",\n            join_message_ids(messages),\n            b\"X-GM-LABELS\",\n        ]\n        if self.use_uid:\n            args.insert(0, \"UID\")\n        tag = self._imap._command(*args)\n        typ, data = self._imap._command_complete(\"FETCH\", tag)\n        self._checkok(\"fetch\", typ, data)\n        typ, data = self._imap._untagged_response(typ, data, \"FETCH\")\n        return {\n            msgid: utf7_decode_sequence(labels)\n            for msgid, labels in parse_fetch_response(data, self.normalise_times, self.use_uid).items()\n        }\n", "idx": 1659}
{"namespace": "imapclient.imapclient.IMAPClient.append", "completion": "        if not msg:\n            raise ValueError(\"msg cannot be empty\")\n\n        if not folder:\n            raise ValueError(\"folder cannot be empty\")\n\n        if not isinstance(flags, tuple):\n            raise ValueError(\"flags must be a tuple\")\n\n        if not isinstance(msg_time, datetime):\n            raise ValueError(\"msg_time must be a datetime instance\")\n\n        if msg_time.tzinfo is None:\n            msg_time = msg_time.replace(tzinfo=timezone.utc)\n\n        if not isinstance(msg_time, datetime):\n            raise ValueError(\"msg_time must be a datetime instance\")\n\n        if not isinstance(msg, str):\n            raise ValueError(\"msg must be a string\")\n\n        if not isinstance(folder, str):\n            raise ValueError(\"folder must be a string\")\n\n        if not isinstance(flags, tuple):\n            raise ValueError(\"flags must be a tuple\")\n\n        if not isinstance(msg_time, datetime):\n            raise ValueError(\"msg_time must be a datetime instance\")\n\n        if not isinstance(msg, str):\n            raise ValueError(\"msg must be a string\")\n\n        if not isinstance(folder, str):\n            raise ValueError(\"folder must be a string\")\n\n        if not isinstance(flags, tuple):\n            raise ValueError(\"flags must be a tuple\")\n\n        if not isinstance(msg_time, datetime):\n            raise ValueError(\"msg_time must be a datetime instance\")\n\n        if not isinstance(msg, str):\n            raise ValueError(\"msg must be a string\")\n\n        if not isinstance(folder, str):\n            raise ValueError(\"folder must be a string\")\n\n        if not isinstance(flags, tuple):\n            raise ValueError(\"flags must be a tuple\")\n\n        if not isinstance(msg_time, datetime):\n            raise ValueError(\"msg_time must be a datetime instance\")\n\n        if not isinstance(msg, str):\n            raise ValueError(\"msg must be a string\")\n\n        if not isinstance(folder, str):\n            raise ValueError(\"folder must be a", "idx": 1660}
{"namespace": "imapclient.imapclient.IMAPClient.multiappend", "completion": "        if not msgs:\n            return {}\n\n        args = [\n            \"MULTIAPPEND\",\n            self._normalise_folder(folder),\n            seq_to_parenstr(msgs),\n        ]\n        if self.use_uid:\n            args.insert(0, \"UID\")\n        tag = self._imap._command(*args)\n        typ, data = self._imap._command_complete(\"APPEND\", tag)\n        self._checkok(\"append\", typ, data)\n        typ, data = self._imap._untagged_response(typ, data, \"APPEND\")\n        return parse_fetch_response(data, self.normalise_times, self.use_uid)", "idx": 1661}
{"namespace": "imapclient.imapclient.IMAPClient.expunge", "completion": "        if messages is None:\n            return self._command_and_check(\"expunge\", unpack=True)\n        else:\n            return self._command_and_check(\"expunge\", join_message_ids(messages))", "idx": 1662}
{"namespace": "imapclient.imapclient.IMAPClient.getacl", "completion": "        return self._command_and_check(\"getacl\", self._normalise_folder(folder))\n\n    ", "idx": 1663}
{"namespace": "imapclient.imapclient.IMAPClient.setacl", "completion": "        return self._command_and_check(\"setacl\", self._normalise_folder(folder), who, what)\n", "idx": 1664}
{"namespace": "imapclient.imapclient.IMAPClient.get_quota_root", "completion": "        # Get the quota roots for the given mailbox\n        quota_roots = self._get_quota_roots(mailbox)\n\n        # Get the quotas for each quota root\n        quotas = []\n        for quota_root in quota_roots:\n            quotas.extend(self._get_quota(quota_root.name))\n\n        return MailboxQuotaRoots(quota_roots), quotas\n", "idx": 1665}
{"namespace": "imapclient.imapclient.IMAPClient.set_quota", "completion": "        if not isinstance(quotas, list):\n            raise TypeError(\"quotas must be a list\")\n        if not all(isinstance(q, Quota) for q in quotas):\n            raise TypeError(\"quotas must be a list of Quota objects\")\n        args = []\n        for q in quotas:\n            args.append(q.resource)\n            args.append(q.limit)\n            args.append(q.usage)\n        return self._raw_command_untagged(b\"SETQUOTA\", args, unpack=True)\n", "idx": 1666}
{"namespace": "imapclient.imapclient.IMAPClient._consume_until_tagged_response", "completion": "        untagged_responses = []\n        while True:\n            resp = self._imap._get_response()\n            if resp is None:\n                raise exceptions.IMAPClientError(\"Unexpected EOF\")\n            if resp[0] == \"*\":\n                if resp[2:].startswith(tag):\n                    return resp[2:], untagged_responses\n                else:\n                    untagged_responses.append(resp[2:])\n            elif resp[0] == \"+\":\n                untagged_responses.append(resp[2:])\n            elif resp[0] == \" \":\n                untagged_responses.append(resp[2:])\n            elif resp[0] == \"!\":\n                raise exceptions.IMAPClientError(\n                    \"unexpected response: %s\" % resp[2:]\n                )\n            elif resp[0] == \"?\":\n                raise exceptions.IMAPClientError(\n                    \"unexpected response: %s\" % resp[2:]\n                )\n            elif resp[0] == \"B\":\n                raise exceptions.IMAPClientError(\n                    \"unexpected response: %s\" % resp[2:]\n                )\n            elif resp[0] == \"A\":\n                raise exceptions.IMAPClientError(\n                    \"unexpected response: %s\" % resp[2:]\n                )\n            elif resp[0] == \"C\":\n                raise exceptions.IMAPClientError(\n                    \"unexpected response: %s\" % resp[2:]\n                )\n            elif resp[0] == \"D\":\n                raise exceptions.IMAPClientError(\n                    \"unexpected response: %s\" % resp[2:]\n                )\n            elif resp[0] == \"E\":\n                raise exceptions.IMAPClientError(\n                    \"unexpected response: %s\" % resp[2:]\n                )\n            elif resp[0] == \"F\":\n                raise exceptions.IMAPClientError(\n                    \"unexpected response: %s\" % resp[2:]\n                )\n            else:\n                raise exceptions.IMAPClientError(\n                    \"unexpected response: %s\"", "idx": 1667}
{"namespace": "imapclient.imapclient._normalise_search_criteria", "completion": "    if isinstance(criteria, (str, bytes)):\n        criteria = [criteria]\n    if not criteria:\n        raise exceptions.InvalidCriteriaError(\"No criteria specified\")\n    if charset is None:\n        charset = \"us-ascii\"\n    if isinstance(criteria, (int, datetime.datetime, datetime.date)):\n        return [_quote(str(criteria))]\n    if isinstance(criteria, (list, tuple)):\n        return [_normalise_search_criteria(item, charset) for item in criteria]\n    if isinstance(criteria, bytes):\n        return [_literal(criteria)]\n    if isinstance(criteria, str):\n        return [_quoted.maybe(to_bytes(criteria, charset))]\n    raise exceptions.InvalidCriteriaError(\"Invalid criteria type: %s\" % type(criteria))", "idx": 1668}
{"namespace": "imapclient.response_lexer.TokenSource.current_literal", "completion": "        return self.lex.current_source.literal\n", "idx": 1669}
{"namespace": "imapclient.imap_utf7.decode", "completion": "    if not isinstance(s, (bytes, str)):\n        return s\n\n    res = bytearray()\n\n    b64_buffer: List[str] = []\n\n    def consume_b64_buffer(buf: List[str]) -> None:\n        \"\"\"\n        Consume the buffer by decoding it from a modified base 64 representation\n        and append it to the result\n        \"\"\"\n        if buf:\n            res.extend(base64_utf7_decode(buf))\n            del buf[:]\n\n    for c in s:\n        o = ord(c)\n\n        # ASCII case\n        if 0x20 <= o <= 0x7E:\n            consume_b64_buffer(b64_buffer)\n            res.append(o)\n\n        # Shift character case\n        elif o == AMPERSAND_ORD:\n            consume_b64_buffer(b64_buffer)\n            b64_buffer.append(\"&\")\n\n        # End of string case\n        elif o == DASH_ORD:\n            consume_b64_buffer(b64_buffer)\n            break\n\n        # Bufferize characters that will be decoded in base64 and append them later\n        # in the result, when iterating over ASCII character or the end of string\n        else:\n            b64_buffer.append(c)\n\n    # Consume the remaining buffer if the string finish with non-ASCII characters\n    consume_b64_buffer(b64_buffer)\n\n    return res.decode(\"utf-8\")", "idx": 1670}
{"namespace": "imapclient.fixed_offset.FixedOffset.for_system", "completion": "        tz = time.localtime()\n        if tz.tm_isdst:\n            offset = tz.tm_gmtoff + tz.tm_dst\n        else:\n            offset = tz.tm_gmtoff\n        return cls(offset / 60)", "idx": 1671}
{"namespace": "imapclient.datetime_util.parse_to_datetime", "completion": "    dt = parsedate_tz(_munge(timestamp))\n    if dt is None:\n        raise ValueError(\"Could not parse timestamp: %r\" % timestamp)\n    dt = datetime(*dt[:6])\n    if normalise:\n        return datetime_to_native(dt)\n    return dt", "idx": 1672}
{"namespace": "imapclient.datetime_util.datetime_to_INTERNALDATE", "completion": "    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=FixedOffset.for_system())\n    return dt.strftime(\"%a, %d %b %Y %H:%M:%S %z\")", "idx": 1673}
{"namespace": "imapclient.datetime_util.format_criteria_date", "completion": "    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=FixedOffset.for_system())\n    return dt.strftime(\"%d-%b-%Y\").encode(\"latin-1\")", "idx": 1674}
{"namespace": "imapclient.util.assert_imap_protocol", "completion": "    if not condition:\n        raise ProtocolError(\"Server replied with a response that violates the IMAP protocol\" + (f\" ({message})\" if message else \"\"))", "idx": 1675}
{"namespace": "ehforwarderbot.utils.get_config_path", "completion": "    profile = coordinator.profile\n    if module_id is None:\n        module_id = profile\n    config_path = get_base_path() / 'profiles' / profile / module_id / 'config.%s' % ext\n    if not config_path.exists():\n        config_path.parent.mkdir(parents=True)\n        config_path.touch()\n    return config_path", "idx": 1676}
{"namespace": "ehforwarderbot.utils.get_custom_modules_path", "completion": "    base_path = get_base_path()\n    channel_path = base_path / \"modules\"\n    if not channel_path.exists():\n        channel_path.mkdir(parents=True)\n    return channel_path", "idx": 1677}
{"namespace": "ehforwarderbot.chat.Chat.add_member", "completion": "        member = ChatMember(self, name=name, alias=alias, id=id, uid=uid,\n                            vendor_specific=vendor_specific, description=description,\n                            middleware=middleware)\n        self.members.append(member)\n        return member", "idx": 1678}
{"namespace": "ehforwarderbot.chat.Chat.add_system_member", "completion": "        return self.add_member(name=name, alias=alias, uid=uid, id=id,\n                               vendor_specific=vendor_specific, description=description,\n                               middleware=middleware)", "idx": 1679}
{"namespace": "ehforwarderbot.chat.PrivateChat.verify", "completion": "        super().verify()\n        assert all(isinstance(member, ChatMember) for member in self.members)", "idx": 1680}
{"namespace": "telethon.utils.get_inner_text", "completion": "    if not entities:\n        return text\n\n    result = []\n    for entity in entities:\n        if entity.offset > len(text):\n            continue\n        if entity.offset + entity.length > len(text):\n            continue\n        result.append(text[entity.offset:entity.offset + entity.length])\n    return result\n\n", "idx": 1681}
{"namespace": "telethon.extensions.html.parse", "completion": "    parser = HTMLToTelegramParser()\n    parser.feed(html)\n    parser.close()\n    return parser.text, parser.entities", "idx": 1682}
{"namespace": "telethon.extensions.html.unparse", "completion": "    from ..helpers import add_surrogate, del_surrogate\n    if not text:\n        return text\n\n    entities = sorted(entities, key=lambda e: e.offset)\n    text = add_surrogate(text)\n    result = []\n    last_offset = 0\n    for entity in entities:\n        if entity.offset > last_offset:\n            result.append(text[last_offset:entity.offset])\n        try:\n            formatter = ENTITY_TO_FORMATTER[type(entity)]\n        except KeyError:\n            continue\n        if callable(formatter):\n            result.append(formatter(entity, text))\n        else:\n            result.append(formatter[0])\n            result.append(text[entity.offset:entity.offset + entity.length])\n            result.append(formatter[1])\n        last_offset = entity.offset + entity.length\n    if last_offset < len(text):\n        result.append(text[last_offset:])\n    return del_surrogate(''.join(result))", "idx": 1683}
{"namespace": "telethon.crypto.rsa.encrypt", "completion": "    global _server_keys\n    if fingerprint not in _server_keys:\n        return None\n    key, old = _server_keys[fingerprint]\n    if old and not use_old:\n        return None\n    data = TLObject.serialize_bytes(data)\n    # Append SHA1 hash of the data\n    data += sha1(data).digest()\n    # Append the data itself\n    data += data\n    # Append padding\n    data += b'\\x00' * (key.size_in_bytes() - len(data))\n    return rsa.encrypt(data, key)", "idx": 1684}
{"namespace": "hbmqtt.codecs.encode_string", "completion": "    data = string.encode('utf-8')\n    data_length = len(data)\n    return int_to_bytes(data_length, 2) + data", "idx": 1685}
{"namespace": "hbmqtt.plugins.manager.PluginManager.get_plugin", "completion": "        for plugin in self._plugins:\n            if plugin.name == name:\n                return plugin\n        return None\n", "idx": 1686}
{"namespace": "pysimplesoap.simplexml.SimpleXMLElement.add_child", "completion": "        if ns is True:\n            ns = self.__ns\n        if ns is False or ns is None:\n            ns = None\n        if isinstance(ns, basestring):\n            ns = self.get_namespace_uri(ns)\n        if ns:\n            name = \"%s:%s\" % (self.__namespaces_map.get(self.__prefix, self.__prefix), name)\n        if text is None:\n            element = self.__document.createElementNS(ns, name)\n        elif isinstance(text, xml.dom.minidom.CDATASection):\n            element = self.__document.createCDATASection(text)\n        else:\n            element = self.__document.createTextNode(text)\n        self._element.appendChild(element)\n        return SimpleXMLElement(\n            elements=[element],\n            document=self.__document,\n            namespace=self.__ns,\n            prefix=self.__prefix,\n            jetty=self.__jetty,\n            namespaces_map=self.__namespaces_map\n        )\n", "idx": 1687}
{"namespace": "pysimplesoap.simplexml.SimpleXMLElement.as_xml", "completion": "        if filename:\n            with open(filename, 'w') as f:\n                f.write(self.as_xml(pretty=pretty))\n        else:\n            if pretty:\n                return self._element.toprettyxml(indent=\"  \", encoding=\"UTF-8\")\n            else:\n                return self._element.toxml(encoding=\"UTF-8\")", "idx": 1688}
{"namespace": "twilio.base.deserialize.iso8601_date", "completion": "    try:\n        return datetime.datetime.strptime(s, ISO8601_DATE_FORMAT).date()\n    except (TypeError, ValueError):\n        return s", "idx": 1689}
{"namespace": "twilio.base.deserialize.iso8601_datetime", "completion": "    try:\n        return datetime.datetime.strptime(s, ISO8601_DATETIME_FORMAT).replace(\n            tzinfo=datetime.timezone.utc\n        )\n    except (TypeError, ValueError):\n        return s", "idx": 1690}
{"namespace": "twilio.base.serialize.iso8601_date", "completion": "    if d == values.unset:\n        return d\n    elif isinstance(d, datetime.datetime) or isinstance(d, datetime.date):\n        return d.strftime(\"%Y-%m-%d\")\n    elif isinstance(d, str):\n        return d", "idx": 1691}
{"namespace": "twilio.base.serialize.iso8601_datetime", "completion": "    if d == values.unset:\n        return d\n    elif isinstance(d, datetime.datetime):\n        return str(d)\n    elif isinstance(d, datetime.date):\n        return str(d)\n    elif isinstance(d, str):\n        return d", "idx": 1692}
{"namespace": "twilio.base.serialize.prefixed_collapsible_map", "completion": "    return {prefix + k: v for k, v in m.items()}", "idx": 1693}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.dial", "completion": "        return self.nest(\n            Dial(\n                number=number,\n                action=action,\n                method=method,\n                timeout=timeout,\n                hangup_on_star=hangup_on_star,\n                time_limit=time_limit,\n                caller_id=caller_id,\n                record=record,\n                trim=trim,\n                recording_status_callback=recording_status_callback,\n                recording_status_callback_method=recording_status_callback_method,\n                recording_status_callback_event=recording_status_callback_event,\n                answer_on_bridge=answer_on_bridge,\n                ring_tone=ring_tone,\n                recording_track=recording_track,\n                sequential=sequential,\n                refer_url=refer_url,\n                refer_method=refer_method,\n                **kwargs\n            )\n        )\n", "idx": 1694}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.enqueue", "completion": "        return self.nest(\n            Enqueue(\n                name=name,\n                action=action,\n                max_queue_size=max_queue_size,\n                method=method,\n                wait_url=wait_url,\n                wait_url_method=wait_url_method,\n                workflow_sid=workflow_sid,\n                **kwargs\n            )\n        )\n", "idx": 1695}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.gather", "completion": "        return self.nest(\n            Gather(\n                input=input,\n                action=action,\n                method=method,\n                timeout=timeout,\n                speech_timeout=speech_timeout,\n                max_speech_time=max_speech_time,\n                profanity_filter=profanity_filter,\n                finish_on_key=finish_on_key,\n                num_digits=num_digits,\n                partial_result_callback=partial_result_callback,\n                partial_result_callback_method=partial_result_callback_method,\n                language=language,\n                hints=hints,\n                barge_in=barge_in,\n                debug=debug,\n                action_on_empty_result=action_on_empty_result,\n                speech_model=speech_model,\n                enhanced=enhanced,\n                **kwargs\n            )\n        )\n", "idx": 1696}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.say", "completion": "        return self.nest(Say(message=message, voice=voice, loop=loop, language=language, **kwargs))\n\n    ", "idx": 1697}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.sms", "completion": "        return self.nest(Sms(message, to=to, from_=from_, action=action, method=method, status_callback=status_callback, **kwargs))\n", "idx": 1698}
{"namespace": "twilio.twiml.voice_response.Gather.say", "completion": "        return self.nest(Say(message, voice=voice, loop=loop, language=language, **kwargs))\n", "idx": 1699}
{"namespace": "twilio.twiml.voice_response.Dial.client", "completion": "        return self.nest(\n            Client(\n                identity=identity,\n                url=url,\n                method=method,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                **kwargs\n            )\n        )\n", "idx": 1700}
{"namespace": "twilio.twiml.voice_response.Dial.conference", "completion": "        return self.nest(\n            Conference(\n                name,\n                muted=muted,\n                beep=beep,\n                start_conference_on_enter=start_conference_on_enter,\n                end_conference_on_exit=end_conference_on_exit,\n                wait_url=wait_url,\n                wait_method=wait_method,\n                max_participants=max_participants,\n                record=record,\n                region=region,\n                coach=coach,\n                trim=trim,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                recording_status_callback=recording_status_callback,\n                recording_status_callback_method=recording_status_callback_method,\n                recording_status_callback_event=recording_status_callback_event,\n                event_callback_url=event_callback_url,\n                jitter_buffer_size=jitter_buffer_size,\n                participant_label=participant_label,\n                **kwargs\n            )\n        )\n", "idx": 1701}
{"namespace": "twilio.twiml.voice_response.Dial.queue", "completion": "        return self.nest(\n            Queue(\n                name,\n                url=url,\n                method=method,\n                reservation_sid=reservation_sid,\n                post_work_activity_sid=post_work_activity_sid,\n                **kwargs\n            )\n        )", "idx": 1702}
{"namespace": "twilio.twiml.voice_response.Dial.sip", "completion": "        return self.nest(\n            Sip(\n                sip_url,\n                username=username,\n                password=password,\n                url=url,\n                method=method,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                machine_detection=machine_detection,\n                amd_status_callback_method=amd_status_callback_method,\n                amd_status_callback=amd_status_callback,\n                machine_detection_timeout=machine_detection_timeout,\n                machine_detection_speech_threshold=machine_detection_speech_threshold,\n                machine_detection_speech_end_threshold=machine_detection_speech_end_threshold,\n                machine_detection_silence_timeout=machine_detection_silence_timeout,\n                **kwargs\n            )\n        )\n\n", "idx": 1703}
{"namespace": "twilio.twiml.messaging_response.MessagingResponse.message", "completion": "        return self.nest(Message(body, to=to, from_=from_, **kwargs))\n\n", "idx": 1704}
{"namespace": "twilio.twiml.TwiML.append", "completion": "        if not isinstance(verb, TwiML) and not isinstance(verb, str):\n            raise TwiMLException(\"Only nesting of TwiML and strings are allowed\")\n\n        self.verbs.append(verb)\n        return self\n\n    ", "idx": 1705}
{"namespace": "twilio.jwt.Jwt.to_jwt", "completion": "        if not self.secret_key:\n            raise ValueError(\"No secret key configured for JWT.\")\n\n        payload = self.payload.copy()\n        headers = self.headers.copy()\n\n        if ttl:\n            payload[\"exp\"] = int(time.time()) + ttl\n\n        return jwt_lib.encode(payload, self.secret_key, algorithm=self.algorithm, headers=headers)", "idx": 1706}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_client_outgoing", "completion": "        scope = ScopeURI(\n            \"client\", \"outgoing\", {\"applicationSid\": application_sid, **kwargs}\n        )\n        self.capabilities[\"outgoing\"] = scope", "idx": 1707}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_client_incoming", "completion": "        self.client_name = client_name\n        self.capabilities[\"incoming\"] = ScopeURI(\"client\", \"incoming\")", "idx": 1708}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_event_stream", "completion": "        self.capabilities[\"events\"] = ScopeURI(\"events\", \"subscribe\", kwargs)", "idx": 1709}
{"namespace": "twilio.jwt.client.ClientCapabilityToken._generate_payload", "completion": "        if \"outgoing\" in self.capabilities and self.client_name:\n            self.capabilities[\"outgoing\"].add_param(\"clientName\", self.client_name)\n\n        payload_values = [\n            self.capabilities[capability].to_payload()\n            for capability in self.capabilities\n        ]\n\n        return {\"scope\": \" \".join(payload_values)}", "idx": 1710}
{"namespace": "twilio.jwt.client.ScopeURI.to_payload", "completion": "        if self.params:\n            sorted_params = sorted(self.params.items())\n            param_string = \"?\" + urlencode(sorted_params, doseq=True)\n        else:\n            param_string = \"\"\n\n        return \"scope:{service}:{privilege}{param_string}\".format(\n            service=self.service, privilege=self.privilege, param_string=param_string\n        )", "idx": 1711}
{"namespace": "twilio.jwt.access_token.AccessToken.add_grant", "completion": "        if not isinstance(grant, AccessTokenGrant):\n            raise ValueError(\"Grant must be an instance of AccessTokenGrant.\")\n        self.grants.append(grant)\n\n    ", "idx": 1712}
{"namespace": "twilio.jwt.taskrouter.capabilities.WorkerCapabilityToken.allow_update_activities", "completion": "        self._make_policy(self.workspace_url + \"/Activities\", \"POST\", True)", "idx": 1713}
{"namespace": "zulipterminal.platform_code.successful_GUI_return_code", "completion": "    if PLATFORM == \"WSL\":\n        return 1\n    else:\n        return 0", "idx": 1714}
{"namespace": "zulipterminal.platform_code.normalized_file_path", "completion": "    if PLATFORM == \"WSL\":\n        return path.replace(\"/\", \"\\\\\")\n    else:\n        return path", "idx": 1715}
{"namespace": "zulipterminal.helper.canonicalize_color", "completion": "    if match(r\"^#[0-9a-fA-F]{3}$\", color):\n        return color.lower()\n    elif match(r\"^#[0-9a-fA-F]{6}$\", color):\n        return \"#\" + color[1:].lower()\n    else:\n        return color\n\n", "idx": 1716}
{"namespace": "zulipterminal.helper.get_unused_fence", "completion": "    from zulipterminal.config.regexes import REGEX_FENCE\n\n    fence_length = len(max(findall(REGEX_FENCE, content), key=len)) + 1\n    return \"`\" * fence_length", "idx": 1717}
{"namespace": "zulipterminal.helper.open_media", "completion": "    from zulipterminal.platform_code import normalized_file_path\n\n    media_path = normalized_file_path(media_path)\n    command = [tool, media_path]\n    try:\n        subprocess.run(command, check=True)\n    except subprocess.CalledProcessError as e:\n        controller.report_error(\n            [\n                \"Error opening media file. Please check if the file exists and if you have the required permissions to open it.\",\n                f\"Error code: {e.returncode}\",\n            ]\n        )\n    except FileNotFoundError:\n        controller.report_error(\n            [\n                \"Error opening media file. Please check if the file exists and if you have the required permissions to open it.\",\n                f\"Error code: {e.returncode}\",\n            ]\n        )\n    except Exception as e:\n        controller.report_error(\n            [\n                \"Error opening media file. Please check if the file exists and if you have the required permissions to open it.\",\n                f\"Error code: {e.returncode}\",\n            ]\n        )", "idx": 1718}
{"namespace": "zulipterminal.server_url.encode_stream", "completion": "    stream_name = stream_name.replace(\" \", \"-\")\n    encoded_stream = hash_util_encode(stream_name)\n    return f\"{stream_id}-{encoded_stream}\"", "idx": 1719}
{"namespace": "zulipterminal.server_url.near_message_url", "completion": "    if message[\"type\"] == \"stream\":\n        return near_stream_message_url(server_url, message)\n    elif message[\"type\"] == \"private\":\n        return near_pm_message_url(server_url, message)\n    else:\n        raise ValueError(\"Invalid message type\")", "idx": 1720}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.update_recipients", "completion": "        # FIXME: This function is not used.\n        # FIXME: This function is not tested.\n        # FIXME: This function is not documented.\n        # FIXME: This function is not type-checked.\n        # FIXME: This function is not formatted.\n        # FIXME: This function is not linted.\n        # FIXME: This function is not pep8.\n        # FIXME: This function is not mypy.\n        # FIXME: This function is not pylint.\n        # FIXME: This function is not flake8.\n        # FIXME: This function is not bandit.\n        # FIXME: This function is not safety.\n        # FIXME: This function is not safety.\n        # FIXME: This function is not safety.\n        # FIXME: This function is not safety.\n        # FIXME: This function is not safety.\n        # FIXME: This function is not safety.\n        # FIXME: This function is not safety.\n        # FIXME: This function is not safety.\n        # FIXME: This function is not safety.\n        # FIXME: This function is not safety.\n        # FIXME: This function is not safety.\n        # FIXME: This function is not safety.\n        # FIXME: This function is not safety.\n        # FIXME: This function is not safety.\n        # FIXME: This function is not safety.\n        # FIXME: This function is not safety.\n        # FIXME: This function is not safety.\n        # FIXME: This function is not safety.\n        # FIXME: This function is not safety.\n        # FIXME: This function is not safety.\n        # FIXME: This function is not safety.\n        # FIXME: This function is not safety.\n        # FIXME: This function is not safety.\n        # FIXME: This function is not safety.\n        # FIXME: This function is not safety.\n        # FIXME: This function is not safety.\n        # FIXME: This", "idx": 1721}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.stream_box_view", "completion": "        self.stream_write_box = urwid.Text(caption)\n        self._setup_common_stream_compose(stream_id, caption, title)\n\n        self.edit_mode_button = EditModeButton(\n            controller=self.model.controller,\n            width=20,\n        )\n        self.header_write_box.widget_list.append(self.edit_mode_button)\n\n        # Use callback to set stream marker - it shouldn't change, so don't need signal\n        self._set_stream_write_box_style(None, caption)\n\n        # FIXME: Refactor when we have ~ Model.is_private_stream\n        from zulipterminal.config.ui_mappings import STREAM_ACCESS_TYPE\n        stream_marker = INVALID_MARKER\n        color = \"general_bar\"\n        if self.model.is_valid_stream(caption):\n            stream_id = self.model.stream_id_from_name(caption)\n            stream_access_type = self.model.stream_access_type(stream_id)\n            stream_marker = STREAM_ACCESS_TYPE[stream_access_type][\"icon\"]\n            stream = self.model.stream_dict[stream_id]\n            color = stream[\"color\"]\n        self.header_write_box[self.FOCUS_HEADER_PREFIX_STREAM].set_text(\n            (color, stream_marker)\n        )\n\n        # FIXME: Refactor when we have ~ Model.is_private_stream\n        from zulipterminal.config.ui_mappings import STREAM_ACCESS_TYPE\n        stream_marker = INVALID_MARKER\n        color = \"general_bar\"\n        if self.model.is_valid_stream(caption):\n            stream_id = self.model.stream_id_from_name(caption)\n            stream_access_type = self.model.stream_access_type(stream_id)\n            stream_marker = STREAM_ACCESS_TYPE[stream_access_type]", "idx": 1722}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.stream_box_edit_view", "completion": "        self._setup_common_stream_compose(stream_id, caption, title)\n\n        self.edit_mode_button = EditModeButton(\n            self.model.controller,\n            self.msg_write_box,\n            self.msg_edit_state,\n            self.msg_body_edit_enabled,\n        )\n        self.header_write_box.contents.insert(\n            self.FOCUS_HEADER_PREFIX_STREAM,\n            (self.edit_mode_button, self.options()),\n        )\n\n        # Use and set a callback to set the stream marker\n        self._set_stream_write_box_style(None, caption)\n        urwid.connect_signal(\n            self.stream_write_box, \"change\", self._set_stream_write_box_style\n        )\n\n        # Add edit mode button to header write box\n        self.header_write_box.contents.insert(\n            self.FOCUS_HEADER_BOX_EDIT,\n            (self.edit_mode_button, self.options()),\n        )\n\n        # Set the focus to the edit mode button\n        self.focus_position = self.FOCUS_CONTAINER_HEADER\n        self.header_write_box.focus_col = self.FOCUS_HEADER_BOX_EDIT\n\n        # Set the stream marker\n        self.header_write_box[self.FOCUS_HEADER_PREFIX_STREAM].set_text(\n            (self.model.stream_dict[stream_id][\"color\"], \"?\")\n        )\n\n        # Set the topic\n        self.title_write_box.edit_text = title\n        self.title_write_box.edit_pos = len(self.title_write_box.edit_text)\n\n        # Set the message\n        self.msg_write_box.edit_text = caption\n        self.msg_write_box.edit_pos = len(self.msg_write_box.edit_text)\n\n    ", "idx": 1723}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._set_stream_write_box_style", "completion": "        stream_name = widget.edit_text\n        if self.model.is_valid_stream(stream_name):\n            stream_id = self.model.stream_id_from_name(stream_name)\n            stream_color = self.model.stream_dict[stream_id][\"color\"]\n            stream_color = f\"s{stream_color}\"\n            stream_marker = f\"s{stream_color}\"\n            self.header_write_box.set_widget_attr(\n                self.FOCUS_HEADER_BOX_STREAM, \"stream_marker\", stream_marker\n            )\n            self.header_write_box.set_widget_attr(\n                self.FOCUS_HEADER_BOX_STREAM, \"stream_color\", stream_color\n            )\n            self.header_write_box.set_widget_attr(\n                self.FOCUS_HEADER_BOX_STREAM, \"stream_name\", stream_name\n            )\n        else:\n            self.header_write_box.set_widget_attr(\n                self.FOCUS_HEADER_BOX_STREAM, \"stream_marker\", \"s\"\n            )\n            self.header_write_box.set_widget_attr(\n                self.FOCUS_HEADER_BOX_STREAM, \"stream_color\", \"s\"\n            )\n            self.header_write_box.set_widget_attr(\n                self.FOCUS_HEADER_BOX_STREAM, \"stream_name\", stream_name\n            )\n\n        if new_text:\n            self.title_write_box.set_edit_text(new_text)\n", "idx": 1724}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._to_box_autocomplete", "completion": "        if state is None:\n            state = 0\n\n        if text:\n            user_names = [\n                user[\"full_name\"]\n                for user in self.view.users\n                if match_user(user[\"full_name\"], text)\n            ]\n            user_names_counter = Counter(user_names)\n            user_names_with_distinct_duplicates = [\n                f\"{user_name}|{user['user_id']}\"\n                if user_names_counter[user_name] > 1\n                else user_name\n                for user_name in user_names\n            ]\n            typeahead = format_string(\n                user_names_with_distinct_duplicates, \"@{}**\"\n            )\n        else:\n            typeahead = []\n\n        # NOTE: The following block only executes if any of the autocomplete\n        # prefixes exist.\n        typeahead = self._process_typeaheads(typeahead, state, user_names)\n        if typeahead:\n            typeahead = \"@\" + typeahead\n        return typeahead\n", "idx": 1725}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._topic_box_autocomplete", "completion": "        topic_names = self.model.topics_in_stream(self.stream_id)\n        topic_typeaheads, topic_suggestions = match_topics(topic_names, text)\n\n        return self._process_typeaheads(topic_typeaheads, state, topic_suggestions)", "idx": 1726}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._stream_box_autocomplete", "completion": "        streams_list = self.view.pinned_streams + self.view.unpinned_streams\n        streams = [stream[\"name\"] for stream in streams_list]\n        stream_typeahead = format_string(streams, \"#**{}**\")\n        stream_data = list(zip(stream_typeahead, streams))\n\n        prefix_length = len(prefix_string)\n\n        matched_data = match_stream(\n            stream_data, text[prefix_length:], self.view.pinned_streams\n        )\n        return matched_data\n", "idx": 1727}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.generic_autocomplete", "completion": "        # FIXME: Refactor when we have ~ Model.is_private_stream\n        from zulipterminal.config.ui_mappings import STREAM_ACCESS_TYPE\n        text = text.strip()\n        prefix_string = \"\"\n        prefix_indices = {}\n        autocomplete_map = OrderedDict()\n\n        if text.startswith(\"@\"):\n            prefix_string = \"@\"\n            autocomplete_map.update(\n                {\n                    prefix_string: self.autocomplete_mentions,\n                }\n            )\n        elif text.startswith(\":\"):\n            prefix_string = \":\"\n            autocomplete_map.update(\n                {\n                    prefix_string: self.autocomplete_emojis,\n                }\n            )\n        elif text.startswith(\"#\"):\n            prefix_string = \"#\"\n            autocomplete_map.update(\n                {\n                    prefix_string: self.autocomplete_streams,\n                }\n            )\n        elif text.startswith(\"**\"):\n            prefix_string = \"**\"\n            autocomplete_map.update(\n                {\n                    prefix_string: self.autocomplete_streams,\n                }\n            )\n        elif text.startswith(\"@**\"):\n            prefix_string = \"@**\"\n            autocomplete_map.update(\n                {\n                    prefix_string: self.autocomplete_streams,\n                }\n            )\n        elif text.startswith(\"@**\"):\n            prefix_string = \"@**\"\n            autocomplete_map.update(\n                {\n                    prefix_string: self.autocomplete_streams,\n                }\n            )\n        elif text.startswith(\"@**\"):\n            prefix_string = \"@**\"\n            autocomplete_map.update(\n                {\n                    prefix_string: self.autocomplete_streams,\n                }\n            )\n\n        return self._process_typeaheads(\n            autocom", "idx": 1728}
{"namespace": "zulipterminal.ui_tools.boxes.PanelSearchBox.reset_search_text", "completion": "        self.caption = self.search_text\n        self.set_edit_text(\"\")\n\n    ", "idx": 1729}
{"namespace": "zulipterminal.ui_tools.boxes.PanelSearchBox.valid_char", "completion": "        if self.get_edit_text() != \"\":\n            return super().valid_char(ch)\n        else:\n            if ch == \" \":\n                return False\n            return ch.isprintable()", "idx": 1730}
{"namespace": "zulipterminal.ui_tools.utils.is_muted", "completion": "    if msg[\"type\"] == \"private\":\n        return False\n    if model.narrow and model.narrow[0][\"oper\"] == \"stream\":\n        return False\n    if model.narrow and model.narrow[0][\"oper\"] == \"topic\":\n        return False\n    if model.is_stream_muted(msg[\"stream_id\"]):\n        return True\n    if model.is_topic_muted(msg[\"stream_id\"], msg[\"subject\"]):\n        return True\n    return False", "idx": 1731}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.update_count", "completion": "        if text_color is None:\n            text_color = self.original_color\n        if count > 0:\n            count_text = str(count)\n        else:\n            count_text = \"\"\n        self.update_widget((count_text, count_style), text_color)\n", "idx": 1732}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.update_widget", "completion": "        if count_text[0] is None:\n            self.button_prefix.set_text(\"\")\n        else:\n            self.button_prefix.set_text(\n                (count_text[0], f\"{self.prefix_character} \")\n            )\n\n        self.button_suffix.set_text(count_text[1])\n        self._label.set_text(self._caption)\n        self._label.set_text_color(text_color)\n", "idx": 1733}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.keypress", "completion": "        if key == \"enter\":\n            self.activate(key)\n            return None\n        return super().keypress(size, key)", "idx": 1734}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._parse_narrow_link", "completion": "        from zulipterminal.config.regexes import REGEX_INTERNAL_LINK_NARROW\n        match = re.match(REGEX_INTERNAL_LINK_NARROW, link)\n        if not match:\n            return ParsedNarrowLink()\n\n        # Extract the narrow parameters.\n        narrow = match.group(\"narrow\")\n        stream_data = match.group(\"stream_data\")\n        topic_name = match.group(\"topic_name\")\n        message_id = match.group(\"message_id\")\n\n        # Decode the stream data.\n        stream = cls._decode_stream_data(stream_data)\n\n        # Decode the message ID.\n        message_id = cls._decode_message_id(message_id)\n\n        return ParsedNarrowLink(\n            narrow=narrow,\n            stream=stream,\n            topic_name=topic_name,\n            message_id=message_id,\n        )\n\n    ", "idx": 1735}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._validate_and_patch_stream_data", "completion": "        stream_id = parsed_link[\"stream\"][\"stream_id\"]\n        stream_name = parsed_link[\"stream\"][\"stream_name\"]\n\n        if stream_id is None and stream_name is None:\n            return \"Invalid stream data\"\n\n        if stream_id is not None:\n            if stream_id not in self.model.stream_dict:\n                return \"Invalid stream ID\"\n            stream_name = self.model.stream_dict[stream_id][\"name\"]\n\n        if stream_name is not None:\n            if stream_name not in self.model.stream_dict:\n                return \"Invalid stream name\"\n            stream_id = self.model.stream_dict[stream_name][\"id\"]\n\n        parsed_link[\"stream\"][\"stream_id\"] = stream_id\n        parsed_link[\"stream\"][\"stream_name\"] = stream_name\n\n        if stream_id not in self.model.subscribed_stream_ids:\n            return \"You are not subscribed to this stream\"\n\n        return \"\"", "idx": 1736}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._validate_narrow_link", "completion": "        model = self.model\n        stream_id = parsed_link[\"stream\"][\"stream_id\"]\n        stream_name = parsed_link[\"stream\"][\"stream_name\"]\n        topic_name = parsed_link[\"topic_name\"]\n        message_id = parsed_link[\"message_id\"]\n        narrow = parsed_link[\"narrow\"]\n\n        if \"stream\" == narrow:\n            if not stream_id and not stream_name:\n                return \"Stream ID or name is missing\"\n            elif stream_id and not model.is_user_subscribed_to_stream(stream_id):\n                return \"The stream seems to be either unknown or unsubscribed\"\n            elif stream_name and not model.is_valid_stream(stream_name):\n                return \"The stream seems to be unknown\"\n        elif \"stream:topic\" == narrow:\n            if not stream_id and not stream_name:\n                return \"Stream ID or name is missing\"\n            elif stream_id and not model.is_user_subscribed_to_stream(stream_id):\n                return \"The stream seems to be either unknown or unsubscribed\"\n            elif stream_name and not model.is_valid_stream(stream_name):\n                return \"The stream seems to be unknown\"\n            elif not topic_name:\n                return \"Topic name is missing\"\n            elif not model.is_valid_topic(stream_id, topic_name):\n                return \"The topic seems to be unknown\"\n        elif \"stream:near\" == narrow:\n            if not stream_id and not stream_name:\n                return \"Stream ID or name is missing\"\n            elif stream_id and not model.is_user_subscribed_to_stream(stream_id):\n                return \"The stream seems to be either unknown or unsubscribed\"\n            elif stream_name and not model.is_valid_stream(stream_name):\n                return \"The stream seems to be unknown\"\n            elif not message_id:\n                return \"Message ID is missing\"\n            elif not model.is_valid_message_id(message_id):\n                return \"The message seems to be unknown", "idx": 1737}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._switch_narrow_to", "completion": "        if parsed_link[\"narrow\"] == \"stream\":\n            self.controller.narrow_to_stream(\n                stream_id=parsed_link[\"stream\"][\"stream_id\"],\n                stream_name=parsed_link[\"stream\"][\"stream_name\"],\n            )\n        elif parsed_link[\"narrow\"] == \"stream:topic\":\n            self.controller.narrow_to_topic(\n                stream_name=parsed_link[\"stream\"][\"stream_name\"],\n                topic_name=parsed_link[\"topic_name\"],\n            )\n        elif parsed_link[\"narrow\"] == \"stream:near\":\n            self.controller.narrow_to_near(\n                stream_id=parsed_link[\"stream\"][\"stream_id\"],\n                stream_name=parsed_link[\"stream\"][\"stream_name\"],\n                message_id=parsed_link[\"message_id\"],\n            )\n        elif parsed_link[\"narrow\"] == \"stream:topic:near\":\n            self.controller.narrow_to_near(\n                stream_id=parsed_link[\"stream\"][\"stream_id\"],\n                stream_name=parsed_link[\"stream\"][\"stream_name\"],\n                topic_name=parsed_link[\"topic_name\"],\n                message_id=parsed_link[\"message_id\"],\n            )", "idx": 1738}
{"namespace": "zulipterminal.config.themes.complete_and_incomplete_themes", "completion": "    complete_themes = []\n    incomplete_themes = []\n    for theme_name, theme in THEMES.items():\n        try:\n            theme_styles = theme.STYLES\n            theme_meta = theme.META\n        except AttributeError:\n            incomplete_themes.append(theme_name)\n            continue\n\n        if not all(\n            style_name in theme_styles for style_name in REQUIRED_STYLES.keys()\n        ):\n            incomplete_themes.append(theme_name)\n            continue\n\n        if not all(\n            meta_name in theme_meta for meta_name in REQUIRED_META.keys()\n        ):\n            incomplete_themes.append(theme_name)\n            continue\n\n        complete_themes.append(theme_name)\n\n    return sorted(complete_themes), sorted(incomplete_themes)\n\n", "idx": 1739}
{"namespace": "zulipterminal.config.themes.validate_colors", "completion": "    theme_styles = THEMES[theme_name].STYLES\n    for style_name, (fg, bg) in theme_styles.items():\n        fg_code16, fg_code256, fg_code24, *fg_props = fg.value.split()\n        bg_code16, bg_code256, bg_code24, *bg_props = bg.value.split()\n\n        if color_depth == 1:\n            if fg_code16 not in valid_16_color_codes:\n                raise InvalidThemeColorCode(\n                    f\"Invalid color code {fg_code16} for style {style_name} in theme {theme_name}\"\n                )\n            if bg_code16 not in valid_16_color_codes:\n                raise InvalidThemeColorCode(\n                    f\"Invalid color code {bg_code16} for style {style_name} in theme {theme_name}\"\n                )\n\n        elif color_depth == 16:\n            if fg_code16 not in valid_16_color_codes:\n                raise InvalidThemeColorCode(\n                    f\"Invalid color code {fg_code16} for style {style_name} in theme {theme_name}\"\n                )\n            if bg_code16 not in valid_16_color_codes:\n                raise InvalidThemeColorCode(\n                    f\"Invalid color code {bg_code16} for style {style_name} in theme {theme_name}\"\n                )\n\n        elif color_depth == 256:\n            if fg_code256 not in valid_16_color_codes:\n                raise InvalidThemeColorCode(\n                    f\"Invalid color code {fg_code256} for style {style_name} in theme {theme_name}\"\n                )\n            if bg_code256 not in valid_16_color_codes:\n                raise InvalidThemeColorCode(\n                    f\"Invalid color code {bg_code256} for style {style_name} in theme {theme_name}\"\n                )", "idx": 1740}
{"namespace": "zulipterminal.config.themes.parse_themefile", "completion": "    from zulipterminal.config.color import term16\n\n    urwid_theme = []\n    for style_name, (fg, bg) in theme_styles.items():\n        if color_depth == 1:\n            fg = term16.styles[fg]\n            bg = term16.background_color\n        elif color_depth == 16:\n            fg = term16.styles[fg]\n            bg = term16.styles[bg]\n        elif color_depth == 256:\n            fg = fg\n            bg = bg\n        elif color_depth == 2**24:\n            fg = fg\n            bg = bg\n        else:\n            raise ValueError(f\"Invalid color depth: {color_depth}\")\n        urwid_theme.append((style_name, fg, bg))\n    return urwid_theme", "idx": 1741}
{"namespace": "zulipterminal.config.themes.add_pygments_style", "completion": "    \"\"\"\n    This function adds Pygments styles for syntax highlighting of code blocks and inline code. It takes the theme metadata and the Urwid theme as input, and modifies the Pygments styles and Urwid theme accordingly.\n    Input-Output Arguments\n    :param theme_meta: Dict. The theme metadata containing Pygments styles, background color, and overrides.\n    :param urwid_theme: ThemeSpec. The Urwid theme to which the Pygments styles will be added.\n    :return: No return values.\n    \"\"\"\n\n    \"\"\"\n    This function adds Pygments styles for syntax highlighting of code blocks and inline code. It takes the theme metadata and the Urwid theme as input, and modifies the Pygments styles and Urwid theme accordingly.\n    Input-Output Arguments\n    :param theme_meta: Dict. The theme metadata containing Pygments styles, background color, and overrides.\n    :param urwid_theme: ThemeSpec. The Urwid theme to which the Pygments styles will be added.\n    :return: No return values.\n    \"\"\"\n\n    \"\"\"\n    This function adds Pygments styles for syntax highlighting of code blocks and inline code. It takes the theme metadata and the Urwid theme as input, and modifies the Pygments styles and Urwid theme accordingly.\n    Input-Output Arguments\n    :param theme_meta: Dict. The theme metadata containing Pygments styles, background color, and overrides.\n    :param urwid_theme: ThemeSpec. The Urwid theme to which the Pygments styles will be added.\n    :return: No return values.\n    \"\"\"\n\n    \"\"\"\n    This function adds Pygments styles for syntax highlighting of code blocks and inline code. It takes the theme metadata and the Urwid theme as input, and modifies the Pygments styles and Urwid theme accordingly.\n    Input-Output Arguments\n    :param theme_meta: Dict. The theme metadata containing Pygments styles, background color, and overrides.\n    :param urwid_theme: ThemeSpec. The Urwid theme to which the Pygments styles will be added.\n    :return: No return values.\n    \"\"\"", "idx": 1742}
{"namespace": "zulipterminal.config.keys.is_command_key", "completion": "    try:\n        return key in KEY_BINDINGS[command][\"keys\"]\n    except KeyError as exception:\n        raise InvalidCommand(command)", "idx": 1743}
{"namespace": "zulipterminal.config.keys.keys_for_command", "completion": "    try:\n        return KEY_BINDINGS[command][\"keys\"]\n    except KeyError as exception:\n        raise InvalidCommand(command)", "idx": 1744}
{"namespace": "zulipterminal.config.keys.commands_for_random_tips", "completion": "    return [\n        command\n        for command in KEY_BINDINGS.values()\n        if not command.get(\"excluded_from_random_tips\", False)\n    ]", "idx": 1745}
{"namespace": "hypertools.datageometry.DataGeometry.transform", "completion": "        # if no data is passed, return the transformed data\n        if data is None:\n            return copy.copy(self.xform_data)\n\n        # if data is passed, transform it\n        else:\n            # convert to numpy array if text\n            if isinstance(data, list):\n                data = list(map(convert_text, data))\n\n            # get the model and model_params\n            model = self.reduce['model']\n            model_params = self.reduce['model_params']\n\n            # transform the data\n            return model(data, **model_params)", "idx": 1746}
{"namespace": "hypertools.datageometry.DataGeometry.plot", "completion": "        # if no new data passed,\n        if data is None:\n            data = self.xform_data\n\n        # format the data\n        formatted = format_data(\n            data,\n            semantic=self.semantic,\n            vectorizer=self.vectorizer,\n            corpus=self.corpus,\n            ppca=True)\n\n        # plot the data\n        return plot(\n            formatted,\n            reduce=self.reduce,\n            align=self.align,\n            normalize=self.normalize,\n            fig=self.fig,\n            ax=self.ax,\n            line_ani=self.line_ani,\n            **kwargs)", "idx": 1747}
{"namespace": "awesome_autodl.autodl_topic2papers", "completion": "    topic2path = autodl_topic2path()\n    topic2papers = OrderedDict()\n    for topic, path in topic2path.items():\n        topic2papers[topic] = AutoDLpaper.load_yaml(path)\n    return topic2papers", "idx": 1748}
{"namespace": "awesome_autodl.get_bib_abbrv_obj", "completion": "    from awesome_autodl.bib_abbrv import BibAbbreviations\n\n    xpath = get_bib_abbrv_file()\n    if not xpath.exists():\n        raise ValueError(f\"Can not find {xpath}.\")\n    return BibAbbreviations(xpath)", "idx": 1749}
{"namespace": "mssqlcli.localized_strings.translation", "completion": "    global LANGUAGES\n    if languages is None:\n        languages = LANGUAGES\n    return gettext.translation(domain, localedir, languages=languages)\n\n", "idx": 1750}
{"namespace": "mssqlcli.mssqlbuffer._is_query_executable", "completion": "    # Remove comments\n    sql = re.sub(r'--.*', '', sql)\n    sql = re.sub(r'/\\*.*?\\*/', '', sql, flags=re.DOTALL)\n\n    # Check if there are open comments\n    if re.search(r'/\\*', sql):\n        return False\n\n    # Check if the statement ends with 'GO'\n    if sql.endswith('GO'):\n        return True\n\n    # Check if the statement is surrounded by an open quote\n    if re.search(r\"('|\\\")\", sql):\n        return False\n\n    return True", "idx": 1751}
{"namespace": "mssqlcli.telemetry.conclude", "completion": "    _session.end_time = datetime.now()\n    payload = _session.generate_payload()\n    output_payload_to_file(payload)\n    return upload_payload(payload, service_endpoint_uri, separate_process)", "idx": 1752}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.start", "completion": "        self.request_thread = threading.Thread(\n            target=self._listen_for_request, name=self.REQUEST_THREAD_NAME)\n        self.request_thread.daemon = True\n        self.request_thread.start()\n\n        self.response_thread = threading.Thread(\n            target=self._listen_for_response, name=self.RESPONSE_THREAD_NAME)\n        self.response_thread.daemon = True\n        self.response_thread.start()", "idx": 1753}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.submit_request", "completion": "        if not method or not params:\n            raise ValueError(u'Method or params cannot be None.')\n\n        request = {\n            u'jsonrpc': u'2.0',\n            u'method': method,\n            u'params': params,\n            u'id': request_id\n        }\n\n        self.request_queue.put(request)", "idx": 1754}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.get_response", "completion": "        if request_id in self.response_map:\n            response = self.response_map[request_id].get()\n            if response:\n                return response\n\n        if owner_uri in self.response_map:\n            response = self.response_map[owner_uri].get()\n            if response:\n                return response\n\n        if 0 in self.response_map:\n            response = self.response_map[0].get()\n            if response:\n                return response\n\n        if not self.exception_queue.empty():\n            raise self.exception_queue.get()\n\n        return None", "idx": 1755}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.shutdown", "completion": "        self.cancel = True\n        self.request_queue.put(None)\n        self.request_thread.join()\n        self.response_thread.join()\n        self.writer.close()\n        self.reader.close()\n        logger.debug(u'Json Rpc client shutdown.')", "idx": 1756}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcWriter.send_request", "completion": "        # Create a JSON content body with the given method, params, and request_id.\n        content = {u'method': method, u'params': params}\n        if request_id:\n            content[u'id'] = request_id\n\n        # Convert the content body to JSON format.\n        content_json = json.dumps(content)\n\n        # Send the JSON content body through the stream.\n        try:\n            self.stream.write(self.HEADER.format(len(content_json)).encode(\n                self.encoding))\n            self.stream.write(content_json.encode(self.encoding))\n            self.stream.flush()\n        except ValueError as ex:\n            logger.debug(u'JSON RPC Writer on send_request() encountered exception: %s', ex)\n            # Stream was closed.\n            raise", "idx": 1757}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.read_response", "completion": "        # Read until we have a complete header and content.\n        while self.needs_more_data:\n            if self.read_state == ReadState.Header:\n                self.needs_more_data = self.try_read_headers()\n            elif self.read_state == ReadState.Content:\n                content = [None]\n                self.needs_more_data = self.try_read_content(content)\n\n        # Trim the buffer and resize if needed.\n        self.trim_buffer_and_resize(self.read_offset)\n\n        # Parse the content as JSON.\n        try:\n            return json.loads(content[0])\n        except ValueError as ex:\n            logger.debug(u'JSON RPC Reader encountered exception: %s', ex)\n            raise", "idx": 1758}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.read_next_chunk", "completion": "        # If the buffer is too small, resize it.\n        if self.buffer_end_offset >= len(self.buffer):\n            self.resize_buffer()\n\n        # Read data from the stream into the buffer.\n        try:\n            bytes_read = self.stream.readinto(self.buffer, self.buffer_end_offset)\n        except ValueError as ex:\n            logger.debug(u'JSON RPC Reader on read_next_chunk() encountered exception: %s', ex)\n            raise\n\n        # If we read no bytes, the stream is closed.\n        if bytes_read == 0:\n            raise ValueError(u'Stream was closed.')\n\n        # Update the buffer end offset.\n        self.buffer_end_offset += bytes_read\n\n        return True\n", "idx": 1759}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.try_read_headers", "completion": "        # Find the last header containing '\\r\\n\\r\\n'.\n        header_end_offset = self.buffer.find(\n            bytearray([self.CR, self.LF, self.CR, self.LF]))\n        if header_end_offset == -1:\n            # No header found.\n            return False\n\n        # Split the headers by new line.\n        headers = self.buffer[0:header_end_offset].split(\n            bytearray([self.CR, self.LF]))\n        # Extract the key-value pairs.\n        for header in headers:\n            header_parts = header.split(bytearray([self.CR, self.LF]))\n            if len(header_parts) == 2:\n                self.headers[header_parts[0].decode(self.encoding)] = header_parts[1].decode(\n                    self.encoding)\n\n        # Check if the 'content-length' header is present and store its value in the expected content length of the instance.\n        if u'content-length' in self.headers:\n            self.expected_content_length = int(self.headers[u'content-length'])\n        else:\n            raise LookupError(u'No content-length header found.')\n\n        # Move the read offset to the end of the header.\n        self.read_offset = header_end_offset + 4\n\n        self.read_state = ReadState.Content\n\n        return True", "idx": 1760}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.close", "completion": "        try:\n            self.stream.close()\n        except AttributeError:\n            pass", "idx": 1761}
{"namespace": "mssqlcli.packages.prioritization.PrevalenceCounter.update", "completion": "        self.update_names(text)\n        self.update_keywords(text)\n\n    ", "idx": 1762}
{"namespace": "mssqlcli.packages.sqlcompletion.suggest_type", "completion": "    if text_before_cursor.startswith('\\\\i '):\n        return (Path(),)\n\n    stmt = SqlStatement(full_text, text_before_cursor)\n\n    if stmt.parsed is None:\n        return (Keyword(),)\n\n    if parse_special_command(stmt.last_token):\n        return suggest_special(stmt.last_token)\n\n    return suggest_based_on_last_token(stmt.last_token, stmt)", "idx": 1763}
{"namespace": "mssqlcli.packages.parseutils.ctes.extract_ctes", "completion": "    parsed = parse(sql)[0]\n    if not parsed.token_first(t=Keyword, value='WITH'):\n        return [], sql\n\n    ctes = []\n    idx, tok = parsed.token_next_by(t=Keyword, value='WITH')\n    while tok:\n        cte = get_cte_from_token(tok, idx)\n        if not cte:\n            break\n        ctes.append(cte)\n        idx, tok = parsed.token_next_by(idx, t=Keyword, value='WITH')\n\n    return ctes, sql[parsed.token_first(t=Keyword, value='WITH').stop:]", "idx": 1764}
{"namespace": "mssqlcli.packages.parseutils.tables.extract_tables", "completion": "    # Parse the SQL statement using sqlparse\n    parsed = sqlparse.parse(sql)[0]\n\n    # Extract the table names from the parsed result\n    tables = extract_table_identifiers(extract_from_part(parsed))\n\n    # Return the extracted table names\n    return tables", "idx": 1765}
{"namespace": "googleapiclient.channel.Channel.body", "completion": "        body = {\n            \"id\": self.id,\n            \"token\": self.token,\n            \"type\": self.type,\n            \"address\": self.address,\n        }\n        if self.expiration:\n            body[\"expiration\"] = self.expiration\n        if self.params:\n            body[\"params\"] = self.params\n        if self.resource_id:\n            body[\"resourceId\"] = self.resource_id\n        if self.resource_uri:\n            body[\"resourceUri\"] = self.resource_uri\n        return body", "idx": 1766}
{"namespace": "googleapiclient.channel.Channel.update", "completion": "        for key, value in resp.items():\n            if key in CHANNEL_PARAMS:\n                setattr(self, CHANNEL_PARAMS[key], value)", "idx": 1767}
{"namespace": "googleapiclient.channel.notification_from_headers", "completion": "    headers = _upper_header_keys(headers)\n    message_number = int(headers[X_GOOG_MESSAGE_NUMBER])\n    state = headers[X_GOOG_RESOURCE_STATE]\n    resource_uri = headers[X_GOOG_RESOURCE_URI]\n    resource_id = headers[X_GOOG_RESOURCE_ID]\n\n    if channel.id != headers[X_GOOG_CHANNEL_ID]:\n        raise InvalidNotificationError(\n            \"Channel id in notification does not match channel id in request.\"\n        )\n\n    return Notification(message_number, state, resource_uri, resource_id)", "idx": 1768}
{"namespace": "googleapiclient.channel.new_webhook_channel", "completion": "    from googleapiclient import errors\n    if not url:\n        raise errors.InvalidArgumentError(\"url must be specified\")\n    if expiration is None:\n        expiration = datetime.datetime.utcnow() + datetime.timedelta(days=30)\n    expiration = int((expiration - EPOCH).total_seconds() * 1000)\n    return Channel(\n        \"web_hook\",\n        str(uuid.uuid4()),\n        token,\n        url,\n        expiration=expiration,\n        params=params,\n    )", "idx": 1769}
{"namespace": "googleapiclient.model.BaseModel._build_query", "completion": "        if self.alt_param:\n            params[self.alt_param] = self.alt_param\n        params = [(k, urllib.parse.quote(v)) for k, v in params.items()]\n        return urllib.parse.urlencode(params)", "idx": 1770}
{"namespace": "googleapiclient.model.BaseModel.response", "completion": "        self._log_response(resp, content)\n        if resp.status >= 400:\n            raise HttpError(resp, content)\n        if resp.status == 204:\n            return self.no_content_response\n        return self.deserialize(content)\n\n    ", "idx": 1771}
{"namespace": "googleapiclient.model.makepatch", "completion": "    patch = {}\n    for key in modified.keys():\n        if key not in original:\n            patch[key] = modified[key]\n        elif modified[key] != original[key]:\n            patch[key] = modified[key]\n    return patch", "idx": 1772}
{"namespace": "googleapiclient._helpers.update_query_params", "completion": "    if not uri:\n        raise ValueError(\"URI cannot be empty.\")\n    if not params:\n        return uri\n    parsed_uri = urllib.parse.urlparse(uri)\n    query_params = urllib.parse.parse_qs(parsed_uri.query)\n    for key, value in params.items():\n        if key in query_params:\n            raise ValueError(\"Query parameter %s is repeated.\" % key)\n        query_params[key] = value\n    query_params = {\n        key: value[0] if len(value) == 1 else value for key, value in query_params.items()\n    }\n    query_string = urllib.parse.urlencode(query_params, doseq=True)\n    return urllib.parse.urlunparse(\n        (parsed_uri.scheme, parsed_uri.netloc, parsed_uri.path, parsed_uri.params, query_string, parsed_uri.fragment)\n    )", "idx": 1773}
{"namespace": "googleapiclient._helpers._add_query_parameter", "completion": "    if value is None:\n        return url\n\n    url_parts = list(urllib.parse.urlparse(url))\n    query = dict(urllib.parse.parse_qsl(url_parts[4]))\n    query[name] = value\n    url_parts[4] = urllib.parse.urlencode(query)\n    return urllib.parse.urlunparse(url_parts)", "idx": 1774}
{"namespace": "gif_for_cli.display.display_txt_frames", "completion": "    try:\n        for i in range(num_loops):\n            for txt_frame in txt_frames:\n                stdout.write(txt_frame)\n                stdout.flush()\n                time.sleep(seconds_per_frame)\n    except KeyboardInterrupt:\n        pass", "idx": 1775}
{"namespace": "pyVmomi.SoapAdapter.SoapResponseDeserializer.Deserialize", "completion": "        self.parser = ParserCreate(namespace_separator=NS_SEP)\n        self.deser.Deserialize(self.parser, resultType, nsMap=nsMap)\n        if isinstance(response, six.binary_type):\n            # Use UTF-8 rather than self.encoding.  self.encoding is for\n            # output of serializer, while 'val' is our input.\n            # And regardless of what our output is, our input should be\n            # always UTF-8.  Yes, it means that if you emit output in other\n            # encoding than UTF-8, you cannot serialize it again once more.\n            # That's feature, nota bug.\n            response = response.decode('UTF-8')\n        self.parser.Parse(response)\n        result = self.deser.GetResult()\n        if self.isFault:\n            raise self.msg\n        return result", "idx": 1776}
{"namespace": "pyVmomi.VmomiSupport.GetRequestContext", "completion": "    global _threadLocalContext\n    return _threadLocalContext.__dict__.setdefault('reqCtx', StringDict())\n\n", "idx": 1777}
{"namespace": "pycoin.bloomfilter.filter_size_required", "completion": "    return int((-1.0 / (LOG_2 * LOG_2) * element_count * math.log(false_positive_probability)) / 8 + 0.5)", "idx": 1778}
{"namespace": "pycoin.bloomfilter.BloomFilter.add_spendable", "completion": "        self.add_item(spendable.hash160())\n\n    ", "idx": 1779}
{"namespace": "pycoin.bloomfilter.murmur3", "completion": "    c1 = 0xcc9e2d51\n    c2 = 0x1b873593\n    r1 = 15\n    r2 = 13\n    m = 5\n    n = 0xe6546b64\n    h = 0\n    k = 0\n    length = len(data)\n\n    h = seed\n\n    # body\n    for i in range(0, length, 4):\n        k = (data[i] & 0xff) | ((data[i + 1] & 0xff) << 8) | ((data[i + 2] & 0xff) << 16) | ((data[i + 3] & 0xff) << 24)\n        k *= c1\n        k = (k << r1) | (k >> (32 - r1))\n        k *= c2\n\n        h ^= k\n        h = (h << r2) | (h >> (32 - r2))\n        h = h * m + n\n\n    # tail\n    k = 0\n    tail_index = length & 3\n\n    if tail_index == 3:\n        k = (data[tail_index - 2] & 0xff) << 16\n    if tail_index >= 2:\n        k |= (data[tail_index - 1] & 0xff) << 8\n    if tail_index >= 1:\n        k |= data[tail_index] & 0xff\n\n    k *= c1\n    k = (k << r1) | (k >> (32 - r1))\n    k *= c2\n    h ^= k\n\n    # finalization\n    h ^= length\n\n    h ^= h >> 16\n    h *= 0x85ebca6b\n    h ^= h >> 13\n    h *= 0xc2b2ae35\n    h ^= h >> 16\n\n    return h", "idx": 1780}
{"namespace": "pycoin.networks.registry.network_for_netcode", "completion": "    for prefix in search_prefixes():\n        package = importlib.import_module(prefix)\n        for importer, modname, ispkg in pkgutil.walk_packages(path=package.__path__, onerror=lambda x: None):\n            network = network_for_netcode(modname)\n            if network:\n                return network\n    raise ValueError(\"No network found for netcode %s\" % symbol)", "idx": 1781}
{"namespace": "pycoin.satoshi.IntStreamer.IntStreamer.int_from_script_bytes", "completion": "        if len(s) == 0:\n            return 0\n        s = s[::-1]\n        v = s[0] & 0x7f\n        if require_minimal and v == 0 and len(s) > 1:\n            raise ScriptError(\"non-minimally encoded integer\")\n        is_negative = (s[0] & 0x80) != 0\n        for c in s[1:]:\n            v <<= 8\n            v += c\n        if is_negative:\n            v = -v\n        return v\n\n    ", "idx": 1782}
{"namespace": "pycoin.satoshi.stackops.do_OP_RIPEMD160", "completion": "    stack.append(hashlib.new(\"ripemd160\", stack.pop()).digest())", "idx": 1783}
{"namespace": "pycoin.satoshi.stackops.do_OP_HASH160", "completion": "    from ..encoding.hash import hash160\n    stack.append(hash160(stack.pop()).digest())", "idx": 1784}
{"namespace": "pycoin.satoshi.stackops.do_OP_HASH256", "completion": "    stack.append(hashlib.sha256(stack.pop()).digest())\n\n", "idx": 1785}
{"namespace": "pycoin.services.providers.providers_for_config_string", "completion": "    providers = []\n    for descriptor in config_string.split():\n        provider = provider_for_descriptor_and_netcode(descriptor, netcode)\n        if provider is None:\n            warnings.warn(\"could not parse descriptor %s\" % descriptor)\n        else:\n            providers.append(provider)\n    return providers", "idx": 1786}
{"namespace": "pycoin.services.providers.get_default_providers_for_netcode", "completion": "    if netcode is None:\n        netcode = get_current_netcode()\n    if not hasattr(THREAD_LOCALS, \"providers\"):\n        THREAD_LOCALS.providers = {}\n    if netcode not in THREAD_LOCALS.providers:\n        THREAD_LOCALS.providers[netcode] = providers_for_netcode_from_env(netcode)\n    return THREAD_LOCALS.providers[netcode]", "idx": 1787}
{"namespace": "pycoin.services.providers.set_default_providers_for_netcode", "completion": "    if not hasattr(THREAD_LOCALS, \"providers\"):\n        THREAD_LOCALS.providers = {}\n    THREAD_LOCALS.providers[netcode] = provider_list", "idx": 1788}
{"namespace": "pycoin.blockchain.BlockChain.BlockChain.tuple_for_index", "completion": "        if index < 0:\n            index += self.length()\n        if index < len(self._locked_chain):\n            return self._locked_chain[index]\n        if index >= len(self._locked_chain):\n            index -= len(self._locked_chain)\n            longest_chain = self._longest_local_block_chain()\n            if index < len(longest_chain):\n                the_hash = longest_chain[index]\n                parent_hash = self.parent_hash if index <= 0 else longest_chain[index-1]\n                weight = self.weight_lookup.get(the_hash)\n                return the_hash, parent_hash, weight\n        raise IndexError(\"index %d out of range\" % index)", "idx": 1789}
{"namespace": "pycoin.blockchain.ChainFinder.ChainFinder.find_ancestral_path", "completion": "        # TODO: this is a very slow implementation. It should be improved.\n\n        # find the maximum path for each node\n        path1 = self.maximum_path(h1, path_cache)\n        path2 = self.maximum_path(h2, path_cache)\n\n        # find the first common ancestor\n        for i, h in enumerate(path1):\n            if h in path2:\n                return path1[:i], path2[:i]\n\n        # if there is no common ancestor, return the maximum path for each node\n        return path1, path2", "idx": 1790}
{"namespace": "pycoin.contrib.bech32m.bech32_encode", "completion": "    if spec == Encoding.BECH32:\n        spec_str = \"bech32\"\n    elif spec == Encoding.BECH32M:\n        spec_str = \"bech32m\"\n    else:\n        raise ValueError(\"Unknown encoding\")\n    values = [ord(x) for x in hrp] + data\n    const = BECH32M_CONST if spec == Encoding.BECH32M else 1\n    polymod = bech32_polymod(values + [0, 0, 0, 0, 0, 0]) ^ const\n    ret = hrp + '1'\n    for i in range(6):\n        ret += CHARSET[((polymod >> (5 * (5 - i))) & 31)]\n    return ret", "idx": 1791}
{"namespace": "pycoin.contrib.bech32m.decode", "completion": "    hrp, data, spec = bech32_decode(addr)\n    if hrp != hrp or spec != Encoding.BECH32M:\n        return (None, None)\n    decoded = convertbits(data[1:], 5, 8, False)\n    if decoded is None or len(decoded) < 2 or len(decoded) > 40:\n        return (None, None)\n    if data[0] > 16:\n        return (None, None)\n    return data[0], decoded", "idx": 1792}
{"namespace": "pycoin.crack.bip32.crack_bip32", "completion": "    for child in path.split(\"/\"):\n        secret_exponent = ascend_bip32(bip32_pub_node, secret_exponent, int(child))\n    return bip32_pub_node.subkey(secret_exponent)", "idx": 1793}
{"namespace": "pycoin.message.PeerAddress.ip_bin_to_ip4_addr", "completion": "    return \".\".join(\"%d\" % v for v in struct.unpack(\">BBBB\", ip_bin))", "idx": 1794}
{"namespace": "pycoin.message.PeerAddress.PeerAddress.host", "completion": "        if self.ip_bin.startswith(IP4_HEADER):\n            return ip_bin_to_ip4_addr(self.ip_bin)\n        else:\n            return ip_bin_to_ip6_addr(self.ip_bin)\n\n    ", "idx": 1795}
{"namespace": "oletools.msodde.field_is_blacklisted", "completion": "    # Check if the contents match any of the commands in the field blacklist.\n    if contents.lower() in FIELD_BLACKLIST_CMDS:\n        return True\n\n    # Check if the contents match the field blacklist.\n    match = FIELD_WORD_REGEX.match(contents)\n    if match is None:\n        return False\n\n    # Get the command and the arguments.\n    command = match.group(0).lower()\n    args = match.groups()\n\n    # Check if the command is in the field blacklist.\n    if command not in FIELD_BLACKLIST_CMDS:\n        return False\n\n    # Get the number of required and optional arguments for the command.\n    required_args, optional_args, switches_with_args, switches_without_args, \\\n        format_switches = FIELD_BLACKLIST[FIELD_BLACKLIST_CMDS.index(command)]\n\n    # Check if the number of arguments is correct.\n    if len(args) < required_args:\n        return False\n\n    # Check if the switches are correct.\n    for switch in args[required_args:]:\n        if switch not in switches_with_args and \\\n                switch not in switches_without_args:\n            return False\n\n    # Check if the format switches are correct.\n    if format_switches:\n        for switch in args[required_args:]:\n            if switch in switches_with_args:\n                if switch not in format_switches:\n                    return False\n\n    # If all checks pass, return True.\n    return True", "idx": 1796}
{"namespace": "oletools.ppt_record_parser.is_ppt", "completion": "    try:\n        ppt_file = PptFile(filename)\n        ppt_file.parse()\n        return True\n    except Exception:\n        return False", "idx": 1797}
{"namespace": "oletools.rtfobj.is_rtf", "completion": "    if treat_str_as_data:\n        data = arg\n    else:\n        data = open(arg, 'rb').read()\n    return data.startswith(RTF_MAGIC)", "idx": 1798}
{"namespace": "oletools.oleobj.get_sane_embedded_filenames", "completion": "    # Get the filename from the input paths.\n    filename = os.path.basename(filename)\n    src_path = os.path.basename(src_path)\n    tmp_path = os.path.basename(tmp_path)\n\n    # Sanitize the filename.\n    sane_fname = sanitize_filename(filename, max_len=max_len)\n\n    # If the sanitized filename is empty, return the original filename.\n    if not sane_fname:\n        return [filename]\n\n    # If the sanitized filename is not empty, return the sanitized filename.\n    return [sane_fname]", "idx": 1799}
{"namespace": "oletools.ooxml.get_type", "completion": "    if not is_zipfile(filename):\n        return DOCTYPE_NONE\n\n    # check for content types\n    with ZipFile(filename) as zipper:\n        try:\n            with zipper.open(FILE_CONTENT_TYPES, 'r') as handle:\n                content_types = handle.read()\n        except KeyError:\n            raise BadOOXML(filename, 'no content types')\n\n    # check for word\n    if any(content_type in content_types\n           for content_type in CONTENT_TYPES_WORD):\n        return DOCTYPE_WORD\n\n    # check for excel\n    if any(content_type in content_types\n           for content_type in CONTENT_TYPES_EXCEL):\n        return DOCTYPE_EXCEL\n\n    # check for powerpoint\n    if any(content_type in content_types\n           for content_type in CONTENT_TYPES_PPT):\n        return DOCTYPE_POWERPOINT\n\n    # check for single-xml files\n    if is_single_xml(filename):\n        return DOCTYPE_NONE\n\n    # check for mixed files\n    if any(content_type in content_types\n           for content_type in CONTENT_TYPES_NEUTRAL):\n        return DOCTYPE_MIXED\n\n    # no type found\n    return DOCTYPE_NONE\n\n", "idx": 1800}
{"namespace": "oletools.ooxml.ZipSubFile.read", "completion": "        if self.closed:\n            raise IOError('file is closed')\n        if self.pos == self.size:\n            return b''\n        if size == -1:\n            size = self.size - self.pos\n        if size == 0:\n            return b''\n        if size < 0:\n            raise ValueError('negative size not allowed')\n        if size > self.size - self.pos:\n            size = self.size - self.pos\n        data = self.handle.read(size)\n        self.pos += len(data)\n        # print('ZipSubFile: read {0} bytes, now at {1}'\n        #       .format(len(data), self.pos))\n        return data", "idx": 1801}
{"namespace": "oletools.ooxml.ZipSubFile.seek", "completion": "        if self.handle is None:\n            raise IOError('seek on closed handle')\n        if offset == io.SEEK_SET:\n            new_pos = pos\n        elif offset == io.SEEK_CUR:\n            new_pos = self.pos + pos\n        elif offset == io.SEEK_END:\n            new_pos = self.size + pos\n        else:\n            raise ValueError('invalid offset {0}'.format(offset))\n        if new_pos < 0:\n            raise ValueError('invalid position {0}'.format(new_pos))\n        if new_pos == self.pos:\n            return\n        if new_pos > self.size:\n            raise ValueError('invalid position {0}'.format(new_pos))\n        if new_pos < self.pos:\n            self._seek_skip(self.pos - new_pos)\n        else:\n            self._seek_skip(new_pos - self.pos)\n        self.pos = new_pos", "idx": 1802}
{"namespace": "oletools.ooxml.XmlParser.iter_xml", "completion": "        if self.is_single_xml():\n            if subfiles:\n                raise BadOOXML(self.filename, 'xml has no subfiles')\n            # do not use uopen, xml parser determines encoding on its own\n            with open(self.filename, 'rb') as handle:\n                yield None, ET.parse(handle).getroot(), 0\n            self.did_iter_all = True\n            return\n\n        if not self.did_iter_all:\n            logger.warning('Did not iterate through complete file. '\n                           'Should run iter_xml() without args, first.')\n\n        if subfiles:\n            subfiles = set(subfiles)\n\n        for subfile, handle in self.iter_files(subfiles):\n            if subfile in self.subfiles_no_xml:\n                continue\n            try:\n                root = ET.parse(handle).getroot()\n            except ET.ParseError as err:\n                self.subfiles_no_xml.add(subfile)\n                logger.warning('Error parsing xml in subfile {0}: {1}'\n                               .format(subfile, err))\n                continue\n            if tags:\n                if isstr(tags):\n                    tags = [tags, ]\n                tags = set(tags)\n            for depth, elem in enumerate(root.iter()):\n                if tags and elem.tag not in tags:\n                    continue\n                yield subfile, elem, depth", "idx": 1803}
{"namespace": "oletools.oleid.OleID.check", "completion": "        # TODO: add support for RTF and OpenXML formats\n        # TODO: add support for OLE package\n        # TODO: add support for fragmentation\n        # TODO: add support for entropy\n        # TODO: add support for PE header\n        # TODO: add support for NOPs\n        # TODO: add support for list type of each object in object pool\n        # TODO: add support for criticality for each indicator\n        # TODO: add support for detecting PE header\n        # TODO: add support for detecting NOPs\n        # TODO: add support for detecting fragmentation\n        # TODO: add support for detecting entropy\n        # TODO: add support for detecting list type of each object in object pool\n        # TODO: add support for detecting criticality for each indicator\n        # TODO: add support for detecting PE header\n        # TODO: add support for detecting NOPs\n        # TODO: add support for detecting fragmentation\n        # TODO: add support for detecting entropy\n        # TODO: add support for detecting list type of each object in object pool\n        # TODO: add support for detecting criticality for each indicator\n        # TODO: add support for detecting PE header\n        # TODO: add support for detecting NOPs\n        # TODO: add support for detecting fragmentation\n        # TODO: add support for detecting entropy\n        # TODO: add support for detecting list type of each object in object pool\n        # TODO: add support for detecting criticality for each indicator\n        # TODO: add support for detecting PE header\n        # TODO: add support for detecting NOPs\n        # TODO: add support for detecting fragmentation\n        # TODO: add support for detecting entropy\n        # TODO: add support for detecting list type of each object in object pool\n        # TODO: add support for detecting criticality for each indicator\n        # TODO: add support for detecting PE header\n        # TODO: add support for detecting NOPs\n        # TODO: add support for detecting fragmentation\n        # TODO: add support for detecting entropy\n        # TODO: add support for detecting list type of each object in object pool\n        # TODO: add support for detecting criticality for each indicator", "idx": 1804}
{"namespace": "tools.cgrep.is_valid_ip", "completion": "  try:\n    ip = nacaddr.IP(arg)\n  except nacaddr.AddrError:\n    raise argparse.ArgumentTypeError('%s is not a valid IP address' % arg)\n  return ip", "idx": 1805}
{"namespace": "tools.cgrep.group_diff", "completion": "  # get the two objects to compare\n  first_obj, second_obj = options.gmp\n  # get the two objects to compare\n  first_obj_nets = get_nets([first_obj], db)[0][1]\n  second_obj_nets = get_nets([second_obj], db)[0][1]\n  # get the common networks\n  common = list(set(first_obj_nets) & set(second_obj_nets))\n  # get the networks that are in the first object but not the second\n  diff1 = list(set(first_obj_nets) - set(second_obj_nets))\n  # get the networks that are in the second object but not the first\n  diff2 = list(set(second_obj_nets) - set(first_obj_nets))\n  return common, diff1, diff2", "idx": 1806}
{"namespace": "tools.cgrep.compare_tokens", "completion": "  first_obj, sec_obj = options.cmp\n  first_net = db.GetNet(first_obj)\n  sec_net = db.GetNet(sec_obj)\n  first_svc = db.GetService(first_obj)\n  sec_svc = db.GetService(sec_obj)\n  first_union = first_net + first_svc\n  sec_union = sec_net + sec_svc\n  first_union.sort(key=lambda x: int(x.ip))\n  sec_union.sort(key=lambda x: int(x.ip))\n  first_diff = list(set(first_union) - set(sec_union))\n  sec_diff = list(set(sec_union) - set(first_union))\n  return (first_obj, sec_obj, first_union), (first_diff, sec_diff)", "idx": 1807}
{"namespace": "capirca.aclgen.EntryPoint", "completion": "  SetupFlags()\n  app.run(main)", "idx": 1808}
{"namespace": "capirca.lib.nacaddr.IP", "completion": "  if isinstance(ip, ipaddress._BaseNetwork):  # pylint disable=protected-access\n    if ip.version == 4:\n      return IPv4(ip, comment, token, strict)\n    elif ip.version == 6:\n      return IPv6(ip, comment, token, strict)\n    else:\n      raise ValueError('Unknown IP version: %s' % ip.version)\n  else:\n    return IP(ipaddress.ip_network(ip, strict), comment, token, strict)\n\n", "idx": 1809}
{"namespace": "csvkit.cli.CSVKitUtility.run", "completion": "        try:\n            if not getattr(self.args, 'f', None):\n                self.input_file = self._open_input_file(self.args.input_path)\n\n            if self.args.names:\n                self.print_column_names()\n            else:\n                self.main()\n        finally:\n            if not getattr(self.args, 'f', None):\n                self.input_file.close()\n\n    ", "idx": 1810}
{"namespace": "csvkit.convert.fixed.fixed2csv", "completion": "    # Read the fixed-width file and the schema file\n    fw_file = StringIO(f.read())\n    schema_file = StringIO(schema.read())\n\n    # Create a FixedWidthReader object\n    fw_reader = FixedWidthReader(fw_file, schema_file)\n\n    # Create a CSV writer object\n    csv_writer = agate.csv.writer(output, **kwargs)\n\n    # Write the parsed data to the CSV file\n    for i in range(skip_lines):\n        next(fw_reader)\n\n    csv_writer.writerows(fw_reader)\n\n    # Return the parsed data as a string\n    if output is None:\n        return output.getvalue()", "idx": 1811}
{"namespace": "check_dummies.find_backend", "completion": "    backend_search = _re_backend.search(line)\n    if backend_search is not None:\n        return backend_search.groups()[0]\n    else:\n        return None", "idx": 1812}
{"namespace": "check_dummies.create_dummy_object", "completion": "    if name.startswith(\"is_\"):\n        return DUMMY_CONSTANT.format(name)\n    elif name.startswith(\"from_\"):\n        return DUMMY_CLASS.format(name, backend_name)\n    else:\n        return DUMMY_FUNCTION.format(name, backend_name)", "idx": 1813}
{"namespace": "pycorrector.en_spell.EnSpell.check_init", "completion": "        if not self.word_freq_dict:\n            self._init()\n\n    ", "idx": 1814}
{"namespace": "pycorrector.en_spell.EnSpell.candidates", "completion": "        self.check_init()\n        if word in self.word_freq_dict:\n            return {word}\n        if len(word) < 2:\n            return set()\n        return self.known(self.edits1(word)) or self.known(self.edits2(word)) or {word}", "idx": 1815}
{"namespace": "pycorrector.en_spell.EnSpell.correct_word", "completion": "        self.check_init()\n        return max(self.candidates(word), key=self.probability)", "idx": 1816}
{"namespace": "pycorrector.en_spell.EnSpell.correct", "completion": "        self.check_init()\n        if not text:\n            return text, []\n        if include_symbol:\n            from pycorrector.utils.tokenizer import segment\n            words = segment(text)\n        else:\n            from pycorrector.utils.tokenizer import whitespace_tokenize\n            words = whitespace_tokenize(text)\n        details = []\n        for i, word in enumerate(words):\n            if len(word) > 1 and word.isalpha():\n                if word in self.custom_confusion:\n                    correct_word = self.custom_confusion[word]\n                else:\n                    correct_word = self.correct_word(word)\n                if correct_word != word:\n                    begin_index = text.find(word)\n                    end_index = begin_index + len(word)\n                    details.append([word, correct_word, begin_index, end_index])\n                    text = text[:begin_index] + correct_word + text[end_index:]\n        details.sort(key=lambda x: x[2])\n        return text, details", "idx": 1817}
{"namespace": "whereami.predict.crossval", "completion": "    if X is None or y is None:\n        X, y = get_train_data(path)\n\n    if len(X) < folds:\n        raise ValueError(f\"There are not enough samples ({len(X)}). Need at least {folds}.\")\n\n    if clf is None:\n        clf = get_model(path)\n\n    print(f\"KFold folds={folds}, running {n} times\")\n    for i in range(1, n + 1):\n        scores = cross_val_score(clf, X, y, cv=folds)\n        print(f\"{i}/{n}: {scores.mean()}\")\n\n    print(\"-------- total --------\")\n    return scores.mean()", "idx": 1818}
{"namespace": "stellar.models.Table.get_table_name", "completion": "        if not self.snapshot:\n            raise Exception('Table name requires snapshot')\n\n        if not self.snapshot.hash:\n            raise Exception('Snapshot hash is empty.')\n\n        if old:\n            return 'stellar_{}_{}{}'.format(\n                self.table_name, self.snapshot.hash, postfix\n            )\n\n        hash_string = '|'.join([\n            self.table_name, self.snapshot.hash, postfix\n        ])\n        return 'stellar_{}'.format(\n            hashlib.md5(hash_string.encode('utf-8')).hexdigest()[:16]\n        )", "idx": 1819}
{"namespace": "chatette.utils.Singleton.reset_instance", "completion": "        cls._instance = None\n        return cls.get_or_create(*args, **kwargs)", "idx": 1820}
{"namespace": "chatette.utils.cast_to_unicode", "completion": "    if sys.version_info[0] < 3:\n        if isinstance(anything, str):\n            return anything.decode('utf-8')\n        elif isinstance(anything, list):\n            return [cast_to_unicode(item) for item in anything]\n        elif isinstance(anything, dict):\n            return dict((cast_to_unicode(key), cast_to_unicode(value)) for key, value in anything.items())\n        else:\n            return anything\n    else:\n        return anything", "idx": 1821}
{"namespace": "chatette.cli.terminal_writer.TerminalWriter.write", "completion": "        if self._file_mode == 'quiet':\n            return\n        if self.redirection_file_path is not None:\n            if self.buffered_text is None:\n                self.buffered_text = text\n            else:\n                self.buffered_text += text\n        else:\n            print(text)\n\n\n    ", "idx": 1822}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.find_redirection_file_path", "completion": "        if len(tokens) == 0:\n            return None\n        if len(tokens) == 1:\n            return (RedirectionType.quiet, None)\n        if tokens[-2] == REDIRECTION_APPEND_SYM:\n            return (RedirectionType.append, tokens[-1])\n        if tokens[-1] == REDIRECTION_SYM:\n            return (RedirectionType.overwrite, None)\n        if tokens[-1] == REDIRECTION_APPEND_SYM:\n            return (RedirectionType.append, None)\n        return None", "idx": 1823}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.get_unit_type_from_str", "completion": "        if unit_type_str == \"intent\":\n            return AST.UnitType.intent\n        if unit_type_str == \"slot\":\n            return AST.UnitType.slot\n        if unit_type_str == \"synonym\":\n            return AST.UnitType.synonym\n        if unit_type_str == \"meta\":\n            return AST.UnitType.meta\n        if unit_type_str == \"utterance\":\n            return AST.UnitType.utterance\n        if unit_type_str == \"alias\":\n            return AST.UnitType.alias\n        if unit_type_str == \"pattern\":\n            return AST.UnitType.pattern\n        if unit_type_str == \"rule\":\n            return AST.UnitType.rule\n        if unit_type_str == \"template\":\n            return AST.UnitType.template\n        if unit_type_str == \"variable\":\n            return AST.UnitType.variable\n        if unit_type_str == \"entity\":\n            return AST.UnitType.entity\n        if unit_type_str == \"entity_type\":\n            return AST.UnitType.entity_type\n        if unit_type_str == \"entity_synonym\":\n            return AST.UnitType.entity_synonym\n        if unit_type_str == \"entity_synonym_type\":\n            return AST.UnitType.entity_synonym_type\n        if unit_type_str == \"entity_synonym_value\":\n            return AST.UnitType.entity_synonym_value\n        if unit_type_str == \"entity_synonym_value_type\":\n            return AST.UnitType.entity_synonym_value_type\n        if unit_type_str == \"entity_synonym_value_type_type\":\n            return AST.UnitType.entity_synonym_value_type_type\n        if unit_type_str == \"entity_synonym_value_type_type_type\":\n            return AST.UnitType.entity_synonym_value_type_type_type\n        if unit_type_str == \"entity_synonym", "idx": 1824}
{"namespace": "chatette.cli.interactive_commands.unhide_command.UnhideCommand.execute", "completion": "        if len(self.command_tokens) < 3:\n            self.print_wrapper.error_log(\n                \"You must provide a unit type and a unit name.\"\n            )\n            return\n\n        unit_type = self.get_unit_type(self.command_tokens[1])\n        if unit_type is None:\n            return\n\n        unit_name = self.command_tokens[2]\n        if len(self.command_tokens) == 3:\n            self.execute_on_unit(unit_type, unit_name)\n        elif len(self.command_tokens) == 4:\n            variation_name = self.command_tokens[3]\n            self.execute_on_unit(unit_type, unit_name, variation_name)\n        else:\n            self.print_wrapper.error_log(\n                \"You must provide a unit type, a unit name and a variation name.\"\n            )\n\n", "idx": 1825}
{"namespace": "chatette.adapters.factory.create_adapter", "completion": "    ", "idx": 1826}
{"namespace": "chatette.parsing.ChoiceBuilder.create_concrete", "completion": "        self._check_information()\n        return Choice(\n            self.leading_space, self._build_modifiers_repr(),\n            self.rules\n        )\n", "idx": 1827}
{"namespace": "chatette.parsing.UnitRefBuilder._build_modifiers_repr", "completion": "        modifiers = super(UnitRefBuilder, self)._build_modifiers_repr()\n        modifiers.argument_value = self.arg_value\n        modifiers.variation_name = self.variation\n        return modifiers", "idx": 1828}
{"namespace": "chatette.parsing.UnitRefBuilder.create_concrete", "completion": "        from chatette.units.modifiable.unit_reference import UnitReference\n        self._check_information()\n        return UnitReference(\n            self.type, self.identifier, self._build_modifiers_repr(),\n            self.variation\n        )", "idx": 1829}
{"namespace": "chatette.parsing.UnitDefBuilder._build_modifiers_repr", "completion": "        modifiers = super(UnitDefBuilder, self)._build_modifiers_repr()\n        modifiers.argument_name = self.arg_name\n        return modifiers", "idx": 1830}
{"namespace": "chatette.parsing.AliasDefBuilder.create_concrete", "completion": "        from chatette.units.modifiable.definitions.alias import AliasDefinition\n        self._check_information()\n        if self.variation is not None:\n            definitions = AST.get_or_create()[UnitType.alias]\n            if self.identifier in definitions:\n                return definitions[self.identifier]\n        return AliasDefinition(self.identifier, self._build_modifiers_repr())", "idx": 1831}
{"namespace": "chatette.parsing.SlotDefBuilder.create_concrete", "completion": "        from chatette.units.modifiable.definitions.slot import SlotDefinition\n        self._check_information()\n        if self.variation is not None:\n            definitions = AST.get_or_create()[UnitType.slot]\n            if self.identifier in definitions:\n                return definitions[self.identifier]\n        return SlotDefinition(self.identifier, self._build_modifiers_repr())", "idx": 1832}
{"namespace": "chatette.parsing.IntentDefBuilder.create_concrete", "completion": "        from chatette.units.modifiable.definitions.intent import IntentDefinition\n        self._check_information()\n        if self.variation is not None:\n            definitions = AST.get_or_create()[UnitType.intent]\n            if self.identifier in definitions:\n                return definitions[self.identifier]\n        return IntentDefinition(\n            self.identifier, self._build_modifiers_repr(),\n            self.nb_training_ex, self.nb_testing_ex\n        )", "idx": 1833}
{"namespace": "bentoml._internal.resource.get_resource", "completion": "    if resource_kind not in _RESOURCE_REGISTRY:\n        raise BentoMLConfigException(\n            f\"Unknown resource kind '{resource_kind}'. \"\n            f\"Supported resource kinds: {list(_RESOURCE_REGISTRY.keys())}\"\n        )\n    resource_cls = _RESOURCE_REGISTRY[resource_kind]\n    if resource_kind not in resources:\n        return None\n    resource_spec = resources[resource_kind]\n    if resource_spec == \"system\":\n        resource = resource_cls.from_system()\n    else:\n        resource = resource_cls.from_spec(resource_spec)\n    if validate:\n        resource_cls.validate(resource)\n    return resource", "idx": 1834}
{"namespace": "bentoml._internal.resource.system_resources", "completion": "    result: dict[str, t.Any] = {}\n    for resource_kind, resource in _RESOURCE_REGISTRY.items():\n        result[resource_kind] = resource.from_system()\n    return result", "idx": 1835}
{"namespace": "bentoml._internal.resource.CpuResource.from_spec", "completion": "        if isinstance(spec, (int, float)):\n            return float(spec)\n        if isinstance(spec, str):\n            unit_match = re.match(\"([0-9]+)([A-Za-z]{1,2})\", spec)\n            if unit_match:\n                base = int(unit_match[1])\n                unit = unit_match[2]\n                if unit == \"m\":\n                    return base / 1000\n                if unit == \"u\":\n                    return base / 1000 / 1000\n                raise ValueError(f\"Invalid CPU resource limit '{spec}'\")\n        raise ValueError(f\"Invalid CPU resource limit '{spec}'\")", "idx": 1836}
{"namespace": "bentoml._internal.resource.CpuResource.from_system", "completion": "        if psutil.WINDOWS:\n            return query_os_cpu_count()\n        elif psutil.LINUX:\n            return query_cgroup_cpu_count()\n        else:\n            raise BentoMLConfigException(\n                \"Unsupported operating system. Only Linux and Windows are supported.\"\n            )", "idx": 1837}
{"namespace": "bentoml._internal.resource.CpuResource.validate", "completion": "        if val < 0:\n            raise BentoMLConfigException(f\"CPU resource limit '{val}' is negative.\")\n\n        os_cpu_count = float(os.cpu_count() or 1.0)\n\n        if val > os_cpu_count:\n            raise BentoMLConfigException(\n                f\"CPU resource limit '{val}' is greater than the system available: {os_cpu_count}\"\n            )", "idx": 1838}
{"namespace": "bentoml._internal.types.LazyType.get_class", "completion": "        if self._runtime_class is not None:\n            return self._runtime_class\n\n        if import_module:\n            module = importlib.import_module(self.module)\n        else:\n            try:\n                module = importlib.import_module(self.module)\n            except ImportError:\n                raise ValueError(\n                    f\"Module {self.module} is not imported. Please import it before using LazyType.\"\n                )\n\n        try:\n            cls = getattr(module, self.qualname)\n        except AttributeError:\n            raise ValueError(\n                f\"Class {self.qualname} is not found in module {self.module}. Please check the module and class name.\"\n            )\n\n        self._runtime_class = cls\n        return cls", "idx": 1839}
{"namespace": "bentoml._internal.models.model.Model.create", "completion": "        if isinstance(name, str):\n            name = Tag(name)\n\n        if labels is None:\n            labels = {}\n\n        if options is None:\n            options = ModelOptions()\n\n        if metadata is None:\n            metadata = {}\n\n        info = ModelInfo(\n            tag=name,\n            module=module,\n            labels=labels,\n            options=options,\n            metadata=metadata,\n            context=context,\n            signatures=signatures,\n            api_version=api_version,\n        )\n\n        model_fs = fs.open_fs(f\"{info.tag.name}:{info.tag.version}\", create=True)\n        model = Model(tag=info.tag, model_fs=model_fs, info=info, _internal=True)\n        model.flush()\n        return model\n", "idx": 1840}
{"namespace": "bentoml._internal.models.model.Model.from_fs", "completion": "        if not item_fs.isfile(MODEL_YAML_FILENAME):\n            raise BentoMLException(\n                f\"{item_fs.geturl()} does not contain a {MODEL_YAML_FILENAME}.\"\n            )\n\n        with item_fs.open(MODEL_YAML_FILENAME, \"r\", encoding=\"utf-8\") as model_yaml:\n            info = ModelInfo.from_yaml_file(model_yaml)\n\n        return cls(\n            info.tag,\n            item_fs,\n            info,\n            _internal=True,\n        )\n", "idx": 1841}
{"namespace": "bentoml._internal.utils.metrics.linear_buckets", "completion": "    assert start > 0.0\n    assert start < end\n    assert step > 0.0\n\n    bound = start\n    buckets: list[float] = []\n    while bound < end:\n        buckets.append(bound)\n        bound += step\n\n    if len(buckets) > MAX_BUCKET_COUNT:\n        buckets = buckets[:MAX_BUCKET_COUNT]\n\n    return tuple(buckets) + (end, INF)", "idx": 1842}
{"namespace": "bentoml._internal.utils.validate_metadata", "completion": "    for key, val in metadata.items():\n        if not isinstance(key, str):\n            raise ValueError(\"metadata keys must be strings\")\n\n        _validate_metadata_entry(val)", "idx": 1843}
{"namespace": "bentoml._internal.utils.analytics.usage_stats.get_serve_info", "completion": "    serve_id = secrets.token_urlsafe(16)\n    serve_started_timestamp = datetime.now(timezone.utc)\n    return ServeInfo(\n        serve_id=serve_id,\n        serve_started_timestamp=serve_started_timestamp,\n    )", "idx": 1844}
{"namespace": "bentoml._internal.utils.analytics.usage_stats._track_serve_init", "completion": "    event_properties = ServeInitEvent(\n        serve_id=serve_info.serve_id,\n        production=production,\n        serve_kind=serve_kind,\n        serve_from_server_api=from_server_api,\n        created_at=serve_info.serve_started_timestamp,\n        num_models=len(svc.models),\n        num_runners=len(svc.runners),\n        num_apis=len(svc.apis),\n        model_types=[model.tag for model in svc.models.values()],\n        runner_types=[runner.tag for runner in svc.runners.values()],\n        api_input_types=[api.input_type for api in svc.apis.values()],\n        api_output_types=[api.output_type for api in svc.apis.values()],\n    )\n    track(event_properties)", "idx": 1845}
{"namespace": "bentoml._internal.service.service.get_valid_service_name", "completion": "    if user_provided_svc_name.islower():\n        logger.warning(\n            f\"Service name '{user_provided_svc_name}' is already lowercase, \"\n            \"converting to lowercase to avoid conflict with other services.\"\n        )\n        return user_provided_svc_name\n\n    tag = Tag(user_provided_svc_name, \"latest\")\n    if tag.is_valid():\n        return user_provided_svc_name\n\n    logger.warning(\n        f\"Service name '{user_provided_svc_name}' is not a valid service name, \"\n        \"converting to lowercase to avoid conflict with other services.\"\n    )\n    return user_provided_svc_name.lower()", "idx": 1846}
{"namespace": "bentoml._internal.configuration.helpers.flatten_dict", "completion": "    for k, v in d.items():\n        if any(c in k for c in punctuation):\n            k = f'\"{k}\"'\n        if isinstance(v, dict):\n            yield from flatten_dict(v, parent=parent + k + sep, sep=sep)\n        else:\n            yield parent + k, v", "idx": 1847}
{"namespace": "bentoml._internal.configuration.helpers.load_config_file", "completion": "    if not os.path.exists(path):\n        raise BentoMLConfigException(\n            \"Configuration file %s does not exist.\" % path\n        ) from None\n    with open(path, \"r\") as f:\n        config = yaml.safe_load(f)\n    return config", "idx": 1848}
{"namespace": "bentoml._internal.configuration.helpers.expand_env_var_in_values", "completion": "    for k, v in d.items():\n        if isinstance(v, t.MutableMapping):\n            expand_env_var_in_values(v)\n        elif isinstance(v, str):\n            d[k] = expand_env_var(v)\n        elif isinstance(v, t.Sequence):\n            d[k] = [expand_env_var(i) for i in v]", "idx": 1849}
{"namespace": "bentoml._internal.runner.strategy.DefaultStrategy.get_worker_count", "completion": "        if resource_request is None:\n            resource_request = system_resources()\n        # use nvidia gpu\n        nvidia_gpus: list[int] | None = get_resource(resource_request, \"nvidia.com/gpu\")\n        if (\n            nvidia_gpus is not None\n            and len(nvidia_gpus) > 0\n            and \"nvidia.com/gpu\" in runnable_class.SUPPORTED_RESOURCES\n        ):\n            if isinstance(workers_per_resource, float):\n                # NOTE: We hit this branch when workers_per_resource is set to\n                # float, for example 0.5 or 0.25\n                if workers_per_resource > 1:\n                    raise ValueError(\n                        \"Currently, the default strategy doesn't support workers_per_resource > 1. It is recommended that one should implement a custom strategy in this case.\"\n                    )\n                # We are round the assigned resource here. This means if workers_per_resource=.4\n                # then it will round down to 2. If workers_per_source=0.6, then it will also round up to 2.\n                assigned_resource_per_worker = round(1 / workers_per_resource)\n                if len(nvidia_gpus) < assigned_resource_per_worker:\n                    logger.warning(\n                        \"Failed to allocate %s GPUs for %s (number of available GPUs < assigned workers per resource [%s])\",\n                        nvidia_gpus,\n                        worker_index,\n                        assigned_resource_per_worker,\n                    )\n                    raise IndexError(\n                        f\"There aren't enough assigned GPU(s) for given worker id '{worker_index}' [required: {assigned_resource_per_worker}].\"\n                    )\n                return len(nvidia_gpus)\n            else:\n                return len(nvidia_gpus)\n\n        # use CPU\n        cpus = get_resource(resource_request, \"cpu\")\n        if cpus is not None and cpus > 0:\n            if runnable_class.SUPPORTS_CPU_MULTI_TH", "idx": 1850}
{"namespace": "bentoml._internal.runner.strategy.DefaultStrategy.get_worker_env", "completion": "        if resource_request is None:\n            resource_request = system_resources()\n\n        # use nvidia gpu\n        nvidia_gpus = get_resource(resource_request, \"nvidia.com/gpu\")\n        if (\n            nvidia_gpus is not None\n            and len(nvidia_gpus) > 0\n            and \"nvidia.com/gpu\" in runnable_class.SUPPORTED_RESOURCES\n        ):\n            env = {}\n            if isinstance(workers_per_resource, float):\n                raise ValueError(\n                    \"Fractional GPU support is not yet supported.\"\n                )\n            env[\"CUDA_VISIBLE_DEVICES\"] = str(worker_index % len(nvidia_gpus))\n            return env\n\n        # use CPU\n        cpus = get_resource(resource_request, \"cpu\")\n        if cpus is not None and cpus > 0:\n            if \"cpu\" not in runnable_class.SUPPORTED_RESOURCES:\n                logger.warning(\n                    \"No known supported resource available for %s, falling back to using CPU.\",\n                    runnable_class,\n                )\n\n            if runnable_class.SUPPORTS_CPU_MULTI_THREADING:\n                if isinstance(workers_per_resource, float):\n                    raise ValueError(\n                        \"Fractional CPU multi threading support is not yet supported.\"\n                    )\n                env = {}\n                env[\"BENTOML_NUM_THREAD\"] = str(workers_per_resource)\n                return env\n\n            env = {}\n            env[\"OMP_NUM_THREADS\"] = str(workers_per_resource)\n            env[\"OPENBLAS_NUM_THREADS\"] = str(workers_per_resource)\n            env[\"MKL_NUM_THREADS\"] = str(workers_per_resource)\n            env[\"VECLIB_MAXIMUM_THREADS\"] = str(workers_per_resource)\n            env[\"NUMEXPR_NUM_THREADS\"] = str(workers_per_", "idx": 1851}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.batches_to_batch", "completion": "        # numpy.concatenate may consume lots of memory, need optimization later\n        indices = list(\n            itertools.accumulate(subbatch.shape[batch_dim] for subbatch in batches)\n        )\n        indices = [0] + indices\n        return np.concatenate(batches, axis=batch_dim), indices", "idx": 1852}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.to_payload", "completion": "        # skip 0-dimensional array\n        if batch.ndim == 0:\n            data = pickle.dumps(batch)\n            return cls.create_payload(data, batch_size=1)\n\n        # ensure that the ndarray is either C-contiguous or F-contiguous\n        if not batch.flags[\"C_CONTIGUOUS\"]:\n            batch = np.ascontiguousarray(batch)\n        if not batch.flags[\"F_CONTIGUOUS\"]:\n            batch = np.asfortranarray(batch)\n\n        # convert the ndarray into a byte string using the dump function with PEP 574 support\n        bs: bytes\n        concat_buffer_bs: bytes\n        indices: list[int]\n        bs, concat_buffer_bs, indices = pep574_dumps(batch)\n\n        if indices:\n            meta: dict[str, bool | int | float | str | list[int]] = {\n                \"format\": \"pickle5\",\n                \"with_buffer\": True,\n            }\n            data = concat_buffer_bs\n            meta[\"pickle_bytes_str\"] = base64.b64encode(bs).decode(\"ascii\")\n            meta[\"indices\"] = indices\n        else:\n            meta: dict[str, bool | int | float | str | list[int]] = {\n                \"format\": \"pickle5\",\n                \"with_buffer\": False,\n            }\n            data = bs\n\n        return cls.create_payload(\n            data,\n            batch.shape[batch_dim],\n            meta=meta,\n        )", "idx": 1853}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.from_payload", "completion": "        if payload.meta[\"format\"] == \"pickle5\":\n            bs_str = t.cast(str, payload.meta[\"pickle_bytes_str\"])\n            bs = base64.b64decode(bs_str)\n            indices = t.cast(t.List[int], payload.meta[\"indices\"])\n            return pep574_loads(bs, payload.data, indices)\n        else:\n            return fixed_torch_loads(payload.data)\n\n    ", "idx": 1854}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.batch_to_payloads", "completion": "        batches = cls.batch_to_batches(batch, indices, batch_dim)\n\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads", "idx": 1855}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.from_batch_payloads", "completion": "        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)", "idx": 1856}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.to_payload", "completion": "        if batch_dim != 0:\n            raise ValueError(\n                \"PandasDataFrameContainer only supports batch_dim of 0, but got {}\".format(\n                    batch_dim\n                )\n            )\n        if isinstance(batch, ext.PdSeries):\n            batch = batch.to_frame()\n        meta = {\"format\": \"pickle5\"}\n        bs: bytes\n        concat_buffer_bs: bytes\n        indices: list[int]\n        bs, concat_buffer_bs, indices = pep574_dumps(batch)\n        if indices:\n            meta[\"with_buffer\"] = True\n            bs_str = base64.b64encode(bs).decode(\"ascii\")\n            meta[\"pickle_bytes_str\"] = bs_str\n            meta[\"indices\"] = indices\n        else:\n            meta[\"with_buffer\"] = False\n            bs = concat_buffer_bs\n        return cls.create_payload(\n            bs,\n            batch.shape[batch_dim],\n            meta,\n        )\n\n    ", "idx": 1857}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.from_payload", "completion": "        import pandas as pd\n\n        format = payload.meta.get(\"format\", \"default\")\n        if format == \"pickle5\":\n            bs_str = t.cast(str, payload.meta[\"pickle_bytes_str\"])\n            bs = base64.b64decode(bs_str)\n            indices = t.cast(t.List[int], payload.meta[\"indices\"])\n            return t.cast(\"ext.PdDataFrame\", pep574_loads(bs, payload.data, indices))\n\n        if payload.meta.get(\"with_buffer\", False):\n            return t.cast(\"ext.PdDataFrame\", pd.read_pickle(io.BytesIO(payload.data)))\n\n        return t.cast(\"ext.PdDataFrame\", pd.read_pickle(io.BytesIO(payload.data)))\n\n    ", "idx": 1858}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batch_to_payloads", "completion": "        batches = cls.batch_to_batches(batch, indices, batch_dim)\n\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads\n\n    ", "idx": 1859}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.from_batch_payloads", "completion": "        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)", "idx": 1860}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.to_payload", "completion": "        bs = pickle.dumps(batch)\n        return cls.create_payload(bs, batch_size=len(batch))\n", "idx": 1861}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batch_to_payloads", "completion": "        batches = cls.batch_to_batches(batch, indices, batch_dim)\n\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads", "idx": 1862}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.from_batch_payloads", "completion": "        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)", "idx": 1863}
{"namespace": "sslyze.cli.server_string_parser.CommandLineServerStringParser.parse_server_string", "completion": "        if \"{\" in server_str:\n            ip = server_str.split(\"{\")[1].split(\"}\")[0]\n            server_str = server_str.split(\"{\")[0]\n        else:\n            ip = None\n\n        if \"[\" in server_str:\n            if \"]\" in server_str:\n                return cls._parse_ipv6_server_string(server_str)\n            else:\n                raise InvalidServerStringError(server_string=server_str, error_message=cls.SERVER_STRING_ERROR_BAD_PORT)\n        elif ip and \"[\" in ip:\n            return cls._parse_ipv6_server_string(ip)\n        else:\n            return cls._parse_ipv4_server_string(server_str)\n\n    ", "idx": 1864}
{"namespace": "sslyze.plugins.heartbleed_plugin._HeartbleedCliConnector.result_to_console_output", "completion": "        return [\n            \"Heartbleed\",\n            \"=\" * 79,\n            f\"Is the server vulnerable to Heartbleed? {result.is_vulnerable_to_heartbleed}\",\n        ]", "idx": 1865}
{"namespace": "sslyze.plugins.http_headers_plugin._HttpHeadersCliConnector.result_to_console_output", "completion": "        output = []\n        if result.http_error_trace:\n            output.append(\"The server returned an error when receiving the initial HTTP request:\")\n            for line in result.http_error_trace.format(chain=False):\n                output.append(line)\n        else:\n            output.append(f\"The server returned the following HTTP headers:\")\n            if result.strict_transport_security_header:\n                output.append(\n                    f\"Strict-Transport-Security: max-age={result.strict_transport_security_header.max_age}, \"\n                    f\"preload={result.strict_transport_security_header.preload}, \"\n                    f\"includesubdomains={result.strict_transport_security_header.include_subdomains}\"\n                )\n            if result.expect_ct_header:\n                output.append(\n                    f\"Expect-CT: max-age={result.expect_ct_header.max_age}, \"\n                    f\"report-uri={result.expect_ct_header.report_uri}, \"\n                    f\"enforce={result.expect_ct_header.enforce}\"\n                )\n            if result.http_path_redirected_to:\n                output.append(f\"The server redirected the initial HTTP request to {result.http_path_redirected_to}\")\n\n        return output", "idx": 1866}
{"namespace": "sslyze.plugins.http_headers_plugin._detect_http_redirection", "completion": "    # Check if the response contains a Location header\n    location_header = http_response.getheader(\"Location\", None)\n    if not location_header:\n        return None\n\n    # Check if the Location header contains the same server\n    location_url = urlsplit(location_header)\n    if location_url.hostname != server_host_name or location_url.port != server_port:\n        return None\n\n    # Return the path of the new location\n    return location_url.path", "idx": 1867}
{"namespace": "sslyze.plugins.session_renegotiation_plugin._SessionRenegotiationCliConnector.result_to_console_output", "completion": "        result_txt = []\n        result_txt.append(\"Session Renegotiation:\")\n        result_txt.append(f\"  - Supports secure renegotiation: {result.supports_secure_renegotiation}\")\n        result_txt.append(f\"  - Is vulnerable to client-initiated renegotiation DoS: {result.is_vulnerable_to_client_renegotiation_dos}\")\n        return result_txt", "idx": 1868}
{"namespace": "sslyze.plugins.certificate_info._cli_connector._CertificateInfoCliConnector.result_to_console_output", "completion": "        result_as_txt = [\n            cls._format_subtitle(\"Certificate Information\"),\n            cls._format_field(\"Hostname:\", result.hostname),\n            cls._format_field(\"Number of certificates detected:\", str(len(result.certificate_deployments))),\n        ]\n\n        for index, cert_deployment in enumerate(result.certificate_deployments):\n            result_as_txt.extend(cls._cert_deployment_to_console_output(index, cert_deployment))\n\n        return result_as_txt\n\n    ", "idx": 1869}
{"namespace": "sslyze.plugins.certificate_info._cli_connector._get_name_as_short_text", "completion": "    # Check if there is a common name (CN) in the name field\n    for field in name_field:\n        if field.oid == x509.NameOID.COMMON_NAME:\n            return field.value\n\n    # If there is no CN, return the entire name field as a string\n    return str(name_field)", "idx": 1870}
{"namespace": "sslyze.plugins.certificate_info._symantec.SymantecDistructTester.get_distrust_timeline", "completion": "        # Check if the chain contains any blacklisted certificates\n        for cert in verified_certificate_chain:\n            if binascii.hexlify(cert.public_key().public_bytes(serialization.Encoding.DER, serialization.PublicFormat.SubjectPublicKeyInfo)).decode() in cls._CA_KEYS_BLACKLIST:\n                return SymantecDistrustTimelineEnum.MARCH_2018\n\n        # Check if the chain contains any whitelisted certificates\n        for cert in verified_certificate_chain:\n            if binascii.hexlify(cert.public_key().public_bytes(serialization.Encoding.DER, serialization.PublicFormat.SubjectPublicKeyInfo)).decode() in cls._CA_KEYS_WHITELIST:\n                return SymantecDistrustTimelineEnum.SEPTEMBER_2018\n\n        # If no blacklisted or whitelisted certificates are found, return None\n        return None\n", "idx": 1871}
{"namespace": "sslyze.plugins.certificate_info._certificate_utils.parse_subject_alternative_name_extension", "completion": "    san_extension = cast(\n        SubjectAlternativeName, certificate.extensions.get_extension_for_oid(ExtensionOID.SUBJECT_ALTERNATIVE_NAME)\n    )\n    dns_names = [cast(DNSName, name).value for name in san_extension.get_values_for_type(DNSName)]\n    ip_addresses = [cast(IPAddress, name).value for name in san_extension.get_values_for_type(IPAddress)]\n    return SubjectAlternativeNameExtension(dns_names=dns_names, ip_addresses=ip_addresses)\n\n", "idx": 1872}
{"namespace": "sslyze.plugins.certificate_info._cert_chain_analyzer._certificate_matches_hostname", "completion": "    try:\n        match_hostname(certificate, server_hostname)\n        return True\n    except CertificateError:\n        return False", "idx": 1873}
