{"namespace": "aioxmpp.roster.service.RosterClient.approve", "completion": "        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.SUBSCRIBED,\n                            to=peer_jid)\n        )", "idx": 0}
{"namespace": "aioxmpp.roster.service.RosterClient.subscribe", "completion": "        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.SUBSCRIBE,\n                            to=peer_jid)\n        )", "idx": 1}
{"namespace": "aioxmpp.roster.service.RosterClient.unsubscribe", "completion": "        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.UNSUBSCRIBE,\n                            to=peer_jid)\n        )", "idx": 2}
{"namespace": "aioxmpp.forms.fields.BoundSingleValueField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass", "idx": 3}
{"namespace": "aioxmpp.forms.fields.BoundMultiValueField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass", "idx": 4}
{"namespace": "aioxmpp.forms.fields.BoundOptionsField.options", "completion": "        try:\n            del self._options\n        except AttributeError:\n            pass", "idx": 5}
{"namespace": "aioxmpp.forms.fields.BoundSelectField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass", "idx": 6}
{"namespace": "aioxmpp.forms.fields.BoundMultiSelectField.value", "completion": "        try:\n            del self._value\n        except AttributeError:\n            pass", "idx": 7}
{"namespace": "cupy.random._generator.reset_states", "completion": "    global _random_states\n    _random_states = {}", "idx": 8}
{"namespace": "cupy.random._generator._check_and_get_dtype", "completion": "    dtype = numpy.dtype(dtype)\n    if dtype.char not in ('f', 'd'):\n        raise ValueError('cupy.random only supports float32 and float64')\n    return dtype", "idx": 9}
{"namespace": "cupy_builder._command.filter_files_by_extension", "completion": "    files_with_extension = [file for file in sources if file.endswith(extension)]\n    other_files = [file for file in sources if not file.endswith(extension)]\n    return files_with_extension, other_files", "idx": 10}
{"namespace": "datasets.table._in_memory_arrow_table_from_file", "completion": "    with open(filename, \"rb\") as source:\n        reader = pa.ipc.open_file(source)\n        table = reader.read_all()\n    return table", "idx": 11}
{"namespace": "datasets.table._in_memory_arrow_table_from_buffer", "completion": "    in_memory_stream = pa.BufferReader(buffer)\n    opened_stream = pa.ipc.open_stream(in_memory_stream)\n    pa_table = opened_stream.read_all()\n    return pa_table", "idx": 12}
{"namespace": "datasets.table._interpolation_search", "completion": "    if not arr:\n        raise IndexError(\"The array is empty.\")\n    if x < arr[0] or x > arr[-1]:\n        raise IndexError(\"The query is outside the array values.\")\n    \n    low = 0\n    high = len(arr) - 1\n\n    while arr[low] <= x and arr[high] >= x:\n        if arr[high] - arr[low] == 0:\n            return low if arr[low] == x else low + 1\n        pos = low + int(((float(x - arr[low]) * (high - low)) / (arr[high] - arr[low])))\n\n        if arr[pos] < x:\n            low = pos + 1\n        elif arr[pos] > x:\n            high = pos - 1\n        else:\n            return pos\n\n    if arr[low] == x:\n        return low\n\n    return low + 1", "idx": 13}
{"namespace": "datasets.data_files._is_inside_unrequested_special_dir", "completion": "    special_dirs_in_path = [\n        part for part in PurePath(matched_rel_path).parts if part.startswith(\"__\") and not set(part) == {\"__\"}\n    ]\n    special_dirs_in_pattern = [\n        part for part in PurePath(pattern).parts if part.startswith(\"__\") and not set(part) == {\"__\"}\n    ]\n    return len(special_dirs_in_path) != len(special_dirs_in_pattern)", "idx": 14}
{"namespace": "datasets.data_files._is_unrequested_hidden_file_or_is_inside_unrequested_hidden_dir", "completion": "    # Check if the path is a hidden file\n    if matched_rel_path.startswith('.'):\n        return True\n\n    # Check if the path is inside a hidden directory\n    hidden_dir_pattern = re.compile(r'/\\..+?/')\n    if hidden_dir_pattern.search(matched_rel_path):\n        return True\n\n    return False", "idx": 15}
{"namespace": "datasets.iterable_dataset._batch_to_examples", "completion": "    # Get the number of examples in the batch\n    num_examples = len(next(iter(batch.values())))\n\n    # Create a list of examples\n    examples = [{} for _ in range(num_examples)]\n\n    # Iterate over each column in the batch\n    for column_name, column_values in batch.items():\n        # For each example, add the value for the current column\n        for i, value in enumerate(column_values):\n            examples[i][column_name] = value\n\n    return examples", "idx": 16}
{"namespace": "datasets.iterable_dataset._examples_to_batch", "completion": "    columns = list({k for example in examples for k in example.keys()})\n    arrays = [[example[col] for example in examples] for col in columns]\n    return dict(zip(columns, arrays))", "idx": 17}
{"namespace": "datasets.iterable_dataset.RandomlyCyclingMultiSourcesExamplesIterable._iter_random_indices", "completion": "        while True:\n            yield from rng.choice(num_sources, size=random_batch_size, p=p)", "idx": 18}
{"namespace": "datasets.iterable_dataset.BufferShuffledExamplesIterable._iter_random_indices", "completion": "        while True:\n            yield from rng.integers(0, buffer_size, size=random_batch_size)", "idx": 19}
{"namespace": "datasets.iterable_dataset.IterableDataset.remove_columns", "completion": "        if isinstance(column_names, str):\n            column_names = [column_names]\n\n        # Create a new ex_iterable with the specified columns removed\n        ex_iterable = MappedExamplesIterable(\n            self._ex_iterable,\n            function=lambda example: {k: v for k, v in example.items() if k not in column_names},\n            remove_columns=column_names,\n        )\n\n        # Update the features in the dataset info\n        info = self._info.copy()\n        if self._info.features is not None:\n            info.features = Features({k: v for k, v in self._info.features.items() if k not in column_names})\n\n        return IterableDataset(\n            ex_iterable=ex_iterable,\n            info=info,\n            split=self._split,\n            formatting=self._formatting,\n            shuffling=copy.deepcopy(self._shuffling),\n            distributed=copy.deepcopy(self._distributed),\n            token_per_repo_id=self._token_per_repo_id,\n        )", "idx": 20}
{"namespace": "datasets.dataset_dict.DatasetDict.with_format", "completion": "        self._check_values_type()\n        return DatasetDict(\n            {\n                k: dataset.with_format(\n                    type=type, columns=columns, output_all_columns=output_all_columns, **format_kwargs\n                )\n                for k, dataset in self.items()\n            }\n        )", "idx": 21}
{"namespace": "datasets.dataset_dict.DatasetDict.with_transform", "completion": "        self._check_values_type()\n        return DatasetDict({k: dataset.with_transform(transform=transform, columns=columns, output_all_columns=output_all_columns) for k, dataset in self.items()})", "idx": 22}
{"namespace": "datasets.dataset_dict.DatasetDict.align_labels_with_mapping", "completion": "        def align_labels(example):\n            example[label_column] = label2id[example[label_column]]\n            return example\n\n        return self.map(align_labels)", "idx": 23}
{"namespace": "datasets.dataset_dict.IterableDatasetDict.map", "completion": "        return IterableDatasetDict(\n            {\n                k: dataset.map(\n                    function=function,\n                    with_indices=with_indices,\n                    input_columns=input_columns,\n                    batched=batched,\n                    batch_size=batch_size,\n                    drop_last_batch=drop_last_batch,\n                    remove_columns=remove_columns,\n                    fn_kwargs=fn_kwargs,\n                )\n                for k, dataset in self.items()\n            }\n        )", "idx": 24}
{"namespace": "datasets.dataset_dict.IterableDatasetDict.filter", "completion": "        return IterableDatasetDict(\n            {\n                k: dataset.filter(\n                    function=function,\n                    with_indices=with_indices,\n                    input_columns=input_columns,\n                    batched=batched,\n                    batch_size=batch_size,\n                    fn_kwargs=fn_kwargs,\n                )\n                for k, dataset in self.items()\n            }\n        )", "idx": 25}
{"namespace": "datasets.arrow_dataset.Dataset.num_rows", "completion": "        if self._indices is not None:\n            return self._indices.num_rows\n        else:\n            return self._data.num_rows", "idx": 26}
{"namespace": "datasets.filesystems.extract_path_from_uri", "completion": "    parsed_uri = fsspec.utils.parse_path(dataset_path)\n    return parsed_uri['path']", "idx": 27}
{"namespace": "datasets.filesystems.is_remote_filesystem", "completion": "    return fs.protocol not in [\"file\", \"memory\"]", "idx": 28}
{"namespace": "datasets.utils.file_utils.hash_url_to_filename", "completion": "    url_bytes = url.encode(\"utf-8\")\n    url_hash = sha256(url_bytes)\n    filename = url_hash.hexdigest()\n\n    if etag:\n        etag_bytes = etag.encode(\"utf-8\")\n        etag_hash = sha256(etag_bytes)\n        filename += \".\" + etag_hash.hexdigest()\n\n    if url.endswith(\".h5\"):\n        filename += \".h5\"\n\n    return filename", "idx": 29}
{"namespace": "datasets.utils.hub.hf_hub_url", "completion": "\n    if version.parse(hfh.__version__) < version.parse(\"0.11.0\"):\n        path = quote(path)\n\n    if revision is None:\n        url = f\"https://huggingface.co/{repo_id}/resolve/main/{path}\"\n    else:\n        url = f\"https://huggingface.co/{repo_id}/resolve/{revision}/{path}\"\n\n    return url", "idx": 30}
{"namespace": "datasets.utils.sharding._number_of_shards_in_gen_kwargs", "completion": "\n    list_lengths = [len(value) for value in gen_kwargs.values() if isinstance(value, list)]\n    if not list_lengths:\n        return 1\n    if len(set(list_lengths)) > 1:\n        raise ValueError(\"All lists in gen_kwargs must have the same length, but got lengths: {}\".format(list_lengths))\n    return list_lengths[0]", "idx": 31}
{"namespace": "datasets.utils.sharding._distribute_shards", "completion": "\n    # Calculate the number of shards per job\n    shards_per_job = num_shards // max_num_jobs\n    # Calculate the remaining shards after equal distribution\n    remaining_shards = num_shards % max_num_jobs\n\n    # Initialize the list of shard ranges\n    shard_ranges = []\n\n    # Distribute the shards among the jobs\n    start = 0\n    for i in range(max_num_jobs):\n        # Calculate the end index of the shard range for the current job\n        end = start + shards_per_job\n        # If there are remaining shards, add one to the current job and decrease the remaining shards\n        if remaining_shards > 0:\n            end += 1\n            remaining_shards -= 1\n        # Add the shard range to the list\n        shard_ranges.append(range(start, end))\n        # Update the start index for the next job\n        start = end\n\n    return shard_ranges", "idx": 32}
{"namespace": "datasets.utils.py_utils.temporary_assignment", "completion": "    original_value = getattr(obj, attr)\n    setattr(obj, attr, value)\n    try:\n        yield\n    finally:\n        setattr(obj, attr, original_value)", "idx": 33}
{"namespace": "datasets.utils.extract.TarExtractor.extract", "completion": "        os.makedirs(output_path, exist_ok=True)\n        with tarfile.open(input_path, \"r\") as tar_file:\n            tar_file.extractall(path=output_path, members=TarExtractor.safemembers(tar_file.getmembers(), output_path))\n            tar_file.close()", "idx": 34}
{"namespace": "datasets.utils.extract.Extractor.infer_extractor_format", "completion": "        magic_number = cls._read_magic_number(path, cls._get_magic_number_max_length())\n        for extractor_format, extractor in cls.extractors.items():\n            if extractor.is_extractable(path, magic_number=magic_number):\n                return extractor_format\n        return \"\"", "idx": 35}
{"namespace": "datasets.utils.py_utils.asdict", "completion": "    if is_dataclass(obj):\n        return {field.name: asdict(getattr(obj, field.name)) for field in fields(obj)}\n    elif isinstance(obj, tuple) and hasattr(obj, '_fields'):  # namedtuple\n        return {field: asdict(getattr(obj, field)) for field in obj._fields}\n    elif isinstance(obj, (list, tuple)):\n        return [asdict(item) for item in obj]\n    elif isinstance(obj, dict):\n        return {key: asdict(value) for key, value in obj.items()}\n    else:\n        return obj", "idx": 36}
{"namespace": "datasets.utils.metadata.MetadataConfigs.from_dataset_card_data", "completion": "        metadata_configs = {}\n        if cls.FIELD_NAME in dataset_card_data:\n            for metadata_config in dataset_card_data[cls.FIELD_NAME]:\n                config_name = metadata_config.pop(\"config_name\", None)\n                if config_name is not None:\n                    cls._raise_if_data_files_field_not_valid(metadata_config)\n                    metadata_configs[config_name] = metadata_config\n        return cls(metadata_configs)", "idx": 37}
{"namespace": "pymorphy2.analyzer.lang_dict_path", "completion": "    lang_paths = _lang_dict_paths()\n    if lang in lang_paths:\n        return lang_paths[lang]\n    else:\n        raise ValueError(f\"Dictionary for language '{lang}' is not installed. Please install the dictionary for '{lang}'.\")", "idx": 38}
{"namespace": "pymorphy2.dawg.assert_can_create", "completion": "    if not EXTENSION_AVAILABLE:\n        raise NotImplementedError(\"The required extension is not available.\")", "idx": 39}
{"namespace": "pymorphy2.opencorpora_dict.compile._to_paradigm", "completion": "    stem = None\n    paradigm = []\n    for word, tag in lexeme:\n        prefix = next((p for p in paradigm_prefixes if word.startswith(p)), '')\n        if stem is None:\n            stem = word[len(prefix):]\n        elif word[len(prefix):].startswith(stem):\n            prefix = word[:-len(stem)]\n        else:\n            stem = ''\n            prefix = word\n        suffix = word[len(prefix)+len(stem):]\n        paradigm.append((suffix, tag, prefix))\n    return stem, tuple(paradigm)", "idx": 40}
{"namespace": "pymorphy2.units.by_analogy.KnownPrefixAnalyzer.tag", "completion": "        result = []\n        for prefix, unprefixed_word in self.possible_splits(word_lower):\n            tags = self.morph.tag(unprefixed_word)\n            for tag in tags:\n                if not tag.is_productive():\n                    continue\n                add_tag_if_not_seen(tag, result, seen_tags)\n        return result", "idx": 41}
{"namespace": "pymorphy2.units.by_analogy.UnknownPrefixAnalyzer.tag", "completion": "        result = []\n        for prefix, unprefixed_word in word_splits(word_lower):\n            tags = self.dict_analyzer.tag(unprefixed_word, unprefixed_word, seen_tags)\n            for tag in tags:\n                if not tag.is_productive():\n                    continue\n                add_tag_if_not_seen(tag, result, seen_tags)\n        return result", "idx": 42}
{"namespace": "benedict.dicts.keylist.keylist_util.get_item", "completion": "    items = get_items(d, keys)\n    if items:\n        return items[-1]\n    else:\n        return (None, None, None)", "idx": 43}
{"namespace": "benedict.dicts.keylist.keylist_util.set_item", "completion": "\n    item = d\n    for key in keys[:-1]:\n        subkey = keys[keys.index(key) + 1]\n        item = _get_or_new_item_value(item, key, subkey)\n    _set_item_value(item, keys[-1], value)", "idx": 44}
{"namespace": "benedict.dicts.keypath.keypath_util._split_key_indexes", "completion": "    match = re.search(KEY_INDEX_RE, key)\n    if match:\n        index = int(match.group(1))\n        key = re.sub(KEY_INDEX_RE, '', key)\n        return [key, index] if key else [index]\n    return [key]", "idx": 45}
{"namespace": "feedparser.urls.make_safe_absolute_uri", "completion": "    if not ACCEPTABLE_URI_SCHEMES:\n        return ''\n\n    \"\"\"\n    This function creates a safe absolute URI by joining a base URL and a relative URL. If the base URL is empty, it returns the relative URL. If the relative URL is empty, it outputs the base URL. Finally, if the resulting URI's scheme is not acceptable, it returns an empty string. Otherwise, it returns the resulting URI.\n    Input-Output Arguments\n    :param base: String. The base URL to join with the relative URL.\n    :param rel: String. The relative URL to join with the base URL. Defaults to None.\n    :return: String. The safe absolute URI created by joining the base and relative URLs.\n    \"\"\"\n    if not base:\n        return rel\n    if not rel:\n        return base\n\n    # Join the base and relative URLs\n    uri = _urljoin(base, rel)\n\n    # Parse the resulting URI\n    parts = urllib.parse.urlsplit(uri)\n\n    # If the scheme is not acceptable, return an empty string\n    if parts.scheme not in ACCEPTABLE_URI_SCHEMES:\n        return ''\n\n    # Otherwise, return the resulting URI\n    return uri", "idx": 46}
{"namespace": "feedparser.api._open_resource", "completion": "    if hasattr(url_file_stream_or_string, 'read'):\n        return url_file_stream_or_string.read()\n    if url_file_stream_or_string.startswith('http:') or url_file_stream_or_string.startswith('https:'):\n        return _fetch_url(url_file_stream_or_string, etag, modified, agent, referrer, handlers, request_headers, result)\n    elif os.path.exists(url_file_stream_or_string):\n        with open(url_file_stream_or_string, 'rb') as f:\n            return f.read()\n    else:\n        return url_file_stream_or_string.encode('utf-8')", "idx": 47}
{"namespace": "feedparser.http._build_urllib2_request", "completion": "    request = urllib.request.Request(url)\n    if agent:\n        request.add_header(\"User-Agent\", agent)\n    if accept_header:\n        request.add_header(\"Accept\", accept_header)\n    if etag:\n        request.add_header(\"If-None-Match\", etag)\n    if modified:\n        if isinstance(modified, datetime.datetime):\n            modified = modified.strftime('%a, %d %b %Y %H:%M:%S GMT')\n        request.add_header(\"If-Modified-Since\", modified)\n    if referrer:\n        request.add_header(\"Referer\", referrer)\n    if auth:\n        request.add_header(\"Authorization\", \"Basic \" + auth)\n    if request_headers:\n        for key, value in request_headers.items():\n            request.add_header(key, value)\n    return request", "idx": 48}
{"namespace": "pylatex.utils.dumps_list", "completion": "\n    if mapper is None:\n        mapper = _latex_item_to_string\n\n    if not _is_iterable(mapper):\n        mapper = [mapper]\n\n    for m in mapper:\n        l = map(m, l)\n\n    return NoEscape(token.join(l))", "idx": 49}
{"namespace": "pylatex.utils._latex_item_to_string", "completion": "\n    if isinstance(item, pylatex.base_classes.LatexObject):\n        if as_content:\n            s = item.dumps_as_content()\n        else:\n            s = item.dumps()\n    else:\n        s = str(item)\n\n    if escape:\n        s = escape_latex(s)\n\n    return NoEscape(s)", "idx": 50}
{"namespace": "mistune.markdown.Markdown.read", "completion": "        with open(filepath, 'r', encoding=encoding) as f:\n            content = f.read()\n        return self.parse(content, state)[0]", "idx": 51}
{"namespace": "mistune.create_markdown", "completion": "    if renderer == 'html':\n        renderer = HTMLRenderer(escape=escape, hard_wrap=hard_wrap)\n    elif isinstance(renderer, str):\n        raise ValueError('Unknown renderer: %s' % renderer)\n\n    if plugins is None:\n        plugins = []\n\n    md = Markdown(renderer)\n    for name in plugins:\n        plugin = import_plugin(name)\n        if plugin:\n            md.use(plugin)\n\n    return md", "idx": 52}
{"namespace": "parsel.utils.extract_regex", "completion": "    if isinstance(regex, str):\n        regex = re.compile(regex)\n    matches = regex.finditer(text)\n    extracted = []\n    for match in matches:\n        if \"extract\" in match.groupdict():\n            extracted.append(match.group(\"extract\"))\n        elif match.groups():\n            extracted.extend(flatten(match.groups()))\n        else:\n            extracted.append(match.group(0))\n    if replace_entities:\n        extracted = [w3lib_replace_entities(e) for e in extracted]\n    return extracted", "idx": 53}
{"namespace": "dominate.dom_tag.dom_tag.render", "completion": "    sb = []\n    self._render(sb, 0, indent, pretty, xhtml)\n    return ''.join(sb)", "idx": 54}
{"namespace": "dominate.util.include", "completion": "  with open(f, 'r') as file:\n    data = file.read()\n  return data", "idx": 55}
{"namespace": "dominate.util.unescape", "completion": "  def replace_entities(match):\n    entity = match.group(1)\n    if entity in _unescape:\n      return unichr(_unescape[entity])\n    else:\n      return match.group(0)\n\n  return re.sub(r'&([^;]+);', replace_entities, data)", "idx": 56}
{"namespace": "onlinejudge_command.pretty_printers._tokenize_line", "completion": "    tokens = []\n    l = 0\n    while l < len(line):\n        r = l + 1\n        while r < len(line) and (line[l] in ' \\t') == (line[r] in ' \\t'):\n            r += 1\n        if line[l] in ' \\t':\n            typ = _PrettyTokenType.WHITESPACE\n        elif line[l] == '\\n':\n            typ = _PrettyTokenType.NEWLINE\n        else:\n            typ = _PrettyTokenType.BODY\n        tokens.append(_PrettyToken(typ, line[l:r]))\n        l = r\n    return tokens", "idx": 57}
{"namespace": "onlinejudge_command.pretty_printers._render_tokens", "completion": "\n    if font_bold is None:\n        font_bold = lambda s: s\n    if font_dim is None:\n        font_dim = lambda s: s\n    if font_red is None:\n        font_red = lambda s: s\n    if font_blue is None:\n        font_blue = lambda s: s\n    if font_normal is None:\n        font_normal = lambda s: s\n\n    result = []\n    for token in tokens:\n        if token.type == _PrettyTokenType.BODY:\n            result.append(font_normal(token.value))\n        elif token.type == _PrettyTokenType.BODY_HIGHLIGHT_LEFT:\n            result.append(font_red(font_bold(token.value)))\n        elif token.type == _PrettyTokenType.BODY_HIGHLIGHT_RIGHT:\n            result.append(font_blue(font_bold(token.value)))\n        elif token.type == _PrettyTokenType.WHITESPACE:\n            result.append(font_dim(_replace_whitespace(token.value)))\n        elif token.type == _PrettyTokenType.NEWLINE:\n            result.append(font_dim(_replace_whitespace(token.value)))\n        elif token.type == _PrettyTokenType.HINT:\n            result.append(font_dim(token.value))\n        elif token.type == _PrettyTokenType.LINENO:\n            result.append(font_dim(token.value))\n        elif token.type == _PrettyTokenType.OTHERS:\n            result.append(font_dim(token.value))\n        else:\n            assert False\n    return ''.join(result)", "idx": 58}
{"namespace": "onlinejudge_command.pretty_printers._tokenize_file_content_without_snipping", "completion": "\n    tokens, text = _decode_with_recovery(content)\n    if text:\n        for line in text.splitlines(keepends=True):\n            tokens += _tokenize_line(line)\n    tokens = _warn_if_empty(tokens)\n    return tokens", "idx": 59}
{"namespace": "jinja2.environment.Environment.get_template", "completion": "        if isinstance(name, Template):\n            return name\n        if parent is not None:\n            name = self.join_path(name, parent)\n        return self._load_template(name, globals)", "idx": 60}
{"namespace": "jinja2.environment.Environment.from_string", "completion": "        if template_class is None:\n            template_class = self.template_class\n\n        if isinstance(source, nodes.Template):\n            ast = source\n        else:\n            ast = self._parse(source, None, None)\n\n        code = self._generate(ast, None, None)\n        return template_class.from_code(self, code, self.make_globals(globals))", "idx": 61}
{"namespace": "jinja2.environment.Template.render", "completion": "                    import asyncio\n        if self.environment.is_async:\n            import asyncio\n            return asyncio.run(self.render_async(*args, **kwargs))\n        ctx = self.new_context(dict(*args, **kwargs))\n        try:\n            return self.environment.concat(self.root_render_func(ctx))  # type: ignore\n        except Exception:\n            return self.environment.handle_exception()", "idx": 62}
{"namespace": "jinja2.utils.generate_lorem_ipsum", "completion": "    words = (\n        \"lorem ipsum dolor sit amet consectetur adipisicing elit sed do eiusmod tempor incididunt ut labore et dolore magna aliqua ut enim ad minim veniam quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur excepteur sint occaecat cupidatat non proident sunt in culpa qui officia deserunt mollit anim id est laborum\".split()\n    )\n    result = []\n\n    for _ in range(n):\n        paragraph = [choice(words).capitalize()]\n        for i in range(randrange(min, max)):\n            paragraph.append(choice(words))\n            if i % randrange(3, 8) == 0:\n                paragraph.append(\",\")\n            if i % randrange(10, 20) == 0:\n                paragraph.append(\".\")\n                paragraph.append(choice(words).capitalize())\n        paragraph[-1] = \".\"\n        result.append(\" \".join(paragraph))\n\n    if html:\n        return \"\\n\".join(f\"<p>{p}</p>\" for p in result)\n    else:\n        return \"\\n\\n\".join(result)", "idx": 63}
{"namespace": "jinja2.utils.LRUCache.clear", "completion": "        with self._wlock:\n            self._mapping.clear()\n            self._queue.clear()", "idx": 64}
{"namespace": "jinja2.utils.LRUCache.items", "completion": "        with self._wlock:\n            return [(key, self._mapping[key]) for key in reversed(self._queue)]", "idx": 65}
{"namespace": "jinja2.idtracking.symbols_for_node", "completion": "    sym = Symbols(parent=parent_symbols)\n    visitor = FrameSymbolVisitor(sym)\n    visitor.visit(node)\n    return sym", "idx": 66}
{"namespace": "jinja2.idtracking.Symbols.find_ref", "completion": "        if name in self.refs:\n            return self.refs[name]\n\n        if self.parent is not None:\n            return self.parent.find_ref(name)\n\n        return None", "idx": 67}
{"namespace": "jinja2.idtracking.Symbols.dump_stores", "completion": "        rv = {}\n        node: t.Optional[\"Symbols\"] = self\n\n        while node is not None:\n            for target in node.stores:\n                rv[target] = node.ref(target)\n            node = node.parent\n\n        return rv", "idx": 68}
{"namespace": "jinja2.meta.find_undeclared_variables", "completion": "    code_gen = TrackingCodeGenerator(ast.environment)\n    code_gen.visit(ast)\n    return code_gen.undeclared_identifiers", "idx": 69}
{"namespace": "jinja2.loaders.split_template_path", "completion": "    pieces = []\n    for piece in template.split(\"/\"):\n        if path.sep in piece or (path.altsep and path.altsep in piece) or piece == path.pardir:\n            raise TemplateNotFound(template)\n        elif piece and piece != '.':\n            pieces.append(piece)\n    return pieces", "idx": 70}
{"namespace": "jinja2.bccache.MemcachedBytecodeCache.load_bytecode", "completion": "        key = self.prefix + bucket.key\n        try:\n            value = self.client.get(key)\n            if value is not None:\n                bucket.bytecode_from_string(value)\n        except Exception:\n            if not self.ignore_memcache_errors:\n                raise", "idx": 71}
{"namespace": "jinja2.bccache.MemcachedBytecodeCache.dump_bytecode", "completion": "        try:\n            self.client.set(self.prefix + bucket.key, bucket.bytecode_to_string(), self.timeout)\n        except Exception:\n            if not self.ignore_memcache_errors:\n                raise", "idx": 72}
{"namespace": "sumy.utils.get_stop_words", "completion": "    language = normalize_language(language)\n    path = expand_resource_path(\"stopwords/%s.txt\" % language)\n    if not exists(path):\n        raise LookupError(\"Stop-words for language '%s' not found.\" % language)\n\n    return read_stop_words(path)", "idx": 73}
{"namespace": "sumy._compat.to_bytes", "completion": "    if isinstance(object, bytes):\n        return object\n    elif isinstance(object, unicode):\n        return object.encode(\"utf-8\")\n    else:\n        # try encode instance to bytes\n        return instance_to_bytes(object)", "idx": 74}
{"namespace": "sumy._compat.to_unicode", "completion": "    if isinstance(object, unicode):\n        return object\n    elif isinstance(object, bytes):\n        return object.decode(\"utf-8\")\n    else:\n        # try to decode instance to unicode\n        return instance_to_unicode(object)", "idx": 75}
{"namespace": "sumy.summarizers.lsa.LsaSummarizer._create_dictionary", "completion": "        words = map(self.normalize_word, document.words)\n        words = [word for word in words if word not in self._stop_words]\n\n        unique_words = frozenset(words)\n\n        return dict((word, i) for i, word in enumerate(unique_words))", "idx": 76}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._get_content_words_in_sentence", "completion": "        words = sentence.words\n        normalized_words = self._normalize_words(words)\n        content_words = self._filter_out_stop_words(normalized_words)\n        stemmed_content_words = self._stem_words(content_words)\n        return stemmed_content_words", "idx": 77}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._get_all_content_words_in_doc", "completion": "        all_words = self._get_all_words_in_doc(sentences)\n        content_words = self._filter_out_stop_words(all_words)\n        normalized_content_words = self._normalize_words(content_words)\n        return normalized_content_words", "idx": 78}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_tf", "completion": "        content_words_in_doc = self._get_all_content_words_in_doc(sentences)\n        word_freq_in_doc = self._compute_word_freq(content_words_in_doc)\n        total_content_words_count = len(content_words_in_doc)\n\n        tf = {}\n        for word, freq in word_freq_in_doc.items():\n            tf[word] = freq / total_content_words_count\n\n        return tf", "idx": 79}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_ratings", "completion": "        ratings = {}\n        word_freq = self._compute_tf(sentences)\n        sentences_as_words = [self._get_content_words_in_sentence(s) for s in sentences]\n        for i in range(len(sentences)):\n            best_sentence_index = self._find_index_of_best_sentence(word_freq, sentences_as_words)\n            best_sentence = sentences[best_sentence_index]\n            ratings[best_sentence] = (i + 1) * -1\n            word_freq = self._update_tf(word_freq, sentences_as_words[best_sentence_index])\n            del sentences_as_words[best_sentence_index]\n        return ratings", "idx": 80}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.cue_method", "completion": "        self.__check_bonus_words()\n        self.__check_stigma_words()\n\n        # Create an instance of EdmundsonCueMethod with adjusted weights for bonus and stigma words\n        cue_method_instance = EdmundsonCueMethod(\n            self._stemmer, self._bonus_words, self._stigma_words,\n            bonus_word_value=bonus_word_value, stigma_word_value=stigma_word_value\n        )\n\n        # Generate ratings for sentences in the document using the cue method\n        ratings = cue_method_instance.rate_sentences(document)\n\n        # Select and return the best sentences based on the generated ratings and requested sentence count\n        return self._get_best_sentences(document.sentences, sentences_count, ratings)", "idx": 81}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.key_method", "completion": "        summarization_method = self._build_key_method_instance()\n        return summarization_method(document, sentences_count, weight)", "idx": 82}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.title_method", "completion": "        summarization_method = self._build_title_method_instance()\n        return summarization_method(document, sentences_count)", "idx": 83}
{"namespace": "sumy.summarizers.edmundson.EdmundsonSummarizer.location_method", "completion": "        summarization_method = self._build_location_method_instance()\n        return summarization_method(document, sentences_count, w_h, w_p1, w_p2, w_s1, w_s2)", "idx": 84}
{"namespace": "sumy.summarizers.reduction.ReductionSummarizer.rate_sentences", "completion": "        ratings = defaultdict(float)\n        sentences = document.sentences\n        for (i, j) in combinations(range(len(sentences)), 2):\n            sentence1 = sentences[i]\n            sentence2 = sentences[j]\n            words1 = self._to_words_set(sentence1)\n            words2 = self._to_words_set(sentence2)\n            rank = self._rate_sentences_edge(words1, words2)\n            ratings[sentence1] += rank\n            ratings[sentence2] += rank\n        return ratings", "idx": 85}
{"namespace": "sumy.summarizers.reduction.ReductionSummarizer._to_words_set", "completion": "        words = map(self.normalize_word, sentence.words)\n        return [word for word in words if word not in self._stop_words]", "idx": 86}
{"namespace": "sumy.summarizers.text_rank.TextRankSummarizer._to_words_set", "completion": "        words = map(self.normalize_word, sentence.words)\n        return [word for word in words if word not in self._stop_words]", "idx": 87}
{"namespace": "sumy.summarizers.kl.KLSummarizer.compute_tf", "completion": "        # Get all content words in the document\n        content_words = self._get_all_content_words_in_doc(sentences)\n        # Compute the frequency of each content word\n        word_freq = self._compute_word_freq(content_words)\n        # Compute the total number of content words in the document\n        total_words = len(content_words)\n        # Normalize the term frequency by dividing the frequency of each content word by the total number of content words\n        tf = {word: freq / total_words for word, freq in word_freq.items()}\n        return tf", "idx": 88}
{"namespace": "sumy.evaluation.rouge._get_word_ngrams", "completion": "    assert n > 0\n    assert len(sentences) > 0\n    ngram_set = set()\n    for sentence in sentences:\n        words = _split_into_words([sentence])\n        for i in range(len(words) - n + 1):\n            ngram_set.add(tuple(words[i:i + n]))\n    return ngram_set", "idx": 89}
{"namespace": "sumy.evaluation.rouge._len_lcs", "completion": "    table = _lcs(x, y)\n    n, m = _get_index_of_lcs(x, y)\n    return table[n, m]", "idx": 90}
{"namespace": "sumy.evaluation.rouge._recon_lcs", "completion": "    n, m = _get_index_of_lcs(x, y)\n    table = _lcs(x, y)\n\n    def _recon(i, j):\n        if i == 0 or j == 0:\n            return []\n        elif x[i - 1] == y[j - 1]:\n            return _recon(i - 1, j - 1) + [x[i - 1]]\n        elif table[i - 1, j] > table[i, j - 1]:  # index out of range when i=0\n            return _recon(i - 1, j)\n        else:\n            return _recon(i, j - 1)\n\n    return _recon(n, m)", "idx": 91}
{"namespace": "sumy.evaluation.rouge._union_lcs", "completion": "    if len(evaluated_sentences) <= 0:\n        raise (ValueError(\"Collections must contain at least 1 sentence.\"))\n\n    lcs_union = set()\n    reference_words = _split_into_words([reference_sentence])\n    for evaluated_sentence in evaluated_sentences:\n        evaluated_words = _split_into_words([evaluated_sentence])\n        lcs = set(_recon_lcs(evaluated_words, reference_words))\n        lcs_union = lcs_union.union(lcs)\n\n    return len(lcs_union) / len(reference_words)", "idx": 92}
{"namespace": "sumy.parsers.html.HtmlParser.from_file", "completion": "        with open(file_path, 'r') as file:\n            data = file.read()\n        return cls(data, tokenizer, url)", "idx": 93}
{"namespace": "sumy.parsers.plaintext.PlaintextParser.document", "completion": "        paragraphs = []\n        lines = self._text.split(\"\\n\")\n        paragraph_lines = []\n\n        for line in lines:\n            line = line.strip()\n            if line:\n                paragraph_lines.append(line)\n            elif paragraph_lines:\n                paragraphs.append(self._to_paragraph(paragraph_lines))\n                paragraph_lines = []\n\n        if paragraph_lines:\n            paragraphs.append(self._to_paragraph(paragraph_lines))\n\n        return ObjectDocumentModel(paragraphs)", "idx": 94}
{"namespace": "sumy.nlp.tokenizers.Tokenizer.to_sentences", "completion": "        if hasattr(self._sentence_tokenizer, '_params'):\n            extra_abbreviations = self.LANGUAGE_EXTRA_ABREVS.get(self._language, [])\n            self._sentence_tokenizer._params.abbrev_types.update(extra_abbreviations)\n        sentences = self._sentence_tokenizer.tokenize(to_unicode(paragraph))\n        return tuple(sentences)", "idx": 95}
{"namespace": "sumy.nlp.stemmers.null_stemmer", "completion": "    return unicode(object).lower()", "idx": 96}
{"namespace": "rows.fields.BinaryField.serialize", "completion": "        if value is not None:\n            if isinstance(value, six.binary_type):\n                try:\n                    return b64encode(value).decode(\"ascii\")\n                except (TypeError, ValueError, binascii.Error):\n                    return value\n            else:\n                value_error(value, cls)\n        else:\n            return \"\"", "idx": 97}
{"namespace": "rows.fields.BoolField.deserialize", "completion": "        value = super(BoolField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value).lower()\n        if value in cls.TRUE_VALUES:\n            return True\n        elif value in cls.FALSE_VALUES:\n            return False\n        else:\n            raise ValueError(\"Value is not boolean\")", "idx": 98}
{"namespace": "rows.fields.DateField.serialize", "completion": "        if value is None:\n            return \"\"\n        return value.strftime(cls.OUTPUT_FORMAT)", "idx": 99}
{"namespace": "rows.fields.DateField.deserialize", "completion": "        value = super(DateField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n\n        value = as_string(value)\n        try:\n            dt = datetime.datetime.strptime(value, cls.INPUT_FORMAT)\n            return datetime.date(dt.year, dt.month, dt.day)\n        except ValueError:\n            value_error(value, cls)", "idx": 100}
{"namespace": "rows.fields.TextField.deserialize", "completion": "        value = super(TextField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            return six.text_type(value)", "idx": 101}
{"namespace": "rows.fields.EmailField.deserialize", "completion": "        value = super(EmailField, cls).deserialize(value)\n        if value is None or value == \"\":\n            return None\n        elif cls.EMAIL_REGEXP.match(value):\n            return value\n        else:\n            raise ValueError(\"Invalid email format\")", "idx": 102}
{"namespace": "rows.fields.JSONField.deserialize", "completion": "        value = super(JSONField, cls).deserialize(value)\n        if value is None or isinstance(value, cls.TYPE):\n            return value\n        else:\n            try:\n                return json.loads(value)\n            except json.JSONDecodeError:\n                raise ValueError(\"Invalid JSON data\")", "idx": 103}
{"namespace": "falcon.http_error.HTTPError.to_json", "completion": "                        import json\n        if handler is None:\n            global _DEFAULT_JSON_HANDLER\n            if _DEFAULT_JSON_HANDLER is None:\n                import json\n                _DEFAULT_JSON_HANDLER = json\n            handler = _DEFAULT_JSON_HANDLER\n\n        error_dict = self.to_dict()\n        return handler.dumps(error_dict).encode('utf-8')", "idx": 104}
{"namespace": "falcon.inspect.inspect_app", "completion": "    routes = inspect_routes(app)\n    middleware = inspect_middleware(app)\n    static_routes = inspect_static_routes(app)\n    sinks = inspect_sinks(app)\n    error_handlers = inspect_error_handlers(app)\n    asgi = app._ASGI\n\n    return AppInfo(routes, middleware, static_routes, sinks, error_handlers, asgi)", "idx": 105}
{"namespace": "falcon.inspect.inspect_routes", "completion": "    router = app._router\n    router_class = type(router)\n    if router_class not in _supported_routers:\n        raise ValueError(\n            'The router {} is not supported'.format(router_class)\n        )\n    return _supported_routers[router_class](router)", "idx": 106}
{"namespace": "falcon.inspect.inspect_static_routes", "completion": "    static_routes = []\n    for route in app._router._static_routes:\n        info = StaticRouteInfo(route.prefix, route.directory, route.fallback_filename)\n        static_routes.append(info)\n    return static_routes", "idx": 107}
{"namespace": "falcon.inspect.inspect_sinks", "completion": "    sinks = []\n    for prefix, sink in app._sinks:\n        source_info, name = _get_source_info_and_name(sink)\n        info = SinkInfo(prefix, name, source_info)\n        sinks.append(info)\n    return sinks", "idx": 108}
{"namespace": "falcon.inspect.inspect_error_handlers", "completion": "    error_handlers = []\n    for error, handler in app._error_handlers.items():\n        source_info, name = _get_source_info_and_name(handler)\n        info = ErrorHandlerInfo(error.__name__, name, source_info, _is_internal(handler))\n        error_handlers.append(info)\n    return error_handlers", "idx": 109}
{"namespace": "falcon.inspect.inspect_middleware", "completion": "    middleware_tree = []\n    middleware_classes = []\n    for component in app._middleware:\n        if isinstance(component, tuple):\n            for method in component:\n                if method is not None:\n                    source_info, name = _get_source_info_and_name(method)\n                    middleware_tree.append(MiddlewareTreeItemInfo(name, source_info))\n        else:\n            source_info, name = _get_source_info_and_name(component)\n            methods = []\n            for method_name in ('process_request', 'process_resource', 'process_response'):\n                method = getattr(component, method_name, None)\n                if method is not None:\n                    method_source_info, method_name = _get_source_info_and_name(method)\n                    methods.append(MiddlewareMethodInfo(method_name, method_source_info))\n            middleware_classes.append(MiddlewareClassInfo(name, source_info, methods))\n    return MiddlewareInfo(middleware_tree, middleware_classes, app._independent_middleware)", "idx": 110}
{"namespace": "falcon.inspect.InspectVisitor.process", "completion": "        method_name = 'visit_' + instance.__visit_name__\n        method = getattr(self, method_name, None)\n        if method is None:\n            raise RuntimeError(\n                'No visit method found for class {}'.format(instance.__class__.__name__)\n            )\n        return method(instance)", "idx": 111}
{"namespace": "falcon.request.Request.forwarded", "completion": "        if self._cached_forwarded is None:\n            header_value = self.env.get('HTTP_FORWARDED')\n            if header_value:\n                self._cached_forwarded = helpers.parse_forwarded_header(header_value)\n            else:\n                self._cached_forwarded = None\n\n        return self._cached_forwarded", "idx": 112}
{"namespace": "falcon.request.Request.client_accepts_msgpack", "completion": "        return self.client_accepts('application/x-msgpack') or self.client_accepts('application/msgpack')", "idx": 113}
{"namespace": "falcon.request.Request.content_length", "completion": "\n        try:\n            # PERF(kgriffs): The result of this expression is slightly\n            # faster than int(self.get_header('content-length', required=True))\n            value = self.env['CONTENT_LENGTH']\n        except KeyError:\n            return None\n\n        try:\n            value_as_int = int(value)\n        except ValueError:\n            msg = 'The value must be a positive integer.'\n            raise errors.HTTPInvalidHeader(msg, 'Content-Length')\n\n        if value_as_int < 0:\n            msg = 'The value must be a positive integer.'\n            raise errors.HTTPInvalidHeader(msg, 'Content-Length')\n\n        return value_as_int", "idx": 114}
{"namespace": "falcon.request.Request.bounded_stream", "completion": "        if self._bounded_stream is None:\n            self._bounded_stream = self._get_wrapped_wsgi_input()\n\n        return self._bounded_stream", "idx": 115}
{"namespace": "falcon.request.Request.uri", "completion": "        if self._cached_uri is None:\n            # PERF: For small numbers of items, '+' is faster\n            # than ''.join(...). Concatenation is also generally\n            # faster than formatting.\n            value = (\n                self.scheme + '://' + self.netloc + self.relative_uri\n            )\n\n            self._cached_uri = value\n\n        return self._cached_uri", "idx": 116}
{"namespace": "falcon.request.Request.forwarded_uri", "completion": "        if self._cached_forwarded_uri is None:\n            self._cached_forwarded_uri = (\n                self.forwarded_scheme + '://' + self.forwarded_host + self.relative_uri\n            )\n\n        return self._cached_forwarded_uri", "idx": 117}
{"namespace": "falcon.request.Request.relative_uri", "completion": "        if self._cached_relative_uri is None:\n            if self.query_string:\n                self._cached_relative_uri = self.app + self.path + '?' + self.query_string\n            else:\n                self._cached_relative_uri = self.app + self.path\n\n        return self._cached_relative_uri", "idx": 118}
{"namespace": "falcon.request.Request.prefix", "completion": "        if self._cached_prefix is None:\n            # PERF: For small numbers of items, '+' is faster\n            # than ''.join(...). Concatenation is also generally\n            # faster than formatting.\n            value = self.scheme + '://' + self.netloc + self.app\n\n            self._cached_prefix = value\n\n        return self._cached_prefix", "idx": 119}
{"namespace": "falcon.request.Request.forwarded_prefix", "completion": "        if self._cached_forwarded_prefix is None:\n            # PERF: For small numbers of items, '+' is faster\n            # than ''.join(...). Concatenation is also generally\n            # faster than formatting.\n            value = (\n                self.forwarded_scheme + '://' + self.forwarded_host + self.app\n            )\n\n            self._cached_forwarded_prefix = value\n\n        return self._cached_forwarded_prefix", "idx": 120}
{"namespace": "falcon.request.Request.host", "completion": "\n        return self.get_header('Host')", "idx": 121}
{"namespace": "falcon.request.Request.subdomain", "completion": "        subdomain, separator, remainder = self.host.partition('.')\n        if separator:\n            return subdomain\n        else:\n            return None", "idx": 122}
{"namespace": "falcon.request.Request.headers", "completion": "\n        if self._cached_headers is None:\n            env = self.env\n            headers = {}\n\n            for name in env:\n                if name.startswith('HTTP_'):\n                    # Convert HTTP_CONTENT_TYPE to Content-Type, etc.\n                    translated_name = name[5:].replace('_', '-').title()\n                    headers[translated_name] = env[name]\n                elif name in WSGI_CONTENT_HEADERS:\n                    headers[name.title().replace('_', '-')] = env[name]\n\n            self._cached_headers = headers\n\n        return self._cached_headers.copy()", "idx": 123}
{"namespace": "falcon.request.Request.remote_addr", "completion": "        return self.env.get('REMOTE_ADDR', '127.0.0.1')", "idx": 124}
{"namespace": "falcon.request.Request.client_accepts", "completion": "        accept = self.accept\n        if accept == '*/*':\n            return True\n\n        # PERF(kgriffs): mimeparse will return None if there\n        # is no match for the requested media type. It seems\n        # to be a bit faster to just check ourselves, plus\n        # this saves us from having to worry about handling\n        # the case in which the client sends something like\n        # 'application/*'.\n        #\n        # NOTE(kgriffs): We only need to check the \"type\"\n        # and \"subtype\" parts of the media type, since\n        # \"params\" aren't relevant here. This means we can\n        # just split on '/' and ignore anything after the\n        # second item in the resulting list.\n        if '/' in media_type:\n            media_main_type, media_sub_type = media_type.split('/', 1)\n        else:\n            media_main_type = media_type\n            media_sub_type = ''\n\n        # Check each media type from the Accept header\n        for accept_type in accept.split(','):\n            # PERF(kgriffs): If the type includes params (a la\n            # 'application/json; charset=utf-8'), remove them\n            if ';' in accept_type:\n                accept_type = accept_type.split(';', 1)[0]\n\n            # PERF(kgriffs): Just to be safe\n            accept_type = accept_type.strip()\n\n            if '/' in accept_type:\n                accept_main_type, accept_sub_type = accept_type.split('/', 1)\n            else:\n                accept_main_type = accept_type\n                accept_sub_type = ''\n\n            if accept_main_type == media_main_type:\n                if accept_sub_type in ('*', media_sub_type):\n                    return True\n\n        return False", "idx": 125}
{"namespace": "falcon.request.Request.client_prefers", "completion": "        accept = self.accept\n        preferred_type = mimeparse.best_match(media_types, accept)\n        return preferred_type or None", "idx": 126}
{"namespace": "falcon.request.Request.get_header", "completion": "\n        # PERF(kgriffs): .get() is not used in order to avoid having to\n        # create a default value for the dict lookup.\n        wsgi_name = 'HTTP_' + name.upper().replace('-', '_')\n        try:\n            return self.env[wsgi_name]\n        except KeyError:\n            if not required:\n                return default\n\n            raise errors.HTTPMissingHeader(name)", "idx": 127}
{"namespace": "falcon.request.Request.get_cookie_values", "completion": "        if self._cookies is None:\n            header_value = self.get_header('Cookie')\n            if header_value:\n                self._cookies = helpers.parse_cookie_header(header_value)\n            else:\n                self._cookies = {}\n\n        return self._cookies.get(name)", "idx": 128}
{"namespace": "falcon.response.Response.unset_cookie", "completion": "        if self._cookies is not None and name in self._cookies:\n            del self._cookies[name]\n\n        self.set_cookie(name, '', max_age=0, domain=domain, path=path)", "idx": 129}
{"namespace": "falcon.response.Response.get_header", "completion": "        name = name.lower()\n\n        if name == 'set-cookie':\n            raise HeaderNotSupported('This method cannot be used to get cookies')\n\n        return self._headers.get(name, default)", "idx": 130}
{"namespace": "falcon.response.Response.set_header", "completion": "\n        # Normalize name by lowercasing it\n        name = name.lower()\n\n        if name == 'set-cookie':\n            raise HeaderNotSupported('This method cannot be used to set cookies')\n\n        # Convert value to string and set the header\n        self._headers[name] = str(value)", "idx": 131}
{"namespace": "falcon.response.Response.delete_header", "completion": "        # Normalize name by lowercasing it\n        name = name.lower()\n\n        # Check if the header is in the _headers dictionary and if so, delete it\n        if name in self._headers:\n            del self._headers[name]\n\n        # Check if the header is in the _extra_headers list and if so, remove it\n        if self._extra_headers:\n            self._extra_headers = [(n, v) for n, v in self._extra_headers if n != name]", "idx": 132}
{"namespace": "falcon.cmd.inspect_app.route_main", "completion": "    print('The \"falcon-print-routes\" command is deprecated. ')\n    print('Please use \"falcon-inspect-app\"')\n    main()", "idx": 133}
{"namespace": "falcon.util.uri.decode", "completion": "\n    if unquote_plus:\n        encoded_uri = encoded_uri.replace('+', ' ')\n\n    tokens = encoded_uri.split('%')\n\n    if len(tokens) == 1:\n        return encoded_uri\n\n    return _join_tokens(tokens)", "idx": 134}
{"namespace": "falcon.util.structures.ETag.dumps", "completion": "        if self.is_weak:\n            return 'W/\"{}\"'.format(self)\n        else:\n            return '\"{}\"'.format(self)", "idx": 135}
{"namespace": "falcon.util.structures.ETag.loads", "completion": "\n        is_weak = False\n        if etag_str[:2] == 'W/':\n            is_weak = True\n            etag_str = etag_str[2:]\n\n        # Strip quotes\n        etag_str = etag_str.strip('\"')\n\n        etag = cls(etag_str)\n        etag.is_weak = is_weak\n\n        return etag", "idx": 136}
{"namespace": "falcon.util.misc.secure_filename", "completion": "\n    # Normalize the filename to NKFD Unicode form\n    filename = unicodedata.normalize('NFKD', filename)\n\n    # Replace any non-ASCII character with an underscore\n    filename = _UNSAFE_CHARS.sub('_', filename)\n\n    # If the filename starts with a period, replace it with an underscore\n    if filename and filename[0] == '.':\n        filename = '_' + filename[1:]\n\n    return filename", "idx": 137}
{"namespace": "falcon.asgi.reader.BufferedReader.peek", "completion": "        if size < -1:\n            raise ValueError('size must be -1 or greater')\n\n        if size == -1:\n            return self._buffer[self._buffer_pos:]\n\n        if size <= self._buffer_len - self._buffer_pos:\n            return self._buffer[self._buffer_pos:self._buffer_pos + size]\n\n        while not self._exhausted and size > self._buffer_len - self._buffer_pos:\n            try:\n                chunk = await self._source.__anext__()\n                self._buffer += chunk\n                self._buffer_len += len(chunk)\n            except StopAsyncIteration:\n                self._exhausted = True\n\n        return self._buffer[self._buffer_pos:self._buffer_pos + size]", "idx": 138}
{"namespace": "falcon.asgi.reader.BufferedReader.read_until", "completion": "        data = await self._read_from(self._iter_delimited(delimiter, size_hint=size or 0), size)\n        if consume_delimiter:\n            await self._consume_delimiter(delimiter)\n        return data", "idx": 139}
{"namespace": "falcon.routing.converters.IntConverter.convert", "completion": "\n        # Check if the value has the correct number of digits\n        if self._num_digits is not None and len(value) != self._num_digits:\n            return None\n\n        # Check if the value contains any whitespace characters\n        if value.strip() != value:\n            return None\n\n        # Try to convert the value to an integer\n        try:\n            value = int(value)\n        except ValueError:\n            return None\n\n        # Check if the value is within the specified range\n        if (self._min is not None and value < self._min) or (self._max is not None and value > self._max):\n            return None\n\n        return value", "idx": 140}
{"namespace": "falcon.routing.converters.DateTimeConverter.convert", "completion": "        try:\n            return strptime(value, self._format_string)\n        except ValueError:\n            return None", "idx": 141}
{"namespace": "falcon.routing.util.map_http_methods", "completion": "\n    method_map = {}\n\n    for method in constants.HTTP_METHODS:\n        try:\n            responder = getattr(resource, 'on_' + method.lower())\n        except AttributeError:\n            if suffix is not None:\n                try:\n                    responder = getattr(resource, 'on_' + method.lower() + '_' + suffix)\n                except AttributeError:\n                    continue\n            else:\n                continue\n\n        method_map[method] = responder\n\n    return method_map", "idx": 142}
{"namespace": "falcon.routing.static._BoundedFile.read", "completion": "        if size < 0 or size > self.remaining:\n            size = self.remaining\n\n        data = self.fh.read(size)\n        self.remaining -= len(data)\n\n        return data", "idx": 143}
{"namespace": "authlib.oauth2.rfc6749.util.list_to_scope", "completion": "    if isinstance(scope, (tuple, list, set)):\n        return ' '.join(to_unicode(s) for s in scope)\n    elif scope is None:\n        return None\n    return to_unicode(scope)", "idx": 144}
{"namespace": "authlib.oauth2.rfc6749.util.extract_basic_authorization", "completion": "    auth_header = headers.get('Authorization')\n    if not auth_header or ' ' not in auth_header:\n        return None, None\n    auth_type, auth_token = auth_header.split(' ', 1)\n    auth_type = auth_type.lower()\n    if auth_type != 'basic':\n        return None, None\n    try:\n        auth_token = to_unicode(base64.b64decode(auth_token))\n    except (TypeError, binascii.Error):\n        return None, None\n    if ':' not in auth_token:\n        return auth_token, None\n    username, password = auth_token.split(':', 1)\n    return username, password", "idx": 145}
{"namespace": "authlib.oauth2.rfc6749.parameters.prepare_grant_uri", "completion": "    params = [\n        ('response_type', response_type),\n        ('client_id', client_id),\n    ]\n    if redirect_uri:\n        params.append(('redirect_uri', redirect_uri))\n    if scope:\n        if not isinstance(scope, str):\n            scope = list_to_scope(scope)\n        params.append(('scope', scope))\n    if state:\n        params.append(('state', state))\n\n    for k, v in kwargs.items():\n        if v:\n            params.append((k, v))\n\n    return add_params_to_qs(uri, params)", "idx": 146}
{"namespace": "authlib.oauth2.rfc6749.parameters.parse_authorization_code_response", "completion": "        from .errors import MissingCodeException, MismatchingStateException\n    from .errors import MissingCodeException, MismatchingStateException\n    query = urlparse.urlparse(uri).query\n    params = dict(urlparse.parse_qsl(query, keep_blank_values=True))\n\n    if 'code' not in params:\n        raise MissingCodeException()\n\n    if state and params.get('state', None) != state:\n        raise MismatchingStateException()\n\n    return params", "idx": 147}
{"namespace": "authlib.oauth2.rfc6749.parameters.parse_implicit_response", "completion": "    fragment = urlparse.urlparse(uri).fragment\n    params = dict(urlparse.parse_qsl(fragment))\n\n    if 'access_token' not in params or 'token_type' not in params:\n        raise MissingTokenException()\n\n    params_state = params.get('state')\n    if state and params_state != state:\n        raise MismatchingStateException()\n\n    return params", "idx": 148}
{"namespace": "authlib.common.encoding.json_b64encode", "completion": "    if isinstance(text, dict):\n        text = json_dumps(text)\n    elif not isinstance(text, str):\n        raise ValueError('Input must be a string or a dictionary')\n    \n    text_bytes = to_bytes(text)\n    encoded_text = urlsafe_b64encode(text_bytes)\n    return to_unicode(encoded_text)", "idx": 149}
{"namespace": "authlib.jose.util.extract_header", "completion": "    try:\n        header_data = extract_segment(header_segment, error_cls, 'header')\n        header = json_loads(to_unicode(header_data))\n    except (ValueError, TypeError):\n        raise error_cls('Invalid header segment')\n\n    return ensure_dict(header, 'header')", "idx": 150}
{"namespace": "twitter.models.TwitterModel.AsDict", "completion": "        data = {}\n        for (key, value) in self.__dict__.items():\n            if hasattr(value, 'AsDict'):\n                data[key] = value.AsDict()\n            elif isinstance(value, list):\n                data[key] = []\n                for item in value:\n                    if hasattr(item, 'AsDict'):\n                        data[key].append(item.AsDict())\n                    else:\n                        data[key].append(item)\n            else:\n                data[key] = value\n        return data", "idx": 151}
{"namespace": "twitter.models.TwitterModel.NewFromJsonDict", "completion": "        new_instance = cls(**kwargs)\n        for (key, value) in data.items():\n            if key in new_instance.param_defaults:\n                setattr(new_instance, key, value)\n        return new_instance", "idx": 152}
{"namespace": "twitter.api.Api._TweetTextWrap", "completion": "        # Split status into a list of words\n        words = status.split()\n        if not all(len(word) <= char_lim for word in words):\n            # If any single word is longer than the character limit, raise an error.\n            raise ValueError(\"A word in the status exceeds the maximum allowed character limit.\")\n\n        tweets = []  # This list will contain the final tweets\n        current_tweet = \"\"  # This string will contain the current tweet's content\n\n        # Iterate through all words in the status\n        for word in words:\n            # Check if adding the next word would exceed the limit\n            if len(current_tweet) + len(word) + 1 > char_lim:\n                # If so, add the current tweet to the list of tweets and reset it\n                tweets.append(current_tweet)\n                current_tweet = word\n            else:\n                # If not, add the word to the current tweet\n                current_tweet += (\" \" + word) if current_tweet else word\n\n        # Add the last tweet to the list of tweets, if it's not empty\n        if current_tweet:\n            tweets.append(current_tweet)\n\n        return tweets", "idx": 153}
{"namespace": "databases.importer.import_from_string", "completion": "\n    try:\n        module_str, attribute_str = import_str.split(\":\")\n        module = importlib.import_module(module_str)\n        attribute = getattr(module, attribute_str)\n    except (ImportError, AttributeError) as e:\n        raise ImportFromStringError(f\"Failed to import {import_str}\") from e\n\n    return attribute", "idx": 154}
{"namespace": "rest_framework.reverse.reverse", "completion": "    scheme = getattr(request, 'versioning_scheme', None)\n    if scheme is not None:\n        return scheme.reverse(viewname, args, kwargs, request, format, **extra)\n    return _reverse(viewname, args, kwargs, request, format, **extra)", "idx": 155}
{"namespace": "rest_framework.serializers.Serializer.fields", "completion": "        fields = OrderedDict()\n\n        for key, value in self.get_fields().items():\n            fields[key] = value\n\n        return fields", "idx": 156}
{"namespace": "rest_framework.parsers.JSONParser.parse", "completion": "        parser_context = parser_context or {}\n        encoding = parser_context.get('encoding', settings.DEFAULT_CHARSET)\n\n        try:\n            data = stream.read().decode(encoding)\n            return json.loads(data)\n        except ValueError as exc:\n            raise ParseError('JSON parse error - %s' % str(exc))", "idx": 157}
{"namespace": "rest_framework.parsers.FileUploadParser.get_filename", "completion": "        try:\n            content_disposition_header = parser_context['request'].META['HTTP_CONTENT_DISPOSITION']\n            filename = re.findall('filename=\"(.+)\"', content_disposition_header)[0]\n        except KeyError:\n            filename = None\n\n        return filename", "idx": 158}
{"namespace": "rest_framework.fields.is_simple_callable", "completion": "    if not callable(obj):\n        return False\n\n    if isinstance(obj, types.BuiltinFunctionType):\n        raise BuiltinSignatureError()\n\n    if isinstance(obj, (types.FunctionType, types.MethodType)):\n        signature = inspect.signature(obj)\n    elif isinstance(obj, functools.partial):\n        signature = inspect.signature(obj.func)\n    else:\n        return False\n\n    params = signature.parameters.values()\n    return all(\n        param.default is not inspect.Parameter.empty or\n        param.kind in (param.VAR_POSITIONAL, param.VAR_KEYWORD)\n        for param in params\n    )", "idx": 159}
{"namespace": "rest_framework.fields.Field.bind", "completion": "        self.field_name = field_name\n        self.parent = parent\n        self.source = self.source or field_name\n\n        # Set `read_only` to `True` if this field is nested within a\n        # `ListSerializer` which is `read_only`.\n        if self.parent is not None and getattr(self.parent, 'child', None) is self:\n            if getattr(self.parent, 'read_only', False):\n                self.read_only = True", "idx": 160}
{"namespace": "rest_framework.fields.Field.run_validation", "completion": "        if data is empty:\n            if getattr(self.root, 'partial', False):\n                return empty\n            if self.required:\n                self.fail('required')\n            return self.get_default()\n\n        value = self.to_internal_value(data)\n        self.run_validators(value)\n        return value", "idx": 161}
{"namespace": "rest_framework.fields.Field.root", "completion": "        root = self\n        while root.parent is not None:\n            root = root.parent\n        return root", "idx": 162}
{"namespace": "rest_framework.fields.CharField.run_validation", "completion": "        if data == '' or (self.trim_whitespace and str(data).strip() == ''):\n            if not self.allow_blank:\n                self.fail('required')\n            return ''\n        return super().run_validation(data)", "idx": 163}
{"namespace": "rest_framework.fields.CharField.to_internal_value", "completion": "        if isinstance(data, bool) or not isinstance(data, (str, int, float)):\n            self.fail('invalid')\n        value = str(data)\n        if self.trim_whitespace:\n            return value.strip()\n        return value", "idx": 164}
{"namespace": "rest_framework.fields.DecimalField.to_internal_value", "completion": "        data = super().to_internal_value(data)\n        if self.max_digits is not None:\n            if round(abs(data)) > self.max_digits - self.decimal_places:\n                self.fail('max_digits')\n            if data.as_tuple().exponent < -self.decimal_places:\n                self.fail('max_decimal_places')\n        return data", "idx": 165}
{"namespace": "rest_framework.fields.DateTimeField.to_representation", "completion": "        if not value:\n            return None\n\n        output_format = getattr(self, 'format', api_settings.DATETIME_FORMAT)\n\n        if output_format is None or isinstance(value, str):\n            return value\n\n        # Applying a `DateTimeField` to a datetime value is almost always\n        # not a sensible thing to do, as it means naively dropping\n        # any explicit or implicit timezone info.\n        assert not isinstance(value, datetime.datetime), (\n            'Expected a `date`, but got a `datetime`. Refusing to coerce, '\n            'as this may mean losing timezone information. Use a custom '\n            'read-only field and deal with timezone issues explicitly.'\n        )\n\n        if output_format.lower() == ISO_8601:\n            return value.isoformat()\n\n        return value.strftime(output_format)", "idx": 166}
{"namespace": "rest_framework.fields.ChoiceField.iter_options", "completion": "        class StartOptionGroup:\n            start_option_group = True\n            end_option_group = False\n\n            def __init__(self, label):\n                self.label = label\n\n        class EndOptionGroup:\n            start_option_group = False\n            end_option_group = True\n\n        class Option:\n            start_option_group = False\n            end_option_group = False\n\n            def __init__(self, value, display_text, disabled=False):\n                self.value = value\n                self.display_text = display_text\n                self.disabled = disabled\n\n        count = 0\n\n        for key, value in self.grouped_choices.items():\n            if self.html_cutoff and count >= self.html_cutoff:\n                break\n\n            if isinstance(value, dict):\n                yield StartOptionGroup(label=key)\n                for sub_key, sub_value in value.items():\n                    if self.html_cutoff and count >= self.html_cutoff:\n                        break\n                    yield Option(value=sub_key, display_text=sub_value)\n                    count += 1\n                yield EndOptionGroup()\n            else:\n                yield Option(value=key, display_text=value)\n                count += 1\n\n        if self.html_cutoff and count >= self.html_cutoff and self.html_cutoff_text:\n            cutoff_text = self.html_cutoff_text.format(count=self.html_cutoff)\n            yield Option(value='n/a', display_text=cutoff_text, disabled=True)", "idx": 167}
{"namespace": "rest_framework.fields.MultipleChoiceField.get_value", "completion": "        if self.field_name not in dictionary:\n            if getattr(self.root, 'partial', False):\n                return empty\n        # We override the default field access in order to support\n        # lists in HTML forms.\n        if html.is_html_input(dictionary):\n            val = dictionary.getlist(self.field_name, [])\n            if len(val) > 0:\n                # Support QueryDict lists in HTML input.\n                return val\n            return html.parse_html_list(dictionary, prefix=self.field_name, default=empty)\n\n        return dictionary.get(self.field_name, empty)", "idx": 168}
{"namespace": "rest_framework.exceptions._get_error_details", "completion": "    if isinstance(data, list):\n        return [_get_error_details(item, default_code) for item in data]\n    elif isinstance(data, dict):\n        return {key: _get_error_details(value, default_code) for key, value in data.items()}\n    elif isinstance(data, str):\n        return ErrorDetail(data, default_code)\n    elif isinstance(data, ErrorDetail):\n        return data\n    else:\n        return ErrorDetail(str(data), default_code)", "idx": 169}
{"namespace": "rest_framework.exceptions.server_error", "completion": "    data = {\n        'error': 'Internal Server Error (500)'\n    }\n    return JsonResponse(data, status=status.HTTP_500_INTERNAL_SERVER_ERROR)", "idx": 170}
{"namespace": "rest_framework.exceptions.bad_request", "completion": "    data = {\n        'error': str(exception)\n    }\n    return JsonResponse(data, status=status.HTTP_400_BAD_REQUEST)", "idx": 171}
{"namespace": "rest_framework.relations.RelatedField.iter_options", "completion": "        return iter_options(\n            self.get_choices(cutoff=self.html_cutoff),\n            cutoff=self.html_cutoff,\n            cutoff_text=self.html_cutoff_text\n        )", "idx": 172}
{"namespace": "rest_framework.relations.PrimaryKeyRelatedField.to_internal_value", "completion": "        if self.pk_field is not None:\n            data = self.pk_field.to_internal_value(data)\n        try:\n            return self.get_queryset().get(pk=data)\n        except ObjectDoesNotExist:\n            self.fail('does_not_exist', pk_value=data)\n        except (TypeError, ValueError):\n            self.fail('incorrect_type', data_type=type(data).__name__)", "idx": 173}
{"namespace": "rest_framework.relations.PrimaryKeyRelatedField.to_representation", "completion": "        if self.pk_field is not None:\n            return self.pk_field.to_representation(value.pk)\n        return value.pk", "idx": 174}
{"namespace": "rest_framework.relations.SlugRelatedField.to_internal_value", "completion": "        queryset = self.get_queryset()\n        try:\n            return queryset.get(**{self.slug_field: data})\n        except ObjectDoesNotExist:\n            self.fail('does_not_exist', slug_name=self.slug_field, value=smart_str(data))\n        except (TypeError, ValueError):\n            self.fail('invalid')", "idx": 175}
{"namespace": "rest_framework.templatetags.rest_framework.add_query_param", "completion": "    from urllib.parse import urlencode, urlsplit, urlunsplit\n    url = request.get_full_path()\n    url_parts = list(urlsplit(url))\n    query = dict(urlparse.parse_qsl(url_parts[3]))\n    query.update({key: val})\n    url_parts[3] = urlencode(query)\n    return urlunsplit(url_parts)", "idx": 176}
{"namespace": "rest_framework.utils.mediatypes._MediaType.match", "completion": "        if other.main_type != '*' and other.main_type != self.main_type:\n            return False\n        if other.sub_type != '*' and other.sub_type != self.sub_type:\n            return False\n        for key, val in other.params.items():\n            if key != 'q' and (key not in self.params or self.params[key] != val):\n                return False\n        return True", "idx": 177}
{"namespace": "rest_framework.utils.mediatypes._MediaType.precedence", "completion": "        if self.main_type == '*':\n            return 0\n        elif self.sub_type == '*':\n            return 1\n        elif self.params:\n            return 3\n        else:\n            return 2", "idx": 178}
{"namespace": "rest_framework.utils.mediatypes._MediaType.__str__", "completion": "        result = self.main_type + '/' + self.sub_type\n        for key, value in self.params.items():\n            result += '; ' + key + '=' + str(value)\n        return result", "idx": 179}
{"namespace": "asyncpg._testbase.TestCase.assertLoopErrorHandlerCalled", "completion": "        old_handler = self.loop.get_exception_handler()\n        messages = []\n\n        def handler(loop, context):\n            messages.append(context.get('message', ''))\n            old_handler(loop, context)\n\n        self.loop.set_exception_handler(handler)\n\n        try:\n            yield\n        finally:\n            self.loop.set_exception_handler(old_handler)\n\n        for message in messages:\n            if re.search(msg_re, message):\n                break\n        else:\n            raise AssertionError(\n                'loop error handler was not called with a message '\n                'matching {!r}'.format(msg_re))", "idx": 180}
{"namespace": "csvs_to_sqlite.utils.refactor_dataframes", "completion": "    lookup_tables = {}\n    for column, (table, value_column) in foreign_keys.items():\n        lookup_tables[column] = LookupTable(conn, table, value_column, index_fts)\n\n    for df in dataframes:\n        for column, lookup_table in lookup_tables.items():\n            if column in df.columns:\n                df[column] = df[column].apply(lookup_table.id_for_value)\n\n    return dataframes", "idx": 181}
{"namespace": "sqlitedict.SqliteDict.iteritems", "completion": "        GET_ITEMS = 'SELECT key, value FROM \"%s\" ORDER BY rowid' % self.tablename\n        for key, value in self.conn.select(GET_ITEMS):\n            yield self.decode_key(key), self.decode(value)", "idx": 182}
{"namespace": "sqlitedict.SqliteDict.update", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to update read-only SqliteDict')\n        try:\n            items = items.items()\n        except AttributeError:\n            pass\n        items = ((self.encode_key(k), self.encode(v)) for k, v in items)\n\n        UPDATE_ITEMS = 'REPLACE INTO \"%s\" (key, value) VALUES (?, ?)' % self.tablename\n        self.conn.executemany(UPDATE_ITEMS, items)\n\n        if kwds:\n            self.update(kwds)\n\n        if self.autocommit:\n            self.commit()", "idx": 183}
{"namespace": "sqlitedict.SqliteDict.clear", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to clear read-only SqliteDict')\n        CLEAR_ALL = 'DELETE FROM \"%s\"' % self.tablename\n        self.conn.execute(CLEAR_ALL)\n        self.conn.commit()```", "idx": 184}
{"namespace": "sqlitedict.SqliteDict.commit", "completion": "        if self.conn is not None:\n            if blocking:\n                self.conn.commit(blocking=True)\n            else:\n                self.conn.execute(_REQUEST_COMMIT)", "idx": 185}
{"namespace": "sqlitedict.SqliteDict.terminate", "completion": "        if self.flag == 'r':\n            raise RuntimeError('Refusing to terminate read-only SqliteDict')\n        self.close()\n        if self.filename != \":memory:\":\n            try:\n                os.remove(self.filename)\n            except Exception:\n                pass", "idx": 186}
{"namespace": "boto.utils.retry_url", "completion": "    for i in range(0, num_retries):\n        try:\n            req = urllib.request.Request(url)\n            opener = urllib.request.build_opener()\n            if timeout:\n                response = opener.open(req, timeout=timeout)\n            else:\n                response = opener.open(req)\n            return response.read()\n        except urllib.error.HTTPError as e:\n            if e.code == 404 and not retry_on_404:\n                raise\n            boto.log.exception('Caught exception reading instance data %s' % url)\n            time.sleep(2**i)\n        except urllib.error.URLError:\n            boto.log.exception('Caught exception reading instance data %s' % url)\n            time.sleep(2**i)\n    boto.log.error('Unable to read instance data, giving up')\n    return None", "idx": 187}
{"namespace": "boto.utils.LazyLoadMetadata.values", "completion": "        self._materialize()\n        return super(LazyLoadMetadata, self).values()", "idx": 188}
{"namespace": "boto.utils.get_instance_userdata", "completion": "    user_data_url = _build_instance_metadata_url(url, version, 'user-data/')\n    user_data = retry_url(user_data_url, num_retries=num_retries, timeout=timeout)\n    if user_data and sep:\n        user_data = dict(u.split(sep, 1) for u in user_data.splitlines())\n    return user_data", "idx": 189}
{"namespace": "boto.utils.pythonize_name", "completion": "    s1 = _first_cap_regex.sub(r'\\1_\\2', name)\n    s2 = _number_cap_regex.sub(r'\\1_\\2', s1)\n    return _end_cap_regex.sub(r'\\1_\\2', s2).lower()", "idx": 190}
{"namespace": "boto.cloudsearchdomain.connect_to_region", "completion": "        from boto.cloudsearchdomain.layer1 import CloudSearchDomainConnection\n    from boto.cloudsearchdomain.layer1 import CloudSearchDomainConnection\n    for region in regions():\n        if region.name == region_name:\n            return region.connect(connection_cls=CloudSearchDomainConnection, **kw_params)\n    raise ValueError('Invalid region name: %s' % region_name)", "idx": 191}
{"namespace": "boto.redshift.connect_to_region", "completion": "        from boto.redshift.layer1 import RedshiftConnection\n    from boto.redshift.layer1 import RedshiftConnection\n\n    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n\n    raise ValueError('Invalid region name: %s' % region_name)", "idx": 192}
{"namespace": "boto.support.connect_to_region", "completion": "        from boto.support.layer1 import SupportConnection\n    from boto.support.layer1 import SupportConnection\n\n    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n\n    raise ValueError('Invalid region name: %s' % region_name)", "idx": 193}
{"namespace": "boto.configservice.connect_to_region", "completion": "        from boto.configservice.layer1 import ConfigServiceConnection\n    from boto.configservice.layer1 import ConfigServiceConnection\n    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    raise ValueError('Invalid region name: %s' % region_name)", "idx": 194}
{"namespace": "boto.cloudhsm.connect_to_region", "completion": "        from boto.cloudhsm.layer1 import CloudHSMConnection\n    from boto.cloudhsm.layer1 import CloudHSMConnection\n    for region in regions():\n        if region.name == region_name:\n            return region.connect(connection_cls=CloudHSMConnection, **kw_params)\n    raise ValueError('Invalid region name: %s' % region_name)", "idx": 195}
{"namespace": "boto.logs.connect_to_region", "completion": "        from boto.regioninfo import get_region\n        from boto.logs.layer1 import CloudWatchLogsConnection\n    from boto.logs.layer1 import CloudWatchLogsConnection\n    from boto.regioninfo import get_region\n\n    region = get_region(region_name)\n    if not region:\n        raise ValueError('Unknown region %s' % region_name)\n    return region.connect(connection_cls=CloudWatchLogsConnection, **kw_params)", "idx": 196}
{"namespace": "boto.cloudsearch.connect_to_region", "completion": "        from boto.cloudsearch.layer1 import Layer1\n    from boto.cloudsearch.layer1 import Layer1\n    for region in get_regions('cloudsearch', connection_cls=Layer1):\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    raise ValueError('Invalid region name: %s' % region_name)", "idx": 197}
{"namespace": "boto.glacier.job.Job.download_to_fileobj", "completion": "        num_chunks = self._calc_num_chunks(chunk_size)\n        for i in range(num_chunks):\n            byte_range = ((i * chunk_size), ((i + 1) * chunk_size) - 1)\n            data, expected_tree_hash = self._download_byte_range(\n                byte_range, retry_exceptions)\n            if verify_hashes:\n                actual_tree_hash = tree_hash_from_str(data)\n                if expected_tree_hash != actual_tree_hash:\n                    raise TreeHashDoesNotMatchError(\n                        \"The calculated tree hash %s does not match the \"\n                        \"expected tree hash %s for the byte range %s\" % (\n                            actual_tree_hash, expected_tree_hash, byte_range))\n            output_file.write(data)", "idx": 198}
{"namespace": "boto.glacier.utils.minimum_part_size", "completion": "    if size_in_bytes <= default_part_size * MAXIMUM_NUMBER_OF_PARTS:\n        return default_part_size\n    else:\n        part_size = math.ceil(size_in_bytes / MAXIMUM_NUMBER_OF_PARTS)\n        if part_size > 4 * _MEGABYTE * _MEGABYTE:\n            raise ValueError('File size exceeds maximum allowed archive size')\n        return part_size", "idx": 199}
{"namespace": "boto.glacier.utils.chunk_hashes", "completion": "    chunks = [bytestring[i:i+chunk_size] for i in range(0, len(bytestring), chunk_size)]\n    hashes = [hashlib.sha256(chunk).digest() for chunk in chunks]\n    if not hashes:\n        hashes = [hashlib.sha256(b'').digest()]\n    return hashes", "idx": 200}
{"namespace": "boto.glacier.utils.compute_hashes_from_fileobj", "completion": "    linear_hash = hashlib.sha256()\n    chunks = []\n    while True:\n        chunk = fileobj.read(chunk_size)\n        if not chunk:\n            break\n        linear_hash.update(chunk)\n        chunks.append(hashlib.sha256(chunk).digest())\n    linear_hash = linear_hash.hexdigest()\n    tree_hash = bytes_to_hex(tree_hash(chunks))\n    return linear_hash, tree_hash", "idx": 201}
{"namespace": "boto.glacier.concurrent.ConcurrentTransferer._calculate_required_part_size", "completion": "        min_part_size = max(int(math.ceil(total_size / 10000.0)), self._part_size)\n        total_parts = int(math.ceil(total_size / float(min_part_size)))\n        return total_parts, min_part_size", "idx": 202}
{"namespace": "boto.glacier.connect_to_region", "completion": "        from boto.glacier.layer2 import Layer2\n    from boto.glacier.layer2 import Layer2\n    for region in get_regions('glacier', connection_cls=Layer2):\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    raise ValueError('Invalid region name: %s' % region_name)", "idx": 203}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.update", "completion": "        updated = self.connection.get_network_interface_attribute(self.id, 'description', dry_run=dry_run)\n        if updated is not None:\n            self._update(updated)\n            return self.status\n        elif validate:\n            raise ValueError('No data returned for %s' % self.id)\n        else:\n            return None", "idx": 204}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.attach", "completion": "        attachment = self.connection.attach_network_interface(\n            self.id,\n            instance_id,\n            device_index,\n            dry_run=dry_run\n        )\n        if attachment:\n            self.attachment = attachment\n            return True\n        else:\n            return False", "idx": 205}
{"namespace": "boto.ec2.networkinterface.NetworkInterface.detach", "completion": "        if self.attachment:\n            return self.connection.detach_network_interface(\n                self.attachment.id,\n                force,\n                dry_run=dry_run\n            )\n        else:\n            raise BotoClientError('Cannot detach a network interface that is not attached')", "idx": 206}
{"namespace": "boto.ec2.address.Address.release", "completion": "        if self.allocation_id:\n            return self.connection.release_address(\n                public_ip=None,\n                allocation_id=self.allocation_id,\n                dry_run=dry_run\n            )\n        else:\n            return self.connection.release_address(\n                public_ip=self.public_ip,\n                dry_run=dry_run\n            )", "idx": 207}
{"namespace": "boto.ec2.address.Address.associate", "completion": "        if self.allocation_id:\n            return self.connection.associate_address(\n                instance_id=instance_id,\n                public_ip=None,\n                allocation_id=self.allocation_id,\n                network_interface_id=network_interface_id,\n                private_ip_address=private_ip_address,\n                allow_reassociation=allow_reassociation,\n                dry_run=dry_run\n            )\n        else:\n            return self.connection.associate_address(\n                instance_id=instance_id,\n                public_ip=self.public_ip,\n                allocation_id=None,\n                network_interface_id=network_interface_id,\n                private_ip_address=private_ip_address,\n                allow_reassociation=allow_reassociation,\n                dry_run=dry_run\n            )", "idx": 208}
{"namespace": "boto.ec2.address.Address.disassociate", "completion": "        if self.association_id:\n            return self.connection.disassociate_address(\n                association_id=self.association_id,\n                dry_run=dry_run\n            )\n        else:\n            return self.connection.disassociate_address(\n                public_ip=self.public_ip,\n                dry_run=dry_run\n            )", "idx": 209}
{"namespace": "boto.ec2.ec2object.TaggedEC2Object.add_tags", "completion": "        status = self.connection.create_tags([self.id], tags, dry_run=dry_run)\n        if status:\n            for key, value in tags.items():\n                self.tags[key] = value", "idx": 210}
{"namespace": "boto.ec2.ec2object.TaggedEC2Object.remove_tags", "completion": "        status = self.connection.delete_tags(\n            [self.id],\n            tags,\n            dry_run=dry_run\n        )\n        for key in tags.keys():\n            if key in self.tags:\n                del self.tags[key]", "idx": 211}
{"namespace": "boto.ec2.connection.EC2Connection.get_all_instance_status", "completion": "        params = {}\n        if instance_ids:\n            self.build_list_params(params, instance_ids, 'InstanceId')\n        if max_results is not None:\n            params['MaxResults'] = max_results\n        if next_token:\n            params['NextToken'] = next_token\n        if filters:\n            self.build_filter_params(params, filters)\n        if dry_run:\n            params['DryRun'] = 'true'\n        if include_all_instances:\n            params['IncludeAllInstances'] = 'true'\n        return self.get_list('DescribeInstanceStatus', params,\n                             [('item', InstanceStatus)], verb='POST')", "idx": 212}
{"namespace": "boto.ec2.volume.Volume.update", "completion": "        if dry_run:\n            return 'Dry run successful. No changes were made.'\n        else:\n            updated = self.connection.get_all_volumes([self.id])\n            if len(updated) == 1:\n                self._update(updated[0])\n                return self.status\n            elif validate:\n                raise ValueError('Volume %s not found' % self.id)\n            else:\n                return None", "idx": 213}
{"namespace": "boto.ec2.volume.Volume.attach", "completion": "        return self.connection.attach_volume(self.id, instance_id, device, dry_run=dry_run)", "idx": 214}
{"namespace": "boto.ec2.volume.Volume.detach", "completion": "        return self.connection.detach_volume(\n            self.id,\n            instance_id=self.attach_data.instance_id,\n            device=self.attach_data.device,\n            force=force,\n            dry_run=dry_run\n        )", "idx": 215}
{"namespace": "boto.ec2.volume.Volume.create_snapshot", "completion": "        return self.connection.create_snapshot(self.id, description, dry_run=dry_run)", "idx": 216}
{"namespace": "boto.ec2.volume.Volume.attachment_state", "completion": "        if self.attach_data is not None:\n            return self.attach_data.status\n        else:\n            return None", "idx": 217}
{"namespace": "boto.ec2.securitygroup.SecurityGroup.add_rule", "completion": "    rule = IPPermissions()\n    rule.ip_protocol = ip_protocol\n    rule.from_port = from_port\n    rule.to_port = to_port\n\n    # If cidr_ip is specified, set up a CIDR-based grant\n    if cidr_ip:\n        grant = GroupOrCIDR()\n        grant.cidr_ip = cidr_ip\n        rule.grants.append(grant)\n\n    # If a source group is specified, set up a source group-based grant\n    if src_group_group_id or src_group_name:\n        grant = GroupOrCIDR()\n        grant.owner_id = src_group_owner_id\n        grant.group_id = src_group_group_id\n        grant.name = src_group_name\n        rule.grants.append(grant)\n\n    self.rules.append(rule)", "idx": 218}
{"namespace": "boto.ec2.connect_to_region", "completion": "    region = get_region(region_name, **kw_params)\n    if region:\n        return region.connect(**kw_params)\n    else:\n        return None", "idx": 219}
{"namespace": "boto.ec2.cloudwatch.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 220}
{"namespace": "boto.ec2.autoscale.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 221}
{"namespace": "boto.ec2.elb.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 222}
{"namespace": "boto.ec2.elb.ELBConnection.get_all_load_balancers", "completion": "        params = {}\n        if load_balancer_names:\n            self.build_list_params(params, load_balancer_names,\n                                   'LoadBalancerNames.member.%d')\n        if marker:\n            params['Marker'] = marker\n        return self.get_list('DescribeLoadBalancers', params,\n                             [('member', LoadBalancer)])", "idx": 223}
{"namespace": "boto.ec2.elb.ELBConnection.disable_availability_zones", "completion": "        params = {'LoadBalancerName': load_balancer_name}\n        self.build_list_params(params, zones_to_remove,\n                               'AvailabilityZones.member.%d')\n        obj = self.get_object('DisableAvailabilityZonesForLoadBalancer',\n                              params, LoadBalancerZones)\n        return obj.zones", "idx": 224}
{"namespace": "boto.awslambda.connect_to_region", "completion": "        from boto.awslambda.layer1 import AWSLambdaConnection\n    from boto.awslambda.layer1 import AWSLambdaConnection\n    region = next((r for r in regions() if r.name == region_name), None)\n    if not region:\n        raise ValueError(\"Region name {} not found.\".format(region_name))\n    return AWSLambdaConnection(region=region, **kw_params)", "idx": 225}
{"namespace": "boto.cognito.identity.connect_to_region", "completion": "        from boto.cognito.identity.layer1 import CognitoIdentityConnection\n    from boto.cognito.identity.layer1 import CognitoIdentityConnection\n    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    raise ValueError('Invalid region name: %s' % region_name)", "idx": 226}
{"namespace": "boto.cognito.sync.connect_to_region", "completion": "        from boto.cognito.sync.layer1 import CognitoSyncConnection\n    from boto.cognito.sync.layer1 import CognitoSyncConnection\n    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    raise ValueError('Invalid region name: %s' % region_name)", "idx": 227}
{"namespace": "boto.cloudformation.connect_to_region", "completion": "    region = next((r for r in regions() if r.name == region_name), None)\n    if region:\n        return CloudFormationConnection(region=region, **kw_params)\n    else:\n        return None", "idx": 228}
{"namespace": "boto.route53.zone.Zone.find_records", "completion": "        name = self.route53connection._make_qualified(name)\n        rrsets = self.route53connection.get_all_rrsets(self.id, type, name, identifier)\n        if len(rrsets) == 0:\n            return None\n        elif len(rrsets) == 1 or all:\n            return rrsets\n        elif len(rrsets) <= desired:\n            return rrsets[0]\n        else:\n            raise TooManyRecordsException(\n                \"Too many records: %d. Desired: %d\" % (len(rrsets), desired))", "idx": 229}
{"namespace": "boto.route53.domains.connect_to_region", "completion": "        from boto.route53.domains.layer1 import Route53DomainsConnection\n    from boto.route53.domains.layer1 import Route53DomainsConnection\n\n    for region in regions():\n        if region.name == region_name:\n            return region.connect(connection_cls=Route53DomainsConnection, **kw_params)\n    raise ValueError('Invalid region name: %s' % region_name)", "idx": 230}
{"namespace": "boto.s3.key.Key.get_contents_to_filename", "completion": "        with open(filename, 'wb') as fp:\n            self.get_contents_to_file(fp, headers, cb, num_cb, torrent=torrent,\n                                      version_id=version_id,\n                                      res_download_handler=res_download_handler,\n                                      response_headers=response_headers)", "idx": 231}
{"namespace": "boto.s3.cors.CORSConfiguration.add_rule", "completion": "        rule = CORSRule(allowed_method, allowed_origin, id, allowed_header, max_age_seconds, expose_header)\n        self.append(rule)", "idx": 232}
{"namespace": "boto.s3.bucket.Bucket.get_key", "completion": "        if validate:\n            query_args_l = []\n            if version_id:\n                query_args_l.append('versionId=%s' % version_id)\n            if response_headers:\n                for rk, rv in response_headers.items():\n                    query_args_l.append('%s=%s' % (rk, urllib.parse.quote(rv)))\n            key, resp = self._get_key_internal(key_name, headers, query_args_l)\n            if resp.status == 200:\n                return key\n        else:\n            return self.new_key(key_name)", "idx": 233}
{"namespace": "boto.s3.bucket.Bucket.new_key", "completion": "        return self.key_class(self, key_name)", "idx": 234}
{"namespace": "boto.s3.bucket.Bucket.delete_key", "completion": "        query_args_l = []\n        if version_id:\n            query_args_l.append('versionId=%s' % version_id)\n        return self._delete_key_internal(key_name, headers,\n                                         version_id=version_id,\n                                         mfa_token=mfa_token,\n                                         query_args_l=query_args_l)", "idx": 235}
{"namespace": "boto.s3.bucket.Bucket.get_tags", "completion": "        body = self.get_xml_tags(headers)\n        tags = Tags()\n        h = handler.XmlHandler(tags, self)\n        if not isinstance(body, bytes):\n            body = body.encode('utf-8')\n        xml.sax.parseString(body, h)\n        return tags", "idx": 236}
{"namespace": "boto.s3.connection.S3Connection._required_auth_capability", "completion": "        if self.anon:\n            return ['anon']\n        else:\n            return ['s3']", "idx": 237}
{"namespace": "boto.s3.connection.S3Connection.generate_url_sigv4", "completion": "\n        if headers is None:\n            headers = {}\n\n        if response_headers:\n            headers.update(response_headers)\n\n        if version_id is not None:\n            headers['versionId'] = version_id\n\n        if iso_date is None:\n            iso_date = time.strftime(\"%Y%m%dT%H%M%SZ\", time.gmtime())\n\n        headers['Date'] = iso_date\n\n        if self.provider.security_token:\n            headers['x-amz-security-token'] = self.provider.security_token\n\n        auth_path = self.calling_format.build_auth_path(bucket, key)\n        auth_path = self.get_path(auth_path)\n\n        host = self.calling_format.build_host(self.server_name(), bucket)\n\n        if force_http:\n            protocol = 'http'\n            port = 80\n        else:\n            protocol = self.protocol\n            port = self.port\n\n        url = self.calling_format.build_url_base(self, protocol, host, bucket, key)\n\n        canonical_request = self._auth_handler.canonical_request(method, auth_path, headers)\n        string_to_sign = self._auth_handler.string_to_sign(canonical_request, iso_date)\n        signature = self._auth_handler.signature(string_to_sign)\n\n        signed_headers = ';'.join(sorted([h.lower() for h in headers.keys() if h.startswith('x-amz-')]))\n\n        query_args = {\n            'X-Amz-Algorithm': 'AWS4-HMAC-SHA256',\n            'X-Amz-Credential': '{}/{}/{}/s3/aws4_request'.format(self.aws_access_key_id, iso_date[:8], self.region_name),\n            'X-Amz-Date': iso_date,\n            'X-Amz-Expires': str(expires_in),\n            'X-Amz-SignedHeaders': signed_headers,\n            'X-Amz-Signature': signature\n        }\n\n        url += '?' + urllib.parse.urlencode(query_args)\n\n        return url", "idx": 238}
{"namespace": "boto.s3.lifecycle.Lifecycle.add_rule", "completion": "        rule = Rule(id=id, prefix=prefix, status=status, expiration=expiration, transition=transition)\n        self.append(rule)", "idx": 239}
{"namespace": "boto.s3.website.WebsiteConfiguration.to_xml", "completion": "        parts = []\n        if self.suffix is not None:\n            parts.append(tag('IndexDocument', tag('Suffix', self.suffix)))\n        if self.error_key is not None:\n            parts.append(tag('ErrorDocument', tag('Key', self.error_key)))\n        if self.redirect_all_requests_to is not None:\n            parts.append(self.redirect_all_requests_to.to_xml())\n        if self.routing_rules is not None:\n            parts.append(self.routing_rules.to_xml())\n        return '<WebsiteConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">%s</WebsiteConfiguration>' % ''.join(parts)", "idx": 240}
{"namespace": "boto.s3.website.RoutingRules.to_xml", "completion": "        parts = ['<RoutingRules>']\n        for rule in self:\n            parts.append(rule.to_xml())\n        parts.append('</RoutingRules>')\n        return ''.join(parts)", "idx": 241}
{"namespace": "boto.s3.website.RoutingRule.when", "completion": "        condition = Condition(key_prefix=key_prefix, http_error_code=http_error_code)\n        return cls(condition=condition)", "idx": 242}
{"namespace": "boto.s3.website.RoutingRule.then_redirect", "completion": "        self.redirect = Redirect(hostname=hostname, protocol=protocol, replace_key=replace_key,\n                                 replace_key_prefix=replace_key_prefix, http_redirect_code=http_redirect_code)\n        return self", "idx": 243}
{"namespace": "boto.s3.connect_to_region", "completion": "        from boto.s3.connection import S3Connection\n    from boto.s3.connection import S3Connection\n    if 'host' in kw_params:\n        region = S3RegionInfo(name=region_name, endpoint=kw_params['host'])\n        return region.connect(**kw_params)\n    else:\n        for region in regions():\n            if region.name == region_name:\n                return region.connect(**kw_params)\n    raise ValueError('Unknown region name: %s' % region_name)", "idx": 244}
{"namespace": "boto.directconnect.connect_to_region", "completion": "        from boto.directconnect.layer1 import DirectConnectConnection\n    from boto.directconnect.layer1 import DirectConnectConnection\n    for region in get_regions('directconnect', connection_cls=DirectConnectConnection):\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    raise ValueError('Invalid region name: %s' % region_name)", "idx": 245}
{"namespace": "boto.rds.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 246}
{"namespace": "boto.datapipeline.connect_to_region", "completion": "        from boto.regioninfo import get_region\n        from boto.datapipeline.layer1 import DataPipelineConnection\n    from boto.datapipeline.layer1 import DataPipelineConnection\n    from boto.regioninfo import get_region\n\n    region = get_region(region_name)\n    if not region:\n        raise ValueError('Invalid region name: %s' % region_name)\n    return region.connect(connection_cls=DataPipelineConnection, **kw_params)", "idx": 247}
{"namespace": "boto.dynamodb.batch.Batch.to_dict", "completion": "        d = {}\n        d['Keys'] = []\n        for key in self.keys:\n            if isinstance(key, tuple):\n                hash_key, range_key = key\n            else:\n                hash_key = key\n                range_key = None\n            k = self.table.layer2.build_key_from_values(self.table.schema,\n                                                        hash_key, range_key)\n            d['Keys'].append(k)\n        if self.attributes_to_get:\n            d['AttributesToGet'] = self.attributes_to_get\n        if self.consistent_read:\n            d['ConsistentRead'] = self.consistent_read\n        return d", "idx": 248}
{"namespace": "boto.dynamodb.batch.BatchList.to_dict", "completion": "        batch_list_dict = {}\n        for batch in self:\n            table_name, batch_dict = batch.to_dict()\n            batch_list_dict[table_name] = batch_dict\n        return batch_list_dict", "idx": 249}
{"namespace": "boto.dynamodb.types.Dynamizer.encode", "completion": "        dynamodb_type = self._get_dynamodb_type(attr)\n        try:\n            encoder = getattr(self, '_encode_%s' % dynamodb_type.lower())\n        except AttributeError:\n            msg = 'No encoder for type \"%s\"' % dynamodb_type\n            raise TypeError(msg)\n        return {dynamodb_type: encoder(attr)}", "idx": 250}
{"namespace": "boto.dynamodb.types.Dynamizer.decode", "completion": "        if len(attr.keys()) > 1:\n            return attr\n        dynamodb_type, value = next(iter(attr.items()))\n        try:\n            decoder = getattr(self, '_decode_%s' % dynamodb_type.lower())\n        except AttributeError:\n            raise ValueError(\"Unable to decode dynamodb type: %s\" %\n                             dynamodb_type)\n        return decoder(value)", "idx": 251}
{"namespace": "boto.dynamodb.connect_to_region", "completion": "        from boto.dynamodb.layer2 import Layer2\n    from boto.dynamodb.layer2 import Layer2\n\n    for region in regions():\n        if region.name == region_name:\n            return Layer2(region=region, **kw_params)\n    raise ValueError('Invalid region name: %s' % region_name)", "idx": 252}
{"namespace": "boto.beanstalk.connect_to_region", "completion": "        from boto.beanstalk.layer1 import Layer1\n    from boto.beanstalk.layer1 import Layer1\n    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params, connection_cls=Layer1)\n    raise ValueError('Invalid region name: %s' % region_name)", "idx": 253}
{"namespace": "boto.swf.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    raise ValueError('Invalid region name: %s' % region_name)", "idx": 254}
{"namespace": "boto.opsworks.regions", "completion": "        from boto.opsworks.layer1 import OpsWorksConnection\n    from boto.opsworks.layer1 import OpsWorksConnection\n    return RegionInfo.get_regions('opsworks', connection_cls=OpsWorksConnection)", "idx": 255}
{"namespace": "boto.opsworks.connect_to_region", "completion": "        from boto.regioninfo import get_region\n        from boto.opsworks.layer1 import OpsWorksConnection\n    from boto.opsworks.layer1 import OpsWorksConnection\n    from boto.regioninfo import get_region\n\n    region = get_region(region_name)\n    if region is None:\n        raise ValueError('Invalid region name: %s' % region_name)\n    return OpsWorksConnection(region=region, **kw_params)", "idx": 256}
{"namespace": "boto.sqs.connect_to_region", "completion": "        from boto.regioninfo import connect\n        from boto.sqs.connection import SQSConnection\n\n    from boto.sqs.connection import SQSConnection\n    from boto.regioninfo import connect\n\n    for region in regions():\n        if region.name == region_name:\n            return connect('sqs', region=region, connection_cls=SQSConnection, **kw_params)\n\n    raise ValueError('Invalid region name: %s' % region_name)", "idx": 257}
{"namespace": "boto.rds2.connect_to_region", "completion": "        from boto.rds2.layer1 import RDSConnection\n    from boto.rds2.layer1 import RDSConnection\n    for region in get_regions('rds'):\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 258}
{"namespace": "boto.cloudsearch2.connect_to_region", "completion": "        from boto.regioninfo import RegionInfo, get_regions\n        from boto.cloudsearch2.layer1 import CloudSearchConnection\n    from boto.cloudsearch2.layer1 import CloudSearchConnection\n    from boto.regioninfo import RegionInfo, get_regions\n\n    for region in get_regions('cloudsearch'):\n        if region.name == region_name:\n            return CloudSearchConnection(region=region, **kw_params)\n\n    raise ValueError(\"Invalid region name: %s\" % region_name)", "idx": 259}
{"namespace": "boto.cloudtrail.connect_to_region", "completion": "        from boto.cloudtrail.layer1 import CloudTrailConnection\n    from boto.cloudtrail.layer1 import CloudTrailConnection\n    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    raise ValueError('Invalid region name: %s' % region_name)", "idx": 260}
{"namespace": "boto.elasticache.connect_to_region", "completion": "        from boto.elasticache.layer1 import ElastiCacheConnection\n    from boto.elasticache.layer1 import ElastiCacheConnection\n    for region in get_regions('elasticache', connection_cls=ElastiCacheConnection):\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    raise ValueError('Invalid region name: %s' % region_name)", "idx": 261}
{"namespace": "boto.ses.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 262}
{"namespace": "boto.codedeploy.connect_to_region", "completion": "        from boto.codedeploy.layer1 import CodeDeployConnection\n    from boto.codedeploy.layer1 import CodeDeployConnection\n    region = RegionInfo(name=region_name, endpoint='codedeploy.' + region_name + '.amazonaws.com')\n    return CodeDeployConnection(region=region, **kw_params)", "idx": 263}
{"namespace": "boto.sts.credentials.Credentials.to_dict", "completion": "        return {\n            'access_key': self.access_key,\n            'secret_key': self.secret_key,\n            'session_token': self.session_token,\n            'expiration': self.expiration,\n            'request_id': self.request_id\n        }", "idx": 264}
{"namespace": "boto.sts.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 265}
{"namespace": "boto.machinelearning.connect_to_region", "completion": "        from boto.machinelearning.layer1 import MachineLearningConnection\n    from boto.machinelearning.layer1 import MachineLearningConnection\n\n    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n\n    raise ValueError('Invalid region name: %s' % region_name)", "idx": 266}
{"namespace": "boto.vpc.connect_to_region", "completion": "    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    return None", "idx": 267}
{"namespace": "boto.vpc.VPCConnection.get_all_vpc_peering_connections", "completion": "        params = {}\n        if vpc_peering_connection_ids:\n            self.build_list_params(params, vpc_peering_connection_ids, 'VpcPeeringConnectionId')\n        if filters:\n            self.build_filter_params(params, filters)\n        if dry_run:\n            params['DryRun'] = 'true'\n        return self.get_list('DescribeVpcPeeringConnections', params, [('item', VpcPeeringConnection)])", "idx": 268}
{"namespace": "boto.kinesis.connect_to_region", "completion": "        from boto.kinesis.layer1 import KinesisConnection\n    from boto.kinesis.layer1 import KinesisConnection\n    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n    raise ValueError('Invalid region name: %s' % region_name)", "idx": 269}
{"namespace": "boto.ec2containerservice.connect_to_region", "completion": "        from boto.regioninfo import RegionInfo\n        from boto.ec2containerservice.layer1 import EC2ContainerServiceConnection\n    from boto.ec2containerservice.layer1 import EC2ContainerServiceConnection\n    from boto.regioninfo import RegionInfo\n\n    region = RegionInfo(name=region_name, endpoint=None)\n    return EC2ContainerServiceConnection(region=region, **kw_params)", "idx": 270}
{"namespace": "boto.dynamodb2.table.Table._introspect_indexes", "completion": "        return self._introspect_all_indexes(\n            raw_indexes,\n            self._PROJECTION_TYPE_TO_INDEX.get('local_indexes'))", "idx": 271}
{"namespace": "boto.dynamodb2.table.Table.describe", "completion": "        data = self.connection.describe_table(self.table_name)\n        self.throughput = {\n            'read': data['Table']['ProvisionedThroughput']['ReadCapacityUnits'],\n            'write': data['Table']['ProvisionedThroughput']['WriteCapacityUnits'],\n        }\n        self.schema = self._introspect_schema(data['Table']['KeySchema'], data['Table']['AttributeDefinitions'])\n        self.indexes = self._introspect_indexes(data['Table'].get('LocalSecondaryIndexes', []))\n        self.global_indexes = self._introspect_global_indexes(data['Table'].get('GlobalSecondaryIndexes', []))\n        return data", "idx": 272}
{"namespace": "boto.dynamodb2.table.Table.update", "completion": "        if throughput is not None:\n            self.throughput = throughput\n\n        if global_indexes is not None:\n            self.global_indexes = global_indexes\n\n        # Update the table's provisioned throughput\n        if throughput is not None:\n            self.connection.update_table(\n                self.table_name,\n                provisioned_throughput={\n                    'ReadCapacityUnits': int(self.throughput['read']),\n                    'WriteCapacityUnits': int(self.throughput['write']),\n                }\n            )\n\n        # Update the global secondary indexes' provisioned throughput\n        if global_indexes is not None:\n            gsi_data = []\n\n            for gsi_name, gsi_throughput in global_indexes.items():\n                gsi_data.append({\n                    \"Update\": {\n                        \"IndexName\": gsi_name,\n                        \"ProvisionedThroughput\": {\n                            \"ReadCapacityUnits\": int(gsi_throughput['read']),\n                            \"WriteCapacityUnits\": int(gsi_throughput['write']),\n                        },\n                    },\n                })\n\n            self.connection.update_table(\n                self.table_name,\n                global_secondary_index_updates=gsi_data,\n            )\n\n        return True", "idx": 273}
{"namespace": "boto.dynamodb2.table.Table.create_global_secondary_index", "completion": "        if global_index:\n            gsi_data = [\n                {\n                    \"Create\": global_index.schema()\n                }\n            ]\n\n            self.connection.update_table(\n                self.table_name,\n                global_secondary_index_updates=gsi_data,\n            )\n\n            return True\n        else:\n            msg = 'You need to provide the global index to ' \\\n                  'create_global_secondary_index method'\n            boto.log.error(msg)\n\n            return False", "idx": 274}
{"namespace": "boto.dynamodb2.table.Table.delete_global_secondary_index", "completion": "        if global_index_name:\n            gsi_data = []\n\n            gsi_data.append({\n                \"Delete\": {\n                    \"IndexName\": global_index_name,\n                },\n            })\n\n            self.connection.update_table(\n                self.table_name,\n                global_secondary_index_updates=gsi_data,\n            )\n\n            return True\n        else:\n            msg = 'You need to provide the global index name to ' \\\n                  'delete_global_secondary_index method'\n            boto.log.error(msg)\n\n            return False", "idx": 275}
{"namespace": "boto.dynamodb2.table.Table.update_global_secondary_index", "completion": "        if global_indexes:\n            gsi_data = []\n\n            for gsi_name, gsi_throughput in global_indexes.items():\n                gsi_data.append({\n                    \"Update\": {\n                        \"IndexName\": gsi_name,\n                        \"ProvisionedThroughput\": {\n                            \"ReadCapacityUnits\": int(gsi_throughput['read']),\n                            \"WriteCapacityUnits\": int(gsi_throughput['write']),\n                        },\n                    },\n                })\n\n            self.connection.update_table(\n                self.table_name,\n                global_secondary_index_updates=gsi_data,\n            )\n\n            return True\n        else:\n            msg = 'You need to provide the global_index to update_global_secondary_index method'\n            boto.log.error(msg)\n\n            return False", "idx": 276}
{"namespace": "boto.dynamodb2.table.Table.delete", "completion": "        self.connection.delete_table(self.table_name)\n        return True", "idx": 277}
{"namespace": "boto.dynamodb2.table.Table.get_item", "completion": "        raw_key = self._encode_keys(kwargs)\n        raw_results = self.connection.get_item(\n            self.table_name,\n            raw_key,\n            consistent_read=consistent,\n            attributes_to_get=attributes,\n        )\n\n        if 'Item' not in raw_results:\n            raise exceptions.ItemNotFound(\n                \"Item not found in the table.\"\n            )\n\n        item = Item(self)\n        item.load(raw_results)\n        return item", "idx": 278}
{"namespace": "boto.dynamodb2.table.Table.has_item", "completion": "        try:\n            self.get_item(consistent=consistent, attributes=attributes, **kwargs)\n            return True\n        except exceptions.ItemNotFound:\n            return False", "idx": 279}
{"namespace": "boto.dynamodb2.table.Table._put_item", "completion": "        kwargs = {}\n\n        if expects is not None:\n            kwargs['expected'] = expects\n\n        self.connection.put_item(self.table_name, item_data, **kwargs)\n        return True", "idx": 280}
{"namespace": "boto.dynamodb2.table.Table.delete_item", "completion": "        raw_key = self._encode_keys(kwargs)\n        kwargs = {}\n\n        if expected is not None:\n            kwargs['expected'] = self._build_filters(\n                expected,\n                using=EXPECTED_OPERATORS\n            )\n\n        if conditional_operator is not None:\n            kwargs['conditional_operator'] = conditional_operator\n\n        self.connection.delete_item(self.table_name, raw_key, **kwargs)\n        return True", "idx": 281}
{"namespace": "boto.dynamodb2.table.Table.get_key_fields", "completion": "        if not self.schema:\n            self.describe()\n        return [field.name for field in self.schema]", "idx": 282}
{"namespace": "boto.dynamodb2.table.Table._build_filters", "completion": "        filters = {}\n\n        for key, value in filter_kwargs.items():\n            parts = key.split('__')\n            field = parts[0]\n            operator = parts[1] if len(parts) > 1 else 'eq'\n\n            if operator not in using:\n                raise exceptions.UnknownFilterTypeError(\n                    \"Operator '%s' does not exist.\" % operator\n                )\n\n            operator = using[operator]\n\n            if not isinstance(value, (set, list, tuple)):\n                value = [value]\n\n            value = [self._dynamizer.encode(v) for v in value]\n\n            filters[field] = {\n                'AttributeValueList': value,\n                'ComparisonOperator': operator,\n            }\n\n        return filters", "idx": 283}
{"namespace": "boto.dynamodb2.table.Table.batch_get", "completion": "        results = BatchGetResultSet()\n        results.to_call(self._batch_get, keys, consistent=consistent, attributes=attributes)\n        return results", "idx": 284}
{"namespace": "boto.dynamodb2.table.Table.count", "completion": "        result = self.describe()\n        return result['Table']['ItemCount']", "idx": 285}
{"namespace": "boto.dynamodb2.table.BatchTable.put_item", "completion": "        if overwrite:\n            for i, existing_item in enumerate(self._to_put):\n                if existing_item.keys() == data.keys():\n                    self._to_put[i] = data\n                    break\n            else:\n                self._to_put.append(data)\n        else:\n            self._to_put.append(data)\n\n        if self.should_flush():\n            self.flush()", "idx": 286}
{"namespace": "boto.dynamodb2.table.BatchTable.delete_item", "completion": "        self._to_delete.append(kwargs)\n\n        if self.should_flush():\n            self.flush()", "idx": 287}
{"namespace": "boto.dynamodb2.table.BatchTable.flush", "completion": "        batch_data = {\n            self.table.table_name: {\n                'PutRequest': [],\n                'DeleteRequest': [],\n            },\n        }\n\n        for item_data in self._to_put:\n            item = {\n                'Item': self.table._encode_keys(item_data),\n            }\n            batch_data[self.table.table_name]['PutRequest'].append(item)\n\n        for key_data in self._to_delete:\n            key = {\n                'Key': self.table._encode_keys(key_data),\n            }\n            batch_data[self.table.table_name]['DeleteRequest'].append(key)\n\n        resp = self.table.connection.batch_write_item(batch_data)\n        self.handle_unprocessed(resp)\n\n        # Clear out the batch.\n        self._to_put = []\n        self._to_delete = []\n\n        return True", "idx": 288}
{"namespace": "boto.dynamodb2.table.BatchTable.resend_unprocessed", "completion": "        while self._unprocessed:\n            batch_data = {\n                self.table.table_name: self._unprocessed[:25],\n            }\n            resp = self.table.connection.batch_write_item(batch_data)\n            self.handle_unprocessed(resp)\n            self._unprocessed = self._unprocessed[25:]", "idx": 289}
{"namespace": "boto.dynamodb2.fields.BaseSchemaField.definition", "completion": "        return {\n            'AttributeName': self.name,\n            'AttributeType': self.data_type,\n        }", "idx": 290}
{"namespace": "boto.dynamodb2.fields.BaseIndexField.definition", "completion": "        attr_definitions = []\n\n        for part in self.parts:\n            attr_definitions.append(part.definition())\n\n        return attr_definitions", "idx": 291}
{"namespace": "boto.dynamodb2.fields.BaseIndexField.schema", "completion": "        key_schema = []\n\n        for part in self.parts:\n            key_schema.append(part.schema())\n\n        return {\n            'IndexName': self.name,\n            'KeySchema': key_schema,\n            'Projection': {\n                'ProjectionType': self.projection_type,\n            },\n        }", "idx": 292}
{"namespace": "boto.dynamodb2.fields.GlobalBaseIndexField.schema", "completion": "        schema_data = super(GlobalBaseIndexField, self).schema()\n        schema_data['ProvisionedThroughput'] = {\n            'ReadCapacityUnits': self.throughput['read'],\n            'WriteCapacityUnits': self.throughput['write'],\n        }\n        return schema_data", "idx": 293}
{"namespace": "boto.dynamodb2.fields.GlobalIncludeIndex.schema", "completion": "        schema_data = IncludeIndex.schema(self)\n        # Update with the throughput.\n        schema_data.update(GlobalBaseIndexField.schema(self))\n        return schema_data\n        \"\"\"\n        This function returns the schema data for the GlobalIncludeIndex class. It first retrieves the schema data from the its superclass and then updates it with the schema data from the GlobalBaseIndexField superclass.\n        Input-Output Arguments\n        :param self: GlobalIncludeIndex. An instance of the GlobalIncludeIndex class.\n        :return: Dictionary. The schema data for the GlobalIncludeIndex class.\n        \"\"\"", "idx": 294}
{"namespace": "boto.dynamodb2.items.Item.get_keys", "completion": "        key_data = {}\n        for key in self.table.schema.keys():\n            key_data[key] = self._data.get(key)\n        return key_data", "idx": 295}
{"namespace": "boto.dynamodb2.items.Item.get_raw_keys", "completion": "        key_fields = self.table.get_key_fields()\n        key_data = {}\n\n        for key in key_fields:\n            key_data[key] = self._dynamizer.encode(self[key])\n\n        return key_data", "idx": 296}
{"namespace": "boto.dynamodb2.items.Item.build_expects", "completion": "        expects = {}\n        if fields is None:\n            fields = self._data.keys()\n\n        for field in fields:\n            if field in self._orig_data:\n                if self._data.get(field, NEWVALUE) == NEWVALUE:\n                    # This is a new field.\n                    expects[field] = {'Exists': False}\n                elif self._data[field] != self._orig_data[field]:\n                    # This field has changed.\n                    expects[field] = {\n                        'Value': self._dynamizer.encode(self._orig_data[field]),\n                        'Exists': True\n                    }\n                else:\n                    # This field is unchanged.\n                    expects[field] = {\n                        'Value': self._dynamizer.encode(self._orig_data[field]),\n                        'Exists': True\n                    }\n            elif field in self._data:\n                # This field is new.\n                expects[field] = {'Exists': False}\n\n        return expects", "idx": 297}
{"namespace": "boto.dynamodb2.items.Item.prepare_full", "completion": "        final_data = {}\n        for key, value in self._data.items():\n            if self._is_storable(value):\n                final_data[key] = self._dynamizer.encode(value)\n        return final_data", "idx": 298}
{"namespace": "boto.dynamodb2.items.Item.prepare_partial", "completion": "        alterations = self._determine_alterations()\n        final_data = {}\n        fields = set()\n\n        for key, value in alterations['adds'].items():\n            final_data[key] = {\n                'Action': 'PUT',\n                'Value': self._dynamizer.encode(value),\n            }\n            fields.add(key)\n\n        for key, value in alterations['changes'].items():\n            final_data[key] = {\n                'Action': 'PUT',\n                'Value': self._dynamizer.encode(value),\n            }\n            fields.add(key)\n\n        for key in alterations['deletes']:\n            final_data[key] = {\n                'Action': 'DELETE',\n            }\n            fields.add(key)\n\n        return final_data, fields", "idx": 299}
{"namespace": "boto.dynamodb2.items.Item.partial_save", "completion": "        if not self.needs_save():\n            return False\n\n        final_data, fields = self.prepare_partial()\n        expects = self.build_expects(fields)\n\n        returned = self.table._update_item(self.get_raw_keys(), final_data, expects=expects)\n        # Mark the object as clean.\n        self.mark_clean()\n        return returned", "idx": 300}
{"namespace": "boto.dynamodb2.items.Item.save", "completion": "        if overwrite:\n            final_data = self.prepare_full()\n            returned = self.table.put_item(final_data)\n        else:\n            if self.needs_save():\n                final_data = self.prepare_full()\n                expects = self.build_expects()\n                returned = self.table.put_item(final_data, expects=expects)\n            else:\n                return False\n\n        # Mark the object as clean.\n        self.mark_clean()\n        return returned", "idx": 301}
{"namespace": "boto.dynamodb2.items.Item.delete", "completion": "        key = self.get_keys()\n        return self.table._delete_item(key)", "idx": 302}
{"namespace": "boto.dynamodb2.connect_to_region", "completion": "        from boto.dynamodb2.layer1 import DynamoDBConnection\n    from boto.dynamodb2.layer1 import DynamoDBConnection\n\n    for region in regions():\n        if region.name == region_name:\n            return region.connect(**kw_params)\n\n    raise ValueError(\"Invalid region name: %s\" % region_name)", "idx": 303}
{"namespace": "litecli.packages.parseutils.extract_tables", "completion": "    # Parse the SQL statement\n    parsed = sqlparse.parse(sql)\n\n    # Initialize an empty list to store the table names\n    tables = []\n\n    # Iterate over the parsed SQL statement\n    for statement in parsed:\n        # Extract the table identifiers from the statement\n        stream = extract_from_part(statement)\n        # Extract the table names from the identifiers and add them to the list\n        tables.extend(list(extract_table_identifiers(stream)))\n\n    return tables", "idx": 304}
{"namespace": "litecli.packages.parseutils.queries_start_with", "completion": "    for query in queries:\n        if query_starts_with(query, prefixes):\n            return True\n    return False", "idx": 305}
{"namespace": "litecli.packages.parseutils.is_destructive", "completion": "    destructive_keywords = [\"drop\", \"delete\", \"truncate\", \"alter\", \"update\", \"insert\"]\n    return queries_start_with(queries, destructive_keywords)", "idx": 306}
{"namespace": "litecli.packages.completion_engine.suggest_type", "completion": "\n    word_before_cursor = last_word(text_before_cursor, include='most_punctuations')\n\n    identifier = None\n    if word_before_cursor[-1] == '(' or word_before_cursor[-1] == ',':\n        p = sqlparse.parse(text_before_cursor)[0]\n        idx, token = p.token_prev(len(p.tokens) - 1)\n\n        if token and isinstance(token, Identifier):\n            identifier = token.get_real_name()\n\n    return suggest_based_on_last_token(word_before_cursor, text_before_cursor, full_text, identifier)", "idx": 307}
{"namespace": "datasette.plugins.get_plugins", "completion": "    plugins = []\n    for plugin in pm.list_name_plugin():\n        info = {}\n        info['name'] = plugin[0]\n        info['static_path'] = getattr(plugin[1], 'static_path', None)\n        info['templates_path'] = getattr(plugin[1], 'templates_path', None)\n        info['hooks'] = [hook for hook in dir(plugin[1]) if hook.startswith('hook_')]\n        try:\n            dist = pkg_resources.get_distribution(plugin[0])\n            info['version'] = dist.version\n            info['project_name'] = dist.project_name\n        except pkg_resources.DistributionNotFound:\n            pass\n        plugins.append(info)\n    return plugins", "idx": 308}
{"namespace": "datasette.facets.ColumnFacet.suggest", "completion": "        row_count = await self.get_row_count()\n        columns = await self.get_columns(self.sql, self.params)\n        facet_size = self.get_facet_size()\n        suggested_facets = []\n        for column in columns:\n            facet_sql = f\"\"\"\n                SELECT {column}, COUNT(*) as count\n                FROM ({self.sql})\n                GROUP BY {column}\n            \"\"\"\n            facet_rows = await self.ds.execute(self.database, facet_sql, self.params)\n            distinct_values = len(facet_rows)\n            if 1 < distinct_values < row_count and distinct_values <= facet_size:\n                if any(row[\"count\"] > 1 for row in facet_rows):\n                    suggested_facets.append({\n                        \"name\": column,\n                        \"toggle_url\": self.ds.absolute_url(\n                            self.request,\n                            self.ds.urls.path(\n                                path_with_added_args(self.request, {\"_facet\": column})\n                            ),\n                        ),\n                    })\n        return suggested_facets", "idx": 309}
{"namespace": "datasette.facets.ColumnFacet.facet_results", "completion": "        facet_results = []\n        facets_timed_out = []\n        facet_size = self.get_facet_size()\n        for source_and_config in self.get_configs():\n            config = source_and_config[\"config\"]\n            source = source_and_config[\"source\"]\n            column = config.get(\"column\") or config[\"simple\"]\n            facet_sql = \"\"\"\n                select {column} as value, count(*) as count from (\n                    {sql}\n                ) where value is not null\n                group by value\n                order by count(*) desc, value limit {limit}\n            \"\"\".format(\n                column=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            try:\n                facet_rows_results = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n                facet_results_values = []\n                facet_results.append(\n                    {\n                        \"name\": column,\n                        \"type\": self.type,\n                        \"results\": facet_results_values,\n                        \"hideable\": source != \"metadata\",\n                        \"toggle_url\": self.ds.urls.path(\n                            path_with_removed_args(\n                                self.request, {\"_facet\": column}\n                            )\n                        ),\n                        \"truncated\": len(facet_rows_results) > facet_size,\n                    }\n                )\n                facet_rows = facet_rows_results.rows[:facet_size]\n                pairs = self.get_querystring_pairs()\n                for row in facet_rows:\n                    value = str(row[\"value\"])\n                    selected = (f\"{column}\", value) in pairs\n                    if selected:\n                        toggle_path = path_with_removed_args(\n                            self.request, {f\"{column}\": value}\n                        )\n                    else:\n                        toggle_path = path_with_added_args(\n                            self.request, {f\"{column}\": value}\n                        )\n                    facet_results_values.append(\n                        {\n                            \"value\": value,\n                            \"label\": value,\n                            \"count\": row[\"count\"],\n                            \"toggle_url\": self.ds.absolute_url(\n                                self.request, toggle_path\n                            ),\n                            \"selected\": selected,\n                        }\n                    )\n            except QueryInterrupted:\n                facets_timed_out.append(column)\n\n        return facet_results, facets_timed_out", "idx": 310}
{"namespace": "datasette.facets.ArrayFacet.suggest", "completion": "        columns = await self.get_columns(self.sql, self.params)\n        already_enabled = [c[\"config\"][\"simple\"] for c in self.get_configs()]\n        suggested_facets = []\n        for column in columns:\n            if column in already_enabled:\n                continue\n            # Does this column contain any JSON arrays in the first 100 rows?\n            suggested_facet_sql = \"\"\"\n                select {column} from (\n                    {sql}\n                ) where json_type({column}) = 'array' limit 100;\n            \"\"\".format(\n                column=escape_sqlite(column), sql=self.sql\n            )\n            try:\n                results = await self.ds.execute(\n                    self.database,\n                    suggested_facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_suggest_time_limit_ms\"),\n                    log_sql_errors=False,\n                )\n                values = tuple(r[0] for r in results.rows)\n                if all(self._is_json_array_of_strings(v) for v in values if v is not None):\n                    suggested_facets.append(\n                        {\n                            \"name\": column,\n                            \"type\": \"array\",\n                            \"toggle_url\": self.ds.absolute_url(\n                                self.request,\n                                self.ds.urls.path(\n                                    path_with_added_args(\n                                        self.request, {\"_facet_array\": column}\n                                    )\n                                ),\n                            ),\n                        }\n                    )\n            except (QueryInterrupted, sqlite3.OperationalError):\n                continue\n        return suggested_facets", "idx": 311}
{"namespace": "datasette.facets.ArrayFacet.facet_results", "completion": "        facet_results = []\n        facets_timed_out = []\n        qs_pairs = self.get_querystring_pairs()\n        facet_size = self.get_facet_size()\n        for source_and_config in self.get_configs():\n            config = source_and_config[\"config\"]\n            source = source_and_config[\"source\"]\n            column = config.get(\"column\") or config[\"simple\"]\n            facet_sql = \"\"\"\n                select json_each.value as value, count(*) as count from (\n                    {sql}\n                ), json_each({col})\n                where json_each.value is not null\n                group by json_each.value order by count desc, value limit {limit}\n            \"\"\".format(\n                col=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            try:\n                facet_rows_results = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n                facet_results_values = []\n                facet_results.append(\n                    {\n                        \"name\": column,\n                        \"type\": self.type,\n                        \"results\": facet_results_values,\n                        \"hideable\": source != \"metadata\",\n                        \"toggle_url\": path_with_removed_args(\n                            self.request, {\"_facet_array\": column}\n                        ),\n                        \"truncated\": len(facet_rows_results) > facet_size,\n                    }\n                )\n                facet_rows = facet_rows_results.rows[:facet_size]\n                for row in facet_rows:\n                    selected = (f\"{column}__array\", str(row[\"value\"])) in qs_pairs\n                    if selected:\n                        toggle_path = path_with_removed_args(\n                            self.request, {f\"{column}__array\": str(row[\"value\"])}\n                        )\n                    else:\n                        toggle_path = path_with_added_args(\n                            self.request, {f\"{column}__array\": row[\"value\"]}\n                        )\n                    facet_results_values.append(\n                        {\n                            \"value\": row[\"value\"],\n                            \"label\": row[\"value\"],\n                            \"count\": row[\"count\"],\n                            \"toggle_url\": self.ds.absolute_url(\n                                self.request, toggle_path\n                            ),\n                            \"selected\": selected,\n                        }\n                    )\n            except QueryInterrupted:\n                facets_timed_out.append(column)\n\n        return facet_results, facets_timed_out", "idx": 312}
{"namespace": "datasette.facets.DateFacet.facet_results", "completion": "        facet_results = []\n        facets_timed_out = []\n\n        facet_size = self.get_facet_size()\n        for source_and_config in self.get_configs():\n            config = source_and_config[\"config\"]\n            source = source_and_config[\"source\"]\n            column = config.get(\"column\") or config[\"simple\"]\n            facet_sql = \"\"\"\n                select date({col}) as value, count(*) as count from (\n                    {sql}\n                )\n                where {col} is not null and {col} glob \"????-??-*\"\n                group by value order by count desc, value limit {limit}\n            \"\"\".format(\n                col=escape_sqlite(column), sql=self.sql, limit=facet_size + 1\n            )\n            try:\n                facet_rows_results = await self.ds.execute(\n                    self.database,\n                    facet_sql,\n                    self.params,\n                    truncate=False,\n                    custom_time_limit=self.ds.setting(\"facet_time_limit_ms\"),\n                )\n                facet_results_values = []\n                facet_results.append(\n                    {\n                        \"name\": column,\n                        \"type\": self.type,\n                        \"results\": facet_results_values,\n                        \"hideable\": source != \"metadata\",\n                        \"toggle_url\": self.ds.urls.path(\n                            path_with_removed_args(\n                                self.request, {\"_facet_date\": column}\n                            )\n                        ),\n                        \"truncated\": len(facet_rows_results) > facet_size,\n                    }\n                )\n                facet_rows = facet_rows_results.rows[:facet_size]\n                pairs = self.get_querystring_pairs()\n                for row in facet_rows:\n                    value = str(row[\"value\"])\n                    selected = (f\"{column}__date\", value) in pairs\n                    if selected:\n                        toggle_path = path_with_removed_args(\n                            self.request, {f\"{column}__date\": value}\n                        )\n                    else:\n                        toggle_path = path_with_added_args(\n                            self.request, {f\"{column}__date\": value}\n                        )\n                    facet_results_values.append(\n                        {\n                            \"value\": value,\n                            \"label\": value,\n                            \"count\": row[\"count\"],\n                            \"toggle_url\": self.ds.absolute_url(\n                                self.request, toggle_path\n                            ),\n                            \"selected\": selected,\n                        }\n                    )\n            except QueryInterrupted:\n                facets_timed_out.append(column)\n\n        return facet_results, facets_timed_out", "idx": 313}
{"namespace": "datasette.app.Datasette.invoke_startup", "completion": "        if self._startup_invoked:\n            return\n        self._startup_invoked = True\n        # pylint: disable=no-member\n        for hook in pm.hook.startup(datasette=self):\n            await await_me_maybe(hook)", "idx": 314}
{"namespace": "datasette.app.Datasette.get_database", "completion": "        if route is not None:\n            for db in self.databases.values():\n                if db.route == route:\n                    return db\n        elif name is not None:\n            return self.databases.get(name)\n        else:\n            for db in self.databases.values():\n                if db.name != \"_internal\":\n                    return db\n        return None", "idx": 315}
{"namespace": "datasette.app.Datasette.add_database", "completion": "        # Create a copy of the existing databases\n        new_databases = self.databases.copy()\n\n        # If no name is provided, generate a unique name\n        if name is None:\n            name = secrets.token_hex(4)\n\n        # If the name already exists, append a number to make it unique\n        if name in new_databases:\n            counter = 1\n            while f\"{name}-{counter}\" in new_databases:\n                counter += 1\n            name = f\"{name}-{counter}\"\n\n        # If no route is provided, use the name as the route\n        if route is None:\n            route = name\n\n        # Assign the name and route to the new database\n        db.name = name\n        db.route = route\n\n        # Add the new database to the copied databases dictionary\n        new_databases[name] = db\n\n        # Assign the copied dictionary back to the instance\n        self.databases = new_databases\n\n        return db", "idx": 316}
{"namespace": "datasette.app.Datasette.ensure_permissions", "completion": "        for permission in permissions:\n            if isinstance(permission, str):\n                action = permission\n                resource = None\n            else:\n                action, resource = permission\n            allowed = await self.permission_allowed(actor, action, resource)\n            if not allowed:\n                raise Forbidden(\"Permission denied for {} on {}\".format(action, resource))", "idx": 317}
{"namespace": "datasette.app.Datasette.check_visibility", "completion": "        visible = False\n        private = False\n\n        if permissions is None:\n            permissions = [(action, resource)]\n\n        for permission in permissions:\n            if isinstance(permission, str):\n                action, resource = permission, None\n            else:\n                action, resource = permission\n\n            allowed = await self.permission_allowed(actor, action, resource)\n            if allowed:\n                visible = True\n                if actor is not None:\n                    private = True\n\n        return visible, private", "idx": 318}
{"namespace": "datasette.app.Datasette.render_template", "completion": "        if not self._startup_invoked:\n            raise Exception(\"You must call .invoke_startup() before .render_template()\")\n        context = dict(context or {})\n        context.update(\n            {\n                \"app_css_hash\": self.app_css_hash(),\n                \"select_templates\": self.select_templates,\n                \"view_name\": view_name,\n                \"current_path\": request.path if request else \"\",\n                \"extra_css_urls\": await self._asset_urls(\n                    \"extra_css_urls\", templates, context, request, view_name\n                ),\n                \"extra_js_urls\": await self._asset_urls(\n                    \"extra_js_urls\", templates, context, request, view_name\n                ),\n                \"body_scripts\": await self._asset_urls(\n                    \"extra_body_script\", templates, context, request, view_name\n                ),\n            }\n        )\n        if request:\n            context[\"request\"] = request\n            context[\"actor\"] = request.actor\n            context[\"messages\"] = self._show_messages(request)\n        else:\n            context[\"request\"] = None\n            context[\"actor\"] = None\n            context[\"messages\"] = []\n        context[\"database_url\"] = lambda: self.urls.database(context[\"database\"])\n        context[\"table_url\"] = lambda: self.urls.table(\n            context[\"database\"], context[\"table\"]\n        )\n        context[\"logout_url\"] = lambda: self.urls.logout(context[\"database\"])\n        context[\"permissions\"] = lambda: self._permissions_info()\n        context[\"static\"] = lambda path: self.urls.static(path)\n        context[\"url\"] = self.urls.url\n        context[\"metadata\"] = self.metadata\n        context[\"render_markdown\"] = lambda text: Markup(\n            self.render_markdown(text, context)\n        )\n        context[\"render_date\"] = self.render_date\n        context[\"render_time\"] = self.render_time\n        context[\"render_datetime\"] = self.render_datetime\n        context[\"format_bytes\"] = self.format_bytes\n        context[\"to_css_class\"] = to_css_class\n        context[\"select_templates\"] = self.select_templates\n        context[\"prettify_url\"] = self.prettify_url\n        context[\"resolve_env_secrets\"] = resolve_env_secrets\n        context[\"escape_css_string\"] = escape_css_string\n        context[\"escape_sqlite\"] = escape_sqlite\n        context[\"settings\"] = self.settings_dict()\n        context[\"versions\"] = self._versions()\n        context[\"plugins\"] = self._plugins\n        context[\"threads\"] = self._threads\n        context[\"actor\"] = self._actor\n        context[\"connected_databases\"] = self._connected_databases\n        context[\"permission_allowed\"] = self.permission_allowed\n        context[\"check_visibility\"] = self.check_visibility\n        context[\"ensure_permissions\"] = self.ensure_permissions\n        context[\"expand_foreign_keys\"] = self.expand_foreign_keys\n        context[\"absolute_url\"] = self.absolute_url\n        context[\"sign\"] = self.sign\n        context[\"unsign\"] = self.unsign\n        context[\"app\"] = self\n        context[\"datasette\"] = self\n        # Now call any plugin hooks that modify the context\n        for hook in pm.hook.prepare_jinja2_context(\n            send_templated=self.send_templated,\n            render_template=self.render_template,\n            datasette=self,\n            request=request,\n            context=context,\n        ):\n            await await_me_maybe(hook)\n        template = self.jinja_env.select_template(templates)\n        return await template.render_async(context)", "idx": 319}
{"namespace": "datasette.app.DatasetteClient.get", "completion": "        async with httpx.AsyncClient(app=self.app) as client:\n            return await client.get(self._fix(path), **kwargs)", "idx": 320}
{"namespace": "datasette.utils.asgi.Request.full_path", "completion": "        if self.query_string:\n            return \"{}?{}\".format(self.path, self.query_string)\n        else:\n            return self.path", "idx": 321}
{"namespace": "datasette.utils.asgi.Request.post_body", "completion": "        body = b\"\"\n        more_body = True\n        while more_body:\n            message = await self.receive()\n            body += message.get(\"body\", b\"\")\n            more_body = message.get(\"more_body\", False)\n        return body", "idx": 322}
{"namespace": "datasette.utils.asgi.Request.fake", "completion": "        path, _, query_string = path_with_query_string.partition(\"?\")\n        scope = {\n            \"method\": method,\n            \"scheme\": scheme,\n            \"path\": path,\n            \"query_string\": query_string.encode(\"latin-1\"),\n            \"headers\": [],\n            \"url_route\": {\"kwargs\": url_vars or {}},\n        }\n        return cls(scope, receive=None)", "idx": 323}
{"namespace": "datasette.utils.asgi.Response.asgi_send", "completion": "\n        # Remove any existing content-type header\n        headers = {k: v for k, v in self.headers.items() if k.lower() != \"content-type\"}\n        headers[\"content-type\"] = self.content_type\n\n        # Add set-cookie headers\n        for cookie_header in self._set_cookie_headers:\n            headers[\"set-cookie\"] = cookie_header\n\n        # Send the response start message\n        await send(\n            {\n                \"type\": \"http.response.start\",\n                \"status\": self.status,\n                \"headers\": [\n                    [key.encode(\"latin1\"), value.encode(\"latin1\")]\n                    for key, value in headers.items()\n                ],\n            }\n        )\n\n        # Send the response body\n        if self.body is not None:\n            await send(\n                {\n                    \"type\": \"http.response.body\",\n                    \"body\": self.body.encode(\"utf-8\"),\n                }\n            )", "idx": 324}
{"namespace": "datasette.utils.asgi.Response.set_cookie", "completion": "        cookie = SimpleCookie()\n        cookie[key] = value\n        morsel = cookie[key]\n        if max_age is not None:\n            morsel[\"max-age\"] = max_age\n        if expires is not None:\n            morsel[\"expires\"] = expires\n        morsel[\"path\"] = path\n        if domain is not None:\n            morsel[\"domain\"] = domain\n        if secure:\n            morsel[\"secure\"] = True\n        if httponly:\n            morsel[\"httponly\"] = True\n        if samesite is not None:\n            assert samesite.lower() in SAMESITE_VALUES, \"Invalid samesite value\"\n            morsel[\"samesite\"] = samesite\n        self._set_cookie_headers.append(morsel.OutputString())", "idx": 325}
{"namespace": "datasette.utils.asgi.Response.html", "completion": "        return cls(\n            str(body),\n            status=status,\n            headers=headers,\n            content_type=\"text/html; charset=utf-8\",\n        )", "idx": 326}
{"namespace": "datasette.utils.asgi.Response.text", "completion": "        return cls(\n            body,\n            status=status,\n            headers=headers,\n            content_type=\"text/plain; charset=utf-8\",\n        )", "idx": 327}
{"namespace": "datasette.utils.asgi.Response.json", "completion": "        return cls(\n            json.dumps(body, default=default),\n            status=status,\n            headers=headers,\n            content_type=\"application/json; charset=utf-8\",\n        )", "idx": 328}
{"namespace": "datasette.utils.asgi.Response.redirect", "completion": "        headers = headers or {}\n        headers[\"Location\"] = path\n        return cls(\n            body=\"\",\n            status=status,\n            headers=headers,\n            content_type=\"text/html; charset=utf-8\",\n        )", "idx": 329}
{"namespace": "datasette.utils.validate_sql_select", "completion": "    # Remove comments from SQL\n    sql = re.sub(r\"/\\*.*?\\*/\", \"\", sql, flags=re.DOTALL)\n    sql = re.sub(r\"--.*?$\", \"\", sql, flags=re.MULTILINE)\n    sql = sql.lower().strip()\n\n    # Check if SQL matches any of the allowed patterns\n    if not any(pattern.match(sql) for pattern in allowed_sql_res):\n        raise InvalidSql(f\"Invalid SQL: {sql}\")\n\n    # Check if SQL matches any of the disallowed patterns\n    for pattern, message in disallawed_sql_res:\n        if pattern.search(sql):\n            raise InvalidSql(f\"{message}: {sql}\")", "idx": 330}
{"namespace": "datasette.utils.escape_css_string", "completion": "    return _css_re.sub(lambda m: \"\\\\{:06x}\".format(ord(m.group(0))), s.replace(\"\\r\\n\", \"\\n\"))", "idx": 331}
{"namespace": "datasette.utils.detect_fts", "completion": "    rows = conn.execute(detect_fts_sql(table)).fetchall()\n    if rows:\n        return rows[0][0]", "idx": 332}
{"namespace": "datasette.utils.is_url", "completion": "    if not isinstance(value, str):\n        return False\n    if not (value.startswith(\"http://\") or value.startswith(\"https://\")):\n        return False\n    if \" \" in value:\n        return False\n    return True", "idx": 333}
{"namespace": "datasette.utils.to_css_class", "completion": "    if css_class_re.match(s):\n        return s\n    else:\n        s = css_invalid_chars_re.sub(\"\", s)\n        return s + hashlib.md5(s.encode(\"utf-8\")).hexdigest()[:6]", "idx": 334}
{"namespace": "datasette.utils.escape_fts", "completion": "    if query.count('\"') % 2:\n        query += '\"'\n    terms = _escape_fts_re.findall(query)\n    terms = [term for term in terms if term.strip()]\n    terms = list(OrderedDict.fromkeys(terms))  # Deduplicate preserving order\n    return ' '.join('\"{}\"'.format(term.strip('\"')) for term in terms)", "idx": 335}
{"namespace": "datasette.utils.check_connection", "completion": "    try:\n        # Execute a query to retrieve the names of all tables in the database\n        tables = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()\n        \n        # For each table, execute a query to retrieve the table information\n        for table in tables:\n            conn.execute(f\"PRAGMA table_info({table[0]});\").fetchall()\n            \n    except sqlite3.OperationalError as e:\n        if 'no such table' in str(e):\n            raise sqlite3.OperationalError(\"Table does not exist in the database.\")\n        elif 'database is locked' in str(e):\n            raise sqlite3.OperationalError(\"Database is locked.\")\n        else:\n            raise sqlite3.OperationalError(\"An error occurred while checking the connection.\")\n    except Exception as e:\n        raise Exception(\"An unexpected error occurred while checking the connection.\")", "idx": 336}
{"namespace": "datasette.utils.parse_metadata", "completion": "    try:\n        # Try to parse as JSON\n        return json.loads(content)\n    except json.JSONDecodeError:\n        try:\n            # If JSON parsing fails, try to parse as YAML\n            return yaml.safe_load(content)\n        except yaml.YAMLError:\n            raise BadMetadataError(\"Metadata is not valid JSON or YAML\")", "idx": 337}
{"namespace": "datasette.utils.call_with_supported_arguments", "completion": "    call_with = _gather_arguments(fn, kwargs)\n    return fn(*call_with)", "idx": 338}
{"namespace": "datasette.utils.derive_named_parameters", "completion": "    # Construct the \"explain\" statement\n    explain_sql = \"EXPLAIN \" + sql.rstrip(\";\")\n\n    # Find all possible named parameters in the query\n    possible_named_parameters = set(_re_named_parameter.findall(sql))\n\n    # Execute the \"explain\" statement with a dictionary of named parameters\n    named_parameters = {param: None for param in possible_named_parameters}\n    try:\n        explain_results = await db.execute(explain_sql, named_parameters)\n    except Exception:\n        # If there is an error executing the \"explain\" statement, return the list of possible named parameters\n        return list(possible_named_parameters)\n\n    # Identify the named parameters in the \"explain\" results\n    named_parameters_in_explain = set()\n    for row in explain_results:\n        if row[\"opcode\"] == \"Variable\":\n            named_parameters_in_explain.add(row[\"p4\"].lstrip(\":\"))\n\n    return list(named_parameters_in_explain)", "idx": 339}
{"namespace": "pyramid.path.Resolver.get_package_name", "completion": "        if self.package is CALLER_PACKAGE:\n            package_name = caller_package().__name__\n        else:\n            package_name = getattr(self.package, '__name__', None)\n        return package_name", "idx": 340}
{"namespace": "pyramid.path.Resolver.get_package", "completion": "        if self.package is CALLER_PACKAGE:\n            return caller_package()\n        else:\n            return self.package", "idx": 341}
{"namespace": "pyramid.path.DottedNameResolver.resolve", "completion": "        if not isinstance(dotted, str):\n            raise ValueError('dotted name must be a string')\n\n        package = self.package\n        if package is CALLER_PACKAGE:\n            package = caller_package()\n\n        if ':' in dotted or '.' in dotted:\n            return self._resolve(dotted, package)\n        else:\n            raise ValueError('dotted name must be in \"pkg_resources\" or \"zope.dottedname\" style')", "idx": 342}
{"namespace": "pyramid.path.DottedNameResolver.maybe_resolve", "completion": "        if isinstance(dotted, str):\n            return self.resolve(dotted)\n        return dotted", "idx": 343}
{"namespace": "pyramid.path.PkgResourcesAssetDescriptor.abspath", "completion": "        return self.pkg_resources.resource_filename(self.pkg_name, self.path)", "idx": 344}
{"namespace": "pyramid.renderers.render_to_response", "completion": "            from pyramid.response import Response\n    try:\n        registry = request.registry\n    except AttributeError:\n        registry = None\n    if package is None:\n        package = caller_package()\n    helper = RendererHelper(\n        name=renderer_name, package=package, registry=registry\n    )\n\n    with hide_attrs(request, 'response'):\n        result = helper.render(value, None, request=request)\n\n    if response is None:\n        from pyramid.response import Response\n        response = Response()\n\n    response.body = result\n    return response", "idx": 345}
{"namespace": "pyramid.renderers.JSON.add_adapter", "completion": "        self.components.registerAdapter(adapter, (type_or_iface,), IJSONAdapter)", "idx": 346}
{"namespace": "pyramid.renderers.RendererHelper.settings", "completion": "        return getattr(self.registry, 'settings', {})", "idx": 347}
{"namespace": "pyramid.renderers.RendererHelper.render_view", "completion": "        system = {\n            'view': view,\n            'renderer_name': self.name,\n            'renderer_info': self,\n            'context': context,\n            'request': request,\n            'req': request,\n            'get_csrf_token': partial(get_csrf_token, request),\n        }\n        return self.render(response, system, request=request)", "idx": 348}
{"namespace": "pyramid.renderers.RendererHelper.render", "completion": "        system = system_values.copy()\n        system.update(\n            {\n                'view': None,\n                'renderer_name': self.name,  # b/c\n                'renderer_info': self,\n                'context': None,\n                'request': request,\n                'req': request,\n                'get_csrf_token': partial(get_csrf_token, request),\n            }\n        )\n        self.registry.notify(BeforeRender(system))\n        result = self.renderer(value, system)\n        return result", "idx": 349}
{"namespace": "pyramid.renderers.RendererHelper.render_to_response", "completion": "        result = self.render(value, system_values, request=request)\n        response = self._make_response(result, request)\n        return response", "idx": 350}
{"namespace": "pyramid.renderers.RendererHelper.clone", "completion": "        name = name if name is not None else self.name\n        package = package if package is not None else self.package\n        registry = registry if registry is not None else self.registry\n        return RendererHelper(name=name, package=package, registry=registry)", "idx": 351}
{"namespace": "pyramid.urldispatch.RoutesMapper.get_routes", "completion": "        if include_static:\n            return self.routelist + self.static_routes\n        else:\n            return self.routelist", "idx": 352}
{"namespace": "pyramid.urldispatch.RoutesMapper.connect", "completion": "        route = Route(name, pattern, factory, predicates, pregenerator)\n        self.routes[name] = route\n        if static:\n            self.static_routes.append(route)\n        else:\n            self.routelist.append(route)\n        return route", "idx": 353}
{"namespace": "pyramid.testing.DummyTemplateRenderer.assert_", "completion": "        for k, v in kw.items():\n            if k not in self._received:\n                raise AssertionError(f\"Key {k} was not received by the renderer\")\n            if self._received[k] != v:\n                raise AssertionError(f\"Value of {k} received by the renderer ({self._received[k]}) does not match the asserted value ({v})\")\n        return True", "idx": 354}
{"namespace": "pyramid.testing.DummyResource.__getitem__", "completion": "        return self.subs[name]", "idx": 355}
{"namespace": "pyramid.testing.DummyResource.clone", "completion": "        if __name__ is _marker:\n            __name__ = self.__name__\n        if __parent__ is _marker:\n            __parent__ = self.__parent__\n        kw = dict(self.kw, **kw)\n        resource = self.__class__(__name__, __parent__, **kw)\n        resource.subs = self.subs.copy()\n        return resource", "idx": 356}
{"namespace": "pyramid.testing.DummySession.get_csrf_token", "completion": "        token = self.get('_csrft_', None)\n        if token is None:\n            token = self.new_csrf_token()\n        return token", "idx": 357}
{"namespace": "pyramid.testing.DummyRequest.response", "completion": "        return self.response_factory(self)", "idx": 358}
{"namespace": "pyramid.testing.DummyRendererFactory.add", "completion": "        self.renderers[spec] = renderer", "idx": 359}
{"namespace": "pyramid.authorization.ACLAuthorizationPolicy.principals_allowed_by_permission", "completion": "        allowed = set()\n\n        for location in reversed(list(lineage(context))):\n            try:\n                acl = location.__acl__\n            except AttributeError:\n                continue\n\n            allowed_here = set()\n            denied_here = set()\n\n            if acl and callable(acl):\n                acl = acl()\n\n            for ace_action, ace_principal, ace_permissions in acl:\n                if not is_nonstr_iter(ace_permissions):\n                    ace_permissions = [ace_permissions]\n                if (ace_action == Allow) and (permission in ace_permissions):\n                    if ace_principal not in denied_here:\n                        allowed_here.add(ace_principal)\n                if (ace_action == Deny) and (permission in ace_permissions):\n                    denied_here.add(ace_principal)\n                    if ace_principal == Everyone:\n                        allowed = set()\n                        break\n                    elif ace_principal in allowed:\n                        allowed.remove(ace_principal)\n\n            allowed.update(allowed_here)\n\n        return allowed", "idx": 360}
{"namespace": "pyramid.url.URLMethodsMixin.route_url", "completion": "        try:\n            reg = self.registry\n        except AttributeError:\n            reg = get_current_registry()  # b/c\n\n        mapper = reg.queryUtility(IRoutesMapper)\n        if mapper is None:\n            raise URLGenerationError('No routes mapper is registered')\n        route = mapper.get_route(route_name)\n        if route is None:\n            raise KeyError('No such route named %s' % route_name)\n\n        app_url, qs, anchor = parse_url_overrides(self, kw)\n\n        path = route.generate(kw)  # raises KeyError if generate fails\n\n        prefix = route.prefix\n\n        if prefix:\n            # we only want SCRIPT_NAME to be added when a prefix is used\n            app_url = app_url.rstrip('/') + '/' + prefix\n\n        url = app_url + path\n\n        if elements:\n            suffix = _join_elements(elements)\n            url += suffix\n\n        return url + qs + anchor", "idx": 361}
{"namespace": "pyramid.predicates.CustomPredicate.text", "completion": "        if hasattr(self.func, '__text__'):\n            return self.func.__text__\n        else:\n            return 'custom predicate: %s' % self.func.__doc__ or self.func.__name__", "idx": 362}
{"namespace": "pyramid.threadlocal.ThreadLocalManager.pop", "completion": "        try:\n            return self.stack.pop()\n        except IndexError:\n            return self.default", "idx": 363}
{"namespace": "pyramid.threadlocal.ThreadLocalManager.get", "completion": "        if self.stack:\n            return self.stack[-1]\n        else:\n            return self.default()", "idx": 364}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.authenticated_userid", "completion": "        identity = self._get_identity(request)\n        if identity is None:\n            return None\n        userid = identity['repoze.who.userid']\n        if self.callback is None:\n            return userid\n        if self.callback(identity, request) is not None:\n            return userid\n        return None", "idx": 365}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.unauthenticated_userid", "completion": "        identity = self._get_identity(request)\n        if identity is None:\n            return None\n        return identity.get('repoze.who.userid')", "idx": 366}
{"namespace": "pyramid.authentication.RepozeWho1AuthenticationPolicy.forget", "completion": "        identifier = self._get_identifier(request)\n        if identifier is None:\n            return []\n        environ = request.environ\n        return identifier.forget(environ)", "idx": 367}
{"namespace": "pyramid.authentication.AuthTktAuthenticationPolicy.unauthenticated_userid", "completion": "        identity = self.helper.identify(request)\n        if identity is None:\n            return None\n        return identity['userid']", "idx": 368}
{"namespace": "pyramid.authentication.SessionAuthenticationHelper.remember", "completion": "        request.session[self.userid_key] = userid\n        return []", "idx": 369}
{"namespace": "pyramid.authentication.SessionAuthenticationHelper.forget", "completion": "        if self.userid_key in request.session:\n            del request.session[self.userid_key]\n        return []", "idx": 370}
{"namespace": "pyramid.authentication.BasicAuthAuthenticationPolicy.unauthenticated_userid", "completion": "        credentials = extract_http_basic_credentials(request)\n        if credentials:\n            return credentials.username\n        return None", "idx": 371}
{"namespace": "pyramid.request.CallbackMethodsMixin._process_response_callbacks", "completion": "        callbacks = self.response_callbacks\n        while callbacks:\n            callback = callbacks.popleft()\n            callback(self, response)", "idx": 372}
{"namespace": "pyramid.request.CallbackMethodsMixin._process_finished_callbacks", "completion": "        callbacks = self.finished_callbacks\n        while callbacks:\n            callback = callbacks.popleft()\n            callback(self)", "idx": 373}
{"namespace": "pyramid.request.Request.session", "completion": "        session_factory = self.registry.queryUtility(ISessionFactory)\n        if session_factory is None:\n            raise ConfigurationError('No session factory registered '\n                                     '(see the Sessions chapter of the Pyramid '\n                                     'documentation)')\n        return session_factory(self)", "idx": 374}
{"namespace": "pyramid.request.RequestLocalCache.get_or_create", "completion": "        value = self.get(request)\n        if value is self.NO_VALUE:\n            if creator is None:\n                creator = self._creator\n            value = creator(request)\n            self.set(request, value)\n        return value", "idx": 375}
{"namespace": "pyramid.request.RequestLocalCache.set", "completion": "        if request not in self._store:\n            request.add_finished_callback(self.clear)\n        self._store[request] = value", "idx": 376}
{"namespace": "pyramid.i18n.Localizer.pluralize", "completion": "        if self.pluralizer is None:\n            self.pluralizer = Pluralizer(self.translations)\n        return self.pluralizer(singular, plural, n, domain=domain, mapping=mapping)", "idx": 377}
{"namespace": "pyramid.i18n.Translations.load", "completion": "        if locales is None:\n            locales = []\n        if isinstance(locales, str):\n            locales = [locales]\n        for locale in locales:\n            if isinstance(locale, str):\n                locale = Locale.parse(locale)\n            for localedir in os.listdir(dirname):\n                if localedir.startswith('.'):\n                    continue\n                mofile = os.path.join(dirname, localedir, 'LC_MESSAGES', domain + '.mo')\n                if os.path.exists(mofile):\n                    with open(mofile, 'rb') as fp:\n                        return cls(fp, domain)\n        return gettext.NullTranslations()", "idx": 378}
{"namespace": "pyramid.i18n.Translations.add", "completion": "        if translations.domain not in self._domains:\n            self._domains[translations.domain] = translations\n        elif merge:\n            self._domains[translations.domain].merge(translations)\n        return self", "idx": 379}
{"namespace": "pyramid.i18n.Translations.merge", "completion": "        self._catalog.update(translations._catalog)\n        self.files.extend(translations.files)\n        return self", "idx": 380}
{"namespace": "pyramid.i18n.LocalizerRequestMixin.locale_name", "completion": "        return negotiate_locale_name(self)", "idx": 381}
{"namespace": "pyramid.csrf.LegacySessionCSRFStoragePolicy.check_csrf_token", "completion": "        expected_token = self.get_csrf_token(request)\n        return not strings_differ(\n            bytes_(expected_token), bytes_(supplied_token)\n        )", "idx": 382}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.new_csrf_token", "completion": "        token = self._token_factory()\n        request.session[self.key] = token\n        return token", "idx": 383}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.get_csrf_token", "completion": "        if self.key in request.session:\n            return request.session[self.key]\n        else:\n            return self.new_csrf_token(request)", "idx": 384}
{"namespace": "pyramid.csrf.SessionCSRFStoragePolicy.check_csrf_token", "completion": "        expected_token = self.get_csrf_token(request)\n        return not strings_differ(\n            bytes_(expected_token), bytes_(supplied_token)\n        )", "idx": 385}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.new_csrf_token", "completion": "        token = self._token_factory()\n        def set_cookie_callback(request, response):\n            self.cookie_profile.set_cookies(response, token)\n        request.add_response_callback(set_cookie_callback)\n        return token", "idx": 386}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.get_csrf_token", "completion": "        token = request.cookies.get(self.cookie_name, None)\n        if not token:\n            token = self.new_csrf_token(request)\n        return token", "idx": 387}
{"namespace": "pyramid.csrf.CookieCSRFStoragePolicy.check_csrf_token", "completion": "        expected_token = self.get_csrf_token(request)\n        return not strings_differ(\n            bytes_(expected_token), bytes_(supplied_token)\n        )", "idx": 388}
{"namespace": "pyramid.security.PermitsResult.__repr__", "completion": "        return \"<%s instance at %s with msg %s>\" % (\n            self.__class__.__name__,\n            id(self),\n            self.msg,\n        )", "idx": 389}
{"namespace": "pyramid.util.InstancePropertyHelper.make_property", "completion": "        if name is None:\n            if hasattr(callable, '__name__'):\n                name = callable.__name__\n            else:\n                raise ValueError('Callable has no __name__ attribute, please specify a name for the property')\n\n        if reify:\n            prop = reify(callable)\n        else:\n            prop = property(callable)\n\n        return name, prop", "idx": 390}
{"namespace": "pyramid.util.InstancePropertyHelper.set_property", "completion": "        name, fn = cls.make_property(callable, name=name, reify=reify)\n        cls.apply_properties(target, {name: fn})", "idx": 391}
{"namespace": "pyramid.util.InstancePropertyHelper.add_property", "completion": "        prop = self.make_property(callable, name=name, reify=reify)\n        self.properties[prop[0]] = prop[1]", "idx": 392}
{"namespace": "pyramid.util.InstancePropertyHelper.apply", "completion": "        self.apply_properties(target, self.properties.items())", "idx": 393}
{"namespace": "pyramid.util.InstancePropertyMixin.set_property", "completion": "        prop = self.make_property(callable, name=name, reify=reify)\n        self.apply_properties(self, [prop])", "idx": 394}
{"namespace": "pyramid.util.TopologicalSorter.remove", "completion": "        self.names.remove(name)\n        self.name2val.pop(name, None)\n        self.name2before.pop(name, None)\n        self.name2after.pop(name, None)\n        self.req_before.discard(name)\n        self.req_after.discard(name)\n        self.order = [(a, b) for a, b in self.order if a != name and b != name]```", "idx": 395}
{"namespace": "pyramid.util.TopologicalSorter.add", "completion": "        if name in self.names:\n            self.remove(name)\n        self.names.append(name)\n        self.name2val[name] = val\n        if after is self.first:\n            after = None\n        if before is self.last:\n            before = None\n        if after is not None:\n            if not isinstance(after, (list, tuple)):\n                after = (after,)\n            self.name2after[name] = after\n            self.req_after.add(name)\n            for a in after:\n                self.order.append((a, name))\n        if before is not None:\n            if not isinstance(before, (list, tuple)):\n                before = (before,)\n            self.name2before[name] = before\n            self.req_before.add(name)\n            for b in before:\n                self.order.append((name, b))\n        if after is None and before is None:\n            if self.default_before is not None:\n                self.order.append((name, self.default_before))\n            if self.default_after is not None:\n                self.order.append((self.default_after, name))", "idx": 396}
{"namespace": "pyramid.traversal.find_resource", "completion": "    if isinstance(path, str):\n        path = traversal_path(path)\n    elif not is_nonstr_iter(path):\n        raise TypeError('path must be a string or a non-string iterable')\n\n    if path[:1] == ('',):\n        resource = find_root(resource)\n        path = path[1:]\n\n    for name in path:\n        try:\n            getitem = resource.__getitem__\n        except AttributeError:\n            raise KeyError(name)\n        try:\n            resource = getitem(name)\n        except KeyError:\n            raise KeyError(name)\n\n    return resource", "idx": 397}
{"namespace": "pyramid.static.ManifestCacheBuster.manifest", "completion": "        if self.reload:\n            if self.exists(self.manifest_path):\n                mtime = self.getmtime(self.manifest_path)\n                if self._mtime is None or mtime > self._mtime:\n                    self._mtime = mtime\n                    self._manifest = self.get_manifest()\n            else:\n                self._manifest = {}\n        return self._manifest", "idx": 398}
{"namespace": "pyramid.registry.Registry.registerSubscriptionAdapter", "completion": "        result = Components.registerSubscriptionAdapter(self, *arg, **kw)\n        self.has_listeners = True\n        return result", "idx": 399}
{"namespace": "pyramid.registry.Registry.registerHandler", "completion": "        result = Components.registerHandler(self, *arg, **kw)\n        self.has_listeners = True\n        return result", "idx": 400}
{"namespace": "pyramid.registry.Registry.notify", "completion": "        if not self.has_listeners:\n            return\n        for event in events:\n            self.subscribers((event,), None)", "idx": 401}
{"namespace": "pyramid.registry.Introspector.add", "completion": "        category_name = intr.category_name\n        discriminator = intr.discriminator\n        category = self._categories.setdefault(category_name, {})\n        if discriminator in category:\n            raise ValueError('discriminator conflict')\n        intr.order = self._counter\n        self._counter += 1\n        category[discriminator] = intr\n        category[intr.discriminator_hash] = intr", "idx": 402}
{"namespace": "pyramid.registry.Introspector.get", "completion": "        category = self._categories.get(category_name, {})\n        return category.get(discriminator, default)", "idx": 403}
{"namespace": "pyramid.registry.Introspector.get_category", "completion": "        category = self._categories.get(category_name, default)\n        if category is default:\n            return default\n        if sort_key is None:\n            sort_key = operator.attrgetter('order')\n        introspectables = sorted(category.values(), key=sort_key)\n        result = []\n        for introspectable in introspectables:\n            related = self.related(introspectable)\n            result.append({'introspectable': introspectable, 'related': related})\n        return result", "idx": 404}
{"namespace": "pyramid.registry.Introspector.categorized", "completion": "        if sort_key is None:\n            sort_key = operator.attrgetter('order')\n        categories = []\n        for category_name in self._categories.keys():\n            category = self.get_category(category_name, sort_key=sort_key)\n            categories.append((category_name, category))\n        return categories", "idx": 405}
{"namespace": "pyramid.registry.Introspector.remove", "completion": "        category = self._categories.get(category_name, {})\n        intr = category.get(discriminator)\n        if intr is not None:\n            # Remove all references to the introspection object\n            for refs in self._refs.values():\n                if intr in refs:\n                    refs.remove(intr)\n            # Delete the introspection object from the category dictionary\n            del category[discriminator]\n            if not category:\n                del self._categories[category_name]", "idx": 406}
{"namespace": "pyramid.registry.Introspector.relate", "completion": "        introspectables = self._get_intrs_by_pairs(pairs)\n        relatable = ((x, y) for x in introspectables for y in introspectables)\n        for x, y in relatable:\n            L = self._refs.setdefault(x, [])\n            if y not in L:\n                L.append(y)", "idx": 407}
{"namespace": "pyramid.registry.Introspector.related", "completion": "        category_name = intr.category_name\n        discriminator = intr.discriminator\n        intr = self._categories.get(category_name, {}).get(discriminator)\n        if intr is None:\n            raise KeyError((category_name, discriminator))\n        return self._refs.get(intr, [])", "idx": 408}
{"namespace": "pyramid.registry.Introspectable.discriminator_hash", "completion": "        self._assert_resolved()\n        return hash(self.discriminator)", "idx": 409}
{"namespace": "pyramid.registry.Introspectable.__repr__", "completion": "        return '<%s category %r, discriminator %r>' % (\n            self.__class__.__name__,\n            self.category_name,\n            self.discriminator,\n        )", "idx": 410}
{"namespace": "pyramid.scripts.proutes.PRoutesCommand._get_mapper", "completion": "        return registry.queryUtility(IRoutesMapper)", "idx": 411}
{"namespace": "pyramid.scripts.pshell.PShellCommand.make_shell", "completion": "        shells = self.find_all_shells()\n        if self.args.python_shell:\n            if self.args.python_shell in shells:\n                return shells[self.args.python_shell]\n            else:\n                raise ValueError('could not find a shell named \"%s\"' % self.args.python_shell)\n        else:\n            for shell in self.preferred_shells:\n                if shell in shells:\n                    return shells[shell]\n            for shell in shells:\n                if shell != 'python':\n                    return shells[shell]\n            return self.default_runner", "idx": 412}
{"namespace": "pyramid.config.assets.PackageOverrides.insert", "completion": "        if path == '' or path.endswith('/'):\n            override = DirectoryOverride(path, source)\n        else:\n            override = FileOverride(path, source)\n        self.overrides.insert(0, override)\n        return override", "idx": 413}
{"namespace": "pyramid.config.assets.PackageOverrides.filtered_sources", "completion": "        for override in self.overrides:\n            result = override(resource_name)\n            if result is not None:\n                yield result", "idx": 414}
{"namespace": "pyramid.config.assets.PackageOverrides.real_loader", "completion": "        if self._real_loader is None:\n            raise NotImplementedError(\n                \"This PackageOverrides instance does not have a real loader set.\"\n            )\n        return self._real_loader", "idx": 415}
{"namespace": "pyramid.config.views.MultiView.add", "completion": "        if phash is not None:\n            for views in (self.views, *self.media_views.values()):\n                for idx, (old_order, old_view, old_phash) in enumerate(views):\n                    if old_phash == phash:\n                        views[idx] = (order, view, phash)\n                        return\n\n        if accept is None:\n            self.views.append((order, view, phash))\n            self.views.sort()\n        else:\n            if accept not in self.media_views:\n                self.media_views[accept] = []\n                self.accepts.append(accept)\n                if accept_order is not None:\n                    self.accepts.sort(key=accept_order)\n                else:\n                    self.accepts.sort()\n            self.media_views[accept].append((order, view, phash))\n            self.media_views[accept].sort()", "idx": 416}
{"namespace": "pyramid.config.views.MultiView.get_views", "completion": "        if request.accept and self.accepts:\n            views = []\n            for offer in request.accept.offers():\n                if offer in self.media_views:\n                    views.extend(self.media_views[offer])\n            views.extend(self.views)\n            return views\n        else:\n            return self.views", "idx": 417}
{"namespace": "pyramid.config.views.MultiView.match", "completion": "        views = self.get_views(request)\n        for order, view, phash in views:\n            if not hasattr(view, '__predicated__') or view.__predicated__(context, request):\n                return view\n        raise PredicateMismatch(self.name)", "idx": 418}
{"namespace": "pyramid.config.views.MultiView.__permitted__", "completion": "        view = self.match(context, request)\n        permitted = getattr(view, '__permitted__', None)\n        if permitted is not None:\n            return permitted(context, request)\n        return True", "idx": 419}
{"namespace": "pyramid.config.views.MultiView.__call_permissive__", "completion": "        for order, view, phash in self.get_views(request):\n            if hasattr(view, '__call_permissive__'):\n                try:\n                    return view.__call_permissive__(context, request)\n                except PredicateMismatch:\n                    continue\n        raise PredicateMismatch(self.name)", "idx": 420}
{"namespace": "pyramid.config.actions.ActionState.processSpec", "completion": "        if spec not in self._seen_files:\n            self._seen_files.add(spec)\n            return True\n        else:\n            return False", "idx": 421}
{"namespace": "pyramid.config.actions.ActionState.action", "completion": "        action = {\n            'discriminator': discriminator,\n            'callable': callable,\n            'args': args,\n            'kw': kw or {},\n            'order': order,\n            'includepath': includepath,\n            'info': info,\n            'introspectables': introspectables,\n        }\n        action.update(extra)\n        self.actions.append(action)", "idx": 422}
{"namespace": "pyramid.config.actions.ActionInfo.__str__", "completion": "        if self.file is None:\n            return '<unknown>'\n        if self.src:\n            src = '\\n'.join(['  ' + x for x in self.src.split('\\n')])\n            return 'Line %s of file %s:\\n%s' % (self.line, self.file, src)\n        else:\n            return 'Line %s of file %s' % (self.line, self.file)", "idx": 423}
{"namespace": "pyramid.config.Configurator.__getattr__", "completion": "        directives = getattr(self.registry, '_directives', None)\n        if directives is not None:\n            directive = directives.get(name)\n            if directive is not None:\n                c, action_wrap = directive\n                if action_wrap:\n                    c = action_method(c)\n                return types.MethodType(c, self)\n        raise AttributeError(name)", "idx": 424}
{"namespace": "pyramid.config.Configurator.with_package", "completion": "        package = self.maybe_dotted(package)\n        configurator = self.__class__(\n            registry=self.registry,\n            package=package,\n            root_package=self.root_package,\n            autocommit=self.autocommit,\n            route_prefix=self.route_prefix,\n            introspection=self.introspection,\n        )\n        return configurator", "idx": 425}
{"namespace": "pyramid.config.Configurator.absolute_asset_spec", "completion": "        if not isinstance(relative_spec, str):\n            return relative_spec\n        if ':' in relative_spec:\n            return relative_spec\n        return '%s:%s' % (self.package_name, relative_spec)", "idx": 426}
{"namespace": "pyramid.config.Configurator.begin", "completion": "        data = {'registry': self.registry, 'request': request}\n        self.manager.push(data)", "idx": 427}
{"namespace": "pyramid.config.Configurator.scan", "completion": "        if package is None:\n            package = caller_package()\n        package = self.maybe_dotted(package)\n        scanner = self.venusian.Scanner(config=self, **kw)\n        scanner.scan(package, categories=categories, onerror=onerror, ignore=ignore)", "idx": 428}
{"namespace": "pyramid.config.Configurator.make_wsgi_app", "completion": "        self.commit()\n        self.action_state.execute_actions()\n        self._set_root_factory()\n        self._set_authentication_policy()\n        self._set_authorization_policy()\n        self._set_security_policy()\n        self._set_locale_negotiator()\n        self._set_request_factory()\n        self._set_response_factory()\n        self._set_default_permission()\n        self._set_tweens()\n        self._set_exception_views()\n        self._set_default_view_mapper()\n        self._set_debug_logger()\n        self._set_request_extensions()\n        self._set_default_accept_view_order()\n        self._set_introspector()\n        self._set_settings_adapter()\n        self._set_default_view_predicates()\n        self._set_default_route_predicates()\n        self._set_default_view_derivers()\n        self._set_application_registry()\n        self._set_request_factory()\n        self._set_response_factory()\n        self._set_default_permission()\n        self._set_tweens()\n        self._set_exception_views()\n        self._set_default_view_mapper()\n        self._set_debug_logger()\n        self._set_request_extensions()\n        self._set_default_accept_view_order()\n        self._set_introspector()\n        self._set_settings_adapter()\n        self._set_default_view_predicates()\n        self._set_default_route_predicates()\n        self._set_default_view_derivers()\n        self._set_application_registry()\n        self.registry._init_zca()\n        app = Router(self.registry)\n        self.registry.notify(ApplicationCreated(app))\n        global_registries.add(self.registry)\n        return app", "idx": 429}
{"namespace": "aiohappybase._util.camel_case_to_pep8", "completion": "    return CAPITALS.sub(lambda m: '_' + m.group(0).lower(), name).lstrip('_')", "idx": 430}
{"namespace": "kinto.authorization._relative_object_uri", "completion": "    parts = object_uri.split('/')\n    for i in range(len(parts), 0, -2):\n        parent_uri = '/'.join(parts[:i])\n        parent_resource_name, _ = _resource_endpoint(parent_uri)\n        if parent_resource_name == resource_name:\n            return parent_uri\n    raise ValueError(f\"Cannot find {resource_name} in URI {object_uri}\")", "idx": 431}
{"namespace": "kinto.core.openapi.OpenAPI.expose_authentication_method", "completion": "        cls.security_definitions[method_name] = definition\n        if 'scopes' in definition:\n            cls.security_roles[method_name] = definition['scopes']", "idx": 432}
{"namespace": "kinto.core.openapi.OpenAPI.generate", "completion": "\n        # Create a base specification dictionary\n        base_spec = {\n            \"host\": self.request.host,\n            \"schemes\": [self.request.scheme],\n            \"securityDefinitions\": self.security_definitions,\n        }\n\n        # Call the generate method of the parent class\n        return super().generate(swagger=base_spec)", "idx": 433}
{"namespace": "kinto.core.testing.get_user_headers", "completion": "    import base64\n    credentials = f\"{user}:{password}\"\n    encoded_credentials = base64.b64encode(credentials.encode('ascii')).decode('ascii')\n    return {\"Authorization\": f\"Basic {encoded_credentials}\"}", "idx": 434}
{"namespace": "kinto.core.authorization.RouteFactory.fetch_shared_objects", "completion": "        if get_bound_permissions is not None:\n            bound_perms = get_bound_permissions(self._object_id_match, perm)\n        else:\n            bound_perms = [(self._object_id_match, perm)]\n        object_ids = self._get_accessible_objects(principals, bound_perms)\n        self.shared_ids = [self._extract_object_id(o) for o, _ in object_ids]\n        return self.shared_ids", "idx": 435}
{"namespace": "kinto.core.authorization.RouteFactory.get_permission_object_id", "completion": "        service = utils.current_service(request)\n        object_id = object_id or request.matchdict.get(\"id\")\n        if self.on_plural_endpoint and object_id is not None:\n            # In the case of a \"POST\" on a plural endpoint, if an \"id\" was\n            # specified, then the object is returned. The required permission\n            # is thus \"read\" on this object.\n            object_uri = service.path_pattern.format_map({\"id\": object_id})\n        else:\n            object_uri = request.path\n        return object_uri", "idx": 436}
{"namespace": "kinto.core.utils.recursive_update_dict", "completion": "    for key, value in changes.items():\n        if isinstance(value, dict):\n            root[key] = recursive_update_dict(root.get(key, {}), value, ignores)\n        elif value in ignores:\n            root.pop(key, None)\n        else:\n            root[key] = value\n    return root", "idx": 437}
{"namespace": "kinto.core.utils.native_value", "completion": "    try:\n        return json.loads(value)\n    except ValueError:\n        return value", "idx": 438}
{"namespace": "kinto.core.utils.dict_subset", "completion": "    subset = {}\n    for key in keys:\n        if '.' in key:\n            parts = key.split('.')\n            value = d\n            for part in parts:\n                if part in value:\n                    value = value[part]\n                else:\n                    break\n            else:\n                subset[key] = value\n        elif key in d:\n            subset[key] = d[key]\n    return subset", "idx": 439}
{"namespace": "kinto.core.utils.dict_merge", "completion": "    result = a.copy()\n    for key, value in b.items():\n        if key in result and isinstance(result[key], collections_abc.Mapping):\n            result[key] = dict_merge(result[key], value)\n        else:\n            result[key] = value\n    return result", "idx": 440}
{"namespace": "kinto.core.utils.find_nested_value", "completion": "    if path in d:\n        return d[path]\n    parts = path.split('.')\n    for i in range(len(parts), 0, -1):\n        root = '.'.join(parts[:i])\n        if root in d:\n            value = d[root]\n            if isinstance(value, dict):\n                subpath = '.'.join(parts[i:])\n                return find_nested_value(value, subpath, default)\n            else:\n                return default\n    return default", "idx": 441}
{"namespace": "kinto.core.utils.instance_uri_registry", "completion": "    # Create a dummy request object\n    dummy_request = Request.blank('/')\n    # Set the registry of the dummy request object to the given registry\n    dummy_request.registry = registry\n    # Return the URI for the given resource\n    return strip_uri_prefix(dummy_request.route_path(f\"{resource_name}-object\", **params))", "idx": 442}
{"namespace": "kinto.core.statsd.load_from_config", "completion": "    if statsd_module is None:\n        message = \"You must install the `statsd` package to use the 'statsd' metrics backend\"\n        raise ConfigurationError(message)\n\n    statsd_url = urlparse(config.get('statsd_url'))\n    host = statsd_url.hostname\n    port = statsd_url.port\n    prefix = config.get('statsd_prefix')\n\n    return Client(host, port, prefix)", "idx": 443}
{"namespace": "kinto.core.errors.http_error", "completion": "    if errno is None:\n        errno = ERRORS.UNDEFINED.value\n    if code is None:\n        code = httpexception.code\n    if error is None:\n        error = httpexception.title\n    if details is colander.drop:\n        details = None\n\n    response = httpexception\n    response.content_type = 'application/json'\n    response.json = {\n        'code': code,\n        'errno': errno,\n        'error': error,\n        'message': message,\n        'info': info,\n        'details': details\n    }\n    return response", "idx": 444}
{"namespace": "kinto.core.resource.schema.ResourceReponses.get_and_bind", "completion": "\n        # Get default schemas\n        responses = {**self.default_schemas}\n\n        # Get endpoint type responses\n        type_responses = getattr(self, f\"default_{endpoint_type}_schemas\", {})\n        responses.update(type_responses)\n\n        # Get method responses\n        method_responses = getattr(self, f\"default_{method.lower()}_schemas\", {})\n        responses.update(method_responses)\n\n        # Get endpoint method responses\n        endpoint_method_responses = getattr(self, f\"{endpoint_type}_{method.lower()}_schemas\", {})\n        responses.update(endpoint_method_responses)\n\n        # Clone and bind responses\n        for status, response in responses.items():\n            responses[status] = response.clone().bind(**kwargs)\n\n        return responses", "idx": 445}
{"namespace": "kinto.core.resource.Resource.timestamp", "completion": "        try:\n            return self.model.timestamp()\n        except storage_exceptions.UnicityError as e:\n            self._raise_conflict(e)\n        except Exception:\n            error_msg = \"Unable to fetch collection timestamp\"\n            logger.exception(error_msg)\n            raise HTTPServiceUnavailable(error_msg)", "idx": 446}
{"namespace": "kinto.core.resource.Resource.plural_post", "completion": "        new_id = self.object_id\n        if new_id:\n            # If the new object id is already used, just return existing object.\n            try:\n                existing = self._get_object_or_404(new_id)\n                self._raise_412_if_modified(existing)\n                return self.postprocess(existing)\n            except HTTPNotFound:\n                pass\n\n        new_object = self.request.validated[\"body\"].get(\"data\", {})\n        new_object[self.model.id_field] = new_id\n        new_object = self.process_object(new_object)\n        new_object = self.model.create_object(new_object)\n        self.request.response.status_code = 201\n        return self.postprocess(new_object, action=ACTIONS.CREATE)", "idx": 447}
{"namespace": "kinto.core.resource.Resource.get", "completion": "        self._raise_400_if_invalid_id(self.object_id)\n        obj = self._get_object_or_404(self.object_id)\n        self._raise_304_if_not_modified(obj)\n        self._raise_412_if_modified(obj)\n\n        partial_fields = self._extract_partial_fields()\n        if partial_fields:\n            obj = dict_subset(obj, partial_fields)\n\n        self._add_timestamp_header(self.request.response)\n        self._add_cache_header(self.request.response)\n\n        return self.postprocess(obj)", "idx": 448}
{"namespace": "kinto.core.resource.Resource.delete", "completion": "        self._raise_400_if_invalid_id(self.object_id)\n        obj = self._get_object_or_404(self.object_id)\n        self._raise_412_if_modified(obj)\n\n        deleted = self.model.delete_object(self.object_id)\n        timestamp = deleted[self.model.modified_field]\n        self._add_timestamp_header(self.request.response, timestamp=timestamp)\n\n        return self.postprocess(deleted, action=ACTIONS.DELETE, old=obj)", "idx": 449}
{"namespace": "kinto.core.permission.memory.Permission.add_principal_to_ace", "completion": "        permission_key = f\"permission:{object_id}:{permission}\"\n        object_permission_principals = self._store.get(permission_key, set())\n        object_permission_principals.add(principal)\n        self._store[permission_key] = object_permission_principals", "idx": 450}
{"namespace": "kinto.core.permission.memory.Permission.get_object_permission_principals", "completion": "        permission_key = f\"permission:{object_id}:{permission}\"\n        object_permission_principals = self._store.get(permission_key, set())\n        return object_permission_principals", "idx": 451}
{"namespace": "kinto.core.storage.generators.Generator.match", "completion": "        if self._regexp is None:\n            self._regexp = re.compile(self.regexp)\n        return self._regexp.match(object_id) is not None", "idx": 452}
{"namespace": "kinto.core.storage.postgresql.migrator.MigratorMixin.create_or_migrate_schema", "completion": "        current_version = self.get_installed_version()\n        if current_version is None:\n            self.create_schema(dry_run)\n        elif current_version < self.schema_version:\n            self.migrate_schema(current_version, dry_run)\n        else:\n            logger.info(f\"PostgreSQL {self.name} schema is up-to-date (version {self.schema_version}).\")", "idx": 453}
{"namespace": "kinto.core.views.batch.BatchPayloadSchema.deserialize", "completion": "        if cstruct is colander.null:\n            return colander.null\n\n        deserialized = super().deserialize(cstruct)\n\n        if deserialized is colander.null:\n            return colander.null\n\n        defaults = deserialized.get(\"defaults\", {})\n        requests = deserialized.get(\"requests\", [])\n\n        for request in requests:\n            for key, value in defaults.items():\n                if key not in request or request[key] is colander.null:\n                    request[key] = value\n\n        return deserialized", "idx": 454}
{"namespace": "kinto.plugins.accounts.utils.get_cached_reset_password", "completion": "    hmac_secret = registry.settings[\"userid_hmac_secret\"]\n    cache_key = utils.hmac_digest(hmac_secret, ACCOUNT_RESET_PASSWORD_CACHE_KEY.format(username))\n    cache = registry.cache\n    reset_password = cache.get(cache_key)\n    return reset_password", "idx": 455}
{"namespace": "kinto.plugins.accounts.utils.get_cached_validation_key", "completion": "    hmac_secret = registry.settings[\"userid_hmac_secret\"]\n    cache_key = utils.hmac_digest(hmac_secret, ACCOUNT_VALIDATION_CACHE_KEY.format(username))\n\n    cache = registry.cache\n    cache_result = cache.get(cache_key)\n    return cache_result", "idx": 456}
{"namespace": "kinto.plugins.accounts.views.validation.on_account_activated", "completion": "    settings = event.request.registry.settings\n    account_validation = settings.get(\"account_validation\", False)\n\n    if not account_validation:\n        return\n\n    for change in event.impacted_objects:\n        old = change[\"old\"]\n        new = change[\"new\"]\n\n        if old.get(\"validated\", False) or not new.get(\"validated\", False):\n            continue\n\n        user = new\n        Emailer(event.request, user).send_account_activated()", "idx": 457}
{"namespace": "kinto.plugins.openid.OpenIDConnectPolicy._verify_token", "completion": "        headers = {\"Authorization\": f\"Bearer {access_token}\"}\n        try:\n            response = requests.get(self.oid_config[\"userinfo_endpoint\"], headers=headers)\n            response.raise_for_status()\n        except requests.exceptions.RequestException as e:\n            self.logger.debug(f\"Token verification failed: {str(e)}\")\n            return None\n\n        try:\n            return response.json()\n        except ValueError:\n            self.logger.debug(\"Token verification failed: response parsing error\")\n            return None", "idx": 458}
{"namespace": "kinto.plugins.quotas.scripts.rebuild_quotas", "completion": "    for bucket in paginated(storage, resource_name=\"bucket\", sorting=[OLDEST_FIRST]):\n        bucket_id = bucket[\"id\"]\n        bucket_uri = f\"/buckets/{bucket_id}\"\n        bucket_record_count = 0\n        bucket_storage_size = record_size(bucket)\n        bucket_collection_count = 0\n        for collection in paginated(\n            storage, resource_name=\"collection\", parent_id=bucket_uri, sorting=[OLDEST_FIRST]\n        ):\n            collection_record_count, collection_storage_size = rebuild_quotas_collection(\n                storage, bucket_id, collection, dry_run=dry_run\n            )\n            bucket_record_count += collection_record_count\n            bucket_storage_size += collection_storage_size\n            bucket_collection_count += 1\n\n        logger.info(\n            f\"Bucket {bucket_id}. Final size: {bucket_record_count} records, {bucket_storage_size} bytes, {bucket_collection_count} collections.\"\n        )\n        new_quota_info = {\n            \"record_count\": bucket_record_count,\n            \"storage_size\": bucket_storage_size,\n            \"collection_count\": bucket_collection_count,\n        }\n        if not dry_run:\n            storage.update(\n                resource_name=\"quota\",\n                parent_id=bucket_uri,\n                object_id=COLLECTION_QUOTA_OBJECT_ID,\n                obj=new_quota_info,\n            )", "idx": 459}
{"namespace": "kinto.config.render_template", "completion": "    with codecs.open(template, \"r\", encoding=\"utf-8\") as f:\n        raw_template = f.read()\n\n    rendered_template = raw_template.format(**kwargs)\n\n    with codecs.open(destination, \"w\", encoding=\"utf-8\") as f:\n        f.write(rendered_template)", "idx": 460}
{"namespace": "trafilatura.sitemaps.SitemapObject.extract_sitemap_langlinks", "completion": "        # Extract all xhtml:link elements\n        for match in XHTML_REGEX.finditer(self.content):\n            # Extract hreflang and href attributes\n            hreflang = match.group(1)\n            href = HREFLANG_REGEX.search(match.group(0))\n            if href is not None:\n                href = href.group(1)\n            # Check if hreflang matches the target language\n            if hreflang == self.target_lang:\n                # Handle the link\n                self.handle_link(href)\n        LOGGER.debug('%s sitemaps and %s links with hreflang found for %s', len(self.sitemap_urls), len(self.urls), self.sitemap_url)", "idx": 461}
{"namespace": "trafilatura.sitemaps.SitemapObject.extract_sitemap_links", "completion": "        for match in (m[0] for m in islice(LINK_REGEX.finditer(self.content), MAX_LINKS)):\n            self.handle_link(match)\n        LOGGER.debug('%s sitemaps and %s links found for %s', len(self.sitemap_urls), len(self.urls), self.sitemap_url)", "idx": 462}
{"namespace": "trafilatura.sitemaps.SitemapObject.process", "completion": "        # check if the sitemap is plausible\n        if not is_plausible_sitemap(self.sitemap_url, self.content):\n            return\n        # try to extract links from a TXT file\n        if not SITEMAP_FORMAT.match(self.content):\n            for match in (m[0] for m in islice(DETECT_LINKS.finditer(self.content), MAX_LINKS)):\n                self.handle_link(match)\n        # if the content matches the format of an XML sitemap\n        else:\n            # if a target language is specified, extract language links from the sitemap\n            if self.target_lang is not None:\n                self.extract_sitemap_langlinks()\n            # extract the links from the sitemap\n            self.extract_sitemap_links()\n        # if there are sitemap URLs or URLs extracted from the sitemap, return\n        if self.sitemap_urls or self.urls:\n            return\n        # otherwise, extract the links from the sitemap\n        self.extract_sitemap_links()", "idx": 463}
{"namespace": "trafilatura.sitemaps.is_plausible_sitemap", "completion": "    # Check if the URL ends with .xml or .txt\n    if not url.endswith(('.xml', '.txt')):\n        LOGGER.warning('sitemap URL does not end with .xml or .txt: %s', url)\n        return False\n\n    # Check if the contents are None or empty\n    if contents is None or contents.strip() == '':\n        LOGGER.warning('sitemap contents are empty: %s', url)\n        return False\n\n    # Check if the contents match the expected XML sitemap format\n    if not SITEMAP_FORMAT.match(contents):\n        LOGGER.warning('sitemap contents do not match expected XML format: %s', url)\n        return False\n\n    return True", "idx": 464}
{"namespace": "trafilatura.sitemaps.extract_robots_sitemaps", "completion": "    sitemap_urls = []\n    if robotstxt is None:\n        return sitemap_urls\n    for line in robotstxt.split('\\n'):\n        line = line.split('#', 1)[0].strip()  # remove comments\n        if line.lower().startswith('sitemap:'):\n            sitemap_url = line.split(':', 1)[1].strip()\n            sitemap_url = fix_relative_urls(baseurl, sitemap_url)\n            sitemap_url = clean_url(sitemap_url)\n            if sitemap_url is not None:\n                sitemap_urls.append(sitemap_url)\n    return sitemap_urls", "idx": 465}
{"namespace": "trafilatura.feeds.handle_link_list", "completion": "    output_links = []\n    for link in sorted(set(linklist)):\n        link = fix_relative_urls(baseurl, link)\n        link = clean_url(link)\n        if link is None or validate_url(link)[0] is False:\n            continue\n        if BLACKLIST.search(link):\n            continue\n        output_links.append(link)\n    # filter links based on target language\n    if target_lang is not None:\n        output_links = [link for link in output_links if target_lang in link]\n    return output_links", "idx": 466}
{"namespace": "trafilatura.feeds.find_feed_urls", "completion": "    # extract domain name and base URL\n    domainname, baseurl = get_hostinfo(url)\n    # fetch webpage content\n    htmlstring = fetch_url(url)\n    # check if it is a feed or a web page\n    if is_feed(htmlstring):\n        # if it is a feed, extract feed links\n        feed_links = extract_links(htmlstring, domainname, baseurl, url, target_lang)\n    else:\n        # if it is a web page, determine the feed\n        feed_urls = determine_feed(htmlstring, baseurl, url)\n        # fetch feed content and extract feed links\n        for feed_url in feed_urls:\n            feed_content = fetch_url(feed_url)\n            feed_links = extract_links(feed_content, domainname, baseurl, url, target_lang)\n    # return the extracted feed links as a sorted list of unique links\n    return sorted(set(feed_links))", "idx": 467}
{"namespace": "playhouse.sqlite_changelog.ChangeLog.install", "completion": "\n        if create_table:\n            self.model.create_table()\n\n        for action in self._actions:\n            if drop:\n                self.db.execute_sql(self.drop_trigger_sql(model, action))\n\n            if action == 'INSERT' and insert:\n                self.db.execute_sql(self.trigger_sql(model, action, skip_fields))\n            elif action == 'UPDATE' and update:\n                self.db.execute_sql(self.trigger_sql(model, action, skip_fields))\n            elif action == 'DELETE' and delete:\n                self.db.execute_sql(self.trigger_sql(model, action, skip_fields))", "idx": 468}
{"namespace": "playhouse.kv.KeyValue.pop", "completion": "        with self._database.atomic():\n            try:\n                value = self[key]\n                del self[key]\n                return value\n            except KeyError:\n                if default is Sentinel:\n                    raise\n                return default", "idx": 469}
{"namespace": "playhouse.signals.Signal.connect", "completion": "        name = name or receiver.__name__\n        key = (name, sender)\n        if key in self._receivers:\n            raise ValueError('receiver named %s for sender=%s already exists.' %\n                             (name, sender or 'any'))\n\n        self._receivers.add(key)\n        self._receiver_list.append((name, receiver, sender))", "idx": 470}
{"namespace": "playhouse.signals.Signal.disconnect", "completion": "        name = name or receiver.__name__\n        key = (name, sender)\n        if key in self._receivers:\n            self._receivers.remove(key)\n            self._receiver_list = [(n, r, s) for n, r, s in self._receiver_list if not (n == name and r == receiver and s == sender)]\n        else:\n            raise ValueError('receiver named %s (for sender=%s) not connected' % (name, sender or 'any'))", "idx": 471}
{"namespace": "backtrader.trade.Trade.update", "completion": "        self.commission += commission\n        oldsize = self.size\n        self.size += size\n\n        self.justopened = not self.isopen\n        self.isopen = self.size != 0\n\n        self.barlen += 1\n\n        self.isclosed = not self.size\n\n        if self.isclosed:\n            self.barclose = len(self.data)\n            self.dtclose = self.data.datetime[0]\n\n        if abs(size) > abs(oldsize):\n            self.price = price\n        elif self.isclosed:\n            self.price = 0.0\n        else:\n            self.price = (oldsize * self.price + size * price) / self.size\n\n        self.value = self.size * self.price\n        self.pnl = self.value - value\n        self.pnlcomm = self.pnl - self.commission\n\n        self.status = self.Open if self.size else self.Closed\n\n        if self.historyon:\n            self.history.append(\n                TradeHistory(self.status, self.data.datetime[0], self.barlen,\n                             self.size, self.price, self.value, self.pnl,\n                             self.pnlcomm, self.data._tz)\n            )\n            self.history[-1].doupdate(order, size, price, commission)", "idx": 472}
{"namespace": "ydata_profiling.profile_report.ProfileReport.typeset", "completion": "        if self._typeset is None:\n            if self._type_schema is not None:\n                self._typeset = self.config.get_typeset(self._type_schema)\n            else:\n                self._typeset = self.config.get_typeset()\n        return self._typeset", "idx": 473}
{"namespace": "ydata_profiling.report.presentation.flavours.html.frequency_table.HTMLFrequencyTable.render", "completion": "\n        # Check if the content is a list of rows\n        if isinstance(self.content, list):\n            rows = \"\".join([self.row_to_html(row) for row in self.content])\n        else:\n            rows = self.row_to_html(self.content)\n\n        # Use a template to generate the HTML code for the frequency table\n        template = \"\"\"\n        <table>\n            <thead>\n                <tr>\n                    <th>Value</th>\n                    <th>Count</th>\n                    <th>Frequency</th>\n                </tr>\n            </thead>\n            <tbody>\n                {rows}\n            </tbody>\n        </table>\n        \"\"\"\n\n        return template.format(rows=rows)", "idx": 474}
{"namespace": "ydata_profiling.report.presentation.flavours.html.image.HTMLImage.render", "completion": "\n        # Load the template file\n        with open(\"diagram.html\", \"r\") as file:\n            template = file.read()\n\n        # Replace placeholders in the template with the image content\n        rendered_html = template.replace(\"{image_content}\", self.content)\n\n        return rendered_html", "idx": 475}
{"namespace": "ydata_profiling.model.summary_algorithms.histogram_compute", "completion": "\n    # Determine the number of bins\n    bins = min(config.n_bins, n_unique)\n\n    # If the number of bins exceeds the maximum allowed bins, reduce the number of bins to the maximum value\n    if bins > config.max_bins:\n        bins = config.max_bins\n\n    # Compute the histogram\n    hist, bin_edges = np.histogram(finite_values, bins=bins, weights=weights)\n\n    # Return the histogram statistics\n    return {name: {\"histogram\": hist.tolist(), \"bin_edges\": bin_edges.tolist()}}", "idx": 476}
{"namespace": "ydata_profiling.model.summarizer.BaseSummarizer.summarize", "completion": "\n        # Get the summary functions for the given data type\n        summary_funcs = self.summary_map.get(dtype, [])\n\n        # Initialize an empty dictionary to store the summary\n        summary = {}\n\n        # Apply each summary function to the series\n        for func in summary_funcs:\n            summary.update(func(series, config))\n\n        return summary", "idx": 477}
{"namespace": "ydata_profiling.model.pandas.discretize_pandas.Discretizer.discretize_dataframe", "completion": "        # Create a copy of the dataframe\n        discretized_dataframe = dataframe.copy()\n\n        # Get the numerical columns\n        numerical_columns = self._get_numerical_columns(discretized_dataframe)\n\n        # Discretize each numerical column\n        for column in numerical_columns:\n            discretized_dataframe[column] = self._discretize_column(discretized_dataframe[column])\n\n        # Reset index if required\n        if self.reset_index:\n            discretized_dataframe.reset_index(drop=True, inplace=True)\n\n        return discretized_dataframe", "idx": 478}
{"namespace": "ydata_profiling.model.pandas.correlations_pandas.pandas_cramers_compute", "completion": "\n    threshold = config.categorical_maximum_correlation_distinct\n    categorical_columns = [\n        key\n        for key, value in summary.items()\n        if value[\"type\"] in {\"Categorical\", \"Boolean\"}\n        and 1 < value[\"n_distinct\"] <= threshold\n    ]\n\n    if len(categorical_columns) <= 1:\n        return None\n\n    correlation_matrix = pd.DataFrame(\n        np.ones((len(categorical_columns), len(categorical_columns))),\n        index=categorical_columns,\n        columns=categorical_columns,\n    )\n\n    for col_1_name, col_2_name in itertools.combinations(categorical_columns, 2):\n        score = _pairwise_cramers(df[col_1_name], df[col_2_name])\n        (\n            correlation_matrix.loc[col_1_name, col_2_name],\n            correlation_matrix.loc[col_2_name, col_1_name],\n        ) = (score, score)\n\n    return correlation_matrix", "idx": 479}
{"namespace": "ydata_profiling.model.pandas.correlations_pandas.pandas_auto_compute", "completion": "\n    # Identify numerical and categorical columns\n    numericals = [key for key, value in summary.items() if value[\"type\"] == \"Numeric\"]\n    categoricals = [key for key, value in summary.items() if value[\"type\"] in {\"Categorical\", \"Boolean\"}]\n\n    # If there are no numerical or categorical columns, return None\n    if len(numericals) + len(categoricals) <= 1:\n        return None\n\n    # Discretize the DataFrame\n    df = df.copy()\n    for col in numericals:\n        df[col] = pd.qcut(df[col], q=config.n_bins, duplicates='drop')\n\n    # Initialize the correlation matrix\n    correlation_matrix = pd.DataFrame(index=df.columns, columns=df.columns)\n\n    # Calculate the correlation scores\n    for col1, col2 in itertools.combinations(df.columns, 2):\n        if col1 in numericals and col2 in numericals:\n            correlation_matrix.loc[col1, col2] = _pairwise_spearman(df[col1], df[col2])\n        else:\n            correlation_matrix.loc[col1, col2] = _pairwise_cramers(df[col1], df[col2])\n        correlation_matrix.loc[col2, col1] = correlation_matrix.loc[col1, col2]\n\n    return correlation_matrix", "idx": 480}
{"namespace": "ydata_profiling.controller.console.main", "completion": "\n    # Parse the arguments\n    args = parse_args(args)\n\n    # If output file is not specified, replace the input file's extension with .html\n    if args.output_file is None:\n        args.output_file = Path(args.input_file).with_suffix(\".html\")\n\n    # Generate the profiling report\n    report = ProfileReport(\n        df=args.input_file,\n        minimal=args.minimal,\n        explorative=args.explorative,\n        pool_size=args.pool_size,\n        title=args.title,\n        infer_dtypes=args.infer_dtypes,\n        config_file=args.config_file\n    )\n\n    # Save the report to the output file\n    report.to_file(args.output_file)\n\n    # If not in silent mode, open the report\n    if not args.silent:\n        report.to_browser()\n\n    return None", "idx": 481}
{"namespace": "ydata_profiling.utils.cache.cache_file", "completion": "\n    data_path = get_data_path()\n    data_path.mkdir(exist_ok=True)\n\n    file_path = data_path / file_name\n\n    # If not exists, download and create file\n    if not file_path.exists():\n        response = request.urlopen(url)\n        file_path.write_bytes(response.read())\n\n    return file_path", "idx": 482}
{"namespace": "ydata_profiling.utils.dataframe.expand_mixed", "completion": "    if types is None:\n        types = [list, dict, tuple]\n\n    for col in df.columns:\n        for t in types:\n            if isinstance(df[col].iloc[0], t):\n                if t == list or t == tuple:\n                    expanded = pd.DataFrame(df[col].to_list(), columns=[f\"{col}_{i}\" for i in range(len(df[col].iloc[0]))])\n                elif t == dict:\n                    expanded = pd.DataFrame(df[col].to_list()).add_prefix(f\"{col}_\")\n                df = df.drop(col, axis=1)\n                df = pd.concat([df, expanded], axis=1)\n                break\n    return df", "idx": 483}
{"namespace": "pysnooper.utils.ensure_tuple", "completion": "    if isinstance(x, collections_abc.Iterable) and not isinstance(x, str):\n        return tuple(x)\n    else:\n        return (x,)", "idx": 484}
{"namespace": "rq.serializers.resolve_serializer", "completion": "\n    if serializer is None:\n        return DefaultSerializer\n\n    if isinstance(serializer, str):\n        module_name, class_name = serializer.rsplit('.', 1)\n        module = __import__(module_name, fromlist=[class_name])\n        serializer = getattr(module, class_name)\n\n    if not hasattr(serializer, 'dumps') or not hasattr(serializer, 'loads'):\n        raise NotImplementedError('The serializer must implement the dumps and loads methods.')\n\n    return serializer", "idx": 485}
{"namespace": "lux.vis.Vis.Vis.get_attr_by_channel", "completion": "        return list(\n            filter(\n                lambda x: x.channel == channel and x.value == \"\" if hasattr(x, \"channel\") else False,\n                self._inferred_intent,\n            )\n        )", "idx": 486}
{"namespace": "lux.action.default.register_default_actions", "completion": "        from lux.action.generalize import generalize_actions\n        from lux.action.filter import filter_actions\n        from lux.action.enhance import enhance_actions\n        from lux.action.column_group import column_group_actions\n        from lux.action.row_group import row_group_actions\n        from lux.action.custom import custom_actions\n        from lux.action import default\n    from lux.action import default\n    from lux.action.custom import custom_actions\n    from lux.action.row_group import row_group_actions\n    from lux.action.column_group import column_group_actions\n    from lux.action.enhance import enhance_actions\n    from lux.action.filter import filter_actions\n    from lux.action.generalize import generalize_actions\n\n    default.register_default_display_condition()\n    custom_actions.register_default_display_condition()\n    row_group_actions.register_default_display_condition()\n    column_group_actions.register_default_display_condition()\n    enhance_actions.register_default_display_condition()\n    filter_actions.register_default_display_condition()\n    generalize_actions.register_default_display_condition()", "idx": 487}
{"namespace": "folium.utilities.get_bounds", "completion": "    lat_min, lat_max, lon_min, lon_max = None, None, None, None\n    for coord in iter_coords(locations):\n        if lonlat:\n            lon, lat = coord\n        else:\n            lat, lon = coord\n        lat_min = none_min(lat_min, lat)\n        lat_max = none_max(lat_max, lat)\n        lon_min = none_min(lon_min, lon)\n        lon_max = none_max(lon_max, lon)\n    return [[lat_min, lon_min], [lat_max, lon_max]]", "idx": 488}
{"namespace": "folium.features.VegaLite.vegalite_major_version", "completion": "        schema = self.data.get(\"$schema\", \"\")\n        version = schema.split(\"/\")[-1]\n        major_version = int(version.split(\".\")[0])\n        return major_version", "idx": 489}
{"namespace": "music_dl.utils.colorize", "completion": "    if platform.system() == \"Windows\":\n        return string\n    color_code = colors.get(color)\n    if color_code:\n        return color_code + string + \"\\033[0m\"\n    else:\n        return string", "idx": 490}
{"namespace": "music_dl.source.MusicSource.search", "completion": "        ret_songs_list = []\n        ret_errors = []\n        threads = []\n\n        for source in sources_list:\n            t = threading.Thread(target=self.search_thread, args=(source, keyword, ret_songs_list, ret_errors))\n            threads.append(t)\n            t.start()\n\n        for t in threads:\n            t.join()\n\n        # Sort and remove duplicates\n        ret_songs_list = sorted(ret_songs_list, key=lambda s: (s.title, s.singer, s.size))\n        ret_songs_list = list(k for k, _ in itertools.groupby(ret_songs_list))\n\n        if ret_errors:\n            click.echo(\"\\n\" + _(\"Errors occurred:\"))\n            for source, err in ret_errors:\n                click.echo(_(\"Error occurred in %s: %s\" % (source, err)))\n\n        return ret_songs_list", "idx": 491}
{"namespace": "jwt.utils.base64url_decode", "completion": "    if isinstance(input, str):\n        input = input.encode(\"utf-8\")\n    padding = b'=' * (4 - (len(input) % 4))\n    input += padding\n    return base64.urlsafe_b64decode(input)", "idx": 492}
{"namespace": "jwt.utils.to_base64url_uint", "completion": "    if not isinstance(val, int) or val < 0:\n        raise ValueError(\"Value must be a positive integer\")\n\n    byte_string = val.to_bytes((val.bit_length() + 7) // 8, byteorder=\"big\")\n\n    if byte_string == b\"\":\n        byte_string = b\"\\0\"\n\n    return base64url_encode(byte_string)", "idx": 493}
{"namespace": "jwt.algorithms.HMACAlgorithm.prepare_key", "completion": "        key = force_bytes(key)\n\n        if key.startswith(b\"-----BEGIN CERTIFICATE\") or key.startswith(b\"ssh-rsa\"):\n            raise InvalidKeyError(\n                \"The specified key is an asymmetric key or x509 certificate and \"\n                \"should not be used as an HMAC secret.\"\n            )\n\n        return key", "idx": 494}
{"namespace": "jwt.algorithms.HMACAlgorithm.to_jwk", "completion": "        key_bytes = force_bytes(key_obj)\n        jwk_dict = {\n            \"kty\": \"oct\",\n            \"k\": base64url_encode(key_bytes).decode(),\n        }\n        if as_dict:\n            return jwk_dict\n        else:\n            return json.dumps(jwk_dict)", "idx": 495}
{"namespace": "jwt.algorithms.HMACAlgorithm.from_jwk", "completion": "        try:\n            if isinstance(jwk, str):\n                obj = json.loads(jwk)\n            elif isinstance(jwk, dict):\n                obj = jwk\n            else:\n                raise ValueError\n        except ValueError:\n            raise InvalidKeyError(\"Key is not valid JSON\")\n\n        if obj.get(\"kty\") != \"oct\":\n            raise InvalidKeyError(\"Not an HMAC key\")\n\n        if \"k\" not in obj:\n            raise InvalidKeyError('HMAC key should have \"k\" parameter')\n\n        return base64url_decode(obj.get(\"k\"))", "idx": 496}
{"namespace": "sacred.arg_parser._convert_value", "completion": "    try:\n        return ast.literal_eval(value)\n    except (ValueError, SyntaxError):\n        return value", "idx": 497}
{"namespace": "sacred.utils.recursive_update", "completion": "    for k, v in u.items():\n        if isinstance(v, collections.abc.Mapping):\n            d[k] = recursive_update(d.get(k, {}), v)\n        else:\n            d[k] = v\n    return d", "idx": 498}
{"namespace": "sacred.utils.iterate_flattened_separately", "completion": "    if manually_sorted_keys is None:\n        manually_sorted_keys = []\n\n    for key in manually_sorted_keys:\n        if key in dictionary:\n            value = dictionary[key]\n            if is_non_empty_dict(value):\n                yield key, PATHCHANGE\n                for k, v in iterate_flattened_separately(value):\n                    yield join_paths(key, k), v\n            else:\n                yield key, value\n\n    for key in sorted(dictionary.keys()):\n        if key in manually_sorted_keys:\n            continue\n        value = dictionary[key]\n        if is_non_empty_dict(value):\n            yield key, PATHCHANGE\n            for k, v in iterate_flattened_separately(value):\n                yield join_paths(key, k), v\n        else:\n            yield key, value", "idx": 499}
{"namespace": "sacred.utils.iterate_flattened", "completion": "    for key, value in d.items():\n        if isinstance(value, dict):\n            for sub_key, sub_value in iterate_flattened(value):\n                yield f\"{key}.{sub_key}\", sub_value\n        else:\n            yield key, value", "idx": 500}
{"namespace": "sacred.utils.iter_prefixes", "completion": "    parts = path.split(\".\")\n    for i in range(1, len(parts) + 1):\n        yield \".\".join(parts[:i])", "idx": 501}
{"namespace": "sacred.utils.rel_path", "completion": "    assert path.startswith(base), f\"{base} not a prefix of {path}\"\n    return path[len(base) :].lstrip(\".\")", "idx": 502}
{"namespace": "sacred.utils.convert_to_nested_dict", "completion": "    nested_dict = {}\n    for key, value in dotted_dict.items():\n        parts = key.split('.')\n        d = nested_dict\n        for part in parts[:-1]:\n            if part not in d:\n                d[part] = {}\n            d = d[part]\n        d[parts[-1]] = value\n    return nested_dict", "idx": 503}
{"namespace": "sacred.utils.format_sacred_error", "completion": "    lines = []\n    if e.print_usage:\n        lines.append(short_usage)\n    if e.print_traceback:\n        lines.append(format_filtered_stacktrace(e.filter_traceback))\n    else:\n        lines.append(\"{}: {}\".format(type(e).__name__, str(e)))\n    return \"\\n\".join(lines)", "idx": 504}
{"namespace": "sacred.utils.get_package_version", "completion": "    try:\n        package = importlib.import_module(name)\n    except ImportError:\n        return None\n\n    version_string = getattr(package, \"__version__\", None)\n    if version_string is None:\n        return None\n\n    return parse_version(version_string)", "idx": 505}
{"namespace": "sacred.experiment.Experiment.main", "completion": "        captured = self.command(function)\n        self.default_command = captured.__name__\n        return captured", "idx": 506}
{"namespace": "sacred.experiment.Experiment.run", "completion": "        run = self._create_run(\n            command_name=command_name,\n            config_updates=config_updates,\n            named_configs=named_configs,\n            info=info,\n            meta_info=meta_info,\n            options=options,\n        )\n        run()\n        return run", "idx": 507}
{"namespace": "sacred.host_info.host_info_getter", "completion": "    if name is None:\n        name = func.__name__\n    if name in host_info_gatherers:\n        warnings.warn(f\"Overwriting existing host info getter: {name}\")\n    host_info_gatherers[name] = func\n    return func", "idx": 508}
{"namespace": "sacred.ingredient.Ingredient.command", "completion": "        if function is None:\n            return lambda f: self.command(f, prefix=prefix, unobserved=unobserved)\n\n        if function.__name__ in self.commands:\n            raise KeyError('Command \"{}\" already exists!'.format(function.__name__))\n\n        captured_function = self.capture(function, prefix=prefix)\n        self.commands[function.__name__] = (captured_function, unobserved)\n        return captured_function", "idx": 509}
{"namespace": "sacred.ingredient.Ingredient.config", "completion": "        config_scope = ConfigScope(function)\n        self.configurations.append(config_scope)\n        return config_scope", "idx": 510}
{"namespace": "sacred.ingredient.Ingredient.named_config", "completion": "        named_config = ConfigScope(func)\n        self.named_configs[func.__name__] = named_config\n        return named_config", "idx": 511}
{"namespace": "sacred.ingredient.Ingredient.gather_commands", "completion": "        for ingredient, _ in self.traverse_ingredients():\n            for cmd_name, cmd in ingredient.commands.items():\n                cmd_name = join_paths(ingredient.path, cmd_name)\n                cmd_name = self.post_process_name(cmd_name, ingredient)\n                yield cmd_name, cmd", "idx": 512}
{"namespace": "sacred.ingredient.Ingredient.gather_named_configs", "completion": "        for ingredient, _ in self.traverse_ingredients():\n            for config_name, config in ingredient.named_configs.items():\n                full_name = join_paths(ingredient.path, config_name)\n                full_name = self.post_process_name(full_name, ingredient)\n                yield full_name, config", "idx": 513}
{"namespace": "sacred.dependencies.Source.create", "completion": "\n        if not os.path.isfile(filename):\n            raise ValueError(f\"invalid filename or file not found {filename}\")\n\n        digest = get_digest(filename)\n        repo, commit, is_dirty = get_commit_if_possible(filename, save_git_info)\n\n        return Source(filename, digest, repo, commit, is_dirty)", "idx": 514}
{"namespace": "sacred.dependencies.Source.to_json", "completion": "        if base_dir:\n            filename = os.path.relpath(self.filename, base_dir)\n        else:\n            filename = self.filename\n        return {\"filename\": filename, \"digest\": self.digest}", "idx": 515}
{"namespace": "sacred.dependencies.PackageDependency.create", "completion": "        modname = mod.__name__.split(\".\")[0]\n        if modname not in cls.modname_to_dist:\n            for dist in pkg_resources.working_set:\n                if dist.has_metadata(\"top_level.txt\"):\n                    top_level = dist.get_metadata(\"top_level.txt\")\n                    if modname in top_level.split():\n                        cls.modname_to_dist[modname] = dist\n                        break\n            else:\n                cls.modname_to_dist[modname] = None\n        dist = cls.modname_to_dist[modname]\n        if dist is not None:\n            return cls(dist.project_name, dist.version)\n        else:\n            return cls(modname, None)", "idx": 516}
{"namespace": "sacred.dependencies.is_local_source", "completion": "    if not os.path.isfile(filename):\n        return False\n\n    filename = os.path.abspath(filename)\n    experiment_path = os.path.abspath(experiment_path)\n\n    if not filename.startswith(experiment_path):\n        return False\n\n    module_parts = convert_path_to_module_parts(Path(filename).relative_to(experiment_path))\n    return \".\".join(module_parts) == modname", "idx": 517}
{"namespace": "sacred.dependencies.gather_sources_and_dependencies", "completion": "            import numpy\n    experiment_path, main = get_main_file(globs, save_git_info)\n    base_path = base_dir or experiment_path\n\n    source_discovery_strategy = SETTINGS[\"DISCOVER_SOURCES\"]\n    dependency_discovery_strategy = SETTINGS[\"DISCOVER_DEPENDENCIES\"]\n\n    sources = source_discovery_strategies[source_discovery_strategy](\n        globs, base_path, save_git_info\n    )\n    dependencies = dependency_discovery_strategies[dependency_discovery_strategy](\n        globs, base_path\n    )\n\n    if main is not None:\n        sources.add(main)\n\n    try:\n        import numpy\n\n        dependencies.add(PackageDependency.create(numpy))\n    except ImportError:\n        pass\n\n    return main, sources, dependencies", "idx": 518}
{"namespace": "sacred.observers.file_storage.FileStorageObserver.resource_event", "completion": "        store_path = self.find_or_save(filename, self.resource_dir)\n        relative_resource = os.path.relpath(str(store_path), self.basedir)\n        self.run_entry[\"resources\"].append(relative_resource)\n        self.save_json(self.run_entry, \"run.json\")", "idx": 519}
{"namespace": "sacred.config.signature.Signature.get_free_parameters", "completion": "        expected_args = self._get_expected_args(bound)\n        num_args = len(args)\n        free_params = [param for param in expected_args[num_args:] if param not in kwargs]\n        return free_params", "idx": 520}
{"namespace": "sacred.config.signature.Signature.construct_arguments", "completion": "\n        expected_args = self._get_expected_args(bound)\n\n        self._assert_no_unexpected_args(expected_args, args)\n        self._assert_no_unexpected_kwargs(expected_args, kwargs)\n        self._assert_no_duplicate_args(expected_args, args, kwargs)\n\n        args, kwargs = self._fill_in_options(args, kwargs, options, bound)\n\n        self._assert_no_missing_args(args, kwargs, bound)\n\n        return args, kwargs", "idx": 521}
{"namespace": "sacred.config.config_files.load_config_file", "completion": "    handler = get_handler(filename)\n    with open(filename, \"r\" + handler.mode) as f:\n        return handler.load(f)", "idx": 522}
{"namespace": "sacred.config.custom_containers.DogmaticDict.get", "completion": "        if k in self:\n            return self[k]\n        else:\n            return self.fallback.get(k, d)", "idx": 523}
{"namespace": "sacred.config.custom_containers.DogmaticDict.revelation", "completion": "        missing_keys = set()\n        for key in self.fixed:\n            if key not in self:\n                missing_keys.add(key)\n                self[key] = self.fixed[key]\n            elif isinstance(self.fixed[key], DogmaticDict):\n                sub_missing_keys = self.fixed[key].revelation()\n                missing_keys.update({f'{key}.{subkey}' for subkey in sub_missing_keys})\n        return missing_keys", "idx": 524}
{"namespace": "sacred.config.custom_containers.make_read_only", "completion": "    if isinstance(o, dict):\n        return ReadOnlyDict({k: make_read_only(v) for k, v in o.items()})\n    elif isinstance(o, list):\n        return ReadOnlyList([make_read_only(v) for v in o])\n    elif isinstance(o, tuple):\n        return tuple(make_read_only(v) for v in o)\n    else:\n        return o", "idx": 525}
{"namespace": "sacred.config.config_scope.dedent_function_body", "completion": "    body_lines = body.splitlines(True)\n    indent = None\n    for line in body_lines:\n        if not is_empty_or_comment(line):\n            indent = re.match(r'\\s*', line).group()\n            break\n    if indent is None:\n        return body\n    return \"\".join(dedent_line(line, indent) for line in body_lines)", "idx": 526}
{"namespace": "boltons.funcutils.FunctionBuilder.get_sig_str", "completion": "            formatters = {}\n            if with_annotations:\n                formatters['formatannotation'] = self.annotations.get\n            sig = inspect_formatargspec(self.args,\n                                        self.varargs,\n                                        self.varkw,\n                                        [],\n                                        self.kwonlyargs,\n                                        self.kwonlydefaults,\n                                        self.annotations,\n                                        **formatters)\n            return sig", "idx": 527}
{"namespace": "boltons.funcutils.FunctionBuilder.get_invocation_str", "completion": "            if self._KWONLY_MARKER.search(self.get_sig_str(with_annotations=False)):\n                return inspect_formatargspec(self.args, self.varargs,\n                                             self.varkw, [],\n                                             self.kwonlyargs,\n                                             self.kwonlydefaults)[1:-1]\n            else:\n                return inspect_formatargspec(self.args, self.varargs,\n                                             self.varkw, [])[1:-1]", "idx": 528}
{"namespace": "boltons.funcutils.FunctionBuilder.from_func", "completion": "        fb = cls(func.__name__)\n        fb.argspec = cls._argspec_to_dict(func)\n        fb.doc = inspect.getdoc(func) or ''\n        fb.module = func.__module__\n        fb.dict = getattr(func, '__dict__', {})\n        fb.is_async = _inspect_iscoroutinefunction(func)\n\n        if isinstance(func, functools.partial):\n            fb.body = '_call(%s)' % fb.get_invocation_str()\n            fb.dict.update({'_call': func})\n        else:\n            fb.body = 'return _call(%s)' % fb.get_invocation_str()\n            fb.dict.update({'_call': func})\n\n        return fb", "idx": 529}
{"namespace": "boltons.funcutils.FunctionBuilder.get_defaults_dict", "completion": "        defaults = self.defaults or ()\n        kwonlydefaults = getattr(self, 'kwonlydefaults', None) or {}\n        args = self.args or ()\n        kwonlyargs = getattr(self, 'kwonlyargs', None) or ()\n        ret = dict(zip(args[-len(defaults):], defaults))\n        ret.update(kwonlydefaults)\n        return ret", "idx": 530}
{"namespace": "boltons.funcutils.FunctionBuilder.get_arg_names", "completion": "        if only_required:\n            if self.defaults:\n                required_args = self.args[:-len(self.defaults)]\n            else:\n                required_args = self.args[:]\n            if hasattr(self, 'kwonlydefaults'):\n                required_kwonlyargs = [arg for arg in self.kwonlyargs\n                                       if arg not in self.kwonlydefaults]\n            else:\n                required_kwonlyargs = []\n            return tuple(required_args + required_kwonlyargs)\n        else:\n            return tuple(self.args + getattr(self, 'kwonlyargs', []))", "idx": 531}
{"namespace": "boltons.ioutils.SpooledIOBase.writelines", "completion": "        self._checkClosed()\n        for line in lines:\n            self.write(line)", "idx": 532}
{"namespace": "boltons.ioutils.SpooledBytesIO.write", "completion": "        self._checkClosed()\n        if not isinstance(s, binary_type):\n            raise TypeError(\"bytes expected, got {0}\".format(type(s).__name__))\n        if self.buffer.tell() + len(s) >= self._max_size:\n            self.rollover()\n        self.buffer.write(s)", "idx": 533}
{"namespace": "boltons.ioutils.SpooledBytesIO.seek", "completion": "        self._checkClosed()\n        return self.buffer.seek(pos, mode)", "idx": 534}
{"namespace": "boltons.ioutils.SpooledBytesIO.len", "completion": "        pos = self.tell()\n        if self._rolled:\n            self.seek(0, os.SEEK_END)\n            length = self.tell()\n        else:\n            length = len(self.buffer.getvalue())\n        self.seek(pos)\n        return length", "idx": 535}
{"namespace": "boltons.ioutils.SpooledStringIO.read", "completion": "        self._checkClosed()\n        ret = self.buffer.read(n).decode('utf-8')\n        self._tell = self.tell() + len(ret)\n        return ret", "idx": 536}
{"namespace": "boltons.ioutils.SpooledStringIO.write", "completion": "        self._checkClosed()\n        if not isinstance(s, text_type):\n            raise TypeError(\"str expected, got {}\".format(\n                type(s).__name__\n            ))\n\n        if self.tell() + len(s) >= self._max_size:\n            self.rollover()\n        self.buffer.write(s.encode('utf-8'))\n        self._tell = self.tell() + len(s)", "idx": 537}
{"namespace": "boltons.ioutils.SpooledStringIO.seek", "completion": "        self._checkClosed()\n        if mode == os.SEEK_SET:\n            if pos < 0:\n                raise ValueError(\"Negative seek position {}\".format(pos))\n            self.buffer.seek(0)\n            self._tell = self._traverse_codepoints(0, pos)\n        elif mode == os.SEEK_CUR:\n            if pos < 0:\n                raise ValueError(\"Negative seek position {}\".format(pos))\n            self._tell = self._traverse_codepoints(self._tell, pos)\n        elif mode == os.SEEK_END:\n            if pos > 0:\n                raise ValueError(\"Positive seek position {}\".format(pos))\n            self.buffer.seek(0, os.SEEK_END)\n            self._tell = self._traverse_codepoints(0, pos)\n        else:\n            raise ValueError(\"Invalid whence ({}, should be 0, 1, or 2)\".format(mode))\n        return self._tell", "idx": 538}
{"namespace": "boltons.ioutils.SpooledStringIO.tell", "completion": "        self._checkClosed()\n        return self._tell", "idx": 539}
{"namespace": "boltons.ioutils.SpooledStringIO.len", "completion": "        pos = self.tell()\n        self.seek(0)\n        total = 0\n        while True:\n            chunk = self.read(READ_CHUNK_SIZE)\n            if not chunk:\n                break\n            total += len(chunk)\n        self.seek(pos)\n        return total", "idx": 540}
{"namespace": "boltons.ioutils.MultiFileReader.read", "completion": "        ret = []\n        while amt is None or amt > 0:\n            try:\n                cur_file = self._fileobjs[self._index]\n            except IndexError:\n                break\n            cur_amt = amt\n            if amt is not None:\n                cur_amt = min(amt, os.fstat(cur_file.fileno()).st_size - cur_file.tell())\n                amt -= cur_amt\n            ret.append(cur_file.read(cur_amt))\n            if cur_file.tell() == os.fstat(cur_file.fileno()).st_size:\n                self._index += 1\n        return self._joiner.join(ret)", "idx": 541}
{"namespace": "boltons.ioutils.MultiFileReader.seek", "completion": "        if whence != os.SEEK_SET:\n            raise NotImplementedError('MultiFileReader.seek() only supports os.SEEK_SET')\n        if offset != 0:\n            raise NotImplementedError('MultiFileReader only supports seeking to start at this time')\n        for f in self._fileobjs:\n            f.seek(offset, whence)\n        self._index = 0", "idx": 542}
{"namespace": "boltons.listutils.BarrelList.insert", "completion": "        list_idx, rel_idx = self._translate_index(index)\n        if list_idx is None:\n            raise IndexError()\n        self.lists[list_idx].insert(rel_idx, item)\n        self._balance_list(list_idx)", "idx": 543}
{"namespace": "boltons.listutils.BarrelList.pop", "completion": "        if len(self.lists) == 1:\n            return self.lists[0].pop(*a)\n        else:\n            if not a:\n                index = -1\n            else:\n                index = a[0]\n            list_idx, rel_idx = self._translate_index(index)\n            if list_idx is None:\n                raise IndexError()\n            return self.lists[list_idx].pop(rel_idx)", "idx": 544}
{"namespace": "boltons.listutils.BarrelList.sort", "completion": "        for cur in self.lists:\n            cur.sort()\n        sorted_list = list(chain.from_iterable(self.lists))\n        sorted_list.sort()\n        self.lists[:] = [sorted_list]\n        self._balance_list(0)", "idx": 545}
{"namespace": "boltons.urlutils.URL.path", "completion": "        self.path_parts = tuple([unquote(p) if '%' in p else p for p in path_text.split(u'/')])", "idx": 546}
{"namespace": "boltons.urlutils.URL.navigate", "completion": "        if isinstance(dest, URL):\n            dest = dest.to_text()\n        elif not isinstance(dest, str):\n            raise TypeError('expected str or URL, not %s' % type(dest).__name__)\n\n        if '://' in dest:\n            return URL(dest).normalize()\n\n        if not self.scheme:\n            raise ValueError('cannot navigate from relative URL %r' % self)\n\n        if dest.startswith('//'):\n            return URL(self.scheme + ':' + dest).normalize()\n\n        if dest.startswith('/'):\n            return URL(self.scheme + '://' + self.get_authority() + dest).normalize()\n\n        if not self.path:\n            base_path = '/'\n        elif '/' not in self.path:\n            base_path = '/'\n        else:\n            base_path = self.path.rsplit('/', 1)[0] + '/'\n\n        return URL(self.scheme + '://' + self.get_authority() + base_path + dest).normalize()", "idx": 547}
{"namespace": "boltons.urlutils.URL.to_text", "completion": "        parts = []\n        _add = parts.append\n\n        if self.scheme:\n            _add(self.scheme)\n            _add(':')\n            if self.uses_netloc:\n                _add('//')\n        if self.username:\n            _add(quote_userinfo_part(self.username, full_quote=full_quote))\n            if self.password:\n                _add(':')\n                _add(quote_userinfo_part(self.password, full_quote=full_quote))\n            _add('@')\n        if self.host:\n            if self.family == socket.AF_INET6:\n                _add('[')\n                _add(self.host)\n                _add(']')\n            elif full_quote:\n                _add(self.host.encode('idna').decode('ascii'))\n            else:\n                _add(self.host)\n            if self.port and self.port != self.default_port:\n                _add(':')\n                _add(unicode(self.port))\n        if self.path_parts:\n            _add(u'/'.join([quote_path_part(p, full_quote=full_quote)\n                            for p in self.path_parts]))\n        if self.query_params:\n            _add('?')\n            _add(self.query_params.to_text(full_quote=full_quote))\n        if self.fragment:\n            _add('#')\n            _add(quote_fragment_part(self.fragment, full_quote=full_quote))\n        return u''.join(parts)", "idx": 548}
{"namespace": "boltons.urlutils.QueryParamDict.to_text", "completion": "        parts = []\n        _add = parts.append\n        for k, v in self.iteritems(multi=True):\n            k = quote_query_part(k, full_quote=full_quote)\n            v = quote_query_part(v, full_quote=full_quote)\n            _add(u'='.join([k, v]))\n        return u'&'.join(parts)", "idx": 549}
{"namespace": "boltons.tbutils.TracebackInfo.from_traceback", "completion": "        if tb is None:\n            tb = sys.exc_info()[2]\n            if tb is None:\n                raise ValueError('no tb set and no exception being handled')\n        if limit is None:\n            limit = getattr(sys, 'tracebacklimit', 1000)\n        ret = []\n        n = 0\n        while tb is not None and n < limit:\n            item = cls.callpoint_type.from_tb(tb)\n            ret.append(item)\n            tb = tb.tb_next\n            n += 1\n        ret.reverse()\n        return cls(ret)", "idx": 550}
{"namespace": "boltons.tbutils.ExceptionInfo.get_formatted", "completion": "        ret = self.tb_info.get_formatted()\n        ret += self.get_formatted_exception_only()\n        return ret", "idx": 551}
{"namespace": "boltons.tbutils.print_exception", "completion": "    if file is None:\n        file = sys.stderr\n    if etype is SyntaxError:\n        # Special case for syntax errors: we want to show exactly where the problem was found.\n        print('Traceback (most recent call last):', file=file)\n        lines = format_exception_only(etype, value)\n        for line in lines:\n            print(line, end='', file=file)\n    else:\n        # General case: print the traceback from the given traceback object.\n        ExceptionInfo = ExceptionInfo.from_exc_info(etype, value, tb)\n        print(ExceptionInfo.get_formatted(), file=file, end=\"\")", "idx": 552}
{"namespace": "boltons.tbutils.ParsedException.to_string", "completion": "        tb_str = 'Traceback (most recent call last):\\n'\n        for frame in self.frames:\n            tb_str += '  File \"{filepath}\", line {lineno}, in {funcname}\\n'.format(**frame)\n            if frame['source_line']:\n                tb_str += '    {source_line}\\n'.format(**frame)\n        tb_str += '{exc_type}: {exc_msg}\\n'.format(exc_type=self.exc_type, exc_msg=self.exc_msg)\n        return tb_str", "idx": 553}
{"namespace": "boltons.tbutils.ParsedException.from_string", "completion": "        lines = tb_str.splitlines()\n        frames = []\n        exc_type = exc_msg = None\n\n        while lines:\n            line = lines.pop(0)\n            match = _frame_re.match(line)\n            if match:\n                frame = match.groupdict()\n                if lines:\n                    frame['source_line'] = lines.pop(0).strip()\n                frames.append(frame)\n            else:\n                match = _se_frame_re.match(line)\n                if match:\n                    frame = match.groupdict()\n                    frames.append(frame)\n                else:\n                    exc_type, _, exc_msg = line.partition(':')\n                    exc_msg = exc_msg.strip()\n                    break\n\n        return cls(exc_type, exc_msg, frames)", "idx": 554}
{"namespace": "boltons.tableutils.Table.extend", "completion": "        if not data:\n            return\n        self._data.extend(data)\n        self._set_width()\n        self._fill()", "idx": 555}
{"namespace": "boltons.tableutils.Table.from_object", "completion": "        return cls.from_data(data=data, headers=headers,\n                             max_depth=max_depth, _data_type=ObjectInputType(),\n                             metadata=metadata)", "idx": 556}
{"namespace": "boltons.tableutils.Table.__repr__", "completion": "        if self.headers:\n            return '{type name}(headers={headers!r}, data={data!r})'.format(type_name=type(self).__name__, headers=self.headers, data=self._data)\n        else:\n            return '{type name}({data!r})'.format(type_name=type(self).__name__, data=self._data)", "idx": 557}
{"namespace": "boltons.tableutils.Table.to_text", "completion": "        lines = []\n        if with_headers and self.headers:\n            lines.append(' | '.join([to_text(h, maxlen=maxlen).center(maxlen) for h in self.headers]))\n            lines.append('-|-'.join(['-' * maxlen for _ in self.headers]))\n\n        for row in self._data:\n            lines.append(' | '.join([to_text(cell, maxlen=maxlen).center(maxlen) for cell in row]))\n\n        return '\\n'.join(lines)", "idx": 558}
{"namespace": "boltons.statsutils.Stats.get_histogram_counts", "completion": "        if not self.data:\n            return [(0.0, 0)]  # TODO: raise?\n\n        if isinstance(bins, int):\n            bin_bounds = self._get_bin_bounds(count=bins, with_max=True)\n        elif isinstance(bins, list):\n            bin_bounds = sorted(bins)\n            if bin_bounds[0] > min(self.data):\n                bin_bounds.insert(0, min(self.data))\n            if bin_bounds[-1] < max(self.data):\n                bin_bounds.append(max(self.data))\n        else:\n            bin_bounds = self._get_bin_bounds(with_max=True)\n\n        bin_bounds = [round(b, bin_digits) for b in bin_bounds]\n        bin_counts = [(b, 0) for b in bin_bounds]\n        for val in self.data:\n            bin_idx = bisect.bisect_right(bin_bounds, val) - 1\n            bin_val, count = bin_counts[bin_idx]\n            bin_counts[bin_idx] = (bin_val, count + 1)\n\n        return bin_counts", "idx": 559}
{"namespace": "boltons.setutils.IndexedSet.add", "completion": "        if item not in self.item_index_map:\n            self.item_index_map[item] = len(self.item_list)\n            self.item_list.append(item)", "idx": 560}
{"namespace": "boltons.setutils.IndexedSet.pop", "completion": "        if index is None:\n            index = len(self) - 1\n        else:\n            index = self._get_real_index(index)\n        try:\n            item = self.item_list[index]\n        except IndexError:\n            raise IndexError('pop index out of range')\n        del self.item_index_map[item]\n        if index == len(self.item_list) - 1:\n            self.item_list.pop()\n        else:\n            self.item_list[index] = _MISSING\n            self._add_dead(index)\n        self._cull()\n        return item", "idx": 561}
{"namespace": "boltons.setutils.IndexedSet.index", "completion": "        try:\n            return self._get_apparent_index(self.item_index_map[val])\n        except KeyError:\n            raise ValueError('{0!r} is not in {1}'.format(val, self.__class__.__name__))", "idx": 562}
{"namespace": "boltons.setutils.complement", "completion": "    if type(wrapped) is _ComplementSet:\n        return wrapped.complemented()\n    elif type(wrapped) in (set, frozenset):\n        return _ComplementSet(excluded=wrapped)\n    else:\n        return _ComplementSet(excluded=frozenset(wrapped))", "idx": 563}
{"namespace": "boltons.strutils.strip_ansi", "completion": "    if isinstance(text, bytes):\n        return ANSI_SEQUENCES.sub(b'', text)\n    return ANSI_SEQUENCES.sub('', text)", "idx": 564}
{"namespace": "boltons.strutils.asciify", "completion": "    if isinstance(text, bytes):\n        return text\n    if not isinstance(text, unicode):\n        text = unicode(text)\n    text = unicodedata.normalize('NFKD', text)\n    text = text.translate(DEACCENT_MAP)\n    return text.encode('ascii', 'ignore' if ignore else 'replace')```", "idx": 565}
{"namespace": "boltons.strutils.indent", "completion": "    lines = text.splitlines()\n    for i, line in enumerate(lines):\n        if key(line):\n            lines[i] = margin + line\n    return newline.join(lines)", "idx": 566}
{"namespace": "boltons.strutils.multi_replace", "completion": "    replacer = MultiReplace(sub_map, **kwargs)\n    return replacer.sub(text)", "idx": 567}
{"namespace": "boltons.cacheutils.LRI._get_flattened_ll", "completion": "        flattened_ll = []\n        current = self._anchor[NEXT]\n        while current is not self._anchor:\n            flattened_ll.append((current[KEY], current[VALUE]))\n            current = current[NEXT]\n        return flattened_ll", "idx": 568}
{"namespace": "boltons.cacheutils.LRI.pop", "completion": "        with self._lock:\n            try:\n                value = super(LRI, self).pop(key)\n                self._remove_from_ll(key)\n                return value\n            except KeyError:\n                if default is _MISSING:\n                    raise\n                return default", "idx": 569}
{"namespace": "boltons.cacheutils.LRI.popitem", "completion": "        with self._lock:\n            if not self._link_lookup:\n                raise KeyError('popitem(): dictionary is empty')\n            key = next(iter(self._link_lookup))\n            value = super(LRI, self).pop(key)\n            self._remove_from_ll(key)\n            return key, value", "idx": 570}
{"namespace": "boltons.cacheutils.LRI.clear", "completion": "        with self._lock:\n            super(LRI, self).clear()\n            self._init_ll()\n            self.hit_count = self.miss_count = self.soft_miss_count = 0", "idx": 571}
{"namespace": "boltons.cacheutils.LRI.setdefault", "completion": "        with self._lock:\n            try:\n                return self[key]\n            except KeyError:\n                self.soft_miss_count += 1\n                self[key] = default\n                return default", "idx": 572}
{"namespace": "boltons.cacheutils.LRI.update", "completion": "        with self._lock:\n            if E is not None:\n                if callable(getattr(E, 'keys', None)):\n                    for k in E.keys():\n                        self[k] = E[k]\n                else:\n                    for (k, v) in E:\n                        self[k] = v\n            for k in F:\n                self[k] = F[k]", "idx": 573}
{"namespace": "boltons.cacheutils.LRI.__repr__", "completion": "        return f'{self.__class__.__name__}(max_size={self.max_size}, on_miss={self.on_miss}, values={dict(self)})'", "idx": 574}
{"namespace": "boltons.cacheutils.CachedFunction.__repr__", "completion": "        cn = self.__class__.__name__\n        if self.typed or not self.scoped:\n            return ('%s(func=%r, scoped=%r, typed=%r)' % (cn, self.func, self.scoped, self.typed))\n        return ('%s(func=%r)' % (cn, self.func))", "idx": 575}
{"namespace": "boltons.cacheutils.ThresholdCounter.elements", "completion": "        for key, count in self._count_map.items():\n            for _ in range(count[0]):\n                yield key", "idx": 576}
{"namespace": "boltons.cacheutils.ThresholdCounter.most_common", "completion": "        if n is None:\n            return sorted(self._count_map.items(), key=lambda x: x[1][0], reverse=True)\n        else:\n            return heapq.nlargest(n, self._count_map.items(), key=lambda x: x[1][0])", "idx": 577}
{"namespace": "boltons.cacheutils.ThresholdCounter.update", "completion": "        if iterable is not None:\n            if isinstance(iterable, Mapping):\n                for key, value in iterable.items():\n                    for _ in range(value):\n                        self.add(key)\n            else:\n                for item in iterable:\n                    self.add(item)\n\n        for key, value in kwargs.items():\n            for _ in range(value):\n                self.add(key)", "idx": 578}
{"namespace": "boltons.cacheutils.MinIDMap.get", "completion": "        if a in self.mapping:\n            return self.mapping[a][0]\n        else:\n            if self.free:\n                new_id = heapq.heappop(self.free)\n            else:\n                new_id = len(self.mapping)\n            ref = weakref.ref(a, self._clean)\n            self.mapping[a] = (new_id, ref)\n            self.ref_map[ref] = new_id\n            return new_id", "idx": 579}
{"namespace": "boltons.iterutils.chunked", "completion": "    if not is_iterable(src):\n        raise TypeError('expected an iterable')\n    size = _validate_positive_int(size, 'chunk size')\n    if count is not None:\n        count = _validate_positive_int(count, 'count')\n    do_fill = True\n    try:\n        fill_val = kw.pop('fill')\n    except KeyError:\n        do_fill = False\n        fill_val = None\n    if kw:\n        raise ValueError('got unexpected keyword arguments: %r' % kw.keys())\n    if not src:\n        return []\n    chunks = []\n    chunk = []\n    for i, s in enumerate(src):\n        chunk.append(s)\n        if (i + 1) % size == 0:\n            chunks.append(chunk)\n            chunk = []\n            if count is not None and len(chunks) >= count:\n                break\n    if chunk and (count is None or len(chunks) < count):\n        if do_fill:\n            chunk.extend([fill_val] * (size - len(chunk)))\n        chunks.append(chunk)\n    return chunks", "idx": 580}
{"namespace": "boltons.iterutils.chunk_ranges", "completion": "    if align:\n        input_offset = ((input_offset + chunk_size - overlap_size - 1) // (chunk_size - overlap_size)) * (chunk_size - overlap_size)\n\n    while input_offset < input_size:\n        yield (input_offset, min(input_size, input_offset + chunk_size))\n        input_offset += chunk_size - overlap_size", "idx": 581}
{"namespace": "boltons.iterutils.remap", "completion": "    path, key, new_parent, new_items = [()], None, root, []\n\n    # stack is a list of iterators\n    stack = [(None, root.__class__(), iter([(None, root)]))]\n\n    while stack:\n        path, new_parent, it = stack[-1]\n        try:\n            key, value = next(it)\n        except StopIteration:\n            stack.pop()\n            if stack:\n                new_items.append((key, exit(path, key, new_parent, new_parent, new_items)))\n            continue\n\n        new_path = path + (key,)\n\n        try:\n            new_parent, new_items = enter(new_path, key, value)\n        except TypeError as te:\n            if not is_iterable(value):\n                raise TypeError('not an iterable: %r' % value)\n            else:\n                raise te\n\n        if new_items is False:\n            new_items.append((key, value))\n        else:\n            stack.append((new_path, new_parent, new_items))\n\n    return new_parent", "idx": 582}
{"namespace": "boltons.iterutils.get_path", "completion": "    try:\n        for seg in path:\n            root = root[seg]\n    except Exception as exc:\n        if default is _UNSET:\n            raise PathAccessError(exc, seg, path)\n        return default\n    return root", "idx": 583}
{"namespace": "boltons.iterutils.research", "completion": "    results = []\n    def visit(path, key, value):\n        try:\n            if query(path, key, value):\n                results.append((path, value))\n        except Exception:\n            if reraise:\n                raise\n        return key, value\n\n    remap(root, visit=visit)\n    return results", "idx": 584}
{"namespace": "boltons.socketutils.BufferedSocket.getrecvbuffer", "completion": "        with self._recv_lock:\n            return self.rbuf", "idx": 585}
{"namespace": "boltons.socketutils.BufferedSocket.getsendbuffer", "completion": "        with self._send_lock:\n            return self.sbuf[:]", "idx": 586}
{"namespace": "boltons.socketutils.BufferedSocket.recv", "completion": "        if flags:\n            raise ValueError('non-zero flags not supported: {flags!r}'.format(flags=flags))\n        if timeout is _UNSET:\n            timeout = self.timeout\n        with self._recv_lock:\n            if len(self.rbuf) >= size:\n                data, self.rbuf = self.rbuf[:size], self.rbuf[size:]\n            else:\n                if self.rbuf:\n                    data, self.rbuf = self.rbuf, b''\n                else:\n                    self.sock.settimeout(timeout)\n                    try:\n                        data = self.sock.recv(size)\n                    except socket.timeout:\n                        raise Timeout(timeout, 'recv timed out')\n            return data", "idx": 587}
{"namespace": "boltons.socketutils.BufferedSocket.recv_close", "completion": "        with self._recv_lock:\n            if maxsize is _UNSET:\n                maxsize = self.maxsize\n            if maxsize is None:\n                maxsize = _RECV_LARGE_MAXSIZE\n            if timeout is _UNSET:\n                timeout = self.timeout\n\n            sock = self.sock\n            recvd = bytearray(self.rbuf)\n            start = time.time()\n\n            if not timeout:  # covers None (no timeout) and 0 (nonblocking)\n                sock.settimeout(timeout)\n            try:\n                while 1:\n                    if len(recvd) > maxsize:\n                        raise MessageTooLong(maxsize)  # see rbuf\n                    if timeout:\n                        cur_timeout = timeout - (time.time() - start)\n                        if cur_timeout <= 0.0:\n                            raise socket.timeout()\n                        sock.settimeout(cur_timeout)\n                    nxt = sock.recv(self._recvsize)\n                    if not nxt:\n                        break\n                    recvd.extend(nxt)\n            except socket.timeout:\n                self.rbuf = bytes(recvd)\n                msg = ('read %s bytes without finding delimiter: %r'\n                       % (len(recvd), delimiter))\n                raise Timeout(timeout, msg)  # check the recv buffer\n            except Exception:\n                self.rbuf = bytes(recvd)\n                raise\n            val, self.rbuf = bytes(recvd), b''\n        return val", "idx": 588}
{"namespace": "boltons.socketutils.BufferedSocket.flush", "completion": "        with self._send_lock:\n            if self.sbuf:\n                self.sock.sendall(b''.join(self.sbuf))\n                self.sbuf[:] = []\n        return", "idx": 589}
{"namespace": "boltons.socketutils.BufferedSocket.buffer", "completion": "        with self._send_lock:\n            self.sbuf.append(data)\n        return", "idx": 590}
{"namespace": "boltons.socketutils.BufferedSocket.close", "completion": "        with self._recv_lock:\n            with self._send_lock:\n                self.sock.close()\n                self.rbuf = b''\n                self.sbuf = []\n        return", "idx": 591}
{"namespace": "boltons.socketutils.NetstringSocket.setmaxsize", "completion": "        self.maxsize = maxsize\n        self._msgsize_maxsize = self._calc_msgsize_maxsize(maxsize)", "idx": 592}
{"namespace": "boltons.socketutils.NetstringSocket.write_ns", "completion": "        size = len(payload)\n        if size > self.maxsize:\n            raise NetstringMessageTooLong(size, self.maxsize)\n        size_str = str(size).encode('ascii')\n        data = size_str + b':' + payload + b','\n        self.bsock.sendall(data)", "idx": 593}
{"namespace": "boltons.fileutils.FilePerms.__repr__", "completion": "        return '%s(user=%r, group=%r, other=%r)' % (\n            self.__class__.__name__, self.user, self.group, self.other)", "idx": 594}
{"namespace": "boltons.mathutils.Bits.as_hex", "completion": "        template = '{{0:0{0}x}}'.format((self.len + 3) // 4)\n        return template.format(self.val)", "idx": 595}
{"namespace": "boltons.mathutils.Bits.from_hex", "completion": "        if isinstance(hex_, bytes):\n            hex_ = hex_.decode('ascii')\n        if not hex_.startswith('0x'):\n            hex_ = '0x' + hex_\n        return cls(hex_)", "idx": 596}
{"namespace": "boltons.formatutils.split_format_str", "completion": "    formatter = Formatter()\n    result = []\n    for lit, fname, fspec, conv in formatter.parse(fstr):\n        if lit:\n            result.append((lit, construct_format_field_str(fname, fspec, conv)))\n    return result", "idx": 597}
{"namespace": "boltons.formatutils.infer_positional_format_args", "completion": "    pos_count = [0]  # mutable, so it can be updated in-place\n\n    def replace_anon(m):\n        if m.group(1):  # escaped open-brace\n            return '{{'\n        elif m.group(2):  # escaped close-brace\n            return '}}'\n        else:  # anon positional format arg\n            ret = '{%d%s' % (pos_count[0], m.group(3))\n            pos_count[0] += 1\n            return ret\n\n    return _pos_farg_re.sub(replace_anon, fstr)", "idx": 598}
{"namespace": "boltons.formatutils.tokenize_format_str", "completion": "    if resolve_pos:\n        fstr = infer_positional_format_args(fstr)\n    ret = []\n    for lit, fname, fspec, conv in Formatter().parse(fstr):\n        if fname is None:\n            ret.append(lit)\n            continue\n        field_str = construct_format_field_str(fname, fspec, conv)\n        ret.append(BaseFormatField(fname, fspec, conv))\n    return ret", "idx": 599}
{"namespace": "boltons.dictutils.OneToOne.clear", "completion": "        self.inv.clear()\n        super(OneToOne, self).clear()", "idx": 600}
{"namespace": "boltons.dictutils.OneToOne.pop", "completion": "        if key in self:\n            val = self[key]\n            del self[key]\n            return val\n        elif default is not _MISSING:\n            return default\n        else:\n            raise KeyError(key)", "idx": 601}
{"namespace": "boltons.dictutils.OneToOne.popitem", "completion": "        key, val = dict.popitem(self)\n        dict.__delitem__(self.inv, val)\n        return key, val", "idx": 602}
{"namespace": "boltons.dictutils.ManyToMany.update", "completion": "        if isinstance(iterable, ManyToMany):\n            for key, val in iterable.data.items():\n                self.add(key, val)\n            for key, val in iterable.inv.data.items():\n                self.inv.add(key, val)\n        elif hasattr(iterable, 'keys'):\n            for key in iterable.keys():\n                self.add(key, iterable[key])\n        else:\n            for key, val in iterable:\n                self.add(key, val)", "idx": 603}
{"namespace": "boltons.dictutils.ManyToMany.add", "completion": "        self.data.setdefault(key, set()).add(val)\n        self.inv.data.setdefault(val, set()).add(key)", "idx": 604}
{"namespace": "boltons.dictutils.ManyToMany.remove", "completion": "        self.data[key].remove(val)\n        if not self.data[key]:\n            del self.data[key]\n        self.inv.data[val].remove(key)\n        if not self.inv.data[val]:\n            del self.inv.data[val]", "idx": 605}
{"namespace": "boltons.dictutils.ManyToMany.replace", "completion": "        if key not in self.data:\n            raise KeyError(f\"Key {key} not found in the dictionary\")\n\n        # Get the values associated with the key\n        values = self.data[key]\n\n        # Remove the old key from the dictionary\n        del self.data[key]\n\n        # Add the new key to the dictionary with the associated values\n        self.data[newkey] = values\n\n        # Update the inverse dictionary\n        for value in values:\n            self.inv.data[value].remove(key)\n            if newkey not in self.inv.data[value]:\n                self.inv.data[value].add(newkey)", "idx": 606}
{"namespace": "boltons.dictutils.ManyToMany.iteritems", "completion": "        for key, values in self.data.items():\n            for value in values:\n                yield key, value", "idx": 607}
{"namespace": "gunicorn.config.Config.__str__", "completion": "        settings = []\n        key_max_length = max(len(key) for key in self.settings.keys())\n        for key, value in sorted(self.settings.items()):\n            if callable(value):\n                value = \"<{0.__module__}.{0.__qualname__}()>\".format(value)\n            settings.append(\"{0:{1}} = {2}\".format(key, key_max_length, value))\n        return \"\\n\".join(settings)", "idx": 608}
{"namespace": "gunicorn.config.Config.set", "completion": "        if name not in self.settings:\n            raise AttributeError(\"No configuration setting for: %s\" % name)\n        self.settings[name].set(value)", "idx": 609}
{"namespace": "gunicorn.config.Config.worker_class", "completion": "        uri = self.settings['worker_class'].get()\n\n        # are we using a threaded worker?\n        is_sync = uri.endswith('SyncWorker') or uri == 'sync'\n        if is_sync and self.threads > 1:\n            uri = \"gthread\"\n\n        worker_class = util.load_class(\n            uri,\n            default=\"gunicorn.workers.sync.SyncWorker\",\n            section=\"gunicorn.workers\")\n\n        if hasattr(worker_class, \"setup\"):\n            worker_class.setup()\n\n        return worker_class", "idx": 610}
{"namespace": "gunicorn.config.Config.address", "completion": "        addresses = self.settings['bind'].get()\n        parsed_addresses = [util.parse_address(addr) for addr in addresses]\n        return parsed_addresses", "idx": 611}
{"namespace": "gunicorn.config.Config.logger_class", "completion": "        logger_class = self.settings['logger_class'].get()\n        if logger_class == \"simple\":\n            logger_class = \"gunicorn.glogging.Logger\"\n        if self.statsd_host:\n            logger_class = \"gunicorn.instrument.statsd.Statsd\"\n        logger_class = util.load_class(logger_class)\n        if hasattr(logger_class, \"install\"):\n            logger_class.install()\n        return logger_class", "idx": 612}
{"namespace": "gunicorn.sock.create_sockets", "completion": "    listeners = []\n    # get the addresses from the configuration\n    addresses = conf.address\n    # if file descriptors are provided, use them to create sockets\n    if fds is not None:\n        for fd in fds:\n            sock = socket.fromfd(fd, socket.AF_INET, socket.SOCK_STREAM)\n            listeners.append(sock)\n    else:\n        # otherwise, create new sockets for each address\n        for addr in addresses:\n            sock_type = _sock_type(addr)\n            sock = sock_type(addr, conf, log)\n            listeners.append(sock)\n\n    # if SSL is enabled, wrap the sockets with SSL\n    if conf.is_ssl:\n        for i in range(len(listeners)):\n            listeners[i] = ssl_wrap_socket(listeners[i], conf)\n\n    return listeners", "idx": 613}
{"namespace": "gunicorn.http.body.LengthReader.read", "completion": "        if not isinstance(size, int):\n            raise TypeError(\"size must be an integral type\")\n        if size < 0:\n            raise ValueError(\"Size must be positive.\")\n        if size == 0:\n            return b\"\"\n\n        buf = io.BytesIO()\n        while size > 0 and self.length > 0:\n            data = self.unreader.read(min(size, self.length))\n            if not data:\n                break\n            buf.write(data)\n            size -= len(data)\n            self.length -= len(data)\n\n        data = buf.getvalue()\n        ret, rest = data[:size], data[size:]\n        self.unreader.unread(rest)\n        return ret", "idx": 614}
{"namespace": "gunicorn.http.body.EOFReader.read", "completion": "        if not isinstance(size, int):\n            raise TypeError(\"size must be an integral type\")\n        if size < 0:\n            raise ValueError(\"Size must be positive.\")\n        if size == 0:\n            return b\"\"\n\n        if self.finished:\n            data = self.buf.getvalue()\n            ret, rest = data[:size], data[size:]\n            self.buf = io.BytesIO()\n            self.buf.write(rest)\n            return ret\n\n        while self.buf.tell() < size:\n            data = self.unreader.read()\n            if not data:\n                self.finished = True\n                break\n            self.buf.write(data)\n\n        data = self.buf.getvalue()\n        ret, rest = data[:size], data[size:]\n        self.buf = io.BytesIO()\n        self.buf.write(rest)\n        return ret", "idx": 615}
{"namespace": "gunicorn.http.body.Body.read", "completion": "        size = self.getsize(size)\n        if size == 0:\n            return b\"\"\n\n        if self.buf.tell() < size:\n            data = self.reader.read(min(1024, size - self.buf.tell()))\n            while data:\n                self.buf.write(data)\n                if self.buf.tell() >= size:\n                    break\n                data = self.reader.read(min(1024, size - self.buf.tell()))\n\n        data = self.buf.getvalue()\n        ret, rest = data[:size], data[size:]\n        self.buf = io.BytesIO()\n        self.buf.write(rest)\n        return ret", "idx": 616}
{"namespace": "gunicorn.http.unreader.Unreader.read", "completion": "        if size is not None and not isinstance(size, (int, long)):\n            raise TypeError(\"size parameter must be an int or long.\")\n        if size == 0:\n            return b\"\"\n        if size < 0:\n            size = None\n\n        self.buf.seek(0, os.SEEK_END)\n        if size is None:\n            if self.buf.tell():\n                data = self.buf.getvalue()\n                self.buf = io.BytesIO()\n                return data\n            else:\n                return self.chunk()\n\n        while self.buf.tell() < size:\n            chunk = self.chunk()\n            if not chunk:\n                data = self.buf.getvalue()\n                self.buf = io.BytesIO()\n                return data\n            self.buf.write(chunk)\n\n        self.buf.seek(0)\n        data = self.buf.read(size)\n        rest = self.buf.read()\n        self.buf = io.BytesIO()\n        self.buf.write(rest)\n        return data", "idx": 617}
{"namespace": "gunicorn.http.unreader.Unreader.unread", "completion": "        self.buf.seek(0, os.SEEK_END)\n        self.buf.write(data)", "idx": 618}
{"namespace": "gunicorn.http.unreader.IterUnreader.chunk", "completion": "        try:\n            return next(self.iter)\n        except StopIteration:\n            self.iter = None\n            return b\"\"", "idx": 619}
{"namespace": "gunicorn.instrument.statsd.Statsd.critical", "completion": "        Logger.critical(self, msg, *args, **kwargs)\n        self.increment(\"gunicorn.log.critical\", 1)", "idx": 620}
{"namespace": "gunicorn.instrument.statsd.Statsd.access", "completion": "        # Calculate the request duration in milliseconds\n        request_duration = request_time.seconds * 1000 + request_time.microseconds / 1000\n\n        # Log the request duration as a histogram\n        self.histogram(\"gunicorn.request.duration\", request_duration)\n\n        # Increment the total request count\n        self.increment(\"gunicorn.request\", 1)\n\n        # Get the status code from the response\n        status_code = resp.status.split(None, 1)[0]\n        status_code = int(status_code)\n\n        # Increment the count for the specific status code\n        self.increment(\"gunicorn.request.status.{0}\".format(status_code), 1)", "idx": 621}
{"namespace": "praw.exceptions.RedditErrorItem.error_message", "completion": "        message = self.error_type\n        if self.message:\n            message += f\": {self.message}\"\n        if self.field:\n            message += f\" on field {self.field}\"\n        return message", "idx": 622}
{"namespace": "praw.exceptions.RedditErrorItem.__repr__", "completion": "        return f\"{self.__class__.__name__}(error_type={self.error_type!r}, message={self.message!r}, field={self.field!r})\"", "idx": 623}
{"namespace": "praw.models.util.BoundedSet.add", "completion": "        self._set[item] = None\n        self._set.move_to_end(item)\n        if len(self._set) > self.max_items:\n            self._set.popitem(last=False)", "idx": 624}
{"namespace": "praw.models.util.ExponentialCounter.counter", "completion": "        jitter = self._base / 16\n        value = self._base + random.uniform(-jitter / 2, jitter / 2)\n        if self._base < self._max / 2:\n            self._base *= 2\n        else:\n            self._base = self._max\n        return value", "idx": 625}
{"namespace": "praw.models.listing.generator.ListingGenerator._extract_sublist", "completion": "        if isinstance(listing, list):\n            if isinstance(listing[1], (FlairListing, ModNoteListing)):\n                return listing[1]\n        elif isinstance(listing, dict):\n            for possible_list in (\"children\", \"users\"):\n                if possible_list in listing:\n                    return listing[possible_list]\n        raise ValueError(\n            \"The generator returned a dictionary PRAW didn't recognize. \"\n            \"File a bug report at PRAW.\"\n        )", "idx": 626}
{"namespace": "praw.util.token_manager.FileTokenManager.post_refresh_callback", "completion": "        with open(self._filename, 'w') as fp:\n            fp.write(authorizer.refresh_token)", "idx": 627}
{"namespace": "praw.util.token_manager.FileTokenManager.pre_refresh_callback", "completion": "        if authorizer.refresh_token is None:\n            with open(self._filename, \"r\") as fp:\n                authorizer.refresh_token = fp.read()", "idx": 628}
{"namespace": "praw.util.token_manager.SQLiteTokenManager._get", "completion": "        cursor = self._connection.execute(\n            \"SELECT refresh_token FROM tokens WHERE id=?\", (self.key,)\n        )\n        result = cursor.fetchone()\n        if result is None:\n            raise KeyError(f\"No refresh token for key: {self.key}\")\n        return result[0]", "idx": 629}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.is_registered", "completion": "        cursor = self._connection.execute(\n            \"SELECT 1 FROM tokens WHERE id=?\", (self.key,)\n        )\n        result = cursor.fetchone()\n        return result is not None", "idx": 630}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.post_refresh_callback", "completion": "        self._set(authorizer.refresh_token)\n        authorizer.refresh_token = None", "idx": 631}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.pre_refresh_callback", "completion": "        if authorizer.refresh_token is None:\n            authorizer.refresh_token = self._get()", "idx": 632}
{"namespace": "praw.util.token_manager.SQLiteTokenManager.register", "completion": "        if not self.is_registered():\n            self._set(refresh_token)\n            return True\n        else:\n            return False", "idx": 633}
{"namespace": "jc.cli.JcCli.about_jc", "completion": "        py_ver: str = '.'.join((str(sys.version_info.major), str(sys.version_info.minor), str(sys.version_info.micro)))\n        all_parsers = all_parser_info(show_hidden=True, show_deprecated=False)\n        standard_parsers = [p for p in all_parsers if 'standard' in p.get('tags', [])]\n        streaming_parsers = [p for p in all_parsers if p.get('streaming')]\n        plugin_parsers = [p for p in all_parsers if 'plugin' in p.get('tags', [])]\n\n        about_dict: JSONDictType = {\n            'library': {\n                'name': 'jc',\n                'version': info.version,\n                'description': info.description,\n                'author': info.author,\n                'author_email': info.author_email,\n                'website': info.website,\n                'copyright': info.copyright,\n                'license': info.license,\n                'python_version': py_ver,\n                'python_path': sys.executable\n            },\n            'parsers': {\n                'count': len(all_parsers),\n                'standard_count': len(standard_parsers),\n                'streaming_count': len(streaming_parsers),\n                'plugin_count': len(plugin_parsers),\n                'all': all_parsers\n            }\n        }\n\n        return about_dict", "idx": 634}
{"namespace": "jc.cli.JcCli.yaml_out", "completion": "                    from ruamel.yaml.compat import StringIO\n                    from ruamel.yaml import YAML\n        try:\n            from ruamel.yaml import YAML\n            from ruamel.yaml.compat import StringIO\n        except ImportError:\n            utils.warning_message(['ruamel.yaml library not installed. Falling back to JSON output.'])\n            return self.json_out()\n\n        yaml = YAML()\n        yaml.indent(mapping=2, sequence=4, offset=2)\n        yaml_out = StringIO()\n        yaml.dump(self.data_out, yaml_out)\n\n        if not self.mono:\n            class JcStyle(Style):\n                styles: CustomColorType = self.custom_colors\n\n            return str(highlight(yaml_out.getvalue(), YamlLexer(), Terminal256Formatter(style=JcStyle))[0:-1])\n\n        return yaml_out.getvalue()", "idx": 635}
{"namespace": "jc.parsers.os_release.parse", "completion": "\n    if not quiet:\n        jc.utils.compatibility(__name__, info.compatible)\n\n    kv_output = jc.parsers.kv.parse(data, raw=raw, quiet=quiet)\n\n    if raw:\n        return kv_output\n    else:\n        return _process(kv_output)", "idx": 636}
{"namespace": "jc.parsers.xrandr._parse_screen", "completion": "\n    if not next_lines:\n        return None\n\n    next_line = next_lines.pop()\n    result = re.match(_screen_pattern, next_line)\n    if not result:\n        next_lines.append(next_line)\n        return None\n\n    matches = result.groupdict()\n\n    screen: Screen = {\n        \"screen_number\": int(matches[\"screen_number\"]),\n        \"minimum_width\": int(matches[\"minimum_width\"]),\n        \"minimum_height\": int(matches[\"minimum_height\"]),\n        \"current_width\": int(matches[\"current_width\"]),\n        \"current_height\": int(matches[\"current_height\"]),\n        \"maximum_width\": int(matches[\"maximum_width\"]),\n        \"maximum_height\": int(matches[\"maximum_height\"]),\n        \"devices\": [],\n    }\n\n    while next_lines:\n        next_device: Optional[Device] = _parse_device(next_lines)\n        if next_device:\n            screen[\"devices\"].append(next_device)\n        else:\n            break\n\n    return screen", "idx": 637}
{"namespace": "jc.parsers.xrandr._parse_model", "completion": "    if not next_lines:\n        return None\n\n    next_line = next_lines.pop()\n    result = re.match(_edid_head_pattern, next_line)\n    if not result:\n        next_lines.append(next_line)\n        return None\n\n    edid_hex = \"\"\n    while next_lines:\n        next_line = next_lines.pop()\n        result = re.match(_edid_line_pattern, next_line)\n        if not result:\n            next_lines.append(next_line)\n            break\n        edid_hex += result.group(\"edid_line\")\n\n    edid_bytes = bytes.fromhex(edid_hex)\n    edid = parse_edid(edid_bytes)\n\n    model: Model = {\n        \"name\": edid[\"name\"],\n        \"product_id\": edid[\"product_id\"],\n        \"serial_number\": edid[\"serial_number\"],\n    }\n\n    return model", "idx": 638}
{"namespace": "jc.parsers.xrandr._parse_mode", "completion": "    result = re.match(_mode_pattern, line)\n    if not result:\n        return None\n\n    matches = result.groupdict()\n\n    mode: Mode = {\n        \"resolution_width\": int(matches[\"resolution_width\"]),\n        \"resolution_height\": int(matches[\"resolution_height\"]),\n        \"is_high_resolution\": matches[\"is_high_resolution\"] is not None,\n        \"frequencies\": [],\n    }\n\n    frequencies = re.findall(_frequencies_pattern, matches[\"rest\"])\n    for frequency in frequencies:\n        freq_dict: Frequency = {\n            \"frequency\": float(frequency[0]),\n            \"is_current\": frequency[1] == \"*\",\n            \"is_preferred\": frequency[2] == \"+\",\n        }\n        mode[\"frequencies\"].append(freq_dict)\n\n    return mode", "idx": 639}
{"namespace": "pythonforandroid.archs.Arch.include_dirs", "completion": "        include_dirs = []\n        for include_dir in self.common_cppflags:\n            if include_dir.startswith('-I'):\n                include_dirs.append(include_dir.format(ctx=self.ctx, command_prefix=self.command_prefix, python_includes=join(\n                    self.ctx.get_python_install_dir(self.arch),\n                    'include/python{}'.format(self.ctx.python_recipe.version[0:3]),\n                )))\n        return include_dirs", "idx": 640}
{"namespace": "pythonforandroid.archs.Arch.target", "completion": "        return '{}-{}'.format(self.command_prefix, self.ctx.ndk_api)", "idx": 641}
{"namespace": "pythonforandroid.archs.ArchARM.target", "completion": "        return '{triplet}{ndk_api}'.format(\n            triplet=self.command_prefix, ndk_api=self.ctx.ndk_api\n        )", "idx": 642}
{"namespace": "pythonforandroid.recipe.Recipe.get_recipe", "completion": "        if name in cls.recipes:\n            return cls.recipes[name]\n\n        forbidden_dirs = ('__pycache__', )\n        for recipe_dir in cls.recipe_dirs(ctx):\n            if recipe_dir and exists(recipe_dir):\n                for file_name in listdir(recipe_dir):\n                    if file_name in forbidden_dirs:\n                        continue\n                    if file_name == name:\n                        mod = importlib.import_module('pythonforandroid.recipes.{}'.format(name))\n                        recipe = mod.recipe\n                        cls.recipes[name] = recipe\n                        return recipe\n\n        raise ValueError('No recipe named {} exists'.format(name))", "idx": 643}
{"namespace": "pythonforandroid.prerequisites.HomebrewPrerequisite.darwin_helper", "completion": "        info(\n            \"python-for-android requires Homebrew to be installed on macOS,\"\n            \"but seems like you don't have it installed.\"\n        )\n        info(\n            \"If you think that Homebrew is already installed, please verify that \"\n            \"`brew` command is available in your PATH.\"\n        )\n        info(\n            \"If you don't have Homebrew installed, you can install it by following the instructions at \"\n            \"https://brew.sh/\"\n        )", "idx": 644}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(self.homebrew_formula_name, installed=True)\n            is not None\n        )", "idx": 645}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_pkg_config_location", "completion": "        prefix = self._darwin_get_brew_formula_location_prefix(self.homebrew_formula_name)\n        if prefix is not None:\n            return os.path.join(prefix, \"lib\", \"pkgconfig\")\n        else:\n            warning(\n                f\"pkg-config location is not supported on macOS for prerequisite: {self.name}\"\n            )\n            return \"\"", "idx": 646}
{"namespace": "pythonforandroid.prerequisites.OpenSSLPrerequisite.darwin_installer", "completion": "        info(\"Installing OpenSSL ...\")\n        subprocess.check_output([\"brew\", \"install\", self.homebrew_formula_name])", "idx": 647}
{"namespace": "pythonforandroid.prerequisites.AutoconfPrerequisite.darwin_installer", "completion": "        info(\"Installing Autoconf ...\")\n        subprocess.check_output([\"brew\", \"install\", \"autoconf\"])", "idx": 648}
{"namespace": "pythonforandroid.prerequisites.AutomakePrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"automake\", installed=True)\n            is not None\n        )", "idx": 649}
{"namespace": "pythonforandroid.prerequisites.AutomakePrerequisite.darwin_installer", "completion": "        info(\"Installing Automake ...\")\n        subprocess.check_output([\"brew\", \"install\", \"automake\"])", "idx": 650}
{"namespace": "pythonforandroid.prerequisites.LibtoolPrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"libtool\", installed=True)\n            is not None\n        )", "idx": 651}
{"namespace": "pythonforandroid.prerequisites.LibtoolPrerequisite.darwin_installer", "completion": "        info(\"Installing Libtool ...\")\n        subprocess.check_output([\"brew\", \"install\", \"libtool\"])", "idx": 652}
{"namespace": "pythonforandroid.prerequisites.PkgConfigPrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"pkg-config\", installed=True)\n            is not None\n        )", "idx": 653}
{"namespace": "pythonforandroid.prerequisites.PkgConfigPrerequisite.darwin_installer", "completion": "        info(\"Installing Pkg-Config ...\")\n        subprocess.check_output([\"brew\", \"install\", \"pkg-config\"])", "idx": 654}
{"namespace": "pythonforandroid.prerequisites.CmakePrerequisite.darwin_checker", "completion": "        return (\n            self._darwin_get_brew_formula_location_prefix(\"cmake\", installed=True)\n            is not None\n        )", "idx": 655}
{"namespace": "pythonforandroid.prerequisites.CmakePrerequisite.darwin_installer", "completion": "        info(\"Installing Cmake ...\")\n        subprocess.check_output([\"brew\", \"install\", \"cmake\"])", "idx": 656}
{"namespace": "pythonforandroid.prerequisites.get_required_prerequisites", "completion": "    prerequisites = [\n        HomebrewPrerequisite,\n        JDKPrerequisite,\n        OpenSSLPrerequisite,\n        AutoconfPrerequisite,\n        AutomakePrerequisite,\n        LibtoolPrerequisite,\n        PkgConfigPrerequisite,\n        CmakePrerequisite,\n    ]\n\n    return [prerequisite() for prerequisite in prerequisites if prerequisite.mandatory[platform]]", "idx": 657}
{"namespace": "pythonforandroid.pythonpackage.parse_as_folder_reference", "completion": "    # Check if the dependency is a local filesystem path\n    # (and not a package on PyPi)\n    if os.path.isdir(dep):\n        return dep\n\n    # Check if the dependency is a file:// URL, which points to a local\n    # filesystem path.\n    parsed_url = urlparse(dep)\n    if parsed_url.scheme == 'file':\n        return urlunquote(parsed_url.path)\n\n    # If the dependency is neither a local filesystem path nor a file:// URL,\n    # it's not a folder reference.\n    return None", "idx": 658}
{"namespace": "pythonforandroid.pythonpackage.get_package_name", "completion": "    if use_cache and dependency in package_name_cache:\n        return package_name_cache[dependency]\n\n    name = _extract_info_from_package(dependency, extract_type=\"name\")\n    package_name_cache[dependency] = name\n    return name", "idx": 659}
{"namespace": "pythonforandroid.recommendations.read_ndk_version", "completion": "    source_properties = join(ndk_dir, 'source.properties')\n    try:\n        with open(source_properties, 'r') as fileh:\n            version_line = [\n                line.strip() for line in fileh if 'Pkg.Revision' in line\n            ][0]\n    except (IndexError, FileNotFoundError, IsADirectoryError):\n        warning(UNKNOWN_NDK_MESSAGE)\n        return None\n    except IOError:\n        warning(PARSE_ERROR_NDK_MESSAGE)\n        return None\n\n    version = version_line.split('=')[1].strip()\n    return LooseVersion(version)", "idx": 660}
{"namespace": "pythonforandroid.recommendations.check_target_api", "completion": "    if api < MIN_TARGET_API:\n        warning(OLD_API_MESSAGE)\n    elif api > RECOMMENDED_TARGET_API:\n        warning(\n            'Target API is greater than the recommended target API {recommended_target_api}'.format(\n                recommended_target_api=RECOMMENDED_TARGET_API\n            )\n        )\n    if arch == 'armeabi' and api > ARMEABI_MAX_TARGET_API:\n        raise BuildInterruptingException(\n            UNSUPPORTED_NDK_API_FOR_ARMEABI_MESSAGE.format(\n                req_ndk_api=api, max_ndk_api=ARMEABI_MAX_TARGET_API\n            )\n        )", "idx": 661}
{"namespace": "pythonforandroid.recommendations.check_ndk_api", "completion": "    if ndk_api > android_api:\n        raise BuildInterruptingException(\n            TARGET_NDK_API_GREATER_THAN_TARGET_API_MESSAGE.format(\n                ndk_api=ndk_api, android_api=android_api\n            ),\n            instructions='Please ensure that your NDK API version is not higher than your target Android API version.'\n        )\n\n    if ndk_api < MIN_NDK_API:\n        warning('NDK API {} < {}'.format(ndk_api, MIN_NDK_API))\n        warning(OLD_NDK_API_MESSAGE)", "idx": 662}
{"namespace": "pythonforandroid.androidndk.AndroidNDK.llvm_prebuilt_dir", "completion": "        return os.path.join(self.ndk_dir, \"toolchains\", \"llvm\", \"prebuilt\", self.host_tag)", "idx": 663}
{"namespace": "pythonforandroid.build.Context.setup_dirs", "completion": "        self.storage_dir = abspath(expanduser(storage_dir))\n        self.build_dir = join(self.storage_dir, 'build')\n        self.dist_dir = join(self.storage_dir, 'dists')\n        ensure_dir(self.storage_dir)\n        ensure_dir(self.build_dir)\n        ensure_dir(self.dist_dir)", "idx": 664}
{"namespace": "pythonforandroid.graph.get_dependency_tuple_list_for_recipe", "completion": "    if blacklist is None:\n        blacklist = set()\n    dependencies = fix_deplist(recipe.depends)\n    # filter out blacklisted items and turn lowercase\n    dependencies = [\n        tuple([dep for dep in deps if dep not in blacklist])\n        for deps in dependencies\n    ]\n    return dependencies", "idx": 665}
{"namespace": "pythonforandroid.graph.obvious_conflict_checker", "completion": "    all_deps = []\n    for name_tuple in name_tuples:\n        try:\n            recipe_to_add = Recipe.get_recipe(name_tuple[0], ctx)\n        except ValueError:\n            continue\n\n        deps = get_dependency_tuple_list_for_recipe(recipe_to_add, blacklist=blacklist)\n        deps.append(name_tuple)\n\n        conflicts = [dep.lower() for dep in recipe_to_add.conflicts]\n        for dep in deps:\n            if dep in all_deps:\n                raise BuildInterruptingException(\n                    'Conflict detected: {} is a dependency of {}, but conflicts with {}.'.format(\n                        dep, name_tuple, all_deps[dep]\n                    )\n                )\n            if dep in conflicts:\n                raise BuildInterruptingException(\n                    'Conflict detected: {} is a dependency of {}, but conflicts with {}.'.format(\n                        dep, name_tuple, conflicts[dep]\n                    )\n                )\n        all_deps.extend(deps)", "idx": 666}
{"namespace": "pythonforandroid.graph.get_recipe_order_and_bootstrap", "completion": "    if blacklist is None:\n        blacklist = set()\n    # Turn input into dependencies we actually want to add\n    names = set([name.lower() for name in names])\n    # Remove all the blacklisted items\n    names = names - blacklist\n    # Add bootstrap deps if any\n    if bs is not None:\n        names = names.union(set([dep.lower() for dep in bs.recipe_depends]))\n\n    # Run the preflight checker to see if we have any obvious conflicts\n    obvious_conflict_checker(ctx, [tuple(names)], blacklist=blacklist)\n\n    # Generate all possible order graphs\n    possible_orders = recursively_collect_orders(\n        None, ctx, names, [RecipeOrder(ctx)], blacklist=blacklist\n    )\n\n    # Turn each order graph into a linear list\n    possible_orders = [list(find_order(order)) for order in possible_orders]\n\n    # Sort the orders based on preference\n    possible_orders.sort(key=prefer_ssl_with_libffi)\n\n    # Choose the first order\n    chosen_order = possible_orders[0]\n\n    # Get the corresponding recipes and python modules\n    recipes = [Recipe.get_recipe(name, ctx) for name in chosen_order]\n    python_modules = [name for name in chosen_order if name not in ctx.recipe_build_order]\n\n    return chosen_order, recipes, python_modules, bs", "idx": 667}
{"namespace": "pythonforandroid.util.ensure_dir", "completion": "    if not exists(dn):\n        LOGGER.debug(\"Creating directory {}\".format(dn))\n        makedirs(dn)", "idx": 668}
{"namespace": "pythonforandroid.util.move", "completion": "    LOGGER.debug(\"Moving {0} to {1}\".format(source, destination))\n    shutil.move(source, destination)", "idx": 669}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.get_bootstrap_from_recipes", "completion": "        acceptable_bootstraps = cls.get_usable_bootstraps_for_recipes(recipes, ctx)\n        if not acceptable_bootstraps:\n            raise BuildInterruptingException(\n                'No acceptable bootstraps found. '\n                'This might mean your requirements are impossible, '\n                'or simply that we don\\'t know how to handle them yet.')\n        acceptable_bootstraps = sorted(\n            acceptable_bootstraps, key=functools.cmp_to_key(_cmp_bootstraps_by_priority))\n        info('Chose the best bootstrap {}: {}'.format(\n            acceptable_bootstraps[0].name, acceptable_bootstraps))\n        return acceptable_bootstraps[0]", "idx": 670}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.get_bootstrap", "completion": "        # Import the bootstrap module\n        bootstrap_module_name = 'pythonforandroid.bootstraps.{}'.format(name)\n        bootstrap_module = importlib.import_module(bootstrap_module_name)\n\n        # Get the bootstrap class from the module\n        bootstrap_class = getattr(bootstrap_module, 'Bootstrap')\n\n        # Create an instance of the bootstrap class\n        bootstrap = bootstrap_class()\n\n        # Set the bootstrap directory\n        bootstrap.bootstrap_dir = join(ctx.root_dir, 'bootstraps', name)\n\n        # Set the context\n        bootstrap.ctx = ctx\n\n        return bootstrap", "idx": 671}
{"namespace": "pythonforandroid.bootstrap.expand_dependencies", "completion": "    possible_combinations = [[]]\n    for recipe in recipes:\n        if isinstance(recipe, (list, tuple)):\n            new_combinations = []\n            for alternative in recipe:\n                for combination in possible_combinations:\n                    new_combination = combination + [alternative]\n                    new_combinations.append(new_combination)\n            possible_combinations = new_combinations\n        else:\n            for combination in possible_combinations:\n                combination.append(recipe)\n    for combination in possible_combinations:\n        for recipe in combination:\n            recipe_instance = Recipe.get_recipe(recipe, ctx)\n            for dependency in recipe_instance.depends:\n                if dependency not in combination:\n                    combination.append(dependency)\n    return possible_combinations", "idx": 672}
{"namespace": "pythonforandroid.recipes.icu.ICURecipe.get_recipe_dir", "completion": "        local_recipes_dir = self.ctx.local_recipes\n        if local_recipes_dir is not None:\n            local_icu_dir = join(local_recipes_dir, self.name)\n            if exists(local_icu_dir):\n                return local_icu_dir\n        return join(self.ctx.root_dir, 'recipes', self.name)", "idx": 673}
{"namespace": "mmcv.transforms.wrappers.KeyMapper.__repr__", "completion": "        repr_str = self.__class__.__name__\n        repr_str += f'(transforms = {self.transforms}'\n        repr_str += f', mapping = {self.mapping}'\n        repr_str += f', remapping = {self.remapping}'\n        repr_str += f', auto_remap = {self.auto_remap}'\n        repr_str += f', allow_nonexist_keys = {self.allow_nonexist_keys})'\n        return repr_str", "idx": 674}
{"namespace": "mmcv.transforms.wrappers.TransformBroadcaster.__repr__", "completion": "        repr_str = self.__class__.__name__\n        repr_str += f'(transforms = {self.transforms}'\n        repr_str += f', mapping = {self.mapping}'\n        repr_str += f', remapping = {self.remapping}'\n        repr_str += f', auto_remap = {self.auto_remap}'\n        repr_str += f', allow_nonexist_keys = {self.allow_nonexist_keys}'\n        repr_str += f', share_random_params = {self.share_random_params})'\n        return repr_str", "idx": 675}
{"namespace": "mackup.utils.delete", "completion": "    assert isinstance(filepath, str)\n    assert os.path.exists(filepath)\n\n    # Remove the ACL of the file or folder\n    remove_acl(filepath)\n\n    # Remove the immutable attribute of the file or folder\n    remove_immutable_attribute(filepath)\n\n    # Delete the file or folder\n    if os.path.isfile(filepath) or os.path.islink(filepath):\n        os.remove(filepath)\n    elif os.path.isdir(filepath):\n        shutil.rmtree(filepath)\n    else:\n        raise ValueError(\"Unsupported file type: {}\".format(filepath))", "idx": 676}
{"namespace": "mackup.utils.copy", "completion": "    assert isinstance(src, str)\n    assert os.path.exists(src)\n    assert isinstance(dst, str)\n\n    # Create the path to the destination if it does not exist\n    abs_path = os.path.dirname(os.path.abspath(dst))\n    if not os.path.isdir(abs_path):\n        os.makedirs(abs_path)\n\n    # Copy the file or folder\n    if os.path.isfile(src):\n        shutil.copy2(src, dst)  # copy2 preserves file metadata\n    elif os.path.isdir(src):\n        shutil.copytree(src, dst)\n    else:\n        raise ValueError(\"Unsupported file type: {}\".format(src))\n\n    # Set the appropriate file permissions\n    chmod(dst)", "idx": 677}
{"namespace": "mackup.utils.get_dropbox_folder_location", "completion": "    # Define the path to the Dropbox host.db file\n    dropbox_host_db_path = os.path.join(os.environ[\"HOME\"], \".dropbox/host.db\")\n\n    # Check if the host.db file exists\n    if os.path.isfile(dropbox_host_db_path):\n        # Open the host.db file\n        with open(dropbox_host_db_path, \"r\") as f:\n            # Read the lines in the file\n            lines = f.readlines()\n\n            # The Dropbox path is base64 encoded on the second line\n            dropbox_home = base64.b64decode(lines[1]).decode()\n\n            # Return the Dropbox path\n            return dropbox_home\n\n    # If the host.db file does not exist, raise an error\n    else:\n        error(constants.ERROR_UNABLE_TO_FIND_STORAGE.format(provider=\"Dropbox install\"))", "idx": 678}
{"namespace": "mackup.utils.get_copy_folder_location", "completion": "    copy_db_path = \"Library/Application Support/Copy Agent/config.db\"\n    copy_home = None\n\n    copy_db = os.path.join(os.environ[\"HOME\"], copy_db_path)\n    if os.path.isfile(copy_db):\n        con = sqlite3.connect(copy_db)\n        if con:\n            cur = con.cursor()\n            query = (\n                \"SELECT value \"\n                \"FROM option \"\n                \"WHERE option = 'csmRootPath';\"\n            )\n            cur.execute(query)\n            data = cur.fetchone()\n            copy_home = str(data[0])\n            con.close()\n\n    if not copy_home:\n        error(\n            constants.ERROR_UNABLE_TO_FIND_STORAGE.format(\n                provider=\"Copy install\"\n            )\n        )\n\n    return copy_home", "idx": 679}
{"namespace": "mackup.utils.can_file_be_synced_on_current_platform", "completion": "    # Check if the path exists\n    if not os.path.exists(path):\n        return False\n\n    # Check if the path is a file or a directory\n    if not os.path.isfile(path) and not os.path.isdir(path):\n        return False\n\n    # Check if the path is in a system directory that should not be synced\n    system_directories = [\"/bin\", \"/boot\", \"/dev\", \"/etc\", \"/lib\", \"/proc\", \"/sbin\", \"/sys\", \"/usr\"]\n    if any(path.startswith(dir) for dir in system_directories):\n        return False\n\n    # Check if the path is in a user directory that should not be synced\n    user_directories = [\"~/Library\", \"~/.Trash\", \"~/.cache\", \"~/.config\", \"~/.local\"]\n    if any(path.startswith(os.path.expanduser(dir)) for dir in user_directories):\n        return False\n\n    # If none of the above conditions are met, the file can be synced\n    return True", "idx": 680}
{"namespace": "hl7.client.MLLPClient.send_message", "completion": "        if isinstance(message, hl7.Message):\n            # convert to a string\n            message = str(message)\n        if isinstance(message, str):\n            # encode the string into bytes\n            message = message.encode(self.encoding)\n        if not isinstance(message, bytes):\n            raise TypeError(\"message must be bytes, str, or hl7.Message\")\n\n        # wrap the message in the MLLP container\n        message = SB + message + EB + CR\n\n        # send the message and return the response\n        return self.send(message)", "idx": 681}
{"namespace": "hl7.client.MLLPClient.send", "completion": "        try:\n            # Send the data\n            self.socket.sendall(data)\n            \n            # Now wait for a response\n            response = self.socket.recv(RECV_BUFFER)\n            if not response:\n                raise MLLPException(\"No response received from the server.\")\n            \n            # Validate if response is properly enclosed in MLLP frame\n            if not (response.startswith(SB) and response.endswith(EB + CR)):\n                raise MLLPException(\"Improperly formatted response received from the server.\")\n            \n            return response\n        \n        except socket.error as e:\n            raise MLLPException(f\"Socket error occurred: {e}\")", "idx": 682}
{"namespace": "hl7.datatypes._UTCOffset.tzname", "completion": "        hours, minutes = divmod(self.minutes, 60)\n        return \"{0:+03d}{1:02d}\".format(hours, minutes)", "idx": 683}
{"namespace": "hl7.datatypes.parse_datetime", "completion": "    if not value:\n        return None\n\n    match = DTM_TZ_RE.match(value)\n    if not match:\n        raise ValueError(\"Invalid DTM value: {0}\".format(value))\n\n    dtm, tz_hours, tz_minutes = match.groups()\n    dtm = dtm.ljust(14, \"0\")\n    year = int(dtm[:4])\n    month = int(dtm[4:6]) if dtm[4:6] else 1\n    day = int(dtm[6:8]) if dtm[6:8] else 1\n    hour = int(dtm[8:10]) if dtm[8:10] else 0\n    minute = int(dtm[10:12]) if dtm[10:12] else 0\n    second = int(dtm[12:14]) if dtm[12:14] else 0\n    microsecond = int(float(\"0.\" + dtm[14:]) * 1e6)\n\n    if tz_hours or tz_minutes:\n        tzinfo = _UTCOffset(int(tz_hours or \"0\") * 60 + int(tz_minutes or \"0\"))\n    else:\n        tzinfo = None\n\n    return datetime.datetime(year, month, day, hour, minute, second, microsecond, tzinfo)", "idx": 684}
{"namespace": "hl7.parser._ParsePlan.container", "completion": "        # The first container in the list is the one we want to create\n        container_type = self.containers[0]\n        # Create an instance of the container with the appropriate parameters\n        return container_type(\n            sequence=data,\n            esc=self.esc,\n            separators=self.separators,\n            factory=self.factory\n        )", "idx": 685}
{"namespace": "hl7.parser._ParsePlan.next", "completion": "        return _ParsePlan(\n            self.separators[self.separators.find(self.separator) + 1],\n            self.separators,\n            self.containers[1:],\n            self.esc,\n            self.factory,\n        )", "idx": 686}
{"namespace": "hl7.version.get_version", "completion": "    assert len(VERSION) == 4\n    assert VERSION[3] in (\"dev\", \"a\", \"b\", \"rc\", \"final\")\n\n    main = \".\".join(str(x) for x in VERSION[:3])\n\n    sub = \"\"\n    if VERSION[3] == \"dev\":\n        sub = \".dev\"\n    elif VERSION[3] != \"final\":\n        sub = str(VERSION[3])\n\n    return main + sub", "idx": 687}
{"namespace": "twtxt.config.Config.from_file", "completion": "        if not os.path.exists(file):\n            logger.error(\"Config file does not exist: {}\".format(file))\n            raise FileNotFoundError(\"Config file not found at '{}'\".format(file))\n        \n        cfg = configparser.ConfigParser()\n        try:\n            cfg.read(file)\n        except configparser.Error as e:\n            logger.error(\"Error parsing config file: {}\".format(e))\n            raise ValueError(\"Error parsing config file at '{}': {}\".format(file, e))\n        \n        config = cls(file, cfg)\n        \n        if not config.check_config_sanity():\n            raise ValueError(\"Config file at '{}' is not valid\".format(file))\n        \n        return config", "idx": 688}
{"namespace": "twtxt.config.Config.discover", "completion": "        config_file = os.path.join(cls.config_dir, cls.config_name)\n        return cls.from_file(config_file)", "idx": 689}
{"namespace": "twtxt.config.Config.create_config", "completion": "        cfg = configparser.ConfigParser()\n        cfg.add_section(\"twtxt\")\n        cfg.set(\"twtxt\", \"nick\", nick)\n        cfg.set(\"twtxt\", \"twtfile\", twtfile)\n        cfg.set(\"twtxt\", \"twturl\", twturl)\n        cfg.set(\"twtxt\", \"disclose_identity\", str(disclose_identity))\n\n        if add_news:\n            cfg.add_section(\"following\")\n            cfg.set(\"following\", \"news\", \"https://twtxt.net/user/news.txt\")\n\n        with open(cfgfile, \"w\") as config_file:\n            cfg.write(config_file)\n\n        return cls(cfgfile, cfg)", "idx": 690}
{"namespace": "twtxt.config.Config.following", "completion": "        following_list = []\n        if self.cfg.has_section(\"following\"):\n            for nick, url in self.cfg.items(\"following\"):\n                following_list.append(Source(nick, url))\n        else:\n            logger.debug(\"The 'following' section does not exist in the config.\")\n\n        return following_list", "idx": 691}
{"namespace": "twtxt.config.Config.options", "completion": "        options = {}\n        if self.cfg.has_section(\"twtxt\"):\n            options = dict(self.cfg.items(\"twtxt\"))\n        return options", "idx": 692}
{"namespace": "twtxt.models.Tweet.relative_datetime", "completion": "        now = datetime.now(timezone.utc)\n        delta = humanize.naturaltime(now - self.created_at)\n        return delta", "idx": 693}
{"namespace": "twtxt.mentions.format_mentions", "completion": "\n    def handle_mention(match):\n        name = match.group('name')\n        url = match.group('url')\n        return format_callback(name, url)\n\n    return mention_re.sub(handle_mention, text)", "idx": 694}
{"namespace": "twtxt.parser.parse_tweets", "completion": "        from twtxt.models import Tweet\n    from twtxt.models import Tweet\n    if now is None:\n        now = datetime.now(timezone.utc)\n\n    parsed_tweets = []\n    for raw_tweet in raw_tweets:\n        try:\n            parsed_tweet = parse_tweet(raw_tweet, source, now)\n            parsed_tweets.append(parsed_tweet)\n        except Exception as e:\n            logger.error(f\"Failed to parse tweet: {raw_tweet}. Error: {str(e)}\")\n\n    return parsed_tweets", "idx": 695}
{"namespace": "wikipediaapi.Wikipedia.page", "completion": "        if unquote:\n            title = parse.unquote(title)\n        return WikipediaPage(\n            wiki=self,\n            title=title,\n            ns=ns,\n            language=self.language,\n        )", "idx": 696}
{"namespace": "wikipediaapi.Wikipedia.article", "completion": "        if unquote:\n            title = parse.unquote(title)\n\n        return WikipediaPage(self, title=title, ns=ns, language=self.language)", "idx": 697}
{"namespace": "wikipediaapi.WikipediaPageSection.__repr__", "completion": "        return f\"WikipediaPageSection(title={self.title}, level={self.level}, text={self.text}, sections={len(self.sections)}, subsections={self.sections})\"", "idx": 698}
{"namespace": "wikipediaapi.WikipediaPage.sections", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        return self._section", "idx": 699}
{"namespace": "wikipediaapi.WikipediaPage.section_by_title", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        sections = self._section_mapping.get(title)\n        if sections:\n            return sections[-1]\n        return None", "idx": 700}
{"namespace": "wikipediaapi.WikipediaPage.sections_by_title", "completion": "        if not self._called[\"extracts\"]:\n            self._fetch(\"extracts\")\n        return self._section_mapping.get(title, [])", "idx": 701}
{"namespace": "wikipediaapi.WikipediaPage.text", "completion": "        text = self.summary + \"\\n\\n\"\n        for section in self.sections:\n            text += section.full_text() + \"\\n\\n\"\n        return text.strip()", "idx": 702}
{"namespace": "wikipediaapi.WikipediaPage.langlinks", "completion": "        if not self._called[\"langlinks\"]:\n            self._fetch(\"langlinks\")\n        return self._langlinks", "idx": 703}
{"namespace": "wikipediaapi.WikipediaPage.links", "completion": "        if not self._called[\"links\"]:\n            self._fetch(\"links\")\n        return self._links", "idx": 704}
{"namespace": "wikipediaapi.WikipediaPage.backlinks", "completion": "        if not self._called[\"backlinks\"]:\n            self._fetch(\"backlinks\")\n        return self._backlinks", "idx": 705}
{"namespace": "wikipediaapi.WikipediaPage.categorymembers", "completion": "        if not self._called[\"categorymembers\"]:\n            self._fetch(\"categorymembers\")\n        return self._categorymembers", "idx": 706}
{"namespace": "wikipediaapi.WikipediaPage._fetch", "completion": "        if call in self._called and not self._called[call]:\n            getattr(self.wiki, call)(self)\n            self._called[call] = True\n        return self", "idx": 707}
{"namespace": "wikipediaapi.WikipediaPage.__repr__", "completion": "        if any(self._called.values()):\n            return \"{} (id: {}, ns: {})\".format(self.title, self.pageid, self.namespace)\n        else:\n            return \"{} (id: ??, ns: {})\".format(self.title, self.namespace)", "idx": 708}
{"namespace": "imapclient.imapclient.IMAPClient.starttls", "completion": "        if not self.has_capability(\"STARTTLS\"):\n            raise exceptions.CapabilityError(\"Server does not support STARTTLS\")\n\n        if ssl_context is None:\n            ssl_context = ssl_lib.create_default_context()\n\n        self._command_and_check(\"starttls\", unpack=True)\n\n        # Wrap the socket in an SSL context\n        self._imap.sock = ssl_context.wrap_socket(\n            self._imap.sock,\n            server_hostname=self.host,\n        )\n        self._imap.file = self._imap.sock.makefile(\"rb\")\n        self._starttls_done = True\n\n        # Capability list is now out of date\n        self._cached_capabilities = None\n\n        return self.capabilities()", "idx": 709}
{"namespace": "imapclient.imapclient.IMAPClient.shutdown", "completion": "        try:\n            self._imap.shutdown()\n            logger.info(\"Connection to IMAP server has been closed\")\n        except Exception as e:\n            logger.error(\"Failed to close connection to IMAP server: %s\", e)", "idx": 710}
{"namespace": "imapclient.imapclient.IMAPClient.enable", "completion": "        if not capabilities:\n            raise ValueError(\"No capabilities specified\")\n\n        args = [to_bytes(capability) for capability in capabilities]\n        data = self._command_and_check(\"enable\", *args)\n        return [to_unicode(capability) for capability in parse_response(data)]", "idx": 711}
{"namespace": "imapclient.imapclient.IMAPClient._proc_folder_list", "completion": "        folder_data = [item for item in folder_data if item not in (b\"\", None)]\n        out = []\n        for line in folder_data:\n            flags, delimiter, name = parse_list_response(line)\n            if isinstance(name, int):\n                name = str(name)\n            if self.folder_encode:\n                name = decode_utf7(name)\n            out.append((flags, delimiter, name))\n        return out", "idx": 712}
{"namespace": "imapclient.imapclient.IMAPClient.select_folder", "completion": "        self._check_folder_name(folder)\n        self._imap_class.select_folder(folder, readonly)\n        return self._process_select_response(self._imap_class.untagged_responses)", "idx": 713}
{"namespace": "imapclient.imapclient.IMAPClient.unselect_folder", "completion": "        return self._command_and_check(\"unselect\", unpack=True)", "idx": 714}
{"namespace": "imapclient.imapclient.IMAPClient.noop", "completion": "        tag = self._imap._command(\"NOOP\")\n        return self._consume_until_tagged_response(tag, \"NOOP\")", "idx": 715}
{"namespace": "imapclient.imapclient.IMAPClient.idle", "completion": "        logger.debug(\"< IDLE\")\n        self._idle_tag = self._imap._command(\"IDLE\")\n        resp = self._imap._get_response()\n        if resp is None:\n            raise exceptions.IMAPClientError(\"IDLE command error: no response\")\n        if resp != b\"+ idling\":\n            raise exceptions.IMAPClientError(\n                \"IDLE command error: unexpected response: %s\" % resp\n            )\n        logger.debug(\"* server ready for IDLE\")", "idx": 716}
{"namespace": "imapclient.imapclient.IMAPClient.idle_check", "completion": "        if self._idle_tag is None:\n            raise exceptions.IMAPClientError(\"Not in IDLE state\")\n\n        sock = self.socket()\n        if POLL_SUPPORT:\n            events = self._poll_socket(sock, timeout)\n        else:\n            events = self._select_poll_socket(sock, timeout)\n\n        if not events:\n            return []\n\n        # Server sent some data. Read the response(s)\n        resps = []\n        while True:\n            try:\n                line = self._imap._get_line()\n            except imaplib.IMAP4.abort as e:\n                # An 'abort' response from _get_line means the server\n                # closed the connection\n                raise exceptions.IMAPClientAbortError(str(e))\n\n            if line == b\")\":\n                # The server may send \"* IDLE\" as a continuation when\n                # IDLE mode is entered. This isn't an actual IDLE\n                # response so ignore it.\n                continue\n\n            if line == b\"\":\n                # A blank line indicates the server is done sending\n                # unsolicited responses.\n                break\n\n            if line.startswith(b\"* \"):\n                # Eat the \"* \" at the beginning of the response\n                line = line[2:]\n\n            resps.append(_parse_untagged_response(line))\n\n        return resps", "idx": 717}
{"namespace": "imapclient.imapclient.IMAPClient.idle_done", "completion": "        if self._idle_tag is None:\n            raise exceptions.IMAPClientError(\"Not in IDLE mode\")\n        self._imap.send(b\"DONE\\r\\n\")\n        return self._consume_until_tagged_response(self._idle_tag, \"IDLE\")", "idx": 718}
{"namespace": "imapclient.imapclient.IMAPClient.folder_status", "completion": "        if what is None:\n            what = [\"MESSAGES\", \"RECENT\", \"UIDNEXT\", \"UIDVALIDITY\", \"UNSEEN\"]\n        if isinstance(what, (str, bytes)):\n            what = [what]\n        data = self._command_and_check(\n            \"status\", self._normalise_folder(folder), seq_to_parenstr_upper(what)\n        )\n        return parse_response(data, untagged_response=\"STATUS\")[1]", "idx": 719}
{"namespace": "imapclient.imapclient.IMAPClient.sort", "completion": "        if not self.has_capability(\"SORT\"):\n            raise exceptions.CapabilityError(\"The server does not support the SORT extension\")\n\n        sort_criteria = _normalise_sort_criteria(sort_criteria, charset)\n        criteria = _normalise_search_criteria(criteria, charset)\n\n        args = [sort_criteria] + criteria\n        data = self._raw_command_untagged(b\"SORT\", args, uid=self.use_uid)\n        return parse_message_list(data)", "idx": 720}
{"namespace": "imapclient.imapclient.IMAPClient.thread", "completion": "        if not self.has_capability(\"THREAD=\" + algorithm):\n            raise exceptions.CapabilityError(\n                \"Server does not support {} threading algorithm\".format(algorithm)\n            )\n\n        args = [to_bytes(algorithm), to_bytes(charset)]\n        args.extend(_normalise_search_criteria(criteria, charset))\n\n        data = self._raw_command_untagged(b\"THREAD\", args, uid=False)\n        return parse_response(data)", "idx": 721}
{"namespace": "imapclient.imapclient.IMAPClient.get_flags", "completion": "        response = self.fetch(messages, ['FLAGS'])\n        return self._filter_fetch_dict(response, b'FLAGS')", "idx": 722}
{"namespace": "imapclient.imapclient.IMAPClient.get_gmail_labels", "completion": "        response = self.fetch(messages, [\"X-GM-LABELS\"])\n        return {msg: utf7_decode_sequence(labels) for msg, labels in response.items()}", "idx": 723}
{"namespace": "imapclient.imapclient.IMAPClient.append", "completion": "        if msg_time is None:\n            msg_time = datetime.now()\n        if msg_time.tzinfo is None:\n            msg_time = datetime_to_INTERNALDATE(msg_time)\n        else:\n            msg_time = datetime_to_INTERNALDATE(msg_time.astimezone())\n\n        args = [\n            self._normalise_folder(folder),\n            seq_to_parenstr(flags),\n            msg_time,\n            _literal(to_bytes(msg)),\n        ]\n\n        return self._raw_command_untagged(b\"APPEND\", args, uid=False)", "idx": 724}
{"namespace": "imapclient.imapclient.IMAPClient.multiappend", "completion": "        if not msgs:\n            return {}\n\n        args = [self._normalise_folder(folder)]\n        for msg in msgs:\n            if isinstance(msg, dict):\n                if \"flags\" in msg:\n                    args.append(seq_to_parenstr(msg[\"flags\"]))\n                if \"date\" in msg:\n                    args.append(datetime_to_INTERNALDATE(msg[\"date\"]))\n                args.append(_literal(to_bytes(msg[\"msg\"])))\n            else:\n                args.append(_literal(to_bytes(msg)))\n\n        return self._command_and_check(\"append\", *args, unpack=True)", "idx": 725}
{"namespace": "imapclient.imapclient.IMAPClient.expunge", "completion": "        if messages is not None:\n            if not self.has_capability(\"UIDPLUS\"):\n                raise exceptions.CapabilityError(\"Server does not support UIDPLUS\")\n            return self.uid_expunge(messages)\n        else:\n            typ, data = self._imap.expunge()\n            self._checkok(\"expunge\", typ, data)\n            return self._imap._untagged_response(typ, data, \"EXPUNGE\")", "idx": 726}
{"namespace": "imapclient.imapclient.IMAPClient.getacl", "completion": "        data = self._command_and_check(\"getacl\", self._normalise_folder(folder))\n        response = parse_response(data)\n        return list(as_pairs(response[1:]))", "idx": 727}
{"namespace": "imapclient.imapclient.IMAPClient.setacl", "completion": "        return self._command_and_check(\n            \"setacl\",\n            self._normalise_folder(folder),\n            to_bytes(who),\n            to_bytes(what),\n            unpack=True,\n        )", "idx": 728}
{"namespace": "imapclient.imapclient.IMAPClient.get_quota_root", "completion": "        response = self._command_and_check(\"getquotaroot\", self._normalise_folder(mailbox))\n        quota_root = parse_response(response)\n        mailbox_quota_roots = MailboxQuotaRoots(mailbox=mailbox, quota_roots=quota_root)\n        quotas = self._get_quota(quota_root)\n        return mailbox_quota_roots, quotas", "idx": 729}
{"namespace": "imapclient.imapclient.IMAPClient.set_quota", "completion": "        for quota in quotas:\n            quota_root = _quote(quota.quota_root)\n            quota_resource = _quote(quota.resource)\n            quota_limit = str(quota.limit).encode(\"ascii\")\n            self._command_and_check(\"setquota\", quota_root, \"(%s %s)\" % (quota_resource, quota_limit))", "idx": 730}
{"namespace": "imapclient.imapclient.IMAPClient._consume_until_tagged_response", "completion": "        untagged_responses = []\n        while True:\n            typ, data = self._imap._get_response()\n            if typ == \"TAGGED\":\n                if data[0] != tag:\n                    raise exceptions.IMAPClientError(\"Unexpected tagged response: %s\" % data[0])\n                self._check_resp(\"OK\", command, typ, data[1:])\n                return data[1], untagged_responses\n            elif typ == \"RESPONSE\":\n                untagged_responses.append(data)\n            else:\n                raise exceptions.IMAPClientError(\"Unexpected response: %s %s\" % (typ, data))", "idx": 731}
{"namespace": "imapclient.imapclient._normalise_search_criteria", "completion": "    if isinstance(criteria, (str, bytes)):\n        criteria = [criteria]\n    elif isinstance(criteria, (list, tuple)):\n        new_criteria = []\n        for item in criteria:\n            if isinstance(item, (list, tuple)):\n                new_criteria.append(_normalise_search_criteria(item, charset))\n            else:\n                new_criteria.append(_normalise_search_item(item, charset))\n        criteria = new_criteria\n    else:\n        raise ValueError(\"search criteria must be string, bytes, or a sequence\")\n\n    return criteria", "idx": 732}
{"namespace": "imapclient.response_lexer.TokenSource.current_literal", "completion": "        if self.lex.current_source is not None:\n            return self.lex.current_source.literal\n        return None", "idx": 733}
{"namespace": "imapclient.imap_utf7.decode", "completion": "    if not isinstance(s, (bytes, str)):\n        return s\n\n    if isinstance(s, str):\n        s = s.encode(\"ascii\")\n\n    res = []\n    buffer = bytearray()\n\n    for c in s:\n        if c == AMPERSAND_ORD:\n            if buffer:\n                res.append(base64_utf7_decode(buffer))\n                buffer = bytearray()\n            continue\n        elif c == DASH_ORD and buffer:\n            res.append(base64_utf7_decode(buffer))\n            buffer = bytearray()\n            continue\n        elif c == DASH_ORD:  # &-\n            res.append(\"&\")\n            continue\n\n        buffer.append(c)\n\n    if buffer:\n        res.append(base64_utf7_decode(buffer))\n\n    return \"\".join(res)", "idx": 734}
{"namespace": "imapclient.fixed_offset.FixedOffset.for_system", "completion": "        if time.daylight and time.localtime().tm_isdst:\n            offset_minutes = time.altzone / 60\n        else:\n            offset_minutes = time.timezone / 60\n        return cls(offset_minutes)", "idx": 735}
{"namespace": "imapclient.datetime_util.parse_to_datetime", "completion": "    dt_tuple = parsedate_tz(_munge(timestamp))\n    if dt_tuple is None:\n        raise ValueError(\"Unable to parse timestamp string: %r\" % timestamp)\n    dt = datetime(*dt_tuple[:6])\n    if dt_tuple[9] is not None:\n        dt = dt.replace(tzinfo=FixedOffset.from_seconds(dt_tuple[9]))\n    if normalise:\n        dt = datetime_to_native(dt)\n    return dt", "idx": 736}
{"namespace": "imapclient.datetime_util.datetime_to_INTERNALDATE", "completion": "    if dt.tzinfo is None:\n        dt = dt.replace(tzinfo=FixedOffset.for_system())\n    return dt.strftime('\"%d-%b-%Y %H:%M:%S %z\"')", "idx": 737}
{"namespace": "imapclient.datetime_util.format_criteria_date", "completion": "    if not dt.tzinfo:\n        dt = dt.replace(tzinfo=FixedOffset.for_system())\n    fmt = \"%d-\" + _SHORT_MONTHS[dt.month] + \"-%Y\"\n    return dt.strftime(fmt).encode('utf-8')", "idx": 738}
{"namespace": "imapclient.util.assert_imap_protocol", "completion": "    if not condition:\n        error_message = \"Server replied with a response that violates the IMAP protocol\"\n        if message:\n            error_message += \": \" + to_unicode(message)\n        raise ProtocolError(error_message)", "idx": 739}
{"namespace": "ehforwarderbot.utils.get_config_path", "completion": "    profile = coordinator.profile\n    if module_id is None:\n        config_path = get_base_path() / 'profiles' / profile / f'config.{ext}'\n    else:\n        config_path = get_base_path() / 'profiles' / profile / module_id / f'config.{ext}'\n    if not config_path.parent.exists():\n        config_path.parent.mkdir(parents=True)\n    return config_path", "idx": 740}
{"namespace": "ehforwarderbot.utils.get_custom_modules_path", "completion": "    base_path = get_base_path()\n    custom_modules_path = base_path / \"modules\"\n    if not custom_modules_path.exists():\n        custom_modules_path.mkdir(parents=True)\n    return custom_modules_path", "idx": 741}
{"namespace": "ehforwarderbot.chat.Chat.add_member", "completion": "        member = ChatMember(self, name=name, alias=alias, id=id, uid=uid,\n                            vendor_specific=vendor_specific, description=description,\n                            middleware=middleware)\n        self.members.append(member)\n        return member", "idx": 742}
{"namespace": "ehforwarderbot.chat.Chat.add_system_member", "completion": "        if id:\n            warnings.warn(\"`id` argument is deprecated, use `uid` instead.\", DeprecationWarning)\n            uid = uid or id\n        member = SystemChatMember(self, name=name, alias=alias, uid=uid,\n                                  vendor_specific=vendor_specific, description=description,\n                                  middleware=middleware)\n        self.members.append(member)\n        return member", "idx": 743}
{"namespace": "ehforwarderbot.chat.PrivateChat.verify", "completion": "        super().verify()\n        assert all(isinstance(member, ChatMember) for member in self.members)", "idx": 744}
{"namespace": "telethon.utils.get_inner_text", "completion": "    result = []\n    for entity in entities:\n        start = entity.offset\n        end = start + entity.length\n        result.append(text[start:end])\n    return result", "idx": 745}
{"namespace": "telethon.extensions.html.parse", "completion": "    parser = HTMLToTelegramParser()\n    parser.feed(html)\n    parser.close()\n    return parser.text, parser.entities", "idx": 746}
{"namespace": "telethon.extensions.html.unparse", "completion": "    if not text or not entities:\n        return escape(text)\n\n    entities = sorted(entities, key=lambda e: (e.offset, -e.length))\n    result = []\n    current_offset = 0\n\n    for entity in entities:\n        if entity.offset > current_offset:\n            result.append(escape(text[current_offset:entity.offset]))\n            current_offset = entity.offset\n\n        formatter = ENTITY_TO_FORMATTER[type(entity)]\n        if callable(formatter):\n            open_tag, close_tag = formatter(entity, text[entity.offset:entity.offset + entity.length])\n        else:\n            open_tag, close_tag = formatter\n\n        result.append(open_tag)\n        result.append(escape(text[entity.offset:entity.offset + entity.length]))\n        result.append(close_tag)\n\n        current_offset = entity.offset + entity.length\n\n    if current_offset < len(text):\n        result.append(escape(text[current_offset:]))\n\n    return ''.join(result)", "idx": 747}
{"namespace": "telethon.crypto.rsa.encrypt", "completion": "    global _server_keys\n    key, old = _server_keys.get(fingerprint, (None, None))\n    if not key or old != use_old:\n        return None\n\n    # RSA requires the message to be of the same length as the key\n    # (256 bytes for 2048-bit keys, a byte less for padding)\n    padding = os.urandom(255 - len(data) - 20)  # 20 for the SHA1 hash\n    plaintext = sha1(data).digest() + data + padding\n\n    # rsa module uses a transformation from a byte string to a number\n    # for encryption, and the reverse for decryption. We need to\n    # manually perform the reverse operation to get the original byte string.\n    encrypted = rsa.core.encrypt_int(\n        int.from_bytes(plaintext, byteorder='big', signed=False),\n        key.e, key.n)\n    block = int.to_bytes(encrypted, 256, byteorder='big', signed=False)\n\n    return block", "idx": 748}
{"namespace": "hbmqtt.codecs.encode_string", "completion": "    encoded_string = string.encode('utf-8')\n    length = len(encoded_string)\n    return int_to_bytes(length, 2) + encoded_string", "idx": 749}
{"namespace": "hbmqtt.plugins.manager.PluginManager.get_plugin", "completion": "        for plugin in self._plugins:\n            if plugin.name == name:\n                return plugin\n        return None", "idx": 750}
{"namespace": "pysimplesoap.simplexml.SimpleXMLElement.add_child", "completion": "        if ns is True:\n            ns = self.__ns\n        elif ns is False:\n            ns = None\n        if ns:\n            element = self.__document.createElementNS(ns, name)\n        else:\n            element = self.__document.createElement(name)\n        if text is not None:\n            if isinstance(text, xml.dom.minidom.CDATASection):\n                element.appendChild(text)\n            else:\n                text_node = self.__document.createTextNode(text)\n                element.appendChild(text_node)\n        self._element.appendChild(element)\n        return SimpleXMLElement(elements=[element], document=self.__document, namespace=self.__ns, prefix=self.__prefix, jetty=self.__jetty, namespaces_map=self.__namespaces_map)", "idx": 751}
{"namespace": "pysimplesoap.simplexml.SimpleXMLElement.as_xml", "completion": "        if pretty:\n            xml = self.__document.toprettyxml(indent=\"  \")\n        else:\n            xml = self.__document.toxml()\n\n        if filename:\n            with open(filename, 'w') as f:\n                f.write(xml)\n\n        return xml", "idx": 752}
{"namespace": "twilio.base.deserialize.iso8601_date", "completion": "    try:\n        return datetime.datetime.strptime(s, ISO8601_DATE_FORMAT).date()\n    except (TypeError, ValueError):\n        return s", "idx": 753}
{"namespace": "twilio.base.deserialize.iso8601_datetime", "completion": "    try:\n        return (\n            datetime.datetime.strptime(s, ISO8601_DATETIME_FORMAT)\n            .replace(tzinfo=datetime.timezone.utc)\n        )\n    except (TypeError, ValueError):\n        return s", "idx": 754}
{"namespace": "twilio.base.serialize.iso8601_date", "completion": "    if d == values.unset:\n        return d\n    elif isinstance(d, datetime.datetime) or isinstance(d, datetime.date):\n        return d.strftime(\"%Y-%m-%d\")\n    elif isinstance(d, str):\n        return d\n    return None", "idx": 755}
{"namespace": "twilio.base.serialize.iso8601_datetime", "completion": "    if d == values.unset:\n        return d\n    elif isinstance(d, datetime.datetime):\n        return d.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n    elif isinstance(d, datetime.date):\n        return d.strftime(\"%Y-%m-%d\")\n    elif isinstance(d, str):\n        return d\n    else:\n        return None", "idx": 756}
{"namespace": "twilio.base.serialize.prefixed_collapsible_map", "completion": "    if not isinstance(m, dict):\n        return m\n    return {f\"{prefix}{k}\": v for k, v in m.items()}", "idx": 757}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.dial", "completion": "        return self.nest(\n            Dial(\n                number=number,\n                action=action,\n                method=method,\n                timeout=timeout,\n                hangup_on_star=hangup_on_star,\n                time_limit=time_limit,\n                caller_id=caller_id,\n                record=record,\n                trim=trim,\n                recording_status_callback=recording_status_callback,\n                recording_status_callback_method=recording_status_callback_method,\n                recording_status_callback_event=recording_status_callback_event,\n                answer_on_bridge=answer_on_bridge,\n                ring_tone=ring_tone,\n                recording_track=recording_track,\n                sequential=sequential,\n                refer_url=refer_url,\n                refer_method=refer_method,\n                **kwargs\n            )\n        )", "idx": 758}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.enqueue", "completion": "        return self.nest(\n            Enqueue(\n                name,\n                action=action,\n                max_queue_size=max_queue_size,\n                method=method,\n                wait_url=wait_url,\n                wait_url_method=wait_url_method,\n                workflow_sid=workflow_sid,\n                **kwargs\n            )\n        )", "idx": 759}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.gather", "completion": "        return self.nest(\n            Gather(\n                input=input,\n                action=action,\n                method=method,\n                timeout=timeout,\n                speech_timeout=speech_timeout,\n                max_speech_time=max_speech_time,\n                profanity_filter=profanity_filter,\n                finish_on_key=finish_on_key,\n                num_digits=num_digits,\n                partial_result_callback=partial_result_callback,\n                partial_result_callback_method=partial_result_callback_method,\n                language=language,\n                hints=hints,\n                barge_in=barge_in,\n                debug=debug,\n                action_on_empty_result=action_on_empty_result,\n                speech_model=speech_model,\n                enhanced=enhanced,\n                **kwargs\n            )\n        )", "idx": 760}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.say", "completion": "        return self.nest(\n            Say(message=message, voice=voice, loop=loop, language=language, **kwargs)\n        )", "idx": 761}
{"namespace": "twilio.twiml.voice_response.VoiceResponse.sms", "completion": "        return self.nest(\n            Sms(\n                message,\n                to=to,\n                from_=from_,\n                action=action,\n                method=method,\n                status_callback=status_callback,\n                **kwargs\n            )\n        )", "idx": 762}
{"namespace": "twilio.twiml.voice_response.Gather.say", "completion": "        return self.nest(\n            Say(message=message, voice=voice, loop=loop, language=language, **kwargs)\n        )", "idx": 763}
{"namespace": "twilio.twiml.voice_response.Dial.client", "completion": "        return self.nest(\n            Client(\n                identity=identity,\n                url=url,\n                method=method,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                **kwargs\n            )\n        )", "idx": 764}
{"namespace": "twilio.twiml.voice_response.Dial.conference", "completion": "        return self.nest(\n            Conference(\n                name,\n                muted=muted,\n                beep=beep,\n                start_conference_on_enter=start_conference_on_enter,\n                end_conference_on_exit=end_conference_on_exit,\n                wait_url=wait_url,\n                wait_method=wait_method,\n                max_participants=max_participants,\n                record=record,\n                region=region,\n                coach=coach,\n                trim=trim,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                recording_status_callback=recording_status_callback,\n                recording_status_callback_method=recording_status_callback_method,\n                recording_status_callback_event=recording_status_callback_event,\n                event_callback_url=event_callback_url,\n                jitter_buffer_size=jitter_buffer_size,\n                participant_label=participant_label,\n                **kwargs\n            )\n        )", "idx": 765}
{"namespace": "twilio.twiml.voice_response.Dial.queue", "completion": "        return self.nest(\n            Queue(\n                name,\n                url=url,\n                method=method,\n                reservation_sid=reservation_sid,\n                post_work_activity_sid=post_work_activity_sid,\n                **kwargs\n            )\n        )", "idx": 766}
{"namespace": "twilio.twiml.voice_response.Dial.sip", "completion": "        return self.nest(\n            Sip(\n                sip_url,\n                username=username,\n                password=password,\n                url=url,\n                method=method,\n                status_callback_event=status_callback_event,\n                status_callback=status_callback,\n                status_callback_method=status_callback_method,\n                machine_detection=machine_detection,\n                amd_status_callback_method=amd_status_callback_method,\n                amd_status_callback=amd_status_callback,\n                machine_detection_timeout=machine_detection_timeout,\n                machine_detection_speech_threshold=machine_detection_speech_threshold,\n                machine_detection_speech_end_threshold=machine_detection_speech_end_threshold,\n                machine_detection_silence_timeout=machine_detection_silence_timeout,\n                **kwargs\n            )\n        )", "idx": 767}
{"namespace": "twilio.twiml.messaging_response.MessagingResponse.message", "completion": "        message = Message(body=body, **kwargs)\n        if to:\n            message.set('to', to)\n        if from_:\n            message.set('from', from_)\n        if action:\n            message.set('action', action)\n        if method:\n            message.set('method', method)\n        if status_callback:\n            message.set('statusCallback', status_callback)\n        return self.nest(message)", "idx": 768}
{"namespace": "twilio.twiml.TwiML.append", "completion": "        if not isinstance(verb, TwiML) and not isinstance(verb, str):\n            raise TwiMLException(\"Only appending of TwiML and strings are allowed\")\n\n        self.verbs.append(verb)\n        return self", "idx": 769}
{"namespace": "twilio.jwt.Jwt.to_jwt", "completion": "        if not self.secret_key:\n            raise ValueError(\"No secret key configured for JWT\")\n\n        headers = self.headers.copy()\n        payload = self.payload.copy()\n\n        if ttl:\n            payload[\"exp\"] = int(time.time()) + ttl\n\n        return jwt_lib.encode(payload, self.secret_key, algorithm=self.algorithm, headers=headers)", "idx": 770}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_client_outgoing", "completion": "        params = {\"appSid\": application_sid}\n        if kwargs:\n            params.update(kwargs)\n        self.capabilities[\"outgoing\"] = ScopeURI(\"client\", \"outgoing\", params)", "idx": 771}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_client_incoming", "completion": "        self.client_name = client_name\n        scope = ScopeURI(\"client\", \"incoming\", {\"clientName\": client_name})\n        self.capabilities[\"incoming\"] = scope", "idx": 772}
{"namespace": "twilio.jwt.client.ClientCapabilityToken.allow_event_stream", "completion": "        scope = ScopeURI(\"event_stream\", \"all\", kwargs)\n        self.capabilities[\"event_stream\"] = scope", "idx": 773}
{"namespace": "twilio.jwt.client.ClientCapabilityToken._generate_payload", "completion": "        if \"outgoing\" in self.capabilities and self.client_name is not None:\n            self.capabilities[\"outgoing\"].add_param(\"clientName\", self.client_name)\n\n        scope_uris = [cap.to_payload() for cap in self.capabilities.values()]\n\n        return {\"scope\": \" \".join(scope_uris)}", "idx": 774}
{"namespace": "twilio.jwt.client.ScopeURI.to_payload", "completion": "        if self.params:\n            params = sorted(self.params.items())\n            param_str = \"?\" + urlencode(params, doseq=True)\n        else:\n            param_str = \"\"\n\n        return \"scope:{}:{}{}\".format(self.service, self.privilege, param_str)", "idx": 775}
{"namespace": "twilio.jwt.access_token.AccessToken.add_grant", "completion": "        if not isinstance(grant, AccessTokenGrant):\n            raise ValueError(\"Grant must be an instance of AccessTokenGrant.\")\n        self.grants.append(grant)", "idx": 776}
{"namespace": "twilio.jwt.taskrouter.capabilities.WorkerCapabilityToken.allow_update_activities", "completion": "        self._make_policy(self.resource_url, \"POST\", True, post_filter={\"ActivitySid\": {\"required\": True}})", "idx": 777}
{"namespace": "zulipterminal.platform_code.successful_GUI_return_code", "completion": "    if PLATFORM == \"WSL\":\n        return 1\n    else:\n        return 0", "idx": 778}
{"namespace": "zulipterminal.platform_code.normalized_file_path", "completion": "    if PLATFORM == \"WSL\":\n        return path.replace('/', '\\\\')\n    else:\n        return path", "idx": 779}
{"namespace": "zulipterminal.helper.canonicalize_color", "completion": "    if match(r\"^#[0-9a-fA-F]{6}$\", color):\n        return '#' + ''.join(color[i] for i in range(1, 6, 2)).lower()\n    elif match(r\"^#[0-9a-fA-F]{3}$\", color):\n        return color.lower()\n    else:\n        raise ValueError(\"Invalid color format. Expected '#xxxxxx' or '#xxx'.\")", "idx": 780}
{"namespace": "zulipterminal.helper.get_unused_fence", "completion": "    fence_pattern = r\"`+\"\n    fences_in_content = findall(fence_pattern, content, MULTILINE)\n    if fences_in_content:\n        max_fence_length = max(len(fence) for fence in fences_in_content)\n    else:\n        max_fence_length = 0\n    return \"`\" * (max_fence_length + 1)", "idx": 781}
{"namespace": "zulipterminal.helper.open_media", "completion": "    command = [tool, media_path]\n    try:\n        process = subprocess.run(command, check=True)\n    except subprocess.CalledProcessError as e:\n        controller.report_error(f\"Failed to open media: {e}\")\n    except FileNotFoundError as e:\n        controller.report_error(f\"Cannot find tool: {e}\")\n    else:\n        if process.returncode != 0:\n            controller.report_error(f\"Failed to open media, return code: {process.returncode}\")", "idx": 782}
{"namespace": "zulipterminal.server_url.encode_stream", "completion": "    # Replace whitespace with hyphen\n    stream_name = stream_name.replace(\" \", \"-\")\n    # Encode the stream name\n    encoded_stream_name = hash_util_encode(stream_name)\n    # Return the encoded string prefixed with the stream id\n    return str(stream_id) + \"-\" + encoded_stream_name", "idx": 783}
{"namespace": "zulipterminal.server_url.near_message_url", "completion": "    if message['type'] == 'stream':\n        return near_stream_message_url(server_url, message)\n    else:\n        return near_pm_message_url(server_url, message)", "idx": 784}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.update_recipients", "completion": "        recipient_emails = [\n            email.strip()\n            for email in write_box.edit_text.split(\",\")\n            if email.strip()  # This condition avoids whitespace recipients (\",  ,\")\n        ]\n        recipient_user_ids = [\n            self.model.user_dict[email][\"user_id\"]\n            for email in recipient_emails\n            if email in self.model.user_dict\n        ]\n        self._set_regular_and_typing_recipient_user_ids(recipient_user_ids)", "idx": 785}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.stream_box_view", "completion": "        self.stream_write_box = ReadlineEdit(\n            caption=stream_id, edit_text=caption, max_char=self.model.max_stream_name_length\n        )\n        self.stream_write_box.enable_autocomplete(\n            func=self._stream_box_autocomplete,\n            key=primary_key_for_command(\"AUTOCOMPLETE\"),\n            key_reverse=primary_key_for_command(\"AUTOCOMPLETE_REVERSE\"),\n        )\n        self.stream_write_box.set_completer_delims(\"\")\n\n        self._setup_common_stream_compose(stream_id, caption, title)\n\n        # Use callback to set stream marker - it shouldn't change, so don't need signal\n        self._set_stream_write_box_style(None, caption)\n\n        # Connect signal to update stream write box style\n        urwid.connect_signal(\n            self.stream_write_box, \"change\", self._set_stream_write_box_style\n        )", "idx": 786}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.stream_box_edit_view", "completion": "        self.stream_write_box = ReadlineEdit(\n            edit_text=caption, max_char=self.model.max_stream_name_length\n        )\n        self.stream_write_box.enable_autocomplete(\n            func=self._stream_box_autocomplete,\n            key=primary_key_for_command(\"AUTOCOMPLETE\"),\n            key_reverse=primary_key_for_command(\"AUTOCOMPLETE_REVERSE\"),\n        )\n        self.stream_write_box.set_completer_delims(\"\")\n        self._setup_common_stream_compose(stream_id, caption, title)\n\n        self.edit_mode_button = EditModeButton(self)\n        self.header_write_box.contents.append(\n            (self.edit_mode_button, self.header_write_box.options(\"pack\"))\n        )\n\n        # Use and set a callback to set the stream marker\n        self._set_stream_write_box_style(None, caption)\n        urwid.connect_signal(\n            self.stream_write_box, \"change\", self._set_stream_write_box_style\n        )", "idx": 787}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._set_stream_write_box_style", "completion": "        stream_marker = self.header_write_box[self.FOCUS_HEADER_PREFIX_STREAM]\n        if new_text == \"\":\n            stream_marker.set_text((\"default\", \"?\"))\n            return\n\n        stream = self.model.stream_dict.get(new_text)\n        if stream is not None:\n            color = stream[\"color\"]\n            stream_marker.set_text((\"s\" + color, STREAM_TOPIC_SEPARATOR))\n        else:\n            stream_marker.set_text((\"default\", \"?\"))", "idx": 788}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._to_box_autocomplete", "completion": "        users_list = self.view.users\n        recipients = text.split(\", \")\n\n        latest_recipient = recipients[-1]\n        matching_users = [\n            user for user in users_list if match_user(user, latest_recipient)\n        ]\n\n        # Construct the text with the autocompleted recipient added\n        autocompleted_recipients = \", \".join(recipients[:-1] + [matching_users[state]])\n\n        user_names = [user[\"full_name\"] for user in matching_users]\n\n        return self._process_typeaheads(autocompleted_recipients, state, user_names)", "idx": 789}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._topic_box_autocomplete", "completion": "        topic_names = self.model.topics_in_stream(self.stream_id)\n        topic_typeahead = format_string(topic_names, \"{}\")\n        topic_data = list(zip(topic_typeahead, topic_names))\n\n        matched_topics = match_topics(topic_data, text)\n\n        return self._process_typeaheads(matched_topics[0], state, matched_topics[1])", "idx": 790}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox._stream_box_autocomplete", "completion": "        streams_list = self.view.streams\n        matching_streams = [\n            stream for stream in streams_list if match_stream(stream, text)\n        ]\n        stream_typeaheads = format_string(matching_streams, \"#**{}**\")\n        return self._process_typeaheads(stream_typeaheads, state, matching_streams)", "idx": 791}
{"namespace": "zulipterminal.ui_tools.boxes.WriteBox.generic_autocomplete", "completion": "        # Initialize autocomplete parameters\n        autocomplete_map = OrderedDict([\n            (\"@_\", self.autocomplete_mentions),\n            (\"@**\", self.autocomplete_mentions),\n            (\"@*\", self.autocomplete_mentions),\n            (\"@\", self.autocomplete_mentions),\n            (\"#**\", self.autocomplete_streams),\n            (\"#\", self.autocomplete_streams),\n            (\":\", self.autocomplete_emojis),\n            (\"```\", self.autocomplete_language_list),\n        ])\n\n        # Initialize prefix indices\n        prefix_indices = {prefix: text.find(prefix) for prefix in autocomplete_map}\n\n        # Validate and patch autocomplete stream and topic\n        text = self.validate_and_patch_autocomplete_stream_and_topic(\n            text, autocomplete_map, prefix_indices\n        )\n\n        # Get valid prefixes\n        valid_prefixes = [\n            prefix for prefix, index in prefix_indices.items() if index != -1\n        ]\n\n        if valid_prefixes:\n            prefix = min(valid_prefixes, key=prefix_indices.get)\n            autocomplete_func = autocomplete_map[prefix]\n            prefix_index = prefix_indices[prefix]\n            autocomplete_list, matched_text = autocomplete_func(\n                text[prefix_index:], prefix\n            )\n            return self._process_typeaheads(autocomplete_list, state, matched_text)\n\n        return None", "idx": 792}
{"namespace": "zulipterminal.ui_tools.boxes.PanelSearchBox.reset_search_text", "completion": "        self.set_caption(self.search_text)\n        self.set_edit_text(\"\")", "idx": 793}
{"namespace": "zulipterminal.ui_tools.boxes.PanelSearchBox.valid_char", "completion": "        if self.get_edit_text():\n            return super().valid_char(ch)\n        else:\n            return (\n                unicodedata.category(ch)[0] not in [\"C\", \"Z\"]\n                and ch in string.printable\n            )", "idx": 794}
{"namespace": "zulipterminal.ui_tools.utils.is_muted", "completion": "    if msg[\"type\"] == \"private\":\n        return False\n    if model.narrow and model.narrow[0][1] == msg[\"display_recipient\"]:\n        return False\n    if model.is_muted_stream(msg[\"stream_id\"]):\n        return True\n    if model.is_muted_topic(msg[\"stream_id\"], msg[\"subject\"]):\n        return True\n    return False", "idx": 795}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.update_count", "completion": "        if text_color is None:\n            text_color = self.original_color\n        if count == 0:\n            count_text = (\"\", \"\")\n        else:\n            count_text = (self.count_style, str(count))\n        self.update_widget(count_text, text_color)\n        self.count = count", "idx": 796}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.update_widget", "completion": "        prefix, main_text = count_text\n        if prefix is not None:\n            self.button_prefix.set_text([prefix, \" \"])\n        self._label.set_text((text_color, main_text))\n        self.button_suffix.set_text((text_color, self._caption))", "idx": 797}
{"namespace": "zulipterminal.ui_tools.buttons.TopButton.keypress", "completion": "        if is_command_key(\"ENTER\", key):\n            self.activate(None)\n            return None\n        return super().keypress(size, key)", "idx": 798}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._parse_narrow_link", "completion": "        parsed_link = ParsedNarrowLink()\n        fragment = urlparse(link).fragment\n        if not fragment.startswith(\"narrow\"):\n            return parsed_link\n\n        # Extract the narrow type and the optional message ID.\n        fragment_parts = fragment.split(\"/\")\n        narrow = \":\".join(fragment_parts[1:-1:2])\n        message_id = cls._decode_message_id(fragment_parts[-1]) if \"near\" in narrow else None\n\n        # Extract the stream data and topic name.\n        if \"stream\" in narrow:\n            parsed_link[\"stream\"] = cls._decode_stream_data(fragment_parts[2])\n            if \"topic\" in narrow:\n                parsed_link[\"topic_name\"] = hash_util_decode(fragment_parts[4])\n\n        parsed_link[\"narrow\"] = narrow\n        parsed_link[\"message_id\"] = message_id\n        return parsed_link", "idx": 799}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._validate_and_patch_stream_data", "completion": "        stream_id = parsed_link[\"stream\"][\"stream_id\"]\n        stream_name = parsed_link[\"stream\"][\"stream_name\"]\n\n        if stream_id is not None:\n            # Validate stream ID.\n            if not self.model.is_user_subscribed_to_stream(stream_id):\n                return \"Invalid stream ID\"\n\n            # Patch the stream name.\n            if stream_name is None:\n                parsed_link[\"stream\"][\"stream_name\"] = self.model.stream_dict[stream_id][\"name\"]\n        else:\n            # Validate stream name.\n            if not self.model.is_valid_stream(stream_name):\n                return \"Invalid stream name\"\n\n            # Patch the stream ID.\n            parsed_link[\"stream\"][\"stream_id\"] = self.model.stream_id_from_name(stream_name)\n\n        return \"\"", "idx": 800}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._validate_narrow_link", "completion": "        if not parsed_link:\n            return \"The link is not a narrow link\"\n\n        if \"stream\" in parsed_link:\n            stream_error = self._validate_and_patch_stream_data(parsed_link)\n            if stream_error:\n                return stream_error\n\n        if \"topic_name\" in parsed_link and not parsed_link[\"topic_name\"]:\n            return \"The topic name is missing\"\n\n        if \"message_id\" in parsed_link and not parsed_link[\"message_id\"]:\n            return \"The message ID is missing\"\n\n        return \"\"", "idx": 801}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._switch_narrow_to", "completion": "        narrow = parsed_link[\"narrow\"]\n        stream_id = parsed_link[\"stream\"][\"stream_id\"]\n        stream_name = parsed_link[\"stream\"][\"stream_name\"]\n        topic_name = parsed_link.get(\"topic_name\")\n        message_id = parsed_link.get(\"message_id\")\n\n        if narrow == \"stream\":\n            self.controller.narrow_to_stream(stream_name)\n        elif narrow == \"stream:topic\":\n            self.controller.narrow_to_topic(stream_name, topic_name)\n        elif narrow == \"stream:near\":\n            self.controller.narrow_to_stream_and_focus_message(stream_name, message_id)\n        elif narrow == \"stream:topic:near\":\n            self.controller.narrow_to_topic_and_focus_message(stream_name, topic_name, message_id)", "idx": 802}
{"namespace": "zulipterminal.config.themes.complete_and_incomplete_themes", "completion": "    complete_themes = []\n    incomplete_themes = []\n\n    for theme_name, theme in THEMES.items():\n        if all(style in theme.STYLES for style in REQUIRED_STYLES) and all(meta in theme.META for meta in REQUIRED_META):\n            complete_themes.append(theme_name)\n        else:\n            incomplete_themes.append(theme_name)\n\n    return sorted(complete_themes), sorted(incomplete_themes)", "idx": 803}
{"namespace": "zulipterminal.config.themes.validate_colors", "completion": "\n    theme_styles = THEMES[theme_name].STYLES\n    invalid_colors = []\n\n    for style_name, (fg, bg) in theme_styles.items():\n        fg_code16, _, _, *fg_props = fg.value.split()\n        bg_code16, _, _, *bg_props = bg.value.split()\n\n        if color_depth == 16:\n            if fg_code16 not in valid_16_color_codes:\n                invalid_colors.append(fg_code16)\n            if bg_code16 not in valid_16_color_codes:\n                invalid_colors.append(bg_code16)\n\n    if invalid_colors:\n        raise InvalidThemeColorCode(f\"Invalid color codes in theme '{theme_name}': {invalid_colors}\")", "idx": 804}
{"namespace": "zulipterminal.config.themes.parse_themefile", "completion": "    urwid_theme = []\n    for style_name, style_conf in theme_styles.items():\n        fg, bg = style_conf\n        if color_depth == 1:\n            fg = bg = \"default\"\n        elif color_depth == 16:\n            fg = fg.split()[0]\n            bg = bg.split()[0]\n        elif color_depth == 256:\n            fg = fg.split()[1]\n            bg = bg.split()[1]\n        else:  # 2^24\n            fg = fg.split()[2]\n            bg = bg.split()[2]\n        mono = REQUIRED_STYLES[style_name]\n        urwid_theme.append((style_name, mono, fg, bg))\n    return urwid_theme", "idx": 805}
{"namespace": "zulipterminal.config.themes.add_pygments_style", "completion": "    pygments_styles = theme_meta.get('pygments', {}).get('styles', {})\n    for token, style in pygments_styles.items():\n        if token in STANDARD_TYPES:\n            urwid_theme.append((f'pygments:{token}',) + style)\n\n    pygments_background = theme_meta.get('pygments', {}).get('background', '')\n    if pygments_background:\n        urwid_theme.append(('pygments:background',) + pygments_background)\n\n    pygments_overrides = theme_meta.get('pygments', {}).get('overrides', {})\n    for token, style in pygments_overrides.items():\n        urwid_theme.append((f'pygments:{token}',) + style)", "idx": 806}
{"namespace": "zulipterminal.config.keys.is_command_key", "completion": "    try:\n        return key in KEY_BINDINGS[command][\"keys\"]\n    except KeyError:\n        return False", "idx": 807}
{"namespace": "zulipterminal.config.keys.keys_for_command", "completion": "    try:\n        return KEY_BINDINGS[command][\"keys\"]\n    except KeyError as exception:\n        raise InvalidCommand(command)", "idx": 808}
{"namespace": "zulipterminal.config.keys.commands_for_random_tips", "completion": "    return [\n        command\n        for command in KEY_BINDINGS.values()\n        if not command.get('excluded_from_random_tips', False)\n    ]", "idx": 809}
{"namespace": "hypertools.datageometry.DataGeometry.transform", "completion": "                    from .transformers import transform as transformer\n        if data is None:\n            return self.xform_data\n        else:\n            # import the transform function here to avoid circular imports\n            from .transformers import transform as transformer\n            return transformer(data, reduce=self.reduce, align=self.align, normalize=self.normalize)", "idx": 810}
{"namespace": "hypertools.datageometry.DataGeometry.plot", "completion": "                from .plot import plot as hyp_plot\n        from .plot import plot as hyp_plot\n\n        if data is None:\n            data = self.xform_data\n\n        # merge kwargs from plot and DataGeometry object\n        all_kwargs = {**self.kwargs, **kwargs}\n\n        return hyp_plot(data, **all_kwargs)", "idx": 811}
{"namespace": "awesome_autodl.autodl_topic2papers", "completion": "        from awesome_autodl.data_cls import AutoDLpapers\n    from awesome_autodl.data_cls import AutoDLpapers\n\n    topic2path = autodl_topic2path()\n    topic2papers = OrderedDict()\n    for topic, path in topic2path.items():\n        topic2papers[topic] = AutoDLpapers(str(path))\n    return topic2papers", "idx": 812}
{"namespace": "awesome_autodl.get_bib_abbrv_obj", "completion": "        from awesome_autodl.data_cls import BibAbbreviations\n    from awesome_autodl.data_cls import BibAbbreviations\n\n    bib_abbrv_file = get_bib_abbrv_file()\n    if not bib_abbrv_file.exists():\n        raise ValueError(f\"Can not find the bib abbreviations file at {bib_abbrv_file}.\")\n    bib_abbrv_obj = BibAbbreviations(bib_abbrv_file)\n    return bib_abbrv_obj", "idx": 813}
{"namespace": "mssqlcli.localized_strings.translation", "completion": "    if languages is None:\n        languages = LANGUAGES\n    return gettext.translation(domain, localedir, languages)", "idx": 814}
{"namespace": "mssqlcli.mssqlbuffer._is_query_executable", "completion": "    # Remove comments from the SQL statement\n    sql = re.sub(r'/\\*.*\\*/', '', sql)\n\n    # Check for open comments\n    if '/*' in sql:\n        return False\n\n    # Check if the statement ends with 'GO'\n    if sql.strip().upper().endswith('GO'):\n        # Check if 'GO' is surrounded by an open quote\n        open_quote = False\n        for char in sql:\n            if char in ('\"', \"'\"):\n                open_quote = not open_quote\n        if not open_quote:\n            return True\n\n    return False", "idx": 815}
{"namespace": "mssqlcli.telemetry.conclude", "completion": "    _session.end_time = datetime.now()\n    payload = _session.generate_payload()\n    output_payload_to_file(payload)\n    return upload_payload(payload, service_endpoint_uri, separate_process)", "idx": 816}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.start", "completion": "        self.request_thread = threading.Thread(target=self._listen_for_request, name=self.REQUEST_THREAD_NAME)\n        self.request_thread.daemon = True\n        self.request_thread.start()\n\n        self.response_thread = threading.Thread(target=self._listen_for_response, name=self.RESPONSE_THREAD_NAME)\n        self.response_thread.daemon = True\n        self.response_thread.start()", "idx": 817}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.submit_request", "completion": "        if method is None or params is None:\n            raise ValueError(\"Method and params cannot be None\")\n\n        request = {\n            u'method': method,\n            u'params': params,\n            u'id': request_id\n        }\n\n        self.request_queue.put(request)", "idx": 818}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.get_response", "completion": "        # Check if there is a response for the given request_id\n        if request_id in self.response_map:\n            return self.response_map[request_id].get()\n\n        # Check if there is a response for the given owner_uri\n        elif owner_uri in self.response_map:\n            return self.response_map[owner_uri].get()\n\n        # Check if there is an exception\n        elif not self.exception_queue.empty():\n            raise self.exception_queue.get()\n\n        # If no response or exception is found, return None\n        else:\n            return None", "idx": 819}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcClient.shutdown", "completion": "        self.cancel = True\n        self.request_queue.put(None)\n        self.request_thread.join()\n        self.writer.close()\n        logger.debug('Json Rpc client shutdown.')", "idx": 820}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcWriter.send_request", "completion": "        # Create a JSON content body\n        content_body = {\n            \"jsonrpc\": \"2.0\",\n            \"method\": method,\n            \"params\": params,\n            \"id\": request_id\n        }\n\n        # Convert the content body to JSON format\n        content = json.dumps(content_body, ensure_ascii=False)\n\n        # Calculate the content length\n        content_length = len(content.encode(self.encoding))\n\n        # Create a header with the content length\n        header = self.HEADER.format(content_length)\n\n        # Combine the header and the content\n        message = header + content\n\n        # Send the message through the stream\n        try:\n            self.stream.write(message.encode(self.encoding))\n            self.stream.flush()\n        except ValueError as ex:\n            logger.debug(u'JSON RPC Writer on send_request() encountered exception: %s', ex)\n            # Stream was closed.\n            raise", "idx": 821}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.read_response", "completion": "        while True:\n            if self.read_state == ReadState.Header:\n                if not self.try_read_headers():\n                    if not self.read_next_chunk():\n                        break\n            elif self.read_state == ReadState.Content:\n                content = [None]\n                if not self.try_read_content(content):\n                    if not self.read_next_chunk():\n                        break\n                else:\n                    self.trim_buffer_and_resize(self.read_offset)\n                    self.read_offset = 0\n                    try:\n                        return json.loads(content[0])\n                    except ValueError as ex:\n                        logger.debug(u'JSON RPC Reader on read_response encountered exception: %s', ex)\n                        raise ValueError(u'Invalid JSON received.')\n        return None", "idx": 822}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.read_next_chunk", "completion": "        # Check if buffer needs to be resized.\n        if self.buffer_end_offset >= len(self.buffer) * self.BUFFER_RESIZE_TRIGGER:\n            self.trim_buffer_and_resize(self.read_offset)\n\n        # Read data from stream into buffer.\n        try:\n            bytes_read = self.stream.readinto(\n                self.buffer[self.buffer_end_offset:])\n        except ValueError as ex:\n            # Stream was closed externally.\n            logger.debug(u'Read Next Chunk encountered exception %s', ex)\n            raise\n\n        # If no data was read, return False.\n        if bytes_read == 0:\n            return False\n\n        # Update buffer offset.\n        self.buffer_end_offset += bytes_read\n\n        return True", "idx": 823}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.try_read_headers", "completion": "        # Find the end of the header\n        end_of_header = self.buffer.find(b'\\r\\n\\r\\n', self.read_offset, self.buffer_end_offset)\n        if end_of_header == -1:\n            # We did not find the end of the header in the buffer\n            return False\n\n        # Extract the header\n        header = self.buffer[self.read_offset:end_of_header].decode(self.encoding)\n        self.read_offset = end_of_header + 4  # Skip past the '\\r\\n\\r\\n'\n\n        # Split the header into lines\n        lines = header.split('\\r\\n')\n        for line in lines:\n            # Split each line into a key-value pair\n            key, value = line.split(': ')\n            self.headers[key.lower()] = value\n\n        # Check if the 'content-length' header is present\n        if 'content-length' not in self.headers:\n            raise LookupError('Content-Length header not found')\n\n        # Store the expected content length\n        self.expected_content_length = int(self.headers['content-length'])\n\n        # We are now expecting to read the content\n        self.read_state = ReadState.Content\n\n        return True", "idx": 824}
{"namespace": "mssqlcli.jsonrpc.jsonrpcclient.JsonRpcReader.close", "completion": "        try:\n            self.stream.close()\n        except AttributeError:\n            logger.debug('Failed to close the stream')\n            raise", "idx": 825}
{"namespace": "mssqlcli.packages.prioritization.PrevalenceCounter.update", "completion": "        self.update_keywords(text)\n        self.update_names(text)", "idx": 826}
{"namespace": "mssqlcli.packages.sqlcompletion.suggest_type", "completion": "\n    # If the input text starts with \"\\\\i \", suggest a Path type\n    if full_text.startswith('\\\\i '):\n        return (Path(),)\n\n    # Create a SqlStatement instance with the input text and text before the cursor\n    stmt = SqlStatement(full_text, text_before_cursor)\n\n    # If the SqlStatement is successfully parsed\n    if stmt.parsed:\n        # Check for special commands and handle them separately\n        if stmt.parsed.token_first().value == '\\\\':\n            return suggest_special(text_before_cursor)\n\n        # Suggest the completion type and scope based on the last token of the SqlStatement\n        return suggest_based_on_last_token(stmt.last_token, stmt)\n\n    # If the SqlStatement is not successfully parsed, suggest a Keyword or Special type\n    return (Keyword(), Special())", "idx": 827}
{"namespace": "mssqlcli.packages.parseutils.ctes.extract_ctes", "completion": "    parsed = parse(sql)[0]\n    idx, tok = parsed.token_next_by(t=Keyword)\n\n    if not (tok and tok.normalized == 'WITH'):\n        return (), sql\n\n    idx, tok = parsed.token_next_by(t=CTE, idx=idx)\n    if not tok:\n        return (), sql\n\n    ctes = []\n    pos0 = len(str(parsed.tokens[:idx]))\n    while tok:\n        cte = get_cte_from_token(tok, pos0)\n        if cte:\n            ctes.append(cte)\n            pos0 += len(str(tok)) + 1  # +1 for comma\n        idx, tok = parsed.token_next_by(t=CTE, idx=idx)\n\n    return ctes, sql[pos0:]", "idx": 828}
{"namespace": "mssqlcli.packages.parseutils.tables.extract_tables", "completion": "    # Parse the SQL statement\n    parsed = sqlparse.parse(sql)\n\n    # Extract the table names from the parsed result\n    tables = set()\n    for statement in parsed:\n        stream = extract_from_part(statement)\n        tables.update(extract_table_identifiers(stream))\n\n    return tuple(tables)", "idx": 829}
{"namespace": "googleapiclient.channel.Channel.body", "completion": "        body = {\n            \"id\": self.id,\n            \"token\": self.token,\n            \"type\": self.type,\n            \"address\": self.address,\n        }\n        if self.params is not None:\n            body[\"params\"] = self.params\n        if self.resource_id:\n            body[\"resourceId\"] = self.resource_id\n        if self.resource_uri:\n            body[\"resourceUri\"] = self.resource_uri\n        if self.expiration is not None:\n            body[\"expiration\"] = self.expiration\n        return body", "idx": 830}
{"namespace": "googleapiclient.channel.Channel.update", "completion": "        for param, attr in CHANNEL_PARAMS.items():\n            if param in resp:\n                setattr(self, attr, resp[param])", "idx": 831}
{"namespace": "googleapiclient.channel.notification_from_headers", "completion": "    headers = _upper_header_keys(headers)\n\n    if headers.get(X_GOOG_CHANNEL_ID) != channel.id:\n        raise InvalidNotificationError(\"Mismatched channel id in notification.\")\n\n    message_number = int(headers.get(X_GOOG_MESSAGE_NUMBER, 0))\n    state = headers.get(X_GOOG_RESOURCE_STATE)\n    resource_uri = headers.get(X_GOOG_RESOURCE_URI)\n    resource_id = headers.get(X_GOOG_RESOURCE_ID)\n\n    if not state or not resource_uri or not resource_id:\n        raise InvalidNotificationError(\"Missing required headers in notification.\")\n\n    return Notification(message_number, state, resource_uri, resource_id)", "idx": 832}
{"namespace": "googleapiclient.channel.new_webhook_channel", "completion": "    if expiration is not None:\n        expiration = int((expiration - EPOCH).total_seconds() * 1000)\n    return Channel(\n        type=\"web_hook\",\n        id=str(uuid.uuid4()),\n        token=token,\n        address=url,\n        expiration=expiration,\n        params=params,\n    )", "idx": 833}
{"namespace": "googleapiclient.model.BaseModel._build_query", "completion": "        if self.alt_param:\n            params[\"alt\"] = self.alt_param\n        pairs = []\n        for key, value in params.items():\n            if value is None:\n                continue\n            if isinstance(value, list):\n                for v in value:\n                    v = str(v).encode(\"utf-8\")\n                    pairs.append((key, v))\n            else:\n                if callable(value):\n                    value = value()\n                value = str(value).encode(\"utf-8\")\n                pairs.append((key, value))\n        return urllib.parse.urlencode(pairs, doseq=True)", "idx": 834}
{"namespace": "googleapiclient.model.BaseModel.response", "completion": "        self._log_response(resp, content)\n        if resp.status < 200 or resp.status >= 300:\n            raise googleapiclient.errors.HttpError(resp, content)\n        elif resp.status == 204:\n            return self.no_content_response\n        else:\n            return self.deserialize(content)", "idx": 835}
{"namespace": "googleapiclient.model.makepatch", "completion": "    patch = {}\n    for key in original.keys():\n        if key in modified:\n            if original[key] != modified[key]:\n                patch[key] = modified[key]\n    return patch", "idx": 836}
{"namespace": "googleapiclient._helpers.update_query_params", "completion": "    parsed_uri = urllib.parse.urlparse(uri)\n    updated_query = dict(urllib.parse.parse_qsl(parsed_uri.query))\n    updated_query.update(params)\n    parsed_uri = parsed_uri._replace(query=urllib.parse.urlencode(updated_query))\n    return urllib.parse.urlunparse(parsed_uri)", "idx": 837}
{"namespace": "googleapiclient._helpers._add_query_parameter", "completion": "    if value is None:\n        return url\n\n    parsed_url = urllib.parse.urlparse(url)\n    query_params = dict(urllib.parse.parse_qsl(parsed_url.query))\n\n    query_params[name] = value\n    new_query = urllib.parse.urlencode(query_params)\n\n    new_url = urllib.parse.urlunparse(\n        (parsed_url.scheme, parsed_url.netloc, parsed_url.path, parsed_url.params, new_query, parsed_url.fragment)\n    )\n\n    return new_url", "idx": 838}
{"namespace": "gif_for_cli.display.display_txt_frames", "completion": "    try:\n        for _ in range(num_loops):\n            for frame in txt_frames:\n                print(frame, file=stdout)\n                time.sleep(seconds_per_frame)\n    except KeyboardInterrupt:\n        pass", "idx": 839}
{"namespace": "pyVmomi.SoapAdapter.SoapResponseDeserializer.Deserialize", "completion": "        parser = ParserCreate(namespace_separator=NS_SEP)\n        self.isFault = False\n        self.resultType = resultType\n        self.result = None\n        self.stack = []\n        self.data = \"\"\n        self.serverGuid = None\n        self.nsMap = nsMap and nsMap or {}\n        self.origHandlers = GetHandlers(parser)\n        SetHandlers(parser, GetHandlers(self))\n        parser.Parse(response)\n        if self.isFault:\n            raise self.result\n        return self.result", "idx": 840}
{"namespace": "pyVmomi.VmomiSupport.GetRequestContext", "completion": "    global _threadLocalContext\n    return _threadLocalContext.__dict__.setdefault('reqCtx', StringDict())", "idx": 841}
{"namespace": "pycoin.bloomfilter.filter_size_required", "completion": "    filter_size = int((-1 / pow(LOG_2, 2) * element_count * math.log(false_positive_probability)) / 8)\n    if filter_size > 36000:\n        return 36000\n    else:\n        return filter_size", "idx": 842}
{"namespace": "pycoin.bloomfilter.BloomFilter.add_spendable", "completion": "        self.add_item(spendable.coin_hash)\n        self.add_item(struct.pack(\"<I\", spendable.tx_out_index))", "idx": 843}
{"namespace": "pycoin.bloomfilter.murmur3", "completion": "    c1 = 0xcc9e2d51\n    c2 = 0x1b873593\n    r1 = 15\n    r2 = 13\n    m = 5\n    n = 0xe6546b64\n\n    hash = seed\n\n    for k in data:\n        k = k * c1\n        k = (k << r1) | (k >> (32 - r1))\n        k = k * c2\n\n        hash = hash ^ k\n        hash = (hash << r2) | (hash >> (32 - r2))\n        hash = hash * m + n\n\n    hash = hash ^ len(data)\n    hash = hash ^ (hash >> 16)\n    hash = hash * 0x85ebca6b\n    hash = hash ^ (hash >> 13)\n    hash = hash * 0xc2b2ae35\n    hash = hash ^ (hash >> 16)\n\n    return hash", "idx": 844}
{"namespace": "pycoin.networks.registry.network_for_netcode", "completion": "    for prefix in search_prefixes():\n        try:\n            module = importlib.import_module(f\"{prefix}.{symbol}\")\n            if hasattr(module, \"network\") and module.network.symbol == symbol:\n                module.network.symbol = symbol\n                return module.network\n        except ImportError:\n            continue\n    raise ValueError(f\"No network found for symbol {symbol}\")", "idx": 845}
{"namespace": "pycoin.satoshi.IntStreamer.IntStreamer.int_from_script_bytes", "completion": "        if len(s) == 0:\n            return 0\n        v = s[-1] & 0x7f\n        if require_minimal and v == 0 and len(s) > 1:\n            raise ScriptError('non-minimally encoded script number')\n        is_negative = (s[-1] & 0x80) != 0\n        for ch in reversed(s[:-1]):\n            v <<= 8\n            v += ch\n        if is_negative:\n            v = -v\n        return v", "idx": 846}
{"namespace": "pycoin.satoshi.stackops.do_OP_RIPEMD160", "completion": "        from ..encoding.hash import ripemd160\n    from ..encoding.hash import ripemd160\n    stack.append(ripemd160(stack.pop()))", "idx": 847}
{"namespace": "pycoin.satoshi.stackops.do_OP_HASH160", "completion": "        from ..encoding.hash import hash160\n    from ..encoding.hash import hash160\n    stack.append(hash160(stack.pop()))```", "idx": 848}
{"namespace": "pycoin.satoshi.stackops.do_OP_HASH256", "completion": "    stack.append(hashlib.sha256(hashlib.sha256(stack.pop()).digest()).digest())", "idx": 849}
{"namespace": "pycoin.services.providers.providers_for_config_string", "completion": "    providers = []\n    for descriptor in config_string.split():\n        provider = provider_for_descriptor_and_netcode(descriptor, netcode)\n        if provider is None:\n            warnings.warn(\"can't parse provider %s\" % descriptor)\n        else:\n            providers.append(provider)\n    return providers", "idx": 850}
{"namespace": "pycoin.services.providers.get_default_providers_for_netcode", "completion": "    if netcode is None:\n        netcode = get_current_netcode()\n    if not hasattr(THREAD_LOCALS, \"providers\"):\n        THREAD_LOCALS.providers = {}\n    if netcode not in THREAD_LOCALS.providers:\n        THREAD_LOCALS.providers[netcode] = providers_for_netcode_from_env(netcode)\n    return THREAD_LOCALS.providers[netcode]", "idx": 851}
{"namespace": "pycoin.services.providers.set_default_providers_for_netcode", "completion": "    if not hasattr(THREAD_LOCALS, \"providers\"):\n        THREAD_LOCALS.providers = {}\n    THREAD_LOCALS.providers[netcode] = provider_list", "idx": 852}
{"namespace": "pycoin.blockchain.BlockChain.BlockChain.tuple_for_index", "completion": "        if index < 0:\n            index += self.length()\n        if index < len(self._locked_chain):\n            return self._locked_chain[index]\n        index -= len(self._locked_chain)\n        longest_chain = self._longest_local_block_chain()\n        the_hash = longest_chain[-index-1]\n        parent_hash = self.parent_hash if index <= 0 else longest_chain[-index]\n        weight = self.weight_lookup.get(the_hash)\n        return the_hash, parent_hash, weight", "idx": 853}
{"namespace": "pycoin.blockchain.ChainFinder.ChainFinder.find_ancestral_path", "completion": "        if h1 not in path_cache:\n            path_cache[h1] = self.maximum_path(h1)\n        if h2 not in path_cache:\n            path_cache[h2] = self.maximum_path(h2)\n\n        path1 = path_cache[h1]\n        path2 = path_cache[h2]\n\n        common_ancestor = None\n        for node in path1:\n            if node in path2:\n                common_ancestor = node\n                break\n\n        if common_ancestor is None:\n            return None\n\n        index1 = path1.index(common_ancestor)\n        index2 = path2.index(common_ancestor)\n\n        return path1[:index1+1], path2[:index2+1]", "idx": 854}
{"namespace": "pycoin.contrib.bech32m.bech32_encode", "completion": "    combined = data + bech32_create_checksum(hrp, data, spec)\n    return hrp + '1' + ''.join([CHARSET[d] for d in combined])", "idx": 855}
{"namespace": "pycoin.contrib.bech32m.decode", "completion": "    hrpgot, data, spec = bech32_decode(addr)\n    if hrpgot != hrp:\n        return (None, None)\n    if spec == Encoding.BECH32 and data[0] != 0:\n        return (None, None)\n    if spec == Encoding.BECH32M and (data[0] == 0 or data[0] > 16):\n        return (None, None)\n    decoded = convertbits(data[1:], 5, 8, False)\n    if decoded is None or len(decoded) < 2 or len(decoded) > 40:\n        return (None, None)\n    if data[0] > 0 and decoded[0] & 0x80:\n        return (None, None)\n    return (data[0], decoded)", "idx": 856}
{"namespace": "pycoin.crack.bip32.crack_bip32", "completion": "    path = path.split('/')\n    for child in path:\n        if child == 'm':\n            continue\n        child = int(child)\n        secret_exponent = ascend_bip32(bip32_pub_node, secret_exponent, child)\n        bip32_pub_node = bip32_pub_node.subkey_for_path(str(child))\n    return bip32_pub_node", "idx": 857}
{"namespace": "pycoin.message.PeerAddress.ip_bin_to_ip4_addr", "completion": "    return \".\".join(str(b) for b in struct.unpack(\">BBBB\", ip_bin))", "idx": 858}
{"namespace": "pycoin.message.PeerAddress.PeerAddress.host", "completion": "        if self.ip_bin[:12] == IP4_HEADER:\n            return ip_bin_to_ip4_addr(self.ip_bin)\n        else:\n            return ip_bin_to_ip6_addr(self.ip_bin)", "idx": 859}
{"namespace": "oletools.msodde.field_is_blacklisted", "completion": "    # split field into words\n    words = FIELD_WORD_REGEX.findall(contents)\n    if not words:\n        return False\n    cmd = words[0].lower()\n    if cmd not in FIELD_BLACKLIST_CMDS:\n        return False\n    # command is blacklisted, check arguments and switches\n    args = [word for word in words[1:] if not FIELD_SWITCH_REGEX.match(word)]\n    switches = [word for word in words[1:] if FIELD_SWITCH_REGEX.match(word)]\n    for field in FIELD_BLACKLIST:\n        if field[0].lower() != cmd:\n            continue\n        if len(args) < field[1] or len(args) > field[1] + field[2]:\n            continue\n        if any(switch not in field[3] for switch in switches):\n            continue\n        return True\n    return False", "idx": 860}
{"namespace": "oletools.ppt_record_parser.is_ppt", "completion": "    try:\n        ppt_file = PptFile(filename)\n        required_streams = ['PowerPoint Document', 'Current User']\n        required_records = ['DocumentContainer', 'SlideContainer', 'UserEditAtom', 'CurrentUserAtom']\n\n        # Check for required streams\n        for stream in required_streams:\n            if not ppt_file.exists(stream):\n                return False\n\n        # Check for required records\n        for record in required_records:\n            if not any(isinstance(rec, globals()[record]) for rec in ppt_file.iter_records()):\n                return False\n\n        return True\n    except:\n        return False", "idx": 861}
{"namespace": "oletools.rtfobj.is_rtf", "completion": "    if treat_str_as_data:\n        data = arg\n    else:\n        with open(arg, 'rb') as f:\n            data = f.read(5)\n    return data.startswith(RTF_MAGIC)", "idx": 862}
{"namespace": "oletools.oleobj.get_sane_embedded_filenames", "completion": "    # Extract the filename from the paths\n    filename = os.path.basename(filename)\n    src_filename = os.path.basename(src_path)\n    tmp_filename = os.path.basename(tmp_path)\n\n    # Sanitize the filenames and preserve the file suffix\n    filename = sanitize_filename(filename, max_len=max_len)\n    src_filename = sanitize_filename(src_filename, max_len=max_len)\n    tmp_filename = sanitize_filename(tmp_filename, max_len=max_len)\n\n    # Generate multiple filename candidates\n    filenames = [filename, src_filename, tmp_filename]\n\n    # Remove the suffix for the next set of candidates\n    filenames += [os.path.splitext(fname)[0] for fname in filenames]\n\n    # Add a random suffix for the next set of candidates\n    for _ in range(MAX_FILENAME_ATTEMPTS):\n        random_suffix = ''.join(random.choice(string.ascii_lowercase) for _ in range(5))\n        filenames += [fname + '.' + random_suffix for fname in filenames]\n\n    # One last attempt ignoring the maximum length\n    filenames.append('noname_{0}'.format(noname_index))\n\n    return filenames", "idx": 863}
{"namespace": "oletools.ooxml.get_type", "completion": "    parser = XmlParser(filename)\n    content_types, _ = parser.get_content_types()\n\n    if parser.is_single_xml():\n        with uopen(filename, 'r') as handle:\n            match = re.search(OFFICE_XML_PROGID_REGEX, handle.read(1024))\n        if match:\n            prog_id = match.group(1)\n            if prog_id == WORD_XML_PROG_ID:\n                return DOCTYPE_WORD_XML\n            elif prog_id == EXCEL_XML_PROG_ID:\n                return DOCTYPE_EXCEL_XML\n        else:\n            raise BadOOXML(filename, 'single xml but no prog_id')\n\n    # check for mixed content types\n    types_found = set()\n    for content_type in content_types.values():\n        if content_type.startswith(CONTENT_TYPES_EXCEL):\n            types_found.add(DOCTYPE_EXCEL)\n        elif content_type.startswith(CONTENT_TYPES_WORD):\n            types_found.add(DOCTYPE_WORD)\n        elif content_type.startswith(CONTENT_TYPES_PPT):\n            types_found.add(DOCTYPE_POWERPOINT)\n\n    if len(types_found) > 1:\n        return DOCTYPE_MIXED\n    elif DOCTYPE_WORD in types_found:\n        return DOCTYPE_WORD\n    elif DOCTYPE_EXCEL in types_found:\n        return DOCTYPE_EXCEL\n    elif DOCTYPE_POWERPOINT in types_found:\n        return DOCTYPE_POWERPOINT\n    else:\n        return DOCTYPE_NONE", "idx": 864}
{"namespace": "oletools.ooxml.ZipSubFile.read", "completion": "        if self.handle is None or self.pos >= self.size:\n            return b''\n        data = self.handle.read(size)\n        self.pos += len(data)\n        return data", "idx": 865}
{"namespace": "oletools.ooxml.ZipSubFile.seek", "completion": "        if self.handle is None:\n            raise IOError('seek on closed handle')\n        if offset == io.SEEK_SET:\n            new_pos = pos\n        elif offset == io.SEEK_CUR:\n            new_pos = self.pos + pos\n        elif offset == io.SEEK_END:\n            new_pos = self.size + pos\n        else:\n            raise ValueError('Invalid offset: {0}'.format(offset))\n\n        if new_pos < 0:\n            raise IOError('Invalid seek: negative position {0}'.format(new_pos))\n        if new_pos > self.size:\n            # fake being at the end, even if we are not\n            self.pos = self.size\n            return\n        if new_pos < self.pos:\n            # cannot seek backwards, have to reset and seek from start\n            self.reset()\n        self._seek_skip(new_pos - self.pos)", "idx": 866}
{"namespace": "oletools.ooxml.XmlParser.iter_xml", "completion": "        if tags is not None and isstr(tags):\n            tags = [tags, ]\n\n        for subfile, handle in self.iter_files(subfiles):\n            try:\n                for event, elem in ET.iterparse(handle, events=('start', 'end')):\n                    if event == 'start':\n                        depth = len(elem.xpath('ancestor::*'))\n                        if tags is None or elem.tag in tags:\n                            yield subfile, elem, depth\n                    if not need_children:\n                        elem.clear()\n            except ET.ParseError:\n                self.subfiles_no_xml.add(subfile)\n                logger.debug('subfile {0} is not xml'.format(subfile))\n            except UnicodeDecodeError:\n                self.subfiles_no_xml.add(subfile)\n                logger.debug('subfile {0} is not unicode'.format(subfile))", "idx": 867}
{"namespace": "oletools.oleid.OleID.check", "completion": "        # Open the file\n        if self.filename:\n            self.ole = olefile.OleFileIO(self.filename)\n        else:\n            self.ole = olefile.OleFileIO(self.data_bytesio)\n\n        # Run various checks on the file\n        self.check_properties()\n        self.check_encrypted()\n        self.check_external_relationships()\n        self.check_object_pool()\n        self.check_macros()\n        self.check_flash()\n\n        # Return the list of Indicator objects\n        return self.indicators", "idx": 868}
{"namespace": "tools.cgrep.is_valid_ip", "completion": "  try:\n    nacaddr.IP(arg)\n  except ValueError:\n    raise argparse.ArgumentTypeError('%s does not appear to be an IP or CIDR notation' % arg)\n  return arg", "idx": 869}
{"namespace": "tools.cgrep.group_diff", "completion": "  ip1, ip2 = options.gmp\n  groups1 = db.GetIpParents(ip1)\n  groups2 = db.GetIpParents(ip2)\n\n  common = list(set(groups1) & set(groups2))\n  diff1 = list(set(groups1) - set(groups2))\n  diff2 = list(set(groups2) - set(groups1))\n\n  return common, diff1, diff2", "idx": 870}
{"namespace": "tools.cgrep.compare_tokens", "completion": "  first_obj, sec_obj = options.cmp\n  first = get_nets([first_obj], db)[0][1]\n  second = get_nets([sec_obj], db)[0][1]\n  union = sorted(list(set(first) | set(second)))\n  diff = sorted(list(set(first) - set(second)))\n  return (first_obj, sec_obj, union), diff", "idx": 871}
{"namespace": "capirca.aclgen.EntryPoint", "completion": "  SetupFlags()\n  app.run(main)", "idx": 872}
{"namespace": "capirca.lib.nacaddr.IP", "completion": "  if not isinstance(ip, ipaddress._BaseNetwork):  # pylint: disable=protected-access\n    ip = ipaddress.ip_network(ip, strict=strict)\n  if ip.version == 4:\n    return IPv4(ip, comment=comment, token=token, strict=strict)\n  elif ip.version == 6:\n    return IPv6(ip, comment=comment, token=token, strict=strict)\n  else:\n    raise ValueError('Invalid IP version: %s' % ip.version)", "idx": 873}
{"namespace": "csvkit.cli.CSVKitUtility.run", "completion": "        if 'f' not in self.override_flags:\n            self.input_file = self._open_input_file(self.args.input_path)\n\n        try:\n            with warnings.catch_warnings():\n                if getattr(self.args, 'no_header_row', None):\n                    warnings.filterwarnings(action='ignore', message='Column names not specified')\n                self.main()\n        finally:\n            if 'f' not in self.override_flags:\n                self.input_file.close()", "idx": 874}
{"namespace": "csvkit.convert.fixed.fixed2csv", "completion": "\n    # Skip the specified number of lines\n    for _ in range(skip_lines):\n        next(f)\n\n    # Create a FixedWidthReader object\n    reader = FixedWidthReader(f, schema, **kwargs)\n\n    # If an output file is specified, write the parsed data to the output file\n    if output is not None:\n        writer = agate.csv.writer(output)\n        for row in reader:\n            writer.writerow(row)\n        return None\n\n    # If an output file is not specified, return the parsed data as a string\n    else:\n        output = StringIO()\n        writer = agate.csv.writer(output)\n        for row in reader:\n            writer.writerow(row)\n        return output.getvalue()", "idx": 875}
{"namespace": "check_dummies.find_backend", "completion": "    backend_search = _re_backend.search(line)\n    if backend_search is not None:\n        return backend_search.groups()[0]\n    else:\n        return None", "idx": 876}
{"namespace": "check_dummies.create_dummy_object", "completion": "    if name.isupper():\n        return DUMMY_CONSTANT.format(name)\n    elif name[0].isupper():\n        return DUMMY_CLASS.format(name, backend_name)\n    else:\n        return DUMMY_FUNCTION.format(name, backend_name)", "idx": 877}
{"namespace": "pycorrector.en_spell.EnSpell.check_init", "completion": "        if not self.word_freq_dict:\n            self._init()", "idx": 878}
{"namespace": "pycorrector.en_spell.EnSpell.candidates", "completion": "        self.check_init()\n        return (self.known([word]) or self.known(self.edits1(word)) or self.known(self.edits2(word)) or {word})", "idx": 879}
{"namespace": "pycorrector.en_spell.EnSpell.correct_word", "completion": "        self.check_init()\n        return max(self.candidates(word), key=self.probability)", "idx": 880}
{"namespace": "pycorrector.en_spell.EnSpell.correct", "completion": "                from pycorrector.utils.text_utils import split_text_by_symbol\n        self.check_init()\n        from pycorrector.utils.text_utils import split_text_by_symbol\n        blocks = split_text_by_symbol(text, include_symbol)\n        corrected_blocks = []\n        details = []\n        for blk, start_idx in blocks:\n            if len(blk) > 1 and blk.isalpha():\n                if blk in self.custom_confusion:\n                    corrected_item = self.custom_confusion[blk]\n                else:\n                    corrected_item = self.correct_word(blk)\n                if corrected_item != blk:\n                    end_idx = start_idx + len(blk)\n                    detail = [blk, corrected_item, start_idx, end_idx]\n                    details.append(detail)\n                    blk = corrected_item\n            corrected_blocks.append(blk)\n        details = sorted(details, key=operator.itemgetter(2))\n        corrected_text = ''.join(corrected_blocks)\n        return corrected_text, details", "idx": 881}
{"namespace": "whereami.predict.crossval", "completion": "    if X is None or y is None:\n        X, y = get_train_data(path)\n    if len(X) < folds:\n        raise ValueError(f'There are not enough samples ({len(X)}). Need at least {folds}.')\n    if clf is None:\n        clf = get_model(path)\n    print(f\"KFold folds={folds}, running {n} times\")\n    total_scores = []\n    for i in range(n):\n        scores = cross_val_score(clf, X, y, cv=folds)\n        avg_score = sum(scores) / len(scores)\n        total_scores.append(avg_score)\n        print(f\"{i+1}/{n}: {avg_score}\")\n    total_avg_score = sum(total_scores) / len(total_scores)\n    print(\"-------- total --------\")\n    print(total_avg_score)\n    return total_avg_score", "idx": 882}
{"namespace": "stellar.models.Table.get_table_name", "completion": "        if not self.snapshot:\n            raise Exception('Table name requires snapshot')\n        if not self.snapshot.hash:\n            raise Exception('Snapshot hash is empty.')\n        if old:\n            return 'stellar_{table_name}{snapshot_hash}{postfix}'.format(\n                table_name=self.table_name,\n                snapshot_hash=self.snapshot.hash,\n                postfix=postfix\n            )\n        else:\n            hash_string = '|'.join([self.table_name, self.snapshot.hash, postfix])\n            hash_object = hashlib.md5(hash_string.encode('utf-8'))\n            return 'stellar_{table_name}'.format(\n                table_name=hash_object.hexdigest()[:16]\n            )", "idx": 883}
{"namespace": "chatette.utils.Singleton.reset_instance", "completion": "        cls._instance = None\n        return cls.get_or_create(*args, **kwargs)", "idx": 884}
{"namespace": "chatette.utils.cast_to_unicode", "completion": "    if sys.version_info[0] == 2:\n        if isinstance(anything, str):\n            return unicode(anything, 'utf-8')\n        elif isinstance(anything, list):\n            return [cast_to_unicode(item) for item in anything]\n        elif isinstance(anything, dict):\n            return {cast_to_unicode(key): cast_to_unicode(value) for key, value in anything.items()}\n        else:\n            return anything\n    else:\n        return anything", "idx": 885}
{"namespace": "chatette.cli.terminal_writer.TerminalWriter.write", "completion": "        if self._file_mode == 'quiet':\n            return\n        if self.redirection_file_path is None:\n            print(text)\n        else:\n            if self.buffered_text is None:\n                self.buffered_text = text\n            else:\n                self.buffered_text += '\\n' + text", "idx": 886}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.find_redirection_file_path", "completion": "        if len(tokens) < 2:\n            return None\n        if tokens[-2] == REDIRECTION_APPEND_SYM:\n            return (RedirectionType.append, tokens[-1])\n        elif tokens[-2] == REDIRECTION_SYM:\n            return (RedirectionType.overwrite, tokens[-1])\n        elif tokens[-1] == REDIRECTION_SYM:\n            return (RedirectionType.quiet, None)\n        else:\n            return None", "idx": 887}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.get_unit_type_from_str", "completion": "        unit_type_str = unit_type_str.lower()\n        for unit_type in UnitType:\n            if unit_type_str == unit_type.name.lower():\n                return unit_type\n        return None", "idx": 888}
{"namespace": "chatette.cli.interactive_commands.unhide_command.UnhideCommand.execute", "completion": "        if len(self.command_tokens) < 3:\n            self.print_wrapper.error_log(\n                \"Missing some arguments\\nUsage: \" + \\\n                'unhide <unit-type> \"<unit-name>\"'\n            )\n            return\n\n        unit_type = self._get_unit_type(self.command_tokens[1])\n        if unit_type is None:\n            self.print_wrapper.error_log(\n                \"Unknown unit type: '\" + self.command_tokens[1] + \"'.\"\n            )\n            return\n\n        try:\n            unit_regex = self._get_regex(self.command_tokens[2])\n        except IndexError:\n            self.print_wrapper.error_log(\n                \"Missing some arguments\\nUsage: \" + \\\n                'unhide <unit-type> \"<unit-name>\"'\n            )\n            return\n\n        self._execute_on_units(unit_type, unit_regex)", "idx": 889}
{"namespace": "chatette.adapters.factory.create_adapter", "completion": "            from .jsonl_adapter import JsonlAdapter\n            from .rasa_adapter import RasaAdapter\n    if adapter_name.lower() in ['rasa', 'rasa-md', 'rasamd']:\n        from .rasa_adapter import RasaAdapter\n        return RasaAdapter(base_filepath)\n    elif adapter_name.lower() == 'jsonl':\n        from .jsonl_adapter import JsonlAdapter\n        return JsonlAdapter(base_filepath)\n    else:\n        raise ValueError(\"Invalid adapter name. Available options are 'rasa', 'rasa-md', 'rasamd', 'jsonl'.\")", "idx": 890}
{"namespace": "chatette.parsing.ChoiceBuilder.create_concrete", "completion": "                from chatette.units.modifiable.choices import Choice\n        from chatette.units.modifiable.choices import Choice\n        self._check_information()\n        modifiers = self._build_modifiers_repr()\n        return Choice(self.rules, self.leading_space, modifiers)", "idx": 891}
{"namespace": "chatette.parsing.UnitRefBuilder._build_modifiers_repr", "completion": "        modifiers = super(UnitRefBuilder, self)._build_modifiers_repr()\n        modifiers.argument_value = self.arg_value\n        modifiers.variation_name = self.variation\n        return modifiers", "idx": 892}
{"namespace": "chatette.parsing.UnitRefBuilder.create_concrete", "completion": "                from chatette.units.modifiable.references.unit import UnitReference\n        from chatette.units.modifiable.references.unit import UnitReference\n        self._check_information()\n        return UnitReference(\n            self.leading_space, self.type, self.identifier,\n            self._build_modifiers_repr()\n        )", "idx": 893}
{"namespace": "chatette.parsing.UnitDefBuilder._build_modifiers_repr", "completion": "        modifiers = super(UnitDefBuilder, self)._build_modifiers_repr()\n        modifiers.argument_name = self.arg_name\n        return modifiers", "idx": 894}
{"namespace": "chatette.parsing.AliasDefBuilder.create_concrete", "completion": "                from chatette.units.modifiable.definitions.alias import AliasDefinition\n        from chatette.units.modifiable.definitions.alias import AliasDefinition\n        self._check_information()\n        if self.variation is not None:\n            definitions = AST.get_or_create()[UnitType.alias]\n            if self.identifier in definitions:\n                return definitions[self.identifier]\n        return AliasDefinition(self.identifier, self._build_modifiers_repr())", "idx": 895}
{"namespace": "chatette.parsing.SlotDefBuilder.create_concrete", "completion": "                from chatette.units.modifiable.definitions.slot import SlotDefinition\n        from chatette.units.modifiable.definitions.slot import SlotDefinition\n        self._check_information()\n        if self.variation is not None:\n            definitions = AST.get_or_create()[UnitType.slot]\n            if self.identifier in definitions:\n                return definitions[self.identifier]\n        return SlotDefinition(self.identifier, self._build_modifiers_repr())", "idx": 896}
{"namespace": "chatette.parsing.IntentDefBuilder.create_concrete", "completion": "                from chatette.units.modifiable.definitions.intent import IntentDefinition\n        from chatette.units.modifiable.definitions.intent import IntentDefinition\n        self._check_information()\n        if self.variation is not None:\n            definitions = AST.get_or_create()[UnitType.intent]\n            if self.identifier in definitions:\n                return definitions[self.identifier]\n        return IntentDefinition(\n            self.identifier, self._build_modifiers_repr(),\n            self.nb_training_ex, self.nb_testing_ex\n        )", "idx": 897}
{"namespace": "bentoml._internal.resource.get_resource", "completion": "\n    resource_class = _RESOURCE_REGISTRY.get(resource_kind)\n    if resource_class is None:\n        raise BentoMLConfigException(f\"Unknown resource kind '{resource_kind}'\")\n\n    resource_spec = resources.get(resource_kind)\n    if resource_spec is None:\n        return None\n\n    if resource_spec == \"system\":\n        resource = resource_class.from_system()\n    else:\n        resource = resource_class.from_spec(resource_spec)\n\n    if validate:\n        resource_class.validate(resource)\n\n    return resource", "idx": 898}
{"namespace": "bentoml._internal.resource.system_resources", "completion": "    result = {}\n    for resource_kind, resource in _RESOURCE_REGISTRY.items():\n        result[resource_kind] = resource.from_system()\n    return result", "idx": 899}
{"namespace": "bentoml._internal.resource.CpuResource.from_spec", "completion": "        if isinstance(spec, (int, float)):\n            return float(spec)\n        elif isinstance(spec, str):\n            try:\n                return float(spec)\n            except ValueError:\n                raise BentoMLConfigException(\n                    f\"Invalid CPU resource limit '{spec}'. Must be a number.\"\n                )\n        else:\n            raise BentoMLConfigException(\n                f\"Invalid CPU resource limit '{spec}'. Must be a number.\"\n            )", "idx": 900}
{"namespace": "bentoml._internal.resource.CpuResource.from_system", "completion": "        if os.name == 'nt':  # for Windows\n            return float(query_os_cpu_count())\n        else:  # for Linux and others\n            return query_cgroup_cpu_count()", "idx": 901}
{"namespace": "bentoml._internal.resource.CpuResource.validate", "completion": "        if val < 0:\n            raise BentoMLConfigException(f\"Negative CPU resource limit '{val}'.\")\n        if val > cls.from_system():\n            raise BentoMLConfigException(\n                f\"CPU resource limit '{val}' is greater than the system available: {cls.from_system()}\"\n            )", "idx": 902}
{"namespace": "bentoml._internal.types.LazyType.get_class", "completion": "        if self._runtime_class is None and import_module:\n            try:\n                module = __import__(self.module, fromlist=[self.qualname])\n                self._runtime_class = getattr(module, self.qualname)\n            except ImportError as e:\n                logger.error(f\"Failed to import module {self.module}. Error: {str(e)}\")\n                raise\n            except AttributeError as e:\n                logger.error(f\"Failed to get class {self.qualname} from module {self.module}. Error: {str(e)}\")\n                raise\n        elif self._runtime_class is None:\n            raise ValueError(f\"Class {self.qualname} from module {self.module} is not imported.\")\n        return self._runtime_class", "idx": 903}
{"namespace": "bentoml._internal.models.model.Model.create", "completion": "\n        if isinstance(name, str):\n            tag = Tag(name)\n        else:\n            tag = name\n\n        if labels is None:\n            labels = {}\n\n        if options is None:\n            options = ModelOptions()\n\n        if custom_objects is None:\n            custom_objects = {}\n\n        if metadata is None:\n            metadata = {}\n\n        info = ModelInfo(\n            tag=tag,\n            module=module,\n            labels=labels,\n            options=options,\n            metadata=metadata,\n            context=context,\n            signatures=signatures,\n            api_version=api_version,\n            creation_time=datetime.now(timezone.utc),\n        )\n\n        model_fs = fs.open_fs(\"temp://bentoml_model_\" + tag.to_url())\n\n        return Model(tag, model_fs, info, custom_objects, _internal=True)", "idx": 904}
{"namespace": "bentoml._internal.models.model.Model.from_fs", "completion": "        with item_fs.open(MODEL_YAML_FILENAME, \"r\", encoding=\"utf-8\") as model_yaml:\n            info = ModelInfo.from_yaml_file(t.cast(io.StringIO, model_yaml))\n\n        model = cls(\n            info.tag,\n            item_fs,\n            info,\n            _internal=True,\n        )\n        model.validate()\n        return model", "idx": 905}
{"namespace": "bentoml._internal.utils.metrics.linear_buckets", "completion": "\n    assert start < end\n    assert step > 0.0\n\n    buckets: list[float] = []\n    bound = start\n    while bound < end:\n        buckets.append(bound)\n        bound += step\n\n    if len(buckets) > MAX_BUCKET_COUNT:\n        buckets = buckets[:MAX_BUCKET_COUNT]\n\n    return tuple(buckets) + (end, INF)", "idx": 906}
{"namespace": "bentoml._internal.utils.validate_metadata", "completion": "    if not isinstance(metadata, dict):\n        raise ValueError(\"metadata must be a dict!\")\n\n    for key, val in metadata.items():\n        if not isinstance(key, str):\n            raise ValueError(\"metadata keys must be strings\")\n\n        _validate_metadata_entry(val)", "idx": 907}
{"namespace": "bentoml._internal.utils.analytics.usage_stats.get_serve_info", "completion": "    serve_id = secrets.token_urlsafe(32)\n    serve_started_timestamp = datetime.now(timezone.utc)\n    return ServeInfo(serve_id=serve_id, serve_started_timestamp=serve_started_timestamp)", "idx": 908}
{"namespace": "bentoml._internal.utils.analytics.usage_stats._track_serve_init", "completion": "    event_properties = ServeInitEvent(\n        serve_id=serve_info.serve_id,\n        production=production,\n        serve_kind=serve_kind,\n        serve_from_server_api=from_server_api,\n        triggered_at=serve_info.serve_started_timestamp,\n        num_models=len(svc.models),\n        num_runners=len(svc.runners),\n        num_apis=len(svc.apis),\n        model_types=[model.type for model in svc.models],\n        runner_types=[runner.type for runner in svc.runners],\n        api_input_types=[api.input_type for api in svc.apis],\n        api_output_types=[api.output_type for api in svc.apis],\n    )\n    track(event_properties)", "idx": 909}
{"namespace": "bentoml._internal.service.service.get_valid_service_name", "completion": "    svc_name = user_provided_svc_name.lower()\n    if svc_name != user_provided_svc_name:\n        logger.warning(\n            \"Service name '%s' has been converted to lowercase '%s'. \"\n            \"BentoML by default converts the service name to lowercase\",\n            user_provided_svc_name,\n            svc_name,\n        )\n\n    # validate service name with Tag\n    Tag.validate_tag_name(svc_name)\n\n    return svc_name", "idx": 910}
{"namespace": "bentoml._internal.configuration.helpers.flatten_dict", "completion": "    for k, v in d.items():\n        new_key = f'\"{k}\"' if any(c in punctuation for c in k) else k\n        new_key = f\"{parent}{sep}{new_key}\" if parent else new_key\n        if isinstance(v, dict):\n            yield from flatten_dict(v, new_key, sep=sep)\n        else:\n            yield new_key, v", "idx": 911}
{"namespace": "bentoml._internal.configuration.helpers.load_config_file", "completion": "    if not os.path.exists(path):\n        raise BentoMLConfigException(f\"Configuration file {path} does not exist.\")\n\n    with open(path, 'r') as file:\n        config = yaml.safe_load(file)\n\n    return config", "idx": 912}
{"namespace": "bentoml._internal.configuration.helpers.expand_env_var_in_values", "completion": "    for k, v in d.items():\n        if isinstance(v, t.MutableMapping):\n            expand_env_var_in_values(v)\n        elif isinstance(v, str):\n            d[k] = expand_env_var(v)\n        elif isinstance(v, t.Sequence) and not isinstance(v, str):\n            d[k] = [expand_env_var(i) if isinstance(i, str) else i for i in v]", "idx": 913}
{"namespace": "bentoml._internal.runner.strategy.DefaultStrategy.get_worker_count", "completion": "        if resource_request is None:\n            resource_request = system_resources()\n\n        # Check for Nvidia GPU support\n        nvidia_gpus: list[int] | None = get_resource(resource_request, \"nvidia.com/gpu\")\n        if nvidia_gpus is not None and \"nvidia.com/gpu\" in runnable_class.SUPPORTED_RESOURCES:\n            return len(nvidia_gpus) * workers_per_resource\n\n        # Check for CPU support\n        cpus = get_resource(resource_request, \"cpu\")\n        if cpus is not None and runnable_class.SUPPORTS_CPU_MULTI_THREADING:\n            return cpus * workers_per_resource\n\n        raise ValueError(\"No known supported resources available for runnable class.\")", "idx": 914}
{"namespace": "bentoml._internal.runner.strategy.DefaultStrategy.get_worker_env", "completion": "        env = {}\n\n        # use nvidia gpu\n        nvidia_gpus = get_resource(resource_request, \"nvidia.com/gpu\")\n        if (\n            nvidia_gpus is not None\n            and len(nvidia_gpus) > 0\n            and \"nvidia.com/gpu\" in runnable_class.SUPPORTED_RESOURCES\n        ):\n            env[\"CUDA_VISIBLE_DEVICES\"] = str(worker_index % len(nvidia_gpus))\n\n        # use CPU\n        cpus = get_resource(resource_request, \"cpu\")\n        if cpus is not None and cpus > 0:\n            if \"cpu\" not in runnable_class.SUPPORTED_RESOURCES:\n                logger.warning(\n                    \"No known supported resource available for %s, falling back to using CPU.\",\n                    runnable_class,\n                )\n\n            if runnable_class.SUPPORTS_CPU_MULTI_THREADING:\n                if isinstance(workers_per_resource, float):\n                    raise ValueError(\n                        \"Fractional CPU multi threading support is not yet supported.\"\n                    )\n                env.update({env_var: str(workers_per_resource) for env_var in THREAD_ENVS})\n\n        return env", "idx": 915}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.batches_to_batch", "completion": "        # numpy.concatenate may consume lots of memory, need optimization later\n        indices = list(itertools.accumulate(subbatch.shape[batch_dim] for subbatch in batches))\n        indices = [0] + indices\n        return np.concatenate(batches, axis=batch_dim), indices", "idx": 916}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.to_payload", "completion": "        if batch.ndim == 0:\n            return cls.create_payload(\n                data=pickle.dumps(batch),\n                batch_size=1,\n                meta={\"format\": \"pickle5\"},\n            )\n\n        # ensure the array is either C-contiguous or F-contiguous\n        if not (batch.flags[\"C_CONTIGUOUS\"] or batch.flags[\"F_CONTIGUOUS\"]):\n            batch = np.ascontiguousarray(batch)\n\n        meta: dict[str, bool | int | float | str | list[int]] = {\"format\": \"pickle5\"}\n\n        bs: bytes\n        concat_buffer_bs: bytes\n        indices: list[int]\n        bs, concat_buffer_bs, indices = pep574_dumps(batch)\n\n        if indices:\n            meta[\"with_buffer\"] = True\n            data = concat_buffer_bs\n            meta[\"pickle_bytes_str\"] = base64.b64encode(bs).decode(\"ascii\")\n            meta[\"indices\"] = indices\n        else:\n            meta[\"with_buffer\"] = False\n            data = bs\n\n        return cls.create_payload(\n            data,\n            batch.shape[batch_dim],\n            meta=meta,\n        )", "idx": 917}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.from_payload", "completion": "        if payload.meta.get(\"format\") == \"pickle5\":\n            bs_str = payload.meta[\"pickle_bytes_str\"]\n            bs = base64.b64decode(bs_str)\n            indices = payload.meta[\"indices\"]\n            return pep574_loads(bs, payload.data, indices)\n        else:\n            return pickle.loads(payload.data)", "idx": 918}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.batch_to_payloads", "completion": "        # Divide the batch into smaller batches\n        subbatches = cls.batch_to_batches(batch, indices, batch_dim)\n        \n        # Convert each subbatch into a payload\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in subbatches]\n        \n        return payloads", "idx": 919}
{"namespace": "bentoml._internal.runner.container.NdarrayContainer.from_batch_payloads", "completion": "        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)", "idx": 920}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.to_payload", "completion": "        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n\n        if isinstance(batch, ext.PdSeries):\n            batch = batch.to_frame()\n\n        meta = {\"format\": \"pickle5\"}\n\n        bs: bytes\n        concat_buffer_bs: bytes\n        indices: list[int]\n        bs, concat_buffer_bs, indices = pep574_dumps(batch)\n\n        if indices:\n            meta[\"with_buffer\"] = True\n            bs_str = base64.b64encode(bs).decode(\"ascii\")\n            meta[\"pickle_bytes_str\"] = bs_str\n            meta[\"indices\"] = indices\n            data = concat_buffer_bs\n        else:\n            meta[\"with_buffer\"] = False\n            data = bs\n\n        return cls.create_payload(data, batch.shape[0], meta)", "idx": 921}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.from_payload", "completion": "        format = payload.meta.get(\"format\", \"default\")\n        if format == \"pickle5\":\n            if payload.meta.get(\"with_buffer\", False):\n                bs_str = t.cast(str, payload.meta[\"pickle_bytes_str\"])\n                bs = base64.b64decode(bs_str)\n                indices = t.cast(t.List[int], payload.meta[\"indices\"])\n                return t.cast(\"ext.PdDataFrame\", pep574_loads(bs, payload.data, indices))\n            else:\n                return pickle.loads(payload.data)\n        else:\n            return pickle.loads(payload.data)", "idx": 922}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batch_to_payloads", "completion": "        batches = cls.batch_to_batches(batch, indices, batch_dim)\n        payloads = [cls.to_payload(subbatch, batch_dim) for subbatch in batches]\n        return payloads", "idx": 923}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.from_batch_payloads", "completion": "        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)", "idx": 924}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.to_payload", "completion": "        if isinstance(batch, t.Generator):\n            batch = list(batch)\n        data = pickle.dumps(batch)\n        batch_size = len(batch) if batch_dim == 0 else len(batch[0])\n        return cls.create_payload(data, batch_size)", "idx": 925}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batch_to_payloads", "completion": "        # Convert the batch into a list of batches based on the given indices and batch dimension\n        batches = cls.batch_to_batches(batch, indices, batch_dim)\n\n        # Initialize an empty list to store the payloads\n        payloads = []\n\n        # Iterate over each subbatch\n        for subbatch in batches:\n            # Convert the subbatch into a payload and append it to the list of payloads\n            payloads.append(cls.to_payload(subbatch, batch_dim))\n\n        # Return the list of payloads\n        return payloads", "idx": 926}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.from_batch_payloads", "completion": "        batches = [cls.from_payload(payload) for payload in payloads]\n        return cls.batches_to_batch(batches, batch_dim)", "idx": 927}
{"namespace": "sslyze.cli.server_string_parser.CommandLineServerStringParser.parse_server_string", "completion": "        ip = None\n        if \"{\" in server_str and \"}\" in server_str:\n            ip = server_str.split(\"{\")[1].split(\"}\")[0]\n            server_str = server_str.replace(\"{\" + ip + \"}\", \"\")\n\n        if \"[\" in server_str and \"]\" in server_str:\n            host, port = cls._parse_ipv6_server_string(server_str)\n        elif ip and \"[\" in ip and \"]\" in ip:\n            host, port = cls._parse_ipv6_server_string(ip)\n        else:\n            host, port = cls._parse_ipv4_server_string(server_str)\n\n        return host, ip, port", "idx": 928}
{"namespace": "sslyze.plugins.heartbleed_plugin._HeartbleedCliConnector.result_to_console_output", "completion": "        output = [\"OpenSSL Heartbleed Vulnerability\"]\n        if result.is_vulnerable_to_heartbleed:\n            output.append(\"VULNERABLE - Server is vulnerable to Heartbleed\")\n        else:\n            output.append(\"NOT VULNERABLE - Server is not vulnerable to Heartbleed\")\n        return output", "idx": 929}
{"namespace": "sslyze.plugins.http_headers_plugin._HttpHeadersCliConnector.result_to_console_output", "completion": "        output = []\n\n        output.append(f\"HTTP Request Sent:\\n{result.http_request_sent}\\n\")\n\n        if result.http_error_trace:\n            output.append(f\"HTTP Error Trace:\\n{result.http_error_trace}\\n\")\n\n        if result.http_path_redirected_to:\n            output.append(f\"HTTP Path Redirected To:\\n{result.http_path_redirected_to}\\n\")\n\n        if result.strict_transport_security_header:\n            sts_header = result.strict_transport_security_header\n            output.append(\"Strict-Transport-Security Header:\")\n            output.append(f\"Max Age: {sts_header.max_age}\")\n            output.append(f\"Preload: {sts_header.preload}\")\n            output.append(f\"Include Subdomains: {sts_header.include_subdomains}\\n\")\n\n        return output", "idx": 930}
{"namespace": "sslyze.plugins.http_headers_plugin._detect_http_redirection", "completion": "    # Check if the HTTP response status is a redirection\n    if http_response.status // 100 != 3:\n        return None\n\n    # Extract the location header\n    location_header = http_response.getheader(\"Location\")\n    if not location_header:\n        return None\n\n    # Parse the location header\n    location = urlsplit(location_header)\n\n    # Check if the location is on the same server\n    if location.hostname != server_host_name or location.port != server_port:\n        return None\n\n    # Return the path to the new location\n    return location.path", "idx": 931}
{"namespace": "sslyze.plugins.session_renegotiation_plugin._SessionRenegotiationCliConnector.result_to_console_output", "completion": "        result_txt = []\n        result_txt.append(cls._cli_option)\n        result_txt.append(f\"  Supports Secure Renegotiation: {result.supports_secure_renegotiation}\")\n        result_txt.append(f\"  Vulnerable to Client Renegotiation DOS: {result.is_vulnerable_to_client_renegotiation_dos}\")\n        return result_txt", "idx": 932}
{"namespace": "sslyze.plugins.certificate_info._cli_connector._CertificateInfoCliConnector.result_to_console_output", "completion": "        result_as_txt = [cls._format_title(\"Certificate Information\")]\n        result_as_txt.append(cls._format_field(\"Hostname sent for SNI:\", result.server_hostname_indication))\n        result_as_txt.append(cls._format_field(\"Number of certificates detected:\", str(len(result.certificate_deployments))))\n\n        for index, cert_deployment in enumerate(result.certificate_deployments, start=1):\n            result_as_txt.extend(cls._cert_deployment_to_console_output(index, cert_deployment))\n\n        return result_as_txt", "idx": 933}
{"namespace": "sslyze.plugins.certificate_info._cli_connector._get_name_as_short_text", "completion": "    for attribute in name_field:\n        if attribute.oid == x509.NameOID.COMMON_NAME:\n            return attribute.value\n    return name_field.rfc4514_string()", "idx": 934}
{"namespace": "sslyze.plugins.certificate_info._symantec.SymantecDistructTester.get_distrust_timeline", "completion": "\n        for certificate in verified_certificate_chain:\n            # Get the certificate's public key and convert it to a hex string\n            public_key = binascii.hexlify(certificate.public_key().public_bytes()).decode()\n\n            # Check if the public key is in the blacklist\n            if public_key in cls._CA_KEYS_BLACKLIST:\n                return SymantecDistrustTimelineEnum.MARCH_2018\n\n            # Check if the public key is in the whitelist\n            if public_key in cls._CA_KEYS_WHITELIST:\n                return SymantecDistrustTimelineEnum.SEPTEMBER_2018\n\n        # If no distrust is detected, return None\n        return None", "idx": 935}
{"namespace": "sslyze.plugins.certificate_info._certificate_utils.parse_subject_alternative_name_extension", "completion": "    try:\n        san_extension = certificate.extensions.get_extension_for_oid(ExtensionOID.SUBJECT_ALTERNATIVE_NAME)\n    except ExtensionNotFound:\n        return SubjectAlternativeNameExtension([], [])\n\n    san = cast(SubjectAlternativeName, san_extension.value)\n\n    dns_names = [i.value for i in san.get_values_for_type(DNSName)]\n    ip_addresses = [i.exploded for i in san.get_values_for_type(IPAddress)]\n\n    return SubjectAlternativeNameExtension(dns_names, ip_addresses)", "idx": 936}
{"namespace": "sslyze.plugins.certificate_info._cert_chain_analyzer._certificate_matches_hostname", "completion": "    try:\n        # Extract the names from the certificate\n        names = certificate.subject.get_attributes_for_oid(NameOID.COMMON_NAME)\n        # Create a dictionary with the properly formatted names\n        names_dict = {name.oid._name: name.value for name in names}\n        # Check if the server_hostname matches any of the names in the certificate\n        match_hostname(names_dict, server_hostname)\n        return True\n    except CertificateError:\n        return False", "idx": 937}
{"namespace": "trafilatura.hashing.generate_hash_filename", "completion": "    # Remove XML tags\n    content = CLEAN_XML.sub(\"\", content)\n    # Generate bag-of-word hash\n    hash_value = generate_bow_hash(content, length=12)\n    # Encode hash using urlsafe_b64encode\n    encoded_hash = urlsafe_b64encode(hash_value)\n    # Return decoded string\n    return encoded_hash.decode()", "idx": 938}
{"namespace": "trafilatura.cli_utils.download_queue_processing", "completion": "    errors = []\n    sleep_time = config.getfloat('DEFAULT', 'SLEEP_TIME')\n    while url_store.done is False:\n        bufferlist, url_store = load_download_buffer(url_store, sleep_time)\n        # start several threads\n        for url, result in buffered_downloads(bufferlist, args.parallel):\n            # handle result\n            if result is not None:\n                counter = process_result(result, args, url, counter, config)\n            else:\n                errors.append(url)\n    return errors, counter", "idx": 939}
{"namespace": "trafilatura.utils.decode_response", "completion": "    # If the response is a urllib3 response object, get the data\n    if isinstance(response, HTTPResponse):\n        response = response.data\n\n    # Handle possible compression\n    response = handle_compressed_file(response)\n\n    # Try to guess the encoding and decode the response\n    for guessed_encoding in detect_encoding(response):\n        try:\n            decoded_response = response.decode(guessed_encoding)\n        except (LookupError, UnicodeDecodeError): # VISCII: lookup\n            LOGGER.warning('wrong encoding detected: %s', guessed_encoding)\n            decoded_response = None\n        else:\n            break\n\n    # If decoding failed, use utf-8 with replacement of errors\n    if decoded_response is None:\n        decoded_response = str(response, encoding='utf-8', errors='replace')\n\n    return decoded_response", "idx": 940}
{"namespace": "trafilatura.utils.txttocsv", "completion": "    import io\n    import csv\n    # Create a StringIO object\n    output = io.StringIO()\n    # Create a writer object\n    writer = csv.writer(output, delimiter='\\t', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n    # Write the header\n    writer.writerow(['URL', 'Fingerprint', 'Hostname', 'Title', 'Image', 'Date', 'License', 'Pagetype', 'ID', 'Text', 'Comments'])\n    # Write the data\n    writer.writerow([docmeta.get('url', ''), docmeta.get('fingerprint', ''), docmeta.get('hostname', ''), docmeta.get('title', ''), docmeta.get('image', ''), docmeta.get('date', ''), docmeta.get('license', ''), docmeta.get('pagetype', ''), docmeta.get('id', ''), text, comments])\n    # Get the CSV data\n    csv_data = output.getvalue()\n    # Close the StringIO object\n    output.close()\n    # Return the CSV data\n    return csv_data", "idx": 941}
{"namespace": "trafilatura.utils.is_similar_domain", "completion": "    # Remove file extensions\n    reference = STRIP_EXTENSION.sub('', reference)\n    new_string = STRIP_EXTENSION.sub('', new_string)\n\n    # Calculate similarity ratio\n    similarity_ratio = SequenceMatcher(None, reference, new_string).ratio()\n\n    # Return True if similarity ratio is above the threshold, False otherwise\n    return similarity_ratio > threshold", "idx": 942}
{"namespace": "trafilatura.xml.remove_empty_elements", "completion": "    for element in reversed(tree.xpath(\".//*\")):\n        if len(element) == 0 and not (element.text and element.text.strip()) and not (element.tail and element.tail.strip()):\n            parent = element.getparent()\n            if parent is not None:\n                parent.remove(element)\n    return tree", "idx": 943}
{"namespace": "trafilatura.xml.strip_double_tags", "completion": "    for element in reversed(list(tree.iter())):\n        if element.tag in NESTING_WHITELIST:\n            continue\n        parent = element.getparent()\n        if parent is not None and parent.tag == element.tag:\n            merge_with_parent(element)\n    return tree", "idx": 944}
{"namespace": "trafilatura.xml.check_tei", "completion": "    # remove unwanted elements\n    for element in xmldoc.iter('*'):\n        if element.tag not in TEI_VALID_TAGS:\n            LOGGER.debug('removing unwanted element: %s', element.tag)\n            element.getparent().remove(element)\n        else:\n            # remove unwanted attributes\n            for attribute in element.attrib:\n                if attribute not in TEI_VALID_ATTRS:\n                    LOGGER.debug('removing unwanted attribute: %s', attribute)\n                    del element.attrib[attribute]\n            # handle text content of div nodes\n            if element.tag == 'div':\n                _handle_text_content_of_div_nodes(element)\n            # handle unwanted tails\n            if element.tag in TEI_REMOVE_TAIL and element.tail and element.tail.strip():\n                _handle_unwanted_tails(element)\n            # handle complex head\n            if element.tag == 'head' and len(element) > 0:\n                new_element = _tei_handle_complex_head(element)\n                element.getparent().replace(element, new_element)\n    # wrap unwanted siblings of div\n    for div_element in xmldoc.iter('div'):\n        _wrap_unwanted_siblings_of_div(div_element)\n    # move elements one level up\n    for element in reversed(xmldoc.xpath(\".//div/p | .//div/list | .//div/table | .//div/quote | .//div/ab\")):\n        _move_element_one_level_up(element)\n    return xmldoc", "idx": 945}
{"namespace": "trafilatura.xml.validate_tei", "completion": "    global TEI_RELAXNG\n    # load schema\n    if TEI_RELAXNG is None:\n        try:\n            with lzma.open(TEI_SCHEMA, 'rb') as schemafile:\n                TEI_RELAXNG = RelaxNG(file=schemafile)\n        except Exception as err:\n            LOGGER.error('TEI schema could not be loaded: %s', err)\n            return False\n    # validate\n    try:\n        TEI_RELAXNG.assertValid(xmldoc)\n        return True\n    except Exception as err:\n        LOGGER.error('TEI validation error: %s', err)\n        return False", "idx": 946}
{"namespace": "trafilatura.xml.merge_with_parent", "completion": "    parent = element.getparent()\n    if parent is None:\n        return\n    # Determine element text based on text and tail\n    textelement = replace_element_text(element, include_formatting)\n    # Append element text to parent text\n    if parent.text is None:\n        parent.text = textelement\n    else:\n        parent.text += textelement\n    # Remove element from parent\n    parent.remove(element)", "idx": 947}
{"namespace": "trafilatura.downloads._determine_headers", "completion": "    # Parse the config to get the user-agents and cookie\n    myagents, mycookie = _parse_config(config)\n    \n    # If headers are not provided, use the default headers\n    if headers is None:\n        headers = DEFAULT_HEADERS.copy()\n    \n    # If user-agents are available, randomly select one and set it as the \"User-Agent\" header\n    if myagents is not None:\n        headers['User-Agent'] = random.choice(myagents)\n    \n    # If a cookie is available, set it as the \"Cookie\" header\n    if mycookie is not None:\n        headers['Cookie'] = mycookie\n    \n    return headers", "idx": 948}
{"namespace": "trafilatura.meta.reset_caches", "completion": "\n    # Reset caches for courlan\n    reset_caches_courlan()\n\n    # Reset caches for htmldate\n    reset_caches_htmldate()\n\n    # Clear Python's garbage collector\n    gc.collect()", "idx": 949}
{"namespace": "trafilatura.core.handle_table", "completion": "    # create new table and row elements\n    newtable = Element('table')\n    newrow = Element('row')\n    # strip structural elements\n    strip_tags(table_elem, 'thead', 'tbody', 'tfoot')\n    # iterate through sub-elements\n    for child in table_elem.iter('*'):\n        # handle row elements\n        if child.tag == 'tr':\n            # check if the previous row is complete\n            if len(newrow) > 0:\n                newtable.append(newrow)\n                newrow = Element('row')\n        # handle cell elements\n        elif child.tag in TABLE_ELEMS:\n            cell_element = define_cell_type(child)\n            # process text content of the cell\n            if len(child) == 0:\n                processed_child = process_node(child, options)\n                if processed_child is not None:\n                    cell_element.text = processed_child.text\n            # handle child elements with further descendants\n            else:\n                for subelem in child.iterdescendants('*'):\n                    # handle table related elements\n                    if subelem.tag in TABLE_ALL:\n                        processed_subchild = handle_textnode(subelem, options, comments_fix=False)\n                        if processed_subchild is not None:\n                            cell_element.append(processed_subchild)\n                    # ignore nested tables\n                    elif subelem.tag == 'table':\n                        continue\n                    else:\n                        processed_subchild = handle_textnode(subelem, options, comments_fix=False)\n                        if processed_subchild is not None:\n                            cell_element.append(processed_subchild)\n                    subelem.tag = 'done'\n            # append the cell to the row\n            newrow.append(cell_element)\n        child.tag = 'done'\n    # append the last row if it contains elements\n    if len(newrow) > 0:\n        newtable.append(newrow)\n    # return the new table if it contains rows, otherwise return None\n    if len(newtable) > 0:\n        return newtable\n    return None", "idx": 950}
{"namespace": "trafilatura.filters.language_filter", "completion": "    if target_language:\n        detected_language = language_classifier(temp_text, temp_comments)\n        if detected_language != target_language:\n            LOGGER.warning('Detected language (%s) does not match target language (%s)', detected_language, target_language)\n            docmeta['wronglang'] = True\n            docmeta['detected_lang'] = detected_language\n            return True, docmeta\n    return False, docmeta", "idx": 951}
{"namespace": "trafilatura.filters.textfilter", "completion": "    # Check if the element is None or empty\n    if element is None or element.text is None:\n        return True\n\n    # Check if the text is in the filter\n    if RE_FILTER.search(element.text):\n        return True\n\n    # Check if the text is only composed of spaces and/or control characters\n    if not text_chars_test(element.text):\n        return True\n\n    # If none of the conditions are met, return False\n    return False", "idx": 952}
{"namespace": "trafilatura.metadata.extract_meta_json", "completion": "    for elem in tree.xpath('.//script[@type=\"application/ld+json\" or @type=\"application/settings+json\"]'):\n        if elem.text:\n            json_text = JSON_MINIFY.sub(r'\\1', elem.text)\n            try:\n                json_obj = json.loads(json_text)\n                metadata = extract_json_metadata(json_obj, metadata)\n            except json.JSONDecodeError:\n                metadata = extract_json_metadata(json_text, metadata)\n    return metadata", "idx": 953}
{"namespace": "trafilatura.external.try_justext", "completion": "    # Initialize a new body element\n    body = Element('body')\n    # Determine the language for the stoplist\n    if target_language is None or target_language not in JT_STOPLIST:\n        stoplist = get_stoplist('English')\n    else:\n        stoplist = get_stoplist(target_language)\n    # Extract non-boilerplate paragraphs from the tree\n    try:\n        paragraphs = custom_justext(tree, stoplist)\n        for paragraph in paragraphs:\n            if not paragraph.is_boilerplate:\n                p_elem = Element('p')\n                p_elem.text = paragraph.text\n                body.append(p_elem)\n        return body\n    except Exception as err:\n        LOGGER.warning('justext failed on %s: %s', url, err)\n        return None", "idx": 954}
{"namespace": "mongoengine.base.datastructures.BaseDict.get", "completion": "        try:\n            return self.__getitem__(key)\n        except KeyError:\n            return default", "idx": 955}
{"namespace": "sqlite_utils.utils.suggest_column_types", "completion": "    all_column_types = {}\n    for record in records:\n        for key, value in record.items():\n            if key not in all_column_types:\n                all_column_types[key] = set()\n            all_column_types[key].add(type(value))\n    return types_for_column_types(all_column_types)", "idx": 956}
{"namespace": "sqlite_utils.plugins.get_plugins", "completion": "    plugins = []\n    for plugin_name, plugin in pm.list_name_plugin_pairs():\n        plugin_info = {\n            \"name\": plugin_name,\n            \"hooks\": [hook.name for hook in pm.hook.get_hookimpls(plugin)],\n        }\n        dist_info = pm.get_distribution(plugin)\n        if dist_info is not None:\n            plugin_info[\"version\"] = dist_info.version\n            plugin_info[\"project_name\"] = dist_info.project_name\n        plugins.append(plugin_info)\n    return plugins", "idx": 957}
{"namespace": "alembic.config.Config.print_stdout", "completion": "        if not self.messaging_opts.get(\"quiet\"):\n            print(text % arg, file=self.stdout)", "idx": 958}
{"namespace": "alembic.config.Config.set_section_option", "completion": "        if not self.file_config.has_section(section):\n            self.file_config.add_section(section)\n        self.file_config.set(section, name, value)", "idx": 959}
{"namespace": "alembic.command.merge", "completion": "\n    script_directory = ScriptDirectory.from_config(config)\n\n    command_args = dict(\n        message=message,\n        autogenerate=False,\n        sql=False,\n        head=\"head\",\n        splice=False,\n        branch_label=branch_label,\n        version_path=None,\n        rev_id=rev_id,\n        depends_on=None,\n    )\n    revision_context = autogen.RevisionContext(\n        config,\n        script_directory,\n        command_args,\n    )\n\n    def retrieve_migrations(rev, context):\n        revision_context.run_no_autogenerate(rev, context)\n        return []\n\n    with EnvironmentContext(\n        config,\n        script_directory,\n        fn=retrieve_migrations,\n        as_sql=False,\n        template_args=revision_context.template_args,\n        revision_context=revision_context,\n    ):\n        script_directory.run_env()\n\n    # the revision_context now has MigrationScript structure(s) present.\n\n    migration_script = revision_context.generated_revisions[-1]\n    return migration_script", "idx": 960}
{"namespace": "alembic.command.upgrade", "completion": "\n    script = ScriptDirectory.from_config(config)\n    if sql and \":\" in revision:\n        starting_rev, revision = revision.split(\":\", 2)\n    else:\n        starting_rev = None\n\n    def upgrade(rev, context):\n        return script._upgrade_revs(revision, rev)\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=upgrade,\n        as_sql=sql,\n        starting_rev=starting_rev,\n        destination_rev=revision,\n        tag=tag,\n    ):\n        script.run_env()", "idx": 961}
{"namespace": "alembic.command.downgrade", "completion": "\n    script = ScriptDirectory.from_config(config)\n\n    starting_rev = None\n    if \":\" in revision:\n        if not sql:\n            raise util.CommandError(\"Range revision not allowed\")\n        starting_rev, revision = revision.split(\":\", 2)\n\n    def downgrade(rev, context):\n        return script._downgrade_revs(revision, rev)\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=downgrade,\n        as_sql=sql,\n        starting_rev=starting_rev,\n        destination_rev=revision,\n        tag=tag,\n    ):\n        script.run_env()", "idx": 962}
{"namespace": "alembic.command.history", "completion": "\n    script = ScriptDirectory.from_config(config)\n\n    if rev_range:\n        base = rev_range\n    else:\n        base = \"base\"\n\n    if indicate_current:\n\n        def indicate_current(rev, context):\n            revs = script.get_all_current(rev)\n            if revs:\n                current = revs[0]\n            else:\n                current = None\n            return current\n\n        with EnvironmentContext(config, script, fn=indicate_current):\n            current = script.run_env()\n    else:\n        current = None\n\n    for sc in script.walk_revisions(base=base, head=\"heads\"):\n        if sc.revision == current:\n            config.print_stdout(\n                sc.cmd_format(verbose, include_branches=True)\n                + \" (current)\"\n            )\n        else:\n            config.print_stdout(sc.cmd_format(verbose, include_branches=True))", "idx": 963}
{"namespace": "alembic.command.stamp", "completion": "\n    script = ScriptDirectory.from_config(config)\n\n    def do_stamp(rev, context):\n        if purge:\n            context._purge_version_table()\n        context.stamp(script, revision)\n        return []\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=do_stamp,\n        as_sql=sql,\n        tag=tag,\n    ):\n        script.run_env()", "idx": 964}
{"namespace": "alembic.command.ensure_version", "completion": "\n    script = ScriptDirectory.from_config(config)\n\n    def ensure_version_table(rev, context):\n        if not context.get_context().impl.has_table(context.version_table):\n            if sql:\n                with context.begin_transaction():\n                    context.run_migrations()\n            else:\n                context.run_migrations()\n\n    with EnvironmentContext(\n        config,\n        script,\n        fn=ensure_version_table,\n        as_sql=sql,\n        starting_rev=None,\n        destination_rev=None,\n    ):\n        script.run_env()", "idx": 965}
{"namespace": "alembic.autogenerate.compare._compare_server_default", "completion": "    metadata_default = metadata_col.server_default\n    conn_col_default = conn_col.server_default\n    alter_column_op.existing_server_default = conn_col_default\n\n    if metadata_default is not None or conn_col_default is not None:\n        if metadata_default is None:\n            rendered_metadata_default = None\n        else:\n            rendered_metadata_default = _render_server_default_for_compare(\n                metadata_default, autogen_context\n            )\n\n        if conn_col_default is None:\n            rendered_conn_default = None\n        else:\n            rendered_conn_default = _render_server_default_for_compare(\n                conn_col_default, autogen_context\n            )\n\n        if rendered_metadata_default != rendered_conn_default:\n            if (\n                sqla_compat._server_default_is_computed(\n                    metadata_default, conn_col_default\n                )\n                and sqla_compat._nullability_might_be_unset(metadata_col)\n                or (\n                    sqla_compat._server_default_is_identity(\n                        metadata_default, conn_col_default\n                    )\n                )\n            ):\n                log.info(\n                    \"Ignoring server default on identity column '%s.%s'\",\n                    tname,\n                    cname,\n                )\n            else:\n                alter_column_op.modify_server_default = metadata_default\n                log.info(\n                    \"Detected server default on column '%s.%s'\",\n                    tname,\n                    cname,\n                )\n    return None", "idx": 966}
{"namespace": "alembic.autogenerate.render._render_server_default", "completion": "    if default is None:\n        return None\n\n    rendered = _user_defined_render(\"server_default\", default, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    if sqla_compat._server_default_is_computed(default):\n        return _render_computed(default.arg, autogen_context)\n    elif sqla_compat._server_default_is_identity(default):\n        return _render_identity(default.arg, autogen_context)\n\n    if isinstance(default.arg, sql.ColumnElement):\n        return _render_potential_expr(default.arg, autogen_context)\n\n    if repr_:\n        default = repr(default.arg)\n        if default.startswith(\"'\") and default.endswith(\"'\"):\n            default = default[1:-1]\n    return default", "idx": 967}
{"namespace": "alembic.autogenerate.render._render_constraint", "completion": "    renderer = _constraint_renderers.dispatch(constraint.__class__)\n    if renderer is not None:\n        return renderer(constraint, autogen_context, namespace_metadata)\n    else:\n        return \"UNKNOWN PYTHON OBJECT: %s\" % repr(constraint)", "idx": 968}
{"namespace": "alembic.autogenerate.render._render_unique_constraint", "completion": "\n    rendered = _user_defined_render(\"unique\", constraint, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    opts = []\n    if constraint.name:\n        opts.append(\n            (\"name\", repr(_render_gen_name(autogen_context, constraint.name)))\n        )\n\n    return \"%(prefix)sUniqueConstraint(%(cols)s%(opts)s)\" % {\n        \"prefix\": _sqlalchemy_autogenerate_prefix(autogen_context),\n        \"cols\": \", \".join(\n            \"%r\" % _ident(cast(\"Column\", c).name) for c in constraint.columns\n        ),\n        \"opts\": \", \" + (\", \".join(\"%s=%s\" % (k, v) for k, v in opts))\n        if opts\n        else \"\",\n    }", "idx": 969}
{"namespace": "alembic.autogenerate.render._render_check_constraint", "completion": "    rendered = _user_defined_render(\"check\", constraint, autogen_context)\n    if rendered is not False:\n        return rendered\n\n    if constraint._create_rule is None or constraint._create_rule(constraint):\n        if constraint.sqltext is not None:\n            sqltext_str = _render_potential_expr(constraint.sqltext, autogen_context)\n            opts = []\n            if constraint.name:\n                opts.append((\"name\", repr(_render_gen_name(autogen_context, constraint.name))))\n            return \"%(prefix)sCheckConstraint(%(sqltext)s, %(opts)s)\" % {\n                \"prefix\": _sqlalchemy_autogenerate_prefix(autogen_context),\n                \"sqltext\": sqltext_str,\n                \"opts\": \", \".join(\"%s=%s\" % (kwname, val) for kwname, val in opts),\n            }\n    return None", "idx": 970}
{"namespace": "alembic.autogenerate.api.compare_metadata", "completion": "    autogen_context = AutogenContext(context, metadata=metadata)\n    diffs = compare._produce_net_changes(autogen_context, upgrade_ops=ops.UpgradeOps([]))\n    return diffs", "idx": 971}
{"namespace": "alembic.autogenerate.api.AutogenContext._within_batch", "completion": "\n        # Set flag to indicate we are within a batch operation\n        self._has_batch = True\n        try:\n            # Yield control back to the caller to perform operations within the batch context\n            yield\n        finally:\n            # Always reset the flag when exiting the batch operation context\n            self._has_batch = False", "idx": 972}
{"namespace": "alembic.util.sqla_compat._connectable_has_table", "completion": "    if sqla_14:\n        return connectable.engine.has_table(tablename, schema=schemaname)\n    else:\n        return connectable.dialect.has_table(connectable, tablename, schema=schemaname)", "idx": 973}
{"namespace": "alembic.util.sqla_compat._get_constraint_final_name", "completion": "\n    if constraint.name is None:\n        return None\n\n    if sqla_14:\n        assert dialect is not None\n        name = dialect.identifier_preparer.format_constraint(\n            constraint, _alembic_quote=False\n        )\n    else:\n        name = constraint.name\n        if name is _NONE_NAME:\n            name = None\n        elif isinstance(name, quoted_name):\n            if name.quote is None:\n                # this is a SQLAlchemy 1.0 name that was generated using\n                # conv() - it has 'quote' set to None.  make it a plain\n                # string.\n                name = quoted_name(name, quote=False)\n            else:\n                # SQLAlchemy 1.0 name, leave as is\n                pass\n        elif isinstance(name, _NoneName):\n            name = None\n        else:\n            # SQLAlchemy 0.9 style, name is assumed to be deliverable\n            # exactly as is, so wrap in quoted_name\n            name = quoted_name(name, quote=True)\n    return name", "idx": 974}
{"namespace": "alembic.testing.env.env_file_fixture", "completion": "    dir_ = os.path.join(_get_staging_directory(), \"scripts\")\n    path = os.path.join(dir_, \"env.py\")\n    with open(path, \"w\") as f:\n        f.write(txt)", "idx": 975}
{"namespace": "alembic.testing.env._no_sql_testing_config", "completion": "    dir_ = os.path.join(_get_staging_directory(), \"scripts\")\n\n    sqlalchemy_future = \"future\" in config.db.__class__.__module__\n\n    return _write_config_file(\n        \"\"\"\n[alembic]\nscript_location = %s\nsqlalchemy.url = %s\nsqlalchemy.future = %s\nsourceless = false\n%s\n\n[loggers]\nkeys = root\n\n[handlers]\nkeys = console\n\n[logger_root]\nlevel = WARN\nhandlers = console\nqualname =\n\n[handler_console]\nclass = StreamHandler\nargs = (sys.stderr,)\nlevel = NOTSET\nformatter = generic\n\n[formatters]\nkeys = generic\n\n[formatter_generic]\nformat = %%(levelname)-5.5s [%%(name)s] %%(message)s\ndatefmt = %%H:%%M:%%S\n    \"\"\"\n        % (\n            dir_,\n            dialect,\n            \"true\" if sqlalchemy_future else \"false\",\n            directives,\n        )\n    )", "idx": 976}
{"namespace": "alembic.testing.env._write_config_file", "completion": "    cfg = _testing_config()\n    with open(cfg.config_file_name, 'w') as config_file:\n        config_file.write(text)\n    return cfg", "idx": 977}
{"namespace": "alembic.testing.env.three_rev_fixture", "completion": "    a = util.rev_id()\n    b = util.rev_id()\n    c = util.rev_id()\n\n    script = ScriptDirectory.from_config(cfg)\n    script.generate_revision(a, \"revision a\")\n    write_script(\n        script,\n        a,\n        \"\"\"\\\n\"Rev A\"\nrevision = '%s'\ndown_revision = None\n\nfrom alembic import op\n\n\ndef upgrade():\n    op.execute(\"CREATE STEP 1\")\n\n\ndef downgrade():\n    op.execute(\"DROP STEP 1\")\n\n\"\"\"\n        % a,\n    )\n\n    script.generate_revision(b, \"revision b\", head=a)\n    write_script(\n        script,\n        b,\n        \"\"\"\\\n\"Rev B\"\nrevision = '%s'\ndown_revision = '%s'\n\nfrom alembic import op\n\n\ndef upgrade():\n    op.execute(\"CREATE STEP 2\")\n\n\ndef downgrade():\n    op.execute(\"DROP STEP 2\")\n\n\"\"\"\n        % (b, a),\n    )\n\n    script.generate_revision(c, \"revision c\", head=b)\n    write_script(\n        script,\n        c,\n        \"\"\"\\\n\"Rev C\"\nrevision = '%s'\ndown_revision = '%s'\n\nfrom alembic import op\n\n\ndef upgrade():\n    op.execute(\"CREATE STEP 3\")\n\n\ndef downgrade():\n    op.execute(\"DROP STEP 3\")\n\n\"\"\"\n        % (c, b),\n    )\n\n    return a, b, c", "idx": 978}
{"namespace": "alembic.testing.env.multi_heads_fixture", "completion": "    script = ScriptDirectory.from_config(cfg)\n\n    # Generate new revisions d, e, f based on a, b, c\n    d = util.rev_id()\n    script.generate_revision(d, \"revision d\", refresh=True, head=a)\n    write_script(\n        script,\n        d,\n        \"\"\"\\\n\"Rev D\"\nrevision = '%s'\ndown_revision = '%s'\n\nfrom alembic import op\n\n\ndef upgrade():\n    op.execute(\"CREATE STEP 4\")\n\n\ndef downgrade():\n    op.execute(\"DROP STEP 4\")\n\n\"\"\" % (d, a),\n    )\n\n    e = util.rev_id()\n    script.generate_revision(e, \"revision e\", refresh=True, head=b)\n    write_script(\n        script,\n        e,\n        \"\"\"\\\n\"Rev E\"\nrevision = '%s'\ndown_revision = '%s'\n\nfrom alembic import op\n\n\ndef upgrade():\n    op.execute(\"CREATE STEP 5\")\n\n\ndef downgrade():\n    op.execute(\"DROP STEP 5\")\n\n\"\"\" % (e, b),\n    )\n\n    f = util.rev_id()\n    script.generate_revision(f, \"revision f\", refresh=True, head=c)\n    write_script(\n        script,\n        f,\n        \"\"\"\\\n\"Rev F\"\nrevision = '%s'\ndown_revision = '%s'\n\nfrom alembic import op\n\n\ndef upgrade():\n    op.execute(\"CREATE STEP 6\")\n\n\ndef downgrade():\n    op.execute(\"DROP STEP 6\")\n\n\"\"\" % (f, c),\n    )\n\n    return d, e, f", "idx": 979}
{"namespace": "alembic.testing.fixtures.capture_db", "completion": "        from sqlalchemy import event\n        from sqlalchemy import create_engine\n\n    from sqlalchemy import create_engine\n    from sqlalchemy import event\n\n    buf = io.StringIO()\n\n    if dialect not in _engs:\n        eng = create_engine(dialect, strategy=\"mock\", executor=lambda sql, *a, **kw: buf.write(sql + \"\\n\"))\n        _engs[dialect] = eng\n    else:\n        eng = _engs[dialect]\n\n    @event.listens_for(eng, \"before_cursor_execute\")\n    def before_cursor_execute(conn, cursor, statement, parameters, context, executemany):\n        buf.write(statement + \"\\n\")\n\n    return eng, buf", "idx": 980}
{"namespace": "alembic.testing.fixtures.capture_engine_context_buffer", "completion": "    engine, buf = capture_db(kw.pop(\"dialect\", \"sqlite://\"))\n    kw.update({\"dialect_name\": \"sqlite\", \"output_buffer\": buf})\n    conf = EnvironmentContext.configure\n\n    def configure(*arg, **opt):\n        opt.update(**kw)\n        return conf(*arg, **opt)\n\n    with mock.patch.object(EnvironmentContext, \"configure\", configure):\n        yield buf\n    engine.dispose()", "idx": 981}
{"namespace": "alembic.operations.schemaobj.SchemaObjects.unique_constraint", "completion": "        m = self.metadata()\n        columns = [sa_schema.Column(n, NULLTYPE) for n in local_cols]\n        t = sa_schema.Table(source, m, *columns, schema=schema)\n        u = sa_schema.UniqueConstraint(*[t.c[n] for n in local_cols], name=name, **kw)\n        t.append_constraint(u)\n        return u", "idx": 982}
{"namespace": "alembic.operations.schemaobj.SchemaObjects.index", "completion": "        t = sa_schema.Table(\n            tablename,\n            self.metadata(),\n            *[sa_schema.Column(n, NULLTYPE) for n in columns],\n            schema=schema,\n        )\n        idx = sa_schema.Index(name, *[t.c[n] for n in columns], **kw)\n        t.append_constraint(idx)\n        return idx", "idx": 983}
{"namespace": "alembic.operations.ops.DropConstraintOp.from_constraint", "completion": "        constraint_table = sqla_compat._table_for_constraint(constraint)\n        return cls(\n            sqla_compat.constraint_name_or_none(constraint.name),\n            constraint_table.name,\n            type_=constraint.__visit_name__,\n            schema=constraint_table.schema,\n            _reverse=AddConstraintOp.from_constraint(constraint),\n        )", "idx": 984}
{"namespace": "alembic.operations.ops.DropConstraintOp.to_constraint", "completion": "        if self._reverse is not None:\n            constraint = self._reverse.to_constraint()\n            constraint.name = self.constraint_name\n            constraint.table_name = self.table_name\n            constraint.schema = self.schema\n            return constraint\n        else:\n            raise ValueError(\"No reverse operation available to create a Constraint from this DropConstraintOp\")", "idx": 985}
{"namespace": "alembic.operations.ops.CreatePrimaryKeyOp.to_constraint", "completion": "        schema_obj = schemaobj.SchemaObjects(migration_context)\n        return schema_obj.primary_key_constraint(\n            self.constraint_name,\n            self.table_name,\n            self.columns,\n            schema=self.schema,\n            **self.kw,\n        )", "idx": 986}
{"namespace": "alembic.operations.ops.CreateIndexOp.from_index", "completion": "        assert index.table is not None\n        return cls(\n            index.name,  # type: ignore[arg-type]\n            index.table.name,\n            [c.name for c in index.columns],\n            schema=index.table.schema,\n            unique=index.unique,\n            **index.kwargs,\n        )", "idx": 987}
{"namespace": "alembic.operations.ops.DropIndexOp.from_index", "completion": "        assert index.table is not None\n        return cls(\n            index.name,\n            index.table.name,\n            schema=index.table.schema,\n            _reverse=CreateIndexOp.from_index(index),\n        )", "idx": 988}
{"namespace": "alembic.operations.ops.DropIndexOp.to_index", "completion": "        schema_obj = schemaobj.SchemaObjects(migration_context)\n        idx = schema_obj.index(\n            self.index_name,\n            self.table_name,\n            self.columns,\n            schema=self.schema,\n            **self.kw,\n        )\n        return idx", "idx": 989}
{"namespace": "alembic.operations.ops.CreateTableOp.from_table", "completion": "        constraints = [\n            constraint\n            for constraint in table.constraints\n            if not isinstance(constraint, PrimaryKeyConstraint)\n        ]\n        return cls(\n            table.name,\n            list(table.columns) + constraints,\n            schema=table.schema,\n            _namespace_metadata=_namespace_metadata or MetaData(),\n            _constraints_included=True,\n            comment=table.comment,\n            info=table.info,\n            prefixes=table._prefixes,\n            **table.kwargs,\n        )", "idx": 990}
{"namespace": "alembic.operations.ops.DropTableOp.from_table", "completion": "        return cls(\n            table.name,\n            schema=table.schema,\n            _namespace_metadata=_namespace_metadata,\n            _reverse=CreateTableOp.from_table(table, _namespace_metadata=_namespace_metadata),\n        )", "idx": 991}
{"namespace": "alembic.operations.ops.DropTableOp.to_table", "completion": "        schema_obj = schemaobj.SchemaObjects(migration_context)\n\n        return schema_obj.table(\n            self.table_name,\n            schema=self.schema,\n            comment=self.comment,\n            info=self.info.copy() if self.info else {},\n            prefixes=list(self.prefixes) if self.prefixes else [],\n            **self.table_kw,\n        )", "idx": 992}
{"namespace": "alembic.operations.ops.AlterColumnOp.to_diff_tuple", "completion": "        \n        mods = {\n            \"existing_type\": self.existing_type,\n            \"existing_nullable\": self.existing_nullable,\n            \"existing_server_default\": self.existing_server_default,\n            \"existing_comment\": self.existing_comment,\n            \"modify_type\": self.modify_type,\n            \"modify_nullable\": self.modify_nullable,\n            \"modify_server_default\": self.modify_server_default,\n            \"modify_comment\": self.modify_comment,\n        }\n        # Include additional keyword modifications if any\n        mods.update({k: v for k, v in self.kw.items() if k.startswith(\"modify_\")})\n        \n        return (\"alter_column\", self.schema, self.table_name, self.column_name, mods)", "idx": 993}
{"namespace": "alembic.operations.ops.AddColumnOp.reverse", "completion": "        return DropColumnOp(\n            self.table_name,\n            self.column.name,\n            schema=self.schema,\n            _reverse=self,\n        )", "idx": 994}
{"namespace": "alembic.operations.ops.DropColumnOp.reverse", "completion": "        if self._reverse is not None:\n            return self._reverse\n        else:\n            raise ValueError(\n                \"No reverse operation available for this DropColumnOp\"\n            )", "idx": 995}
{"namespace": "alembic.operations.ops.DropColumnOp.from_column_and_tablename", "completion": "        return cls(\n            tname,\n            col.name,\n            schema=schema,\n            _reverse=AddColumnOp.from_column_and_tablename(schema, tname, col),\n        )", "idx": 996}
{"namespace": "alembic.operations.ops.DropColumnOp.to_column", "completion": "        if self._reverse is not None:\n            return self._reverse.column\n        else:\n            schema_obj = schemaobj.SchemaObjects(migration_context)\n            return schema_obj.column(self.column_name, NULLTYPE)", "idx": 997}
{"namespace": "alembic.script.revision.RevisionMap.heads", "completion": "        self._revision_map\n        return self.heads", "idx": 998}
{"namespace": "alembic.script.revision.RevisionMap.add_revision", "completion": "        if revision.revision in self._revision_map and not _replace:\n            raise RevisionError(\n                \"Revision %s is already present\" % revision.revision\n            )\n\n        self._revision_map[revision.revision] = revision\n\n        if revision.branch_labels:\n            self._map_branch_labels([revision], self._revision_map)\n\n        self._add_depends_on([revision], self._revision_map)\n\n        self.bases = tuple(\n            rev.revision\n            for rev in self._revision_map.values()\n            if rev is not None and rev.is_base\n        )\n\n        self._real_bases = tuple(\n            rev.revision\n            for rev in self._revision_map.values()\n            if rev is not None and rev._is_real_base\n        )\n\n        for downrev in revision._all_down_revisions:\n            if downrev not in self._revision_map:\n                util.warn(\n                    \"Revision %s referenced from %s is not present\"\n                    % (downrev, revision)\n                )\n            else:\n                down_revision = self._revision_map[downrev]\n                down_revision.add_nextrev(revision)\n\n        self._normalize_depends_on([revision], self._revision_map)\n\n        self._real_heads = tuple(\n            rev.revision\n            for rev in self._revision_map.values()\n            if rev is not None and rev._is_real_head\n        )\n\n        self.heads = tuple(\n            rev.revision\n            for rev in self._revision_map.values()\n            if rev is not None and rev.is_head\n        )\n\n        self._add_branches([revision], self._revision_map)", "idx": 999}
{"namespace": "alembic.script.revision.RevisionMap.get_revisions", "completion": "        resolved_id, branch_label = self._resolve_revision_number(id_)\n        if len(resolved_id) > 1:\n            return tuple(self._revision_for_ident(rev_id, branch_label) for rev_id in resolved_id)\n        else:\n            resolved: Union[str, Tuple[()]] = resolved_id[0] if resolved_id else ()\n            return (self._revision_for_ident(resolved, branch_label),)", "idx": 1000}
{"namespace": "alembic.script.revision.RevisionMap.get_revision", "completion": "        if id_ is None:\n            return None\n\n        resolved_id, branch_label = self._resolve_revision_number(id_)\n        if len(resolved_id) > 1:\n            raise MultipleHeads(resolved_id, id_)\n\n        return self._revision_for_ident(resolved_id[0], branch_label)", "idx": 1001}
{"namespace": "alembic.script.revision.RevisionMap.filter_for_lineage", "completion": "        if check_against is None:\n            return tuple(targets)\n\n        check_against_rev = self.get_revision(check_against)\n        if check_against_rev is None:\n            return tuple()\n\n        filtered_targets = []\n        for target in targets:\n            target_rev = self.get_revision(target)\n            if target_rev is None:\n                continue\n\n            if self._shares_lineage(\n                target_rev, check_against_rev, include_dependencies=include_dependencies\n            ):\n                filtered_targets.append(target)\n\n        return tuple(filtered_targets)", "idx": 1002}
{"namespace": "alembic.script.revision.RevisionMap.iterate_revisions", "completion": "\n        if select_for_downgrade:\n            revisions, heads = self._collect_downgrade_revisions(\n                upper, lower, inclusive, implicit_base, assert_relative_length\n            )\n        else:\n            revisions, heads = self._collect_upgrade_revisions(\n                upper, lower, inclusive, implicit_base, assert_relative_length\n            )\n\n        for rev_id in self._topological_sort(revisions, heads):\n            yield self.get_revision(rev_id)", "idx": 1003}
{"namespace": "alembic.script.revision.RevisionMap._topological_sort", "completion": "\n        # Create a dictionary to store the revisions and their dependencies\n        graph = {rev.revision: set(rev._all_down_revisions) for rev in revisions}\n\n        # Create a list to store the sorted revisions\n        sorted_revisions = []\n\n        # While there are revisions in the graph\n        while graph:\n\n            # Find all revisions with no dependencies\n            leaves = [rev for rev, deps in graph.items() if not deps]\n\n            # If there are no leaves and the graph is not empty, there is a cycle\n            if not leaves and graph:\n                raise CycleDetected(list(graph.keys()))\n\n            # Add the leaves to the sorted revisions\n            sorted_revisions.extend(leaves)\n\n            # Remove the leaves from the graph\n            graph = {rev: (deps - set(leaves)) for rev, deps in graph.items() if rev not in leaves}\n\n        # Return the sorted revisions\n        return sorted_revisions", "idx": 1004}
{"namespace": "alembic.script.revision.Revision._all_down_revisions", "completion": "        return util.dedupe_tuple(\n            util.to_tuple(self.down_revision, default=())\n            + self._resolved_dependencies\n        )", "idx": 1005}
{"namespace": "alembic.script.revision.Revision._normalized_down_revisions", "completion": "        return self._normalized_resolved_dependencies", "idx": 1006}
{"namespace": "alembic.script.write_hooks._invoke", "completion": "\n    formatter = _registry.get(name)\n    if formatter is None:\n        raise util.CommandError(f\"No formatter with name '{name}' registered\")\n\n    formatter(revision, options)", "idx": 1007}
{"namespace": "bplustree.memory.FileMemory.get_node", "completion": "        node = self._cache.get(page)\n        if node is None:\n            data = self.get_page(page)\n            node = Node.load(data, self._tree_conf)\n            self._cache[page] = node\n        return node", "idx": 1008}
{"namespace": "bplustree.memory.FileMemory.next_available_page", "completion": "        page = self.last_page\n        self.last_page += 1\n        return page", "idx": 1009}
{"namespace": "bplustree.memory.FileMemory.get_metadata", "completion": "        data = self._read_page(0)\n        root_node_page = int.from_bytes(data[0:PAGE_REFERENCE_BYTES], ENDIAN)\n        page_size = int.from_bytes(data[PAGE_REFERENCE_BYTES:PAGE_REFERENCE_BYTES+OTHERS_BYTES], ENDIAN)\n        order = int.from_bytes(data[PAGE_REFERENCE_BYTES+OTHERS_BYTES:PAGE_REFERENCE_BYTES+2*OTHERS_BYTES], ENDIAN)\n        key_size = int.from_bytes(data[PAGE_REFERENCE_BYTES+2*OTHERS_BYTES:PAGE_REFERENCE_BYTES+3*OTHERS_BYTES], ENDIAN)\n        value_size = int.from_bytes(data[PAGE_REFERENCE_BYTES+3*OTHERS_BYTES:PAGE_REFERENCE_BYTES+4*OTHERS_BYTES], ENDIAN)\n        tree_conf = TreeConf(page_size, order, key_size, value_size)\n        return root_node_page, tree_conf", "idx": 1010}
{"namespace": "bplustree.memory.FileMemory.set_metadata", "completion": "        data = (\n            root_node_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN) +\n            tree_conf.page_size.to_bytes(OTHERS_BYTES, ENDIAN) +\n            tree_conf.order.to_bytes(OTHERS_BYTES, ENDIAN) +\n            tree_conf.key_size.to_bytes(OTHERS_BYTES, ENDIAN) +\n            tree_conf.value_size.to_bytes(OTHERS_BYTES, ENDIAN)\n        )\n        self._write_page_in_tree(0, data)\n        self._tree_conf = tree_conf", "idx": 1011}
{"namespace": "bplustree.memory.WAL.checkpoint", "completion": "        if self._not_committed_pages:\n            logger.warning('WAL has uncommitted data, discarding it')\n            self._not_committed_pages = dict()\n\n        fsync_file_and_dir(self._fd.fileno(), self._dir_fd)\n\n        for page, page_start in self._committed_pages.items():\n            page_data = read_from_file(self._fd, page_start,\n                                       page_start + self._page_size)\n            yield page, page_data\n\n        self._fd.close()\n        os.remove(self.filename)\n        if self._dir_fd is not None:\n            os.fsync(self._dir_fd)", "idx": 1012}
{"namespace": "bplustree.memory.WAL.commit", "completion": "        if self._not_committed_pages:\n            self._add_frame(FrameType.COMMIT)\n\n        \"\"\"\n        If there are uncommitted pages in the WAL, a commit frame is added.\n        Input-Output Arguments\n        :param self: WAL. An instance of the WAL class.\n        :return: No return values.\n        \"\"\"", "idx": 1013}
{"namespace": "bplustree.memory.WAL.rollback", "completion": "        if self._not_committed_pages:\n            self._add_frame(FrameType.ROLLBACK)\n\n        \"\"\"\n        If there are uncommitted pages in the WAL, a rollback frame is added.\n        Input-Output Arguments\n        :param self: WAL. An instance of the WAL class.\n        :return: No return values.\n        \"\"\"", "idx": 1014}
{"namespace": "bplustree.entry.Record.dump", "completion": "        key_as_bytes = self._tree_conf.serializer.serialize(\n            self.key, self._tree_conf.key_size\n        )\n        used_key_length = len(key_as_bytes)\n\n        if self.overflow_page:\n            value_as_bytes = b''\n            used_value_length = 0\n        else:\n            value_as_bytes = self.value\n            used_value_length = len(value_as_bytes)\n\n        data = (\n            used_key_length.to_bytes(USED_KEY_LENGTH_BYTES, ENDIAN) +\n            key_as_bytes +\n            bytes(self._tree_conf.key_size - used_key_length) +\n            used_value_length.to_bytes(USED_VALUE_LENGTH_BYTES, ENDIAN) +\n            value_as_bytes +\n            bytes(self._tree_conf.value_size - used_value_length) +\n            (self.overflow_page or 0).to_bytes(PAGE_REFERENCE_BYTES, ENDIAN)\n        )\n        return data", "idx": 1015}
{"namespace": "bplustree.entry.Reference.__repr__", "completion": "        return '<Reference: key={} before={} after={}>'.format(self.key, self.before, self.after)", "idx": 1016}
{"namespace": "bplustree.node.Node.dump", "completion": "        data = bytearray()\n        for entry in self.entries:\n            data.extend(entry.dump())\n\n        used_page_length = len(data) + NODE_TYPE_BYTES + USED_PAGE_LENGTH_BYTES + PAGE_REFERENCE_BYTES\n        header = bytearray()\n        header.extend(self._node_type_int.to_bytes(NODE_TYPE_BYTES, ENDIAN))\n        header.extend(used_page_length.to_bytes(USED_PAGE_LENGTH_BYTES, ENDIAN))\n        next_page = self.next_page if self.next_page is not None else 0\n        header.extend(next_page.to_bytes(PAGE_REFERENCE_BYTES, ENDIAN))\n\n        data = header + data\n        padding_length = self._tree_conf.page_size - len(data)\n        data.extend(bytearray(padding_length))\n\n        return data", "idx": 1017}
{"namespace": "bplustree.node.Node._find_entry_index", "completion": "        entry = self._entry_class(self._tree_conf, key=key)\n        index = bisect.bisect_left(self.entries, entry)\n        if index != len(self.entries) and self.entries[index].key == key:\n            return index\n        raise ValueError('No entry found with the given key.')", "idx": 1018}
{"namespace": "bplustree.node.Node.from_page_data", "completion": "        node_type = int.from_bytes(data[:NODE_TYPE_BYTES], ENDIAN)\n        if node_type == 1:\n            return LonelyRootNode(tree_conf, data, page)\n        elif node_type == 2:\n            return RootNode(tree_conf, data, page)\n        elif node_type == 3:\n            return InternalNode(tree_conf, data, page)\n        elif node_type == 4:\n            return LeafNode(tree_conf, data, page)\n        else:\n            raise ValueError('Invalid node type: {}'.format(node_type))", "idx": 1019}
{"namespace": "bplustree.tree.BPlusTree._root_node", "completion": "        return self._mem.get_node(self._root_node_page)", "idx": 1020}
{"namespace": "bplustree.tree.BPlusTree._left_record_node", "completion": "        node = self._root_node\n        while not isinstance(node, (LonelyRootNode, LeafNode)):\n            node = self._mem.get_node(node.smallest_entry.before)\n        return node", "idx": 1021}
{"namespace": "mopidy.ext.Extension.get_cache_dir", "completion": "        if cls.ext_name is None:\n            raise AssertionError\n        cache_dir_path = (\n            path.expand_path(config[\"core\"][\"cache_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(cache_dir_path)\n        return cache_dir_path", "idx": 1022}
{"namespace": "mopidy.ext.Extension.get_config_dir", "completion": "        if cls.ext_name is None:\n            raise AssertionError\n        config_dir_path = (\n            path.expand_path(config[\"core\"][\"config_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(config_dir_path)\n        return config_dir_path", "idx": 1023}
{"namespace": "mopidy.ext.Extension.get_data_dir", "completion": "        if cls.ext_name is None:\n            raise AssertionError\n        data_dir_path = (\n            path.expand_path(config[\"core\"][\"data_dir\"]) / cls.ext_name\n        )\n        path.get_or_create_dir(data_dir_path)\n        return data_dir_path", "idx": 1024}
{"namespace": "mopidy.ext.load_extensions", "completion": "    installed_extensions = []\n\n    for entry_point in pkg_resources.iter_entry_points(\"mopidy.ext\"):\n        try:\n            extension_class = entry_point.load()\n            if issubclass(extension_class, Extension):\n                extension = extension_class()\n                extension_data = ExtensionData(\n                    extension=extension,\n                    entry_point=entry_point,\n                    config_schema=extension.get_config_schema(),\n                    config_defaults=extension.get_default_config(),\n                    command=extension.get_command(),\n                )\n                installed_extensions.append(extension_data)\n            else:\n                logger.warning(\n                    \"Entry point %s does not point to an extension\",\n                    entry_point.name,\n                )\n        except Exception as e:\n            logger.exception(\n                \"Loading entry point %s failed: %s\", entry_point.name, e\n            )\n\n    return installed_extensions", "idx": 1025}
{"namespace": "mopidy.ext.validate_extension_data", "completion": "    # Check if the extension's entry point name matches its extension name\n    if data.entry_point.name != data.extension.ext_name:\n        logger.error(\n            \"Extension's entry point name does not match its extension name: %s != %s\",\n            data.entry_point.name,\n            data.extension.ext_name,\n        )\n        return False\n\n    # Check if the required dependencies are installed\n    try:\n        pkg_resources.require(data.extension.dist_name)\n    except pkg_resources.DistributionNotFound:\n        logger.error(\n            \"Required dependencies for extension %s are not installed\",\n            data.extension.ext_name,\n        )\n        return False\n\n    # Check if the environment is valid\n    try:\n        data.extension.validate_environment()\n    except Exception as e:\n        logger.error(\n            \"Extension %s cannot run in the current environment: %s\",\n            data.extension.ext_name,\n            e,\n        )\n        return False\n\n    # Check if the extension has a valid config schema and default config\n    if not isinstance(data.config_schema, ConfigSchema) or not data.config_defaults:\n        logger.error(\n            \"Extension %s does not have a valid config schema and default config\",\n            data.extension.ext_name,\n        )\n        return False\n\n    return True", "idx": 1026}
{"namespace": "mopidy.httpclient.format_user_agent", "completion": "    if name is None:\n        name = \"Unknown\"\n\n    mopidy_version = platform.python_version()\n    python_version = platform.python_version()\n\n    return f\"{name} Mopidy/{mopidy_version} Python/{python_version}\"", "idx": 1027}
{"namespace": "mopidy.models.immutable.ValidatedImmutableObject.replace", "completion": "        other = copy.copy(self)\n        for key, value in kwargs.items():\n            if not self._is_valid_field(key):\n                raise TypeError(\n                    f\"replace() got an unexpected keyword argument {key!r}\"\n                )\n            other._set_field(key, value)\n        return self.__class__._instances.setdefault(weakref.ref(other), other)", "idx": 1028}
{"namespace": "mopidy.http.Extension.get_default_config", "completion": "        conf_file = os.path.join(os.path.dirname(__file__), \"ext.conf\")\n        return config_lib.load(conf_file)", "idx": 1029}
{"namespace": "mopidy.http.Extension.get_config_schema", "completion": "        schema = super().get_config_schema()\n        schema[\"hostname\"] = config_lib.Hostname()\n        schema[\"port\"] = config_lib.Port()\n        schema[\"zeroconf\"] = config_lib.String()\n        schema[\"allowed_origins\"] = config_lib.List(optional=True)\n        schema[\"csrf_protection\"] = config_lib.Boolean()\n        return schema", "idx": 1030}
{"namespace": "mopidy.internal.network.try_ipv6_socket", "completion": "    try:\n        socket.socket(socket.AF_INET6)\n        return True\n    except OSError:\n        logger.debug(\"IPv6 not supported on this system.\")\n        return False", "idx": 1031}
{"namespace": "mopidy.internal.network.format_hostname", "completion": "    # Check if the hostname is an IPv6 address\n    if re.match(r'^([0-9a-fA-F]{0,4}:){1,7}[0-9a-fA-F]{0,4}$', hostname):\n        # Convert to IPv4-mapped IPv6 address format\n        ipv4_parts = hostname.split(':')[-4:]\n        ipv4_address = '.'.join(str(int(part, 16)) for part in ipv4_parts)\n        return '::ffff:' + ipv4_address\n    else:\n        return hostname", "idx": 1032}
{"namespace": "mopidy.internal.xdg.get_dirs", "completion": "    xdg_dirs = {\n        \"XDG_CACHE_HOME\": pathlib.Path(\"~/.cache\").expanduser(),\n        \"XDG_CONFIG_HOME\": pathlib.Path(\"~/.config\").expanduser(),\n        \"XDG_DATA_HOME\": pathlib.Path(\"~/.local/share\").expanduser(),\n        \"XDG_RUNTIME_DIR\": pathlib.Path(\"/run/user/{}\".format(os.getuid())),\n    }\n\n    for var, default in xdg_dirs.items():\n        path = os.environ.get(var, default)\n        xdg_dirs[var] = pathlib.Path(path).expanduser()\n\n    xdg_config_dir = xdg_dirs[\"XDG_CONFIG_HOME\"]\n    user_dirs = _get_user_dirs(xdg_config_dir)\n    xdg_dirs.update(user_dirs)\n\n    return xdg_dirs", "idx": 1033}
{"namespace": "mopidy.internal.log.get_verbosity_level", "completion": "\n    if args_verbosity_level is not None:\n        verbosity_level = base_verbosity_level + args_verbosity_level\n    else:\n        verbosity_level = base_verbosity_level + logging_config[\"verbosity\"]\n\n    min_level = min(LOG_LEVELS.keys())\n    max_level = max(LOG_LEVELS.keys())\n\n    if verbosity_level < min_level:\n        verbosity_level = min_level\n    elif verbosity_level > max_level:\n        verbosity_level = max_level\n\n    return verbosity_level", "idx": 1034}
{"namespace": "mopidy.internal.validation.check_instance", "completion": "    if not isinstance(arg, cls):\n        raise exceptions.ValidationError(msg.format(arg=arg, name=cls.__name__))", "idx": 1035}
{"namespace": "mopidy.internal.validation.check_instances", "completion": "    _check_iterable(arg, msg)\n    for a in arg:\n        check_instance(a, cls, msg)", "idx": 1036}
{"namespace": "mopidy.internal.validation.check_uri", "completion": "\n    if not isinstance(arg, str):\n        raise exceptions.ValidationError(msg.format(arg=arg))\n    parsed_uri = urllib.parse.urlparse(arg)\n    if not parsed_uri.scheme:\n        raise exceptions.ValidationError(msg.format(arg=arg))", "idx": 1037}
{"namespace": "mopidy.internal.validation.check_uris", "completion": "    _check_iterable(arg, msg)\n    for uri in arg:\n        check_uri(uri, msg)", "idx": 1038}
{"namespace": "mopidy.internal.playlists.parse", "completion": "\n    handlers = {\n        detect_extm3u_header: parse_extm3u,\n        detect_pls_header: parse_pls,\n        detect_xspf_header: parse_xspf,\n        detect_asx_header: parse_asx,\n    }\n\n    for detector, parser in handlers.items():\n        if detector(data):\n            return list(parser(data))\n\n    return list(parse_urilist(data))", "idx": 1039}
{"namespace": "mopidy.config.schemas.ConfigSchema.deserialize", "completion": "        errors = {}\n        result = {}\n        for key in self.keys():\n            if key in values:\n                try:\n                    result[key] = self[key].deserialize(values[key])\n                except ValueError as e:  # deserialization failed\n                    result[key] = None\n                    errors[key] = str(e)\n            else:\n                errors[key] = 'Key not found in schema'\n        # Check for deprecated keys\n        for key in list(result.keys()):\n            if self[key].deprecated:\n                del result[key]\n        return result, errors", "idx": 1040}
{"namespace": "mopidy.config.types.String.deserialize", "completion": "        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        if self._transformer:\n            value = self._transformer(value)\n        validators.validate_choice(value, self._choices)\n        return value", "idx": 1041}
{"namespace": "mopidy.config.types.String.serialize", "completion": "        if value is None:\n            return \"\"\n        if isinstance(value, _TransformedValue):\n            value = value.original\n        return encode(str(value))", "idx": 1042}
{"namespace": "mopidy.config.types.Secret.serialize", "completion": "        if value is not None and display:\n            return \"********\"\n        return super().serialize(value)", "idx": 1043}
{"namespace": "mopidy.config.types.Integer.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = int(value)\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        validators.validate_choice(value, self._choices)\n        return value", "idx": 1044}
{"namespace": "mopidy.config.types.Float.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = float(value)\n        validators.validate_minimum(value, self._minimum)\n        validators.validate_maximum(value, self._maximum)\n        return value", "idx": 1045}
{"namespace": "mopidy.config.types.Boolean.deserialize", "completion": "        value = decode(value)\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        value = value.lower()\n        if value in self.true_values:\n            return True\n        elif value in self.false_values:\n            return False\n        else:\n            raise ValueError(f\"{value!r} is not a boolean\")", "idx": 1046}
{"namespace": "mopidy.config.types.Pair.deserialize", "completion": "        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n\n        if self._separator in value:\n            first_value, second_value = value.split(self._separator, 1)\n        elif self._optional_pair:\n            first_value = second_value = value\n        else:\n            raise ValueError(f\"config value must include separator {self._separator}\")\n\n        first_value = self._subtypes[0].deserialize(first_value.strip())\n        second_value = self._subtypes[1].deserialize(second_value.strip())\n\n        return first_value, second_value", "idx": 1047}
{"namespace": "mopidy.config.types.Pair.serialize", "completion": "        if value is None:\n            return \"\"\n\n        serialized_first = self._subtypes[0].serialize(value[0], display=display)\n        serialized_second = self._subtypes[1].serialize(value[1], display=display)\n\n        if not display and self._optional_pair and serialized_first == serialized_second:\n            return serialized_first\n\n        return self._separator.join([serialized_first, serialized_second])", "idx": 1048}
{"namespace": "mopidy.config.types.List.serialize", "completion": "        if value is None:\n            return \"\"\n        serialized_values = [\n            self._subtype.serialize(v, display=display) for v in value\n        ]\n        return \"\\n\".join(serialized_values)", "idx": 1049}
{"namespace": "mopidy.config.types.LogColor.deserialize", "completion": "        value = decode(value)\n        validators.validate_choice(value.lower(), log.COLORS)\n        return value.lower()", "idx": 1050}
{"namespace": "mopidy.config.types.LogColor.serialize", "completion": "        if value in log.COLORS:\n            return encode(value)\n        return \"\"", "idx": 1051}
{"namespace": "mopidy.config.types.LogLevel.deserialize", "completion": "        value = decode(value)\n        if value.lower() not in self.levels:\n            raise ValueError(f\"invalid log level: {value!r}\")\n        return self.levels[value.lower()]", "idx": 1052}
{"namespace": "mopidy.config.types.LogLevel.serialize", "completion": "        for level, level_value in self.levels.items():\n            if level_value == value:\n                return level\n        return \"\"", "idx": 1053}
{"namespace": "mopidy.config.types.Hostname.deserialize", "completion": "        value = decode(value).strip()\n        validators.validate_required(value, self._required)\n        if not value:\n            return None\n        if path.is_unix_socket(value):\n            return str(value)\n        if socket.gethostbyname(value):\n            return value\n        raise ValueError(f\"invalid hostname: {value!r}\")", "idx": 1054}
{"namespace": "mopidy.config.load", "completion": "\n    config_dir = pathlib.Path(__file__).parent\n    defaults = [read(config_dir / \"default.conf\")]\n    defaults.extend(ext_defaults)\n\n    raw_config = _load(files, defaults, overrides)\n\n    schemas = _schemas[:]\n    schemas.extend(ext_schemas)\n\n    config, errors = _validate(raw_config, schemas)\n\n    if errors:\n        for section, error in errors.items():\n            logger.error(f\"Config validation error at {section}: {error}\")\n        raise ValueError(\"Config validation failed\")\n\n    return Proxy(config)", "idx": 1055}
{"namespace": "mopidy.config.format_initial", "completion": "        from mopidy.config import keyring\n    from mopidy.config import keyring\n    config_dir = pathlib.Path(__file__).parent\n    defaults = [read(config_dir / \"default.conf\")]\n    ext_defaults = [ext.default_config for ext in extensions_data]\n    defaults.extend(ext_defaults)\n    raw_config = _load([], defaults, keyring.fetch())\n\n    schemas = _schemas[:]\n    ext_schemas = [ext.config_schema for ext in extensions_data]\n    schemas.extend(ext_schemas)\n    config, errors = _validate(raw_config, schemas)\n\n    versions = \"\\n#   \".join(\n        f\"{ext.dist_name} {ext.version}\" for ext in extensions_data\n    )\n    header = _INITIAL_HELP.format(versions=versions)\n    formatted_config = format(config, ext_schemas, display=False)\n    return f\"{header}\\n\\n{formatted_config}\"", "idx": 1056}
{"namespace": "mopidy.config._load", "completion": "    parser = configparser.RawConfigParser()\n    parser.optionxform = str  # Preserve case\n    parser.inline_comment_prefixes = (\";\",)\n\n    # Load defaults\n    for default in defaults:\n        parser.read_string(_preprocess(default))\n\n    # Load from files\n    for file in files:\n        if isinstance(file, pathlib.Path) and file.is_dir():\n            for child in file.iterdir():\n                if child.suffix == \".conf\":\n                    _load_file(parser, child)\n        else:\n            _load_file(parser, pathlib.Path(file))\n\n    # Convert to raw config\n    raw_config = {}\n    for section in parser.sections():\n        raw_config[section] = {}\n        for key, value in parser.items(section):\n            raw_config[section][key] = value\n\n    # Apply overrides\n    for section, key, value in overrides:\n        if section not in raw_config:\n            raw_config[section] = {}\n        raw_config[section][key] = value\n\n    return raw_config", "idx": 1057}
{"namespace": "mopidy.config._validate", "completion": "    validated_config = {}\n    errors = {}\n\n    for schema in schemas:\n        section = raw_config.get(schema.name, {})\n        try:\n            validated_config[schema.name] = schema.deserialize(section)\n        except ValueError as e:\n            errors[schema.name] = str(e)\n\n    for section in raw_config:\n        if section not in validated_config:\n            logger.warning(f\"Unknown config section '{section}'\")\n\n    return validated_config, errors", "idx": 1058}
{"namespace": "mingus.extra.tunings.get_tunings", "completion": "    result = []\n    searchi = str.upper(instrument) if instrument else None\n    keys = list(_known.keys())\n    for x in keys:\n        if (\n            searchi is None\n            or searchi not in keys and x.find(searchi) == 0\n            or searchi in keys and x == searchi\n        ):\n            for (desc, tun) in six.iteritems(_known[x][1]):\n                if nr_of_strings is None and nr_of_courses is None:\n                    result.append(tun)\n                elif nr_of_strings is not None and nr_of_courses is None:\n                    if tun.count_strings() == nr_of_strings:\n                        result.append(tun)\n                elif nr_of_strings is None and nr_of_courses is not None:\n                    if tun.count_courses() == nr_of_courses:\n                        result.append(tun)\n                else:\n                    if (\n                        tun.count_courses() == nr_of_courses\n                        and tun.count_strings() == nr_of_strings\n                    ):\n                        result.append(tun)\n    return result", "idx": 1059}
{"namespace": "mingus.containers.instrument.Instrument.note_in_range", "completion": "        if isinstance(note, six.string_types):\n            note = Note(note)\n        if not hasattr(note, \"name\"):\n            raise UnexpectedObjectError(\n                \"Unexpected object '%s'. \"\n                \"Expecting a mingus.containers.Note object\" % note\n            )\n        return self.range[0] <= note <= self.range[1]", "idx": 1060}
{"namespace": "mingus.containers.instrument.Guitar.can_play_notes", "completion": "        if len(notes) > 6:\n            return False\n        else:\n            return super(Guitar, self).can_play_notes(notes)", "idx": 1061}
{"namespace": "mingus.containers.bar.Bar.get_range", "completion": "        if not self.bar:\n            return None\n\n        highest_note = self.bar[0][2].get_highest_note()\n        lowest_note = self.bar[0][2].get_lowest_note()\n\n        for cont in self.bar:\n            note_container = cont[2]\n            highest_note = max(highest_note, note_container.get_highest_note())\n            lowest_note = min(lowest_note, note_container.get_lowest_note())\n\n        return highest_note, lowest_note", "idx": 1062}
{"namespace": "mingus.containers.bar.Bar.transpose", "completion": "        for cont in self.bar:\n            cont[2].transpose(interval, up)", "idx": 1063}
{"namespace": "mingus.containers.bar.Bar.determine_chords", "completion": "                from mingus.core import chords\n        from mingus.core import chords\n        res = []\n        for x in self.bar:\n            res.append(\n                [\n                    x[0],\n                    chords.determine(\n                        x[2].get_note_names(), shorthand\n                    ),\n                ]\n            )\n        return res", "idx": 1064}
{"namespace": "mingus.containers.note.Note.transpose", "completion": "        if up:\n            self.name = notes.transpose(self.name, interval)\n        else:\n            self.name = notes.transpose_down(self.name, interval)", "idx": 1065}
{"namespace": "mingus.containers.note.Note.from_int", "completion": "        self.name = notes.int_to_note(integer % 12)\n        self.octave = integer // 12\n        return self", "idx": 1066}
{"namespace": "mingus.containers.note.Note.to_hertz", "completion": "        value = (self.octave + 1) * 12 + notes.note_to_int(self.name) - 9\n        return standard_pitch * (2 ** ((value - 12) / 12.0))", "idx": 1067}
{"namespace": "mingus.containers.note.Note.from_hertz", "completion": "        diff = 12 * log(hertz / standard_pitch, 2)\n        integer = round(diff) + 57\n        return self.from_int(integer)", "idx": 1068}
{"namespace": "mingus.containers.note.Note.to_shorthand", "completion": "        if self.octave < 3:\n            return self.name.lower() + (\",\" * (3 - self.octave))\n        elif self.octave == 3:\n            return self.name.lower()\n        elif self.octave == 4:\n            return self.name\n        else:\n            return self.name + (\"'\" * (self.octave - 4))", "idx": 1069}
{"namespace": "mingus.containers.note_container.NoteContainer.from_chord_shorthand", "completion": "        self.empty()\n        notes = chords.from_shorthand(shorthand)\n        if notes is not None:\n            self.add_notes(notes)\n        return self", "idx": 1070}
{"namespace": "mingus.containers.note_container.NoteContainer.from_interval_shorthand", "completion": "        self.empty()\n        if isinstance(startnote, six.string_types):\n            startnote = Note(startnote)\n        interval_note = intervals.from_shorthand(startnote, shorthand, up)\n        self.add_notes(interval_note)\n        return self", "idx": 1071}
{"namespace": "mingus.containers.note_container.NoteContainer.from_progression_shorthand", "completion": "        self.empty()\n        progression = chords.progression_from_shorthand(shorthand, key)\n        for chord in progression:\n            self.add_notes(chord)\n        return self", "idx": 1072}
{"namespace": "mingus.containers.note_container.NoteContainer.transpose", "completion": "        for note in self.notes:\n            note.transpose(interval, up)\n        return self", "idx": 1073}
{"namespace": "mingus.containers.note_container.NoteContainer.get_note_names", "completion": "        note_names = []\n        for note in self.notes:\n            if note.name not in note_names:\n                note_names.append(note.name)\n        return note_names", "idx": 1074}
{"namespace": "mingus.core.notes.int_to_note", "completion": "    if not 0 <= note_int <= 11:\n        raise RangeError(\"Note integer out of range\")\n\n    note_dict_reverse = {v: k for k, v in _note_dict.items()}\n    note = note_dict_reverse[note_int % 7]\n\n    if accidentals == \"#\":\n        for _ in range(note_int // 7):\n            note = augment(note)\n    elif accidentals == \"b\":\n        for _ in range(note_int // 7):\n            note = diminish(note)\n    else:\n        raise NoteFormatError(\"Unknown accidental format '%s'\" % accidentals)\n\n    return note", "idx": 1075}
{"namespace": "mingus.core.notes.is_valid_note", "completion": "    if len(note) == 0:\n        return False\n    if note[0] not in _note_dict:\n        return False\n    for char in note[1:]:\n        if char not in ['#', 'b']:\n            return False\n    return True", "idx": 1076}
{"namespace": "mingus.core.notes.reduce_accidentals", "completion": "    # Convert the note to an integer\n    note_int = note_to_int(note)\n\n    # Determine the type of accidental to use based on the note\n    if '#' in note:\n        accidental = '#'\n    else:\n        accidental = 'b'\n\n    # Convert the integer back to a note using the chosen accidental\n    reduced_note = int_to_note(note_int, accidental)\n\n    return reduced_note", "idx": 1077}
{"namespace": "mingus.core.notes.remove_redundant_accidentals", "completion": "    # Convert the note to an integer\n    note_int = note_to_int(note)\n    \n    # Convert the integer back to a note, this will automatically remove any redundant accidentals\n    return int_to_note(note_int)", "idx": 1078}
{"namespace": "mingus.core.intervals.minor_second", "completion": "    sec = second(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sec, 1)", "idx": 1079}
{"namespace": "mingus.core.intervals.major_second", "completion": "    sec = second(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sec, 2)", "idx": 1080}
{"namespace": "mingus.core.intervals.minor_third", "completion": "    trd = third(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, trd, 3)", "idx": 1081}
{"namespace": "mingus.core.intervals.minor_fourth", "completion": "    frt = fourth(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, frt, 4)", "idx": 1082}
{"namespace": "mingus.core.intervals.minor_seventh", "completion": "    sth = seventh(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sth, 10)", "idx": 1083}
{"namespace": "mingus.core.intervals.major_seventh", "completion": "    sth = seventh(note[0], \"C\")\n    return augment_or_diminish_until_the_interval_is_right(note, sth, 11)", "idx": 1084}
{"namespace": "mingus.core.intervals.measure", "completion": "    # Convert the notes to integers\n    n1 = notes.note_to_int(note1)\n    n2 = notes.note_to_int(note2)\n\n    # Calculate the difference between the notes\n    diff = n2 - n1\n\n    # If the difference is negative, add 12 to get the correct interval\n    if diff < 0:\n        diff += 12\n\n    return diff", "idx": 1085}
{"namespace": "mingus.core.intervals.determine", "completion": "    interval_names = [\n        \"unison\",\n        \"minor second\",\n        \"major second\",\n        \"minor third\",\n        \"major third\",\n        \"perfect fourth\",\n        \"tritone\",\n        \"perfect fifth\",\n        \"minor sixth\",\n        \"major sixth\",\n        \"minor seventh\",\n        \"major seventh\",\n        \"octave\",\n    ]\n\n    interval_shorthand = [\"1\", \"b2\", \"2\", \"b3\", \"3\", \"4\", \"b5\", \"5\", \"b6\", \"6\", \"b7\", \"7\", \"8\"]\n\n    interval = measure(note1, note2)\n\n    if shorthand:\n        return interval_shorthand[interval]\n    else:\n        return interval_names[interval]", "idx": 1086}
{"namespace": "mingus.core.intervals.from_shorthand", "completion": "    if not notes.is_valid_note(note):\n        return False\n\n    if interval[0] == '#':\n        interval = interval[1:]\n        note = notes.augment(note)\n    elif interval[0] == 'b':\n        interval = interval[1:]\n        note = notes.diminish(note)\n\n    if not interval.isdigit() or int(interval) < 1 or int(interval) > 7:\n        return False\n\n    if up:\n        for _ in range(int(interval)):\n            note = notes.augment(note)\n    else:\n        for _ in range(int(interval)):\n            note = notes.diminish(note)\n\n    return note", "idx": 1087}
{"namespace": "mingus.core.intervals.is_consonant", "completion": "    return is_perfect_consonant(note1, note2, include_fourths) or is_imperfect_consonant(note1, note2)", "idx": 1088}
{"namespace": "mingus.core.intervals.is_perfect_consonant", "completion": "    interval = measure(note1, note2)\n    if include_fourths:\n        return interval in [0, 5, 7, 12]\n    else:\n        return interval in [0, 7, 12]", "idx": 1089}
{"namespace": "mingus.core.keys.get_key", "completion": "    if not isinstance(accidentals, int):\n        raise TypeError(\"accidentals must be an integer\")\n    if accidentals < -7 or accidentals > 7:\n        raise ValueError(\"accidentals must be between -7 and 7\")\n\n    return keys[accidentals + 7]", "idx": 1090}
{"namespace": "mingus.core.keys.get_key_signature", "completion": "    if not is_valid_key(key):\n        raise NoteFormatError(\"unrecognized format for key '%s'\" % key)\n    for i, couple in enumerate(keys):\n        if key in couple:\n            return i - 7\n    return 0", "idx": 1091}
{"namespace": "mingus.core.keys.get_key_signature_accidentals", "completion": "    accidentals = get_key_signature(key)\n    if accidentals == 0:\n        return []\n    elif accidentals > 0:\n        return base_scale[1:accidentals+1]  # sharps\n    else:\n        return base_scale[-accidentals:]  # flats", "idx": 1092}
{"namespace": "mingus.core.keys.get_notes", "completion": "    if not is_valid_key(key):\n        raise NoteFormatError(\"unrecognized format for key '%s'\" % key)\n\n    accidentals = get_key_signature(key)\n    scale = list(islice(cycle(base_scale), base_scale.index(key), len(base_scale) + base_scale.index(key)))\n\n    if accidentals < 0:\n        for i in range(-accidentals):\n            note = list(reversed(base_scale))[i]\n            scale[scale.index(note)] = \"{0}{1}\".format(note, \"b\")\n    elif accidentals > 0:\n        for i in range(accidentals):\n            note = base_scale[i]\n            scale[scale.index(note)] = \"{0}{1}\".format(note, \"#\")\n\n    return scale", "idx": 1093}
{"namespace": "mingus.core.keys.relative_major", "completion": "    for couple in keys:\n        if key == couple[1]:\n            return couple[0]\n    raise NoteFormatError(\"'%s' is not a minor key\" % key)", "idx": 1094}
{"namespace": "mingus.core.chords.augmented_triad", "completion": "    return [note, intervals.major_third(note), notes.augment(intervals.perfect_fifth(note))]", "idx": 1095}
{"namespace": "mingus.core.chords.determine", "completion": "    if len(chord) == 3:\n        return determine_triad(chord, shorthand, no_inversions)\n    elif len(chord) == 4:\n        return determine_seventh(chord, shorthand, no_inversions, no_polychords)\n    elif len(chord) == 5:\n        return determine_extended_chord5(chord, shorthand, no_inversions, no_polychords)\n    elif len(chord) == 6:\n        return determine_extended_chord6(chord, shorthand, no_inversions, no_polychords)\n    elif len(chord) == 7:\n        return determine_extended_chord7(chord, shorthand, no_inversions, no_polychords)\n    else:\n        return []", "idx": 1096}
{"namespace": "mingus.core.value.septuplet", "completion": "    if in_fourths:\n        return tuplet(value, 7, 4)\n    else:\n        return tuplet(value, 7, 8)", "idx": 1097}
{"namespace": "mingus.core.value.determine", "completion": "    for i in range(4):\n        for j in base_values:\n            if abs(value - dots(j, i)) < 0.0001:\n                return (j, i, (1, 1))\n    for i in base_triplets:\n        if abs(value - i) < 0.0001:\n            return (i * 2 / 3, 0, (3, 2))\n    for i in base_quintuplets:\n        if abs(value - i) < 0.0001:\n            return (i * 4 / 5, 0, (5, 4))\n    for i in base_septuplets:\n        if abs(value - i) < 0.0001:\n            return (i * 4 / 7, 0, (7, 4))\n    return (value, 0, (1, 1))```", "idx": 1098}
{"namespace": "mingus.core.progressions.substitute_major_for_minor", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Major to minor substitution\n    if (\n        suff == \"M\"\n        or suff == \"M7\"\n        or suff == \"\"\n        and roman in [\"I\", \"IV\", \"V\"]\n        or ignore_suffix\n    ):\n        n = skip(roman, 5)\n        a = interval_diff(roman, n, 9) + acc\n        if suff == \"M\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"m\")))\n        elif suff == \"M7\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"m7\")))\n        elif suff == \"\" or ignore_suffix:\n            res.append(tuple_to_string((n, a, \"\")))\n    return res", "idx": 1099}
{"namespace": "mingus.core.progressions.substitute_diminished_for_diminished", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Diminished progressions\n    if (\n        suff == \"dim7\"\n        or suff == \"dim\"\n        or suff == \"\"\n        and roman in [\"VII\"]\n        or ignore_suffix\n    ):\n        if suff == \"\":\n            suff = \"dim\"\n\n        # Add diminished chord\n        last = roman\n        for x in range(3):\n            next = skip(last, 2)\n            acc += interval_diff(last, next, 3)\n            res.append(tuple_to_string((next, acc, suff)))\n            last = next\n    return res", "idx": 1100}
{"namespace": "mingus.core.progressions.substitute_diminished_for_dominant", "completion": "    (roman, acc, suff) = parse_string(progression[substitute_index])\n    res = []\n\n    # Diminished progressions\n    if (\n        suff == \"dim7\"\n        or suff == \"dim\"\n        or suff == \"\"\n        and roman in [\"VII\"]\n        or ignore_suffix\n    ):\n        if suff == \"\":\n            suff = \"dim\"\n\n        # Add diminished chord\n        last = roman\n        for x in range(4):\n            next = skip(last, 2)\n            acc += interval_diff(last, next, 3)\n            res.append(tuple_to_string((next, acc, suff)))\n            last = next\n    return res", "idx": 1101}
{"namespace": "mingus.core.progressions.substitute", "completion": "    res = []\n    res += substitute_harmonic(progression, substitute_index)\n    res += substitute_minor_for_major(progression, substitute_index)\n    res += substitute_major_for_minor(progression, substitute_index)\n    res += substitute_diminished_for_diminished(progression, substitute_index)\n    res += substitute_diminished_for_dominant(progression, substitute_index)\n\n    if depth > 0:\n        for i in range(len(res)):\n            res += substitute(res, i, depth - 1)\n    return res", "idx": 1102}
{"namespace": "mingus.core.progressions.skip", "completion": "    index = numerals.index(roman_numeral)\n    new_index = (index + skip_count) % 7\n    return numerals[new_index]", "idx": 1103}
{"namespace": "exodus_bundler.cli.configure_logging", "completion": "    # Set the level.\n    if quiet:\n        level = logging.ERROR\n    elif verbose:\n        level = logging.DEBUG\n    else:\n        level = logging.WARNING\n\n    # Configure the root logger.\n    logging.basicConfig(level=level, format='%(levelname)s: %(message)s')\n\n    # Add a stderr handler for warnings and errors.\n    stderr_handler = logging.StreamHandler(sys.stderr)\n    stderr_handler.setLevel(logging.WARNING)\n    logging.getLogger().addHandler(stderr_handler)\n\n    # Optionally add a stdout handler for debug and info messages.\n    if not suppress_stdout:\n        stdout_handler = logging.StreamHandler(sys.stdout)\n        stdout_handler.setLevel(logging.INFO)\n        logging.getLogger().addHandler(stdout_handler)", "idx": 1104}
{"namespace": "exodus_bundler.bundling.create_unpackaged_bundle", "completion": "    # Create a temporary directory for the bundle.\n    bundle = Bundle(working_directory=True, chroot=chroot)\n\n    # Add the main executables to the bundle.\n    for i, executable in enumerate(executables):\n        entry_point = rename[i] if i < len(rename) else True\n        bundle.add_file(executable, entry_point=entry_point)\n\n    # Add additional files to the bundle.\n    for file in add:\n        bundle.add_file(file)\n\n    # Set the no_symlink property for specified files.\n    for file in no_symlink:\n        file = bundle.file_factory(file, chroot=chroot)\n        file.no_symlink = True\n\n    # Create the unpackaged bundle.\n    bundle.create_bundle(shell_launchers=shell_launchers)\n\n    # If detect is True, automatically detect dependencies for the executables.\n    if detect:\n        for file in bundle.files:\n            if file.entry_point:\n                file.elf.dependencies\n\n    return bundle.working_directory", "idx": 1105}
{"namespace": "exodus_bundler.bundling.detect_elf_binary", "completion": "    with open(filename, 'rb') as f:\n        return f.read(4) == b'\\x7fELF'", "idx": 1106}
{"namespace": "exodus_bundler.bundling.resolve_binary", "completion": "    if os.path.exists(binary):\n        return os.path.abspath(binary)\n\n    for path in os.environ['PATH'].split(os.pathsep):\n        full_path = os.path.join(path, binary)\n        if os.path.exists(full_path):\n            return full_path\n\n    raise MissingFileError('The \"%s\" binary could not be found in $PATH.' % binary)", "idx": 1107}
{"namespace": "exodus_bundler.bundling.resolve_file_path", "completion": "    # Normalize the path and check if it exists\n    path = os.path.normpath(os.path.abspath(path))\n    if os.path.exists(path):\n        if os.path.isdir(path):\n            raise UnexpectedDirectoryError('The \"%s\" path is a directory, not a file.' % path)\n        return path\n\n    # If the file doesn't exist and we're allowed to search the PATH, do so\n    if search_environment_path:\n        for directory in os.getenv('PATH', '').split(os.pathsep):\n            potential_path = os.path.join(directory, path)\n            if os.path.exists(potential_path):\n                return os.path.normpath(os.path.abspath(potential_path))\n\n    # If the file still hasn't been found, raise an error\n    raise MissingFileError('The \"%s\" file could not be found.' % path)", "idx": 1108}
{"namespace": "exodus_bundler.bundling.run_ldd", "completion": "    if not detect_elf_binary(binary):\n        raise InvalidElfBinaryError('The \"%s\" file is not a binary ELF file.' % binary)\n\n    process = Popen([ldd, binary], stdout=PIPE, stderr=PIPE)\n    stdout, stderr = process.communicate()\n    combined_output = stdout.decode('utf-8').split('\\n') + stderr.decode('utf-8').split('\\n')\n\n    return combined_output", "idx": 1109}
{"namespace": "exodus_bundler.bundling.Elf.dependencies", "completion": "        dependencies = set()\n        new_dependencies = self.direct_dependencies\n        while new_dependencies:\n            dependencies |= new_dependencies\n            new_dependencies = set(dependency for file in new_dependencies\n                                  for dependency in file.elf.direct_dependencies) - dependencies\n        return dependencies", "idx": 1110}
{"namespace": "exodus_bundler.bundling.File.hash", "completion": "        with open(self.path, 'rb') as f:\n            content = f.read()\n        return hashlib.sha256(content).hexdigest()", "idx": 1111}
{"namespace": "exodus_bundler.bundling.Bundle.add_file", "completion": "        # Resolve the path and check if it's a directory.\n        path = resolve_file_path(path, search_environment_path=(entry_point is not None))\n        if os.path.isdir(path):\n            if entry_point:\n                raise UnexpectedDirectoryError(\n                    'The \"%s\" directory cannot be an entry point.' % path)\n            for root, dirs, files in os.walk(path):\n                for filename in files:\n                    self.add_file(os.path.join(root, filename))\n            return None\n\n        # Create a new `File` instance and add it to the bundle.\n        file = self.file_factory(path, entry_point=entry_point, chroot=self.chroot)\n        self.files.add(file)\n\n        # If the file is an ELF binary, then we'll also add all of its dependencies.\n        if file.elf:\n            self.linker_files.add(file.elf.linker_file)\n            for dependency in file.elf.dependencies:\n                self.files.add(dependency)\n\n        return file", "idx": 1112}
{"namespace": "exodus_bundler.bundling.Bundle.bundle_root", "completion": "        bundle_root = os.path.join(self.working_directory, 'bundles', self.hash)\n        return os.path.normpath(os.path.abspath(bundle_root))", "idx": 1113}
{"namespace": "exodus_bundler.bundling.Bundle.hash", "completion": "        file_hashes = sorted(file.hash for file in self.files)\n        combined_hash_string = ''.join(file_hashes).encode('utf-8')\n        return hashlib.sha256(combined_hash_string).hexdigest()", "idx": 1114}
{"namespace": "exodus_bundler.launchers.construct_bash_launcher", "completion": "    linker_dirname, linker_basename = os.path.split(linker)\n    full_linker = '1' if full_linker else '0'\n    bash_script = render_template_file('bash_launcher.sh', linker_basename=linker_basename,\n                                       linker_dirname=linker_dirname, library_path=library_path,\n                                       executable=executable, full_linker=full_linker)\n    return bash_script", "idx": 1115}
{"namespace": "exodus_bundler.input_parsing.extract_open_path", "completion": "    line = strip_pid_prefix(line)\n    open_methods = ['open(\"', 'openat(\"']\n    for method in open_methods:\n        if line.startswith(method):\n            parts = line[len(method):].split('\", ')\n            if len(parts) > 1 and 'ENOENT' not in parts[1]:\n                return parts[0]\n    return None", "idx": 1116}
{"namespace": "exodus_bundler.input_parsing.extract_paths", "completion": "    paths = set()\n    for line in content.split('\\n'):\n        for extract_func in [extract_exec_path, extract_open_path, extract_stat_path]:\n            path = extract_func(line)\n            if path and not any(path.startswith(blacklist) for blacklist in blacklisted_directories):\n                if existing_only:\n                    if os.path.exists(path) and not os.path.isdir(path):\n                        paths.add(path)\n                else:\n                    paths.add(path)\n    return list(paths)", "idx": 1117}
{"namespace": "fs.time.epoch_to_datetime", "completion": "    if t is None:\n        return None\n    else:\n        return datetime.fromtimestamp(t, timezone.utc)", "idx": 1118}
{"namespace": "fs.path.normpath", "completion": "    if not path:\n        return path\n    initial_slashes = path.startswith('/')\n    # POSIX allows one or two initial slashes, but treats three or more\n    # as single slash.\n    if (initial_slashes and\n        path.startswith('//') and not path.startswith('///')):\n        initial_slashes = 2\n    comps = path.split('/')\n    new_comps = []\n    for comp in comps:\n        if comp in ('', '.'):\n            continue\n        if (comp != '..' or (not initial_slashes and not new_comps) or\n             (new_comps and new_comps[-1] == '..')):\n            new_comps.append(comp)\n        elif new_comps:\n            new_comps.pop()\n    comps = new_comps\n    path = '/'.join(comps)\n    if initial_slashes:\n        path = '/'*initial_slashes + path\n    return path or '/'", "idx": 1119}
{"namespace": "fs.path.iteratepath", "completion": "    path = normpath(path)\n    components = path.strip(\"/\").split(\"/\")\n    return components", "idx": 1120}
{"namespace": "fs.path.recursepath", "completion": "    path_parts = iteratepath(path)\n    paths = [\"/\" + \"/\".join(path_parts[:i]) for i in range(1, len(path_parts) + 1)]\n    if reverse:\n        paths.reverse()\n    return paths", "idx": 1121}
{"namespace": "fs.path.join", "completion": "    return \"/\".join([str(p).strip(\"/\") for p in paths])", "idx": 1122}
{"namespace": "fs.path.parts", "completion": "    path = path.strip('/')\n    return ['/' if i == 0 else part for i, part in enumerate(path.split('/'))]", "idx": 1123}
{"namespace": "fs.path.splitext", "completion": "    if \".\" not in path:\n        return (path, \"\")\n    split = path.rsplit(\".\", 1)\n    return (split[0], \".\" + split[1])", "idx": 1124}
{"namespace": "fs.path.isbase", "completion": "    path1 = abspath(normpath(path1))\n    path2 = abspath(normpath(path2))\n    return path2.startswith(path1)", "idx": 1125}
{"namespace": "fs.path.frombase", "completion": "    if not isparent(path1, path2):\n        raise ValueError(\"`path1` is not a parent directory of `path2`\")\n\n    path1_parts = list(iteratepath(path1))\n    path2_parts = list(iteratepath(path2))\n\n    return \"/\".join(path2_parts[len(path1_parts):])", "idx": 1126}
{"namespace": "fs.path.relativefrom", "completion": "    base_parts = iteratepath(base)\n    path_parts = iteratepath(path)\n\n    # Find common prefix\n    common_length = 0\n    for base_part, path_part in zip(base_parts, path_parts):\n        if base_part != path_part:\n            break\n        common_length += 1\n\n    backrefs = ['..'] * (len(base_parts) - common_length)\n    return join(*(backrefs + path_parts[common_length:]))", "idx": 1127}
{"namespace": "fs.path.iswildcard", "completion": "    return any(char in _WILD_CHARS for char in path)", "idx": 1128}
{"namespace": "fs.wildcard.match", "completion": "    try:\n        re_pat = _PATTERN_CACHE[(pattern, True)]\n    except KeyError:\n        res = \"(?ms)\" + _translate(pattern, case_sensitive=True) + r\"\\Z\"\n        _PATTERN_CACHE[(pattern, True)] = re_pat = re.compile(res)\n    return re_pat.match(name) is not None", "idx": 1129}
{"namespace": "fs.wildcard.imatch", "completion": "    try:\n        re_pat = _PATTERN_CACHE[(pattern, False)]\n    except KeyError:\n        res = \"(?i)(?ms)\" + _translate(pattern, False) + r\"\\Z\"\n        _PATTERN_CACHE[(pattern, False)] = re_pat = re.compile(res)\n    return re_pat.match(name) is not None", "idx": 1130}
{"namespace": "fs.wildcard.get_matcher", "completion": "    if case_sensitive:\n        return partial(match_any, patterns)\n    else:\n        return partial(imatch_any, patterns)", "idx": 1131}
{"namespace": "fs._url_tools.url_quote", "completion": "    from six.moves.urllib.request import pathname2url\n\n    if _WINDOWS_PLATFORM and _has_drive_letter(path_snippet):\n        drive, path = path_snippet.split(\":\", 1)\n        return \"{}:{}\".format(drive, pathname2url(path))\n    else:\n        return pathname2url(path_snippet)", "idx": 1132}
{"namespace": "fs._ftp_parse.parse", "completion": "    parsed_info = []\n    for line in lines:\n        if line.strip():  # if line is not blank\n            parsed_line = parse_line(line)\n            if parsed_line is not None:\n                parsed_info.append(parsed_line)\n    return parsed_info", "idx": 1133}
{"namespace": "fs._ftp_parse._parse_time", "completion": "    for fmt in formats:\n        try:\n            dt = datetime.strptime(t, fmt)\n            if dt.year < 100:\n                dt = dt.replace(year=dt.year + 2000)\n            return (dt - EPOCH_DT).total_seconds()\n        except ValueError:\n            pass\n    return None", "idx": 1134}
{"namespace": "fs.permissions.Permissions.parse", "completion": "        if len(ls) != 10:\n            raise ValueError(\"invalid permissions string\")\n\n        perms = []\n        for i, c in enumerate(ls):\n            if c == \"-\":\n                continue\n            if i == 0:\n                if c == \"d\":\n                    perms.append(\"directory\")\n                elif c == \"l\":\n                    perms.append(\"link\")\n                else:\n                    raise ValueError(\"invalid permissions string\")\n            elif i == 1:\n                if c == \"r\":\n                    perms.append(\"u_r\")\n            elif i == 2:\n                if c == \"w\":\n                    perms.append(\"u_w\")\n            elif i == 3:\n                if c == \"x\":\n                    perms.append(\"u_x\")\n                elif c == \"s\":\n                    perms.append(\"u_x\")\n                    perms.append(\"setuid\")\n                elif c == \"S\":\n                    perms.append(\"setuid\")\n            elif i == 4:\n                if c == \"r\":\n                    perms.append(\"g_r\")\n            elif i == 5:\n                if c == \"w\":\n                    perms.append(\"g_w\")\n            elif i == 6:\n                if c == \"x\":\n                    perms.append(\"g_x\")\n                elif c == \"s\":\n                    perms.append(\"g_x\")\n                    perms.append(\"setguid\")\n                elif c == \"S\":\n                    perms.append(\"setguid\")\n            elif i == 7:\n                if c == \"r\":\n                    perms.append(\"o_r\")\n            elif i == 8:\n                if c == \"w\":\n                    perms.append(\"o_w\")\n            elif i == 9:\n                if c == \"x\":\n                    perms.append(\"o_x\")\n                elif c == \"t\":\n                    perms.append(\"o_x\")\n                    perms.append(\"sticky\")\n                elif c == \"T\":\n                    perms.append(\"sticky\")\n\n        return cls(names=perms)", "idx": 1135}
{"namespace": "fs.permissions.Permissions.create", "completion": "        if isinstance(init, int):\n            return cls(mode=init)\n        elif isinstance(init, Iterable):\n            return cls(names=init)\n        else:\n            return cls()", "idx": 1136}
{"namespace": "fs.info.Info.suffix", "completion": "        name = self.get(\"basic\", \"name\")\n        if \".\" in name:\n            return \".\" + name.split(\".\")[-1]\n        else:\n            return \"\"", "idx": 1137}
{"namespace": "fs.info.Info.suffixes", "completion": "        name = self.get(\"basic\", \"name\")\n        if name.startswith(\".\") and name.count(\".\") == 1:\n            return []\n        basename, dot, ext = name.rpartition(\".\")\n        return [\".\" + e for e in ext.split(\".\")] if dot else []", "idx": 1138}
{"namespace": "fs.info.Info.stem", "completion": "        name = self.get(\"basic\", \"name\")\n        return name.split(\".\")[0]", "idx": 1139}
{"namespace": "fs.info.Info.type", "completion": "        self._require_namespace(\"details\")\n        return self.get(\"details\", \"type\")", "idx": 1140}
{"namespace": "fs.info.Info.created", "completion": "        self._require_namespace(\"details\")\n        _time = self._make_datetime(self.get(\"details\", \"created\"))\n        return _time", "idx": 1141}
{"namespace": "pyinfra.connectors.mech.MechInventoryConnector.make_names_data", "completion": "        names_data = []\n\n        for host in get_mech_config(limit):\n            name, data, groups = _make_name_data(host)\n            names_data.append({\n                'name': name,\n                'data': data,\n                'groups': groups\n            })\n\n        return names_data", "idx": 1142}
{"namespace": "pyinfra.connectors.ansible.AnsibleInventoryConnector.make_names_data", "completion": "        if not inventory_filename:\n            raise InventoryError(\"No Ansible inventory filename provided!\")\n\n        if not path.exists(inventory_filename):\n            raise InventoryError(\"Could not find Ansible inventory file: {0}\".format(inventory_filename))\n\n        return parse_inventory(inventory_filename)", "idx": 1143}
{"namespace": "pyinfra.operations.files.rsync", "completion": "    show_rsync_warning()\n    yield RsyncCommand(src, dest, flags=flags)", "idx": 1144}
{"namespace": "pyinfra.operations.files.get", "completion": "\n    # Add deploy directory?\n    if add_deploy_dir and state.deploy_dir:\n        dest = os.path.join(state.deploy_dir, dest)\n\n    # Ensure the destination directory exists\n    if create_local_dir:\n        os.makedirs(os.path.dirname(dest), exist_ok=True)\n\n    # Get the remote file\n    remote_file = host.get_fact(File, path=src)\n\n    # If the remote file doesn't exist, fail the operation\n    if not remote_file:\n        raise OperationError(f\"Remote file {src} does not exist\")\n\n    # If we're not forcing and the local file exists\n    if not force and os.path.isfile(dest):\n        # Get the local & remote SHA1SUMs\n        local_sha1sum = get_file_sha1(dest)\n        remote_sha1sum = host.get_fact(Sha1File, path=src)\n\n        # If they match, no-op\n        if local_sha1sum == remote_sha1sum:\n            host.noop(f\"file {src} is already downloaded\")\n            return\n\n    # Download the file\n    yield FileDownloadCommand(src, dest)", "idx": 1145}
{"namespace": "pyinfra.operations.files.put", "completion": "\n    if not hasattr(src, \"read\") and add_deploy_dir and state.cwd:\n        src = os.path.join(state.cwd, src)\n\n    # If we're not forcing, check if the file is already there & the same\n    if not force and not hasattr(src, \"read\"):\n        # If we can't assume the file exists, check it does\n        if not assume_exists and not os.path.isfile(src):\n            raise IOError(\"No such file: {0}\".format(src))\n\n        remote_file = host.get_fact(File, path=dest)\n\n        # If the file doesn't exist, we'll be uploading something\n        if remote_file is None:\n            pass\n\n        # If the file is a directory, fail unless we want to remove it\n        elif remote_file is False:\n            if not force:\n                raise OperationError(\"Destination {0} already exists and is not a file\".format(dest))\n\n            # Remove the directory\n            yield StringCommand(\"rm\", \"-r\", QuoteString(dest))\n\n        # We have a file!\n        else:\n            local_sha1 = get_file_sha1(src)\n            remote_sha1 = host.get_fact(Sha1File, path=dest)\n\n            # Compare the sha1s\n            if local_sha1 == remote_sha1:\n                host.noop(\"file {0} is already uploaded\".format(dest))\n                return\n\n    # If we're uploading a file IO object, just do it\n    if hasattr(src, \"read\"):\n        yield FileUploadCommand(src, dest)\n\n    # Otherwise we're uploading a file from disk\n    else:\n        # If the file doesn't exist, error out\n        if not os.path.isfile(src):\n            raise IOError(\"No such file: {0}\".format(src))\n\n        # If the remote directory doesn't exist, create it\n        if create_remote_dir:\n            yield from _create_remote_dir(state, host, dest, user, group)\n\n        # Upload the file\n        yield FileUploadCommand(src, dest)\n\n    # Chown the file if needed\n    if user or group:\n        yield file_utils.chown(dest, user, group)\n\n    # Chmod the file if needed\n    if mode:\n        yield file_utils.chmod(dest, mode)", "idx": 1146}
{"namespace": "pyinfra.operations.files.file", "completion": "\n    path = _validate_path(path)\n\n    mode = ensure_mode_int(mode)\n    info = host.get_fact(File, path=path)\n\n    if info is False:  # not a file\n        yield from _raise_or_remove_invalid_path(\n            \"file\",\n            path,\n            force,\n            force_backup,\n            force_backup_dir,\n        )\n        info = None\n\n    if not present:\n        if info:\n            yield StringCommand(\"rm\", \"-f\", QuoteString(path))\n        else:\n            host.noop(\"file {0} does not exist\")\n        return\n\n    if info is None:  # create\n        if create_remote_dir:\n            yield from _create_remote_dir(state, host, path, user, group)\n\n        yield StringCommand(\"touch\", QuoteString(path))\n\n        if mode:\n            yield file_utils.chmod(path, mode)\n\n        if user or group:\n            yield file_utils.chown(path, user, group)\n\n    else:  # update\n        changed = False\n\n        if touch:\n            yield StringCommand(\"touch\", QuoteString(path))\n            changed = True\n\n        if mode and info[\"mode\"] != mode:\n            yield file_utils.chmod(path, mode)\n            changed = True\n\n        if (user and info[\"user\"] != user) or (group and info[\"group\"] != group):\n            yield file_utils.chown(path, user, group)\n            changed = True\n\n        if not changed:\n            host.noop(\"file {0} already exists\".format(path))", "idx": 1147}
{"namespace": "pyinfra.operations.python.call", "completion": "\n    kwargs.pop(\"state\", None)\n    kwargs.pop(\"host\", None)\n    yield FunctionCommand(function, args, kwargs)", "idx": 1148}
{"namespace": "pyinfra.api.operation.add_op", "completion": "\n    # Loop over all hosts in the state\n    for host in state.inventory.iter_active_hosts():\n        # Set the current host in the context\n        context.set(state=state, host=host)\n        # Execute the operation function with the provided arguments\n        op_func(*args, **kwargs)\n    # Clear the context\n    context.clear()", "idx": 1149}
{"namespace": "pyinfra.api.facts.get_facts", "completion": "\n    # Create a list to store the greenlets\n    greenlets = []\n\n    # Iterate over the active hosts in the state's inventory\n    for host in state.active_hosts:\n\n        # Spawn a greenlet for each host to retrieve the facts\n        greenlet = gevent.spawn(get_fact, state, host, *args, **kwargs)\n\n        # Add the greenlet to the list\n        greenlets.append(greenlet)\n\n    # Wait for the greenlets to complete\n    gevent.joinall(greenlets)\n\n    # Create a dictionary to store the results\n    results = {}\n\n    # Iterate over the greenlets\n    for greenlet in greenlets:\n\n        # Get the host and facts from the greenlet's value\n        host, facts = greenlet.value\n\n        # Store the facts in the dictionary with the host as the key\n        results[host] = facts\n\n    # Return the dictionary containing the retrieved facts\n    return results", "idx": 1150}
{"namespace": "pyinfra.api.operations.run_ops", "completion": "\n    if serial:\n        _run_serial_ops(state)\n    elif no_wait:\n        _run_no_wait_ops(state)\n    else:\n        for op_hash in state.get_op_order():\n            _run_single_op(state, op_hash)", "idx": 1151}
{"namespace": "pyinfra.api.connect.connect_all", "completion": "    # Create a list of greenlets for each host\n    greenlets = [\n        gevent.spawn(host.connect)\n        for host in state.inventory\n    ]\n\n    # Wait for all greenlets to finish\n    gevent.joinall(greenlets)\n\n    # Update the state with the hosts that are connected\n    for host in state.inventory:\n        if host.is_connected:\n            state.activated_hosts.append(host)", "idx": 1152}
{"namespace": "pyinfra.api.arguments.pop_global_arguments", "completion": "\n    if keys_to_check is None:\n        keys_to_check = all_argument_meta.keys()\n\n    popped_args = {}\n    found_keys = []\n\n    for key in keys_to_check:\n        if key in kwargs:\n            popped_args[key] = kwargs.pop(key)\n            found_keys.append(key)\n        elif state and key in state.deploy_kwargs:\n            popped_args[key] = state.deploy_kwargs[key]\n        elif host and key in host.data:\n            popped_args[key] = host.data[key]\n        elif state:\n            popped_args[key] = all_argument_meta[key].default(state.config)\n\n    return popped_args, found_keys", "idx": 1153}
{"namespace": "pyinfra_cli.commands.get_func_and_args", "completion": "    func = None\n    args = {}\n\n    for command in commands:\n        if \"=\" in command:\n            if not func:\n                raise CliError(\"Invalid function commands: `{0}`\".format(commands))\n\n            key, value = command.split(\"=\", 1)\n            args[key] = value\n            continue\n\n        if \".\" not in command:\n            raise CliError(f\"Invalid function: `{command}`, should be in the format `module.cls`\")\n\n        func = try_import_module_attribute(command, prefix=\"pyinfra.operations\")\n\n    return func, args", "idx": 1154}
{"namespace": "viztracer.tracer._VizTracer.start", "completion": "        self.enable = True\n        self.parsed = False\n        if self.log_print:\n            self.overload_print()\n        if self.include_files and self.exclude_files:\n            raise ValueError(\"You can't specify both include_files and exclude_files\")\n        self.config()\n        self._tracer.start()", "idx": 1155}
{"namespace": "viztracer.tracer._VizTracer.stop", "completion": "        self.enable = False\n        if self.log_print:\n            self.restore_print()\n        self._tracer.stop()", "idx": 1156}
{"namespace": "viztracer.report_builder.ReportBuilder.save", "completion": "        if isinstance(output_file, str):\n            file_name, file_extension = os.path.splitext(output_file)\n            output_format = file_extension[1:]  # Remove dot from extension\n            # Open output file if it's a path\n            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n                self.__save_report(f, output_format, file_info)\n        else:\n            # Assume output_file is a file-like object and try to infer format from name attribute\n            try:\n                file_name, file_extension = os.path.splitext(output_file.name)\n                output_format = file_extension[1:]  # Remove dot from extension\n            except AttributeError:\n                # If name attribute doesn't exist, default to HTML\n                output_format = \"html\"\n            self.__save_report(output_file, output_format, file_info)\n\n        # Append the \"view_command\" message with the absolute path\n        if isinstance(output_file, str):\n            self.final_messages.append((\"view_command\", {\"output_file\": os.path.abspath(output_file)}))\n        else:\n            try:\n                self.final_messages.append((\"view_command\", {\"output_file\": os.path.abspath(output_file.name)}))\n            except AttributeError:\n                # If output_file doesn't have name attribute, do not add view command message\n                pass\n        \n        # Print all messages    \n        self.print_messages()", "idx": 1157}
{"namespace": "viztracer.code_monkey.AstTransformer.get_assign_targets_with_attr", "completion": "        if isinstance(node, ast.Attribute):\n            return [node]\n        elif isinstance(node, (ast.Name, ast.Subscript, ast.Starred)):\n            return []\n        elif isinstance(node, (ast.Tuple, ast.List)):\n            return reduce(lambda a, b: a + b, [self.get_assign_targets_with_attr(elt) for elt in node.elts])\n        color_print(\"WARNING\", \"Unexpected node type {} for ast.Assign. \\\n            Please report to the author github.com/gaogaotiantian/viztracer\".format(type(node)))\n        return []", "idx": 1158}
{"namespace": "viztracer.code_monkey.SourceProcessor.process", "completion": "        # Check if the source is bytes and decode it to utf-8\n        if isinstance(source, bytes):\n            source = source.decode('utf-8')\n        # If the source is not a string, return it as is\n        if not isinstance(source, str):\n            return source\n\n        # Split the source into lines for processing\n        lines = source.splitlines()\n        processed_lines = []\n\n        # Iterate over each line and apply transformations if they match the patterns\n        for line in lines:\n            transformed_line = line\n            for pattern, transform_func in self.re_patterns:\n                match = pattern.match(line)\n                if match:\n                    transformed_line = transform_func(self, match)\n                    break\n            processed_lines.append(transformed_line)\n\n        # Join the processed lines with newline characters\n        processed_source = '\\n'.join(processed_lines)\n        return processed_source", "idx": 1159}
{"namespace": "wal_e.log_help.WalELogger.fmt_logline", "completion": "        logline = ['MSG: {0}'.format(msg)]\n        if detail is not None:\n            logline.append('DETAIL: {0}'.format(detail))\n        if hint is not None:\n            logline.append('HINT: {0}'.format(hint))\n        if structured is not None:\n            logline.append('STRUCTURED: {0}'.format(self._fmt_structured(structured)))\n        return '\\n'.join(logline)", "idx": 1160}
{"namespace": "wal_e.blobstore.file.calling_format.Bucket.delete_keys", "completion": "        for key in keys:\n            file_path = os.path.join(\"/\", key.strip(\"/\"))\n            if os.path.isfile(file_path):\n                os.remove(file_path)\n                dir_path = os.path.dirname(file_path)\n                remove_empty_dirs(dir_path)", "idx": 1161}
{"namespace": "wal_e.worker.upload_pool.TarUploadPool.put", "completion": "\n        # Prevent any new uploads after closing the pool\n        if self.closed:\n            raise Exception(\"Cannot add new tasks to closed TarUploadPool\")\n\n        # Enforce the max concurrency constraints\n        while self.concurrency_burden >= self.max_concurrency or self.member_burden + len(tpart) > self.max_members:\n            self._wait()  # Wait for an upload to finish\n\n        # If no resources available, raise an exception\n        if self.member_burden + len(tpart) > self.max_members:\n            raise Exception(\"Resource limits reached; cannot start upload\")\n        \n        # Otherwise, start uploading the tar volume\n        self._start(tpart)", "idx": 1162}
{"namespace": "wal_e.worker.pg.wal_transfer.WalSegment.from_ready_archive_status", "completion": "        archive_status_dir = path.join(xlog_dir, 'archive_status')\n        for filename in os.listdir(archive_status_dir):\n            if filename.endswith('.ready'):\n                seg_path = path.join(xlog_dir, filename[:-6])  # remove '.ready' from filename\n                yield WalSegment(seg_path)", "idx": 1163}
{"namespace": "wal_e.worker.pg.wal_transfer.WalTransferGroup.join", "completion": "\n        # Close the transfer group to prevent further transfers\n        self.closed = True\n\n        # Wait for all transfers to complete\n        while self.expect > 0:\n            e = self.wait_change.get()\n            self.expect -= 1\n\n            # If an exception occurred during transfer, raise it\n            if e is not None:\n                raise e\n\n        # Wait for all greenlets to exit\n        gevent.joinall(self.greenlets, timeout=30)\n\n        # If any greenlets are still running, kill them\n        if self.greenlets:\n            gevent.killall(self.greenlets)", "idx": 1164}
{"namespace": "wal_e.worker.pg.wal_transfer.WalTransferGroup.start", "completion": "        if self.closed:\n            raise UserCritical(\n                msg='attempt to start new transfer after join',\n                hint='report a bug')\n\n        g = gevent.Greenlet(self.transferer, segment)\n        g.link(self._complete_execution)\n\n        self.greenlets.add(g)\n        self.expect += 1\n\n        g.start()", "idx": 1165}
{"namespace": "mrjob.py2.to_unicode", "completion": "    if isinstance(s, bytes):\n        try:\n            return s.decode('utf-8')\n        except UnicodeDecodeError:\n            return s.decode('latin-1')\n    elif isinstance(s, string_types):  # string_types are defined earlier to support Python 2/3 compatibility\n        return s\n    else:\n        raise TypeError(\"Expected bytes or string, but got %s\" % type(s).__name__)", "idx": 1166}
{"namespace": "mrjob.job.MRJob.steps", "completion": "        kwargs = {}\n\n        for method_name in ('mapper', 'combiner', 'reducer'):\n            method = getattr(self, method_name)\n            if _im_func(method) is not getattr(MRJob, method_name):\n                kwargs[method_name] = method\n\n        for method_name in ('mapper_init', 'mapper_final',\n                            'combiner_init', 'combiner_final',\n                            'reducer_init', 'reducer_final'):\n            method = getattr(self, method_name)\n            if _im_func(method) is not getattr(MRJob, method_name):\n                kwargs[method_name + '_func'] = method\n\n        for method_name in ('mapper_cmd', 'combiner_cmd', 'reducer_cmd'):\n            method = getattr(self, method_name)\n            if _im_func(method) is not getattr(MRJob, method_name):\n                kwargs[method_name] = method()\n\n        for method_name in ('mapper_pre_filter', 'combiner_pre_filter',\n                            'reducer_pre_filter'):\n            method = getattr(self, method_name)\n            if _im_func(method) is not getattr(MRJob, method_name):\n                kwargs[method_name] = method()\n\n        if getattr(self, 'mapper_raw') is not MRJob.mapper_raw:\n            kwargs['mapper_raw'] = self.mapper_raw\n\n        if getattr(self, 'spark') is not MRJob.spark:\n            return [SparkStep(spark=self.spark)]\n\n        return [MRStep(**kwargs)]", "idx": 1167}
{"namespace": "mrjob.job.MRJob.increment_counter", "completion": "        # don't allow people to pass in floats\n        if not isinstance(amount, int):\n            raise TypeError('amount must be an int: %r' % (amount,))\n\n        # Extra commas screw up hadoop and there's no way to escape them. So\n        # replace them with a character that's highly unlikely to show up in\n        # counter names. If you're using parse_counters(), this substitution\n        # will be reversed.\n        #\n        # cast to str() just in case people pass in an Exception or whatever\n        group = str(group).replace(',', ';')\n        counter = str(counter).replace(',', ';')\n\n        line = 'reporter:counter:%s,%s,%d\\n' % (group, counter, amount)\n        if not isinstance(line, bytes):\n            line = line.encode('utf_8')\n\n        self.stderr.write(line)\n        self.stderr.flush()", "idx": 1168}
{"namespace": "mrjob.job.MRJob.set_status", "completion": "        # don't allow people to pass in floats\n        if not isinstance(msg, string_types):\n            raise TypeError('status message must be a string, not %r' % (msg,))\n\n        line = 'reporter:status:%s\\n' % (msg,)\n        if not isinstance(line, bytes):\n            line = line.encode('utf_8')\n\n        self.stderr.write(line)\n        self.stderr.flush()", "idx": 1169}
{"namespace": "mrjob.job.MRJob.run_job", "completion": "        self.set_up_logging(quiet=self.options.quiet, verbose=self.options.verbose)\n\n        with self.make_runner() as runner:\n            try:\n                runner.run()\n            except Exception as e:\n                log.error(str(e))\n                sys.exit(1)\n\n            if self._should_cat_output():\n                try:\n                    runner.cat_output(self.stdout)\n                except IOError as e:\n                    # ignore broken pipe errors\n                    if e.errno != errno.EPIPE:\n                        raise", "idx": 1170}
{"namespace": "mrjob.job.MRJob.set_up_logging", "completion": "        if quiet:\n            log_to_null(name='mrjob')\n            log_to_null(name='__main__')\n        else:\n            if stream is None:\n                stream = sys.stderr\n\n            if verbose:\n                log_to_stream(name='mrjob', debug=True, stream=stream)\n                log_to_stream(name='__main__', debug=True, stream=stream)\n            else:\n                log_to_stream(name='mrjob', debug=False, stream=stream)\n                log_to_stream(name='__main__', debug=False, stream=stream)", "idx": 1171}
{"namespace": "mrjob.job.MRJob.execute", "completion": "        if self.options.run_mapper:\n            self.run_mapper(self.options.step_num)\n        elif self.options.run_combiner:\n            self.run_combiner(self.options.step_num)\n        elif self.options.run_reducer:\n            self.run_reducer(self.options.step_num)\n        elif self.options.run_spark:\n            self.run_spark(self.options.step_num)\n        else:\n            self.run_job()", "idx": 1172}
{"namespace": "mrjob.job.MRJob._runner_kwargs", "completion": "        kwargs = self._non_option_kwargs()\n        kwargs.update(self._kwargs_from_switches(self._runner_class().OPT_NAMES))\n        kwargs.update(self._job_kwargs())\n        kwargs['steps_desc'] = self._steps_desc()\n\n        if self.options.runner in ('inline', 'spark'):\n            kwargs['mrjob_cls'] = self.__class__\n\n        return kwargs", "idx": 1173}
{"namespace": "mrjob.job.MRJob.run_mapper", "completion": "        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'mapper')\n\n        for k, v in self.map_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)", "idx": 1174}
{"namespace": "mrjob.job.MRJob.run_combiner", "completion": "        # pick input and output protocol\n        read_lines, write_line = self._wrap_protocols(step_num, 'combiner')\n\n        for k, v in self.combine_pairs(read_lines(), step_num=step_num):\n            write_line(k, v)", "idx": 1175}
{"namespace": "mrjob.job.MRJob.add_passthru_arg", "completion": "        pass_opt = self.arg_parser.add_argument(*args, **kwargs)\n\n        self._passthru_arg_dests.add(pass_opt.dest)", "idx": 1176}
{"namespace": "mrjob.job.MRJob.is_task", "completion": "        return self.options.run_mapper or self.options.run_combiner or self.options.run_reducer or self.options.run_spark", "idx": 1177}
{"namespace": "mrjob.job.MRJob.parse_output", "completion": "        # Get the output protocol\n        output_protocol = self.output_protocol()\n\n        # Convert chunks to lines\n        lines = to_lines(chunks)\n\n        # Parse each line using the output protocol and yield the result\n        for line in lines:\n            yield output_protocol.read(line)", "idx": 1178}
{"namespace": "mrjob.job.MRJob.sandbox", "completion": "        if stdin is None:\n            stdin = BytesIO()\n        if stdout is None:\n            stdout = BytesIO()\n        if stderr is None:\n            stderr = BytesIO()\n\n        self._stdin = stdin\n        self._stdout = stdout\n        self._stderr = stderr\n\n        return self", "idx": 1179}
{"namespace": "mrjob.hadoop.fully_qualify_hdfs_path", "completion": "    if path.startswith('hdfs://'):\n        return path\n    elif path.startswith('/'):\n        return 'hdfs://' + path\n    else:\n        username = getpass.getuser()\n        return 'hdfs:///user/{username}/{path}'.format(username=username, path=path)", "idx": 1180}
{"namespace": "mrjob.hadoop.HadoopJobRunner.fs", "completion": "        if not hasattr(self, '_fs'):\n            self._fs = CompositeFilesystem()\n            self._fs.add_fs('hdfs', HadoopFilesystem(self.get_hadoop_bin()))\n            self._fs.add_fs('local', LocalFilesystem())\n        return self._fs", "idx": 1181}
{"namespace": "mrjob.hadoop.HadoopJobRunner._find_hadoop_streaming_jar", "completion": "        for path in self._hadoop_streaming_jar_dirs():\n            log.info('Looking for Hadoop streaming jar in %s...' % path)\n\n            matches = list(self.fs.ls(path, glob=_HADOOP_STREAMING_JAR_RE))\n            if matches:\n                # sort matches so we always select the same one\n                return sorted(matches)[0]\n\n        return None", "idx": 1182}
{"namespace": "mrjob.hadoop.HadoopJobRunner._find_binaries_and_jars", "completion": "        # Load Hadoop binary\n        self.get_hadoop_bin()\n\n        # Check if there are Hadoop streaming steps or Spark steps in the job\n        for step in self._get_steps():\n            if step['type'] == 'streaming':\n                self.get_hadoop_streaming_jar()\n            elif _is_spark_step_type(step['type']):\n                self._load_spark_submit()", "idx": 1183}
{"namespace": "mrjob.hadoop.HadoopJobRunner._args_for_streaming_step", "completion": "        if not self.get_hadoop_streaming_jar():\n            raise Exception('no Hadoop streaming jar')\n\n        step = self._get_step(step_num)\n\n        # construct the command line\n        args = []\n        args.extend(self.get_hadoop_bin())\n        args.append('jar')\n        args.append(self.get_hadoop_streaming_jar())\n\n        # add Hadoop streaming arguments\n        for k, v in self._hadoop_streaming_jar_args(step_num).items():\n            args.append('-%s' % k)\n            if v is not None:\n                args.append(str(v))\n\n        return args", "idx": 1184}
{"namespace": "mrjob.hadoop.HadoopJobRunner._stream_history_log_dirs", "completion": "        if not self._read_logs():\n            return\n\n        for log_dir in unique(self._hadoop_log_dirs(output_dir=output_dir)):\n            path = self.fs.join(log_dir, 'history')\n\n            if _logs_exist(self.fs, path):\n                log.info('Looking for history log in %s...' % path)\n                yield [path]", "idx": 1185}
{"namespace": "mrjob.hadoop.HadoopJobRunner._stream_task_log_dirs", "completion": "        if not self._read_logs():\n            return\n\n        for log_dir in unique(self._hadoop_log_dirs(output_dir=output_dir)):\n            if application_id:\n                # logs are in a subdirectory for the application\n                log_dir = posixpath.join(log_dir, 'userlogs', application_id)\n            else:\n                # logs are directly in userlogs/\n                log_dir = posixpath.join(log_dir, 'userlogs')\n\n            if _logs_exist(self.fs, log_dir):\n                log.info('Looking for task logs in %s...' % log_dir)\n                yield [log_dir]", "idx": 1186}
{"namespace": "mrjob.setup.UploadDirManager.add", "completion": "        if is_uri(path):\n            return path\n\n        if path not in self._path_to_name:\n            name = name_uniquely(path, names_taken=self._names_taken, unhide=True)\n            self._path_to_name[path] = name\n            self._names_taken.add(name)\n\n        return self.uri(path)", "idx": 1187}
{"namespace": "mrjob.setup.UploadDirManager.uri", "completion": "        if is_uri(path):\n            return path\n\n        if path in self._path_to_name:\n            return posixpath.join(self.prefix, self._path_to_name[path])\n\n        raise ValueError('%r is not a URI or a known local file' % path)", "idx": 1188}
{"namespace": "mrjob.setup.UploadDirManager.path_to_uri", "completion": "        path_to_uri = {}\n        for path in self._path_to_name:\n            path_to_uri[path] = self.uri(path)\n        return path_to_uri", "idx": 1189}
{"namespace": "mrjob.setup.WorkingDirManager.name_to_path", "completion": "        name_to_path = {}\n\n        for name, (path_type, path) in self._name_to_typed_path.items():\n            if type is None or path_type == type:\n                name_to_path[name] = path\n\n        for (path_type, path), name in self._typed_path_to_auto_name.items():\n            if type is None or path_type == type:\n                if name is not None:\n                    name_to_path[name] = path\n\n        return name_to_path", "idx": 1190}
{"namespace": "mrjob.setup.WorkingDirManager.paths", "completion": "        if type is not None:\n            self._check_type(type)\n\n        return set(path\n                   for typed_path in self._typed_path_to_auto_name\n                   if (type is None or typed_path[0] == type))", "idx": 1191}
{"namespace": "mrjob.compat.jobconf_from_env", "completion": "    if variable in os.environ:\n        return os.environ[variable]\n\n    # try alternatives (arbitrary order)\n    for alternative in _JOBCONF_MAP.get(variable, {}).values():\n        if alternative in os.environ:\n            return os.environ[alternative]\n\n    return default", "idx": 1192}
{"namespace": "mrjob.compat.jobconf_from_dict", "completion": "    # try variable verbatim first\n    if name in jobconf:\n        return jobconf[name]\n\n    # try alternatives (arbitrary order)\n    for var in _JOBCONF_MAP.get(name, {}).values():\n        if var in jobconf:\n            return jobconf[var]\n\n    return default", "idx": 1193}
{"namespace": "mrjob.compat.translate_jobconf", "completion": "    if variable in _JOBCONF_MAP:\n        version_dict = _JOBCONF_MAP[variable]\n        return map_version(version, version_dict)\n    else:\n        return variable", "idx": 1194}
{"namespace": "mrjob.compat.translate_jobconf_for_all_versions", "completion": "    if variable in _JOBCONF_MAP:\n        return sorted(_JOBCONF_MAP[variable].values())\n    else:\n        return [variable]", "idx": 1195}
{"namespace": "mrjob.compat.translate_jobconf_dict", "completion": "    translated_jobconf = {}\n    warnings = []\n\n    for key, value in jobconf.items():\n        translated_key = translate_jobconf(key, hadoop_version)\n        translated_jobconf[translated_key] = value\n\n        if key != translated_key:\n            warnings.append(f\"{key} : {translated_key}\")\n\n    if warnings:\n        log.warning(\"Detected hadoop configuration property names that do not match version %s:\\n\"\n                    \"They have been translated to the following names:\\n%s\",\n                    hadoop_version, \"\\n\".join(sorted(warnings)))\n\n    return translated_jobconf", "idx": 1196}
{"namespace": "mrjob.compat.uses_yarn", "completion": "    if not isinstance(version, string_types):\n        raise TypeError('%r is not a string' % version)\n\n    return version_gte(version, '0.23')", "idx": 1197}
{"namespace": "mrjob.local.LocalMRJobRunner._spark_master", "completion": "        num_executors = self._opts['num_executors']\n        cores_per_executor = self._opts['cores_per_executor']\n        executor_memory = self._opts.get('executor_memory') or _DEFAULT_EXECUTOR_MEMORY\n        executor_memory_in_mb = int(math.ceil(_to_num_bytes(executor_memory) / 1024.0 / 1024.0))\n\n        return 'local-cluster[%d,%d,%d]' % (num_executors, cores_per_executor, executor_memory_in_mb)", "idx": 1198}
{"namespace": "mrjob.runner.MRJobRunner._bootstrap_mrjob", "completion": "        return bool(self._opts.get('bootstrap_mrjob', True))", "idx": 1199}
{"namespace": "mrjob.conf._fix_clear_tags", "completion": "    if isinstance(x, list):\n        return [_fix_clear_tags(item) for item in x]\n    elif isinstance(x, dict):\n        result = {}\n        for k, v in x.items():\n            if isinstance(k, ClearedValue):\n                k = k.value\n                v = ClearedValue(v)\n            if isinstance(v, ClearedValue) and v.value is None:\n                result.pop(k, None)\n            else:\n                result[k] = _fix_clear_tags(v)\n        return result\n    elif isinstance(x, ClearedValue):\n        return _fix_clear_tags(x.value)\n    else:\n        return x", "idx": 1200}
{"namespace": "mrjob.conf.load_opts_from_mrjob_conf", "completion": "    if already_loaded is None:\n        already_loaded = []\n\n    conf_path = _expanded_mrjob_conf_path(conf_path)\n\n    return _load_opts_from_mrjob_conf(runner_alias, conf_path, already_loaded)", "idx": 1201}
{"namespace": "mrjob.conf.load_opts_from_mrjob_confs", "completion": "    if conf_paths is None:\n        conf_paths = [None]\n\n    results = []\n\n    for conf_path in conf_paths:\n        results.extend(load_opts_from_mrjob_conf(runner_alias, conf_path=conf_path))\n\n    if not any(values for path, values in results):\n        log.warning('No configs specified for %s runner' % runner_alias)\n\n    return results", "idx": 1202}
{"namespace": "mrjob.conf.dump_mrjob_conf", "completion": "    if yaml:\n        _dump_yaml_with_clear_tags(conf, f)\n    else:\n        json.dump(conf, f)", "idx": 1203}
{"namespace": "mrjob.conf.combine_lists", "completion": "    result = []\n\n    for seq in seqs:\n        if seq is None:\n            continue\n        elif isinstance(seq, string_types) or not hasattr(seq, '__iter__'):\n            result.append(seq)\n        else:\n            result.extend(seq)\n\n    return result", "idx": 1204}
{"namespace": "mrjob.conf.combine_cmds", "completion": "    cmd = combine_values(*cmds)\n\n    if cmd is None:\n        return None\n    elif isinstance(cmd, string_types):\n        return shlex_split(cmd)\n    else:\n        return list(cmd)", "idx": 1205}
{"namespace": "mrjob.conf.combine_dicts", "completion": "    result = {}\n    for dictionary in dicts:\n        if dictionary is not None:\n            for key, value in dictionary.items():\n                if isinstance(value, ClearedValue) and value.value is None:\n                    result.pop(key, None)\n                else:\n                    result[key] = _strip_clear_tag(value)\n    return result", "idx": 1206}
{"namespace": "mrjob.conf.combine_jobconfs", "completion": "    final_jobconf = {}\n\n    keys = set()\n    for jobconf in jobconfs:\n        if isinstance(jobconf, ClearedValue):\n            raise TypeError\n        elif jobconf:\n            keys.update(jobconf)\n\n    for key in keys:\n        values = _resolve_clear_tags_in_list(\n            jobconf[key] for jobconf in jobconfs if jobconf and key in jobconf)\n\n        final_value = combine_values(*values)\n        if final_value is not None:\n            final_jobconf[key] = _to_java_str(final_value)\n\n    return final_jobconf", "idx": 1207}
{"namespace": "mrjob.conf.combine_path_lists", "completion": "    result = []\n\n    for path_seq in path_seqs:\n        if path_seq is None:\n            continue\n\n        if isinstance(path_seq, (bytes, string_types)):\n            path_seq = [path_seq]\n\n        for path in path_seq:\n            path = expand_path(path)\n            if '*' in path:\n                result.extend(sorted(glob.glob(path)))\n            else:\n                result.append(path)\n\n    return result", "idx": 1208}
{"namespace": "mrjob.conf.combine_opts", "completion": "    # collect keys\n    keys = set(k for opts in opts_list if opts for k in opts)\n\n    # combine each key separately\n    return {key: _combine_opt(combiners, key, opts_list) for key in keys}", "idx": 1209}
{"namespace": "mrjob.bin.MRJobBinRunner._task_python_bin", "completion": "        return self._opts['task_python_bin'] or self._python_bin()", "idx": 1210}
{"namespace": "mrjob.bin.MRJobBinRunner.get_spark_submit_bin", "completion": "        if not self._spark_submit_bin:\n            self._spark_submit_bin = self._find_spark_submit_bin()\n        return self._spark_submit_bin", "idx": 1211}
{"namespace": "mrjob.step.StepFailedException.__str__", "completion": "        if self.step_desc:\n            step_desc = self.step_desc\n        elif self.step_num is not None:\n            if self.num_steps is not None:\n                step_desc = 'Step %d of %d' % (self.step_num + 1, self.num_steps)\n            else:\n                step_desc = 'Step %d' % (self.step_num + 1)\n\n            if self.last_step_num is not None:\n                step_desc = 'Steps %d-%d' % (self.step_num + 1, self.last_step_num + 1)\n        else:\n            step_desc = 'Step'\n\n        if self.reason:\n            return '%s failed: %s' % (step_desc, self.reason)\n        else:\n            return '%s failed' % step_desc", "idx": 1212}
{"namespace": "mrjob.step.StepFailedException.__repr__", "completion": "        return '%s(%s)' % (\n            self.__class__.__name__,\n            ', '.join('%s=%r' % (field, getattr(self, field)) for field in self._FIELDS))", "idx": 1213}
{"namespace": "mrjob.step.MRStep.description", "completion": "        desc = {'type': 'streaming'}\n\n        if step_num == 0 or self.has_explicit_mapper or self.has_explicit_combiner:\n            desc['mapper'] = self.render_mapper()\n\n        if self.has_explicit_combiner:\n            desc['combiner'] = self.render_combiner()\n\n        if self.has_explicit_reducer:\n            desc['reducer'] = self.render_reducer()\n\n        if self['mapper_raw']:\n            desc['input_manifest'] = True\n\n        if self['jobconf']:\n            desc['jobconf'] = self['jobconf']\n\n        return desc", "idx": 1214}
{"namespace": "mrjob.step._Step.description", "completion": "        desc = {'type': self._STEP_TYPE}\n\n        for k in self._STEP_ATTRS:\n            if k not in self._HIDDEN_ATTRS and hasattr(self, k):\n                desc[k] = getattr(self, k)\n\n        return desc", "idx": 1215}
{"namespace": "mrjob.protocol._KeyCachingProtocol.read", "completion": "        key_encoded, value_encoded = line.split(b'\\t', 1)\n\n        if key_encoded != self._last_key_encoded:\n            self._last_key_encoded = key_encoded\n            self._last_key_decoded = self._loads(key_encoded)\n\n        return (self._last_key_decoded, self._loads(value_encoded))", "idx": 1216}
{"namespace": "mrjob.util.safeeval", "completion": "    # Create a dictionary of safe globals\n    safe_globals = {\n        '__builtins__': {'True': True, 'False': False, 'None': None, 'set': set, 'range': range},\n    }\n\n    # Update the globals with the provided globals\n    if globals:\n        safe_globals.update(globals)\n\n    # Handle 'open' specially\n    safe_globals['__builtins__']['open'] = lambda *args, **kwargs: NameError(\"name 'open' is not defined\")\n\n    # Evaluate the expression\n    return eval(expr, safe_globals, locals)", "idx": 1217}
{"namespace": "mrjob.util.to_lines", "completion": "    if hasattr(chunks, 'readline'):\n        return chunks\n\n    return _to_lines(chunks)", "idx": 1218}
{"namespace": "mrjob.parse.is_s3_uri", "completion": "    try:\n        components = urlparse(uri)\n        if components.scheme in ('s3', 's3n', 's3a'):\n            return True\n        else:\n            return False\n    except ValueError:\n        return False", "idx": 1219}
{"namespace": "mrjob.parse.parse_s3_uri", "completion": "    # Check if the URI is a valid S3 URI\n    if not uri.startswith('s3://'):\n        raise ValueError('Invalid S3 URI: %s' % uri)\n\n    # Remove the s3:// prefix\n    uri = uri[len('s3://'):]\n\n    # Split the remaining URI into bucket and key\n    bucket, key = uri.split('/', 1)\n\n    return bucket, key", "idx": 1220}
{"namespace": "mrjob.parse.to_uri", "completion": "    if is_uri(path_or_uri):\n        return path_or_uri\n    else:\n        return urljoin('file:', pathname2url(abspath(path_or_uri)))", "idx": 1221}
{"namespace": "mrjob.parse.parse_mr_job_stderr", "completion": "    if counters is None:\n        counters = {}\n\n    statuses = []\n    other = []\n\n    if isinstance(stderr, bytes):\n        stderr = BytesIO(stderr)\n\n    for line in stderr:\n        line = line.rstrip(b'\\n')\n\n        m = _COUNTER_RE.match(line)\n        if m:\n            group, counter, amount = m.groups()\n            counters.setdefault(group, {}).setdefault(counter, 0)\n            counters[group][counter] += int(amount)\n            continue\n\n        m = _STATUS_RE.match(line)\n        if m:\n            statuses.append(m.group(1))\n            continue\n\n        other.append(line)\n\n    return {'counters': counters, 'statuses': statuses, 'other': other}", "idx": 1222}
{"namespace": "mrjob.parse._parse_progress_from_job_tracker", "completion": "    map_percent = None\n    reduce_percent = None\n\n    # Convert bytes to string\n    html_str = html_bytes.decode('utf-8')\n\n    # Extract the content between 'Running Jobs' and 'Jobs'\n    running_jobs_index = html_str.find('Running Jobs')\n    jobs_index = html_str.find('Jobs')\n    if running_jobs_index != -1 and jobs_index != -1:\n        content = html_str[running_jobs_index:jobs_index]\n\n        # Extract map_percent and reduce_percent\n        map_index = content.find('map')\n        reduce_index = content.find('reduce')\n        if map_index != -1 and reduce_index != -1:\n            map_percent = float(content[map_index+4:map_index+8])\n            reduce_percent = float(content[reduce_index+7:reduce_index+11])\n\n    return map_percent, reduce_percent", "idx": 1223}
{"namespace": "mrjob.parse._parse_progress_from_resource_manager", "completion": "    # search for the progress percentage in the HTML content\n    match = _RESOURCE_MANAGER_JS_RE.search(html_bytes)\n    if match:\n        # if found, return it as a float\n        return float(match.group('percent'))\n    else:\n        # if not found, return None\n        return None", "idx": 1224}
{"namespace": "mrjob.logs.task._match_task_log_path", "completion": "    # Check if the path matches the YARN task log path pattern\n    match = _YARN_TASK_LOG_PATH_RE.match(path)\n    if match:\n        if application_id and match.group('application_id') != application_id:\n            return None\n        return match.groupdict()\n\n    # Check if the path matches the pre-YARN task log path pattern\n    match = _PRE_YARN_TASK_LOG_PATH_RE.match(path)\n    if match:\n        if job_id and not _attempt_id_matches_job_id(match.group('attempt_id'), job_id):\n            return None\n        return match.groupdict()\n\n    return None", "idx": 1225}
{"namespace": "mrjob.logs.task._parse_task_syslog", "completion": "    records = parse.parse_log4j_records(lines)\n    return _parse_task_syslog_records(records)", "idx": 1226}
{"namespace": "mrjob.logs.ids._sort_for_spark", "completion": "    # First, sort by timestamp and step\n    ds = sorted(ds, key=_step_sort_key)\n    # Then, sort by time\n    ds = sorted(ds, key=_time_sort_key, reverse=True)\n    return ds", "idx": 1227}
{"namespace": "mrjob.logs.spark._parse_spark_log", "completion": "    records = _parse_hadoop_log4j_records(lines, record_callback=record_callback)\n    return _parse_spark_log_from_log4j_records(records)", "idx": 1228}
{"namespace": "mrjob.logs.mixin.LogInterpretationMixin._pick_error", "completion": "        if 'error' in log_interpretation:\n            return  # already picked error\n\n        if not self._read_logs():\n            return  # nothing to do\n\n        log.info('Scanning logs for probable cause of failure...')\n\n        for log_type in self._logs_needed_to_pick_error(step_type):\n            if log_type not in log_interpretation:\n                self._interpret_logs(log_interpretation, step_type, log_type)\n\n        if 'error' not in log_interpretation:\n            log_interpretation['error'] = _pick_error(log_interpretation)", "idx": 1229}
{"namespace": "mrjob.logs.history._match_history_log_path", "completion": "    match = _HISTORY_LOG_PATH_RE.match(path)\n    if not match:\n        return None\n\n    if job_id and match.group('job_id') != job_id:\n        return None\n\n    return {\n        'job_id': match.group('job_id'),\n        'path': path,\n        'yarn': '.jhist' in match.group('suffix'),\n    }", "idx": 1230}
{"namespace": "mrjob.logs.history._parse_pre_yarn_history_log", "completion": "    result = {}\n    task_to_counters = {}  # used for successful tasks in failed jobs\n\n    for record in _parse_pre_yarn_history_records(lines):\n        record_type = record['type']\n        fields = record['fields']\n\n        if record_type == 'Task':\n            if 'COUNTERS' in fields and 'TASKID' in fields:\n                task_id = fields['TASKID']\n                counters_str = fields['COUNTERS']\n\n                task_to_counters[task_id] = _parse_pre_yarn_counters(counters_str)\n\n        elif record_type == 'TaskAttempt' and fields.get('TASK_STATUS') == 'FAILED':\n            if 'ERROR' in fields and fields['ERROR'].strip():\n                error = dict(\n                    hadoop_error=dict(\n                        message=fields['ERROR'],\n                        start_line=record['start_line'],\n                        num_lines=record['num_lines']))\n\n                if 'TASK_ATTEMPT_ID' in fields:\n                    error['attempt_id'] = fields['TASK_ATTEMPT_ID']\n\n                result.setdefault('errors', [])\n                result['errors'].append(error)\n\n        elif record_type == 'Job' and 'COUNTERS' in fields:\n            counters_str = fields['COUNTERS']\n            result['counters'] = _parse_pre_yarn_counters(counters_str)\n\n    # if job failed, patch together counters from successful tasks\n    if 'counters' not in result and task_to_counters:\n        result['counters'] = _sum_counters(*task_to_counters.values())\n\n    return result", "idx": 1231}
{"namespace": "mrjob.logs.history._parse_pre_yarn_history_records", "completion": "    record_lines = []\n    for line_num, line in enumerate(lines):\n        if line.strip() == '.':\n            # end of record\n            if record_lines:\n                record_str = ' '.join(record_lines)\n                m = _PRE_YARN_HISTORY_RECORD.match(record_str)\n                if m:\n                    record = dict(\n                        fields={},\n                        num_lines=len(record_lines),\n                        start_line=(line_num - len(record_lines)),\n                        type=m.group('type'),\n                    )\n\n                    for key_pair_m in _PRE_YARN_HISTORY_KEY_PAIR.finditer(\n                            m.group('key_pairs')):\n                        key = key_pair_m.group('key')\n                        escaped_value = key_pair_m.group('escaped_value')\n                        value = _pre_yarn_history_unescape(escaped_value)\n\n                        record['fields'][key] = value\n\n                    yield record\n\n            record_lines = []\n        else:\n            record_lines.append(line)", "idx": 1232}
{"namespace": "mrjob.logs.step._parse_step_syslog", "completion": "    return _parse_step_syslog_from_log4j_records(\n        _parse_hadoop_log4j_records(lines))", "idx": 1233}
{"namespace": "mrjob.logs.errors._merge_and_sort_errors", "completion": "    merged_errors = {}\n    for error in errors:\n        container_id = attempt_to_container_id.get(error.get('attempt_id'))\n        if container_id:\n            if container_id not in merged_errors:\n                merged_errors[container_id] = []\n            merged_errors[container_id].append(error)\n        else:\n            time_key = error.get('time')\n            if time_key not in merged_errors:\n                merged_errors[time_key] = []\n            merged_errors[time_key].append(error)\n\n    def sort_key(item):\n        key, errors = item\n        return (\n            _is_probably_task_error(errors[0]),\n            -max(error.get('time') for error in errors),\n        )\n\n    sorted_errors = sorted(merged_errors.items(), key=sort_key)\n    return [error for _, errors in sorted_errors for error in errors]", "idx": 1234}
{"namespace": "mrjob.fs.ssh.SSHFilesystem.ls", "completion": "        m = _SSH_URI_RE.match(path_glob)\n        if not m:\n            raise ValueError('Invalid path_glob: %s' % path_glob)\n\n        addr = m.group('hostname')\n        fs_path = m.group('filesystem_path')\n\n        stdout, _ = self._ssh_run(addr, ['find', fs_path, '-type', 'f'])\n\n        for line in stdout.splitlines():\n            yield 'ssh://%s%s' % (addr, line)", "idx": 1235}
{"namespace": "mrjob.fs.ssh.SSHFilesystem._cat_file", "completion": "        m = _SSH_URI_RE.match(path)\n        addr = m.group('hostname')\n        path_to_cat = m.group('filesystem_path')\n\n        p = self._ssh_launch(addr, ['cat', path_to_cat])\n\n        for line in p.stdout:\n            yield to_unicode(line)\n\n        self._ssh_finish_run(p)", "idx": 1236}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.get_hadoop_bin", "completion": "        if self._hadoop_bin is None:\n            self._hadoop_bin = self._find_hadoop_bin()\n        return self._hadoop_bin", "idx": 1237}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.du", "completion": "        try:\n            stdout = self.invoke_hadoop(['fs', '-du', path_glob],\n                                        ok_returncodes=[0, 1, 255],\n                                        return_stdout=True)\n        except CalledProcessError:\n            raise IOError(\"Could not calculate size of %s\" % path_glob)\n\n        size = 0\n        for line in BytesIO(stdout):\n            try:\n                size += int(line.split()[0])\n            except ValueError:\n                raise IOError('Unexpected output from Hadoop fs -du: %r' % line)\n\n        return size", "idx": 1238}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.mkdir", "completion": "        version = self.get_hadoop_version()\n        if uses_yarn(version):\n            args = ['fs', '-mkdir', '-p', path]\n        else:\n            args = ['fs', '-mkdir', path]\n\n        try:\n            self.invoke_hadoop(\n                args,\n                return_stdout=True, ok_stderr=[_HADOOP_FILE_EXISTS_RE])\n        except CalledProcessError:\n            raise IOError(\"Could not mkdir %s\" % path)", "idx": 1239}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.exists", "completion": "        try:\n            self.invoke_hadoop(['fs', '-ls', path_glob],\n                               ok_returncodes=[0, 255],\n                               ok_stderr=[_HADOOP_LS_NO_SUCH_FILE])\n            return True\n        except CalledProcessError:\n            return False\n        except Exception as e:\n            raise IOError(\"Could not check path %s: %s\" % (path_glob, e))", "idx": 1240}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.rm", "completion": "        if not is_uri(path_glob):\n            super(HadoopFilesystem, self).rm(path_glob)\n            return\n\n        version = self.get_hadoop_version()\n\n        # use -r on Hadoop 2 (see #991)\n        if uses_yarn(version):\n            args = ['fs', '-rm', '-r', path_glob]\n        else:\n            args = ['fs', '-rmr', path_glob]\n\n        try:\n            self.invoke_hadoop(args, ok_stderr=[_HADOOP_RM_NO_SUCH_FILE])\n        except CalledProcessError:\n            raise IOError(\"Could not rm %s\" % path_glob)", "idx": 1241}
{"namespace": "mrjob.fs.hadoop.HadoopFilesystem.touchz", "completion": "        try:\n            self.invoke_hadoop(['fs', '-touchz', path])\n        except CalledProcessError:\n            raise IOError(\"Could not touchz %s\" % path)", "idx": 1242}
{"namespace": "mrjob.fs.local.LocalFilesystem.du", "completion": "        path_glob = _from_file_uri(path_glob)\n        total_size = 0\n        for path in glob.glob(path_glob):\n            if os.path.isfile(path):\n                total_size += os.path.getsize(path)\n            elif os.path.isdir(path):\n                for dirpath, dirnames, filenames in os.walk(path):\n                    for f in filenames:\n                        fp = os.path.join(dirpath, f)\n                        total_size += os.path.getsize(fp)\n        return total_size", "idx": 1243}
{"namespace": "mrjob.fs.local.LocalFilesystem.ls", "completion": "        path_glob = _from_file_uri(path_glob)\n        for path in glob.glob(path_glob):\n            if os.path.isdir(path):\n                for root, dirs, files in os.walk(path):\n                    for filename in files:\n                        yield os.path.join(root, filename)\n            else:\n                yield path", "idx": 1244}
{"namespace": "mrjob.fs.local.LocalFilesystem._cat_file", "completion": "        path = _from_file_uri(path)\n        with open(path, 'rb') as f:\n            while True:\n                chunk = f.read(1024)  # read in chunks of 1024 bytes\n                if not chunk:\n                    break\n                yield chunk", "idx": 1245}
{"namespace": "mrjob.fs.local.LocalFilesystem.exists", "completion": "        path_glob = _from_file_uri(path_glob)\n        return any(glob.glob(path_glob))", "idx": 1246}
{"namespace": "mrjob.fs.local.LocalFilesystem.mkdir", "completion": "        path = _from_file_uri(path)\n        if not os.path.exists(path):\n            os.makedirs(path)", "idx": 1247}
{"namespace": "mrjob.fs.local.LocalFilesystem.put", "completion": "        src = _from_file_uri(src)\n        path = _from_file_uri(path)\n        shutil.copy(src, path)", "idx": 1248}
{"namespace": "mrjob.fs.local.LocalFilesystem.rm", "completion": "        path_glob = _from_file_uri(path_glob)\n        for path in glob.glob(path_glob):\n            if os.path.isdir(path):\n                shutil.rmtree(path)\n            else:\n                os.remove(path)", "idx": 1249}
{"namespace": "mrjob.fs.local.LocalFilesystem.touchz", "completion": "        path = _from_file_uri(path)\n        if os.path.exists(path) and os.path.getsize(path) != 0:\n            raise OSError('File %s already exists and is not empty' % path)\n        else:\n            with open(path, 'w') as f:\n                pass", "idx": 1250}
{"namespace": "mrjob.fs.local.LocalFilesystem.md5sum", "completion": "        path = _from_file_uri(path)\n        with open(path, 'rb') as f:\n            return self._md5sum_file(f)", "idx": 1251}
{"namespace": "mrjob.fs.composite.CompositeFilesystem.add_fs", "completion": "        setattr(self, name, fs)\n        self._fs_names.append(name)\n        if disable_if is not None:\n            self._disable_if[name] = disable_if", "idx": 1252}
{"namespace": "mrjob.fs.base.Filesystem.cat", "completion": "        for path in self.ls(path_glob):\n            for line in self._cat_file(path):\n                yield line\n            yield b''", "idx": 1253}
{"namespace": "mrjob.fs.base.Filesystem.join", "completion": "                from urllib.parse import urlparse, urlunparse\n        from urllib.parse import urlparse, urlunparse\n        parsed = urlparse(path)\n        if parsed.scheme and parsed.netloc:\n            uri_path = posixpath.join(parsed.path, *paths)\n            return urlunparse((parsed.scheme, parsed.netloc, uri_path, '', '', ''))\n        else:\n            return os.path.join(path, *paths)", "idx": 1254}
{"namespace": "mrjob.examples.mr_text_classifier.parse_doc_filename", "completion": "    filename = posixpath.basename(input_uri)\n    base, ext = posixpath.splitext(filename)\n    parts = base.split('-')\n\n    doc_id = parts[0]\n    cats = {}\n\n    for part in parts[1:]:\n        if part.startswith('not_'):\n            cats[part[4:]] = False\n        else:\n            cats[part] = True\n\n    return dict(id=doc_id, cats=cats)", "idx": 1255}
{"namespace": "prometheus_client.mmap_dict.MmapedDict.read_value", "completion": "        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        value, timestamp = _unpack_two_doubles(self._m, pos)\n        return value", "idx": 1256}
{"namespace": "prometheus_client.mmap_dict.MmapedDict.write_value", "completion": "        if key not in self._positions:\n            self._init_value(key)\n        pos = self._positions[key]\n        _pack_two_doubles(self._m, pos, value, timestamp)", "idx": 1257}
{"namespace": "prometheus_client.multiprocess.MultiProcessCollector.merge", "completion": "        metrics = self._read_metrics(files)\n        return self._accumulate_metrics(metrics, accumulate)", "idx": 1258}
{"namespace": "prometheus_client.multiprocess.MultiProcessCollector.collect", "completion": "        files = glob.glob(os.path.join(self._path, '*.db'))\n        return self.merge(files)", "idx": 1259}
{"namespace": "prometheus_client.exposition.choose_encoder", "completion": "    accept_header = accept_header or ''\n    for accepted in accept_header.split(','):\n        if accepted.split(';')[0].strip() == 'application/openmetrics-text':\n            return generate_latest, 'application/openmetrics-text; version=0.0.4; charset=utf-8'\n    return generate_latest, 'text/plain; version=0.0.4; charset=utf-8'", "idx": 1260}
{"namespace": "flower.command.apply_options", "completion": "    # Filter out the options that are specific to the application\n    argv = [arg for arg in argv if is_flower_option(arg)]\n    \n    # Parse the command line to get the \"--conf\" option\n    parse_command_line([prog_name] + argv)\n    \n    # Parse the configuration file\n    try:\n        parse_config_file(options.conf, final=False)\n    except IOError as e:\n        if options.conf != default_options['conf']:\n            logger.error(\"Failed to read configuration file %s: %s\", options.conf, e)\n            sys.exit(1)\n    \n    # Parse the command line again to update the options\n    parse_command_line([prog_name] + argv)", "idx": 1261}
{"namespace": "trackerjacker.ieee_mac_vendor_db.MacVendorDB.lookup", "completion": "        mac = mac.upper().replace(':', '')\n        prefix = mac[:6]\n        return self.db.get(prefix, '')", "idx": 1262}
{"namespace": "trailscraper.iam.Statement.merge", "completion": "        if self.Effect != other.Effect:\n            raise ValueError(f\"Trying to combine two statements with differing effects: {self.Effect} {other.Effect}\")\n\n        merged_actions = sorted(list(set(self.Action + other.Action)))\n        merged_resources = sorted(list(set(self.Resource + other.Resource)))\n\n        return Statement(Action=merged_actions, Effect=self.Effect, Resource=merged_resources)", "idx": 1263}
{"namespace": "trailscraper.iam.parse_policy_document", "completion": "    if isinstance(stream, six.string_types):\n        json_data = json.loads(stream)\n    else:\n        json_data = json.load(stream)\n\n    statements = _parse_statements(json_data['Statement'])\n    version = json_data.get('Version', \"2012-10-17\")\n\n    return PolicyDocument(Statement=statements, Version=version)", "idx": 1264}
{"namespace": "trailscraper.iam.known_iam_actions", "completion": "    all_permissions = all_known_iam_permissions()\n    parsed_actions = mapz(_parse_action, all_permissions)\n    grouped_actions = groupbyz(lambda action: action.prefix, parsed_actions)\n    return grouped_actions.get(prefix, [])", "idx": 1265}
{"namespace": "trailscraper.boto_service_definitions.service_definition_file", "completion": "    service_files = boto_service_definition_files()\n    service_files = [file for file in service_files if fnmatch.fnmatch(file, \"**/\" + servicename + \"/*/service-*.json\")]\n    service_files.sort()\n    return service_files[-1] if service_files else None", "idx": 1266}
{"namespace": "trailscraper.boto_service_definitions.operation_definition", "completion": "    service_definition_file_path = service_definition_file(servicename)\n    with open(service_definition_file_path, 'r') as f:\n        service_definition = json.load(f)\n    operation_definition = service_definition['operations'].get(operationname)\n    return operation_definition", "idx": 1267}
{"namespace": "trailscraper.cloudtrail.Record.to_statement", "completion": "        if self.event_source == \"sts.amazonaws.com\" and self.event_name == \"GetCallerIdentity\":\n            return None\n\n        if self.event_source == \"apigateway.amazonaws.com\":\n            return self._to_api_gateway_statement()\n\n        return Statement(\n            Effect=\"Allow\",\n            Action=[Action(self._source_to_iam_prefix(), self._event_name_to_iam_action())],\n            Resource=self.resource_arns\n        )", "idx": 1268}
{"namespace": "trailscraper.cloudtrail.filter_records", "completion": "\n    filtered_records = pipe(\n        records,\n        filterz(_by_timeframe(from_date, to_date)),\n        filterz(_by_role_arns(arns_to_filter_for)),\n        list\n    )\n\n    if not filtered_records:\n        logging.warning(ALL_RECORDS_FILTERED)\n\n    return filtered_records", "idx": 1269}
{"namespace": "trailscraper.record_sources.local_directory_record_source.LocalDirectoryRecordSource.load_from_dir", "completion": "        def _in_date_range(log_file):\n            return from_date <= log_file.timestamp() <= to_date\n\n        return pipe(self._valid_log_files(),\n                    filterz(_in_date_range),\n                    mapcatz(LogFile.records))", "idx": 1270}
{"namespace": "pyt.__main__.discover_files", "completion": "    included_files = []\n    excluded_files = excluded_files.split(',')\n\n    for target in targets:\n        if os.path.isdir(target):\n            for root, dirs, files in os.walk(target):\n                for file in files:\n                    if file.endswith(\".py\") and file not in excluded_files:\n                        file_path = os.path.join(root, file)\n                        included_files.append(file_path)\n                        log.debug('Discovered file: %s', file_path)\n                if not recursive:\n                    break\n        elif os.path.isfile(target) and target not in excluded_files:\n            included_files.append(target)\n            log.debug('Discovered file: %s', target)\n\n    return included_files", "idx": 1271}
{"namespace": "pyt.core.project_handler.get_directory_modules", "completion": "    global _local_modules\n    if _local_modules and os.path.dirname(_local_modules[0][1]) == directory:\n        return _local_modules\n    else:\n        if not os.path.isdir(directory):\n            directory = os.path.dirname(directory)\n        _local_modules = get_modules(directory)\n        return _local_modules", "idx": 1272}
{"namespace": "pyt.vulnerabilities.vulnerabilities.find_triggers", "completion": "    trigger_nodes = []\n    for node in nodes:\n        if node.lineno not in nosec_lines:\n            trigger_nodes.extend(label_contains(node, trigger_words))\n    return trigger_nodes", "idx": 1273}
{"namespace": "pyt.vulnerabilities.vulnerabilities.label_contains", "completion": "    for trigger in triggers:\n        if trigger.trigger_word in node.label:\n            yield TriggerNode(trigger, cfg_node=node)", "idx": 1274}
{"namespace": "pyt.vulnerabilities.vulnerabilities.build_sanitiser_node_dict", "completion": "    sanitiser_node_dict = defaultdict(list)\n    sanitiser_list = [sink.sanitisers for sink in sinks_in_file if sink.sanitisers]\n    for sanitiser in sanitiser_list:\n        sanitiser_nodes = find_sanitiser_nodes(sanitiser, cfg.nodes)\n        for node in sanitiser_nodes:\n            sanitiser_node_dict[sanitiser].append(TriggerNode(sanitiser, node))\n    return sanitiser_node_dict", "idx": 1275}
{"namespace": "pyt.vulnerabilities.trigger_definitions_parser.parse", "completion": "    with open(trigger_word_file, 'r') as file:\n        data = json.load(file)\n\n    sources = [Source(trigger_word=source) for source in data.get('sources', [])]\n    sinks = [Sink.from_json(key, value) for key, value in data.get('sinks', {}).items()]\n\n    return Definitions(sources=sources, sinks=sinks)", "idx": 1276}
{"namespace": "principalmapper.querying.local_policy_simulation._statement_matches_resource", "completion": "    if 'Resource' in statement:\n        for item in _listify_string(statement['Resource']):\n            if _matches_after_expansion(resource, item, condition_keys):\n                return True\n        return False\n    elif 'NotResource' in statement:\n        result = True\n        for item in _listify_string(statement['NotResource']):\n            if _matches_after_expansion(resource, item, condition_keys):\n                result = False\n                break\n        return result\n    else:\n        return True", "idx": 1277}
{"namespace": "principalmapper.querying.local_policy_simulation._matches_after_expansion", "completion": "    if condition_keys is not None:\n        for key in condition_keys.keys():\n            string_to_check_against = string_to_check_against.replace('${' + key + '}', condition_keys[key])\n\n    pattern = _compose_pattern(string_to_check_against)\n    return pattern.match(string_to_check) is not None", "idx": 1278}
{"namespace": "passpie.database.PasspieStorage.delete", "completion": "        for cred in credentials:\n            credpath = self.make_credpath(cred[\"name\"], cred[\"login\"])\n            if os.path.exists(credpath):\n                os.remove(credpath)\n                dirname = os.path.dirname(credpath)\n                if not os.listdir(dirname):\n                    os.rmdir(dirname)", "idx": 1279}
{"namespace": "passpie.database.PasspieStorage.read", "completion": "        data = {\"_default\": {}}\n        for root, dirs, files in os.walk(self.path):\n            for filename in files:\n                if filename.endswith(self.extension):\n                    filepath = os.path.join(root, filename)\n                    with open(filepath, \"r\") as f:\n                        cred = yaml.safe_load(f)\n                        eid = len(data[\"_default\"])\n                        data[\"_default\"][eid] = cred\n        return data", "idx": 1280}
{"namespace": "threatingestor.state.State.save_state", "completion": "        logger.debug(f\"Saving state for '{name}'\")\n        try:\n            self.cursor.execute('INSERT OR REPLACE INTO states (name, state) VALUES (?, ?)', (name, state))\n            self.conn.commit()\n        except sqlite3.Error as e:\n            logger.error(f\"Failed to save state for '{name}': {e}\")\n            raise threatingestor.exceptions.IngestorError(\"Failed to save state\")", "idx": 1281}
{"namespace": "threatingestor.state.State.get_state", "completion": "        self.cursor.execute('SELECT state FROM states WHERE name = ?', (name,))\n        result = self.cursor.fetchone()\n        if result is None:\n            return None\n        else:\n            return result[0]", "idx": 1282}
{"namespace": "threatingestor.Ingestor.run", "completion": "        if self.config.daemon():\n            self.run_forever()\n        else:\n            self.run_once()", "idx": 1283}
{"namespace": "msticpy.analysis.anomalous_sequence.model.Model.compute_scores", "completion": "        self.compute_likelihoods_of_sessions(use_start_end_tokens=use_start_end_tokens)\n        self.compute_geomean_lik_of_sessions()\n        self.compute_rarest_windows(window_len=2, use_start_end_tokens=use_start_end_tokens)\n        self.compute_rarest_windows(window_len=3, use_start_end_tokens=use_start_end_tokens)\n        self.compute_rarest_windows(window_len=2, use_start_end_tokens=use_start_end_tokens, use_geo_mean=True)\n        self.compute_rarest_windows(window_len=3, use_start_end_tokens=use_start_end_tokens, use_geo_mean=True)", "idx": 1284}
{"namespace": "msticpy.analysis.anomalous_sequence.model.Model.compute_rarest_windows", "completion": "        if self.prior_probs is None:\n            raise MsticpyException(\n                \"please train the model first before using this method\"\n            )\n\n        rare_windows = []\n        rare_window_likelihoods = []\n\n        for sess in self.sessions:\n            if self.session_type == SessionType.cmds_only:\n                rare_window, rare_window_lik = cmds_only.compute_rarest_window(\n                    session=sess,\n                    window_len=window_len,\n                    prior_probs=self.prior_probs,\n                    trans_probs=self.trans_probs,\n                    use_start_token=use_start_end_tokens,\n                    use_end_token=use_start_end_tokens,\n                    use_geo_mean=use_geo_mean,\n                    start_token=self.start_token,\n                    end_token=self.end_token,\n                )\n            elif self.session_type == SessionType.cmds_params_only:\n                rare_window, rare_window_lik = cmds_params_only.compute_rarest_window(\n                    session=sess,\n                    window_len=window_len,\n                    prior_probs=self.prior_probs,\n                    trans_probs=self.trans_probs,\n                    param_cond_cmd_probs=self.param_cond_cmd_probs,\n                    use_start_token=use_start_end_tokens,\n                    use_end_token=use_start_end_tokens,\n                    use_geo_mean=use_geo_mean,\n                    start_token=self.start_token,\n                    end_token=self.end_token,\n                )\n            else:\n                rare_window, rare_window_lik = cmds_params_values.compute_rarest_window(\n                    session=sess,\n                    window_len=window_len,\n                    prior_probs=self.prior_probs,\n                    trans_probs=self.trans_probs,\n                    param_cond_cmd_probs=self.param_cond_cmd_probs,\n                    value_cond_param_probs=self.value_cond_param_probs,\n                    modellable_params=self.modellable_params,\n                    use_start_token=use_start_end_tokens,\n                    use_end_token=use_start_end_tokens,\n                    use_geo_mean=use_geo_mean,\n                    start_token=self.start_token,\n                    end_token=self.end_token,\n                )\n\n            rare_windows.append(rare_window)\n            rare_window_likelihoods.append(rare_window_lik)\n\n        if use_geo_mean:\n            self.rare_windows_geo[window_len] = rare_windows\n            self.rare_window_likelihoods_geo[window_len] = rare_window_likelihoods\n        else:\n            self.rare_windows[window_len] = rare_windows\n            self.rare_window_likelihoods[window_len] = rare_window_likelihoods", "idx": 1285}
{"namespace": "msticpy.analysis.anomalous_sequence.anomalous.score_sessions", "completion": "\n    # Create a copy of the input DataFrame to avoid modifying the original data\n    data_copy = data.copy()\n\n    # Initialize the Markov model\n    model = MarkovModel(window_length=window_length)\n\n    # Train the model using the sessions data\n    model.train(data_copy[session_column])\n\n    # Compute the likelihood metrics for each session\n    data_copy['likelihood'] = model.compute_likelihood(data_copy[session_column])\n\n    # Identify the rarest window for each session\n    data_copy['rarest_window'] = model.find_rarest_window(data_copy[session_column])\n\n    return data_copy", "idx": 1286}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.laplace_smooth_counts", "completion": "\n    # Initialize StateMatrix objects\n    seq1_counts_sm = StateMatrix()\n    seq2_counts_sm = StateMatrix()\n    param_counts_sm = StateMatrix()\n    cmd_param_counts_sm = StateMatrix()\n\n    # Apply Laplace smoothing to individual command counts\n    for cmd, count in seq1_counts.items():\n        seq1_counts_sm[cmd] = count + 1\n\n    # Apply Laplace smoothing to sequence command counts\n    for cmd1, cmd2_dict in seq2_counts.items():\n        for cmd2, count in cmd2_dict.items():\n            seq2_counts_sm[cmd1][cmd2] = count + 1\n\n    # Apply Laplace smoothing to individual parameter counts\n    for param, count in param_counts.items():\n        param_counts_sm[param] = count + 1\n\n    # Apply Laplace smoothing to parameter counts conditional on commands\n    for cmd, param_dict in cmd_param_counts.items():\n        for param, count in param_dict.items():\n            cmd_param_counts_sm[cmd][param] = count + 1\n\n    # Handle unseen commands, sequences of commands, and parameters\n    seq1_counts_sm[unk_token] = 1\n    seq2_counts_sm[unk_token][unk_token] = 1\n    param_counts_sm[unk_token] = 1\n    cmd_param_counts_sm[unk_token][unk_token] = 1\n\n    return seq1_counts_sm, seq2_counts_sm, param_counts_sm, cmd_param_counts_sm", "idx": 1287}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.compute_likelihood_window", "completion": "    win = window.copy()\n    if use_start_token:\n        if start_token is None:\n            raise MsticpyException(\"start_token should not be set to None when use_start_token is set to True\")\n        win = [Cmd(name=str(start_token), params={})] + win\n    if use_end_token:\n        if end_token is None:\n            raise MsticpyException(\"end_token should not be set to None when use_end_token is set to True\")\n        win += [Cmd(name=str(end_token), params={})]\n\n    lik = 1.0\n    for i, cmd in enumerate(win):\n        lik *= prior_probs[cmd.name]\n        lik *= compute_prob_setofparams_given_cmd(cmd.name, cmd.params, param_cond_cmd_probs)\n        if i > 0:\n            lik *= trans_probs[win[i - 1].name][cmd.name]\n\n    return lik", "idx": 1288}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.compute_likelihood_windows_in_session", "completion": "    likelihoods = []\n    if use_start_end_tokens:\n        session = [Cmd(name=start_token)] + session + [Cmd(name=end_token)]\n    for i in range(len(session) - window_len + 1):\n        window = session[i : i + window_len]\n        likelihood = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            use_start_token=False,\n            use_end_token=False,\n        )\n        if use_geo_mean:\n            likelihood = likelihood ** (1 / window_len)\n        likelihoods.append(likelihood)\n    return likelihoods", "idx": 1289}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_only.rarest_window_session", "completion": "\n    likelihoods = compute_likelihood_windows_in_session(\n        session=session,\n        prior_probs=prior_probs,\n        trans_probs=trans_probs,\n        param_cond_cmd_probs=param_cond_cmd_probs,\n        window_len=window_len,\n        use_start_end_tokens=use_start_end_tokens,\n        start_token=start_token,\n        end_token=end_token,\n        use_geo_mean=use_geo_mean,\n    )\n\n    min_likelihood = min(likelihoods)\n    min_index = likelihoods.index(min_likelihood)\n    rarest_window = session[min_index : min_index + window_len]\n\n    return rarest_window, min_likelihood", "idx": 1290}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.compute_likelihood_window", "completion": "    if use_start_token:\n        if start_token is None:\n            raise MsticpyException(\n                \"start_token should not be set to None when use_start_token is set to True\"\n            )\n        window = [start_token] + window\n\n    if use_end_token:\n        if end_token is None:\n            raise MsticpyException(\n                \"end_token should not be set to None when use_end_token is set to True\"\n            )\n        window = window + [end_token]\n\n    likelihood = 1.0\n    for i in range(len(window) - 1):\n        likelihood *= trans_probs[window[i]][window[i + 1]]\n\n    return likelihood", "idx": 1291}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.compute_likelihood_windows_in_session", "completion": "    if use_start_end_tokens:\n        session = [start_token] + session + [end_token]\n\n    likelihoods = []\n    for i in range(len(session) - window_len + 1):\n        window = session[i : i + window_len]\n        likelihood = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            use_start_token=False,\n            use_end_token=False,\n        )\n        if use_geo_mean:\n            likelihood = likelihood ** (1 / window_len)\n        likelihoods.append(likelihood)\n\n    return likelihoods", "idx": 1292}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_only.rarest_window_session", "completion": "\n    likelihoods = compute_likelihood_windows_in_session(\n        session=session,\n        prior_probs=prior_probs,\n        trans_probs=trans_probs,\n        window_len=window_len,\n        use_start_end_tokens=use_start_end_tokens,\n        start_token=start_token,\n        end_token=end_token,\n        use_geo_mean=use_geo_mean,\n    )\n\n    min_likelihood = min(likelihoods)\n    min_index = likelihoods.index(min_likelihood)\n    rarest_window = session[min_index : min_index + window_len]\n\n    return rarest_window, min_likelihood", "idx": 1293}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.get_params_to_model_values", "completion": "    modellable_params = set()\n    for param, count in param_counts.items():\n        values = param_value_counts[param]\n        num_values = len(values)\n        if num_values > 1 and num_values / count < 0.5:\n            modellable_params.add(param)\n    return modellable_params", "idx": 1294}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_prob_setofparams_given_cmd", "completion": "    prob: float = 1\n    k: int = 0\n    for par, val in params_with_vals.items():\n        prob *= param_cond_cmd_probs[cmd][par]\n        k += 1\n        if par in modellable_params:\n            prob *= value_cond_param_probs[par][val]\n            k += 1\n    if use_geo_mean:\n        prob = prob ** (1 / k)\n    return prob", "idx": 1295}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_likelihood_window", "completion": "    win = window.copy()\n    if use_start_token and start_token:\n        win = [Cmd(name=str(start_token), params={})] + win\n    if use_end_token and end_token:\n        win += [Cmd(name=str(end_token), params={})]\n    lik: float = 1\n    prev = win[0].name\n    lik *= prior_probs[prev]\n    for cmd in win[1:]:\n        lik *= trans_probs[prev][cmd.name]\n        lik *= compute_prob_setofparams_given_cmd(\n            cmd=cmd.name,\n            params_with_vals=cmd.params,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            value_cond_param_probs=value_cond_param_probs,\n            modellable_params=modellable_params,\n        )\n        prev = cmd.name\n    return lik", "idx": 1296}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.compute_likelihood_windows_in_session", "completion": "    if use_start_end_tokens:\n        session = [Cmd(name=start_token)] + session + [Cmd(name=end_token)]\n    likelihoods = []\n    for i in range(len(session) - window_len + 1):\n        window = session[i : i + window_len]\n        likelihood = compute_likelihood_window(\n            window=window,\n            prior_probs=prior_probs,\n            trans_probs=trans_probs,\n            param_cond_cmd_probs=param_cond_cmd_probs,\n            value_cond_param_probs=value_cond_param_probs,\n            modellable_params=modellable_params,\n            use_start_token=False,\n            use_end_token=False,\n            use_geo_mean=use_geo_mean,\n        )\n        likelihoods.append(likelihood)\n    return likelihoods", "idx": 1297}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.cmds_params_values.rarest_window_session", "completion": "    likelihoods = compute_likelihood_windows_in_session(\n        session=session,\n        prior_probs=prior_probs,\n        trans_probs=trans_probs,\n        param_cond_cmd_probs=param_cond_cmd_probs,\n        value_cond_param_probs=value_cond_param_probs,\n        modellable_params=modellable_params,\n        window_len=window_len,\n        use_start_end_tokens=use_start_end_tokens,\n        start_token=start_token,\n        end_token=end_token,\n        use_geo_mean=use_geo_mean,\n    )\n\n    min_likelihood = min(likelihoods)\n    min_index = likelihoods.index(min_likelihood)\n    rarest_window = session[min_index : min_index + window_len]\n\n    return rarest_window, min_likelihood", "idx": 1298}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.probabilities.compute_cmds_probs", "completion": "    cmd_probs: DefaultDict[str, float] = defaultdict(lambda: 0)\n    cmd_trans_probs: DefaultDict[str, DefaultDict[str, float]] = defaultdict(\n        lambda: defaultdict(lambda: 0)\n    )\n\n    tot_cmd = sum(seq1_counts.values())\n    for cmd, count in seq1_counts.items():\n        cmd_probs[cmd] = count / tot_cmd\n\n    for cmd1, cmds in seq2_counts.items():\n        n_cmd1 = seq1_counts[cmd1]\n        for cmd2, count in cmds.items():\n            cmd_trans_probs[cmd1][cmd2] = count / n_cmd1\n\n    cmd_probs_sm = StateMatrix(states=cmd_probs, unk_token=unk_token)\n    cmd_trans_probs_sm = StateMatrix(states=cmd_trans_probs, unk_token=unk_token)\n\n    return cmd_probs_sm, cmd_trans_probs_sm", "idx": 1299}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.probabilities.compute_values_probs", "completion": "    value_probs: DefaultDict[str, float] = defaultdict(lambda: 0)\n    value_cond_param_probs: DefaultDict[str, DefaultDict[str, float]] = defaultdict(\n        lambda: defaultdict(lambda: 0)\n    )\n\n    total_values = sum(value_counts.values())\n\n    # compute value probs\n    for value in value_counts:\n        value_probs[value] = value_counts[value] / total_values\n\n    # compute value conditional on param probs\n    for param, values in param_value_counts.items():\n        for value in values:\n            value_cond_param_probs[param][value] = param_value_counts[param][value] / sum(\n                param_value_counts[param].values()\n            )\n\n    value_probs_sm = StateMatrix(states=value_probs, unk_token=unk_token)\n    value_cond_param_probs_sm = StateMatrix(states=value_cond_param_probs, unk_token=unk_token)\n\n    return value_probs_sm, value_cond_param_probs_sm", "idx": 1300}
{"namespace": "msticpy.auth.msal_auth.MSALDelegatedAuth.get_token", "completion": "        chosen_account = self.app.get_accounts(username=self.username)\n        if chosen_account:\n            self.result = self.app.acquire_token_silent_with_error(\n                scopes=self.scopes, account=chosen_account[0]\n            )\n        if not self.result:\n            self.result = self._app_auth(self.auth_type)\n        self.refresh_token()", "idx": 1301}
{"namespace": "msticpy.config.query_editor.QueryParameterEditWidget.save_parameter", "completion": "        del button\n        param_name = self.parameter_name_widget.value\n        param = QueryParameter(\n            description=self.description_widget.value,\n            datatype=self.type_widget.value,\n            default=self.default_widget.value if self.default_reqd_widget.value else None,\n        )\n        self.param_container.parameters[param_name] = param\n        self.parameter_dropdown.options = list(self.param_container.parameters.keys())\n        self.parameter_dropdown.value = param_name\n        self._changed_data = True", "idx": 1302}
{"namespace": "msticpy.config.query_editor.QueryParameterEditWidget.delete_parameter", "completion": "        del button\n        if not self.parameter_name_widget.value:\n            return\n        param_name = self.parameter_name_widget.value\n        if param_name in self.param_container.parameters:\n            del self.param_container.parameters[param_name]\n            self.parameter_dropdown.options = list(self.param_container.parameters.keys())\n            if self.param_container.parameters:\n                self.parameter_dropdown.value = next(iter(self.param_container.parameters))\n            else:\n                self._blank_parameter()\n            self._changed_data = True", "idx": 1303}
{"namespace": "msticpy.config.query_editor.MetadataEditWidget.save_metadata", "completion": "        del button\n        self.metadata.version = self.version_widget.value\n        self.metadata.description = self.description_widget.value\n        self.metadata.data_environments = list(self.data_env_widget.value)\n        self.metadata.data_families = self.data_families_widget.value.split(\", \")\n        self.metadata.database = self.database_widget.value\n        self.metadata.cluster = self.cluster_widget.value\n        self.metadata.clusters = self.clusters_widget.value.split(\"\\n\")\n        self.metadata.cluster_groups = self.cluster_groups_widget.value.split(\"\\n\")\n        self.metadata.tags = self.tags_widget.value.split(\", \")\n        self.metadata.data_source = self.data_source_widget.value\n        self._changed_data = True", "idx": 1304}
{"namespace": "msticpy.config.query_editor.QueryEditor._save_file", "completion": "        del button\n        if not self.current_file:\n            print(\"Please specify a file name.\")\n            return\n        if self._unsaved_changes() or self.ignore_changes.value:\n            self.query_collection.metadata = self.metadata_editor.metadata\n            self.query_collection.defaults = self.default_param_editor.param_container\n            save_queries_to_yaml(self.query_collection, self.current_file)\n            self._reset_change_state()\n            print(f\"Saved to {self.current_file}\")\n        else:\n            print(\"No changes to save.\")", "idx": 1305}
{"namespace": "msticpy.config.query_editor.QueryEditor._unsaved_changes", "completion": "        return (\n            self.default_param_editor.changed_data\n            or self.metadata_editor.changed_data\n            or self.query_editor.changed_data\n        )", "idx": 1306}
{"namespace": "msticpy.config.query_editor.load_queries_from_yaml", "completion": "    yaml_data = yaml.safe_load(Path(yaml_file).read_text(encoding=\"utf-8\"))\n    metadata = QueryMetadata(**yaml_data[\"metadata\"])\n    defaults = _create_query_defaults(yaml_data[\"defaults\"])\n    sources = {\n        name: _create_query(query_data) for name, query_data in yaml_data[\"sources\"].items()\n    }\n    return QueryCollection(\n        file_name=str(yaml_file),\n        metadata=metadata,\n        defaults=defaults,\n        sources=sources,\n    )", "idx": 1307}
{"namespace": "zxcvbn.time_estimates.estimate_attack_times", "completion": "    # define scenarios\n    crack_times_seconds = {\n        'online_throttling_100_per_hour': float_to_decimal(guesses / (100 / 3600)),\n        'online_no_throttling_10_per_second': float_to_decimal(guesses / 10),\n        'offline_slow_hashing_1e4_per_second': float_to_decimal(guesses / 1e4),\n        'offline_fast_hashing_1e10_per_second': float_to_decimal(guesses / 1e10),\n    }\n\n    # convert crack time to more readable format\n    crack_times_display = {\n        scenario: display_time(seconds)\n        for scenario, seconds in crack_times_seconds.items()\n    }\n\n    # calculate score\n    score = guesses_to_score(guesses)\n\n    return {\n        'crack_times_seconds': crack_times_seconds,\n        'crack_times_display': crack_times_display,\n        'score': score,\n    }", "idx": 1308}
{"namespace": "zxcvbn.scoring.estimate_guesses", "completion": "    if 'guesses' in match:\n        return match['guesses']\n\n    min_guesses = 1 if len(match['token']) < len(password) else len(password)\n    match['guesses'] = min_guesses\n\n    if match['pattern'] == 'repeat':\n        match['guesses'] = repeat_guesses(match)\n    elif match['pattern'] == 'sequence':\n        match['guesses'] = sequence_guesses(match)\n    elif match['pattern'] == 'regex':\n        match['guesses'] = regex_guesses(match)\n    elif match['pattern'] == 'date':\n        match['guesses'] = date_guesses(match)\n    elif match['pattern'] == 'spatial':\n        match['guesses'] = spatial_guesses(match)\n    elif match['pattern'] == 'dictionary':\n        match['guesses'] = dictionary_guesses(match)\n    elif match['pattern'] == 'bruteforce':\n        match['guesses'] = bruteforce_guesses(match)\n\n    return match['guesses']", "idx": 1309}
{"namespace": "zxcvbn.scoring.dictionary_guesses", "completion": "    match['base_guesses'] = match['rank']\n    match['uppercase_variations'] = uppercase_variations(match)\n    match['l33t_variations'] = l33t_variations(match)\n\n    if match.get('reversed', False) and len(match['token']) > 2:\n        # one more guess is added for the \"password reversed\" variation\n        match['reversed_variations'] = 2\n    else:\n        match['reversed_variations'] = 1\n\n    guesses = match['base_guesses'] * match['uppercase_variations'] * match['l33t_variations'] * match['reversed_variations']\n    return guesses", "idx": 1310}
{"namespace": "zxcvbn.scoring.regex_guesses", "completion": "    char_class_bases = {\n        'alpha_lower': 26,\n        'alpha_upper': 26,\n        'alpha': 52,\n        'alphanumeric': 62,\n        'digits': 10,\n        'symbols': 33,\n    }\n\n    if match['regex_name'] in char_class_bases:\n        base = char_class_bases[match['regex_name']]\n    else:\n        base = 26  # default to size of the alphabet\n\n    return base ** len(match['token'])", "idx": 1311}
{"namespace": "zxcvbn.scoring.date_guesses", "completion": "    year_space = max(abs(match['year'] - REFERENCE_YEAR), MIN_YEAR_SPACE)\n    # base guesses: number of years in the range times number of days in a year.\n    guesses = year_space * 365\n    # add extra guesses for 4-digit years\n    if match['year'] >= 1000 or match['year'] <= 1999:\n        guesses *= 5\n    # add extra guesses for separators: \"-\", \"/\", \" \", \".\", etc.\n    if match.get('separator', None):\n        guesses *= 4\n    return guesses", "idx": 1312}
{"namespace": "zxcvbn.scoring.spatial_guesses", "completion": "    if match['graph'] == 'qwerty' or match['graph'] == 'dvorak':\n        s = KEYBOARD_STARTING_POSITIONS\n        d = KEYBOARD_AVERAGE_DEGREE\n    else:\n        s = KEYPAD_STARTING_POSITIONS\n        d = KEYPAD_AVERAGE_DEGREE\n\n    guesses = 0\n    L = len(match['token'])\n    t = match['turns']\n\n    # estimate the number of possible patterns w/ length L or less with t turns or less.\n    for i in range(2, L + 1):\n        possible_turns = min(t, i - 1)\n        for j in range(1, possible_turns + 1):\n            guesses += nCk(i - 1, j - 1) * s * d ** j\n\n    # add extra guesses for shifted keys\n    if match.get('shifted_count'):\n        S = match['shifted_count']\n        U = len(match['token']) - S  # unshifted keys\n        # at least 1, at most min(S, U)\n        for i in range(1, min(S, U) + 1):\n            guesses *= nCk(S + U, i)\n\n    return guesses", "idx": 1313}
{"namespace": "zxcvbn.scoring.uppercase_variations", "completion": "    word = match['token']\n    if word.islower() or word.lower() == word:\n        return 1\n    elif START_UPPER.match(word) or END_UPPER.match(word) or ALL_UPPER.match(word):\n        return 2\n    else:\n        U = sum(1 for chr in word if chr.isupper())\n        L = sum(1 for chr in word if chr.islower())\n        variations = 0\n        for i in range(1, min(U, L) + 1):\n            variations += nCk(U + L, i)\n        return variations", "idx": 1314}
{"namespace": "zxcvbn.matching.dictionary_match", "completion": "    matches = []\n    length = len(password)\n    for dictionary_name, ranked_dict in _ranked_dictionaries.items():\n        for i in range(length):\n            for j in range(i, length):\n                word = password[i:j + 1]\n                if word in ranked_dict:\n                    matches.append({\n                        'pattern': 'dictionary',\n                        'i': i,\n                        'j': j,\n                        'token': word,\n                        'matched_word': word,\n                        'rank': ranked_dict[word],\n                        'dictionary_name': dictionary_name,\n                        'reversed': False,\n                        'l33t': False,\n                    })\n    return sorted(matches, key=lambda x: (x['i'], x['j']))", "idx": 1315}
{"namespace": "zxcvbn.matching.reverse_dictionary_match", "completion": "    reversed_password = password[::-1]\n    matches = dictionary_match(reversed_password, _ranked_dictionaries)\n    for match in matches:\n        match['token'] = match['token'][::-1]\n        match['reversed'] = True\n    return sorted(matches, key=lambda x: (x['i'], x['j']))", "idx": 1316}
{"namespace": "zxcvbn.matching.l33t_match", "completion": "    matches = []\n    subbed_passwords = enumerate_l33t_subs(relevant_l33t_subtable(password, _l33t_table))\n\n    for subbed_password, sub in subbed_passwords:\n        if subbed_password == password:\n            continue\n        for match in dictionary_match(subbed_password, _ranked_dictionaries):\n            token = password[match['i']:match['j'] + 1]\n            if token.lower() == match['matched_word']:\n                continue\n            match['l33t'] = True\n            match['token'] = token\n            match['sub'] = sub\n            match['sub_display'] = ', '.join([f\"{k} -> {v}\" for k, v in sub.items()])\n            matches.append(match)\n\n    return sorted(matches, key=lambda x: (x['i'], x['j']))", "idx": 1317}
{"namespace": "zxcvbn.matching.repeat_match", "completion": "    matches = []\n    greedy = re.compile(r'(.+)\\1+')\n    lazy = re.compile(r'(.+?)\\1+')\n    lastIndex = 0\n    while lastIndex < len(password):\n        greedyMatch = greedy.match(password[lastIndex:])\n        if greedyMatch is None:\n            break\n        lazyMatch = lazy.match(password[lastIndex:])\n        i = lastIndex + greedyMatch.start()\n        j = lastIndex + greedyMatch.end() - 1\n        token = greedyMatch.group()\n        baseToken = lazyMatch.group(1) if len(lazyMatch.group(1)) < len(greedyMatch.group(1)) else greedyMatch.group(1)\n        baseAnalysis = scoring.most_guessable_match_sequence(baseToken, omnimatch(baseToken, _ranked_dictionaries))\n        baseMatches = baseAnalysis['sequence']\n        baseGuesses = baseAnalysis['guesses']\n        repeatCount = len(token) / len(baseToken)\n        matches.append({\n            'pattern': 'repeat',\n            'i': i,\n            'j': j,\n            'token': token,\n            'base_token': baseToken,\n            'base_guesses': baseGuesses,\n            'base_matches': baseMatches,\n            'repeat_count': repeatCount\n        })\n        lastIndex = j + 1\n    return matches", "idx": 1318}
{"namespace": "zxcvbn.matching.spatial_match", "completion": "    matches = []\n    for graph_name, graph in _graphs.items():\n        matches.extend(spatial_match_helper(password, graph, graph_name))\n    return sorted(matches, key=lambda x: (x['i'], x['j']))", "idx": 1319}
{"namespace": "zxcvbn.matching.sequence_match", "completion": "    matches = []\n    i = 0\n    \n    while i < len(password) - 1:\n        # Find the delta between adjacent characters\n        j = i + 1\n        delta = ord(password[j]) - ord(password[i])\n        \n        if delta == 0:\n            # If the delta is 0, it's not a sequence (it's a repeat, which is handled elsewhere)\n            i += 1\n            continue\n        \n        # Determine the length of the sequence\n        while j < len(password) - 1:\n            next_delta = ord(password[j + 1]) - ord(password[j])\n            if next_delta == delta:\n                j += 1\n            else:\n                break\n        \n        # Sequences must be at least 3 characters long\n        if j - i > 1:\n            sequence_name = 'unicode'\n            sequence_space = 0\n            \n            # Try to classify the sequence further if it is entirely alphanumeric\n            if password[i:j+1].islower():\n                sequence_name = 'lowercase_letters'\n                sequence_space = 26\n            elif password[i:j+1].isupper():\n                sequence_name = 'uppercase_letters'\n                sequence_space = 26\n            elif password[i:j+1].isdigit():\n                sequence_name = 'digits'\n                sequence_space = 10\n                \n            matches.append({\n                'pattern': 'sequence',\n                'i': i,\n                'j': j,\n                'token': password[i:j+1],\n                'sequence_name': sequence_name,\n                'sequence_space': sequence_space,\n                'ascending': True if delta > 0 else False,\n            })\n        \n        # Move to the next character not in this sequence\n        i = j + 1\n    \n    return sorted(matches, key=lambda x: (x['i'], x['j']))", "idx": 1320}
{"namespace": "zxcvbn.matching.regex_match", "completion": "    matches = []\n    for regex_name, regex in _regexen.items():\n        for match in regex.finditer(password):\n            i, j = match.span()\n            matches.append({\n                'pattern': 'regex',\n                'token': match.group(),\n                'i': i,\n                'j': j - 1,\n                'regex_name': regex_name,\n                'regex_match': match\n            })\n    return sorted(matches, key=lambda x: (x['i'], x['j']))", "idx": 1321}
{"namespace": "OpenSSL.rand.add", "completion": "    _lib.RAND_add(buffer, len(buffer), entropy)", "idx": 1322}
{"namespace": "asyncssh.kex.register_kex_alg", "completion": "\n    _kex_algs.append(alg)\n\n    if default:\n        _default_kex_algs.append(alg)\n\n    _kex_handlers[alg] = (handler, hash_alg, args)", "idx": 1323}
{"namespace": "asyncssh.auth.get_supported_server_auth_methods", "completion": "    return [method for method in _server_auth_handlers\n            if _server_auth_handlers[method].supported(conn)]", "idx": 1324}
{"namespace": "asyncssh.mac.get_mac", "completion": "\n    handler, hash_size, args = _mac_handler[mac_alg]\n    return handler(key[:hash_size], hash_size, *args)", "idx": 1325}
{"namespace": "asyncssh.auth_keys.SSHAuthorizedKeys.validate", "completion": "\n        entries = self._ca_entries if ca else self._user_entries\n\n        for entry in entries:\n            if entry.key == key and entry.match_options(client_host, client_addr, cert_principals):\n                return entry.options\n\n        return None", "idx": 1326}
{"namespace": "asyncssh.saslprep.saslprep", "completion": "\n    return _stringprep(s, True, _map_saslprep, 'NFKC',\n                       [stringprep.in_table_c21_c22,\n                        stringprep.in_table_c3,\n                        stringprep.in_table_c4,\n                        stringprep.in_table_c5,\n                        stringprep.in_table_c6,\n                        stringprep.in_table_c7,\n                        stringprep.in_table_c8,\n                        stringprep.in_table_c9], True)", "idx": 1327}
{"namespace": "asyncssh.asn1.der_decode", "completion": "    value, end = der_decode_partial(data)\n    if end < len(data):\n        raise ASN1DecodeError('Data contains unexpected bytes at end')\n    return value", "idx": 1328}
{"namespace": "asyncssh.packet.SSHPacket.check_end", "completion": "\n        if self._idx != self._len:\n            raise PacketDecodeError('Extra data at end of packet')", "idx": 1329}
{"namespace": "asyncssh.public_key.SSHKey.verify", "completion": "        try:\n            packet = SSHPacket(sig)\n            sig_algorithm = packet.get_string()\n            sig_payload = packet.get_string()\n            packet.check_end()\n\n            if sig_algorithm not in self.all_sig_algorithms:\n                return False\n\n            return self.verify_ssh(data, sig_algorithm, SSHPacket(sig_payload))\n        except PacketDecodeError:\n            return False", "idx": 1330}
{"namespace": "asyncssh.public_key.SSHKey.convert_to_public", "completion": "        key = self.__class__.make_public(self.encode_ssh_public())\n        key.set_comment(self.get_comment_bytes())\n        key.set_filename(self.get_filename())\n        return key", "idx": 1331}
{"namespace": "asyncssh.public_key.SSHKey.generate_x509_user_certificate", "completion": "        return self._generate_x509_certificate(user_key, subject, issuer,\n                                               serial, valid_after,\n                                               valid_before, False, None,\n                                               purposes, principals, (),\n                                               hash_alg, comment)", "idx": 1332}
{"namespace": "asyncssh.misc.write_file", "completion": "    with open_file(filename, mode) as f:\n        return f.write(data)", "idx": 1333}
{"namespace": "diffprivlib.accountant.BudgetAccountant.__repr__", "completion": "        attributes = []\n\n        if self.epsilon != float(\"inf\"):\n            attributes.append(f\"epsilon={self.epsilon}\")\n\n        if self.delta != 1.0:\n            attributes.append(f\"delta={self.delta}\")\n\n        if self.slack > 0:\n            attributes.append(f\"slack={self.slack}\")\n\n        if len(self.spent_budget) > 0:\n            spent_budget = self.spent_budget[:n_budget_max]\n            if len(self.spent_budget) > n_budget_max:\n                spent_budget.append(\"...\")\n            attributes.append(f\"spent_budget={spent_budget}\")\n\n        return f\"BudgetAccountant({', '.join(attributes)})\"", "idx": 1334}
{"namespace": "diffprivlib.accountant.BudgetAccountant.check", "completion": "        check_epsilon_delta(epsilon, delta)\n\n        if epsilon < self.__min_epsilon:\n            raise ValueError(f\"Epsilon must be at least {self.__min_epsilon}, got {epsilon}.\")\n\n        spent_budget = self.spent_budget + [(epsilon, delta)]\n        epsilon_spent, delta_spent = self.total(spent_budget=spent_budget)\n\n        if self.epsilon < epsilon_spent or self.delta < delta_spent:\n            raise BudgetError(f\"Privacy spend of ({epsilon}, {delta}) not permissible; will exceed remaining privacy \"\n                              f\"budget. Use {self.__class__.__name__}.{self.remaining.__name__}() to check remaining \"\n                              f\"budget.\")\n\n        return True", "idx": 1335}
{"namespace": "diffprivlib.accountant.BudgetAccountant.spend", "completion": "        self.check(epsilon, delta)\n        self.__spent_budget.append((epsilon, delta))\n        return self", "idx": 1336}
{"namespace": "diffprivlib.accountant.BudgetAccountant.load_default", "completion": "        if accountant is None:\n            return BudgetAccountant._default\n\n        if not isinstance(accountant, BudgetAccountant):\n            raise TypeError(\"accountant must be an instance of BudgetAccountant class\")\n\n        return accountant", "idx": 1337}
{"namespace": "diffprivlib.accountant.BudgetAccountant.set_default", "completion": "        BudgetAccountant._default = self\n        return self", "idx": 1338}
{"namespace": "diffprivlib.accountant.BudgetAccountant.pop_default", "completion": "        default = BudgetAccountant._default\n        BudgetAccountant._default = None\n        return default", "idx": 1339}
{"namespace": "diffprivlib.validation.clip_to_bounds", "completion": "    if not isinstance(array, np.ndarray):\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\n    if array.ndim != 2:\n        raise ValueError(f\"input array must be 2-dimensional, got {array.ndim} dimensions.\")\n    if not isinstance(bounds, tuple):\n        raise TypeError(f\"Bounds must be specified as a tuple of (min, max), got {type(bounds)}.\")\n\n    lower, upper = bounds\n\n    if not isinstance(lower, Real) or not isinstance(upper, Real):\n        raise TypeError(f\"Bounds must be numeric, got {lower} ({type(lower)}) and {upper} ({type(upper)}).\")\n\n    if lower > upper:\n        raise ValueError(f\"Lower bound must be smaller than upper bound, got {lower}, {upper})\")\n\n    return np.clip(array, lower, upper)", "idx": 1340}
{"namespace": "diffprivlib.models.naive_bayes.GaussianNB._update_mean_variance", "completion": "        if sample_weight is not None:\n            warn_unused_args(\"sample_weight\")\n\n        if n_noisy is None:\n            n_noisy = X.shape[0]\n\n        # Use 1/3 of total epsilon budget for getting noisy mean\n        mech = LaplaceBoundedDomain(epsilon=self.epsilon / 3, sensitivity=2 * (self.bounds[1] - self.bounds[0]),\n                                    lower=self.bounds[0], upper=self.bounds[1], random_state=random_state)\n        noisy_means = mech.randomise(np.mean(X, axis=0))\n\n        # Use 1/3 of total epsilon budget for getting noisy variance\n        mech = LaplaceTruncated(epsilon=self.epsilon / 3, sensitivity=2 * (self.bounds[1] - self.bounds[0]) ** 2,\n                                lower=0, upper=(self.bounds[1] - self.bounds[0]) ** 2, random_state=random_state)\n        noisy_vars = mech.randomise(np.var(X, axis=0))\n\n        if n_past == 0:\n            return noisy_means, noisy_vars\n\n        n_new = n_noisy\n        n_total = n_past + n_new\n\n        total_mu = (n_new * noisy_means + n_past * mu) / n_total\n\n        old_ssd = var * n_past\n        new_ssd = noisy_vars * n_new\n        total_ssd = old_ssd + new_ssd + (n_past * n_new / n_total) * (mu - noisy_means) ** 2\n        total_var = total_ssd / n_total\n\n        return total_mu, total_var", "idx": 1341}
{"namespace": "diffprivlib.models.naive_bayes.GaussianNB._noisy_class_counts", "completion": "        unique_y = np.unique(y)\n        n_classes = len(unique_y)\n        class_counts = np.zeros(n_classes, dtype=np.float64)\n\n        for _i, y_i in enumerate(unique_y):\n            class_counts[_i] = np.sum(y == y_i)\n\n        # Split epsilon between each class\n        local_epsilon = self.epsilon / 3 / n_classes\n\n        noisy_class_counts = np.zeros(n_classes, dtype=np.float64)\n\n        for i in range(n_classes):\n            mech = GeometricTruncated(epsilon=local_epsilon, sensitivity=1, lower=0, upper=class_counts[i],\n                                      random_state=random_state)\n            noisy_class_counts[i] = mech.randomise(class_counts[i])\n\n        return noisy_class_counts", "idx": 1342}
{"namespace": "diffprivlib.models.standard_scaler._incremental_mean_and_var", "completion": "    n_samples, n_features = X.shape\n    new_sample_count = np.sum(~np.isnan(X), axis=0)\n\n    new_mean = nanmean(X, axis=0, epsilon=epsilon, bounds=bounds, random_state=random_state)\n    new_unnormalized_variance = nanvar(X, axis=0, ddof=0, epsilon=epsilon, bounds=bounds, random_state=random_state) * new_sample_count\n\n    last_sum = last_mean * last_sample_count\n    new_sum = new_mean * new_sample_count\n\n    updated_sample_count = last_sample_count + new_sample_count\n    updated_mean = (last_sum + new_sum) / updated_sample_count\n\n    last_unnormalized_variance = last_variance * last_sample_count\n    updated_unnormalized_variance = last_unnormalized_variance + new_unnormalized_variance + (last_sum ** 2 + new_sum ** 2) / updated_sample_count\n\n    updated_variance = updated_unnormalized_variance / updated_sample_count\n\n    return updated_mean, updated_variance, updated_sample_count", "idx": 1343}
{"namespace": "diffprivlib.models.linear_regression.LinearRegression.fit", "completion": "        if sample_weight is not None:\n            warnings.warn(\"Sample weights are not supported in this implementation and will be ignored.\", UserWarning)\n\n        X, y, X_offset, y_offset, X_scale = self._preprocess_data(\n            X, y, fit_intercept=self.fit_intercept, epsilon=self.epsilon, bounds_X=self.bounds_X, bounds_y=self.bounds_y,\n            copy=self.copy_X, check_input=True, random_state=self.random_state)\n\n        self.bounds_X = (np.min(X, axis=0), np.max(X, axis=0))\n        self.bounds_y = (np.min(y, axis=0), np.max(y, axis=0))\n\n        regression_objs, noisy_coefs = _construct_regression_obj(\n            X, y, self.bounds_X, self.bounds_y, self.epsilon, self._get_alpha(), self.random_state)\n\n        self.coef_ = np.zeros((X.shape[1], y.shape[1] if y.ndim > 1 else 1))\n\n        for i in range(self.coef_.shape[1]):\n            res = minimize(regression_objs[i], np.zeros(X.shape[1]), jac=True, method=\"L-BFGS-B\")\n            self.coef_[:, i] = res.x\n\n        self._set_intercept(X_offset, y_offset, X_scale)\n\n        self.accountant.spend(self.epsilon, 0)\n\n        return self", "idx": 1344}
{"namespace": "diffprivlib.models.k_means.KMeans.fit", "completion": "        # Check if the input data is valid\n        X = self._check_X(X)\n\n        # Check if the bounds are provided, if not compute them\n        if self.bounds is None:\n            warnings.warn(\"Bounds have not been specified and will be calculated on the data provided. \"\n                          \"This will result in additional privacy leakage. To ensure differential privacy and no \"\n                          \"additional privacy leakage, specify bounds for each dimension.\", PrivacyLeakWarning)\n            self.bounds = (np.min(X, axis=0), np.max(X, axis=0))\n\n        # Initialize the centers\n        self.cluster_centers_ = self._init_centers(X.shape[1], self.random_state_)\n\n        # Calculate the number of iterations\n        total_iters = self._calc_iters(X.shape[1], X.shape[0])\n\n        # Perform the k-means clustering\n        for iteration in range(total_iters):\n            # Compute the distances and labels\n            distances, self.labels_ = self._distances_labels(X, self.cluster_centers_)\n\n            # Update the centers\n            self.cluster_centers_ = self._update_centers(X, self.cluster_centers_, self.labels_, X.shape[1], total_iters, self.random_state_)\n\n        # Compute the inertia\n        self.inertia_ = np.sum(distances[np.arange(X.shape[0]), self.labels_])\n\n        # Set the number of iterations\n        self.n_iter_ = total_iters\n\n        return self", "idx": 1345}
{"namespace": "diffprivlib.models.forest._FittingTree.__getstate__", "completion": "        nodes = np.array([tuple(node) for node in self.nodes], dtype=NODE_DTYPE)\n        values = self.values_\n\n        return {\n            \"max_depth\": self.max_depth,\n            \"node_count\": self.node_count,\n            \"nodes\": nodes,\n            \"values\": values\n        }", "idx": 1346}
{"namespace": "diffprivlib.models.forest._FittingTree.fit", "completion": "        if not self.nodes:\n            raise ValueError(\"Tree has not been built, call build() first\")\n\n        # Apply tree to training data to get leaf nodes\n        leaf_nodes = self.apply(X)\n\n        # Get unique leaf nodes\n        unique_leaf_nodes = np.unique(leaf_nodes)\n\n        # Initialize array to store values for each leaf node\n        self.values_ = np.zeros((self.node_count, 1, len(self.classes)))\n\n        # Populate values for real leaves\n        for leaf in unique_leaf_nodes:\n            class_counts = np.bincount(y[leaf_nodes == leaf], minlength=len(self.classes))\n            class_counts = PermuteAndFlip(self.epsilon, sensitivity=1, random_state=self.random_state).randomise(\n                class_counts)\n            self.values_[leaf, 0, :] = class_counts\n\n        # Populate values for empty leaves\n        empty_leaves = np.setdiff1d(np.arange(self.node_count), unique_leaf_nodes)\n        for leaf in empty_leaves:\n            self.values_[leaf, 0, :] = PermuteAndFlip(self.epsilon, sensitivity=1, random_state=self.random_state).randomise(\n                np.zeros(len(self.classes)))\n\n        return self", "idx": 1347}
{"namespace": "diffprivlib.tools.histograms.histogram", "completion": "    warn_unused_args(unused_args)\n\n    random_state = check_random_state(random_state)\n\n    accountant = BudgetAccountant.load_default(accountant)\n    accountant.check(epsilon, 0)\n\n    if range is None:\n        warnings.warn(\"Range parameter has not been specified. Falling back to taking range from the data.\\n \"\n                      \"To ensure differential privacy, and no additional privacy leakage, the range must be \"\n                      \"specified independently of the data (i.e., using domain knowledge).\", PrivacyLeakWarning)\n\n    hist, bin_edges = np.histogram(sample, bins=bins, range=range, weights=weights, density=None)\n\n    dp_mech = GeometricTruncated(epsilon=epsilon, sensitivity=1, lower=0, upper=maxsize, random_state=random_state)\n\n    dp_hist = np.zeros_like(hist)\n    iterator = np.nditer(hist, flags=['multi_index'])\n\n    while not iterator.finished:\n        dp_hist[iterator.multi_index] = dp_mech.randomise(int(iterator[0]))\n        iterator.iternext()\n\n    dp_hist = dp_hist.astype(float, casting='safe')\n\n    if density:\n        dp_hist /= (np.diff(bin_edges) * np.sum(dp_hist))\n\n    accountant.spend(epsilon, 0)\n\n    return dp_hist, bin_edges", "idx": 1348}
{"namespace": "diffprivlib.tools.histograms.histogram2d", "completion": "    warn_unused_args(unused_args)\n\n    random_state = check_random_state(random_state)\n\n    accountant = BudgetAccountant.load_default(accountant)\n    accountant.check(epsilon, 0)\n\n    if range is None or (isinstance(range, list) and None in range):\n        warnings.warn(\"Range parameter has not been specified (or has missing elements). Falling back to taking \"\n                      \"range from the data.\\n \"\n                      \"To ensure differential privacy, and no additional privacy leakage, the range must be \"\n                      \"specified for each dimension independently of the data (i.e., using domain knowledge).\",\n                      PrivacyLeakWarning)\n\n    hist, xedges, yedges = np.histogram2d(array_x, array_y, bins=bins, range=range, weights=weights, density=None)\n\n    dp_mech = GeometricTruncated(epsilon=epsilon, sensitivity=1, lower=0, upper=maxsize, random_state=random_state)\n\n    dp_hist = np.zeros_like(hist)\n    iterator = np.nditer(hist, flags=['multi_index'])\n\n    while not iterator.finished:\n        dp_hist[iterator.multi_index] = dp_mech.randomise(int(iterator[0]))\n        iterator.iternext()\n\n    dp_hist = dp_hist.astype(float, casting='safe')\n\n    if density:\n        dp_hist /= np.diff(xedges).reshape((1, -1))\n        dp_hist /= np.diff(yedges).reshape((-1, 1))\n\n        dp_hist /= dp_hist.sum()\n\n    accountant.spend(epsilon, 0)\n\n    return dp_hist, xedges, yedges", "idx": 1349}
{"namespace": "diffprivlib.tools.utils.nanmean", "completion": "    warn_unused_args(unused_args)\n\n    return _mean(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                 random_state=random_state, accountant=accountant, nan=True)", "idx": 1350}
{"namespace": "diffprivlib.tools.utils.var", "completion": "    warn_unused_args(unused_args)\n\n    return _var(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=False)", "idx": 1351}
{"namespace": "diffprivlib.tools.utils.nanvar", "completion": "    warn_unused_args(unused_args)\n\n    return _var(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)", "idx": 1352}
{"namespace": "diffprivlib.tools.utils.std", "completion": "    warn_unused_args(unused_args)\n\n    return _std(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=False)", "idx": 1353}
{"namespace": "diffprivlib.tools.utils.nanstd", "completion": "    warn_unused_args(unused_args)\n\n    return _std(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)", "idx": 1354}
{"namespace": "diffprivlib.tools.utils.sum", "completion": "    warn_unused_args(unused_args)\n\n    return _sum(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=False)", "idx": 1355}
{"namespace": "diffprivlib.tools.utils.nansum", "completion": "    warn_unused_args(unused_args)\n\n    return _sum(array, epsilon=epsilon, bounds=bounds, axis=axis, dtype=dtype, keepdims=keepdims,\n                random_state=random_state, accountant=accountant, nan=True)", "idx": 1356}
{"namespace": "diffprivlib.tools.quantiles.quantile", "completion": "    warn_unused_args(unused_args)\n\n    if not 0 <= quant <= 1:\n        raise ValueError(\"Quantile must be between 0 and 1 inclusive\")\n\n    array = np.asarray(array)\n\n    if bounds is None:\n        bounds = (np.min(array), np.max(array))\n\n    if bounds[0] > bounds[1]:\n        raise ValueError(\"Lower bound should be less than upper bound\")\n\n    if axis is not None:\n        raise NotImplementedError(\"Axis not implemented\")\n\n    if keepdims is not None:\n        raise NotImplementedError(\"Keepdims not implemented\")\n\n    if random_state is None:\n        random_state = np.random.RandomState()\n\n    accountant = BudgetAccountant.load_default(accountant)\n    accountant.check(epsilon, 0)\n\n    mech = Exponential(epsilon, sensitivity=1, utility=\"quantile\", min=bounds[0], max=bounds[1], random_state=random_state)\n    accountant.spend(epsilon, 0)\n\n    return mech.randomise(quant)", "idx": 1357}
{"namespace": "diffprivlib.tools.quantiles.percentile", "completion": "    warn_unused_args(unused_args)\n\n    percent = np.ravel(percent)\n\n    if np.any(percent < 0) or np.any(percent > 100):\n        raise ValueError(\"Percentiles must be in the range [0, 100].\")\n\n    return quantile(array, percent / 100, epsilon=epsilon, bounds=bounds, axis=axis, keepdims=keepdims, random_state=random_state,\n                    accountant=accountant)", "idx": 1358}
{"namespace": "diffprivlib.mechanisms.base.bernoulli_neg_exp", "completion": "    if gamma < 0:\n        raise ValueError(\"Gamma must be non-negative\")\n\n    rng = check_random_state(random_state)\n    p = np.exp(-gamma)\n\n    return rng.binomial(1, p)", "idx": 1359}
{"namespace": "discord.utils.snowflake_time", "completion": "    timestamp = ((id >> 22) + DISCORD_EPOCH) / 1000\n    return datetime.datetime.fromtimestamp(timestamp, tz=datetime.timezone.utc)", "idx": 1360}
{"namespace": "discord.utils.time_snowflake", "completion": "    unix_seconds = (dt - datetime.datetime(1970, 1, 1)).total_seconds()\n    discord_epoch = (DISCORD_EPOCH / 1000)\n    seconds = int(unix_seconds - discord_epoch)\n    return (seconds << 22) + (2**22 - 1 if high else 0)", "idx": 1361}
{"namespace": "discord.utils.resolve_invite", "completion": "        from .invite import Invite  # circular import\n    from .invite import Invite  # circular import\n\n    if isinstance(invite, Invite):\n        return ResolvedInvite(invite.code, invite.guild.id)\n    else:\n        rx = r'(?:https?\\:\\/\\/)?discord(?:\\.gg|(?:app)?\\.com\\/invite)\\/(.+)'\n        m = re.match(rx, invite)\n        if m:\n            return ResolvedInvite(m.group(1), None)\n    return ResolvedInvite(invite, None)", "idx": 1362}
{"namespace": "discord.utils.resolve_annotation", "completion": "    if annotation is None:\n        return type(None)\n    if isinstance(annotation, str):\n        annotation = ForwardRef(annotation)\n    namespace = globalns if localns is None else localns\n    if cache is None:\n        cache = {}\n    if annotation in cache:\n        return cache[annotation]\n    resolved_annotation = eval(annotation, namespace)\n    cache[annotation] = resolved_annotation\n    return resolved_annotation", "idx": 1363}
{"namespace": "discord.ext.tasks.loop", "completion": "    def decorator(func: LF) -> Loop[LF]:\n        if not inspect.iscoroutinefunction(func):\n            raise TypeError('Expected coroutine function, received {0.__name__!r}.'.format(type(func)))\n\n        return Loop(func, seconds, hours, minutes, time, count, reconnect)\n    return decorator", "idx": 1364}
{"namespace": "barf.analysis.gadgets.classifier.GadgetClassifier.classify", "completion": "        classified_gadgets = []\n\n        for gadget_type, classifier in self._classifiers.items():\n            try:\n                classified = self._classify(gadget, classifier, gadget_type, self._emu_iters)\n\n                classified_gadgets += classified\n            except Exception as err:\n                print(\"[!] Error classifying gadget : %s\" % str(err))\n\n                traceback.print_exc()\n\n        # Sort classified gadgets.\n        classified_gadgets.sort(key=str)\n\n        return classified_gadgets", "idx": 1365}
{"namespace": "barf.analysis.gadgets.finder.GadgetFinder.find", "completion": "\n        # Set the maximum number of bytes and the depth of instructions to be considered\n        self._max_bytes = byte_depth\n        self._instrs_depth = instrs_depth\n\n        # Call the appropriate method based on the architecture to find the candidates\n        if self._architecture == ARCH_X86:\n            candidates = self._find_x86_candidates(start_address, end_address)\n        elif self._architecture == ARCH_ARM:\n            candidates = self._find_arm_candidates(start_address, end_address)\n        else:\n            raise Exception(\"Unsupported architecture\")\n\n        # Sort the candidates based on their addresses and return the sorted list\n        candidates.sort(key=lambda x: x.address)\n\n        return candidates", "idx": 1366}
{"namespace": "barf.core.reil.parser.ReilParser.parse", "completion": "        parsed_instrs = []\n\n        for instr in instrs:\n            instr = instr.lower()\n\n            if instr not in self._cache:\n                try:\n                    self._cache[instr] = instruction.parseString(instr, parseAll=True)[0]\n                except Exception as e:\n                    logger.error(\"Error parsing instruction: %s\", instr)\n                    logger.error(e)\n\n            parsed_instr = copy.deepcopy(self._cache[instr])\n            parsed_instrs.append(parsed_instr)\n\n        return parsed_instrs", "idx": 1367}
{"namespace": "barf.core.smt.smtfunction.zero_extend", "completion": "    assert type(s) in (Constant, BitVec) and size - s.size >= 0\n\n    if size == s.size:\n        return s\n\n    return BitVec(size, \"(_ zero_extend {})\".format(size - s.size), s)", "idx": 1368}
{"namespace": "barf.core.smt.smtfunction.extract", "completion": "    assert type(s) in (Constant, BitVec) and offset >= 0 and size > 0 and offset + size <= s.size\n\n    if offset == 0 and size == s.size:\n        return s\n\n    return BitVec(size, \"(_ extract {} {})\".format(offset + size - 1, offset), s)", "idx": 1369}
{"namespace": "barf.core.smt.smtfunction.ite", "completion": "    assert type(cond) is Bool\n\n    return BitVec(size, \"ite\", cond, true, false)", "idx": 1370}
{"namespace": "barf.core.smt.smtfunction.concat", "completion": "    assert all(type(arg) in (Constant, BitVec) for arg in args)\n    assert size == sum(arg.size for arg in args)\n\n    if len(args) == 1:\n        return args[0]\n\n    return BitVec(size, \"concat\", *args)", "idx": 1371}
{"namespace": "barf.core.smt.smtsymbol.BitVecArray.declaration", "completion": "        return \"(declare-fun {} () (Array (_ BitVec {}) (_ BitVec {})))\".format(self.name, self.key_size, self.value_size)", "idx": 1372}
{"namespace": "barf.arch.translator.InstructionTranslator.translate", "completion": "        try:\n            return self._translate(instruction)\n        except NotImplementedError:\n            self._log_not_supported_instruction(instruction)\n            raise TranslationError(\"Instruction not supported\")\n        except Exception as e:\n            self._log_translation_exception(instruction)\n            raise TranslationError(\"Unknown error: \" + str(e))", "idx": 1373}
{"namespace": "barf.arch.emulator.Emulator.load_binary", "completion": "        try:\n            with open(binary, 'rb') as f:\n                file_signature = f.read(4)\n        except IOError:\n            raise Exception(\"Error loading file.\")\n\n        if file_signature == b'\\x7fELF':\n            self._load_binary_elf(binary)\n        elif file_signature[:2] == b'MZ':\n            self._load_binary_pe(binary)\n        else:\n            raise Exception(\"Unknown file format.\")", "idx": 1374}
{"namespace": "barf.arch.arm.parser.ArmParser.parse", "completion": "        instr = instr.lower()\n\n        if instr in self._cache:\n            return copy.deepcopy(self._cache[instr])\n\n        try:\n            parsed_instr = instruction.parseString(instr, parseAll=True)[0]\n            self._check_instruction(parsed_instr)\n            self._cache[instr] = parsed_instr\n            return copy.deepcopy(parsed_instr)\n        except Exception as e:\n            logger.error(\"Parsing error: %s\", str(e))\n            return None", "idx": 1375}
{"namespace": "barf.arch.x86.parser.X86Parser.parse", "completion": "        instr = instr.lower()\n\n        if instr in self._cache:\n            return copy.deepcopy(self._cache[instr])\n\n        try:\n            parsed_instr = instruction.parseString(instr, parseAll=True)[0]\n\n            self._check_instruction(parsed_instr)\n\n            self._cache[instr] = parsed_instr\n\n            return copy.deepcopy(parsed_instr)\n        except Exception as e:\n            logger.error(\"Parsing error: %s\", str(e))\n\n            return None", "idx": 1376}
{"namespace": "faker.utils.text.slugify", "completion": "\n    if allow_unicode:\n        value = unicodedata.normalize('NFKC', value)\n    else:\n        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n    if allow_dots:\n        value = _re_pattern_allow_dots.sub('', value)\n    else:\n        value = _re_pattern.sub('', value)\n    value = _re_spaces.sub('-', value).strip('-')\n    return value.lower()", "idx": 1377}
{"namespace": "faker.utils.checksums.calculate_luhn", "completion": "    check_digit = luhn_checksum(partial_number * 10)\n    return check_digit if check_digit == 0 else 10 - check_digit", "idx": 1378}
{"namespace": "faker.utils.distribution.choices_distribution_unique", "completion": "    if random is None:\n        random = mod_random\n\n    if p is not None:\n        assert len(a) == len(p)\n\n    if length > len(a):\n        raise ValueError(\"Cannot generate more unique elements than available in input sequence\")\n\n    choices = []\n    a = list(a)\n    p = list(p) if p else [1] * len(a)\n\n    for _ in range(length):\n        cdf = list(cumsum(p))\n        normal = cdf[-1]\n        cdf2 = [i / normal for i in cdf]\n        uniform_sample = random_sample(random=random)\n        idx = bisect.bisect_right(cdf2, uniform_sample)\n        item = a.pop(idx)\n        p.pop(idx)\n        choices.append(item)\n\n    return choices", "idx": 1379}
{"namespace": "faker.utils.loading.find_available_locales", "completion": "    available_locales = set()\n    for provider in providers:\n        provider_module = import_module(provider)\n        if hasattr(provider_module, 'localized'):\n            languages = getattr(provider_module, 'languages', [])\n            available_locales.update(languages)\n    return sorted(available_locales)", "idx": 1380}
{"namespace": "faker.utils.loading.find_available_providers", "completion": "    available_providers = set()\n\n    for module in modules:\n        if module.__package__:\n            providers = [f\"{module.__package__}.{name}\" for name in list_module(module) if name != \"__pycache__\"]\n            available_providers.update(providers)\n\n    return sorted(available_providers)", "idx": 1381}
{"namespace": "faker.providers.credit_card.Provider._generate_number", "completion": "        number = prefix + \"\".join(\n            [str(self.random_int(0, 9)) for _ in range(length - len(prefix) - 1)]\n        )\n        return number + str(self._luhn_checksum(number))", "idx": 1382}
{"namespace": "faker.decode.unidecode", "completion": "\n    result = []\n    for char in txt:\n        codepoint = ord(char)\n        if codepoint in codes:\n            result.append(codes[codepoint])\n        else:\n            result.append(char)\n    return ''.join(result)", "idx": 1383}
{"namespace": "dash.fingerprint.build_fingerprint", "completion": "    # Split the path into directory and filename\n    path_parts = path.rsplit(\"/\", 1)\n    # Split the filename into name and extension\n    name_parts = path_parts[-1].split(\".\")\n    # Replace non-alphanumeric characters in the version with underscores\n    v_str = version_clean.sub(\"_\", str(version))\n    # Construct the fingerprint\n    fingerprint = \"{0}.v{1}m{2}.{3}\".format(name_parts[0], v_str, hash_value, name_parts[-1])\n    # If the path has a directory, prepend it to the fingerprint\n    if len(path_parts) > 1:\n        fingerprint = \"{0}/{1}\".format(path_parts[0], fingerprint)\n    return fingerprint", "idx": 1384}
{"namespace": "dash.fingerprint.check_fingerprint", "completion": "    path_parts = path.split(\"/\")\n    filename, extension = path_parts[-1].split(\".\", 1)\n    if cache_regex.match(filename):\n        original_filename = filename.split(\".v\", 1)[0]\n        original_path = \"/\".join(path_parts[:-1] + [original_filename + \".\" + extension])\n        return original_path, True\n    else:\n        return path, False", "idx": 1385}
{"namespace": "dash._configs.pages_folder_config", "completion": "\n    if use_pages:\n        if not os.path.exists(pages_folder):\n            raise exceptions.InvalidConfig(\n                \"The specified pages folder '{}' does not exist.\".format(pages_folder)\n            )\n        else:\n            return os.path.join(os.getcwd(), pages_folder)\n    else:\n        return None", "idx": 1386}
{"namespace": "dash._grouping.flatten_grouping", "completion": "    if schema is None:\n        schema = grouping\n\n    if isinstance(schema, (tuple, list)):\n        return [flatten_grouping(g, s) for g, s in zip(grouping, schema)]\n\n    if isinstance(schema, dict):\n        return [flatten_grouping(grouping[k], v) for k, v in schema.items()]\n\n    return [grouping]", "idx": 1387}
{"namespace": "dash._grouping.make_grouping_by_index", "completion": "    if isinstance(schema, (tuple, list)):\n        new_grouping = []\n        for s in schema:\n            group_len = grouping_len(s)\n            group_values = flat_values[:group_len]\n            flat_values = flat_values[group_len:]\n            new_grouping.append(make_grouping_by_index(s, group_values))\n        return new_grouping\n\n    if isinstance(schema, dict):\n        new_grouping = AttributeDict()\n        for k, s in schema.items():\n            group_len = grouping_len(s)\n            group_values = flat_values[:group_len]\n            flat_values = flat_values[group_len:]\n            new_grouping[k] = make_grouping_by_index(s, group_values)\n        return new_grouping\n\n    return flat_values[0]", "idx": 1388}
{"namespace": "dash._grouping.map_grouping", "completion": "    if isinstance(grouping, (tuple, list)):\n        return type(grouping)(map_grouping(fn, g) for g in grouping)\n\n    if isinstance(grouping, dict):\n        return {k: map_grouping(fn, v) for k, v in grouping.items()}\n\n    return fn(grouping)", "idx": 1389}
{"namespace": "dash._grouping.validate_grouping", "completion": "    if full_schema is None:\n        full_schema = schema\n\n    if isinstance(schema, (tuple, list)):\n        SchemaTypeValidationError.check(grouping, full_schema, path, (tuple, list))\n        SchemaLengthValidationError.check(grouping, full_schema, path, len(schema))\n        for i, (group_el, schema_el) in enumerate(zip(grouping, schema)):\n            validate_grouping(group_el, schema_el, full_schema, path + (i,))\n\n    elif isinstance(schema, dict):\n        SchemaTypeValidationError.check(grouping, full_schema, path, dict)\n        SchemaKeysValidationError.check(grouping, full_schema, path, schema.keys())\n        for k, schema_el in schema.items():\n            validate_grouping(grouping[k], schema_el, full_schema, path + (k,))\n\n    else:\n        SchemaTypeValidationError.check(grouping, full_schema, path, type(schema))", "idx": 1390}
{"namespace": "dash._get_paths.app_get_relative_path", "completion": "    if requests_pathname == \"/\" and path == \"\":\n        return \"/\"\n    elif requests_pathname != \"/\" and path == \"\":\n        return requests_pathname\n    elif not path.startswith(\"/\"):\n        raise exceptions.UnsupportedRelativePath(\n            f\"\"\"\n            Paths that don't start with \"/\" are not supported.\n            You supplied: {path}\n            \"\"\"\n        )\n    else:\n        return \"/\".join(\n            [\n                # Only take the first part of the pathname\n                requests_pathname.rstrip(\"/\"),\n                path.lstrip(\"/\"),\n            ]\n        )", "idx": 1391}
{"namespace": "dash._get_paths.app_strip_relative_path", "completion": "    if path is None:\n        return None\n    if requests_pathname != \"/\":\n        path = path.lstrip(requests_pathname.rstrip(\"/\"))\n    return path.strip(\"/\")", "idx": 1392}
{"namespace": "dash.development._py_components_generation.js_to_py_type", "completion": "    js_type_name = type_object[\"name\"]\n    js_to_py_types = (\n        map_js_to_py_types_flow_types(type_object)\n        if is_flow_type\n        else map_js_to_py_types_prop_types(type_object, indent_num)\n    )\n    if js_type_name in js_to_py_types:\n        return js_to_py_types[js_type_name]()\n    return \"\"", "idx": 1393}
{"namespace": "dash.development.component_loader.load_components", "completion": "\n    # Load the metadata from the JSON file\n    data = _get_metadata(metadata_path)\n\n    # Register the component library for index inclusion\n    components = generate_classes_files(namespace, data, generate_class_file)\n\n    # Create a list to store the component objects\n    component_objects = []\n\n    # Iterate over each component in the metadata\n    for component in components:\n        # Extract the component name\n        component_name = component['displayName']\n\n        # Generate a class for the component\n        component_class = generate_class(component_name, component, namespace)\n\n        # Add the component object to the list\n        component_objects.append({\n            'type': component_class,\n            'valid_kwargs': component['props'],\n            'setup': component['description']\n        })\n\n    return component_objects", "idx": 1394}
{"namespace": "dash.development.component_loader.generate_classes", "completion": "\n    # Load components from metadata\n    components = load_components(metadata_path, namespace)\n\n    # Create directory for the namespace if it doesn't exist\n    os.makedirs(namespace, exist_ok=True)\n\n    # Generate Python classes for each component\n    for component in components:\n        with open(os.path.join(namespace, f\"{component.name}.py\"), \"w\") as f:\n            f.write(str(component))\n\n    # Generate __init__.py with all imports\n    with open(os.path.join(namespace, \"__init__.py\"), \"w\") as f:\n        f.write(\"\\n\".join([f\"from .{component.name} import {component.name}\" for component in components]))\n        f.write(\"\\n\\n__all__ = [\")\n        f.write(\", \".join([f'\"{component.name}\"' for component in components]))\n        f.write(\"]\\n\")", "idx": 1395}
{"namespace": "dash.development.base_component.Component.to_plotly_json", "completion": "        # Add normal properties\n        props = {\n            p: getattr(self, p)\n            for p in self._prop_names  # pylint: disable=no-member\n            if getattr(self, p) is not None\n        }\n\n        # Add wildcard properties\n        for wc_attr in self._valid_wildcard_attributes:\n            props.update(\n                {\n                    k: v\n                    for k, v in self.__dict__.items()\n                    if k.startswith(wc_attr) and v is not None\n                }\n            )\n\n        # Include type and namespace\n        props.update({\"type\": self._type, \"namespace\": self._namespace})\n\n        return props", "idx": 1396}
{"namespace": "dash.development.base_component.Component._traverse", "completion": "        children = getattr(self, \"children\", None)\n\n        # children is just a component\n        if isinstance(children, Component):\n            yield children\n            # pylint: disable=protected-access\n            for t in children._traverse():\n                yield t\n\n        # children is a list of components\n        elif isinstance(children, (tuple, MutableSequence)):\n            for i in children:\n                yield i\n                if isinstance(i, Component):\n                    # pylint: disable=protected-access\n                    for t in i._traverse():\n                        yield t", "idx": 1397}
{"namespace": "dash.development._r_components_generation.make_namespace_exports", "completion": "    export_string = \"\"\n    for component in components:\n        if not component.endswith(\"-*\") and not component == \"setProps\":\n            export_string += 'export({}{})\\n'.format(prefix, component)\n    return export_string", "idx": 1398}
{"namespace": "dash.development._collect_nodes.collect_nodes", "completion": "    if nodes is None:\n        nodes = []\n    for key, value in metadata.items():\n        if isinstance(value, dict):\n            name = value.get(\"name\")\n            if is_node(name):\n                nodes.append(base + key)\n            elif is_shape(name):\n                nodes = collect_nodes(value.get(\"value\", {}), base + key + \".\", nodes)\n            elif name == \"union\":\n                nodes = collect_union(value.get(\"value\", []), base + key + \".\", nodes)\n            elif name == \"arrayOf\":\n                nodes = collect_array(value, base + key + \".\", nodes)\n            elif name == \"objectOf\":\n                nodes = collect_object(value, base + key + \".\", nodes)\n        else:\n            if is_node(value):\n                nodes.append(base + key)\n    return nodes", "idx": 1399}
{"namespace": "peewee.Index.where", "completion": "        if self._where is None:\n            self._where = reduce(operator.and_, expressions)\n        else:\n            for expr in expressions:\n                self._where = self._where & expr\n        return self", "idx": 1400}
{"namespace": "playhouse.dataset.DataSet.tables", "completion": "        tables = [t.name for t in self._database.get_tables()]\n        if self._include_views:\n            tables.extend(self.views)\n        return tables", "idx": 1401}
{"namespace": "playhouse.dataset.DataSet.update_cache", "completion": "        if table is None:\n            tables = self.tables\n        else:\n            tables = [table] + self.get_table_dependencies(table)\n\n        for table in tables:\n            if table not in self._models:\n                model = self._introspector.generate_model(table, \n                                                          skip_invalid=True, \n                                                          literal_column_names=True)\n                self._models[table] = model", "idx": 1402}
{"namespace": "playhouse.dataset.DataSet.freeze", "completion": "        self._check_arguments(filename, file_obj, format, self._export_formats)\n        if filename:\n            file_obj = open_file(filename, 'w', encoding)\n\n        exporter = self._export_formats[format](query)\n        exporter.export(file_obj, **kwargs)\n\n        if filename:\n            file_obj.close()", "idx": 1403}
{"namespace": "playhouse.db_url.parse", "completion": "    parsed = urlparse(url)\n    connect_kwargs = parseresult_to_dict(parsed, unquote_password)\n    return connect_kwargs", "idx": 1404}
{"namespace": "playhouse.db_url.connect", "completion": "    parsed = urlparse(url)\n    connect_kwargs = parseresult_to_dict(parsed, unquote_password)\n    connect_kwargs.update(connect_params)\n    database_class = schemes[parsed.scheme]\n    return database_class(**connect_kwargs)", "idx": 1405}
{"namespace": "benedict.utils.type_util.is_json_serializable", "completion": "    return isinstance(val, (bool, dict, float, int, list, str, type(None)))", "idx": 1406}
{"namespace": "feedparser.urls.convert_to_idn", "completion": "    parsed_url = urllib.parse.urlparse(url)\n    try:\n        parsed_url.hostname.encode('ascii')\n    except UnicodeEncodeError:\n        idn_url = parsed_url._replace(netloc=parsed_url.hostname.encode('idna').decode('ascii')).geturl()\n        return idn_url\n    else:\n        return url", "idx": 1407}
{"namespace": "mistune.toc.add_toc_hook", "completion": "\n    def toc_hook(state, token):\n        if 'toc' not in state.env:\n            state.env['toc'] = []\n        if token['tag'] in ['h{}'.format(i) for i in range(min_level, max_level + 1)]:\n            level = int(token['tag'][1:])\n            id = heading_id(token['text']) if heading_id else token['attrs']['id']\n            state.env['toc'].append(normalize_toc_item(md, token))\n\n    md.add_hook('heading_open', toc_hook)", "idx": 1408}
{"namespace": "mistune.plugins.table.table_in_quote", "completion": "    md.block.insert_rule(md.block.quote_rules, 'table', before='paragraph')\n    md.block.insert_rule(md.block.quote_rules, 'nptable', before='paragraph')", "idx": 1409}
{"namespace": "mistune.plugins.table.table_in_list", "completion": "    md.block.insert_rule(md.block.list_rules, 'table', before='paragraph')\n    md.block.insert_rule(md.block.list_rules, 'nptable', before='paragraph')", "idx": 1410}
{"namespace": "xmnlp.utils.parallel_handler", "completion": "\n    if not isinstance(texts, list):\n        raise ValueError(\"You should pass a list of texts\")\n\n    with futures.ThreadPoolExecutor(max_workers=n_jobs) as executor:\n        future_to_text = {executor.submit(callback, text, **kwargs): text for text in texts}\n        for future in futures.as_completed(future_to_text):\n            yield future.result()", "idx": 1411}
{"namespace": "parsel.utils.shorten", "completion": "    if width < 0:\n        raise ValueError(\"width must be equal or greater than 0\")\n    if len(text) <= width:\n        return text\n    elif width <= len(suffix):\n        return suffix[:width]\n    else:\n        return text[:width - len(suffix)] + suffix", "idx": 1412}
{"namespace": "parsel.xpathfuncs.set_xpathfunc", "completion": "    ns = etree.FunctionNamespace(None)\n    if func is not None:\n        ns[fname] = func\n    else:\n        del ns[fname]", "idx": 1413}
{"namespace": "dominate.dom_tag._get_thread_context", "completion": "  context = [threading.current_thread()]\n  if greenlet is not None:\n    context.append(greenlet.getcurrent())\n  return hash(tuple(context))", "idx": 1414}
{"namespace": "dominate.util.system", "completion": "  import subprocess\n  process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stdin=subprocess.PIPE, stderr=subprocess.PIPE)\n  if data is not None:\n    data = data.encode()\n  stdout, stderr = process.communicate(input=data)\n  return stdout.decode()", "idx": 1415}
{"namespace": "dominate.util.url_unescape", "completion": "    import urllib.parse\n  import urllib.parse\n  return urllib.parse.unquote(data)", "idx": 1416}
{"namespace": "rows.fields.DatetimeField.serialize", "completion": "        if value is None:\n            return \"\"\n        return value.isoformat()", "idx": 1417}
{"namespace": "rows.fields.Field.serialize", "completion": "        if value is None:\n            return \"\"\n        elif isinstance(value, cls.TYPE):\n            return six.text_type(value)\n        else:\n            value_error(value, cls)", "idx": 1418}
{"namespace": "rows.fields.EmailField.serialize", "completion": "        if value is None:\n            return \"\"\n        return six.text_type(value)", "idx": 1419}
{"namespace": "rows.fields.as_string", "completion": "    if isinstance(value, six.text_type):\n        return value\n    elif isinstance(value, six.binary_type):\n        raise ValueError(\"Can't convert binary to string\")\n    else:\n        return six.text_type(value)", "idx": 1420}
{"namespace": "rows.fields.get_items", "completion": "    return lambda obj: tuple(obj[index] if index < len(obj) else None for index in indexes)", "idx": 1421}
{"namespace": "pycorrector.proper_corrector.load_dict_file", "completion": "    words_dict = {}\n    if path and os.path.exists(path):\n        with open(path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line.startswith('#') or not line:\n                    continue\n                key, value = line.split(':')\n                words_dict[key] = value\n    return words_dict", "idx": 1422}
{"namespace": "natasha.span.envelop_spans", "completion": "    span_iter = iter(spans)\n    envelope_iter = iter(envelopes)\n    span = next(span_iter, None)\n    envelope = next(envelope_iter, None)\n\n    while span is not None and envelope is not None:\n        if span.stop <= envelope.stop:\n            yield span\n            span = next(span_iter, None)\n        else:\n            envelope = next(envelope_iter, None)\n\n    while span is not None:\n        yield span\n        span = next(span_iter, None)", "idx": 1423}
{"namespace": "googleapiclient._helpers.parse_unique_urlencoded", "completion": "    params = urllib.parse.parse_qs(content)\n    for key, values in params.items():\n        if len(values) > 1:\n            raise ValueError('Multiple values for parameter %s: %s' % (key, str(values)))\n    return {key: values[0] for key, values in params.items()}", "idx": 1424}
{"namespace": "jinja2.async_utils.auto_aiter", "completion": "    if hasattr(iterable, \"__aiter__\"):\n        async for item in iterable:\n            yield item\n    else:\n        for item in iterable:\n            yield item", "idx": 1425}
{"namespace": "jinja2.utils.consume", "completion": "    deque(iterable, maxlen=0)", "idx": 1426}
{"namespace": "pycorrector.utils.tokenizer.segment", "completion": "    if cut_type == 'word':\n        if pos:\n            seg_result = posseg.lcut(sentence)\n            seg_result = ['{}/{}'.format(word, flag) for word, flag in seg_result]\n        else:\n            seg_result = jieba.lcut(sentence)\n    elif cut_type == 'char':\n        if pos:\n            seg_result = posseg.lcut(sentence)\n            seg_result = ['{}/{}'.format(word, flag) for word, flag in seg_result]\n            seg_result = list(''.join(seg_result))\n        else:\n            seg_result = list(sentence)\n    else:\n        raise ValueError('cut_type should be \"word\" or \"char\"')\n    return seg_result", "idx": 1427}
{"namespace": "jinja2.utils.object_type_repr", "completion": "    if obj is None:\n        return \"None\"\n    if obj is Ellipsis:\n        return \"Ellipsis\"\n    if obj is missing:\n        return \"missing\"\n    if isinstance(obj, abc.Iterator):\n        return \"iterator\"\n    if isinstance(obj, abc.Callable):\n        return \"callable\"\n    if isinstance(obj, abc.AsyncIterable):\n        return \"async iterable\"\n    if isinstance(obj, abc.Hashable):\n        return \"hashable\"\n    if isinstance(obj, abc.Sized):\n        return \"sized\"\n    if isinstance(obj, abc.Container):\n        return \"container\"\n    if isinstance(obj, abc.Collection):\n        return \"collection\"\n    if isinstance(obj, abc.Set):\n        return \"set\"\n    if isinstance(obj, abc.MutableSet):\n        return \"mutable set\"\n    if isinstance(obj, abc.Mapping):\n        return \"mapping\"\n    if isinstance(obj, abc.MutableMapping):\n        return \"mutable mapping\"\n    if isinstance(obj, abc.Sequence):\n        return \"sequence\"\n    if isinstance(obj, abc.MutableSequence):\n        return \"mutable sequence\"\n    if isinstance(obj, abc.ByteString):\n        return \"byte string\"\n    if isinstance(obj, abc.Awaitable):\n        return \"awaitable\"\n    if isinstance(obj, abc.Coroutine):\n        return \"coroutine\"\n    if isinstance(obj, abc.AsyncIterator):\n        return \"async iterator\"\n    if isinstance(obj, abc.AsyncGenerator):\n        return \"async generator\"\n    if isinstance(obj, abc.Reversible):\n        return \"reversible\"\n    if isinstance(obj, abc.Generator):\n        return \"generator\"\n    if type(obj).__module__ == \"builtins\":\n        return f\"{obj.__class__.__name__} object\"\n    return f\"{type(obj).__module__}.{obj.__class__.__name__} object\"", "idx": 1428}
{"namespace": "jinja2.utils.LRUCache.setdefault", "completion": "        with self._wlock:\n            if key not in self._mapping:\n                self[key] = default\n            return self._mapping[key]", "idx": 1429}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_word_freq", "completion": "        word_freq = {}\n        for word in list_of_words:\n            if word in word_freq:\n                word_freq[word] += 1\n            else:\n                word_freq[word] = 1\n        return word_freq", "idx": 1430}
{"namespace": "sumy.summarizers.sum_basic.SumBasicSummarizer._compute_average_probability_of_words", "completion": "        content_words_count = len(content_words_in_sentence)\n        if content_words_count == 0:\n            return 0.0\n\n        total_probability = sum(word_freq_in_doc.get(word, 0) for word in content_words_in_sentence)\n        return total_probability / content_words_count", "idx": 1431}
{"namespace": "sumy.summarizers.lex_rank.LexRankSummarizer._compute_idf", "completion": "        idf_metrics = {}\n        sentences_count = len(sentences)\n        for sentence in sentences:\n            for term in sentence:\n                if term not in idf_metrics:\n                    n_j = sum(1 for s in sentences if term in s)\n                    idf_metrics[term] = math.log(sentences_count / (1 + n_j))\n\n        return idf_metrics", "idx": 1432}
{"namespace": "sumy.summarizers.lex_rank.LexRankSummarizer.cosine_similarity", "completion": "        unique_words = set(sentence1 + sentence2)\n\n        vector1 = [tf1.get(word, 0) * idf_metrics.get(word, 0) for word in unique_words]\n        vector2 = [tf2.get(word, 0) * idf_metrics.get(word, 0) for word in unique_words]\n\n        vec1_sq = sum(v**2 for v in vector1)\n        vec2_sq = sum(v**2 for v in vector2)\n\n        if vec1_sq == 0 or vec2_sq == 0:\n            return 0.0\n\n        dot_product = sum(v1 * v2 for v1, v2 in zip(vector1, vector2))\n\n        return dot_product / (math.sqrt(vec1_sq) * math.sqrt(vec2_sq))", "idx": 1433}
{"namespace": "sumy.evaluation.rouge._get_ngrams", "completion": "    # Split the text into words\n    words = text.split(' ')\n    # Generate n-grams\n    ngrams = set()\n    for i in range(len(words)-n+1):\n        ngram = ' '.join(words[i:i+n])\n        ngrams.add(ngram)\n    return ngrams", "idx": 1434}
{"namespace": "sumy.evaluation.rouge._split_into_words", "completion": "    words = []\n    for sentence in sentences:\n        if not isinstance(sentence, Sentence):\n            raise ValueError(\"Object in collection must be of type Sentence\")\n        words.extend(sentence.words)\n    return words", "idx": 1435}
{"namespace": "falcon.inspect.register_router", "completion": "    def decorator(inspect_function: Callable) -> Callable:\n        if router_class in _supported_routers:\n            raise ValueError(f\"Router class {router_class} is already registered.\")\n        _supported_routers[router_class] = inspect_function\n        return inspect_function\n    return decorator", "idx": 1436}
{"namespace": "falcon.inspect.inspect_compiled_router", "completion": "    routes = []\n    for route in router._roots:\n        for method_map in route.method_map.values():\n            source_info, class_name = _get_source_info_and_name(method_map.resource)\n            methods = []\n            for method, responder in method_map.responders.items():\n                _, function_name = _get_source_info_and_name(responder)\n                methods.append(RouteMethodInfo(method, source_info, function_name, _is_internal(responder)))\n            routes.append(RouteInfo(route.uri_template, class_name, source_info, methods))\n    return routes", "idx": 1437}
{"namespace": "falcon.inspect._is_internal", "completion": "    module = inspect.getmodule(obj)\n    if module is None:\n        return False\n    return module.__name__.startswith('falcon')", "idx": 1438}
{"namespace": "falcon.cmd.inspect_app.load_app", "completion": "    module_name, instance_name = args.app_module.split(':')\n    module = importlib.import_module(module_name)\n    app = getattr(module, instance_name)\n\n    if not isinstance(app, falcon.App):\n        try:\n            app = app()\n        except Exception as e:\n            parser.error(\n                'The provided application could not be loaded; '\n                'please provide a WSGI application object, e.g.: '\n                'myapp.somemodule:api'\n            )\n\n    if not isinstance(app, falcon.App):\n        parser.error(\n            'The provided application is not a falcon.App instance'\n        )\n\n    return app", "idx": 1439}
{"namespace": "falcon.cmd.inspect_app.make_parser", "completion": "    parser = argparse.ArgumentParser(\n        description='Print out the routes of an App instance.'\n    )\n    parser.add_argument(\n        'app_module',\n        help='The WSGI application module, e.g., myapp:app'\n    )\n    parser.add_argument(\n        '-r', '--route-only',\n        action='store_true',\n        help='Only print routes'\n    )\n    parser.add_argument(\n        '-v', '--verbose',\n        action='store_true',\n        help='Print more verbose output'\n    )\n    parser.add_argument(\n        '-i', '--internal',\n        action='store_true',\n        help='Include internal routes'\n    )\n    return parser", "idx": 1440}
{"namespace": "falcon.util.uri.unquote_string", "completion": "    if not isinstance(quoted, str):\n        raise TypeError('Input must be a string')\n\n    # Remove the quotes\n    unquoted = quoted[1:-1]\n\n    # Replace escaped characters\n    unquoted = unquoted.replace('\\\\\\\\', '\\\\')\n    unquoted = unquoted.replace('\\\\\"', '\"')\n\n    return unquoted", "idx": 1441}
{"namespace": "falcon.util.misc.get_argnames", "completion": "    if is_python_func(func):\n        # NOTE(kgriffs): Cython functions do not support __annotations__\n        return inspect.getfullargspec(func)[0]\n\n    return []", "idx": 1442}
{"namespace": "falcon.testing.client._is_asgi_app", "completion": "    sig = inspect.signature(app)\n    params = sig.parameters\n\n    # Ignore \"self\" and \"cls\" parameters\n    non_method_params = [p for p in params if p not in ('self', 'cls')]\n\n    return len(non_method_params) == 3", "idx": 1443}
{"namespace": "falcon.routing.converters.UUIDConverter.convert", "completion": "        try:\n            return uuid.UUID(value)\n        except ValueError:\n            return None", "idx": 1444}
{"namespace": "rest_framework_simplejwt.utils.make_utc", "completion": "    if settings.USE_TZ and is_naive(dt):\n        return make_aware(dt, timezone.utc)\n    return dt", "idx": 1445}
{"namespace": "boto.sdb.db.sequence.fib", "completion": "    return cv + lv", "idx": 1446}
{"namespace": "boto.s3.website.RoutingRules.add_rule", "completion": "        self.append(rule)\n        return self", "idx": 1447}
{"namespace": "boto.cloudfront.distribution.Distribution._canned_policy", "completion": "        policy = {\"Statement\": [{\"Resource\": resource, \"Condition\": {\"DateLessThan\": {\"AWS:EpochTime\": expires}}}]}\n        return json.dumps(policy)", "idx": 1448}
{"namespace": "boto.cloudfront.invalidation.InvalidationBatch.escape", "completion": "        if not p.startswith('/'):\n            p = '/' + p\n        return urllib.parse.quote(p, safe='/*')", "idx": 1449}
{"namespace": "proxybroker.utils.get_status_code", "completion": "    try:\n        status_code = int(resp[start:stop])\n    except ValueError:\n        status_code = 400\n    return status_code", "idx": 1450}
{"namespace": "authlib.oauth2.rfc6749.util.scope_to_list", "completion": "    if isinstance(scope, (set, tuple, list)):\n        return [to_unicode(s) for s in scope]\n    if scope is None:\n        return scope\n    return [to_unicode(s) for s in scope.split(\" \")]", "idx": 1451}
{"namespace": "authlib.common.encoding.to_unicode", "completion": "    if x is None or isinstance(x, str):\n        return x\n    if isinstance(x, bytes):\n        return x.decode(charset, errors)\n    return str(x)", "idx": 1452}
{"namespace": "authlib.common.encoding.to_bytes", "completion": "    if x is None:\n        return None\n    if isinstance(x, bytes):\n        return x\n    if isinstance(x, str):\n        return x.encode(charset, errors)\n    if isinstance(x, int):\n        return str(x).encode(charset, errors)\n    if isinstance(x, float):\n        return str(x).encode(charset, errors)\n    return str(x).encode(charset, errors)", "idx": 1453}
{"namespace": "authlib.common.encoding.urlsafe_b64decode", "completion": "    padding = b'=' * ((4 - len(s) % 4) % 4)\n    return base64.urlsafe_b64decode(s + padding)", "idx": 1454}
{"namespace": "csvs_to_sqlite.utils.table_exists", "completion": "    cursor = conn.cursor()\n    cursor.execute(\"\"\"\n        SELECT count(name) \n        FROM sqlite_master \n        WHERE type='table' AND name=?\n    \"\"\", (table,))\n    # if the count is 1, then table exists\n    return cursor.fetchone()[0] == 1", "idx": 1455}
{"namespace": "sqlitedict.SqliteDict.get_tablenames", "completion": "        if not os.path.isfile(filename):\n            raise IOError('file {} does not exist'.format(filename))\n\n        conn = sqlite3.connect(filename)\n        cursor = conn.cursor()\n\n        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n        table_names = cursor.fetchall()\n\n        conn.close()\n\n        return [name[0] for name in table_names]", "idx": 1456}
{"namespace": "litecli.packages.parseutils.query_starts_with", "completion": "    # First, lower the case of both the query and prefixes for case-insensitive comparison\n    query = query.lower()\n    prefixes = [prefix.lower() for prefix in prefixes]\n    \n    # Comments in SQL can obscure the actual start of the query, remove them using sqlparse\n    # Token.Comment was not directly shown in the context but is used for proper cleaning of the query\n    cleaned_query = \"\".join(token.value for token in sqlparse.parse(query)[0] if not token.ttype in sqlparse.tokens.Comment)\n\n    # Check if the cleaned query is not empty\n    if not cleaned_query:\n        return False\n\n    # Extract the first word from the cleaned query. Assuming SQL queries are space delimited.\n    first_word = cleaned_query.split()[0]\n\n    # Check if the first word is in the list of prefixes\n    return first_word in prefixes", "idx": 1457}
{"namespace": "rest_framework.negotiation.DefaultContentNegotiation.filter_renderers", "completion": "        filtered_renderers = [renderer for renderer in renderers if renderer.format == format]\n        if not filtered_renderers:\n            raise Http404\n        return filtered_renderers", "idx": 1458}
{"namespace": "rest_framework.templatetags.rest_framework.as_string", "completion": "    if value is None:\n        return ''\n    else:\n        return str(value)", "idx": 1459}
{"namespace": "rest_framework.templatetags.rest_framework.add_nested_class", "completion": "    if isinstance(value, dict):\n        return 'class=nested'\n    elif isinstance(value, list):\n        for item in value:\n            if isinstance(item, (dict, list)):\n                return 'class=nested'\n    return ''", "idx": 1460}
{"namespace": "pyramid.session.PickleSerializer.loads", "completion": "        try:\n            return pickle.loads(bstruct)\n        except Exception:\n            raise ValueError(\"Failed to deserialize byte stream\")", "idx": 1461}
{"namespace": "pyramid.testing.DummySession.flash", "completion": "        storage = self.setdefault('_f_' + queue, [])\n        if allow_duplicate or (msg not in storage):\n            storage.append(msg)", "idx": 1462}
{"namespace": "pyramid.testing.DummySession.pop_flash", "completion": "        storage = self.get('_f_' + queue, [])\n        if '_f_' + queue in self:\n            del self['_f_' + queue]\n        return storage", "idx": 1463}
{"namespace": "pyramid.testing.DummySession.peek_flash", "completion": "        storage = self.get('_f_' + queue, [])\n        return storage", "idx": 1464}
{"namespace": "pyramid.testing.DummySession.new_csrf_token", "completion": "        token = '0123456789012345678901234567890123456789'\n        self['_csrft_'] = token\n        return token", "idx": 1465}
{"namespace": "pyramid.view.view_defaults", "completion": "    def wrapper(cls):\n        cls.__view_defaults__ = settings\n        return cls\n    return wrapper", "idx": 1466}
{"namespace": "pyramid.util.bytes_", "completion": "    if isinstance(s, str):\n        return s.encode(encoding, errors)\n    return s", "idx": 1467}
{"namespace": "pyramid.scripts.common.parse_vars", "completion": "    vars_dict = {}\n    for arg in args:\n        key, value = arg.split('=')\n        vars_dict[key] = value\n    return vars_dict", "idx": 1468}
{"namespace": "pyramid.scripts.pviews.PViewsCommand._find_multi_routes", "completion": "        infos = []\n        for route in mapper.get_routes():\n            match = route.match(request.path_info)\n            if match is not None:\n                infos.append({'match': match, 'route': route})\n        return infos", "idx": 1469}
{"namespace": "pyramid.scripts.pserve.PServeCommand.guess_server_url", "completion": "        if not server_name:\n            server_name = 'main'\n        server_settings = loader.get_settings('server:' + server_name, global_conf)\n        if 'port' in server_settings:\n            return 'http://127.0.0.1:{port}'.format(port=server_settings['port'])\n        return None", "idx": 1470}
{"namespace": "aiohappybase._util.pep8_to_camel_case", "completion": "    components = name.split('_')\n    if initial:\n        return ''.join(x.title() for x in components)\n    else:\n        return components[0].lower() + ''.join(x.title() for x in components[1:])", "idx": 1471}
{"namespace": "aiohappybase._util.bytes_increment", "completion": "\n    for i in range(len(b) - 1, -1, -1):\n        if b[i] < 0xFF:\n            return b[:i] + bytes([b[i] + 1])\n    return None", "idx": 1472}
{"namespace": "mssqlcli.config.ensure_dir_exists", "completion": "    dir_path = dirname(path)\n    if not exists(dir_path):\n        os.makedirs(dir_path)", "idx": 1473}
{"namespace": "mssqlcli.telemetry._user_id_file_is_old", "completion": "    # Get the time that the file was last modified.\n    file_time = os.path.getmtime(id_file_path)\n    # Get the current time.\n    current_time = datetime.now().timestamp()\n    # Calculate the difference between the current time and the file time.\n    time_difference = current_time - file_time\n    # If the time difference is greater than 24 hours (in seconds), return True.\n    if time_difference > 24 * 60 * 60:\n        return True\n    else:\n        return False", "idx": 1474}
{"namespace": "mssqlcli.util.is_command_valid", "completion": "    if not command:\n        return False\n    try:\n        with open(devnull, 'w') as DEVNULL:\n            subprocess.check_call(command, stdout=DEVNULL, stderr=DEVNULL)\n    except OSError:\n        return False\n    return True", "idx": 1475}
{"namespace": "mssqlcli.packages.parseutils.utils.find_prev_keyword", "completion": "\n    parsed = sqlparse.parse(sql)[0]\n    tokens = list(reversed([t for t in parsed.tokens if not t.is_whitespace]))\n\n    # Skip tokens\n    for _ in range(n_skip):\n        if tokens:\n            tokens.pop(0)\n\n    # Find the last keyword\n    for token in tokens:\n        if token.ttype is Token.Keyword:\n            return token.value.upper(), sql[:token.start]\n    return None, sql", "idx": 1476}
{"namespace": "trafilatura.settings.use_config", "completion": "    if config is not None:\n        return config\n    else:\n        config = ConfigParser()\n        if filename is None:\n            filename = Path(__file__).parent / \"settings.cfg\"\n        config.read(filename)\n        return config", "idx": 1477}
{"namespace": "pyramid.util.text_", "completion": "    if isinstance(s, bytes):\n        return s.decode(encoding, errors)\n    return s", "idx": 1478}
{"namespace": "trafilatura.downloads._parse_config", "completion": "    try:\n        myagents = config.get('DEFAULT', 'USER_AGENTS').split('\\n')\n    except (configparser.NoOptionError, configparser.NoSectionError):\n        myagents = None\n    try:\n        mycookie = config.get('DEFAULT', 'COOKIE')\n    except (configparser.NoOptionError, configparser.NoSectionError):\n        mycookie = None\n    return myagents, mycookie", "idx": 1479}
{"namespace": "trafilatura.downloads.load_download_buffer", "completion": "    bufferlist = []\n    while True:\n        # draw URLs respecting domain-based back-off rules\n        url = url_store.draw_url()\n        if url is not None:\n            bufferlist.append(url)\n        # if the bufferlist is empty, sleep\n        elif not bufferlist:\n            sleep(sleep_time)\n        else:\n            break\n    return bufferlist, url_store", "idx": 1480}
{"namespace": "trafilatura.metadata.check_authors", "completion": "    author_blacklist = set([author.lower() for author in author_blacklist])\n    authors_list = authors.split(';')\n    new_authors = [author for author in authors_list if author.lower() not in author_blacklist]\n    return '; '.join(new_authors) if new_authors else None", "idx": 1481}
{"namespace": "datasette.filters.where_filters", "completion": "    async def inner():\n        where_clauses = []\n        extra_wheres_for_ui = []\n        if \"_where\" in request.args:\n            if not datasette.can_execute_sql(database, request.actor):\n                raise BadRequest(\"You do not have permission to execute SQL\")\n            where_clauses.extend(request.args.getlist(\"_where\"))\n            for where in where_clauses:\n                extra_wheres_for_ui.append({\"sql\": where, \"label\": where})\n        return FilterArguments(where_clauses, extra_context={\"extra_wheres_for_ui\": extra_wheres_for_ui})\n\n    return inner", "idx": 1482}
{"namespace": "datasette.utils.path_with_added_args", "completion": "    if path is None:\n        path = request.path\n    else:\n        if \"?\" in path:\n            bits = path.split(\"?\", 1)\n            path, query_string = bits\n    current = []\n    for key, value in urllib.parse.parse_qsl(request.query_string):\n        current.append((key, value))\n    for key, value in args.items():\n        current.append((key, value))\n    query_string = urllib.parse.urlencode(current)\n    if query_string:\n        query_string = f\"?{query_string}\"\n    return path + query_string", "idx": 1483}
{"namespace": "datasette.utils.path_with_replaced_args", "completion": "    path = path or request.path\n    if isinstance(args, dict):\n        args = args.items()\n    args_to_replace = {k for k, v in args}\n    current = []\n    for key, value in urllib.parse.parse_qsl(request.query_string):\n        if key not in args_to_replace:\n            current.append((key, value))\n    current.extend([(key, value) for key, value in args])\n    query_string = urllib.parse.urlencode(current)\n    if query_string:\n        query_string = f\"?{query_string}\"\n    return path + query_string", "idx": 1484}
{"namespace": "datasette.utils.format_bytes", "completion": "    units = ['bytes', 'KB', 'MB', 'GB', 'TB']\n    index = 0\n    while bytes >= 1024 and index < len(units) - 1:\n        bytes /= 1024\n        index += 1\n    return f\"{bytes:.2f} {units[index]}\"", "idx": 1485}
{"namespace": "datasette.utils.actor_matches_allow", "completion": "    if not allow:\n        return False\n    if allow == \"*\":\n        return True\n    if isinstance(allow, str):\n        return actor == allow\n    if isinstance(allow, list):\n        return actor in allow\n    if isinstance(allow, dict):\n        for key, value in allow.items():\n            if key not in actor or actor[key] != value:\n                return False\n        return True\n    return False", "idx": 1486}
{"namespace": "datasette.utils.resolve_env_secrets", "completion": "    if isinstance(config, dict):\n        return {\n            key: resolve_env_secrets(value, environ)\n            for key, value in config.items()\n        }\n    elif isinstance(config, list):\n        return [resolve_env_secrets(item, environ) for item in config]\n    elif isinstance(config, str):\n        return config\n    elif isinstance(config, dict) and len(config) == 1:\n        if \"$env\" in config:\n            return environ.get(config[\"$env\"])\n        elif \"$file\" in config:\n            with open(config[\"$file\"], \"r\") as file:\n                return file.read().strip()\n    return config", "idx": 1487}
{"namespace": "datasette.utils.display_actor", "completion": "    if actor is None:\n        return None\n    if \"display\" in actor:\n        return actor[\"display\"]\n    if \"name\" in actor:\n        return actor[\"name\"]\n    if \"username\" in actor:\n        return actor[\"username\"]\n    if \"login\" in actor:\n        return actor[\"login\"]\n    if \"id\" in actor:\n        return actor[\"id\"]\n    return str(actor)", "idx": 1488}
{"namespace": "datasette.utils.initial_path_for_datasette", "completion": "    databases = await datasette.databases()\n    if len(databases) == 1:\n        db_name = list(databases.keys())[0]\n        db = databases[db_name]\n        table_names = await db.table_names()\n        if len(table_names) == 1:\n            return f'/{db_name}/{table_names[0]}'\n        else:\n            return f'/{db_name}'\n    else:\n        return '/'", "idx": 1489}
{"namespace": "datasette.utils.tilde_decode", "completion": "    # Replace % with a temporary string\n    s = s.replace(\"%\", \"___PERCENT___\")\n    \n    # Decode the tilde-encoded string\n    s = re.sub(r\"~([0-9A-Fa-f]{2})\", lambda m: chr(int(m.group(1), 16)), s)\n    \n    # Replace the temporary string with %\n    s = s.replace(\"___PERCENT___\", \"%\")\n    \n    return s", "idx": 1490}
{"namespace": "datasette.utils.resolve_routes", "completion": "    for route in routes:\n        match = route[0].match(path)\n        if match:\n            return match, route[1]\n    return None, None", "idx": 1491}
{"namespace": "datasette.utils.truncate_url", "completion": "    if len(url) <= length:\n        return url\n\n    # Check if the URL ends with a file extension\n    url_parts = url.split('.')\n    if len(url_parts) > 1 and 1 <= len(url_parts[-1]) <= 4 and '/' not in url_parts[-1]:\n        # Truncate the URL, add ellipsis and the extension\n        return url[:length - 3 - len(url_parts[-1])] + '...' + url_parts[-1]\n    else:\n        # Truncate the URL and add ellipsis\n        return url[:length - 3] + '...'", "idx": 1492}
{"namespace": "kinto.core.authorization.groupfinder", "completion": "    principals = []\n    if hasattr(request.registry, 'permission'):\n        permission = request.registry.permission\n        if userid:\n            principals = permission.get_user_principals(userid)\n    return principals", "idx": 1493}
{"namespace": "kinto.core.utils.json.dumps", "completion": "        kw.setdefault(\"bytes_mode\", rapidjson.BM_NONE)\n        return rapidjson.dumps(v, **kw)", "idx": 1494}
{"namespace": "kinto.core.utils.json.loads", "completion": "        kw.setdefault(\"number_mode\", rapidjson.NM_NATIVE)\n        return rapidjson.loads(v, **kw)", "idx": 1495}
{"namespace": "kinto.core.utils.hmac_digest", "completion": "    if isinstance(secret, str):\n        secret = secret.encode(encoding)\n    if isinstance(message, str):\n        message = message.encode(encoding)\n    return hmac.new(secret, message, hashlib.sha256).hexdigest()", "idx": 1496}
{"namespace": "kinto.core.utils.current_service", "completion": "    # Get the route that matches the request\n    route = request.matched_route\n\n    # If no route matches the request, return None\n    if route is None:\n        return None\n\n    # Get the Cornice services from the request's registry\n    services = request.registry.cornice_services\n\n    # Return the service that matches the route pattern\n    return services.get(route.pattern)", "idx": 1497}
{"namespace": "kinto.core.utils.prefixed_principals", "completion": "    principals = request.effective_principals\n    if Authenticated not in principals:\n        return principals\n\n    userid = request.unauthenticated_userid\n    if userid in principals:\n        principals.remove(userid)\n\n    prefixed_userid = prefixed_userid(request)\n    if prefixed_userid is not None:\n        principals.insert(0, prefixed_userid)\n\n    return principals", "idx": 1498}
{"namespace": "kinto.plugins.accounts.views.on_account_created", "completion": "    request = event.request\n    settings = request.registry.settings\n    validation_enabled = settings.get(\"account_validation\", False)\n\n    if validation_enabled:\n        for obj in event.impacted_objects:\n            account = obj[\"new\"]\n            user_email = account[\"id\"]\n            activation_key = get_cached_validation_key(user_email, request.registry)\n\n            if activation_key is None:\n                continue\n\n            emailer = Emailer(request)\n            emailer.send_activation(user_email, activation_key)", "idx": 1499}
{"namespace": "kinto.plugins.accounts.utils.hash_password", "completion": "    hashed_password = bcrypt.hashpw(password.encode('utf-8'), bcrypt.gensalt())\n    return hashed_password.decode('utf-8')", "idx": 1500}
{"namespace": "kinto.views.admin.get_parent_uri", "completion": "    path = object_uri.split('/')\n    if len(path) < 3:\n        return ''\n    else:\n        return '/'.join(path[:-1])", "idx": 1501}
{"namespace": "alembic.script.write_hooks.register", "completion": "\n    def decorator(func: Callable) -> Callable:\n        _registry[name] = func\n        return func\n\n    return decorator", "idx": 1502}
{"namespace": "mongo_connector.namespace_config.match_replace_regex", "completion": "    match = regex.match(src_namespace)\n    if match:\n        groups = match.groups()\n        new_namespace = dest_namespace\n        for i, group in enumerate(groups, start=1):\n            new_namespace = new_namespace.replace(\"*\", group, 1)\n        return new_namespace\n    return None", "idx": 1503}
{"namespace": "mongo_connector.namespace_config.namespace_to_regex", "completion": "    db, coll = namespace.split(\".\", 1)\n    regex = \"^%s\\..*$\" % (re.escape(db).replace(\"\\\\*\", \"([^.]*)\"),)\n    if \"*\" in coll:\n        regex = \"^%s\\..*$\" % (re.escape(db).replace(\"\\\\*\", \"([^.]*)\"),)\n        regex = regex.replace(\".\", \"\\\\.\")\n        regex = regex.replace(\"*\", \"(.*)\")\n    return re.compile(regex)", "idx": 1504}
{"namespace": "mongo_connector.util.long_to_bson_ts", "completion": "    time = val >> 32\n    inc = val & 0xFFFFFFFF\n    return Timestamp(time, inc)", "idx": 1505}
{"namespace": "mongo_connector.doc_managers.formatters.DocumentFlattener.format_document", "completion": "        def _kernel(doc):\n            for key in doc:\n                value = doc[key]\n                for new_k, new_v in self.transform_element(key, value):\n                    yield new_k, new_v\n\n        return dict(_kernel(document))", "idx": 1506}
{"namespace": "bplustree.memory.open_file_in_dir", "completion": "    dir_path = os.path.dirname(path)\n    dir_fd = None\n    if platform.system() != 'Windows':\n        dir_fd = os.open(dir_path, os.O_DIRECTORY)\n    file_fd = open(path, 'ab+')\n    return file_fd, dir_fd", "idx": 1507}
{"namespace": "bplustree.memory.FileMemory.read_transaction", "completion": "        class ReadTransaction:\n\n            def __enter__(self2):\n                self._lock.reader_lock.acquire()\n\n            def __exit__(self2, exc_type, exc_val, exc_tb):\n                self._lock.reader_lock.release()\n\n        return ReadTransaction()", "idx": 1508}
{"namespace": "bplustree.utils.pairwise", "completion": "    a, b = itertools.tee(iterable)\n    next(b, None)\n    return zip(a, b)", "idx": 1509}
{"namespace": "bplustree.utils.iter_slice", "completion": "    it = iter(iterable)\n    while True:\n        slice = bytes(itertools.islice(it, n))\n        if slice:\n            yield slice, not bool(bytes(itertools.islice(it, 1)))\n        else:\n            break", "idx": 1510}
{"namespace": "bplustree.serializer.StrSerializer.serialize", "completion": "        serialized_obj = obj.encode(encoding='utf-8')\n        assert len(serialized_obj) <= key_size, \"Serialized object is larger than the specified key size\"\n        return serialized_obj", "idx": 1511}
{"namespace": "psd_tools.utils.pack", "completion": "    fmt = str(\">\" + fmt)\n    return struct.pack(fmt, *args)", "idx": 1512}
{"namespace": "psd_tools.utils.unpack", "completion": "    fmt = str(\">\" + fmt)\n    return struct.unpack(fmt, data)", "idx": 1513}
{"namespace": "psd_tools.api.numpy_io.get_pattern", "completion": "    height, width = pattern.data.rectangle[2:4]\n    channels = [\n        _parse_array(channel.data, pattern.data.depth)\n        for channel in pattern.data.channels\n    ]\n    if len(channels) and channels[0].size > 0:\n        result = np.stack(channels, axis=1).reshape((height, width, -1))\n        expected_channels = EXPECTED_CHANNELS.get(pattern.data.color_mode)\n        if result.shape[2] > expected_channels:\n            logger.debug('Extra channel found')\n            return result[:, :, :expected_channels]\n        return result\n    return None", "idx": 1514}
{"namespace": "sqlite_utils.utils.maximize_csv_field_size_limit", "completion": "    maxInt = sys.maxsize\n    decrement = True\n\n    while decrement:\n        # decrease the maxInt value by factor 10 \n        # as long as the OverflowError occurs.\n        decrement = False\n        try:\n            csv.field_size_limit(maxInt)\n        except OverflowError:\n            maxInt = int(maxInt/10)\n            decrement = True", "idx": 1515}
{"namespace": "sqlite_utils.utils.column_affinity", "completion": "    column_type = column_type.upper()\n    if \"INT\" in column_type:\n        return \"INTEGER\"\n    elif \"CHAR\" in column_type or \"CLOB\" in column_type or \"TEXT\" in column_type:\n        return \"TEXT\"\n    elif \"BLOB\" in column_type:\n        return \"NONE\"\n    elif \"REAL\" in column_type or \"FLOA\" in column_type or \"DOUB\" in column_type:\n        return \"REAL\"\n    else:\n        return \"NUMERIC\"", "idx": 1516}
{"namespace": "sqlite_utils.utils.decode_base64_values", "completion": "    for key, value in doc.items():\n        if isinstance(value, dict) and value.get(\"$base64\", False) and \"encoded\" in value:\n            doc[key] = base64.b64decode(value[\"encoded\"]).decode()\n        elif isinstance(value, dict):\n            decode_base64_values(value)\n        elif isinstance(value, list):\n            for item in value:\n                if isinstance(item, dict):\n                    decode_base64_values(item)\n    return doc", "idx": 1517}
{"namespace": "sqlite_utils.utils.chunks", "completion": "    iterator = iter(sequence)\n    while True:\n        chunk = list(itertools.islice(iterator, size))\n        if not chunk:\n            return\n        yield chunk", "idx": 1518}
{"namespace": "sqlite_utils.utils.hash_record", "completion": "    if keys is None:\n        keys = record.keys()\n    hash_obj = hashlib.sha1()\n    for key in sorted(keys):\n        if key in record:\n            hash_obj.update(str(key).encode('utf-8'))\n            hash_obj.update(str(record[key]).encode('utf-8'))\n    return hash_obj.hexdigest()", "idx": 1519}
{"namespace": "arctic.decorators._get_host", "completion": "    if not store:\n        return {}\n    if isinstance(store, (list, tuple)):\n        store = store[0]\n    library_name = store._arctic_lib.get_name() if hasattr(store, '_arctic_lib') else None\n    nodes = store._arctic_lib.arctic._conn.nodes if hasattr(store, '_arctic_lib') else None\n    nodes = ', '.join(['%s:%d' % n for n in nodes]) if nodes else None\n    host = store._arctic_lib.arctic.mongo_host if hasattr(store, '_arctic_lib') else None\n    return {'library': library_name, 'nodes': nodes, 'host': host}", "idx": 1520}
{"namespace": "arctic.decorators.mongo_retry", "completion": "    @wraps(f)\n    def wrapper(*args, **kwargs):\n        global _in_retry, _retry_count\n        if _in_retry:\n            return f(*args, **kwargs)\n        _in_retry = True\n        _retry_count = 0\n        while True:\n            try:\n                result = f(*args, **kwargs)\n                _in_retry = False\n                return result\n            except (AutoReconnect, OperationFailure, DuplicateKeyError, ServerSelectionTimeoutError, BulkWriteError) as e:\n                _retry_count += 1\n                _handle_error(f, e, _retry_count, **kwargs)\n            except Exception as e:\n                _in_retry = False\n                if 'arctic' in f.__module__:\n                    logger.exception(e)\n                raise\n    return wrapper", "idx": 1521}
{"namespace": "arctic._util.are_equals", "completion": "    try:\n        if isinstance(o1, DataFrame) and isinstance(o2, DataFrame):\n            assert_frame_equal(o1, o2, **kwargs)\n        else:\n            assert o1 == o2\n        return True\n    except Exception as e:\n        logger.error(f\"Error comparing objects: {e}\")\n        return False", "idx": 1522}
{"namespace": "arctic.hooks.register_resolve_mongodb_hook", "completion": "    global _resolve_mongodb_hook\n    _resolve_mongodb_hook = hook", "idx": 1523}
{"namespace": "arctic.hooks.register_log_exception_hook", "completion": "    global _log_exception_hook\n    _log_exception_hook = hook", "idx": 1524}
{"namespace": "arctic.hooks.register_get_auth_hook", "completion": "    global _get_auth_hook\n    _get_auth_hook = hook", "idx": 1525}
{"namespace": "arctic.store._version_store_utils._split_arrs", "completion": "    # Initialize the list to store the sub-arrays\n    sub_arrays = []\n    \n    # Initialize the start index\n    start = 0\n    \n    # Loop through the slices\n    for end in slices:\n        # Slice the array and append it to the list\n        sub_arrays.append(array_2d[start:end])\n        # Update the start index\n        start = end\n    \n    # Append the last sub-array\n    sub_arrays.append(array_2d[start:])\n    \n    # Convert the list to a numpy array and return\n    return np.array(sub_arrays)", "idx": 1526}
{"namespace": "arctic.store._version_store_utils.checksum", "completion": "    sha = hashlib.sha1()\n    sha.update(pickle.dumps(doc, protocol=2))\n    sha.update(symbol.encode('ascii'))\n    return Binary(sha.digest())", "idx": 1527}
{"namespace": "arctic.store.versioned_item.VersionedItem.__str__", "completion": "        return f\"VersionedItem(symbol={self.symbol},library={self.library},data={self.data},version={self.version},metadata={self.metadata},host={self.host})\"", "idx": 1528}
{"namespace": "arctic.store._ndarray_store.NdarrayStore._dtype", "completion": "        if metadata is None:\n            metadata = {}\n        return np.dtype((string, metadata))", "idx": 1529}
{"namespace": "arctic.store._ndarray_store._promote_struct_dtypes", "completion": "    if set(dtype1.names) < set(dtype2.names):\n        raise ValueError(\"dtype1 must be a superset of dtype2\")\n\n    new_dtype = []\n    for name in dtype1.names:\n        if name in dtype2.names:\n            new_dtype.append((name, np.promote_types(dtype1.fields[name][0], dtype2.fields[name][0])))\n        else:\n            new_dtype.append((name, dtype1.fields[name][0]))\n\n    return np.dtype(new_dtype)", "idx": 1530}
{"namespace": "arctic.chunkstore.passthrough_chunker.PassthroughChunker.exclude", "completion": "        if isinstance(data, DataFrame):\n            return DataFrame()\n        elif isinstance(data, Series):\n            return Series()\n        else:\n            raise TypeError(\"Data should be a DataFrame or Series.\")", "idx": 1531}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.to_chunks", "completion": "        if 'date' not in df.index.names:\n            raise ValueError(\"Dataframe must be indexed by 'date'\")\n\n        df = df.sort_index()\n        start_date = df.index[0]\n        end_date = df.index[-1]\n\n        while start_date <= end_date:\n            chunk_end_date = start_date + pd.DateOffset(**{chunk_size: 1}) - pd.DateOffset(days=1)\n            chunk = df.loc[start_date:chunk_end_date]\n\n            if func is not None:\n                chunk = func(chunk, **kwargs)\n\n            yield start_date, chunk_end_date, chunk_size, chunk\n\n            start_date = chunk_end_date + pd.DateOffset(days=1)", "idx": 1532}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.exclude", "completion": "        if isinstance(range_obj, (pd.DatetimeIndex, tuple)):\n            range_obj = DateRange(range_obj[0], range_obj[-1])\n\n        range_obj = to_pandas_closed_closed(range_obj, add_tz=False)\n        start = range_obj.start\n        end = range_obj.end\n\n        if 'date' in data.index.names:\n            return data[(data.index.get_level_values('date') < start) | (data.index.get_level_values('date') > end)]\n        elif 'date' in data.columns:\n            if start and end:\n                return data[(data.date < start) | (data.date > end)]\n            elif start:\n                return data[(data.date < start)]\n            elif end:\n                return data[(data.date > end)]\n            else:\n                return data\n        else:\n            return data", "idx": 1533}
{"namespace": "mopidy.httpclient.format_proxy", "completion": "    if not proxy_config or not proxy_config.get('hostname'):\n        return None\n\n    scheme = proxy_config.get('scheme', 'http')\n    host = proxy_config['hostname']\n    port = proxy_config.get('port', '')\n\n    if auth and proxy_config.get('username') and proxy_config.get('password'):\n        user = proxy_config['username']\n        password = proxy_config['password']\n        return f\"{scheme}://{user}:{password}@{host}:{port}\"\n\n    return f\"{scheme}://{host}:{port}\"", "idx": 1534}
{"namespace": "arctic.chunkstore.date_chunker.DateChunker.filter", "completion": "        if isinstance(range_obj, (pd.DatetimeIndex, tuple)):\n            range_obj = DateRange(range_obj[0], range_obj[-1])\n        if 'date' in data.index.names:\n            return data[(data.index.get_level_values('date') >= range_obj.start) & (data.index.get_level_values('date') <= range_obj.end)]\n        elif 'date' in data.columns:\n            return data[(data.date >= range_obj.start) & (data.date <= range_obj.end)]\n        else:\n            return data", "idx": 1535}
{"namespace": "mopidy.config.validators.validate_required", "completion": "    if required and not value:\n        raise ValueError(\"This value is required but not set.\")", "idx": 1536}
{"namespace": "mopidy.config.validators.validate_choice", "completion": "    if value not in choices:\n        names = ', '.join(str(choice) for choice in choices)\n        raise ValueError(f\"must be one of {names}, not {value}.\")", "idx": 1537}
{"namespace": "mopidy.config.validators.validate_minimum", "completion": "    if minimum is not None and value < minimum:\n        raise ValueError(f\"{value!r} must be larger than {minimum!r}.\")", "idx": 1538}
{"namespace": "mopidy.config.validators.validate_maximum", "completion": "    if maximum is not None and value > maximum:\n        raise ValueError(f\"{value!r} must be smaller than {maximum!r}.\")", "idx": 1539}
{"namespace": "mopidy.config.schemas._did_you_mean", "completion": "    if not choices:\n        return None\n\n    distances = {choice: _levenshtein(name, choice) for choice in choices}\n    sorted_distances = sorted(distances.items(), key=lambda x: x[1])\n\n    if sorted_distances[0][1] <= 3:\n        return sorted_distances[0][0]\n    else:\n        return None", "idx": 1540}
{"namespace": "mopidy.config.types.encode", "completion": "    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char, replacement in [(\"\\\\\", \"\\\\\\\\n\"), (\"\\n\", \"\\\\n\"), (\"\\t\", \"\\\\t\")]:\n        value = value.replace(char, replacement)\n\n    return value", "idx": 1541}
{"namespace": "mopidy.config.types.decode", "completion": "    if isinstance(value, bytes):\n        value = value.decode(errors=\"surrogateescape\")\n\n    for char in (\"\\\\\\\\\", \"\\\\n\", \"\\\\t\"):\n        value = value.replace(\n            char, char.encode(encoding=\"unicode-escape\").decode()\n        )\n\n    return value", "idx": 1542}
{"namespace": "mopidy.config.types.ConfigValue.serialize", "completion": "        if value is None:\n            return \"\"\n        else:\n            return str(value)", "idx": 1543}
{"namespace": "mopidy.config.types.Boolean.serialize", "completion": "        if value is True:\n            return \"true\"\n        elif value is False or value is None:\n            return \"false\"\n        else:\n            raise ValueError(f\"{value!r} is not a boolean\")", "idx": 1544}
{"namespace": "hypertools.tools.df2mat.df2mat", "completion": "    data_dummies = pd.get_dummies(data)\n    if return_labels:\n        return data_dummies.values, data_dummies.columns.tolist()\n    else:\n        return data_dummies.values", "idx": 1545}
{"namespace": "hypertools._shared.helpers.center", "completion": "    assert type(x) is list, \"Input data to center must be list\"\n    mean_x = np.mean(x)\n    return [i - mean_x for i in x]", "idx": 1546}
{"namespace": "hypertools._shared.helpers.group_by_category", "completion": "    # flatten if list of lists\n    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n\n    # create a sorted set of unique values\n    categories = list(sorted(set(vals), key=list(vals).index))\n\n    # return the index of each value in the sorted set\n    return [categories.index(val) for val in vals]", "idx": 1547}
{"namespace": "hypertools._shared.helpers.vals2colors", "completion": "    # flatten if list of lists\n    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n    # get color palette\n    palette = sns.color_palette(cmap, res)\n    # map values to colors\n    colors = [palette[int(val)] for val in np.digitize(vals, np.linspace(np.min(vals), np.max(vals)+1, res+1)) - 1]\n    return colors", "idx": 1548}
{"namespace": "hypertools._shared.helpers.vals2bins", "completion": "    # flatten if list of lists\n    if any(isinstance(el, list) for el in vals):\n        vals = list(itertools.chain(*vals))\n\n    # map values to bins\n    bins = np.digitize(vals, np.linspace(np.min(vals), np.max(vals)+1, res+1)) - 1\n    return bins", "idx": 1549}
{"namespace": "hypertools._shared.helpers.interp_array", "completion": "    # Check if the array is 1D or 2D\n    if len(arr.shape) == 1:\n        # If 1D, create an interpolator object\n        interpolator = pchip(np.arange(len(arr)), arr)\n        # Generate new x values\n        x_new = np.linspace(0, len(arr)-1, len(arr)*interp_val)\n        # Interpolate y values\n        y_new = interpolator(x_new)\n        return y_new\n    elif len(arr.shape) == 2:\n        # If 2D, create an interpolator object for each column\n        interpolators = [pchip(np.arange(len(arr[:,i])), arr[:,i]) for i in range(arr.shape[1])]\n        # Generate new x values\n        x_new = np.linspace(0, len(arr)-1, len(arr)*interp_val)\n        # Interpolate y values for each column\n        y_new = np.array([interpolator(x_new) for interpolator in interpolators]).T\n        return y_new\n    else:\n        raise ValueError(\"Array must be 1D or 2D.\")", "idx": 1550}
{"namespace": "hypertools._shared.helpers.parse_args", "completion": "    args_list = []\n    for i, item in enumerate(x):\n        tmp = []\n        for arg in args:\n            if isinstance(arg, (tuple, list)):\n                if len(arg) == len(x):\n                    tmp.append(arg[i])\n                else:\n                    print(\"Error: The length of the argument list does not match the length of the input list.\")\n                    sys.exit(1)\n            else:\n                tmp.append(arg)\n        args_list.append(tuple(tmp))\n    return args_list", "idx": 1551}
{"namespace": "hypertools._shared.helpers.parse_kwargs", "completion": "    kwargs_list = []\n    for i, item in enumerate(x):\n        tmp = {}\n        for key, value in kwargs.items():\n            if isinstance(value, (tuple, list)):\n                if len(value) == len(x):\n                    tmp[key] = value[i]\n                else:\n                    print('Error: arguments must be a list of the same length as x')\n                    sys.exit(1)\n            else:\n                tmp[key] = value\n        kwargs_list.append(tmp)\n    return kwargs_list", "idx": 1552}
{"namespace": "gif_for_cli.utils._get_default_display_mode", "completion": "    term = environ.get('TERM', '')\n    colorterm = environ.get('COLORTERM', '')\n    if 'truecolor' in term or 'truecolor' in colorterm:\n        return 'truecolor'\n    elif '256' in term or '256' in colorterm:\n        return '256fgbg'\n    else:\n        return 'nocolor'", "idx": 1553}
{"namespace": "gif_for_cli.utils._pool_type", "completion": "    val = int(val)\n    if val <= 0:\n        raise argparse.ArgumentTypeError(\"Pool size must be greater than 0\")\n    return val", "idx": 1554}
{"namespace": "gif_for_cli.generate.utils.get_avg_for_em", "completion": "\n    # Initialize the sum of red, green, and blue values\n    r_sum, g_sum, b_sum = 0, 0, 0\n\n    # Calculate the total number of pixels in the area\n    total_pixels = cell_height * cell_width\n\n    # Iterate over the pixels in the area\n    for i in range(x, x + cell_width):\n        for j in range(y, y + cell_height):\n            # Get the red, green, and blue values of the pixel\n            r, g, b = px[i, j]\n            # Add the values to the sums\n            r_sum += r\n            g_sum += g\n            b_sum += b\n\n    # Calculate the average red, green, and blue values\n    r_avg = r_sum / total_pixels\n    g_avg = g_sum / total_pixels\n    b_avg = b_sum / total_pixels\n\n    # Return the average values as a list\n    return [r_avg, g_avg, b_avg]", "idx": 1555}
{"namespace": "gif_for_cli.generate.utils.process_input_source", "completion": "    if input_source.startswith(\"https://tenor.com/view/\"):\n        gif_id = input_source.split(\"/\")[-1]\n        gif_url = f\"https://api.tenor.com/v1/gifs/{gif_id}?key={api_key}\"\n    else:\n        response = requests.get(f\"https://api.tenor.com/v1/search?q={input_source}&key={api_key}&limit=1\")\n        if response.status_code == 200:\n            gif_url = response.json()[\"results\"][0][\"media\"][0][\"gif\"][\"url\"]\n        else:\n            raise Exception(\"Unable to fetch GIF from Tenor API\")\n    return gif_url", "idx": 1556}
{"namespace": "hypertools._shared.helpers.reshape_data", "completion": "    # Check if hue is not None\n    if hue is not None:\n        # Get unique categories in hue\n        categories = np.unique(hue)\n        # Initialize lists to store reshaped data and labels\n        reshaped_data = []\n        reshaped_labels = []\n        # Loop over each category\n        for category in categories:\n            # Get indices where hue is equal to the current category\n            indices = np.where(hue == category)[0]\n            # Append the corresponding data to reshaped_data\n            reshaped_data.append(x[indices])\n            # If labels is not None, append the corresponding labels to reshaped_labels\n            if labels is not None:\n                reshaped_labels.append(labels[indices])\n        # If labels is not None, return reshaped_data and reshaped_labels\n        if labels is not None:\n            return reshaped_data, reshaped_labels\n        # If labels is None, return only reshaped_data\n        else:\n            return reshaped_data\n    # If hue is None, return x and labels as they are\n    else:\n        return x, labels", "idx": 1557}
{"namespace": "mingus.extra.lilypond.from_Note", "completion": "    # Throw exception\n    if note is not None and not isinstance(note, Note):\n        return False\n\n    # Process the note\n    result = note.name.lower()\n    if note.accidental == \"#\":\n        result += \"is\"\n    elif note.accidental == \"b\":\n        result += \"es\"\n\n    # Process the octaves\n    if process_octaves:\n        if note.octave < 4:\n            result += \",\" * (4 - note.octave)\n        elif note.octave > 4:\n            result += \"'\" * (note.octave - 4)\n\n    # Return the result\n    if not standalone:\n        return result\n    else:\n        return \"{ %s }\" % result", "idx": 1558}
{"namespace": "mingus.extra.tablature._get_qsize", "completion": "    # Calculate the quarter note size based on the tuning and width\n    qsize = width // (len(tuning.tuning) * 2)\n    return qsize", "idx": 1559}
{"namespace": "mingus.core.notes.augment", "completion": "    if note[-1] != \"b\":\n        return note + \"#\"\n    else:\n        return note[:-1]", "idx": 1560}
{"namespace": "mingus.core.meter.valid_beat_duration", "completion": "    import math\n    return math.log2(duration).is_integer()", "idx": 1561}
{"namespace": "mingus.core.notes.diminish", "completion": "    if note[-1] != \"#\":\n        return note + \"b\"\n    else:\n        return note[:-1]", "idx": 1562}
{"namespace": "mingus.core.intervals.invert", "completion": "    return interval[::-1]", "idx": 1563}
{"namespace": "mingus.core.progressions.parse_string", "completion": "    acc = 0\n    suffix = \"\"\n    roman_numeral = \"\"\n    for c in progression:\n        if c == \"b\":\n            acc -= 1\n        elif c == \"#\":\n            acc += 1\n        elif c in \"123456789\":\n            suffix = progression[progression.index(c):]\n            break\n        else:\n            roman_numeral += c\n    return (roman_numeral, acc, suffix)", "idx": 1564}
{"namespace": "exodus_bundler.bundling.bytes_to_int", "completion": "    return int.from_bytes(bytes, byteorder)", "idx": 1565}
{"namespace": "exodus_bundler.templating.render_template", "completion": "    for key, value in context.items():\n        placeholder = '{{' + key + '}}'\n        string = string.replace(placeholder, str(value))\n    return string", "idx": 1566}
{"namespace": "exodus_bundler.input_parsing.strip_pid_prefix", "completion": "    # Regular expression pattern for the prefix\n    pattern = r'\\[pid \\d+\\] '\n\n    # Use re.sub to replace the prefix with an empty string\n    line = re.sub(pattern, '', line)\n\n    return line", "idx": 1567}
{"namespace": "fs.path.abspath", "completion": "    return path if path.startswith(\"/\") else \"/\" + path", "idx": 1568}
{"namespace": "fs.path.combine", "completion": "    if path2.startswith(\"/\"):\n        raise ValueError(\"path2 must be a relative path\")\n    if \"..\" in path1.split(\"/\") or \"..\" in path2.split(\"/\"):\n        raise ValueError(\"paths must not contain back references\")\n    return path1.rstrip(\"/\") + \"/\" + path2.lstrip(\"/\")", "idx": 1569}
{"namespace": "fs.path.split", "completion": "    if not path:\n        return \"\", \"\"\n    if path == \"/\":\n        return \"/\", \"\"\n    parts = path.rstrip(\"/\").split(\"/\")\n    tail = parts.pop()\n    head = \"/\" + \"/\".join(parts)\n    return head, tail", "idx": 1570}
{"namespace": "fs.path.isparent", "completion": "    _path1 = forcedir(abspath(path1))\n    _path2 = abspath(path2)\n    return _path2.startswith(_path1)  # longer one is child", "idx": 1571}
{"namespace": "fs.path.forcedir", "completion": "    if not path.endswith(\"/\"):\n        return path + \"/\"\n    return path", "idx": 1572}
{"namespace": "fs.wildcard.match_any", "completion": "    if not patterns:\n        return True\n    return any(match(pattern, name) for pattern in patterns)", "idx": 1573}
{"namespace": "fs.wildcard.imatch_any", "completion": "    if not patterns:\n        return True\n    return any(imatch(pattern, name) for pattern in patterns)", "idx": 1574}
{"namespace": "wal_e.cmd.parse_boolean_envvar", "completion": "    if val is None:\n        return None\n    elif val.lower() in ['false', '0']:\n        return False\n    elif val.lower() in ['true', '1']:\n        return True\n    else:\n        raise ValueError('Invalid boolean environment variable value: {0}'.format(val))", "idx": 1575}
{"namespace": "wal_e.log_help.get_log_destinations", "completion": "    log_destinations = os.getenv('WALE_LOG_DESTINATION', 'stderr,syslog')\n    return log_destinations.split(',')", "idx": 1576}
{"namespace": "wal_e.log_help.WalELogger._fmt_structured", "completion": "        # Get current time in the specified format\n        time_str = datetime.datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%S.%f-00')\n\n        # Get current process id\n        pid_str = str(os.getpid())\n\n        # Create a list of formatted key-value pairs, sorted lexically\n        kv_pairs = sorted([f'{k}={v}' for k, v in d.items()])\n\n        # Combine time, pid and the key-value pairs into the final string\n        return ' '.join(['time=' + time_str, 'pid=' + pid_str] + kv_pairs)", "idx": 1577}
{"namespace": "wal_e.tar_partition._fsync_files", "completion": "    for filename in filenames:\n        with open(filename, 'r') as f:\n            os.fsync(f.fileno())\n        dir_fd = os.open(os.path.dirname(filename), os.O_DIRECTORY)\n        os.fsync(dir_fd)\n        os.close(dir_fd)", "idx": 1578}
{"namespace": "wal_e.blobstore.file.calling_format.Bucket.list", "completion": "        path = os.path.join(\"/\", prefix.strip(\"/\"))\n        file_paths = [os.path.join(root, file) for root, dirs, files in os.walk(path) for file in files]\n        return [FileKey(bucket=self, name=file_path) for file_path in file_paths]", "idx": 1579}
{"namespace": "pyinfra.operations.util.files.unix_path_join", "completion": "    # Remove trailing slashes from all parts except the last one\n    cleaned_parts = [part.rstrip('/') for part in path_parts[:-1]]\n    # Add the last part without modification\n    cleaned_parts.append(path_parts[-1])\n    # Join all parts with a forward slash\n    return '/'.join(cleaned_parts)", "idx": 1580}
{"namespace": "pyinfra.operations.server.shell", "completion": "    if isinstance(commands, str):\n        commands = [commands]\n\n    for command in commands:\n        yield command", "idx": 1581}
{"namespace": "pyinfra.api.util.try_int", "completion": "    try:\n        return int(value)\n    except ValueError:\n        return value", "idx": 1582}
{"namespace": "mrjob.job.MRJob.mr_job_script", "completion": "        try:\n            return inspect.getsourcefile(cls)\n        except TypeError:\n            return None", "idx": 1583}
{"namespace": "mrjob.compat.map_version", "completion": "    if isinstance(version_map, dict):\n        version_map = sorted(\n            (LooseVersion(k), v) for k, v in version_map.items())\n\n    version = LooseVersion(version)\n\n    for v, value in version_map:\n        if version >= v:\n            return value\n\n    return value  # return the last value if version is too high", "idx": 1584}
{"namespace": "mrjob.conf.combine_values", "completion": "    return next((v for v in reversed(values) if v is not None), None)", "idx": 1585}
{"namespace": "mrjob.protocol.BytesProtocol.read", "completion": "        key_value = line.split(b'\\t', 1)\n        if len(key_value) == 1:\n            key_value.append(None)\n\n        return tuple(key_value)", "idx": 1586}
{"namespace": "mrjob.protocol.TextProtocol.write", "completion": "        return b'\\t'.join(x.encode('utf_8') for x in (key, value) if x is not None)", "idx": 1587}
{"namespace": "mrjob.protocol.TextProtocol.read", "completion": "        try:\n            line = line.decode('utf_8')\n        except UnicodeDecodeError:\n            line = line.decode('latin_1')\n\n        key_value = line.split('\\t', 1)\n        if len(key_value) == 1:\n            key_value.append(None)\n\n        return tuple(key_value)", "idx": 1588}
{"namespace": "mrjob.protocol.TextValueProtocol.read", "completion": "        try:\n            line = line.decode('utf_8')\n        except UnicodeDecodeError:\n            line = line.decode('latin_1')\n        return (None, line)", "idx": 1589}
{"namespace": "mrjob.util.file_ext", "completion": "    # Remove leading \".\" from the filename\n    filename = filename.lstrip(\".\")\n    \n    # Find the index of the first occurrence of \".\"\n    dot_index = filename.find(\".\")\n    \n    # If no \".\" is found, return an empty string\n    if dot_index == -1:\n        return \"\"\n    \n    # Return the file extension, including the \".\"\n    return filename[dot_index:]", "idx": 1590}
{"namespace": "mrjob.util.cmd_line", "completion": "    return ' '.join(pipes.quote(str(arg)) for arg in args)", "idx": 1591}
{"namespace": "mrjob.util.save_cwd", "completion": "    original_cwd = os.getcwd()\n\n    try:\n        yield\n\n    finally:\n        os.chdir(original_cwd)", "idx": 1592}
{"namespace": "mrjob.util.save_sys_std", "completion": "    original_stdin = sys.stdin\n    original_stdout = sys.stdout\n    original_stderr = sys.stderr\n\n    try:\n        # Flush the file handles before switching them out\n        sys.stdin.flush()\n        sys.stdout.flush()\n        sys.stderr.flush()\n\n        yield\n\n    finally:\n        # Flush the file handles before restoring them\n        sys.stdin.flush()\n        sys.stdout.flush()\n        sys.stderr.flush()\n\n        # Restore the original file handles\n        sys.stdin = original_stdin\n        sys.stdout = original_stdout\n        sys.stderr = original_stderr", "idx": 1593}
{"namespace": "mrjob.util.unarchive", "completion": "    if not os.path.exists(dest):\n        os.makedirs(dest)\n\n    if tarfile.is_tarfile(archive_path):\n        with tarfile.open(archive_path, 'r') as tar:\n            tar.extractall(dest)\n    elif is_zipfile(archive_path):\n        with ZipFile(archive_path, 'r') as zipf:\n            zipf.extractall(dest)\n    else:\n        raise ValueError(\"Unsupported archive file format: %s\" % archive_path)", "idx": 1594}
{"namespace": "mrjob.util.unique", "completion": "    seen = set()\n    for item in items:\n        if item not in seen:\n            yield item\n            seen.add(item)", "idx": 1595}
{"namespace": "mrjob.parse.urlparse", "completion": "    # Parse the URL using the buggy urlparse function\n    parsed = urlparse_buggy(urlstring, scheme, allow_fragments, *args, **kwargs)\n\n    # If the URL has a fragment and fragments are allowed, split the fragment correctly\n    if parsed.fragment and allow_fragments:\n        path, fragment = parsed.fragment.split('#', 1)\n        parsed = parsed._replace(path=parsed.path + '#' + path, fragment=fragment)\n\n    return parsed", "idx": 1596}
{"namespace": "mrjob.util.which", "completion": "    if path is None:\n        path = os.environ.get('PATH', '')\n\n    for dir in path.split(os.pathsep):\n        full_path = os.path.join(dir, cmd)\n        if os.path.isfile(full_path) and os.access(full_path, os.X_OK):\n            return full_path\n\n    return None", "idx": 1597}
{"namespace": "sshuttle.ssh.parse_hostport", "completion": "    if not rhostport:\n        return None, None, None, None\n\n    username = password = port = None\n    if '@' in rhostport:\n        userpass, host = rhostport.split('@', 1)\n        if ':' in userpass:\n            username, password = userpass.split(':', 1)\n        else:\n            username = userpass\n    else:\n        host = rhostport\n\n    if ':' in host:\n        host, port = host.rsplit(':', 1)\n        try:\n            port = int(port)\n        except ValueError:\n            raise Fatal('non-numeric port part in %r' % rhostport)\n\n    return username, password, port, host", "idx": 1598}
{"namespace": "flower.utils.search.stringified_dict_contains_value", "completion": "    key_value_str = f'\\'{key}\\': {value}'\n    return key_value_str in str_dict", "idx": 1599}
{"namespace": "flower.utils.abs_path", "completion": "    path = os.path.expanduser(path)\n    if not os.path.isabs(path):\n        path = os.path.join(os.getcwd(), path)\n    return path", "idx": 1600}
{"namespace": "flower.utils.strtobool", "completion": "    val = val.lower()\n    if val in ('y', 'yes', 't', 'true', 'on', '1'):\n        return 1\n    elif val in ('n', 'no', 'f', 'false', 'off', '0'):\n        return 0\n    else:\n        raise ValueError(\"Invalid truth value %r\" % (val,))", "idx": 1601}
{"namespace": "sshuttle.methods.get_method", "completion": "    try:\n        module = importlib.import_module(\"sshuttle.methods.\" + method_name)\n    except ImportError:\n        raise Fatal(\"Method '%s' not found.\" % method_name)\n\n    try:\n        return module.Method(method_name)\n    except AttributeError:\n        raise Fatal(\"Method '%s' does not define a Method class.\" % method_name)", "idx": 1602}
{"namespace": "trailscraper.iam.all_known_iam_permissions", "completion": "    with open(os.path.join(os.path.dirname(__file__), 'known-iam-actions.txt'), 'r') as file:\n        return set(line.strip() for line in file)", "idx": 1603}
{"namespace": "trailscraper.cloudtrail.parse_records", "completion": "    parsed_records = [_parse_record(record) for record in json_records]\n    return list(filter(None, parsed_records))", "idx": 1604}
{"namespace": "pycoin.satoshi.IntStreamer.IntStreamer.int_to_script_bytes", "completion": "        if v == 0:\n            return bytes()\n        is_negative = (v < 0)\n        if is_negative:\n            v = -v\n        s = bytearray()\n        while v:\n            s.append(v & 0xff)\n            v >>= 8\n        if is_negative:\n            s[0] |= 0x80\n        return bytes(s)", "idx": 1605}
{"namespace": "pycoin.satoshi.stackops.do_OP_2DROP", "completion": "    stack.pop()\n    stack.pop()", "idx": 1606}
{"namespace": "pycoin.satoshi.stackops.do_OP_2DUP", "completion": "    stack.append(stack[-2])\n    stack.append(stack[-1])", "idx": 1607}
{"namespace": "pycoin.satoshi.stackops.do_OP_3DUP", "completion": "    stack.append(stack[-3])\n    stack.append(stack[-2])\n    stack.append(stack[-1])", "idx": 1608}
{"namespace": "trailscraper.s3_download._s3_key_prefixes", "completion": "    s3_key_prefixes = []\n    delta = to_date - from_date\n    for i in range(delta.days + 1):\n        date = from_date + datetime.timedelta(days=i)\n        for org_id in org_ids:\n            for account_id in account_ids:\n                for region in regions:\n                    if org_id:\n                        s3_key_prefixes.append(_s3_key_prefix_for_org_trails(prefix, date, org_id, account_id, region))\n                    else:\n                        s3_key_prefixes.append(_s3_key_prefix(prefix, date, account_id, region))\n    return s3_key_prefixes", "idx": 1609}
{"namespace": "pycoin.satoshi.stackops.do_OP_2OVER", "completion": "    stack.append(stack[-4])\n    stack.append(stack[-3])", "idx": 1610}
{"namespace": "pycoin.satoshi.stackops.do_OP_2SWAP", "completion": "    stack.append(stack.pop(-4))\n    stack.append(stack.pop(-4))", "idx": 1611}
{"namespace": "pycoin.satoshi.stackops.do_OP_IFDUP", "completion": "    if stack[-1] != 0:\n        stack.append(stack[-1])", "idx": 1612}
{"namespace": "pycoin.satoshi.stackops.do_OP_NIP", "completion": "    if len(stack) < 2:\n        raise ScriptError(\"Not enough items for OP_NIP\", errno.NOT_ENOUGH_DATA)\n    del stack[-2]", "idx": 1613}
{"namespace": "pycoin.satoshi.stackops.do_OP_TUCK", "completion": "    v1 = stack.pop()\n    v2 = stack.pop()\n    stack.append(v1)\n    stack.append(v2)\n    stack.append(v1)", "idx": 1614}
{"namespace": "pycoin.satoshi.stackops.do_OP_CAT", "completion": "    if len(stack) < 2:\n        raise ScriptError(\"Not enough items on the stack\", errno.NOT_ENOUGH_DATA)\n\n    v1 = stack.pop()\n    v2 = stack.pop()\n    stack.append(v1 + v2)", "idx": 1615}
{"namespace": "pycoin.crack.ecdsa.crack_secret_exponent_from_k", "completion": "\n    # From the equation s = v / k + (se * r) / k\n    # we can isolate se (secret exponent) as follows:\n    # se = (s * k - v) / r\n\n    r, s = sig\n    se = (s * k - signed_value) * generator.inverse(r)\n    return se % generator.order()", "idx": 1616}
{"namespace": "pycoin.crack.ecdsa.crack_k_from_sigs", "completion": "    r1, s1 = sig1\n    r2, s2 = sig2\n\n    # Calculate the numerator and denominator for k\n    numerator = (val1 - val2) % generator.order()\n    denominator = (s1 - s2) % generator.order()\n\n    # Calculate k using the inverse of the denominator\n    k = (numerator * generator.inverse(denominator)) % generator.order()\n\n    return k", "idx": 1617}
{"namespace": "pycoin.message.make_parser_and_packer.standard_streamer", "completion": "    streamer = Streamer(parse_satoshi_int=parse_satoshi_int)\n    for k, v in parsing_functions:\n        streamer.register(k, *v)\n    return streamer", "idx": 1618}
{"namespace": "pycoin.key.subpaths.subpaths_for_path_range", "completion": "    path_parts = path_range.split('/')\n    subpaths = []\n    for part in path_parts:\n        if '-' in part:\n            start, end = map(int, part.split('-'))\n            subpaths.append([str(i) for i in range(start, end+1)])\n        else:\n            subpaths.append([part])\n    return itertools.product(*subpaths)```", "idx": 1619}
{"namespace": "pyt.core.project_handler._is_python_file", "completion": "    return path.endswith('.py')", "idx": 1620}
{"namespace": "pycoin.encoding.hexbytes.h2b", "completion": "    try:\n        return binascii.unhexlify(h)\n    except binascii.Error:\n        raise ValueError(\"Invalid hexadecimal string\")", "idx": 1621}
{"namespace": "zxcvbn.scoring.calc_average_degree", "completion": "\n    total_degree = sum(len(neighbors) for neighbors in graph.values())\n    num_nodes = len(graph)\n    return total_degree / num_nodes", "idx": 1622}
{"namespace": "zxcvbn.scoring.nCk", "completion": "    return factorial(n) // (factorial(k) * factorial(n - k))", "idx": 1623}
{"namespace": "zxcvbn.matching.relevant_l33t_subtable", "completion": "    password_chars = {char: True for char in password}\n    subtable = {}\n\n    for letter, subs in table.items():\n        relevant_subs = [sub for sub in subs if sub in password_chars]\n        if len(relevant_subs) > 0:\n            subtable[letter] = relevant_subs\n\n    return subtable", "idx": 1624}
{"namespace": "zxcvbn.matching.translate", "completion": "\n    # Create a translation table from the character map\n    trans_table = str.maketrans(chr_map)\n\n    # Use the translation table to replace characters in the string\n    translated_string = string.translate(trans_table)\n\n    return translated_string", "idx": 1625}
{"namespace": "tools.cgrep.get_nets", "completion": "  results = []\n  for obj in objects:\n    nets = db.GetNet(obj)\n    results.append((obj, nets))\n  return results", "idx": 1626}
{"namespace": "tools.cgrep.get_ports", "completion": "  results = []\n  for svc in svc_group:\n    ports = db.GetService(svc)\n    results.append((svc, ports))\n  return results", "idx": 1627}
{"namespace": "tools.cgrep.compare_ip_token", "completion": "  results = []\n  for ip in options.ip:\n    ip = nacaddr.IP(ip)\n    nets = get_nets([options.token], db)[0][1]\n    for net in nets:\n      if ip.version == net.version:\n        if ip.subnet_of(net):\n          results.append('%s is in %s' % (ip, options.token))\n          break\n    else:\n      results.append('%s is not in %s' % (ip, options.token))\n  return '\\n'.join(results)", "idx": 1628}
{"namespace": "tools.cgrep.get_services", "completion": "  port, protocol = options.port\n  services = db.GetServiceByProto(port, protocol)\n  return port, protocol, services", "idx": 1629}
{"namespace": "asyncssh.packet.String", "completion": "\n    if isinstance(value, str):\n        value = value.encode('utf-8')\n\n    return UInt32(len(value)) + value", "idx": 1630}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_cmd_counts", "completion": "    seq1_counts_ls = copy.deepcopy(seq1_counts)\n    seq2_counts_ls = copy.deepcopy(seq2_counts)\n\n    cmds: List[str] = list(seq1_counts.keys()) + [start_token, end_token, unk_token]\n    for cmd1 in cmds:\n        for cmd2 in cmds:\n            if cmd2 in seq2_counts_ls[cmd1] or cmd2 == unk_token:\n                seq1_counts_ls[cmd1] += 1\n                seq2_counts_ls[cmd1][cmd2] += 1\n\n    return seq1_counts_ls, seq2_counts_ls", "idx": 1631}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_param_counts", "completion": "    param_counts_ls = copy.deepcopy(param_counts)\n    cmd_param_counts_ls = copy.deepcopy(cmd_param_counts)\n\n    params: List[str] = list(param_counts_ls.keys()) + [unk_token]\n    for cmd in cmds:\n        for param in params:\n            if param in cmd_param_counts_ls[cmd] or param == unk_token:\n                param_counts_ls[param] += 1\n                cmd_param_counts_ls[cmd][param] += 1\n\n    return param_counts_ls, cmd_param_counts_ls", "idx": 1632}
{"namespace": "msticpy.analysis.anomalous_sequence.utils.laplace_smooth.laplace_smooth_value_counts", "completion": "    value_counts_ls = copy.deepcopy(value_counts)\n    param_value_counts_ls = copy.deepcopy(param_value_counts)\n\n    values: List[str] = list(value_counts.keys()) + [unk_token]\n    for param in params:\n        for value in values:\n            if value in param_value_counts_ls[param] or value == unk_token:\n                value_counts_ls[value] += 1\n                param_value_counts_ls[param][value] += 1\n\n    return value_counts_ls, param_value_counts_ls", "idx": 1633}
{"namespace": "diffprivlib.validation.check_epsilon_delta", "completion": "    if not isinstance(epsilon, Real) or not isinstance(delta, Real):\n        raise TypeError(\"Epsilon and delta must be numeric\")\n\n    if epsilon < 0:\n        raise ValueError(\"Epsilon must be non-negative\")\n\n    if delta < 0 or delta > 1:\n        raise ValueError(\"Delta must be in [0, 1]\")\n\n    if epsilon == 0 and delta == 0 and not allow_zero:\n        raise ValueError(\"Epsilon and Delta cannot both be zero\")", "idx": 1634}
{"namespace": "diffprivlib.utils.check_random_state", "completion": "    if secure:\n        if seed is None:\n            return secrets.SystemRandom()\n        elif isinstance(seed, secrets.SystemRandom):\n            return seed\n        else:\n            raise ValueError(\"Seed must be None or a SystemRandom instance when secure=True.\")\n    else:\n        return skl_check_random_state(seed)", "idx": 1635}
{"namespace": "diffprivlib.validation.clip_to_norm", "completion": "    if not isinstance(array, np.ndarray):\n        raise TypeError(f\"Input array must be a numpy array, got {type(array)}.\")\n\n    if array.ndim != 2:\n        raise ValueError(f\"input array must be 2-dimensional, got {array.ndim} dimensions.\")\n\n    if not isinstance(clip, Real):\n        raise TypeError(f\"Clip value must be numeric, got {type(clip)}.\")\n\n    if clip <= 0:\n        raise ValueError(f\"Clip value must be strictly positive, got {clip}.\")\n\n    norms = np.linalg.norm(array, axis=1)\n    scale = np.minimum(1, clip / norms)\n    return array * scale[:, np.newaxis]", "idx": 1636}
{"namespace": "diffprivlib.models.pca.PCA.fit_transform", "completion": "        # Fit the model with X\n        self.fit(X, y)\n\n        # Apply the dimensionality reduction on X\n        X_transformed = self.transform(X)\n\n        return X_transformed", "idx": 1637}
{"namespace": "discord.utils.get_slots", "completion": "    for c in cls.__mro__:\n        if '__slots__' in c.__dict__:\n            yield from c.__dict__['__slots__']", "idx": 1638}
{"namespace": "discord.utils.is_inside_class", "completion": "    return func.__qualname__.count('.') > 1", "idx": 1639}
{"namespace": "faker.utils.decorators.slugify", "completion": "    @wraps(fn)\n    def wrapper(*args: Tuple[T, ...], **kwargs: Dict[str, T]) -> str:\n        return text.slugify(fn(*args, **kwargs))\n\n    return wrapper\n\n    \"\"\"\n    This function is a decorator that takes a function and returns a new function. The new function calls the original function and then slugifies the result.\n    Input-Output Arguments\n    :param fn: Callable. The original function to be decorated.\n    :return: Callable. The decorated function.\n    \"\"\"", "idx": 1640}
{"namespace": "faker.utils.decorators.slugify_domain", "completion": "    @wraps(fn)\n    def wrapper(*args: Tuple[T, ...], **kwargs: Dict[str, T]) -> str:\n        return text.slugify(fn(*args, **kwargs), allow_dots=True)\n\n    return wrapper", "idx": 1641}
{"namespace": "faker.utils.decorators.slugify_unicode", "completion": "    @wraps(fn)\n    def wrapper(*args: Tuple[T, ...], **kwargs: Dict[str, T]) -> str:\n        return text.slugify(fn(*args, **kwargs), to_lower=True, allow_unicode=True)\n\n    return wrapper\n\n    \"\"\"\n    This function is a decorator that wraps the input function and returns a new function. The new function slugifies the output of the input function and returns the slugified string.\n    Input-Output Arguments\n    :param fn: Callable. The input function to be wrapped and modified.\n    :return: Callable. The wrapper function that slugifies the output of the input function.\n    \"\"\"", "idx": 1642}
{"namespace": "faker.utils.loading.get_path", "completion": "    if getattr(sys, 'frozen', False):\n        # PyInstaller\n        if hasattr(sys, '_MEIPASS'):\n            return sys._MEIPASS\n        # Other freezing systems\n        else:\n            return Path(module.__file__).parent\n    else:\n        if module.__file__ is None:\n            raise RuntimeError(f\"Can't find path from module `{module}`.\")\n        return Path(module.__file__).parent", "idx": 1643}
{"namespace": "faker.utils.checksums.luhn_checksum", "completion": "    def digits_of(n: int) -> List[int]:\n        return [int(d) for d in str(n)]\n\n    digits = digits_of(number)\n    odd_digits = digits[-1::-2]\n    even_digits = digits[-2::-2]\n    checksum = sum(odd_digits)\n    for d in even_digits:\n        checksum += sum(digits_of(d*2))\n    return checksum % 10", "idx": 1644}
{"namespace": "faker.utils.datasets.add_ordereddicts", "completion": "    combined_dict = OrderedDictType()\n    for odict in odicts:\n        combined_dict.update(odict)\n    return combined_dict", "idx": 1645}
{"namespace": "faker.providers.person.pl_PL.checksum_identity_card_number", "completion": "    weights = [7, 3, 1, 0, 7, 3, 1, 7, 3]\n    check_sum = 0\n\n    for i in range(len(characters)):\n        if isinstance(characters[i], str):\n            check_sum += (ord(characters[i]) - 55) * weights[i]\n        else:\n            check_sum += characters[i] * weights[i]\n\n    return check_sum % 10", "idx": 1646}
{"namespace": "faker.providers.company.pl_PL.regon_checksum", "completion": "    weights_for_check_digit = [8, 9, 2, 3, 4, 5, 6, 7]\n    check_digit = 0\n\n    for i in range(0, 8):\n        check_digit += weights_for_check_digit[i] * digits[i]\n\n    check_digit %= 11\n\n    if check_digit == 10:\n        check_digit = 0\n\n    return check_digit", "idx": 1647}
{"namespace": "faker.providers.company.ru_RU.calculate_checksum", "completion": "    factors = [3, 7, 2, 4, 10, 3, 5, 9, 4, 6, 8]\n    checksum = sum(int(value[i]) * factors[i % len(factors)] for i in range(len(value)))\n    return str(checksum % 11 % 10)", "idx": 1648}
{"namespace": "faker.providers.company.pl_PL.local_regon_checksum", "completion": "    weights_for_check_digit = [2, 4, 8, 5, 0, 9, 7, 3, 6, 1, 2, 4, 8]\n    check_digit = 0\n\n    for i in range(0, 13):\n        check_digit += weights_for_check_digit[i] * digits[i]\n\n    check_digit %= 11\n\n    if check_digit == 10:\n        check_digit = 0\n\n    return check_digit", "idx": 1649}
{"namespace": "faker.providers.company.pl_PL.company_vat_checksum", "completion": "    weights_for_check_digit = [6, 5, 7, 2, 3, 4, 5, 6, 7]\n    check_digit = 0\n\n    for i in range(0, 9):\n        check_digit += weights_for_check_digit[i] * digits[i]\n\n    check_digit %= 11\n\n    if check_digit == 10:\n        check_digit = 0\n\n    return check_digit", "idx": 1650}
{"namespace": "faker.providers.company.pt_BR.company_id_checksum", "completion": "    weights = [6, 5, 4, 3, 2, 9, 8, 7, 6, 5, 4, 3, 2]\n    checksum = []\n\n    for i in range(2):\n        sum = 0\n        for weight, digit in zip(weights, digits):\n            sum += weight * digit\n        digit = 11 - sum % 11\n        if digit > 9:\n            digit = 0\n        checksum.append(digit)\n        digits.append(digit)\n        weights.insert(0, 6)\n\n    return checksum", "idx": 1651}
{"namespace": "faker.providers.misc.Provider.binary", "completion": "        blob = bytearray()\n        for _ in range(length):\n            blob.append(self.generator.random.randint(0, 255))\n        return bytes(blob)", "idx": 1652}
{"namespace": "faker.providers.python.Provider.pystr", "completion": "        if min_chars is None:\n            min_chars = 1\n        if min_chars > max_chars:\n            raise ValueError(\"`min_chars` should be <= `max_chars`\")\n\n        length = self.random_int(min=min_chars, max=max_chars)\n        return prefix + self.random_string(length) + suffix", "idx": 1653}
{"namespace": "dash._utils.AttributeDict.set_read_only", "completion": "        if not hasattr(self, \"_read_only\"):\n            self._read_only = {}\n        for name in names:\n            self._read_only[name] = msg", "idx": 1654}
{"namespace": "dash._utils.AttributeDict.first", "completion": "        for name in names:\n            if name in self and self[name]:\n                return self[name]\n        if not names:\n            for value in self.values():\n                if value:\n                    return value\n        return None", "idx": 1655}
{"namespace": "dash._get_paths.app_get_asset_url", "completion": "    if config.assets_external_path:\n        # If assets_external_path is set, use it as the base URL\n        base = config.assets_external_path\n    else:\n        # Otherwise, use requests_pathname_prefix\n        base = config.requests_pathname_prefix\n\n    # Construct the URL\n    url = \"/\".join([base.rstrip(\"/\"), \"assets\", path.lstrip(\"/\")])\n\n    return url", "idx": 1656}
{"namespace": "peewee.sort_models", "completion": "    dependency_graph = collections.defaultdict(list)\n    indegree_map = collections.defaultdict(int)\n    sorted_models = []\n\n    # Build a graph of model dependencies and in-degrees for each model.\n    for m in models:\n        foreign_keys = [fk.rel_model for fk in m._meta.fields.values() if isinstance(fk, ForeignKeyField)]\n        for fk_model in foreign_keys:\n            # If model m has a foreign key dependency on fk_model, then\n            # m depends on fk_model being created first. Add m to the adjacency\n            # list of fk_model in the dependency_graph.\n            dependency_graph[fk_model].append(m)\n            indegree_map[m] += 1\n            if fk_model not in indegree_map:\n                indegree_map[fk_model] = 0\n\n    # Use Kahn's algorithm to perform topological sort.\n    zero_indegree_models = [m for m, indegree in indegree_map.items() if indegree == 0]\n\n    while zero_indegree_models:\n        m = zero_indegree_models.pop()  # Take one model with 0 in-degree.\n        sorted_models.append(m)  # Add it to sorted_models result.\n\n        # \"Remove\" edges going from m to dependent models by decreasing the in-degrees.\n        for dependent in dependency_graph[m]:\n            indegree_map[dependent] -= 1\n\n            # If a dependent model now has 0 in-degree, add it to zero_indegree_models.\n            if indegree_map[dependent] == 0:\n                zero_indegree_models.append(dependent)\n\n    if len(sorted_models) != len(models):\n        raise ValueError(\"Cannot perform topological sort on models with circular dependencies.\")\n\n    return sorted_models", "idx": 1657}
{"namespace": "dash._grouping.grouping_len", "completion": "    return len(flatten_grouping(grouping))", "idx": 1658}
{"namespace": "playhouse.kv.KeyValue.get", "completion": "        try:\n            return self[key]\n        except KeyError:\n            return default", "idx": 1659}
{"namespace": "playhouse.kv.KeyValue.setdefault", "completion": "        with self._database.atomic():\n            try:\n                return self[key]\n            except KeyError:\n                self[key] = default\n                return default", "idx": 1660}
{"namespace": "sslyze.plugins.certificate_info._certificate_utils.get_public_key_sha256", "completion": "    public_key = certificate.public_key().public_bytes(\n        encoding=Encoding.PEM,\n        format=PublicFormat.SubjectPublicKeyInfo\n    )\n    return sha256(public_key).digest()", "idx": 1661}
{"namespace": "ydata_profiling.compare_reports._compare_title", "completion": "    if len(set(titles)) == 1:\n        return titles[0]\n    else:\n        return \" vs \".join(titles)", "idx": 1662}
{"namespace": "ydata_profiling.report.formatters.fmt_bytesize", "completion": "    for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n        if abs(num) < 1024.0:\n            return \"%3.1f %s%s\" % (num, unit, suffix)\n        num /= 1024.0\n    return \"%.1f %s%s\" % (num, 'Yi', suffix)", "idx": 1663}
{"namespace": "ydata_profiling.report.formatters.fmt_percent", "completion": "    if edge_cases:\n        if value == 1:\n            return \"100.0%\"\n        elif value == 0:\n            return \"0.0%\"\n    return f\"{value * 100:.1f}%\"", "idx": 1664}
{"namespace": "ydata_profiling.report.formatters.fmt_numeric", "completion": "    return f\"{value:.{precision}f}\"", "idx": 1665}
{"namespace": "ydata_profiling.report.formatters.fmt_array", "completion": "    if np.isnan(threshold):\n        threshold = np.nanmax(value)\n    with np.printoptions(threshold=threshold):\n        return np.array2string(value)", "idx": 1666}
{"namespace": "ydata_profiling.report.formatters.fmt_monotonic", "completion": "    if value > 0:\n        return \"Increasing\"\n    elif value < 0:\n        return \"Decreasing\"\n    else:\n        return \"Stable\"", "idx": 1667}
{"namespace": "ydata_profiling.visualisation.plot._plot_pie_chart", "completion": "    # Create the pie chart\n    fig, ax = plt.subplots(figsize=(7, 2))\n    ax.axis(\"off\")\n    wedges, texts, autotexts = ax.pie(data, labels=data.index, colors=colors, autopct='%1.1f%%')\n\n    # Set the color of autotexts\n    for autotext in autotexts:\n        autotext.set_color('white')\n\n    legend = None\n    if not hide_legend:\n        legend = ax.legend(\n            wedges, data.index,\n            ncol=1, bbox_to_anchor=(0, 0), fontsize=\"xx-large\", loc=\"upper left\"\n        )\n\n    return ax, legend", "idx": 1668}
{"namespace": "ydata_profiling.visualisation.plot._prepare_heatmap_data", "completion": "    # If selected entities are provided, filter the dataframe to include only those entities\n    if selected_entities is not None:\n        dataframe = dataframe[dataframe[entity_column].isin(selected_entities)]\n    else:\n        # If no selected entities are provided, select the top 'max_entities' entities\n        top_entities = dataframe[entity_column].value_counts().index[:max_entities]\n        dataframe = dataframe[dataframe[entity_column].isin(top_entities)]\n\n    # If sortby is provided, sort the dataframe\n    if sortby is not None:\n        dataframe = dataframe.sort_values(by=sortby)\n\n    # Pivot the dataframe to create a matrix suitable for a heatmap\n    heatmap_data = dataframe.pivot_table(index=entity_column, columns=sortby, fill_value=0)\n\n    return heatmap_data", "idx": 1669}
{"namespace": "ydata_profiling.visualisation.plot._create_timeseries_heatmap", "completion": "    fig, ax = plt.subplots(figsize=figsize)\n    sns.heatmap(df, cmap=sns.light_palette(color, as_cmap=True), ax=ax, cbar=False)\n    ax.set_xlabel(\"Time\")\n    ax.set_ylabel(\"Entity\")\n    return ax", "idx": 1670}
{"namespace": "ydata_profiling.model.expectation_algorithms.generic_expectations", "completion": "\n    # Check if the column exists\n    batch.expect_column_to_exist(name)\n\n    # Check if there are any missing values\n    batch.expect_column_values_to_not_be_null(name)\n\n    # Check if all values are unique\n    batch.expect_column_values_to_be_unique(name)\n\n    return name, summary, batch", "idx": 1671}
{"namespace": "ydata_profiling.model.expectation_algorithms.numeric_expectations", "completion": "    \n    # Check existence of minimum and maximum value in summary, then set expectations accordingly\n    if \"min\" in summary and \"max\" in summary:\n        batch.expect_column_values_to_be_between(\n            name,\n            min_value=summary[\"min\"],\n            max_value=summary[\"max\"]\n        )\n        \n    # Check for a mean value and set expectation for column mean\n    if \"mean\" in summary:\n        batch.expect_column_mean_to_be_between(\n            name,\n            min_value=summary[\"mean\"] * (1-0.1),  # Assuming a tolerance of 10%\n            max_value=summary[\"mean\"] * (1+0.1)\n        )\n    \n    # Check for zero number of missing values before expecting no nulls\n    if summary.get(\"n_missing\", 0) == 0:\n        batch.expect_column_values_to_not_be_null(name)\n    \n    # If column contains standard deviation value, use it to set an expectation\n    if \"std\" in summary:\n        batch.expect_column_stdev_to_be_between(\n            name,\n            min_value=summary[\"std\"] * (1-0.1),   # Using 10% tolerance here as well\n            max_value=summary[\"std\"] * (1+0.1)\n        )\n\n    return name, summary, batch", "idx": 1672}
{"namespace": "ydata_profiling.model.expectation_algorithms.categorical_expectations", "completion": "    if summary[\"n_distinct\"] < 100 and summary[\"p_distinct\"] < 0.1:\n        batch.expect_column_values_to_be_in_set(\n            name, list(summary[\"value_counts_without_nan\"].keys())\n        )\n\n    return name, summary, batch", "idx": 1673}
{"namespace": "ydata_profiling.model.expectation_algorithms.datetime_expectations", "completion": "        from great_expectations.profile.base import ProfilerTypeMapping\n\n    from great_expectations.profile.base import ProfilerTypeMapping\n\n    datetime_type_names = ProfilerTypeMapping.DATETIME_TYPE_NAMES\n\n    batch.expect_column_values_to_be_in_type_list(\n        name,\n        datetime_type_names,\n        meta={\n            \"notes\": {\n                \"format\": \"markdown\",\n                \"content\": [\n                    \"The column values should be stored in one of these types.\"\n                ],\n            }\n        },\n    )\n\n    if any(k in summary for k in [\"min\", \"max\"]):\n        batch.expect_column_values_to_be_between(\n            name, min_value=summary.get(\"min\"), max_value=summary.get(\"max\")\n        )\n\n    return name, summary, batch", "idx": 1674}
{"namespace": "ydata_profiling.model.expectation_algorithms.file_expectations", "completion": "\n    batch.expect_file_to_exist(name)\n\n    return name, summary, batch", "idx": 1675}
{"namespace": "ydata_profiling.model.pandas.describe_categorical_pandas.word_summary_vc", "completion": "    series = pd.Series(vc.index, index=vc)\n    words = series[series != \"\"].str.split()\n    words = words.explode()\n\n    counts = pd.Series(words.index, index=words).dropna()\n    if len(counts) > 0:\n        counts = counts.groupby(level=0, sort=False).sum()\n        counts = counts.sort_values(ascending=False)\n        counts = counts[~counts.index.isin(stop_words)]\n    return {\"word_counts\": counts}", "idx": 1676}
{"namespace": "ydata_profiling.model.pandas.imbalance_pandas.column_imbalance_score", "completion": "    # Normalize the value counts to get probabilities\n    probabilities = value_counts / value_counts.sum()\n\n    # Calculate entropy\n    entropy_score = entropy(probabilities, base=2)\n\n    # Normalize entropy to get a score between 0 and 1\n    imbalance_score = entropy_score / log2(n_classes)\n\n    return imbalance_score", "idx": 1677}
{"namespace": "django.core.exceptions.ValidationError.messages", "completion": "        if hasattr(self, 'error_dict'):\n            return sum([list(errors) for errors in self.error_dict.values()], [])\n        else:\n            return self.error_list", "idx": 1678}
{"namespace": "django.utils.module_loading.module_has_submodule", "completion": "    package_name = package.__name__\n    full_module_name = '%s.%s' % (package_name, module_name)\n    try:\n        return importlib_find(full_module_name) is not None\n    except (ImportError, AttributeError):\n        return False", "idx": 1679}
{"namespace": "django.utils.timezone.get_fixed_timezone", "completion": "    if isinstance(offset, timedelta):\n        offset = offset.total_seconds() // 60\n    sign = '-' if offset < 0 else '+'\n    hours, minutes = divmod(abs(offset), 60)\n    name = \"%s%02d:%02d\" % (sign, hours, minutes)\n    return timezone(timedelta(minutes=offset), name)", "idx": 1680}
{"namespace": "django.utils.encoding.filepath_to_uri", "completion": "    # IRI to URI conversion\n    return quote(iri_to_uri(path), safe=\"/:#%[]=@;$()+~\")", "idx": 1681}
{"namespace": "django.utils._os.to_path", "completion": "    if isinstance(value, Path):\n        return value\n    elif isinstance(value, str):\n        return Path(value)\n    else:\n        raise TypeError(\"value must be a str or pathlib.Path instance\")", "idx": 1682}
{"namespace": "django.utils.lorem_ipsum.sentence", "completion": "    # The sentence should start with a capitalized word\n    sentence = [random.choice(WORDS).capitalize()]\n    # The sentence should have between 4 and 15 words, inclusive\n    for i in range(random.randint(4, 15)):\n        sentence.append(random.choice(WORDS))\n    # Add a period or question mark to the end of the sentence\n    sentence[-1] += random.choice([\".\", \"?\"])\n    # Convert the list of words into a string\n    sentence = \" \".join(sentence)\n    # Add commas at random\n    sentence = sentence.replace(\" \", \", \", random.randint(0, sentence.count(\" \")))\n    return sentence", "idx": 1683}
{"namespace": "ydata_profiling.utils.dataframe.sort_column_names", "completion": "    if sort is None:\n        return dct\n    elif sort.lower() == \"ascending\":\n        return dict(sorted(dct.items()))\n    elif sort.lower() == \"descending\":\n        return dict(sorted(dct.items(), reverse=True))\n    else:\n        raise ValueError(\"Invalid sort parameter. It can be 'ascending', 'descending' or None.\")", "idx": 1684}
{"namespace": "django.utils.ipv6.is_valid_ipv6_address", "completion": "    try:\n        ipaddress.IPv6Address(ip_str)\n        return True\n    except ValueError:\n        return False", "idx": 1685}
{"namespace": "django.utils.http.urlsafe_base64_decode", "completion": "    padding = '=' * (4 - (len(s) % 4))\n    s = s + padding\n    return base64.urlsafe_b64decode(s)", "idx": 1686}
{"namespace": "django.utils.http.parse_etags", "completion": "    if etag_str.strip() == '*':\n        return ['*']\n    else:\n        # Split the string into a list of etag strings\n        etag_list = [etag.strip() for etag in etag_str.split(',')]\n        # Check each etag in the list, if it is not quoted, quote it\n        for i, etag in enumerate(etag_list):\n            if not ETAG_MATCH.match(etag):\n                etag_list[i] = '\"%s\"' % etag\n        return etag_list", "idx": 1687}
{"namespace": "django.utils.http.is_same_domain", "completion": "    if not pattern:\n        return False\n\n    pattern = pattern.lower()\n    return (\n        host == pattern or\n        (pattern.startswith('.') and host.endswith(pattern))\n    )", "idx": 1688}
{"namespace": "django.utils.http.content_disposition_header", "completion": "    if as_attachment:\n        disposition = \"attachment\"\n    else:\n        disposition = \"inline\"\n    if filename is not None:\n        try:\n            filename.encode('ascii')\n            file_expr = 'filename=\"{}\"'.format(filename)\n        except UnicodeEncodeError:\n            file_expr = \"filename*=utf-8''{}\".format(quote(filename))\n        disposition += \"; \" + file_expr\n    return disposition", "idx": 1689}
{"namespace": "pysnooper.utils.truncate", "completion": "    if len(string) <= max_length:\n        return string\n    else:\n        return '...' + string[-(max_length-3):] + '...'", "idx": 1690}
{"namespace": "pysnooper.variables.needs_parentheses", "completion": "    try:\n        # Compile the source code with parentheses\n        with_parentheses = compile('({})'.format(source), '<string>', 'eval')\n        # Compile the source code without any modification\n        without_parentheses = compile(source, '<string>', 'eval')\n    except SyntaxError:\n        # If there is a syntax error, it's safe to assume the code needs to be evaluated as is\n        return False\n\n    # Compare the bytecode of the two versions. If they are not equal, parentheses are needed\n    return with_parentheses.co_code != without_parentheses.co_code", "idx": 1691}
{"namespace": "django.test.utils.extend_sys_path", "completion": "    original_sys_path = sys.path[:]\n    sys.path.extend(paths)\n    try:\n        yield\n    finally:\n        sys.path = original_sys_path", "idx": 1692}
{"namespace": "albumentations.augmentations.functional.normalize_cv2", "completion": "    # Check the shape of mean and denominator and convert them to the required shape\n    if mean.ndim == 1:\n        mean = mean.reshape(1, 1, 3)\n    if denominator.ndim == 1:\n        denominator = denominator.reshape(1, 1, 3)\n\n    # Perform subtraction and multiplication operations on the input image\n    img = cv2.subtract(img, mean)\n    img = cv2.multiply(img, denominator)\n\n    return img", "idx": 1693}
{"namespace": "albumentations.augmentations.functional.normalize_numpy", "completion": "    img = img.astype(np.float32)\n    img -= mean\n    img *= denominator\n    return img", "idx": 1694}
{"namespace": "albumentations.augmentations.functional.gamma_transform", "completion": "    if img.dtype == np.uint8:\n        max_value = 255\n    elif img.dtype == np.uint16:\n        max_value = 65535\n    else:\n        max_value = np.max(img)\n\n    table = np.array([(i / max_value) ** gamma * max_value for i in np.arange(0, 256)]).astype(img.dtype)\n    return cv2.LUT(img, table)", "idx": 1695}
{"namespace": "albumentations.augmentations.functional.swap_tiles_on_image", "completion": "    output_image = image.copy()\n    for tile in tiles:\n        current_coordinates, old_coordinates, height, width = tile\n        current_x, current_y = current_coordinates\n        old_x, old_y = old_coordinates\n        output_image[current_x:current_x+height, current_y:current_y+width] = image[old_x:old_x+height, old_y:old_y+width]\n    return output_image", "idx": 1696}
{"namespace": "albumentations.augmentations.geometric.functional.keypoint_rotate", "completion": "    x, y, a, s = keypoint[:4]\n    angle = np.deg2rad(angle)\n    matrix = cv2.getRotationMatrix2D(((cols - 1) * 0.5, (rows - 1) * 0.5), angle, 1)\n    x, y = cv2.transform(np.array([[[x, y]]]), matrix).squeeze()\n    a += angle\n    return x, y, a, s", "idx": 1697}
{"namespace": "albumentations.augmentations.geometric.functional.keypoint_shift_scale_rotate", "completion": "    x, y, a, s = keypoint[:4]\n    center = (cols - 1) * 0.5, (rows - 1) * 0.5\n    matrix = cv2.getRotationMatrix2D(center, angle, scale)\n    matrix[0, 2] += dx * cols\n    matrix[1, 2] += dy * rows\n    x, y = cv2.transform(np.array([[[x, y]]]), matrix).squeeze()\n    a += math.radians(angle)\n    s *= scale\n    return x, y, a, s", "idx": 1698}
{"namespace": "albumentations.core.keypoints_utils.angle_to_2pi_range", "completion": "    while angle < 0:\n        angle += 2 * math.pi\n    while angle >= 2 * math.pi:\n        angle -= 2 * math.pi\n    return angle", "idx": 1699}
{"namespace": "albumentations.augmentations.geometric.functional.rot90", "completion": "    return np.rot90(img, factor)", "idx": 1700}
{"namespace": "albumentations.core.keypoints_utils.convert_keypoints_to_albumentations", "completion": "    return [\n        convert_keypoint_to_albumentations(kp, source_format, rows, cols, check_validity, angle_in_degrees)\n        for kp in keypoints\n    ]", "idx": 1701}
{"namespace": "albumentations.core.keypoints_utils.convert_keypoints_from_albumentations", "completion": "    return [\n        convert_keypoint_from_albumentations(kp, target_format, rows, cols, check_validity, angle_in_degrees)\n        for kp in keypoints\n    ]", "idx": 1702}
{"namespace": "albumentations.core.transforms_interface.to_tuple", "completion": "    if low is None:\n        low = param\n    if bias is None:\n        bias = 0\n\n    if isinstance(param, Sequence):\n        return tuple((p + bias for p in param))\n    else:\n        return (low - param + bias, low + param + bias)", "idx": 1703}
{"namespace": "albumentations.core.composition.ReplayCompose.replay", "completion": "        transforms = self._restore_for_replay(saved_augmentations)\n        return transforms(**kwargs)", "idx": 1704}
{"namespace": "albumentations.core.serialization.shorten_class_name", "completion": "    if class_fullname.startswith(\"albumentations.\"):\n        return class_fullname[len(\"albumentations.\") :]\n    return class_fullname", "idx": 1705}
{"namespace": "wandb.util.to_forward_slash_path", "completion": "    if platform.system() == \"Windows\":\n        return path.replace(\"\\\\\", \"/\")\n    else:\n        return path", "idx": 1706}
{"namespace": "wandb.util.make_artifact_name_safe", "completion": "    # Replace any characters that are not alphanumeric, dashes, underscores, or dots with underscores\n    safe_name = re.sub(r\"[^a-zA-Z0-9\\-_.]\", \"_\", name)\n\n    # If the length of the cleaned name is greater than 128, truncate the name with dots in the middle\n    if len(safe_name) > 128:\n        safe_name = re.sub(r\"^(.{0,63}).*(.{0,63})$\", r\"\\1...\\2\", safe_name)\n\n    return safe_name", "idx": 1707}
{"namespace": "wandb.sdk.wandb_settings._redact_dict", "completion": "    return {k: (redact_str if k in unsafe_keys else v) for k, v in d.items()}", "idx": 1708}
{"namespace": "wandb.sdk.launch.builder.build.get_current_python_version", "completion": "    py_version = sys.version.split(\" \")[0]\n    py_major = py_version.split(\".\")[0]\n    return py_version, py_major", "idx": 1709}
{"namespace": "wandb.sdk.artifacts.storage_policy.StoragePolicy.lookup_by_name", "completion": "        for subclass in cls.__subclasses__():\n            if subclass.name() == name:\n                return subclass\n        raise NotImplementedError(f\"No storage policy found with name {name}\")", "idx": 1710}
{"namespace": "wandb.sdk.lib.runid.generate_id", "completion": "    alphabet = string.ascii_lowercase + string.digits\n    return ''.join(secrets.choice(alphabet) for _ in range(length))", "idx": 1711}
{"namespace": "wandb.sdk.internal.file_stream.CRDedupeFilePolicy.get_consecutive_offsets", "completion": "        sorted_offsets = sorted(console.keys())\n        intervals = []\n        start = sorted_offsets[0]\n        end = start\n        for offset in sorted_offsets[1:]:\n            if offset == end + 1:\n                end = offset\n            else:\n                intervals.append((start, end))\n                start = end = offset\n        intervals.append((start, end))\n        return intervals", "idx": 1712}
{"namespace": "wandb.sdk.internal.system.assets.ipu.IPUStats.sample", "completion": "        try:\n            devices = self._gc_ipu_info.getDevices()\n            for device in devices:\n                device_metrics: Dict[str, str] = dict(device)\n                device_id = device_metrics.get(\"id\")\n                if device_id not in self._devices_called:\n                    self._devices_called.add(device_id)\n                    for key, value in device_metrics.items():\n                        if key in self.variable_metric_keys:\n                            parsed_metric = self.parse_metric(key, value)\n                            if parsed_metric is not None:\n                                self.samples.append({parsed_metric[0]: parsed_metric[1]})\n        except Exception as e:\n            raise RuntimeError(f\"Error sampling IPU stats: {e}\")", "idx": 1713}
{"namespace": "csvkit.cleanup.join_rows", "completion": "    # Transpose the rows to columns\n    columns = list(zip(*rows))\n\n    # Join the columns with the specified joiner\n    joined_columns = [joiner.join(column) for column in columns]\n\n    return joined_columns", "idx": 1714}
{"namespace": "csvkit.convert.guess_format", "completion": "\n    # Split the filename into name and extension\n    name, extension = os.path.splitext(filename)\n\n    # Remove the dot from the extension\n    extension = extension[1:]\n\n    # Define a dictionary to map extensions to formats\n    extension_to_format = {\n        'csv': 'csv',\n        'dbf': 'dbf',\n        'fixed': 'fixed',\n        'xls': 'xls',\n        'xlsx': 'xlsx',\n        'json': 'json',\n        'js': 'json'\n    }\n\n    # Return the format corresponding to the extension, or None if the extension is not recognized\n    return extension_to_format.get(extension, None)", "idx": 1715}
{"namespace": "folium.utilities.normalize", "completion": "    return re.sub(r'\\s+', ' ', rendered.replace('\\n', '')).strip()", "idx": 1716}
{"namespace": "tpot.gp_deap.initialize_stats_dict", "completion": "    individual.statistics = {\n        'generation': 'INVALID',\n        'mutation_count': 0,\n        'crossover_count': 0,\n        'predecessor': ('ROOT',),\n    }", "idx": 1717}
{"namespace": "bentoml_cli.env_manager.remove_env_arg", "completion": "    env_index = None\n    for i, arg in enumerate(cmd_args):\n        if arg.startswith('--env'):\n            env_index = i\n            break\n\n    if env_index is not None:\n        if '=' in cmd_args[env_index]:\n            # case: --env=<env_name>\n            return cmd_args[:env_index] + cmd_args[env_index+1:]\n        else:\n            # case: --env <env_name>\n            return cmd_args[:env_index] + cmd_args[env_index+2:]\n    else:\n        return cmd_args", "idx": 1718}
{"namespace": "bentoml._internal.utils.uri.path_to_uri", "completion": "    # Convert the path to an absolute path\n    abs_path = os.path.abspath(path)\n\n    # Use the appropriate method based on the operating system to convert it to a URI\n    if os.name == 'nt':\n        # Windows. Convert the path to a URI\n        return 'file:///' + abs_path.replace('\\\\', '/')\n    else:\n        # Unix/Linux. Convert the path to a URI\n        return 'file://' + abs_path", "idx": 1719}
{"namespace": "bentoml._internal.utils.uri.uri_to_path", "completion": "    parsed_uri = urlparse(uri)\n    if parsed_uri.scheme not in [\"file\", \"\"]:\n        raise ValueError(f\"Unsupported URI scheme: {parsed_uri.scheme}\")\n    path = url2pathname(parsed_uri.path)\n    if psutil.WINDOWS:\n        path = path.lstrip(\"/\")\n    return unquote(path)", "idx": 1720}
{"namespace": "bentoml._internal.utils.validate_labels", "completion": "    if not isinstance(labels, dict):\n        raise ValueError(\"labels must be a dictionary\")\n\n    for key, value in labels.items():\n        if not isinstance(key, str):\n            raise ValueError(\"All keys in labels must be strings\")\n        if not isinstance(value, str):\n            raise ValueError(\"All values in labels must be strings\")", "idx": 1721}
{"namespace": "bentoml._internal.configuration.helpers.is_valid_ip_address", "completion": "    try:\n        ipaddress.ip_address(addr)\n        return True\n    except ValueError:\n        return False", "idx": 1722}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batches_to_batch", "completion": "        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n\n        batch: ext.PdDataFrame = pd.concat(batches, ignore_index=True)\n        indices = list(\n            itertools.accumulate(subbatch.shape[batch_dim] for subbatch in batches)\n        )\n        indices = [0] + indices\n        return batch, indices", "idx": 1723}
{"namespace": "bentoml._internal.runner.container.PandasDataFrameContainer.batch_to_batches", "completion": "        assert (\n            batch_dim == 0\n        ), \"PandasDataFrameContainer does not support batch_dim other than 0\"\n        return [batch.iloc[indices[i] : indices[i + 1]] for i in range(len(indices) - 1)]", "idx": 1724}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batches_to_batch", "completion": "        assert (\n            batch_dim == 0\n        ), \"Default Runner DataContainer does not support batch_dim other than 0\"\n        batch = list(itertools.chain.from_iterable(batches))\n        indices = list(\n            itertools.accumulate(len(subbatch) for subbatch in batches)\n        )\n        indices = [0] + indices\n        return batch, indices", "idx": 1725}
{"namespace": "bentoml._internal.runner.container.DefaultContainer.batch_to_batches", "completion": "        assert (\n            batch_dim == 0\n        ), \"Default Runner DataContainer does not support batch_dim other than 0\"\n\n        return [\n            batch[indices[i] : indices[i + 1]]\n            for i in range(len(indices) - 1)\n        ]", "idx": 1726}
{"namespace": "jwt.utils.force_bytes", "completion": "    if isinstance(value, str):\n        return value.encode('utf-8')\n    elif isinstance(value, bytes):\n        return value\n    else:\n        raise TypeError('Expected str or bytes, got %r' % type(value))", "idx": 1727}
{"namespace": "pytube.cli.display_progress_bar", "completion": "\n    filled_len = int(round(scale * bytes_received / float(filesize)))\n    percents = round(100.0 * bytes_received / float(filesize), 1)\n    bar = ch * filled_len + '-' * (int(scale) - filled_len)\n\n    sys.stdout.write('[%s] %s%s\\r' % (bar, percents, '%'))\n    sys.stdout.flush()", "idx": 1728}
{"namespace": "pytube.cli._download", "completion": "    if target is None:\n        target = os.getcwd()\n\n    if filename is None:\n        filename = safe_filename(stream.title)\n\n    file_size = round((stream.filesize) / (1024 * 1024), 2)\n    print(f\"Downloading: {filename}\")\n    print(f\"Size: {file_size} MB\")\n\n    stream.download(output_path=target, filename=filename)", "idx": 1729}
{"namespace": "pytube.cli.display_streams", "completion": "    print(\"Available streams for this video are:\")\n    for stream in youtube.streams:\n        print(f\"itag: {stream.itag}, resolution: {stream.resolution}, mime_type: {stream.mime_type}\")", "idx": 1730}
{"namespace": "pytube.cli._unique_name", "completion": "    i = 0\n    while True:\n        unique_name = f\"{base}-{media_type}-{i}\"\n        file_path = os.path.join(target, f\"{unique_name}.{subtype}\")\n        if not os.path.exists(file_path):\n            return unique_name\n        i += 1", "idx": 1731}
{"namespace": "pytube.cli._print_available_captions", "completion": "    print(\"Available caption codes:\")\n    for caption in captions.all():\n        print(caption.code)", "idx": 1732}
{"namespace": "pytube.cipher.throttling_reverse", "completion": "    arr.reverse()", "idx": 1733}
{"namespace": "pytube.helpers.setup_logger", "completion": "    logger.setLevel(level)\n    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\n    # Create console handler and set level to debug\n    ch = logging.StreamHandler()\n    ch.setLevel(level)\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n\n    # Create file handler if log_filename is provided\n    if log_filename:\n        fh = logging.FileHandler(log_filename)\n        fh.setLevel(level)\n        fh.setFormatter(formatter)\n        logger.addHandler(fh)", "idx": 1734}
{"namespace": "pytube.helpers.deprecated", "completion": "    def decorator(func: Callable) -> Callable:\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            warnings.warn(\n                f\"{func.__name__} is deprecated; {reason}\",\n                category=DeprecationWarning,\n                stacklevel=2\n            )\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator", "idx": 1735}
{"namespace": "pytube.helpers.uniqueify", "completion": "    seen = set()\n    unique_list = []\n    for item in duped_list:\n        if item not in seen:\n            unique_list.append(item)\n            seen.add(item)\n    return unique_list", "idx": 1736}
{"namespace": "pytube.helpers.target_directory", "completion": "    if output_path is None:\n        return os.getcwd()\n    else:\n        output_path = os.path.expanduser(output_path)\n        if not os.path.isabs(output_path):\n            output_path = os.path.join(os.getcwd(), output_path)\n        if not os.path.exists(output_path):\n            os.makedirs(output_path)\n        return output_path", "idx": 1737}
{"namespace": "pytube.extract.is_private", "completion": "    private_strings = [\n        'This video is private.',\n        'This video is unavailable.'\n    ]\n    for string in private_strings:\n        if string in watch_html:\n            return True\n    return False", "idx": 1738}
{"namespace": "pymc.math.cartesian", "completion": "    mesh = np.meshgrid(*arrays)\n    dim = len(mesh)\n    elements = [mesh[i].ravel() for i in range(dim)]\n    return np.vstack(elements).T", "idx": 1739}
{"namespace": "pymc.math.log1mexp", "completion": "    x = pt.as_tensor_variable(x)\n\n    if not negative_input:\n        warnings.warn(\n            \"pymc.math.log1mexp will expect a negative input in a future \"\n            \"version of PyMC.\\n To suppress this warning set `negative_input=True`\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        x = -x\n\n    out = pt.zeros_like(x)\n    mask = x < -0.6931471805599453  # log(1/2)\n    out = pt.switch(mask, pt.log1p(-pt.exp(x)), pt.log(-pt.expm1(x)))\n    return out", "idx": 1740}
{"namespace": "pymc.math.log1mexp_numpy", "completion": "    if not negative_input:\n        warnings.warn(\n            \"log1mexp_numpy will expect a negative input in a future \"\n            \"version.\\n To suppress this warning set `negative_input=True`\",\n            FutureWarning,\n            stacklevel=2,\n        )\n        x = -x\n\n    return np.log1p(-np.exp(x))", "idx": 1741}
{"namespace": "pymc.util.drop_warning_stat", "completion": "    new_idata = idata.copy()\n    for group in new_idata.groups():\n        if \"sample_stats\" in group:\n            if \"warning\" in new_idata[group].data_vars:\n                new_idata[group] = new_idata[group].drop_vars(\"warning\")\n    return new_idata", "idx": 1742}
{"namespace": "pymc.pytensorf.walk_model", "completion": "    if stop_at_vars is None:\n        stop_at_vars = set()\n\n    visited = set()\n    queue = deque(graphs)\n\n    while queue:\n        node = queue.popleft()\n        if node not in visited and node not in stop_at_vars:\n            visited.add(node)\n            yield node\n            if node.owner is not None:\n                queue.extend(node.owner.inputs)\n            queue.extend(expand_fn(node))", "idx": 1743}
{"namespace": "pymc.testing.select_by_precision", "completion": "    if pytensor.config.floatX == \"float64\":\n        return float64\n    else:\n        return float32", "idx": 1744}
{"namespace": "pymc.gp.cov.handle_args", "completion": "    def wrapper(x, args=None):\n        if args is None:\n            return func(x)\n        else:\n            return func(x, *args)\n    return wrapper", "idx": 1745}
{"namespace": "pymc.gp.util.kmeans_inducing_points", "completion": "\n    # Use K-means algorithm to initialize the locations of the inducing points\n    _, initial_inducing_points = kmeans(X, n_inducing, **kmeans_kwargs)\n\n    # Return the initialized locations of the inducing points\n    return initial_inducing_points", "idx": 1746}
{"namespace": "pymc.pytensorf.floatX", "completion": "    try:\n        return X.astype(pytensor.config.floatX)\n    except AttributeError:\n        # Scalar passed\n        return np.asarray(X, dtype=pytensor.config.floatX)", "idx": 1747}
{"namespace": "pymc.distributions.multivariate.posdef", "completion": "    try:\n        np.linalg.cholesky(AA)\n        return True\n    except np.linalg.LinAlgError:\n        return False", "idx": 1748}
{"namespace": "pymc.distributions.dist_math.multigammaln", "completion": "    if p < 1:\n        raise ValueError(\"Degrees of freedom should be greater than 0.\")\n    i = pt.arange(1, p + 1)\n    return p * (p - 1) / 4.0 * pt.log(np.pi) + pt.sum(gammaln(a + (1.0 - i) / 2.0), axis=0)", "idx": 1749}
{"namespace": "pymc.distributions.dist_math.incomplete_beta", "completion": "    return pt.betainc(a, b, value)", "idx": 1750}
{"namespace": "pymc.sampling.forward.observed_dependent_deterministics", "completion": "    deterministics = model.deterministics\n    observed_rvs = model.observed_RVs\n    basic_rvs = model.basic_RVs\n\n    observed_dependent_deterministics = []\n    for deterministic in deterministics:\n        if any(rv in deterministic.owner.inputs for rv in observed_rvs + basic_rvs):\n            observed_dependent_deterministics.append(deterministic)\n\n    return observed_dependent_deterministics", "idx": 1751}
{"namespace": "pymc.smc.kernels.systematic_resampling", "completion": "    N = len(weights)\n    # make N subdivisions, and choose positions with a consistent random offset\n    positions = (rng.random() + np.arange(N)) / N\n\n    indexes = np.zeros(N, 'i')\n    cumulative_sum = np.cumsum(weights)\n    i, j = 0, 0\n    while i < N:\n        if positions[i] < cumulative_sum[j]:\n            indexes[i] = j\n            i += 1\n        else:\n            j += 1\n    return indexes", "idx": 1752}
{"namespace": "pymc.backends.base._squeeze_cat", "completion": "    if combine:\n        results = np.concatenate(results)\n    if squeeze and len(results) == 1:\n        results = results[0]\n    return results", "idx": 1753}
{"namespace": "pymc.logprob.transforms.SimplexTransform.forward", "completion": "        value = pt.as_tensor(value)\n        k = value.shape[-1] + 1\n        value = pt.concatenate([value, pt.zeros((value.shape[:-1] + (1,)))], -1)\n        value = pt.log(value) - pt.log1mexp(pt.log(value)).sum(-1, keepdims=True)\n        return value - value.cumsum(-1) - pt.log(k)", "idx": 1754}
{"namespace": "pymc.logprob.transforms.SimplexTransform.backward", "completion": "        value = pt.as_tensor(value)\n        K = value.shape[-1] + 1\n        value = pt.concatenate([value, pt.zeros((value.shape[:-1] + (1,)))], -1)\n        value = pt.exp(value - pt.logsumexp(value, -1, keepdims=True))\n        sum_to_1 = pt.sum(value, -1, keepdims=True)\n        return value / sum_to_1", "idx": 1755}
{"namespace": "pymc.logprob.utils.walk_model", "completion": "\n    def expand(node):\n        if stop_at_vars and node in stop_at_vars:\n            return []\n        elif node.owner and isinstance(node.owner.op, MeasurableVariable) and not walk_past_rvs:\n            return []\n        else:\n            return node.owner.inputs if node.owner else []\n\n    return walk(graphs, expand_fn=expand_fn, expand_inputs_fn=expand)", "idx": 1756}
{"namespace": "sacred.metrics_logger.linearize_metrics", "completion": "    metrics_dict = {}\n    for metric in logged_metrics:\n        if metric.name not in metrics_dict:\n            metrics_dict[metric.name] = {\"steps\": [], \"values\": [], \"timestamps\": []}\n        metrics_dict[metric.name][\"steps\"].append(metric.step)\n        metrics_dict[metric.name][\"values\"].append(metric.value)\n        metrics_dict[metric.name][\"timestamps\"].append(metric.timestamp)\n    return metrics_dict", "idx": 1757}
{"namespace": "sacred.utils.set_by_dotted_path", "completion": "    keys = path.split('.')\n    for key in keys[:-1]:\n        d = d.setdefault(key, {})\n    d[keys[-1]] = value", "idx": 1758}
{"namespace": "sacred.utils.get_by_dotted_path", "completion": "    split_path = path.split(\".\")\n    current_option = d\n    for p in split_path:\n        if p not in current_option:\n            return default\n        current_option = current_option[p]\n    return current_option", "idx": 1759}
{"namespace": "pymc.logprob.scan.construct_scan", "completion": "    scan_op = Scan(scan_args.inner_inputs, scan_args.inner_outputs, scan_args.info, **kwargs)\n    scan_node = scan_op.make_node(*scan_args.outer_inputs)\n    return scan_node.outputs, scan_op.updates(scan_node)", "idx": 1760}
{"namespace": "sacred.utils.is_prefix", "completion": "    pre_path_parts = pre_path.split(\".\")\n    path_parts = path.split(\".\")\n    if len(pre_path_parts) > len(path_parts):\n        return False\n    return all(part1 == part2 for part1, part2 in zip(pre_path_parts, path_parts))", "idx": 1761}
{"namespace": "sacred.utils.get_inheritors", "completion": "    subclasses = set()\n    work = [cls]\n    while work:\n        parent = work.pop()\n        for child in parent.__subclasses__():\n            if child not in subclasses:\n                subclasses.add(child)\n                work.append(child)\n    return subclasses", "idx": 1762}
{"namespace": "sacred.utils.convert_camel_case_to_snake_case", "completion": "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()", "idx": 1763}
{"namespace": "sacred.utils.module_exists", "completion": "    return pkgutil.find_loader(modname) is not None", "idx": 1764}
{"namespace": "sacred.utils.apply_backspaces_and_linefeeds", "completion": "    lines = text.split('\\n')\n    for i in range(len(lines)):\n        while '\\b' in lines[i]:\n            j = lines[i].index('\\b')\n            lines[i] = lines[i][:j-1] + lines[i][j+1:]\n    return '\\n'.join(lines)", "idx": 1765}
{"namespace": "sacred.commands.help_for_command", "completion": "    # Retrieve the help text for the command\n    help_text = pydoc.getdoc(command)\n\n    # Remove any backspaces from the help text\n    help_text = re.sub('\\b', '', help_text)\n\n    return help_text", "idx": 1766}
{"namespace": "sacred.optional.optional_import", "completion": "    for package_name in package_names:\n        try:\n            package = importlib.import_module(package_name)\n            return True, package\n        except ImportError:\n            continue\n    return False, None", "idx": 1767}
{"namespace": "sacred.dependencies.get_py_file_if_possible", "completion": "    if pyc_name.endswith((\".py\", \".so\", \".pyd\", \".ipynb\")):\n        return pyc_name\n    py_name = pyc_name[:-1]  # remove the 'c' from '.pyc'\n    if os.path.exists(py_name):\n        return py_name\n    return pyc_name", "idx": 1768}
{"namespace": "sacred.config.custom_containers.DogmaticDict.update", "completion": "        if iterable is not None:\n            if hasattr(iterable, 'keys'):\n                for key in iterable:\n                    self[key] = iterable[key]\n            else:\n                for item in iterable:\n                    self[item] = iterable[item]\n        for key, value in kwargs.items():\n            self[key] = value", "idx": 1769}
{"namespace": "sacred.config.config_scope.is_empty_or_comment", "completion": "    line = line.strip()\n    return not line or line.startswith(\"#\")", "idx": 1770}
{"namespace": "boltons.funcutils.copy_function", "completion": "    if not isinstance(orig, FunctionType):\n        raise TypeError('expected orig to be a function, not %r' % (orig,))\n\n    fb = FunctionBuilder.from_func(orig)\n    fb.name = orig.__name__\n    fb.doc = orig.__doc__\n    fb.module = orig.__module__\n    fb.dict = getattr(orig, '__dict__', {}).copy() if copy_dict else {}\n    fb.filename = orig.__code__.co_filename\n    fb.code = orig.__code__\n\n    return fb.get_func()", "idx": 1771}
{"namespace": "sacred.config.config_scope.dedent_line", "completion": "    if line.startswith(indent):\n        return line[len(indent):]\n    else:\n        return line", "idx": 1772}
{"namespace": "boltons.funcutils.format_invocation", "completion": "    args = ', '.join(map(repr, args))\n    kwargs = ', '.join('%s=%r' % (k, v) for k, v in (kwargs or {}).items())\n    all_args = ', '.join(filter(bool, [args, kwargs]))\n    return '%s(%s)' % (name, all_args)", "idx": 1773}
{"namespace": "boltons.listutils.SplayList.shift", "completion": "        item = self.pop(item_index)\n        self.insert(dest_index, item)", "idx": 1774}
{"namespace": "boltons.strutils.gzip_bytes", "completion": "    out = StringIO()\n    with GzipFile(fileobj=out, mode=\"w\", compresslevel=level) as f:\n        f.write(bytestring)\n    return out.getvalue()", "idx": 1775}
{"namespace": "boltons.strutils.is_uuid", "completion": "    if isinstance(obj, uuid.UUID):\n        return obj.version == version if version else True\n    if not isinstance(obj, basestring):\n        return False\n    try:\n        parsed = uuid.UUID(obj)\n    except ValueError:\n        return False\n    else:\n        return parsed.version == version if version else True", "idx": 1776}
{"namespace": "boltons.strutils.parse_int_list", "completion": "    int_list = []\n    for group in range_string.split(delim):\n        if range_delim in group:\n            start, end = map(int, group.split(range_delim))\n            int_list.extend(range(start, end + 1))\n        else:\n            int_list.append(int(group))\n    return sorted(int_list)", "idx": 1777}
{"namespace": "boltons.cacheutils.ThresholdCounter.get", "completion": "        return self._count_map.get(key, [default])[0]", "idx": 1778}
{"namespace": "boltons.iterutils.backoff_iter", "completion": "    if count == 'repeat':\n        while True:\n            yield start\n            start *= factor\n            if start > stop:\n                start = stop\n    else:\n        for _ in range(count or int(math.log(stop / start, factor)) + 1):\n            yield start\n            start *= factor\n            if start > stop:\n                start = stop\n    if jitter:\n        if jitter == True:\n            jitter = 1.0\n        elif jitter < -1.0 or jitter > 1.0:\n            raise ValueError('jitter must be between -1.0 and 1.0')\n        while True:\n            interval = start * (1 + jitter * (random.random() - 0.5))\n            yield min(max(0, interval), stop)", "idx": 1779}
{"namespace": "boltons.cacheutils.cached", "completion": "    def cached_decorator(func):\n        return CachedFunction(func, cache, scoped=scoped, typed=typed, key=key)\n    return cached_decorator", "idx": 1780}
{"namespace": "boltons.timeutils.total_seconds", "completion": "    return (td.days * 24 * 60 * 60) + td.seconds + (td.microseconds / 1e6)", "idx": 1781}
{"namespace": "boltons.gcutils.get_all", "completion": "    instances = []\n    for obj in gc.get_objects():\n        if isinstance(obj, type_obj):\n            if include_subtypes or type(obj) is type_obj:\n                instances.append(obj)\n    return instances", "idx": 1782}
{"namespace": "boltons.timeutils.daterange", "completion": "    # Check if we have a date or datetime\n    if not isinstance(start, (date, datetime)):\n        raise TypeError(\"start must be a date or datetime instance\")\n    if stop is not None and not isinstance(stop, (date, datetime)):\n        raise TypeError(\"stop must be a date or datetime instance or None\")\n\n    # Check if we have a timedelta or need to create one\n    if isinstance(step, timedelta):\n        pass\n    elif isinstance(step, int):\n        step = timedelta(days=step)\n    elif isinstance(step, tuple):\n        step = timedelta(*step)\n    else:\n        raise TypeError(\"step must be a timedelta, int, or tuple\")\n\n    # Now, the actual logic of the function\n    current = start\n    if step.days > 0:\n        while stop is None or current < stop:\n            yield current\n            current = current + step\n            if inclusive and current == stop:\n                yield current\n    elif step.days < 0:\n        while stop is None or current > stop:\n            yield current\n            current = current + step\n            if inclusive and current == stop:\n                yield current\n    else:\n        raise ValueError(\"step must not be 0\")", "idx": 1783}
{"namespace": "boltons.mathutils.clamp", "completion": "    return max(lower, min(x, upper))", "idx": 1784}
{"namespace": "boltons.mathutils.ceil", "completion": "    if options is None:\n        return _ceil(x)\n    options = sorted(options)\n\n    i = bisect.bisect_left(options, x)\n    if i == len(options):\n        raise ValueError(\"no ceiling options greater than or equal to: %r\" % x)\n    return options[i]", "idx": 1785}
{"namespace": "boltons.formatutils.get_format_args", "completion": "    pos_args = []\n    named_args = []\n    formatter = Formatter()\n    for lit, fname, fspec, conv in formatter.parse(fstr):\n        if fname is None:\n            continue\n        field = BaseFormatField(fname, fspec, conv)\n        if field.is_positional:\n            pos_args.append((field.fname, field.type_func))\n        else:\n            named_args.append((field.fname, field.type_func))\n    return pos_args, named_args", "idx": 1786}
{"namespace": "boltons.mathutils.floor", "completion": "    if options is None:\n        return _floor(x)\n    options = sorted(options)\n    i = bisect.bisect_right(options, x)\n    if i:\n        return options[i-1]\n    raise ValueError(\"no floor options less than or equal to: %r\" % x)", "idx": 1787}
{"namespace": "boltons.dictutils.OneToOne.setdefault", "completion": "        if key not in self:\n            self[key] = default\n        return self[key]", "idx": 1788}
{"namespace": "boltons.dictutils.OneToOne.update", "completion": "        if isinstance(dict_or_iterable, dict):\n            for key, value in dict_or_iterable.items():\n                self[key] = value\n        else:\n            for key, value in dict_or_iterable:\n                self[key] = value\n\n        for key, value in kw.items():\n            self[key] = value", "idx": 1789}
{"namespace": "boltons.dictutils.ManyToMany.get", "completion": "        return self.data.get(key, default)", "idx": 1790}
{"namespace": "boltons.dictutils.FrozenDict.updated", "completion": "        if a:\n            if isinstance(a[0], dict):\n                a = a[0]\n            else:\n                a = dict(a)\n        else:\n            a = {}\n        a.update(kw)\n        return self.__class__(dict(self, **a))", "idx": 1791}
{"namespace": "boltons.dictutils.subdict", "completion": "    if keep is None:\n        keep = set(d.keys())\n    if drop is not None:\n        keep = keep - set(drop)\n    return {k: d[k] for k in keep if k in d}", "idx": 1792}
{"namespace": "boltons.dictutils.FrozenDict.__repr__", "completion": "        cn = self.__class__.__name__\n        dict_repr = dict.__repr__(self)\n        return \"%s(%s)\" % (cn, dict_repr)", "idx": 1793}
{"namespace": "gunicorn.config.validate_callable", "completion": "    def validator(val):\n        if isinstance(val, str):\n            module, obj = val.rsplit('.', 1)\n            try:\n                __import__(module)\n                val = getattr(sys.modules[module], obj)\n            except ImportError:\n                raise TypeError(\"Unable to import %s\" % module)\n            except AttributeError:\n                raise TypeError(\"Unable to find %s\" % obj)\n\n        if not callable(val):\n            raise TypeError(\"Setting is not callable: %s\" % val)\n\n        if arity != -1 and val.__code__.co_argcount != arity:\n            raise TypeError(\"The callable %s should accept %d arguments.\" % (val, arity))\n\n        return val\n\n    return validator", "idx": 1794}
{"namespace": "gunicorn.config.get_default_config_file", "completion": "    default_config_file = os.path.join(os.getcwd(), 'gunicorn.conf.py')\n    if os.path.exists(default_config_file):\n        return default_config_file\n    else:\n        return None", "idx": 1795}
{"namespace": "gunicorn.util.is_ipv6", "completion": "    try:\n        socket.inet_pton(socket.AF_INET6, addr)\n    except socket.error:  # not a valid address\n        return False\n\n    return True", "idx": 1796}
{"namespace": "gunicorn.systemd.listen_fds", "completion": "\n    listen_pid = os.environ.get('LISTEN_PID')\n    if listen_pid is None:\n        return 0\n\n    if int(listen_pid) != os.getpid():\n        return 0\n\n    listen_fds = os.environ.get('LISTEN_FDS')\n    if listen_fds is None:\n        return 0\n\n    if unset_environment:\n        os.environ.pop('LISTEN_PID', None)\n        os.environ.pop('LISTEN_FDS', None)\n\n    return int(listen_fds)", "idx": 1797}
{"namespace": "gunicorn.util.http_date", "completion": "    if timestamp is None:\n        timestamp = time.time()\n    return email.utils.formatdate(timestamp, usegmt=True)", "idx": 1798}
{"namespace": "gunicorn.util.parse_address", "completion": "    if netloc.startswith(\"unix:\"):\n        return netloc[5:], 0\n    elif netloc.startswith(\"fd://\"):\n        fd = netloc[5:]\n        if not fd.isdigit():\n            raise RuntimeError(\"%s is not a valid file descriptor.\" % fd)\n        return int(fd), 0\n\n    if netloc.isdigit():\n        return '', int(netloc)\n\n    if ':' in netloc:\n        host, port = netloc.rsplit(':', 1)\n        if not port.isdigit():\n            raise RuntimeError(\"%s is not a valid port number.\" % port)\n        port = int(port)\n    else:\n        host = netloc\n        port = int(default_port)\n\n    # Check if the host is an IPv6 address.\n    if is_ipv6(host):\n        host = \"[%s]\" % host\n\n    return host, port", "idx": 1799}
{"namespace": "gunicorn.util.to_bytestring", "completion": "    if isinstance(value, bytes):\n        return value\n    return value.encode(encoding)", "idx": 1800}
{"namespace": "gunicorn.util.warn", "completion": "    sys.stderr.write(\"WARNING: %s\\n\" % msg)", "idx": 1801}
{"namespace": "gunicorn.util.split_request_uri", "completion": "    if uri.startswith(\"//\"):\n        uri = \".\" + uri\n    return urllib.parse.urlsplit(uri)", "idx": 1802}
{"namespace": "praw.models.listing.listing.ModNoteListing.after", "completion": "        if not getattr(self, \"has_next_page\", False):\n            return None\n        return getattr(self, \"end_cursor\", None)", "idx": 1803}
{"namespace": "praw.models.util.permissions_string", "completion": "    if permissions is None:\n        return \"+all\"\n    prefixed = any(p.startswith((\"+\", \"-\")) for p in permissions)\n    if prefixed:\n        return \",\".join(permissions)\n    additions = set(permissions)\n    removals = known_permissions - additions\n    changes = [f\"+{p}\" for p in additions] + [f\"-{p}\" for p in removals]\n    return \",\".join(changes)", "idx": 1804}
{"namespace": "jc.cli.JcCli.json_out", "completion": "        if self.pretty:\n            self.json_indent = 2\n            self.json_separators = (',', ': ')\n\n        j_string = json.dumps(\n            self.data_out,\n            ensure_ascii=self.ascii_only,\n            separators=self.json_separators,\n            indent=self.json_indent\n        )\n\n        if not self.mono:\n            class JcStyle(Style):\n                styles: CustomColorType = self.custom_colors\n\n            return str(highlight(j_string, JsonLexer(), Terminal256Formatter(style=JcStyle))[0:-1])\n\n        return j_string", "idx": 1805}
{"namespace": "pythonforandroid.pythonpackage.transform_dep_for_pip", "completion": "\n    if \"@\" in dependency and \"://\" in dependency:\n        # This is a URL dependency. Transform it to a format that pip can install.\n        url_parts = dependency.split(\"@\")\n        return url_parts[0] + \"@\" + \"http://\" + url_parts[1]\n    else:\n        # This is not a URL dependency. Return it as is.\n        return dependency", "idx": 1806}
{"namespace": "pythonforandroid.graph.fix_deplist", "completion": "    fixed_deps = []\n    for dep in deps:\n        if isinstance(dep, str):\n            fixed_deps.append((dep.lower(),))\n        else:\n            fixed_deps.append(tuple([d.lower() for d in dep]))\n    return fixed_deps", "idx": 1807}
{"namespace": "pythonforandroid.util.walk_valid_filens", "completion": "    for dirpath, dirnames, filenames in walk(base_dir):\n        dirnames[:] = [d for d in dirnames if d not in invalid_dir_names]\n        for filename in filenames:\n            full_path = join(dirpath, filename)\n            if not any(fnmatch(full_path, pattern) for pattern in invalid_file_patterns):\n                yield full_path", "idx": 1808}
{"namespace": "pythonforandroid.bootstrap._cmp_bootstraps_by_priority", "completion": "\n    # Get the priority of the bootstraps\n    a_priority = default_recipe_priorities.index(a.name) if a.name in default_recipe_priorities else -1\n    b_priority = default_recipe_priorities.index(b.name) if b.name in default_recipe_priorities else -1\n\n    # Compare the priority of the bootstraps\n    if a_priority != b_priority:\n        return a_priority - b_priority\n\n    # If the priority is the same, compare the names of the bootstraps\n    return (a.name > b.name) - (a.name < b.name)", "idx": 1809}
{"namespace": "pythonforandroid.bootstrap.Bootstrap.all_bootstraps", "completion": "        bootstraps_dir = join(cls.ctx.root_dir, 'bootstraps')\n        return set(\n            basename(d) for d in glob.glob(join(bootstraps_dir, '*'))\n            if isdir(d)\n        )", "idx": 1810}
{"namespace": "mmcv.image.colorspace._convert_input_type_range", "completion": "    if img.dtype == np.uint8:\n        img = img.astype(np.float32) / 255.\n    elif img.dtype != np.float32:\n        raise TypeError('The img should be np.float32 or np.uint8, '\n                        f'but got {img.dtype}')\n    return img", "idx": 1811}
{"namespace": "mackup.utils.error", "completion": "    sys.exit(\"Error: {}\".format(message))", "idx": 1812}
{"namespace": "mmcv.image.colorspace._convert_output_type_range", "completion": "    if dst_type == np.uint8:\n        img = np.round(img)\n        img = img.astype(np.uint8)\n    elif dst_type == np.float32:\n        img /= 255.\n    else:\n        raise TypeError('The dst_type should be np.float32 or np.uint8, '\n                        f'but got {dst_type}')\n    return img", "idx": 1813}
{"namespace": "mackup.utils.is_process_running", "completion": "    try:\n        output = subprocess.check_output(\"pgrep -f \" + process_name, shell=True)\n        if output.strip():\n            return True\n        else:\n            return False\n    except Exception as e:\n        return False", "idx": 1814}
{"namespace": "stellar.operations._get_pid_column", "completion": "    server_version = raw_conn.execute('SHOW server_version').fetchone()[0]\n    version_number = re.search(r'\\d+', server_version).group()\n    if int(version_number) >= 90200:\n        return 'pg_stat_activity.pid'\n    else:\n        return 'pg_stat_activity.procpid'", "idx": 1815}
{"namespace": "imapclient.imap_utf7.encode", "completion": "    if not isinstance(s, str):\n        return s\n\n    res = []\n    buffer = []\n    for c in s:\n        ord_c = ord(c)\n        # ASCII printable char\n        if 0x20 <= ord_c <= 0x7e:\n            # Flush the buffer\n            if buffer:\n                res.append(b\"&\" + base64_utf7_encode(buffer) + b\"-\")\n                buffer = []\n            # Special case for '&'\n            if ord_c == AMPERSAND_ORD:\n                res.append(b\"&-\")\n            else:\n                res.append(bytes([ord_c]))\n        # Non-ASCII char\n        else:\n            buffer.append(c)\n\n    # Flush the remaining buffer if any\n    if buffer:\n        res.append(b\"&\" + base64_utf7_encode(buffer) + b\"-\")\n\n    return b\"\".join(res)", "idx": 1816}
{"namespace": "imapclient.version._imapclient_version_string", "completion": "\n    major, minor, micro, releaselevel = vinfo\n    version_string = f\"{major}.{minor}.{micro}\"\n    if releaselevel != \"final\":\n        version_string += f\"-{releaselevel}\"\n    return version_string", "idx": 1817}
{"namespace": "telethon.helpers.generate_key_data_from_nonce", "completion": "    server_nonce = int.to_bytes(server_nonce, length=8, byteorder='little', signed=True)\n    new_nonce = int.to_bytes(new_nonce, length=16, byteorder='little', signed=True)\n\n    # Prepare our parameters\n    tmp_aes_key = new_nonce + server_nonce\n    tmp_aes_iv = server_nonce + new_nonce[0:8]\n\n    # Hashes\n    hash1 = sha1(new_nonce + server_nonce).digest()\n    hash2 = sha1(server_nonce + new_nonce).digest()\n    hash3 = sha1(new_nonce + new_nonce).digest()\n\n    # Prepare the final values\n    key = hash1 + hash2[0:12]\n    iv = hash2[12:20] + hash3 + new_nonce[0:4]\n\n    return key, iv", "idx": 1818}
{"namespace": "hbmqtt.codecs.bytes_to_int", "completion": "    return int.from_bytes(data, byteorder='big')", "idx": 1819}
{"namespace": "zulipterminal.helper.display_error_if_present", "completion": "    if 'result' in response and response['result'] == 'error':\n        if hasattr(controller, 'view'):\n            controller.view.set_footer_text(response['msg'], 3)\n        else:\n            print(response['msg'])", "idx": 1820}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton._decode_message_id", "completion": "        try:\n            return int(message_id)\n        except ValueError:\n            return None", "idx": 1821}
{"namespace": "zulipterminal.ui_tools.buttons.MessageLinkButton.handle_narrow_link", "completion": "        parsed_link = self._parse_narrow_link(self.link)\n        validation_error = self._validate_narrow_link(parsed_link)\n\n        if validation_error:\n            self.view.set_footer_text([(\"error\", validation_error)], duration=3)\n        else:\n            self._switch_narrow_to(parsed_link)", "idx": 1822}
{"namespace": "zulipterminal.config.color.color_properties", "completion": "    class ColorWithProperties(Enum):\n        for color in colors:\n            locals()[color.name] = f\"{color.value} {', '.join(prop)}\"\n    return ColorWithProperties", "idx": 1823}
{"namespace": "twilio.base.deserialize.decimal", "completion": "    if d is None:\n        return d\n    try:\n        return Decimal(d, context=BasicContext)\n    except (TypeError, ValueError):\n        return d", "idx": 1824}
{"namespace": "twilio.base.deserialize.integer", "completion": "    try:\n        return int(i)\n    except ValueError:\n        return i", "idx": 1825}
{"namespace": "twilio.base.serialize.object", "completion": "    try:\n        return json.dumps(obj)\n    except (TypeError, ValueError):\n        return obj", "idx": 1826}
{"namespace": "twilio.base.serialize.map", "completion": "    return [serialize_func(item) for item in lst]", "idx": 1827}
{"namespace": "twilio.base.obsolete.deprecated_method", "completion": "    def deprecated_method_wrapper(old_func):\n        @functools.wraps(old_func)\n        def function_wrapper(*args, **kwargs):\n            warnings.warn(\n                \"{} is a deprecated method. Use {} instead.\".format(old_func.__name__, new_func.__name__ if new_func else \"the appropriate method\"),\n                category=DeprecationWarning,\n                stacklevel=2\n            )\n            if new_func:\n                return new_func(*args, **kwargs)\n            else:\n                return old_func(*args, **kwargs)\n        return function_wrapper\n    return deprecated_method_wrapper", "idx": 1828}
{"namespace": "chatette.utils.sample_indulgent", "completion": "    if nb_items > len(array):\n        return deepcopy(array)\n    else:\n        return sample(array, nb_items)", "idx": 1829}
{"namespace": "chatette.utils.rchop", "completion": "    if string.endswith(ending):\n        return string[:-len(ending)]\n    return string", "idx": 1830}
{"namespace": "chatette.utils.str_to_bool", "completion": "    if text == 'True':\n        return True\n    elif text == 'False':\n        return False\n    else:\n        raise ValueError(\"Invalid boolean string: \" + text)", "idx": 1831}
{"namespace": "chatette.utils.min_if_exist", "completion": "    if n1 is not None and n2 is not None:\n        return min(n1, n2)\n    elif n1 is not None:\n        return n1\n    elif n2 is not None:\n        return n2\n    else:\n        return None", "idx": 1832}
{"namespace": "chatette.utils.append_to_list_in_dict", "completion": "    if key not in dict_of_lists:\n        dict_of_lists[key] = [value]\n    else:\n        dict_of_lists[key].append(value)", "idx": 1833}
{"namespace": "chatette.utils.extend_list_in_dict", "completion": "    if key not in dict_of_lists:\n        dict_of_lists[key] = values\n    else:\n        dict_of_lists[key].extend(values)", "idx": 1834}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy._is_end_regex", "completion": "        return re.search(r'\\/(g?i?|i?g?)$', word) is not None", "idx": 1835}
{"namespace": "chatette.cli.interactive_commands.command_strategy.CommandStrategy.execute", "completion": "        try:\n            unit_type = CommandStrategy.get_unit_type_from_str(self.command_tokens[0])\n            if unit_type is None:\n                self.print_wrapper.error_log(\n                    \"Unknown unit type: '\" + self.command_tokens[0] + \"'.\"\n                )\n                return\n\n            unit_name = CommandStrategy.remove_quotes(self.command_tokens[1])\n            regex = self.get_regex_name(unit_name)\n            if regex is not None:\n                for unit_name in self.next_matching_unit_name(unit_type, regex):\n                    self.execute_on_unit(unit_type, unit_name)\n            else:\n                exact_unit_name = CommandStrategy.split_exact_unit_name(unit_name)\n                self.execute_on_unit(unit_type, exact_unit_name[0], exact_unit_name[1])\n        except IndexError:\n            self.print_wrapper.error_log(\n                \"Some arguments ('<unit-type>' and '<unit-name>') are missing \" +\n                \"to the command '\" + self.command_tokens[0] + \"'.\"\n            )\n        except SyntaxError as e:\n            self.print_wrapper.error_log(str(e))\n        finally:\n            self.finish_execution()", "idx": 1836}
{"namespace": "aioxmpp.network.group_and_order_srv_records", "completion": "    if rng is None:\n        rng = random\n\n    # Group by priority\n    by_prio = itertools.groupby(\n        sorted(all_records, key=lambda item: item[0]),\n        key=lambda item: item[0]\n    )\n\n    for prio, group in by_prio:\n        group = list(group)\n\n        # Calculate total weight\n        total_weight = sum(item[1] for item in group)\n\n        while group:\n            # Select a random number\n            selected = rng.uniform(0, total_weight)\n            for item in group:\n                selected -= item[1]\n                if selected <= 0:\n                    break\n\n            # Remove the selected item from the group\n            group.remove(item)\n            total_weight -= item[1]\n\n            # Yield the hostname and port\n            yield item[2]", "idx": 1837}
{"namespace": "aioxmpp.nonza.StreamFeatures.get_feature", "completion": "        try:\n            return self.features[feature_cls.TAG][0]\n        except IndexError:\n            return default", "idx": 1838}
{"namespace": "aioxmpp.connector.XMPPOverTLSConnector._context_factory_factory", "completion": "\n        def context_factory():\n            ssl_context = metadata.ssl_context_factory()\n            if hasattr(ssl_context, \"set_alpn_protocols\"):\n                ssl_context.set_alpn_protocols([\"xmpp-client\"])\n            verifier.setup_context(ssl_context)\n            return ssl_context\n\n        return context_factory", "idx": 1839}
{"namespace": "aioxmpp.xmltestutils.element_path", "completion": "    path = []\n    while el is not upto:\n        parent = el.getparent()\n        if parent is None:\n            break\n        path.insert(0, \"{}/{}\".format(parent.tag, parent.index(el)))\n        el = parent\n    return \"/\".join(path)", "idx": 1840}
{"namespace": "aioxmpp.structs.JID.fromstr", "completion": "        localpart, sep, remainder = s.partition(\"@\")\n        if not sep:\n            localpart = None\n            domain, sep, resource = remainder.partition(\"/\")\n        else:\n            domain, sep, resource = remainder.partition(\"/\")\n            if not strict:\n                localpart = jid_unescape(localpart)\n\n        if not sep:\n            resource = None\n\n        return cls(localpart=localpart, domain=domain, resource=resource, strict=strict)", "idx": 1841}
{"namespace": "aioxmpp.security_layer.extract_python_dict_from_x509", "completion": "    cert_dict = {}\n    subject = x509.get_subject()\n    for name, value in subject.get_components():\n        cert_dict[name.decode()] = value.decode()\n\n    # Extracting subjectAltName\n    alt_names = []\n    for i in range(x509.get_extension_count()):\n        ext = x509.get_extension(i)\n        if 'subjectAltName' in str(ext.get_short_name()):\n            alt_names = [e.strip() for e in str(ext).split(',')]\n    if alt_names:\n        cert_dict['subjectAltName'] = alt_names\n\n    return cert_dict", "idx": 1842}
{"namespace": "aioxmpp.security_layer.extract_blob", "completion": "    return OpenSSL.crypto.dump_certificate(OpenSSL.crypto.FILETYPE_ASN1, x509)", "idx": 1843}
{"namespace": "aioxmpp.security_layer.blob_to_pyasn1", "completion": "    return pyasn1.codec.der.decoder.decode(blob, asn1Spec=pyasn1_modules.rfc2459.Certificate())[0]", "idx": 1844}
{"namespace": "aioxmpp.security_layer.extract_pk_blob_from_pyasn1", "completion": "    tbs_certificate = pyasn1_struct.getComponentByName(\"tbsCertificate\")\n    subject_public_key_info = tbs_certificate.getComponentByName(\"subjectPublicKeyInfo\")\n    return pyasn1.codec.der.encoder.encode(subject_public_key_info)", "idx": 1845}
{"namespace": "aioxmpp.callbacks.AdHocSignal.ASYNC_WITH_LOOP", "completion": "        loop = asyncio.get_event_loop() if loop is None else loop\n\n        def async_wrapper(f):\n            if not hasattr(f, \"__call__\"):\n                raise TypeError(\"must be callable, got {!r}\".format(f))\n\n            def wrapper(args, kwargs):\n                if kwargs:\n                    loop.call_soon(functools.partial(f, *args, **kwargs))\n                else:\n                    loop.call_soon(f, *args)\n                return True\n\n            return wrapper\n\n        return async_wrapper", "idx": 1846}
{"namespace": "aioxmpp.callbacks.AdHocSignal.SPAWN_WITH_LOOP", "completion": "        if loop is None:\n            loop = asyncio.get_event_loop()\n\n        def create_wrapper(f):\n            if not asyncio.iscoroutinefunction(f):\n                raise TypeError(\"must be a coroutine function, got {!r}\".format(f))\n            return functools.partial(cls._spawn_wrapper, f, loop)\n\n        return create_wrapper", "idx": 1847}
{"namespace": "aioxmpp.callbacks.first_signal", "completion": "    futures = [signal.future() for signal in signals]\n    done, pending = await asyncio.wait(futures, return_when=asyncio.FIRST_COMPLETED)\n    for future in pending:\n        future.cancel()\n    result = done.pop().result()\n    if isinstance(result, Exception):\n        raise result\n    return result", "idx": 1848}
{"namespace": "aioxmpp.tasks.TaskPool.spawn", "completion": "        # Check if the limit on any of the groups or the total limit is exhausted\n        for group in __groups:\n            if self.get_task_count(group) >= self.get_limit(group):\n                raise RuntimeError(\"Limit on group or total limit is exhausted\")\n\n        # Start a new coroutine\n        coro = __coro_fun(*args, **kwargs)\n        task = asyncio.create_task(coro)\n\n        # Add the running coroutine to the given pool groups\n        self.add(__groups, task)\n\n        return task", "idx": 1849}
{"namespace": "aioxmpp.protocol.send_and_wait_for", "completion": "\n    # Send the message\n    xmlstream.send_xso(send)\n\n    # Create a future for the response\n    fut = xmlstream.features_future()\n\n    # Set a timeout for the response\n    if timeout is not None:\n        try:\n            result = await asyncio.wait_for(fut, timeout=timeout)\n        except asyncio.TimeoutError:\n            raise TimeoutError from None\n    else:\n        result = await fut\n\n    # If a callback function is provided, call it with the result\n    if cb is not None:\n        cb(result)\n\n    return result", "idx": 1850}
{"namespace": "aioxmpp.testutils.run_coroutine_with_peer", "completion": "    if not loop:\n        loop = asyncio.get_event_loop()\n\n    local_future = asyncio.ensure_future(coroutine, loop=loop)\n    peer_future = asyncio.ensure_future(peer_coroutine, loop=loop)\n\n    done, pending = loop.run_until_complete(\n        asyncio.wait(\n            [local_future, peer_future],\n            timeout=timeout,\n            loop=loop))\n\n    if pending:\n        for future in pending:\n            future.cancel()\n        raise asyncio.TimeoutError()\n\n    if local_future.exception():\n        raise local_future.exception()\n\n    return local_future.result()", "idx": 1851}
{"namespace": "aioxmpp.testutils.make_listener", "completion": "    listener = unittest.mock.Mock()\n    for attr in dir(instance):\n        if isinstance(getattr(instance, attr), callbacks.Signal):\n            getattr(instance, attr).connect(\n                getattr(listener, attr)\n            )\n    return listener", "idx": 1852}
{"namespace": "aioxmpp.vcard.service.VCardService.set_vcard", "completion": "\n        iq = aioxmpp.IQ(\n            type_=aioxmpp.IQType.SET,\n            to=jid,\n            payload=vcard,\n        )\n\n        try:\n            await self.client.send(iq)\n        except aioxmpp.XMPPCancelError as e:\n            if e.condition in (\n                    aioxmpp.ErrorCondition.FEATURE_NOT_IMPLEMENTED,\n                    aioxmpp.ErrorCondition.ITEM_NOT_FOUND):\n                raise\n            else:\n                raise", "idx": 1853}
{"namespace": "aioxmpp.rsm.xso.ResultSetMetadata.limit", "completion": "        result = type(self)()\n        result.max_ = max_\n        return result", "idx": 1854}
{"namespace": "aioxmpp.muc.service.Room.features", "completion": "        return {\n            namespaces.xep0045_muc,\n            namespaces.xep0045_muc_user,\n            namespaces.xep0045_muc_admin,\n            namespaces.xep0045_muc_owner,\n            namespaces.xep0045_muc_config,\n            namespaces.xep0045_muc_register,\n            namespaces.xep0045_muc_unique,\n            namespaces.xep0045_muc_rooms,\n        }", "idx": 1855}
{"namespace": "aioxmpp.xso.query.EvaluationContext.eval_bool", "completion": "        result = self.eval(expr)\n        return any(result)", "idx": 1856}
{"namespace": "aioxmpp.xso.query._BoolOpMixin.eval", "completion": "        return [self.eval_leaf(ec)]", "idx": 1857}
{"namespace": "aioxmpp.xso.model.drop_handler", "completion": "    depth = 1\n    while depth > 0:\n        ev_type, *_ = yield\n        if ev_type == \"start\":\n            depth += 1\n        elif ev_type == \"end\":\n            depth -= 1", "idx": 1858}
{"namespace": "aioxmpp.xso.model.guard", "completion": "    depth = 1\n    try:\n        yield from dest.send(ev_args)\n        while depth:\n            ev_type, *ev_args = yield\n            if ev_type == \"start\":\n                depth += 1\n            elif ev_type == \"end\":\n                depth -= 1\n            yield from dest.send((ev_type, *ev_args))\n    except Exception as exc:\n        if depth != 0:\n            raise ValueError(\"unexpected depth at end of guarded block\")\n        raise exc\n    if depth != 0:\n        raise ValueError(\"unexpected depth at end of guarded block\")\n    return dest.value", "idx": 1859}
{"namespace": "aioxmpp.xso.model.capture_events", "completion": "    try:\n        while True:\n            event = yield\n            dest.append(event)\n            receiver.send(event)\n    except GeneratorExit:\n        dest.clear()\n        receiver.close()\n    except Exception:\n        dest.clear()\n        raise", "idx": 1860}
{"namespace": "aioxmpp.xso.model.events_to_sax", "completion": "    for ev_type, *ev_args in events:\n        if ev_type == \"start\":\n            dest.startElementNS(*ev_args)\n        elif ev_type == \"end\":\n            dest.endElementNS(*ev_args)\n        elif ev_type == \"text\":\n            dest.characters(*ev_args)\n        elif ev_type == \"startPrefixMapping\":\n            dest.startPrefixMapping(*ev_args)\n        elif ev_type == \"endPrefixMapping\":\n            dest.endPrefixMapping(*ev_args)\n        else:\n            raise ValueError(\"Unknown event type: {}\".format(ev_type))", "idx": 1861}
{"namespace": "aioxmpp.adhoc.service.AdHocClient.get_command_info", "completion": "\n        disco = self.dependencies[aioxmpp.disco.DiscoClient]\n        response = await disco.query_info(\n            peer_jid,\n            node=command_name,\n        )\n        return response", "idx": 1862}
{"namespace": "aioxmpp.entitycaps.caps115.build_identities_string", "completion": "    identities = list(\n        escape(\"{category}/{type}/{lang}/{name}\".format(\n            category=identity.category,\n            type=identity.type_,\n            lang=identity.lang or \"\",\n            name=identity.name or \"\",\n        )).encode(\"utf-8\")\n        for identity in identities\n    )\n\n    if len(set(identities)) != len(identities):\n        raise ValueError(\"duplicate identity\")\n\n    identities.sort()\n    identities.append(b\"\")\n    return b\"<\".join(identities)", "idx": 1863}
{"namespace": "aioxmpp.entitycaps.caps115.build_features_string", "completion": "    features = [escape(feature).encode(\"utf-8\") for feature in features]\n\n    if len(set(features)) != len(features):\n        raise ValueError(\"duplicate feature\")\n\n    features.sort()\n    features.append(b\"\")\n    return b\"<\".join(features)", "idx": 1864}
{"namespace": "aioxmpp.entitycaps.caps115.build_forms_string", "completion": "    forms = [\n        b\"/\".join([\n            escape(form.type).encode(\"utf-8\"),\n            escape(form.title).encode(\"utf-8\"),\n            escape(str(form.instructions or \"\")).encode(\"utf-8\"),\n            escape(form.fields or \"\").encode(\"utf-8\"),\n        ])\n        for form in forms\n    ]\n\n    if len(set(forms)) != len(forms):\n        raise ValueError(\"duplicate form\")\n\n    forms.sort()\n    forms.append(b\"\")\n    return b\"<\".join(forms)", "idx": 1865}
{"namespace": "aioxmpp.entitycaps.caps115.Key.path", "completion": "        quoted_node = urllib.parse.quote(self.node, safe=\"\")\n        return pathlib.Path(\"hashes\") / self.algo / quoted_node", "idx": 1866}
{"namespace": "aioxmpp.entitycaps.caps390._process_features", "completion": "    parts = [\n        (feature or \"\").encode(\"utf-8\") + b\"\\x1f\"\n        for feature in features\n    ]\n    parts.sort()\n    return b\"\".join(parts) + b\"\\x1e\"", "idx": 1867}
{"namespace": "aioxmpp.entitycaps.caps390._process_identities", "completion": "    parts = [\n        _process_identity(identity)\n        for identity in identities\n    ]\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1d\"", "idx": 1868}
{"namespace": "aioxmpp.entitycaps.caps390._process_extensions", "completion": "    parts = [\n        _process_form(form)\n        for form in exts\n    ]\n    parts.sort()\n    return b\"\".join(parts)+b\"\\x1c\"", "idx": 1869}
{"namespace": "aioxmpp.entitycaps.caps390._calculate_hash", "completion": "    hash_func = aioxmpp.hashes.get_hash_by_algo(algo)\n    hash_func.update(hash_input)\n    return hash_func.digest()", "idx": 1870}
{"namespace": "aioxmpp.entitycaps.caps390.Key.node", "completion": "        encoded = base64.b32encode(\n            self.digest\n        ).decode(\"ascii\").rstrip(\"=\").lower()\n        return \"urn:xmpp:hashes:2.{}:{}\".format(self.algo, encoded)", "idx": 1871}
{"namespace": "aioxmpp.entitycaps.caps390.Key.path", "completion": "        encoded_digest = base64.urlsafe_b64encode(self.digest).decode(\"ascii\")\n        return pathlib.Path(\"{}/{}.bin\".format(self.algo, encoded_digest))", "idx": 1872}
{"namespace": "aioxmpp.entitycaps.caps390.Implementation.extract_keys", "completion": "        if presence.xep0390_caps is not None:\n            for algo, digest in presence.xep0390_caps.digests.items():\n                yield Key(algo, digest)\n        else:\n            return iter(())", "idx": 1873}
